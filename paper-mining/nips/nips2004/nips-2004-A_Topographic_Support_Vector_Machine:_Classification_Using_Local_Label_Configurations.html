<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>14 nips-2004-A Topographic Support Vector Machine: Classification Using Local Label Configurations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-14" href="#">nips2004-14</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>14 nips-2004-A Topographic Support Vector Machine: Classification Using Local Label Configurations</h1>
<br/><p>Source: <a title="nips-2004-14-pdf" href="http://papers.nips.cc/paper/2653-a-topographic-support-vector-machine-classification-using-local-label-configurations.pdf">pdf</a></p><p>Author: Johannes Mohr, Klaus Obermayer</p><p>Abstract: The standard approach to the classiﬁcation of objects is to consider the examples as independent and identically distributed (iid). In many real world settings, however, this assumption is not valid, because a topographical relationship exists between the objects. In this contribution we consider the special case of image segmentation, where the objects are pixels and where the underlying topography is a 2D regular rectangular grid. We introduce a classiﬁcation method which not only uses measured vectorial feature information but also the label conﬁguration within a topographic neighborhood. Due to the resulting dependence between the labels of neighboring pixels, a collective classiﬁcation of a set of pixels becomes necessary. We propose a new method called ’Topographic Support Vector Machine’ (TSVM), which is based on a topographic kernel and a self-consistent solution to the label assignment shown to be equivalent to a recurrent neural network. The performance of the algorithm is compared to a conventional SVM on a cell image segmentation task. 1</p><p>Reference: <a title="nips-2004-14-reference" href="../nips2004_reference/nips-2004-A_Topographic_Support_Vector_Machine%3A_Classification_Using_Local_Label_Configurations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this contribution we consider the special case of image segmentation, where the objects are pixels and where the underlying topography is a 2D regular rectangular grid. [sent-7, score-0.13]
</p><p>2 We introduce a classiﬁcation method which not only uses measured vectorial feature information but also the label conﬁguration within a topographic neighborhood. [sent-8, score-0.485]
</p><p>3 Due to the resulting dependence between the labels of neighboring pixels, a collective classiﬁcation of a set of pixels becomes necessary. [sent-9, score-0.238]
</p><p>4 We propose a new method called ’Topographic Support Vector Machine’ (TSVM), which is based on a topographic kernel and a self-consistent solution to the label assignment shown to be equivalent to a recurrent neural network. [sent-10, score-0.65]
</p><p>5 The performance of the algorithm is compared to a conventional SVM on a cell image segmentation task. [sent-11, score-0.228]
</p><p>6 1  Introduction  The segmentation of natural images into semantically meaningful subdivisions can be considered as one or more binary pixel classiﬁcation problems, where two classes of pixels are characterized by some measurement data (features). [sent-12, score-0.388]
</p><p>7 For each binary problem the task is to assign a set of new pixels to one of the two classes using a classiﬁer trained on a set of labeled pixels (training data). [sent-13, score-0.157]
</p><p>8 In conventional classiﬁcation approaches usually the assumption of iid examples is made, so the classiﬁcation result is determined solely by the measurement data. [sent-14, score-0.187]
</p><p>9 Natural images, however, possess a topographic structure, in which there are dependencies between the labels of topographic neighbors, making the data non-iid. [sent-15, score-0.78]
</p><p>10 Therefore, not only the measurement data, but also the labels of the topographic neighbors can be used in the classiﬁcation of a pixel. [sent-16, score-0.554]
</p><p>11 Combining this idea with local discriminative models, in [2] a discriminative random ﬁeld was used to model the dependencies between the labels of image blocks in a probabilistic framework. [sent-19, score-0.191]
</p><p>12 A collective classiﬁcation relational dependency network was used in [3] for movie box-ofﬁce receipts prediction and paper topic classiﬁcation. [sent-20, score-0.156]
</p><p>13 The maximization of the per label margin of pairwise Markov networks was applied in [4] to handwritten character recognition and collective hypertext classiﬁcation. [sent-21, score-0.274]
</p><p>14 In this work, we propose a method which is also based on margin maximization and allows the collective assignments of a large number of binary labels which have a regular grid topography. [sent-23, score-0.299]
</p><p>15 The method called topographic support vector machine (TSVM) is based on the assumption that knowledge about the local label conﬁguration can improve the classiﬁcation of a single data point. [sent-25, score-0.557]
</p><p>16 Consider as example the segmentation of a collection of images depicting physical objects of similar shape, but high variability in gray level and texture. [sent-26, score-0.186]
</p><p>17 In this case, the measurements are dissimilar, while the local label conﬁgurations show high similarity. [sent-27, score-0.187]
</p><p>18 Here, we apply the TSVM to the supervised bottom-up segmentation of microscopic images of Papanicolaou stained cervical cell nuclei from the CSSIP pap smear dataset 1 . [sent-28, score-0.421]
</p><p>19 Segmentation of these images is important for the detection of cervical cancer or precancerous cells. [sent-29, score-0.165]
</p><p>20 A previously used bottom-up segmentation approach for this data using morphological watersheds was reported to have difﬁculties with weak gradients and the presence of other large gradients adjacent to the target [5]. [sent-33, score-0.125]
</p><p>21 , on } be a set of n sites on a 2D pixel-grid and G = {Go , o ∈ O} be a neighborhood system for O, where Go is the set of neighbors of o and neighborhood is deﬁned by o ∈ Go and o ∈ Gp ⇔ p ∈ Go . [sent-38, score-0.281]
</p><p>22 For each pixel site oi from the set O, a binary label yi ∈ {−1, +1} giving the class assignment is assumed to be known. [sent-39, score-0.322]
</p><p>23 We deﬁne the neighborhood of order c as G c = {Gi , i ∈ O}; Gi = {k ∈ O : 0 < (k−i)2 ≤ c}. [sent-41, score-0.088]
</p><p>24 This way, G 1 describes the ﬁrst order neighborhood system (4 neighbors), G 2 the second order system (8 neighbors), and so on. [sent-42, score-0.088]
</p><p>25 Each pixel site is characterized by some measurement vector. [sent-43, score-0.207]
</p><p>26 This could for example be the vector of gray value intensities at a pixel site, the gray value patch around a central pixel location, or the responses to a bank of linear or nonlinear ﬁlters (e. [sent-44, score-0.264]
</p><p>27 Using a training set composed of (possibly several) sets of pixel sites, each accompanied by a set of measurement vectors X = {xi , ∀i ∈ [1. [sent-47, score-0.228]
</p><p>28 n]} and a set of 1  Centre for Sensor Signal and Information Processing, University of Queensland  labels Y = {yi , ∀i ∈ [1. [sent-49, score-0.083]
</p><p>29 a manually labeled image), the task of classiﬁcation is to assign class labels to a set of κ pixels sites U = {u1 , . [sent-53, score-0.219]
</p><p>30 For the classiﬁcation we x will use a support vector machine. [sent-59, score-0.072]
</p><p>31 1  Support Vector Classiﬁcation  In Support Vector Classiﬁcation (SVC) methods ([7]), a kernel is used to solve a complex classiﬁcation task in a usually high-dimensional feature space via a separating hyperplane. [sent-61, score-0.101]
</p><p>32 In practice, the optimal margin hyperplane can be obtained solving a quadratic programming problem. [sent-63, score-0.053]
</p><p>33 Several schemes have been introduced to deal with noisy measurements via the introduction of slack variables. [sent-64, score-0.054]
</p><p>34 In order to classify a new object h with unknown label, the following decision rule is evaluated: m  f (xh ) = sgn  αi yi K(xh , xi ) + b ,  (2)  i=1  where the sum runs over all m support vectors. [sent-70, score-0.117]
</p><p>35 2  Topographic Kernel  We now assume that the label of each pixel is determined by both the measurement and the set of labels of its topographic neighbors. [sent-72, score-0.741]
</p><p>36 We deﬁne a vector yGh where the labels of the q topographic neighbors of the pixel h are concatenated in an arbitrary, but ﬁxed order. [sent-73, score-0.594]
</p><p>37 We propose a support vector classiﬁer using an extended kernel, which in addition to the measurement vector xh , also includes the vector yGh : K(xh , xj , yGh , yGj ) = K1 (xh , xj ) + λ · K2 (yGh , yGj ),  (3)  where λ is a hyper-parameter. [sent-74, score-0.321]
</p><p>38 Kernel K1 can be an arbitrary kernel working on the measurements. [sent-75, score-0.101]
</p><p>39 For kernel K2 an arbitrary dot-product kernel might be used. [sent-76, score-0.202]
</p><p>40 In the following we restrict ourselves to a linear kernel (corresponding to the normalized Hamming distance between the local label conﬁgurations) K2 (yGh , yGj ) =  1 yGh |yGj , q  (4)  where . [sent-77, score-0.254]
</p><p>41 For a neighborhood G h of order c we obtain 1 K2 (yGh , yGj ) = yh+s · yj+s (5) q √ |s|< c,s=0  The linear kernel K2 in (4) takes on its maximum value, if the label conﬁgurations are identical, and its lowest value if the label conﬁguration is inverted. [sent-86, score-0.495]
</p><p>42 3  Learning phase  If a SVM is trained using the topographic kernel (3), the topographic label conﬁguration is included in the learning process. [sent-88, score-0.918]
</p><p>43 The resulting support vectors will still contain the relevant information about the measurements, but additionally the label neighborhood information relevant for a good distinction of the classes. [sent-89, score-0.32]
</p><p>44 4  Classiﬁcation phase  In order to collectively classify a set of κ new pixel sites with unknown topographic label conﬁguration, we propose the following iterative approach to achieve a self-consistent solution to the classiﬁcation problem. [sent-91, score-0.624]
</p><p>45 At each step τ new labels are assigned according to m  yh (τ ) = sgn j=1  αj · yv(j) · K(xh , xv(j) , yGh (τ − 1), yGv(j) ) + b , ∀h. [sent-93, score-0.407]
</p><p>46 (6)  The sum runs over all m support vectors, whose indices on the 2D grid are denoted by the vector v(j). [sent-94, score-0.12]
</p><p>47 Since initially the labels are unknown, we use at step τ = 0 the results from a conventional support-vector machine (λ = 0) as initialization for the labels. [sent-95, score-0.157]
</p><p>48 For the following steps some estimates of the neighboring labels are available from the previous iteration. [sent-96, score-0.083]
</p><p>49 Using this new topographic label information in addition to the measurement information, using (6) a new assignment decision for the labels is made. [sent-97, score-0.693]
</p><p>50 If we write the contributions from kernel K1 , which depend only on x and do not change with τ , as ch (j) = αj · yv(j) · K1 (xh , xv(j) ) equation (6) becomes m  yh (τ ) = sgn j=1  λαj yv(j) K2 (yGh (τ − 1), yGv(j) ) + ch (j) + b , ∀h. [sent-99, score-0.543]
</p><p>51 (7)  Putting in the linear kernel from equation (5), we get m  yh (τ ) = sgn  αj yv(j) j=1  λ q  √ |s|< c,s=0  [yh+s (τ − 1) · yv(j)+s ] + ch (j) + b , ∀h. [sent-100, score-0.466]
</p><p>52 (8)  Interchanging the sums, using the deﬁnitions wh,k =  λ q  m j=1  αj yv(j) yv(j)+(k−h) 0  and  : k ∈ Gh : k ∈ Gh /  (9)  m  θh = −  ch (j) + b ,  (10)  j=1  we obtain yh (τ ) = sgn  yk (τ − 1) · wh,k − θh , ∀h. [sent-101, score-0.386]
</p><p>53 (11)  k  This corresponds to the equations describing the dynamics of a recurrent neural network composed of McCulloch-Pitts neurons [9]. [sent-102, score-0.12]
</p><p>54 The condition for symmetric weights w h,k = wk,h is equivalent to an inversion symmetry of the label conﬁgurations of the support vectors in the neighborhood topology, therefore the weights in equation (9) are not necessarily symmetric. [sent-103, score-0.474]
</p><p>55 A suitable stopping criterion for the iteration is that the net reaches either a ﬁxed point yh (τ ) = yh (τ − 1), ∀h, or an attractor cycle yh (τ ) = yh (ρ), ρ < τ − 1, ∀h. [sent-104, score-1.033]
</p><p>56 (11) corresponds to a diluted network of κ binary neurons with no self-interaction and asymmetric weights. [sent-106, score-0.109]
</p><p>57 (9) that the network has only local connections, corresponding to the topographic neighborhood G h . [sent-108, score-0.454]
</p><p>58 The measurement xh only inﬂuences the individual unit threshold θh of the network, via the weighted sum over all support vectors of the contributions from kernel K1 (eq. [sent-109, score-0.401]
</p><p>59 The label conﬁgurations of the support vectors, on the other hand, are contained in the network weights via eq. [sent-111, score-0.257]
</p><p>60 The weights are multiplied by the hyper-parameter λ, which determines how much the label conﬁguration inﬂuences the class decision in comparison to the measurements. [sent-113, score-0.174]
</p><p>61 5  Symmetrization of the weights  sym In order to ensure convergence, we suggest to use an inversion symmetric version K 2 of kernel K2 . [sent-117, score-0.413]
</p><p>62 For the pixel grid we can deﬁne the inversion operation as l + t → l − t, t ∈ ¯ N2 , ∀l + t ∈ Gl , and denote the inverse of a by a. [sent-118, score-0.202]
</p><p>63 The beneﬁt from the chosen inversion symmetric kernel is that the self consistency equations for the labels will turn out to be equivalent to a Hopﬁeld net, which has proven convergence properties. [sent-120, score-0.296]
</p><p>64 We deﬁne the new kernel as sym K2 (yGh , yGj ) =  1 ( yGh |yGj + yGh |¯ Gj ). [sent-121, score-0.28]
</p><p>65 y q  (12)  Although only the second argument is inverted within the kernel, the value of this kernel does not depend on the order of the arguments. [sent-122, score-0.101]
</p><p>66 Proof It follows from the deﬁnition of the inversion operator and the dot product that ¯ y ¯ y ¯ yGh |yGj = yGh |¯ Gj = yGj |yGh = yGj |¯ Gh and yGh |yGj = yGh |¯ Gj = y ¯ yGj |¯ Gh = yGj |yGh . [sent-123, score-0.073]
</p><p>67 Therefore, y sym K2 (yGh , yGj )  = =  1 1 ¯ ( yGh |yGj + yGh |¯ Gj ) = ( yGj |yGh + yGj |yGh ) y q q 1 sym ( yGj |yGh + yGj |¯ Gh ) = K2 (yGj , yGh ) y q  . [sent-124, score-0.358]
</p><p>68 (7) and deﬁning sym wh,k =  λ q  m j=1  αj yv(j) (yv(j)+(k−h) + yv(j)−(k−h) ) 0  : k ∈ Gh : k ∈ Gh /  (13)  we get sym yk (τ − 1) · wh,k − θh , ∀h. [sent-126, score-0.379]
</p><p>69 yh (τ ) = sgn  (14)  k sym Since the network weights wh,j deﬁned in eq. [sent-127, score-0.54]
</p><p>70 (13) are symmetric this corresponds to the equation describing the dynamics during the retrieval phase of a Hopﬁeld network [10]. [sent-128, score-0.073]
</p><p>71 The weight between two neurons in the original Hopﬁeld net corresponds to the correlation between two components (over all fundamental patterns). [sent-130, score-0.081]
</p><p>72 In (13) the weight only depends on the difference vector k-h between the two neurons on the 2D grid and is proportional to the correlation (over all support vectors) between the label of a support vector and the label in the distance k-h. [sent-131, score-0.547]
</p><p>73 05  Experiments  We applied the above algorithms to the binary classiﬁcation of pixel sites of cell images from the CSSIP pap smear dataset. [sent-144, score-0.379]
</p><p>74 The goal was to assign the label +1 to the pixels belonging to the nucleus, and -1 to all others. [sent-145, score-0.231]
</p><p>75 The dataset contains three manual segmentations of the nucleus’ boundaries, from which we generated a ’ground truth’ label for the area of the nucleus using a majority voting. [sent-146, score-0.281]
</p><p>76 Only the ﬁrst 300 images were used in the experiments. [sent-147, score-0.053]
</p><p>77 As a measurement vector we took a column-ordering of a 3x3 gray value patch centered on a pixel site. [sent-148, score-0.246]
</p><p>78 In order to measure the classiﬁcation performance for a noniid data set, we estimated the test error based on the collective classiﬁcation of all pixels in several randomly chosen test images. [sent-149, score-0.155]
</p><p>79 We compared three algorithms: A conventional SVM, the ’TSVM’ with the topographic kernel K2 from eq. [sent-150, score-0.507]
</p><p>80 (4) and the ’STSVM’ with the sym inversion symmetric topographic kernel K2 from eq. [sent-151, score-0.724]
</p><p>81 In the experiments we used a label neighborhood of order 32, which corresponds to q = 100 neighbors. [sent-153, score-0.241]
</p><p>82 For kernel K1 we used an RBF kernel K1 (x1 , x2 ) = exp(− x1 − x2 2 /S 2 ) with hyper-parameter S. [sent-154, score-0.202]
</p><p>83 Then, the test of the classiﬁers was conducted at the in each case optimal hyper-parameters for 20 yet unused test images and 50 randomly sampled disjoint training sets. [sent-159, score-0.073]
</p><p>84 In all experiments using synchronous update either a ﬁxed point or an attractor cycle of length two was reached. [sent-160, score-0.078]
</p><p>85 Although the convergence properties have only been formally proven for the symmetric weight STSVM, experimental evidence suggests the same convergence properties for the TSVM. [sent-162, score-0.058]
</p><p>86 The performance of both topographic algorithms is superior to the conventional SVM, while the TSVM performed slightly better than the STSVM. [sent-165, score-0.406]
</p><p>87 1 shows 10 typical test images and their segmentations achieved by an STSVM at different values of λ for ﬁxed S and C. [sent-168, score-0.079]
</p><p>88 For increasing λ the label images become less noisy, and at λ = 0. [sent-169, score-0.206]
</p><p>89 This is caused by the increasing weight put on the label conﬁguration via kernel sym K2 . [sent-171, score-0.452]
</p><p>90 Increasing λ even further will eventually lead to the appearance of spurious artifacts, as the inﬂuence of the label conﬁguration will dominate the classiﬁcation decision. [sent-172, score-0.153]
</p><p>91 4  Conclusions  We have presented a classiﬁcation method for a special case of non-iid data in which the objects are linked by a regular grid structure. [sent-173, score-0.095]
</p><p>92 The proposed algorithm is composed of two  Figure 1: Final labels assigned by the STSVM at ﬁxed hyper-parameters C = 2 6 , S = 22 . [sent-174, score-0.126]
</p><p>93 components: The ﬁrst part is a topographic kernel which integrates conventional feature information and the information of the label conﬁgurations within a topographic neighborhood. [sent-180, score-0.992]
</p><p>94 The second part consists of a collective classiﬁcation with recurrent neural network dynamics which lets local label conﬁgurations converge to attractors determined by the label conﬁgurations of the support vectors. [sent-181, score-0.517]
</p><p>95 For the asymmetric weight TSVM, the dimensionality of the problem is increased by the neighborhood size as compared to a conventional SVM (twice the neighborhood size for the symmetric weight STSVM). [sent-182, score-0.351]
</p><p>96 Therefore, the TSVM and the STSVM can be applied to image segmentation problems, where a large number of pixel labels have to be assigned simultaneously. [sent-184, score-0.29]
</p><p>97 The algorithms were applied to the bottom-up cell nucleus segmentation in pap smear images needed for the detection of cervical cancer. [sent-185, score-0.46]
</p><p>98 The classiﬁcation performance of the TSVM and STSVM were compared to a conventional SVM, and it was shown that the inclusion of the topographic label conﬁguration lead to a substantial decrease in the average misclassiﬁcation rate. [sent-186, score-0.559]
</p><p>99 The two topographic algorithms were much more resistant to noise and smaller artifacts. [sent-187, score-0.332]
</p><p>100 A removal of artifacts which have similar size and the same measurement features as some of the nuclei cannot be achieved by a pure bottom-up method, as this requires a priori model knowledge. [sent-188, score-0.174]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ygh', 0.496), ('ygj', 0.397), ('topographic', 0.332), ('yh', 0.238), ('stsvm', 0.218), ('tsvm', 0.205), ('sym', 0.179), ('yv', 0.173), ('label', 0.153), ('xh', 0.111), ('gh', 0.11), ('kernel', 0.101), ('collective', 0.097), ('measurement', 0.092), ('neighborhood', 0.088), ('segmentation', 0.083), ('labels', 0.083), ('gurations', 0.082), ('pixel', 0.081), ('cervical', 0.079), ('nucleus', 0.079), ('conventional', 0.074), ('inversion', 0.073), ('classi', 0.072), ('guration', 0.069), ('sgn', 0.068), ('pap', 0.06), ('smear', 0.06), ('ch', 0.059), ('sites', 0.058), ('pixels', 0.058), ('con', 0.057), ('images', 0.053), ('cation', 0.051), ('berlin', 0.05), ('support', 0.049), ('grid', 0.048), ('neighbors', 0.047), ('gj', 0.047), ('cell', 0.046), ('hop', 0.044), ('artifacts', 0.042), ('cssip', 0.04), ('nuclei', 0.04), ('queensland', 0.04), ('ygl', 0.04), ('ygv', 0.04), ('symmetric', 0.039), ('svm', 0.038), ('bernstein', 0.035), ('measurements', 0.034), ('network', 0.034), ('site', 0.034), ('assignment', 0.033), ('cancer', 0.033), ('dependencies', 0.033), ('net', 0.032), ('recurrent', 0.031), ('vectors', 0.03), ('neurons', 0.03), ('synchronous', 0.029), ('xv', 0.029), ('gray', 0.029), ('hyperplane', 0.029), ('attractor', 0.028), ('concatenated', 0.028), ('segmentations', 0.026), ('regular', 0.026), ('go', 0.026), ('relational', 0.025), ('sebastian', 0.025), ('image', 0.025), ('composed', 0.025), ('discriminative', 0.025), ('asymmetric', 0.024), ('margin', 0.024), ('manual', 0.023), ('scholkopf', 0.023), ('eld', 0.023), ('vector', 0.023), ('weights', 0.021), ('bernhard', 0.021), ('yk', 0.021), ('putting', 0.021), ('iid', 0.021), ('uences', 0.021), ('objects', 0.021), ('binary', 0.021), ('cycle', 0.021), ('gradients', 0.021), ('lawrence', 0.021), ('patch', 0.021), ('misclassi', 0.02), ('slack', 0.02), ('disjoint', 0.02), ('assign', 0.02), ('gi', 0.019), ('weight', 0.019), ('assigned', 0.018), ('contributions', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="14-tfidf-1" href="./nips-2004-A_Topographic_Support_Vector_Machine%3A_Classification_Using_Local_Label_Configurations.html">14 nips-2004-A Topographic Support Vector Machine: Classification Using Local Label Configurations</a></p>
<p>Author: Johannes Mohr, Klaus Obermayer</p><p>Abstract: The standard approach to the classiﬁcation of objects is to consider the examples as independent and identically distributed (iid). In many real world settings, however, this assumption is not valid, because a topographical relationship exists between the objects. In this contribution we consider the special case of image segmentation, where the objects are pixels and where the underlying topography is a 2D regular rectangular grid. We introduce a classiﬁcation method which not only uses measured vectorial feature information but also the label conﬁguration within a topographic neighborhood. Due to the resulting dependence between the labels of neighboring pixels, a collective classiﬁcation of a set of pixels becomes necessary. We propose a new method called ’Topographic Support Vector Machine’ (TSVM), which is based on a topographic kernel and a self-consistent solution to the label assignment shown to be equivalent to a recurrent neural network. The performance of the algorithm is compared to a conventional SVM on a cell image segmentation task. 1</p><p>2 0.07162331 <a title="14-tfidf-2" href="./nips-2004-Common-Frame_Model_for_Object_Recognition.html">40 nips-2004-Common-Frame Model for Object Recognition</a></p>
<p>Author: Pierre Moreels, Pietro Perona</p><p>Abstract: A generative probabilistic model for objects in images is presented. An object consists of a constellation of features. Feature appearance and pose are modeled probabilistically. Scene images are generated by drawing a set of objects from a given database, with random clutter sprinkled on the remaining image surface. Occlusion is allowed. We study the case where features from the same object share a common reference frame. Moreover, parameters for shape and appearance densities are shared across features. This is to be contrasted with previous work on probabilistic ‘constellation’ models where features depend on each other, and each feature and model have different pose and appearance statistics [1, 2]. These two differences allow us to build models containing hundreds of features, as well as to train each model from a single example. Our model may also be thought of as a probabilistic revisitation of Lowe’s model [3, 4]. We propose an efﬁcient entropy-minimization inference algorithm that constructs the best interpretation of a scene as a collection of objects and clutter. We test our ideas with experiments on two image databases. We compare with Lowe’s algorithm and demonstrate better performance, in particular in presence of large amounts of background clutter.</p><p>3 0.069365911 <a title="14-tfidf-3" href="./nips-2004-Maximum_Margin_Clustering.html">115 nips-2004-Maximum Margin Clustering</a></p>
<p>Author: Linli Xu, James Neufeld, Bryce Larson, Dale Schuurmans</p><p>Abstract: We propose a new method for clustering based on ﬁnding maximum margin hyperplanes through data. By reformulating the problem in terms of the implied equivalence relation matrix, we can pose the problem as a convex integer program. Although this still yields a difﬁcult computational problem, the hard-clustering constraints can be relaxed to a soft-clustering formulation which can be feasibly solved with a semidefinite program. Since our clustering technique only depends on the data through the kernel matrix, we can easily achieve nonlinear clusterings in the same manner as spectral clustering. Experimental results show that our maximum margin clustering technique often obtains more accurate results than conventional clustering methods. The real beneﬁt of our approach, however, is that it leads naturally to a semi-supervised training method for support vector machines. By maximizing the margin simultaneously on labeled and unlabeled training data, we achieve state of the art performance by using a single, integrated learning principle. 1</p><p>4 0.068428472 <a title="14-tfidf-4" href="./nips-2004-Semigroup_Kernels_on_Finite_Sets.html">168 nips-2004-Semigroup Kernels on Finite Sets</a></p>
<p>Author: Marco Cuturi, Jean-philippe Vert</p><p>Abstract: Complex objects can often be conveniently represented by ﬁnite sets of simpler components, such as images by sets of patches or texts by bags of words. We study the class of positive deﬁnite (p.d.) kernels for two such objects that can be expressed as a function of the merger of their respective sets of components. We prove a general integral representation of such kernels and present two particular examples. One of them leads to a kernel for sets of points living in a space endowed itself with a positive deﬁnite kernel. We provide experimental results on a benchmark experiment of handwritten digits image classiﬁcation which illustrate the validity of the approach. 1</p><p>5 0.066438273 <a title="14-tfidf-5" href="./nips-2004-Object_Classification_from_a_Single_Example_Utilizing_Class_Relevance_Metrics.html">134 nips-2004-Object Classification from a Single Example Utilizing Class Relevance Metrics</a></p>
<p>Author: Michael Fink</p><p>Abstract: We describe a framework for learning an object classiﬁer from a single example. This goal is achieved by emphasizing the relevant dimensions for classiﬁcation using available examples of related classes. Learning to accurately classify objects from a single training example is often unfeasible due to overﬁtting effects. However, if the instance representation provides that the distance between each two instances of the same class is smaller than the distance between any two instances from different classes, then a nearest neighbor classiﬁer could achieve perfect performance with a single training example. We therefore suggest a two stage strategy. First, learn a metric over the instances that achieves the distance criterion mentioned above, from available examples of other related classes. Then, using the single examples, deﬁne a nearest neighbor classiﬁer where distance is evaluated by the learned class relevance metric. Finding a metric that emphasizes the relevant dimensions for classiﬁcation might not be possible when restricted to linear projections. We therefore make use of a kernel based metric learning algorithm. Our setting encodes object instances as sets of locality based descriptors and adopts an appropriate image kernel for the class relevance metric learning. The proposed framework for learning from a single example is demonstrated in a synthetic setting and on a character classiﬁcation task. 1</p><p>6 0.06390027 <a title="14-tfidf-6" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>7 0.062342688 <a title="14-tfidf-7" href="./nips-2004-Breaking_SVM_Complexity_with_Cross-Training.html">34 nips-2004-Breaking SVM Complexity with Cross-Training</a></p>
<p>8 0.061990108 <a title="14-tfidf-8" href="./nips-2004-Maximal_Margin_Labeling_for_Multi-Topic_Text_Categorization.html">111 nips-2004-Maximal Margin Labeling for Multi-Topic Text Categorization</a></p>
<p>9 0.061864834 <a title="14-tfidf-9" href="./nips-2004-Modeling_Nonlinear_Dependencies_in_Natural_Images_using_Mixture_of_Laplacian_Distribution.html">121 nips-2004-Modeling Nonlinear Dependencies in Natural Images using Mixture of Laplacian Distribution</a></p>
<p>10 0.060358707 <a title="14-tfidf-10" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>11 0.058559779 <a title="14-tfidf-11" href="./nips-2004-On_Semi-Supervised_Classification.html">136 nips-2004-On Semi-Supervised Classification</a></p>
<p>12 0.057210553 <a title="14-tfidf-12" href="./nips-2004-Kernel_Methods_for_Implicit_Surface_Modeling.html">92 nips-2004-Kernel Methods for Implicit Surface Modeling</a></p>
<p>13 0.056509845 <a title="14-tfidf-13" href="./nips-2004-Nonparametric_Transforms_of_Graph_Kernels_for_Semi-Supervised_Learning.html">133 nips-2004-Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning</a></p>
<p>14 0.054014817 <a title="14-tfidf-14" href="./nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine.html">187 nips-2004-The Entire Regularization Path for the Support Vector Machine</a></p>
<p>15 0.053782254 <a title="14-tfidf-15" href="./nips-2004-Conditional_Random_Fields_for_Object_Recognition.html">44 nips-2004-Conditional Random Fields for Object Recognition</a></p>
<p>16 0.053312823 <a title="14-tfidf-16" href="./nips-2004-Semi-Markov_Conditional_Random_Fields_for_Information_Extraction.html">162 nips-2004-Semi-Markov Conditional Random Fields for Information Extraction</a></p>
<p>17 0.052725054 <a title="14-tfidf-17" href="./nips-2004-Joint_MRI_Bias_Removal_Using_Entropy_Minimization_Across_Images.html">89 nips-2004-Joint MRI Bias Removal Using Entropy Minimization Across Images</a></p>
<p>18 0.050128993 <a title="14-tfidf-18" href="./nips-2004-Parallel_Support_Vector_Machines%3A_The_Cascade_SVM.html">144 nips-2004-Parallel Support Vector Machines: The Cascade SVM</a></p>
<p>19 0.049588751 <a title="14-tfidf-19" href="./nips-2004-Learning_Preferences_for_Multiclass_Problems.html">100 nips-2004-Learning Preferences for Multiclass Problems</a></p>
<p>20 0.048919473 <a title="14-tfidf-20" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.17), (1, 0.042), (2, -0.058), (3, 0.01), (4, 0.039), (5, 0.035), (6, 0.058), (7, -0.037), (8, -0.014), (9, -0.004), (10, -0.017), (11, 0.059), (12, -0.028), (13, -0.015), (14, -0.023), (15, 0.019), (16, -0.036), (17, 0.061), (18, 0.056), (19, -0.046), (20, 0.03), (21, -0.0), (22, -0.031), (23, 0.03), (24, -0.087), (25, -0.06), (26, 0.025), (27, 0.025), (28, -0.06), (29, -0.044), (30, 0.018), (31, -0.086), (32, 0.023), (33, -0.026), (34, -0.044), (35, 0.035), (36, -0.0), (37, 0.029), (38, 0.015), (39, -0.034), (40, -0.0), (41, 0.014), (42, 0.067), (43, 0.018), (44, 0.066), (45, 0.038), (46, -0.086), (47, 0.04), (48, -0.093), (49, -0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90414667 <a title="14-lsi-1" href="./nips-2004-A_Topographic_Support_Vector_Machine%3A_Classification_Using_Local_Label_Configurations.html">14 nips-2004-A Topographic Support Vector Machine: Classification Using Local Label Configurations</a></p>
<p>Author: Johannes Mohr, Klaus Obermayer</p><p>Abstract: The standard approach to the classiﬁcation of objects is to consider the examples as independent and identically distributed (iid). In many real world settings, however, this assumption is not valid, because a topographical relationship exists between the objects. In this contribution we consider the special case of image segmentation, where the objects are pixels and where the underlying topography is a 2D regular rectangular grid. We introduce a classiﬁcation method which not only uses measured vectorial feature information but also the label conﬁguration within a topographic neighborhood. Due to the resulting dependence between the labels of neighboring pixels, a collective classiﬁcation of a set of pixels becomes necessary. We propose a new method called ’Topographic Support Vector Machine’ (TSVM), which is based on a topographic kernel and a self-consistent solution to the label assignment shown to be equivalent to a recurrent neural network. The performance of the algorithm is compared to a conventional SVM on a cell image segmentation task. 1</p><p>2 0.70187569 <a title="14-lsi-2" href="./nips-2004-Maximal_Margin_Labeling_for_Multi-Topic_Text_Categorization.html">111 nips-2004-Maximal Margin Labeling for Multi-Topic Text Categorization</a></p>
<p>Author: Hideto Kazawa, Tomonori Izumitani, Hirotoshi Taira, Eisaku Maeda</p><p>Abstract: In this paper, we address the problem of statistical learning for multitopic text categorization (MTC), whose goal is to choose all relevant topics (a label) from a given set of topics. The proposed algorithm, Maximal Margin Labeling (MML), treats all possible labels as independent classes and learns a multi-class classiﬁer on the induced multi-class categorization problem. To cope with the data sparseness caused by the huge number of possible labels, MML combines some prior knowledge about label prototypes and a maximal margin criterion in a novel way. Experiments with multi-topic Web pages show that MML outperforms existing learning algorithms including Support Vector Machines. 1 Multi-topic Text Categorization (MTC) This paper addresses the problem of learning for multi-topic text categorization (MTC), whose goal is to select all topics relevant to a text from a given set of topics. In MTC, multiple topics may be relevant to a single text. We thus call a set of topics label, and say that a text is assigned a label, not a topic. In almost all previous text categorization studies (e.g. [1, 2]), the label is predicted by judging each topic’s relevance to the text. In this decomposition approach, the features speciﬁc to a topic, not a label, are regarded as important features. However, the approach may result in inefﬁcient learning as we will explain in the following example. Imagine an MTC problem of scientiﬁc papers where quantum computing papers are assigned multi-topic label “quantum physics (QP) & computer science (CS)”. (QP and CS are topics in this example.) Since there are some words speciﬁc to quantum computing such as “qbit1 ”, one can say that efﬁcient MTC learners should use such words to assign label QP & CS. However, the decomposition approach is likely to ignore these words since they are only speciﬁc to a small portion of the whole QP or CS papers (there are many more QP and CS papers than quantum computing papers), and therefore are not discriminative features for either topic QP or CS. 1 Qbit is a unit of quantum information, and frequently appears in real quantum computing literatures, but rarely seen in other literatures. Symbol x(∈ Rd ) t1 , t2 , . . . , tl T L, λ(⊂ T ) L[j] Λ(= 2T ) {(xi , Li )}m i=1 Meaning A document vector Topics The set of all topics A label The binary representation of L. 1 if tj ∈ L and 0 otherwise. The set of all possible labels Training samples Table 1: Notation Parametric Mixture Model (PMM) [3] adopts another approach to MTC. It is assumed in PMM that multi-topic texts are generated from a mixture of topic-speciﬁc word distributions. Its decision on labeling is done at once, not separately for each topic. However, PMM also has a problem with multi-topic speciﬁc features such as “qbit” since it is impossible for texts to have such features given PMM’s mixture process. These problems with multi-topic speciﬁc features are caused by dependency assumptions between labels, which are explicitly or implicitly made in existing methods. To solve these problems, we propose Maximal Margin Labeling, which treats labels as independent classes and learns a multi-class classiﬁer on the induced multi-class problem. In this paper, we ﬁrst discuss why multi-class classiﬁers cannot be directly applied to MTC in Section 2. We then propose MML in Section 3, and address implementation issues in Section 4. In Section 5, MML is experimentally compared with existing methods using a collection of multi-topic Web pages. We summarize this paper in Section 6. 2 Solving MTC as a Multi-Class Categorization To discuss why existing multi-class classiﬁers do not work in MTC, we start from the multi-class classiﬁer proposed in [4]. Hereafter we use the notation given in Table 1. The multi-class classiﬁer in [4] categorizes an object into the class whose prototype vector is the closest to the object’s feature vector. By substituting label for class, the classiﬁer can be written as follows. f (x) = arg max x, mλ (1) X λ∈Λ where , X is the the inner product of Rd , and mλ ∈ Rd is the prototype vector of label λ. Following the similar argument as in [4], the prototype vectors are learned by solving the following maximal margin problem2 . min M s.t. 1 M 2 2 λ ξi +C xi , mLi 1≤i≤m λ∈Λ,λ=Li X − xi , m λ X λ ≥ 1 − ξi for 1 ≤ i ≤ m, ∀λ = Li , (2) where M is the prototype matrix whose columns are the prototype vectors, and M is the Frobenius matrix norm of M . Note that Eq. (1) and Eq. (2) cover only training samples’ labels, but also all possible labels. This is because the labels unseen in training samples may be relevant to test samples. In 2 In Eq.(2), we penalize all violation of the margin constraints. On the other hand, Crammer and Singer penalize only the largest violation of the margin constraint for each training sample [4]. We chose the “penalize-all” approach since it leads to an optimization problem without equality constraints (see Eq.(7)), which is much easier to solve than the one in [4]. usual multi-class problems, such unseen labels seldom exist. In MTC, however, the number of labels is generally very large (e.g. one of our datasets has 1,054 labels (Table 2)), and unseen labels often exist. Thus it is necessary to consider all possible labels in Eq. (1) and Eq. (2) since it is impossible to know which unseen labels are present in the test samples. There are two problems with Eq. (1) and Eq. (2). The ﬁrst problem is that they involve the prototype vectors of seldom or never seen labels. Without the help of prior knowledge about where the prototype vectors should be, it is impossible to obtain appropriate prototype vectors for such labels. The second problem is that these equations are computationally too demanding since they involve combinatorial maximization and summation over all possible labels, whose number can be quite large. (For example, the number is around 230 in the datasets used in our experiments.) We will address the ﬁrst problem in Section 3 and the second problem in Section 4. 3 Maximal Margin Labeling In this section, we incorporate some prior knowledge about the location of prototype vectors into Eq. (1) and Eq. (2), and propose a novel MTC learning algorithm, Maximal Margin Labeling (MML). As prior knowledge, we simply assume that the prototype vectors of similar labels should be placed close to each other. Based on this assumption, we ﬁrst rewrite Eq. (1) to yield f (x) = arg max M T x, eλ L, (3) λ∈Λ where , L is the inner product of R|Λ| and {eλ }λ∈Λ is the orthonormal basis of R|Λ| . The classiﬁer of Eq. (3) can be interpreted as a two-step process: the ﬁrst step is to map the vector x into R|Λ| by M T , and the second step is to ﬁnd the closest eλ to image M T x. Then we replace {eλ }λ∈Λ with (generally) non-orthogonal vectors {φ(λ)}λ∈Λ whose geometrical conﬁguration reﬂects label similarity. More formally speaking, we use vectors {φ(λ)}λ∈Λ that satisfy the condition φ(λ1 ), φ(λ2 ) S = S(λ1 , λ2 ) for ∀λ1 , λ2 ∈ Λ, (4) where , S is an inner product of the vector space spanned by {φ(λ)}λ∈Λ , and S is a Mercer kernel [5] on Λ × Λ and is a similarity measure between labels. We call the vector space spanned by {φ(λ)} VS . With this replacement, MML’s classiﬁer is written as follows. f (x) = arg max W x, φ(λ) S , (5) λ∈Λ where W is a linear map from Rd to VS . W is the solution of the following problem. min W 1 W 2 m 2 λ ξi +C i=1 λ∈Λ,λ=Li φ(Li )−φ(λ) λ λ s.t. W xi , ≥ 1−ξi , ξi ≥ 0 for 1 ≤ i ≤ m, ∀λ = Li . (6) φ(Li )−φ(λ) Note that if φ(λ) is replaced by eλ , Eq. (6) becomes identical to Eq. (2) except for a scale factor. Thus Eq. (5) and Eq. (6) are natural extensions of the multi-class classiﬁer in [4]. We call the MTC classiﬁer of Eq. (5) and Eq. (6) “Maximal Margin Labeling (MML)”. Figure 1 explains the margin (the inner product in Eq. (6)) in MML. The margin represents the distance from the image of the training sample xi to the boundary between the correct label Li and wrong label λ. MML optimizes the linear map W so that the smallest margin between all training samples and all possible labels becomes maximal, along with a penalty C for the case that samples penetrate into the margin. Figure 1: Maximal Margin Labeling Dual Form For numerical computation, the following Wolfe dual form of Eq. (6) is more convenient. (We omit its derivation due to space limits.) S(Li , Li′ )−S(Li , λ′ )−S(λ, Li′ )+S(λ, λ′ ) 1 λ λ′ λ αi αi′ (xi ·xi′ ) max αi − 2 αλ 2 (1−S(Li , λ))(1−S(Li′ , λ′ )) i ′ ′ i,λ i,λ i ,λ λ s.t. 0 ≤ αi ≤ C for 1 ≤ i ≤ m, ∀λ = Li , (7) m λ where we denote i=1 λ∈Λ,∀λ=Li by i,λ , and αi are the dual variables corresponding to the ﬁrst inequality constraints in Eq. (6). Note that Eq. (7) does not contain φ(λ): all the computations involving φ can be done through the label similarity S. Additionally xi only appears in the inner products, and therefore can be replaced by any kernel of x. λ Using the solution αi of Eq. (7), the MML’s classiﬁer in Eq. (5) can be written as follows. S(Li , L)−S(λ, L) λ . (8) f (x) = arg max αi (x·xi ) 2(1−S(Li , λ)) L∈Λ i,λ Label Similarity3 As examples of label similarity, we use two similarity measures: Dice measure and cosine measure. Dice measure4 4.1 SC (λ1 , λ2 ) = |λ1 ∩ λ2 | 2 |λ1 | |λ2 | = l j=1 l j=1 2|λ1 ∩λ2 | = |λ1 |+|λ2 | Cosine measure 4 SD (λ1 , λ2 ) = λ1 [j] + λ1 [j]λ2 [j] l j=1 l j=1 l j=1 λ2 [j] . λ1 [j]λ2 [j] λ1 [j] l j=1 (9) . 10) ( λ2 [j] Efﬁcient Implementation Approximation in Learning Eq. (7) contains the sum over all possible labels. As the number of topics (l) increases, this summation rapidly becomes intractable since |Λ| grows exponentially as 2l . To circumvent 3 The following discussion is easily extended to include the case that both λ1 and λ2 are empty although we do not discuss the case due to space limits. this problem, we approximate the sum over all possible labels in Eq. (7) by the partial sum λ λ over αi of |(A ∩ B c ) ∪ (Ac ∩ B)| = 1 and set all the other αi to zero. This approximation reduces the burden of the summation quite a lot: the number of summands is reduced from 2l to l, which is a huge reduction especially when many topics exist. λ To understand the rationale behind the approximation, ﬁrst note that αi is the dual variable λ corresponding to the ﬁrst inequality constraint (the margin constraint) in Eq. (7). Thus αi is non-zero if and only if W xi falls in the margin between φ(Li ) and φ(λ). We assume that this margin violation mainly occurs when φ(λ) is “close” to φ(Li ), i.e. |(A ∩ B c ) ∪ (Ac ∩ B)| = 1. If this assumption holds well, the proposed approximation of the sum will lead to a good approximation of the exact solution. 4.2 Polynomial Time Algorithms for Classiﬁcation The classiﬁcation of MML (Eq. (8)) involves the combinatorial maximization over all possible labels, so it can be a computationally demanding process. However, efﬁcient classiﬁcation algorithms are available when either the cosine measure or dice measure is used as label similarity. Eq. (8) can be divided into the subproblems by the number of topics in a label. f (x) = arg max g(x, L), (11) ˆ ˆ ˆ L∈{L1 ,L2 ,...,Ll } ˆ Ln = arg max g(x, L). (12) L∈Λ,|L|=n where g(x) is l g(x, L) cn [j]L[j], = j=1 cn [j] =      i,λ √ i,λ √ αλ (x·xi ) i 2(1−SD (Li ,λ)) αλ (x·xi ) i 2(1−SC (Li ,λ)) . 2Li [j] |Li |+n − 2λ[j] |λ|+n √Li [j] − √λ[j] √ √ |Li | n |λ| n if SD is used. (13) if SC is used. Here n = |L|. The computational cost of Eq. (13) for all j is O(nα l) (nα is the number of non-zero α), and that of Eq. (12) is O(l log l). Thus the total cost of the classiﬁcation by Eq. (11) is O(nα l2 + l2 log l). On the other hand, nα is O(ml) under the approximation described above. Therefore, the classiﬁcation can be done within O(ml3 ) computational steps, which is a signiﬁcant reduction from the case that the brute force search is used in Eq. (8). 5 Experiments In this section, we report experiments that compared MML to PMM [3], SVM5 [6], and BoosTexter [2] using a collection of Web pages. We used a normalized linear kernel k(x, x′ ) = x · x′ / x x′ in MML and SVM. As for BoosTexter, “real abstaining AdaBoost.MH” was used as the weak learner. 5.1 Experimental Setup The datasets used in our experiment represent the Web page collection used in [3] (Table 2). The Web pages were collected through the hyperlinks from Yahoo!’s top directory 5 For each topic, an SVM classiﬁer is trained to predict whether the topic is relevant (positive) or irrelevant (negative) to input doucments. Dataset Name (Abbrev.) Arts & Humanities (Ar) Business & Economy (Bu) Computers & Internet (Co) Education (Ed) Entertainment (En) Health (He) Recreation (Rc) Reference (Rf) Science (Si) Social Science (SS) Society & Culture (SC) #Text #Voc 7,484 11,214 12,444 12,030 12,730 9,205 12,828 8,027 6,428 12,111 14,512 #Tpc #Lbl 23,146 21,924 34,096 27,534 32,001 30,605 30,324 39,679 37,187 52,350 31,802 26 30 33 33 21 32 22 33 40 39 27 Label Size Frequency (%) 1 2 3 4 ≥5 599 55.6 30.5 9.7 2.8 1.4 233 57.6 28.8 11.0 1.7 0.8 428 69.8 18.2 7.8 3.0 1.1 511 66.9 23.4 7.3 1.9 0.6 337 72.3 21.1 4.5 1.0 1.1 335 53.2 34.0 9.5 2.4 0.9 530 69.2 23.1 5.6 1.4 0.6 275 85.5 12.6 1.5 0.3 0.1 457 68.0 22.3 7.3 1.9 0.5 361 78.4 17.0 3.7 0.7 0.3 1054 59.6 26.1 9.2 2.9 2.2 Table 2: A summary of the web page datasets. “#Text” is the number of texts in the dataset, “#Voc” the number of vocabularies (i.e. features), “#Tpc” the number of topics, “#Lbl” the number of labels, and “Label Size Frequency” is the relative frequency of each label size. (Label size is the number of topics in a label.) Method MML PMM SVM Boost Feature Type TF, TF×IDF TF TF, TF×IDF Binary Parameter C = 0.1, 1, 10 Model1, Model2 C = 0.1, 1, 10 R = {2, 4, 6, 8, 10}×103 Table 3: Candidate feature types and learning parameters. (R is the number of weak hypotheses.) The underlined fetures and parameters were selected for the evaluation with the test data. (www.yahoo.com), and then divided into 11 datasets by Yahoo’s top category. Each page is labeled with the Yahoo’s second level sub-categories from which the page is hyperlinked. (Thus, the sub-categories are topics in our term.) See [3] for more details about the collection. Then the Web pages were converted into three types of feature vectors: (a) Binary vectors, where each feature indicates the presence (absence) of a term by 1 (0); (b) TF vectors, where each feature is the number of appearances of a term (term frequency); and (c) TF×IDF vectors, where each feature is the product of term frequency and inverse document frequency [7]. To select the best combinations of feature types and learning parameters such as the penalty C for MML, the learners were trained on 2,000 Web pages with all combinations of feature and parameter listed in Table 3, and then were evaluated by labeling F-measure on independently drawn development data. The combinations which achieve the best labeling F-measures (underlined in Table 3) were used in the following experiments. 5.2 Evaluation Measures We used three measures to evaluate labeling performance: labeling F-measure, exact match ratio, and retrieval F-measure. In the following deﬁnitions, {Lpred }n and {Ltrue }n i=1 i i=1 i mean the predicted labels and the true labels, respectively. Labeling F-measure Labeling F-measure FL evaluates the average labeling performance while taking partial match into account. FL = 1 n n i=1 2|Lpred ∩ Ltrue | i i |Lpred | + |Ltrue | i i = 1 n n i=1 l pred [j]Ltrue [j] i j=1 Li . l (Lpred [j] + Ltrue [j]) i i j=1 2 (14) Dataset Ar Bu Co Ed En He Rc Rf Si SS SC Avg MD 0.55 0.80 0.62 0.56 0.64 0.74 0.63 0.67 0.61 0.73 0.60 0.65 Labeling F-measure MC PM SV BO 0.44 0.50 0.46 0.38 0.81 0.75 0.76 0.75 0.59 0.61 0.55 0.47 0.43 0.51 0.48 0.37 0.52 0.61 0.54 0.49 0.74 0.66 0.67 0.60 0.46 0.55 0.49 0.44 0.58 0.63 0.56 0.50 0.54 0.52 0.47 0.39 0.71 0.66 0.64 0.59 0.55 0.54 0.49 0.44 0.58 0.59 0.56 0.49 Exact Match Ratio MC PM SV 0.32 0.21 0.29 0.62 0.48 0.57 0.46 0.35 0.41 0.34 0.19 0.30 0.44 0.31 0.42 0.53 0.34 0.47 0.38 0.25 0.37 0.51 0.39 0.49 0.43 0.22 0.36 0.60 0.45 0.55 0.40 0.21 0.32 0.46 0.31 0.41 MD 0.44 0.63 0.51 0.45 0.55 0.58 0.54 0.60 0.52 0.65 0.44 0.54 BO 0.22 0.53 0.34 0.23 0.36 0.39 0.33 0.41 0.28 0.49 0.27 0.35 MD 0.30 0.25 0.27 0.25 0.37 0.35 0.47 0.29 0.37 0.36 0.29 0.32 Retrieval F-measure MC PM SV BO 0.26 0.24 0.29 0.22 0.27 0.20 0.29 0.20 0.25 0.19 0.30 0.17 0.23 0.21 0.25 0.16 0.33 0.30 0.35 0.29 0.35 0.23 0.35 0.26 0.39 0.36 0.40 0.33 0.25 0.24 0.25 0.16 0.35 0.28 0.31 0.19 0.35 0.18 0.31 0.15 0.28 0.25 0.26 0.20 0.30 0.24 0.31 0.21 Table 4: The performance comparison by labeling F-measure (left), exact match ratio (middle) and retrieval F-measure (right). The bold ﬁgures are the best ones among the ﬁve methods, and the underlined ﬁgures the second best ones. MD, MC, PM, SV, and BO represent MML with SD , MML with SC , PMM, SVM and BoosTexter, respectively. Exact Match Ratio Exact match ratio EX counts only exact matches between the predicted label and the true label. EX = 1 n n I[Lpred = Ltrue ], i i (15) i=1 where I[S] is 1 if the statement S is true and 0 otherwise. Retrieval F-measure6 For real tasks, it is also important to evaluate retrieval performance, i.e. how accurately classiﬁers can ﬁnd relevant texts for a given topic. Retrieval F-measure FR measures the average retrieval performance over all topics. FR = 5.3 1 l l j=1 n pred [j]Ltrue [j] i i=1 Li . n (Lpred [j] + Ltrue [j]) i i i=1 2 (16) Results First we trained the classiﬁers with randomly chosen 2,000 samples. We then calculated the three evaluation measures for 3,000 other randomly chosen samples. This process was repeated ﬁve times, and the resulting averaged values are shown in Table 4. Table 4 shows that the MMLs with Dice measure outperform other methods in labeling F-measure and exact match ratio. The MMLs also show the best performance with regard to retrieval Fmeasure although the margins to the other methods are not as large as observed in labeling F-measure and exact match ratio. Note that no classiﬁer except MML with Dice measure achieves good labeling on all the three measures. For example, PMM shows high labeling F-measures, but its performance is rather poor when evaluated in retrieval F-measure. As the second experiment, we evaluated the classiﬁers trained with 250–2000 training samples on the same test samples. Figure 2 shows each measure averaged over all datasets. It is observed that the MMLs show high generalization even when training data is small. An interesting point is that MML with cosine measure achieves rather high labeling F-measures and retrieval F-measure with training data of smaller size. Such high-performace, however, does not continue when trained on larger data. 6 FR is called “the macro average of F-measures” in the text categorization community. Figure 2: The learning curve of labeling F-measure (left), exact match ratio (middle) and retrieval F-measure (right). MD, MC, PM, SV, BO mean the same as in Table 4. 6 Conclusion In this paper, we proposed a novel learning algorithm for multi-topic text categorization. The algorithm, Maximal Margin Labeling, embeds labels (sets of topics) into a similarityinduced vector space, and learns a large margin classiﬁer in the space. To overcome the demanding computational cost of MML, we provide an approximation method in learning and efﬁcient classiﬁcation algorithms. In experiments on a collection of Web pages, MML outperformed other methods including SVM and showed better generalization. Acknowledgement The authors would like to thank Naonori Ueda, Kazumi Saito and Yuji Kaneda of Nippon Telegraph and Telephone Corporation for providing PMM’s codes and the datasets. References [1] Thorsten Joachims. Text categorization with support vector machines: learning with many relevant features. In Claire N´ dellec and C´ line Rouveirol, editors, Proc. of the e e 10th European Conference on Machine Learning, number 1398, pages 137–142, 1998. [2] Robert E. Schapire and Yoram Singer. BoosTexter: A boosting-based system for text categorization. Machine Learning, 39(2/3):135–168, 2000. [3] Naonori Ueda and Kazumi Saito. Parametoric mixture models for multi-topic text. In Advances in Neural Information Processing Systems 15, pages 1261–1268, 2003. [4] Koby Crammer and Yoram Singer. On the algorithmic implementation of multiclass kernel-based vector machines. Journal of Machine Learning Research, 2:265–292, 2001. [5] Klaus-Robert M¨ ller, Sebastian Mika, Gunnar R¨ tsch, Koji Tsuda, and Bernhard u a Sch¨ lkopf. An introduction to kernel-based learning algorithms. IEEE Transactions o on Neural Networks, 12(2):181–201, 2001. [6] Vladimir N. Vapnik. Statistical Learning Theory. John Wiley & Sons, Inc., 1998. [7] Ricardo Baeza-Yates and Berthier Ribeiro-Neto. Modern Information Retrieval. Addison-Wealy, 1999.</p><p>3 0.52988052 <a title="14-lsi-3" href="./nips-2004-Conditional_Random_Fields_for_Object_Recognition.html">44 nips-2004-Conditional Random Fields for Object Recognition</a></p>
<p>Author: Ariadna Quattoni, Michael Collins, Trevor Darrell</p><p>Abstract: We present a discriminative part-based approach for the recognition of object classes from unsegmented cluttered scenes. Objects are modeled as ﬂexible constellations of parts conditioned on local observations found by an interest operator. For each object class the probability of a given assignment of parts to local features is modeled by a Conditional Random Field (CRF). We propose an extension of the CRF framework that incorporates hidden variables and combines class conditional CRFs into a uniﬁed framework for part-based object recognition. The parameters of the CRF are estimated in a maximum likelihood framework and recognition proceeds by ﬁnding the most likely class under our model. The main advantage of the proposed CRF framework is that it allows us to relax the assumption of conditional independence of the observed data (i.e. local features) often used in generative approaches, an assumption that might be too restrictive for a considerable number of object classes.</p><p>4 0.52451563 <a title="14-lsi-4" href="./nips-2004-Object_Classification_from_a_Single_Example_Utilizing_Class_Relevance_Metrics.html">134 nips-2004-Object Classification from a Single Example Utilizing Class Relevance Metrics</a></p>
<p>Author: Michael Fink</p><p>Abstract: We describe a framework for learning an object classiﬁer from a single example. This goal is achieved by emphasizing the relevant dimensions for classiﬁcation using available examples of related classes. Learning to accurately classify objects from a single training example is often unfeasible due to overﬁtting effects. However, if the instance representation provides that the distance between each two instances of the same class is smaller than the distance between any two instances from different classes, then a nearest neighbor classiﬁer could achieve perfect performance with a single training example. We therefore suggest a two stage strategy. First, learn a metric over the instances that achieves the distance criterion mentioned above, from available examples of other related classes. Then, using the single examples, deﬁne a nearest neighbor classiﬁer where distance is evaluated by the learned class relevance metric. Finding a metric that emphasizes the relevant dimensions for classiﬁcation might not be possible when restricted to linear projections. We therefore make use of a kernel based metric learning algorithm. Our setting encodes object instances as sets of locality based descriptors and adopts an appropriate image kernel for the class relevance metric learning. The proposed framework for learning from a single example is demonstrated in a synthetic setting and on a character classiﬁcation task. 1</p><p>5 0.51787537 <a title="14-lsi-5" href="./nips-2004-Joint_MRI_Bias_Removal_Using_Entropy_Minimization_Across_Images.html">89 nips-2004-Joint MRI Bias Removal Using Entropy Minimization Across Images</a></p>
<p>Author: Erik G. Learned-miller, Parvez Ahammad</p><p>Abstract: The correction of bias in magnetic resonance images is an important problem in medical image processing. Most previous approaches have used a maximum likelihood method to increase the likelihood of the pixels in a single image by adaptively estimating a correction to the unknown image bias ﬁeld. The pixel likelihoods are deﬁned either in terms of a pre-existing tissue model, or non-parametrically in terms of the image’s own pixel values. In both cases, the speciﬁc location of a pixel in the image is not used to calculate the likelihoods. We suggest a new approach in which we simultaneously eliminate the bias from a set of images of the same anatomy, but from different patients. We use the statistics from the same location across different images, rather than within an image, to eliminate bias ﬁelds from all of the images simultaneously. The method builds a “multi-resolution” non-parametric tissue model conditioned on image location while eliminating the bias ﬁelds associated with the original image set. We present experiments on both synthetic and real MR data sets, and present comparisons with other methods. 1</p><p>6 0.51748002 <a title="14-lsi-6" href="./nips-2004-The_power_of_feature_clustering%3A_An_application_to_object_detection.html">192 nips-2004-The power of feature clustering: An application to object detection</a></p>
<p>7 0.51035452 <a title="14-lsi-7" href="./nips-2004-Assignment_of_Multiplicative_Mixtures_in_Natural_Images.html">25 nips-2004-Assignment of Multiplicative Mixtures in Natural Images</a></p>
<p>8 0.50390989 <a title="14-lsi-8" href="./nips-2004-Implicit_Wiener_Series_for_Higher-Order_Image_Analysis.html">81 nips-2004-Implicit Wiener Series for Higher-Order Image Analysis</a></p>
<p>9 0.49785894 <a title="14-lsi-9" href="./nips-2004-Semigroup_Kernels_on_Finite_Sets.html">168 nips-2004-Semigroup Kernels on Finite Sets</a></p>
<p>10 0.48873746 <a title="14-lsi-10" href="./nips-2004-Face_Detection_---_Efficient_and_Rank_Deficient.html">68 nips-2004-Face Detection --- Efficient and Rank Deficient</a></p>
<p>11 0.48412964 <a title="14-lsi-11" href="./nips-2004-The_Variational_Ising_Classifier_%28VIC%29_Algorithm_for_Coherently_Contaminated_Data.html">191 nips-2004-The Variational Ising Classifier (VIC) Algorithm for Coherently Contaminated Data</a></p>
<p>12 0.47300273 <a title="14-lsi-12" href="./nips-2004-Learning_Preferences_for_Multiclass_Problems.html">100 nips-2004-Learning Preferences for Multiclass Problems</a></p>
<p>13 0.46515378 <a title="14-lsi-13" href="./nips-2004-Semi-Markov_Conditional_Random_Fields_for_Information_Extraction.html">162 nips-2004-Semi-Markov Conditional Random Fields for Information Extraction</a></p>
<p>14 0.46227011 <a title="14-lsi-14" href="./nips-2004-The_Laplacian_PDF_Distance%3A_A_Cost_Function_for_Clustering_in_a_Kernel_Feature_Space.html">188 nips-2004-The Laplacian PDF Distance: A Cost Function for Clustering in a Kernel Feature Space</a></p>
<p>15 0.4614132 <a title="14-lsi-15" href="./nips-2004-Algebraic_Set_Kernels_with_Application_to_Inference_Over_Local_Image_Representations.html">18 nips-2004-Algebraic Set Kernels with Application to Inference Over Local Image Representations</a></p>
<p>16 0.4548685 <a title="14-lsi-16" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>17 0.4535661 <a title="14-lsi-17" href="./nips-2004-PAC-Bayes_Learning_of_Conjunctions_and_Classification_of_Gene-Expression_Data.html">143 nips-2004-PAC-Bayes Learning of Conjunctions and Classification of Gene-Expression Data</a></p>
<p>18 0.45170432 <a title="14-lsi-18" href="./nips-2004-Breaking_SVM_Complexity_with_Cross-Training.html">34 nips-2004-Breaking SVM Complexity with Cross-Training</a></p>
<p>19 0.44416332 <a title="14-lsi-19" href="./nips-2004-An_Application_of_Boosting_to_Graph_Classification.html">19 nips-2004-An Application of Boosting to Graph Classification</a></p>
<p>20 0.43852955 <a title="14-lsi-20" href="./nips-2004-Kernels_for_Multi--task_Learning.html">94 nips-2004-Kernels for Multi--task Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.284), (13, 0.074), (15, 0.118), (16, 0.015), (17, 0.02), (24, 0.035), (26, 0.059), (31, 0.01), (32, 0.01), (33, 0.154), (35, 0.028), (39, 0.019), (50, 0.041), (56, 0.014), (76, 0.017), (87, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82917762 <a title="14-lda-1" href="./nips-2004-Inference%2C_Attention%2C_and_Decision_in_a_Bayesian_Neural_Architecture.html">84 nips-2004-Inference, Attention, and Decision in a Bayesian Neural Architecture</a></p>
<p>Author: Angela J. Yu, Peter Dayan</p><p>Abstract: We study the synthesis of neural coding, selective attention and perceptual decision making. A hierarchical neural architecture is proposed, which implements Bayesian integration of noisy sensory input and topdown attentional priors, leading to sound perceptual discrimination. The model offers an explicit explanation for the experimentally observed modulation that prior information in one stimulus feature (location) can have on an independent feature (orientation). The network’s intermediate levels of representation instantiate known physiological properties of visual cortical neurons. The model also illustrates a possible reconciliation of cortical and neuromodulatory representations of uncertainty. 1</p><p>same-paper 2 0.78969228 <a title="14-lda-2" href="./nips-2004-A_Topographic_Support_Vector_Machine%3A_Classification_Using_Local_Label_Configurations.html">14 nips-2004-A Topographic Support Vector Machine: Classification Using Local Label Configurations</a></p>
<p>Author: Johannes Mohr, Klaus Obermayer</p><p>Abstract: The standard approach to the classiﬁcation of objects is to consider the examples as independent and identically distributed (iid). In many real world settings, however, this assumption is not valid, because a topographical relationship exists between the objects. In this contribution we consider the special case of image segmentation, where the objects are pixels and where the underlying topography is a 2D regular rectangular grid. We introduce a classiﬁcation method which not only uses measured vectorial feature information but also the label conﬁguration within a topographic neighborhood. Due to the resulting dependence between the labels of neighboring pixels, a collective classiﬁcation of a set of pixels becomes necessary. We propose a new method called ’Topographic Support Vector Machine’ (TSVM), which is based on a topographic kernel and a self-consistent solution to the label assignment shown to be equivalent to a recurrent neural network. The performance of the algorithm is compared to a conventional SVM on a cell image segmentation task. 1</p><p>3 0.60993499 <a title="14-lda-3" href="./nips-2004-At_the_Edge_of_Chaos%3A_Real-time_Computations_and_Self-Organized_Criticality_in_Recurrent_Neural_Networks.html">26 nips-2004-At the Edge of Chaos: Real-time Computations and Self-Organized Criticality in Recurrent Neural Networks</a></p>
<p>Author: Nils Bertschinger, Thomas Natschläger, Robert A. Legenstein</p><p>Abstract: In this paper we analyze the relationship between the computational capabilities of randomly connected networks of threshold gates in the timeseries domain and their dynamical properties. In particular we propose a complexity measure which we ﬁnd to assume its highest values near the edge of chaos, i.e. the transition from ordered to chaotic dynamics. Furthermore we show that the proposed complexity measure predicts the computational capabilities very well: only near the edge of chaos are such networks able to perform complex computations on time series. Additionally a simple synaptic scaling rule for self-organized criticality is presented and analyzed. 1</p><p>4 0.60373229 <a title="14-lda-4" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>Author: Ofer Dekel, Shai Shalev-shwartz, Yoram Singer</p><p>Abstract: Prediction sufﬁx trees (PST) provide a popular and effective tool for tasks such as compression, classiﬁcation, and language modeling. In this paper we take a decision theoretic view of PSTs for the task of sequence prediction. Generalizing the notion of margin to PSTs, we present an online PST learning algorithm and derive a loss bound for it. The depth of the PST generated by this algorithm scales linearly with the length of the input. We then describe a self-bounded enhancement of our learning algorithm which automatically grows a bounded-depth PST. We also prove an analogous mistake-bound for the self-bounded algorithm. The result is an efﬁcient algorithm that neither relies on a-priori assumptions on the shape or maximal depth of the target PST nor does it require any parameters. To our knowledge, this is the ﬁrst provably-correct PST learning algorithm which generates a bounded-depth PST while being competitive with any ﬁxed PST determined in hindsight. 1</p><p>5 0.60191381 <a title="14-lda-5" href="./nips-2004-Common-Frame_Model_for_Object_Recognition.html">40 nips-2004-Common-Frame Model for Object Recognition</a></p>
<p>Author: Pierre Moreels, Pietro Perona</p><p>Abstract: A generative probabilistic model for objects in images is presented. An object consists of a constellation of features. Feature appearance and pose are modeled probabilistically. Scene images are generated by drawing a set of objects from a given database, with random clutter sprinkled on the remaining image surface. Occlusion is allowed. We study the case where features from the same object share a common reference frame. Moreover, parameters for shape and appearance densities are shared across features. This is to be contrasted with previous work on probabilistic ‘constellation’ models where features depend on each other, and each feature and model have different pose and appearance statistics [1, 2]. These two differences allow us to build models containing hundreds of features, as well as to train each model from a single example. Our model may also be thought of as a probabilistic revisitation of Lowe’s model [3, 4]. We propose an efﬁcient entropy-minimization inference algorithm that constructs the best interpretation of a scene as a collection of objects and clutter. We test our ideas with experiments on two image databases. We compare with Lowe’s algorithm and demonstrate better performance, in particular in presence of large amounts of background clutter.</p><p>6 0.6006707 <a title="14-lda-6" href="./nips-2004-Efficient_Kernel_Machines_Using_the_Improved_Fast_Gauss_Transform.html">60 nips-2004-Efficient Kernel Machines Using the Improved Fast Gauss Transform</a></p>
<p>7 0.59893692 <a title="14-lda-7" href="./nips-2004-Face_Detection_---_Efficient_and_Rank_Deficient.html">68 nips-2004-Face Detection --- Efficient and Rank Deficient</a></p>
<p>8 0.59769982 <a title="14-lda-8" href="./nips-2004-Non-Local_Manifold_Tangent_Learning.html">131 nips-2004-Non-Local Manifold Tangent Learning</a></p>
<p>9 0.5965963 <a title="14-lda-9" href="./nips-2004-Outlier_Detection_with_One-class_Kernel_Fisher_Discriminants.html">142 nips-2004-Outlier Detection with One-class Kernel Fisher Discriminants</a></p>
<p>10 0.59649974 <a title="14-lda-10" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>11 0.59637588 <a title="14-lda-11" href="./nips-2004-Fast_Rates_to_Bayes_for_Kernel_Machines.html">69 nips-2004-Fast Rates to Bayes for Kernel Machines</a></p>
<p>12 0.59593326 <a title="14-lda-12" href="./nips-2004-Worst-Case_Analysis_of_Selective_Sampling_for_Linear-Threshold_Algorithms.html">206 nips-2004-Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms</a></p>
<p>13 0.59523237 <a title="14-lda-13" href="./nips-2004-Assignment_of_Multiplicative_Mixtures_in_Natural_Images.html">25 nips-2004-Assignment of Multiplicative Mixtures in Natural Images</a></p>
<p>14 0.5950973 <a title="14-lda-14" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>15 0.59477234 <a title="14-lda-15" href="./nips-2004-A_Generalized_Bradley-Terry_Model%3A_From_Group_Competition_to_Individual_Skill.html">4 nips-2004-A Generalized Bradley-Terry Model: From Group Competition to Individual Skill</a></p>
<p>16 0.59453368 <a title="14-lda-16" href="./nips-2004-Blind_One-microphone_Speech_Separation%3A_A_Spectral_Learning_Approach.html">31 nips-2004-Blind One-microphone Speech Separation: A Spectral Learning Approach</a></p>
<p>17 0.5939461 <a title="14-lda-17" href="./nips-2004-Nonparametric_Transforms_of_Graph_Kernels_for_Semi-Supervised_Learning.html">133 nips-2004-Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning</a></p>
<p>18 0.59385163 <a title="14-lda-18" href="./nips-2004-Self-Tuning_Spectral_Clustering.html">161 nips-2004-Self-Tuning Spectral Clustering</a></p>
<p>19 0.59313828 <a title="14-lda-19" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>20 0.59311783 <a title="14-lda-20" href="./nips-2004-Limits_of_Spectral_Clustering.html">103 nips-2004-Limits of Spectral Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
