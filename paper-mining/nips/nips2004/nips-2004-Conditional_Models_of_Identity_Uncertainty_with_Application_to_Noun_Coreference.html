<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>43 nips-2004-Conditional Models of Identity Uncertainty with Application to Noun Coreference</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-43" href="#">nips2004-43</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>43 nips-2004-Conditional Models of Identity Uncertainty with Application to Noun Coreference</h1>
<br/><p>Source: <a title="nips-2004-43-pdf" href="http://papers.nips.cc/paper/2557-conditional-models-of-identity-uncertainty-with-application-to-noun-coreference.pdf">pdf</a></p><p>Author: Andrew McCallum, Ben Wellner</p><p>Abstract: Coreference analysis, also known as record linkage or identity uncertainty, is a difﬁcult and important problem in natural language processing, databases, citation matching and many other tasks. This paper introduces several discriminative, conditional-probability models for coreference analysis, all examples of undirected graphical models. Unlike many historical approaches to coreference, the models presented here are relational—they do not assume that pairwise coreference decisions should be made independently from each other. Unlike other relational models of coreference that are generative, the conditional model here can incorporate a great variety of features of the input without having to be concerned about their dependencies—paralleling the advantages of conditional random ﬁelds over hidden Markov models. We present positive results on noun phrase coreference in two standard text data sets. 1</p><p>Reference: <a title="nips-2004-43-reference" href="../nips2004_reference/nips-2004-Conditional_Models_of_Identity_Uncertainty_with_Application_to_Noun_Coreference_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu ∗  Abstract Coreference analysis, also known as record linkage or identity uncertainty, is a difﬁcult and important problem in natural language processing, databases, citation matching and many other tasks. [sent-5, score-0.216]
</p><p>2 This paper introduces several discriminative, conditional-probability models for coreference analysis, all examples of undirected graphical models. [sent-6, score-0.658]
</p><p>3 Unlike many historical approaches to coreference, the models presented here are relational—they do not assume that pairwise coreference decisions should be made independently from each other. [sent-7, score-0.624]
</p><p>4 Unlike other relational models of coreference that are generative, the conditional model here can incorporate a great variety of features of the input without having to be concerned about their dependencies—paralleling the advantages of conditional random ﬁelds over hidden Markov models. [sent-8, score-0.8]
</p><p>5 We present positive results on noun phrase coreference in two standard text data sets. [sent-9, score-0.64]
</p><p>6 1  Introduction  In many domains—including computer vision, databases and natural language processing—we ﬁnd multiple views, descriptions, or names for the same underlying object. [sent-10, score-0.108]
</p><p>7 In natural language processing, coreference analysis ﬁnds the nouns, pronouns and phrases that refer to the same entity, enabling the extraction of relations among entities as well as more complex propositions. [sent-14, score-0.823]
</p><p>8 Consider, for example, the text in a news article that discusses the entities George Bush, Colin Powell, and Donald Rumsfeld. [sent-15, score-0.281]
</p><p>9 The article contains multiple mentions of Colin Powell by different strings—“Secretary of State Colin Powell,” “he,” “Mr. [sent-16, score-0.507]
</p><p>10 Powell,” “the Secretary”—and also refers to the other two entities with sometimes overlapping strings. [sent-17, score-0.235]
</p><p>11 The coreference task is to use the content and context of all the mentions to determine how many entities are in the article, and which mention corresponds to which entity. [sent-18, score-1.337]
</p><p>12 This task is most frequently solved by examining individual pair-wise distance measures between mentions independently of each other. [sent-19, score-0.535]
</p><p>13 For example, database record-linkage and citation reference matching has been performed by learning a pairwise distance metric between records, and setting a distance threshold below which records are merged (Monge  & Elkan, 1997; McCallum et al. [sent-20, score-0.374]
</p><p>14 Coreference in NLP has also been performed with distance thresholds or pairwise classiﬁers (McCarthy & Lehnert, 1995; Ge et al. [sent-22, score-0.2]
</p><p>15 But these distance measures are inherently noisy and the answer to one pair-wise coreference decision may not be independent of another. [sent-25, score-0.551]
</p><p>16 Powell” may be correctly coresolved with “Powell,” but particular grammatical circumstances may make the model incorrectly believe that “Powell” is coreferent with a nearby occurrence of “she. [sent-28, score-0.098]
</p><p>17 ” Inconsistencies might be better resolved if the coreference decisions are made in dependent relation to each other, and in a way that accounts for the values of the multiple distances, instead of a threshold on single pairs independently. [sent-29, score-0.544]
</p><p>18 (2003) have proposed a formal, relational approach to the problem of identity uncertainty using a type of Bayesian network called a Relational Probabilistic Model (Friedman et al. [sent-31, score-0.394]
</p><p>19 A great strength of this model is that it explicitly captures the dependence among multiple coreference decisions. [sent-33, score-0.489]
</p><p>20 However, it is a generative model of the entities, mentions and all their features, and thus has difﬁculty using many features that are highly overlapping, non-independent, at varying levels of granularity, and with long-range dependencies. [sent-34, score-0.514]
</p><p>21 In this area signiﬁcant recent success has been achieved by replacing a generative model—hidden Markov models—with a conditional model—conditional random ﬁelds (CRFs) (Lafferty et al. [sent-38, score-0.191]
</p><p>22 ), matched champion noun phrase segmentation results (Sha & Pereira, 2003), and signiﬁcantly improved extraction of named entities (McCallum & Li, 2003), citation data (Peng & McCallum, 2004), and the segmentation of tables in government reports (Pinto et al. [sent-41, score-0.547]
</p><p>23 This paper introduces three conditional undirected graphical models for identity uncertainty. [sent-45, score-0.311]
</p><p>24 The models condition on the mentions, and generate the coreference decisions, (and in some cases also generate attributes of the entities). [sent-46, score-0.637]
</p><p>25 In the ﬁrst most general model, the dependency structure is unrestricted, and the number of underlying entities explicitly appears in the model structure. [sent-47, score-0.207]
</p><p>26 The second and third models have no structural dependence on the number of entities, and fall into a class of Markov random ﬁelds in which inference corresponds to graph partitioning (Boykov et al. [sent-48, score-0.262]
</p><p>27 In both domains we take advantage of the ability to use arbitrary, overlapping features of the input, including multiple grammatical features, string equality, substring, and acronym matches. [sent-51, score-0.116]
</p><p>28 Using the same features, in comparison with an alternative natural language processing technique, we reduce error by 33% and 28% in the two domains on proper nouns and by 10% on all nouns in the MUC-6 data. [sent-52, score-0.269]
</p><p>29 2  Three Conditional Models of Identity Uncertainty  We now describe three possible conﬁgurations for conditional models of identity uncertainty, each progressively simpler and more speciﬁc than its predecessor. [sent-53, score-0.17]
</p><p>30 All three are based on conditionally-trained, undirected graphical models. [sent-54, score-0.141]
</p><p>31 Undirected graphical models, also known as Markov networks or Markov random ﬁelds, are a type of probabilistic model that excels at capturing interdependent data in which causality among attributes is not apparent. [sent-55, score-0.188]
</p><p>32 We begin by introducing notation for mentions, entities and attributes of entities, then in the following subsections describe the likelihood, inference and estimation procedures for the speciﬁc undirected graphical models. [sent-56, score-0.499]
</p><p>33 Yn ) be a collection of random variables over integer identiﬁers, unique to each entity, specifying to which entity a mention refers. [sent-67, score-0.341]
</p><p>34 Thus the y’s are integers ranging from 1 to m, and if Yi = Yj , then mention Xi is said to refer to the same underlying entity as Xj . [sent-68, score-0.313]
</p><p>35 Powell, may be mentioned multiple times in a news article that also contains mentions of other entities: x6 may be “Colin Powell”; x9 may be “he”; x17 may be “the Secretary of State. [sent-72, score-0.547]
</p><p>36 Let A be a random variable over the collection of all attributes for all entities. [sent-75, score-0.148]
</p><p>37 Borrowing the notation of Relational Markov Networks (Taskar et al. [sent-76, score-0.086]
</p><p>38 , 2002), we write the random variable over the attributes of entity Es as Es . [sent-77, score-0.265]
</p><p>39 For example, these three attributes may be gender, birth year, and surname. [sent-85, score-0.12]
</p><p>40 One can interpret the attributes as the values that should appear in the ﬁelds of a database record for the given entity. [sent-90, score-0.12]
</p><p>41 Attributes such as surname may take on one of the ﬁnite number of values that appear in the mentions of the data set. [sent-91, score-0.538]
</p><p>42 Separate measured features of the mentions and entity-assignments, y, are captured in different feature functions, f (·), over cliques in the graphical model. [sent-93, score-0.624]
</p><p>43 The task is then to ﬁnd the most likely collection of entity-assignments, y, (and optionally also the most likely entity attributes, a), given a collection of mentions and their context, x. [sent-97, score-0.674]
</p><p>44 A generative probabilistic model of identity uncertainty is trained to maximize P (Y, A, X). [sent-98, score-0.194]
</p><p>45 A conditional probabilistic model of identity uncertainty is instead trained to maximize P (Y, A|X), or simply P (Y|X). [sent-99, score-0.217]
</p><p>46 1  Model 1: Groups of nodes for entities  First we consider an extremely general undirected graphical model in which there is a node for the mentions, x,1 a node for the entity-assignment of each mention, y, and a node for each of the attributes of each entity, e. [sent-101, score-0.635]
</p><p>47 1 Even though there are many mentions in x, because we are not generating them, we can represent them as a single node. [sent-104, score-0.473]
</p><p>48 This helps show that feature functions can ask arbitrary questions about various large and small subsets of the mentions and their context. [sent-105, score-0.502]
</p><p>49 Typically the parameters on many different cliques would be tied in patterns that reﬂect the nature of the repeated relational structure in the data. [sent-108, score-0.252]
</p><p>50 Patterns of tied parameters are common in many graphical models, including HMMs and other ﬁnite state machines (Lafferty et al. [sent-109, score-0.197]
</p><p>51 , 2001), where they are tied across different positions in the input sequence, and by more complex patterns based on SQL-like queries, as in Markov Relational Networks (Taskar et al. [sent-110, score-0.129]
</p><p>52 Following the nomenclature of the later, these parameter-tying-patterns are called clique templates; each particular instance a template in the graph we call a hit. [sent-112, score-0.121]
</p><p>53 For example, one clique template may specify a pattern consisting of two mentions, their entity-assignment nodes, and an entity’s surname attribute node. [sent-113, score-0.193]
</p><p>54 One feature function might have value 1 if, for example, both mentions were assigned to the same entity as the surname node, and if the surname value appears as a substring in both mention strings (and value 0 otherwise). [sent-116, score-0.994]
</p><p>55 The Hammersley-Clifford theorem stipulates that the probability of a particular set of values on the random variables in an undirected graphical model is a product of potential functions over cliques of the graph. [sent-117, score-0.195]
</p><p>56 But in our problem the number of entities (and thus number of attribute nodes, and the domain of the entity-assignment nodes) is unknown. [sent-133, score-0.259]
</p><p>57 In related work on a generative probabilistic model of identity uncertainty, Pasula et al. [sent-135, score-0.205]
</p><p>58 (2) The per-entity attribute nodes A are removed and replaced with attribute nodes associated with each mention; we write xi . [sent-139, score-0.285]
</p><p>59 Even though the clique templates are now restricted to pairs of mentions, this does not imply that pairwise coreference decisions are made independently of each other—they are  still highly dependent. [sent-141, score-0.709]
</p><p>60 For example, we may have an “inconsistent triangle” of coreference decisions in which yij and yjk are 1, while yik is 0. [sent-145, score-0.829]
</p><p>61 We can enforce the impossibility of all inconsistent conﬁgurations by adding inconsistencychecking functions f∗ (yij , yjk , yik ) for all mention triples, with the corresponding λ∗ ’s ﬁxed at negative inﬁnity—thus assigning zero probability to them. [sent-146, score-0.35]
</p><p>62 ) Thus we have   1 P (y, a|x) = exp  λl fl (xi , xj , yij , xi . [sent-148, score-0.338]
</p><p>63 Zx i,j,l  i,j,k  We can also enforce consistency among the attributes of coreferent mentions by similar means. [sent-151, score-0.658]
</p><p>64 In this case, we could also restrict fl (xi , xj , yij ) ≡ 0, ∀yij = 0. [sent-156, score-0.305]
</p><p>65 3  Model 3: Nodes for mention pairs, graph partitioning with learned distance  When gathering attributes of entities is not necessary, we can avoid the extra complication of attributes by removing them from the model. [sent-158, score-0.794]
</p><p>66 What results is a straightforward, yet highly expressive, discriminatively-trained, undirected graphical model that can use rich feature sets and relational inference to solve identity uncertainty tasks. [sent-159, score-0.509]
</p><p>67 Determining the most likely number of entities falls naturally out of inference. [sent-160, score-0.207]
</p><p>68 The model is   1 P (y|x) = λl fl (xi , xj , yij ) + λ∗ f∗ (yij , yjk , yik ) . [sent-161, score-0.435]
</p><p>69 (1) exp  Zx i,j,l  i,j,k  Recently there has been increasing interest in study of the equivalence between graph partitioning algorithms and inference in certain kinds of undirected graphical models, e. [sent-162, score-0.289]
</p><p>70 Unlike classic mincut/maxﬂow binary partitioning, here the number of partitions (corresponding to entities) is unknown, but a single optimal number of partitions exists; negative edge weights encourage more partitions. [sent-168, score-0.104]
</p><p>71 The resulting solution does not make pairwise coreference decisions independently of each other. [sent-176, score-0.596]
</p><p>72 Powell”/“Powell”/“she” problem discussed in the introduction would be  prevented by this model because, although the distance between “Powell” and “she” might grammatically look low, the distance from “she” to another member of the same partition, (“Mr. [sent-179, score-0.124]
</p><p>73 Interestingly, in our model, the distance measure between nodes is learned from labeled training data. [sent-181, score-0.136]
</p><p>74 That is, we use data, D, in which the correct coreference partitions are known in order to learn a distance metric such that, when the same data is clustered, the correct partitions emerge. [sent-182, score-0.655]
</p><p>75 The derivative of the log-likelihood, L, is   ∂L  = fl (xi , xj , yij ) − PΛ (y |x) fl (xi , xj , yij ) , ∂λl x,y ∈D  i,j,l  y  i,j,l  where PΛ (y |x) is deﬁned by Equation 1, using the current set of λ parameters, Λ, and y is a sum over all possible partitionings. [sent-185, score-0.61]
</p><p>76 3  Experiments with Noun Coreference  We present experimental results on natural language noun phrase coreference using Model 3 applied to two applicable data sets: the DARPA MUC-6 corpus, and a set of 117 stories from the broadcast news portion of the DARPA ACE data set. [sent-191, score-0.78]
</p><p>77 The ﬁrst is single-link clustering with a threshold, (single-link-threshold), which is universally used in database record-linkage and citation reference matching (Monge & Elkan, 1997; Bilenko & Mooney, 2002; McCallum et al. [sent-195, score-0.157]
</p><p>78 It forms partitions by simply collapsing the spanning trees of all mentions with pairwise distances below some threshold. [sent-197, score-0.577]
</p><p>79 The second technique, which we call best-previous-match, has been used in natural language processing applications (Morton, 1997; Ge et al. [sent-199, score-0.153]
</p><p>80 It works by scanning linearly through a document, and associating each mention with its best-matching predecessor—best as measured with a single pairwise distance. [sent-201, score-0.22]
</p><p>81 The features, typical of those used in many other NLP coreference systems, are modeled after those in Ng and Cardie (2002). [sent-204, score-0.489]
</p><p>82 They include tests for string and substring matches, acronym  matches, parse-derived head-word matches, gender, W ORD N ET subsumption, sentence distance, distance in the parse tree; etc. [sent-205, score-0.139]
</p><p>83 (2001), we consider only mentions that have a coreferent. [sent-212, score-0.473]
</p><p>84 Model 3 out-performs both the single-link-threshold and the best-previousmatch techniques, reducing error by 28% over single-link-threshold on the ACE proper noun data, by 24% on the MUC-6 proper noun data, and by 10% over the best-previousmatch technique on the full MUC-6 task. [sent-213, score-0.324]
</p><p>85 4  Related Work and Conclusions  There has been much related work on identity uncertainty in various speciﬁc ﬁelds. [sent-228, score-0.153]
</p><p>86 Traditional work in de-duplication for databases or reference-matching for citations measures the distance between two records by some metric, and then collapses all records at a distance below a threshold, e. [sent-229, score-0.247]
</p><p>87 Pairwise coreference learned distance measures have used decision trees (McCarthy & Lehnert, 1995; Ng & Cardie, 2002), SVMs (Zelenko et al. [sent-236, score-0.637]
</p><p>88 , 2003), maximum entropy classiﬁers (Morton, 1997), and generative probabilistic models (Ge et al. [sent-237, score-0.155]
</p><p>89 But all use thresholds on a single pairwise distance, or the maximum of a single pairwise distance to determine if or where a coreferent merge should occur. [sent-239, score-0.231]
</p><p>90 (2003) introduce a generative probability model for identity uncertainty based on Probabilistic Relational Networks networks. [sent-241, score-0.194]
</p><p>91 Our work is an attempt to gain some of the same advantages that CRFs have over HMMs by creating conditional models of identity uncertainty. [sent-242, score-0.17]
</p><p>92 The models presented here, as instances of conditionally-trained undirected graphical models, are also instances of relational Markov networks (Taskar et al. [sent-243, score-0.41]
</p><p>93 (2002) brieﬂy discuss clustering of dyadic data, such as people and their movie preferences, but not identity uncertainty or inference by graph partitioning. [sent-247, score-0.229]
</p><p>94 In natural language processing, it is not only especially difﬁcult, but also extremely important, since improved coreference resolution is one of the chief barriers to effective data mining of text data. [sent-249, score-0.585]
</p><p>95 Natural language data is a domain that has particularly beneﬁted from rich and overlapping feature representations—representations that lend themselves better to conditional probability models than generative ones (Lafferty et al. [sent-250, score-0.343]
</p><p>96 Hence our interest in conditional models of identity uncertainty. [sent-252, score-0.17]
</p><p>97 Learning to combine trained distance metrics for duplicate detection in databases (Technical Report Technical Report AI 02-296). [sent-266, score-0.136]
</p><p>98 Early results for named entity recognition with conditional random ﬁelds, feature induction and web-enhanced lexicons. [sent-316, score-0.238]
</p><p>99 Accurate information extraction from research papers using conditional random ﬁelds. [sent-360, score-0.096]
</p><p>100 A machine learning approach to coreference resolution of noun phrases. [sent-383, score-0.606]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('coreference', 0.489), ('mentions', 0.473), ('entities', 0.207), ('powell', 0.184), ('mention', 0.168), ('yij', 0.155), ('relational', 0.155), ('entity', 0.145), ('attributes', 0.12), ('noun', 0.117), ('fl', 0.103), ('mccallum', 0.095), ('et', 0.086), ('elkan', 0.082), ('monge', 0.082), ('morton', 0.082), ('richman', 0.082), ('identity', 0.078), ('clique', 0.076), ('uncertainty', 0.075), ('nodes', 0.074), ('undirected', 0.073), ('taskar', 0.073), ('partitioning', 0.072), ('citation', 0.071), ('colin', 0.071), ('zx', 0.071), ('graphical', 0.068), ('language', 0.067), ('bilenko', 0.065), ('coreferent', 0.065), ('mooney', 0.065), ('secretary', 0.065), ('surname', 0.065), ('yik', 0.065), ('yjk', 0.065), ('nouns', 0.065), ('conditional', 0.064), ('distance', 0.062), ('cardie', 0.06), ('pasula', 0.057), ('decisions', 0.055), ('cohen', 0.054), ('ht', 0.054), ('cliques', 0.054), ('partitions', 0.052), ('pairwise', 0.052), ('inconsistent', 0.052), ('attribute', 0.052), ('elds', 0.051), ('boykov', 0.049), ('harabagiu', 0.049), ('lehnert', 0.049), ('substring', 0.049), ('ace', 0.048), ('xj', 0.047), ('ge', 0.046), ('graph', 0.045), ('proper', 0.045), ('partition', 0.045), ('darpa', 0.045), ('ng', 0.044), ('lafferty', 0.044), ('mccarthy', 0.043), ('tied', 0.043), ('records', 0.041), ('linguistics', 0.041), ('generative', 0.041), ('databases', 0.041), ('markov', 0.04), ('news', 0.04), ('pereira', 0.04), ('bansal', 0.039), ('templates', 0.037), ('nlp', 0.036), ('collins', 0.035), ('phrase', 0.034), ('article', 0.034), ('xi', 0.033), ('broadcast', 0.033), ('duplicate', 0.033), ('grammatical', 0.033), ('pinto', 0.033), ('zelenko', 0.033), ('crfs', 0.032), ('extraction', 0.032), ('inference', 0.031), ('node', 0.031), ('feature', 0.029), ('hmms', 0.029), ('mining', 0.029), ('acronym', 0.028), ('austin', 0.028), ('pronouns', 0.028), ('wellner', 0.028), ('models', 0.028), ('collection', 0.028), ('overlapping', 0.028), ('domains', 0.027), ('ascent', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="43-tfidf-1" href="./nips-2004-Conditional_Models_of_Identity_Uncertainty_with_Application_to_Noun_Coreference.html">43 nips-2004-Conditional Models of Identity Uncertainty with Application to Noun Coreference</a></p>
<p>Author: Andrew McCallum, Ben Wellner</p><p>Abstract: Coreference analysis, also known as record linkage or identity uncertainty, is a difﬁcult and important problem in natural language processing, databases, citation matching and many other tasks. This paper introduces several discriminative, conditional-probability models for coreference analysis, all examples of undirected graphical models. Unlike many historical approaches to coreference, the models presented here are relational—they do not assume that pairwise coreference decisions should be made independently from each other. Unlike other relational models of coreference that are generative, the conditional model here can incorporate a great variety of features of the input without having to be concerned about their dependencies—paralleling the advantages of conditional random ﬁelds over hidden Markov models. We present positive results on noun phrase coreference in two standard text data sets. 1</p><p>2 0.13034458 <a title="43-tfidf-2" href="./nips-2004-Semi-Markov_Conditional_Random_Fields_for_Information_Extraction.html">162 nips-2004-Semi-Markov Conditional Random Fields for Information Extraction</a></p>
<p>Author: Sunita Sarawagi, William W. Cohen</p><p>Abstract: We describe semi-Markov conditional random ﬁelds (semi-CRFs), a conditionally trained version of semi-Markov chains. Intuitively, a semiCRF on an input sequence x outputs a “segmentation” of x, in which labels are assigned to segments (i.e., subsequences) of x rather than to individual elements xi of x. Importantly, features for semi-CRFs can measure properties of segments, and transitions within a segment can be non-Markovian. In spite of this additional power, exact learning and inference algorithms for semi-CRFs are polynomial-time—often only a small constant factor slower than conventional CRFs. In experiments on ﬁve named entity recognition problems, semi-CRFs generally outperform conventional CRFs. 1</p><p>3 0.082739845 <a title="43-tfidf-3" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>Author: Peter L. Bartlett, Michael Collins, Ben Taskar, David A. McAllester</p><p>Abstract: We consider the problem of structured classiﬁcation, where the task is to predict a label y from an input x, and y has meaningful internal structure. Our framework includes supervised training of Markov random ﬁelds and weighted context-free grammars as special cases. We describe an algorithm that solves the large-margin optimization problem deﬁned in [12], using an exponential-family (Gibbs distribution) representation of structured objects. The algorithm is efﬁcient—even in cases where the number of labels y is exponential in size—provided that certain expectations under Gibbs distributions can be calculated efﬁciently. The method for structured labels relies on a more general result, speciﬁcally the application of exponentiated gradient updates [7, 8] to quadratic programs. 1</p><p>4 0.080689959 <a title="43-tfidf-4" href="./nips-2004-Learning_Syntactic_Patterns_for_Automatic_Hypernym_Discovery.html">101 nips-2004-Learning Syntactic Patterns for Automatic Hypernym Discovery</a></p>
<p>Author: Rion Snow, Daniel Jurafsky, Andrew Y. Ng</p><p>Abstract: Semantic taxonomies such as WordNet provide a rich source of knowledge for natural language processing applications, but are expensive to build, maintain, and extend. Motivated by the problem of automatically constructing and extending such taxonomies, in this paper we present a new algorithm for automatically learning hypernym (is-a) relations from text. Our method generalizes earlier work that had relied on using small numbers of hand-crafted regular expression patterns to identify hypernym pairs. Using “dependency path” features extracted from parse trees, we introduce a general-purpose formalization and generalization of these patterns. Given a training set of text containing known hypernym pairs, our algorithm automatically extracts useful dependency paths and applies them to new corpora to identify novel pairs. On our evaluation task (determining whether two nouns in a news article participate in a hypernym relationship), our automatically extracted database of hypernyms attains both higher precision and higher recall than WordNet. 1</p><p>5 0.075868629 <a title="43-tfidf-5" href="./nips-2004-Conditional_Random_Fields_for_Object_Recognition.html">44 nips-2004-Conditional Random Fields for Object Recognition</a></p>
<p>Author: Ariadna Quattoni, Michael Collins, Trevor Darrell</p><p>Abstract: We present a discriminative part-based approach for the recognition of object classes from unsegmented cluttered scenes. Objects are modeled as ﬂexible constellations of parts conditioned on local observations found by an interest operator. For each object class the probability of a given assignment of parts to local features is modeled by a Conditional Random Field (CRF). We propose an extension of the CRF framework that incorporates hidden variables and combines class conditional CRFs into a uniﬁed framework for part-based object recognition. The parameters of the CRF are estimated in a maximum likelihood framework and recognition proceeds by ﬁnding the most likely class under our model. The main advantage of the proposed CRF framework is that it allows us to relax the assumption of conditional independence of the observed data (i.e. local features) often used in generative approaches, an assumption that might be too restrictive for a considerable number of object classes.</p><p>6 0.061032001 <a title="43-tfidf-6" href="./nips-2004-Semi-supervised_Learning_with_Penalized_Probabilistic_Clustering.html">167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</a></p>
<p>7 0.057823859 <a title="43-tfidf-7" href="./nips-2004-Who%27s_In_the_Picture.html">205 nips-2004-Who's In the Picture</a></p>
<p>8 0.049714003 <a title="43-tfidf-8" href="./nips-2004-Semi-supervised_Learning_on_Directed_Graphs.html">165 nips-2004-Semi-supervised Learning on Directed Graphs</a></p>
<p>9 0.04891035 <a title="43-tfidf-9" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>10 0.045270715 <a title="43-tfidf-10" href="./nips-2004-Markov_Networks_for_Detecting_Overalpping_Elements_in_Sequence_Data.html">108 nips-2004-Markov Networks for Detecting Overalpping Elements in Sequence Data</a></p>
<p>11 0.045019485 <a title="43-tfidf-11" href="./nips-2004-Euclidean_Embedding_of_Co-Occurrence_Data.html">62 nips-2004-Euclidean Embedding of Co-Occurrence Data</a></p>
<p>12 0.044665456 <a title="43-tfidf-12" href="./nips-2004-Hierarchical_Distributed_Representations_for_Statistical_Language_Modeling.html">78 nips-2004-Hierarchical Distributed Representations for Statistical Language Modeling</a></p>
<p>13 0.043632932 <a title="43-tfidf-13" href="./nips-2004-Using_Random_Forests_in_the_Structured_Language_Model.html">200 nips-2004-Using Random Forests in the Structured Language Model</a></p>
<p>14 0.043629237 <a title="43-tfidf-14" href="./nips-2004-PAC-Bayes_Learning_of_Conjunctions_and_Classification_of_Gene-Expression_Data.html">143 nips-2004-PAC-Bayes Learning of Conjunctions and Classification of Gene-Expression Data</a></p>
<p>15 0.042893969 <a title="43-tfidf-15" href="./nips-2004-The_Laplacian_PDF_Distance%3A_A_Cost_Function_for_Clustering_in_a_Kernel_Feature_Space.html">188 nips-2004-The Laplacian PDF Distance: A Cost Function for Clustering in a Kernel Feature Space</a></p>
<p>16 0.04235315 <a title="43-tfidf-16" href="./nips-2004-Integrating_Topics_and_Syntax.html">87 nips-2004-Integrating Topics and Syntax</a></p>
<p>17 0.0409717 <a title="43-tfidf-17" href="./nips-2004-Distributed_Occlusion_Reasoning_for_Tracking_with_Nonparametric_Belief_Propagation.html">55 nips-2004-Distributed Occlusion Reasoning for Tracking with Nonparametric Belief Propagation</a></p>
<p>18 0.04058386 <a title="43-tfidf-18" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>19 0.040510863 <a title="43-tfidf-19" href="./nips-2004-Instance-Specific_Bayesian_Model_Averaging_for_Classification.html">86 nips-2004-Instance-Specific Bayesian Model Averaging for Classification</a></p>
<p>20 0.039907776 <a title="43-tfidf-20" href="./nips-2004-Maximum_Margin_Clustering.html">115 nips-2004-Maximum Margin Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.147), (1, 0.033), (2, -0.003), (3, -0.042), (4, 0.059), (5, 0.07), (6, -0.008), (7, 0.101), (8, -0.065), (9, -0.058), (10, 0.016), (11, 0.028), (12, -0.11), (13, 0.084), (14, 0.01), (15, 0.017), (16, 0.049), (17, 0.036), (18, 0.033), (19, 0.063), (20, 0.014), (21, 0.011), (22, -0.044), (23, -0.063), (24, -0.055), (25, -0.012), (26, 0.059), (27, -0.001), (28, -0.092), (29, 0.01), (30, -0.124), (31, 0.038), (32, 0.028), (33, 0.019), (34, -0.004), (35, 0.025), (36, -0.079), (37, -0.158), (38, -0.011), (39, -0.018), (40, -0.121), (41, -0.06), (42, 0.127), (43, 0.062), (44, 0.008), (45, -0.054), (46, -0.006), (47, 0.16), (48, 0.034), (49, -0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91428119 <a title="43-lsi-1" href="./nips-2004-Conditional_Models_of_Identity_Uncertainty_with_Application_to_Noun_Coreference.html">43 nips-2004-Conditional Models of Identity Uncertainty with Application to Noun Coreference</a></p>
<p>Author: Andrew McCallum, Ben Wellner</p><p>Abstract: Coreference analysis, also known as record linkage or identity uncertainty, is a difﬁcult and important problem in natural language processing, databases, citation matching and many other tasks. This paper introduces several discriminative, conditional-probability models for coreference analysis, all examples of undirected graphical models. Unlike many historical approaches to coreference, the models presented here are relational—they do not assume that pairwise coreference decisions should be made independently from each other. Unlike other relational models of coreference that are generative, the conditional model here can incorporate a great variety of features of the input without having to be concerned about their dependencies—paralleling the advantages of conditional random ﬁelds over hidden Markov models. We present positive results on noun phrase coreference in two standard text data sets. 1</p><p>2 0.70075846 <a title="43-lsi-2" href="./nips-2004-Semi-Markov_Conditional_Random_Fields_for_Information_Extraction.html">162 nips-2004-Semi-Markov Conditional Random Fields for Information Extraction</a></p>
<p>Author: Sunita Sarawagi, William W. Cohen</p><p>Abstract: We describe semi-Markov conditional random ﬁelds (semi-CRFs), a conditionally trained version of semi-Markov chains. Intuitively, a semiCRF on an input sequence x outputs a “segmentation” of x, in which labels are assigned to segments (i.e., subsequences) of x rather than to individual elements xi of x. Importantly, features for semi-CRFs can measure properties of segments, and transitions within a segment can be non-Markovian. In spite of this additional power, exact learning and inference algorithms for semi-CRFs are polynomial-time—often only a small constant factor slower than conventional CRFs. In experiments on ﬁve named entity recognition problems, semi-CRFs generally outperform conventional CRFs. 1</p><p>3 0.65833908 <a title="43-lsi-3" href="./nips-2004-Learning_Syntactic_Patterns_for_Automatic_Hypernym_Discovery.html">101 nips-2004-Learning Syntactic Patterns for Automatic Hypernym Discovery</a></p>
<p>Author: Rion Snow, Daniel Jurafsky, Andrew Y. Ng</p><p>Abstract: Semantic taxonomies such as WordNet provide a rich source of knowledge for natural language processing applications, but are expensive to build, maintain, and extend. Motivated by the problem of automatically constructing and extending such taxonomies, in this paper we present a new algorithm for automatically learning hypernym (is-a) relations from text. Our method generalizes earlier work that had relied on using small numbers of hand-crafted regular expression patterns to identify hypernym pairs. Using “dependency path” features extracted from parse trees, we introduce a general-purpose formalization and generalization of these patterns. Given a training set of text containing known hypernym pairs, our algorithm automatically extracts useful dependency paths and applies them to new corpora to identify novel pairs. On our evaluation task (determining whether two nouns in a news article participate in a hypernym relationship), our automatically extracted database of hypernyms attains both higher precision and higher recall than WordNet. 1</p><p>4 0.47443169 <a title="43-lsi-4" href="./nips-2004-Conditional_Random_Fields_for_Object_Recognition.html">44 nips-2004-Conditional Random Fields for Object Recognition</a></p>
<p>Author: Ariadna Quattoni, Michael Collins, Trevor Darrell</p><p>Abstract: We present a discriminative part-based approach for the recognition of object classes from unsegmented cluttered scenes. Objects are modeled as ﬂexible constellations of parts conditioned on local observations found by an interest operator. For each object class the probability of a given assignment of parts to local features is modeled by a Conditional Random Field (CRF). We propose an extension of the CRF framework that incorporates hidden variables and combines class conditional CRFs into a uniﬁed framework for part-based object recognition. The parameters of the CRF are estimated in a maximum likelihood framework and recognition proceeds by ﬁnding the most likely class under our model. The main advantage of the proposed CRF framework is that it allows us to relax the assumption of conditional independence of the observed data (i.e. local features) often used in generative approaches, an assumption that might be too restrictive for a considerable number of object classes.</p><p>5 0.44550416 <a title="43-lsi-5" href="./nips-2004-Markov_Networks_for_Detecting_Overalpping_Elements_in_Sequence_Data.html">108 nips-2004-Markov Networks for Detecting Overalpping Elements in Sequence Data</a></p>
<p>Author: Mark Craven, Joseph Bockhorst</p><p>Abstract: Many sequential prediction tasks involve locating instances of patterns in sequences. Generative probabilistic language models, such as hidden Markov models (HMMs), have been successfully applied to many of these tasks. A limitation of these models however, is that they cannot naturally handle cases in which pattern instances overlap in arbitrary ways. We present an alternative approach, based on conditional Markov networks, that can naturally represent arbitrarily overlapping elements. We show how to eﬃciently train and perform inference with these models. Experimental results from a genomics domain show that our models are more accurate at locating instances of overlapping patterns than are baseline models based on HMMs. 1</p><p>6 0.43225667 <a title="43-lsi-6" href="./nips-2004-Integrating_Topics_and_Syntax.html">87 nips-2004-Integrating Topics and Syntax</a></p>
<p>7 0.41932696 <a title="43-lsi-7" href="./nips-2004-Modelling_Uncertainty_in_the_Game_of_Go.html">122 nips-2004-Modelling Uncertainty in the Game of Go</a></p>
<p>8 0.4191514 <a title="43-lsi-8" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>9 0.41786149 <a title="43-lsi-9" href="./nips-2004-Maximal_Margin_Labeling_for_Multi-Topic_Text_Categorization.html">111 nips-2004-Maximal Margin Labeling for Multi-Topic Text Categorization</a></p>
<p>10 0.40945074 <a title="43-lsi-10" href="./nips-2004-Mass_Meta-analysis_in_Talairach_Space.html">109 nips-2004-Mass Meta-analysis in Talairach Space</a></p>
<p>11 0.40507212 <a title="43-lsi-11" href="./nips-2004-Who%27s_In_the_Picture.html">205 nips-2004-Who's In the Picture</a></p>
<p>12 0.3959353 <a title="43-lsi-12" href="./nips-2004-Using_Machine_Learning_to_Break_Visual_Human_Interaction_Proofs_%28HIPs%29.html">199 nips-2004-Using Machine Learning to Break Visual Human Interaction Proofs (HIPs)</a></p>
<p>13 0.39025459 <a title="43-lsi-13" href="./nips-2004-Result_Analysis_of_the_NIPS_2003_Feature_Selection_Challenge.html">156 nips-2004-Result Analysis of the NIPS 2003 Feature Selection Challenge</a></p>
<p>14 0.3849968 <a title="43-lsi-14" href="./nips-2004-The_Laplacian_PDF_Distance%3A_A_Cost_Function_for_Clustering_in_a_Kernel_Feature_Space.html">188 nips-2004-The Laplacian PDF Distance: A Cost Function for Clustering in a Kernel Feature Space</a></p>
<p>15 0.38380486 <a title="43-lsi-15" href="./nips-2004-Theories_of_Access_Consciousness.html">193 nips-2004-Theories of Access Consciousness</a></p>
<p>16 0.37668079 <a title="43-lsi-16" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>17 0.37369642 <a title="43-lsi-17" href="./nips-2004-Hierarchical_Distributed_Representations_for_Statistical_Language_Modeling.html">78 nips-2004-Hierarchical Distributed Representations for Statistical Language Modeling</a></p>
<p>18 0.35867959 <a title="43-lsi-18" href="./nips-2004-On_Semi-Supervised_Classification.html">136 nips-2004-On Semi-Supervised Classification</a></p>
<p>19 0.3462534 <a title="43-lsi-19" href="./nips-2004-Trait_Selection_for_Assessing_Beef_Meat_Quality_Using_Non-linear_SVM.html">195 nips-2004-Trait Selection for Assessing Beef Meat Quality Using Non-linear SVM</a></p>
<p>20 0.3437252 <a title="43-lsi-20" href="./nips-2004-Economic_Properties_of_Social_Networks.html">57 nips-2004-Economic Properties of Social Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.048), (15, 0.099), (17, 0.406), (26, 0.04), (31, 0.035), (32, 0.024), (33, 0.16), (35, 0.014), (39, 0.017), (50, 0.031), (51, 0.013), (53, 0.014), (86, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76554 <a title="43-lda-1" href="./nips-2004-Conditional_Models_of_Identity_Uncertainty_with_Application_to_Noun_Coreference.html">43 nips-2004-Conditional Models of Identity Uncertainty with Application to Noun Coreference</a></p>
<p>Author: Andrew McCallum, Ben Wellner</p><p>Abstract: Coreference analysis, also known as record linkage or identity uncertainty, is a difﬁcult and important problem in natural language processing, databases, citation matching and many other tasks. This paper introduces several discriminative, conditional-probability models for coreference analysis, all examples of undirected graphical models. Unlike many historical approaches to coreference, the models presented here are relational—they do not assume that pairwise coreference decisions should be made independently from each other. Unlike other relational models of coreference that are generative, the conditional model here can incorporate a great variety of features of the input without having to be concerned about their dependencies—paralleling the advantages of conditional random ﬁelds over hidden Markov models. We present positive results on noun phrase coreference in two standard text data sets. 1</p><p>2 0.73255134 <a title="43-lda-2" href="./nips-2004-The_Variational_Ising_Classifier_%28VIC%29_Algorithm_for_Coherently_Contaminated_Data.html">191 nips-2004-The Variational Ising Classifier (VIC) Algorithm for Coherently Contaminated Data</a></p>
<p>Author: Oliver Williams, Andrew Blake, Roberto Cipolla</p><p>Abstract: There has been substantial progress in the past decade in the development of object classiﬁers for images, for example of faces, humans and vehicles. Here we address the problem of contaminations (e.g. occlusion, shadows) in test images which have not explicitly been encountered in training data. The Variational Ising Classiﬁer (VIC) algorithm models contamination as a mask (a ﬁeld of binary variables) with a strong spatial coherence prior. Variational inference is used to marginalize over contamination and obtain robust classiﬁcation. In this way the VIC approach can turn a kernel classiﬁer for clean data into one that can tolerate contamination, without any speciﬁc training on contaminated positives. 1</p><p>3 0.67848277 <a title="43-lda-3" href="./nips-2004-Newscast_EM.html">130 nips-2004-Newscast EM</a></p>
<p>Author: Wojtek Kowalczyk, Nikos A. Vlassis</p><p>Abstract: We propose a gossip-based distributed algorithm for Gaussian mixture learning, Newscast EM. The algorithm operates on network topologies where each node observes a local quantity and can communicate with other nodes in an arbitrary point-to-point fashion. The main difference between Newscast EM and the standard EM algorithm is that the M-step in our case is implemented in a decentralized manner: (random) pairs of nodes repeatedly exchange their local parameter estimates and combine them by (weighted) averaging. We provide theoretical evidence and demonstrate experimentally that, under this protocol, nodes converge exponentially fast to the correct estimates in each M-step of the EM algorithm. 1</p><p>4 0.57820743 <a title="43-lda-4" href="./nips-2004-Face_Detection_---_Efficient_and_Rank_Deficient.html">68 nips-2004-Face Detection --- Efficient and Rank Deficient</a></p>
<p>Author: Wolf Kienzle, Matthias O. Franz, Bernhard Schölkopf, Gökhan H. Bakir</p><p>Abstract: This paper proposes a method for computing fast approximations to support vector decision functions in the ﬁeld of object detection. In the present approach we are building on an existing algorithm where the set of support vectors is replaced by a smaller, so-called reduced set of synthesized input space points. In contrast to the existing method that ﬁnds the reduced set via unconstrained optimization, we impose a structural constraint on the synthetic points such that the resulting approximations can be evaluated via separable ﬁlters. For applications that require scanning large images, this decreases the computational complexity by a signiﬁcant amount. Experimental results show that in face detection, rank deﬁcient approximations are 4 to 6 times faster than unconstrained reduced set systems. 1</p><p>5 0.46749634 <a title="43-lda-5" href="./nips-2004-Synergistic_Face_Detection_and_Pose_Estimation_with_Energy-Based_Models.html">182 nips-2004-Synergistic Face Detection and Pose Estimation with Energy-Based Models</a></p>
<p>Author: Margarita Osadchy, Matthew L. Miller, Yann L. Cun</p><p>Abstract: We describe a novel method for real-time, simultaneous multi-view face detection and facial pose estimation. The method employs a convolutional network to map face images to points on a manifold, parametrized by pose, and non-face images to points far from that manifold. This network is trained by optimizing a loss function of three variables: image, pose, and face/non-face label. We test the resulting system, in a single conﬁguration, on three standard data sets – one for frontal pose, one for rotated faces, and one for proﬁles – and ﬁnd that its performance on each set is comparable to previous multi-view face detectors that can only handle one form of pose variation. We also show experimentally that the system’s accuracy on both face detection and pose estimation is improved by training for the two tasks together.</p><p>6 0.46401602 <a title="43-lda-6" href="./nips-2004-Contextual_Models_for_Object_Detection_Using_Boosted_Random_Fields.html">47 nips-2004-Contextual Models for Object Detection Using Boosted Random Fields</a></p>
<p>7 0.46315667 <a title="43-lda-7" href="./nips-2004-The_power_of_feature_clustering%3A_An_application_to_object_detection.html">192 nips-2004-The power of feature clustering: An application to object detection</a></p>
<p>8 0.46287328 <a title="43-lda-8" href="./nips-2004-Semi-Markov_Conditional_Random_Fields_for_Information_Extraction.html">162 nips-2004-Semi-Markov Conditional Random Fields for Information Extraction</a></p>
<p>9 0.45639938 <a title="43-lda-9" href="./nips-2004-Hierarchical_Distributed_Representations_for_Statistical_Language_Modeling.html">78 nips-2004-Hierarchical Distributed Representations for Statistical Language Modeling</a></p>
<p>10 0.45377308 <a title="43-lda-10" href="./nips-2004-Resolving_Perceptual_Aliasing_In_The_Presence_Of_Noisy_Sensors.html">154 nips-2004-Resolving Perceptual Aliasing In The Presence Of Noisy Sensors</a></p>
<p>11 0.45186818 <a title="43-lda-11" href="./nips-2004-Distributed_Occlusion_Reasoning_for_Tracking_with_Nonparametric_Belief_Propagation.html">55 nips-2004-Distributed Occlusion Reasoning for Tracking with Nonparametric Belief Propagation</a></p>
<p>12 0.44856265 <a title="43-lda-12" href="./nips-2004-A_Topographic_Support_Vector_Machine%3A_Classification_Using_Local_Label_Configurations.html">14 nips-2004-A Topographic Support Vector Machine: Classification Using Local Label Configurations</a></p>
<p>13 0.44815609 <a title="43-lda-13" href="./nips-2004-Markov_Networks_for_Detecting_Overalpping_Elements_in_Sequence_Data.html">108 nips-2004-Markov Networks for Detecting Overalpping Elements in Sequence Data</a></p>
<p>14 0.44582352 <a title="43-lda-14" href="./nips-2004-Analysis_of_a_greedy_active_learning_strategy.html">23 nips-2004-Analysis of a greedy active learning strategy</a></p>
<p>15 0.44444177 <a title="43-lda-15" href="./nips-2004-Semi-supervised_Learning_with_Penalized_Probabilistic_Clustering.html">167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</a></p>
<p>16 0.44144022 <a title="43-lda-16" href="./nips-2004-Hierarchical_Clustering_of_a_Mixture_Model.html">77 nips-2004-Hierarchical Clustering of a Mixture Model</a></p>
<p>17 0.44105756 <a title="43-lda-17" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>18 0.4403609 <a title="43-lda-18" href="./nips-2004-Self-Tuning_Spectral_Clustering.html">161 nips-2004-Self-Tuning Spectral Clustering</a></p>
<p>19 0.44000962 <a title="43-lda-19" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>20 0.43997946 <a title="43-lda-20" href="./nips-2004-Nonparametric_Transforms_of_Graph_Kernels_for_Semi-Supervised_Learning.html">133 nips-2004-Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
