<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-11" href="#">nips2004-11</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</h1>
<br/><p>Source: <a title="nips-2004-11-pdf" href="http://papers.nips.cc/paper/2670-a-second-order-cone-programming-formulation-for-classifying-missing-data.pdf">pdf</a></p><p>Author: Chiranjib Bhattacharyya, Pannagadatta K. Shivaswamy, Alex J. Smola</p><p>Abstract: We propose a convex optimization based strategy to deal with uncertainty in the observations of a classiﬁcation problem. We assume that instead of a sample (xi , yi ) a distribution over (xi , yi ) is speciﬁed. In particular, we derive a robust formulation when the distribution is given by a normal distribution. It leads to Second Order Cone Programming formulation. Our method is applied to the problem of missing data, where it outperforms direct imputation. 1</p><p>Reference: <a title="nips-2004-11-reference" href="../nips2004_reference/nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 au  Abstract We propose a convex optimization based strategy to deal with uncertainty in the observations of a classiﬁcation problem. [sent-13, score-0.371]
</p><p>2 We assume that instead of a sample (xi , yi ) a distribution over (xi , yi ) is speciﬁed. [sent-14, score-0.338]
</p><p>3 In particular, we derive a robust formulation when the distribution is given by a normal distribution. [sent-15, score-0.455]
</p><p>4 Our method is applied to the problem of missing data, where it outperforms direct imputation. [sent-17, score-0.369]
</p><p>5 The typical machine learning formulation only deals with the case where (x, y) are given exactly. [sent-19, score-0.17]
</p><p>6 Quite often, however, this is not the case — for instance in the case of missing values we may be able (using a secondary estimation procedure) to estimate the values of the missing variables, albeit with a certain degree of uncertainty. [sent-20, score-0.738]
</p><p>7 What we propose in the present paper goes beyond the traditional imputation strategy where missing values are estimated and then used as if they had actually been observed. [sent-22, score-0.607]
</p><p>8 The key difference in what follows is ˜ that we will require that with high probability any (˜ i , yi ) pair, where xi is drawn from a x distribution of possible xi , will be estimated correctly. [sent-23, score-0.521]
</p><p>9 We solve the equations arising in the context of normal random variables in Section 3 which leads to a Second Order Cone Program (SOCP). [sent-26, score-0.187]
</p><p>10 As an application the problem of classiﬁcation with missing variables is described in Section 4. [sent-27, score-0.444]
</p><p>11 2  Linear Classiﬁcation using Convex Optimization  Assume we have m observations (xi , yi ) drawn iid (independently and identically distributed) from a distribution over X × Y, where X is the set of patterns and Y = {±1} are the labels (e. [sent-29, score-0.237]
</p><p>12 Unfortunately, such separation is not always possible and we need to allow for slack in the separation of the two sets. [sent-36, score-0.103]
</p><p>13 Consider the formulation m  ξi  minimize  (2a)  w,b,ξ i=1  subject to yi ( w, xi + b) ≥ 1 − ξi , ξi ≥ 0, w ≤ W for all 1 ≤ i ≤ m  (2b)  It is well known that this problem minimizes an upper bound on the number of errors. [sent-37, score-0.6]
</p><p>14 The resulting discriminant surface is called the generalized optimal hyperplane [9]. [sent-41, score-0.147]
</p><p>15 Typically one states the SVM optimization problem as follows [3]:  minimize w,b,ξ  1 w 2  m 2  ξi  +C  (3a)  i=1  subject to yi ( w, xi + b) ≥ 1 − ξi , ξi ≥ 0 for all 1 ≤ i ≤ m  (3b)  Instead of the user deﬁned parameter W , the formulation (3) uses another parameter C. [sent-43, score-0.718]
</p><p>16 For the purpose of the present paper, however, (2) will be much more easily amenable to modiﬁcations and to cast the resulting problem as a second order cone program (SOCP). [sent-45, score-0.316]
</p><p>17 2  Classiﬁcation with Uncertainty  So far we assumed that the (xi , yi ) pairs are known with certainty. [sent-47, score-0.169]
</p><p>18 We now relax this to the assumption that we only have a distribution over the xi , that is (Pi , yi ) at our disposition (due to a sampling procedure, missing variables, etc. [sent-48, score-0.714]
</p><p>19 This means that by choosing a value of κi close to 1 we can ﬁnd a conservative classiﬁer which will classify even very infrequent (x i , yi ) pairs correctly. [sent-52, score-0.169]
</p><p>20 Hence κi provides robustness of the estimate with respect to deviating xi . [sent-53, score-0.176]
</p><p>21 It is clear that unless we impose further restrictions on Pi , it will be difﬁcult to minimize the m objective i=1 ξi with the constraints (4) efﬁciently. [sent-54, score-0.109]
</p><p>22 In the following we will consider the special cases of gaussian uncertainty for which a mathematical programming formulation can be found. [sent-55, score-0.349]
</p><p>23 , xi is drawn from a x Gaussian distribution with mean xi and covariance Σi . [sent-58, score-0.489]
</p><p>24 This means that the uncertainty about xi may be limited to individual coordinates or to a subspace of X . [sent-60, score-0.281]
</p><p>25 Consequently (zi − zi )/σzi is a random variable with zero mean and unit variance ¯ and we can compute the lhs of (6) by evaluating the cumulative distribution function for normal distributions u s2 1 φ(u) := √ e− 2 ds. [sent-64, score-0.301]
</p><p>26 2π −∞ In summary, (6) is equivalent to the condition  φ  yi b + ξ i − 1 − z i σ zi  ≥ κi . [sent-65, score-0.304]
</p><p>27 which can be solved (since φ(u) is monotonic and invertible), for the argument of φ and obtain a condition on its argument yi (w xi + b) ≥ 1 − ξi + γi ¯  wT Σi w , γi = φ−1 (κi )  (7)  We now proceed to deriving a mathematical programming formulation. [sent-66, score-0.419]
</p><p>28 This means that the second order cone part of the constraint (7) reduces to the linear inequality of (2b). [sent-71, score-0.256]
</p><p>29 This means that the constraint (7) describes a concave set, which turns the linear classiﬁcation task into a hard optimization problem. [sent-75, score-0.188]
</p><p>30 However, it is not very likely that anyone would like to impose such constraints which hold only with low probability. [sent-76, score-0.112]
</p><p>31 After all, uncertain data requires the constraint to become more restrictive in holding not only for a guaranteed point xi but rather for an entire set. [sent-77, score-0.284]
</p><p>32 In this case (7) describes a convex set in in w, b, ξi . [sent-80, score-0.103]
</p><p>33 As a special case of convex nonlinear optimization SOCPs have gained much attention in recent times. [sent-82, score-0.16]
</p><p>34 3  Worst Case Prediction  Note that if at optimality ξi > 0, the hyperplane intersects with the constraint set B(xi , Σi , γi ). [sent-85, score-0.335]
</p><p>35 Moreover, at a later stage we will need to predict the class label to asses on which side of the hyperplane B lies. [sent-86, score-0.24]
</p><p>36 If the hyperplane intersects B we will end up with different predictions for points in the different half spaces. [sent-87, score-0.236]
</p><p>37 In such a scenario a worst case prediction, y can be w, xi + b y = sgn(z) sgn(h − γ) where γ = φ−1 (κ), z = √ and h = |z|. [sent-88, score-0.31]
</p><p>38 (9) w Σw Here sgn(z) gives us the sign of the point in the center of the ellipsoid and (h − γ) is the distance of z from the center. [sent-89, score-0.089]
</p><p>39 If the hyperplane intersects the ellipsoid, the worst case prediction is then the prediction for all points which are in the opposite half space of the center (xi ). [sent-90, score-0.516]
</p><p>40 In such a case h can serve as a measure of conﬁdence as to how well the discriminating hyperplane classiﬁes the mean(xi ) correctly. [sent-95, score-0.147]
</p><p>41 In other words, we have ξi = 0 only when the hyperplane w x + b = 0 does not intersect the ball B(xi , Σi , γi ). [sent-98, score-0.147]
</p><p>42 Note that this puts our optimization setting into the same category as the knowledge-based SVM, and SDP for invariances as all three deal with the above type of constraint (11). [sent-99, score-0.249]
</p><p>43 More to the point, in [5] Si = S(xi , β) is a polynomial in β which describes the set of invariance transforms of xi (such as distortion or translation). [sent-100, score-0.207]
</p><p>44 [4] deﬁne Si to be a polyhedral “knowledge” set, speciﬁed by the intersection of linear constraints. [sent-101, score-0.118]
</p><p>45 Such considerations suggest yet another optimization setting: instead of specifying a polyhedral set Si by constraints we can also specify it by its vertices. [sent-102, score-0.249]
</p><p>46 In particular, we may set Si to be the convex hull of a set as in Si = co{xij for 1 ≤ j ≤ mi }. [sent-103, score-0.155]
</p><p>47 By the convexity of the constraint set itself it follows that a necessary and sufﬁcient condition for (11) to hold is that the inequality holds for all x ∈ {xij for 1 ≤ j ≤ mi }. [sent-104, score-0.145]
</p><p>48 Consequently we can replace (11) by yi ( w, xij + b) ≥ 1 − ξi Note that the index ranges over j rather than i. [sent-105, score-0.246]
</p><p>49 4  Missing Variables  In this section we discuss how to address the missing value problem. [sent-111, score-0.369]
</p><p>50 Key is how to obtain estimates of the uncertainty in the missing variables. [sent-112, score-0.474]
</p><p>51 Since our optimization setting allows for uncertainty in terms of a normal distribution we attempt to estimate the latter directly. [sent-113, score-0.305]
</p><p>52 In other words, we assume that x|y is jointly normal with mean µy and covariance Σy . [sent-114, score-0.286]
</p><p>53 Hence we have the following two-stage procedure to deal with missing variables: • Estimate Σy , µy from incomplete data, e. [sent-115, score-0.407]
</p><p>54 • Use the conditionally normal estimates of xmissing |(xobserved , y) in the optimization problem. [sent-118, score-0.2]
</p><p>55 Note that there is nothing to prevent us from using other estimates of uncertainty and use e. [sent-120, score-0.105]
</p><p>56 However, for the sake of simplicity we focus on normal distributions in this paper. [sent-123, score-0.144]
</p><p>57 1  Estimation of the model parameters  We now detail the computation of the mean and covariance matrices for the datapoints which have missing values. [sent-125, score-0.625]
</p><p>58 Let x ∈ Rd , where xa ∈ Rda be the vector whose values are known, while xm ∈ Rd−da be the vector consisting of missing variables. [sent-129, score-0.547]
</p><p>59 Assuming a jointly normal distribution in x with mean µ and covariance Σ it follows that xm |xa ∼ N (µm + Σam Σ−1 (xa − µa ), Σmm − Σam Σ−1 Σam ). [sent-130, score-0.351]
</p><p>60 aa aa  (12)  Here we decomposed µ, Σ according to (xa , xm ) into µ = (µa , µm ) and Σ =  Σaa Σam  Σam Σmm  . [sent-131, score-0.189]
</p><p>61 (13)  Hence, knowing Σ, µ we can estimate the missing variables and determine their degree of uncertainty. [sent-132, score-0.444]
</p><p>62 2  Robust formulation for missing values  As stated above, we model the missing variables as Gaussian random variables, with its mean and covariance given by the model described in the previous section. [sent-140, score-1.12]
</p><p>63 The standard practice for imputation is to discard the covariance and treat the problem as a deterministic problem, using the mean as surrogate. [sent-141, score-0.375]
</p><p>64 But using the robust formulation (8) one can as well account for the covariance. [sent-142, score-0.343]
</p><p>65 Let ma be number of datapoints for which all the values are available, while mm be the number of datapoints containing missing values. [sent-143, score-0.766]
</p><p>66 The matrix Σj has all entries zero except those involving the ˆ missing values, given by Cj , computed via (12). [sent-145, score-0.369]
</p><p>67 The formulation (14) is an optimization problem which involves minimizing a linear objective over linear and second order cone constraints. [sent-146, score-0.445]
</p><p>68 The resulting discriminator can be used to predict the the class label of a test datapoint having missing variables by a process of conditional imputation as follows. [sent-148, score-0.856]
</p><p>69 Perform the imputation process assuming that the datapoint comes from class 1(class with label y = 1). [sent-149, score-0.412]
</p><p>70 Assuming that the test data comes from class 2 (with label y = −1) redo the entire process and denote the resulting mean, covariance, and h by µ2 , Σ2 , h2 respectively. [sent-155, score-0.093]
</p><p>71 We decide that the observation belongs to class with label yµ as yµ = y2 if h1 < h2 and yµ = y1 otherwise  (15)  The above rule chooses the prediction with higher h value or in other words the classiﬁer chooses the prediction about which it is more conﬁdent. [sent-157, score-0.299]
</p><p>72 Using yµ , h1 , h2 as in (15), the worst case prediction rule (9) can be modiﬁed as follows y = yµ sgn(h − γ) where γ = φ−1 (κ) and h = max(h1 , h2 )  (16)  It is our hypothesis that the formulation (14) along with this decision rule is robust to uncertainty in the data. [sent-158, score-0.655]
</p><p>73 5  Experiments with the Robust formulation for missing values  Experiments were conducted to evaluate the proposed formulation (14), against the standard imputation strategy. [sent-159, score-0.947]
</p><p>74 The experiment methodology consisted of creating a dataset of missing values from a completely speciﬁed dataset. [sent-160, score-0.444]
</p><p>75 The robust formulation (14) was used to learn a classiﬁer on the dataset having missing values. [sent-161, score-0.752]
</p><p>76 The resulting classiﬁer was used to give a worst case prediction (16), on the test data. [sent-162, score-0.207]
</p><p>77 Consider a fully speciﬁed dataset, D = {(xi , yi )|xi ∈ Rd , yi ∈ {±1}1 ≤ i ≤ N } having N observations, each observation is a d dimensional vector (xi ) and labels yi . [sent-165, score-0.507]
</p><p>78 This then creates a dataset having N datapoints out of which Nm (= f N, 0 ≤ f ≤ 1) of them have missing values. [sent-169, score-0.528]
</p><p>79 Assuming that the conditional probability distribution of the missing variables given the ˆ other variables is a gaussian, the mean(xj ) and the covariance (Cj ) can be estimated by the methods described in (4. [sent-172, score-0.602]
</p><p>80 The robust optimization problem was then solved for different values of κ. [sent-174, score-0.261]
</p><p>81 For each value of κ the worst case error is recorded. [sent-176, score-0.134]
</p><p>82 Experimental results are reported for three public domain datasets downloaded from uci repository ([2]). [sent-177, score-0.124]
</p><p>83 The generalized optimal hyperplane will be referred to as the nominal classiﬁer. [sent-181, score-0.282]
</p><p>84 The nominal classiﬁer considers the missing values are well approximated by the mean (xj ), and there is no uncertainty. [sent-182, score-0.558]
</p><p>85 6  robust nomwc robustwc  robust nomwc robustwc  robust nomwc robustwc  0. [sent-185, score-1.947]
</p><p>86 45 robust nomwc robustwc  robust nomwc robustwc  robust nomwc robustwc  0. [sent-220, score-1.947]
</p><p>87 5  Figure 1: Performance of the robust programming solution for various datasets of the UCI database. [sent-247, score-0.29]
</p><p>88 Top: small fraction of data with missing variables (50%), Bottom: large number of observations with missing variables (90%) The experimental results are summarized by the graphs(1). [sent-249, score-0.956]
</p><p>89 The robust classiﬁer almost always outperforms the nominal classiﬁer in the worst case sense (compare nomwc and robustwc). [sent-250, score-0.71]
</p><p>90 The results show that for low number of missing values(f = 0. [sent-254, score-0.369]
</p><p>91 5) the robust classiﬁer is marginally better than the nominal classiﬁer the gain but for large f = 0. [sent-255, score-0.308]
</p><p>92 This conﬁrms that the imputation strategy fails for high noise. [sent-257, score-0.238]
</p><p>93 The standard misclassiﬁcation error for the robust classiﬁer, using the standard prediction (1), is also shown in the graph with the legend robust. [sent-258, score-0.246]
</p><p>94 As expected the robust classiﬁer performance does not deteriorate in the standard misclassiﬁcation sense as κ is increased. [sent-259, score-0.203]
</p><p>95 In summary the results seems to suggest that for low noise level the nominal classiﬁer trained on imputed data performs as good as the robust formulation. [sent-260, score-0.376]
</p><p>96 But for high noise level the robust formulation yields dividends in the worst case sense. [sent-261, score-0.477]
</p><p>97 6  Conclusions  An SOCP formulation was proposed for classifying noisy observations and the resulting formulation was applied to the missing data case. [sent-262, score-0.807]
</p><p>98 In the worst case sense the classiﬁer shows a better performance over the standard imputation strategy. [sent-263, score-0.402]
</p><p>99 The TSVC formulation tries to reconstruct the original maximal margin classiﬁer in the presence of noisy data. [sent-265, score-0.17]
</p><p>100 Both TSVC formulation and the approach in this paper address the issue of uncertainty in input data and it would be an important research direction to compare the two approaches. [sent-266, score-0.275]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('missing', 0.369), ('imputation', 0.238), ('nomwc', 0.238), ('robustwc', 0.238), ('cone', 0.187), ('xi', 0.176), ('robust', 0.173), ('formulation', 0.17), ('yi', 0.169), ('socp', 0.148), ('hyperplane', 0.147), ('nominal', 0.135), ('zi', 0.135), ('worst', 0.134), ('classi', 0.123), ('datapoints', 0.119), ('polyhedral', 0.118), ('mm', 0.113), ('xa', 0.113), ('normal', 0.112), ('er', 0.109), ('uncertainty', 0.105), ('sgn', 0.097), ('australia', 0.095), ('program', 0.094), ('ellipsoid', 0.089), ('intersects', 0.089), ('tsvc', 0.089), ('optimization', 0.088), ('si', 0.086), ('covariance', 0.083), ('datapoint', 0.081), ('xij', 0.077), ('variables', 0.075), ('programming', 0.074), ('prediction', 0.073), ('convex', 0.072), ('constraint', 0.069), ('bangalore', 0.068), ('imputed', 0.068), ('observations', 0.068), ('xm', 0.065), ('aa', 0.062), ('indian', 0.059), ('label', 0.059), ('pi', 0.056), ('mean', 0.054), ('australian', 0.054), ('ict', 0.054), ('invariances', 0.054), ('india', 0.05), ('subject', 0.05), ('pima', 0.047), ('uci', 0.047), ('pr', 0.047), ('ma', 0.046), ('hull', 0.045), ('reads', 0.045), ('datasets', 0.043), ('constraints', 0.043), ('ionosphere', 0.041), ('cation', 0.04), ('dataset', 0.04), ('heart', 0.04), ('co', 0.039), ('uncertain', 0.039), ('deal', 0.038), ('hold', 0.038), ('mi', 0.038), ('jointly', 0.037), ('svm', 0.037), ('funded', 0.036), ('minimize', 0.035), ('semide', 0.035), ('cast', 0.035), ('methodology', 0.035), ('separation', 0.035), ('repository', 0.034), ('nm', 0.034), ('interior', 0.034), ('misclassi', 0.034), ('class', 0.034), ('slack', 0.033), ('xj', 0.033), ('cj', 0.033), ('quadratic', 0.032), ('sake', 0.032), ('rd', 0.031), ('describes', 0.031), ('impose', 0.031), ('sense', 0.03), ('user', 0.03), ('chooses', 0.03), ('classifying', 0.03), ('optimality', 0.03), ('initiative', 0.03), ('additions', 0.03), ('anu', 0.03), ('backing', 0.03), ('canberra', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="11-tfidf-1" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>Author: Chiranjib Bhattacharyya, Pannagadatta K. Shivaswamy, Alex J. Smola</p><p>Abstract: We propose a convex optimization based strategy to deal with uncertainty in the observations of a classiﬁcation problem. We assume that instead of a sample (xi , yi ) a distribution over (xi , yi ) is speciﬁed. In particular, we derive a robust formulation when the distribution is given by a normal distribution. It leads to Second Order Cone Programming formulation. Our method is applied to the problem of missing data, where it outperforms direct imputation. 1</p><p>2 0.32458037 <a title="11-tfidf-2" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>Author: Jinbo Bi, Tong Zhang</p><p>Abstract: This paper investigates a new learning model in which the input data is corrupted with noise. We present a general statistical framework to tackle this problem. Based on the statistical reasoning, we propose a novel formulation of support vector classiﬁcation, which allows uncertainty in input data. We derive an intuitive geometric interpretation of the proposed formulation, and develop algorithms to efﬁciently solve it. Empirical results are included to show that the newly formed method is superior to the standard SVM for problems with noisy input. 1</p><p>3 0.14836226 <a title="11-tfidf-3" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>Author: Saharon Rosset, Ji Zhu, Hui Zou, Trevor J. Hastie</p><p>Abstract: We consider the situation in semi-supervised learning, where the “label sampling” mechanism stochastically depends on the true response (as well as potentially on the features). We suggest a method of moments for estimating this stochastic dependence using the unlabeled data. This is potentially useful for two distinct purposes: a. As an input to a supervised learning procedure which can be used to “de-bias” its results using labeled data only and b. As a potentially interesting learning task in itself. We present several examples to illustrate the practical usefulness of our method.</p><p>4 0.14121276 <a title="11-tfidf-4" href="./nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine.html">187 nips-2004-The Entire Regularization Path for the Support Vector Machine</a></p>
<p>Author: Saharon Rosset, Robert Tibshirani, Ji Zhu, Trevor J. Hastie</p><p>Abstract: In this paper we argue that the choice of the SVM cost parameter can be critical. We then derive an algorithm that can ﬁt the entire path of SVM solutions for every value of the cost parameter, with essentially the same computational cost as ﬁtting one SVM model. 1</p><p>5 0.12572968 <a title="11-tfidf-5" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>Author: Peter L. Bartlett, Michael Collins, Ben Taskar, David A. McAllester</p><p>Abstract: We consider the problem of structured classiﬁcation, where the task is to predict a label y from an input x, and y has meaningful internal structure. Our framework includes supervised training of Markov random ﬁelds and weighted context-free grammars as special cases. We describe an algorithm that solves the large-margin optimization problem deﬁned in [12], using an exponential-family (Gibbs distribution) representation of structured objects. The algorithm is efﬁcient—even in cases where the number of labels y is exponential in size—provided that certain expectations under Gibbs distributions can be calculated efﬁciently. The method for structured labels relies on a more general result, speciﬁcally the application of exponentiated gradient updates [7, 8] to quadratic programs. 1</p><p>6 0.11769518 <a title="11-tfidf-6" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>7 0.1146201 <a title="11-tfidf-7" href="./nips-2004-Kernel_Methods_for_Implicit_Surface_Modeling.html">92 nips-2004-Kernel Methods for Implicit Surface Modeling</a></p>
<p>8 0.11349239 <a title="11-tfidf-8" href="./nips-2004-Maximum_Margin_Clustering.html">115 nips-2004-Maximum Margin Clustering</a></p>
<p>9 0.097304039 <a title="11-tfidf-9" href="./nips-2004-Incremental_Algorithms_for_Hierarchical_Classification.html">82 nips-2004-Incremental Algorithms for Hierarchical Classification</a></p>
<p>10 0.093971558 <a title="11-tfidf-10" href="./nips-2004-Learning_Gaussian_Process_Kernels_via_Hierarchical_Bayes.html">98 nips-2004-Learning Gaussian Process Kernels via Hierarchical Bayes</a></p>
<p>11 0.093870096 <a title="11-tfidf-11" href="./nips-2004-Joint_Probabilistic_Curve_Clustering_and_Alignment.html">90 nips-2004-Joint Probabilistic Curve Clustering and Alignment</a></p>
<p>12 0.090278283 <a title="11-tfidf-12" href="./nips-2004-Breaking_SVM_Complexity_with_Cross-Training.html">34 nips-2004-Breaking SVM Complexity with Cross-Training</a></p>
<p>13 0.089849763 <a title="11-tfidf-13" href="./nips-2004-Distributed_Occlusion_Reasoning_for_Tracking_with_Nonparametric_Belief_Propagation.html">55 nips-2004-Distributed Occlusion Reasoning for Tracking with Nonparametric Belief Propagation</a></p>
<p>14 0.088935815 <a title="11-tfidf-14" href="./nips-2004-On_Semi-Supervised_Classification.html">136 nips-2004-On Semi-Supervised Classification</a></p>
<p>15 0.087573737 <a title="11-tfidf-15" href="./nips-2004-Conditional_Random_Fields_for_Object_Recognition.html">44 nips-2004-Conditional Random Fields for Object Recognition</a></p>
<p>16 0.0860238 <a title="11-tfidf-16" href="./nips-2004-Co-Training_and_Expansion%3A_Towards_Bridging_Theory_and_Practice.html">37 nips-2004-Co-Training and Expansion: Towards Bridging Theory and Practice</a></p>
<p>17 0.084152162 <a title="11-tfidf-17" href="./nips-2004-An_Application_of_Boosting_to_Graph_Classification.html">19 nips-2004-An Application of Boosting to Graph Classification</a></p>
<p>18 0.082892694 <a title="11-tfidf-18" href="./nips-2004-Outlier_Detection_with_One-class_Kernel_Fisher_Discriminants.html">142 nips-2004-Outlier Detection with One-class Kernel Fisher Discriminants</a></p>
<p>19 0.080921099 <a title="11-tfidf-19" href="./nips-2004-Object_Classification_from_a_Single_Example_Utilizing_Class_Relevance_Metrics.html">134 nips-2004-Object Classification from a Single Example Utilizing Class Relevance Metrics</a></p>
<p>20 0.07584241 <a title="11-tfidf-20" href="./nips-2004-Semi-supervised_Learning_by_Entropy_Minimization.html">164 nips-2004-Semi-supervised Learning by Entropy Minimization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.272), (1, 0.127), (2, -0.035), (3, 0.164), (4, 0.047), (5, 0.049), (6, 0.057), (7, 0.05), (8, -0.087), (9, 0.049), (10, 0.227), (11, 0.114), (12, -0.084), (13, 0.033), (14, 0.078), (15, 0.106), (16, -0.158), (17, 0.1), (18, -0.053), (19, 0.085), (20, -0.065), (21, 0.09), (22, 0.009), (23, 0.047), (24, 0.076), (25, 0.008), (26, 0.059), (27, 0.1), (28, -0.02), (29, 0.035), (30, -0.04), (31, -0.046), (32, -0.014), (33, -0.018), (34, -0.054), (35, 0.092), (36, -0.057), (37, 0.029), (38, 0.087), (39, 0.014), (40, 0.062), (41, -0.006), (42, -0.131), (43, 0.059), (44, -0.017), (45, 0.006), (46, -0.024), (47, -0.122), (48, 0.07), (49, -0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97332436 <a title="11-lsi-1" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>Author: Chiranjib Bhattacharyya, Pannagadatta K. Shivaswamy, Alex J. Smola</p><p>Abstract: We propose a convex optimization based strategy to deal with uncertainty in the observations of a classiﬁcation problem. We assume that instead of a sample (xi , yi ) a distribution over (xi , yi ) is speciﬁed. In particular, we derive a robust formulation when the distribution is given by a normal distribution. It leads to Second Order Cone Programming formulation. Our method is applied to the problem of missing data, where it outperforms direct imputation. 1</p><p>2 0.86345458 <a title="11-lsi-2" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>Author: Jinbo Bi, Tong Zhang</p><p>Abstract: This paper investigates a new learning model in which the input data is corrupted with noise. We present a general statistical framework to tackle this problem. Based on the statistical reasoning, we propose a novel formulation of support vector classiﬁcation, which allows uncertainty in input data. We derive an intuitive geometric interpretation of the proposed formulation, and develop algorithms to efﬁciently solve it. Empirical results are included to show that the newly formed method is superior to the standard SVM for problems with noisy input. 1</p><p>3 0.69990027 <a title="11-lsi-3" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>Author: Peter L. Bartlett, Michael Collins, Ben Taskar, David A. McAllester</p><p>Abstract: We consider the problem of structured classiﬁcation, where the task is to predict a label y from an input x, and y has meaningful internal structure. Our framework includes supervised training of Markov random ﬁelds and weighted context-free grammars as special cases. We describe an algorithm that solves the large-margin optimization problem deﬁned in [12], using an exponential-family (Gibbs distribution) representation of structured objects. The algorithm is efﬁcient—even in cases where the number of labels y is exponential in size—provided that certain expectations under Gibbs distributions can be calculated efﬁciently. The method for structured labels relies on a more general result, speciﬁcally the application of exponentiated gradient updates [7, 8] to quadratic programs. 1</p><p>4 0.62267911 <a title="11-lsi-4" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>Author: Saharon Rosset, Ji Zhu, Hui Zou, Trevor J. Hastie</p><p>Abstract: We consider the situation in semi-supervised learning, where the “label sampling” mechanism stochastically depends on the true response (as well as potentially on the features). We suggest a method of moments for estimating this stochastic dependence using the unlabeled data. This is potentially useful for two distinct purposes: a. As an input to a supervised learning procedure which can be used to “de-bias” its results using labeled data only and b. As a potentially interesting learning task in itself. We present several examples to illustrate the practical usefulness of our method.</p><p>5 0.58222306 <a title="11-lsi-5" href="./nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine.html">187 nips-2004-The Entire Regularization Path for the Support Vector Machine</a></p>
<p>Author: Saharon Rosset, Robert Tibshirani, Ji Zhu, Trevor J. Hastie</p><p>Abstract: In this paper we argue that the choice of the SVM cost parameter can be critical. We then derive an algorithm that can ﬁt the entire path of SVM solutions for every value of the cost parameter, with essentially the same computational cost as ﬁtting one SVM model. 1</p><p>6 0.5759728 <a title="11-lsi-6" href="./nips-2004-Adaptive_Manifold_Learning.html">17 nips-2004-Adaptive Manifold Learning</a></p>
<p>7 0.56135619 <a title="11-lsi-7" href="./nips-2004-Incremental_Algorithms_for_Hierarchical_Classification.html">82 nips-2004-Incremental Algorithms for Hierarchical Classification</a></p>
<p>8 0.53357315 <a title="11-lsi-8" href="./nips-2004-Distributed_Occlusion_Reasoning_for_Tracking_with_Nonparametric_Belief_Propagation.html">55 nips-2004-Distributed Occlusion Reasoning for Tracking with Nonparametric Belief Propagation</a></p>
<p>9 0.51765341 <a title="11-lsi-9" href="./nips-2004-Rate-_and_Phase-coded_Autoassociative_Memory.html">151 nips-2004-Rate- and Phase-coded Autoassociative Memory</a></p>
<p>10 0.50894439 <a title="11-lsi-10" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>11 0.50113392 <a title="11-lsi-11" href="./nips-2004-Machine_Learning_Applied_to_Perception%3A_Decision_Images_for_Gender_Classification.html">106 nips-2004-Machine Learning Applied to Perception: Decision Images for Gender Classification</a></p>
<p>12 0.50039297 <a title="11-lsi-12" href="./nips-2004-Outlier_Detection_with_One-class_Kernel_Fisher_Discriminants.html">142 nips-2004-Outlier Detection with One-class Kernel Fisher Discriminants</a></p>
<p>13 0.49453679 <a title="11-lsi-13" href="./nips-2004-Breaking_SVM_Complexity_with_Cross-Training.html">34 nips-2004-Breaking SVM Complexity with Cross-Training</a></p>
<p>14 0.48784548 <a title="11-lsi-14" href="./nips-2004-An_Application_of_Boosting_to_Graph_Classification.html">19 nips-2004-An Application of Boosting to Graph Classification</a></p>
<p>15 0.48147032 <a title="11-lsi-15" href="./nips-2004-Conditional_Random_Fields_for_Object_Recognition.html">44 nips-2004-Conditional Random Fields for Object Recognition</a></p>
<p>16 0.47902891 <a title="11-lsi-16" href="./nips-2004-On_Semi-Supervised_Classification.html">136 nips-2004-On Semi-Supervised Classification</a></p>
<p>17 0.46735975 <a title="11-lsi-17" href="./nips-2004-Comparing_Beliefs%2C_Surveys%2C_and_Random_Walks.html">41 nips-2004-Comparing Beliefs, Surveys, and Random Walks</a></p>
<p>18 0.46360606 <a title="11-lsi-18" href="./nips-2004-Kernel_Methods_for_Implicit_Surface_Modeling.html">92 nips-2004-Kernel Methods for Implicit Surface Modeling</a></p>
<p>19 0.45938423 <a title="11-lsi-19" href="./nips-2004-Maximum_Margin_Clustering.html">115 nips-2004-Maximum Margin Clustering</a></p>
<p>20 0.45275864 <a title="11-lsi-20" href="./nips-2004-Neighbourhood_Components_Analysis.html">127 nips-2004-Neighbourhood Components Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.1), (15, 0.154), (19, 0.013), (26, 0.059), (27, 0.252), (31, 0.023), (33, 0.232), (35, 0.015), (39, 0.033), (50, 0.035), (87, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87095231 <a title="11-lda-1" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>Author: Chiranjib Bhattacharyya, Pannagadatta K. Shivaswamy, Alex J. Smola</p><p>Abstract: We propose a convex optimization based strategy to deal with uncertainty in the observations of a classiﬁcation problem. We assume that instead of a sample (xi , yi ) a distribution over (xi , yi ) is speciﬁed. In particular, we derive a robust formulation when the distribution is given by a normal distribution. It leads to Second Order Cone Programming formulation. Our method is applied to the problem of missing data, where it outperforms direct imputation. 1</p><p>2 0.83910584 <a title="11-lda-2" href="./nips-2004-Conditional_Random_Fields_for_Object_Recognition.html">44 nips-2004-Conditional Random Fields for Object Recognition</a></p>
<p>Author: Ariadna Quattoni, Michael Collins, Trevor Darrell</p><p>Abstract: We present a discriminative part-based approach for the recognition of object classes from unsegmented cluttered scenes. Objects are modeled as ﬂexible constellations of parts conditioned on local observations found by an interest operator. For each object class the probability of a given assignment of parts to local features is modeled by a Conditional Random Field (CRF). We propose an extension of the CRF framework that incorporates hidden variables and combines class conditional CRFs into a uniﬁed framework for part-based object recognition. The parameters of the CRF are estimated in a maximum likelihood framework and recognition proceeds by ﬁnding the most likely class under our model. The main advantage of the proposed CRF framework is that it allows us to relax the assumption of conditional independence of the observed data (i.e. local features) often used in generative approaches, an assumption that might be too restrictive for a considerable number of object classes.</p><p>3 0.81584758 <a title="11-lda-3" href="./nips-2004-Semi-supervised_Learning_with_Penalized_Probabilistic_Clustering.html">167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</a></p>
<p>Author: Zhengdong Lu, Todd K. Leen</p><p>Abstract: While clustering is usually an unsupervised operation, there are circumstances in which we believe (with varying degrees of certainty) that items A and B should be assigned to the same cluster, while items A and C should not. We would like such pairwise relations to inﬂuence cluster assignments of out-of-sample data in a manner consistent with the prior knowledge expressed in the training set. Our starting point is probabilistic clustering based on Gaussian mixture models (GMM) of the data distribution. We express clustering preferences in the prior distribution over assignments of data points to clusters. This prior penalizes cluster assignments according to the degree with which they violate the preferences. We ﬁt the model parameters with EM. Experiments on a variety of data sets show that PPC can consistently improve clustering results.</p><p>4 0.76365739 <a title="11-lda-4" href="./nips-2004-Blind_One-microphone_Speech_Separation%3A_A_Spectral_Learning_Approach.html">31 nips-2004-Blind One-microphone Speech Separation: A Spectral Learning Approach</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: We present an algorithm to perform blind, one-microphone speech separation. Our algorithm separates mixtures of speech without modeling individual speakers. Instead, we formulate the problem of speech separation as a problem in segmenting the spectrogram of the signal into two or more disjoint sets. We build feature sets for our segmenter using classical cues from speech psychophysics. We then combine these features into parameterized afﬁnity matrices. We also take advantage of the fact that we can generate training examples for segmentation by artiﬁcially superposing separately-recorded signals. Thus the parameters of the afﬁnity matrices can be tuned using recent work on learning spectral clustering [1]. This yields an adaptive, speech-speciﬁc segmentation algorithm that can successfully separate one-microphone speech mixtures. 1</p><p>5 0.76333946 <a title="11-lda-5" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>Author: Jinbo Bi, Tong Zhang</p><p>Abstract: This paper investigates a new learning model in which the input data is corrupted with noise. We present a general statistical framework to tackle this problem. Based on the statistical reasoning, we propose a novel formulation of support vector classiﬁcation, which allows uncertainty in input data. We derive an intuitive geometric interpretation of the proposed formulation, and develop algorithms to efﬁciently solve it. Empirical results are included to show that the newly formed method is superior to the standard SVM for problems with noisy input. 1</p><p>6 0.76315844 <a title="11-lda-6" href="./nips-2004-Distributed_Occlusion_Reasoning_for_Tracking_with_Nonparametric_Belief_Propagation.html">55 nips-2004-Distributed Occlusion Reasoning for Tracking with Nonparametric Belief Propagation</a></p>
<p>7 0.76180983 <a title="11-lda-7" href="./nips-2004-Nonparametric_Transforms_of_Graph_Kernels_for_Semi-Supervised_Learning.html">133 nips-2004-Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning</a></p>
<p>8 0.7611258 <a title="11-lda-8" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>9 0.76068264 <a title="11-lda-9" href="./nips-2004-Learning_first-order_Markov_models_for_control.html">102 nips-2004-Learning first-order Markov models for control</a></p>
<p>10 0.76066673 <a title="11-lda-10" href="./nips-2004-Self-Tuning_Spectral_Clustering.html">161 nips-2004-Self-Tuning Spectral Clustering</a></p>
<p>11 0.76024914 <a title="11-lda-11" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>12 0.75964624 <a title="11-lda-12" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>13 0.75910687 <a title="11-lda-13" href="./nips-2004-%E2%84%93%E2%82%80-norm_Minimization_for_Basis_Selection.html">207 nips-2004-ℓ₀-norm Minimization for Basis Selection</a></p>
<p>14 0.7587418 <a title="11-lda-14" href="./nips-2004-Non-Local_Manifold_Tangent_Learning.html">131 nips-2004-Non-Local Manifold Tangent Learning</a></p>
<p>15 0.75823432 <a title="11-lda-15" href="./nips-2004-Neighbourhood_Components_Analysis.html">127 nips-2004-Neighbourhood Components Analysis</a></p>
<p>16 0.7581979 <a title="11-lda-16" href="./nips-2004-Hierarchical_Clustering_of_a_Mixture_Model.html">77 nips-2004-Hierarchical Clustering of a Mixture Model</a></p>
<p>17 0.75787777 <a title="11-lda-17" href="./nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine.html">187 nips-2004-The Entire Regularization Path for the Support Vector Machine</a></p>
<p>18 0.75780582 <a title="11-lda-18" href="./nips-2004-A_Direct_Formulation_for_Sparse_PCA_Using_Semidefinite_Programming.html">2 nips-2004-A Direct Formulation for Sparse PCA Using Semidefinite Programming</a></p>
<p>19 0.75576568 <a title="11-lda-19" href="./nips-2004-Efficient_Out-of-Sample_Extension_of_Dominant-Set_Clusters.html">61 nips-2004-Efficient Out-of-Sample Extension of Dominant-Set Clusters</a></p>
<p>20 0.75514001 <a title="11-lda-20" href="./nips-2004-Semi-parametric_Exponential_Family_PCA.html">163 nips-2004-Semi-parametric Exponential Family PCA</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
