<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>152 nips-2004-Real-Time Pitch Determination of One or More Voices by Nonnegative Matrix Factorization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-152" href="#">nips2004-152</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>152 nips-2004-Real-Time Pitch Determination of One or More Voices by Nonnegative Matrix Factorization</h1>
<br/><p>Source: <a title="nips-2004-152-pdf" href="http://papers.nips.cc/paper/2631-real-time-pitch-determination-of-one-or-more-voices-by-nonnegative-matrix-factorization.pdf">pdf</a></p><p>Author: Fei Sha, Lawrence K. Saul</p><p>Abstract: An auditory “scene”, composed of overlapping acoustic sources, can be viewed as a complex object whose constituent parts are the individual sources. Pitch is known to be an important cue for auditory scene analysis. In this paper, with the goal of building agents that operate in human environments, we describe a real-time system to identify the presence of one or more voices and compute their pitch. The signal processing in the front end is based on instantaneous frequency estimation, a method for tracking the partials of voiced speech, while the pattern-matching in the back end is based on nonnegative matrix factorization, an unsupervised algorithm for learning the parts of complex objects. While supporting a framework to analyze complicated auditory scenes, our system maintains real-time operability and state-of-the-art performance in clean speech.</p><p>Reference: <a title="nips-2004-152-reference" href="../nips2004_reference/nips-2004-Real-Time_Pitch_Determination_of_One_or_More_Voices_by_Nonnegative_Matrix_Factorization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract An auditory “scene”, composed of overlapping acoustic sources, can be viewed as a complex object whose constituent parts are the individual sources. [sent-5, score-0.266]
</p><p>2 In this paper, with the goal of building agents that operate in human environments, we describe a real-time system to identify the presence of one or more voices and compute their pitch. [sent-7, score-0.229]
</p><p>3 The signal processing in the front end is based on instantaneous frequency estimation, a method for tracking the partials of voiced speech, while the pattern-matching in the back end is based on nonnegative matrix factorization, an unsupervised algorithm for learning the parts of complex objects. [sent-8, score-1.206]
</p><p>4 While supporting a framework to analyze complicated auditory scenes, our system maintains real-time operability and state-of-the-art performance in clean speech. [sent-9, score-0.174]
</p><p>5 The algorithm represents high dimensional inputs (“objects”) by a linear superposition of basis functions (“parts”) in which both the linear coefﬁcients and basis functions are constrained to be nonnegative. [sent-11, score-0.202]
</p><p>6 Applied to images of faces, NMF learns basis functions that correspond to eyes, noses, and mouths; applied to handwritten digits, it learns basis functions that correspond to cursive strokes. [sent-12, score-0.202]
</p><p>7 Recently, it has been suggested that NMF can play a similarly useful role in speech and audio processing [16, 17]. [sent-14, score-0.312]
</p><p>8 An auditory “scene”, composed of overlapping acoustic sources, can be viewed as a complex object whose constituent parts are the individual sources. [sent-15, score-0.266]
</p><p>9 In particular, we can imagine the basis functions in NMF as harmonic stacks of individual periodic sources (e. [sent-18, score-0.379]
</p><p>10 The pattern-matching computations of NMF are reminiscent of longstanding template-based models of pitch perception [6]. [sent-21, score-0.432]
</p><p>11 Our interest in NMF lies mainly in its use for speech processing. [sent-22, score-0.247]
</p><p>12 In this paper, we describe a real-time system to detect the presence of one or more voices and determine their pitch. [sent-23, score-0.184]
</p><p>13 Learning plays a crucial role in our system: the basis functions of NMF are trained ofﬂine from data to model the particular timbres of voiced speech, which vary across different phonetic contexts and speakers. [sent-24, score-0.397]
</p><p>14 A long-term goal is to develop interactive voice-driven agents that respond to the pitch contours of human speech [15]. [sent-27, score-0.789]
</p><p>15 To be truly interactive, these agents must be able to process input from distant sources and to operate in noisy environments with overlapping speakers. [sent-28, score-0.23]
</p><p>16 In this paper, we have taken an important step toward this goal by maintaining real-time operability and state-of-the-art performance in clean speech while developing a framework that can analyze more complicated auditory scenes. [sent-29, score-0.421]
</p><p>17 Our focus on actual system-building also distinguishes our work from many other studies of overlapping periodic sources [5, 9, 19, 20, 21]. [sent-31, score-0.221]
</p><p>18 In section 2, we describe the signal processing in our front end that converts speech signals into a form that can be analyzed by NMF. [sent-33, score-0.402]
</p><p>19 In section 3, we describe the use of NMF for pitch tracking—namely, the learning of basis functions for voiced speech, and the nonnegative deconvolution for real-time analysis. [sent-34, score-1.149]
</p><p>20 2 Signal processing A periodic signal is characterized by its fundamental frequency, f0 . [sent-37, score-0.212]
</p><p>21 For periodic signals with unknown f0 , the frequencies of the partials can be inferred from peaks in the magnitude spectrum, as computed by an FFT. [sent-39, score-0.557]
</p><p>22 Voiced speech is perceived as having a pitch at the fundamental frequency of vocal cord vibration. [sent-40, score-0.867]
</p><p>23 Perfect periodicity is an idealization, however; the waveforms of voiced speech are non-stationary, quasiperiodic signals. [sent-41, score-0.508]
</p><p>24 In practice, one cannot reliably extract the partials of voiced speech by simply computing windowed FFTs and locating peaks in the magnitude spectrum. [sent-42, score-0.874]
</p><p>25 In this section, we review a more robust method, known as instantaneous frequency (IF) estimation [1], for extracting the stable sinusoidal components of voiced speech. [sent-43, score-0.583]
</p><p>26 The starting point of IF estimation is to model the voiced speech signal, s(t), by a sum of amplitude and frequency-modulated sinusoids: t  s(t) =  αi (t) cos i  dt ωi (t) + θi . [sent-45, score-0.577]
</p><p>27 (1) are called the instantaneous phases; their derivatives with respect to time yield the so-called instantaneous frequencies ωi (t). [sent-47, score-0.326]
</p><p>28 For nonstationary signals, ωi (t) intuitively represents the instantaneous frequency of the ith partial at time t. [sent-50, score-0.253]
</p><p>29 (2)  Let z(ω, t) = ejωt F (ω, t) denote the analytic signal of the Fourier component of s(t) with frequency ω, and let a = Re[z] and b = Im[z] denote its real and imaginary parts. [sent-53, score-0.189]
</p><p>30 0  200 100 0 0  Time (second)  Figure 1: Top: instantaneous frequencies of estimated partials for the utterance “The north wind and the sun were disputing. [sent-74, score-0.534]
</p><p>31 IF estimation identiﬁes the stable ﬁxed points [7, 8] of this mapping, given by λ(ω ∗ , t) = ω ∗ and  (∂λ/∂ω)|ω=ω∗ < 1,  (4)  as the instantaneous frequencies of the partials that appear in eq. [sent-79, score-0.572]
</p><p>32 1 shows the IFs of partials extracted by this method for a speech signal with sliding and overlapping analysis windows. [sent-84, score-0.76]
</p><p>33 Note that in regions of voiced speech, indicated by nonzero f0 values, the IFs exhibit a clear harmonic structure, while in regions of unvoiced speech, they do not. [sent-86, score-0.561]
</p><p>34 ∗ In summary, the signal processing in our front-end extracts partials with frequencies ωi (t) ∗ and nonnegative amplitudes |F (ωi (t), t)|, where t indexes the time of the analysis window and i indexes the number of extracted partials. [sent-87, score-0.758]
</p><p>35 Further analysis of the signal is performed by the NMF algorithm described in the next section, which is used to detect the presence of one or more voices and to estimate their f0 values. [sent-88, score-0.243]
</p><p>36 Similar front ends have been used in other studies of pitch tracking and source separation [1, 2, 7, 13]. [sent-89, score-0.507]
</p><p>37 3 Nonnegative matrix factorization For mixed signals of overlapping speakers, our front-end outputs the mixture of partials extracted from several voices. [sent-90, score-0.543]
</p><p>38 In this section, we show: (i) how to learn nonnegative basis functions that model the characteristic timbres of voiced speech, and (ii) how to decompose mixed signals in terms of these basis functions. [sent-92, score-0.715]
</p><p>39 Given observations yt , the goal of NMF is to compute basis ˜ functions W and linear coefﬁcients xt such that the reconstructed vectors yt = Wxt  best match the original observations. [sent-94, score-0.362]
</p><p>40 NMF works by opti˜ mizing the total reconstruction error t G(yt , yt ) in terms of the basis functions W and ˜ coefﬁcients xt . [sent-97, score-0.288]
</p><p>41 We form three matrices by concatenating the column vectors yt , yt and xt ˜ and X respectively. [sent-98, score-0.248]
</p><p>42 In our application of NMF to pitch estimation, the vectors yt store vertical “time slices” of the IF representation in Fig. [sent-101, score-0.533]
</p><p>43 Speciﬁcally, the elements of yt store the magnitude ∗ spectra |F (ωi (t), t)| of extracted partials at time t; the instantaneous frequency axis is discretized on a log scale so that each element of yt covers 1/36 octave of the frequency spectrum. [sent-103, score-1.078]
</p><p>44 The columns of W store basis functions, or harmonic templates, for the magnitude spectra of voiced speech with different fundamental frequencies. [sent-104, score-0.941]
</p><p>45 (An additional column in W stores a non-harmonic template for unvoiced speech. [sent-105, score-0.228]
</p><p>46 ) In this study, only one harmonic template was used per fundamental frequency. [sent-106, score-0.341]
</p><p>47 The fundamental frequencies range from 50Hz to 400Hz, spaced and discretized on a log scale. [sent-107, score-0.195]
</p><p>48 We constrained the harmonic templates for different fundamental frequencies to be related by a simple translation on the log-frequency axis. [sent-108, score-0.384]
</p><p>49 Finally, the elements of xt store the coefﬁcients that best reconstruct yt by linearly superposing harmonic templates of W. [sent-110, score-0.388]
</p><p>50 Note that only partials from the same source form harmonic relations. [sent-111, score-0.448]
</p><p>51 Thus, the number of nonzero elements in xt indicates the number of periodic sources at time t, while the indices of nonzero elements indicate their fundamental frequencies. [sent-112, score-0.377]
</p><p>52 It is in this sense that the reconstruction yt ≈ Wxt provides an analysis of the auditory scene. [sent-113, score-0.226]
</p><p>53 1 Learning the basis functions of voiced speech The harmonic templates in W were estimated from the voiced speech of (non-overlapping) speakers in the Keele database [14]. [sent-115, score-1.454]
</p><p>54 The Keele database provides aligned pitch contours derived from laryngograph recordings. [sent-116, score-0.534]
</p><p>55 Given the vectors yt computed by IF estimation in the front end, the problem of NMF is to estimate the columns of W and the reconstruction coefﬁcients xt . [sent-118, score-0.308]
</p><p>56 Each xt has only two nonzero elements (one indicating the reference value for f0 , the other corresponding to the non-harmonic template of the basis matrix W); their magnitudes must still be estimated by NMF. [sent-119, score-0.342]
</p><p>57 2 (left) compares the harmonic template at 100 Hz before and after learning. [sent-123, score-0.255]
</p><p>58 2 (right) shows four examples from the Keele database (from snippets of voiced speech with f0 = 100 Hz) that were used to train this template. [sent-126, score-0.553]
</p><p>59 The learned template is derived to minimize the total reconstruction error over all segments of voiced speech in the training data. [sent-128, score-0.672]
</p><p>60 5  0 0  500  1000 1500 Frequency (Hz)  2000  2500  0  500 1000 1500 2000 2500 Frequency (Hz)  0  500 1000 1500 2000 2500 Frequency (Hz)  Figure 2: Left: harmonic template before and after learning for voiced speech at f0 = 100 Hz. [sent-131, score-0.763]
</p><p>61 Right: observed partials from four speakers with f0 = 100 Hz. [sent-133, score-0.343]
</p><p>62 2 Nonnegative deconvolution for estimating f0 of one or more voices Once the basis functions in W have been estimated, computing x such that y ≈ Wx under the measure of eq. [sent-135, score-0.468]
</p><p>63 Nonnegative deconvolution has been applied to problems in fundamental frequency estimation [16], music analysis [17] and sound localization [12]. [sent-137, score-0.505]
</p><p>64 In our model, nonnegative deconvolution of y ≈ Wx yields an estimate of the number of periodic sources in y as well as their f0 values. [sent-138, score-0.513]
</p><p>65 Ideally, the number of nonzero reconstruction weights in x reveal the number of sources, and the corresponding columns in the basis matrix W reveal their f0 values. [sent-139, score-0.191]
</p><p>66 Unvoiced speech is detected by a simple frame-based classiﬁer trained to make voiced/unvoiced distinctions from the observation y and its nonnegative deconvolution x. [sent-143, score-0.63]
</p><p>67 The pattern-matching computations in NMF are reminiscent of well-known models of harmonic template matching [6]. [sent-144, score-0.283]
</p><p>68 First, the templates in NMF are learned from labeled speech data. [sent-146, score-0.317]
</p><p>69 It is not obvious how to craft a harmonic template “by hand” that manages the variability of partial proﬁles in Fig. [sent-148, score-0.255]
</p><p>70 Speciﬁcally, the algorithm models observed partials by a nonnegative superposition of harmonic stacks. [sent-151, score-0.648]
</p><p>71 4 Implementation and results We have implemented both the IF estimation in section 2 and the nonnegative deconvolution in section 3. [sent-156, score-0.452]
</p><p>72 To achieve real-time performance and reduce system latency, the system does not postprocess the f0 values obtained in each frame from nonnegative deconvolution: in particular, there is no dynamic programming to smooth the pitch contour, as commonly done in many pitch tracking algorithms [18]. [sent-180, score-1.059]
</p><p>73 We have found that our algorithm performs well and yields smooth pitch contours (for non-overlapping voices) even without this postprocessing. [sent-181, score-0.458]
</p><p>74 1 Pitch determination of clean speech signals Table 1 compares the performance of our algorithm on clean speech to RAPT [18], a stateof-the-art pitch tracker based on autocorrelation and dynamic programming. [sent-183, score-1.099]
</p><p>75 The results were obtained on the second halves of utterances reserved for testing in the Keele database, as well as the full set of utterances in the Edinburgh database [3]. [sent-185, score-0.162]
</p><p>76 2 Pitch determination of overlapping voices and noisy speech We have also examined the robustness of our system to noise and overlapping speakers. [sent-188, score-0.652]
</p><p>77 The dominant and secondary f0 values extracted in each frame by nonnegative deconvolution are shown. [sent-192, score-0.51]
</p><p>78 The algorithm recovers the f0 values of the individual voices almost perfectly, though it does not currently make any effort to track the voices through time. [sent-193, score-0.368]
</p><p>79 4 shows in more detail how IF estimation and nonnegative deconvolution are affected by interfering speakers and noise. [sent-196, score-0.495]
</p><p>80 A clean signal from a single speaker is shown in the top row of the plot, along with its log power spectra, partials extracted by IF estimation, estimated f0 , and reconstructed harmonic stack. [sent-197, score-0.764]
</p><p>81 Both types of interference degrade the harmonic structure in the log power spectra and extracted partials. [sent-199, score-0.331]
</p><p>82 However, nonnegative deconvolution is still able to recover the pitch of the original speaker, as well as the pitch of the second speaker. [sent-200, score-1.191]
</p><p>83 1000  200 dominant pitch secondary pitch  150 600  F0 (Hz)  Frequency(Hz)  800  400  100 200 0  0  0. [sent-202, score-0.872]
</p><p>84 5  3  Figure 3: Left: Spectrogram of a mixture of two voices with ascending and descending f0 contours. [sent-208, score-0.184]
</p><p>85 Both types of interference degrade the harmonic structure in the log power spectra (second column) and the partials extracted by IF estimation (third column). [sent-211, score-0.7]
</p><p>86 The results of nonnegative deconvolution (fourth column), however, are fairly robust. [sent-212, score-0.383]
</p><p>87 Both the pitch of the original speaker at f0 = 200 Hz and the overlapping speaker at f0 = 300 Hz are recovered. [sent-213, score-0.619]
</p><p>88 The ﬁfth column displays the reconstructed proﬁle of extracted partials from activated harmonic templates. [sent-214, score-0.582]
</p><p>89 5 Discussion There exists a large body of related work on fundamental frequency estimation of overlapping sources [5, 7, 9, 19, 20, 21]. [sent-215, score-0.439]
</p><p>90 Nonnegative deconvolution is similar to EM algorithms [7] for harmonic template matching, but it does not impose normalization constraints on spectral peaks as if they represented a probability distribution. [sent-217, score-0.476]
</p><p>91 Important directions for future work are to train a richer set of harmonic templates by NMF, to incorporate the frame-based computations of nonnegative deconvolution into a dynamical model, and to embed our real-time system in interactive agents that respond to the pitch contours of human speech. [sent-218, score-1.143]
</p><p>92 Harmonics tracking and pitch extraction based on instantaneous frequency. [sent-224, score-0.578]
</p><p>93 Robust pitch estimation with harmonics enhancement in noisy environments based on instantaneous frequency. [sent-231, score-0.655]
</p><p>94 Enhanced pitch tracking and the processing of f0 contours for computer aided intonation teaching. [sent-240, score-0.509]
</p><p>95 An optimum processor theory for the central formation of the pitch of complex tones. [sent-255, score-0.404]
</p><p>96 Fixed point analysis of e frequency to instantaneous frequency mapping for accurate estimation of f0 and periodicity. [sent-272, score-0.452]
</p><p>97 Robust multipitch estimation for the analysis and manipulation of polyphonic musical signals. [sent-280, score-0.193]
</p><p>98 Learning the parts of objects with nonnegative matrix factorization. [sent-296, score-0.23]
</p><p>99 Robust fundamental frequency estimation against background noise and spectral distortion. [sent-310, score-0.285]
</p><p>100 Separation of harmonic sounds using multipitch analysis and iterative parameter estimation. [sent-372, score-0.219]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pitch', 0.404), ('nmf', 0.353), ('partials', 0.3), ('voiced', 0.261), ('speech', 0.247), ('hz', 0.208), ('nonnegative', 0.2), ('voices', 0.184), ('deconvolution', 0.183), ('harmonic', 0.148), ('frequency', 0.13), ('instantaneous', 0.123), ('template', 0.107), ('keele', 0.106), ('rapt', 0.106), ('unvoiced', 0.092), ('overlapping', 0.091), ('yt', 0.089), ('fundamental', 0.086), ('frequencies', 0.08), ('auditory', 0.08), ('basis', 0.074), ('gpe', 0.071), ('multipitch', 0.071), ('templates', 0.07), ('estimation', 0.069), ('periodic', 0.067), ('audio', 0.065), ('sources', 0.063), ('extracted', 0.063), ('speaker', 0.062), ('stft', 0.061), ('nonzero', 0.06), ('clean', 0.059), ('signal', 0.059), ('reconstruction', 0.057), ('spectra', 0.057), ('contours', 0.054), ('eurospeech', 0.053), ('polyphonic', 0.053), ('ue', 0.052), ('front', 0.052), ('tracking', 0.051), ('database', 0.045), ('factorization', 0.045), ('agents', 0.045), ('signals', 0.044), ('scene', 0.043), ('speakers', 0.043), ('gross', 0.042), ('reconstructed', 0.042), ('xt', 0.041), ('store', 0.04), ('utterances', 0.039), ('halves', 0.039), ('interactive', 0.039), ('determination', 0.039), ('peaks', 0.038), ('fourier', 0.038), ('music', 0.037), ('rms', 0.037), ('icslp', 0.035), ('ifs', 0.035), ('interference', 0.035), ('kobayashi', 0.035), ('operability', 0.035), ('smaragdis', 0.035), ('timbres', 0.035), ('virtanen', 0.035), ('icassp', 0.035), ('secondary', 0.035), ('male', 0.034), ('acoustic', 0.034), ('pro', 0.032), ('edinburgh', 0.032), ('estimated', 0.031), ('cue', 0.031), ('environments', 0.031), ('laryngograph', 0.031), ('constituent', 0.031), ('wxt', 0.031), ('abe', 0.031), ('parts', 0.03), ('column', 0.029), ('dominant', 0.029), ('discretized', 0.029), ('contour', 0.029), ('lee', 0.029), ('reference', 0.029), ('magnitude', 0.028), ('cients', 0.028), ('harmonics', 0.028), ('nonnegativity', 0.028), ('degrade', 0.028), ('indexes', 0.028), ('sinusoids', 0.028), ('reminiscent', 0.028), ('functions', 0.027), ('wx', 0.026), ('voice', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="152-tfidf-1" href="./nips-2004-Real-Time_Pitch_Determination_of_One_or_More_Voices_by_Nonnegative_Matrix_Factorization.html">152 nips-2004-Real-Time Pitch Determination of One or More Voices by Nonnegative Matrix Factorization</a></p>
<p>Author: Fei Sha, Lawrence K. Saul</p><p>Abstract: An auditory “scene”, composed of overlapping acoustic sources, can be viewed as a complex object whose constituent parts are the individual sources. Pitch is known to be an important cue for auditory scene analysis. In this paper, with the goal of building agents that operate in human environments, we describe a real-time system to identify the presence of one or more voices and compute their pitch. The signal processing in the front end is based on instantaneous frequency estimation, a method for tracking the partials of voiced speech, while the pattern-matching in the back end is based on nonnegative matrix factorization, an unsupervised algorithm for learning the parts of complex objects. While supporting a framework to analyze complicated auditory scenes, our system maintains real-time operability and state-of-the-art performance in clean speech.</p><p>2 0.35583708 <a title="152-tfidf-2" href="./nips-2004-Blind_One-microphone_Speech_Separation%3A_A_Spectral_Learning_Approach.html">31 nips-2004-Blind One-microphone Speech Separation: A Spectral Learning Approach</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: We present an algorithm to perform blind, one-microphone speech separation. Our algorithm separates mixtures of speech without modeling individual speakers. Instead, we formulate the problem of speech separation as a problem in segmenting the spectrogram of the signal into two or more disjoint sets. We build feature sets for our segmenter using classical cues from speech psychophysics. We then combine these features into parameterized afﬁnity matrices. We also take advantage of the fact that we can generate training examples for segmentation by artiﬁcially superposing separately-recorded signals. Thus the parameters of the afﬁnity matrices can be tuned using recent work on learning spectral clustering [1]. This yields an adaptive, speech-speciﬁc segmentation algorithm that can successfully separate one-microphone speech mixtures. 1</p><p>3 0.32272512 <a title="152-tfidf-3" href="./nips-2004-A_Harmonic_Excitation_State-Space_Approach_to_Blind_Separation_of_Speech.html">5 nips-2004-A Harmonic Excitation State-Space Approach to Blind Separation of Speech</a></p>
<p>Author: Rasmus K. Olsson, Lars K. Hansen</p><p>Abstract: We discuss an identiﬁcation framework for noisy speech mixtures. A block-based generative model is formulated that explicitly incorporates the time-varying harmonic plus noise (H+N) model for a number of latent sources observed through noisy convolutive mixtures. All parameters including the pitches of the source signals, the amplitudes and phases of the sources, the mixing ﬁlters and the noise statistics are estimated by maximum likelihood, using an EM-algorithm. Exact averaging over the hidden sources is obtained using the Kalman smoother. We show that pitch estimation and source separation can be performed simultaneously. The pitch estimates are compared to laryngograph (EGG) measurements. Artiﬁcial and real room mixtures are used to demonstrate the viability of the approach. Intelligible speech signals are re-synthesized from the estimated H+N models.</p><p>4 0.20569341 <a title="152-tfidf-4" href="./nips-2004-Bayesian_Regularization_and_Nonnegative_Deconvolution_for_Time_Delay_Estimation.html">27 nips-2004-Bayesian Regularization and Nonnegative Deconvolution for Time Delay Estimation</a></p>
<p>Author: Yuanqing Lin, Daniel D. Lee</p><p>Abstract: Bayesian Regularization and Nonnegative Deconvolution (BRAND) is proposed for estimating time delays of acoustic signals in reverberant environments. Sparsity of the nonnegative ﬁlter coefﬁcients is enforced using an L1 -norm regularization. A probabilistic generative model is used to simultaneously estimate the regularization parameters and ﬁlter coefﬁcients from the signal data. Iterative update rules are derived under a Bayesian framework using the Expectation-Maximization procedure. The resulting time delay estimation algorithm is demonstrated on noisy acoustic data.</p><p>5 0.12152267 <a title="152-tfidf-5" href="./nips-2004-Learning_Efficient_Auditory_Codes_Using_Spikes_Predicts_Cochlear_Filters.html">97 nips-2004-Learning Efficient Auditory Codes Using Spikes Predicts Cochlear Filters</a></p>
<p>Author: Evan C. Smith, Michael S. Lewicki</p><p>Abstract: The representation of acoustic signals at the cochlear nerve must serve a wide range of auditory tasks that require exquisite sensitivity in both time and frequency. Lewicki (2002) demonstrated that many of the ﬁltering properties of the cochlea could be explained in terms of efﬁcient coding of natural sounds. This model, however, did not account for properties such as phase-locking or how sound could be encoded in terms of action potentials. Here, we extend this theoretical approach with algorithm for learning efﬁcient auditory codes using a spiking population code. Here, we propose an algorithm for learning efﬁcient auditory codes using a theoretical model for coding sound in terms of spikes. In this model, each spike encodes the precise time position and magnitude of a localized, time varying kernel function. By adapting the kernel functions to the statistics natural sounds, we show that, compared to conventional signal representations, the spike code achieves far greater coding efﬁciency. Furthermore, the inferred kernels show both striking similarities to measured cochlear ﬁlters and a similar bandwidth versus frequency dependence. 1</p><p>6 0.10710765 <a title="152-tfidf-6" href="./nips-2004-Modeling_Conversational_Dynamics_as_a_Mixed-Memory_Markov_Process.html">120 nips-2004-Modeling Conversational Dynamics as a Mixed-Memory Markov Process</a></p>
<p>7 0.074083589 <a title="152-tfidf-7" href="./nips-2004-An_Auditory_Paradigm_for_Brain-Computer_Interfaces.html">20 nips-2004-An Auditory Paradigm for Brain-Computer Interfaces</a></p>
<p>8 0.072695658 <a title="152-tfidf-8" href="./nips-2004-Harmonising_Chorales_by_Probabilistic_Inference.html">74 nips-2004-Harmonising Chorales by Probabilistic Inference</a></p>
<p>9 0.062405106 <a title="152-tfidf-9" href="./nips-2004-Worst-Case_Analysis_of_Selective_Sampling_for_Linear-Threshold_Algorithms.html">206 nips-2004-Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms</a></p>
<p>10 0.058504403 <a title="152-tfidf-10" href="./nips-2004-Incremental_Learning_for_Visual_Tracking.html">83 nips-2004-Incremental Learning for Visual Tracking</a></p>
<p>11 0.05503111 <a title="152-tfidf-11" href="./nips-2004-Nonlinear_Blind_Source_Separation_by_Integrating_Independent_Component_Analysis_and_Slow_Feature_Analysis.html">132 nips-2004-Nonlinear Blind Source Separation by Integrating Independent Component Analysis and Slow Feature Analysis</a></p>
<p>12 0.047150236 <a title="152-tfidf-12" href="./nips-2004-Sparse_Coding_of_Natural_Images_Using_an_Overcomplete_Set_of_Limited_Capacity_Units.html">172 nips-2004-Sparse Coding of Natural Images Using an Overcomplete Set of Limited Capacity Units</a></p>
<p>13 0.046616841 <a title="152-tfidf-13" href="./nips-2004-Synergistic_Face_Detection_and_Pose_Estimation_with_Energy-Based_Models.html">182 nips-2004-Synergistic Face Detection and Pose Estimation with Energy-Based Models</a></p>
<p>14 0.046172712 <a title="152-tfidf-14" href="./nips-2004-Adaptive_Discriminative_Generative_Model_and_Its_Applications.html">16 nips-2004-Adaptive Discriminative Generative Model and Its Applications</a></p>
<p>15 0.045766797 <a title="152-tfidf-15" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>16 0.045336615 <a title="152-tfidf-16" href="./nips-2004-Beat_Tracking_the_Graphical_Model_Way.html">29 nips-2004-Beat Tracking the Graphical Model Way</a></p>
<p>17 0.04415682 <a title="152-tfidf-17" href="./nips-2004-Pictorial_Structures_for_Molecular_Modeling%3A_Interpreting_Density_Maps.html">146 nips-2004-Pictorial Structures for Molecular Modeling: Interpreting Density Maps</a></p>
<p>18 0.043557651 <a title="152-tfidf-18" href="./nips-2004-Unsupervised_Variational_Bayesian_Learning_of_Nonlinear_Models.html">198 nips-2004-Unsupervised Variational Bayesian Learning of Nonlinear Models</a></p>
<p>19 0.039979279 <a title="152-tfidf-19" href="./nips-2004-Multiple_Alignment_of_Continuous_Time_Series.html">124 nips-2004-Multiple Alignment of Continuous Time Series</a></p>
<p>20 0.037951354 <a title="152-tfidf-20" href="./nips-2004-Modeling_Nonlinear_Dependencies_in_Natural_Images_using_Mixture_of_Laplacian_Distribution.html">121 nips-2004-Modeling Nonlinear Dependencies in Natural Images using Mixture of Laplacian Distribution</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.152), (1, -0.027), (2, 0.007), (3, -0.208), (4, -0.239), (5, -0.303), (6, 0.305), (7, 0.118), (8, 0.017), (9, 0.031), (10, 0.021), (11, -0.005), (12, -0.058), (13, 0.148), (14, -0.194), (15, 0.045), (16, 0.033), (17, -0.059), (18, -0.029), (19, 0.116), (20, -0.044), (21, 0.157), (22, 0.1), (23, -0.034), (24, 0.155), (25, -0.094), (26, 0.046), (27, -0.081), (28, -0.057), (29, -0.016), (30, 0.005), (31, -0.096), (32, 0.03), (33, -0.051), (34, -0.043), (35, -0.021), (36, 0.065), (37, 0.011), (38, -0.044), (39, -0.013), (40, 0.051), (41, -0.009), (42, 0.022), (43, 0.006), (44, 0.06), (45, 0.126), (46, 0.008), (47, -0.005), (48, -0.028), (49, -0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98026466 <a title="152-lsi-1" href="./nips-2004-Real-Time_Pitch_Determination_of_One_or_More_Voices_by_Nonnegative_Matrix_Factorization.html">152 nips-2004-Real-Time Pitch Determination of One or More Voices by Nonnegative Matrix Factorization</a></p>
<p>Author: Fei Sha, Lawrence K. Saul</p><p>Abstract: An auditory “scene”, composed of overlapping acoustic sources, can be viewed as a complex object whose constituent parts are the individual sources. Pitch is known to be an important cue for auditory scene analysis. In this paper, with the goal of building agents that operate in human environments, we describe a real-time system to identify the presence of one or more voices and compute their pitch. The signal processing in the front end is based on instantaneous frequency estimation, a method for tracking the partials of voiced speech, while the pattern-matching in the back end is based on nonnegative matrix factorization, an unsupervised algorithm for learning the parts of complex objects. While supporting a framework to analyze complicated auditory scenes, our system maintains real-time operability and state-of-the-art performance in clean speech.</p><p>2 0.80967629 <a title="152-lsi-2" href="./nips-2004-Blind_One-microphone_Speech_Separation%3A_A_Spectral_Learning_Approach.html">31 nips-2004-Blind One-microphone Speech Separation: A Spectral Learning Approach</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: We present an algorithm to perform blind, one-microphone speech separation. Our algorithm separates mixtures of speech without modeling individual speakers. Instead, we formulate the problem of speech separation as a problem in segmenting the spectrogram of the signal into two or more disjoint sets. We build feature sets for our segmenter using classical cues from speech psychophysics. We then combine these features into parameterized afﬁnity matrices. We also take advantage of the fact that we can generate training examples for segmentation by artiﬁcially superposing separately-recorded signals. Thus the parameters of the afﬁnity matrices can be tuned using recent work on learning spectral clustering [1]. This yields an adaptive, speech-speciﬁc segmentation algorithm that can successfully separate one-microphone speech mixtures. 1</p><p>3 0.78332317 <a title="152-lsi-3" href="./nips-2004-A_Harmonic_Excitation_State-Space_Approach_to_Blind_Separation_of_Speech.html">5 nips-2004-A Harmonic Excitation State-Space Approach to Blind Separation of Speech</a></p>
<p>Author: Rasmus K. Olsson, Lars K. Hansen</p><p>Abstract: We discuss an identiﬁcation framework for noisy speech mixtures. A block-based generative model is formulated that explicitly incorporates the time-varying harmonic plus noise (H+N) model for a number of latent sources observed through noisy convolutive mixtures. All parameters including the pitches of the source signals, the amplitudes and phases of the sources, the mixing ﬁlters and the noise statistics are estimated by maximum likelihood, using an EM-algorithm. Exact averaging over the hidden sources is obtained using the Kalman smoother. We show that pitch estimation and source separation can be performed simultaneously. The pitch estimates are compared to laryngograph (EGG) measurements. Artiﬁcial and real room mixtures are used to demonstrate the viability of the approach. Intelligible speech signals are re-synthesized from the estimated H+N models.</p><p>4 0.69166172 <a title="152-lsi-4" href="./nips-2004-Modeling_Conversational_Dynamics_as_a_Mixed-Memory_Markov_Process.html">120 nips-2004-Modeling Conversational Dynamics as a Mixed-Memory Markov Process</a></p>
<p>Author: Tanzeem Choudhury, Sumit Basu</p><p>Abstract: In this work, we quantitatively investigate the ways in which a given person influences the joint turn-taking behavior in a conversation. After collecting an auditory database of social interactions among a group of twenty-three people via wearable sensors (66 hours of data each over two weeks), we apply speech and conversation detection methods to the auditory streams. These methods automatically locate the conversations, determine their participants, and mark which participant was speaking when. We then model the joint turn-taking behavior as a Mixed-Memory Markov Model [1] that combines the statistics of the individual subjects' self-transitions and the partners ' cross-transitions. The mixture parameters in this model describe how much each person 's individual behavior contributes to the joint turn-taking behavior of the pair. By estimating these parameters, we thus estimate how much influence each participant has in determining the joint turntaking behavior. We show how this measure correlates significantly with betweenness centrality [2], an independent measure of an individual's importance in a social network. This result suggests that our estimate of conversational influence is predictive of social influence. 1</p><p>5 0.51413661 <a title="152-lsi-5" href="./nips-2004-Bayesian_Regularization_and_Nonnegative_Deconvolution_for_Time_Delay_Estimation.html">27 nips-2004-Bayesian Regularization and Nonnegative Deconvolution for Time Delay Estimation</a></p>
<p>Author: Yuanqing Lin, Daniel D. Lee</p><p>Abstract: Bayesian Regularization and Nonnegative Deconvolution (BRAND) is proposed for estimating time delays of acoustic signals in reverberant environments. Sparsity of the nonnegative ﬁlter coefﬁcients is enforced using an L1 -norm regularization. A probabilistic generative model is used to simultaneously estimate the regularization parameters and ﬁlter coefﬁcients from the signal data. Iterative update rules are derived under a Bayesian framework using the Expectation-Maximization procedure. The resulting time delay estimation algorithm is demonstrated on noisy acoustic data.</p><p>6 0.40756375 <a title="152-lsi-6" href="./nips-2004-Learning_Efficient_Auditory_Codes_Using_Spikes_Predicts_Cochlear_Filters.html">97 nips-2004-Learning Efficient Auditory Codes Using Spikes Predicts Cochlear Filters</a></p>
<p>7 0.32228786 <a title="152-lsi-7" href="./nips-2004-Harmonising_Chorales_by_Probabilistic_Inference.html">74 nips-2004-Harmonising Chorales by Probabilistic Inference</a></p>
<p>8 0.30185503 <a title="152-lsi-8" href="./nips-2004-Beat_Tracking_the_Graphical_Model_Way.html">29 nips-2004-Beat Tracking the Graphical Model Way</a></p>
<p>9 0.29629537 <a title="152-lsi-9" href="./nips-2004-Nonlinear_Blind_Source_Separation_by_Integrating_Independent_Component_Analysis_and_Slow_Feature_Analysis.html">132 nips-2004-Nonlinear Blind Source Separation by Integrating Independent Component Analysis and Slow Feature Analysis</a></p>
<p>10 0.20744067 <a title="152-lsi-10" href="./nips-2004-An_Auditory_Paradigm_for_Brain-Computer_Interfaces.html">20 nips-2004-An Auditory Paradigm for Brain-Computer Interfaces</a></p>
<p>11 0.19759212 <a title="152-lsi-11" href="./nips-2004-Making_Latin_Manuscripts_Searchable_using_gHMM%27s.html">107 nips-2004-Making Latin Manuscripts Searchable using gHMM's</a></p>
<p>12 0.19269088 <a title="152-lsi-12" href="./nips-2004-Linear_Multilayer_Independent_Component_Analysis_for_Large_Natural_Scenes.html">104 nips-2004-Linear Multilayer Independent Component Analysis for Large Natural Scenes</a></p>
<p>13 0.18928899 <a title="152-lsi-13" href="./nips-2004-Schema_Learning%3A_Experience-Based_Construction_of_Predictive_Action_Models.html">159 nips-2004-Schema Learning: Experience-Based Construction of Predictive Action Models</a></p>
<p>14 0.18404587 <a title="152-lsi-14" href="./nips-2004-Markov_Networks_for_Detecting_Overalpping_Elements_in_Sequence_Data.html">108 nips-2004-Markov Networks for Detecting Overalpping Elements in Sequence Data</a></p>
<p>15 0.17810495 <a title="152-lsi-15" href="./nips-2004-Heuristics_for_Ordering_Cue_Search_in_Decision_Making.html">75 nips-2004-Heuristics for Ordering Cue Search in Decision Making</a></p>
<p>16 0.17423253 <a title="152-lsi-16" href="./nips-2004-Pictorial_Structures_for_Molecular_Modeling%3A_Interpreting_Density_Maps.html">146 nips-2004-Pictorial Structures for Molecular Modeling: Interpreting Density Maps</a></p>
<p>17 0.16581663 <a title="152-lsi-17" href="./nips-2004-Multiple_Alignment_of_Continuous_Time_Series.html">124 nips-2004-Multiple Alignment of Continuous Time Series</a></p>
<p>18 0.16397807 <a title="152-lsi-18" href="./nips-2004-Using_Machine_Learning_to_Break_Visual_Human_Interaction_Proofs_%28HIPs%29.html">199 nips-2004-Using Machine Learning to Break Visual Human Interaction Proofs (HIPs)</a></p>
<p>19 0.15013772 <a title="152-lsi-19" href="./nips-2004-Common-Frame_Model_for_Object_Recognition.html">40 nips-2004-Common-Frame Model for Object Recognition</a></p>
<p>20 0.14583944 <a title="152-lsi-20" href="./nips-2004-Triangle_Fixing_Algorithms_for_the_Metric_Nearness_Problem.html">196 nips-2004-Triangle Fixing Algorithms for the Metric Nearness Problem</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.143), (15, 0.077), (26, 0.063), (31, 0.013), (33, 0.15), (35, 0.022), (39, 0.025), (50, 0.026), (71, 0.017), (94, 0.338)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8506614 <a title="152-lda-1" href="./nips-2004-Real-Time_Pitch_Determination_of_One_or_More_Voices_by_Nonnegative_Matrix_Factorization.html">152 nips-2004-Real-Time Pitch Determination of One or More Voices by Nonnegative Matrix Factorization</a></p>
<p>Author: Fei Sha, Lawrence K. Saul</p><p>Abstract: An auditory “scene”, composed of overlapping acoustic sources, can be viewed as a complex object whose constituent parts are the individual sources. Pitch is known to be an important cue for auditory scene analysis. In this paper, with the goal of building agents that operate in human environments, we describe a real-time system to identify the presence of one or more voices and compute their pitch. The signal processing in the front end is based on instantaneous frequency estimation, a method for tracking the partials of voiced speech, while the pattern-matching in the back end is based on nonnegative matrix factorization, an unsupervised algorithm for learning the parts of complex objects. While supporting a framework to analyze complicated auditory scenes, our system maintains real-time operability and state-of-the-art performance in clean speech.</p><p>2 0.72355509 <a title="152-lda-2" href="./nips-2004-Learning%2C_Regularization_and_Ill-Posed_Inverse_Problems.html">96 nips-2004-Learning, Regularization and Ill-Posed Inverse Problems</a></p>
<p>Author: Lorenzo Rosasco, Andrea Caponnetto, Ernesto D. Vito, Francesca Odone, Umberto D. Giovannini</p><p>Abstract: Many works have shown that strong connections relate learning from examples to regularization techniques for ill-posed inverse problems. Nevertheless by now there was no formal evidence neither that learning from examples could be seen as an inverse problem nor that theoretical results in learning theory could be independently derived using tools from regularization theory. In this paper we provide a positive answer to both questions. Indeed, considering the square loss, we translate the learning problem in the language of regularization theory and show that consistency results and optimal regularization parameter choice can be derived by the discretization of the corresponding inverse problem. 1</p><p>3 0.67524445 <a title="152-lda-3" href="./nips-2004-An_Application_of_Boosting_to_Graph_Classification.html">19 nips-2004-An Application of Boosting to Graph Classification</a></p>
<p>Author: Taku Kudo, Eisaku Maeda, Yuji Matsumoto</p><p>Abstract: This paper presents an application of Boosting for classifying labeled graphs, general structures for modeling a number of real-world data, such as chemical compounds, natural language texts, and bio sequences. The proposal consists of i) decision stumps that use subgraph as features, and ii) a Boosting algorithm in which subgraph-based decision stumps are used as weak learners. We also discuss the relation between our algorithm and SVMs with convolution kernels. Two experiments using natural language data and chemical compounds show that our method achieves comparable or even better performance than SVMs with convolution kernels as well as improves the testing efﬁciency. 1</p><p>4 0.67335463 <a title="152-lda-4" href="./nips-2004-Parametric_Embedding_for_Class_Visualization.html">145 nips-2004-Parametric Embedding for Class Visualization</a></p>
<p>Author: Tomoharu Iwata, Kazumi Saito, Naonori Ueda, Sean Stromsten, Thomas L. Griffiths, Joshua B. Tenenbaum</p><p>Abstract: In this paper, we propose a new method, Parametric Embedding (PE), for visualizing the posteriors estimated over a mixture model. PE simultaneously embeds both objects and their classes in a low-dimensional space. PE takes as input a set of class posterior vectors for given data points, and tries to preserve the posterior structure in an embedding space by minimizing a sum of Kullback-Leibler divergences, under the assumption that samples are generated by a Gaussian mixture with equal covariances in the embedding space. PE has many potential uses depending on the source of the input data, providing insight into the classiﬁer’s behavior in supervised, semi-supervised and unsupervised settings. The PE algorithm has a computational advantage over conventional embedding methods based on pairwise object relations since its complexity scales with the product of the number of objects and the number of classes. We demonstrate PE by visualizing supervised categorization of web pages, semi-supervised categorization of digits, and the relations of words and latent topics found by an unsupervised algorithm, Latent Dirichlet Allocation. 1</p><p>5 0.60483521 <a title="152-lda-5" href="./nips-2004-A_Harmonic_Excitation_State-Space_Approach_to_Blind_Separation_of_Speech.html">5 nips-2004-A Harmonic Excitation State-Space Approach to Blind Separation of Speech</a></p>
<p>Author: Rasmus K. Olsson, Lars K. Hansen</p><p>Abstract: We discuss an identiﬁcation framework for noisy speech mixtures. A block-based generative model is formulated that explicitly incorporates the time-varying harmonic plus noise (H+N) model for a number of latent sources observed through noisy convolutive mixtures. All parameters including the pitches of the source signals, the amplitudes and phases of the sources, the mixing ﬁlters and the noise statistics are estimated by maximum likelihood, using an EM-algorithm. Exact averaging over the hidden sources is obtained using the Kalman smoother. We show that pitch estimation and source separation can be performed simultaneously. The pitch estimates are compared to laryngograph (EGG) measurements. Artiﬁcial and real room mixtures are used to demonstrate the viability of the approach. Intelligible speech signals are re-synthesized from the estimated H+N models.</p><p>6 0.55399895 <a title="152-lda-6" href="./nips-2004-Synergies_between_Intrinsic_and_Synaptic_Plasticity_in_Individual_Model_Neurons.html">181 nips-2004-Synergies between Intrinsic and Synaptic Plasticity in Individual Model Neurons</a></p>
<p>7 0.55172253 <a title="152-lda-7" href="./nips-2004-Class-size_Independent_Generalization_Analsysis_of_Some_Discriminative_Multi-Category_Classification.html">36 nips-2004-Class-size Independent Generalization Analsysis of Some Discriminative Multi-Category Classification</a></p>
<p>8 0.54812777 <a title="152-lda-8" href="./nips-2004-Variational_Minimax_Estimation_of_Discrete_Distributions_under_KL_Loss.html">204 nips-2004-Variational Minimax Estimation of Discrete Distributions under KL Loss</a></p>
<p>9 0.54466754 <a title="152-lda-9" href="./nips-2004-Blind_One-microphone_Speech_Separation%3A_A_Spectral_Learning_Approach.html">31 nips-2004-Blind One-microphone Speech Separation: A Spectral Learning Approach</a></p>
<p>10 0.54364628 <a title="152-lda-10" href="./nips-2004-Learning_first-order_Markov_models_for_control.html">102 nips-2004-Learning first-order Markov models for control</a></p>
<p>11 0.54317212 <a title="152-lda-11" href="./nips-2004-Semi-parametric_Exponential_Family_PCA.html">163 nips-2004-Semi-parametric Exponential Family PCA</a></p>
<p>12 0.54304612 <a title="152-lda-12" href="./nips-2004-Non-Local_Manifold_Tangent_Learning.html">131 nips-2004-Non-Local Manifold Tangent Learning</a></p>
<p>13 0.54138631 <a title="152-lda-13" href="./nips-2004-Message_Errors_in_Belief_Propagation.html">116 nips-2004-Message Errors in Belief Propagation</a></p>
<p>14 0.54129261 <a title="152-lda-14" href="./nips-2004-Kernel_Projection_Machine%3A_a_New_Tool_for_Pattern_Recognition.html">93 nips-2004-Kernel Projection Machine: a New Tool for Pattern Recognition</a></p>
<p>15 0.53860766 <a title="152-lda-15" href="./nips-2004-Following_Curved_Regularized_Optimization_Solution_Paths.html">70 nips-2004-Following Curved Regularized Optimization Solution Paths</a></p>
<p>16 0.53786546 <a title="152-lda-16" href="./nips-2004-Multiple_Alignment_of_Continuous_Time_Series.html">124 nips-2004-Multiple Alignment of Continuous Time Series</a></p>
<p>17 0.53759038 <a title="152-lda-17" href="./nips-2004-Experts_in_a_Markov_Decision_Process.html">64 nips-2004-Experts in a Markov Decision Process</a></p>
<p>18 0.53709507 <a title="152-lda-18" href="./nips-2004-Worst-Case_Analysis_of_Selective_Sampling_for_Linear-Threshold_Algorithms.html">206 nips-2004-Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms</a></p>
<p>19 0.5365302 <a title="152-lda-19" href="./nips-2004-An_Investigation_of_Practical_Approximate_Nearest_Neighbor_Algorithms.html">22 nips-2004-An Investigation of Practical Approximate Nearest Neighbor Algorithms</a></p>
<p>20 0.53607678 <a title="152-lda-20" href="./nips-2004-Bayesian_inference_in_spiking_neurons.html">28 nips-2004-Bayesian inference in spiking neurons</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
