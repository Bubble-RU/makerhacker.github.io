<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>36 nips-2004-Class-size Independent Generalization Analsysis of Some Discriminative Multi-Category Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-36" href="#">nips2004-36</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>36 nips-2004-Class-size Independent Generalization Analsysis of Some Discriminative Multi-Category Classification</h1>
<br/><p>Source: <a title="nips-2004-36-pdf" href="http://papers.nips.cc/paper/2657-class-size-independent-generalization-analsysis-of-some-discriminative-multi-category-classification.pdf">pdf</a></p><p>Author: Tong Zhang</p><p>Abstract: We consider the problem of deriving class-size independent generalization bounds for some regularized discriminative multi-category classiﬁcation methods. In particular, we obtain an expected generalization bound for a standard formulation of multi-category support vector machines. Based on the theoretical result, we argue that the formulation over-penalizes misclassiﬁcation error, which in theory may lead to poor generalization performance. A remedy, based on a generalization of multi-category logistic regression (conditional maximum entropy), is then proposed, and its theoretical properties are examined. 1</p><p>Reference: <a title="nips-2004-36-reference" href="../nips2004_reference/nips-2004-Class-size_Independent_Generalization_Analsysis_of_Some_Discriminative_Multi-Category_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We consider the problem of deriving class-size independent generalization bounds for some regularized discriminative multi-category classiﬁcation methods. [sent-5, score-0.204]
</p><p>2 In particular, we obtain an expected generalization bound for a standard formulation of multi-category support vector machines. [sent-6, score-0.235]
</p><p>3 Based on the theoretical result, we argue that the formulation over-penalizes misclassiﬁcation error, which in theory may lead to poor generalization performance. [sent-7, score-0.155]
</p><p>4 A remedy, based on a generalization of multi-category logistic regression (conditional maximum entropy), is then proposed, and its theoretical properties are examined. [sent-8, score-0.129]
</p><p>5 In many applications, the output space Y can be extremely large, and may be regarded as inﬁnity for practical purposes. [sent-10, score-0.035]
</p><p>6 As another example, in machine learning based webpage search and ranking, the input is the keywords and the output space consists of all web-pages. [sent-13, score-0.047]
</p><p>7 In order to handle such application tasks, from the theoretical point of view, we do not need to assume that the output space Y is ﬁnite, so that it is crucial to obtain generalization bounds that are independent of the size of Y. [sent-14, score-0.175]
</p><p>8 For example, for web-page search, GEN(x) consists of all pages that contain one or more keywords in x. [sent-16, score-0.022]
</p><p>9 For sequence annotation, GEN(x) may include all annotation sequences that are consistent. [sent-17, score-0.022]
</p><p>10 Therefore it is important that our learning bounds are independent of the size of GEN(x). [sent-19, score-0.056]
</p><p>11 Our classiﬁer is characterized by a weight vector w ∈ H, with the following classiﬁcation rule: pw (x) = arg  max c∈GEN(x)  w · fx,c . [sent-24, score-0.075]
</p><p>12 The quality of the predictor w is measured by some loss function. [sent-30, score-0.039]
</p><p>13 The important issue of class-size independent (or weakly dependent) generalization analysis has also been discussed there. [sent-33, score-0.109]
</p><p>14 In this paper, we focus on some loss functions of the following form:   L(w, X, Y ) = ψ   φ(w · (fX,Y − fX,c )) ,  c∈GEN(X)\Y  where ψ and φ are appropriately chosen real-valued functions. [sent-40, score-0.04]
</p><p>15 Typically ψ is chosen as an increasing function and φ as a decreasing function, selected so that (3) is a convex optimization problem. [sent-41, score-0.022]
</p><p>16 The intuition behind this method is that the resulting optimization formulation favors large values w · (fXi ,Yi − fXi ,c ) for all c ∈ GEN(Xi )\Yi . [sent-42, score-0.079]
</p><p>17 Therefore, it favors a weight vector w ∈ H such that w · fXi ,Yi = arg maxc∈GEN(Xi ) w · fXi ,c , which encourages the correct classiﬁcation rule in (1). [sent-43, score-0.054]
</p><p>18 Two of the most important methods used in practice, multi-category support vector machines [7] and penalized multi-category logistic regression (conditional maximum entropy with Gaussian smoothing [1]), can be regarded as special cases of (3). [sent-45, score-0.17]
</p><p>19 The purpose of this paper is to study their generalization behaviors. [sent-46, score-0.091]
</p><p>20 In particular, we are interested in generalization bounds that are independent of the size of GEN(Xi ). [sent-47, score-0.159]
</p><p>21 2  Multi-category Support Vector Machines  We consider the multi-category support vector machine method proposed in [7]. [sent-48, score-0.037]
</p><p>22 It is a ˆ special case of (3) with wS computed based on the following formula:   n λ 1 ˆ h(w · (fXi ,Yi − ·fXi ,c )) + w 2 , (4) wS = arg min  H w∈H n 2 i=1 c∈GEN(Xi )\Yi  where h(z) = max(1 − z, 0) is the hinge loss used in the standard SVM formulation. [sent-49, score-0.063]
</p><p>23 From the asymptotic statistical point of view, this formulation has some drawbacks in that there are cases such that the method does not lead to a classiﬁer that achieves the Bayes error [9] (inconsistency). [sent-50, score-0.096]
</p><p>24 A Bayes consistent remedy has been proposed in [4]. [sent-51, score-0.046]
</p><p>25 However, method based on (4) has some attractive properties, and has been successfully used for some practical problems. [sent-52, score-0.025]
</p><p>26 We are interested in the generalization performance of (4). [sent-53, score-0.103]
</p><p>27 As we shall see, this formulation performs very well in the linearly separable (or near separable) case. [sent-54, score-0.097]
</p><p>28 We start with the following theorem, which speciﬁes a generalization bound in a form often referred to as the oracle inequality. [sent-58, score-0.145]
</p><p>29 That is, it bounds the generalization performance of the SVM method (4) in terms of the best possible true multi-category SVM loss. [sent-59, score-0.141]
</p><p>30 The expected general-  ˆ h(wS · (fX,Y − fX,c ))  sup c∈GEN(X)\Y  2  ≤  max(λn, M ) + M λn    2  h(w · (fX,Y − fX,c )) +  inf E(X,Y )  w∈H  c∈GEN(X)\Y  2 H    λn w  , 2(n + 1)  where ES is the expectation with respect to the training data. [sent-63, score-0.143]
</p><p>31 Note that the generalization bound does not depend on the size of GEN(X), which is what we want to achieve. [sent-64, score-0.145]
</p><p>32 The left-hand side of the theorem bounds the classiﬁcation error ˆ of the multi-category SVM classiﬁer in terms of supc∈GEN(X)\Y h(wS · (fX,Y − fX,c )), while the right hand side in terms of c∈GEN(X)\Y h(w · (fX,Y − fX,c )). [sent-65, score-0.176]
</p><p>33 The latter is a very loose bound since it over-counts classiﬁcation errors in the summation when multiple errors are made at the same point. [sent-67, score-0.128]
</p><p>34 In fact, although the class-size dependency does not come into our generalization analysis, it may well come into the summation term c∈GEN(X)\Y h(w · (fX,Y − fX,c )) when multiple errors are made at the same point. [sent-68, score-0.149]
</p><p>35 We believe that this is a serious ﬂaw of the method, which we will try to remedy later. [sent-69, score-0.046]
</p><p>36 However, the bound can be quite tight in the near separable case, ˆ when c∈GEN(X)\Y h(wS · (fX,Y − fX,c )) is small. [sent-70, score-0.101]
</p><p>37 1 Assume that there is a large margin separator w∗ ∈ H such that for each data point (X, Y ), the following margin condition holds: ∀c ∈ GEN(X)\Y : w∗ · fX,Y ≥ w∗ · fX,c + 1. [sent-72, score-0.076]
</p><p>38 Then in the limit of λ → 0, the expected generalization error of (4) can be bounded as: ES  ˆ D (wS )  ≤  w∗ 2 H sup sup fX,Y − fX,Y n + 1 X Y,Y ∈GEN(X)  2 H,  where ES is the expectation with respect to the training data. [sent-73, score-0.319]
</p><p>39 Just choose w∗ on the right hand side of Theorem 2. [sent-75, score-0.047]
</p><p>40 2 The above result for (4) gives a class-size independent bound for large margin separable problems. [sent-77, score-0.157]
</p><p>41 The bound generalizes a similar result for two-class hard-margin SVM. [sent-78, score-0.054]
</p><p>42 It also matches a bound for multi-class perceptron in [2]. [sent-79, score-0.054]
</p><p>43 To our knowledge, this is the ﬁrst result showing that the generalization performance of a batch large margin algorithm such as (4) can be class-size independent (at least in the separable case). [sent-80, score-0.194]
</p><p>44 Previous results in [2, 6], relying on the covering number analysis, lead to bounds that depend on the size of Y (although the result in [6] is of a different style). [sent-81, score-0.052]
</p><p>45 Our analysis also implies that the multi-category classiﬁcation method (4) has good generalization behavior for separable problems. [sent-82, score-0.167]
</p><p>46 However, as pointed out earlier, for nonseparable problems, the formulation over-penalize classiﬁcation error since in the summation, it may count classiﬁcation error at a point multiple times when multiple mistakes are made at the point. [sent-83, score-0.09]
</p><p>47 A remedy is to replace the summation symbol c∈GEN(Xi )\Yi in (4) by the sup operator supc∈GEN(Xi )\Yi , as we have used for bounding the classiﬁcation error on the left hand side of Theorem 2. [sent-84, score-0.28]
</p><p>48 However, like (4), the resulting formulation is also inconsistent. [sent-87, score-0.05]
</p><p>49 For example, consider the equality supc |hc | = limp→∞ ( c |hc |p )1/p , we may approximate the right hand side limit with a large p. [sent-89, score-0.16]
</p><p>50 Another more interesting formulation is to consider supc hc = limp→∞ p−1 ln( c exp(phc )), which leads to a generalization of the conditional maximum entropy method. [sent-90, score-0.356]
</p><p>51 3  Large Margin Discriminative Maximum Entropy Method  Based on the motivation given at the end of the last section, we propose the following generalization of maximum entropy (multi-category logistic regression) with Gaussian prior (see [1]). [sent-91, score-0.179]
</p><p>52 If we choose γ = 0, then this formulation is equivalent to the standard maximum entropy method. [sent-93, score-0.121]
</p><p>53 If we pick the margin parameter γ = 1, and let p → ∞, then   1  ln 1 + ep(γ−w·(fXi ,Yi −fXi ,c ))  → sup h(w·(fXi ,Yi −fXi ,c )), p c∈GEN(Xi )\Yi c∈GEN(Xi )\Yi  where h(z) = max(0, 1 − z) is used in (4). [sent-94, score-0.163]
</p><p>54 In this case, the formulation reduces to (4) but with c∈GEN(Xi )\Yi replaced by supc∈GEN(Xi )\Yi . [sent-95, score-0.062]
</p><p>55 In general, even with a ﬁnite scaling factor p, the log-transform in (4) guarantees that 1 one penalizes misclassiﬁcation error at most p ln |GEN(Xi )| times at a point, where  |GEN(Xi )| is the size of GEN(Xi ), while in (4), one may potentially over-penalize |GEN(Xi )| times. [sent-97, score-0.058]
</p><p>56 In particular, we are able to derive class-size independent generalization bounds for this method. [sent-100, score-0.147]
</p><p>57 The proof of the following theorem is given in Appendix C. [sent-101, score-0.064]
</p><p>58 1 Let M = supX supY,Y as:   ∈GEN(X)  1  ln 1 + p  L(w, x, y) =  fX,Y − fX,Y  H. [sent-103, score-0.038]
</p><p>59 Deﬁne loss L(w, x, y)   ep(γ−w·(fx,y −fx,c ))  , c∈GEN(x)\y  and let λn w 2(n + 1) The expected generalization error of (5) can be bounded as: Qλ = inf  w∈H  E(X,Y ) L(w, X, Y ) +  ˆ ES E(X,Y ) L(wS , X, Y ) ≤ Qλ +  2 H  . [sent-104, score-0.16]
</p><p>60 λn  where ES is the expectation with respect to the training data. [sent-106, score-0.034]
</p><p>61 Note that the left ˆ hand side is the true loss of the wS from (5), and the right hand size is speciﬁed in terms of the best possible regularized true loss Qλ , plus a penalty term that is no larger than M 2 /(λn). [sent-109, score-0.146]
</p><p>62 It is clear that this generalization bound is class-size independent. [sent-110, score-0.145]
</p><p>63 1, the loss function on the left hand side matches the loss function on the right hand side in Theorem 3. [sent-112, score-0.148]
</p><p>64 We believe this is a great advantage for the maximum entropy-type discriminative learning method in (5). [sent-116, score-0.062]
</p><p>65 Moreover, we can see that the generalization performance is well-behaved no matter what values of p and γ we choose. [sent-118, score-0.091]
</p><p>66 If we take γ = 0 and p = 1, then we obtain a generalization bound for the popular maximum entropy method with Gaussian prior, which has been widely used in natural language processing applications. [sent-119, score-0.243]
</p><p>67 To our knowledge, this is the ﬁrst generalization bound derived for this method. [sent-120, score-0.145]
</p><p>68 Our result not only shows the importance of Gaussian prior regularization, but also implies that the regularized conditional maximum entropy method has very desirable generalization behavior. [sent-121, score-0.246]
</p><p>69 In this case, we note that 0 ≤ L(w, X, Y ) − supc∈GEN(X)\Y h(w · (fX,Y − fX,c )) ≤ ln |GEN(X)| . [sent-124, score-0.038]
</p><p>70 1 a bound ES E(X,Y )  ˆ h(wS · (fX,Y − fX,c )) ≤  sup c∈GEN(X)\Y  + inf  w∈H  E(X,Y )  sup c∈GEN(X)\Y  EX ln |GEN(X)| M 2 + p λn  h(w · (fX,Y − fX,c )) +  λ w 2  2 H  . [sent-126, score-0.288]
</p><p>71 Now we can take a sufﬁciently large p such that the term EX ln |GEN(X)|/p becomes negligible. [sent-127, score-0.038]
</p><p>72 Let p → ∞, the result implies a bound for the SVM method in [3]. [sent-128, score-0.083]
</p><p>73 For non-separable problems, this bound is clearly superior to the SVM bound in Theorem 2. [sent-129, score-0.108]
</p><p>74 1 since the right hand side replaces the summation c∈GEN(X)\Y by the sup operator  supc∈GEN(X)\Y . [sent-130, score-0.214]
</p><p>75 In theory, this satisfactorily solves the problem of over-penalizing misclassiﬁcation error. [sent-131, score-0.026]
</p><p>76 Our analysis also establishes a bridge between the Gaussian smoothed maximum entropy method [1] and the SVM method in [3]. [sent-133, score-0.095]
</p><p>77 4  Conclusion  We studied the generalization performance of some regularized multi-category classiﬁcation methods. [sent-134, score-0.119]
</p><p>78 In particular, we derived a class-size independent generalization bound for a standard formulation of multi-category support vector machines. [sent-135, score-0.238]
</p><p>79 Based on the theoretical investigation, we showed that this method works well for linearly separable problems. [sent-136, score-0.059]
</p><p>80 However, it over-penalizes mis-classiﬁcation error, leading to loose generalization bounds in the non-separable case. [sent-137, score-0.145]
</p><p>81 A remedy, based on a generalization of the maximum entropy method, is proposed. [sent-138, score-0.162]
</p><p>82 Moreover, we are able to derive class-size independent bounds for the newly proposed formulation, which implies that this class of methods (including the standard maximum entropy) are suitable for classiﬁcation problems with very large number of classes. [sent-139, score-0.094]
</p><p>83 We showed that in theory, the new formulation provides a satisfactory solution to the problem of over-penalizing mis-classiﬁcation error. [sent-140, score-0.05]
</p><p>84 A  A general stability bound  The following lemma is essentially a variant of similar stability results for regularized learning systems used in [8, 10]. [sent-141, score-0.126]
</p><p>85 Li (w) + wk = arg min H w 2 i=1 Then for all k ≥ 1, there exists subgradient (cf. [sent-151, score-0.572]
</p><p>86 [5]) that wk+1 = −  1 λn  Lk+1 (wk+1 ) of Li at wk+1 such  k+1  wk − wk+1  Li (wk+1 ),  H  i=1  ≤  1 λn  Lk+1 (wk+1 )  H. [sent-152, score-0.519]
</p><p>87 Now, subtracting this equality at wk and wk+1 , we have: k  −λn(wk+1 − wk ) =  ( Li (wk+1 ) −  Lk+1 (wk+1 ) +  Li (wk )). [sent-155, score-1.054]
</p><p>88 i=1  Multiply the two sides by wk+1 − wk , we obtain k  −λn wk+1 −wk  2 H  =  Lk+1 (wk+1 )·(wk+1 −wk )+  ( Li (wk+1 )− Li (wk ))·(wk+1 −wk ). [sent-156, score-0.534]
</p><p>89 We thus have ( Li (wk+1 )− Li (wk ))·(wk+1 − wk ) ≥ 0. [sent-158, score-0.519]
</p><p>90 It follows that −λn wk+1 − wk  2 H  ≥  Lk+1 (wk+1 ) · (wk+1 − wk ) ≥ −  By canceling the factor wk+1 − wk  H,  Lk+1 (wk+1 )  H  wk+1 − wk  we obtain the second inequality. [sent-159, score-2.091]
</p><p>91 Let wk be the solution of (4) with n+1 the training sample (Xk , Yk ) removed from the set (that is, the summation is i=1,i=k ), n n+1 ˜ and let w be the solution of (4) but with the summation i=1 replaced by i=1 . [sent-166, score-0.676]
</p><p>92 1 that ˜ w  2 H  =−  1 λn  n+1  h (zk,c )zk,c ,  ˜ ˜ wk − w  H  ≤−  k=1 c∈GEN(X)  M λn  h (zk,c ), c∈GEN(X)\Y  where h (·) denotes a subgradient of h(·). [sent-169, score-0.547]
</p><p>93 Therefore using the inequality −h (z) ≤ h(z) − h (z)z, we have ˜ ˜ ˜ [h(wk · (fXk ,Yk − fXk ,c )) − h(zk,c )] ≤ wk − w  sup  HM  c∈GEN(Xk )\Yk  ≤−  M2 λn  h (zk,c ) ≤ c∈GEN(Xk )\Yk  M2 λn  [h(zk,c ) − h (zk,c )zk,c ]. [sent-170, score-0.606]
</p><p>94 , n + 1, we obtain n+1  ˜ [h(wk · (fXk ,Yk − fXk ,c )) − h(zk,c )]  sup k=1 c∈GEN(Xk )\Yk  M2 ≤ λn =  M2 λn  n+1  [h(zk,c ) − h (zk,c )zk,c ] c∈GEN(Xk )\Yk k=1 n+1  ˜ h(zk,c ) + w  2 2 HM . [sent-174, score-0.102]
</p><p>95 c∈GEN(Xk )\Yk k=1  Now, taking expectation with respect to the training data, we obtain the bound. [sent-176, score-0.049]
</p><p>96 Let wk be the solution of (5) with the training sample (Xk , Yk ) removed  n+1 ˜ from the set (that is, the summation is i=1,i=k ), and let w be the solution of (5) but with n n+1 the summation i=1 replaced by i=1 . [sent-183, score-0.676]
</p><p>97 1 that 1 M ˜ ˜ ˜ ˜ wk − w H ≤ L(w, Xk , Yk ) H ≤ (1 − e−pL(w,Xk ,Yk ) ). [sent-185, score-0.519]
</p><p>98 L(wk , Xk , Yk ) − L(w, Xk , Yk ) ≤ λn Now summing over k, we obtain  1 n+1  n+1  ˜ L(wk , Xk , Yk ) ≤ k=1  1 n+1  n+1  ˜ L(w, Xk , Yk ) + k=1  M2 λn  1−  1 n+1  n+1 ˜ e−pL(w,Xk ,Yk )  . [sent-187, score-0.032]
</p><p>99 Multicategory support vector machines, theory, and application to the classiﬁcation of microarray data and satellite radiance data. [sent-208, score-0.025]
</p><p>100 Statistical analysis of some multi-category large margin classiﬁcation methods. [sent-227, score-0.038]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gen', 0.723), ('wk', 0.519), ('fxi', 0.166), ('yk', 0.148), ('ws', 0.145), ('fxk', 0.139), ('xk', 0.122), ('supc', 0.097), ('generalization', 0.091), ('sup', 0.087), ('li', 0.059), ('summation', 0.058), ('bound', 0.054), ('lk', 0.053), ('entropy', 0.05), ('formulation', 0.05), ('pl', 0.048), ('separable', 0.047), ('remedy', 0.046), ('yi', 0.044), ('theorem', 0.041), ('classi', 0.039), ('bounds', 0.038), ('ln', 0.038), ('margin', 0.038), ('xi', 0.038), ('hc', 0.036), ('cation', 0.033), ('tong', 0.031), ('hm', 0.031), ('side', 0.03), ('discriminative', 0.029), ('regularized', 0.028), ('dli', 0.028), ('limp', 0.028), ('subgradient', 0.028), ('loss', 0.027), ('svm', 0.027), ('arg', 0.025), ('pes', 0.024), ('pw', 0.024), ('es', 0.024), ('proof', 0.023), ('operator', 0.022), ('inf', 0.022), ('convex', 0.022), ('annotation', 0.022), ('keywords', 0.022), ('supx', 0.022), ('regarded', 0.022), ('misclassi', 0.021), ('maximum', 0.021), ('expectation', 0.02), ('error', 0.02), ('sketch', 0.019), ('ep', 0.019), ('lemma', 0.018), ('independent', 0.018), ('hand', 0.017), ('logistic', 0.017), ('favors', 0.017), ('princeton', 0.017), ('summing', 0.017), ('implies', 0.017), ('equality', 0.016), ('loose', 0.016), ('desirable', 0.016), ('removed', 0.015), ('ex', 0.015), ('obtain', 0.015), ('lead', 0.014), ('max', 0.014), ('appendix', 0.014), ('solves', 0.014), ('corollary', 0.014), ('training', 0.014), ('support', 0.013), ('stability', 0.013), ('appropriately', 0.013), ('attractive', 0.013), ('hilbert', 0.013), ('moreover', 0.013), ('output', 0.013), ('regularization', 0.013), ('replaced', 0.012), ('vector', 0.012), ('predictor', 0.012), ('interested', 0.012), ('carlos', 0.012), ('phc', 0.012), ('ronald', 0.012), ('satisfactorily', 0.012), ('stanley', 0.012), ('tzhang', 0.012), ('webpage', 0.012), ('method', 0.012), ('smoothing', 0.012), ('machines', 0.012), ('conditional', 0.011), ('special', 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="36-tfidf-1" href="./nips-2004-Class-size_Independent_Generalization_Analsysis_of_Some_Discriminative_Multi-Category_Classification.html">36 nips-2004-Class-size Independent Generalization Analsysis of Some Discriminative Multi-Category Classification</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We consider the problem of deriving class-size independent generalization bounds for some regularized discriminative multi-category classiﬁcation methods. In particular, we obtain an expected generalization bound for a standard formulation of multi-category support vector machines. Based on the theoretical result, we argue that the formulation over-penalizes misclassiﬁcation error, which in theory may lead to poor generalization performance. A remedy, based on a generalization of multi-category logistic regression (conditional maximum entropy), is then proposed, and its theoretical properties are examined. 1</p><p>2 0.20377815 <a title="36-tfidf-2" href="./nips-2004-Using_Random_Forests_in_the_Structured_Language_Model.html">200 nips-2004-Using Random Forests in the Structured Language Model</a></p>
<p>Author: Peng Xu, Frederick Jelinek</p><p>Abstract: In this paper, we explore the use of Random Forests (RFs) in the structured language model (SLM), which uses rich syntactic information in predicting the next word based on words already seen. The goal in this work is to construct RFs by randomly growing Decision Trees (DTs) using syntactic information and investigate the performance of the SLM modeled by the RFs in automatic speech recognition. RFs, which were originally developed as classiﬁers, are a combination of decision tree classiﬁers. Each tree is grown based on random training data sampled independently and with the same distribution for all trees in the forest, and a random selection of possible questions at each node of the decision tree. Our approach extends the original idea of RFs to deal with the data sparseness problem encountered in language modeling. RFs have been studied in the context of n-gram language modeling and have been shown to generalize well to unseen data. We show in this paper that RFs using syntactic information can also achieve better performance in both perplexity (PPL) and word error rate (WER) in a large vocabulary speech recognition system, compared to a baseline that uses Kneser-Ney smoothing. 1</p><p>3 0.097662807 <a title="36-tfidf-3" href="./nips-2004-Nearly_Tight_Bounds_for_the_Continuum-Armed_Bandit_Problem.html">126 nips-2004-Nearly Tight Bounds for the Continuum-Armed Bandit Problem</a></p>
<p>Author: Robert D. Kleinberg</p><p>Abstract: In the multi-armed bandit problem, an online algorithm must choose from a set of strategies in a sequence of n trials so as to minimize the total cost of the chosen strategies. While nearly tight upper and lower bounds are known in the case when the strategy set is ﬁnite, much less is known when there is an inﬁnite strategy set. Here we consider the case when the set of strategies is a subset of Rd , and the cost functions are continuous. In the d = 1 case, we improve on the best-known upper and lower bounds, closing the gap to a sublogarithmic factor. We also consider the case where d > 1 and the cost functions are convex, adapting a recent online convex optimization algorithm of Zinkevich to the sparser feedback model of the multi-armed bandit problem. 1</p><p>4 0.091246434 <a title="36-tfidf-4" href="./nips-2004-Learning_Preferences_for_Multiclass_Problems.html">100 nips-2004-Learning Preferences for Multiclass Problems</a></p>
<p>Author: Fabio Aiolli, Alessandro Sperduti</p><p>Abstract: Many interesting multiclass problems can be cast in the general framework of label ranking deﬁned on a given set of classes. The evaluation for such a ranking is generally given in terms of the number of violated order constraints between classes. In this paper, we propose the Preference Learning Model as a unifying framework to model and solve a large class of multiclass problems in a large margin perspective. In addition, an original kernel-based method is proposed and evaluated on a ranking dataset with state-of-the-art results. 1</p><p>5 0.074471287 <a title="36-tfidf-5" href="./nips-2004-Efficient_Out-of-Sample_Extension_of_Dominant-Set_Clusters.html">61 nips-2004-Efficient Out-of-Sample Extension of Dominant-Set Clusters</a></p>
<p>Author: Massimiliano Pavan, Marcello Pelillo</p><p>Abstract: Dominant sets are a new graph-theoretic concept that has proven to be relevant in pairwise data clustering problems, such as image segmentation. They generalize the notion of a maximal clique to edgeweighted graphs and have intriguing, non-trivial connections to continuous quadratic optimization and spectral-based grouping. We address the problem of grouping out-of-sample examples after the clustering process has taken place. This may serve either to drastically reduce the computational burden associated to the processing of very large data sets, or to efﬁciently deal with dynamic situations whereby data sets need to be updated continually. We show that the very notion of a dominant set offers a simple and efﬁcient way of doing this. Numerical experiments on various grouping problems show the effectiveness of the approach. 1</p><p>6 0.071960963 <a title="36-tfidf-6" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>7 0.067573473 <a title="36-tfidf-7" href="./nips-2004-Following_Curved_Regularized_Optimization_Solution_Paths.html">70 nips-2004-Following Curved Regularized Optimization Solution Paths</a></p>
<p>8 0.062528491 <a title="36-tfidf-8" href="./nips-2004-A_Large_Deviation_Bound_for_the_Area_Under_the_ROC_Curve.html">7 nips-2004-A Large Deviation Bound for the Area Under the ROC Curve</a></p>
<p>9 0.055624094 <a title="36-tfidf-9" href="./nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine.html">187 nips-2004-The Entire Regularization Path for the Support Vector Machine</a></p>
<p>10 0.054130051 <a title="36-tfidf-10" href="./nips-2004-Unsupervised_Variational_Bayesian_Learning_of_Nonlinear_Models.html">198 nips-2004-Unsupervised Variational Bayesian Learning of Nonlinear Models</a></p>
<p>11 0.0529873 <a title="36-tfidf-11" href="./nips-2004-Generalization_Error_Bounds_for_Collaborative_Prediction_with_Low-Rank_Matrices.html">71 nips-2004-Generalization Error Bounds for Collaborative Prediction with Low-Rank Matrices</a></p>
<p>12 0.047483269 <a title="36-tfidf-12" href="./nips-2004-The_Variational_Ising_Classifier_%28VIC%29_Algorithm_for_Coherently_Contaminated_Data.html">191 nips-2004-The Variational Ising Classifier (VIC) Algorithm for Coherently Contaminated Data</a></p>
<p>13 0.046663575 <a title="36-tfidf-13" href="./nips-2004-Variational_Minimax_Estimation_of_Discrete_Distributions_under_KL_Loss.html">204 nips-2004-Variational Minimax Estimation of Discrete Distributions under KL Loss</a></p>
<p>14 0.045530196 <a title="36-tfidf-14" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>15 0.044627734 <a title="36-tfidf-15" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>16 0.042654205 <a title="36-tfidf-16" href="./nips-2004-Semi-supervised_Learning_by_Entropy_Minimization.html">164 nips-2004-Semi-supervised Learning by Entropy Minimization</a></p>
<p>17 0.041889995 <a title="36-tfidf-17" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>18 0.041123245 <a title="36-tfidf-18" href="./nips-2004-Learning%2C_Regularization_and_Ill-Posed_Inverse_Problems.html">96 nips-2004-Learning, Regularization and Ill-Posed Inverse Problems</a></p>
<p>19 0.04070057 <a title="36-tfidf-19" href="./nips-2004-Breaking_SVM_Complexity_with_Cross-Training.html">34 nips-2004-Breaking SVM Complexity with Cross-Training</a></p>
<p>20 0.040221304 <a title="36-tfidf-20" href="./nips-2004-Maximum_Margin_Clustering.html">115 nips-2004-Maximum Margin Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.117), (1, 0.049), (2, 0.026), (3, 0.089), (4, 0.009), (5, 0.015), (6, 0.043), (7, 0.094), (8, 0.036), (9, 0.052), (10, 0.019), (11, 0.03), (12, 0.01), (13, 0.076), (14, -0.08), (15, -0.037), (16, 0.003), (17, 0.042), (18, -0.052), (19, -0.001), (20, 0.019), (21, -0.182), (22, -0.144), (23, -0.14), (24, 0.062), (25, 0.002), (26, 0.204), (27, 0.15), (28, 0.029), (29, -0.052), (30, 0.02), (31, 0.128), (32, -0.144), (33, 0.151), (34, -0.031), (35, 0.006), (36, 0.228), (37, 0.056), (38, 0.125), (39, 0.085), (40, 0.033), (41, 0.159), (42, 0.073), (43, 0.011), (44, -0.093), (45, 0.084), (46, -0.176), (47, -0.034), (48, -0.021), (49, -0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92951506 <a title="36-lsi-1" href="./nips-2004-Class-size_Independent_Generalization_Analsysis_of_Some_Discriminative_Multi-Category_Classification.html">36 nips-2004-Class-size Independent Generalization Analsysis of Some Discriminative Multi-Category Classification</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We consider the problem of deriving class-size independent generalization bounds for some regularized discriminative multi-category classiﬁcation methods. In particular, we obtain an expected generalization bound for a standard formulation of multi-category support vector machines. Based on the theoretical result, we argue that the formulation over-penalizes misclassiﬁcation error, which in theory may lead to poor generalization performance. A remedy, based on a generalization of multi-category logistic regression (conditional maximum entropy), is then proposed, and its theoretical properties are examined. 1</p><p>2 0.66322935 <a title="36-lsi-2" href="./nips-2004-Using_Random_Forests_in_the_Structured_Language_Model.html">200 nips-2004-Using Random Forests in the Structured Language Model</a></p>
<p>Author: Peng Xu, Frederick Jelinek</p><p>Abstract: In this paper, we explore the use of Random Forests (RFs) in the structured language model (SLM), which uses rich syntactic information in predicting the next word based on words already seen. The goal in this work is to construct RFs by randomly growing Decision Trees (DTs) using syntactic information and investigate the performance of the SLM modeled by the RFs in automatic speech recognition. RFs, which were originally developed as classiﬁers, are a combination of decision tree classiﬁers. Each tree is grown based on random training data sampled independently and with the same distribution for all trees in the forest, and a random selection of possible questions at each node of the decision tree. Our approach extends the original idea of RFs to deal with the data sparseness problem encountered in language modeling. RFs have been studied in the context of n-gram language modeling and have been shown to generalize well to unseen data. We show in this paper that RFs using syntactic information can also achieve better performance in both perplexity (PPL) and word error rate (WER) in a large vocabulary speech recognition system, compared to a baseline that uses Kneser-Ney smoothing. 1</p><p>3 0.42784894 <a title="36-lsi-3" href="./nips-2004-Nearly_Tight_Bounds_for_the_Continuum-Armed_Bandit_Problem.html">126 nips-2004-Nearly Tight Bounds for the Continuum-Armed Bandit Problem</a></p>
<p>Author: Robert D. Kleinberg</p><p>Abstract: In the multi-armed bandit problem, an online algorithm must choose from a set of strategies in a sequence of n trials so as to minimize the total cost of the chosen strategies. While nearly tight upper and lower bounds are known in the case when the strategy set is ﬁnite, much less is known when there is an inﬁnite strategy set. Here we consider the case when the set of strategies is a subset of Rd , and the cost functions are continuous. In the d = 1 case, we improve on the best-known upper and lower bounds, closing the gap to a sublogarithmic factor. We also consider the case where d > 1 and the cost functions are convex, adapting a recent online convex optimization algorithm of Zinkevich to the sparser feedback model of the multi-armed bandit problem. 1</p><p>4 0.41272607 <a title="36-lsi-4" href="./nips-2004-Learning_Preferences_for_Multiclass_Problems.html">100 nips-2004-Learning Preferences for Multiclass Problems</a></p>
<p>Author: Fabio Aiolli, Alessandro Sperduti</p><p>Abstract: Many interesting multiclass problems can be cast in the general framework of label ranking deﬁned on a given set of classes. The evaluation for such a ranking is generally given in terms of the number of violated order constraints between classes. In this paper, we propose the Preference Learning Model as a unifying framework to model and solve a large class of multiclass problems in a large margin perspective. In addition, an original kernel-based method is proposed and evaluated on a ranking dataset with state-of-the-art results. 1</p><p>5 0.36842775 <a title="36-lsi-5" href="./nips-2004-The_Variational_Ising_Classifier_%28VIC%29_Algorithm_for_Coherently_Contaminated_Data.html">191 nips-2004-The Variational Ising Classifier (VIC) Algorithm for Coherently Contaminated Data</a></p>
<p>Author: Oliver Williams, Andrew Blake, Roberto Cipolla</p><p>Abstract: There has been substantial progress in the past decade in the development of object classiﬁers for images, for example of faces, humans and vehicles. Here we address the problem of contaminations (e.g. occlusion, shadows) in test images which have not explicitly been encountered in training data. The Variational Ising Classiﬁer (VIC) algorithm models contamination as a mask (a ﬁeld of binary variables) with a strong spatial coherence prior. Variational inference is used to marginalize over contamination and obtain robust classiﬁcation. In this way the VIC approach can turn a kernel classiﬁer for clean data into one that can tolerate contamination, without any speciﬁc training on contaminated positives. 1</p><p>6 0.30989558 <a title="36-lsi-6" href="./nips-2004-A_Machine_Learning_Approach_to_Conjoint_Analysis.html">8 nips-2004-A Machine Learning Approach to Conjoint Analysis</a></p>
<p>7 0.28871265 <a title="36-lsi-7" href="./nips-2004-Who%27s_In_the_Picture.html">205 nips-2004-Who's In the Picture</a></p>
<p>8 0.28589237 <a title="36-lsi-8" href="./nips-2004-Learning%2C_Regularization_and_Ill-Posed_Inverse_Problems.html">96 nips-2004-Learning, Regularization and Ill-Posed Inverse Problems</a></p>
<p>9 0.26721036 <a title="36-lsi-9" href="./nips-2004-Variational_Minimax_Estimation_of_Discrete_Distributions_under_KL_Loss.html">204 nips-2004-Variational Minimax Estimation of Discrete Distributions under KL Loss</a></p>
<p>10 0.25948012 <a title="36-lsi-10" href="./nips-2004-Unsupervised_Variational_Bayesian_Learning_of_Nonlinear_Models.html">198 nips-2004-Unsupervised Variational Bayesian Learning of Nonlinear Models</a></p>
<p>11 0.24424893 <a title="36-lsi-11" href="./nips-2004-A_Large_Deviation_Bound_for_the_Area_Under_the_ROC_Curve.html">7 nips-2004-A Large Deviation Bound for the Area Under the ROC Curve</a></p>
<p>12 0.23688966 <a title="36-lsi-12" href="./nips-2004-Online_Bounds_for_Bayesian_Algorithms.html">138 nips-2004-Online Bounds for Bayesian Algorithms</a></p>
<p>13 0.22183546 <a title="36-lsi-13" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>14 0.22129172 <a title="36-lsi-14" href="./nips-2004-On_the_Adaptive_Properties_of_Decision_Trees.html">137 nips-2004-On the Adaptive Properties of Decision Trees</a></p>
<p>15 0.21734241 <a title="36-lsi-15" href="./nips-2004-Maximal_Margin_Labeling_for_Multi-Topic_Text_Categorization.html">111 nips-2004-Maximal Margin Labeling for Multi-Topic Text Categorization</a></p>
<p>16 0.21470927 <a title="36-lsi-16" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>17 0.21386009 <a title="36-lsi-17" href="./nips-2004-Efficient_Out-of-Sample_Extension_of_Dominant-Set_Clusters.html">61 nips-2004-Efficient Out-of-Sample Extension of Dominant-Set Clusters</a></p>
<p>18 0.20997655 <a title="36-lsi-18" href="./nips-2004-Hierarchical_Distributed_Representations_for_Statistical_Language_Modeling.html">78 nips-2004-Hierarchical Distributed Representations for Statistical Language Modeling</a></p>
<p>19 0.20682402 <a title="36-lsi-19" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>20 0.19781514 <a title="36-lsi-20" href="./nips-2004-Incremental_Algorithms_for_Hierarchical_Classification.html">82 nips-2004-Incremental Algorithms for Hierarchical Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.516), (15, 0.083), (26, 0.059), (31, 0.025), (33, 0.114), (39, 0.016), (50, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9827022 <a title="36-lda-1" href="./nips-2004-Instance-Based_Relevance_Feedback_for_Image_Retrieval.html">85 nips-2004-Instance-Based Relevance Feedback for Image Retrieval</a></p>
<p>Author: Giorgio Gia\-cin\-to, Fabio Roli</p><p>Abstract: High retrieval precision in content-based image retrieval can be attained by adopting relevance feedback mechanisms. These mechanisms require that the user judges the quality of the results of the query by marking all the retrieved images as being either relevant or not. Then, the search engine exploits this information to adapt the search to better meet user’s needs. At present, the vast majority of proposed relevance feedback mechanisms are formulated in terms of search model that has to be optimized. Such an optimization involves the modification of some search parameters so that the nearest neighbor of the query vector contains the largest number of relevant images. In this paper, a different approach to relevance feedback is proposed. After the user provides the first feedback, following retrievals are not based on knn search, but on the computation of a relevance score for each image of the database. This score is computed as a function of two distances, namely the distance from the nearest non-relevant image and the distance from the nearest relevant one. Images are then ranked according to this score and the top k images are displayed. Reported results on three image data sets show that the proposed mechanism outperforms other state-of-the-art relevance feedback mechanisms. 1 In t rod u ct i on A large number of content-based image retrieval (CBIR) systems rely on the vector representation of images in a multidimensional feature space representing low-level image characteristics, e.g., color, texture, shape, etc. [1]. Content-based queries are often expressed by visual examples in order to retrieve from the database the images that are “similar” to the examples. This kind of retrieval is often referred to as K nearest-neighbor retrieval. It is easy to see that the effectiveness of content-based image retrieval systems (CBIR) strongly depends on the choice of the set of visual features, on the choice of the “metric” used to model the user’s perception of image similarity, and on the choice of the image used to query the database [1]. Typically, if we allow different users to mark the images retrieved with a given query as relevant or non-relevant, different subsets of images will be marked as relevant. Accordingly, the need for mechanisms to adapt the CBIR system response based on some feedback from the user is widely recognized. It is interesting to note that while relevance feedback mechanisms have been first introduced in the information retrieval field [2], they are receiving more attention in the CBIR field (Huang). The vast majority of relevance feedback techniques proposed in the literature is based on modifying the values of the search parameters as to better represent the concept the user bears in mind. To this end, search parameters are computed as a function of the relevance values assigned by the user to all the images retrieved so far. As an example, relevance feedback is often formulated in terms of the modification of the query vector, and/or in terms of adaptive similarity metrics. [3]-[7]. Recently, pattern classification paradigms such as SVMs have been proposed [8]. Feedback is thus used to model the concept of relevant images and adjust the search consequently. Concept modeling may be difficult on account of the distribution of relevant images in the selected feature space. “Narrow domain” image databases allows extracting good features, so that images bearing similar concepts belong to compact clusters. On the other hand, “broad domain” databases, such as image collection used by graphic professionals, or those made up of images from the Internet, are more difficult to subdivide in cluster because of the high variability of concepts [1]. In these cases, it is worth extracting only low level, non-specialized features, and image retrieval is better formulated in terms of a search problem rather then concept modeling. The present paper aims at offering an original contribution in this direction. Rather then modeling the concept of “relevance” the user bears in mind, feedback is used to assign each image of the database a relevance score. Such a score depends only from two dissimilarities (distances) computed against the images already marked by the user: the dissimilarity from the set of relevant images, and the dissimilarity from the set of non-relevant images. Despite its computational simplicity, this mechanism allows outperforming state-of-the-art relevance feedback mechanisms both on “narrow domain” databases, and on “broad domain” databases. This paper is organized as follows. Section 2 illustrates the idea behind the proposed mechanism and provides the basic assumptions. Section 3 details the proposed relevance feedback mechanism. Results on three image data sets are presented in Section 4, where performances of other relevance feedback mechanisms are compared. Conclusions are drawn in Section 5. 2 In st an ce- b ased rel evan ce est i m at i on The proposed mechanism has been inspired by classification techniques based on the “nearest case” [9]-[10]. Nearest-case theory provided the mechanism to compute the dissimilarity of each image from the sets of relevant and non–relevant images. The ratio between the nearest relevant image and the nearest non-relevant image has been used to compute the degree of relevance of each image of the database [11]. The present section illustrates the rationale behind the use of the nearest-case paradigm. Let us assume that each image of the database has been represented by a number of low-level features, and that a (dis)similarity measure has been defined so that the proximity between pairs of images represents some kind of “conceptual” similarity. In other words, the chosen feature space and similarity metric is meaningful at least for a restricted number of users. A search in image databases is usually performed by retrieving the k most similar images with respect to a given query. The dimension of k is usually small, to avoid displaying a large number of images at a time. Typical values for k are between 10 and 20. However, as the “relevant” images that the user wishes to retrieve may not fit perfectly with the similarity metric designed for the search engine, the user may be interested in exploring other regions of the feature space. To this end, the user marks the subset of “relevant” images out of the k retrieved. Usually, such relevance feedback is used to perform a new k-nn search by modifying some search parameters, i.e., the position of the query point, the similarity metric, and other tuning parameters [1]-[7]. Recent works proposed the use of support vector machine to learn the distribution of relevant images [8]. These techniques require some assumption about the general form of the distribution of relevant images in the feature space. As it is difficult to make any assumption about such a distribution for broad domain databases, we propose to exploit the information about the relevance of the images retrieved so far in a nearest-neighbor fashion. Nearest-neighbor techniques, as used in statistical pattern recognition, case-based reasoning, or instance-based learning, are effective in all applications where it is difficult to produce a high-level generalization of a “class” of objects [9]-[10],[12][13]. Relevance learning in content base image retrieval may well fit into this definition, as it is difficult to provide a general model that can be adapted to represent different concepts of similarity. In addition, the number of available cases may be too small to estimate the optimal set of parameters for such a general model. On the other hand, it can be more effective to use each “relevant” image as well as each “non-relevant” image, as “cases” or “instances” against which the images of the database should be compared. Consequently, we assume that an image is as much as relevant as much as its dissimilarity from the nearest relevant image is small. Analogously, an image is as much as non-relevant as much as its dissimilarity from the nearest non-relevant image is small. 3 Rel evan ce S core Com p u t ati on According to previous section, each image of the database can be thus characterized by a “degree of relevance” and a “degree of non-relevance” according to the dissimilarities from the nearest relevant image, and from the nearest non-relevant image, respectively. However, it should be noted that these degrees should be treated differently because only “relevant” images represent a “concept” in the user’s mind, while “non-relevant” images may represent a number of other concepts different from user’s interest. In other words, while it is meaningful to treat the degree of relevance as a degree of membership to the class of relevant images, the same does not apply to the degree of non-relevance. For this reason, we propose to use the “degree of non-relevance” to weight the “degree of relevance”. Let us denote with R the subset of indexes j ∈ {1,...,k} related to the set of relevant images retrieved so far and the original query (that is relevant by default), and with NR the subset of indexes j ∈ (1,...,k} related to the set of non-relevant images retrieved so far. For each image I of the database, according to the nearest neighbor rule, let us compute the dissimilarity from the nearest image in R and the dissimilarity from the nearest image in NR. Let us denote these dissimilarities as dR(I) and dNR(I), respectively. The value of dR(I) can be clearly used to measure the degree of relevance of image I, assuming that small values of dR(I) are related to very relevant images. On the other hand, the hypothesis that image I is relevant to the user’s query can be supported by a high value of dNR(I). Accordingly, we defined the relevance score ! dR ( I ) $ relevance ( I ) = # 1 + dN ( I ) &</p><p>2 0.97847372 <a title="36-lda-2" href="./nips-2004-Bayesian_Regularization_and_Nonnegative_Deconvolution_for_Time_Delay_Estimation.html">27 nips-2004-Bayesian Regularization and Nonnegative Deconvolution for Time Delay Estimation</a></p>
<p>Author: Yuanqing Lin, Daniel D. Lee</p><p>Abstract: Bayesian Regularization and Nonnegative Deconvolution (BRAND) is proposed for estimating time delays of acoustic signals in reverberant environments. Sparsity of the nonnegative ﬁlter coefﬁcients is enforced using an L1 -norm regularization. A probabilistic generative model is used to simultaneously estimate the regularization parameters and ﬁlter coefﬁcients from the signal data. Iterative update rules are derived under a Bayesian framework using the Expectation-Maximization procedure. The resulting time delay estimation algorithm is demonstrated on noisy acoustic data.</p><p>3 0.97401196 <a title="36-lda-3" href="./nips-2004-Sub-Microwatt_Analog_VLSI_Support_Vector_Machine_for_Pattern_Classification_and_Sequence_Estimation.html">176 nips-2004-Sub-Microwatt Analog VLSI Support Vector Machine for Pattern Classification and Sequence Estimation</a></p>
<p>Author: Shantanu Chakrabartty, Gert Cauwenberghs</p><p>Abstract: An analog system-on-chip for kernel-based pattern classiﬁcation and sequence estimation is presented. State transition probabilities conditioned on input data are generated by an integrated support vector machine. Dot product based kernels and support vector coefﬁcients are implemented in analog programmable ﬂoating gate translinear circuits, and probabilities are propagated and normalized using sub-threshold current-mode circuits. A 14-input, 24-state, and 720-support vector forward decoding kernel machine is integrated on a 3mm×3mm chip in 0.5µm CMOS technology. Experiments with the processor trained for speaker veriﬁcation and phoneme sequence estimation demonstrate real-time recognition accuracy at par with ﬂoating-point software, at sub-microwatt power. 1</p><p>4 0.95180273 <a title="36-lda-4" href="./nips-2004-Online_Bounds_for_Bayesian_Algorithms.html">138 nips-2004-Online Bounds for Bayesian Algorithms</a></p>
<p>Author: Sham M. Kakade, Andrew Y. Ng</p><p>Abstract: We present a competitive analysis of Bayesian learning algorithms in the online learning setting and show that many simple Bayesian algorithms (such as Gaussian linear regression and Bayesian logistic regression) perform favorably when compared, in retrospect, to the single best model in the model class. The analysis does not assume that the Bayesian algorithms’ modeling assumptions are “correct,” and our bounds hold even if the data is adversarially chosen. For Gaussian linear regression (using logloss), our error bounds are comparable to the best bounds in the online learning literature, and we also provide a lower bound showing that Gaussian linear regression is optimal in a certain worst case sense. We also give bounds for some widely used maximum a posteriori (MAP) estimation algorithms, including regularized logistic regression. 1</p><p>5 0.94327742 <a title="36-lda-5" href="./nips-2004-Maximum_Likelihood_Estimation_of_Intrinsic_Dimension.html">114 nips-2004-Maximum Likelihood Estimation of Intrinsic Dimension</a></p>
<p>Author: Elizaveta Levina, Peter J. Bickel</p><p>Abstract: We propose a new method for estimating intrinsic dimension of a dataset derived by applying the principle of maximum likelihood to the distances between close neighbors. We derive the estimator by a Poisson process approximation, assess its bias and variance theoretically and by simulations, and apply it to a number of simulated and real datasets. We also show it has the best overall performance compared with two other intrinsic dimension estimators. 1</p><p>same-paper 6 0.9098537 <a title="36-lda-6" href="./nips-2004-Class-size_Independent_Generalization_Analsysis_of_Some_Discriminative_Multi-Category_Classification.html">36 nips-2004-Class-size Independent Generalization Analsysis of Some Discriminative Multi-Category Classification</a></p>
<p>7 0.86168706 <a title="36-lda-7" href="./nips-2004-A_Harmonic_Excitation_State-Space_Approach_to_Blind_Separation_of_Speech.html">5 nips-2004-A Harmonic Excitation State-Space Approach to Blind Separation of Speech</a></p>
<p>8 0.74394834 <a title="36-lda-8" href="./nips-2004-Synergies_between_Intrinsic_and_Synaptic_Plasticity_in_Individual_Model_Neurons.html">181 nips-2004-Synergies between Intrinsic and Synaptic Plasticity in Individual Model Neurons</a></p>
<p>9 0.68636107 <a title="36-lda-9" href="./nips-2004-Bayesian_inference_in_spiking_neurons.html">28 nips-2004-Bayesian inference in spiking neurons</a></p>
<p>10 0.68160909 <a title="36-lda-10" href="./nips-2004-On-Chip_Compensation_of_Device-Mismatch_Effects_in_Analog_VLSI_Neural_Networks.html">135 nips-2004-On-Chip Compensation of Device-Mismatch Effects in Analog VLSI Neural Networks</a></p>
<p>11 0.6711576 <a title="36-lda-11" href="./nips-2004-Edge_of_Chaos_Computation_in_Mixed-Mode_VLSI_-_A_Hard_Liquid.html">58 nips-2004-Edge of Chaos Computation in Mixed-Mode VLSI - A Hard Liquid</a></p>
<p>12 0.66824919 <a title="36-lda-12" href="./nips-2004-An_Investigation_of_Practical_Approximate_Nearest_Neighbor_Algorithms.html">22 nips-2004-An Investigation of Practical Approximate Nearest Neighbor Algorithms</a></p>
<p>13 0.66759551 <a title="36-lda-13" href="./nips-2004-Non-Local_Manifold_Tangent_Learning.html">131 nips-2004-Non-Local Manifold Tangent Learning</a></p>
<p>14 0.66714275 <a title="36-lda-14" href="./nips-2004-Semi-parametric_Exponential_Family_PCA.html">163 nips-2004-Semi-parametric Exponential Family PCA</a></p>
<p>15 0.66442972 <a title="36-lda-15" href="./nips-2004-Message_Errors_in_Belief_Propagation.html">116 nips-2004-Message Errors in Belief Propagation</a></p>
<p>16 0.663068 <a title="36-lda-16" href="./nips-2004-Learning_first-order_Markov_models_for_control.html">102 nips-2004-Learning first-order Markov models for control</a></p>
<p>17 0.66175497 <a title="36-lda-17" href="./nips-2004-Variational_Minimax_Estimation_of_Discrete_Distributions_under_KL_Loss.html">204 nips-2004-Variational Minimax Estimation of Discrete Distributions under KL Loss</a></p>
<p>18 0.65925664 <a title="36-lda-18" href="./nips-2004-Heuristics_for_Ordering_Cue_Search_in_Decision_Making.html">75 nips-2004-Heuristics for Ordering Cue Search in Decision Making</a></p>
<p>19 0.65848619 <a title="36-lda-19" href="./nips-2004-Mistake_Bounds_for_Maximum_Entropy_Discrimination.html">119 nips-2004-Mistake Bounds for Maximum Entropy Discrimination</a></p>
<p>20 0.65329003 <a title="36-lda-20" href="./nips-2004-Following_Curved_Regularized_Optimization_Solution_Paths.html">70 nips-2004-Following Curved Regularized Optimization Solution Paths</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
