<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>154 nips-2004-Resolving Perceptual Aliasing In The Presence Of Noisy Sensors</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-154" href="#">nips2004-154</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>154 nips-2004-Resolving Perceptual Aliasing In The Presence Of Noisy Sensors</h1>
<br/><p>Source: <a title="nips-2004-154-pdf" href="http://papers.nips.cc/paper/2723-resolving-perceptual-aliasing-in-the-presence-of-noisy-sensors.pdf">pdf</a></p><p>Author: Guy Shani, Ronen I. Brafman</p><p>Abstract: Agents learning to act in a partially observable domain may need to overcome the problem of perceptual aliasing Ă˘&euro;&ldquo; i.e., different states that appear similar but require different responses. This problem is exacerbated when the agentĂ˘&euro;&trade;s sensors are noisy, i.e., sensors may produce different observations in the same state. We show that many well-known reinforcement learning methods designed to deal with perceptual aliasing, such as Utile SufÄ?Ĺš x Memory, Ä?Ĺš nite size history windows, eligibility traces, and memory bits, do not handle noisy sensors well. We suggest a new algorithm, Noisy Utile SufÄ?Ĺš x Memory (NUSM), based on USM, that uses a weighted classiÄ?Ĺš cation of observed trajectories. We compare NUSM to the above methods and show it to be more robust to noise.</p><p>Reference: <a title="nips-2004-154-reference" href="../nips2004_reference/nips-2004-Resolving_Perceptual_Aliasing_In_The_Presence_Of_Noisy_Sensors_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('usm', 0.463), ('nusm', 0.424), ('alias', 0.26), ('fring', 0.231), ('sars', 0.218), ('maz', 0.193), ('ag', 0.186), ('perceiv', 0.165), ('tt', 0.154), ('mem', 0.153), ('ti', 0.144), ('leaf', 0.141), ('hist', 0.135), ('reward', 0.115), ('policy', 0.113), ('window', 0.11), ('noisy', 0.096), ('mccallum', 0.095), ('nod', 0.094), ('sens', 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="154-tfidf-1" href="./nips-2004-Resolving_Perceptual_Aliasing_In_The_Presence_Of_Noisy_Sensors.html">154 nips-2004-Resolving Perceptual Aliasing In The Presence Of Noisy Sensors</a></p>
<p>Author: Guy Shani, Ronen I. Brafman</p><p>Abstract: Agents learning to act in a partially observable domain may need to overcome the problem of perceptual aliasing Ă˘&euro;&ldquo; i.e., different states that appear similar but require different responses. This problem is exacerbated when the agentĂ˘&euro;&trade;s sensors are noisy, i.e., sensors may produce different observations in the same state. We show that many well-known reinforcement learning methods designed to deal with perceptual aliasing, such as Utile SufÄ?Ĺš x Memory, Ä?Ĺš nite size history windows, eligibility traces, and memory bits, do not handle noisy sensors well. We suggest a new algorithm, Noisy Utile SufÄ?Ĺš x Memory (NUSM), based on USM, that uses a weighted classiÄ?Ĺš cation of observed trajectories. We compare NUSM to the above methods and show it to be more robust to noise.</p><p>2 0.15856159 <a title="154-tfidf-2" href="./nips-2004-Multi-agent_Cooperation_in_Diverse_Population_Games.html">123 nips-2004-Multi-agent Cooperation in Diverse Population Games</a></p>
<p>Author: K. Wong, S. W. Lim, Z. Gao</p><p>Abstract: We consider multi-agent systems whose agents compete for resources by striving to be in the minority group. The agents adapt to the environment by reinforcement learning of the preferences of the policies they hold. Diversity of preferences of policies is introduced by adding random biases to the initial cumulative payoffs of their policies. We explain and provide evidence that agent cooperation becomes increasingly important when diversity increases. Analyses of these mechanisms yield excellent agreement with simulations over nine decades of data. 1</p><p>3 0.15549761 <a title="154-tfidf-3" href="./nips-2004-Experts_in_a_Markov_Decision_Process.html">64 nips-2004-Experts in a Markov Decision Process</a></p>
<p>Author: Eyal Even-dar, Sham M. Kakade, Yishay Mansour</p><p>Abstract: We consider an MDP setting in which the reward function is allowed to change during each time step of play (possibly in an adversarial manner), yet the dynamics remain ﬁxed. Similar to the experts setting, we address the question of how well can an agent do when compared to the reward achieved under the best stationary policy over time. We provide efﬁcient algorithms, which have regret bounds with no dependence on the size of state space. Instead, these bounds depend only on a certain horizon time of the process and logarithmically on the number of actions. We also show that in the case that the dynamics change over time, the problem becomes computationally hard. 1</p><p>4 0.15437943 <a title="154-tfidf-4" href="./nips-2004-Approximately_Efficient_Online_Mechanism_Design.html">24 nips-2004-Approximately Efficient Online Mechanism Design</a></p>
<p>Author: David C. Parkes, Dimah Yanovsky, Satinder P. Singh</p><p>Abstract: Online mechanism design (OMD) addresses the problem of sequential decision making in a stochastic environment with multiple self-interested agents. The goal in OMD is to make value-maximizing decisions despite this self-interest. In previous work we presented a Markov decision process (MDP)-based approach to OMD in large-scale problem domains. In practice the underlying MDP needed to solve OMD is too large and hence the mechanism must consider approximations. This raises the possibility that agents may be able to exploit the approximation for selﬁsh gain. We adopt sparse-sampling-based MDP algorithms to implement efﬁcient policies, and retain truth-revelation as an approximate BayesianNash equilibrium. Our approach is empirically illustrated in the context of the dynamic allocation of WiFi connectivity to users in a coffeehouse. 1</p><p>5 0.14668709 <a title="154-tfidf-5" href="./nips-2004-Intrinsically_Motivated_Reinforcement_Learning.html">88 nips-2004-Intrinsically Motivated Reinforcement Learning</a></p>
<p>Author: Nuttapong Chentanez, Andrew G. Barto, Satinder P. Singh</p><p>Abstract: Psychologists call behavior intrinsically motivated when it is engaged in for its own sake rather than as a step toward solving a speciﬁc problem of clear practical value. But what we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efﬁciently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated reinforcement learning aimed at allowing artiﬁcial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy. 1</p><p>6 0.1194865 <a title="154-tfidf-6" href="./nips-2004-Log-concavity_Results_on_Gaussian_Process_Methods_for_Supervised_and_Unsupervised_Learning.html">105 nips-2004-Log-concavity Results on Gaussian Process Methods for Supervised and Unsupervised Learning</a></p>
<p>7 0.11711337 <a title="154-tfidf-7" href="./nips-2004-VDCBPI%3A_an_Approximate_Scalable_Algorithm_for_Large_POMDPs.html">202 nips-2004-VDCBPI: an Approximate Scalable Algorithm for Large POMDPs</a></p>
<p>8 0.094859034 <a title="154-tfidf-8" href="./nips-2004-Coarticulation_in_Markov_Decision_Processes.html">39 nips-2004-Coarticulation in Markov Decision Processes</a></p>
<p>9 0.093934506 <a title="154-tfidf-9" href="./nips-2004-Responding_to_Modalities_with_Different_Latencies.html">155 nips-2004-Responding to Modalities with Different Latencies</a></p>
<p>10 0.072076924 <a title="154-tfidf-10" href="./nips-2004-Temporal-Difference_Networks.html">183 nips-2004-Temporal-Difference Networks</a></p>
<p>11 0.069374844 <a title="154-tfidf-11" href="./nips-2004-Exploration-Exploitation_Tradeoffs_for_Experts_Algorithms_in_Reactive_Environments.html">65 nips-2004-Exploration-Exploitation Tradeoffs for Experts Algorithms in Reactive Environments</a></p>
<p>12 0.06537196 <a title="154-tfidf-12" href="./nips-2004-Constraining_a_Bayesian_Model_of_Human_Visual_Speed_Perception.html">46 nips-2004-Constraining a Bayesian Model of Human Visual Speed Perception</a></p>
<p>13 0.064864241 <a title="154-tfidf-13" href="./nips-2004-Brain_Inspired_Reinforcement_Learning.html">33 nips-2004-Brain Inspired Reinforcement Learning</a></p>
<p>14 0.062348317 <a title="154-tfidf-14" href="./nips-2004-Incremental_Algorithms_for_Hierarchical_Classification.html">82 nips-2004-Incremental Algorithms for Hierarchical Classification</a></p>
<p>15 0.061927814 <a title="154-tfidf-15" href="./nips-2004-Adaptive_Discriminative_Generative_Model_and_Its_Applications.html">16 nips-2004-Adaptive Discriminative Generative Model and Its Applications</a></p>
<p>16 0.061310466 <a title="154-tfidf-16" href="./nips-2004-Probabilistic_Computation_in_Spiking_Populations.html">148 nips-2004-Probabilistic Computation in Spiking Populations</a></p>
<p>17 0.059197761 <a title="154-tfidf-17" href="./nips-2004-An_Investigation_of_Practical_Approximate_Nearest_Neighbor_Algorithms.html">22 nips-2004-An Investigation of Practical Approximate Nearest Neighbor Algorithms</a></p>
<p>18 0.056156572 <a title="154-tfidf-18" href="./nips-2004-Convergence_and_No-Regret_in_Multiagent_Learning.html">48 nips-2004-Convergence and No-Regret in Multiagent Learning</a></p>
<p>19 0.055280499 <a title="154-tfidf-19" href="./nips-2004-New_Criteria_and_a_New_Algorithm_for_Learning_in_Multi-Agent_Systems.html">129 nips-2004-New Criteria and a New Algorithm for Learning in Multi-Agent Systems</a></p>
<p>20 0.055038981 <a title="154-tfidf-20" href="./nips-2004-Efficient_Out-of-Sample_Extension_of_Dominant-Set_Clusters.html">61 nips-2004-Efficient Out-of-Sample Extension of Dominant-Set Clusters</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.142), (1, -0.006), (2, -0.255), (3, -0.029), (4, 0.17), (5, 0.046), (6, 0.017), (7, 0.004), (8, -0.005), (9, 0.011), (10, 0.029), (11, -0.028), (12, 0.069), (13, 0.014), (14, -0.014), (15, -0.023), (16, 0.003), (17, 0.03), (18, 0.038), (19, -0.068), (20, -0.055), (21, 0.032), (22, -0.011), (23, 0.03), (24, 0.026), (25, 0.046), (26, 0.047), (27, 0.14), (28, -0.073), (29, 0.007), (30, -0.055), (31, -0.088), (32, 0.06), (33, -0.013), (34, 0.022), (35, -0.135), (36, -0.005), (37, 0.099), (38, -0.155), (39, 0.096), (40, 0.002), (41, 0.109), (42, -0.01), (43, -0.046), (44, -0.023), (45, -0.16), (46, 0.128), (47, -0.09), (48, -0.154), (49, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94170547 <a title="154-lsi-1" href="./nips-2004-Resolving_Perceptual_Aliasing_In_The_Presence_Of_Noisy_Sensors.html">154 nips-2004-Resolving Perceptual Aliasing In The Presence Of Noisy Sensors</a></p>
<p>Author: Guy Shani, Ronen I. Brafman</p><p>Abstract: Agents learning to act in a partially observable domain may need to overcome the problem of perceptual aliasing Ă˘&euro;&ldquo; i.e., different states that appear similar but require different responses. This problem is exacerbated when the agentĂ˘&euro;&trade;s sensors are noisy, i.e., sensors may produce different observations in the same state. We show that many well-known reinforcement learning methods designed to deal with perceptual aliasing, such as Utile SufÄ?Ĺš x Memory, Ä?Ĺš nite size history windows, eligibility traces, and memory bits, do not handle noisy sensors well. We suggest a new algorithm, Noisy Utile SufÄ?Ĺš x Memory (NUSM), based on USM, that uses a weighted classiÄ?Ĺš cation of observed trajectories. We compare NUSM to the above methods and show it to be more robust to noise.</p><p>2 0.63703007 <a title="154-lsi-2" href="./nips-2004-Approximately_Efficient_Online_Mechanism_Design.html">24 nips-2004-Approximately Efficient Online Mechanism Design</a></p>
<p>Author: David C. Parkes, Dimah Yanovsky, Satinder P. Singh</p><p>Abstract: Online mechanism design (OMD) addresses the problem of sequential decision making in a stochastic environment with multiple self-interested agents. The goal in OMD is to make value-maximizing decisions despite this self-interest. In previous work we presented a Markov decision process (MDP)-based approach to OMD in large-scale problem domains. In practice the underlying MDP needed to solve OMD is too large and hence the mechanism must consider approximations. This raises the possibility that agents may be able to exploit the approximation for selﬁsh gain. We adopt sparse-sampling-based MDP algorithms to implement efﬁcient policies, and retain truth-revelation as an approximate BayesianNash equilibrium. Our approach is empirically illustrated in the context of the dynamic allocation of WiFi connectivity to users in a coffeehouse. 1</p><p>3 0.59007019 <a title="154-lsi-3" href="./nips-2004-Multi-agent_Cooperation_in_Diverse_Population_Games.html">123 nips-2004-Multi-agent Cooperation in Diverse Population Games</a></p>
<p>Author: K. Wong, S. W. Lim, Z. Gao</p><p>Abstract: We consider multi-agent systems whose agents compete for resources by striving to be in the minority group. The agents adapt to the environment by reinforcement learning of the preferences of the policies they hold. Diversity of preferences of policies is introduced by adding random biases to the initial cumulative payoffs of their policies. We explain and provide evidence that agent cooperation becomes increasingly important when diversity increases. Analyses of these mechanisms yield excellent agreement with simulations over nine decades of data. 1</p><p>4 0.53920186 <a title="154-lsi-4" href="./nips-2004-Schema_Learning%3A_Experience-Based_Construction_of_Predictive_Action_Models.html">159 nips-2004-Schema Learning: Experience-Based Construction of Predictive Action Models</a></p>
<p>Author: Michael P. Holmes, Charles Jr.</p><p>Abstract: Schema learning is a way to discover probabilistic, constructivist, predictive action models (schemas) from experience. It includes methods for ﬁnding and using hidden state to make predictions more accurate. We extend the original schema mechanism [1] to handle arbitrary discrete-valued sensors, improve the original learning criteria to handle POMDP domains, and better maintain hidden state by using schema predictions. These extensions show large improvement over the original schema mechanism in several rewardless POMDPs, and achieve very low prediction error in a difﬁcult speech modeling task. Further, we compare extended schema learning to the recently introduced predictive state representations [2], and ﬁnd their predictions of next-step action effects to be approximately equal in accuracy. This work lays the foundation for a schema-based system of integrated learning and planning. 1</p><p>5 0.52524745 <a title="154-lsi-5" href="./nips-2004-Responding_to_Modalities_with_Different_Latencies.html">155 nips-2004-Responding to Modalities with Different Latencies</a></p>
<p>Author: Fredrik Bissmarck, Hiroyuki Nakahara, Kenji Doya, Okihide Hikosaka</p><p>Abstract: Motor control depends on sensory feedback in multiple modalities with different latencies. In this paper we consider within the framework of reinforcement learning how different sensory modalities can be combined and selected for real-time, optimal movement control. We propose an actor-critic architecture with multiple modules, whose output are combined using a softmax function. We tested our architecture in a simulation of a sequential reaching task. Reaching was initially guided by visual feedback with a long latency. Our learning scheme allowed the agent to utilize the somatosensory feedback with shorter latency when the hand is near the experienced trajectory. In simulations with different latencies for visual and somatosensory feedback, we found that the agent depended more on feedback with shorter latency. 1</p><p>6 0.48182178 <a title="154-lsi-6" href="./nips-2004-Coarticulation_in_Markov_Decision_Processes.html">39 nips-2004-Coarticulation in Markov Decision Processes</a></p>
<p>7 0.47789451 <a title="154-lsi-7" href="./nips-2004-Intrinsically_Motivated_Reinforcement_Learning.html">88 nips-2004-Intrinsically Motivated Reinforcement Learning</a></p>
<p>8 0.44446707 <a title="154-lsi-8" href="./nips-2004-Experts_in_a_Markov_Decision_Process.html">64 nips-2004-Experts in a Markov Decision Process</a></p>
<p>9 0.4330582 <a title="154-lsi-9" href="./nips-2004-VDCBPI%3A_an_Approximate_Scalable_Algorithm_for_Large_POMDPs.html">202 nips-2004-VDCBPI: an Approximate Scalable Algorithm for Large POMDPs</a></p>
<p>10 0.4159278 <a title="154-lsi-10" href="./nips-2004-Newscast_EM.html">130 nips-2004-Newscast EM</a></p>
<p>11 0.39041558 <a title="154-lsi-11" href="./nips-2004-Log-concavity_Results_on_Gaussian_Process_Methods_for_Supervised_and_Unsupervised_Learning.html">105 nips-2004-Log-concavity Results on Gaussian Process Methods for Supervised and Unsupervised Learning</a></p>
<p>12 0.37721255 <a title="154-lsi-12" href="./nips-2004-Temporal-Difference_Networks.html">183 nips-2004-Temporal-Difference Networks</a></p>
<p>13 0.34027395 <a title="154-lsi-13" href="./nips-2004-Learning_first-order_Markov_models_for_control.html">102 nips-2004-Learning first-order Markov models for control</a></p>
<p>14 0.30004376 <a title="154-lsi-14" href="./nips-2004-Constraining_a_Bayesian_Model_of_Human_Visual_Speed_Perception.html">46 nips-2004-Constraining a Bayesian Model of Human Visual Speed Perception</a></p>
<p>15 0.2901544 <a title="154-lsi-15" href="./nips-2004-Theories_of_Access_Consciousness.html">193 nips-2004-Theories of Access Consciousness</a></p>
<p>16 0.28721708 <a title="154-lsi-16" href="./nips-2004-Brain_Inspired_Reinforcement_Learning.html">33 nips-2004-Brain Inspired Reinforcement Learning</a></p>
<p>17 0.28688988 <a title="154-lsi-17" href="./nips-2004-Instance-Specific_Bayesian_Model_Averaging_for_Classification.html">86 nips-2004-Instance-Specific Bayesian Model Averaging for Classification</a></p>
<p>18 0.28334072 <a title="154-lsi-18" href="./nips-2004-An_Investigation_of_Practical_Approximate_Nearest_Neighbor_Algorithms.html">22 nips-2004-An Investigation of Practical Approximate Nearest Neighbor Algorithms</a></p>
<p>19 0.2824038 <a title="154-lsi-19" href="./nips-2004-Chemosensory_Processing_in_a_Spiking_Model_of_the_Olfactory_Bulb%3A_Chemotopic_Convergence_and_Center_Surround_Inhibition.html">35 nips-2004-Chemosensory Processing in a Spiking Model of the Olfactory Bulb: Chemotopic Convergence and Center Surround Inhibition</a></p>
<p>20 0.27877656 <a title="154-lsi-20" href="./nips-2004-On_the_Adaptive_Properties_of_Decision_Trees.html">137 nips-2004-On the Adaptive Properties of Decision Trees</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.012), (8, 0.011), (15, 0.129), (26, 0.013), (27, 0.05), (37, 0.105), (41, 0.012), (71, 0.283), (74, 0.041), (77, 0.107), (81, 0.044), (96, 0.096)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77853757 <a title="154-lda-1" href="./nips-2004-Resolving_Perceptual_Aliasing_In_The_Presence_Of_Noisy_Sensors.html">154 nips-2004-Resolving Perceptual Aliasing In The Presence Of Noisy Sensors</a></p>
<p>Author: Guy Shani, Ronen I. Brafman</p><p>Abstract: Agents learning to act in a partially observable domain may need to overcome the problem of perceptual aliasing Ă˘&euro;&ldquo; i.e., different states that appear similar but require different responses. This problem is exacerbated when the agentĂ˘&euro;&trade;s sensors are noisy, i.e., sensors may produce different observations in the same state. We show that many well-known reinforcement learning methods designed to deal with perceptual aliasing, such as Utile SufÄ?Ĺš x Memory, Ä?Ĺš nite size history windows, eligibility traces, and memory bits, do not handle noisy sensors well. We suggest a new algorithm, Noisy Utile SufÄ?Ĺš x Memory (NUSM), based on USM, that uses a weighted classiÄ?Ĺš cation of observed trajectories. We compare NUSM to the above methods and show it to be more robust to noise.</p><p>2 0.63312846 <a title="154-lda-2" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>Author: Aharon Bar-hillel, Adam Spiro, Eran Stark</p><p>Abstract: Spike sorting involves clustering spike trains recorded by a microelectrode according to the source neuron. It is a complicated problem, which requires a lot of human labor, partly due to the non-stationary nature of the data. We propose an automated technique for the clustering of non-stationary Gaussian sources in a Bayesian framework. At a ﬁrst search stage, data is divided into short time frames and candidate descriptions of the data as a mixture of Gaussians are computed for each frame. At a second stage transition probabilities between candidate mixtures are computed, and a globally optimal clustering is found as the MAP solution of the resulting probabilistic model. Transition probabilities are computed using local stationarity assumptions and are based on a Gaussian version of the Jensen-Shannon divergence. The method was applied to several recordings. The performance appeared almost indistinguishable from humans in a wide range of scenarios, including movement, merges, and splits of clusters. 1</p><p>3 0.60134161 <a title="154-lda-3" href="./nips-2004-Analysis_of_a_greedy_active_learning_strategy.html">23 nips-2004-Analysis of a greedy active learning strategy</a></p>
<p>Author: Sanjoy Dasgupta</p><p>Abstract: We abstract out the core search problem of active learning schemes, to better understand the extent to which adaptive labeling can improve sample complexity. We give various upper and lower bounds on the number of labels which need to be queried, and we prove that a popular greedy active learning rule is approximately as good as any other strategy for minimizing this number of labels. 1</p><p>4 0.59988546 <a title="154-lda-4" href="./nips-2004-Markov_Networks_for_Detecting_Overalpping_Elements_in_Sequence_Data.html">108 nips-2004-Markov Networks for Detecting Overalpping Elements in Sequence Data</a></p>
<p>Author: Mark Craven, Joseph Bockhorst</p><p>Abstract: Many sequential prediction tasks involve locating instances of patterns in sequences. Generative probabilistic language models, such as hidden Markov models (HMMs), have been successfully applied to many of these tasks. A limitation of these models however, is that they cannot naturally handle cases in which pattern instances overlap in arbitrary ways. We present an alternative approach, based on conditional Markov networks, that can naturally represent arbitrarily overlapping elements. We show how to eﬃciently train and perform inference with these models. Experimental results from a genomics domain show that our models are more accurate at locating instances of overlapping patterns than are baseline models based on HMMs. 1</p><p>5 0.59801549 <a title="154-lda-5" href="./nips-2004-Instance-Specific_Bayesian_Model_Averaging_for_Classification.html">86 nips-2004-Instance-Specific Bayesian Model Averaging for Classification</a></p>
<p>Author: Shyam Visweswaran, Gregory F. Cooper</p><p>Abstract: Classification algorithms typically induce population-wide models that are trained to perform well on average on expected future instances. We introduce a Bayesian framework for learning instance-specific models from data that are optimized to predict well for a particular instance. Based on this framework, we present a lazy instance-specific algorithm called ISA that performs selective model averaging over a restricted class of Bayesian networks. On experimental evaluation, this algorithm shows superior performance over model selection. We intend to apply such instance-specific algorithms to improve the performance of patient-specific predictive models induced from medical data. 1 In t ro d u c t i o n Commonly used classification algorithms, such as neural networks, decision trees, Bayesian networks and support vector machines, typically induce a single model from a training set of instances, with the intent of applying it to all future instances. We call such a model a population-wide model because it is intended to be applied to an entire population of future instances. A population-wide model is optimized to predict well on average when applied to expected future instances. In contrast, an instance-specific model is one that is constructed specifically for a particular instance. The structure and parameters of an instance-specific model are specialized to the particular features of an instance, so that it is optimized to predict especially well for that instance. Usually, methods that induce population-wide models employ eager learning in which the model is induced from the training data before the test instance is encountered. In contrast, lazy learning defers most or all processing until a response to a test instance is required. Learners that induce instance-specific models are necessarily lazy in nature since they take advantage of the information in the test instance. An example of a lazy instance-specific method is the lazy Bayesian rule (LBR) learner, implemented by Zheng and Webb [1], which induces rules in a lazy fashion from examples in the neighborhood of the test instance. A rule generated by LBR consists of a conjunction of the attribute-value pairs present in the test instance as the antecedent and a local simple (naïve) Bayes classifier as the consequent. The structure of the local simple Bayes classifier consists of the attribute of interest as the parent of all other attributes that do not appear in the antecedent, and the parameters of the classifier are estimated from the subset of training instances that satisfy the antecedent. A greedy step-forward search selects the optimal LBR rule for a test instance to be classified. When evaluated on 29 UCI datasets, LBR had the lowest average error rate when compared to several eager learning methods [1]. Typically, both eager and lazy algorithms select a single model from some model space, ignoring the uncertainty in model selection. Bayesian model averaging is a coherent approach to dealing with the uncertainty in model selection, and it has been shown to improve the predictive performance of classifiers [2]. However, since the number of models in practically useful model spaces is enormous, exact model averaging over the entire model space is usually not feasible. In this paper, we describe a lazy instance-specific averaging (ISA) algorithm for classification that approximates Bayesian model averaging in an instance-sensitive manner. ISA extends LBR by adding Bayesian model averaging to an instance-specific model selection algorithm. While the ISA algorithm is currently able to directly handle only discrete variables and is computationally more intensive than comparable eager algorithms, the results in this paper show that it performs well. In medicine, such lazy instance-specific algorithms can be applied to patient-specific modeling for improving the accuracy of diagnosis, prognosis and risk assessment. The rest of this paper is structured as follows. Section 2 introduces a Bayesian framework for instance-specific learning. Section 3 describes the implementation of ISA. In Section 4, we evaluate ISA and compare its performance to that of LBR. Finally, in Section 5 we discuss the results of the comparison. 2 Deci si on Th eo ret i c F rame wo rk We use the following notation. Capital letters like X, Z, denote random variables and corresponding lower case letters, x, z, denote specific values assigned to them. Thus, X = x denotes that variable X is assigned the value x. Bold upper case letters, such as X, Z, represent sets of variables or random vectors and their realization is denoted by the corresponding bold lower case letters, x, z. Hence, X = x denotes that the variables in X have the states given by x. In addition, Z denotes the target variable being predicted, X denotes the set of attribute variables, M denotes a model, D denotes the training dataset, and   denotes a generic test instance that is not in D. We now characterize population-wide and instance-specific model selection in decision theoretic terms. Given training data D and a separate generic test instance , the Bayes optimal prediction for Zt is obtained by combining the predictions of all models weighted by their posterior probabilities, as follows: P (Z t | X t , D ) = ∫ P( Z t | X t , M ) P ( M | D )dM . (1) M The optimal population-wide model for predicting Zt is as follows:   max∑ U P( Z t | X t , D), P (Z t | X t , M ) P ( X | D) , M  Xt  [ ] (2) where the function U gives the utility of approximating the Bayes optimal estimate P(Zt | Xt , D), with the estimate P(Zt | Xt , M) obtained from model M. The term P(X | D) is given by: P ( X | D) = ∫ P ( X | M ) P ( M | D)dM . (3) M The optimal instance-specific model for predicting Zt is as follows: { [ ]} max U P ( Z t | X t = x t , D), P (Z t | X t = x t , M ) , M (4) where xt are the values of the attributes of the test instance Xt for which we want to predict Zt. The Bayes optimal estimate P(Zt | Xt = xt, D), in Equation 4 is derived using Equation 1, for the special case in which Xt = xt . The difference between the population-wide and the instance-specific models can be noted by comparing Equations 2 and 4. Equation 2 for the population-wide model selects the model that on average will have the greatest utility. Equation 4 for the instance-specific model, however, selects the model that will have the greatest expected utility for the specific instance Xt = xt . For predicting Zt in a given instance Xt = xt, the model selected using Equation 2 can never have an expected utility greater than the model selected using Equation 4. This observation provides support for developing instance-specific models. Equations 2 and 4 represent theoretical ideals for population-wide and instancespecific model selection, respectively; we are not suggesting they are practical to compute. The current paper focuses on model averaging, rather than model selection. Ideal Bayesian model averaging is given by Equation 1. Model averaging has previously been applied using population-wide models. Studies have shown that approximate Bayesian model averaging using population-wide models can improve predictive performance over population-wide model selection [2]. The current paper concentrates on investigating the predictive performance of approximate Bayesian model averaging using instance-specific models. 3 In st an ce- S p eci fi c Algo ri t h m We present the implementation of the lazy instance-specific algorithm based on the above framework. ISA searches the space of a restricted class of Bayesian networks to select a subset of the models over which to derive a weighted (averaged) posterior of the target variable Zt . A key characteristic of the search is the use of a heuristic to select models that will have a significant influence on the weighted posterior. We introduce Bayesian networks briefly and then describe ISA in detail. 3.1 B ay e s i a n N e t w or k s A Bayesian network is a probabilistic model that combines a graphical representation (the Bayesian network structure) with quantitative information (the parameters of the Bayesian network) to represent the joint probability distribution over a set of random variables [3]. Specifically, a Bayesian network M representing the set of variables X consists of a pair (G, ΘG ). G is a directed acyclic graph that contains a node for every variable in X and an arc between every pair of nodes if the corresponding variables are directly probabilistically dependent. Conversely, the absence of an arc between a pair of nodes denotes probabilistic independence between the corresponding variables. ΘG represents the parameterization of the model. In a Bayesian network M, the immediate predecessors of a node X i in X are called the parents of X i and the successors, both immediate and remote, of Xi in X are called the descendants of X i . The immediate successors of X i are called the children of X i . For each node Xi there is a local probability distribution (that may be discrete or continuous) on that node given the state of its parents. The complete joint probability distribution over X, represented by the parameterization ΘG, can be factored into a product of local probability distributions defined on each node in the network. This factorization is determined by the independences captured by the structure of the Bayesian network and is formalized in the Bayesian network Markov condition: A node (representing a variable) is independent of its nondescendants given just its parents. According to this Markov condition, the joint probability distribution on model variables X = (X1 , X 2, …, X n ) can be factored as follows: n P ( X 1 , X 2 , ..., X n ) = ∏ P ( X i | parents( X i )) , (5) i =1 where parents(Xi ) denotes the set of nodes that are the parents of X i . If Xi has no parents, then the set parents(Xi ) is empty and P(Xi | parents(X i)) is just P(Xi ). 3.2 I S A M od e l s The LBR models of Zheng and Webb [1] can be represented as members of a restricted class of Bayesian networks (see Figure 1). We use the same class of Bayesian networks for the ISA models, to facilitate comparison between the two algorithms. In Figure 1, all nodes represent attributes that are discrete. Each node in X has either an outgoing arc into target node, Z, or receives an arc from Z. That is, each node is either a parent or a child of Z. Thus, X is partitioned into two sets: the first containing nodes (X 1 , …, X j in Figure 1) each of which is a parent of Z and every node in the second set, and the second containing nodes (X j+1 , …, X k in Figure 1) that have as parents the node Z and every node in the first set. The nodes in the first set are instantiated to the corresponding values in the test instance for which Zt is to be predicted. Thus, the first set of nodes represents the antecedent of the LBR rule and the second set of nodes represents the consequent. ... X1= x1 Xi = xi Z Xi+1 ... Xk Figure 1: An example of a Bayesian network LBR model with target node Z and k attribute nodes of which X1 , …, X j are instantiated to values x 1 , …, x j in xt . X 1, …, X j are present in the antecedent of the LBR rule and Z, X j+1 , …, X k (that form the local simple Bayes classifier) are present in the consequent. The indices need not be ordered as shown, but are presented in this example for convenience of exposition. 3.3 M od e l A ve r ag i n g For Bayesian networks, Equation 1 can be evaluated as follows: P ( Z t | x t , D ) = ∑ P ( Z t | x t , M ) P( M | D ) , (6) M with M being a Bayesian network comprised of structure G and parameters ΘG. The probability distribution of interest is a weighted average of the posterior distribution over all possible Bayesian networks where the weight is the probability of the Bayesian network given the data. Since exhaustive enumeration of all possible models is not feasible, even for this class of simple Bayesian networks, we approximate exact model averaging with selective model averaging. Let R be the set of models selected by the search procedure from all possible models in the model space, as described in the next section. Then, with selective model averaging, P(Zt | xt, D) is estimated as: ∑RP( Z t | x t , M ) P(M | D) P (Z t | x t , D) ≅ M ∈ . ∑RP(M | D) M∈ (7) Assuming uniform prior belief over all possible models, the model posterior P(M | D) in Equation 7 can be replaced by the marginal likelihood P(D | M), to obtain the following equation: P ( Z | x , D) ≅ t t ∑ P ( Z t | x t , M ) P( D | M ) . ∑RP( D | M ) M∈ M ∈R (8) The (unconditional) marginal likelihood P(D | M) in Equation 8, is a measure of the goodness of fit of the model to the data and is also known as the model score. While this score is suitable for assessing the model’s fit to the joint probability distribution, it is not necessarily appropriate for assessing the goodness of fit to a conditional probability distribution which is the focus in prediction and classification tasks, as is the case here. A more suitable score in this situation is a conditional model score that is computed from training data D of d instances as: d score( D, M ) = ∏ P ( z p | x1 ,..., x p ,z 1 ,...,z p −1 ,M ) . (9) p =1 This score is computed in a predictive and sequential fashion: for the pth training instance the probability of predicting the observed value zp for the target variable is computed based on the values of all the variables in the preceding p-1 training instances and the values xp of the attributes in the pth instance. One limitation of this score is that its value depends on the ordering of the data. Despite this limitation, it has been shown to be an effective scoring criterion for classification models [4]. The parameters of the Bayesian network M, used in the above computations, are defined as follows: P ( X i = k | parents ( X i ) = j ) ≡ θ ijk = N ijk + α ijk N ij + α ij , (10) where (i) Nijk is the number of instances in the training dataset D where variable Xi has value k and the parents of X i are in state j, (ii) N ij = ∑k N ijk , (iii) αijk is a parameter prior that can be interpreted as the belief equivalent of having previously observed αijk instances in which variable Xi has value k and the parents of X i are in state j, and (iv) α ij = ∑k α ijk . 3.4 M od e l Se a r c h We use a two-phase best-first heuristic search to sample the model space. The first phase ignores the evidence xt in the test instance while searching for models that have high scores as given by Equation 9. This is followed by the second phase that searches for models having the greatest impact on the prediction of Zt for the test instance, which we formalize below. The first phase searches for models that predict Z in the training data very well; these are the models that have high conditional model scores. The initial model is the simple Bayes network that includes all the attributes in X as children of Z. A succeeding model is derived from a current model by reversing the arc of a child node in the current model, adding new outgoing arcs from it to Z and the remaining children, and instantiating this node to the value in the test instance. This process is performed for each child in the current model. An incoming arc of a child node is considered for reversal only if the node’s value is not missing in the test instance. The newly derived models are added to a priority queue, Q. During each iteration of the search, the model with the highest score (given by Equation 9) is removed from Q and placed in a set R, following which new models are generated as described just above, scored and added to Q. The first phase terminates after a user-specified number of models have accumulated in R. The second phase searches for models that change the current model-averaged estimate of P(Zt | xt , D) the most. The idea here is to find viable competing models for making this posterior probability prediction. When no competitive models can be found, the prediction becomes stable. During each iteration of the search, the highest ranked model M* is removed from Q and added to R. The ranking is based on how much the model changes the current estimate of P(Zt | xt , D). More change is better. In particular, M* is the model in Q that maximizes the following function: f ( R, M *) = g ( R) − g ( R U {M *}) , (11) where for a set of models S, the function g(S) computes the approximate model averaged prediction for Zt, as follows: g (S ) = ∑ P(Z M ∈S t | x t , M ) score( D, M ) ∑ score( D, M ) ∈ . (12) M S The second phase terminates when no new model can be found that has a value (as given by Equation 11) that is greater than a user-specified minimum threshold T. The final distribution of Zt is then computed from the models in R using Equation 8. 4 Ev a lu a t i o n We evaluated ISA on the 29 UCI datasets that Zheng and Webb used for the evaluation of LBR. On the same datasets, we also evaluated a simple Bayes classifier (SB) and LBR. For SB and LBR, we used the Weka implementations (Weka v3.3.6, http://www.cs.waikato.ac.nz/ml/weka/) with default settings [5]. We implemented the ISA algorithm as a standalone application in Java. The following settings were used for ISA: a maximum of 100 phase-1 models, a threshold T of 0.001 in phase-2, and an upper limit of 500 models in R. For the parameter priors in Equation 10, all αijk were set to 1. All error rates were obtained by averaging the results from two stratified 10-fold cross-validation (20 trials total) similar to that used by Zheng and Webb. Since, both LBR and ISA can handle only discrete attributes, all numeric attributes were discretized in a pre-processing step using the entropy based discretization method described in [6]. For each pair of training and test folds, the discretization intervals were first estimated from the training fold and then applied to both folds. The error rates of two algorithms on a dataset were compared with a paired t-test carried out at the 5% significance level on the error rate statistics obtained from the 20 trials. The results are shown in Table 1. Compared to SB, ISA has significantly fewer errors on 9 datasets and significantly more errors on one dataset. Compared to LBR, ISA has significantly fewer errors on 7 datasets and significantly more errors on two datasets. On two datasets, chess and tic-tac-toe, ISA shows considerable improvement in performance over both SB and LBR. With respect to computation Table 1: Percent error rates of simple Bayes (SB), Lazy Bayesian Rule (LBR) and Instance-Specific Averaging (ISA). A - indicates that the ISA error rate is statistically significantly lower than the marked SB or LBR error rate. A + indicates that the ISA error rate is statistically significantly higher. Dataset Size Annealing Audiology Breast (W) Chess (KR-KP) Credit (A) Echocardiogram Glass Heart (C) Hepatitis Horse colic House votes 84 Hypothyroid Iris Labor LED 24 Liver disorders Lung cancer Lymphography Pima Postoperative Primary tumor Promoters Solar flare Sonar Soybean Splice junction Tic-Tac-Toe Wine Zoo 898 226 699 3169 690 131 214 303 155 368 435 3163 150 57 200 345 32 148 768 90 339 106 1389 208 683 3177 958 178 101 No. of classes 6 24 2 2 2 2 6 2 2 2 2 2 3 2 10 2 3 4 2 3 22 2 2 2 19 3 2 3 7 Num. Attrib. 6 0 9 0 6 6 9 13 6 7 0 7 4 8 0 6 0 0 8 1 0 0 0 60 0 0 0 13 0 Nom. Attrib. 32 69 0 36 9 1 0 0 13 15 16 18 0 8 24 0 56 18 0 7 17 57 10 0 35 60 9 0 16 Percent error rate SB LBR ISA 1.9 3.5 2.7 29.6 29.4 30.9 3.7 2.9 + 2.8 + 1.1 12.1 3.0 13.8 14.0 13.9 33.2 34.0 35.9 26.9 27.8 29.0 16.2 16.2 17.5 14.2 - 14.2 - 11.3 20.2 16.0 17.8 5.1 10.1 7.0 0.9 0.9 1.4 6.0 6.0 5.3 8.8 6.1 7.0 40.5 40.5 40.3 36.8 36.8 36.8 56.3 56.3 56.3 15.5 - 15.5 - 13.2 21.8 22.0 22.3 33.3 33.3 33.3 54.4 53.5 54.2 7.5 7.5 7.5 20.2 18.3 + 19.4 15.4 15.6 15.9 7.1 7.2 7.9 4.7 4.3 4.4 30.3 - 13.7 - 10.3 1.1 1.1 1.1 6.4 8.4 8.4 - times, ISA took 6 times longer to run than LBR on average for a single test instance on a desktop computer with a 2 GHz Pentium 4 processor and 3 GB of RAM. 5 C o n c lu si o n s a n d Fu t u re R e s ea rc h We have introduced a Bayesian framework for instance-specific model averaging and presented ISA as one example of a classification algorithm based on this framework. An instance-specific algorithm like LBR that does model selection has been shown by Zheng and Webb to perform classification better than several eager algorithms [1]. Our results show that ISA, which extends LBR by adding Bayesian model averaging, improves overall on LBR, which provides support that we can obtain additional prediction improvement by performing instance-specific model averaging rather than just instance-specific model selection. In future work, we plan to explore further the behavior of ISA with respect to the number of models being averaged and the effect of the number of models selected in each of the two phases of the search. We will also investigate methods to improve the computational efficiency of ISA. In addition, we plan to examine other heuristics for model search as well as more general model spaces such as unrestricted Bayesian networks. The instance-specific framework is not restricted to the Bayesian network models that we have used in this investigation. In the future, we plan to explore other models using this framework. Our ultimate interest is to apply these instancespecific algorithms to improve patient-specific predictions (for diagnosis, therapy selection, and prognosis) and thereby to improve patient care. A c k n ow l e d g me n t s This work was supported by the grant T15-LM/DE07059 from the National Library of Medicine (NLM) to the University of Pittsburgh’s Biomedical Informatics Training Program. We would like to thank the three anonymous reviewers for their helpful comments. References [1] Zheng, Z. and Webb, G.I. (2000). Lazy Learning of Bayesian Rules. Machine Learning, 41(1):53-84. [2] Hoeting, J.A., Madigan, D., Raftery, A.E. and Volinsky, C.T. (1999). Bayesian Model Averaging: A Tutorial. Statistical Science, 14:382-417. [3] Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, San Mateo, CA. [4] Kontkanen, P., Myllymaki, P., Silander, T., and Tirri, H. (1999). On Supervised Selection of Bayesian Networks. In Proceedings of the 15th International Conference on Uncertainty in Artificial Intelligence, pages 334-342, Stockholm, Sweden. Morgan Kaufmann. [5] Witten, I.H. and Frank, E. (2000). Data Mining: Practical Machine Learning Tools with Java Implementations. Morgan Kaufmann, San Francisco, CA. [6] Fayyad, U.M., and Irani, K.B. (1993). Multi-Interval Discretization of ContinuousValued Attributes for Classification Learning. In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, pages 1022-1027, San Mateo, CA. Morgan Kaufmann.</p><p>6 0.59453785 <a title="154-lda-6" href="./nips-2004-Approximately_Efficient_Online_Mechanism_Design.html">24 nips-2004-Approximately Efficient Online Mechanism Design</a></p>
<p>7 0.59243846 <a title="154-lda-7" href="./nips-2004-Common-Frame_Model_for_Object_Recognition.html">40 nips-2004-Common-Frame Model for Object Recognition</a></p>
<p>8 0.58838654 <a title="154-lda-8" href="./nips-2004-Identifying_Protein-Protein_Interaction_Sites_on_a_Genome-Wide_Scale.html">80 nips-2004-Identifying Protein-Protein Interaction Sites on a Genome-Wide Scale</a></p>
<p>9 0.58798516 <a title="154-lda-9" href="./nips-2004-Schema_Learning%3A_Experience-Based_Construction_of_Predictive_Action_Models.html">159 nips-2004-Schema Learning: Experience-Based Construction of Predictive Action Models</a></p>
<p>10 0.58715039 <a title="154-lda-10" href="./nips-2004-Dynamic_Bayesian_Networks_for_Brain-Computer_Interfaces.html">56 nips-2004-Dynamic Bayesian Networks for Brain-Computer Interfaces</a></p>
<p>11 0.58679008 <a title="154-lda-11" href="./nips-2004-A_Probabilistic_Model_for_Online_Document_Clustering_with_Application_to_Novelty_Detection.html">10 nips-2004-A Probabilistic Model for Online Document Clustering with Application to Novelty Detection</a></p>
<p>12 0.58543402 <a title="154-lda-12" href="./nips-2004-Heuristics_for_Ordering_Cue_Search_in_Decision_Making.html">75 nips-2004-Heuristics for Ordering Cue Search in Decision Making</a></p>
<p>13 0.58488894 <a title="154-lda-13" href="./nips-2004-Semi-supervised_Learning_via_Gaussian_Processes.html">166 nips-2004-Semi-supervised Learning via Gaussian Processes</a></p>
<p>14 0.583978 <a title="154-lda-14" href="./nips-2004-Active_Learning_for_Anomaly_and_Rare-Category_Detection.html">15 nips-2004-Active Learning for Anomaly and Rare-Category Detection</a></p>
<p>15 0.58361751 <a title="154-lda-15" href="./nips-2004-Temporal-Difference_Networks.html">183 nips-2004-Temporal-Difference Networks</a></p>
<p>16 0.58282942 <a title="154-lda-16" href="./nips-2004-Synergistic_Face_Detection_and_Pose_Estimation_with_Energy-Based_Models.html">182 nips-2004-Synergistic Face Detection and Pose Estimation with Energy-Based Models</a></p>
<p>17 0.58184385 <a title="154-lda-17" href="./nips-2004-Semi-Markov_Conditional_Random_Fields_for_Information_Extraction.html">162 nips-2004-Semi-Markov Conditional Random Fields for Information Extraction</a></p>
<p>18 0.58079171 <a title="154-lda-18" href="./nips-2004-At_the_Edge_of_Chaos%3A_Real-time_Computations_and_Self-Organized_Criticality_in_Recurrent_Neural_Networks.html">26 nips-2004-At the Edge of Chaos: Real-time Computations and Self-Organized Criticality in Recurrent Neural Networks</a></p>
<p>19 0.58047152 <a title="154-lda-19" href="./nips-2004-Semi-supervised_Learning_with_Penalized_Probabilistic_Clustering.html">167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</a></p>
<p>20 0.58042425 <a title="154-lda-20" href="./nips-2004-Experts_in_a_Markov_Decision_Process.html">64 nips-2004-Experts in a Markov Decision Process</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
