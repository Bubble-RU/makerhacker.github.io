<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>15 nips-2004-Active Learning for Anomaly and Rare-Category Detection</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-15" href="#">nips2004-15</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>15 nips-2004-Active Learning for Anomaly and Rare-Category Detection</h1>
<br/><p>Source: <a title="nips-2004-15-pdf" href="http://papers.nips.cc/paper/2554-active-learning-for-anomaly-and-rare-category-detection.pdf">pdf</a></p><p>Author: Dan Pelleg, Andrew W. Moore</p><p>Abstract: We introduce a novel active-learning scenario in which a user wants to work with a learning algorithm to identify useful anomalies. These are distinguished from the traditional statistical deﬁnition of anomalies as outliers or merely ill-modeled points. Our distinction is that the usefulness of anomalies is categorized subjectively by the user. We make two additional assumptions. First, there exist extremely few useful anomalies to be hunted down within a massive dataset. Second, both useful and useless anomalies may sometimes exist within tiny classes of similar anomalies. The challenge is thus to identify “rare category” records in an unlabeled noisy set with help (in the form of class labels) from a human expert who has a small budget of datapoints that they are prepared to categorize. We propose a technique to meet this challenge, which assumes a mixture model ﬁt to the data, but otherwise makes no assumptions on the particular form of the mixture components. This property promises wide applicability in real-life scenarios and for various statistical models. We give an overview of several alternative methods, highlighting their strengths and weaknesses, and conclude with a detailed empirical analysis. We show that our method can quickly zoom in on an anomaly set containing a few tens of points in a dataset of hundreds of thousands. 1</p><p>Reference: <a title="nips-2004-15-reference" href="../nips2004_reference/nips-2004-Active_Learning_for_Anomaly_and_Rare-Category_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We introduce a novel active-learning scenario in which a user wants to work with a learning algorithm to identify useful anomalies. [sent-5, score-0.163]
</p><p>2 These are distinguished from the traditional statistical deﬁnition of anomalies as outliers or merely ill-modeled points. [sent-6, score-0.403]
</p><p>3 Our distinction is that the usefulness of anomalies is categorized subjectively by the user. [sent-7, score-0.403]
</p><p>4 First, there exist extremely few useful anomalies to be hunted down within a massive dataset. [sent-9, score-0.436]
</p><p>5 Second, both useful and useless anomalies may sometimes exist within tiny classes of similar anomalies. [sent-10, score-0.54]
</p><p>6 The challenge is thus to identify “rare category” records in an unlabeled noisy set with help (in the form of class labels) from a human expert who has a small budget of datapoints that they are prepared to categorize. [sent-11, score-0.48]
</p><p>7 We propose a technique to meet this challenge, which assumes a mixture model ﬁt to the data, but otherwise makes no assumptions on the particular form of the mixture components. [sent-12, score-0.158]
</p><p>8 We show that our method can quickly zoom in on an anomaly set containing a few tens of points in a dataset of hundreds of thousands. [sent-15, score-0.216]
</p><p>9 1  Introduction  We begin with an example of a rare-category-detection problem: an astronomer needs to sift through a large set of sky survey images, each of which comes with many numerical parameters. [sent-16, score-0.148]
</p><p>10 The remainder are anomalies, but 99% of these anomalies are uninteresting, and only 1% of them (0. [sent-19, score-0.428]
</p><p>11 The ﬁrst type of anomalies, called “boring anomalies”, are records which are strange for uninteresting reasons such as sensor faults or problems in the image processing software. [sent-21, score-0.224]
</p><p>12 The useful anomalies are extraordinary objects which are worthy of further research. [sent-22, score-0.454]
</p><p>13 Two rare categories of “boring” anomalies in our test astrophysics data are shown in Figure 1. [sent-29, score-0.612]
</p><p>14 At this point, objects ﬂagged as “anomalous” can still be almost entirely of the uninteresting class of anomalies. [sent-33, score-0.206]
</p><p>15 The computational and statistical question is then how to use feedback from the human user to iteratively reorder the queue of anomalies to be shown to the user in order to increase the chance that the user will soon see an anomaly of a whole new category. [sent-34, score-0.827]
</p><p>16 Each round starts with the teacher labeling a small number of examples. [sent-37, score-0.226]
</p><p>17 The learner then identiﬁes a small number of input records (“hints”) which are important in the sense that obtaining labels for them would help it improve the model. [sent-39, score-0.314]
</p><p>18 These are shown to the teacher (in our scenario, a human expert) for labeling, and the cycle repeats. [sent-40, score-0.195]
</p><p>19 It may seem too demanding to ask the human expert to give class labels instead of a simple “interesting” or “boring” ﬂag. [sent-42, score-0.324]
</p><p>20 For example, in the astronomical data we have seen a user place most objects into previously-known categories: point sources, low-surface-brightness galaxies, etc. [sent-44, score-0.147]
</p><p>21 This also holds for the negative examples: it is frustrating to have to label all anomalies as “bad” without being able to explain why. [sent-45, score-0.427]
</p><p>22 For all it cares, the label set can be utterly changed by the user from one round to another. [sent-48, score-0.201]
</p><p>23 Our tools allow that: the labels are unconstrained and the user can add, reﬁne, and delete classes at will. [sent-49, score-0.295]
</p><p>24 Our work differs from traditional applications of active learning in that we assume the distribution of class sizes to be extremely skewed. [sent-51, score-0.207]
</p><p>25 Generally in active learning, it is believed that, right from the start, examples from each class need to be presented to the oracle [1, 2, 3]. [sent-53, score-0.174]
</p><p>26 But in datasets with the rare categories property, this no longer holds, and much of our effort is an attempt to remedy the situation. [sent-55, score-0.173]
</p><p>27 1 More precisely, we allow multiple queries and labels in each learning round — the traditional presentation has just one. [sent-59, score-0.234]
</p><p>28 (a)  (b)  (c)  (d)  (e)  (f)  Figure 2: Underlying data distribution for the example (a); behavior of the lowlik method (b–f). [sent-60, score-0.39]
</p><p>29 The anomalous points according to lowlik, given the model in (b), are shown in (c). [sent-63, score-0.258]
</p><p>30 Given labels for the points in (c), the model in (d) is ﬁtted. [sent-64, score-0.227]
</p><p>31 Given the new model, anomalous points according to lowlik are ﬂagged (e). [sent-65, score-0.648]
</p><p>32 Given labels for the points in (c) and (e), this is the new ﬁtted model (f). [sent-66, score-0.227]
</p><p>33 In any case, we need to be able to identify query points in the presence of noise. [sent-71, score-0.196]
</p><p>34 This is a not just a bonus feature: points which the model considers noisy could very well be the key to improvement if presented to the oracle. [sent-72, score-0.17]
</p><p>35 2  Overview of Hint Selection Methods  In this section we survey several proposed methods for active learning as they apply to our setting. [sent-74, score-0.152]
</p><p>36 One is an X-shaped distribution, from which 2000 points are drawn. [sent-80, score-0.131]
</p><p>37 Before each M step we clamp the class membership values for the hinted records to match the hints (i. [sent-84, score-0.616]
</p><p>38 Given fully labeled data, our learner would perfectly predict class membership for this data (although it would be a poor generative model): one Gaussian centered on the circle, and another spherical Gaussian with high variance centered on the X. [sent-87, score-0.227]
</p><p>39 On the ﬁrst iteration (when unsupervised) the algorithm will naturally use the two Gaussians to model the data as in Figure 2(b), with one Gaussian for each of the arms of the “X”, and the points in the circle represented as members of one of them. [sent-96, score-0.261]
</p><p>40 Choosing Points with Low Likelihood: A rather intuitive approach is to select as hints the points which the model performs worst on. [sent-99, score-0.449]
</p><p>41 This can be viewed as model variance  (a)  (b)  (c)  (d)  (e)  Figure 3: Behavior of the ambig (a–c) and interleave (d–e) methods. [sent-100, score-0.512]
</p><p>42 The unsupervised model and the points which ambig ﬂags as anomalous, given this model (a). [sent-101, score-0.401]
</p><p>43 The model learned using labels for these points is (b), along with the point it ﬂags. [sent-102, score-0.227]
</p><p>44 minimization [4] or as selection of points furthest away from any labeled points [5]. [sent-104, score-0.307]
</p><p>45 We do this by ranking each point in order of increasing model likelihood, and choosing the most anomalous items. [sent-105, score-0.153]
</p><p>46 Each subsequent drawing shows a model which EM converged to after including the new labels, and the hints it chooses under a particular scheme (here it is what we call lowlik). [sent-108, score-0.318]
</p><p>47 These hints affect the model shown for the next round. [sent-109, score-0.318]
</p><p>48 In any event, none of the points in the circle is ﬂagged. [sent-115, score-0.186]
</p><p>49 Only after obtaining labels for all of the “outlier” points (that is, those on the extremes of the distribution) will this approach go far enough down the list to hit a point in the circle. [sent-118, score-0.349]
</p><p>50 Choosing Ambiguous Points: Another popular approach is to choose the points which the learner is least certain about. [sent-120, score-0.179]
</p><p>51 This way, the top of the list will have the objects which are “owned” by multiple components. [sent-125, score-0.173]
</p><p>52 As expected, points on the decision boundaries between classes are chosen. [sent-127, score-0.234]
</p><p>53 It may help modeling the points very close to the boundaries, but it does not improve generalization accuracy in the general case. [sent-131, score-0.162]
</p><p>54 Indeed, we see that if we repeatedly apply this criterion we end up asking for labels for a great number of points in close proximity, to very little effect on the overall model. [sent-132, score-0.227]
</p><p>55 Combining Unlikely and Ambiguous Points: Our next candidate is a hybrid method which tries to combine the hints from the two previous methods. [sent-134, score-0.318]
</p><p>56 Recall they both produce a ranked list of all the points. [sent-135, score-0.17]
</p><p>57 We merge the lists into another ranked list in the following way. [sent-136, score-0.317]
</p><p>58 When all elements are taken, the output list is a ranked list as required. [sent-139, score-0.292]
</p><p>59 We now pick the top items from this list for hints. [sent-140, score-0.175]
</p><p>60 As expected we get a good mix of points in both hint sets (not shown). [sent-141, score-0.221]
</p><p>61 The key insight is that our group of anomalies was, in fact, reasonably ordinary when analyzed on a global scale. [sent-147, score-0.403]
</p><p>62 In other words, the mixture density of the region we chose for the group of anomalies is not sufﬁciently low for them to rank high on the hint list. [sent-148, score-0.572]
</p><p>63 Our idea is that if we restrict the focus to match the “point of view” of just one component, these anomalies will become more apparent. [sent-154, score-0.403]
</p><p>64 The hope is that given this restricted view, anomalies that do not ﬁt the component’s own model will stand out. [sent-156, score-0.403]
</p><p>65 For each c′ c component c we create a list of all the points for which c = arg maxc′ zi , ranked by zi . [sent-159, score-0.411]
</p><p>66 Most of the points are along the major axes of the two elongated Gaussians, but two of the points are inside the small circle. [sent-166, score-0.262]
</p><p>67 Correct labels for even just these two points result in perfect classiﬁcation in the next EM run. [sent-167, score-0.227]
</p><p>68 This modiﬁcation lets it nominate hints more often than any other component. [sent-170, score-0.367]
</p><p>69 In terms of list merging, we take one element from each of the lists of standard components, and then several elements from the list produced for the background component. [sent-171, score-0.35]
</p><p>70 In other words, if there are N components (excluding uniform), then the ﬁrst cycle of hint nomination will result in 20 + N hints, 20 of which from uniform. [sent-173, score-0.167]
</p><p>71 The class size distribution is a geometric series with the largest class owning half of the data and each subsequent class being half as small. [sent-177, score-0.21]
</p><p>72 4 0  200  400  600  800 hints  1000  1200  1400  1600  0  500  1000  1500 hints  2000  2500  3000  Figure 4: Learning curves for simulated data drawn from a mixture of dependency trees (left), and for the SHUTTLE set (right). [sent-193, score-0.77]
</p><p>73 The Y axis shows the fraction of classes represented in queries sent to the teacher. [sent-194, score-0.16]
</p><p>74 4  lowlik random ambig interleave  lowlik mix-ambig-lowlik random ambig interleave  0. [sent-209, score-1.852]
</p><p>75 1 0  50  100  150  200 hints  250  300  350  400  0  50  100  150  200 hints  250  300  350  400  Figure 5: Learning curves for the ABALONE (left) and KDD (right) sets. [sent-212, score-0.636]
</p><p>76 There are ten tree classes and a uniform background component. [sent-218, score-0.14]
</p><p>77 Only the results for 15 dimensions and 100 noisy points are shown as they are representative of the other experiments. [sent-220, score-0.17]
</p><p>78 In each round of learning, the learner queries the teacher with a list of 50 points for labeling, and has access to all the queries and replies submitted previously. [sent-221, score-0.641]
</p><p>79 Also included, are results for random, which is a baseline method choosing hints at random. [sent-225, score-0.344]
</p><p>80 Our scoring function is driven by our application, and estimates the amount of effort the teacher has to expend before being presented by representatives of every single class. [sent-226, score-0.167]
</p><p>81 The assumption is that the teacher can generalize from a single example (or a very few examples) to an entire class, and the valuable information is concentrated in the ﬁrst queried member of each class. [sent-227, score-0.169]
</p><p>82 More precisely, if there are n classes, then the score under this metric is 1/n times the number of classes represented in the query set. [sent-228, score-0.168]
</p><p>83 The best performer so far is interleave, taking ﬁve rounds or less to reveal all of the classes, including the very rare ones. [sent-230, score-0.161]
</p><p>84 We can also see that ambig performs worse than random. [sent-232, score-0.244]
</p><p>85 This can be explained by the fact that ambig only chooses points that already have several existing components “competing” for them. [sent-233, score-0.414]
</p><p>86 65 50  100  150  200  250  300  350  400  450  500  20  40  60  hints  80  100  120  140  160  180  200  hints  Figure 6: Learning curves for the EDSGC (left) and SDSS (right) sets. [sent-248, score-0.636]
</p><p>87 6%  SOURCE  [12] [13] [13] [14] [15]  We were concerned that the poor performance of lowlik was just a consequence of our choice of metric. [sent-258, score-0.39]
</p><p>88 These points are genuine anomalies, so it is possible that lowlik is being penalized unfairly for its focusing on the noise points. [sent-261, score-0.521]
</p><p>89 , points drawn from the uniform background component) found by each algorithm, we discovered that lowlik actually scores worse than interleave on this metric as well. [sent-264, score-0.904]
</p><p>90 We see that it takes the interleave algorithm ﬁve rounds to spot all classes, whereas the next best is lowlik, with 11. [sent-269, score-0.383]
</p><p>91 In Figure 5 we see that lowlik performs uncharacteristically poorly. [sent-274, score-0.39]
</p><p>92 Another surprise is that the combination of lowlik and ambig outperforms them both. [sent-275, score-0.634]
</p><p>93 It also matches interleave in performance, and this is the only case where we have seen it do so. [sent-276, score-0.268]
</p><p>94 The class labels relate to the shape and size of the sky object. [sent-278, score-0.217]
</p><p>95 We see in Figure 6 that for the purpose of class discovery, we can do a good job in a small number of rounds: here, a human would have had to label just 250 objects before being presented with a member of the smallest class - comprising just 24 records out of a set of 1. [sent-279, score-0.456]
</p><p>96 4  Conclusion  We have shown that some of the popular methods for active learning perform poorly in realistic active-learning scenarios where classes are imbalanced. [sent-281, score-0.256]
</p><p>97 These methods work well in the presence of noisy data and extremely rare classes and anomalies. [sent-283, score-0.279]
</p><p>98 Our simulations show that a human user only  needs to label one or two hundred examples before being presented with very rare anomalies in huge data sets. [sent-284, score-0.71]
</p><p>99 Consequently, we expect our results to apply to many different kinds of component models, including the case where components are not dependency trees, or even not all from the same distribution. [sent-287, score-0.146]
</p><p>100 Our application presents multiple indicators to help a user spot anomalous data, as well as controls for labeling points and adding classes. [sent-289, score-0.482]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('anomalies', 0.403), ('lowlik', 0.39), ('hints', 0.318), ('interleave', 0.268), ('ambig', 0.244), ('records', 0.139), ('points', 0.131), ('anomalous', 0.127), ('list', 0.122), ('teacher', 0.106), ('rare', 0.104), ('active', 0.104), ('classes', 0.103), ('abalone', 0.098), ('shuttle', 0.098), ('user', 0.096), ('labels', 0.096), ('hint', 0.09), ('anomaly', 0.085), ('uninteresting', 0.085), ('round', 0.081), ('mixture', 0.079), ('discovered', 0.078), ('astrophysics', 0.073), ('boring', 0.073), ('edsgc', 0.073), ('class', 0.07), ('lists', 0.069), ('expert', 0.067), ('query', 0.065), ('agged', 0.064), ('kdd', 0.064), ('spot', 0.058), ('queries', 0.057), ('rounds', 0.057), ('dependency', 0.055), ('circle', 0.055), ('merge', 0.054), ('component', 0.052), ('objects', 0.051), ('sky', 0.051), ('human', 0.051), ('unlabeled', 0.049), ('arms', 0.049), ('astronomer', 0.049), ('diffraction', 0.049), ('hinted', 0.049), ('nominate', 0.049), ('owned', 0.049), ('sdss', 0.049), ('scenarios', 0.049), ('twentieth', 0.049), ('learner', 0.048), ('ranked', 0.048), ('survey', 0.048), ('labeled', 0.045), ('ask', 0.04), ('membership', 0.04), ('components', 0.039), ('submitted', 0.039), ('noisy', 0.039), ('artifact', 0.039), ('labeling', 0.039), ('scenario', 0.038), ('cycle', 0.038), ('background', 0.037), ('effort', 0.037), ('em', 0.036), ('dan', 0.036), ('queried', 0.036), ('sloan', 0.036), ('david', 0.035), ('datapoints', 0.034), ('useless', 0.034), ('extremely', 0.033), ('cohen', 0.032), ('ambiguity', 0.032), ('cohn', 0.032), ('hundred', 0.032), ('categories', 0.032), ('help', 0.031), ('item', 0.031), ('ambiguous', 0.03), ('zi', 0.029), ('wants', 0.029), ('pick', 0.028), ('gaussians', 0.028), ('member', 0.027), ('members', 0.026), ('choosing', 0.026), ('unsupervised', 0.026), ('databases', 0.025), ('items', 0.025), ('remainder', 0.025), ('precisely', 0.024), ('smallest', 0.024), ('label', 0.024), ('random', 0.024), ('driven', 0.024), ('another', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="15-tfidf-1" href="./nips-2004-Active_Learning_for_Anomaly_and_Rare-Category_Detection.html">15 nips-2004-Active Learning for Anomaly and Rare-Category Detection</a></p>
<p>Author: Dan Pelleg, Andrew W. Moore</p><p>Abstract: We introduce a novel active-learning scenario in which a user wants to work with a learning algorithm to identify useful anomalies. These are distinguished from the traditional statistical deﬁnition of anomalies as outliers or merely ill-modeled points. Our distinction is that the usefulness of anomalies is categorized subjectively by the user. We make two additional assumptions. First, there exist extremely few useful anomalies to be hunted down within a massive dataset. Second, both useful and useless anomalies may sometimes exist within tiny classes of similar anomalies. The challenge is thus to identify “rare category” records in an unlabeled noisy set with help (in the form of class labels) from a human expert who has a small budget of datapoints that they are prepared to categorize. We propose a technique to meet this challenge, which assumes a mixture model ﬁt to the data, but otherwise makes no assumptions on the particular form of the mixture components. This property promises wide applicability in real-life scenarios and for various statistical models. We give an overview of several alternative methods, highlighting their strengths and weaknesses, and conclude with a detailed empirical analysis. We show that our method can quickly zoom in on an anomaly set containing a few tens of points in a dataset of hundreds of thousands. 1</p><p>2 0.16182096 <a title="15-tfidf-2" href="./nips-2004-Analysis_of_a_greedy_active_learning_strategy.html">23 nips-2004-Analysis of a greedy active learning strategy</a></p>
<p>Author: Sanjoy Dasgupta</p><p>Abstract: We abstract out the core search problem of active learning schemes, to better understand the extent to which adaptive labeling can improve sample complexity. We give various upper and lower bounds on the number of labels which need to be queried, and we prove that a popular greedy active learning rule is approximately as good as any other strategy for minimizing this number of labels. 1</p><p>3 0.091121465 <a title="15-tfidf-3" href="./nips-2004-Semi-supervised_Learning_by_Entropy_Minimization.html">164 nips-2004-Semi-supervised Learning by Entropy Minimization</a></p>
<p>Author: Yves Grandvalet, Yoshua Bengio</p><p>Abstract: We consider the semi-supervised learning problem, where a decision rule is to be learned from labeled and unlabeled data. In this framework, we motivate minimum entropy regularization, which enables to incorporate unlabeled data in the standard supervised learning. Our approach includes other approaches to the semi-supervised problem as particular or limiting cases. A series of experiments illustrates that the proposed solution beneﬁts from unlabeled data. The method challenges mixture models when the data are sampled from the distribution class spanned by the generative model. The performances are deﬁnitely in favor of minimum entropy regularization when generative models are misspeciﬁed, and the weighting of unlabeled data provides robustness to the violation of the “cluster assumption”. Finally, we also illustrate that the method can also be far superior to manifold learning in high dimension spaces. 1</p><p>4 0.087902457 <a title="15-tfidf-4" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>Author: Aharon Bar-hillel, Adam Spiro, Eran Stark</p><p>Abstract: Spike sorting involves clustering spike trains recorded by a microelectrode according to the source neuron. It is a complicated problem, which requires a lot of human labor, partly due to the non-stationary nature of the data. We propose an automated technique for the clustering of non-stationary Gaussian sources in a Bayesian framework. At a ﬁrst search stage, data is divided into short time frames and candidate descriptions of the data as a mixture of Gaussians are computed for each frame. At a second stage transition probabilities between candidate mixtures are computed, and a globally optimal clustering is found as the MAP solution of the resulting probabilistic model. Transition probabilities are computed using local stationarity assumptions and are based on a Gaussian version of the Jensen-Shannon divergence. The method was applied to several recordings. The performance appeared almost indistinguishable from humans in a wide range of scenarios, including movement, merges, and splits of clusters. 1</p><p>5 0.085130706 <a title="15-tfidf-5" href="./nips-2004-Density_Level_Detection_is_Classification.html">49 nips-2004-Density Level Detection is Classification</a></p>
<p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: We show that anomaly detection can be interpreted as a binary classiﬁcation problem. Using this interpretation we propose a support vector machine (SVM) for anomaly detection. We then present some theoretical results which include consistency and learning rates. Finally, we experimentally compare our SVM with the standard one-class SVM. 1</p><p>6 0.075473502 <a title="15-tfidf-6" href="./nips-2004-Learning_Gaussian_Process_Kernels_via_Hierarchical_Bayes.html">98 nips-2004-Learning Gaussian Process Kernels via Hierarchical Bayes</a></p>
<p>7 0.073318876 <a title="15-tfidf-7" href="./nips-2004-Parametric_Embedding_for_Class_Visualization.html">145 nips-2004-Parametric Embedding for Class Visualization</a></p>
<p>8 0.073055163 <a title="15-tfidf-8" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>9 0.072758354 <a title="15-tfidf-9" href="./nips-2004-Object_Classification_from_a_Single_Example_Utilizing_Class_Relevance_Metrics.html">134 nips-2004-Object Classification from a Single Example Utilizing Class Relevance Metrics</a></p>
<p>10 0.066475831 <a title="15-tfidf-10" href="./nips-2004-Distributed_Information_Regularization_on_Graphs.html">54 nips-2004-Distributed Information Regularization on Graphs</a></p>
<p>11 0.064887784 <a title="15-tfidf-11" href="./nips-2004-An_Investigation_of_Practical_Approximate_Nearest_Neighbor_Algorithms.html">22 nips-2004-An Investigation of Practical Approximate Nearest Neighbor Algorithms</a></p>
<p>12 0.062720113 <a title="15-tfidf-12" href="./nips-2004-Instance-Based_Relevance_Feedback_for_Image_Retrieval.html">85 nips-2004-Instance-Based Relevance Feedback for Image Retrieval</a></p>
<p>13 0.062631488 <a title="15-tfidf-13" href="./nips-2004-Semigroup_Kernels_on_Finite_Sets.html">168 nips-2004-Semigroup Kernels on Finite Sets</a></p>
<p>14 0.061777987 <a title="15-tfidf-14" href="./nips-2004-On_Semi-Supervised_Classification.html">136 nips-2004-On Semi-Supervised Classification</a></p>
<p>15 0.058326878 <a title="15-tfidf-15" href="./nips-2004-Multiple_Relational_Embedding.html">125 nips-2004-Multiple Relational Embedding</a></p>
<p>16 0.056915913 <a title="15-tfidf-16" href="./nips-2004-Semi-parametric_Exponential_Family_PCA.html">163 nips-2004-Semi-parametric Exponential Family PCA</a></p>
<p>17 0.056072071 <a title="15-tfidf-17" href="./nips-2004-Kernel_Methods_for_Implicit_Surface_Modeling.html">92 nips-2004-Kernel Methods for Implicit Surface Modeling</a></p>
<p>18 0.055665761 <a title="15-tfidf-18" href="./nips-2004-Result_Analysis_of_the_NIPS_2003_Feature_Selection_Challenge.html">156 nips-2004-Result Analysis of the NIPS 2003 Feature Selection Challenge</a></p>
<p>19 0.053126231 <a title="15-tfidf-19" href="./nips-2004-Sampling_Methods_for_Unsupervised_Learning.html">158 nips-2004-Sampling Methods for Unsupervised Learning</a></p>
<p>20 0.05297666 <a title="15-tfidf-20" href="./nips-2004-Modeling_Nonlinear_Dependencies_in_Natural_Images_using_Mixture_of_Laplacian_Distribution.html">121 nips-2004-Modeling Nonlinear Dependencies in Natural Images using Mixture of Laplacian Distribution</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.183), (1, 0.057), (2, -0.025), (3, -0.027), (4, 0.008), (5, 0.096), (6, 0.001), (7, 0.065), (8, 0.057), (9, -0.037), (10, 0.008), (11, 0.135), (12, 0.001), (13, -0.076), (14, -0.048), (15, 0.01), (16, 0.117), (17, -0.024), (18, 0.035), (19, -0.093), (20, -0.009), (21, 0.182), (22, 0.1), (23, 0.139), (24, 0.004), (25, -0.026), (26, -0.036), (27, 0.067), (28, 0.026), (29, 0.02), (30, -0.024), (31, 0.054), (32, 0.041), (33, -0.089), (34, 0.059), (35, 0.02), (36, 0.014), (37, -0.005), (38, -0.031), (39, 0.024), (40, -0.098), (41, 0.029), (42, 0.02), (43, -0.008), (44, -0.061), (45, -0.092), (46, 0.084), (47, 0.035), (48, -0.062), (49, 0.083)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93341708 <a title="15-lsi-1" href="./nips-2004-Active_Learning_for_Anomaly_and_Rare-Category_Detection.html">15 nips-2004-Active Learning for Anomaly and Rare-Category Detection</a></p>
<p>Author: Dan Pelleg, Andrew W. Moore</p><p>Abstract: We introduce a novel active-learning scenario in which a user wants to work with a learning algorithm to identify useful anomalies. These are distinguished from the traditional statistical deﬁnition of anomalies as outliers or merely ill-modeled points. Our distinction is that the usefulness of anomalies is categorized subjectively by the user. We make two additional assumptions. First, there exist extremely few useful anomalies to be hunted down within a massive dataset. Second, both useful and useless anomalies may sometimes exist within tiny classes of similar anomalies. The challenge is thus to identify “rare category” records in an unlabeled noisy set with help (in the form of class labels) from a human expert who has a small budget of datapoints that they are prepared to categorize. We propose a technique to meet this challenge, which assumes a mixture model ﬁt to the data, but otherwise makes no assumptions on the particular form of the mixture components. This property promises wide applicability in real-life scenarios and for various statistical models. We give an overview of several alternative methods, highlighting their strengths and weaknesses, and conclude with a detailed empirical analysis. We show that our method can quickly zoom in on an anomaly set containing a few tens of points in a dataset of hundreds of thousands. 1</p><p>2 0.77481592 <a title="15-lsi-2" href="./nips-2004-Analysis_of_a_greedy_active_learning_strategy.html">23 nips-2004-Analysis of a greedy active learning strategy</a></p>
<p>Author: Sanjoy Dasgupta</p><p>Abstract: We abstract out the core search problem of active learning schemes, to better understand the extent to which adaptive labeling can improve sample complexity. We give various upper and lower bounds on the number of labels which need to be queried, and we prove that a popular greedy active learning rule is approximately as good as any other strategy for minimizing this number of labels. 1</p><p>3 0.58605516 <a title="15-lsi-3" href="./nips-2004-Semi-supervised_Learning_by_Entropy_Minimization.html">164 nips-2004-Semi-supervised Learning by Entropy Minimization</a></p>
<p>Author: Yves Grandvalet, Yoshua Bengio</p><p>Abstract: We consider the semi-supervised learning problem, where a decision rule is to be learned from labeled and unlabeled data. In this framework, we motivate minimum entropy regularization, which enables to incorporate unlabeled data in the standard supervised learning. Our approach includes other approaches to the semi-supervised problem as particular or limiting cases. A series of experiments illustrates that the proposed solution beneﬁts from unlabeled data. The method challenges mixture models when the data are sampled from the distribution class spanned by the generative model. The performances are deﬁnitely in favor of minimum entropy regularization when generative models are misspeciﬁed, and the weighting of unlabeled data provides robustness to the violation of the “cluster assumption”. Finally, we also illustrate that the method can also be far superior to manifold learning in high dimension spaces. 1</p><p>4 0.58305031 <a title="15-lsi-4" href="./nips-2004-Co-Validation%3A_Using_Model_Disagreement_on_Unlabeled_Data_to_Validate_Classification_Algorithms.html">38 nips-2004-Co-Validation: Using Model Disagreement on Unlabeled Data to Validate Classification Algorithms</a></p>
<p>Author: Omid Madani, David M. Pennock, Gary W. Flake</p><p>Abstract: In the context of binary classiﬁcation, we deﬁne disagreement as a measure of how often two independently-trained models differ in their classiﬁcation of unlabeled data. We explore the use of disagreement for error estimation and model selection. We call the procedure co-validation, since the two models effectively (in)validate one another by comparing results on unlabeled data, which we assume is relatively cheap and plentiful compared to labeled data. We show that per-instance disagreement is an unbiased estimate of the variance of error for that instance. We also show that disagreement provides a lower bound on the prediction (generalization) error, and a tight upper bound on the “variance of prediction error”, or the variance of the average error across instances, where variance is measured across training sets. We present experimental results on several data sets exploring co-validation for error estimation and model selection. The procedure is especially effective in active learning settings, where training sets are not drawn at random and cross validation overestimates error. 1</p><p>5 0.54607338 <a title="15-lsi-5" href="./nips-2004-Semi-supervised_Learning_via_Gaussian_Processes.html">166 nips-2004-Semi-supervised Learning via Gaussian Processes</a></p>
<p>Author: Neil D. Lawrence, Michael I. Jordan</p><p>Abstract: We present a probabilistic approach to learning a Gaussian Process classiﬁer in the presence of unlabeled data. Our approach involves a “null category noise model” (NCNM) inspired by ordered categorical noise models. The noise model reﬂects an assumption that the data density is lower between the class-conditional densities. We illustrate our approach on a toy problem and present comparative results for the semi-supervised classiﬁcation of handwritten digits. 1</p><p>6 0.51280117 <a title="15-lsi-6" href="./nips-2004-Sampling_Methods_for_Unsupervised_Learning.html">158 nips-2004-Sampling Methods for Unsupervised Learning</a></p>
<p>7 0.4878493 <a title="15-lsi-7" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>8 0.45650008 <a title="15-lsi-8" href="./nips-2004-An_Investigation_of_Practical_Approximate_Nearest_Neighbor_Algorithms.html">22 nips-2004-An Investigation of Practical Approximate Nearest Neighbor Algorithms</a></p>
<p>9 0.44771832 <a title="15-lsi-9" href="./nips-2004-On_Semi-Supervised_Classification.html">136 nips-2004-On Semi-Supervised Classification</a></p>
<p>10 0.44316134 <a title="15-lsi-10" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>11 0.43834937 <a title="15-lsi-11" href="./nips-2004-Distributed_Information_Regularization_on_Graphs.html">54 nips-2004-Distributed Information Regularization on Graphs</a></p>
<p>12 0.41985205 <a title="15-lsi-12" href="./nips-2004-Heuristics_for_Ordering_Cue_Search_in_Decision_Making.html">75 nips-2004-Heuristics for Ordering Cue Search in Decision Making</a></p>
<p>13 0.40741012 <a title="15-lsi-13" href="./nips-2004-Mass_Meta-analysis_in_Talairach_Space.html">109 nips-2004-Mass Meta-analysis in Talairach Space</a></p>
<p>14 0.40419486 <a title="15-lsi-14" href="./nips-2004-Maximal_Margin_Labeling_for_Multi-Topic_Text_Categorization.html">111 nips-2004-Maximal Margin Labeling for Multi-Topic Text Categorization</a></p>
<p>15 0.38658518 <a title="15-lsi-15" href="./nips-2004-Semi-parametric_Exponential_Family_PCA.html">163 nips-2004-Semi-parametric Exponential Family PCA</a></p>
<p>16 0.3755044 <a title="15-lsi-16" href="./nips-2004-Semi-supervised_Learning_with_Penalized_Probabilistic_Clustering.html">167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</a></p>
<p>17 0.36918753 <a title="15-lsi-17" href="./nips-2004-Common-Frame_Model_for_Object_Recognition.html">40 nips-2004-Common-Frame Model for Object Recognition</a></p>
<p>18 0.36224735 <a title="15-lsi-18" href="./nips-2004-Instance-Based_Relevance_Feedback_for_Image_Retrieval.html">85 nips-2004-Instance-Based Relevance Feedback for Image Retrieval</a></p>
<p>19 0.36073744 <a title="15-lsi-19" href="./nips-2004-Neighbourhood_Components_Analysis.html">127 nips-2004-Neighbourhood Components Analysis</a></p>
<p>20 0.34954667 <a title="15-lsi-20" href="./nips-2004-Object_Classification_from_a_Single_Example_Utilizing_Class_Relevance_Metrics.html">134 nips-2004-Object Classification from a Single Example Utilizing Class Relevance Metrics</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.076), (15, 0.119), (17, 0.012), (25, 0.013), (26, 0.043), (31, 0.053), (32, 0.024), (33, 0.183), (35, 0.021), (39, 0.015), (50, 0.047), (87, 0.015), (96, 0.288)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79628432 <a title="15-lda-1" href="./nips-2004-Active_Learning_for_Anomaly_and_Rare-Category_Detection.html">15 nips-2004-Active Learning for Anomaly and Rare-Category Detection</a></p>
<p>Author: Dan Pelleg, Andrew W. Moore</p><p>Abstract: We introduce a novel active-learning scenario in which a user wants to work with a learning algorithm to identify useful anomalies. These are distinguished from the traditional statistical deﬁnition of anomalies as outliers or merely ill-modeled points. Our distinction is that the usefulness of anomalies is categorized subjectively by the user. We make two additional assumptions. First, there exist extremely few useful anomalies to be hunted down within a massive dataset. Second, both useful and useless anomalies may sometimes exist within tiny classes of similar anomalies. The challenge is thus to identify “rare category” records in an unlabeled noisy set with help (in the form of class labels) from a human expert who has a small budget of datapoints that they are prepared to categorize. We propose a technique to meet this challenge, which assumes a mixture model ﬁt to the data, but otherwise makes no assumptions on the particular form of the mixture components. This property promises wide applicability in real-life scenarios and for various statistical models. We give an overview of several alternative methods, highlighting their strengths and weaknesses, and conclude with a detailed empirical analysis. We show that our method can quickly zoom in on an anomaly set containing a few tens of points in a dataset of hundreds of thousands. 1</p><p>2 0.77912384 <a title="15-lda-2" href="./nips-2004-Dynamic_Bayesian_Networks_for_Brain-Computer_Interfaces.html">56 nips-2004-Dynamic Bayesian Networks for Brain-Computer Interfaces</a></p>
<p>Author: Pradeep Shenoy, Rajesh P. Rao</p><p>Abstract: We describe an approach to building brain-computer interfaces (BCI) based on graphical models for probabilistic inference and learning. We show how a dynamic Bayesian network (DBN) can be used to infer probability distributions over brain- and body-states during planning and execution of actions. The DBN is learned directly from observed data and allows measured signals such as EEG and EMG to be interpreted in terms of internal states such as intent to move, preparatory activity, and movement execution. Unlike traditional classiﬁcation-based approaches to BCI, the proposed approach (1) allows continuous tracking and prediction of internal states over time, and (2) generates control signals based on an entire probability distribution over states rather than binary yes/no decisions. We present preliminary results of brain- and body-state estimation using simultaneous EEG and EMG signals recorded during a self-paced left/right hand movement task. 1</p><p>3 0.63929111 <a title="15-lda-3" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>Author: Ofer Dekel, Shai Shalev-shwartz, Yoram Singer</p><p>Abstract: Prediction sufﬁx trees (PST) provide a popular and effective tool for tasks such as compression, classiﬁcation, and language modeling. In this paper we take a decision theoretic view of PSTs for the task of sequence prediction. Generalizing the notion of margin to PSTs, we present an online PST learning algorithm and derive a loss bound for it. The depth of the PST generated by this algorithm scales linearly with the length of the input. We then describe a self-bounded enhancement of our learning algorithm which automatically grows a bounded-depth PST. We also prove an analogous mistake-bound for the self-bounded algorithm. The result is an efﬁcient algorithm that neither relies on a-priori assumptions on the shape or maximal depth of the target PST nor does it require any parameters. To our knowledge, this is the ﬁrst provably-correct PST learning algorithm which generates a bounded-depth PST while being competitive with any ﬁxed PST determined in hindsight. 1</p><p>4 0.63927925 <a title="15-lda-4" href="./nips-2004-Analysis_of_a_greedy_active_learning_strategy.html">23 nips-2004-Analysis of a greedy active learning strategy</a></p>
<p>Author: Sanjoy Dasgupta</p><p>Abstract: We abstract out the core search problem of active learning schemes, to better understand the extent to which adaptive labeling can improve sample complexity. We give various upper and lower bounds on the number of labels which need to be queried, and we prove that a popular greedy active learning rule is approximately as good as any other strategy for minimizing this number of labels. 1</p><p>5 0.63566875 <a title="15-lda-5" href="./nips-2004-Fast_Rates_to_Bayes_for_Kernel_Machines.html">69 nips-2004-Fast Rates to Bayes for Kernel Machines</a></p>
<p>Author: Ingo Steinwart, Clint Scovel</p><p>Abstract: We establish learning rates to the Bayes risk for support vector machines (SVMs) with hinge loss. In particular, for SVMs with Gaussian RBF kernels we propose a geometric condition for distributions which can be used to determine approximation properties of these kernels. Finally, we compare our methods with a recent paper of G. Blanchard et al.. 1</p><p>6 0.63506228 <a title="15-lda-6" href="./nips-2004-Nonparametric_Transforms_of_Graph_Kernels_for_Semi-Supervised_Learning.html">133 nips-2004-Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning</a></p>
<p>7 0.63375169 <a title="15-lda-7" href="./nips-2004-Hierarchical_Clustering_of_a_Mixture_Model.html">77 nips-2004-Hierarchical Clustering of a Mixture Model</a></p>
<p>8 0.63353455 <a title="15-lda-8" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>9 0.63334674 <a title="15-lda-9" href="./nips-2004-A_Probabilistic_Model_for_Online_Document_Clustering_with_Application_to_Novelty_Detection.html">10 nips-2004-A Probabilistic Model for Online Document Clustering with Application to Novelty Detection</a></p>
<p>10 0.63241553 <a title="15-lda-10" href="./nips-2004-Semi-supervised_Learning_with_Penalized_Probabilistic_Clustering.html">167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</a></p>
<p>11 0.63149261 <a title="15-lda-11" href="./nips-2004-Worst-Case_Analysis_of_Selective_Sampling_for_Linear-Threshold_Algorithms.html">206 nips-2004-Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms</a></p>
<p>12 0.63144976 <a title="15-lda-12" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>13 0.63089716 <a title="15-lda-13" href="./nips-2004-Non-Local_Manifold_Tangent_Learning.html">131 nips-2004-Non-Local Manifold Tangent Learning</a></p>
<p>14 0.63021892 <a title="15-lda-14" href="./nips-2004-Neighbourhood_Components_Analysis.html">127 nips-2004-Neighbourhood Components Analysis</a></p>
<p>15 0.62948674 <a title="15-lda-15" href="./nips-2004-Distributed_Information_Regularization_on_Graphs.html">54 nips-2004-Distributed Information Regularization on Graphs</a></p>
<p>16 0.62944049 <a title="15-lda-16" href="./nips-2004-Learning_first-order_Markov_models_for_control.html">102 nips-2004-Learning first-order Markov models for control</a></p>
<p>17 0.62841076 <a title="15-lda-17" href="./nips-2004-Blind_One-microphone_Speech_Separation%3A_A_Spectral_Learning_Approach.html">31 nips-2004-Blind One-microphone Speech Separation: A Spectral Learning Approach</a></p>
<p>18 0.6278677 <a title="15-lda-18" href="./nips-2004-%E2%84%93%E2%82%80-norm_Minimization_for_Basis_Selection.html">207 nips-2004-ℓ₀-norm Minimization for Basis Selection</a></p>
<p>19 0.62777114 <a title="15-lda-19" href="./nips-2004-Assignment_of_Multiplicative_Mixtures_in_Natural_Images.html">25 nips-2004-Assignment of Multiplicative Mixtures in Natural Images</a></p>
<p>20 0.62753606 <a title="15-lda-20" href="./nips-2004-Confidence_Intervals_for_the_Area_Under_the_ROC_Curve.html">45 nips-2004-Confidence Intervals for the Area Under the ROC Curve</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
