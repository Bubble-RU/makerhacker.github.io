<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>38 nips-2004-Co-Validation: Using Model Disagreement on Unlabeled Data to Validate Classification Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-38" href="#">nips2004-38</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>38 nips-2004-Co-Validation: Using Model Disagreement on Unlabeled Data to Validate Classification Algorithms</h1>
<br/><p>Source: <a title="nips-2004-38-pdf" href="http://papers.nips.cc/paper/2603-co-validation-using-model-disagreement-on-unlabeled-data-to-validate-classification-algorithms.pdf">pdf</a></p><p>Author: Omid Madani, David M. Pennock, Gary W. Flake</p><p>Abstract: In the context of binary classiﬁcation, we deﬁne disagreement as a measure of how often two independently-trained models differ in their classiﬁcation of unlabeled data. We explore the use of disagreement for error estimation and model selection. We call the procedure co-validation, since the two models effectively (in)validate one another by comparing results on unlabeled data, which we assume is relatively cheap and plentiful compared to labeled data. We show that per-instance disagreement is an unbiased estimate of the variance of error for that instance. We also show that disagreement provides a lower bound on the prediction (generalization) error, and a tight upper bound on the “variance of prediction error”, or the variance of the average error across instances, where variance is measured across training sets. We present experimental results on several data sets exploring co-validation for error estimation and model selection. The procedure is especially effective in active learning settings, where training sets are not drawn at random and cross validation overestimates error. 1</p><p>Reference: <a title="nips-2004-38-reference" href="../nips2004_reference/nips-2004-Co-Validation%3A_Using_Model_Disagreement_on_Unlabeled_Data_to_Validate_Classification_Algorithms_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract In the context of binary classiﬁcation, we deﬁne disagreement as a measure of how often two independently-trained models differ in their classiﬁcation of unlabeled data. [sent-6, score-0.718]
</p><p>2 We explore the use of disagreement for error estimation and model selection. [sent-7, score-0.782]
</p><p>3 We call the procedure co-validation, since the two models effectively (in)validate one another by comparing results on unlabeled data, which we assume is relatively cheap and plentiful compared to labeled data. [sent-8, score-0.23]
</p><p>4 We show that per-instance disagreement is an unbiased estimate of the variance of error for that instance. [sent-9, score-0.904]
</p><p>5 We also show that disagreement provides a lower bound on the prediction (generalization) error, and a tight upper bound on the “variance of prediction error”, or the variance of the average error across instances, where variance is measured across training sets. [sent-10, score-1.282]
</p><p>6 We present experimental results on several data sets exploring co-validation for error estimation and model selection. [sent-11, score-0.175]
</p><p>7 The procedure is especially effective in active learning settings, where training sets are not drawn at random and cross validation overestimates error. [sent-12, score-0.534]
</p><p>8 All of these methods in some way attempt to estimate or control the prediction (generalization) error of an induced function on unseen data. [sent-22, score-0.236]
</p><p>9 In this paper, we explore a method of error estimation that we call co-validation. [sent-23, score-0.216]
</p><p>10 The method trains two independent functions that in a sense validate (or invalidate) one another by examining their mutual rate of disagreement across a set of unlabeled data. [sent-24, score-0.794]
</p><p>11 For example, empirically we ﬁnd that disagreement goes down  when we increase the training set size, reduce the model’s capacity (complexity), or reduce the inherent difﬁculty of the learning problem. [sent-27, score-0.685]
</p><p>12 Intuitively, the higher the disagreement rate, the higher the average error rate of the learner, where the average is taken over both test instances and training subsets. [sent-28, score-0.969]
</p><p>13 Therefore disagreement is a measure of the ﬁtness of the learner to the learning task. [sent-29, score-0.695]
</p><p>14 However, as researchers have noted in relation to various measures of learner stability in general [Kut02], while robust learners (i. [sent-30, score-0.235]
</p><p>15 In the same vein, we show and explain that the disagreement measure provides only lower bounds on error. [sent-33, score-0.642]
</p><p>16 Still, our empirical results give evidence that disagreement can be a useful estimate in certain circumstances. [sent-34, score-0.599]
</p><p>17 Since we require a source of unlabeled data—preferably a large source in order to accurately measure disagreement—we assume a semi-supervised setting where unlabeled data is relatively cheap and plentiful while labeled data is scarce or expensive. [sent-35, score-0.382]
</p><p>18 In practice, cross validation—especially leave-one-out cross validation—often provides an accurate and reliable error estimate. [sent-38, score-0.395]
</p><p>19 In fact, under the usual assumption that training and test data both arise from the same distribution, k-fold cross validation provides an unbiased estimate of prediction error (for functions trained on m(1 − 1/k) many instances, m being the total number of labeled instances). [sent-39, score-0.8]
</p><p>20 One extreme example of this is active learning, where training samples are explicitly chosen to be maximally informative, using a process that is neither independent nor reﬂective of the test distribution. [sent-41, score-0.271]
</p><p>21 Even beyond active learning, in practice the process of gathering data and obtaining labels often may bias the training set, for example because some inputs are cheaper or easier to label, or are more readily available or obvious to the data collector, etc. [sent-42, score-0.207]
</p><p>22 In these cases, the error estimate obtained from cross validation may not yield an accurate measure of the prediction error of the learned function, and model selection based on cross validation may suffer. [sent-43, score-0.983]
</p><p>23 Empirically we ﬁnd that in active learning settings, disagreement often provides a more accurate estimate of prediction error and is more useful as a guide for model selection. [sent-44, score-0.946]
</p><p>24 Related to the problem of (average) error estimation is the problem of error variance estimation: both variance across test instances and variance across functions (i. [sent-45, score-0.909]
</p><p>25 In this work, we show how disagreement relates to certain measures of variance. [sent-51, score-0.589]
</p><p>26 First, the disagreement on a particular instance provides an unbiased estimate of the variance of error on that instance. [sent-52, score-0.963]
</p><p>27 Second, disagreement provides an upper bound on the variance of prediction error (the type of variance useful for algorithm comparison). [sent-53, score-1.019]
</p><p>28 In § 2 we formally deﬁne disagreement and prove how it lower-bounds prediction error and upper-bounds variance of prediction error. [sent-55, score-0.932]
</p><p>29 In § 3 we empirically explore how error estimates and model selection strategies that we devise based on disagreement compare against cross validation in standard (iid) learning settings and in active learning settings. [sent-56, score-1.224]
</p><p>30 Each instance has a unique true classiﬁcation or label yx ∈ {0, 1}, in general unknown to  the learner. [sent-61, score-0.342]
</p><p>31 Let Z ∗ = {(x, yx )}m be a set of m labeled training instances provided to the learner. [sent-62, score-0.571]
</p><p>32 The learner is an algorithm A : Z ∗ → F , that inputs labeled instances and output a function f ∈ F , where F is the set of all functions (classiﬁers) that A may output (the hypothesis space). [sent-63, score-0.313]
</p><p>33 The goal of the algorithm is to choose f ∈ F to minimize 0/1 error (deﬁned below) on future unlabeled test instances. [sent-65, score-0.298]
</p><p>34 The 0/1 error ex,f of a given function f on a given instance x equals 1 if and only if the function incorrectly classiﬁes the instances, and equals 0 otherwise; that is, e x,f = 1{f (x) = yx }. [sent-67, score-0.535]
</p><p>35 We deﬁne the expected prediction error e of algorithm A as e = Ef,x ef,x , where the expectation is taken over instances drawn from X (x ∼ X ), and functions drawn from F (f ∼ F). [sent-68, score-0.42]
</p><p>36 The variance of prediction error σ 2 is useful for comparing different learners (e. [sent-69, score-0.35]
</p><p>37 f Deﬁne the disagreement between two classiﬁers f1 and f2 on instance x as 1{f1 (x) = f2 (x)}. [sent-76, score-0.603]
</p><p>38 The disagreement rate of learner A is then: d = Ex,f1 ,f2 1{f1 (x) = f2 (x)},  (1)  where recall that the expectation is taken over x ∼ X , f1 ∼ F, f2 ∼ F (with respect to traning sets of some ﬁxed size m). [sent-77, score-0.69]
</p><p>39 Let dx be the (expected) disagreement at x when we sample functions from F: dx = 2 Ef1 ,f2 1{f1 (x) = f2 (x)}. [sent-78, score-0.681]
</p><p>40 Similarly, let ex and σx denote respectively the error and vari2 ance at x: ex = P (f (x) = yx )) = Ef 1{f (x) = yx } = Ef ef,x and σx = V AR(ef ) = 2 Ef [(1{f (x) = yx } − ex ) ] = ex (1 − ex ). [sent-79, score-1.774]
</p><p>41 Furthermore, d = Ex Ef1 ,f2 [1{f1 (x) = 2 f2 (x)}] = Ex dx = 2Ex (σx ) = 2Ex [ex (1 − ex )] = 2(e − Ex e2 ), and therefore: x d = e − E x e2 . [sent-83, score-0.184]
</p><p>42 1  (3)  Bounds on Variance via Disagreement  The variance of prediction error σ 2 can be used to test the signiﬁcance of the difference in two learners’ error rates. [sent-85, score-0.48]
</p><p>43 Bengio and Granvalet [BG03] show that there is no unbiased estimator of the variance of k-fold cross-validation in the supervised setting. [sent-86, score-0.211]
</p><p>44 We can see  from Equation 2 that having access to disagreement at a given instance x (labeled or not) does yield the variance of error at that instance. [sent-87, score-0.841]
</p><p>45 Thus disagreement obtained via 2-fold 2 training gives us an unbaised estimator of σx , the variance of prediction error at instance x, for functions trained on m/2 instances. [sent-88, score-1.091]
</p><p>46 In fact, disagreement yields an upper-bound: Theorem 1 d ≥ 2σ 2 . [sent-92, score-0.566]
</p><p>47 We show that the result holds for any ﬁnite sampling of functions and instances: Consider the binary (0/1) matrix M where the rows correspond to instances and the columns correspond to functions, and the entries are the binary-valued errors (entry Mi,j = 1{fj (xi ) = yxi }). [sent-94, score-0.277]
</p><p>48 Thus the average error is the number of 1 entries when samplings of instances and functions are drawn from X and F respectively, and variances and disagreement can also be readily deﬁned for the matrix. [sent-95, score-0.942]
</p><p>49 For a ﬁxed number of 1 entries N (N ≤ n2 ), we show the difference between disagreement and variance is minimized when the number of edges is maximized. [sent-99, score-0.706]
</p><p>50 We next discuss whether/when el can be far from the actual error, and the related question of whether we can derive a good upperbound or just a good estimator on error using a measure based on disagreement. [sent-112, score-0.245]
</p><p>51 In order to account for weak but stable learners, the error lower bound should be complemented with some measure that ensures that the learner is actually adapting (i. [sent-115, score-0.298]
</p><p>52 We explore using the training (empirical) error for this purpose. [sent-119, score-0.265]
</p><p>53 Let e ˜ 1 denote the average training error of the algorithm: e = Ef ef = Ef m xi ∈Z ∗ 1{f (xi ) = ˜ ˜ yxi }, where Z ∗ is the training set that yielded f . [sent-120, score-0.344]
</p><p>54 Note that a learner can exhibit low disagreement and low training error, yet still have high prediction error. [sent-123, score-0.812]
</p><p>55 For example, the learner could memorize the training data and output a constant on all other instances. [sent-124, score-0.182]
</p><p>56 (Though when disagreement is exactly zero, the test error equals the training error. [sent-125, score-0.856]
</p><p>57 [LBRB02], in conjunction with the empirical training error does yield an upper bound. [sent-127, score-0.224]
</p><p>58 Still, we ﬁnd empirically that, when using SVMs, naive Bayes, or logistic regression, disagreement on unlabeled data does not tend to wildly underestimate error, even though it’s theoretically possible. [sent-128, score-0.72]
</p><p>59 2 For the error estimation experiments, we used linear SVMs with a C value of 10. [sent-144, score-0.175]
</p><p>60 For the model selection experiments, we used polynomial degree as the model selection parameter. [sent-145, score-0.204]
</p><p>61 1  Error Estimation  We ﬁrst examine the use of disagreement for error estimation both in the standard setting where training and test samples are uniformly iid, and in an active learning scenario. [sent-147, score-0.987]
</p><p>62 For each of several training set sizes for each data set, we computed average results and standard deviation across thirty trials. [sent-148, score-0.197]
</p><p>63 In each trial, we ﬁrst generate a training set, sampled either uniformly iid or actively, then set aside 20% of remaining instances as the test set. [sent-149, score-0.311]
</p><p>64 Next, we partition the training set into equal halves, train an SVM on each half, and compute the disagreement rate between the two SVMs across the set of (unlabeled) data that has not been designated for the training or test set (80% of total − m instances). [sent-150, score-0.817]
</p><p>65 We repeat this inner loop of partitioning, dual training, and disagreement computation thirty times and take averages. [sent-151, score-0.636]
</p><p>66 We examined the utility of our disagreement bound (4) as an estimate of the true test error of the algorithm trained on the full data set (“trueE”). [sent-152, score-0.853]
</p><p>67 We also examined using the maximum of the training error (“trainE”) and lower bound on error from our disagreement measure (“disE”) as an estimate of trueE (“MaxDtE = max(trainE, disE)”). [sent-153, score-1.049]
</p><p>68 Note that disE and trainE are respectively unbiased empirical estimates of expected disagreement d and expected training error e of § 2 for the standard setting. [sent-154, score-0.857]
</p><p>69 Since our disagreement measure is actually ˜ a bound on half error (i. [sent-155, score-0.767]
</p><p>70 , error averaged over training sets of size m/2), we also compare against two-fold cross-validation error (“2cvE”), and the true test error of the two functions obtained from training on the two halves (“1/2trueE”). [sent-157, score-0.714]
</p><p>71 com/resources/testcollections/ We observed similar results in error estimation using linear logistic regression and Naive Bayes learners in preliminary experiments. [sent-163, score-0.223]
</p><p>72 disE In the standard scenario, when the training set is chosen uniformly at random from the corpus, leave-one-out cross validated error (“looE”) is generally a very good estimate of trueE, while 2cvE is a good estimate for 1/2trueE. [sent-193, score-0.407]
</p><p>73 In the active learning scenario, the training set is chosen in an attempt to maximize information, and the choice of each new instance depends on the set of previously chosen instances. [sent-196, score-0.244]
</p><p>74 Often this means that especially difﬁcult instances are chosen (or at least instances whose labels are difﬁcult to infer from the current training set). [sent-197, score-0.365]
</p><p>75 Thus cross validation naturally overestimates the difﬁculty of the learning task and so may greatly overestimate error. [sent-198, score-0.33]
</p><p>76 On the other hand, an approximate model of active learning is that the instances are iid sampled from a hard distribution. [sent-199, score-0.309]
</p><p>77 Measuring disagreement on the easier test distribution via subsampling the training set may remain a good estimator of the actual test error. [sent-201, score-0.798]
</p><p>78 In all the datasets experimented with, we have observed the same pattern: the error estimate using disagreement provides a much better estimate of 1/2trueE and trueE than does 2cvE (Fig. [sent-207, score-0.821]
</p><p>79 2a), and can be used as an indication of the error and the progress of active learning. [sent-208, score-0.261]
</p><p>80 The estimation performance may degrade towards the end of active learning when the learner converges (disagreement approaches 0). [sent-213, score-0.255]
</p><p>81 However, we have observed that both 1/2trueE (obtained via subsampling) and disE tend to overestimate the actual error of the active learner even at half the training size (e. [sent-214, score-0.494]
</p><p>82 3 We could use a criterion based on disagreement for selective sampling, but we have not throughly explored this option. [sent-219, score-0.566]
</p><p>83 1  (b) maxDtE  Figure 3: (a) An example were maxDtE performs particularly well as a model selection criteria, tracking the true error curve more closely than looE or 2cvE. [sent-240, score-0.207]
</p><p>84 2  Model Selection  We explore various criteria for selecting the expected best among twenty SVMs, each trained using a different polynomial degree kernel. [sent-243, score-0.214]
</p><p>85 For each data set, we manually identify an interval of polynomial degrees that seems to include the error minimum 4 , then choose twenty degrees equally spaced within that interval. [sent-244, score-0.273]
</p><p>86 We compare our disagreement-based estimate maxDtE with the cross validation estimates looE and 2cvE as model selection criteria. [sent-245, score-0.355]
</p><p>87 In each trial, we identify the polynomial degree that is expected to be best according to each criteria, then train an SVM at that degree on the full training set. [sent-246, score-0.206]
</p><p>88 In the standard uniform iid scenario, though cross validation often does fail as a model selection criteria for regression problems, it seems that cross validation in general is hard to beat for classiﬁcation problems [SS02]. [sent-248, score-0.672]
</p><p>89 We are exploring using the maximum of cross validation and maxDtE as an alternative with preliminary evidence of a slight advantage over cross validation alone. [sent-250, score-0.508]
</p><p>90 In an active learning setting, even though cross validation overestimates error, it is theoretically possible that cross validation would still function well to identify the best or near-best model. [sent-251, score-0.703]
</p><p>91 However, our experiments suggest that the performance of cross validation as a model selection criteria indeed degrades under active learning. [sent-252, score-0.493]
</p><p>92 The active learning model selection experiments proceed as follows. [sent-255, score-0.19]
</p><p>93 For each training size m ∈ {25, 50, 100, 200}, we run thirty experiments using a random shufﬂing of the size-m preﬁx of the 200 actively-picked instances. [sent-257, score-0.182]
</p><p>94 In each trial and for each of the twenty polynomial degrees, we measure trueE and looE, then run an inner loop of thirty random partitionings and dual trainings to measure average d, expE, 2cvE, and 1/2trueE. [sent-258, score-0.231]
</p><p>95 We observe that model selection based on disagreement often outperforms model selection based on cross-validation, and at times signiﬁcantly so. [sent-261, score-0.702]
</p><p>96 4  Related Work  Previous work has already shown that using various measures of stability on unlabeled data is useful for ensemble learning, model selection, and regularization, both in supervised and unsupervised learning [KV95, Sch97, SS02, BC03, LBRB02, LRBB04]. [sent-265, score-0.233]
</p><p>97 al [LBRB02, LRBB04] also explore disagreement on unlabeled data, establishing robust model selection techniques based on disagreement for clustering. [sent-269, score-1.361]
</p><p>98 Theoretical work on algorithmic stability focuses on deriving generalization bounds given that the algorithm has certain inherent stability properties [KN02]. [sent-270, score-0.18]
</p><p>99 In this paper we derived bounds on certain measures of error and variance based on disagreement, then examined empirically when co-validation might be useful. [sent-272, score-0.342]
</p><p>100 No unbiased estimator of the variance of k-fold cross-validation. [sent-284, score-0.211]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('disagreement', 0.566), ('yx', 0.305), ('maxdte', 0.297), ('truee', 0.28), ('dise', 0.21), ('looe', 0.192), ('ex', 0.144), ('instances', 0.14), ('error', 0.139), ('validation', 0.137), ('active', 0.122), ('unlabeled', 0.12), ('cross', 0.117), ('variance', 0.099), ('learner', 0.097), ('traine', 0.087), ('training', 0.085), ('baseball', 0.076), ('optics', 0.076), ('acq', 0.07), ('earn', 0.07), ('thirty', 0.07), ('adult', 0.069), ('chess', 0.069), ('votes', 0.069), ('selection', 0.068), ('unbiased', 0.067), ('stability', 0.067), ('prediction', 0.064), ('documents', 0.055), ('overestimates', 0.052), ('criteria', 0.049), ('learners', 0.048), ('iid', 0.047), ('plentiful', 0.046), ('religion', 0.046), ('estimator', 0.045), ('svms', 0.043), ('across', 0.042), ('explore', 0.041), ('entries', 0.041), ('labeled', 0.041), ('dx', 0.04), ('bengio', 0.04), ('test', 0.039), ('instance', 0.037), ('scenario', 0.036), ('polynomial', 0.036), ('estimation', 0.036), ('baseballvshockey', 0.035), ('degress', 0.035), ('madani', 0.035), ('yxi', 0.035), ('scenarios', 0.035), ('twenty', 0.035), ('functions', 0.035), ('empirically', 0.034), ('estimate', 0.033), ('degree', 0.032), ('measure', 0.032), ('validate', 0.031), ('bow', 0.03), ('braun', 0.03), ('bound', 0.03), ('transductive', 0.03), ('el', 0.029), ('digit', 0.029), ('datasets', 0.028), ('size', 0.027), ('equals', 0.027), ('record', 0.026), ('trial', 0.026), ('errors', 0.026), ('lang', 0.026), ('halves', 0.026), ('newsgroups', 0.026), ('extreme', 0.025), ('examined', 0.025), ('overestimate', 0.024), ('subsampling', 0.024), ('lange', 0.024), ('pasadena', 0.024), ('algorithmic', 0.024), ('svm', 0.024), ('library', 0.023), ('ensemble', 0.023), ('versus', 0.023), ('cheap', 0.023), ('measures', 0.023), ('classi', 0.022), ('bounds', 0.022), ('provides', 0.022), ('libsvm', 0.021), ('actively', 0.021), ('degrees', 0.021), ('trained', 0.021), ('drawn', 0.021), ('events', 0.021), ('identify', 0.021), ('roth', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="38-tfidf-1" href="./nips-2004-Co-Validation%3A_Using_Model_Disagreement_on_Unlabeled_Data_to_Validate_Classification_Algorithms.html">38 nips-2004-Co-Validation: Using Model Disagreement on Unlabeled Data to Validate Classification Algorithms</a></p>
<p>Author: Omid Madani, David M. Pennock, Gary W. Flake</p><p>Abstract: In the context of binary classiﬁcation, we deﬁne disagreement as a measure of how often two independently-trained models differ in their classiﬁcation of unlabeled data. We explore the use of disagreement for error estimation and model selection. We call the procedure co-validation, since the two models effectively (in)validate one another by comparing results on unlabeled data, which we assume is relatively cheap and plentiful compared to labeled data. We show that per-instance disagreement is an unbiased estimate of the variance of error for that instance. We also show that disagreement provides a lower bound on the prediction (generalization) error, and a tight upper bound on the “variance of prediction error”, or the variance of the average error across instances, where variance is measured across training sets. We present experimental results on several data sets exploring co-validation for error estimation and model selection. The procedure is especially effective in active learning settings, where training sets are not drawn at random and cross validation overestimates error. 1</p><p>2 0.11948944 <a title="38-tfidf-2" href="./nips-2004-Semi-supervised_Learning_by_Entropy_Minimization.html">164 nips-2004-Semi-supervised Learning by Entropy Minimization</a></p>
<p>Author: Yves Grandvalet, Yoshua Bengio</p><p>Abstract: We consider the semi-supervised learning problem, where a decision rule is to be learned from labeled and unlabeled data. In this framework, we motivate minimum entropy regularization, which enables to incorporate unlabeled data in the standard supervised learning. Our approach includes other approaches to the semi-supervised problem as particular or limiting cases. A series of experiments illustrates that the proposed solution beneﬁts from unlabeled data. The method challenges mixture models when the data are sampled from the distribution class spanned by the generative model. The performances are deﬁnitely in favor of minimum entropy regularization when generative models are misspeciﬁed, and the weighting of unlabeled data provides robustness to the violation of the “cluster assumption”. Finally, we also illustrate that the method can also be far superior to manifold learning in high dimension spaces. 1</p><p>3 0.086296521 <a title="38-tfidf-3" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>Author: Saharon Rosset, Ji Zhu, Hui Zou, Trevor J. Hastie</p><p>Abstract: We consider the situation in semi-supervised learning, where the “label sampling” mechanism stochastically depends on the true response (as well as potentially on the features). We suggest a method of moments for estimating this stochastic dependence using the unlabeled data. This is potentially useful for two distinct purposes: a. As an input to a supervised learning procedure which can be used to “de-bias” its results using labeled data only and b. As a potentially interesting learning task in itself. We present several examples to illustrate the practical usefulness of our method.</p><p>4 0.080711469 <a title="38-tfidf-4" href="./nips-2004-Analysis_of_a_greedy_active_learning_strategy.html">23 nips-2004-Analysis of a greedy active learning strategy</a></p>
<p>Author: Sanjoy Dasgupta</p><p>Abstract: We abstract out the core search problem of active learning schemes, to better understand the extent to which adaptive labeling can improve sample complexity. We give various upper and lower bounds on the number of labels which need to be queried, and we prove that a popular greedy active learning rule is approximately as good as any other strategy for minimizing this number of labels. 1</p><p>5 0.077626541 <a title="38-tfidf-5" href="./nips-2004-Breaking_SVM_Complexity_with_Cross-Training.html">34 nips-2004-Breaking SVM Complexity with Cross-Training</a></p>
<p>Author: Léon Bottou, Jason Weston, Gökhan H. Bakir</p><p>Abstract: We propose to selectively remove examples from the training set using probabilistic estimates related to editing algorithms (Devijver and Kittler, 1982). This heuristic procedure aims at creating a separable distribution of training examples with minimal impact on the position of the decision boundary. It breaks the linear dependency between the number of SVs and the number of training examples, and sharply reduces the complexity of SVMs during both the training and prediction stages. 1</p><p>6 0.074071467 <a title="38-tfidf-6" href="./nips-2004-Result_Analysis_of_the_NIPS_2003_Feature_Selection_Challenge.html">156 nips-2004-Result Analysis of the NIPS 2003 Feature Selection Challenge</a></p>
<p>7 0.072485305 <a title="38-tfidf-7" href="./nips-2004-Distributed_Information_Regularization_on_Graphs.html">54 nips-2004-Distributed Information Regularization on Graphs</a></p>
<p>8 0.071026161 <a title="38-tfidf-8" href="./nips-2004-On_Semi-Supervised_Classification.html">136 nips-2004-On Semi-Supervised Classification</a></p>
<p>9 0.064752065 <a title="38-tfidf-9" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>10 0.061493535 <a title="38-tfidf-10" href="./nips-2004-Semi-supervised_Learning_via_Gaussian_Processes.html">166 nips-2004-Semi-supervised Learning via Gaussian Processes</a></p>
<p>11 0.057102177 <a title="38-tfidf-11" href="./nips-2004-Nonparametric_Transforms_of_Graph_Kernels_for_Semi-Supervised_Learning.html">133 nips-2004-Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning</a></p>
<p>12 0.055246867 <a title="38-tfidf-12" href="./nips-2004-Maximum_Margin_Clustering.html">115 nips-2004-Maximum Margin Clustering</a></p>
<p>13 0.054290995 <a title="38-tfidf-13" href="./nips-2004-Generalization_Error_Bounds_for_Collaborative_Prediction_with_Low-Rank_Matrices.html">71 nips-2004-Generalization Error Bounds for Collaborative Prediction with Low-Rank Matrices</a></p>
<p>14 0.052523293 <a title="38-tfidf-14" href="./nips-2004-Object_Classification_from_a_Single_Example_Utilizing_Class_Relevance_Metrics.html">134 nips-2004-Object Classification from a Single Example Utilizing Class Relevance Metrics</a></p>
<p>15 0.0520317 <a title="38-tfidf-15" href="./nips-2004-Variational_Minimax_Estimation_of_Discrete_Distributions_under_KL_Loss.html">204 nips-2004-Variational Minimax Estimation of Discrete Distributions under KL Loss</a></p>
<p>16 0.051010992 <a title="38-tfidf-16" href="./nips-2004-Active_Learning_for_Anomaly_and_Rare-Category_Detection.html">15 nips-2004-Active Learning for Anomaly and Rare-Category Detection</a></p>
<p>17 0.050031271 <a title="38-tfidf-17" href="./nips-2004-Density_Level_Detection_is_Classification.html">49 nips-2004-Density Level Detection is Classification</a></p>
<p>18 0.048879344 <a title="38-tfidf-18" href="./nips-2004-Learning_Preferences_for_Multiclass_Problems.html">100 nips-2004-Learning Preferences for Multiclass Problems</a></p>
<p>19 0.048597019 <a title="38-tfidf-19" href="./nips-2004-Maximum-Margin_Matrix_Factorization.html">113 nips-2004-Maximum-Margin Matrix Factorization</a></p>
<p>20 0.046789207 <a title="38-tfidf-20" href="./nips-2004-Trait_Selection_for_Assessing_Beef_Meat_Quality_Using_Non-linear_SVM.html">195 nips-2004-Trait Selection for Assessing Beef Meat Quality Using Non-linear SVM</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.164), (1, 0.062), (2, -0.007), (3, 0.064), (4, 0.006), (5, 0.07), (6, 0.037), (7, 0.073), (8, 0.04), (9, -0.017), (10, -0.024), (11, 0.062), (12, 0.044), (13, -0.048), (14, -0.053), (15, 0.003), (16, 0.045), (17, -0.046), (18, 0.017), (19, -0.082), (20, 0.048), (21, 0.057), (22, 0.066), (23, 0.067), (24, -0.019), (25, 0.066), (26, -0.031), (27, 0.084), (28, 0.055), (29, -0.015), (30, 0.037), (31, -0.003), (32, 0.146), (33, 0.039), (34, -0.02), (35, -0.003), (36, 0.04), (37, 0.044), (38, -0.101), (39, 0.129), (40, 0.016), (41, -0.055), (42, 0.081), (43, -0.053), (44, -0.076), (45, -0.023), (46, 0.044), (47, -0.038), (48, -0.009), (49, -0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93456942 <a title="38-lsi-1" href="./nips-2004-Co-Validation%3A_Using_Model_Disagreement_on_Unlabeled_Data_to_Validate_Classification_Algorithms.html">38 nips-2004-Co-Validation: Using Model Disagreement on Unlabeled Data to Validate Classification Algorithms</a></p>
<p>Author: Omid Madani, David M. Pennock, Gary W. Flake</p><p>Abstract: In the context of binary classiﬁcation, we deﬁne disagreement as a measure of how often two independently-trained models differ in their classiﬁcation of unlabeled data. We explore the use of disagreement for error estimation and model selection. We call the procedure co-validation, since the two models effectively (in)validate one another by comparing results on unlabeled data, which we assume is relatively cheap and plentiful compared to labeled data. We show that per-instance disagreement is an unbiased estimate of the variance of error for that instance. We also show that disagreement provides a lower bound on the prediction (generalization) error, and a tight upper bound on the “variance of prediction error”, or the variance of the average error across instances, where variance is measured across training sets. We present experimental results on several data sets exploring co-validation for error estimation and model selection. The procedure is especially effective in active learning settings, where training sets are not drawn at random and cross validation overestimates error. 1</p><p>2 0.7233808 <a title="38-lsi-2" href="./nips-2004-Semi-supervised_Learning_by_Entropy_Minimization.html">164 nips-2004-Semi-supervised Learning by Entropy Minimization</a></p>
<p>Author: Yves Grandvalet, Yoshua Bengio</p><p>Abstract: We consider the semi-supervised learning problem, where a decision rule is to be learned from labeled and unlabeled data. In this framework, we motivate minimum entropy regularization, which enables to incorporate unlabeled data in the standard supervised learning. Our approach includes other approaches to the semi-supervised problem as particular or limiting cases. A series of experiments illustrates that the proposed solution beneﬁts from unlabeled data. The method challenges mixture models when the data are sampled from the distribution class spanned by the generative model. The performances are deﬁnitely in favor of minimum entropy regularization when generative models are misspeciﬁed, and the weighting of unlabeled data provides robustness to the violation of the “cluster assumption”. Finally, we also illustrate that the method can also be far superior to manifold learning in high dimension spaces. 1</p><p>3 0.62641996 <a title="38-lsi-3" href="./nips-2004-Analysis_of_a_greedy_active_learning_strategy.html">23 nips-2004-Analysis of a greedy active learning strategy</a></p>
<p>Author: Sanjoy Dasgupta</p><p>Abstract: We abstract out the core search problem of active learning schemes, to better understand the extent to which adaptive labeling can improve sample complexity. We give various upper and lower bounds on the number of labels which need to be queried, and we prove that a popular greedy active learning rule is approximately as good as any other strategy for minimizing this number of labels. 1</p><p>4 0.61279309 <a title="38-lsi-4" href="./nips-2004-Active_Learning_for_Anomaly_and_Rare-Category_Detection.html">15 nips-2004-Active Learning for Anomaly and Rare-Category Detection</a></p>
<p>Author: Dan Pelleg, Andrew W. Moore</p><p>Abstract: We introduce a novel active-learning scenario in which a user wants to work with a learning algorithm to identify useful anomalies. These are distinguished from the traditional statistical deﬁnition of anomalies as outliers or merely ill-modeled points. Our distinction is that the usefulness of anomalies is categorized subjectively by the user. We make two additional assumptions. First, there exist extremely few useful anomalies to be hunted down within a massive dataset. Second, both useful and useless anomalies may sometimes exist within tiny classes of similar anomalies. The challenge is thus to identify “rare category” records in an unlabeled noisy set with help (in the form of class labels) from a human expert who has a small budget of datapoints that they are prepared to categorize. We propose a technique to meet this challenge, which assumes a mixture model ﬁt to the data, but otherwise makes no assumptions on the particular form of the mixture components. This property promises wide applicability in real-life scenarios and for various statistical models. We give an overview of several alternative methods, highlighting their strengths and weaknesses, and conclude with a detailed empirical analysis. We show that our method can quickly zoom in on an anomaly set containing a few tens of points in a dataset of hundreds of thousands. 1</p><p>5 0.57138819 <a title="38-lsi-5" href="./nips-2004-Semi-supervised_Learning_via_Gaussian_Processes.html">166 nips-2004-Semi-supervised Learning via Gaussian Processes</a></p>
<p>Author: Neil D. Lawrence, Michael I. Jordan</p><p>Abstract: We present a probabilistic approach to learning a Gaussian Process classiﬁer in the presence of unlabeled data. Our approach involves a “null category noise model” (NCNM) inspired by ordered categorical noise models. The noise model reﬂects an assumption that the data density is lower between the class-conditional densities. We illustrate our approach on a toy problem and present comparative results for the semi-supervised classiﬁcation of handwritten digits. 1</p><p>6 0.55197376 <a title="38-lsi-6" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>7 0.55000782 <a title="38-lsi-7" href="./nips-2004-On_Semi-Supervised_Classification.html">136 nips-2004-On Semi-Supervised Classification</a></p>
<p>8 0.52415973 <a title="38-lsi-8" href="./nips-2004-Result_Analysis_of_the_NIPS_2003_Feature_Selection_Challenge.html">156 nips-2004-Result Analysis of the NIPS 2003 Feature Selection Challenge</a></p>
<p>9 0.4707852 <a title="38-lsi-9" href="./nips-2004-Distributed_Information_Regularization_on_Graphs.html">54 nips-2004-Distributed Information Regularization on Graphs</a></p>
<p>10 0.46579188 <a title="38-lsi-10" href="./nips-2004-A_Machine_Learning_Approach_to_Conjoint_Analysis.html">8 nips-2004-A Machine Learning Approach to Conjoint Analysis</a></p>
<p>11 0.45164588 <a title="38-lsi-11" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>12 0.43217245 <a title="38-lsi-12" href="./nips-2004-Trait_Selection_for_Assessing_Beef_Meat_Quality_Using_Non-linear_SVM.html">195 nips-2004-Trait Selection for Assessing Beef Meat Quality Using Non-linear SVM</a></p>
<p>13 0.40063018 <a title="38-lsi-13" href="./nips-2004-Maximal_Margin_Labeling_for_Multi-Topic_Text_Categorization.html">111 nips-2004-Maximal Margin Labeling for Multi-Topic Text Categorization</a></p>
<p>14 0.4004178 <a title="38-lsi-14" href="./nips-2004-Schema_Learning%3A_Experience-Based_Construction_of_Predictive_Action_Models.html">159 nips-2004-Schema Learning: Experience-Based Construction of Predictive Action Models</a></p>
<p>15 0.38874954 <a title="38-lsi-15" href="./nips-2004-Maximum_Margin_Clustering.html">115 nips-2004-Maximum Margin Clustering</a></p>
<p>16 0.37990081 <a title="38-lsi-16" href="./nips-2004-Neighbourhood_Components_Analysis.html">127 nips-2004-Neighbourhood Components Analysis</a></p>
<p>17 0.37469006 <a title="38-lsi-17" href="./nips-2004-Learning_Preferences_for_Multiclass_Problems.html">100 nips-2004-Learning Preferences for Multiclass Problems</a></p>
<p>18 0.3677114 <a title="38-lsi-18" href="./nips-2004-A_Topographic_Support_Vector_Machine%3A_Classification_Using_Local_Label_Configurations.html">14 nips-2004-A Topographic Support Vector Machine: Classification Using Local Label Configurations</a></p>
<p>19 0.36239505 <a title="38-lsi-19" href="./nips-2004-Breaking_SVM_Complexity_with_Cross-Training.html">34 nips-2004-Breaking SVM Complexity with Cross-Training</a></p>
<p>20 0.35075784 <a title="38-lsi-20" href="./nips-2004-Common-Frame_Model_for_Object_Recognition.html">40 nips-2004-Common-Frame Model for Object Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.067), (15, 0.106), (26, 0.056), (31, 0.044), (33, 0.184), (35, 0.035), (39, 0.037), (50, 0.076), (77, 0.29)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84729439 <a title="38-lda-1" href="./nips-2004-Optimal_sub-graphical_models.html">141 nips-2004-Optimal sub-graphical models</a></p>
<p>Author: Mukund Narasimhan, Jeff A. Bilmes</p><p>Abstract: We investigate the problem of reducing the complexity of a graphical model (G, PG ) by ﬁnding a subgraph H of G, chosen from a class of subgraphs H, such that H is optimal with respect to KL-divergence. We do this by ﬁrst deﬁning a decomposition tree representation for G, which is closely related to the junction-tree representation for G. We then give an algorithm which uses this representation to compute the optimal H ∈ H. Gavril [2] and Tarjan [3] have used graph separation properties to solve several combinatorial optimization problems when the size of the minimal separators in the graph is bounded. We present an extension of this technique which applies to some important choices of H even when the size of the minimal separators of G are arbitrarily large. In particular, this applies to problems such as ﬁnding an optimal subgraphical model over a (k − 1)-tree of a graphical model over a k-tree (for arbitrary k) and selecting an optimal subgraphical model with (a constant) d fewer edges with respect to KL-divergence can be solved in time polynomial in |V (G)| using this formulation. 1 Introduction and Preliminaries The complexity of inference in graphical models is typically exponential in some parameter of the graph, such as the size of the largest clique. Therefore, it is often required to ﬁnd a subgraphical model that has lower complexity (smaller clique size) without introducing a large error in inference results. The KL-divergence between the original probability distribution and the probability distribution on the simpliﬁed graphical model is often used to measure the impact on inference. Existing techniques for reducing the complexity of graphical models including annihilation and edge-removal [4] are greedy in nature and cannot make any guarantees regarding the optimality of the solution. This problem is NP-complete [9] and so, in general, one cannot expect a polynomial time algorithm to ﬁnd the optimal solution. However, we show that when we restrict the problem to some sets of subgraphs, the optimal solution can be found quite quickly using a dynamic programming algorithm in time polynomial in the tree-width of the graph. 1.1 Notation and Terminology A graph G = (V, E) is said to be triangulated if every cycle of length greater than 3 has a chord. A clique of G is a non-empty set S ⊆ V such that {a, b} ∈ E for all ∗ This work was supported by NSF grant IIS-0093430 and an Intel Corporation Grant. {b, c, d} d {c, f, g} {b, c} {b, e, c} b c {f, c} {c, e} {e, c, f } g {b, e} a e f {a, b, e} Figure 1: A triangulated graph G and a junction-tree for G a, b ∈ S. A clique S is maximal if S is not properly contained in another clique. If α and β are non-adjacent vertices of G then a set of vertices S ⊆ V \ {α, β} is called an (α, β)-separator if α and β are in distinct components of G[V \ S]. S is a minimal (α, β)-separator if no proper subset of S is an (α, β)-separator. S is said to be a minimal separator if S is a minimal (α, β)-separator for some non adjacent a, b ∈ V . If T = (K, S) is a junction-tree for G (see [7]), then the nodes K of T correspond to the maximalcliques of G, while the links S correspond to minimal separators of G (We reserve the terms vertices/edges for elements of G, and nodes/links for the elements of T ). If G is triangulated, then the number of maximal cliques is at most |V |. For example, in the graph G shown in Figure 1, K = {{b, c, d} , {a, b, e} , {b, e, c} , {e, c, f } , {c, f, g}}. The links S of T correspond to minimal-separators of G in the following way. If Vi Vj ∈ S (where Vi , Vj ∈ K and hence are cliques of G), then Vi ∩ Vj = φ. We label each edge Vi Vj ∈ S with the set Vij = Vi ∩ Vj , which is a non-empty complete separator in G. The removal of any link Vi Vj ∈ S disconnects T into two subtrees which we denote T (i) and T (j) (chosen so that T (i) contains Vi ). We will let K(i) be the nodes of T (i) , and V (i) = ∪V ∈K (i) V be the set of vertices corresponding to the subtree T (i) . The junction tree property ensures that V (i) ∩ V (j) = Vi ∩ Vj = Vij . We will let G(i) be the subgraph induced by V (i) . A graphical model is a pair (G, P ) where P is the joint probability distribution for random variables X1 , X2 , . . . , Xn , and G is a graph with vertex set V (G) = {X1 , X2 , . . . , Xn } such that the separators in G imply conditional independencies in P (so P factors according to G). If G is triangulated, then the junction-tree algorithm can be used for exact inference in the probability distribution P . The complexity of this algorithm grows with the treewidth of G (which is one less than the size of the largest clique in G when G is triangulated). The growth is exponential when P is a discrete probability distribution, thus rendering exact inference for graphs with large treewidth impractical. Therefore, we seek another graphical model (H, PH ) which allows tractable inference (so H should have lower treewidth than G has). The general problem of ﬁnding a graphical model of tree-width at most k so as to minimize the KL-divergence from a speciﬁed probability distribution is NP complete for general k ([9]) However, it is known that this problem is solvable in polynomial time (in |V (G)|) for some special cases cases (such as when G has bounded treewidth or when k = 1 [1]). If (G, PG ) and (H, PH ) are graphical models, then we say that (H, PH ) is a subgraphical model of (G, PG ) if H is a spanning subgraph of G. Note in particular that separators in G are separators in H, and hence (G, PH ) is also a graphical model. 2 Graph Decompositions and Divide-and-Conquer Algorithms For the remainder of the paper, we will be assuming that G = (V, E) is some triangulated graph, with junction tree T = (K, S). As observed above, if Vi Vj ∈ S, then the removal {b, c, d} d {b, c} {b, e, c} b c c {c, f, g} {f, c} {e, c, f } g {b, e} a e e f {a, b, e} Figure 2: The graphs G(i) , G(j) and junction-trees T (i) and T (j) resulting from the removal of the link Vij = {c, e} of Vij = Vi ∩ Vj disconnects G into two (vertex-induced) subgraphs G(i) and G(j) which are both triangulated, with junction-trees T (i) and T (j) respectively. We can recursively decompose each of G(i) and G(j) into smaller and smaller subgraphs till the resulting subgraphs are cliques. When the size of all the minimal separators are bounded, we may use these decompositions to easily solve problems that are hard in general. For example, in [5] it is shown that NP-complete problems like vertex coloring, and ﬁnding maximum independent sets can be solved in polynomial time on graphs with bounded tree-width (which are equivalent to spanning graphs with bounded size separators). We will be interested in ﬁnding (triangulated) subgraphs of G that satisfy some conditions, such as a bound on the number of edges, or a bound on the tree-width and which optimize separable objective functions (described in Section 2) One reason why problems such as this can often be solved easily when the tree-width of G is bounded by some constant is this : If Vij is a separator decomposing G into G(i) and G(j) , then a divide-and-conquer approach would suggest that we try and ﬁnd optimal subgraphs of G(i) and G(j) and then splice the two together to get an optimal subgraph of G. There are two issues with this approach. First, the optimal subgraphs of G (i) and G(j) need not necessarily match up on Vij , the set of common vertices. Second, even if the two subgraphs agree on the set of common vertices, the graph resulting from splicing the two subgraphs together need not be triangulated (which could happen even if the two subgraphs individually are triangulated). To rectify the situation, we can do the following. We partition the set of subgraphs of G(i) and G(j) into classes, so that any subgraph of G(i) and any subgraph G(j) corresponding to the same class are compatible in the sense that they match up on their intersection namely Vij , and so that by splicing the two subgraphs together, we get a subgraph of G which is acceptable (and in particular is triangulated). Then given optimal subgraphs of both G(i) and G(j) corresponding to each class, we can enumerate over all the classes and pick the best one. Of course, to ensure that we do not repeatedly solve the same problem, we need to work bottom-up (a.k.a dynamic programming) or memoize our solutions. This procedure can be carried out in polynomial (in |V |) time as long as we have only a polynomial number of classes. Now, if we have a polynomial number of classes, these classes need not actually be a partition of all the acceptable subgraphs, though the union of the classes must cover all acceptable subgraphs (so the same subgraph can be contained in more than one class). For our application, every class can be thought of to be the set of subgraphs that satisfy some constraint, and we need to pick a polynomial number of constraints that cover all possibilities. The bound on the tree-width helps us here. If k |Vij | = k, then in any subgraph H of G, H[Vij ] must be one of the 2(2) possible subgraphs k of G[Vij ]. So, if k is sufﬁciently small (so 2(2) is bounded by some polynomial in |V |), then this procedure results in a polynomial time algorithm. In this paper, we show that in some cases we can characterize the space H so that we still have a polynomial number of constraints even when the tree-width of G is not bounded by a small constant. 2.1 Separable objective functions For cases where exact inference in the graphical model (G, PG ) is intractable, it is natural to try to ﬁnd a subgraphical model (H, PH ) such that D(PG PH ) is minimized, and inference using H is tractable. We will denote by H the set of subgraphs of G that are tractable for inference. For example, this set could be the set of subgraphs of G with treewidth one less than the treewidth of G, or perhaps the set of subgraphs of G with at d fewer edges. For a speciﬁed subgraph H of G, there is a unique probability distribution PH factoring over H that minimizes D(PG PH ). Hence, ﬁnding a optimal subgraphical model is equivalent to ﬁnding a subgraph H for which D(PG PH ) is minimized. If Vij is a separator of G, we will attempt to ﬁnd optimal subgraphs of G by ﬁnding optimal subgraphs of G (i) and G(j) and splicing them together. However, to do this, we need to ensure that the objective criteria also decomposes along the separator Vij . Suppose that H is any triangulated subgraph of G. Let PG(i) and PG(j) be the (marginalized) distributions of PG on V (i) and V (j) respectively, and PH (i) and PH (j) be the (marginalized) distributions of the distribution PH on V (i) and V (j) where H (i) = H[V (i) ] and H (j) = H[V (j) ], The following result assures us that the KL-divergence also factors according to the separator Vij . Lemma 1. Suppose that (G, PG ) is a graphical model, H is a triangulated subgraph of G, and PH factors over H. Then D(PG PH ) = D(PG(i) PH (i) ) + D(PG(j) PH (j) ) − D(PG[Vij ] PH[Vij ] ). Proof. Since H is a subgraph of G, and Vij is a separator of G, Vij must also be a separator of H. Therefore, PH {Xv }v∈V = PH (i) ({Xv }v∈V (i) )·PH (j) ({Xv }v∈V (j) ) . PH[Vij ] ({Xv }v∈V ) The result ij follows immediately. Therefore, there is hope that we can reduce our our original problem of ﬁnding an optimal subgraph H ∈ H as one of ﬁnding subgraphs of H (i) ⊆ G(i) and H (j) ⊆ G(j) that are compatible, in the sense that they match up on the overlap Vij , and for which D(PG PH ) is minimized. Throughout this paper, for the sake of concreteness, we will assume that the objective criterion is to minimize the KL-divergence. However, all the results can be extended to other objective functions, as long as they “separate” in the sense that for any separator, the objective function is the sum of the objective functions of the two parts, possibly modulo some correction factor which is purely a function of the separator. Another example might be the complexity r(H) of representing the graphical model H. A very natural representation satisﬁes r(G) = r(G(i) ) + r(G(j) ) if G has a separator G(i) ∩ G(j) . Therefore, the representation cost reduction would satisfy r(G) − r(H) = (r(G (i) ) − r(H (i) )) + (r(G(j) ) − r(H (j) )), and so also factors according to the separators. Finally note that any linear combinations of such separable functions is also separable, and so this technique could also be used to determine tradeoffs (representation cost vs. KL-divergence loss for example). In Section 4 we discuss some issues regarding computing this function. 2.2 Decompositions and decomposition trees For the algorithms considered in this paper, we will be mostly interested in the decompositions that are speciﬁed by the junction tree, and we will represent these decompositions by a rooted tree called a decomposition tree. This representation was introduced in [2, 3], and is similar in spirit to Darwiche’s dtrees [6] which specify decompositions of directed acyclic graphs. In this section and the next, we show how a decomposition tree for a graph may be constructed, and show how it is used to solve a number of optimization problems. abd; ce; gf a; be; cd d; bc; e abe dbc ebc e; cf ; g cef cf g Figure 3: The separator tree corresponding to Figure 1 A decomposition tree for G is a rooted tree whose vertices correspond to separators and cliques of G. We describe the construction of the decomposition tree in terms of a junctiontree T = (K, S) for G. The interior nodes of the decomposition tree R(T ) correspond to S (the links of T and hence the minimal separators of G). The leaf or terminal nodes represent the elements of K (the nodes of T and hence the maximal cliques of G). R(T ) can be recursively constructed from T as follows : If T consists of just one node K, (and hence no edges), then R consists of just one node, which is given the label K as well. If however, T has more than one node, then T must contain at least one link. To begin, let Vi Vj ∈ S be any link in T . Then removal of the link Vi Vj results in two disjoint junctiontrees T (i) and T (j) . We label the root of R by the decomposition (V (i) ; Vij ; V (j) ). The rest of R is recursively built by successively picking links of T (i) and T (j) (decompositions of G(i) and G(j) ) to form the interior nodes of R. The effect of this procedure on the junction tree of Figure 1 is shown in Figure 3, where the decomposition associated with the interior nodes is shown inside the nodes. Let M be the set of all nodes of R(T ). For any interior node M induced by the the link Vi Vj ∈ S of T , then we will let M (i) and M (j) represent the left and right children of M , and R(i) and R(j) be the left and right trees below M . 3 3.1 Finding optimal subgraphical models Optimal sub (k − 1)-trees of k-trees Suppose that G is a k-tree. A sub (k − 1)-tree of G is a subgraph H of G that is (k − 1)tree. Now, if Vij is any minimal separator of G, then both G(i) and G(j) are k-trees on vertex sets V (i) and V (j) respectively. It is clear that the induced subgraphs H[V (i) ] and H[V (j) ] are subgraphs of G(i) and G(j) and are partial (k − 1)-trees. We will be interested in ﬁnding sub (k − 1)-trees of k trees and this problem is trivial by the result of [1] when k = 2. Therefore, we assume that k ≥ 3. The following result characterizes the various possibilities for H[Vij ] in this case. Lemma 2. Suppose that G is a k-tree, and S = Vij is a minimal separator of G corresponding to the link ij of the junction-tree T . In any (k − 1)-tree H ⊆ G either 1. There is a u ∈ S such that u is not connected to vertices in both V (i) \ S and V (j) \ S. Then S \ {u} is a minimal separator in H and hence is complete. 2. Every vertex in S is connected to vertices in both V (i) \S and V (j) \S. Then there are vertices {x, y} ⊆ S such that the edge H[S] is missing only the edge {x, y}. Further either H[V (i) ] or H[V (j) ] does not contain a unchorded x-y path. Proof. We consider two possibilities. In the ﬁrst, there is some vertex u ∈ S such that u is not connected to vertices in both V (i) \S and V (j) \. Since the removal of S disconnects G, the removal of S must also disconnect H. Therefore, S must contain a minimal separator of H. Since H is a (k − 1)-tree, all minimal separators of H must contain k − 1 vertices which must therefore be S \{u}. This corresponds to case (1) above. Clearly this possiblity can occur. If there is no such u ∈ S, then every vertex in S is connected to vertices in both V (i) \ S and V (j) \ S. If x ∈ S is connected to some yi ∈ V (i) \ S and yj ∈ V (j) \ S, then x is contained in every minimal yi /yj separator (see [5]). Therefore, every vertex in S is part of a minimal separator. Since each minimal separator contains k − 1 vertices, there must be at least two distinct minimum separators contained in S. Let Sx = S \ {x} and Sy = S \ {y} be two distinct minimal separators. We claim that H[S] contains all edges except the edge {x, y}. To see this, note that if z, w ∈ S, with z = w and {z, w} = {x, y} (as sets), then either {z, w} ⊂ Sy or {z, w} ⊂ Sx . Since both Sx and Sy are complete in H, this edge must be present in H. The edge {x, y} is not present in H[S] because all minimal separators in H must be of size k − 1. Further, if both V (i) and V (j) contain an unchorded path between x and y, then by joining the two paths at x and y, we get a unchorded cycle in H which contradicts the fact that H is triangulated. Therefore, we may associate k · 2 + 2 · k constraints with each separator Vij of G as 2 follows. There are k possible constraints corresponding to case (1) above (one for each choice of x), and k · 2 choices corresponding to case (2) above. This is because for each 2 pair {x, y} corresponding to the missing edge, we have either V (i) contains no unchorded xy paths or V (j) contains no unchorded xy paths. More explicitly, we can encode the set of constraints CM associated with each separator S corresponding to an interior node M of the decomposition tree as follows: CM = { (x, y, s) : x ∈ S, y ∈ S, s ∈ {i, j}}. If y = x, then this corresponds to case (1) of the above lemma. If s = i, then x is connected only to H (i) and if s = j, then x is connected only to H (j) . If y = x, then this corresponds to case (2) in the above lemma. If s = i, then H (i) does not contain any unchorded path between x and y, and there is no constraint on H (j) . Similarly if s = j, then H (j) does not contain any unchorded path between x and y, and there is no constraint on H (i) . Now suppose that H (i) and H (j) are triangulated subgraphs of G(i) and G(j) respectively, then it is clear that if H (i) and H (j) both satisfy the same constraint they must match up on the common vertices Vij . Therefore to splice together two solutions corresponding to the same constraint, we only need to check that the graph obtained by splicing the graphs is triangulated. Lemma 3. Suppose that H (i) and H (j) are triangulated subgraphs of G(i) and G(j) respectively such that both of them satisfy the same constraint as described above. Then the graph H obtained by splicing H (i) and H (j) together is triangulated. Proof. Suppose that both H (i) and H (j) are both triangulated and both satisfy the same constraint. If both H (i) and H (j) satisfy the same constraint corresponding to case (1) in Lemma 2 and H has an unchorded cycle, then this cycle must involve elements of both H (i) and H (j) . Therefore, there must be two vertices of S \{u} on the cycle, and hence this cycle has a chord as S \ {u} is complete. This contradiction shows that H is triangulated. So assume that both of them satisfy the constraint corresponding to case (2) of Lemma 2. Then if H is not triangulated, there must be a t-cycle (for t ≥ 4) with no chord. Now, since {x, y} is the only missing edge of S in H, and because H (i) and H (j) are individually triangulated, the cycle must contain x, y and vertices of both V (i) \ S and V (j) \ S. We may split this unchorded cycle into two unchorded paths, one contained in V (i) and one in V (j) thus violating our assumption that both H (i) and H (j) satisfy the same constraint. If |S| = k, then there are 2k + 2 · k ∈ O(k 2 ) ∈ O(n2 ). We can use a divide and conquer 2 strategy to ﬁnd the optimal sub (k − 1) tree once we have taken care of the base case, where G is just a single clique (of k + 1) elements. However, for this case, it is easily checked that any subgraph of G obtained by deleting exactly one edge results in a (k − 1) tree, and every sub (k−1)-tree results from this operation. Therefore, the optimal (k−1)-tree can be found using Algorithm 1, and in this case, the complexity of Algorithm 1 is O(n(k + 1) 2 ). This procedure can be generalized to ﬁnd the optimal sub (k − d)- tree for any ﬁxed d. However, the number of constraints grows exponentially with d (though it is still polynomial in n). Therefore for small, ﬁxed values of d, we can compute the optimal sub (k − d)-tree of G. While we can compute (k − d)-trees of G by ﬁrst going from a k tree to a (k − 1) tree, then from a (k − 1)-tree to a (k − 2)-tree, and so on in a greedy fashion, this will not be optimal in general. However, this might be a good algorithm to try when d is large. 3.2 Optimal triangulated subgraphs with |E(G)| − d edges Suppose that we are interested in a (triangulated) subgraph of G that contains d fewer edges that G does. That is, we want to ﬁnd an optimal subgraph H ⊂ G such that |E(H)| = |E(G)| − d. Note that by the result of [4] there is always a triangulated subgraph with d fewer edges (if d < |E(G)|). Two possibilities for ﬁnding such an optimal subgraph are 1. Use the procedure described in [4]. This is a greedy procedure which works in d steps by deleting an edge at each step. At each state, the edge is picked from the set of edges whose deletion leaves a triangulated graph. Then the edge which causes the least increase in KL-divergence is picked at each stage. 2. For each possible subset A of E(G) of size d, whose deletion leaves a triangulated graph, compute the KL divergence using the formula above, and then pick the optimal one. Since there are |E(G)| such sets, this can be done in polynomial d time (in |V (G)|) when d is a constant. The ﬁrst greedy algorithm is not guaranteed to yield the optimal solution. The second takes time that is O(n2d ). Now, let us solve this problem using the framework we’ve described. Let H be the set of subgraphs of G which may be obtained by deletion of d edges. For each M = ij ∈ M corresponding to the separator Vij , let CM = (l, r, c, s, A) : l + r − c = d, s a d bit string, A ∈ E(G[Vij ]) . The constraint reprec sented by (l, r, c, A) is this : A is a set of d edges of G[Vij ] that are missing in H, l edges are missing from the left subgraph, and r edges are missing from the right subgraph. c represents the double count, and so is subtracted from the total. If k is the size of the largest k clique, then the total number of such constraints is bounded by 2d · 2d · (2) ∈ O(k 2d ) d which could be better than O(n2d ) and is polynomial in |V | when d is constant. See [10] for additional details. 4 Conclusions Algorithm 1 will compute the optimal H ∈ H for the two examples discussed above and is polynomial (for ﬁxed constant d) even if k is O(n). In [10] a generalization is presented which will allow ﬁnding the optimal solution for other classes of subgraphical models. Now, we assume an oracle model for computing KL-divergences of probability distributions on vertex sets of cliques. It is clear that these KL-divergences can be computed R ← separator-tree for G; for each vertex M of R in order of increasing height (bottom up) do for each constraint cM of M do if M is an interior vertex of R corresponding to edge ij of the junction tree then Let Ml and Mr be the left and right children of M ; Pick constraint cl ∈ CMl compatible with cM to minimize table[Ml , cl ]; Pick constraint cr ∈ CMr compatible with cM to minimize table[Mr , cr ]; loss ← D(PG [M ] PH [M ]); table[M, cM ] ← table[Ml , cl ] + table[Mr , cr ] − loss; else table[M, cM ] ← D(PG [M ] PH [M ]); end end end Algorithm 1: Finding optimal set of constraints efﬁciently for distributions like Gaussians, but for discrete distributions this may not be possible when k is large. However even in this case this algorithm will result in only polynomial calls to the oracle. The standard algorithm [3] which is exponential in the treewidth will make O(2k ) calls to this oracle. Therefore, when the cost of computing the KL-divergence is large, this algorithm becomes even more attractive as it results in exponential speedup over the standard algorithm. Alternatively, if we can compute approximate KL-divergences, or approximately optimal solutions, then we can compute an approximate solution by using the same algorithm. References [1] C. Chow and C. Liu, “Approximating discrete probability distributions with dependence trees”, IEEE Transactions on Information Theory, v. 14, 1968, Pages 462–467. [2] F. Gavril, “Algorithms on clique separable graphs”, Discrete Mathematics v. 9 (1977), pp. 159–165. [3] R. E. Tarjan. “Decomposition by Clique Separators”, Discrete Mathematics, v. 55 (1985), pp. 221–232. [4] U. Kjaerulff. “Reduction of computational complexity in Bayesian networks through removal of weak dependencies”, Proceedings of the Tenth Annual Conference on Uncertainty in Artiﬁcial Intelligence, pp. 374–382, 1994. [5] T. Kloks, “Treewidth: Computations and Approximations”, Springer-Verlag, 1994. [6] A. Darwiche and M. Hopkins. “Using recursive decomposition to construct elimination orders, jointrees and dtrees”, Technical Report D-122, Computer Science Dept., UCLA. [7] S. Lauritzen. “Graphical Models”, Oxford University Press, Oxford, 1996. [8] T. A. McKee and F. R. McMorris. “Topics in Intersection Graph Theory”, SIAM Monographs on Discrete Mathematics and Applications, 1999. [9] D. Karger and N. Srebro. “Learning Markov networks: Maximum bounded tree-width graphs.” In Symposium on Discrete Algorithms, 2001, Pages 391-401. [10] M. Narasimhan and J. Bilmes. “Optimization on separator-clique trees.”, Technical report UWEETR 2004-10, June 2004.</p><p>same-paper 2 0.79387712 <a title="38-lda-2" href="./nips-2004-Co-Validation%3A_Using_Model_Disagreement_on_Unlabeled_Data_to_Validate_Classification_Algorithms.html">38 nips-2004-Co-Validation: Using Model Disagreement on Unlabeled Data to Validate Classification Algorithms</a></p>
<p>Author: Omid Madani, David M. Pennock, Gary W. Flake</p><p>Abstract: In the context of binary classiﬁcation, we deﬁne disagreement as a measure of how often two independently-trained models differ in their classiﬁcation of unlabeled data. We explore the use of disagreement for error estimation and model selection. We call the procedure co-validation, since the two models effectively (in)validate one another by comparing results on unlabeled data, which we assume is relatively cheap and plentiful compared to labeled data. We show that per-instance disagreement is an unbiased estimate of the variance of error for that instance. We also show that disagreement provides a lower bound on the prediction (generalization) error, and a tight upper bound on the “variance of prediction error”, or the variance of the average error across instances, where variance is measured across training sets. We present experimental results on several data sets exploring co-validation for error estimation and model selection. The procedure is especially effective in active learning settings, where training sets are not drawn at random and cross validation overestimates error. 1</p><p>3 0.77877879 <a title="38-lda-3" href="./nips-2004-Joint_Probabilistic_Curve_Clustering_and_Alignment.html">90 nips-2004-Joint Probabilistic Curve Clustering and Alignment</a></p>
<p>Author: Scott J. Gaffney, Padhraic Smyth</p><p>Abstract: Clustering and prediction of sets of curves is an important problem in many areas of science and engineering. It is often the case that curves tend to be misaligned from each other in a continuous manner, either in space (across the measurements) or in time. We develop a probabilistic framework that allows for joint clustering and continuous alignment of sets of curves in curve space (as opposed to a ﬁxed-dimensional featurevector space). The proposed methodology integrates new probabilistic alignment models with model-based curve clustering algorithms. The probabilistic approach allows for the derivation of consistent EM learning algorithms for the joint clustering-alignment problem. Experimental results are shown for alignment of human growth data, and joint clustering and alignment of gene expression time-course data.</p><p>4 0.75234365 <a title="38-lda-4" href="./nips-2004-Convergence_and_No-Regret_in_Multiagent_Learning.html">48 nips-2004-Convergence and No-Regret in Multiagent Learning</a></p>
<p>Author: Michael Bowling</p><p>Abstract: Learning in a multiagent system is a challenging problem due to two key factors. First, if other agents are simultaneously learning then the environment is no longer stationary, thus undermining convergence guarantees. Second, learning is often susceptible to deception, where the other agents may be able to exploit a learner’s particular dynamics. In the worst case, this could result in poorer performance than if the agent was not learning at all. These challenges are identiﬁable in the two most common evaluation criteria for multiagent learning algorithms: convergence and regret. Algorithms focusing on convergence or regret in isolation are numerous. In this paper, we seek to address both criteria in a single algorithm by introducing GIGA-WoLF, a learning algorithm for normalform games. We prove the algorithm guarantees at most zero average regret, while demonstrating the algorithm converges in many situations of self-play. We prove convergence in a limited setting and give empirical results in a wider variety of situations. These results also suggest a third new learning criterion combining convergence and regret, which we call negative non-convergence regret (NNR). 1</p><p>5 0.63807511 <a title="38-lda-5" href="./nips-2004-Fast_Rates_to_Bayes_for_Kernel_Machines.html">69 nips-2004-Fast Rates to Bayes for Kernel Machines</a></p>
<p>Author: Ingo Steinwart, Clint Scovel</p><p>Abstract: We establish learning rates to the Bayes risk for support vector machines (SVMs) with hinge loss. In particular, for SVMs with Gaussian RBF kernels we propose a geometric condition for distributions which can be used to determine approximation properties of these kernels. Finally, we compare our methods with a recent paper of G. Blanchard et al.. 1</p><p>6 0.63012236 <a title="38-lda-6" href="./nips-2004-Seeing_through_water.html">160 nips-2004-Seeing through water</a></p>
<p>7 0.62969989 <a title="38-lda-7" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>8 0.62582457 <a title="38-lda-8" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>9 0.62551057 <a title="38-lda-9" href="./nips-2004-Density_Level_Detection_is_Classification.html">49 nips-2004-Density Level Detection is Classification</a></p>
<p>10 0.62536699 <a title="38-lda-10" href="./nips-2004-Beat_Tracking_the_Graphical_Model_Way.html">29 nips-2004-Beat Tracking the Graphical Model Way</a></p>
<p>11 0.62438452 <a title="38-lda-11" href="./nips-2004-Linear_Multilayer_Independent_Component_Analysis_for_Large_Natural_Scenes.html">104 nips-2004-Linear Multilayer Independent Component Analysis for Large Natural Scenes</a></p>
<p>12 0.62307018 <a title="38-lda-12" href="./nips-2004-Log-concavity_Results_on_Gaussian_Process_Methods_for_Supervised_and_Unsupervised_Learning.html">105 nips-2004-Log-concavity Results on Gaussian Process Methods for Supervised and Unsupervised Learning</a></p>
<p>13 0.62286586 <a title="38-lda-13" href="./nips-2004-Worst-Case_Analysis_of_Selective_Sampling_for_Linear-Threshold_Algorithms.html">206 nips-2004-Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms</a></p>
<p>14 0.62154156 <a title="38-lda-14" href="./nips-2004-An_Application_of_Boosting_to_Graph_Classification.html">19 nips-2004-An Application of Boosting to Graph Classification</a></p>
<p>15 0.62074459 <a title="38-lda-15" href="./nips-2004-Generalization_Error_Bounds_for_Collaborative_Prediction_with_Low-Rank_Matrices.html">71 nips-2004-Generalization Error Bounds for Collaborative Prediction with Low-Rank Matrices</a></p>
<p>16 0.62056547 <a title="38-lda-16" href="./nips-2004-%E2%84%93%E2%82%80-norm_Minimization_for_Basis_Selection.html">207 nips-2004-ℓ₀-norm Minimization for Basis Selection</a></p>
<p>17 0.62023705 <a title="38-lda-17" href="./nips-2004-Limits_of_Spectral_Clustering.html">103 nips-2004-Limits of Spectral Clustering</a></p>
<p>18 0.61979336 <a title="38-lda-18" href="./nips-2004-Hierarchical_Clustering_of_a_Mixture_Model.html">77 nips-2004-Hierarchical Clustering of a Mixture Model</a></p>
<p>19 0.61957538 <a title="38-lda-19" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>20 0.61913985 <a title="38-lda-20" href="./nips-2004-Semi-parametric_Exponential_Family_PCA.html">163 nips-2004-Semi-parametric Exponential Family PCA</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
