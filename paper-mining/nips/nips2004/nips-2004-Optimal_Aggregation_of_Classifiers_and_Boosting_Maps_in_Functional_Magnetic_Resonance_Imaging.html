<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>139 nips-2004-Optimal Aggregation of Classifiers and Boosting Maps in Functional Magnetic Resonance Imaging</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-139" href="#">nips2004-139</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>139 nips-2004-Optimal Aggregation of Classifiers and Boosting Maps in Functional Magnetic Resonance Imaging</h1>
<br/><p>Source: <a title="nips-2004-139-pdf" href="http://papers.nips.cc/paper/2699-optimal-aggregation-of-classifiers-and-boosting-maps-in-functional-magnetic-resonance-imaging.pdf">pdf</a></p><p>Author: Vladimir Koltchinskii, Manel Martínez-ramón, Stefan Posse</p><p>Abstract: We study a method of optimal data-driven aggregation of classiﬁers in a convex combination and establish tight upper bounds on its excess risk with respect to a convex loss function under the assumption that the solution of optimal aggregation problem is sparse. We use a boosting type algorithm of optimal aggregation to develop aggregate classiﬁers of activation patterns in fMRI based on locally trained SVM classiﬁers. The aggregation coefﬁcients are then used to design a ”boosting map” of the brain needed to identify the regions with most signiﬁcant impact on classiﬁcation. 1</p><p>Reference: <a title="nips-2004-139-reference" href="../nips2004_reference/nips-2004-Optimal_Aggregation_of_Classifiers_and_Boosting_Maps_in_Functional_Magnetic_Resonance_Imaging_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We use a boosting type algorithm of optimal aggregation to develop aggregate classiﬁers of activation patterns in fMRI based on locally trained SVM classiﬁers. [sent-2, score-1.239]
</p><p>2 The aggregation coefﬁcients are then used to design a ”boosting map” of the brain needed to identify the regions with most signiﬁcant impact on classiﬁcation. [sent-3, score-0.557]
</p><p>3 1  Introduction  We consider a problem of optimal aggregation (see [1]) of a ﬁnite set of base classiﬁers in a complex aggregate classiﬁer. [sent-4, score-0.755]
</p><p>4 The aggregate classiﬁers we study are convex combinations of base classiﬁers and we are using boosting type algorithms as aggregation tools. [sent-5, score-1.171]
</p><p>5 Our primary goal is to use this approach in the problem of classiﬁcation of activation patterns in functional Magnetic Resonance Imaging (fMRI) (see, e. [sent-7, score-0.331]
</p><p>6 In these problems it is of interest not only to classify the patterns, but also to determine areas of the brain that are relevant for a particular classiﬁcation task. [sent-10, score-0.225]
</p><p>7 Our approach is based  on splitting the image into a number of functional areas, training base classiﬁers locally in each area and then combining them into a complex aggregate classiﬁer. [sent-11, score-0.514]
</p><p>8 The aggregation coefﬁcients are used to create a special representation of the image we call the boosting map of the brain. [sent-12, score-0.873]
</p><p>9 It is needed to identify the functional areas with the most signiﬁcant impact on classiﬁcation. [sent-13, score-0.271]
</p><p>10 Previous work has focused on classifying patterns within subject [2] and these patterns were located in the occipital lobe. [sent-14, score-0.344]
</p><p>11 Here we are considering a different problem, that is widely distributed patterns in multiple brain regions across groups of subjects. [sent-15, score-0.216]
</p><p>12 We use prior knowledge from functional neuroanatomical brain atlases to subdivide the brain into Regions of Interest, which makes this problem amenable to boosting. [sent-16, score-0.342]
</p><p>13 Classiﬁcation across subjects requires spatial normalization to account for inter-subject differences in brain size and shape, but also needs to be robust with respect to inter-subject differences in activation patterns –shape and amplitude. [sent-17, score-0.383]
</p><p>14 2  Optimal aggregation of classiﬁers  Although we developed a multiclass extension of the method, for simplicity, we are dealing here with a standard binary classiﬁcation. [sent-19, score-0.444]
</p><p>15 The quantity P {(x, y) : yf (x) ≤ 0} (the probability of misclassiﬁcation or abstaining) is called the generalization error or the risk of f. [sent-25, score-0.289]
</p><p>16 Let N  N  conv(H) :=  |λj | ≤ 1  λ j hj : j=1  j=1  be the symmetric convex hull of H. [sent-30, score-0.279]
</p><p>17 One of the versions of optimal aggregation problem would be to ﬁnd a convex combination f ∈ conv(H) that minimizes the generalization error of f in conv(H). [sent-31, score-0.628]
</p><p>18 For a given f ∈ conv(H) its quality is measured by E(f ) := P {(x, y) : yf (x) ≤ 0} −  inf  g∈conv(H)  P {(x, y) : yg(x) ≤ 0},  which is often called the excess risk of f. [sent-32, score-0.435]
</p><p>19 Since the true distribution P of (X, Y ) is unknown, the solution of the optimal aggregation problem is to be found based on the training data (X1 , Y1 ), . [sent-33, score-0.489]
</p><p>20 Let Pn denote the empirical measure based on the training data, i. [sent-37, score-0.089]
</p><p>21 Since the generalization error is not known, it is tempting to try to estimate the optimal convex aggregate classiﬁer by minimizing the training error Pn {(x, y) : yf (x) ≤ 0} over the convex hull conv(H). [sent-42, score-0.77]
</p><p>22 However, this minimization problem is not computationally feasible and, moreover, the accuracy of empirical approximation (approximation of P by Pn ) over the class of sets {{(x, y) : yf (x) ≤ 0} : f ∈ conv(H)} is not good enough when H is a large class. [sent-43, score-0.213]
</p><p>23 An approach that allows one to overcome both difﬁculties and that  proved to be very successful in the recent years is to replace the minimization of the training error by the minimization of the empirical risk with respect to a convex loss function. [sent-44, score-0.536]
</p><p>24 To be speciﬁc, let ℓ be a nonnegative decreasing convex function on R such that ℓ(u) ≥ 1 for u ≤ 0. [sent-45, score-0.111]
</p><p>25 The quantity P (ℓ • f ) =  (ℓ • f )dP = Eℓ(Y f (X))  is called the risk of f with respect to the loss ℓ, or the ℓ-risk of f. [sent-47, score-0.167]
</p><p>26 We will call a function N  λ0 hj ∈ conv(H) j  f0 := j=1  an ℓ-otimal aggregate classiﬁer if it minimizes the ℓ-risk over conv(H). [sent-48, score-0.354]
</p><p>27 Similarly to the excess risk, one can deﬁne the excess ℓ-risk of f as Eℓ (f ) := P (ℓ • f ) −  inf  g∈conv(H)  P (ℓ • g). [sent-49, score-0.364]
</p><p>28 As before, since P is unknown, the minimization of ℓ-risk has to be replaced by the corresponding empirical risk minimization problem Pn (ℓ • f ) = ˆ whose solution f :=  1 n  n  ℓ Yj f (Xj ) −→ min, f ∈ conv(H), i=1  N j=1  ˆ λj hj is called an empirical ℓ-optimal aggregate classiﬁer. [sent-51, score-0.732]
</p><p>29 , λ0 , λj are small for most of the values j of j), then the excess ℓ-risk of the empirical ℓ-optimal aggregate classiﬁer is small and, ˆ moreover, the coefﬁcients of f are close to the coefﬁcients of f0 in ℓ1 -distance. [sent-54, score-0.445]
</p><p>30 The sparsity assumption is almost unavoidable in many problems because of the ”bet on sparsity” principle (see the Introduction). [sent-55, score-0.122]
</p><p>31 , N } such that the sets of random variables {Y, hj (X), j ∈ J} and {hj (X), j ∈ J} are independent and, in addition, Ehj (X) = 0, j ∈ J, then, using Jensen’s inequality, it is easy to check that in an ℓ-optimal aggregate classiﬁer f0 one can take λ0 = 0, j ∈ J. [sent-59, score-0.354]
</p><p>32 j N  We will deﬁne a measure of sparsity of a function f := j=1 λj hj ∈ conv(H) that is somewhat akin to sparsity charactersitics considered in [5, 6]. [sent-60, score-0.278]
</p><p>33 n  dn (f ) := min d : 1 ≤ d ≤ N,  βn (d) ≥ ∆(d) . [sent-65, score-0.15]
</p><p>34 , N } such that λj = 0 for all j ∈ J and card(J) = d, then dn (f ) ≤ d. [sent-69, score-0.15]
</p><p>35 We will also need the following measure of linear independence of functions in H : −1  γ(d) := γ(H; d) =  inf  J⊂{1,. [sent-70, score-0.056]
</p><p>36 This is the case, for instance, for so called regularized boosting [7]. [sent-77, score-0.341]
</p><p>37 Theorem 1 There exist constants K1 , K2 > 0 such that for all t > 0 2  L ˆ P Eℓ (f ) ≥ K1 Λ  ˆ βn (dn (f ))  log N t + n n  ≤ e−t  and N  P  t L ˆ ˆ ˆ |λj − λ0 | ≥ K2 γ(dn (f ) + dn (f0 )) βn (dn (f ) + dn (f0 )) + j Λ n j=1  ≤ e−t . [sent-79, score-0.3]
</p><p>38 Our proof requires some background material on localized Rademacher complexities and their role in bounding of excess risk (see [8]). [sent-80, score-0.324]
</p><p>39 Note that the ﬁrst bound depends only on dn (f ) and the second on dn (f ), dn (f0 ). [sent-82, score-0.45]
</p><p>40 Both quantities can be much smaller than N despite the fact that empirical risk minimization occurs over the whole N -dimensional convex hull. [sent-83, score-0.367]
</p><p>41 However, the approach to convex aggregation based on minimization of the empirical ℓ-risk over the convex hull does not ˆ guarantee that f is sparse even if f0 is. [sent-84, score-0.845]
</p><p>42 To address this problem, we also studied another approach based on minimization of the penalized empirical ℓ-risk with the penalty based on the number of nonzero coefﬁcients of the classiﬁer, but the size of the paper does not allow us to discuss it. [sent-85, score-0.122]
</p><p>43 3  Classiﬁcation of fMRI patterns and boosting maps  We are using optimal aggregation methods described above in the problem of classiﬁcation of activation patterns in fMRI. [sent-86, score-1.147]
</p><p>44 Our approach is based on dividing the training data into two parts: for local training and for aggregation. [sent-87, score-0.119]
</p><p>45 Then, we split the image into N functional areas and train N local classiﬁers h1 , . [sent-88, score-0.409]
</p><p>46 The data reserved for aggregation is then used to construct an aggregate classiﬁer. [sent-92, score-0.655]
</p><p>47 In applications, we are often replacing direct minimization of empirical risk with convex loss by the standard AdaBoost algorithm (see, e. [sent-93, score-0.4]
</p><p>48 A weak (base) learner for AdaBoost simply chooses in this case a local classiﬁer among h1 , . [sent-96, score-0.047]
</p><p>49 , hN with the smallest weighted training error [in more sophisticated versions, we choose a local classiﬁer at random with probability depending on the size of its weighted training error] and after a number of rounds AdaBoost returns a convex combination of local classiﬁers. [sent-99, score-0.338]
</p><p>50 The coefﬁcients of this aggregate classiﬁer are then used to create a new visual representation of the brain (the boosting map) that highlights the functional areas with signiﬁcant impact on classiﬁcation. [sent-100, score-1.042]
</p><p>51 In principle, it is also possible to use the same data for training of local classiﬁers and for aggregation (retraining the local classiﬁers at each round of boosting), but this approach is time consuming. [sent-101, score-0.576]
</p><p>52 Statistical parametric maps (SPMs) are image processes with voxel1 values that are, under the null hypothesis, distributed according to a known probability density function, usually the Student’s 1  A voxel is the amplitude of a position in the 3-D MRI image matrix. [sent-103, score-0.209]
</p><p>53 Figure 1: Masks used to split the image into functional areas in multi-slice and 3 orthogonal slice display representations. [sent-104, score-0.325]
</p><p>54 The resulting statistical parameters are assembled into an image - the SPM. [sent-108, score-0.052]
</p><p>55 The classiﬁcation system essentially transforms the t-map of the image into the boosting map and at the same time it returns the aggregate classiﬁer. [sent-109, score-0.724]
</p><p>56 The system consists of the data preprocessing block that splits the image into functional areas based on speciﬁed masks, and also splits the data into portions corresponding to the areas. [sent-110, score-0.419]
</p><p>57 In one of our examples, we use the main functional areas brainstem, cerebellum, occipital, temporal, parietal, subcortical and frontal. [sent-111, score-0.318]
</p><p>58 We split these masks in left and right, having in total 14 of them. [sent-112, score-0.159]
</p><p>59 The classiﬁer block then trains local classiﬁers based on local data (in the current version we are using SVM classiﬁers). [sent-113, score-0.129]
</p><p>60 Finally, the aggregation or boosting block computes and outputs the aggregate classiﬁer and the boosting map of the image. [sent-114, score-1.435]
</p><p>61 , hN be local classiﬁers (based either on the same, or on a new set of masks). [sent-123, score-0.047]
</p><p>62 After this, the AdaBoost can proceed in a normal fashion creating at the end an aggregate of f and of new local classiﬁers. [sent-125, score-0.285]
</p><p>63 The process can be repeated recursively updating both the classiﬁer and the boosting map. [sent-126, score-0.341]
</p><p>64 Right: Locations of the learners chosen by the boosting procedure (white spots). [sent-128, score-0.398]
</p><p>65 The background image corresponds to the two patterns of left and center ﬁgures superimposed. [sent-129, score-0.194]
</p><p>66 Figure 4: Two t-maps corresponding to visual (left) and motor activations in the same subject used in the real data experiment. [sent-131, score-0.172]
</p><p>67 As a synthetic data example, we generate 40 × 40 pixels images of two classes. [sent-132, score-0.048]
</p><p>68 Each class of images consists of three gaussian clusters placed in different positions. [sent-133, score-0.089]
</p><p>69 We generate the set of images by adding gaussian noise of standard deviation 0. [sent-134, score-0.048]
</p><p>70 Figure 2 (left and center) shows the averages of class 1 and class 2 images respectively. [sent-138, score-0.048]
</p><p>71 Two samples of the images can be seen in Figure 3 We apply a base learner to each one of the 1600 pixels of the images. [sent-139, score-0.112]
</p><p>72 Learners have been trained with 200 data, 100 of each class, and the aggregation has been trained with 200 more data. [sent-140, score-0.417]
</p><p>73 As a proof of concept, we remark that the map is able to focus in the areas in which the clusters corresponding to each class are, discarding those areas in which only randomly placed clusters are present. [sent-147, score-0.377]
</p><p>74 In order to test the algorithm in a real fMRI experiment, we use 20 images taken from 10 healthy subjects on a 1. [sent-148, score-0.115]
</p><p>75 The paradigm consists of four interleaved tasks: visual (8 Hz checkerboard stimulation), motor (2 Hz right index ﬁnger tapping), auditory  Figure 5: Boosting map of the brain corresponding to the classiﬁcation problem with visual and motor activations. [sent-151, score-0.529]
</p><p>76 left brainstem: left cerebellum: left parietal: left temporal: left occipital: left subcortical: left frontal:  0 0. [sent-153, score-0.245]
</p><p>77 29 0 0  right brainstem: right cerebellum: right parietal: right temporal: right occipital: right subcortical: right frontal:  0 0. [sent-157, score-0.252]
</p><p>78 15 0 0  Table 1: Values of the convex aggregation. [sent-161, score-0.111]
</p><p>79 Finger tapping in the motor task was regulated with an auditory tone, subjects were asked to tap onto a button-response pad. [sent-164, score-0.254]
</p><p>80 During the auditory task, subjects were asked to respond on a button-response pad for each ”Ta” (25% of sounds), but not to similar syllables. [sent-165, score-0.126]
</p><p>81 Functional MRI data were acquired using single-shot echo-planar imaging with TR: 2 s, TE: 50 ms, ﬂip angle: 90 degrees, matrix size: 64 × 64 pixels, FOV: 192 mm. [sent-168, score-0.051]
</p><p>82 Statistical parametric mapping was performed to generate t-maps that represent brain activation changes. [sent-170, score-0.237]
</p><p>83 A convex aggregation of the classiﬁer outputs is then trained. [sent-177, score-0.528]
</p><p>84 We tested the algorithm in binary classiﬁcation of visual against auditory activations. [sent-178, score-0.107]
</p><p>85 We train the base learners with 10 images, and the boosting with 9. [sent-179, score-0.499]
</p><p>86 Then, we train the base learners again with 19, leaving one for testing. [sent-180, score-0.189]
</p><p>87 We repeat the experiment leaving out a different image each trial. [sent-181, score-0.083]
</p><p>88 The corresponding boosting map is shown in Fig 5. [sent-184, score-0.404]
</p><p>89 It highlights the right temporal and both occipital areas, where the motor and visual activations are  present (see Fig. [sent-185, score-0.405]
</p><p>90 Also, there is activation in the cerebellum area in some of the motor t-maps, which is highlighted by the boosting map. [sent-187, score-0.601]
</p><p>91 In experiments for the six binary combination of activation stimuli, the average error was less than 10%. [sent-188, score-0.131]
</p><p>92 This is an acceptable result if we take into account that the data included ten different subjects, whose brain activation patterns present noticeable differences. [sent-189, score-0.349]
</p><p>93 The most difﬁcult problem is the choice of functional areas and local classiﬁers so that the ”true” boosting map is identiﬁable based on the data. [sent-191, score-0.691]
</p><p>94 If γ(d) is too large for d = dn (f0 ) ∨ dn (f ), the empirical boosting map can become very unstable and misleading. [sent-193, score-0.757]
</p><p>95 Acknowledgments We want to acknowledge to Jeremy Bockholt (MIND Institute) for providing the brain masks, generated with BRAINS2. [sent-195, score-0.109]
</p><p>96 (2003) Functional magnetic resonance imaging (fMRI) ”brain reading”: detecting and classifying distributed patterns of fMRI activity in human visual cortex, Neuroimage19, 2, 261–70. [sent-210, score-0.332]
</p><p>97 (2003) Bounding the generalization error of combined classiﬁers: balancing the dimensionality and the margins. [sent-232, score-0.064]
</p><p>98 (2003) Generalization bounds for voting classiﬁers based on sparsity and clustering. [sent-240, score-0.081]
</p><p>99 (2003) On the rates of convergence of regularized boosting classiﬁers. [sent-248, score-0.341]
</p><p>100 (1991) Comparing functional (PET) images: the assessment of signiﬁcant change. [sent-264, score-0.124]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('aggregation', 0.417), ('boosting', 0.341), ('conv', 0.294), ('aggregate', 0.238), ('fmri', 0.206), ('classi', 0.185), ('excess', 0.154), ('dn', 0.15), ('risk', 0.134), ('ers', 0.132), ('occipital', 0.13), ('functional', 0.124), ('areas', 0.116), ('hj', 0.116), ('convex', 0.111), ('brain', 0.109), ('patterns', 0.107), ('koltchinskii', 0.104), ('activation', 0.1), ('pn', 0.094), ('yf', 0.091), ('masks', 0.091), ('er', 0.084), ('motor', 0.083), ('sparsity', 0.081), ('albuquerque', 0.078), ('brainstem', 0.078), ('subcortical', 0.078), ('cerebellum', 0.077), ('adaboost', 0.069), ('minimization', 0.069), ('magnetic', 0.068), ('subjects', 0.067), ('hn', 0.066), ('base', 0.064), ('map', 0.063), ('parietal', 0.062), ('mri', 0.062), ('auditory', 0.059), ('resonance', 0.058), ('learners', 0.057), ('inf', 0.056), ('mexico', 0.054), ('empirical', 0.053), ('panchenko', 0.052), ('hull', 0.052), ('image', 0.052), ('imaging', 0.051), ('images', 0.048), ('visual', 0.048), ('local', 0.047), ('psychiatry', 0.045), ('bet', 0.045), ('tapping', 0.045), ('coef', 0.043), ('cients', 0.042), ('clusters', 0.041), ('activations', 0.041), ('unavoidable', 0.041), ('nm', 0.039), ('mind', 0.039), ('maps', 0.039), ('rademacher', 0.038), ('voxel', 0.038), ('train', 0.037), ('right', 0.036), ('complexities', 0.036), ('mental', 0.036), ('schoelkopf', 0.036), ('stimuli', 0.036), ('optimal', 0.036), ('training', 0.036), ('left', 0.035), ('block', 0.035), ('highlights', 0.035), ('card', 0.035), ('loss', 0.033), ('acceptable', 0.033), ('generalization', 0.033), ('split', 0.033), ('sparse', 0.032), ('lecture', 0.032), ('portions', 0.032), ('temporal', 0.032), ('error', 0.031), ('impact', 0.031), ('hz', 0.031), ('leaving', 0.031), ('splits', 0.03), ('notes', 0.03), ('returns', 0.03), ('round', 0.029), ('cation', 0.029), ('parametric', 0.028), ('frontal', 0.028), ('warmuth', 0.028), ('inequalities', 0.027), ('multiclass', 0.027), ('schapire', 0.027), ('convexity', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="139-tfidf-1" href="./nips-2004-Optimal_Aggregation_of_Classifiers_and_Boosting_Maps_in_Functional_Magnetic_Resonance_Imaging.html">139 nips-2004-Optimal Aggregation of Classifiers and Boosting Maps in Functional Magnetic Resonance Imaging</a></p>
<p>Author: Vladimir Koltchinskii, Manel Martínez-ramón, Stefan Posse</p><p>Abstract: We study a method of optimal data-driven aggregation of classiﬁers in a convex combination and establish tight upper bounds on its excess risk with respect to a convex loss function under the assumption that the solution of optimal aggregation problem is sparse. We use a boosting type algorithm of optimal aggregation to develop aggregate classiﬁers of activation patterns in fMRI based on locally trained SVM classiﬁers. The aggregation coefﬁcients are then used to design a ”boosting map” of the brain needed to identify the regions with most signiﬁcant impact on classiﬁcation. 1</p><p>2 0.16834593 <a title="139-tfidf-2" href="./nips-2004-An_Application_of_Boosting_to_Graph_Classification.html">19 nips-2004-An Application of Boosting to Graph Classification</a></p>
<p>Author: Taku Kudo, Eisaku Maeda, Yuji Matsumoto</p><p>Abstract: This paper presents an application of Boosting for classifying labeled graphs, general structures for modeling a number of real-world data, such as chemical compounds, natural language texts, and bio sequences. The proposal consists of i) decision stumps that use subgraph as features, and ii) a Boosting algorithm in which subgraph-based decision stumps are used as weak learners. We also discuss the relation between our algorithm and SVMs with convolution kernels. Two experiments using natural language data and chemical compounds show that our method achieves comparable or even better performance than SVMs with convolution kernels as well as improves the testing efﬁciency. 1</p><p>3 0.16399516 <a title="139-tfidf-3" href="./nips-2004-Boosting_on_Manifolds%3A_Adaptive_Regularization_of_Base_Classifiers.html">32 nips-2004-Boosting on Manifolds: Adaptive Regularization of Base Classifiers</a></p>
<p>Author: Ligen Wang, Balázs Kégl</p><p>Abstract: In this paper we propose to combine two powerful ideas, boosting and manifold learning. On the one hand, we improve A DA B OOST by incorporating knowledge on the structure of the data into base classiﬁer design and selection. On the other hand, we use A DA B OOST’s efﬁcient learning mechanism to signiﬁcantly improve supervised and semi-supervised algorithms proposed in the context of manifold learning. Beside the speciﬁc manifold-based penalization, the resulting algorithm also accommodates the boosting of a large family of regularized learning algorithms. 1</p><p>4 0.15558486 <a title="139-tfidf-4" href="./nips-2004-Generalization_Error_and_Algorithmic_Convergence_of_Median_Boosting.html">72 nips-2004-Generalization Error and Algorithmic Convergence of Median Boosting</a></p>
<p>Author: Balázs Kégl</p><p>Abstract: We have recently proposed an extension of A DA B OOST to regression that uses the median of the base regressors as the ﬁnal regressor. In this paper we extend theoretical results obtained for A DA B OOST to median boosting and to its localized variant. First, we extend recent results on efﬁcient margin maximizing to show that the algorithm can converge to the maximum achievable margin within a preset precision in a ﬁnite number of steps. Then we provide conﬁdence-interval-type bounds on the generalization error. 1</p><p>5 0.131589 <a title="139-tfidf-5" href="./nips-2004-Contextual_Models_for_Object_Detection_Using_Boosted_Random_Fields.html">47 nips-2004-Contextual Models for Object Detection Using Boosted Random Fields</a></p>
<p>Author: Antonio Torralba, Kevin P. Murphy, William T. Freeman</p><p>Abstract: We seek to both detect and segment objects in images. To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random ﬁeld (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efﬁcient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in ofﬁce and street scenes. 1</p><p>6 0.11224493 <a title="139-tfidf-6" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>7 0.10048234 <a title="139-tfidf-7" href="./nips-2004-On_the_Adaptive_Properties_of_Decision_Trees.html">137 nips-2004-On the Adaptive Properties of Decision Trees</a></p>
<p>8 0.098429821 <a title="139-tfidf-8" href="./nips-2004-Conditional_Random_Fields_for_Object_Recognition.html">44 nips-2004-Conditional Random Fields for Object Recognition</a></p>
<p>9 0.093481265 <a title="139-tfidf-9" href="./nips-2004-Following_Curved_Regularized_Optimization_Solution_Paths.html">70 nips-2004-Following Curved Regularized Optimization Solution Paths</a></p>
<p>10 0.089099325 <a title="139-tfidf-10" href="./nips-2004-Density_Level_Detection_is_Classification.html">49 nips-2004-Density Level Detection is Classification</a></p>
<p>11 0.085258007 <a title="139-tfidf-11" href="./nips-2004-Detecting_Significant_Multidimensional_Spatial_Clusters.html">51 nips-2004-Detecting Significant Multidimensional Spatial Clusters</a></p>
<p>12 0.080425881 <a title="139-tfidf-12" href="./nips-2004-PAC-Bayes_Learning_of_Conjunctions_and_Classification_of_Gene-Expression_Data.html">143 nips-2004-PAC-Bayes Learning of Conjunctions and Classification of Gene-Expression Data</a></p>
<p>13 0.078733191 <a title="139-tfidf-13" href="./nips-2004-Fast_Rates_to_Bayes_for_Kernel_Machines.html">69 nips-2004-Fast Rates to Bayes for Kernel Machines</a></p>
<p>14 0.076850384 <a title="139-tfidf-14" href="./nips-2004-An_Auditory_Paradigm_for_Brain-Computer_Interfaces.html">20 nips-2004-An Auditory Paradigm for Brain-Computer Interfaces</a></p>
<p>15 0.076133832 <a title="139-tfidf-15" href="./nips-2004-Joint_MRI_Bias_Removal_Using_Entropy_Minimization_Across_Images.html">89 nips-2004-Joint MRI Bias Removal Using Entropy Minimization Across Images</a></p>
<p>16 0.075300254 <a title="139-tfidf-16" href="./nips-2004-Breaking_SVM_Complexity_with_Cross-Training.html">34 nips-2004-Breaking SVM Complexity with Cross-Training</a></p>
<p>17 0.074474946 <a title="139-tfidf-17" href="./nips-2004-Kernel_Projection_Machine%3A_a_New_Tool_for_Pattern_Recognition.html">93 nips-2004-Kernel Projection Machine: a New Tool for Pattern Recognition</a></p>
<p>18 0.073563129 <a title="139-tfidf-18" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>19 0.073513202 <a title="139-tfidf-19" href="./nips-2004-Object_Classification_from_a_Single_Example_Utilizing_Class_Relevance_Metrics.html">134 nips-2004-Object Classification from a Single Example Utilizing Class Relevance Metrics</a></p>
<p>20 0.071407974 <a title="139-tfidf-20" href="./nips-2004-Machine_Learning_Applied_to_Perception%3A_Decision_Images_for_Gender_Classification.html">106 nips-2004-Machine Learning Applied to Perception: Decision Images for Gender Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.222), (1, 0.07), (2, -0.038), (3, 0.101), (4, 0.033), (5, 0.087), (6, 0.232), (7, 0.02), (8, -0.078), (9, 0.154), (10, -0.176), (11, -0.104), (12, 0.06), (13, -0.007), (14, 0.104), (15, -0.117), (16, 0.105), (17, 0.039), (18, -0.005), (19, 0.004), (20, -0.019), (21, -0.01), (22, 0.052), (23, 0.082), (24, 0.011), (25, 0.049), (26, 0.016), (27, -0.085), (28, -0.08), (29, -0.066), (30, -0.024), (31, -0.013), (32, -0.04), (33, 0.008), (34, -0.041), (35, 0.08), (36, -0.076), (37, -0.138), (38, 0.13), (39, -0.059), (40, 0.019), (41, 0.035), (42, -0.046), (43, -0.035), (44, 0.029), (45, 0.001), (46, -0.042), (47, 0.025), (48, 0.103), (49, -0.178)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94342333 <a title="139-lsi-1" href="./nips-2004-Optimal_Aggregation_of_Classifiers_and_Boosting_Maps_in_Functional_Magnetic_Resonance_Imaging.html">139 nips-2004-Optimal Aggregation of Classifiers and Boosting Maps in Functional Magnetic Resonance Imaging</a></p>
<p>Author: Vladimir Koltchinskii, Manel Martínez-ramón, Stefan Posse</p><p>Abstract: We study a method of optimal data-driven aggregation of classiﬁers in a convex combination and establish tight upper bounds on its excess risk with respect to a convex loss function under the assumption that the solution of optimal aggregation problem is sparse. We use a boosting type algorithm of optimal aggregation to develop aggregate classiﬁers of activation patterns in fMRI based on locally trained SVM classiﬁers. The aggregation coefﬁcients are then used to design a ”boosting map” of the brain needed to identify the regions with most signiﬁcant impact on classiﬁcation. 1</p><p>2 0.62743729 <a title="139-lsi-2" href="./nips-2004-Boosting_on_Manifolds%3A_Adaptive_Regularization_of_Base_Classifiers.html">32 nips-2004-Boosting on Manifolds: Adaptive Regularization of Base Classifiers</a></p>
<p>Author: Ligen Wang, Balázs Kégl</p><p>Abstract: In this paper we propose to combine two powerful ideas, boosting and manifold learning. On the one hand, we improve A DA B OOST by incorporating knowledge on the structure of the data into base classiﬁer design and selection. On the other hand, we use A DA B OOST’s efﬁcient learning mechanism to signiﬁcantly improve supervised and semi-supervised algorithms proposed in the context of manifold learning. Beside the speciﬁc manifold-based penalization, the resulting algorithm also accommodates the boosting of a large family of regularized learning algorithms. 1</p><p>3 0.57314688 <a title="139-lsi-3" href="./nips-2004-An_Application_of_Boosting_to_Graph_Classification.html">19 nips-2004-An Application of Boosting to Graph Classification</a></p>
<p>Author: Taku Kudo, Eisaku Maeda, Yuji Matsumoto</p><p>Abstract: This paper presents an application of Boosting for classifying labeled graphs, general structures for modeling a number of real-world data, such as chemical compounds, natural language texts, and bio sequences. The proposal consists of i) decision stumps that use subgraph as features, and ii) a Boosting algorithm in which subgraph-based decision stumps are used as weak learners. We also discuss the relation between our algorithm and SVMs with convolution kernels. Two experiments using natural language data and chemical compounds show that our method achieves comparable or even better performance than SVMs with convolution kernels as well as improves the testing efﬁciency. 1</p><p>4 0.56817579 <a title="139-lsi-4" href="./nips-2004-Contextual_Models_for_Object_Detection_Using_Boosted_Random_Fields.html">47 nips-2004-Contextual Models for Object Detection Using Boosted Random Fields</a></p>
<p>Author: Antonio Torralba, Kevin P. Murphy, William T. Freeman</p><p>Abstract: We seek to both detect and segment objects in images. To exploit both local image data as well as contextual information, we introduce Boosted Random Fields (BRFs), which uses Boosting to learn the graph structure and local evidence of a conditional random ﬁeld (CRF). The graph structure is learned by assembling graph fragments in an additive model. The connections between individual pixels are not very informative, but by using dense graphs, we can pool information from large regions of the image; dense models also support efﬁcient inference. We show how contextual information from other objects can improve detection performance, both in terms of accuracy and speed, by using a computational cascade. We apply our system to detect stuff and things in ofﬁce and street scenes. 1</p><p>5 0.55208534 <a title="139-lsi-5" href="./nips-2004-On_the_Adaptive_Properties_of_Decision_Trees.html">137 nips-2004-On the Adaptive Properties of Decision Trees</a></p>
<p>Author: Clayton Scott, Robert Nowak</p><p>Abstract: Decision trees are surprisingly adaptive in three important respects: They automatically (1) adapt to favorable conditions near the Bayes decision boundary; (2) focus on data distributed on lower dimensional manifolds; (3) reject irrelevant features. In this paper we examine a decision tree based on dyadic splits that adapts to each of these conditions to achieve minimax optimal rates of convergence. The proposed classiﬁer is the ﬁrst known to achieve these optimal rates while being practical and implementable. 1</p><p>6 0.52647728 <a title="139-lsi-6" href="./nips-2004-Generalization_Error_and_Algorithmic_Convergence_of_Median_Boosting.html">72 nips-2004-Generalization Error and Algorithmic Convergence of Median Boosting</a></p>
<p>7 0.51460826 <a title="139-lsi-7" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>8 0.50373209 <a title="139-lsi-8" href="./nips-2004-Machine_Learning_Applied_to_Perception%3A_Decision_Images_for_Gender_Classification.html">106 nips-2004-Machine Learning Applied to Perception: Decision Images for Gender Classification</a></p>
<p>9 0.48057631 <a title="139-lsi-9" href="./nips-2004-Learning_Syntactic_Patterns_for_Automatic_Hypernym_Discovery.html">101 nips-2004-Learning Syntactic Patterns for Automatic Hypernym Discovery</a></p>
<p>10 0.45921603 <a title="139-lsi-10" href="./nips-2004-Detecting_Significant_Multidimensional_Spatial_Clusters.html">51 nips-2004-Detecting Significant Multidimensional Spatial Clusters</a></p>
<p>11 0.4364244 <a title="139-lsi-11" href="./nips-2004-PAC-Bayes_Learning_of_Conjunctions_and_Classification_of_Gene-Expression_Data.html">143 nips-2004-PAC-Bayes Learning of Conjunctions and Classification of Gene-Expression Data</a></p>
<p>12 0.41680273 <a title="139-lsi-12" href="./nips-2004-Fast_Rates_to_Bayes_for_Kernel_Machines.html">69 nips-2004-Fast Rates to Bayes for Kernel Machines</a></p>
<p>13 0.41239741 <a title="139-lsi-13" href="./nips-2004-A_Topographic_Support_Vector_Machine%3A_Classification_Using_Local_Label_Configurations.html">14 nips-2004-A Topographic Support Vector Machine: Classification Using Local Label Configurations</a></p>
<p>14 0.4103044 <a title="139-lsi-14" href="./nips-2004-Mass_Meta-analysis_in_Talairach_Space.html">109 nips-2004-Mass Meta-analysis in Talairach Space</a></p>
<p>15 0.37057415 <a title="139-lsi-15" href="./nips-2004-Density_Level_Detection_is_Classification.html">49 nips-2004-Density Level Detection is Classification</a></p>
<p>16 0.35416782 <a title="139-lsi-16" href="./nips-2004-Maximal_Margin_Labeling_for_Multi-Topic_Text_Categorization.html">111 nips-2004-Maximal Margin Labeling for Multi-Topic Text Categorization</a></p>
<p>17 0.35008177 <a title="139-lsi-17" href="./nips-2004-The_Variational_Ising_Classifier_%28VIC%29_Algorithm_for_Coherently_Contaminated_Data.html">191 nips-2004-The Variational Ising Classifier (VIC) Algorithm for Coherently Contaminated Data</a></p>
<p>18 0.34990868 <a title="139-lsi-18" href="./nips-2004-An_Auditory_Paradigm_for_Brain-Computer_Interfaces.html">20 nips-2004-An Auditory Paradigm for Brain-Computer Interfaces</a></p>
<p>19 0.34780878 <a title="139-lsi-19" href="./nips-2004-%E2%84%93%E2%82%80-norm_Minimization_for_Basis_Selection.html">207 nips-2004-ℓ₀-norm Minimization for Basis Selection</a></p>
<p>20 0.34769055 <a title="139-lsi-20" href="./nips-2004-Methods_Towards_Invasive_Human_Brain_Computer_Interfaces.html">117 nips-2004-Methods Towards Invasive Human Brain Computer Interfaces</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.054), (15, 0.067), (26, 0.05), (31, 0.02), (33, 0.685), (35, 0.014), (39, 0.011), (81, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9948945 <a title="139-lda-1" href="./nips-2004-Optimal_Aggregation_of_Classifiers_and_Boosting_Maps_in_Functional_Magnetic_Resonance_Imaging.html">139 nips-2004-Optimal Aggregation of Classifiers and Boosting Maps in Functional Magnetic Resonance Imaging</a></p>
<p>Author: Vladimir Koltchinskii, Manel Martínez-ramón, Stefan Posse</p><p>Abstract: We study a method of optimal data-driven aggregation of classiﬁers in a convex combination and establish tight upper bounds on its excess risk with respect to a convex loss function under the assumption that the solution of optimal aggregation problem is sparse. We use a boosting type algorithm of optimal aggregation to develop aggregate classiﬁers of activation patterns in fMRI based on locally trained SVM classiﬁers. The aggregation coefﬁcients are then used to design a ”boosting map” of the brain needed to identify the regions with most signiﬁcant impact on classiﬁcation. 1</p><p>2 0.99449688 <a title="139-lda-2" href="./nips-2004-Expectation_Consistent_Free_Energies_for_Approximate_Inference.html">63 nips-2004-Expectation Consistent Free Energies for Approximate Inference</a></p>
<p>Author: Manfred Opper, Ole Winther</p><p>Abstract: We propose a novel a framework for deriving approximations for intractable probabilistic models. This framework is based on a free energy (negative log marginal likelihood) and can be seen as a generalization of adaptive TAP [1, 2, 3] and expectation propagation (EP) [4, 5]. The free energy is constructed from two approximating distributions which encode different aspects of the intractable model such a single node constraints and couplings and are by construction consistent on a chosen set of moments. We test the framework on a difﬁcult benchmark problem with binary variables on fully connected graphs and 2D grid graphs. We ﬁnd good performance using sets of moments which either specify factorized nodes or a spanning tree on the nodes (structured approximation). Surprisingly, the Bethe approximation gives very inferior results even on grids. 1</p><p>3 0.99365169 <a title="139-lda-3" href="./nips-2004-Coarticulation_in_Markov_Decision_Processes.html">39 nips-2004-Coarticulation in Markov Decision Processes</a></p>
<p>Author: Khashayar Rohanimanesh, Robert Platt, Sridhar Mahadevan, Roderic Grupen</p><p>Abstract: We investigate an approach for simultaneously committing to multiple activities, each modeled as a temporally extended action in a semi-Markov decision process (SMDP). For each activity we deﬁne a set of admissible solutions consisting of the redundant set of optimal policies, and those policies that ascend the optimal statevalue function associated with them. A plan is then generated by merging them in such a way that the solutions to the subordinate activities are realized in the set of admissible solutions satisfying the superior activities. We present our theoretical results and empirically evaluate our approach in a simulated domain. 1</p><p>4 0.9922812 <a title="139-lda-4" href="./nips-2004-Nearly_Tight_Bounds_for_the_Continuum-Armed_Bandit_Problem.html">126 nips-2004-Nearly Tight Bounds for the Continuum-Armed Bandit Problem</a></p>
<p>Author: Robert D. Kleinberg</p><p>Abstract: In the multi-armed bandit problem, an online algorithm must choose from a set of strategies in a sequence of n trials so as to minimize the total cost of the chosen strategies. While nearly tight upper and lower bounds are known in the case when the strategy set is ﬁnite, much less is known when there is an inﬁnite strategy set. Here we consider the case when the set of strategies is a subset of Rd , and the cost functions are continuous. In the d = 1 case, we improve on the best-known upper and lower bounds, closing the gap to a sublogarithmic factor. We also consider the case where d > 1 and the cost functions are convex, adapting a recent online convex optimization algorithm of Zinkevich to the sparser feedback model of the multi-armed bandit problem. 1</p><p>5 0.99196053 <a title="139-lda-5" href="./nips-2004-Modeling_Conversational_Dynamics_as_a_Mixed-Memory_Markov_Process.html">120 nips-2004-Modeling Conversational Dynamics as a Mixed-Memory Markov Process</a></p>
<p>Author: Tanzeem Choudhury, Sumit Basu</p><p>Abstract: In this work, we quantitatively investigate the ways in which a given person influences the joint turn-taking behavior in a conversation. After collecting an auditory database of social interactions among a group of twenty-three people via wearable sensors (66 hours of data each over two weeks), we apply speech and conversation detection methods to the auditory streams. These methods automatically locate the conversations, determine their participants, and mark which participant was speaking when. We then model the joint turn-taking behavior as a Mixed-Memory Markov Model [1] that combines the statistics of the individual subjects' self-transitions and the partners ' cross-transitions. The mixture parameters in this model describe how much each person 's individual behavior contributes to the joint turn-taking behavior of the pair. By estimating these parameters, we thus estimate how much influence each participant has in determining the joint turntaking behavior. We show how this measure correlates significantly with betweenness centrality [2], an independent measure of an individual's importance in a social network. This result suggests that our estimate of conversational influence is predictive of social influence. 1</p><p>6 0.98979074 <a title="139-lda-6" href="./nips-2004-Probabilistic_Inference_of_Alternative_Splicing_Events_in_Microarray_Data.html">149 nips-2004-Probabilistic Inference of Alternative Splicing Events in Microarray Data</a></p>
<p>7 0.98793274 <a title="139-lda-7" href="./nips-2004-Maximum_Margin_Clustering.html">115 nips-2004-Maximum Margin Clustering</a></p>
<p>8 0.98683268 <a title="139-lda-8" href="./nips-2004-The_Correlated_Correspondence_Algorithm_for_Unsupervised_Registration_of_Nonrigid_Surfaces.html">186 nips-2004-The Correlated Correspondence Algorithm for Unsupervised Registration of Nonrigid Surfaces</a></p>
<p>9 0.90323013 <a title="139-lda-9" href="./nips-2004-PAC-Bayes_Learning_of_Conjunctions_and_Classification_of_Gene-Expression_Data.html">143 nips-2004-PAC-Bayes Learning of Conjunctions and Classification of Gene-Expression Data</a></p>
<p>10 0.90178615 <a title="139-lda-10" href="./nips-2004-Dynamic_Bayesian_Networks_for_Brain-Computer_Interfaces.html">56 nips-2004-Dynamic Bayesian Networks for Brain-Computer Interfaces</a></p>
<p>11 0.90066493 <a title="139-lda-11" href="./nips-2004-Identifying_Protein-Protein_Interaction_Sites_on_a_Genome-Wide_Scale.html">80 nips-2004-Identifying Protein-Protein Interaction Sites on a Genome-Wide Scale</a></p>
<p>12 0.89714855 <a title="139-lda-12" href="./nips-2004-Generalization_Error_and_Algorithmic_Convergence_of_Median_Boosting.html">72 nips-2004-Generalization Error and Algorithmic Convergence of Median Boosting</a></p>
<p>13 0.8955487 <a title="139-lda-13" href="./nips-2004-Boosting_on_Manifolds%3A_Adaptive_Regularization_of_Base_Classifiers.html">32 nips-2004-Boosting on Manifolds: Adaptive Regularization of Base Classifiers</a></p>
<p>14 0.89157319 <a title="139-lda-14" href="./nips-2004-Conditional_Random_Fields_for_Object_Recognition.html">44 nips-2004-Conditional Random Fields for Object Recognition</a></p>
<p>15 0.89153337 <a title="139-lda-15" href="./nips-2004-Planning_for_Markov_Decision_Processes_with_Sparse_Stochasticity.html">147 nips-2004-Planning for Markov Decision Processes with Sparse Stochasticity</a></p>
<p>16 0.88639462 <a title="139-lda-16" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>17 0.87847245 <a title="139-lda-17" href="./nips-2004-Semi-supervised_Learning_via_Gaussian_Processes.html">166 nips-2004-Semi-supervised Learning via Gaussian Processes</a></p>
<p>18 0.87845665 <a title="139-lda-18" href="./nips-2004-Experts_in_a_Markov_Decision_Process.html">64 nips-2004-Experts in a Markov Decision Process</a></p>
<p>19 0.87715322 <a title="139-lda-19" href="./nips-2004-Convergence_and_No-Regret_in_Multiagent_Learning.html">48 nips-2004-Convergence and No-Regret in Multiagent Learning</a></p>
<p>20 0.87685353 <a title="139-lda-20" href="./nips-2004-Co-Validation%3A_Using_Model_Disagreement_on_Unlabeled_Data_to_Validate_Classification_Algorithms.html">38 nips-2004-Co-Validation: Using Model Disagreement on Unlabeled Data to Validate Classification Algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
