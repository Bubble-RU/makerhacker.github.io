<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-109" href="#">nips2000-109</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</h1>
<br/><p>Source: <a title="nips-2000-109-pdf" href="http://papers.nips.cc/paper/1792-redundancy-and-dimensionality-reduction-in-sparse-distributed-representations-of-natural-objects-in-terms-of-their-local-features.pdf">pdf</a></p><p>Author: Penio S. Penev</p><p>Abstract: Low-dimensional representations are key to solving problems in highlevel vision, such as face compression and recognition. Factorial coding strategies for reducing the redundancy present in natural images on the basis of their second-order statistics have been successful in accounting for both psychophysical and neurophysiological properties of early vision. Class-specific representations are presumably formed later, at the higher-level stages of cortical processing. Here we show that when retinotopic factorial codes are derived for ensembles of natural objects, such as human faces, not only redundancy, but also dimensionality is reduced. We also show that objects are built from parts in a non-Gaussian fashion which allows these local-feature codes to have dimensionalities that are substantially lower than the respective Nyquist sampling rates.</p><p>Reference: <a title="nips-2000-109-reference" href="../nips2000_reference/nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu/  Abstract Low-dimensional representations are key to solving problems in highlevel vision, such as face compression and recognition. [sent-5, score-0.118]
</p><p>2 Factorial coding strategies for reducing the redundancy present in natural images on the basis of their second-order statistics have been successful in accounting for both psychophysical and neurophysiological properties of early vision. [sent-6, score-0.253]
</p><p>3 Class-specific representations are presumably formed later, at the higher-level stages of cortical processing. [sent-7, score-0.07]
</p><p>4 Here we show that when retinotopic factorial codes are derived for ensembles of natural objects, such as human faces, not only redundancy, but also dimensionality is reduced. [sent-8, score-0.761]
</p><p>5 We also show that objects are built from parts in a non-Gaussian fashion which allows these local-feature codes to have dimensionalities that are substantially lower than the respective Nyquist sampling rates. [sent-9, score-0.453]
</p><p>6 1 Introduction Sensory systems must take advantage of the statistical structure of their inputs in order to process them efficiently, both to suppress noise and to generate compact representations of seemingly complex data. [sent-10, score-0.132]
</p><p>7 Redundancy reduction has been proposed as a design principle for such systems (Barlow, 1961); in the context of Information Theory (Shannon, 1948), it leads to factorial codes (Barlow et aI. [sent-11, score-0.367]
</p><p>8 In the context of the ensemble of natural images, with a specific model for the noise, these codes have been able to account quantitatively for the contrast sensitivity of human subjects in all signal-to-noise regimes (Atick and Redlich, 1992). [sent-14, score-0.387]
</p><p>9 Moreover, when the receptive fields are constrained to have retinotopic organization, their circularly symmetric, center-surround opponent structure is recovered (Atick and Redlich, 1992). [sent-15, score-0.175]
</p><p>10 When KLT representations are derived for ensembles of natural objects, such as human faces (Sirovich and Kirby, 1987), the factorial codes in the resulting families are naturally low-dimensional (Penev, 1998; Penev and Sirovich, 2000). [sent-18, score-0.704]
</p><p>11 Moreover, when a retinotopic organization is imposed, in a procedure called Local Feature Analysis (LFA), the resulting feed-forward receptive fields are a dense set of detectors for the local features from which the objects are built (Penev and Atick, 1996). [sent-19, score-0.533]
</p><p>12 LFA has also been used to derive local features for the natural-objects ensembles of: 3D surfaces of human heads (Penev and Atick, 1996), and 2D images of pedestrians (Poggio and Girosi, 1998). [sent-20, score-0.363]
</p><p>13 Parts-based representations of object classes, including faces, have been recently derived by Non-negative Matrix Factorization (NMF) (Lee and Seung, 1999), "biologically" motivated by the hypothesis that neural systems are incapable of representing negative values. [sent-21, score-0.121]
</p><p>14 As has already been pointed out (Mel, 1999), this hypothesis is incompatible with a wealth of reliably documented neural phenomena, such as center-surround receptive field organization, excitation and inhibition, and ON/OFF visual-pathway processing, among others. [sent-22, score-0.104]
</p><p>15 2  Compact Global Factorial Codes of Natural Objects  A properly registered and normalized object will be represented by the receptor readout values ¢>(x), where {x} is a grid that contains V receptors. [sent-24, score-0.151]
</p><p>16 An ensemble ofT objects will be denoted by {¢>t (X)}tET. [sent-25, score-0.216]
</p><p>17 , Sirovich and Kirby, 1987, for details), when T > V, its Karhunen-Loeve Tran,lform (KLT) representation is given by  v ¢>t(x)  =L  a~O'r1Pr(x)  (1)  r=1  where {an (arranged in non-increasing order) is the eigen. [sent-28, score-0.038]
</p><p>18 lpectrum of the spatial and temporal correlation matrices, and {1Pr (x)} and {a~} are their respective orthonormal eigenvectors. [sent-29, score-0.183]
</p><p>19 The KLT representation of an arbitrary, possibly out-of-sample, object ¢>(x) is given by the joint activation (2)  of the set of global analysis filters {O';I1Pr(x)}, which are indexed with r, and whose outputs, {a r } , are decorrelated. [sent-30, score-0.295]
</p><p>20 2 In the context of the ensemble of natural images, the "whitening" by the factor 0';1 has been found to account for the contrast sensitivity of human subjects (Atick and Redlich, 1992). [sent-31, score-0.299]
</p><p>21 When the output dimensionality is set to N < V, the reconstruction-optimal in the amount of preserved signal power-and the respective error utilize the global synthesis filters {O'r1Pr (x)}, and are given by N  ¢>ISc = Larar1Pr and ¢>JJ'"  = ¢> -  ¢>ISc. [sent-32, score-0.52]
</p><p>22 (3)  r=1  = =  iPor the illustrations in this study, 2T T 11254 frontal-pose facial images were registered and normalized to a grid with V = 64 x 60 = 3840 pixels as previously described (Penev and Sirovich, 2000). [sent-33, score-0.144]
</p><p>23 2This is certainly true for in-sample objects, since {a~} are orthonormal (1). [sent-34, score-0.033]
</p><p>24 The current ensemble has been found to generalize well in the regime for r that is explored here (Penev and Sirovich, 2000). [sent-36, score-0.121]
</p><p>25 20  60  100  150  220  350  500  700  1000  Figure 1: Successive reconstructions, errors, and local entropy densities. [sent-37, score-0.165]
</p><p>26 For the indicated global dimensionalities, N, the reconstructions cP'iV" (3) of an out-oj-sample example are shown in the top row, and the respective residual errors, cP'Nr , in the middle row (the first two errors are amplified 5 x and the rest-20x). [sent-38, score-0.587]
</p><p>27 The respective entropy densities ON (5), are shown in the bottom, low-pass + UN 2) (cf. [sent-39, score-0.2]
</p><p>28 u; /(u;  With the standard multidimensional Gaussian model for the probability density P[4>] (Moghaddam and Pentland, 1997; Penev, 1998), the information content of the reconstruction (3)-equal to the optimal-code length (Shannon, 1948; Barlow, I96I)-is N  -logP[4>rJ  C  ]  ex  Z)a r 2 . [sent-42, score-0.148]
</p><p>29 I  (4)  r=l  Because of the normalization by fIr in (2), all KLT coefficients have unit variance 0); the model (4) is spherically symmetric, and all filters contribute equally to the entropy of the code. [sent-43, score-0.146]
</p><p>30 Following (Atick and Redlich, 1992), when noise is taken into account, N ~ 400 has been found as an estimate of the global dimensionality for the ensemble frontal-pose faces (Penev and Sirovich, 2000). [sent-45, score-0.445]
</p><p>31 This conclusion is reinforced by the perceptual quality of the successive reconstructions and errors, shown in Fig. [sent-46, score-0.108]
</p><p>32 I-the face-specific information crosses over from the error to the reconstruction at N ~ 400, but not much earlier. [sent-47, score-0.115]
</p><p>33 Also, the filters in the beginning of the hierarchy (Fig. [sent-49, score-0.102]
</p><p>34 2, are global, in contrast with the local, retinotopic organization of sensory processing, found throughout most of the visual system. [sent-53, score-0.185]
</p><p>35 Moreover, although the eigenmodes in the regime r E [100,400] are clearly necessary to preserve the object-specific information (Fig. [sent-54, score-0.087]
</p><p>36 2) are ripply, non-intuitive, and resemble the hierarchy of sine/cosine modes of the translationally invariant ensemble of natural images. [sent-56, score-0.251]
</p><p>37 In order to cope with these problems in the context of object ensembles, analogously to the local factorial retinal code (Atick and Redlich, 1992), Local Feature Analysis (LFA) has been developed (Penev and Atick, 1996; Penev, 1998). [sent-57, score-0.555]
</p><p>38 LFA uses a set of local analysisfil-  Figure 2: The basis-vector hierarchy of the global factorial code. [sent-58, score-0.488]
</p><p>39 centers  a  b  c  d  e  Figure 3: Local feature detectors and residual correlations of their outputs. [sent-60, score-0.192]
</p><p>40 2) is marked with the central positions of five of the feature detectors. [sent-62, score-0.032]
</p><p>41 a-e: For those choises of x"', the local filters K(x", , y) (6) are shown in the top row, and the residual correlations of their respective outputs with the outputs of all the rest, P(x", , y) (9), in the bottom. [sent-63, score-0.515]
</p><p>42 ters, K(x, y), whose outputs are topographically indexed with the grid variable x (cf. [sent-66, score-0.086]
</p><p>43 1 ~ O(x) = V " K(x,y)¢(y)  (5)  y  and are as decorrelated as possible. [sent-69, score-0.033]
</p><p>44 For object ensembles, the process of construction-categorization-breaks a number of symmetries and shifts the higher-order statistics into second-order, where they are conveniently exposed to robust estimation and, subsequently, to redundancy reduction. [sent-71, score-0.169]
</p><p>45 The resulting local receptive fields, some of which are shown in the top row of Fig. [sent-72, score-0.259]
</p><p>46 3, turn out to be feature detectors that are optimally tuned to the structures that appear at their respective centers. [sent-73, score-0.219]
</p><p>47 Although the local factorial code does not exhibit the problems discussed earlier, it has  representational properties that are equivalent to those of the global factorial code. [sent-74, score-0.758]
</p><p>48 For natural objects, the code is low-dimensional (N < V), and residual correlations, some shown in the bottom row of Fig. [sent-76, score-0.379]
</p><p>49 3, are unavoidable; they are generally given by the projector to the sub band  PN(X,y)  ~ ~ LO~(x)O~(y) == K~)(x,y)  (9)  t  and are as close to 8(x, y) as possible (Penev and Atick, 1996). [sent-77, score-0.042]
</p><p>50 The smoothness of the local information density is controlled by the width of the band, as shown in Fig. [sent-78, score-0.175]
</p><p>51 Since O(x) is band limited, it can generally be reconstructed exactly from a subsampling over a limited set of grid points M ~ {xm}, from the IMI variables {Om ~ O(Xm)}x~EM' as long as this density is critically sampled (1M I = N). [sent-80, score-0.239]
</p><p>52 When O(x) is critically sampled (IMI = N) on a regular grid, V -t 00, and the eigenmodes (1) are sines and cosines, then (10) is the familiar Nyquist interpolation formula. [sent-82, score-0.097]
</p><p>53 In a recurrent neural-network implementation (Penev, 1998) the dense output O(x) of the feed-forward receptive fields, K(x,y), has been interpreted as sub-threshold activation, which is predictively suppressed through lateral inhibition with weights Pm (x), by the set of active units, at {xm}. [sent-85, score-0.127]
</p><p>54 3  5  Dimensionality Reduction Beyond the Nyquist Sampling Rate  The efficient allocation of resources by the greedy sparsification is evident in Fig. [sent-86, score-0.271]
</p><p>55 4A), and only a handful of active units are used to describe each individual local feature (Fig. [sent-88, score-0.223]
</p><p>56 Moreover, when the dimensionality of the representation is constrained, evidently from Fig. [sent-90, score-0.175]
</p><p>57 4C-F, the sparse local code has a much better perceptual quality of the reconstruction than the compact global one. [sent-91, score-0.578]
</p><p>58 3This type of sparseness is not to be confused with "high kurtosis of the output distribution;" in LFA, the non-active units are completely shut down, rather than "only weakly activated. [sent-92, score-0.039]
</p><p>59 "  c  B  A  D  E  F  Figure 4: Efficiency of the sparse allocation of resources. [sent-93, score-0.082]
</p><p>60 (A): the locations of the first 25 active units, M(25), of the sparsification with N = 220, n = 0"400 (see Fig. [sent-94, score-0.229]
</p><p>61 1 and in (C), are overlayed on ¢(x) and numbered sequentially. [sent-96, score-0.043]
</p><p>62 (B): the locations of the active units in M(64) are overlayed on O(x). [sent-97, score-0.119]
</p><p>63 1), reconstructions with a fixed dimensionality, 64, of its deviation from the typical face (-1/;1 in Fig. [sent-100, score-0.125]
</p><p>64 2), are shown in the top row of (D, E, F), and the respective errors, in the bottom row. [sent-101, score-0.269]
</p><p>65 (D): reconstruction from the sparsification {O(Xm)}X=EM (10) with M = M(64) from (B). [sent-102, score-0.28]
</p><p>66 (E): reconstruction from the first 64 global coefficients (3), N = 64. [sent-103, score-0.225]
</p><p>67 (F): reconstruction from a subsampling of ¢(x) on a regular 8 x 8 grid (64 samples). [sent-104, score-0.219]
</p><p>68 The errors in (D) and (E) are magnified 5 x ; in (F), 1 x . [sent-105, score-0.044]
</p><p>69 6  07  Rallo of Sparse and Global Dimensionalilies  OB  B  Figure 5: The relationship between the dimensionalities of the global and the local factorial codes. [sent-110, score-0.547]
</p><p>70 The entropy of the KLT reconstruction (8) for the out-of-sample example (cf. [sent-111, score-0.138]
</p><p>71 1) is plotted in (A) with a solid line as a function of the global dimensionality, N. [sent-113, score-0.137]
</p><p>72 The entropies of the LFA reconstructions (10) are shown with dashed lines parametrically of the number of active units 1M I for N E {600, 450, 300, 220, 110,64, 32}, from top to bottom, respectively. [sent-114, score-0.217]
</p><p>73 The ratios of the residual, 110~rr 112, and the total, 11011 2 (8), information are plotted in (B) with dashed lines parametrically of 1M II N, for the same values of N; a true exponential dependence is plotted with a solid line. [sent-115, score-0.064]
</p><p>74 Although the global code is optimal in the amount of captured energy, the greedy sparsification optimizes the amount of captured information, which has been shown to be the biologically relevant measure, at least in the retinal case (Atick and Redlich, 1992). [sent-117, score-0.529]
</p><p>75 In order to quantify the relationship between the local dimensionality of the representation and the amount of information it captures, rate-distortion curves are shown in Fig. [sent-118, score-0.317]
</p><p>76 As expected (4), each degree of freedom in the global code contributes approximately equally to the information content. [sent-120, score-0.291]
</p><p>77 On the other hand, the first few local terms in (10) pull off a sizeable fraction of the total information, with only a modest increase thereafter (Fig. [sent-121, score-0.115]
</p><p>78 In all regimes for N, the residual information decreases approximately exponentially with increasing dimensionality ratio IMI/N (Fig. [sent-123, score-0.289]
</p><p>79 5B); 90% of the information is contained in a representation with local dimensionality, 25%-30% of the respective global one; 99%, with 45%-50%. [sent-124, score-0.467]
</p><p>80 This exponential decrease has been shown to be incompatible with the expectation based on the Gaussian (4), or any other spherical, assumption (Penev, 1999). [sent-125, score-0.043]
</p><p>81 Hence, the LFA representation, by learning the building blocks of natural objects-the local features-reduces not only redundancy, but also dimensionality. [sent-126, score-0.206]
</p><p>82 Because LFA captures aspects of the sparse, non-Gaussian structure of natural-object ensembles, it preserves practically all of the information, while allocating resources substantially below the Nyquist sampling rate. [sent-127, score-0.059]
</p><p>83 6 Discussion Here we have shown that, for ensembles of natural objects, with low-dimensional global factorial representations, sparsification of the local information density allows undersampling which results in a substantial additional dimensionality reduction. [sent-128, score-1.085]
</p><p>84 Although more general ensembles, such as those of natural scenes and natural sound, have full-dimensional global representations, the sensory processing of both visual and auditory signals happens in a multi-scale, bandpass fashion. [sent-129, score-0.415]
</p><p>85 Learning the parts of objects by non-negative matrix factorization. [sent-202, score-0.122]
</p><p>86 Dimensionality reduction by sparsification in a local-features representation of human faces. [sent-232, score-0.361]
</p><p>87 Local Feature Analysis: A general statistical theory for object representation. [sent-243, score-0.051]
</p><p>88 Local Feature Analysis: A flexible statistical framework for dimensionality reduction by sparsification of naturalistic sound. [sent-252, score-0.406]
</p><p>89 Low-dimensional procedure for the characterization of human faces . [sent-293, score-0.131]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('penev', 0.535), ('atick', 0.278), ('factorial', 0.203), ('lfa', 0.192), ('sirovich', 0.192), ('sparsification', 0.192), ('redlich', 0.184), ('ensembles', 0.15), ('respective', 0.15), ('global', 0.137), ('dimensionality', 0.137), ('klt', 0.128), ('nyquist', 0.128), ('objects', 0.122), ('redundancy', 0.118), ('local', 0.115), ('code', 0.1), ('residual', 0.096), ('ensemble', 0.094), ('dimensionalities', 0.092), ('natural', 0.091), ('reconstruction', 0.088), ('rockefeller', 0.086), ('faces', 0.077), ('reconstructions', 0.077), ('reduction', 0.077), ('subsampling', 0.074), ('representations', 0.07), ('barlow', 0.069), ('filters', 0.069), ('imi', 0.067), ('retinotopic', 0.067), ('iordanov', 0.064), ('kirby', 0.064), ('sensory', 0.063), ('compact', 0.062), ('receptive', 0.061), ('codes', 0.059), ('retinal', 0.058), ('grid', 0.057), ('row', 0.056), ('cutoff', 0.055), ('organization', 0.055), ('human', 0.054), ('object', 0.051), ('entropy', 0.05), ('xm', 0.049), ('face', 0.048), ('fields', 0.047), ('shannon', 0.046), ('sparse', 0.045), ('images', 0.044), ('errors', 0.044), ('incompatible', 0.043), ('jaynes', 0.043), ('overlayed', 0.043), ('registered', 0.043), ('greedy', 0.042), ('band', 0.042), ('units', 0.039), ('representation', 0.038), ('detectors', 0.037), ('active', 0.037), ('parametrically', 0.037), ('linsker', 0.037), ('mel', 0.037), ('ftp', 0.037), ('allocation', 0.037), ('girosi', 0.037), ('bottom', 0.036), ('hierarchy', 0.033), ('scenes', 0.033), ('pentland', 0.033), ('decorrelated', 0.033), ('eigenmodes', 0.033), ('orthonormal', 0.033), ('translationally', 0.033), ('poggio', 0.033), ('critically', 0.033), ('density', 0.033), ('sensitivity', 0.032), ('feature', 0.032), ('perceptual', 0.031), ('moghaddam', 0.031), ('ruderman', 0.031), ('interpolation', 0.031), ('substantially', 0.03), ('dense', 0.029), ('kj', 0.029), ('preserves', 0.029), ('regimes', 0.029), ('outputs', 0.029), ('context', 0.028), ('top', 0.027), ('fr', 0.027), ('centers', 0.027), ('utilize', 0.027), ('regime', 0.027), ('information', 0.027), ('equally', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="109-tfidf-1" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>Author: Penio S. Penev</p><p>Abstract: Low-dimensional representations are key to solving problems in highlevel vision, such as face compression and recognition. Factorial coding strategies for reducing the redundancy present in natural images on the basis of their second-order statistics have been successful in accounting for both psychophysical and neurophysiological properties of early vision. Class-specific representations are presumably formed later, at the higher-level stages of cortical processing. Here we show that when retinotopic factorial codes are derived for ensembles of natural objects, such as human faces, not only redundancy, but also dimensionality is reduced. We also show that objects are built from parts in a non-Gaussian fashion which allows these local-feature codes to have dimensionalities that are substantially lower than the respective Nyquist sampling rates.</p><p>2 0.11399791 <a title="109-tfidf-2" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>Author: Rafal Bogacz, Malcolm W. Brown, Christophe G. Giraud-Carrier</p><p>Abstract: Olshausen & Field demonstrated that a learning algorithm that attempts to generate a sparse code for natural scenes develops a complete family of localised, oriented, bandpass receptive fields, similar to those of 'simple cells' in VI. This paper describes an algorithm which finds a sparse code for sequences of images that preserves information about the input. This algorithm when trained on natural video sequences develops bases representing the movement in particular directions with particular speeds, similar to the receptive fields of the movement-sensitive cells observed in cortical visual areas. Furthermore, in contrast to previous approaches to learning direction selectivity, the timing of neuronal activity encodes the phase of the movement, so the precise timing of spikes is crucially important to the information encoding.</p><p>3 0.10996415 <a title="109-tfidf-3" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>Author: Michael S. Gray, Terrence J. Sejnowski, Javier R. Movellan</p><p>Abstract: We examine eight different techniques for developing visual representations in machine vision tasks. In particular we compare different versions of principal component and independent component analysis in combination with stepwise regression methods for variable selection. We found that local methods, based on the statistics of image patches, consistently outperformed global methods based on the statistics of entire images. This result is consistent with previous work on emotion and facial expression recognition. In addition, the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance. 1</p><p>4 0.093411095 <a title="109-tfidf-4" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>Author: Odelia Schwartz, Eero P. Simoncelli</p><p>Abstract: We explore the statistical properties of natural sound stimuli preprocessed with a bank of linear filters. The responses of such filters exhibit a striking form of statistical dependency, in which the response variance of each filter grows with the response amplitude of filters tuned for nearby frequencies. These dependencies may be substantially reduced using an operation known as divisive normalization, in which the response of each filter is divided by a weighted sum of the rectified responses of other filters. The weights may be chosen to maximize the independence of the normalized responses for an ensemble of natural sounds. We demonstrate that the resulting model accounts for nonlinearities in the response characteristics of the auditory nerve, by comparing model simulations to electrophysiological recordings. In previous work (NIPS, 1998) we demonstrated that an analogous model derived from the statistics of natural images accounts for non-linear properties of neurons in primary visual cortex. Thus, divisive normalization appears to be a generic mechanism for eliminating a type of statistical dependency that is prevalent in natural signals of different modalities. Signals in the real world are highly structured. For example, natural sounds typically contain both harmonic and rythmic structure. It is reasonable to assume that biological auditory systems are designed to represent these structures in an efficient manner [e.g., 1,2]. Specifically, Barlow hypothesized that a role of early sensory processing is to remove redundancy in the sensory input, resulting in a set of neural responses that are statistically independent. Experimentally, one can test this hypothesis by examining the statistical properties of neural responses under natural stimulation conditions [e.g., 3,4], or the statistical dependency of pairs (or groups) of neural responses. Due to their technical difficulty, such multi-cellular experiments are only recently becoming possible, and the earliest reports in vision appear consistent with the hypothesis [e.g., 5]. An alternative approach, which we follow here, is to develop a neural model from the statistics of natural signals and show that response properties of this model are similar to those of biological sensory neurons. A number of researchers have derived linear filter models using statistical criterion. For visual images, this results in linear filters localized in frequency, orientation and phase [6, 7]. Similar work in audition has yielded filters localized in frequency and phase [8]. Although these linear models provide an important starting point for neural modeling, sensory neurons are highly nonlinear. In addition, the statistical properties of natural signals are too complex to expect a linear transformation to result in an independent set of components. Recent results indicate that nonlinear gain control plays an important role in neural processing. Ruderman and Bialek [9] have shown that division by a local estimate of standard deviation can increase the entropy of responses of center-surround filters to natural images. Such a model is consistent with the properties of neurons in the retina and lateral geniculate nucleus. Heeger and colleagues have shown that the nonlinear behaviors of neurons in primary visual cortex may be described using a form of gain control known as divisive normalization [10], in which the response of a linear kernel is rectified and divided by the sum of other rectified kernel responses and a constant. We have recently shown that the responses of oriented linear filters exhibit nonlinear statistical dependencies that may be substantially reduced using a variant of this model, in which the normalization signal is computed from a weighted sum of other rectified kernel responses [11, 12]. The resulting model, with weighting parameters determined from image statistics, accounts qualitatively for physiological nonlinearities observed in primary visual cortex. In this paper, we demonstrate that the responses of bandpass linear filters to natural sounds exhibit striking statistical dependencies, analogous to those found in visual images. A divisive normalization procedure can substantially remove these dependencies. We show that this model, with parameters optimized for a collection of natural sounds, can account for nonlinear behaviors of neurons at the level of the auditory nerve. Specifically, we show that: 1) the shape offrequency tuning curves varies with sound pressure level, even though the underlying linear filters are fixed; and 2) superposition of a non-optimal tone suppresses the response of a linear filter in a divisive fashion, and the amount of suppression depends on the distance between the frequency of the tone and the preferred frequency of the filter. 1 Empirical observations of natural sound statistics The basic statistical properties of natural sounds, as observed through a linear filter, have been previously documented by Attias [13]. In particular, he showed that, as with visual images, the spectral energy falls roughly according to a power law, and that the histograms of filter responses are more kurtotic than a Gaussian (i.e., they have a sharp peak at zero, and very long tails). Here we examine the joint statistical properties of a pair of linear filters tuned for nearby temporal frequencies. We choose a fixed set of filters that have been widely used in modeling the peripheral auditory system [14]. Figure 1 shows joint histograms of the instantaneous responses of a particular pair of linear filters to five different types of natural sound, and white noise. First note that the responses are approximately decorrelated: the expected value of the y-axis value is roughly zero for all values of the x-axis variable. The responses are not, however, statistically independent: the width of the distribution of responses of one filter increases with the response amplitude of the other filter. If the two responses were statistically independent, then the response of the first filter should not provide any information about the distribution of responses of the other filter. We have found that this type of variance dependency (sometimes accompanied by linear correlation) occurs in a wide range of natural sounds, ranging from animal sounds to music. We emphasize that this dependency is a property of natural sounds, and is not due purely to our choice of linear filters. For example, no such dependency is observed when the input consists of white noise (see Fig. 1). The strength of this dependency varies for different pairs of linear filters . In addition, we see this type of dependency between instantaneous responses of a single filter at two Speech o -1 Drums • Monkey Cat White noise Nocturnal nature I~ ~; ~ • Figure 1: Joint conditional histogram of instantaneous linear responses of two bandpass filters with center frequencies 2000 and 2840 Hz. Pixel intensity corresponds to frequency of occurrence of a given pair of values, except that each column has been independently rescaled to fill the full intensity range. For the natural sounds, responses are not independent: the standard deviation of the ordinate is roughly proportional to the magnitude of the abscissa. Natural sounds were recorded from CDs and converted to sampling frequency of 22050 Hz. nearby time instants. Since the dependency involves the variance of the responses, we can substantially reduce it by dividing. In particular, the response of each filter is divided by a weighted sum of responses of other rectified filters and an additive constant. Specifically: L2 Ri = 2: (1) 12 j WjiLj + 0'2 where Li is the instantaneous linear response of filter i, strength of suppression of filter i by filter j. 0' is a constant and Wji controls the We would like to choose the parameters of the model (the weights Wji, and the constant 0') to optimize the independence of the normalized response to an ensemble of natural sounds. Such an optimization is quite computationally expensive. We instead assume a Gaussian form for the underlying conditional distribution, as described in [15]: P (LiILj,j E Ni ) '</p><p>5 0.092230603 <a title="109-tfidf-5" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>Author: Shimon Edelman, Nathan Intrator</p><p>Abstract: We describe a unified framework for the understanding of structure representation in primate vision. A model derived from this framework is shown to be effectively systematic in that it has the ability to interpret and associate together objects that are related through a rearrangement of common</p><p>6 0.079635039 <a title="109-tfidf-6" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>7 0.075545825 <a title="109-tfidf-7" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>8 0.072648346 <a title="109-tfidf-8" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>9 0.068093419 <a title="109-tfidf-9" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>10 0.065340705 <a title="109-tfidf-10" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>11 0.064106867 <a title="109-tfidf-11" href="./nips-2000-Error-correcting_Codes_on_a_Bethe-like_Lattice.html">47 nips-2000-Error-correcting Codes on a Bethe-like Lattice</a></p>
<p>12 0.058051135 <a title="109-tfidf-12" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>13 0.057251651 <a title="109-tfidf-13" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>14 0.055826031 <a title="109-tfidf-14" href="./nips-2000-From_Mixtures_of_Mixtures_to_Adaptive_Transform_Coding.html">59 nips-2000-From Mixtures of Mixtures to Adaptive Transform Coding</a></p>
<p>15 0.053350676 <a title="109-tfidf-15" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>16 0.052516304 <a title="109-tfidf-16" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>17 0.052007832 <a title="109-tfidf-17" href="./nips-2000-One_Microphone_Source_Separation.html">96 nips-2000-One Microphone Source Separation</a></p>
<p>18 0.047818009 <a title="109-tfidf-18" href="./nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">88 nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>19 0.04551512 <a title="109-tfidf-19" href="./nips-2000-An_Information_Maximization_Approach_to_Overcomplete_and_Recurrent_Representations.html">24 nips-2000-An Information Maximization Approach to Overcomplete and Recurrent Representations</a></p>
<p>20 0.044111792 <a title="109-tfidf-20" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.164), (1, -0.093), (2, 0.014), (3, 0.095), (4, -0.024), (5, 0.04), (6, 0.088), (7, -0.1), (8, 0.137), (9, -0.026), (10, -0.076), (11, -0.03), (12, -0.002), (13, 0.091), (14, -0.115), (15, 0.023), (16, -0.092), (17, 0.073), (18, -0.076), (19, -0.119), (20, 0.019), (21, -0.029), (22, 0.001), (23, 0.01), (24, 0.074), (25, -0.032), (26, 0.193), (27, 0.052), (28, 0.055), (29, -0.138), (30, 0.047), (31, 0.143), (32, 0.027), (33, 0.123), (34, -0.113), (35, 0.008), (36, 0.021), (37, 0.009), (38, -0.032), (39, -0.122), (40, 0.012), (41, -0.04), (42, 0.103), (43, 0.014), (44, -0.081), (45, 0.07), (46, -0.1), (47, 0.045), (48, -0.034), (49, -0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96787494 <a title="109-lsi-1" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>Author: Penio S. Penev</p><p>Abstract: Low-dimensional representations are key to solving problems in highlevel vision, such as face compression and recognition. Factorial coding strategies for reducing the redundancy present in natural images on the basis of their second-order statistics have been successful in accounting for both psychophysical and neurophysiological properties of early vision. Class-specific representations are presumably formed later, at the higher-level stages of cortical processing. Here we show that when retinotopic factorial codes are derived for ensembles of natural objects, such as human faces, not only redundancy, but also dimensionality is reduced. We also show that objects are built from parts in a non-Gaussian fashion which allows these local-feature codes to have dimensionalities that are substantially lower than the respective Nyquist sampling rates.</p><p>2 0.56732851 <a title="109-lsi-2" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>Author: Rafal Bogacz, Malcolm W. Brown, Christophe G. Giraud-Carrier</p><p>Abstract: Olshausen & Field demonstrated that a learning algorithm that attempts to generate a sparse code for natural scenes develops a complete family of localised, oriented, bandpass receptive fields, similar to those of 'simple cells' in VI. This paper describes an algorithm which finds a sparse code for sequences of images that preserves information about the input. This algorithm when trained on natural video sequences develops bases representing the movement in particular directions with particular speeds, similar to the receptive fields of the movement-sensitive cells observed in cortical visual areas. Furthermore, in contrast to previous approaches to learning direction selectivity, the timing of neuronal activity encodes the phase of the movement, so the precise timing of spikes is crucially important to the information encoding.</p><p>3 0.53308553 <a title="109-lsi-3" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>Author: Michael S. Gray, Terrence J. Sejnowski, Javier R. Movellan</p><p>Abstract: We examine eight different techniques for developing visual representations in machine vision tasks. In particular we compare different versions of principal component and independent component analysis in combination with stepwise regression methods for variable selection. We found that local methods, based on the statistics of image patches, consistently outperformed global methods based on the statistics of entire images. This result is consistent with previous work on emotion and facial expression recognition. In addition, the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance. 1</p><p>4 0.49420524 <a title="109-lsi-4" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>Author: Te-Won Lee, Thomas Wachtler, Terrence J. Sejnowski</p><p>Abstract: The human visual system encodes the chromatic signals conveyed by the three types of retinal cone photoreceptors in an opponent fashion. This color opponency has been shown to constitute an efficient encoding by spectral decorrelation of the receptor signals. We analyze the spatial and chromatic structure of natural scenes by decomposing the spectral images into a set of linear basis functions such that they constitute a representation with minimal redundancy. Independent component analysis finds the basis functions that transforms the spatiochromatic data such that the outputs (activations) are statistically as independent as possible, i.e. least redundant. The resulting basis functions show strong opponency along an achromatic direction (luminance edges), along a blueyellow direction, and along a red-blue direction. Furthermore, the resulting activations have very sparse distributions, suggesting that the use of color opponency in the human visual system achieves a highly efficient representation of colors. Our findings suggest that color opponency is a result of the properties of natural spectra and not solely a consequence of the overlapping cone spectral sensitivities. 1 Statistical structure of natural scenes Efficient encoding of visual sensory information is an important task for information processing systems and its study may provide insights into coding principles of biological visual systems. An important goal of sensory information processing Electronic version available at www. cnl. salk . edu/</p><p>5 0.47711629 <a title="109-lsi-5" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>Author: Odelia Schwartz, Eero P. Simoncelli</p><p>Abstract: We explore the statistical properties of natural sound stimuli preprocessed with a bank of linear filters. The responses of such filters exhibit a striking form of statistical dependency, in which the response variance of each filter grows with the response amplitude of filters tuned for nearby frequencies. These dependencies may be substantially reduced using an operation known as divisive normalization, in which the response of each filter is divided by a weighted sum of the rectified responses of other filters. The weights may be chosen to maximize the independence of the normalized responses for an ensemble of natural sounds. We demonstrate that the resulting model accounts for nonlinearities in the response characteristics of the auditory nerve, by comparing model simulations to electrophysiological recordings. In previous work (NIPS, 1998) we demonstrated that an analogous model derived from the statistics of natural images accounts for non-linear properties of neurons in primary visual cortex. Thus, divisive normalization appears to be a generic mechanism for eliminating a type of statistical dependency that is prevalent in natural signals of different modalities. Signals in the real world are highly structured. For example, natural sounds typically contain both harmonic and rythmic structure. It is reasonable to assume that biological auditory systems are designed to represent these structures in an efficient manner [e.g., 1,2]. Specifically, Barlow hypothesized that a role of early sensory processing is to remove redundancy in the sensory input, resulting in a set of neural responses that are statistically independent. Experimentally, one can test this hypothesis by examining the statistical properties of neural responses under natural stimulation conditions [e.g., 3,4], or the statistical dependency of pairs (or groups) of neural responses. Due to their technical difficulty, such multi-cellular experiments are only recently becoming possible, and the earliest reports in vision appear consistent with the hypothesis [e.g., 5]. An alternative approach, which we follow here, is to develop a neural model from the statistics of natural signals and show that response properties of this model are similar to those of biological sensory neurons. A number of researchers have derived linear filter models using statistical criterion. For visual images, this results in linear filters localized in frequency, orientation and phase [6, 7]. Similar work in audition has yielded filters localized in frequency and phase [8]. Although these linear models provide an important starting point for neural modeling, sensory neurons are highly nonlinear. In addition, the statistical properties of natural signals are too complex to expect a linear transformation to result in an independent set of components. Recent results indicate that nonlinear gain control plays an important role in neural processing. Ruderman and Bialek [9] have shown that division by a local estimate of standard deviation can increase the entropy of responses of center-surround filters to natural images. Such a model is consistent with the properties of neurons in the retina and lateral geniculate nucleus. Heeger and colleagues have shown that the nonlinear behaviors of neurons in primary visual cortex may be described using a form of gain control known as divisive normalization [10], in which the response of a linear kernel is rectified and divided by the sum of other rectified kernel responses and a constant. We have recently shown that the responses of oriented linear filters exhibit nonlinear statistical dependencies that may be substantially reduced using a variant of this model, in which the normalization signal is computed from a weighted sum of other rectified kernel responses [11, 12]. The resulting model, with weighting parameters determined from image statistics, accounts qualitatively for physiological nonlinearities observed in primary visual cortex. In this paper, we demonstrate that the responses of bandpass linear filters to natural sounds exhibit striking statistical dependencies, analogous to those found in visual images. A divisive normalization procedure can substantially remove these dependencies. We show that this model, with parameters optimized for a collection of natural sounds, can account for nonlinear behaviors of neurons at the level of the auditory nerve. Specifically, we show that: 1) the shape offrequency tuning curves varies with sound pressure level, even though the underlying linear filters are fixed; and 2) superposition of a non-optimal tone suppresses the response of a linear filter in a divisive fashion, and the amount of suppression depends on the distance between the frequency of the tone and the preferred frequency of the filter. 1 Empirical observations of natural sound statistics The basic statistical properties of natural sounds, as observed through a linear filter, have been previously documented by Attias [13]. In particular, he showed that, as with visual images, the spectral energy falls roughly according to a power law, and that the histograms of filter responses are more kurtotic than a Gaussian (i.e., they have a sharp peak at zero, and very long tails). Here we examine the joint statistical properties of a pair of linear filters tuned for nearby temporal frequencies. We choose a fixed set of filters that have been widely used in modeling the peripheral auditory system [14]. Figure 1 shows joint histograms of the instantaneous responses of a particular pair of linear filters to five different types of natural sound, and white noise. First note that the responses are approximately decorrelated: the expected value of the y-axis value is roughly zero for all values of the x-axis variable. The responses are not, however, statistically independent: the width of the distribution of responses of one filter increases with the response amplitude of the other filter. If the two responses were statistically independent, then the response of the first filter should not provide any information about the distribution of responses of the other filter. We have found that this type of variance dependency (sometimes accompanied by linear correlation) occurs in a wide range of natural sounds, ranging from animal sounds to music. We emphasize that this dependency is a property of natural sounds, and is not due purely to our choice of linear filters. For example, no such dependency is observed when the input consists of white noise (see Fig. 1). The strength of this dependency varies for different pairs of linear filters . In addition, we see this type of dependency between instantaneous responses of a single filter at two Speech o -1 Drums • Monkey Cat White noise Nocturnal nature I~ ~; ~ • Figure 1: Joint conditional histogram of instantaneous linear responses of two bandpass filters with center frequencies 2000 and 2840 Hz. Pixel intensity corresponds to frequency of occurrence of a given pair of values, except that each column has been independently rescaled to fill the full intensity range. For the natural sounds, responses are not independent: the standard deviation of the ordinate is roughly proportional to the magnitude of the abscissa. Natural sounds were recorded from CDs and converted to sampling frequency of 22050 Hz. nearby time instants. Since the dependency involves the variance of the responses, we can substantially reduce it by dividing. In particular, the response of each filter is divided by a weighted sum of responses of other rectified filters and an additive constant. Specifically: L2 Ri = 2: (1) 12 j WjiLj + 0'2 where Li is the instantaneous linear response of filter i, strength of suppression of filter i by filter j. 0' is a constant and Wji controls the We would like to choose the parameters of the model (the weights Wji, and the constant 0') to optimize the independence of the normalized response to an ensemble of natural sounds. Such an optimization is quite computationally expensive. We instead assume a Gaussian form for the underlying conditional distribution, as described in [15]: P (LiILj,j E Ni ) '</p><p>6 0.47583392 <a title="109-lsi-6" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>7 0.41978922 <a title="109-lsi-7" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>8 0.39951256 <a title="109-lsi-8" href="./nips-2000-Error-correcting_Codes_on_a_Bethe-like_Lattice.html">47 nips-2000-Error-correcting Codes on a Bethe-like Lattice</a></p>
<p>9 0.34433734 <a title="109-lsi-9" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>10 0.31361625 <a title="109-lsi-10" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>11 0.31182715 <a title="109-lsi-11" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>12 0.31001997 <a title="109-lsi-12" href="./nips-2000-From_Mixtures_of_Mixtures_to_Adaptive_Transform_Coding.html">59 nips-2000-From Mixtures of Mixtures to Adaptive Transform Coding</a></p>
<p>13 0.30777699 <a title="109-lsi-13" href="./nips-2000-Algorithms_for_Non-negative_Matrix_Factorization.html">22 nips-2000-Algorithms for Non-negative Matrix Factorization</a></p>
<p>14 0.28594384 <a title="109-lsi-14" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>15 0.28585681 <a title="109-lsi-15" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>16 0.27302608 <a title="109-lsi-16" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>17 0.26736918 <a title="109-lsi-17" href="./nips-2000-The_Manhattan_World_Assumption%3A_Regularities_in_Scene_Statistics_which_Enable_Bayesian_Inference.html">135 nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</a></p>
<p>18 0.25045153 <a title="109-lsi-18" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<p>19 0.24327001 <a title="109-lsi-19" href="./nips-2000-Analysis_of_Bit_Error_Probability_of_Direct-Sequence_CDMA_Multiuser_Demodulators.html">25 nips-2000-Analysis of Bit Error Probability of Direct-Sequence CDMA Multiuser Demodulators</a></p>
<p>20 0.24154091 <a title="109-lsi-20" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.016), (4, 0.026), (10, 0.038), (16, 0.013), (17, 0.144), (24, 0.331), (32, 0.015), (33, 0.028), (42, 0.012), (55, 0.042), (62, 0.018), (65, 0.031), (67, 0.073), (75, 0.014), (76, 0.03), (79, 0.021), (81, 0.028), (90, 0.02), (97, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84608489 <a title="109-lda-1" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>Author: Penio S. Penev</p><p>Abstract: Low-dimensional representations are key to solving problems in highlevel vision, such as face compression and recognition. Factorial coding strategies for reducing the redundancy present in natural images on the basis of their second-order statistics have been successful in accounting for both psychophysical and neurophysiological properties of early vision. Class-specific representations are presumably formed later, at the higher-level stages of cortical processing. Here we show that when retinotopic factorial codes are derived for ensembles of natural objects, such as human faces, not only redundancy, but also dimensionality is reduced. We also show that objects are built from parts in a non-Gaussian fashion which allows these local-feature codes to have dimensionalities that are substantially lower than the respective Nyquist sampling rates.</p><p>2 0.79945874 <a title="109-lda-2" href="./nips-2000-An_Adaptive_Metric_Machine_for_Pattern_Classification.html">23 nips-2000-An Adaptive Metric Machine for Pattern Classification</a></p>
<p>Author: Carlotta Domeniconi, Jing Peng, Dimitrios Gunopulos</p><p>Abstract: Nearest neighbor classification assumes locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with finite samples due to the curse of dimensionality. Severe bias can be introduced under these conditions when using the nearest neighbor rule. We propose a locally adaptive nearest neighbor classification method to try to minimize bias. We use a Chi-squared distance analysis to compute a flexible metric for producing neighborhoods that are elongated along less relevant feature dimensions and constricted along most influential ones. As a result, the class conditional probabilities tend to be smoother in the modified neighborhoods, whereby better classification performance can be achieved. The efficacy of our method is validated and compared against other techniques using a variety of real world data. 1</p><p>3 0.63222265 <a title="109-lda-3" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass, Eduardo D. Sontag, Anthony M. Zador</p><p>Abstract: Experimental data show that biological synapses behave quite differently from the symbolic synapses in common artificial neural network models. Biological synapses are dynamic, i.e., their</p><p>4 0.4508231 <a title="109-lda-4" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>Author: Michael S. Gray, Terrence J. Sejnowski, Javier R. Movellan</p><p>Abstract: We examine eight different techniques for developing visual representations in machine vision tasks. In particular we compare different versions of principal component and independent component analysis in combination with stepwise regression methods for variable selection. We found that local methods, based on the statistics of image patches, consistently outperformed global methods based on the statistics of entire images. This result is consistent with previous work on emotion and facial expression recognition. In addition, the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance. 1</p><p>5 0.45014316 <a title="109-lda-5" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>Author: Rafal Bogacz, Malcolm W. Brown, Christophe G. Giraud-Carrier</p><p>Abstract: Olshausen & Field demonstrated that a learning algorithm that attempts to generate a sparse code for natural scenes develops a complete family of localised, oriented, bandpass receptive fields, similar to those of 'simple cells' in VI. This paper describes an algorithm which finds a sparse code for sequences of images that preserves information about the input. This algorithm when trained on natural video sequences develops bases representing the movement in particular directions with particular speeds, similar to the receptive fields of the movement-sensitive cells observed in cortical visual areas. Furthermore, in contrast to previous approaches to learning direction selectivity, the timing of neuronal activity encodes the phase of the movement, so the precise timing of spikes is crucially important to the information encoding.</p><p>6 0.44704282 <a title="109-lda-6" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>7 0.44377157 <a title="109-lda-7" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>8 0.44159201 <a title="109-lda-8" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>9 0.43947449 <a title="109-lda-9" href="./nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">95 nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>10 0.43845955 <a title="109-lda-10" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>11 0.43786079 <a title="109-lda-11" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>12 0.43768734 <a title="109-lda-12" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>13 0.43458584 <a title="109-lda-13" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>14 0.43411651 <a title="109-lda-14" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>15 0.43164274 <a title="109-lda-15" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>16 0.43117473 <a title="109-lda-16" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>17 0.43019313 <a title="109-lda-17" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>18 0.43000028 <a title="109-lda-18" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>19 0.42871305 <a title="109-lda-19" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<p>20 0.42861924 <a title="109-lda-20" href="./nips-2000-Foundations_for_a_Circuit_Complexity_Theory_of_Sensory_Processing.html">56 nips-2000-Foundations for a Circuit Complexity Theory of Sensory Processing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
