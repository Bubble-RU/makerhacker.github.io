<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>135 nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-135" href="#">nips2000-135</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>135 nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</h1>
<br/><p>Source: <a title="nips-2000-135-pdf" href="http://papers.nips.cc/paper/1804-the-manhattan-world-assumption-regularities-in-scene-statistics-which-enable-bayesian-inference.pdf">pdf</a></p><p>Author: James M. Coughlan, Alan L. Yuille</p><p>Abstract: Preliminary work by the authors made use of the so-called</p><p>Reference: <a title="nips-2000-135-reference" href="../nips2000_reference/nips-2000-The_Manhattan_World_Assumption%3A_Regularities_in_Scene_Statistics_which_Enable_Bayesian_Inference_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The Manhattan World Assumption: Regularities in scene statistics which enable Bayesian inference  James M. [sent-1, score-0.175]
</p><p>2 org  Abstract Preliminary work by the authors made use of the so-called "Manhattan world" assumption about the scene statistics of city and indoor scenes. [sent-10, score-0.436]
</p><p>3 This assumption stated that such scenes were built on a cartesian grid which led to regularities in the image edge gradient statistics. [sent-11, score-0.858]
</p><p>4 In this paper we explore the general applicability of this assumption and show that, surprisingly, it holds in a large variety of less structured environments including rural scenes. [sent-12, score-0.23]
</p><p>5 This enables us, from a single image, to determine the orientation of the viewer relative to the scene structure and also to detect target objects which are not aligned with the grid. [sent-13, score-0.586]
</p><p>6 on the image gradient statistics) learnt from real data. [sent-16, score-0.237]
</p><p>7 1  Introduction  In recent years, there has been growing interest in the statistics of natural images (see Huang and Mumford [4] for a recent review). [sent-17, score-0.139]
</p><p>8 Our focus, however, is on the discovery of scene statistics which are useful for solving visual inference problems. [sent-18, score-0.175]
</p><p>9 For example, in related work [5] we have analyzed the statistics of filter responses on and off edges and hence derived effective edge detectors. [sent-19, score-0.375]
</p><p>10 In this paper we present results on statistical regularities of the image gradient responses as a function of the global scene structure. [sent-20, score-0.436]
</p><p>11 This builds on preliminary work [2] on city and indoor scenes. [sent-21, score-0.183]
</p><p>12 This work observed that such scenes are based on a cartesian coordinate system which puts (probabilistic) constraints on the image gradient statistics. [sent-22, score-0.503]
</p><p>13 Our current work shows that this so-called "Manhattan world" assumption about the scene statistics applies far more generally than urban scenes. [sent-23, score-0.296]
</p><p>14 Many rural scenes contain sufficient structure on the distribution of edges to provide a natural cartesian reference frame for the viewer. [sent-24, score-0.619]
</p><p>15 The viewers' orientation relative to this frame can be determined by Bayesian inference. [sent-25, score-0.292]
</p><p>16 In addition, certain structures in the scene stand out by being unaligned to this natural reference frame. [sent-26, score-0.317]
</p><p>17 In our theory such  structures appear as "outlier" edges which makes it easier to detect them. [sent-27, score-0.122]
</p><p>18 Informal evidence that human observers use a form of the Manhattan world assumption is provided by the Ames room illusion, see figure (6), where the observers appear to erroneously make this assumption, thereby grotesquely distorting the sizes of objects in the room. [sent-28, score-0.565]
</p><p>19 2  Previous Work and Three- Dimensional Geometry  Our preliminary work on city scenes was presented in [2]. [sent-29, score-0.21]
</p><p>20 There is related work in computer vision for the detection of vanishing points in 3-d scenes [1], [6] (which proceeds through the stages of edge detection, grouping by Hough transforms, and finally the estimation of the geometry). [sent-30, score-0.525]
</p><p>21 We refer the reader to [3] for details on the geometry of the Manhattan world and report only the main results here. [sent-31, score-0.167]
</p><p>22 Briefly, we calculate expressions for the orientations of x, y, z lines imaged under perspective projection in terms of the orientation of the camera relative to the x, y, z axes. [sent-32, score-0.618]
</p><p>23 The camera orientation relative to the xyz axis system may be specified by three Euler angles: the azimuth (or compass angle) a, corresponding to rotation about the z axis, the elevation (3 above the xy plane, and the twist'Y about the camera's line of sight. [sent-33, score-0.585]
</p><p>24 We use ~ = (a, (3, 'Y) to denote all three Euler angles of the camera orientation. [sent-34, score-0.169]
</p><p>25 Our previous work [2] assumed that the elevation and twist were both zero which turned out to be invalid for many of the images presented in this paper. [sent-35, score-0.138]
</p><p>26 We can then compute the normal orientation of lines parallel to the x, y, z axes, measured in the image plane, as a function of film coordinates (u, v) and the camera orientation ~. [sent-36, score-1.019]
</p><p>27 We express the results in terms of orthogonal unit camera axes ii, b and c, which are aligned to the body of the camera and are determined by ~. [sent-37, score-0.38]
</p><p>28 For x lines (see Figure 1, left panel) we have tan Ox = -(ucx + fax)/(vc x + fb x ), where Ox is the normal orientation of the x line at film coordinates (u, v) and f is the focal length of the camera. [sent-38, score-0.665]
</p><p>29 Similarly, tanOy = -(ucy + fay)/(vc y + fb y) for y lines and tanOz = -(ucz + faz)/(vc z + fb z ) for z lines. [sent-39, score-0.313]
</p><p>30 In the next section will see how to relate the normal orientation of an object boundary (such as x,y,z lines) at a point (u, v) to the magnitude and direction of the image gradient at that location. [sent-40, score-0.696]
</p><p>31 I ~e  ~u vanishing point  ~I I~  Figure 1: (Left) Geometry of an x line projected onto (u,v) image plane. [sent-41, score-0.268]
</p><p>32 0 is the normal orientation of the line in the image. [sent-42, score-0.368]
</p><p>33 (Right) Histogram of edge orientation error (displayed modulo 180°). [sent-43, score-0.506]
</p><p>34 Observe the strong peak at 0°, indicating that the image gradient direction at an edge is usually very close to the true normal orientation of the edge. [sent-44, score-0.822]
</p><p>35 3  Pon and Poff : Characterizing Edges Statistically  Since we do not know where the x, y, z lines are in the image, we have to infer their locations and orientations from image gradient information. [sent-45, score-0.466]
</p><p>36 A key element of our approach is that it allows the model to infer camera orientation without having to group pixels into x, y, z lines. [sent-47, score-0.527]
</p><p>37 Most grouping procedures rely on the use of binary edge maps which often make premature decisions based on too little information. [sent-48, score-0.209]
</p><p>38 The poor quality of some of the images - underexposed and overexposed - makes edge detection particularly difficult, as well as the fact that some of the images lack x, y, z lines that are long enough to group reliably. [sent-49, score-0.595]
</p><p>39 (Ea ) for the probabilities of the image gradient magnitude Ea at position it in the image conditioned on whether we are on or off an edge. [sent-51, score-0.43]
</p><p>40 These distributions quantify the tendency for the image gradient to be high on object boundaries and low off them, see Figure 2. [sent-52, score-0.281]
</p><p>41 They were learned by Konishi et al for the Sowerby image database which contains one hundred presegmented images. [sent-53, score-0.154]
</p><p>42 (Y) (left) and Pon(y)(right), the empirical histograms of edge re-  IVII  is quantized to sponses off and on edges, respectively. [sent-55, score-0.246]
</p><p>43 (Y) occurs at a lower edge response than the peak of Pon (y). [sent-58, score-0.209]
</p><p>44 We extend the work of Konishi et al by putting probability distributions on how accurately the image gradient direction estimates the true normal direction of the edge. [sent-59, score-0.418]
</p><p>45 These were learned for this dataset by measuring the true orientations of the edges and comparing them to those estimated from the image gradients. [sent-60, score-0.346]
</p><p>46 This gives us distributions on the magnitude and direction of the intensity gradient Pon CEaIB), Pof! [sent-61, score-0.181]
</p><p>47 CEa), where Ea = (Ea, CPa), B is the true normal orientation of the edge, and CPa is the gradient direction measured at point it = (u, v). [sent-62, score-0.459]
</p><p>48 We make a factorization assumption that Pon(EaIB) = Pon(Ea)Pang(CPa - B) and POf! [sent-63, score-0.078]
</p><p>49 4  Bayesian Model  We devised a Bayesian model which combines knowledge of the three-dimensional geometry of the Manhattan world with statistical knowledge of edges in images. [sent-75, score-0.289]
</p><p>50 The model assumes that, while the majority of pixels in the image convey no information about camera orientation, most of the pixels with high edge responses arise from the presence of x, y, z lines in the three-dimensional scene. [sent-76, score-0.797]
</p><p>51 An important feature of the Bayesian model is that it does not force us to decide prematurely which pixels  are on and off an object boundary (or whether an on pixel is due to x,y, or z), but allows us to sum over all possible interpretations of each pixel. [sent-77, score-0.169]
</p><p>52 The prior probability P(mil) of each of the edge models was estimated empirically to be 0. [sent-79, score-0.209]
</p><p>53 Using the factorization assumption mentioned before, we assume the probability of the image data Eil has two factors, one for the magnitude of the edge strength and another for the edge direction: P(Eillmil, ~,u)  = P(Eillmil)P(¢illmil, ~,u)  (1)  where P(Eillmil) equals Po/! [sent-88, score-0.689]
</p><p>54 Also, P(¢illmil, ~,u) equals Pang(¢il-O(~,mil'U)) if mil = 1,2,3 or U(¢il) if mil = 4,5. [sent-90, score-0.486]
</p><p>55 Here O(~, mil, u)) is the predicted normal orientation of lines determined by the equation tan Ox = -(ucx + fax)/(vc x + fb x ) for x lines, tanOy = -(ucy+ fay)/(vc y + fb y) for y lines, and tanOz = -(ucz + faz)/(vc z + fb z ) for z lines. [sent-91, score-0.767]
</p><p>56 In summary, the edge strength probability is modeled by Pon for models 1 through 4 and by po/! [sent-92, score-0.209]
</p><p>57 For models 1,2 and 3 the edge orientation is modeled by a distribution which is peaked about the appropriate orientation of an x, y, z line predicted by the camera orientation at pixel location u; for models 4 and 5 the edge orientation is assumed to be uniformly distributed from 0 through 27f. [sent-94, score-1.709]
</p><p>58 ) Thus the posterior distribution on the camera orientation is given by nil P(Eill~, U)P(~)/Z where Z is a normalization factor and P(~) is a uniform prior on the camera orientation. [sent-98, score-0.592]
</p><p>59 To find the MAP (maximum a posterior) estimate, our algorithm maximizes the log posterior term log[P({Eil}I~)P(~)] = logP(~) + L:illog[L:muP(Eillmil,~,u)P(mil)] numerically by searching over a quantized set of compass directions ~ in a certain range. [sent-99, score-0.105]
</p><p>60 5  Experimental Results  This section presents results on the domains for which the viewer orientation relative to the scene can be detected using the Manhattan world assumption. [sent-101, score-0.566]
</p><p>61 The results show strong success for inference using the Manhattan world assumption even for domains in which it might seem unlikely to apply. [sent-103, score-0.194]
</p><p>62 For example, a helicopter in a hilly scene where the algorithm mistakenly interprets the hill silhouettes as horizontal lines ). [sent-105, score-0.299]
</p><p>63 The first set of images were of city and indoor scenes in San Francisco with images taken by the second author [2]. [sent-106, score-0.508]
</p><p>64 Figure 3: Estimates of the camera orientation obtained by our algorithm for two indoor scenes (left) and two outdoor scenes (right). [sent-108, score-0.835]
</p><p>65 The estimated orientations of the x, y lines, derived for the estimated camera orientation q! [sent-109, score-0.493]
</p><p>66 (The z line orientations have been omitted for clarity. [sent-111, score-0.121]
</p><p>67 In the image on the far left, observe how the x directions align with the wall on the right hand side and with features parallel to this wall. [sent-113, score-0.154]
</p><p>68 The y lines align with the wall on the left (and objects parallel to it). [sent-114, score-0.21]
</p><p>69 We now extend this work to less structured scenes in the English countryside. [sent-115, score-0.135]
</p><p>70 Figure (4) shows two images of roads in rural scenes and two fields. [sent-116, score-0.447]
</p><p>71 The next three images were either downloaded from the web or digitized (the painting). [sent-118, score-0.132]
</p><p>72 These are the mid-west broccoli field , the Parthenon ruins, and the painting of the French countryside. [sent-119, score-0.152]
</p><p>73 6  Detecting Objects in Manhattan world  We now consider applying the Manhattan assumption to the alternative problem of detecting target objects in background clutter. [sent-120, score-0.365]
</p><p>74 To perform such a task effectively requires modelling the properties of the background clutter in addition to those of the target object. [sent-121, score-0.141]
</p><p>75 It has recently been appreciated that good statistical modelling of the image background can improve the performance of target recognition [7]. [sent-122, score-0.239]
</p><p>76 The Manhattan world assumption gives an alternative way of probabilistically modelling background clutter. [sent-123, score-0.235]
</p><p>77 The background clutter will correspond to the regular structure of buildings and roads and its edges will be aligned to the Manhattan grid. [sent-124, score-0.326]
</p><p>78 The target object, however, is assumed to be unaligned (at least, in part) to this grid. [sent-125, score-0.152]
</p><p>79 Therefore many of the edges of the target object will be assigned to model 4 by the algorithm. [sent-126, score-0.21]
</p><p>80 * of the  Figure 4: Results on rural images in England without strong Manhattan structure. [sent-128, score-0.247]
</p><p>81 Two images of roads in the countryside (left panels) and two images of fields (right panel). [sent-130, score-0.298]
</p><p>82 Figure 5: Results on an American mid-west broccoli field, the ruins of the Parthenon, and a digitized painting of the French countryside. [sent-131, score-0.254]
</p><p>83 compass orientation, see section (4), and then estimates the model by doing MAP of P(ma! [sent-132, score-0.068]
</p><p>84 ) This enables us to significantly simplify the detection task by removing all edges in the images except those assigned to model 4. [sent-134, score-0.288]
</p><p>85 The Ames room, see figure (6), is a geometrically distorted room which is constructed so as to give the false impression that it is built on a cartesian coordinate frame when viewed from a special viewpoint. [sent-135, score-0.308]
</p><p>86 Human observers assume that the room is indeed cartesian despite all other visual cues to the contrary. [sent-136, score-0.344]
</p><p>87 This distorts the apparent size of objects so that, for example, humans in different parts of the room appear to have very different sizes. [sent-137, score-0.189]
</p><p>88 In fact, a human walking across the room will appear to change size dramatically. [sent-138, score-0.173]
</p><p>89 Our algorithm, like human observers, interprets the room as being cartesian and helps identify the humans in the room as outlier edges which are unaligned to the cartesian reference system. [sent-139, score-0.973]
</p><p>90 7  Summary and Conclusions  We have demonstrated that the Manhattan world assumption applies to a range of images, rural and otherwise, in addition to urban scenes. [sent-140, score-0.389]
</p><p>91 We demonstrated a Bayesian model which used this assumption to infer the orientation of the viewer relative to this reference frame and which could also detect outlier edges which are unaligned to the reference frame. [sent-141, score-0.866]
</p><p>92 A key element of this approach is the use of image gradient statistics, learned from image datasets, which quantify the distribution of the image gradient magnitude and direction on and off object boundaries. [sent-142, score-0.77]
</p><p>93 We expect that there are many further image regularities of this type which can be used for building effective artificial vision systems and which are possibly made use of by biological vision systems. [sent-143, score-0.316]
</p><p>94 The left images (top and bottom) show the estimated scene structure. [sent-153, score-0.261]
</p><p>95 The right images show that people stand out as residual edges which are unaligned to the Manhattan grid. [sent-154, score-0.396]
</p><p>96 The Ames room (top panel) violates the Manhattan assumption but human observers, and our algorithm, interpret it as if it satisfied the assumptions. [sent-155, score-0.251]
</p><p>97 In fact, despite appearances, the two people in the Ames room are really the same size. [sent-156, score-0.173]
</p><p>98 It is a pleasure to acknowledge email conversations with Song Chun Zhu about scene clutter. [sent-159, score-0.166]
</p><p>99 We gratefully acknowledge the use ofthe Sowerby image dataset from Sowerby Research Centre, British Aerospace. [sent-160, score-0.189]
</p><p>100 "Contribution to the determination of vanishing points using Hough transform". [sent-206, score-0.063]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('manhattan', 0.412), ('orientation', 0.254), ('mil', 0.243), ('edge', 0.209), ('pon', 0.195), ('ea', 0.172), ('camera', 0.169), ('image', 0.154), ('rural', 0.152), ('room', 0.139), ('scenes', 0.135), ('scene', 0.131), ('cartesian', 0.131), ('pof', 0.13), ('lines', 0.125), ('edges', 0.122), ('world', 0.116), ('coughlan', 0.108), ('eillmil', 0.108), ('indoor', 0.108), ('unaligned', 0.108), ('images', 0.095), ('fb', 0.094), ('eil', 0.093), ('cpa', 0.087), ('konishi', 0.087), ('painting', 0.087), ('pang', 0.087), ('sowerby', 0.087), ('outlier', 0.085), ('vc', 0.083), ('gradient', 0.083), ('ames', 0.078), ('assumption', 0.078), ('city', 0.075), ('observers', 0.074), ('detection', 0.071), ('orientations', 0.07), ('pixels', 0.07), ('compass', 0.068), ('regularities', 0.068), ('broccoli', 0.065), ('eill', 0.065), ('parthenon', 0.065), ('roads', 0.065), ('ruins', 0.065), ('viewer', 0.065), ('normal', 0.063), ('vanishing', 0.063), ('direction', 0.059), ('clutter', 0.056), ('yuille', 0.056), ('pixel', 0.055), ('geometry', 0.051), ('line', 0.051), ('french', 0.051), ('objects', 0.05), ('vision', 0.047), ('bayesian', 0.045), ('object', 0.044), ('statistics', 0.044), ('target', 0.044), ('countryside', 0.043), ('elevation', 0.043), ('fax', 0.043), ('fay', 0.043), ('faz', 0.043), ('fillmore', 0.043), ('hough', 0.043), ('illmil', 0.043), ('interprets', 0.043), ('modulo', 0.043), ('tan', 0.043), ('tanoy', 0.043), ('tanoz', 0.043), ('ucx', 0.043), ('ucy', 0.043), ('ucz', 0.043), ('urban', 0.043), ('zhu', 0.043), ('aligned', 0.042), ('reference', 0.041), ('background', 0.041), ('magnitude', 0.039), ('frame', 0.038), ('quantized', 0.037), ('stand', 0.037), ('digitized', 0.037), ('huang', 0.037), ('english', 0.036), ('detecting', 0.036), ('panel', 0.035), ('ox', 0.035), ('acknowledge', 0.035), ('left', 0.035), ('human', 0.034), ('infer', 0.034), ('francisco', 0.034), ('people', 0.034), ('outdoor', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="135-tfidf-1" href="./nips-2000-The_Manhattan_World_Assumption%3A_Regularities_in_Scene_Statistics_which_Enable_Bayesian_Inference.html">135 nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</a></p>
<p>Author: James M. Coughlan, Alan L. Yuille</p><p>Abstract: Preliminary work by the authors made use of the so-called</p><p>2 0.095556296 <a title="135-tfidf-2" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>Author: Michael S. Gray, Terrence J. Sejnowski, Javier R. Movellan</p><p>Abstract: We examine eight different techniques for developing visual representations in machine vision tasks. In particular we compare different versions of principal component and independent component analysis in combination with stepwise regression methods for variable selection. We found that local methods, based on the statistics of image patches, consistently outperformed global methods based on the statistics of entire images. This result is consistent with previous work on emotion and facial expression recognition. In addition, the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance. 1</p><p>3 0.093483612 <a title="135-tfidf-3" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>Author: Rafal Bogacz, Malcolm W. Brown, Christophe G. Giraud-Carrier</p><p>Abstract: Olshausen & Field demonstrated that a learning algorithm that attempts to generate a sparse code for natural scenes develops a complete family of localised, oriented, bandpass receptive fields, similar to those of 'simple cells' in VI. This paper describes an algorithm which finds a sparse code for sequences of images that preserves information about the input. This algorithm when trained on natural video sequences develops bases representing the movement in particular directions with particular speeds, similar to the receptive fields of the movement-sensitive cells observed in cortical visual areas. Furthermore, in contrast to previous approaches to learning direction selectivity, the timing of neuronal activity encodes the phase of the movement, so the precise timing of spikes is crucially important to the information encoding.</p><p>4 0.08708398 <a title="135-tfidf-4" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>Author: Yee Whye Teh, Geoffrey E. Hinton</p><p>Abstract: We describe a neurally-inspired, unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual. Individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known. Our method compares favorably with other methods in the literature. The generative model consists of a single layer of rate-coded, non-linear feature detectors and it has the property that, given a data vector, the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation. The weights of the feature detectors are learned by comparing the correlations of pixel intensities and feature activations in two phases: When the network is observing real data and when it is observing reconstructions of real data generated from the feature activations.</p><p>5 0.080318525 <a title="135-tfidf-5" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>Author: Gal Elidan, Noam Lotner, Nir Friedman, Daphne Koller</p><p>Abstract: A serious problem in learning probabilistic models is the presence of hidden variables. These variables are not observed, yet interact with several of the observed variables. As such, they induce seemingly complex dependencies among the latter. In recent years, much attention has been devoted to the development of algorithms for learning parameters, and in some cases structure, in the presence of hidden variables. In this paper, we address the related problem of detecting hidden variables that interact with the observed variables. This problem is of interest both for improving our understanding of the domain and as a preliminary step that guides the learning procedure towards promising models. A very natural approach is to search for</p><p>6 0.078695774 <a title="135-tfidf-6" href="./nips-2000-Feature_Correspondence%3A_A_Markov_Chain_Monte_Carlo_Approach.html">53 nips-2000-Feature Correspondence: A Markov Chain Monte Carlo Approach</a></p>
<p>7 0.076337717 <a title="135-tfidf-7" href="./nips-2000-Accumulator_Networks%3A_Suitors_of_Local_Probability_Propagation.html">15 nips-2000-Accumulator Networks: Suitors of Local Probability Propagation</a></p>
<p>8 0.071396485 <a title="135-tfidf-8" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>9 0.06876111 <a title="135-tfidf-9" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>10 0.063450933 <a title="135-tfidf-10" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>11 0.057767436 <a title="135-tfidf-11" href="./nips-2000-FaceSync%3A_A_Linear_Operator_for_Measuring_Synchronization_of_Video_Facial_Images_and_Audio_Tracks.html">50 nips-2000-FaceSync: A Linear Operator for Measuring Synchronization of Video Facial Images and Audio Tracks</a></p>
<p>12 0.057020202 <a title="135-tfidf-12" href="./nips-2000-Shape_Context%3A_A_New_Descriptor_for_Shape_Matching_and_Object_Recognition.html">117 nips-2000-Shape Context: A New Descriptor for Shape Matching and Object Recognition</a></p>
<p>13 0.054521695 <a title="135-tfidf-13" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>14 0.054041721 <a title="135-tfidf-14" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>15 0.053996939 <a title="135-tfidf-15" href="./nips-2000-Probabilistic_Semantic_Video_Indexing.html">103 nips-2000-Probabilistic Semantic Video Indexing</a></p>
<p>16 0.053413853 <a title="135-tfidf-16" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>17 0.052855644 <a title="135-tfidf-17" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>18 0.051821221 <a title="135-tfidf-18" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>19 0.049520306 <a title="135-tfidf-19" href="./nips-2000-Keeping_Flexible_Active_Contours_on_Track_using_Metropolis_Updates.html">72 nips-2000-Keeping Flexible Active Contours on Track using Metropolis Updates</a></p>
<p>20 0.049352385 <a title="135-tfidf-20" href="./nips-2000-One_Microphone_Source_Separation.html">96 nips-2000-One Microphone Source Separation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.163), (1, -0.074), (2, 0.075), (3, 0.092), (4, -0.029), (5, 0.008), (6, 0.181), (7, -0.038), (8, 0.043), (9, 0.022), (10, -0.01), (11, 0.02), (12, 0.016), (13, -0.026), (14, -0.052), (15, 0.041), (16, -0.072), (17, 0.081), (18, -0.121), (19, -0.043), (20, -0.064), (21, -0.08), (22, -0.034), (23, -0.005), (24, -0.005), (25, 0.101), (26, 0.039), (27, -0.018), (28, -0.028), (29, 0.148), (30, 0.139), (31, -0.053), (32, 0.106), (33, -0.059), (34, -0.068), (35, 0.052), (36, -0.157), (37, 0.096), (38, -0.025), (39, 0.178), (40, 0.224), (41, -0.062), (42, -0.022), (43, 0.057), (44, -0.108), (45, 0.004), (46, -0.015), (47, 0.045), (48, -0.015), (49, 0.238)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96654606 <a title="135-lsi-1" href="./nips-2000-The_Manhattan_World_Assumption%3A_Regularities_in_Scene_Statistics_which_Enable_Bayesian_Inference.html">135 nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</a></p>
<p>Author: James M. Coughlan, Alan L. Yuille</p><p>Abstract: Preliminary work by the authors made use of the so-called</p><p>2 0.49779397 <a title="135-lsi-2" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>Author: Marina Meila, Jianbo Shi</p><p>Abstract: We present a new view of image segmentation by pairwise similarities. We interpret the similarities as edge flows in a Markov random walk and study the eigenvalues and eigenvectors of the walk's transition matrix. This interpretation shows that spectral methods for clustering and segmentation have a probabilistic foundation. In particular, we prove that the Normalized Cut method arises naturally from our framework. Finally, the framework provides a principled method for learning the similarity function as a combination of features. 1</p><p>3 0.49371526 <a title="135-lsi-3" href="./nips-2000-Feature_Correspondence%3A_A_Markov_Chain_Monte_Carlo_Approach.html">53 nips-2000-Feature Correspondence: A Markov Chain Monte Carlo Approach</a></p>
<p>Author: Frank Dellaert, Steven M. Seitz, Sebastian Thrun, Charles E. Thorpe</p><p>Abstract: When trying to recover 3D structure from a set of images, the most difficult problem is establishing the correspondence between the measurements. Most existing approaches assume that features can be tracked across frames, whereas methods that exploit rigidity constraints to facilitate matching do so only under restricted camera motion. In this paper we propose a Bayesian approach that avoids the brittleness associated with singling out one</p><p>4 0.44290692 <a title="135-lsi-4" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>Author: Rafal Bogacz, Malcolm W. Brown, Christophe G. Giraud-Carrier</p><p>Abstract: Olshausen & Field demonstrated that a learning algorithm that attempts to generate a sparse code for natural scenes develops a complete family of localised, oriented, bandpass receptive fields, similar to those of 'simple cells' in VI. This paper describes an algorithm which finds a sparse code for sequences of images that preserves information about the input. This algorithm when trained on natural video sequences develops bases representing the movement in particular directions with particular speeds, similar to the receptive fields of the movement-sensitive cells observed in cortical visual areas. Furthermore, in contrast to previous approaches to learning direction selectivity, the timing of neuronal activity encodes the phase of the movement, so the precise timing of spikes is crucially important to the information encoding.</p><p>5 0.43395302 <a title="135-lsi-5" href="./nips-2000-Accumulator_Networks%3A_Suitors_of_Local_Probability_Propagation.html">15 nips-2000-Accumulator Networks: Suitors of Local Probability Propagation</a></p>
<p>Author: Brendan J. Frey, Anitha Kannan</p><p>Abstract: One way to approximate inference in richly-connected graphical models is to apply the sum-product algorithm (a.k.a. probability propagation algorithm), while ignoring the fact that the graph has cycles. The sum-product algorithm can be directly applied in Gaussian networks and in graphs for coding, but for many conditional probability functions - including the sigmoid function - direct application of the sum-product algorithm is not possible. We introduce</p><p>6 0.43296269 <a title="135-lsi-6" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>7 0.39024845 <a title="135-lsi-7" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>8 0.36661768 <a title="135-lsi-8" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>9 0.33977276 <a title="135-lsi-9" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>10 0.31717309 <a title="135-lsi-10" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>11 0.29683048 <a title="135-lsi-11" href="./nips-2000-Bayesian_Video_Shot_Segmentation.html">30 nips-2000-Bayesian Video Shot Segmentation</a></p>
<p>12 0.27731147 <a title="135-lsi-12" href="./nips-2000-Probabilistic_Semantic_Video_Indexing.html">103 nips-2000-Probabilistic Semantic Video Indexing</a></p>
<p>13 0.27272254 <a title="135-lsi-13" href="./nips-2000-Shape_Context%3A_A_New_Descriptor_for_Shape_Matching_and_Object_Recognition.html">117 nips-2000-Shape Context: A New Descriptor for Shape Matching and Object Recognition</a></p>
<p>14 0.25764766 <a title="135-lsi-14" href="./nips-2000-The_Missing_Link_-_A_Probabilistic_Model_of_Document_Content_and_Hypertext_Connectivity.html">136 nips-2000-The Missing Link - A Probabilistic Model of Document Content and Hypertext Connectivity</a></p>
<p>15 0.25109702 <a title="135-lsi-15" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>16 0.24215522 <a title="135-lsi-16" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>17 0.24029183 <a title="135-lsi-17" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>18 0.23841172 <a title="135-lsi-18" href="./nips-2000-Smart_Vision_Chip_Fabricated_Using_Three_Dimensional_Integration_Technology.html">118 nips-2000-Smart Vision Chip Fabricated Using Three Dimensional Integration Technology</a></p>
<p>19 0.23127589 <a title="135-lsi-19" href="./nips-2000-Competition_and_Arbors_in_Ocular_Dominance.html">34 nips-2000-Competition and Arbors in Ocular Dominance</a></p>
<p>20 0.22825575 <a title="135-lsi-20" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.02), (17, 0.659), (32, 0.015), (33, 0.028), (55, 0.025), (62, 0.018), (65, 0.016), (67, 0.035), (75, 0.018), (76, 0.029), (81, 0.023), (90, 0.018), (97, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99734855 <a title="135-lda-1" href="./nips-2000-Sparse_Kernel_Principal_Component_Analysis.html">121 nips-2000-Sparse Kernel Principal Component Analysis</a></p>
<p>Author: Michael E. Tipping</p><p>Abstract: 'Kernel' principal component analysis (PCA) is an elegant nonlinear generalisation of the popular linear data analysis method, where a kernel function implicitly defines a nonlinear transformation into a feature space wherein standard PCA is performed. Unfortunately, the technique is not 'sparse', since the components thus obtained are expressed in terms of kernels associated with every training vector. This paper shows that by approximating the covariance matrix in feature space by a reduced number of example vectors, using a maximum-likelihood approach, we may obtain a highly sparse form of kernel PCA without loss of effectiveness. 1</p><p>same-paper 2 0.99706292 <a title="135-lda-2" href="./nips-2000-The_Manhattan_World_Assumption%3A_Regularities_in_Scene_Statistics_which_Enable_Bayesian_Inference.html">135 nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</a></p>
<p>Author: James M. Coughlan, Alan L. Yuille</p><p>Abstract: Preliminary work by the authors made use of the so-called</p><p>3 0.99412078 <a title="135-lda-3" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>Author: Jason Weston, Sayan Mukherjee, Olivier Chapelle, Massimiliano Pontil, Tomaso Poggio, Vladimir Vapnik</p><p>Abstract: We introduce a method of feature selection for Support Vector Machines. The method is based upon finding those features which minimize bounds on the leave-one-out error. This search can be efficiently performed via gradient descent. The resulting algorithms are shown to be superior to some standard feature selection algorithms on both toy data and real-life problems of face recognition, pedestrian detection and analyzing DNA micro array data.</p><p>4 0.99261296 <a title="135-lda-4" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>Author: Te-Won Lee, Thomas Wachtler, Terrence J. Sejnowski</p><p>Abstract: The human visual system encodes the chromatic signals conveyed by the three types of retinal cone photoreceptors in an opponent fashion. This color opponency has been shown to constitute an efficient encoding by spectral decorrelation of the receptor signals. We analyze the spatial and chromatic structure of natural scenes by decomposing the spectral images into a set of linear basis functions such that they constitute a representation with minimal redundancy. Independent component analysis finds the basis functions that transforms the spatiochromatic data such that the outputs (activations) are statistically as independent as possible, i.e. least redundant. The resulting basis functions show strong opponency along an achromatic direction (luminance edges), along a blueyellow direction, and along a red-blue direction. Furthermore, the resulting activations have very sparse distributions, suggesting that the use of color opponency in the human visual system achieves a highly efficient representation of colors. Our findings suggest that color opponency is a result of the properties of natural spectra and not solely a consequence of the overlapping cone spectral sensitivities. 1 Statistical structure of natural scenes Efficient encoding of visual sensory information is an important task for information processing systems and its study may provide insights into coding principles of biological visual systems. An important goal of sensory information processing Electronic version available at www. cnl. salk . edu/</p><p>5 0.99260145 <a title="135-lda-5" href="./nips-2000-Foundations_for_a_Circuit_Complexity_Theory_of_Sensory_Processing.html">56 nips-2000-Foundations for a Circuit Complexity Theory of Sensory Processing</a></p>
<p>Author: Robert A. Legenstein, Wolfgang Maass</p><p>Abstract: We introduce total wire length as salient complexity measure for an analysis of the circuit complexity of sensory processing in biological neural systems and neuromorphic engineering. This new complexity measure is applied to a set of basic computational problems that apparently need to be solved by circuits for translation- and scale-invariant sensory processing. We exhibit new circuit design strategies for these new benchmark functions that can be implemented within realistic complexity bounds, in particular with linear or almost linear total wire length.</p><p>6 0.92420965 <a title="135-lda-6" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>7 0.85306901 <a title="135-lda-7" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>8 0.85224742 <a title="135-lda-8" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>9 0.82796443 <a title="135-lda-9" href="./nips-2000-A_Mathematical_Programming_Approach_to_the_Kernel_Fisher_Algorithm.html">5 nips-2000-A Mathematical Programming Approach to the Kernel Fisher Algorithm</a></p>
<p>10 0.8195743 <a title="135-lda-10" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>11 0.81876993 <a title="135-lda-11" href="./nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">95 nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>12 0.81772912 <a title="135-lda-12" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>13 0.80857468 <a title="135-lda-13" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>14 0.8080132 <a title="135-lda-14" href="./nips-2000-Factored_Semi-Tied_Covariance_Matrices.html">51 nips-2000-Factored Semi-Tied Covariance Matrices</a></p>
<p>15 0.80719483 <a title="135-lda-15" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>16 0.79659373 <a title="135-lda-16" href="./nips-2000-Minimum_Bayes_Error_Feature_Selection_for_Continuous_Speech_Recognition.html">84 nips-2000-Minimum Bayes Error Feature Selection for Continuous Speech Recognition</a></p>
<p>17 0.79656357 <a title="135-lda-17" href="./nips-2000-Smart_Vision_Chip_Fabricated_Using_Three_Dimensional_Integration_Technology.html">118 nips-2000-Smart Vision Chip Fabricated Using Three Dimensional Integration Technology</a></p>
<p>18 0.79440773 <a title="135-lda-18" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>19 0.79438168 <a title="135-lda-19" href="./nips-2000-Generalizable_Singular_Value_Decomposition_for_Ill-posed_Datasets.html">61 nips-2000-Generalizable Singular Value Decomposition for Ill-posed Datasets</a></p>
<p>20 0.79325265 <a title="135-lda-20" href="./nips-2000-Constrained_Independent_Component_Analysis.html">36 nips-2000-Constrained Independent Component Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
