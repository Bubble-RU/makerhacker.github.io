<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-142" href="#">nips2000-142</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</h1>
<br/><p>Source: <a title="nips-2000-142-pdf" href="http://papers.nips.cc/paper/1888-using-free-energies-to-represent-q-values-in-a-multiagent-reinforcement-learning-task.pdf">pdf</a></p><p>Author: Brian Sallans, Geoffrey E. Hinton</p><p>Abstract: The problem of reinforcement learning in large factored Markov decision processes is explored. The Q-value of a state-action pair is approximated by the free energy of a product of experts network. Network parameters are learned on-line using a modified SARSA algorithm which minimizes the inconsistency of the Q-values of consecutive state-action pairs. Actions are chosen based on the current value estimates by fixing the current state and sampling actions from the network using Gibbs sampling. The algorithm is tested on a co-operative multi-agent task. The product of experts model is found to perform comparably to table-based Q-Iearning for small instances of the task, and continues to perform well when the problem becomes too large for a table-based representation.</p><p>Reference: <a title="nips-2000-142-reference" href="../nips2000_reference/nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ac, uk  Abstract The problem of reinforcement learning in large factored Markov decision processes is explored. [sent-5, score-0.318]
</p><p>2 The Q-value of a state-action pair is approximated by the free energy of a product of experts network. [sent-6, score-0.498]
</p><p>3 Actions are chosen based on the current value estimates by fixing the current state and sampling actions from the network using Gibbs sampling. [sent-8, score-0.532]
</p><p>4 The product of experts model is found to perform comparably to table-based Q-Iearning for small instances of the task, and continues to perform well when the problem becomes too large for a table-based representation. [sent-10, score-0.393]
</p><p>5 1 Introduction Online Reinforcement Learning (RL) algorithms try to find a policy which maximizes the expected time-discounted reward provided by the environment. [sent-11, score-0.262]
</p><p>6 They do this by performing sample backups to learn a value function over states or state-action pairs [1]. [sent-12, score-0.083]
</p><p>7 If the decision problem is Markov in the observed states, then the optimal value function over state-action pairs (the Q-function) yields all of the information required to find the optimal policy for the decision problem. [sent-13, score-0.328]
</p><p>8 For example, when the Q-function is represented as a table, the optimal action for a given state can be found simply by searching the row of the table corresponding to that state. [sent-14, score-0.541]
</p><p>9 1  Factored Markov Decision Processes  In many cases the dimensionality of the problem makes a table representation impractical, so a more compact representation that makes use of the structure inherent in the problem is required. [sent-16, score-0.07]
</p><p>10 In a co-operative multi-agent system, for example, it is natural to represent both the state and action as sets of variables (one for each agent). [sent-17, score-0.482]
</p><p>11 2 Actor-Critic Architectures If a non-linear function approximator is used to model the Q-function, then it is difficult  and time consuming to extract the policy directly from the Q-function because a non-linear optimization must be solved for each action choice. [sent-21, score-0.545]
</p><p>12 One solution, called an actor-critic architecture, is to use a separate function approximator to model the policy (i. [sent-22, score-0.226]
</p><p>13 Instead we present a method where the Q-value of a state-action pair is represented (up to an additive constant) by the negative free-energy, - F, of the state-action pair under a non-causal graphical model. [sent-27, score-0.267]
</p><p>14 The graphical model is a product of experts [5] which has two very useful properties: Given a state-action pair, the exact free energy is easily computed, and the derivative of this free energy W. [sent-28, score-0.627]
</p><p>15 each parameter of the network is also very simple. [sent-31, score-0.058]
</p><p>16 The model is trained to minimize the inconsistency between the free-energy of a state-action pair and the discounted free energy of the next state-action pair, taking into account the immediate reinforcement. [sent-32, score-0.294]
</p><p>17 After training, a good action for a given state can be found by clamping the state and drawing a sample of the action variables using Gibbs sampling [6]. [sent-33, score-0.96]
</p><p>18 Although finding optimal actions would still be difficult for large problems, selecting an action with a probability that is approximately proportional to exp( - F) can be done with a modest number of iterations of Gibbs sampling. [sent-34, score-0.693]
</p><p>19 3 Markov Decision Processes We will concentrate on finite, factored, Markov decision processes (factored MDPs), in which each state and action is represented as a set of discrete variables. [sent-36, score-0.554]
</p><p>20 A state is an M-tuple and an action is an N-tuple. [sent-38, score-0.44]
</p><p>21 X AN which maximize the total expected reward received over the course of the task:  (Rt) 1r'  =  (rt  + '/'rt+1 + . [sent-45, score-0.07]
</p><p>22 + ,/,T-trT)  1r'  (1)  where,/, is a discount factor and (-) 1r' denotes the expectation taken with respect to policy  7ft. [sent-48, score-0.192]
</p><p>23 We will focus on the case when the policy is stationary: 7ft is identical for all t. [sent-49, score-0.192]
</p><p>24 2 Approximating Q-values with a Product of Experts As the number of state and action variables increases, a table representation quickly becomes intractable. [sent-50, score-0.552]
</p><p>25 We represent the value of a state and action as the negative free-energy (up to a constant) under a product of experts model (see Figure leaÂť~. [sent-51, score-0.768]
</p><p>26 The estimated Q-value (up to an additive constant) of a setting of the state and action units is found by holding these units fixed and computing the free energy of the network. [sent-53, score-1.025]
</p><p>27 Actions are selected by alternating between updating all of the hidden units in parallel and updating all of the action units in parallel, with the state units held constant. [sent-54, score-1.198]
</p><p>28 b) A multinomial state or action variable is represented by a set of "one-of-n" binary units in which exactly one is on. [sent-55, score-0.758]
</p><p>29 , ()K} are parameters of the K experts and (s', a') indexes all possible stateaction pairs. [sent-63, score-0.274]
</p><p>30 In the following, we will assume that there are an equal number of state and action variables (i. [sent-64, score-0.482]
</p><p>31 M = N); and that each state or action variable has the same arity (Va. [sent-66, score-0.476]
</p><p>32 These assumptions are appropriate, for example, when there is one state and action variable for each agent in a multi-agent task. [sent-68, score-0.669]
</p><p>33 We will focus on the case where each expert is a single binary sigmoid unit because it is particularly suited to the discrete tasks we consider here. [sent-72, score-0.155]
</p><p>34 Each agent's (multinomial) state or action is represented using a "one-of-N" set of binary units which are constrained so that exactly one of them is on. [sent-73, score-0.69]
</p><p>35 The product of experts is then a bipartite "Restricted Boltzmann Machine" [5]. [sent-74, score-0.296]
</p><p>36 We use S,Bi to denote agent (3's ith state and a,Bj to denote its jth action. [sent-75, score-0.314]
</p><p>37 We will denote the binary latent variables of the "experts" by hk (see Figure l(b)). [sent-76, score-0.153]
</p><p>38 For a state s = {S,Bi} and an action a = {a,Bj} ' the free energy is given by the expected energy given the posterior distribution of the hidden units minus the entropy of this posterior distribution. [sent-77, score-0.915]
</p><p>39 CF is an additive constant equal to the log of the partition function. [sent-79, score-0.057]
</p><p>40 The first two terms of (3) corresponds to an unnormalized negative log-likelihood, and the third to the negative entropy of the distribution over the hidden units given the data. [sent-80, score-0.318]
</p><p>41 The free energy can be computed tractably because inference is tractable in a product of experts: under the product model each expert is independent of the others given the data. [sent-81, score-0.411]
</p><p>42 We can efficiently compute the exact free energy of a state and action under the product model, up to an additive constant. [sent-82, score-0.74]
</p><p>43 1  + CF  (5)  Learning the Parameters  The parameters of the model must be adjusted so that the goodness of a state-action under the product model approximates its actual Q-value. [sent-84, score-0.128]
</p><p>44 If we consider a delta-rule update where the target for input (st, at) is rt + 'YQ(st+! [sent-86, score-0.087]
</p><p>45 2  Sampling Actions  Given a trained network and the current state st, we need to generate actions according to their goodness. [sent-94, score-0.537]
</p><p>46 We would like to select actions according to a Boltzmann exploration scheme in which the probability of selecting an action is proportional to eQ IT. [sent-95, score-0.703]
</p><p>47 This selection scheme has the desirable property that it optimizes the trade-off between the expected payoff, Q, and the entropy of the selection distribution, where T is the relative importance of exploration versus exploitation. [sent-96, score-0.1]
</p><p>48 Fortunately, the additive constant, CF, does not need to be known in order to select actions in this way. [sent-97, score-0.372]
</p><p>49 We start with an arbitrary initial action represented on the action units. [sent-99, score-0.669]
</p><p>50 Holding the state units fixed we update all of the hidden units in parallel so that we get a sample from the posterior distribution over the hidden units given the state and the action. [sent-100, score-0.993]
</p><p>51 Then we update all of the action units in parallel so that we get a sample from the posterior distribution over actions given the states of the hidden units. [sent-101, score-1.009]
</p><p>52 When updating the states of the action units, we use a "softmax" to enforce the one-of-N constraint within a set of binary units that represent mutually exclusive actions of the same agent. [sent-102, score-0.939]
</p><p>53 When the alternating Gibbs sampling reaches equilibrium it draws unbiased samples of actions according to their Q-value. [sent-103, score-0.487]
</p><p>54 3 Experimental Results To test the algorithm we introduce a co-operative multi-agent task in which there are offensive players trying to reach an end-zone, and defensive players trying to block them (see Figure 2). [sent-105, score-0.12]
</p><p>55 end-zone blockers  ~ agents  \  '0  ~  C)  [C)  Figure 2: An example of the "blocker" task. [sent-106, score-0.579]
</p><p>56 Agents must get past the blockers to the end-zone. [sent-107, score-0.247]
</p><p>57 The blockers are preprogrammed with a strategy to stop them, but if they co-operate the blockers cannot stop them all simultaneously. [sent-108, score-0.494]
</p><p>58 The task is co-operative: As long as one agent reaches the end-zone, the "team" is rewarded. [sent-109, score-0.274]
</p><p>59 The team receives a reward of + 1 when an agent reaches the end-zone, and a reward of -1 otherwise. [sent-110, score-0.421]
</p><p>60 The blockers are pre-programmed with a fixed blocking strategy. [sent-111, score-0.247]
</p><p>61 Each agent occupies one square on the grid, and each blocker occupies three horizontally adjacent squares. [sent-112, score-0.576]
</p><p>62 An agent cannot move into a square occupied by a blocker or another agent. [sent-113, score-0.547]
</p><p>63 The task has non-wrap-around edge conditions on the east, west and south sides of the field, and the blockers and agents can move north, south, east or west. [sent-114, score-0.784]
</p><p>64 A product of experts (PoE) network with 4 hidden units was trained on a 5 x 4 blocker task with two agents and one blocker. [sent-115, score-1.322]
</p><p>65 The combined state consisted of three position variables (two agents and one blocker) which could take on integer values {I, . [sent-116, score-0.658]
</p><p>66 The combined action consisted of two action variables taking on values from {I, . [sent-120, score-0.843]
</p><p>67 The network was run twice, once for 60 000 combined actions and once for 400 000 combined actions, with a learning rate going from 0. [sent-124, score-0.645]
</p><p>68 Each trial was terminated after either the end-zone was reached, or 20 combined actions were taken, whichever occurred first. [sent-129, score-0.591]
</p><p>69 Each trial was initialized with the blocker placed randomly in the top row and the agents placed randomly in the bottom row. [sent-130, score-0.664]
</p><p>70 The same learning rate and temperature schedule were used to train a Q-Iearner with a table containing 128,000 elements (20 3 x 4 2 ), except that the Q-Iearner was allowed to train for 1 million combined actions. [sent-131, score-0.289]
</p><p>71 After training each policy was run for 10,000 steps, and all rewards were totaled. [sent-132, score-0.262]
</p><p>72 The two algorithms were also compared to a hand-coded policy, where the agents first move to opposite sides of the field and then move to the end-zone. [sent-133, score-0.482]
</p><p>73 In this case, all of the algorithms performed comparably, and the POE network performing well even for a short training time. [sent-134, score-0.058]
</p><p>74 A PoE network with 16 hidden units was trained on a 4 x 7 blockers task with three agents and two blockers. [sent-135, score-0.976]
</p><p>75 Again, the input consisted of position variables for each blocker and agent, and and action variables for each agent. [sent-136, score-0.742]
</p><p>76 The network was trained for 400 000 combined actions, with the a learning rate from 0. [sent-137, score-0.222]
</p><p>77 001 and the same temperature schedule as the previous task. [sent-139, score-0.098]
</p><p>78 Each trial was terminated after either the end-zone was reached, or 40 steps were taken, whichever occurred first. [sent-140, score-0.203]
</p><p>79 After training, the resultant policy was run for 10,000 steps and the rewards received were totaled. [sent-141, score-0.359]
</p><p>80 As the table representation would have over a billion elements (28 5 x 43 ), a table based Q-Iearner could not be trained for comparison. [sent-142, score-0.183]
</p><p>81 The hand-coded policy moved agents 1, 2 and 3 to the left, middle and right column respectively, and then moved all agents towards the end-zone. [sent-143, score-0.934]
</p><p>82 In practice the hidden units tend to represent particular strategies that are relevant in particular parts of the combined state space. [sent-147, score-0.496]
</p><p>83 This suggests that the hidden units could be used for hierarchical or temporal learning. [sent-148, score-0.254]
</p><p>84 A reinforcement learner could, for example, learn the dynamics between hidden unit values (useful for POMDPs) and the rewards associated with hidden unit activations. [sent-149, score-0.384]
</p><p>85 Because the PoE network implicitly represents a joint probability distribution over stateaction pairs, it can be queried in ways that are not normally possible for an actor network. [sent-150, score-0.19]
</p><p>86 Given any subset of state and action variables, the remainder can be sampled from the network using Gibbs sampling. [sent-151, score-0.498]
</p><p>87 This makes it easy to answer questions of the form: "How should agent 3 behave given fixed actions for agents 1 and 2? [sent-152, score-0.84]
</p><p>88 " or "I can see some of the state variables but not others. [sent-153, score-0.163]
</p><p>89 Further, because there is an efficient unsupervised learning algorithm for PoE networks, an agent could improve its policy by watching another agent's actions and making them more probable under its own model. [sent-156, score-0.7]
</p><p>90 There are a number of related works, both in the fields of reinforcement learning and unsupervised learning. [sent-157, score-0.108]
</p><p>91 Normally with an actorcritic method, the actor network can be viewed as a biased scheme for selecting actions according to the value assigned by the critic. [sent-162, score-0.525]
</p><p>92 The selection is biased by the choice of parameterization. [sent-163, score-0.062]
</p><p>93 Our method of action selection is unbiased (if the Markov chain is allowed to converge). [sent-164, score-0.394]
</p><p>94 Further, the resultant policy can potentially be much more complicated than a typical parameterized actor network would allow. [sent-165, score-0.391]
</p><p>95 Our algorithm is also related to probability matching [13], in which good actions are made more probable under the model, and the temperature at which the probability is computed is slowly reduced over time in order to move from exploration to exploitation and avoid local minima. [sent-167, score-0.467]
</p><p>96 Unlike our algorithm, the probability matching algorithm used a parameterized distribution which was maximized using gradient descent, and it did not address temporal credit assignment. [sent-168, score-0.073]
</p><p>97 5  Conclusions  We have shown that a product of experts network can be used to learn the values of stateaction pairs (including temporal credit assignment) when both the states and actions have a factored representation. [sent-169, score-0.992]
</p><p>98 An unbiased sample of actions can then be recovered with Gibbs sampling and 50 iterations appear to be sufficient. [sent-170, score-0.428]
</p><p>99 The network performs as well as a tablebased Q-Iearner for small tasks, and continues to perform well when the task becomes too large for a table-based representation. [sent-171, score-0.143]
</p><p>100 Generalization in reinforcement learning: Successful examples using sparse coarse coding. [sent-222, score-0.108]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('agents', 0.332), ('action', 0.319), ('actions', 0.315), ('blocker', 0.297), ('blockers', 0.247), ('poe', 0.222), ('experts', 0.2), ('agent', 0.193), ('policy', 0.192), ('units', 0.171), ('factored', 0.127), ('combined', 0.121), ('state', 0.121), ('reinforcement', 0.108), ('gibbs', 0.105), ('product', 0.096), ('hidden', 0.083), ('stateaction', 0.074), ('energy', 0.074), ('free', 0.073), ('expert', 0.072), ('reward', 0.07), ('table', 0.07), ('sarsa', 0.064), ('hk', 0.063), ('actor', 0.058), ('network', 0.058), ('additive', 0.057), ('move', 0.057), ('touretzky', 0.055), ('temperature', 0.055), ('pair', 0.055), ('st', 0.055), ('comparably', 0.054), ('michael', 0.053), ('rt', 0.053), ('decision', 0.051), ('alternating', 0.05), ('inconsistency', 0.049), ('resultant', 0.049), ('team', 0.049), ('states', 0.049), ('markov', 0.048), ('steps', 0.048), ('binary', 0.048), ('cf', 0.048), ('gatsby', 0.047), ('unbiased', 0.045), ('trained', 0.043), ('whichever', 0.043), ('terminated', 0.043), ('schedule', 0.043), ('continues', 0.043), ('occupies', 0.043), ('pomdps', 0.043), ('task', 0.042), ('variables', 0.042), ('consisted', 0.042), ('exploration', 0.04), ('rewards', 0.04), ('reaches', 0.039), ('holding', 0.039), ('credit', 0.039), ('iai', 0.039), ('players', 0.039), ('moved', 0.039), ('sampling', 0.038), ('parallel', 0.038), ('updating', 0.037), ('graphical', 0.037), ('variable', 0.036), ('sides', 0.036), ('south', 0.036), ('trial', 0.035), ('unit', 0.035), ('pairs', 0.034), ('parameterized', 0.034), ('update', 0.034), ('canada', 0.034), ('occurred', 0.034), ('east', 0.034), ('pk', 0.034), ('approximator', 0.034), ('assigned', 0.033), ('negative', 0.032), ('processes', 0.032), ('boltzmann', 0.032), ('biased', 0.032), ('biases', 0.032), ('goodness', 0.032), ('multinomial', 0.032), ('represented', 0.031), ('mdp', 0.03), ('mdps', 0.03), ('lsi', 0.03), ('kth', 0.03), ('iterations', 0.03), ('run', 0.03), ('selection', 0.03), ('selecting', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="142-tfidf-1" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>Author: Brian Sallans, Geoffrey E. Hinton</p><p>Abstract: The problem of reinforcement learning in large factored Markov decision processes is explored. The Q-value of a state-action pair is approximated by the free energy of a product of experts network. Network parameters are learned on-line using a modified SARSA algorithm which minimizes the inconsistency of the Q-values of consecutive state-action pairs. Actions are chosen based on the current value estimates by fixing the current state and sampling actions from the network using Gibbs sampling. The algorithm is tested on a co-operative multi-agent task. The product of experts model is found to perform comparably to table-based Q-Iearning for small instances of the task, and continues to perform well when the problem becomes too large for a table-based representation.</p><p>2 0.26115081 <a title="142-tfidf-2" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>Author: Christian R. Shelton</p><p>Abstract: For many problems which would be natural for reinforcement learning, the reward signal is not a single scalar value but has multiple scalar components. Examples of such problems include agents with multiple goals and agents with multiple users. Creating a single reward value by combining the multiple components can throwaway vital information and can lead to incorrect solutions. We describe the multiple reward source problem and discuss the problems with applying traditional reinforcement learning. We then present an new algorithm for finding a solution and results on simulated environments.</p><p>3 0.21771783 <a title="142-tfidf-3" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>Author: Geoffrey J. Gordon</p><p>Abstract: Many algorithms for approximate reinforcement learning are not known to converge. In fact, there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point. This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region. The algorithms are SARSA(O) and V(O); the latter algorithm was used in the well-known TD-Gammon program. 1</p><p>4 0.20261574 <a title="142-tfidf-4" href="./nips-2000-Recognizing_Hand-written_Digits_Using_Hierarchical_Products_of_Experts.html">108 nips-2000-Recognizing Hand-written Digits Using Hierarchical Products of Experts</a></p>
<p>Author: Guy Mayraz, Geoffrey E. Hinton</p><p>Abstract: The product of experts learning procedure [1] can discover a set of stochastic binary features that constitute a non-linear generative model of handwritten images of digits. The quality of generative models learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probabilities of test images under the 10 different class-specific models. To improve discriminative performance, it is helpful to learn a hierarchy of separate models for each digit class. Each model in the hierarchy has one layer of hidden units and the nth level model is trained on data that consists of the activities of the hidden units in the already trained (n - l)th level model. After training, each level produces a separate, unnormalized log probabilty score. With a three-level hierarchy for each of the 10 digit classes, a test image produces 30 scores which can be used as inputs to a supervised, logistic classification network that is trained on separate data. On the MNIST database, our system is comparable with current state-of-the-art discriminative methods, demonstrating that the product of experts learning procedure can produce effective generative models of high-dimensional data. 1 Learning products of stochastic binary experts Hinton [1] describes a learning algorithm for probabilistic generative models that are composed of a number of experts. Each expert specifies a probability distribution over the visible variables and the experts are combined by multiplying these distributions together and renormalizing. (1) where d is a data vector in a discrete space, Om is all the parameters of individual model m, Pm(dIOm) is the probability of d under model m, and c is an index over all possible vectors in the data space. A Restricted Boltzmann machine [2, 3] is a special case of a product of experts in which each expert is a single, binary stochastic hidden unit that has symmetrical connections to a set of visible units, and connections between the hidden units are forbidden. Inference in an RBM is much easier than in a general Boltzmann machine and it is also much easier than in a causal belief net because there is no explaining away. There is therefore no need to perform any iteration to determine the activities of the hidden units. The hidden states, Sj , are conditionally independent given the visible states, Si, and the distribution of Sj is given by the standard logistic function : 1 p(Sj = 1) = (2) 1 + exp( - Li WijSi) Conversely, the hidden states of an RBM are marginally dependent so it is easy for an RBM to learn population codes in which units may be highly correlated. It is hard to do this in causal belief nets with one hidden layer because the generative model of a causal belief net assumes marginal independence. An RBM can be trained using the standard Boltzmann machine learning algorithm which follows a noisy but unbiased estimate of the gradient of the log likelihood of the data. One way to implement this algorithm is to start the network with a data vector on the visible units and then to alternate between updating all of the hidden units in parallel and updating all of the visible units in parallel. Each update picks a binary state for a unit from its posterior distribution given the current states of all the units in the other set. If this alternating Gibbs sampling is run to equilibrium, there is a very simple way to update the weights so as to minimize the Kullback-Leibler divergence, QOIIQoo, between the data distribution, QO, and the equilibrium distribution of fantasies over the visible units, Qoo, produced by the RBM [4]: flWij oc QO - Q~ (3) where < SiSj >Qo is the expected value of SiSj when data is clamped on the visible units and the hidden states are sampled from their conditional distribution given the data, and Q ~ is the expected value of SiSj after prolonged Gibbs sampling. This learning rule does not work well because it can take a long time to approach thermal equilibrium and the sampling noise in the estimate of  Q ~ can swamp the gradient. [1] shows that it is far more effective to minimize the difference between QOllQoo and Q111Qoo where Q1 is the distribution of the one-step reconstructions of the data that are produced by first picking binary hidden states from their conditional distribution given the data and then picking binary visible states from their conditional distribution given the hidden states. The exact gradient of this</p><p>5 0.18502127 <a title="142-tfidf-5" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>Author: Anders Jonsson, Andrew G. Barto</p><p>Abstract: Learning a complex task can be significantly facilitated by defining a hierarchy of subtasks. An agent can learn to choose between various temporally abstract actions, each solving an assigned subtask, to accomplish the overall task. In this paper, we study hierarchical learning using the framework of options. We argue that to take full advantage of hierarchical structure, one should perform option-specific state abstraction, and that if this is to scale to larger tasks, state abstraction should be automated. We adapt McCallum's U-Tree algorithm to automatically build option-specific representations of the state feature space, and we illustrate the resulting algorithm using a simple hierarchical task. Results suggest that automated option-specific state abstraction is an attractive approach to making hierarchical learning systems more effective.</p><p>6 0.17009327 <a title="142-tfidf-6" href="./nips-2000-Programmable_Reinforcement_Learning_Agents.html">105 nips-2000-Programmable Reinforcement Learning Agents</a></p>
<p>7 0.16917913 <a title="142-tfidf-7" href="./nips-2000-Hierarchical_Memory-Based_Reinforcement_Learning.html">63 nips-2000-Hierarchical Memory-Based Reinforcement Learning</a></p>
<p>8 0.16687599 <a title="142-tfidf-8" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>9 0.15311366 <a title="142-tfidf-9" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>10 0.13747495 <a title="142-tfidf-10" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>11 0.11239491 <a title="142-tfidf-11" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>12 0.099452682 <a title="142-tfidf-12" href="./nips-2000-Robust_Reinforcement_Learning.html">113 nips-2000-Robust Reinforcement Learning</a></p>
<p>13 0.09628839 <a title="142-tfidf-13" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>14 0.09221936 <a title="142-tfidf-14" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>15 0.090359375 <a title="142-tfidf-15" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>16 0.085799925 <a title="142-tfidf-16" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>17 0.077035561 <a title="142-tfidf-17" href="./nips-2000-Factored_Semi-Tied_Covariance_Matrices.html">51 nips-2000-Factored Semi-Tied Covariance Matrices</a></p>
<p>18 0.068383515 <a title="142-tfidf-18" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>19 0.066867344 <a title="142-tfidf-19" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>20 0.058517601 <a title="142-tfidf-20" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.247), (1, -0.094), (2, 0.194), (3, -0.441), (4, -0.216), (5, 0.031), (6, -0.02), (7, -0.024), (8, 0.04), (9, -0.028), (10, 0.05), (11, -0.201), (12, 0.03), (13, -0.012), (14, -0.024), (15, 0.04), (16, 0.02), (17, 0.019), (18, -0.033), (19, -0.056), (20, -0.006), (21, 0.099), (22, -0.032), (23, 0.055), (24, -0.009), (25, 0.002), (26, -0.038), (27, -0.024), (28, 0.004), (29, 0.049), (30, -0.043), (31, 0.037), (32, -0.019), (33, 0.023), (34, 0.016), (35, -0.03), (36, -0.001), (37, 0.039), (38, 0.034), (39, 0.053), (40, -0.073), (41, 0.05), (42, -0.071), (43, -0.046), (44, 0.0), (45, -0.063), (46, 0.005), (47, 0.088), (48, 0.011), (49, -0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97138184 <a title="142-lsi-1" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>Author: Brian Sallans, Geoffrey E. Hinton</p><p>Abstract: The problem of reinforcement learning in large factored Markov decision processes is explored. The Q-value of a state-action pair is approximated by the free energy of a product of experts network. Network parameters are learned on-line using a modified SARSA algorithm which minimizes the inconsistency of the Q-values of consecutive state-action pairs. Actions are chosen based on the current value estimates by fixing the current state and sampling actions from the network using Gibbs sampling. The algorithm is tested on a co-operative multi-agent task. The product of experts model is found to perform comparably to table-based Q-Iearning for small instances of the task, and continues to perform well when the problem becomes too large for a table-based representation.</p><p>2 0.79543334 <a title="142-lsi-2" href="./nips-2000-Programmable_Reinforcement_Learning_Agents.html">105 nips-2000-Programmable Reinforcement Learning Agents</a></p>
<p>Author: David Andre, Stuart J. Russell</p><p>Abstract: We present an expressive agent design language for reinforcement learning that allows the user to constrain the policies considered by the learning process.The language includes standard features such as parameterized subroutines, temporary interrupts, aborts, and memory variables, but also allows for unspecified choices in the agent program. For learning that which isn't specified, we present provably convergent learning algorithms. We demonstrate by example that agent programs written in the language are concise as well as modular. This facilitates state abstraction and the transferability of learned skills.</p><p>3 0.74459839 <a title="142-lsi-3" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>Author: Christian R. Shelton</p><p>Abstract: For many problems which would be natural for reinforcement learning, the reward signal is not a single scalar value but has multiple scalar components. Examples of such problems include agents with multiple goals and agents with multiple users. Creating a single reward value by combining the multiple components can throwaway vital information and can lead to incorrect solutions. We describe the multiple reward source problem and discuss the problems with applying traditional reinforcement learning. We then present an new algorithm for finding a solution and results on simulated environments.</p><p>4 0.74200004 <a title="142-lsi-4" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>Author: Geoffrey J. Gordon</p><p>Abstract: Many algorithms for approximate reinforcement learning are not known to converge. In fact, there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point. This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region. The algorithms are SARSA(O) and V(O); the latter algorithm was used in the well-known TD-Gammon program. 1</p><p>5 0.65447563 <a title="142-lsi-5" href="./nips-2000-Hierarchical_Memory-Based_Reinforcement_Learning.html">63 nips-2000-Hierarchical Memory-Based Reinforcement Learning</a></p>
<p>Author: Natalia Hernandez-Gardiol, Sridhar Mahadevan</p><p>Abstract: A key challenge for reinforcement learning is scaling up to large partially observable domains. In this paper, we show how a hierarchy of behaviors can be used to create and select among variable length short-term memories appropriate for a task. At higher levels in the hierarchy, the agent abstracts over lower-level details and looks back over a variable number of high-level decisions in time. We formalize this idea in a framework called Hierarchical Suffix Memory (HSM). HSM uses a memory-based SMDP learning method to rapidly propagate delayed reward across long decision sequences. We describe a detailed experimental study comparing memory vs. hierarchy using the HSM framework on a realistic corridor navigation task. 1</p><p>6 0.631486 <a title="142-lsi-6" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>7 0.6084972 <a title="142-lsi-7" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>8 0.58476257 <a title="142-lsi-8" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>9 0.55249697 <a title="142-lsi-9" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>10 0.55205774 <a title="142-lsi-10" href="./nips-2000-Recognizing_Hand-written_Digits_Using_Hierarchical_Products_of_Experts.html">108 nips-2000-Recognizing Hand-written Digits Using Hierarchical Products of Experts</a></p>
<p>11 0.44231343 <a title="142-lsi-11" href="./nips-2000-Robust_Reinforcement_Learning.html">113 nips-2000-Robust Reinforcement Learning</a></p>
<p>12 0.43176362 <a title="142-lsi-12" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>13 0.40015006 <a title="142-lsi-13" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>14 0.38758036 <a title="142-lsi-14" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>15 0.35145327 <a title="142-lsi-15" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>16 0.33622959 <a title="142-lsi-16" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>17 0.3127597 <a title="142-lsi-17" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>18 0.30513191 <a title="142-lsi-18" href="./nips-2000-Stability_and_Noise_in_Biochemical_Switches.html">125 nips-2000-Stability and Noise in Biochemical Switches</a></p>
<p>19 0.2734642 <a title="142-lsi-19" href="./nips-2000-Overfitting_in_Neural_Nets%3A_Backpropagation%2C_Conjugate_Gradient%2C_and_Early_Stopping.html">97 nips-2000-Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping</a></p>
<p>20 0.26301005 <a title="142-lsi-20" href="./nips-2000-Sequentially_Fitting_%60%60Inclusive%27%27_Trees_for_Inference_in_Noisy-OR_Networks.html">115 nips-2000-Sequentially Fitting ``Inclusive'' Trees for Inference in Noisy-OR Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.036), (17, 0.086), (32, 0.022), (33, 0.031), (36, 0.292), (55, 0.03), (62, 0.169), (65, 0.022), (67, 0.057), (75, 0.015), (76, 0.041), (79, 0.023), (81, 0.023), (90, 0.044), (97, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86720616 <a title="142-lda-1" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>Author: Brian Sallans, Geoffrey E. Hinton</p><p>Abstract: The problem of reinforcement learning in large factored Markov decision processes is explored. The Q-value of a state-action pair is approximated by the free energy of a product of experts network. Network parameters are learned on-line using a modified SARSA algorithm which minimizes the inconsistency of the Q-values of consecutive state-action pairs. Actions are chosen based on the current value estimates by fixing the current state and sampling actions from the network using Gibbs sampling. The algorithm is tested on a co-operative multi-agent task. The product of experts model is found to perform comparably to table-based Q-Iearning for small instances of the task, and continues to perform well when the problem becomes too large for a table-based representation.</p><p>2 0.72829098 <a title="142-lda-2" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>Author: Martijn A. R. Leisink, Hilbert J. Kappen</p><p>Abstract: We present a method to bound the partition function of a Boltzmann machine neural network with any odd order polynomial. This is a direct extension of the mean field bound, which is first order. We show that the third order bound is strictly better than mean field. Additionally we show the rough outline how this bound is applicable to sigmoid belief networks. Numerical experiments indicate that an error reduction of a factor two is easily reached in the region where expansion based approximations are useful. 1</p><p>3 0.57738316 <a title="142-lda-3" href="./nips-2000-Recognizing_Hand-written_Digits_Using_Hierarchical_Products_of_Experts.html">108 nips-2000-Recognizing Hand-written Digits Using Hierarchical Products of Experts</a></p>
<p>Author: Guy Mayraz, Geoffrey E. Hinton</p><p>Abstract: The product of experts learning procedure [1] can discover a set of stochastic binary features that constitute a non-linear generative model of handwritten images of digits. The quality of generative models learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probabilities of test images under the 10 different class-specific models. To improve discriminative performance, it is helpful to learn a hierarchy of separate models for each digit class. Each model in the hierarchy has one layer of hidden units and the nth level model is trained on data that consists of the activities of the hidden units in the already trained (n - l)th level model. After training, each level produces a separate, unnormalized log probabilty score. With a three-level hierarchy for each of the 10 digit classes, a test image produces 30 scores which can be used as inputs to a supervised, logistic classification network that is trained on separate data. On the MNIST database, our system is comparable with current state-of-the-art discriminative methods, demonstrating that the product of experts learning procedure can produce effective generative models of high-dimensional data. 1 Learning products of stochastic binary experts Hinton [1] describes a learning algorithm for probabilistic generative models that are composed of a number of experts. Each expert specifies a probability distribution over the visible variables and the experts are combined by multiplying these distributions together and renormalizing. (1) where d is a data vector in a discrete space, Om is all the parameters of individual model m, Pm(dIOm) is the probability of d under model m, and c is an index over all possible vectors in the data space. A Restricted Boltzmann machine [2, 3] is a special case of a product of experts in which each expert is a single, binary stochastic hidden unit that has symmetrical connections to a set of visible units, and connections between the hidden units are forbidden. Inference in an RBM is much easier than in a general Boltzmann machine and it is also much easier than in a causal belief net because there is no explaining away. There is therefore no need to perform any iteration to determine the activities of the hidden units. The hidden states, Sj , are conditionally independent given the visible states, Si, and the distribution of Sj is given by the standard logistic function : 1 p(Sj = 1) = (2) 1 + exp( - Li WijSi) Conversely, the hidden states of an RBM are marginally dependent so it is easy for an RBM to learn population codes in which units may be highly correlated. It is hard to do this in causal belief nets with one hidden layer because the generative model of a causal belief net assumes marginal independence. An RBM can be trained using the standard Boltzmann machine learning algorithm which follows a noisy but unbiased estimate of the gradient of the log likelihood of the data. One way to implement this algorithm is to start the network with a data vector on the visible units and then to alternate between updating all of the hidden units in parallel and updating all of the visible units in parallel. Each update picks a binary state for a unit from its posterior distribution given the current states of all the units in the other set. If this alternating Gibbs sampling is run to equilibrium, there is a very simple way to update the weights so as to minimize the Kullback-Leibler divergence, QOIIQoo, between the data distribution, QO, and the equilibrium distribution of fantasies over the visible units, Qoo, produced by the RBM [4]: flWij oc QO - Q~ (3) where < SiSj >Qo is the expected value of SiSj when data is clamped on the visible units and the hidden states are sampled from their conditional distribution given the data, and Q ~ is the expected value of SiSj after prolonged Gibbs sampling. This learning rule does not work well because it can take a long time to approach thermal equilibrium and the sampling noise in the estimate of  Q ~ can swamp the gradient. [1] shows that it is far more effective to minimize the difference between QOllQoo and Q111Qoo where Q1 is the distribution of the one-step reconstructions of the data that are produced by first picking binary hidden states from their conditional distribution given the data and then picking binary visible states from their conditional distribution given the hidden states. The exact gradient of this</p><p>4 0.57707679 <a title="142-lda-4" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>Author: Geoffrey J. Gordon</p><p>Abstract: Many algorithms for approximate reinforcement learning are not known to converge. In fact, there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point. This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region. The algorithms are SARSA(O) and V(O); the latter algorithm was used in the well-known TD-Gammon program. 1</p><p>5 0.57443923 <a title="142-lda-5" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>Author: Igor V. Cadez, Padhraic Smyth</p><p>Abstract: We investigate a general characteristic of the trade-off in learning problems between goodness-of-fit and model complexity. Specifically we characterize a general class of learning problems where the goodness-of-fit function can be shown to be convex within firstorder as a function of model complexity. This general property of</p><p>6 0.55110186 <a title="142-lda-6" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>7 0.54183125 <a title="142-lda-7" href="./nips-2000-Hierarchical_Memory-Based_Reinforcement_Learning.html">63 nips-2000-Hierarchical Memory-Based Reinforcement Learning</a></p>
<p>8 0.53656512 <a title="142-lda-8" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>9 0.51394659 <a title="142-lda-9" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>10 0.51184928 <a title="142-lda-10" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>11 0.499143 <a title="142-lda-11" href="./nips-2000-Programmable_Reinforcement_Learning_Agents.html">105 nips-2000-Programmable Reinforcement Learning Agents</a></p>
<p>12 0.49765113 <a title="142-lda-12" href="./nips-2000-Learning_Switching_Linear_Models_of_Human_Motion.html">80 nips-2000-Learning Switching Linear Models of Human Motion</a></p>
<p>13 0.48765734 <a title="142-lda-13" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>14 0.48391601 <a title="142-lda-14" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>15 0.4838202 <a title="142-lda-15" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>16 0.4786709 <a title="142-lda-16" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>17 0.46866626 <a title="142-lda-17" href="./nips-2000-The_Use_of_MDL_to_Select_among_Computational_Models_of_Cognition.html">139 nips-2000-The Use of MDL to Select among Computational Models of Cognition</a></p>
<p>18 0.4653253 <a title="142-lda-18" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>19 0.46517479 <a title="142-lda-19" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>20 0.46385935 <a title="142-lda-20" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
