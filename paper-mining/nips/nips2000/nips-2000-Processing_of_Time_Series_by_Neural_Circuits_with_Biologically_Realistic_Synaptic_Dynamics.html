<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-104" href="#">nips2000-104</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</h1>
<br/><p>Source: <a title="nips-2000-104-pdf" href="http://papers.nips.cc/paper/1903-processing-of-time-series-by-neural-circuits-with-biologically-realistic-synaptic-dynamics.pdf">pdf</a></p><p>Author: Thomas Natschläger, Wolfgang Maass, Eduardo D. Sontag, Anthony M. Zador</p><p>Abstract: Experimental data show that biological synapses behave quite differently from the symbolic synapses in common artificial neural network models. Biological synapses are dynamic, i.e., their</p><p>Reference: <a title="nips-2000-104-reference" href="../nips2000_reference/nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 org  Abstract Experimental data show that biological synapses behave quite differently from the symbolic synapses in common artificial neural network models. [sent-9, score-0.797]
</p><p>2 , their "weight" changes on a short time scale by several hundred percent in dependence of the past input to the synapse. [sent-12, score-0.15]
</p><p>3 In this article we explore the consequences that these synaptic dynamics entail for the computational power of feedforward neural networks. [sent-13, score-0.516]
</p><p>4 We show that gradient descent suffices to approximate a given (quadratic) filter by a rather small neural system with dynamic synapses. [sent-14, score-0.704]
</p><p>5 We also compare our network model to artificial neural networks designed for time series processing. [sent-15, score-0.481]
</p><p>6 Our numerical results are complemented by theoretical analysis which show that even with just a single hidden layer such networks can approximate a surprisingly large large class of nonlinear filters: all filters that can be characterized by Volterra series. [sent-16, score-0.426]
</p><p>7 This result is robust with regard to various changes in the model for synaptic dynamics. [sent-17, score-0.324]
</p><p>8 1 Introduction More than two decades of research on artificial neural networks has emphasized the central role of synapses in neural computation. [sent-18, score-0.456]
</p><p>9 In a conventional artificial neural network, all units ("neurons") are assumed to be identical, so that the computation is completely specified by the synaptic "weights," i. [sent-19, score-0.453]
</p><p>10 Synapses in common artificial neural network models are static: the value Wij of a synaptic weight is assumed to change only during "learning". [sent-22, score-0.539]
</p><p>11 In contrast to that, the "weight" Wij (t) of a biological synapse at time t is known to be strongly dependent on the inputs Xj(t - T) that this synapse has received from the presynaptic neuron i at previous time steps t - T, see e. [sent-23, score-0.504]
</p><p>12 We will focus in this article on mean-field models for populations of neurons connected by dynamic synapses. [sent-26, score-0.532]
</p><p>13 5  pure depression  100  time  1 facilitation and depression  00  200  time  100  200  time  Figure 1: A dynamic synapse can produce quite different outputs for the same input. [sent-30, score-1.022]
</p><p>14 The response of a single synapse to a step increase in input activity applied at time step 0 is compared for three different parameter settings. [sent-31, score-0.229]
</p><p>15 Several models for single synapses have been proposed for the dynamic changes in synaptic efficacy. [sent-32, score-0.925]
</p><p>16 In [2] the model of [3] is extended to populations of neurons where the current synaptic efficacy Wij (t) between a population j and a population i at time t is modeled as a product of a facilitation term lij (t) and a depression term dij (t) scaled by the factor W ij . [sent-33, score-1.162]
</p><p>17 We consider a time discrete version of this model defined as follows :  Wij (t) lij(t + 1)  = W ij . [sent-34, score-0.161]
</p><p>18 (1- Uij) + Uij  (4)  with dij (0) = 1 and hj (0) = O. [sent-40, score-0.291]
</p><p>19 Equation (2) models facilitation (with time constant Fij ), whereas equation (3) models the combined effects of synaptic depression (with time constant D ij) and facilitation. [sent-41, score-0.676]
</p><p>20 Depending on the values of the characteristic parameters Uij, D ij , Fij a synaptic connection (ij) maps an input function Xj(t) into the corresponding time varying synaptic output Wij (t) . [sent-42, score-0.958]
</p><p>21 1 compares the output for three different sets of values for the parameters Uij, D ij , Fij . [sent-47, score-0.265]
</p><p>22 In this article we will consider feedforward networks coupled by dynamic synapses. [sent-49, score-0.584]
</p><p>23 One should think of the computational units in such a network as populations of spiking neurons. [sent-50, score-0.304]
</p><p>24 hidden units  dynamic synapses  Figure 2: The dynamic network model. [sent-53, score-1.249]
</p><p>25 Xj(t)), where u is either the sigmoid function u(u) = 1/(1 + exp(-u)) (in the hidden layers) or just the identity function u( u) = u (in the output layer) and Wij (t) is modeled according to Equ. [sent-55, score-0.15]
</p><p>26 In Sections 2 and 3 we demonstrate (by employing gradient descent to find appropriate values for the parameters Uij, D ij , Fij and Wij) that even small dynamic networks can compute complex quadratic filters. [sent-57, score-0.779]
</p><p>27 In Section 4 we address the question which synaptic parameters are important for a dynamic network to learn a given filter. [sent-58, score-0.944]
</p><p>28 In Section 5 we give a precise mathematical characterization of the computational power of such dynamic networks. [sent-59, score-0.404]
</p><p>29 2 Learning Arbitrary Quadratic Filters by Dynamic Networks In order to analyze which filters can be approximated by small dynamic networks we investigate the task of learning a quadratic filter Q randomly chosen from a class Qm. [sent-60, score-1.165]
</p><p>30 The class Qm consists of all quadratic filters Q whose output (Qx) (t) in response to the input time series x(t) is defined by some symmetric m x m matrix HQ = [hkd of filter coefficients hkl E ~ k = 1 . [sent-61, score-0.94]
</p><p>31 An example of the input and output for one choice of quadratic parameters (m = 10) are shown in Figs. [sent-68, score-0.336]
</p><p>32 We view such filter Q as an example for the kinds of complex transformations that are important to an organism's survival, such as those required for motor control and the processing of time-varying sensory inputs. [sent-70, score-0.293]
</p><p>33 The real transformations actually required may be very complex, but the simple filter Q provides a useful starting point for assessing the capacity of this architecture to transform one time-varying signal into another. [sent-72, score-0.293]
</p><p>34 Can a network of units coupled by dynamic synapses implement the filter Q? [sent-73, score-1.149]
</p><p>35 We tested the approximation capabilities of a rather small dynamic network with just 10 hidden units (5 excitatory and 5 inhibitory ones), and one output (Fig. [sent-74, score-0.945]
</p><p>36 The dynamics of inhibitory synapses is described by the same model as that for excitatory synapses. [sent-76, score-0.459]
</p><p>37 For any particular temporal pattern applied at the input and any particular choice of the synaptic parameters, this network generates a temporal pattern as output. [sent-77, score-0.572]
</p><p>38 This output can be thought of, for example, as the activity of a particular population of neurons in the cortex, and the target function as the time series generated for the same input by some unknown quadratic filter Q. [sent-78, score-0.843]
</p><p>39 The synaptic parameters Wij, D ij , Fij and Uij are chosen so that, for each input in the training set, the network minimized the mean-square error E[z, zQ] = ~ z=;=-oI(Z(t) - ZQ(t))2 between its output z(t) and the desired output zQ(t) specified by the filter Q. [sent-79, score-1.167]
</p><p>40 3C compares the network performance before and after training. [sent-86, score-0.208]
</p><p>41 Prior to training, the output is nearly flat, while after training the network output tracks the filter output closely (E[z,zQ] = 0. [sent-87, score-0.774]
</p><p>42 3D shows the performance after training for different randomly chosen quadratic filters  Q E Qm for m = 4, . [sent-90, score-0.38]
</p><p>43 Even for larger values of m the relatively small network with 10 hidden units performs rather well. [sent-94, score-0.291]
</p><p>44 Note that a quadratic filter of dimension m has  m(m + 1)/2 free parameters, whereas the dynamic network has a constant number of 80 adjustable parameters. [sent-95, score-1.013]
</p><p>45 This shows clearly that dynamic synapses enable a small network to mimic a wide range of possible quadratic target filters. [sent-96, score-0.915]
</p><p>46 , for all synapses ~J about conjugate gradient algorithms see e. [sent-105, score-0.349]
</p><p>47 B  0·20  50  100  150  200  time steps  c  D  o -020  50  100  150  200  o 4  6  time steps  B  10  m  12  14  16  Figure 3: A network with units coupled by dynamic synapses can approximate randomly drawn quadratic filters. [sent-113, score-1.177]
</p><p>48 The network had one input unit, 10 hidden units (5 excitatory, 5 inhibitory), and one output unit, see Fig. [sent-115, score-0.445]
</p><p>49 C Output of the network prior to training, with random initialization of the parameters, and the output of the dynamic network after learning. [sent-119, score-0.794]
</p><p>50 The target was the output of a quadratic filter Q E QlQ. [sent-120, score-0.515]
</p><p>51 The filter coefficients hkl (1 :::; k, l :::; 10) were generated randomly by subtracting J-t/2 from a random number generated from an exponential distribution with mean J-t = 3. [sent-121, score-0.449]
</p><p>52 For different sizes of HQ (HQ is a symmetric m x m matrix) we plotted the average performance (mse measured on a test set) over 20 different filters Q, i. [sent-123, score-0.191]
</p><p>53 3  Comparison with the model of Back and Tsoi  Our dynamic network model is not the first to incorporate temporal dynamics via dynamic synapses. [sent-126, score-1.014]
</p><p>54 Perhaps the earliest suggestion for a role for synaptic dynamics in network computation was by [7]. [sent-127, score-0.555]
</p><p>55 More recently, a number of networks have been proposed in which synapses implemented linear filters; in particular [6]. [sent-128, score-0.338]
</p><p>56 To assess the performance of our network model in relation to the model proposed in [6] we have analyzed the performance of our dynamic network model for the same system identification task that was employed as benchmark task in [6]. [sent-129, score-0.875]
</p><p>57 The goal of this task is to learn a filter F with (Fx)(t) = sin(u(t)) where u(t) is the output of a linear filter applied to the input time series X(t). [sent-130, score-0.885]
</p><p>58 It can clearly be seen that our network model (see Fig. [sent-133, score-0.166]
</p><p>59 3A for the network architecture) is able to learn this particular filter. [sent-134, score-0.2]
</p><p>60 Note that the network Back and Tsoi used to learn the task had 130 adjustable parameters (13 parameters per IIR synapse, 10 hidden units) whereas our network model had only 80 adjustable parameters (all parameters U ij , F ij , Dij and W ij were adjusted during learning). [sent-138, score-1.178]
</p><p>61 Hence, u(t) is the output of a linear filter applied to the input x(t). [sent-147, score-0.416]
</p><p>62 5  D  -1  -20  50  100 time  150  200  150  DN  I  = ST  200  50  100  I  150  Figure 4: Performance of our model on the system identification task used in [6]. [sent-153, score-0.132]
</p><p>63 B Output of the network after learning and the target. [sent-157, score-0.166]
</p><p>64 C Comparison of the mean square error (in units of to- 3 ) achieved on test data by the model of Back and Tsoi (BT) and by the dynamic network (DN). [sent-158, score-0.603]
</p><p>65 The network model of Back and Tsoi (BT) utilizes slightly more adjustable parameters than the dynamic network (DN). [sent-160, score-0.829]
</p><p>66 ' W U D  U  - - -  W  F  Figure 5: Impact of different synaptic parameters on the learning capabilities of a dynamic network. [sent-195, score-0.788]
</p><p>67 A In each trial (N = 100) a different quadratic filter matrix HQ (m = 6) was randomly generated as described in Fig. [sent-197, score-0.453]
</p><p>68 Along the diagonal one can see the impact of a single parameter, whereas the off-diagonal elements (which are symmetric) represent the impact of changing pairs of parameters. [sent-199, score-0.314]
</p><p>69 C Same interpretation as for panel A but the results shown (N = 20) are for the filter used in [6]. [sent-201, score-0.305]
</p><p>70 D Same interpretation as for panel B but the results shown (N = 20) are for the same filter as in panel C. [sent-202, score-0.348]
</p><p>71 This shows that a very simple feedforward network with biologically realistic synaptic dynamics yields performance comparable to that of artificial networks that were previously designed to yield good performance in the time series domain without any claims of biological realism. [sent-203, score-1.137]
</p><p>72 It remains an open experimental question which synaptic parameters are subject to usedependent plasticity, and under what conditions. [sent-205, score-0.387]
</p><p>73 For example, long term potentiation appears to change synaptic dynamics between pairs of layer 5 cortical neurons [8] but not in the hippocampus [9]. [sent-206, score-0.568]
</p><p>74 We therefore wondered whether plasticity in the synaptic dynamics is essential for a dynamic network to be able to learn a particular target filter. [sent-207, score-1.055]
</p><p>75 To address this question, we compared network performance when different parameter subsets were optimized using the conjugate gradient algorithm, while the other parameters were held fixed. [sent-208, score-0.378]
</p><p>76 In all experiments, the fixed parameters were chosen to ensure heterogeneity in presynaptic dynamics. [sent-209, score-0.15]
</p><p>77 5 shows that changing only the postsynaptic parameter W has comparable impact to changing only the presynaptic parameters U or D, whereas changing only F has little impact on the dynamics of these networks (see diagonal of Fig. [sent-211, score-0.848]
</p><p>78 Hence, neither plasticity in the presynaptic dynamics (U, D, F) alone nor plasticity of the postsynaptic efficacy (W) alone was sufficient to achieve good performance in this model. [sent-215, score-0.477]
</p><p>79 5 A Universal Approximation Theorem for Dynamic Networks In the preceding sections we had presented empirical evidence for the approximation capabilities of our dynamic network model for computations in the time series domain. [sent-216, score-0.737]
</p><p>80 Furthermore, in spite of the rather complicated system of equations that defines dynamic networks, one can give a precise mathematical characterization of the class of filters that can be approximated by them. [sent-219, score-0.662]
</p><p>81 An arbitrary filter F is called time invariant if a shift of the input functions by a constant to just causes a shift of the output function by the same constant to. [sent-221, score-0.57]
</p><p>82 A filter F has fading memory if and only if the value of F;f(O) can be approximated arbitrarily closely by the value of F~(O) for functions ~ that approximate the functions ;f for sufficiently long bounded intervals [- T, 0]. [sent-223, score-0.501]
</p><p>83 Interesting examples of linear and nonlinear time invariant filters with fading memory can be generated with the help of representations of the form (Fx)(t) = oo . [sent-224, score-0.499]
</p><p>84 Note that for k = 1 it yields the usual representation for a linear time invariant filter. [sent-240, score-0.116]
</p><p>85 The class of filters that can be represented by Volterra series, i. [sent-241, score-0.19]
</p><p>86 Let F be an arbitrary filter that maps vectors offunctions ;f = (Xl, . [sent-245, score-0.3]
</p><p>87 ,x n) E xn into functions from R into ~ Then the following are equivalent: (a) F can be approximated by dynamic networks' N defined in Fig. [sent-248, score-0.425]
</p><p>88 , for any € > 0 there exists such network N such that I(F;f)(t) - (N;f)(t) I < € for all ;f E xn and all t E R) (b) F can be approximated by dynamic networks (see Fig. [sent-251, score-0.685]
</p><p>89 2) with just a single layer of sigmoidal neurons ( c) F is time invariant and has fading memory  (d) F can be approximated by a sequence of (finite or infinite) Volterra series. [sent-252, score-0.548]
</p><p>90 The universal approximation result contained in Theorem 1 turns out to be rather robust with regard to changes in the definition of a dynamic network. [sent-255, score-0.434]
</p><p>91 Dynamic networks with just one layer of dynamic synapses and one subsequent layer of sigmoidal gates can approximate the same class of filters as dynamic networks with an arbitrary number of layers of  dynamic synapses and sigmoidal neurons. [sent-256, score-2.229]
</p><p>92 It can also be shown that Theorem 1 remains valid if one considers networks which have depressing synapses only or if one uses the model for synaptic dynamics proposed in [1]. [sent-257, score-0.727]
</p><p>93 6 Discussion Our central hypothesis is that rapid changes in synaptic strength, mediated by mechanisms such as facilitation and depression, are an integral part of neural processing. [sent-258, score-0.473]
</p><p>94 We have analyzed the computational power of such dynamic networks, which represent a new paradigm for neural computation on time series that is based on biologically realistic models for synaptic dynamics [11]. [sent-259, score-0.997]
</p><p>95 Our analytical results show that the class of nonlinear filters that can be approximated by dynamic networks, even with just a single hidden layer of sigmoidal neurons, is remarkably rich. [sent-260, score-0.817]
</p><p>96 It contains every time invariant filter with fading memory, hence arguable every filter that is potentially useful for a biological organism. [sent-261, score-0.832]
</p><p>97 We have tested dynamic networks on tasks such as the learning of a randomly chosen quadratic filter, as well as on the learning task used in [6], to illustrate the potential of this architecture. [sent-263, score-0.645]
</p><p>98 A quantitative description of short-term plasticity at excitatory synapses in layer 2/3 of rat primary visual cortex. [sent-274, score-0.457]
</p><p>99 A simplified gradient algorithm for 1IR synapse multilayer perceptrons. [sent-305, score-0.165]
</p><p>100 Hippocampal long-term potentiation preserves the fidelity of postsynaptic responses to presynaptic bursts. [sent-326, score-0.185]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dynamic', 0.357), ('synaptic', 0.289), ('filter', 0.262), ('synapses', 0.244), ('uij', 0.213), ('zq', 0.186), ('dij', 0.166), ('network', 0.166), ('wij', 0.162), ('filters', 0.149), ('fading', 0.133), ('volterra', 0.133), ('hj', 0.125), ('quadratic', 0.117), ('facilitation', 0.115), ('synapse', 0.114), ('impact', 0.114), ('hq', 0.106), ('tsoi', 0.106), ('output', 0.105), ('depression', 0.104), ('fij', 0.104), ('dynamics', 0.1), ('ij', 0.095), ('networks', 0.094), ('presynaptic', 0.085), ('units', 0.08), ('hkl', 0.08), ('plasticity', 0.078), ('lij', 0.077), ('capabilities', 0.077), ('adjustable', 0.075), ('series', 0.071), ('excitatory', 0.068), ('approximated', 0.068), ('layer', 0.067), ('neurons', 0.066), ('time', 0.066), ('parameters', 0.065), ('mse', 0.062), ('sigmoidal', 0.06), ('biological', 0.059), ('populations', 0.058), ('xj', 0.057), ('postsynaptic', 0.054), ('conjugate', 0.054), ('fx', 0.053), ('harbor', 0.053), ('qm', 0.053), ('article', 0.051), ('gradient', 0.051), ('changing', 0.05), ('artificial', 0.05), ('theorem', 0.05), ('invariant', 0.05), ('input', 0.049), ('dn', 0.049), ('back', 0.047), ('biologically', 0.047), ('characterization', 0.047), ('inhibitory', 0.047), ('spring', 0.046), ('potentiation', 0.046), ('hidden', 0.045), ('panel', 0.043), ('population', 0.043), ('performance', 0.042), ('feedforward', 0.042), ('cold', 0.042), ('maass', 0.042), ('universal', 0.042), ('randomly', 0.041), ('class', 0.041), ('coupled', 0.04), ('efficacy', 0.04), ('bo', 0.038), ('markram', 0.038), ('arbitrary', 0.038), ('memory', 0.038), ('pyramidal', 0.036), ('qx', 0.036), ('task', 0.036), ('comparable', 0.036), ('whereas', 0.036), ('changes', 0.035), ('bt', 0.034), ('neocortical', 0.034), ('temporal', 0.034), ('neural', 0.034), ('learn', 0.034), ('generated', 0.033), ('realistic', 0.033), ('question', 0.033), ('transformations', 0.031), ('io', 0.031), ('training', 0.031), ('target', 0.031), ('identification', 0.03), ('pure', 0.03), ('nonlinear', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="104-tfidf-1" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass, Eduardo D. Sontag, Anthony M. Zador</p><p>Abstract: Experimental data show that biological synapses behave quite differently from the symbolic synapses in common artificial neural network models. Biological synapses are dynamic, i.e., their</p><p>2 0.25702965 <a title="104-tfidf-2" href="./nips-2000-Finding_the_Key_to_a_Synapse.html">55 nips-2000-Finding the Key to a Synapse</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass</p><p>Abstract: Experimental data have shown that synapses are heterogeneous: different synapses respond with different sequences of amplitudes of postsynaptic responses to the same spike train. Neither the role of synaptic dynamics itself nor the role of the heterogeneity of synaptic dynamics for computations in neural circuits is well understood. We present in this article methods that make it feasible to compute for a given synapse with known synaptic parameters the spike train that is optimally fitted to the synapse, for example in the sense that it produces the largest sum of postsynaptic responses. To our surprise we find that most of these optimally fitted spike trains match common firing patterns of specific types of neurons that are discussed in the literature.</p><p>3 0.24193795 <a title="104-tfidf-3" href="./nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">129 nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<p>Author: Gal Chechik, Naftali Tishby</p><p>Abstract: The paradigm of Hebbian learning has recently received a novel interpretation with the discovery of synaptic plasticity that depends on the relative timing of pre and post synaptic spikes. This paper derives a temporally dependent learning rule from the basic principle of mutual information maximization and studies its relation to the experimentally observed plasticity. We find that a supervised spike-dependent learning rule sharing similar structure with the experimentally observed plasticity increases mutual information to a stable near optimal level. Moreover, the analysis reveals how the temporal structure of time-dependent learning rules is determined by the temporal filter applied by neurons over their inputs. These results suggest experimental prediction as to the dependency of the learning rule on neuronal biophysical parameters 1</p><p>4 0.15062785 <a title="104-tfidf-4" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>Author: Silvia Scarpetta, Zhaoping Li, John A. Hertz</p><p>Abstract: We apply to oscillatory networks a class of learning rules in which synaptic weights change proportional to pre- and post-synaptic activity, with a kernel A(r) measuring the effect for a postsynaptic spike a time r after the presynaptic one. The resulting synaptic matrices have an outer-product form in which the oscillating patterns are represented as complex vectors. In a simple model, the even part of A(r) enhances the resonant response to learned stimulus by reducing the effective damping, while the odd part determines the frequency of oscillation. We relate our model to the olfactory cortex and hippocampus and their presumed roles in forming associative memories and input representations. 1</p><p>5 0.14425139 <a title="104-tfidf-5" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>Author: Odelia Schwartz, Eero P. Simoncelli</p><p>Abstract: We explore the statistical properties of natural sound stimuli preprocessed with a bank of linear filters. The responses of such filters exhibit a striking form of statistical dependency, in which the response variance of each filter grows with the response amplitude of filters tuned for nearby frequencies. These dependencies may be substantially reduced using an operation known as divisive normalization, in which the response of each filter is divided by a weighted sum of the rectified responses of other filters. The weights may be chosen to maximize the independence of the normalized responses for an ensemble of natural sounds. We demonstrate that the resulting model accounts for nonlinearities in the response characteristics of the auditory nerve, by comparing model simulations to electrophysiological recordings. In previous work (NIPS, 1998) we demonstrated that an analogous model derived from the statistics of natural images accounts for non-linear properties of neurons in primary visual cortex. Thus, divisive normalization appears to be a generic mechanism for eliminating a type of statistical dependency that is prevalent in natural signals of different modalities. Signals in the real world are highly structured. For example, natural sounds typically contain both harmonic and rythmic structure. It is reasonable to assume that biological auditory systems are designed to represent these structures in an efficient manner [e.g., 1,2]. Specifically, Barlow hypothesized that a role of early sensory processing is to remove redundancy in the sensory input, resulting in a set of neural responses that are statistically independent. Experimentally, one can test this hypothesis by examining the statistical properties of neural responses under natural stimulation conditions [e.g., 3,4], or the statistical dependency of pairs (or groups) of neural responses. Due to their technical difficulty, such multi-cellular experiments are only recently becoming possible, and the earliest reports in vision appear consistent with the hypothesis [e.g., 5]. An alternative approach, which we follow here, is to develop a neural model from the statistics of natural signals and show that response properties of this model are similar to those of biological sensory neurons. A number of researchers have derived linear filter models using statistical criterion. For visual images, this results in linear filters localized in frequency, orientation and phase [6, 7]. Similar work in audition has yielded filters localized in frequency and phase [8]. Although these linear models provide an important starting point for neural modeling, sensory neurons are highly nonlinear. In addition, the statistical properties of natural signals are too complex to expect a linear transformation to result in an independent set of components. Recent results indicate that nonlinear gain control plays an important role in neural processing. Ruderman and Bialek [9] have shown that division by a local estimate of standard deviation can increase the entropy of responses of center-surround filters to natural images. Such a model is consistent with the properties of neurons in the retina and lateral geniculate nucleus. Heeger and colleagues have shown that the nonlinear behaviors of neurons in primary visual cortex may be described using a form of gain control known as divisive normalization [10], in which the response of a linear kernel is rectified and divided by the sum of other rectified kernel responses and a constant. We have recently shown that the responses of oriented linear filters exhibit nonlinear statistical dependencies that may be substantially reduced using a variant of this model, in which the normalization signal is computed from a weighted sum of other rectified kernel responses [11, 12]. The resulting model, with weighting parameters determined from image statistics, accounts qualitatively for physiological nonlinearities observed in primary visual cortex. In this paper, we demonstrate that the responses of bandpass linear filters to natural sounds exhibit striking statistical dependencies, analogous to those found in visual images. A divisive normalization procedure can substantially remove these dependencies. We show that this model, with parameters optimized for a collection of natural sounds, can account for nonlinear behaviors of neurons at the level of the auditory nerve. Specifically, we show that: 1) the shape offrequency tuning curves varies with sound pressure level, even though the underlying linear filters are fixed; and 2) superposition of a non-optimal tone suppresses the response of a linear filter in a divisive fashion, and the amount of suppression depends on the distance between the frequency of the tone and the preferred frequency of the filter. 1 Empirical observations of natural sound statistics The basic statistical properties of natural sounds, as observed through a linear filter, have been previously documented by Attias [13]. In particular, he showed that, as with visual images, the spectral energy falls roughly according to a power law, and that the histograms of filter responses are more kurtotic than a Gaussian (i.e., they have a sharp peak at zero, and very long tails). Here we examine the joint statistical properties of a pair of linear filters tuned for nearby temporal frequencies. We choose a fixed set of filters that have been widely used in modeling the peripheral auditory system [14]. Figure 1 shows joint histograms of the instantaneous responses of a particular pair of linear filters to five different types of natural sound, and white noise. First note that the responses are approximately decorrelated: the expected value of the y-axis value is roughly zero for all values of the x-axis variable. The responses are not, however, statistically independent: the width of the distribution of responses of one filter increases with the response amplitude of the other filter. If the two responses were statistically independent, then the response of the first filter should not provide any information about the distribution of responses of the other filter. We have found that this type of variance dependency (sometimes accompanied by linear correlation) occurs in a wide range of natural sounds, ranging from animal sounds to music. We emphasize that this dependency is a property of natural sounds, and is not due purely to our choice of linear filters. For example, no such dependency is observed when the input consists of white noise (see Fig. 1). The strength of this dependency varies for different pairs of linear filters . In addition, we see this type of dependency between instantaneous responses of a single filter at two Speech o -1 Drums • Monkey Cat White noise Nocturnal nature I~ ~; ~ • Figure 1: Joint conditional histogram of instantaneous linear responses of two bandpass filters with center frequencies 2000 and 2840 Hz. Pixel intensity corresponds to frequency of occurrence of a given pair of values, except that each column has been independently rescaled to fill the full intensity range. For the natural sounds, responses are not independent: the standard deviation of the ordinate is roughly proportional to the magnitude of the abscissa. Natural sounds were recorded from CDs and converted to sampling frequency of 22050 Hz. nearby time instants. Since the dependency involves the variance of the responses, we can substantially reduce it by dividing. In particular, the response of each filter is divided by a weighted sum of responses of other rectified filters and an additive constant. Specifically: L2 Ri = 2: (1) 12 j WjiLj + 0'2 where Li is the instantaneous linear response of filter i, strength of suppression of filter i by filter j. 0' is a constant and Wji controls the We would like to choose the parameters of the model (the weights Wji, and the constant 0') to optimize the independence of the normalized response to an ensemble of natural sounds. Such an optimization is quite computationally expensive. We instead assume a Gaussian form for the underlying conditional distribution, as described in [15]: P (LiILj,j E Ni ) '</p><p>6 0.12926799 <a title="104-tfidf-6" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>7 0.11511258 <a title="104-tfidf-7" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>8 0.10958463 <a title="104-tfidf-8" href="./nips-2000-Homeostasis_in_a_Silicon_Integrate_and_Fire_Neuron.html">67 nips-2000-Homeostasis in a Silicon Integrate and Fire Neuron</a></p>
<p>9 0.098683372 <a title="104-tfidf-9" href="./nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">88 nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>10 0.09863627 <a title="104-tfidf-10" href="./nips-2000-The_Unscented_Particle_Filter.html">137 nips-2000-The Unscented Particle Filter</a></p>
<p>11 0.097664945 <a title="104-tfidf-11" href="./nips-2000-Permitted_and_Forbidden_Sets_in_Symmetric_Threshold-Linear_Networks.html">100 nips-2000-Permitted and Forbidden Sets in Symmetric Threshold-Linear Networks</a></p>
<p>12 0.096946537 <a title="104-tfidf-12" href="./nips-2000-Who_Does_What%3F_A_Novel_Algorithm_to_Determine_Function_Localization.html">147 nips-2000-Who Does What? A Novel Algorithm to Determine Function Localization</a></p>
<p>13 0.096179098 <a title="104-tfidf-13" href="./nips-2000-An_Information_Maximization_Approach_to_Overcomplete_and_Recurrent_Representations.html">24 nips-2000-An Information Maximization Approach to Overcomplete and Recurrent Representations</a></p>
<p>14 0.087197535 <a title="104-tfidf-14" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>15 0.084115513 <a title="104-tfidf-15" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>16 0.080729231 <a title="104-tfidf-16" href="./nips-2000-Computing_with_Finite_and_Infinite_Networks.html">35 nips-2000-Computing with Finite and Infinite Networks</a></p>
<p>17 0.07907594 <a title="104-tfidf-17" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>18 0.077099599 <a title="104-tfidf-18" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>19 0.076605558 <a title="104-tfidf-19" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>20 0.072061323 <a title="104-tfidf-20" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.262), (1, -0.213), (2, -0.225), (3, -0.07), (4, 0.1), (5, 0.006), (6, -0.038), (7, 0.056), (8, -0.032), (9, -0.025), (10, 0.034), (11, -0.122), (12, 0.005), (13, -0.095), (14, 0.142), (15, -0.138), (16, 0.027), (17, 0.198), (18, 0.159), (19, 0.09), (20, -0.017), (21, -0.188), (22, -0.188), (23, -0.04), (24, -0.031), (25, -0.075), (26, 0.101), (27, -0.014), (28, 0.07), (29, 0.084), (30, -0.013), (31, 0.182), (32, -0.006), (33, 0.07), (34, -0.062), (35, -0.011), (36, 0.082), (37, -0.049), (38, 0.051), (39, 0.102), (40, 0.032), (41, -0.017), (42, -0.094), (43, -0.109), (44, 0.031), (45, 0.088), (46, -0.069), (47, 0.007), (48, -0.045), (49, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96594757 <a title="104-lsi-1" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass, Eduardo D. Sontag, Anthony M. Zador</p><p>Abstract: Experimental data show that biological synapses behave quite differently from the symbolic synapses in common artificial neural network models. Biological synapses are dynamic, i.e., their</p><p>2 0.74965489 <a title="104-lsi-2" href="./nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">129 nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<p>Author: Gal Chechik, Naftali Tishby</p><p>Abstract: The paradigm of Hebbian learning has recently received a novel interpretation with the discovery of synaptic plasticity that depends on the relative timing of pre and post synaptic spikes. This paper derives a temporally dependent learning rule from the basic principle of mutual information maximization and studies its relation to the experimentally observed plasticity. We find that a supervised spike-dependent learning rule sharing similar structure with the experimentally observed plasticity increases mutual information to a stable near optimal level. Moreover, the analysis reveals how the temporal structure of time-dependent learning rules is determined by the temporal filter applied by neurons over their inputs. These results suggest experimental prediction as to the dependency of the learning rule on neuronal biophysical parameters 1</p><p>3 0.65838349 <a title="104-lsi-3" href="./nips-2000-Finding_the_Key_to_a_Synapse.html">55 nips-2000-Finding the Key to a Synapse</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass</p><p>Abstract: Experimental data have shown that synapses are heterogeneous: different synapses respond with different sequences of amplitudes of postsynaptic responses to the same spike train. Neither the role of synaptic dynamics itself nor the role of the heterogeneity of synaptic dynamics for computations in neural circuits is well understood. We present in this article methods that make it feasible to compute for a given synapse with known synaptic parameters the spike train that is optimally fitted to the synapse, for example in the sense that it produces the largest sum of postsynaptic responses. To our surprise we find that most of these optimally fitted spike trains match common firing patterns of specific types of neurons that are discussed in the literature.</p><p>4 0.62314171 <a title="104-lsi-4" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>Author: Silvia Scarpetta, Zhaoping Li, John A. Hertz</p><p>Abstract: We apply to oscillatory networks a class of learning rules in which synaptic weights change proportional to pre- and post-synaptic activity, with a kernel A(r) measuring the effect for a postsynaptic spike a time r after the presynaptic one. The resulting synaptic matrices have an outer-product form in which the oscillating patterns are represented as complex vectors. In a simple model, the even part of A(r) enhances the resonant response to learned stimulus by reducing the effective damping, while the odd part determines the frequency of oscillation. We relate our model to the olfactory cortex and hippocampus and their presumed roles in forming associative memories and input representations. 1</p><p>5 0.44799355 <a title="104-lsi-5" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>Author: Odelia Schwartz, Eero P. Simoncelli</p><p>Abstract: We explore the statistical properties of natural sound stimuli preprocessed with a bank of linear filters. The responses of such filters exhibit a striking form of statistical dependency, in which the response variance of each filter grows with the response amplitude of filters tuned for nearby frequencies. These dependencies may be substantially reduced using an operation known as divisive normalization, in which the response of each filter is divided by a weighted sum of the rectified responses of other filters. The weights may be chosen to maximize the independence of the normalized responses for an ensemble of natural sounds. We demonstrate that the resulting model accounts for nonlinearities in the response characteristics of the auditory nerve, by comparing model simulations to electrophysiological recordings. In previous work (NIPS, 1998) we demonstrated that an analogous model derived from the statistics of natural images accounts for non-linear properties of neurons in primary visual cortex. Thus, divisive normalization appears to be a generic mechanism for eliminating a type of statistical dependency that is prevalent in natural signals of different modalities. Signals in the real world are highly structured. For example, natural sounds typically contain both harmonic and rythmic structure. It is reasonable to assume that biological auditory systems are designed to represent these structures in an efficient manner [e.g., 1,2]. Specifically, Barlow hypothesized that a role of early sensory processing is to remove redundancy in the sensory input, resulting in a set of neural responses that are statistically independent. Experimentally, one can test this hypothesis by examining the statistical properties of neural responses under natural stimulation conditions [e.g., 3,4], or the statistical dependency of pairs (or groups) of neural responses. Due to their technical difficulty, such multi-cellular experiments are only recently becoming possible, and the earliest reports in vision appear consistent with the hypothesis [e.g., 5]. An alternative approach, which we follow here, is to develop a neural model from the statistics of natural signals and show that response properties of this model are similar to those of biological sensory neurons. A number of researchers have derived linear filter models using statistical criterion. For visual images, this results in linear filters localized in frequency, orientation and phase [6, 7]. Similar work in audition has yielded filters localized in frequency and phase [8]. Although these linear models provide an important starting point for neural modeling, sensory neurons are highly nonlinear. In addition, the statistical properties of natural signals are too complex to expect a linear transformation to result in an independent set of components. Recent results indicate that nonlinear gain control plays an important role in neural processing. Ruderman and Bialek [9] have shown that division by a local estimate of standard deviation can increase the entropy of responses of center-surround filters to natural images. Such a model is consistent with the properties of neurons in the retina and lateral geniculate nucleus. Heeger and colleagues have shown that the nonlinear behaviors of neurons in primary visual cortex may be described using a form of gain control known as divisive normalization [10], in which the response of a linear kernel is rectified and divided by the sum of other rectified kernel responses and a constant. We have recently shown that the responses of oriented linear filters exhibit nonlinear statistical dependencies that may be substantially reduced using a variant of this model, in which the normalization signal is computed from a weighted sum of other rectified kernel responses [11, 12]. The resulting model, with weighting parameters determined from image statistics, accounts qualitatively for physiological nonlinearities observed in primary visual cortex. In this paper, we demonstrate that the responses of bandpass linear filters to natural sounds exhibit striking statistical dependencies, analogous to those found in visual images. A divisive normalization procedure can substantially remove these dependencies. We show that this model, with parameters optimized for a collection of natural sounds, can account for nonlinear behaviors of neurons at the level of the auditory nerve. Specifically, we show that: 1) the shape offrequency tuning curves varies with sound pressure level, even though the underlying linear filters are fixed; and 2) superposition of a non-optimal tone suppresses the response of a linear filter in a divisive fashion, and the amount of suppression depends on the distance between the frequency of the tone and the preferred frequency of the filter. 1 Empirical observations of natural sound statistics The basic statistical properties of natural sounds, as observed through a linear filter, have been previously documented by Attias [13]. In particular, he showed that, as with visual images, the spectral energy falls roughly according to a power law, and that the histograms of filter responses are more kurtotic than a Gaussian (i.e., they have a sharp peak at zero, and very long tails). Here we examine the joint statistical properties of a pair of linear filters tuned for nearby temporal frequencies. We choose a fixed set of filters that have been widely used in modeling the peripheral auditory system [14]. Figure 1 shows joint histograms of the instantaneous responses of a particular pair of linear filters to five different types of natural sound, and white noise. First note that the responses are approximately decorrelated: the expected value of the y-axis value is roughly zero for all values of the x-axis variable. The responses are not, however, statistically independent: the width of the distribution of responses of one filter increases with the response amplitude of the other filter. If the two responses were statistically independent, then the response of the first filter should not provide any information about the distribution of responses of the other filter. We have found that this type of variance dependency (sometimes accompanied by linear correlation) occurs in a wide range of natural sounds, ranging from animal sounds to music. We emphasize that this dependency is a property of natural sounds, and is not due purely to our choice of linear filters. For example, no such dependency is observed when the input consists of white noise (see Fig. 1). The strength of this dependency varies for different pairs of linear filters . In addition, we see this type of dependency between instantaneous responses of a single filter at two Speech o -1 Drums • Monkey Cat White noise Nocturnal nature I~ ~; ~ • Figure 1: Joint conditional histogram of instantaneous linear responses of two bandpass filters with center frequencies 2000 and 2840 Hz. Pixel intensity corresponds to frequency of occurrence of a given pair of values, except that each column has been independently rescaled to fill the full intensity range. For the natural sounds, responses are not independent: the standard deviation of the ordinate is roughly proportional to the magnitude of the abscissa. Natural sounds were recorded from CDs and converted to sampling frequency of 22050 Hz. nearby time instants. Since the dependency involves the variance of the responses, we can substantially reduce it by dividing. In particular, the response of each filter is divided by a weighted sum of responses of other rectified filters and an additive constant. Specifically: L2 Ri = 2: (1) 12 j WjiLj + 0'2 where Li is the instantaneous linear response of filter i, strength of suppression of filter i by filter j. 0' is a constant and Wji controls the We would like to choose the parameters of the model (the weights Wji, and the constant 0') to optimize the independence of the normalized response to an ensemble of natural sounds. Such an optimization is quite computationally expensive. We instead assume a Gaussian form for the underlying conditional distribution, as described in [15]: P (LiILj,j E Ni ) '</p><p>6 0.42777532 <a title="104-lsi-6" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>7 0.39856198 <a title="104-lsi-7" href="./nips-2000-Who_Does_What%3F_A_Novel_Algorithm_to_Determine_Function_Localization.html">147 nips-2000-Who Does What? A Novel Algorithm to Determine Function Localization</a></p>
<p>8 0.39034146 <a title="104-lsi-8" href="./nips-2000-An_Information_Maximization_Approach_to_Overcomplete_and_Recurrent_Representations.html">24 nips-2000-An Information Maximization Approach to Overcomplete and Recurrent Representations</a></p>
<p>9 0.37246233 <a title="104-lsi-9" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>10 0.37152165 <a title="104-lsi-10" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>11 0.35906717 <a title="104-lsi-11" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>12 0.32675678 <a title="104-lsi-12" href="./nips-2000-The_Unscented_Particle_Filter.html">137 nips-2000-The Unscented Particle Filter</a></p>
<p>13 0.30270943 <a title="104-lsi-13" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>14 0.30153549 <a title="104-lsi-14" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>15 0.29800239 <a title="104-lsi-15" href="./nips-2000-Accumulator_Networks%3A_Suitors_of_Local_Probability_Propagation.html">15 nips-2000-Accumulator Networks: Suitors of Local Probability Propagation</a></p>
<p>16 0.29619017 <a title="104-lsi-16" href="./nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">42 nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<p>17 0.29089478 <a title="104-lsi-17" href="./nips-2000-Homeostasis_in_a_Silicon_Integrate_and_Fire_Neuron.html">67 nips-2000-Homeostasis in a Silicon Integrate and Fire Neuron</a></p>
<p>18 0.29019839 <a title="104-lsi-18" href="./nips-2000-Robust_Reinforcement_Learning.html">113 nips-2000-Robust Reinforcement Learning</a></p>
<p>19 0.281921 <a title="104-lsi-19" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>20 0.27342305 <a title="104-lsi-20" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.01), (10, 0.021), (17, 0.098), (18, 0.046), (24, 0.169), (26, 0.011), (32, 0.016), (33, 0.047), (42, 0.028), (54, 0.022), (55, 0.037), (62, 0.053), (65, 0.021), (67, 0.077), (76, 0.051), (79, 0.021), (81, 0.081), (87, 0.037), (90, 0.043), (93, 0.017), (97, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87033987 <a title="104-lda-1" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass, Eduardo D. Sontag, Anthony M. Zador</p><p>Abstract: Experimental data show that biological synapses behave quite differently from the symbolic synapses in common artificial neural network models. Biological synapses are dynamic, i.e., their</p><p>2 0.8571161 <a title="104-lda-2" href="./nips-2000-An_Adaptive_Metric_Machine_for_Pattern_Classification.html">23 nips-2000-An Adaptive Metric Machine for Pattern Classification</a></p>
<p>Author: Carlotta Domeniconi, Jing Peng, Dimitrios Gunopulos</p><p>Abstract: Nearest neighbor classification assumes locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with finite samples due to the curse of dimensionality. Severe bias can be introduced under these conditions when using the nearest neighbor rule. We propose a locally adaptive nearest neighbor classification method to try to minimize bias. We use a Chi-squared distance analysis to compute a flexible metric for producing neighborhoods that are elongated along less relevant feature dimensions and constricted along most influential ones. As a result, the class conditional probabilities tend to be smoother in the modified neighborhoods, whereby better classification performance can be achieved. The efficacy of our method is validated and compared against other techniques using a variety of real world data. 1</p><p>3 0.85510522 <a title="104-lda-3" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>Author: Penio S. Penev</p><p>Abstract: Low-dimensional representations are key to solving problems in highlevel vision, such as face compression and recognition. Factorial coding strategies for reducing the redundancy present in natural images on the basis of their second-order statistics have been successful in accounting for both psychophysical and neurophysiological properties of early vision. Class-specific representations are presumably formed later, at the higher-level stages of cortical processing. Here we show that when retinotopic factorial codes are derived for ensembles of natural objects, such as human faces, not only redundancy, but also dimensionality is reduced. We also show that objects are built from parts in a non-Gaussian fashion which allows these local-feature codes to have dimensionalities that are substantially lower than the respective Nyquist sampling rates.</p><p>4 0.69502729 <a title="104-lda-4" href="./nips-2000-Finding_the_Key_to_a_Synapse.html">55 nips-2000-Finding the Key to a Synapse</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass</p><p>Abstract: Experimental data have shown that synapses are heterogeneous: different synapses respond with different sequences of amplitudes of postsynaptic responses to the same spike train. Neither the role of synaptic dynamics itself nor the role of the heterogeneity of synaptic dynamics for computations in neural circuits is well understood. We present in this article methods that make it feasible to compute for a given synapse with known synaptic parameters the spike train that is optimally fitted to the synapse, for example in the sense that it produces the largest sum of postsynaptic responses. To our surprise we find that most of these optimally fitted spike trains match common firing patterns of specific types of neurons that are discussed in the literature.</p><p>5 0.67834681 <a title="104-lda-5" href="./nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">129 nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<p>Author: Gal Chechik, Naftali Tishby</p><p>Abstract: The paradigm of Hebbian learning has recently received a novel interpretation with the discovery of synaptic plasticity that depends on the relative timing of pre and post synaptic spikes. This paper derives a temporally dependent learning rule from the basic principle of mutual information maximization and studies its relation to the experimentally observed plasticity. We find that a supervised spike-dependent learning rule sharing similar structure with the experimentally observed plasticity increases mutual information to a stable near optimal level. Moreover, the analysis reveals how the temporal structure of time-dependent learning rules is determined by the temporal filter applied by neurons over their inputs. These results suggest experimental prediction as to the dependency of the learning rule on neuronal biophysical parameters 1</p><p>6 0.66903275 <a title="104-lda-6" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>7 0.62959296 <a title="104-lda-7" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>8 0.62395084 <a title="104-lda-8" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>9 0.6226514 <a title="104-lda-9" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>10 0.62159204 <a title="104-lda-10" href="./nips-2000-Probabilistic_Semantic_Video_Indexing.html">103 nips-2000-Probabilistic Semantic Video Indexing</a></p>
<p>11 0.62026316 <a title="104-lda-11" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>12 0.60999072 <a title="104-lda-12" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>13 0.60507357 <a title="104-lda-13" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>14 0.60485291 <a title="104-lda-14" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>15 0.60413617 <a title="104-lda-15" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>16 0.60409546 <a title="104-lda-16" href="./nips-2000-Stability_and_Noise_in_Biochemical_Switches.html">125 nips-2000-Stability and Noise in Biochemical Switches</a></p>
<p>17 0.60312074 <a title="104-lda-17" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>18 0.60122317 <a title="104-lda-18" href="./nips-2000-The_Early_Word_Catches_the_Weights.html">131 nips-2000-The Early Word Catches the Weights</a></p>
<p>19 0.59991914 <a title="104-lda-19" href="./nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">88 nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>20 0.59918624 <a title="104-lda-20" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
