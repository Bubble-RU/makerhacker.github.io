<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>146 nips-2000-What Can a Single Neuron Compute?</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-146" href="#">nips2000-146</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>146 nips-2000-What Can a Single Neuron Compute?</h1>
<br/><p>Source: <a title="nips-2000-146-pdf" href="http://papers.nips.cc/paper/1867-what-can-a-single-neuron-compute.pdf">pdf</a></p><p>Author: Blaise Agüera y Arcas, Adrienne L. Fairhall, William Bialek</p><p>Abstract: In this paper we formulate a description of the computation performed by a neuron as a combination of dimensional reduction and nonlinearity. We implement this description for the HodgkinHuxley model, identify the most relevant dimensions and find the nonlinearity. A two dimensional description already captures a significant fraction of the information that spikes carry about dynamic inputs. This description also shows that computation in the Hodgkin-Huxley model is more complex than a simple integrateand-fire or perceptron model. 1</p><p>Reference: <a title="nips-2000-146-reference" href="../nips2000_reference/nips-2000-What_Can_a_Single_Neuron_Compute%3F_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract In this paper we formulate a description of the computation performed by a neuron as a combination of dimensional reduction and nonlinearity. [sent-7, score-0.323]
</p><p>2 We implement this description for the HodgkinHuxley model, identify the most relevant dimensions and find the nonlinearity. [sent-8, score-0.321]
</p><p>3 A two dimensional description already captures a significant fraction of the information that spikes carry about dynamic inputs. [sent-9, score-0.565]
</p><p>4 This description also shows that computation in the Hodgkin-Huxley model is more complex than a simple integrateand-fire or perceptron model. [sent-10, score-0.125]
</p><p>5 1  Introduction  Classical neural network models approximate neurons as devices that sum their inputs and generate a nonzero output if the sum exceeds a threshold. [sent-11, score-0.252]
</p><p>6 From our current state of knowledge in neurobiology it is easy to criticize these models as oversimplified: where is the complex geometry of neurons, or the many different kinds of ion channel, each with its own intricate multistate kinetics? [sent-12, score-0.092]
</p><p>7 Indeed, progress at this more microscopic level of description has led us to the point where we can write (almost) exact models for the electrical dynamics of neurons, at least on short time scales. [sent-13, score-0.26]
</p><p>8 These nearly exact models are complicated by any measure, including tens if not hundreds of differential equations to describe the states of different channels in different spatial compartments of the cell. [sent-14, score-0.102]
</p><p>9 Faced with this detailed microscopic description, we need to answer a question which goes well beyond the biological context: given a continuous dynamical system, what does it compute? [sent-15, score-0.094]
</p><p>10 Our goal in this paper is to make this question about what a neuron computes somewhat more precise, and then to explore what we take to be the simplest example, namely the Hodgkin- Huxley model [1],[2] (and refs therein). [sent-16, score-0.204]
</p><p>11 Real neurons take as inputs signals at their synapses and give as outputs sequences of discrete, identical pulses-action potentials or 'spikes'. [sent-18, score-0.257]
</p><p>12 The inputs themselves are spikes from other neurons, so the neuron is a device which takes N '" 103 pulse trains as inputs and generates one pulse train as output. [sent-19, score-0.641]
</p><p>13 More realistically, if the average spike  rates are'" 10 sec- 1, the input words can be compressed by a factor of ten. [sent-21, score-0.644]
</p><p>14 Thus we might be able to think about neurons as evaluating a Boolean function of roughly 1000 Boolean variables, and then characterizing the computational function of the cell amounts to specifying this Boolean function. [sent-22, score-0.281]
</p><p>15 The above estimate, though crude, makes clear that there will be no direct empirical attack on the question of what a neuron computes: there are too many possibilities to learn the function by brute force from any reasonable set of experiments. [sent-23, score-0.204]
</p><p>16 Progress requires the hypothesis that the function computed by a neuron is not arbitrary, but belongs to a simple class. [sent-24, score-0.165]
</p><p>17 Our suggestion is that this simple class involves functions that vary only over a low dimensional subspace of the inputs, and in fact we will start by searching for linear subspaces. [sent-25, score-0.18]
</p><p>18 Specifically, we begin by simplifying away the spatial structure of neurons and take inputs to be just injected currents into a point- like neuron. [sent-26, score-0.356]
</p><p>19 If the input is an injected current, then the neuron maps the history of this current, I(t < to), into the presence or absence of a spike at time to. [sent-29, score-0.872]
</p><p>20 More generally we might imagine that the cell (or our description) is noisy, so that there is a probability of spiking P[spike@toII(t < to)] which depends on the current history. [sent-30, score-0.201]
</p><p>21 We emphasize that the dependence on the history of the current means that there still are many dimensions to the input signal even though we have collapsed any spatial variations. [sent-31, score-0.272]
</p><p>22 If we work at time resolution flt and assume that currents in a window of size T are relevant to the decision to spike, then the inputs live in a space of D = T / flt, of order 100 dimensions in many interesting cases. [sent-32, score-0.554]
</p><p>23 If the neuron is sensitive only to a low dimensional linear subspace, we can define a set of signals S1, S2,···, SK by filtering the current,  s,. [sent-33, score-0.343]
</p><p>24 (t)I(to - t),  (1)  so that the probability of spiking depends only on this finite set of signals, P[spike@toII(t < to)] = P[spike@to]g(s1,s2,· . [sent-37, score-0.135]
</p><p>25 ,SK), (2) where we include the average probability of spiking so that 9 is dimensionless. [sent-39, score-0.143]
</p><p>26 If we think of the current I(t < to) as a vector, with one dimension for each time sample, then these filtered signals are linear projections of this vector. [sent-40, score-0.293]
</p><p>27 In this formulation, characterizing the computation done by a neuron means estimating the number of relevant stimulus dimensions (K, hopefully much less than D), identifying the filters which project into this relevant subspace,! [sent-41, score-0.759]
</p><p>28 The classical perceptron- like cell of neural network theory has only one relevant dimension and a simple form for g. [sent-43, score-0.167]
</p><p>29 3  Identifying low-dimensional structure  The idea that neurons might be sensitive only to low-dimensional projections of their inputs was developed explicitly in work on a motion sensitive neuron of the fly visual system [3]. [sent-44, score-0.538]
</p><p>30 Thus the spike triggered average stimulus, or reverse correlation function [4], is the first moment  ST A(T)  =j  [ds] P[s(t < to)lspike@to]s(to - T). [sent-48, score-0.773]
</p><p>31 (4)  We can also compute the covariance matrix of fluctuations around this average,  Cspike(T,T') = j[dS] P[s(t  < to)lspike@to]s(to-T)s(to-T')-STA(T)STA(T'). [sent-49, score-0.158]
</p><p>32 (6)  Notice that all of these covariance matrices are D x D in size. [sent-51, score-0.124]
</p><p>33 The surprising finding of [3] was that the change in the covariance matrix, t1C = Cs ike - Cprior, had only a very small number of nonzero eigenvalues. [sent-52, score-0.212]
</p><p>34 In fact it can be shown that if the probability of spiking depends on K linear projections of the stimulus as in eq. [sent-53, score-0.28]
</p><p>35 (2), and if the inputs s(t) are chosen from a Gaussian distribution, then the rank of the matrix t1C is exactly K. [sent-54, score-0.14]
</p><p>36 Further, the eigenvectors associated with nonzero eigenvalues span the relevant subspace (up to a rotation associated with the autocorrelations in the inputs. [sent-55, score-0.37]
</p><p>37 Thus eigenvalue analysis of the spike triggered covariance matrix gives us a direct way to search for a low dimensional linear subspace that captures the relevant stimulus features. [sent-56, score-1.344]
</p><p>38 The subscripted voltages VI and VNa are ion-specific reversal potentials. [sent-60, score-0.046]
</p><p>39 91, 9K and 9Na are empirically determined maximal conductances for the different ions,2 and the gating variables n, m and h (on the interval [0,1]) have their own voltage dependent dynamics:  dn/dt dm/dt dh/dt  = =  =  (O. [sent-61, score-0.046]
</p><p>40 Here we are interested in dynamic inputs I(t), but it is important to remember that for constant inputs the Hodgkin-Huxley model undergoes a Hopf bifurcation to spike at a constant frequency; further, this frequency is rather insensitive to the precise value of the input above onset. [sent-73, score-0.892]
</p><p>41 This 'rigidity' of the system is felt also in 2We have used the original parameters, with a sign change for voltages: C = lJ. [sent-74, score-0.032]
</p><p>42 gK  many regimes of dynamic stimulation, and can be thought of as a strong interaction among successive spikes. [sent-80, score-0.041]
</p><p>43 These interactions lead to long memory times, reflecting the infinite phase memory of the periodic orbit which exists for constant input. [sent-81, score-0.068]
</p><p>44 While spike interactions are interesting, we want to focus on the way that input current modulates the probability of spiking. [sent-82, score-0.649]
</p><p>45 These are defined by accumulating the interspike interval distribution and noticing that for some intervals t > tc the distribution decays exponentially, which means that the system has lost memory of the previous spike; thus spikes which are more than tc after the previous spike are isolated. [sent-84, score-0.896]
</p><p>46 In what follows we consider the response of the Hodgkin- Huxley model to currents I(t) with zero mean, 0. [sent-85, score-0.082]
</p><p>47 1 shows the change in covariance matrix f1C( r, r') for isolated spikes in our HH simulation, and fig. [sent-90, score-0.421]
</p><p>48 2(a) shows the resulting spectrum of eigenvalues as a function of sample size. [sent-91, score-0.118]
</p><p>49 The result strongly suggests that there are many fewer than D relevant dimensions. [sent-92, score-0.114]
</p><p>50 In particular, there seem to be two outstanding modes; the STA itself lies largely in the subspace of these modes, as shown in Fig. [sent-93, score-0.115]
</p><p>51 00  S  ~  t' ({l\se c)  Figure 1: The isolated spike triggered covariance matrix f1C(r,r'). [sent-97, score-0.962]
</p><p>52 If the neuron filtered its inputs and generated a spike when the output of the filter crosses threshold, we would find that there are two significant dimensions, corresponding to the filter and its derivative. [sent-100, score-0.941]
</p><p>53 Notice also that both filters have significant differentiating components- the cell is not simply integrating its inputs. [sent-102, score-0.124]
</p><p>54 2(a) suggests that two modes dominate, it also demonstrates that the smaller nonzero eigenvalues of the other modes are not just noise. [sent-104, score-0.371]
</p><p>55 The width of any spectral band of eigenvalues near zero due to finite sampling should decline with increasing sample size. [sent-105, score-0.156]
</p><p>56 Thus while the system is primarily sensitive to two dimensions, there is something  02  0. [sent-108, score-0.052]
</p><p>57 0 10+3  10+4  10+5  1  10+6  number of spikes accu mulated  Figure 2: (a) Convergence ofthe largest 32 eigenvalues of the isolated spike triggered covariance with increasing sample size_ (b) Projections of the isolated STA onto the covariance modes_  eigenmodes 1 and 2 . [sent-113, score-1.401]
</p><p>58 normalized derivative of mode 1  -30  -25  -20  Figure 3: Most significant two modes of the spike-triggered covariance_  missing in this picture. [sent-121, score-0.148]
</p><p>59 To quantify this, we must first characterize the nonlinear function g(81' 82). [sent-122, score-0.033]
</p><p>60 6  Nonlinearity and information  At each instant of time we can find the relevant projections of the stimulus 81 and 82. [sent-123, score-0.328]
</p><p>61 By construction, the distribution of these signals over the whole experiment, P(81, 82), is Gaussian. [sent-124, score-0.096]
</p><p>62 On the other hand, each time we see a spike we get a sample from the distribution P(81' 82Ispike@to), leading to the picture in fig. [sent-125, score-0.662]
</p><p>63 The prior and spike conditional distributions clearly are better separated in two dimensions than in one, which means that our two dimensional description captures more than the spike triggered average. [sent-127, score-1.685]
</p><p>64 Further, we see that the spike conditional distribution is curved, unlike what we would expect for a simple thresholding device. [sent-128, score-0.631]
</p><p>65 (2) and (3), we have (  ) _ P(81,82Ispike@to) P( ) , 81,82  9 81, 82 -  (9)  so that these two distributions determine the input/output relation of the neuron in this 2D space. [sent-130, score-0.201]
</p><p>66 We emphasize that although the subspace is linear, 9 can have arbitrary nonlinearity. [sent-131, score-0.158]
</p><p>67 4 shows that this input/output relation has sharp edges, but also some fuzziness. [sent-133, score-0.036]
</p><p>68 The HH model is deterministic, so in principle the input/output relation should be a c5 function: spikes occur only when certain exact conditions are met. [sent-134, score-0.223]
</p><p>69 Of course we have blurred things a bit by working at finite time  -w o  2  ~  . [sent-135, score-0.101]
</p><p>70 N  en  -2  -4  ~  4  a  2  s, (standard deviations)  Figure 4: 104 spike-conditional stimuli projected along the first 2 covariance modes. [sent-138, score-0.124]
</p><p>71 Given that we work at finite llt, spikes carry only a finite amount of information, and the quality of our 2D approximation can be judged by asking how much of this information is captured by this description. [sent-141, score-0.338]
</p><p>72 As explained in [5], the arrival time of a single spike provides an information lonespike  = ( r~) log2  [r~)] ),  (10)  where r(t) is the time dependent spike rate, f is the average spike rate, and (. [sent-142, score-1.965]
</p><p>73 With a deterministic model like HH, the rate r(t) either is zero or corresponds to one spike occurring in one bin of size llt, that is r = l/11t. [sent-146, score-0.667]
</p><p>74 On the other hand, if the probability of spiking really depends only on the stimulus dimensions 81 and 82, we can substitute  r(t)  -  f  -+  P(81,82Ispike@t)  ---'--=::::-:-=--'--=----:---"-  P(81,82)'  (11)  and use the ergodicity of the stimulus to replace time averages in Eq. [sent-148, score-0.462]
</p><p>75 Then we find [3, 5]  (12) If our two dimensional approximation were exact we would find l~~~s:pike = lone spike; more generally we will find 1~~~ss2pike ~ lone spike, and the fraction of the information we capture measures the quality of the approximation. [sent-150, score-0.317]
</p><p>76 For comparison, we also show the information captured by considering only the stimulus projection along the STA. [sent-153, score-0.187]
</p><p>77 -+- Covariance modes 1 and 2 (2D) 02 ~~----~~----~----~--~ 6  B  10  time discretization (msec)  Figure 5: Fraction of spike timing information captured by STA (lower curve) and projection onto covariance modes 1 and 2 (upper curve). [sent-154, score-1.176]
</p><p>78 7  Discussion  The simple, low-dimensional model described captures a substantial amount of information about spike timing for a HH neuron. [sent-155, score-0.769]
</p><p>79 However, the absolute information captured saturates for both the 1D and 2D cases, at RJ 3. [sent-158, score-0.077]
</p><p>80 Hence the information fraction captured plummets; recovering precise spike timing requires a more complex, higher dimensional representation of the stimulus. [sent-160, score-0.99]
</p><p>81 Is this effect important, or is timing at this resolution too noisy for this extra complexity to matter in a real neuron? [sent-161, score-0.19]
</p><p>82 Stochastic HH simulations have suggested that, when realistic noise sources are taken into account, the timing of spikes in response to dynamic stimuli is reproducible to within 1- 2 msec [6]. [sent-162, score-0.459]
</p><p>83 This suggests that such timing details may indeed be important. [sent-163, score-0.116]
</p><p>84 Even in 2D, one can observe that the spike conditional distribution is curved (fig. [sent-164, score-0.713]
</p><p>85 4); it is likely to curve along other dimensions as well. [sent-165, score-0.154]
</p><p>86 It may be possible to improve our approximation by considering the computation to take place on a low-dimensional but curved manifold, instead of a linear subspace. [sent-166, score-0.082]
</p><p>87 4 also implies that the computation in the HH model is not well approximated by an integrate and fire model, or a perceptron model limited to linear separations. [sent-168, score-0.032]
</p><p>88 Characterizing the complexity of the computation is an important step toward understanding neural systems. [sent-169, score-0.066]
</p><p>89 How to quantify this complexity theoretically is an area for future work; here, we have made progress toward this goal by describing such computations in a compact way and then evaluating the completeness of the description using information. [sent-170, score-0.272]
</p><p>90 How does the addition of more channels increase the complexity of the computation? [sent-172, score-0.067]
</p><p>91 Will this add more relevant dimensions or does the non-linearity change? [sent-173, score-0.228]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spike', 0.598), ('neuron', 0.165), ('hh', 0.16), ('lspike', 0.159), ('spikes', 0.154), ('msec', 0.148), ('triggered', 0.129), ('covariance', 0.124), ('timing', 0.116), ('modes', 0.115), ('subspace', 0.115), ('relevant', 0.114), ('dimensions', 0.114), ('stimulus', 0.11), ('sta', 0.109), ('inputs', 0.106), ('spiking', 0.097), ('huxley', 0.095), ('vna', 0.095), ('description', 0.093), ('fraction', 0.093), ('neurons', 0.09), ('eigenvalues', 0.085), ('currents', 0.082), ('curved', 0.082), ('isolated', 0.077), ('captured', 0.077), ('ruyter', 0.074), ('projections', 0.073), ('characterizing', 0.068), ('dimensional', 0.065), ('cprior', 0.063), ('cspike', 0.063), ('flt', 0.063), ('hodgkin', 0.063), ('llt', 0.063), ('lone', 0.063), ('lonespike', 0.063), ('toii', 0.063), ('tols', 0.063), ('boolean', 0.061), ('ds', 0.061), ('signals', 0.061), ('nonzero', 0.056), ('bialek', 0.056), ('captures', 0.055), ('adrienne', 0.055), ('microscopic', 0.055), ('pulse', 0.055), ('tc', 0.055), ('cell', 0.053), ('sensitive', 0.052), ('princeton', 0.051), ('current', 0.051), ('progress', 0.048), ('de', 0.046), ('average', 0.046), ('patch', 0.046), ('voltages', 0.046), ('conductances', 0.046), ('injected', 0.046), ('van', 0.044), ('resolution', 0.044), ('emphasize', 0.043), ('vk', 0.043), ('exp', 0.043), ('precise', 0.041), ('ion', 0.041), ('jersey', 0.041), ('dynamic', 0.041), ('curve', 0.04), ('question', 0.039), ('filtered', 0.039), ('membrane', 0.039), ('bin', 0.039), ('finite', 0.038), ('filters', 0.038), ('think', 0.038), ('channels', 0.037), ('relation', 0.036), ('toward', 0.036), ('identifying', 0.036), ('whole', 0.035), ('matrix', 0.034), ('memory', 0.034), ('quantify', 0.033), ('sample', 0.033), ('conditional', 0.033), ('exact', 0.033), ('significant', 0.033), ('spatial', 0.032), ('change', 0.032), ('bit', 0.032), ('history', 0.032), ('perceptron', 0.032), ('evaluating', 0.032), ('time', 0.031), ('carry', 0.031), ('complexity', 0.03), ('deterministic', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="146-tfidf-1" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>Author: Blaise Agüera y Arcas, Adrienne L. Fairhall, William Bialek</p><p>Abstract: In this paper we formulate a description of the computation performed by a neuron as a combination of dimensional reduction and nonlinearity. We implement this description for the HodgkinHuxley model, identify the most relevant dimensions and find the nonlinearity. A two dimensional description already captures a significant fraction of the information that spikes carry about dynamic inputs. This description also shows that computation in the Hodgkin-Huxley model is more complex than a simple integrateand-fire or perceptron model. 1</p><p>2 0.35874942 <a title="146-tfidf-2" href="./nips-2000-Finding_the_Key_to_a_Synapse.html">55 nips-2000-Finding the Key to a Synapse</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass</p><p>Abstract: Experimental data have shown that synapses are heterogeneous: different synapses respond with different sequences of amplitudes of postsynaptic responses to the same spike train. Neither the role of synaptic dynamics itself nor the role of the heterogeneity of synaptic dynamics for computations in neural circuits is well understood. We present in this article methods that make it feasible to compute for a given synapse with known synaptic parameters the spike train that is optimally fitted to the synapse, for example in the sense that it produces the largest sum of postsynaptic responses. To our surprise we find that most of these optimally fitted spike trains match common firing patterns of specific types of neurons that are discussed in the literature.</p><p>3 0.31735134 <a title="146-tfidf-3" href="./nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">88 nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>Author: Adrienne L. Fairhall, Geoffrey D. Lewen, William Bialek, Robert R. de Ruyter van Steveninck</p><p>Abstract: Many neural systems extend their dynamic range by adaptation. We examine the timescales of adaptation in the context of dynamically modulated rapidly-varying stimuli, and demonstrate in the fly visual system that adaptation to the statistical ensemble of the stimulus dynamically maximizes information transmission about the time-dependent stimulus. Further, while the rate response has long transients, the adaptation takes place on timescales consistent with optimal variance estimation.</p><p>4 0.29636052 <a title="146-tfidf-4" href="./nips-2000-Universality_and_Individuality_in_a_Neural_Code.html">141 nips-2000-Universality and Individuality in a Neural Code</a></p>
<p>Author: Elad Schneidman, Naama Brenner, Naftali Tishby, Robert R. de Ruyter van Steveninck, William Bialek</p><p>Abstract: The problem of neural coding is to understand how sequences of action potentials (spikes) are related to sensory stimuli, motor outputs, or (ultimately) thoughts and intentions. One clear question is whether the same coding rules are used by different neurons, or by corresponding neurons in different individuals. We present a quantitative formulation of this problem using ideas from information theory, and apply this approach to the analysis of experiments in the fly visual system. We find significant individual differences in the structure of the code, particularly in the way that temporal patterns of spikes are used to convey information beyond that available from variations in spike rate. On the other hand, all the flies in our ensemble exhibit a high coding efficiency, so that every spike carries the same amount of information in all the individuals. Thus the neural code has a quantifiable mixture of individuality and universality. 1</p><p>5 0.27912211 <a title="146-tfidf-5" href="./nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">129 nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<p>Author: Gal Chechik, Naftali Tishby</p><p>Abstract: The paradigm of Hebbian learning has recently received a novel interpretation with the discovery of synaptic plasticity that depends on the relative timing of pre and post synaptic spikes. This paper derives a temporally dependent learning rule from the basic principle of mutual information maximization and studies its relation to the experimentally observed plasticity. We find that a supervised spike-dependent learning rule sharing similar structure with the experimentally observed plasticity increases mutual information to a stable near optimal level. Moreover, the analysis reveals how the temporal structure of time-dependent learning rules is determined by the temporal filter applied by neurons over their inputs. These results suggest experimental prediction as to the dependency of the learning rule on neuronal biophysical parameters 1</p><p>6 0.16259634 <a title="146-tfidf-6" href="./nips-2000-Homeostasis_in_a_Silicon_Integrate_and_Fire_Neuron.html">67 nips-2000-Homeostasis in a Silicon Integrate and Fire Neuron</a></p>
<p>7 0.10646982 <a title="146-tfidf-7" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>8 0.094812512 <a title="146-tfidf-8" href="./nips-2000-Who_Does_What%3F_A_Novel_Algorithm_to_Determine_Function_Localization.html">147 nips-2000-Who Does What? A Novel Algorithm to Determine Function Localization</a></p>
<p>9 0.09017086 <a title="146-tfidf-9" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>10 0.088650376 <a title="146-tfidf-10" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>11 0.087197535 <a title="146-tfidf-11" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>12 0.078199729 <a title="146-tfidf-12" href="./nips-2000-Learning_Continuous_Distributions%3A_Simulations_With_Field_Theoretic_Priors.html">76 nips-2000-Learning Continuous Distributions: Simulations With Field Theoretic Priors</a></p>
<p>13 0.068999298 <a title="146-tfidf-13" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>14 0.068592466 <a title="146-tfidf-14" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>15 0.067229293 <a title="146-tfidf-15" href="./nips-2000-Sparse_Kernel_Principal_Component_Analysis.html">121 nips-2000-Sparse Kernel Principal Component Analysis</a></p>
<p>16 0.066693462 <a title="146-tfidf-16" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>17 0.065815724 <a title="146-tfidf-17" href="./nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">42 nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<p>18 0.063056313 <a title="146-tfidf-18" href="./nips-2000-Learning_Winner-take-all_Competition_Between_Groups_of_Neurons_in_Lateral_Inhibitory_Networks.html">81 nips-2000-Learning Winner-take-all Competition Between Groups of Neurons in Lateral Inhibitory Networks</a></p>
<p>19 0.062759101 <a title="146-tfidf-19" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>20 0.062439807 <a title="146-tfidf-20" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.254), (1, -0.285), (2, -0.372), (3, -0.016), (4, 0.072), (5, -0.012), (6, -0.123), (7, 0.352), (8, -0.1), (9, 0.024), (10, -0.027), (11, -0.048), (12, 0.052), (13, 0.166), (14, -0.1), (15, 0.065), (16, -0.033), (17, 0.018), (18, -0.17), (19, -0.031), (20, -0.07), (21, 0.029), (22, 0.008), (23, 0.063), (24, -0.048), (25, 0.028), (26, -0.021), (27, 0.048), (28, -0.103), (29, -0.009), (30, -0.018), (31, -0.077), (32, 0.038), (33, -0.082), (34, 0.045), (35, -0.045), (36, 0.024), (37, 0.064), (38, -0.028), (39, -0.091), (40, -0.007), (41, -0.014), (42, 0.039), (43, 0.001), (44, 0.059), (45, -0.033), (46, -0.015), (47, 0.084), (48, 0.01), (49, -0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96972269 <a title="146-lsi-1" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>Author: Blaise Agüera y Arcas, Adrienne L. Fairhall, William Bialek</p><p>Abstract: In this paper we formulate a description of the computation performed by a neuron as a combination of dimensional reduction and nonlinearity. We implement this description for the HodgkinHuxley model, identify the most relevant dimensions and find the nonlinearity. A two dimensional description already captures a significant fraction of the information that spikes carry about dynamic inputs. This description also shows that computation in the Hodgkin-Huxley model is more complex than a simple integrateand-fire or perceptron model. 1</p><p>2 0.84054708 <a title="146-lsi-2" href="./nips-2000-Universality_and_Individuality_in_a_Neural_Code.html">141 nips-2000-Universality and Individuality in a Neural Code</a></p>
<p>Author: Elad Schneidman, Naama Brenner, Naftali Tishby, Robert R. de Ruyter van Steveninck, William Bialek</p><p>Abstract: The problem of neural coding is to understand how sequences of action potentials (spikes) are related to sensory stimuli, motor outputs, or (ultimately) thoughts and intentions. One clear question is whether the same coding rules are used by different neurons, or by corresponding neurons in different individuals. We present a quantitative formulation of this problem using ideas from information theory, and apply this approach to the analysis of experiments in the fly visual system. We find significant individual differences in the structure of the code, particularly in the way that temporal patterns of spikes are used to convey information beyond that available from variations in spike rate. On the other hand, all the flies in our ensemble exhibit a high coding efficiency, so that every spike carries the same amount of information in all the individuals. Thus the neural code has a quantifiable mixture of individuality and universality. 1</p><p>3 0.79447526 <a title="146-lsi-3" href="./nips-2000-Finding_the_Key_to_a_Synapse.html">55 nips-2000-Finding the Key to a Synapse</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass</p><p>Abstract: Experimental data have shown that synapses are heterogeneous: different synapses respond with different sequences of amplitudes of postsynaptic responses to the same spike train. Neither the role of synaptic dynamics itself nor the role of the heterogeneity of synaptic dynamics for computations in neural circuits is well understood. We present in this article methods that make it feasible to compute for a given synapse with known synaptic parameters the spike train that is optimally fitted to the synapse, for example in the sense that it produces the largest sum of postsynaptic responses. To our surprise we find that most of these optimally fitted spike trains match common firing patterns of specific types of neurons that are discussed in the literature.</p><p>4 0.764355 <a title="146-lsi-4" href="./nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">88 nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>Author: Adrienne L. Fairhall, Geoffrey D. Lewen, William Bialek, Robert R. de Ruyter van Steveninck</p><p>Abstract: Many neural systems extend their dynamic range by adaptation. We examine the timescales of adaptation in the context of dynamically modulated rapidly-varying stimuli, and demonstrate in the fly visual system that adaptation to the statistical ensemble of the stimulus dynamically maximizes information transmission about the time-dependent stimulus. Further, while the rate response has long transients, the adaptation takes place on timescales consistent with optimal variance estimation.</p><p>5 0.57451761 <a title="146-lsi-5" href="./nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">129 nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<p>Author: Gal Chechik, Naftali Tishby</p><p>Abstract: The paradigm of Hebbian learning has recently received a novel interpretation with the discovery of synaptic plasticity that depends on the relative timing of pre and post synaptic spikes. This paper derives a temporally dependent learning rule from the basic principle of mutual information maximization and studies its relation to the experimentally observed plasticity. We find that a supervised spike-dependent learning rule sharing similar structure with the experimentally observed plasticity increases mutual information to a stable near optimal level. Moreover, the analysis reveals how the temporal structure of time-dependent learning rules is determined by the temporal filter applied by neurons over their inputs. These results suggest experimental prediction as to the dependency of the learning rule on neuronal biophysical parameters 1</p><p>6 0.41519961 <a title="146-lsi-6" href="./nips-2000-Homeostasis_in_a_Silicon_Integrate_and_Fire_Neuron.html">67 nips-2000-Homeostasis in a Silicon Integrate and Fire Neuron</a></p>
<p>7 0.30034062 <a title="146-lsi-7" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>8 0.2662898 <a title="146-lsi-8" href="./nips-2000-Who_Does_What%3F_A_Novel_Algorithm_to_Determine_Function_Localization.html">147 nips-2000-Who Does What? A Novel Algorithm to Determine Function Localization</a></p>
<p>9 0.25605425 <a title="146-lsi-9" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>10 0.23999463 <a title="146-lsi-10" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>11 0.2327223 <a title="146-lsi-11" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>12 0.22712155 <a title="146-lsi-12" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>13 0.21663469 <a title="146-lsi-13" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>14 0.21450923 <a title="146-lsi-14" href="./nips-2000-Stability_and_Noise_in_Biochemical_Switches.html">125 nips-2000-Stability and Noise in Biochemical Switches</a></p>
<p>15 0.20704025 <a title="146-lsi-15" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>16 0.2044231 <a title="146-lsi-16" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>17 0.20438957 <a title="146-lsi-17" href="./nips-2000-Learning_Continuous_Distributions%3A_Simulations_With_Field_Theoretic_Priors.html">76 nips-2000-Learning Continuous Distributions: Simulations With Field Theoretic Priors</a></p>
<p>18 0.203842 <a title="146-lsi-18" href="./nips-2000-Periodic_Component_Analysis%3A_An_Eigenvalue_Method_for_Representing_Periodic_Structure_in_Speech.html">99 nips-2000-Periodic Component Analysis: An Eigenvalue Method for Representing Periodic Structure in Speech</a></p>
<p>19 0.19306955 <a title="146-lsi-19" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>20 0.18820313 <a title="146-lsi-20" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.018), (17, 0.107), (32, 0.023), (33, 0.045), (42, 0.044), (55, 0.03), (60, 0.262), (62, 0.04), (65, 0.017), (67, 0.091), (75, 0.02), (76, 0.064), (79, 0.022), (81, 0.074), (90, 0.028), (93, 0.033), (97, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84648204 <a title="146-lda-1" href="./nips-2000-Ensemble_Learning_and_Linear_Response_Theory_for_ICA.html">46 nips-2000-Ensemble Learning and Linear Response Theory for ICA</a></p>
<p>Author: Pedro A. d. F. R. Højen-Sørensen, Ole Winther, Lars Kai Hansen</p><p>Abstract: We propose a general Bayesian framework for performing independent component analysis (leA) which relies on ensemble learning and linear response theory known from statistical physics. We apply it to both discrete and continuous sources. For the continuous source the underdetermined (overcomplete) case is studied. The naive mean-field approach fails in this case whereas linear response theory-which gives an improved estimate of covariances-is very efficient. The examples given are for sources without temporal correlations. However, this derivation can easily be extended to treat temporal correlations. Finally, the framework offers a simple way of generating new leA algorithms without needing to define the prior distribution of the sources explicitly.</p><p>same-paper 2 0.8061161 <a title="146-lda-2" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>Author: Blaise Agüera y Arcas, Adrienne L. Fairhall, William Bialek</p><p>Abstract: In this paper we formulate a description of the computation performed by a neuron as a combination of dimensional reduction and nonlinearity. We implement this description for the HodgkinHuxley model, identify the most relevant dimensions and find the nonlinearity. A two dimensional description already captures a significant fraction of the information that spikes carry about dynamic inputs. This description also shows that computation in the Hodgkin-Huxley model is more complex than a simple integrateand-fire or perceptron model. 1</p><p>3 0.55846477 <a title="146-lda-3" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass, Eduardo D. Sontag, Anthony M. Zador</p><p>Abstract: Experimental data show that biological synapses behave quite differently from the symbolic synapses in common artificial neural network models. Biological synapses are dynamic, i.e., their</p><p>4 0.51895463 <a title="146-lda-4" href="./nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">88 nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>Author: Adrienne L. Fairhall, Geoffrey D. Lewen, William Bialek, Robert R. de Ruyter van Steveninck</p><p>Abstract: Many neural systems extend their dynamic range by adaptation. We examine the timescales of adaptation in the context of dynamically modulated rapidly-varying stimuli, and demonstrate in the fly visual system that adaptation to the statistical ensemble of the stimulus dynamically maximizes information transmission about the time-dependent stimulus. Further, while the rate response has long transients, the adaptation takes place on timescales consistent with optimal variance estimation.</p><p>5 0.51520282 <a title="146-lda-5" href="./nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">129 nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<p>Author: Gal Chechik, Naftali Tishby</p><p>Abstract: The paradigm of Hebbian learning has recently received a novel interpretation with the discovery of synaptic plasticity that depends on the relative timing of pre and post synaptic spikes. This paper derives a temporally dependent learning rule from the basic principle of mutual information maximization and studies its relation to the experimentally observed plasticity. We find that a supervised spike-dependent learning rule sharing similar structure with the experimentally observed plasticity increases mutual information to a stable near optimal level. Moreover, the analysis reveals how the temporal structure of time-dependent learning rules is determined by the temporal filter applied by neurons over their inputs. These results suggest experimental prediction as to the dependency of the learning rule on neuronal biophysical parameters 1</p><p>6 0.51224297 <a title="146-lda-6" href="./nips-2000-The_Kernel_Trick_for_Distances.html">134 nips-2000-The Kernel Trick for Distances</a></p>
<p>7 0.51012117 <a title="146-lda-7" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>8 0.50946009 <a title="146-lda-8" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>9 0.50687236 <a title="146-lda-9" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>10 0.50677365 <a title="146-lda-10" href="./nips-2000-Finding_the_Key_to_a_Synapse.html">55 nips-2000-Finding the Key to a Synapse</a></p>
<p>11 0.50371414 <a title="146-lda-11" href="./nips-2000-Periodic_Component_Analysis%3A_An_Eigenvalue_Method_for_Representing_Periodic_Structure_in_Speech.html">99 nips-2000-Periodic Component Analysis: An Eigenvalue Method for Representing Periodic Structure in Speech</a></p>
<p>12 0.50369048 <a title="146-lda-12" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>13 0.50314599 <a title="146-lda-13" href="./nips-2000-Stability_and_Noise_in_Biochemical_Switches.html">125 nips-2000-Stability and Noise in Biochemical Switches</a></p>
<p>14 0.49886394 <a title="146-lda-14" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>15 0.49839717 <a title="146-lda-15" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>16 0.49758792 <a title="146-lda-16" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>17 0.49669588 <a title="146-lda-17" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>18 0.48985341 <a title="146-lda-18" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>19 0.48901477 <a title="146-lda-19" href="./nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">95 nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>20 0.48857147 <a title="146-lda-20" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
