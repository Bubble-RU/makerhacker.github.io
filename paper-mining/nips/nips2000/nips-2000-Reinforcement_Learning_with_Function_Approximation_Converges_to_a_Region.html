<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-112" href="#">nips2000-112</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</h1>
<br/><p>Source: <a title="nips-2000-112-pdf" href="http://papers.nips.cc/paper/1911-reinforcement-learning-with-function-approximation-converges-to-a-region.pdf">pdf</a></p><p>Author: Geoffrey J. Gordon</p><p>Abstract: Many algorithms for approximate reinforcement learning are not known to converge. In fact, there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point. This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region. The algorithms are SARSA(O) and V(O); the latter algorithm was used in the well-known TD-Gammon program. 1</p><p>Reference: <a title="nips-2000-112-reference" href="../nips2000_reference/nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sars', 0.615), ('policy', 0.395), ('wt', 0.324), ('lemm', 0.18), ('reward', 0.158), ('traject', 0.158), ('visit', 0.133), ('upd', 0.124), ('stlwt', 0.123), ('converg', 0.101), ('sphere', 0.097), ('episod', 0.096), ('lipschitz', 0.096), ('td', 0.095), ('transit', 0.081), ('enough', 0.079), ('contract', 0.077), ('awt', 0.074), ('backgammon', 0.074), ('writ', 0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="112-tfidf-1" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>2 0.28216136 <a title="112-tfidf-2" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>3 0.24596246 <a title="112-tfidf-3" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>4 0.1698606 <a title="112-tfidf-4" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>5 0.1681205 <a title="112-tfidf-5" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>6 0.15288216 <a title="112-tfidf-6" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>7 0.12436737 <a title="112-tfidf-7" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>8 0.12349407 <a title="112-tfidf-8" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>9 0.12312873 <a title="112-tfidf-9" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>10 0.11362 <a title="112-tfidf-10" href="./nips-2000-A_Neural_Probabilistic_Language_Model.html">6 nips-2000-A Neural Probabilistic Language Model</a></p>
<p>11 0.098764889 <a title="112-tfidf-11" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>12 0.097665116 <a title="112-tfidf-12" href="./nips-2000-Programmable_Reinforcement_Learning_Agents.html">105 nips-2000-Programmable Reinforcement Learning Agents</a></p>
<p>13 0.096603274 <a title="112-tfidf-13" href="./nips-2000-Robust_Reinforcement_Learning.html">113 nips-2000-Robust Reinforcement Learning</a></p>
<p>14 0.081841953 <a title="112-tfidf-14" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>15 0.080777161 <a title="112-tfidf-15" href="./nips-2000-Hierarchical_Memory-Based_Reinforcement_Learning.html">63 nips-2000-Hierarchical Memory-Based Reinforcement Learning</a></p>
<p>16 0.075468279 <a title="112-tfidf-16" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>17 0.073469654 <a title="112-tfidf-17" href="./nips-2000-Algorithms_for_Non-negative_Matrix_Factorization.html">22 nips-2000-Algorithms for Non-negative Matrix Factorization</a></p>
<p>18 0.067228585 <a title="112-tfidf-18" href="./nips-2000-From_Margin_to_Sparsity.html">58 nips-2000-From Margin to Sparsity</a></p>
<p>19 0.063179404 <a title="112-tfidf-19" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>20 0.06125997 <a title="112-tfidf-20" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.216), (1, 0.017), (2, -0.099), (3, 0.384), (4, -0.227), (5, 0.085), (6, 0.024), (7, 0.073), (8, -0.044), (9, -0.101), (10, -0.086), (11, 0.007), (12, -0.021), (13, 0.007), (14, -0.029), (15, 0.058), (16, -0.046), (17, -0.135), (18, 0.039), (19, 0.09), (20, 0.032), (21, 0.067), (22, 0.088), (23, 0.109), (24, 0.05), (25, 0.072), (26, 0.04), (27, -0.042), (28, 0.033), (29, -0.024), (30, 0.01), (31, 0.064), (32, -0.03), (33, 0.075), (34, 0.003), (35, -0.017), (36, -0.019), (37, 0.062), (38, -0.095), (39, 0.016), (40, 0.107), (41, -0.129), (42, -0.074), (43, 0.007), (44, -0.155), (45, 0.019), (46, -0.039), (47, -0.055), (48, 0.105), (49, -0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93802959 <a title="112-lsi-1" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>2 0.65694422 <a title="112-lsi-2" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>3 0.64277154 <a title="112-lsi-3" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>4 0.59901762 <a title="112-lsi-4" href="./nips-2000-Programmable_Reinforcement_Learning_Agents.html">105 nips-2000-Programmable Reinforcement Learning Agents</a></p>
<p>5 0.5720436 <a title="112-lsi-5" href="./nips-2000-Robust_Reinforcement_Learning.html">113 nips-2000-Robust Reinforcement Learning</a></p>
<p>6 0.55155849 <a title="112-lsi-6" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>7 0.51179588 <a title="112-lsi-7" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>8 0.4943988 <a title="112-lsi-8" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>9 0.43966421 <a title="112-lsi-9" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>10 0.40153152 <a title="112-lsi-10" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>11 0.38943115 <a title="112-lsi-11" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>12 0.38040176 <a title="112-lsi-12" href="./nips-2000-Algorithms_for_Non-negative_Matrix_Factorization.html">22 nips-2000-Algorithms for Non-negative Matrix Factorization</a></p>
<p>13 0.36457071 <a title="112-lsi-13" href="./nips-2000-A_Neural_Probabilistic_Language_Model.html">6 nips-2000-A Neural Probabilistic Language Model</a></p>
<p>14 0.35420626 <a title="112-lsi-14" href="./nips-2000-Hierarchical_Memory-Based_Reinforcement_Learning.html">63 nips-2000-Hierarchical Memory-Based Reinforcement Learning</a></p>
<p>15 0.31941736 <a title="112-lsi-15" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>16 0.3155129 <a title="112-lsi-16" href="./nips-2000-The_Early_Word_Catches_the_Weights.html">131 nips-2000-The Early Word Catches the Weights</a></p>
<p>17 0.27321029 <a title="112-lsi-17" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>18 0.26648843 <a title="112-lsi-18" href="./nips-2000-Sequentially_Fitting_%60%60Inclusive%27%27_Trees_for_Inference_in_Noisy-OR_Networks.html">115 nips-2000-Sequentially Fitting ``Inclusive'' Trees for Inference in Noisy-OR Networks</a></p>
<p>19 0.26070541 <a title="112-lsi-19" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>20 0.25034595 <a title="112-lsi-20" href="./nips-2000-Some_New_Bounds_on_the_Generalization_Error_of_Combined_Classifiers.html">119 nips-2000-Some New Bounds on the Generalization Error of Combined Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.024), (9, 0.15), (11, 0.027), (16, 0.069), (21, 0.064), (26, 0.016), (35, 0.224), (38, 0.073), (54, 0.105), (74, 0.02), (76, 0.022), (80, 0.011), (84, 0.083), (85, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76488519 <a title="112-lda-1" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>2 0.68424714 <a title="112-lda-2" href="./nips-2000-Hierarchical_Memory-Based_Reinforcement_Learning.html">63 nips-2000-Hierarchical Memory-Based Reinforcement Learning</a></p>
<p>3 0.66522223 <a title="112-lda-3" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>4 0.66203547 <a title="112-lda-4" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>5 0.65977466 <a title="112-lda-5" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>6 0.65878642 <a title="112-lda-6" href="./nips-2000-Bayes_Networks_on_Ice%3A_Robotic_Search_for_Antarctic_Meteorites.html">29 nips-2000-Bayes Networks on Ice: Robotic Search for Antarctic Meteorites</a></p>
<p>7 0.65842009 <a title="112-lda-7" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>8 0.65816772 <a title="112-lda-8" href="./nips-2000-The_Use_of_MDL_to_Select_among_Computational_Models_of_Cognition.html">139 nips-2000-The Use of MDL to Select among Computational Models of Cognition</a></p>
<p>9 0.65754545 <a title="112-lda-9" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>10 0.6554547 <a title="112-lda-10" href="./nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">129 nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<p>11 0.65540528 <a title="112-lda-11" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>12 0.65506423 <a title="112-lda-12" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>13 0.65445316 <a title="112-lda-13" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>14 0.65285385 <a title="112-lda-14" href="./nips-2000-Learning_Switching_Linear_Models_of_Human_Motion.html">80 nips-2000-Learning Switching Linear Models of Human Motion</a></p>
<p>15 0.65221518 <a title="112-lda-15" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>16 0.65134376 <a title="112-lda-16" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>17 0.65026546 <a title="112-lda-17" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>18 0.65003228 <a title="112-lda-18" href="./nips-2000-Universality_and_Individuality_in_a_Neural_Code.html">141 nips-2000-Universality and Individuality in a Neural Code</a></p>
<p>19 0.64927024 <a title="112-lda-19" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>20 0.64882749 <a title="112-lda-20" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
