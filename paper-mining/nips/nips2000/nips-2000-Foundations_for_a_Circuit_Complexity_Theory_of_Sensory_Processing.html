<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 nips-2000-Foundations for a Circuit Complexity Theory of Sensory Processing</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-56" href="#">nips2000-56</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>56 nips-2000-Foundations for a Circuit Complexity Theory of Sensory Processing</h1>
<br/><p>Source: <a title="nips-2000-56-pdf" href="http://papers.nips.cc/paper/1910-foundations-for-a-circuit-complexity-theory-of-sensory-processing.pdf">pdf</a></p><p>Author: Robert A. Legenstein, Wolfgang Maass</p><p>Abstract: We introduce total wire length as salient complexity measure for an analysis of the circuit complexity of sensory processing in biological neural systems and neuromorphic engineering. This new complexity measure is applied to a set of basic computational problems that apparently need to be solved by circuits for translation- and scale-invariant sensory processing. We exhibit new circuit design strategies for these new benchmark functions that can be implemented within realistic complexity bounds, in particular with linear or almost linear total wire length.</p><p>Reference: <a title="nips-2000-56-reference" href="../nips2000_reference/nips-2000-Foundations_for_a_Circuit_Complexity_Theory_of_Sensory_Processing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 at  Abstract We introduce total wire length as salient complexity measure for an analysis of the circuit complexity of sensory processing in biological neural systems and neuromorphic engineering. [sent-5, score-1.742]
</p><p>2 This new complexity measure is applied to a set of basic computational problems that apparently need to be solved by circuits for translation- and scale-invariant sensory processing. [sent-6, score-0.571]
</p><p>3 We exhibit new circuit design strategies for these new benchmark functions that can be implemented within realistic complexity bounds, in particular with linear or almost linear total wire length. [sent-7, score-1.338]
</p><p>4 1 Introduction Circuit complexity theory is a classical area of theoretical computer science, that provides estimates for the complexity of circuits for computing specific benchmark functions, such as binary addition, multiplication and sorting (see, e. [sent-8, score-0.631]
</p><p>5 In recent years interest has grown in understanding the complexity of circuits for early sensory processing, both from the biological point of view and from the point of view of neuromorphic engineering (see (Mead, 1989Â». [sent-11, score-0.656]
</p><p>6 We will follow traditional circuit complexity theory in assuming that the underlying graph of each circuit is a directed graph without cycles. [sent-14, score-0.773]
</p><p>7 I Neural circuits in "wetware" as well as most circuits in analog VLSI contain in addition to feedforward connections also lateral and recurrent connections. [sent-16, score-0.678]
</p><p>8 The standard mathematical approach is to model such circuits by larger feedforward circuits, where new "virtual gates" are introduced to represent the state of existing gates at later points in time. [sent-18, score-0.548]
</p><p>9 The latter is defined as the length of the longest directed path in the underlying graph, and is also interpreted as the computation time of the circuit. [sent-20, score-0.134]
</p><p>10 The focus lies in general on the classification of functions that can be computed by circuits whose number of gates can be bounded by a polynomial in the number n of input variables. [sent-21, score-0.626]
</p><p>11 We proceed on the assumption that the area (or volume in the case of neural circuits) occupied by wires is a severe bottleneck for physical implementations of circuits for sensory processing. [sent-23, score-0.761]
</p><p>12 Therefore we wiJI not just count wires, but consider a complexity measure that provides an estimate for the total area or volume occupied by wires. [sent-24, score-0.342]
</p><p>13 In the cortex, neurons occupy an about 2 mm thick 3-dimensional sheet of "grey matter". [sent-25, score-0.238]
</p><p>14 n mm for the wire length of the "average" cortical circuit involving n neurons. [sent-28, score-1.283]
</p><p>15 In order to arrive at a concise mathematical model we project each 3D cortical circuit into 2D, and assume for simplicity that its n gates (neurons) occupy the nodes of a grid. [sent-29, score-0.676]
</p><p>16 Then for a circuit with n gates, the total length of the horizontal components of all wires is on average ~ 80 . [sent-30, score-0.806]
</p><p>17 Here, one grid unit is the distance between adjacent nodes on the grid, which amounts to 1O- 5 / 2 mm for an assumed density of 10 5 neurons per mm 2 of cortical surface. [sent-34, score-0.662]
</p><p>18 More abstractly, we define the following model: Gates, input- and output-ports of a circuit are placed on different nodes of a 2-dimensional grid (with unit distance 1 between adjacent nodes). [sent-37, score-0.578]
</p><p>19 These nodes can be connected by (unidirectional) wires that run through the plane in any way that the designer wants, in particular wires may cross and need not run rectilinearly (wires are thought of as running in the 3 dimensional . [sent-38, score-0.5]
</p><p>20 Ipace above the plane, without charge for vertical wire segmentsp. [sent-39, score-0.61]
</p><p>21 We refer to the minimal value of the sum of all wire lengths that can be achieved by any such arrangement as the total wire length of the circuit. [sent-40, score-1.525]
</p><p>22 The attractiveness of this model lies in its mathematical simplicity, and in its generality. [sent-41, score-0.053]
</p><p>23 There exist quite reliable estimates for the order of magnitudes for the number n of inputs, the number of neurons and the total wire length of biological neural circuits for sensory processing, see (Abeles, 1998; Koch, 1999; Shepherd, 1998; Braitenberg and Schiiz, 1998). [sent-43, score-1.478]
</p><p>24 3 2We will allow that a wire from a gate may branch and provide input to several other gates. [sent-44, score-0.661]
</p><p>25 For reasonable bounds on the maximal fan-out (10 4 in the case of neural circuits) this is realistic both for neural circuits and for VLSI. [sent-45, score-0.421]
</p><p>26 Since most asymptotic bounds in circuit complexity theory have constant factors in front that are much larger than 1, one really has to focus on circuit architectures with clearly subquadratic bounds for the total wire length. [sent-47, score-1.677]
</p><p>27 The complexity bounds for circuits that can realistically be implemented in VLSI are typically even more severe than for "wetware", and linear or almost linear bounds for the total wire length are desirable for that purpose. [sent-48, score-1.451]
</p><p>28 In this article we begin the investigation of algorithms for basic pattern recognition tasks that can be implemented within this low-level complexity regime. [sent-49, score-0.184]
</p><p>29 2 Global Pattern Detection in 2-Dimensional Maps For many important sensory processing tasks - such as for visual or somatosensory input - the input variables are arranged in a 2-dimensional map whose structure reflects spatial relationship in the outside world. [sent-52, score-0.373]
</p><p>30 We assume that local feature detectors are able to detect the presence of salient local features in their specific "receptive field", such as for example a center which emits is estimated to be around 10 6 (all estimates given are for primates, and they only reflect the order of magnitude). [sent-53, score-0.216]
</p><p>31 The total number of neurons that transmit sensory (mostly somatosensory) information to the cortex is estimated to be around 10 8 . [sent-54, score-0.424]
</p><p>32 In the subsequent sections we assume that these inputs represent the outputs of various local feature detectors for n locations in some 2-dimensional map. [sent-55, score-0.132]
</p><p>33 Thus, if one assumes for example that on average there are 10 different feature detectors for each location on this map, one arrives at biologically realistic estimates for n that lie between 10 5 and  10 7 . [sent-56, score-0.223]
</p><p>34 The total number of neurons in the primary visual cortex of primates is estimated to be around 10 9 , occupying an area of roughly 10 4 mm2 of cortical surface. [sent-57, score-0.514]
</p><p>35 There are up to 10 5 neurons under one mm2 of cortical surface, which yields a value of 10- 5 / 2 mm for the distance between adjacent grid points in our model. [sent-58, score-0.483]
</p><p>36 The total length of axonal and dendritic branches below one mm 2 of cortical surface is estimated to be between 1 and 10 km, yielding up to lOll mm total wire length for primary visual cortex. [sent-59, score-1.621]
</p><p>37 Thus if one assumes that 100 separate circuits are implemented in primary visual cortex, each of them can use 10 7 neurons and a total wire length of 10 9 mm. [sent-60, score-1.356]
</p><p>38 Hence realistic bounds for the complexity of a single one of these circuits for visual pattern recognition are 107 = n 7 / 5 neurons (for n = 105 ), and a total wire length of at most 10 1 1. [sent-61, score-1.563]
</p><p>39 The whole cortex receives sensory input from about 108 neurons. [sent-64, score-0.248]
</p><p>40 It processes this input with about 10 10 neurons and less than 10 12 mm total wire length. [sent-65, score-1.007]
</p><p>41 The actual resources available for sensory processing are likely to be substantially smaller, since most cortical neurons and circuits are believed to have many other functions besides online sensory processing. [sent-68, score-0.809]
</p><p>42 higher (or lower) intensity than its immediate surrounding, or a high-intensity line segment in a certain direction, the end of a line, a junction of line segments, or even more complex local visual patterns like an eye or a nose. [sent-69, score-0.057]
</p><p>43 The ultimate computational goal is to detect specific global spatial arrangements of such local patterns, such as the letter "T", or in the end also a human face, in a translation- and scale-invariant manner. [sent-70, score-0.142]
</p><p>44 We formalize 2-dimensional global pattern detection problems by assuming that the input consists of arrays g = (al, . [sent-71, score-0.161]
</p><p>45 of binary variables that are arranged on a 2-dimensional square grid4 â¢ Each index i can be thought of as representing a location within some y'ri x y'ri-square in the outside world. [sent-78, score-0.072]
</p><p>46 We assume that ai = 1 if and only if feature a is detected at location i and that bi = 1 if and only if feature b is detected at location i. [sent-79, score-0.25]
</p><p>47 In our formal model we can reserve a subsquare within the 2-dimensional grid for each index i, where the input variables ai, bi , etc. [sent-80, score-0.134]
</p><p>48 1 The fun ction PI) can be computed - and witnesses i and j with ai = bj = 1 can be exhibited if they exist - by a circuit with total wire length O(n), consisting ofO(n) Boolean gates offan-in 2 (andfan-out 2) in depth o (log n . [sent-83, score-1.612]
</p><p>49 The depth of the circuit can be reduced to o (log n) if one employs threshold gates 6 with fan-in logn. [sent-85, score-0.59]
</p><p>50 This can also be done with total wire length O(n). [sent-86, score-0.881]
</p><p>51 Proof (sketch) At first sight it seems that PI) needs complete connectivity on the plane because of its global character. [sent-87, score-0.118]
</p><p>52 However, we show that there exists a divide and conquer approach with rather small communication cost. [sent-88, score-0.057]
</p><p>53 Divide the input plane into four sub-squares C l , . [sent-89, score-0.111]
</p><p>54 , ~4 for the restrictions of the input to these four sub-areas and assume that the following values have already been computed for each sub-square C i :  gl, . [sent-96, score-0.077]
</p><p>55 The arrangement of the input variables an the grid will in general leave many nodes empty, which can be occupied by gates of the circuit. [sent-102, score-0.47]
</p><p>56 Then "input location) is above and to the right of input location i" means: il < 1t and i2 < )2. [sent-104, score-0.123]
</p><p>57 The circuit complexity of variations of the function PE where one or both of the "<" are replaced by "~" is the same. [sent-105, score-0.441]
</p><p>58 _ _ _ _ _ _ _ I  c)  Figure 1: The 2-dimensional input plane. [sent-125, score-0.051]
</p><p>59 Occurrences of features in Q are indicated by light squares, and occurrences of features in fl. [sent-126, score-0.04]
</p><p>60 The lightly striped areas represent busses of wires that run along the edges of the H-Tree. [sent-134, score-0.203]
</p><p>61 To construct H 2 , replace the leaves of HI by H-trees HI (b). [sent-136, score-0.037]
</p><p>62 To construct H k , replace the leaves of HI by H-trees H k - I (c). [sent-137, score-0.037]
</p><p>63 ) can be sketched as follows: First, check whether p{;/4(Qi ,fl. [sent-140, score-0.04]
</p><p>64 Then, check the spatial relationships between feature occurrences in adjacent sub-squares. [sent-145, score-0.299]
</p><p>65 When checking spatial relationships between features from two horizontally adjacent sub-squares, only the lowest and the highest feature occurrence is crucial for the value of P]5 (see Figure Ib). [sent-146, score-0.37]
</p><p>66 When checking spatial relationships of features from two vertically adjacent sub-squares, only the leftmost and the rightmost feature occurrence is crucial for the value of P]5 (see Figure lc). [sent-148, score-0.396]
</p><p>67 When checking spatial relationships of features from the lower left and the upper right sub-squares, it suffices to check whether there is an a-feature occurrence in the lower left and a b-feature occurrence in the upper right sub-square. [sent-150, score-0.318]
</p><p>68 In the remaining part of the proof sketch, we present an efficient layout for a circuit that implements this recursive algorithm. [sent-152, score-0.42]
</p><p>69 We need a layout strategy that is compatible with the recursive two-dimensional division of the input plane. [sent-153, score-0.139]
</p><p>70 We adopt for this purpose a well known design strategy: the H-tree (see (Mead and Rem, 1979Â». [sent-154, score-0.064]
</p><p>71 To construct an H -Tree H k, build an H -tree HI and replace its four leaves by H-trees H k - I (see Figure 2b,c). [sent-158, score-0.063]
</p><p>72 The inner nodes of the tree are replaced by sub-circuits that implement the merging algorithm. [sent-160, score-0.121]
</p><p>73 Furthermore, each edge of the H-tree is replaced by a "bus" consisting of O(log m) wires if it originates in an area with m inputs. [sent-161, score-0.285]
</p><p>74 It is not difficult to show that â¢ this layout uses only linear total wire length. [sent-162, score-0.835]
</p><p>75 The linear total wire length of this circuit is up to a constant factor optimal for any circuit whose output depends on all of its n inputs. [sent-163, score-1.545]
</p><p>76 Note that most connections in this circuit are local, just like in a biological neural circuit. [sent-164, score-0.377]
</p><p>77 Thus, we see that minimizing total wire length tends to generate biology-like circuit structures. [sent-165, score-1.213]
</p><p>78 by a circuit with smaller depth) if one can afford a somewhat larger total wire length. [sent-168, score-1.079]
</p><p>79 This property allows us to "chain" the global pattern detection problem formalized through the function PI), and to decide within the same complexity bound whether for any fixed number k of input vectors g,(l), . [sent-170, score-0.304]
</p><p>80 ,k and location i(m+1) lies to the right and above location i(m) for m = 1, . [sent-179, score-0.197]
</p><p>81 In fact, one can also compute a k-tuple of witnesses i(l), . [sent-183, score-0.051]
</p><p>82 ,i(k) within the same complexity bounds, provided it exists. [sent-186, score-0.109]
</p><p>83 This circuit design is based on an efficient layout for prefix computations. [sent-187, score-0.458]
</p><p>84 ,Vn} one can compute the function PI) in depth O(:~:~) by a feed-forward circuit consisting ofO(n) AND/OR gates offan-in ~ ~, with total wire length O(n . [sent-192, score-1.503]
</p><p>85 â¢  Another essential ingredient of translation- and scale-invariant global pattern recognition is the capability to detect whether a local feature c occurs in the middle between locations i and j where the local features a and b occur. [sent-195, score-0.228]
</p><p>86 This global pattern detection problem is formalized through the following function PF : {a, 1 -t {a, 1}:  pn  If LA = LQ = 1 thenPF(g"Q,~) = 1, if and only if there existi,j,k so that input location k lies on the middle of the line between locations i and j, and ai = bj = Ck = 1. [sent-196, score-0.378]
</p><p>87 This function PF can be computed very fast by circuits with the least possible total wire length (up to a constant factor), using threshold gates of fan-in up to Vn:  Theorem 2. [sent-197, score-1.403]
</p><p>88 3 The function PF can be computed - and witnesses can be exhibited - by a circuit with total wire length and area O(n), consisting ofO(n) Boolean gates offan-in 2 and 0 (. [sent-198, score-1.542]
</p><p>89 The design of the circuit exploits that the computation of PF can be reduced to the solution of two closely related 1-dimensional problems. [sent-201, score-0.37]
</p><p>90 â¢  3 Discussion There exists a very large literature on neural circuits for translation-invariant pattern recognition see http://www. [sent-202, score-0.384]
</p><p>91 The goal of this article is to show that circuit complexity theory may become a useful ingredient for understanding the computational strategies of biological neural circuits, and for extracting from them portable principles that can be applied to novel artificial circuits 7 . [sent-217, score-0.922]
</p><p>92 For that purpose we have introduced the total wire length as an abstract complexity measure that appears to be among the most salient ones in this context, and which can in principle be applied both to neural circuits in the cortex and to artificial circuitry. [sent-218, score-1.46]
</p><p>93 8 The relevance of the total wire length of cortical circuits has been emphasized by numerous neuroscientists, from Cajal (see for example p. [sent-220, score-1.295]
</p><p>94 On the other hand the total wire length of a circuit layout is also closely related to the area required by a VLSI implementation of such a circuit (see (Savage, 1998)). [sent-222, score-1.683]
</p><p>95 We have formalized some basic computational problems, that appear to underly various translation- and scale-invariant sensory processing tasks, as a first set of benchmark functions for a circuit complexity theory of sensory processing. [sent-223, score-0.817]
</p><p>96 We have presented designs for circuits that compute these benchmark functions with small - in most cases linear or almost linear - total wire length (and constant factors of moderate size). [sent-224, score-1.244]
</p><p>97 The computational strategies of these circuits differ strongly from those that have been considered in previous approaches, which failed to take the limitations imposed by the realistically available amount of total wire length into account. [sent-225, score-1.269]
</p><p>98 7We do not want to argue that learning plays no role in the design and optimization of circuits for specific sensory processing tasks; on the contrary. [sent-282, score-0.533]
</p><p>99 html agreed is that translation- and scaleinvariant pattern recognition is a task which is so demanding, that learning algorithms have to be supported by pre-existing circuit structures. [sent-288, score-0.364]
</p><p>100 80f course there are other important complexity measures for circuits - such as energy consumption - besides those that have been addressed in this article. [sent-289, score-0.435]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wire', 0.61), ('circuit', 0.332), ('circuits', 0.326), ('wires', 0.203), ('gates', 0.196), ('total', 0.137), ('sensory', 0.136), ('length', 0.134), ('mm', 0.119), ('complexity', 0.109), ('adjacent', 0.103), ('neurons', 0.09), ('cortical', 0.088), ('layout', 0.088), ('grid', 0.083), ('mead', 0.079), ('occurrence', 0.076), ('location', 0.072), ('depth', 0.062), ('merging', 0.061), ('cortex', 0.061), ('nodes', 0.06), ('salient', 0.057), ('vlsi', 0.054), ('bounds', 0.053), ('lies', 0.053), ('input', 0.051), ('cajal', 0.051), ('savage', 0.051), ('subquadratic', 0.051), ('witness', 0.051), ('witnesses', 0.051), ('area', 0.05), ('checking', 0.049), ('spatial', 0.046), ('occupied', 0.046), ('hi', 0.045), ('biological', 0.045), ('ofo', 0.044), ('article', 0.043), ('global', 0.043), ('realistic', 0.042), ('connectivity', 0.041), ('pi', 0.04), ('neuromorphic', 0.04), ('occurrences', 0.04), ('detectors', 0.04), ('check', 0.04), ('feature', 0.039), ('pf', 0.038), ('design', 0.038), ('leaves', 0.037), ('benchmark', 0.037), ('detection', 0.035), ('arrangement', 0.034), ('formalized', 0.034), ('plane', 0.034), ('abeles', 0.034), ('braitenberg', 0.034), ('chklovskii', 0.034), ('ingredient', 0.034), ('wetware', 0.034), ('processing', 0.033), ('strategies', 0.033), ('boolean', 0.033), ('pattern', 0.032), ('consisting', 0.032), ('relationships', 0.031), ('divide', 0.031), ('visual', 0.03), ('biologically', 0.03), ('bj', 0.03), ('koch', 0.03), ('surface', 0.029), ('primary', 0.029), ('sheet', 0.029), ('primates', 0.029), ('axonal', 0.029), ('realistically', 0.029), ('shepherd', 0.029), ('stevens', 0.029), ('ai', 0.028), ('local', 0.027), ('exists', 0.026), ('feedforward', 0.026), ('matter', 0.026), ('vn', 0.026), ('horizontally', 0.026), ('leftmost', 0.026), ('branches', 0.026), ('rightmost', 0.026), ('rem', 0.026), ('austria', 0.026), ('maass', 0.026), ('somatosensory', 0.026), ('four', 0.026), ('inputs', 0.026), ('purpose', 0.026), ('detect', 0.026), ('oxford', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="56-tfidf-1" href="./nips-2000-Foundations_for_a_Circuit_Complexity_Theory_of_Sensory_Processing.html">56 nips-2000-Foundations for a Circuit Complexity Theory of Sensory Processing</a></p>
<p>Author: Robert A. Legenstein, Wolfgang Maass</p><p>Abstract: We introduce total wire length as salient complexity measure for an analysis of the circuit complexity of sensory processing in biological neural systems and neuromorphic engineering. This new complexity measure is applied to a set of basic computational problems that apparently need to be solved by circuits for translation- and scale-invariant sensory processing. We exhibit new circuit design strategies for these new benchmark functions that can be implemented within realistic complexity bounds, in particular with linear or almost linear total wire length.</p><p>2 0.17058668 <a title="56-tfidf-2" href="./nips-2000-A_Silicon_Primitive_for_Competitive_Learning.html">11 nips-2000-A Silicon Primitive for Competitive Learning</a></p>
<p>Author: David Hsu, Miguel Figueroa, Chris Diorio</p><p>Abstract: Competitive learning is a technique for training classification and clustering networks. We have designed and fabricated an 11transistor primitive, that we term an automaximizing bump circuit, that implements competitive learning dynamics. The circuit performs a similarity computation, affords nonvolatile storage, and implements simultaneous local adaptation and computation. We show that our primitive is suitable for implementing competitive learning in VLSI, and demonstrate its effectiveness in a standard clustering task.</p><p>3 0.15182681 <a title="56-tfidf-3" href="./nips-2000-Smart_Vision_Chip_Fabricated_Using_Three_Dimensional_Integration_Technology.html">118 nips-2000-Smart Vision Chip Fabricated Using Three Dimensional Integration Technology</a></p>
<p>Author: Hiroyuki Kurino, M. Nakagawa, Kang Wook Lee, Tomonori Nakamura, Yuusuke Yamada, Ki Tae Park, Mitsumasa Koyanagi</p><p>Abstract: The smart VISIOn chip has a large potential for application in general purpose high speed image processing systems . In order to fabricate smart vision chips including photo detector compactly, we have proposed the application of three dimensional LSI technology for smart vision chips. Three dimensional technology has great potential to realize new neuromorphic systems inspired by not only the biological function but also the biological structure. In this paper, we describe our three dimensional LSI technology for neuromorphic circuits and the design of smart vision chips .</p><p>4 0.070109874 <a title="56-tfidf-4" href="./nips-2000-Homeostasis_in_a_Silicon_Integrate_and_Fire_Neuron.html">67 nips-2000-Homeostasis in a Silicon Integrate and Fire Neuron</a></p>
<p>Author: Shih-Chii Liu, Bradley A. Minch</p><p>Abstract: In this work, we explore homeostasis in a silicon integrate-and-fire neuron. The neuron adapts its firing rate over long time periods on the order of seconds or minutes so that it returns to its spontaneous firing rate after a lasting perturbation. Homeostasis is implemented via two schemes. One scheme looks at the presynaptic activity and adapts the synaptic weight depending on the presynaptic spiking rate. The second scheme adapts the synaptic</p><p>5 0.060270529 <a title="56-tfidf-5" href="./nips-2000-Four-legged_Walking_Gait_Control_Using_a_Neuromorphic_Chip_Interfaced_to_a_Support_Vector_Learning_Algorithm.html">57 nips-2000-Four-legged Walking Gait Control Using a Neuromorphic Chip Interfaced to a Support Vector Learning Algorithm</a></p>
<p>Author: Susanne Still, Bernhard SchÃ¶lkopf, Klaus Hepp, Rodney J. Douglas</p><p>Abstract: To control the walking gaits of a four-legged robot we present a novel neuromorphic VLSI chip that coordinates the relative phasing of the robot's legs similar to how spinal Central Pattern Generators are believed to control vertebrate locomotion [3]. The chip controls the leg movements by driving motors with time varying voltages which are the outputs of a small network of coupled oscillators. The characteristics of the chip's output voltages depend on a set of input parameters. The relationship between input parameters and output voltages can be computed analytically for an idealized system. In practice, however, this ideal relationship is only approximately true due to transistor mismatch and offsets. Fine tuning of the chip's input parameters is done automatically by the robotic system, using an unsupervised Support Vector (SV) learning algorithm introduced recently [7]. The learning requires only that the description of the desired output is given. The machine learns from (unlabeled) examples how to set the parameters to the chip in order to obtain a desired motor behavior.</p><p>6 0.058612484 <a title="56-tfidf-6" href="./nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">42 nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<p>7 0.058169041 <a title="56-tfidf-7" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>8 0.057066109 <a title="56-tfidf-8" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>9 0.056513418 <a title="56-tfidf-9" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>10 0.0562963 <a title="56-tfidf-10" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>11 0.055284142 <a title="56-tfidf-11" href="./nips-2000-Who_Does_What%3F_A_Novel_Algorithm_to_Determine_Function_Localization.html">147 nips-2000-Who Does What? A Novel Algorithm to Determine Function Localization</a></p>
<p>12 0.053980492 <a title="56-tfidf-12" href="./nips-2000-Hippocampally-Dependent_Consolidation_in_a_Hierarchical_Model_of_Neocortex.html">66 nips-2000-Hippocampally-Dependent Consolidation in a Hierarchical Model of Neocortex</a></p>
<p>13 0.053443864 <a title="56-tfidf-13" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>14 0.053033646 <a title="56-tfidf-14" href="./nips-2000-Permitted_and_Forbidden_Sets_in_Symmetric_Threshold-Linear_Networks.html">100 nips-2000-Permitted and Forbidden Sets in Symmetric Threshold-Linear Networks</a></p>
<p>15 0.049508382 <a title="56-tfidf-15" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>16 0.048925739 <a title="56-tfidf-16" href="./nips-2000-Learning_Winner-take-all_Competition_Between_Groups_of_Neurons_in_Lateral_Inhibitory_Networks.html">81 nips-2000-Learning Winner-take-all Competition Between Groups of Neurons in Lateral Inhibitory Networks</a></p>
<p>17 0.047324024 <a title="56-tfidf-17" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>18 0.047198948 <a title="56-tfidf-18" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>19 0.046721648 <a title="56-tfidf-19" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>20 0.046263643 <a title="56-tfidf-20" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.165), (1, -0.079), (2, -0.102), (3, -0.01), (4, 0.017), (5, -0.011), (6, 0.081), (7, -0.074), (8, 0.037), (9, 0.08), (10, -0.07), (11, 0.119), (12, -0.087), (13, -0.365), (14, -0.065), (15, 0.116), (16, 0.09), (17, -0.035), (18, -0.074), (19, 0.024), (20, 0.144), (21, 0.022), (22, 0.009), (23, 0.001), (24, 0.072), (25, -0.035), (26, 0.065), (27, 0.035), (28, 0.002), (29, -0.068), (30, -0.059), (31, 0.041), (32, 0.138), (33, -0.004), (34, 0.036), (35, -0.113), (36, -0.006), (37, -0.091), (38, 0.01), (39, -0.07), (40, 0.075), (41, -0.002), (42, -0.238), (43, 0.147), (44, 0.068), (45, 0.05), (46, -0.071), (47, -0.029), (48, 0.038), (49, -0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96961755 <a title="56-lsi-1" href="./nips-2000-Foundations_for_a_Circuit_Complexity_Theory_of_Sensory_Processing.html">56 nips-2000-Foundations for a Circuit Complexity Theory of Sensory Processing</a></p>
<p>Author: Robert A. Legenstein, Wolfgang Maass</p><p>Abstract: We introduce total wire length as salient complexity measure for an analysis of the circuit complexity of sensory processing in biological neural systems and neuromorphic engineering. This new complexity measure is applied to a set of basic computational problems that apparently need to be solved by circuits for translation- and scale-invariant sensory processing. We exhibit new circuit design strategies for these new benchmark functions that can be implemented within realistic complexity bounds, in particular with linear or almost linear total wire length.</p><p>2 0.79234207 <a title="56-lsi-2" href="./nips-2000-Smart_Vision_Chip_Fabricated_Using_Three_Dimensional_Integration_Technology.html">118 nips-2000-Smart Vision Chip Fabricated Using Three Dimensional Integration Technology</a></p>
<p>Author: Hiroyuki Kurino, M. Nakagawa, Kang Wook Lee, Tomonori Nakamura, Yuusuke Yamada, Ki Tae Park, Mitsumasa Koyanagi</p><p>Abstract: The smart VISIOn chip has a large potential for application in general purpose high speed image processing systems . In order to fabricate smart vision chips including photo detector compactly, we have proposed the application of three dimensional LSI technology for smart vision chips. Three dimensional technology has great potential to realize new neuromorphic systems inspired by not only the biological function but also the biological structure. In this paper, we describe our three dimensional LSI technology for neuromorphic circuits and the design of smart vision chips .</p><p>3 0.57651788 <a title="56-lsi-3" href="./nips-2000-A_Silicon_Primitive_for_Competitive_Learning.html">11 nips-2000-A Silicon Primitive for Competitive Learning</a></p>
<p>Author: David Hsu, Miguel Figueroa, Chris Diorio</p><p>Abstract: Competitive learning is a technique for training classification and clustering networks. We have designed and fabricated an 11transistor primitive, that we term an automaximizing bump circuit, that implements competitive learning dynamics. The circuit performs a similarity computation, affords nonvolatile storage, and implements simultaneous local adaptation and computation. We show that our primitive is suitable for implementing competitive learning in VLSI, and demonstrate its effectiveness in a standard clustering task.</p><p>4 0.46427745 <a title="56-lsi-4" href="./nips-2000-Four-legged_Walking_Gait_Control_Using_a_Neuromorphic_Chip_Interfaced_to_a_Support_Vector_Learning_Algorithm.html">57 nips-2000-Four-legged Walking Gait Control Using a Neuromorphic Chip Interfaced to a Support Vector Learning Algorithm</a></p>
<p>Author: Susanne Still, Bernhard SchÃ¶lkopf, Klaus Hepp, Rodney J. Douglas</p><p>Abstract: To control the walking gaits of a four-legged robot we present a novel neuromorphic VLSI chip that coordinates the relative phasing of the robot's legs similar to how spinal Central Pattern Generators are believed to control vertebrate locomotion [3]. The chip controls the leg movements by driving motors with time varying voltages which are the outputs of a small network of coupled oscillators. The characteristics of the chip's output voltages depend on a set of input parameters. The relationship between input parameters and output voltages can be computed analytically for an idealized system. In practice, however, this ideal relationship is only approximately true due to transistor mismatch and offsets. Fine tuning of the chip's input parameters is done automatically by the robotic system, using an unsupervised Support Vector (SV) learning algorithm introduced recently [7]. The learning requires only that the description of the desired output is given. The machine learns from (unlabeled) examples how to set the parameters to the chip in order to obtain a desired motor behavior.</p><p>5 0.27533242 <a title="56-lsi-5" href="./nips-2000-Competition_and_Arbors_in_Ocular_Dominance.html">34 nips-2000-Competition and Arbors in Ocular Dominance</a></p>
<p>Author: Peter Dayan</p><p>Abstract: Hebbian and competitive Hebbian algorithms are almost ubiquitous in modeling pattern formation in cortical development. We analyse in theoretical detail a particular model (adapted from Piepenbrock & Obermayer, 1999) for the development of Id stripe-like patterns, which places competitive and interactive cortical influences, and free and restricted initial arborisation onto a common footing.</p><p>6 0.27064124 <a title="56-lsi-6" href="./nips-2000-Homeostasis_in_a_Silicon_Integrate_and_Fire_Neuron.html">67 nips-2000-Homeostasis in a Silicon Integrate and Fire Neuron</a></p>
<p>7 0.26089603 <a title="56-lsi-7" href="./nips-2000-Hippocampally-Dependent_Consolidation_in_a_Hierarchical_Model_of_Neocortex.html">66 nips-2000-Hippocampally-Dependent Consolidation in a Hierarchical Model of Neocortex</a></p>
<p>8 0.2487926 <a title="56-lsi-8" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>9 0.23238875 <a title="56-lsi-9" href="./nips-2000-Efficient_Learning_of_Linear_Perceptrons.html">44 nips-2000-Efficient Learning of Linear Perceptrons</a></p>
<p>10 0.23196064 <a title="56-lsi-10" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<p>11 0.22500195 <a title="56-lsi-11" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>12 0.22369488 <a title="56-lsi-12" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>13 0.21940339 <a title="56-lsi-13" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>14 0.21750826 <a title="56-lsi-14" href="./nips-2000-The_Use_of_MDL_to_Select_among_Computational_Models_of_Cognition.html">139 nips-2000-The Use of MDL to Select among Computational Models of Cognition</a></p>
<p>15 0.21680969 <a title="56-lsi-15" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>16 0.21626355 <a title="56-lsi-16" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>17 0.21583056 <a title="56-lsi-17" href="./nips-2000-Accumulator_Networks%3A_Suitors_of_Local_Probability_Propagation.html">15 nips-2000-Accumulator Networks: Suitors of Local Probability Propagation</a></p>
<p>18 0.21190965 <a title="56-lsi-18" href="./nips-2000-%60N-Body%27_Problems_in_Statistical_Learning.html">148 nips-2000-`N-Body' Problems in Statistical Learning</a></p>
<p>19 0.20392767 <a title="56-lsi-19" href="./nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">42 nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<p>20 0.19476011 <a title="56-lsi-20" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.018), (17, 0.593), (32, 0.016), (33, 0.035), (54, 0.01), (55, 0.011), (62, 0.023), (65, 0.014), (67, 0.042), (76, 0.031), (79, 0.012), (81, 0.029), (90, 0.021), (91, 0.016), (97, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99737585 <a title="56-lda-1" href="./nips-2000-The_Manhattan_World_Assumption%3A_Regularities_in_Scene_Statistics_which_Enable_Bayesian_Inference.html">135 nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</a></p>
<p>Author: James M. Coughlan, Alan L. Yuille</p><p>Abstract: Preliminary work by the authors made use of the so-called</p><p>2 0.99687791 <a title="56-lda-2" href="./nips-2000-Sparse_Kernel_Principal_Component_Analysis.html">121 nips-2000-Sparse Kernel Principal Component Analysis</a></p>
<p>Author: Michael E. Tipping</p><p>Abstract: 'Kernel' principal component analysis (PCA) is an elegant nonlinear generalisation of the popular linear data analysis method, where a kernel function implicitly defines a nonlinear transformation into a feature space wherein standard PCA is performed. Unfortunately, the technique is not 'sparse', since the components thus obtained are expressed in terms of kernels associated with every training vector. This paper shows that by approximating the covariance matrix in feature space by a reduced number of example vectors, using a maximum-likelihood approach, we may obtain a highly sparse form of kernel PCA without loss of effectiveness. 1</p><p>same-paper 3 0.99609667 <a title="56-lda-3" href="./nips-2000-Foundations_for_a_Circuit_Complexity_Theory_of_Sensory_Processing.html">56 nips-2000-Foundations for a Circuit Complexity Theory of Sensory Processing</a></p>
<p>Author: Robert A. Legenstein, Wolfgang Maass</p><p>Abstract: We introduce total wire length as salient complexity measure for an analysis of the circuit complexity of sensory processing in biological neural systems and neuromorphic engineering. This new complexity measure is applied to a set of basic computational problems that apparently need to be solved by circuits for translation- and scale-invariant sensory processing. We exhibit new circuit design strategies for these new benchmark functions that can be implemented within realistic complexity bounds, in particular with linear or almost linear total wire length.</p><p>4 0.99505198 <a title="56-lda-4" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>Author: Te-Won Lee, Thomas Wachtler, Terrence J. Sejnowski</p><p>Abstract: The human visual system encodes the chromatic signals conveyed by the three types of retinal cone photoreceptors in an opponent fashion. This color opponency has been shown to constitute an efficient encoding by spectral decorrelation of the receptor signals. We analyze the spatial and chromatic structure of natural scenes by decomposing the spectral images into a set of linear basis functions such that they constitute a representation with minimal redundancy. Independent component analysis finds the basis functions that transforms the spatiochromatic data such that the outputs (activations) are statistically as independent as possible, i.e. least redundant. The resulting basis functions show strong opponency along an achromatic direction (luminance edges), along a blueyellow direction, and along a red-blue direction. Furthermore, the resulting activations have very sparse distributions, suggesting that the use of color opponency in the human visual system achieves a highly efficient representation of colors. Our findings suggest that color opponency is a result of the properties of natural spectra and not solely a consequence of the overlapping cone spectral sensitivities. 1 Statistical structure of natural scenes Efficient encoding of visual sensory information is an important task for information processing systems and its study may provide insights into coding principles of biological visual systems. An important goal of sensory information processing Electronic version available at www. cnl. salk . edu/</p><p>5 0.99460399 <a title="56-lda-5" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>Author: Jason Weston, Sayan Mukherjee, Olivier Chapelle, Massimiliano Pontil, Tomaso Poggio, Vladimir Vapnik</p><p>Abstract: We introduce a method of feature selection for Support Vector Machines. The method is based upon finding those features which minimize bounds on the leave-one-out error. This search can be efficiently performed via gradient descent. The resulting algorithms are shown to be superior to some standard feature selection algorithms on both toy data and real-life problems of face recognition, pedestrian detection and analyzing DNA micro array data.</p><p>6 0.9302569 <a title="56-lda-6" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>7 0.86252099 <a title="56-lda-7" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>8 0.86177117 <a title="56-lda-8" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>9 0.83772516 <a title="56-lda-9" href="./nips-2000-A_Mathematical_Programming_Approach_to_the_Kernel_Fisher_Algorithm.html">5 nips-2000-A Mathematical Programming Approach to the Kernel Fisher Algorithm</a></p>
<p>10 0.83022153 <a title="56-lda-10" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>11 0.82891816 <a title="56-lda-11" href="./nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">95 nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>12 0.82791799 <a title="56-lda-12" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>13 0.82274681 <a title="56-lda-13" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>14 0.81799531 <a title="56-lda-14" href="./nips-2000-Factored_Semi-Tied_Covariance_Matrices.html">51 nips-2000-Factored Semi-Tied Covariance Matrices</a></p>
<p>15 0.81773376 <a title="56-lda-15" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>16 0.80657023 <a title="56-lda-16" href="./nips-2000-Generalizable_Singular_Value_Decomposition_for_Ill-posed_Datasets.html">61 nips-2000-Generalizable Singular Value Decomposition for Ill-posed Datasets</a></p>
<p>17 0.80529875 <a title="56-lda-17" href="./nips-2000-Constrained_Independent_Component_Analysis.html">36 nips-2000-Constrained Independent Component Analysis</a></p>
<p>18 0.8050313 <a title="56-lda-18" href="./nips-2000-Minimum_Bayes_Error_Feature_Selection_for_Continuous_Speech_Recognition.html">84 nips-2000-Minimum Bayes Error Feature Selection for Continuous Speech Recognition</a></p>
<p>19 0.80416048 <a title="56-lda-19" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>20 0.80367619 <a title="56-lda-20" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
