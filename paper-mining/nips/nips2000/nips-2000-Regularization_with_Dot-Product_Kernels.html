<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>110 nips-2000-Regularization with Dot-Product Kernels</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-110" href="#">nips2000-110</a> knowledge-graph by maker-knowledge-mining</p><h1>110 nips-2000-Regularization with Dot-Product Kernels</h1>
<br/><p>Source: <a title="nips-2000-110-pdf" href="http://papers.nips.cc/paper/1790-regularization-with-dot-product-kernels.pdf">pdf</a></p><p>Author: Alex J. Smola, Zoltán L. Óvári, Robert C. Williamson</p><p>Abstract: In this paper we give necessary and sufficient conditions under which kernels of dot product type k(x, y) = k(x . y) satisfy Mercer's condition and thus may be used in Support Vector Machines (SVM), Regularization Networks (RN) or Gaussian Processes (GP). In particular, we show that if the kernel is analytic (i.e. can be expanded in a Taylor series), all expansion coefficients have to be nonnegative. We give an explicit functional form for the feature map by calculating its eigenfunctions and eigenvalues. 1</p><p>Reference: <a title="nips-2000-110-reference" href="../nips2000_reference/nips-2000-Regularization_with_Dot-Product_Kernels_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kernel', 0.433), ('eigensystem', 0.337), ('legendr', 0.337), ('polynom', 0.245), ('expand', 0.216), ('merc', 0.197), ('eigenfunct', 0.167), ('dot', 0.163), ('spherical', 0.157), ('regul', 0.156), ('tk', 0.154), ('ud', 0.143), ('harmon', 0.112), ('tanh', 0.103), ('sery', 0.097), ('pn', 0.095), ('kn', 0.092), ('lrd', 0.092), ('nonneg', 0.086), ('sv', 0.083)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="110-tfidf-1" href="./nips-2000-Regularization_with_Dot-Product_Kernels.html">110 nips-2000-Regularization with Dot-Product Kernels</a></p>
<p>2 0.31959757 <a title="110-tfidf-2" href="./nips-2000-The_Kernel_Trick_for_Distances.html">134 nips-2000-The Kernel Trick for Distances</a></p>
<p>3 0.25134951 <a title="110-tfidf-3" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>4 0.22497372 <a title="110-tfidf-4" href="./nips-2000-Sparse_Kernel_Principal_Component_Analysis.html">121 nips-2000-Sparse Kernel Principal Component Analysis</a></p>
<p>5 0.19750951 <a title="110-tfidf-5" href="./nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">95 nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>6 0.1689516 <a title="110-tfidf-6" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>7 0.15321265 <a title="110-tfidf-7" href="./nips-2000-From_Margin_to_Sparsity.html">58 nips-2000-From Margin to Sparsity</a></p>
<p>8 0.14843442 <a title="110-tfidf-8" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>9 0.14663477 <a title="110-tfidf-9" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>10 0.11993633 <a title="110-tfidf-10" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>11 0.1167016 <a title="110-tfidf-11" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>12 0.11560434 <a title="110-tfidf-12" href="./nips-2000-A_Mathematical_Programming_Approach_to_the_Kernel_Fisher_Algorithm.html">5 nips-2000-A Mathematical Programming Approach to the Kernel Fisher Algorithm</a></p>
<p>13 0.11426144 <a title="110-tfidf-13" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>14 0.1047046 <a title="110-tfidf-14" href="./nips-2000-Computing_with_Finite_and_Infinite_Networks.html">35 nips-2000-Computing with Finite and Infinite Networks</a></p>
<p>15 0.10320788 <a title="110-tfidf-15" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>16 0.096572891 <a title="110-tfidf-16" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>17 0.094243683 <a title="110-tfidf-17" href="./nips-2000-Efficient_Learning_of_Linear_Perceptrons.html">44 nips-2000-Efficient Learning of Linear Perceptrons</a></p>
<p>18 0.093369879 <a title="110-tfidf-18" href="./nips-2000-Competition_and_Arbors_in_Ocular_Dominance.html">34 nips-2000-Competition and Arbors in Ocular Dominance</a></p>
<p>19 0.090396665 <a title="110-tfidf-19" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>20 0.085280351 <a title="110-tfidf-20" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.257), (1, 0.241), (2, 0.237), (3, -0.122), (4, -0.198), (5, -0.061), (6, -0.16), (7, 0.023), (8, -0.08), (9, -0.012), (10, 0.057), (11, -0.036), (12, 0.119), (13, -0.019), (14, 0.154), (15, 0.116), (16, -0.112), (17, -0.039), (18, 0.003), (19, 0.063), (20, 0.072), (21, 0.071), (22, -0.001), (23, 0.006), (24, -0.024), (25, -0.022), (26, 0.057), (27, -0.018), (28, -0.016), (29, 0.031), (30, -0.068), (31, 0.055), (32, -0.055), (33, -0.008), (34, -0.125), (35, -0.057), (36, -0.057), (37, -0.127), (38, -0.043), (39, -0.057), (40, -0.059), (41, -0.002), (42, -0.008), (43, -0.086), (44, 0.007), (45, 0.045), (46, -0.027), (47, 0.054), (48, -0.023), (49, -0.107)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97099662 <a title="110-lsi-1" href="./nips-2000-Regularization_with_Dot-Product_Kernels.html">110 nips-2000-Regularization with Dot-Product Kernels</a></p>
<p>2 0.84701806 <a title="110-lsi-2" href="./nips-2000-The_Kernel_Trick_for_Distances.html">134 nips-2000-The Kernel Trick for Distances</a></p>
<p>3 0.73601639 <a title="110-lsi-3" href="./nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">95 nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>4 0.69602406 <a title="110-lsi-4" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>5 0.6493426 <a title="110-lsi-5" href="./nips-2000-Sparse_Kernel_Principal_Component_Analysis.html">121 nips-2000-Sparse Kernel Principal Component Analysis</a></p>
<p>6 0.62325621 <a title="110-lsi-6" href="./nips-2000-A_Mathematical_Programming_Approach_to_the_Kernel_Fisher_Algorithm.html">5 nips-2000-A Mathematical Programming Approach to the Kernel Fisher Algorithm</a></p>
<p>7 0.51934463 <a title="110-lsi-7" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>8 0.48931167 <a title="110-lsi-8" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>9 0.43479279 <a title="110-lsi-9" href="./nips-2000-Competition_and_Arbors_in_Ocular_Dominance.html">34 nips-2000-Competition and Arbors in Ocular Dominance</a></p>
<p>10 0.39955938 <a title="110-lsi-10" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>11 0.39373818 <a title="110-lsi-11" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>12 0.35912243 <a title="110-lsi-12" href="./nips-2000-From_Margin_to_Sparsity.html">58 nips-2000-From Margin to Sparsity</a></p>
<p>13 0.34978986 <a title="110-lsi-13" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>14 0.33248705 <a title="110-lsi-14" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>15 0.32304418 <a title="110-lsi-15" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>16 0.31505367 <a title="110-lsi-16" href="./nips-2000-Efficient_Learning_of_Linear_Perceptrons.html">44 nips-2000-Efficient Learning of Linear Perceptrons</a></p>
<p>17 0.30996829 <a title="110-lsi-17" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>18 0.29333195 <a title="110-lsi-18" href="./nips-2000-Computing_with_Finite_and_Infinite_Networks.html">35 nips-2000-Computing with Finite and Infinite Networks</a></p>
<p>19 0.28492472 <a title="110-lsi-19" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>20 0.2818464 <a title="110-lsi-20" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.03), (9, 0.127), (11, 0.02), (16, 0.073), (21, 0.016), (23, 0.014), (38, 0.029), (54, 0.034), (74, 0.457), (76, 0.015), (84, 0.058), (99, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81430042 <a title="110-lda-1" href="./nips-2000-Regularization_with_Dot-Product_Kernels.html">110 nips-2000-Regularization with Dot-Product Kernels</a></p>
<p>2 0.77005851 <a title="110-lda-2" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>3 0.75870138 <a title="110-lda-3" href="./nips-2000-Bayesian_Video_Shot_Segmentation.html">30 nips-2000-Bayesian Video Shot Segmentation</a></p>
<p>4 0.71042717 <a title="110-lda-4" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>5 0.48937902 <a title="110-lda-5" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>6 0.47430187 <a title="110-lda-6" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>7 0.45544165 <a title="110-lda-7" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>8 0.44836685 <a title="110-lda-8" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>9 0.43499339 <a title="110-lda-9" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>10 0.43058017 <a title="110-lda-10" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>11 0.43020633 <a title="110-lda-11" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>12 0.42376798 <a title="110-lda-12" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>13 0.42233971 <a title="110-lda-13" href="./nips-2000-Efficient_Learning_of_Linear_Perceptrons.html">44 nips-2000-Efficient Learning of Linear Perceptrons</a></p>
<p>14 0.42220646 <a title="110-lda-14" href="./nips-2000-Permitted_and_Forbidden_Sets_in_Symmetric_Threshold-Linear_Networks.html">100 nips-2000-Permitted and Forbidden Sets in Symmetric Threshold-Linear Networks</a></p>
<p>15 0.42138237 <a title="110-lda-15" href="./nips-2000-Machine_Learning_for_Video-Based_Rendering.html">83 nips-2000-Machine Learning for Video-Based Rendering</a></p>
<p>16 0.41817525 <a title="110-lda-16" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>17 0.41789159 <a title="110-lda-17" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>18 0.41508368 <a title="110-lda-18" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>19 0.4134663 <a title="110-lda-19" href="./nips-2000-A_Variational_Mean-Field_Theory_for_Sigmoidal_Belief_Networks.html">14 nips-2000-A Variational Mean-Field Theory for Sigmoidal Belief Networks</a></p>
<p>20 0.4125174 <a title="110-lda-20" href="./nips-2000-Foundations_for_a_Circuit_Complexity_Theory_of_Sensory_Processing.html">56 nips-2000-Foundations for a Circuit Complexity Theory of Sensory Processing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
