<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-9" href="#">nips2000-9</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</h1>
<br/><p>Source: <a title="nips-2000-9-pdf" href="http://papers.nips.cc/paper/1844-a-pac-bayesian-margin-bound-for-linear-classifiers-why-svms-work.pdf">pdf</a></p><p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: We present a bound on the generalisation error of linear classifiers in terms of a refined margin quantity on the training set. The result is obtained in a PAC- Bayesian framework and is based on geometrical arguments in the space of linear classifiers. The new bound constitutes an exponential improvement of the so far tightest margin bound by Shawe-Taylor et al. [8] and scales logarithmically in the inverse margin. Even in the case of less training examples than input dimensions sufficiently large margins lead to non-trivial bound values and - for maximum margins - to a vanishing complexity term. Furthermore, the classical margin is too coarse a measure for the essential quantity that controls the generalisation error: the volume ratio between the whole hypothesis space and the subset of consistent hypotheses. The practical relevance of the result lies in the fact that the well-known support vector machine is optimal w.r.t. the new bound only if the feature vectors are all of the same length. As a consequence we recommend to use SVMs on normalised feature vectors only - a recommendation that is well supported by our numerical experiments on two benchmark data sets. 1</p><p>Reference: <a title="nips-2000-9-reference" href="../nips2000_reference/nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract We present a bound on the generalisation error of linear classifiers in terms of a refined margin quantity on the training set. [sent-5, score-1.228]
</p><p>2 The result is obtained in a PAC- Bayesian framework and is based on geometrical arguments in the space of linear classifiers. [sent-6, score-0.189]
</p><p>3 The new bound constitutes an exponential improvement of the so far tightest margin bound by Shawe-Taylor et al. [sent-7, score-0.763]
</p><p>4 Even in the case of less training examples than input dimensions sufficiently large margins lead to non-trivial bound values and - for maximum margins - to a vanishing complexity term. [sent-9, score-0.451]
</p><p>5 Furthermore, the classical margin is too coarse a measure for the essential quantity that controls the generalisation error: the volume ratio between the whole hypothesis space and the subset of consistent hypotheses. [sent-10, score-1.12]
</p><p>6 The practical relevance of the result lies in the fact that the well-known support vector machine is optimal w. [sent-11, score-0.034]
</p><p>7 the new bound only if the feature vectors are all of the same length. [sent-14, score-0.33]
</p><p>8 As a consequence we recommend to use SVMs on normalised feature vectors only - a recommendation that is well supported by our numerical experiments on two benchmark data sets. [sent-15, score-0.38]
</p><p>9 1  Introduction  Linear classifiers are exceedingly popular in the machine learning community due to their straight-forward applicability and high flexibility which has recently been boosted by the so-called kernel methods [13]. [sent-16, score-0.401]
</p><p>10 A natural and popular framework for the theoretical analysis of classifiers is the PAC (probably approximately correct) framework [11] which is closely related to Vapnik's work on the generalisation error [12]. [sent-17, score-0.709]
</p><p>11 For binary classifiers it turned out that the growth function is an appropriate measure of "complexity" and can tightly be upper bounded by the VC (Vapnik-Chervonenkis) dimension [14]. [sent-18, score-0.507]
</p><p>12 Later, structural risk minimisation [12] was suggested for directly minimising the VC dimension based on a training set and an a priori structuring of the hypothesis space. [sent-19, score-0.479]
</p><p>13 in the case of linear classifiers, often a thresholded real-valued func-  tion is used for classification. [sent-22, score-0.04]
</p><p>14 In 1993, Kearns [4] demonstrated that considerably tighter bounds can be obtained by considering a scale-sensitive complexity measure known as the fat shattering dimension. [sent-23, score-0.223]
</p><p>15 Further results [1] provided bounds on the Growth function similar to those proved by Vapnik and others [14,6]. [sent-24, score-0.08]
</p><p>16 The popularity of the theory was boosted by the invention of the support vector machine (SVM) [13] which aims at directly minimising the complexity as suggested by theory. [sent-25, score-0.313]
</p><p>17 Until recently, however, the success of the SVM remained somewhat obscure because in PAC/VC theory the structuring of the hypothesis space must be independent of the training data - in contrast to the data-dependence of the canonical hyperplane. [sent-26, score-0.362]
</p><p>18 [8] developed the luckiness framework, where luckiness refers to a complexity measure that is a function of both hypothesis and training sample. [sent-29, score-0.528]
</p><p>19 Recently, David McAllester presented some PAC-Bayesian theorems [5] that bound the generalisation error of Bayesian classifiers independently of the correctness of the prior and regardless of the underlying data distribution - thus fulfilling the basic desiderata of PAC theory. [sent-30, score-0.867]
</p><p>20 In [3] McAllester's bounds on the Gibbs classifier were extended to the Bayes (optimal) classifier. [sent-31, score-0.225]
</p><p>21 The PAC-Bayesian framework provides a posteriori bounds and is thus closely related in spirit to the luckiness framework l . [sent-32, score-0.339]
</p><p>22 In this paper we give a tight margin bound for linear classifiers in the PAC-Bayesian framework. [sent-33, score-0.787]
</p><p>23 The main idea is to identify the generalisation error of the classifier h of interest with that of the Bayes (optimal) classifier of a (point-symmetric) subset Q that is summarised by h. [sent-34, score-0.797]
</p><p>24 We show that for a uniform prior the normalised margin of h is directly related to the volume of a large subset Q summarised by h. [sent-35, score-0.698]
</p><p>25 In particular, the result suggests that a learning algorithm for linear classifiers should aim at maximising the normalised margin instead of the classical margin. [sent-36, score-0.846]
</p><p>26 In Section 2 and 3 we review the basic PAC-Bayesian theorem and show how it can be applied to single classifiers. [sent-37, score-0.109]
</p><p>27 In Section 5 we discuss the consequences of the new result for the application of SVMs and demonstrate experimentally that in fact a normalisation of the feature vectors leads to considerably superior generalisation performance. [sent-39, score-0.551]
</p><p>28 X) and vector spaces by calligraphic capitalised letters (e. [sent-49, score-0.045]
</p><p>29 e~ denote a probability measure, the expectation of a random variable, the indicator function and the normed space (2-norm) of sequences of length n, respectively. [sent-53, score-0.057]
</p><p>30 Let a labelled training sample z = (x,y) E (X x y)m = zm be drawn iid according to some unknown probability measure P z = PYIXPx . [sent-56, score-0.17]
</p><p>31 Furthermore for a given hypothesis space 1t ~ yX we assume the existence of a ''true'' hypothesis h * E 1t that labelled the data PYIX=x (y)  = Iy=h*(x). [sent-57, score-0.336]
</p><p>32 (1)  We consider linear hypotheses 1t  = {hw:  X  f-t  sign((w,¢(x)}x:;) I w E W},  W  = {w E K  IllwlllC  = 1},  (2)  lIn fact, even Shawe-Taylor et. [sent-58, score-0.087]
</p><p>33 a Bayesian might say that luckiness is just a complicated way of encoding a prior. [sent-63, score-0.117]
</p><p>34 The sole justification for our particular way of encoding is that it allows us to get the PAC like results we sought . [sent-64, score-0.035]
</p><p>35 where the mapping ¢ : X ~ K ~ f~ maps2 the input data to some feature space K and Ilwll,>;; = 1 leads to a one-to-one correspondence of hypotheses hw to their parameters w. [sent-69, score-0.394]
</p><p>36 From the existence of h* we know that there exists a version space V(z) ~ W, V(z)={wEW IV(x,y)EZ: hw(x)=y}. [sent-70, score-0.098]
</p><p>37 Our analysis aims at bounding the true risk R [w] of consistent hypotheses hw, R[w] = P XY (hw (X):I Y) . [sent-71, score-0.312]
</p><p>38 Since all classifiers w E V (z) are indistinguishable in terms of number of errors committed on the given training set z let us introduce the concept of the margin 'Yz (w) of a classifier w, i. [sent-72, score-0.833]
</p><p>39 = (ximIn ,y;)Ez  ydw, Xi),>;; Ilwll,>;;  (3)  The following theorem due to Shawe-Taylor et al. [sent-75, score-0.109]
</p><p>40 [8] bounds the generalisation errors R [w] of all classifier wE V (z) in terms of the margin 'Yz (w). [sent-76, score-0.817]
</p><p>41 3  PAC-Bayesian Analysis  We first present a result [5] that bounds the risk of the generalised Gibbs classification strategy Gibbsw(z) by the measure Pw (W (z)) on a consistent subset W (z) ~ V (z). [sent-81, score-0.47]
</p><p>42 This average risk is then related via the Bayes-Gibbs lemma to the risk ofthe Bayes classification strategy Bayesw(z) on W (z). [sent-82, score-0.432]
</p><p>43 For a single consistent hypothesis w E W it is then necessary to identify a consistent subset Q (w) such that the Bayes strategy BayesQ(w) on Q (w) always agrees with w. [sent-83, score-0.421]
</p><p>44 Let us define the Gibbs classification strategy Gibbsw(z) w. [sent-84, score-0.095]
</p><p>45 the subset W (z) ~ V (z) by Gibbsw(z) (x) = hw (x) , w '" PWIWEW(z) . [sent-87, score-0.297]
</p><p>46 (5) Then the following theorem [5] holds for the risk of Gibbsw(z). [sent-88, score-0.237]
</p><p>47 (6)  2For notational simplicity we sometimes abbreviate cf> (x) by x which should not be confused with the sample x of training objects. [sent-91, score-0.067]
</p><p>48 Now consider the Bayes classifier Bayesw(z), Bayesw(z) (x)  = sign (EwIWEW(z) [hw (x)])  ,  where the expectation EWIWEW(z) is taken over a cut-off posterior given by combining the PAC-likelihood (1) and the prior P w . [sent-92, score-0.145]
</p><p>49 For any two measures P w and P XY and any setW ~ W  P XY (Bayesw (X) =I Y) :s; 2· P XY (Gibbsw (X) =I Y) . [sent-94, score-0.036]
</p><p>50 At all those points x E X at which Bayesw is wrong by definition at least half ofthe classifiers wE W under consideration make a mistake as well. [sent-97, score-0.279]
</p><p>51 D  The combination of Lemma 1 with Theorem 2 yields a bound on the risk of Bayesw(z). [sent-98, score-0.324]
</p><p>52 For a single hypothesis wE W let us find a (Bayes-admissible) subset Q (w) of version space V (z) such that BayesQ(w) on Q (w) agrees with w on every point in X. [sent-99, score-0.392]
</p><p>53 Given the hypothesis space in (2) and a prior measure Pw over W we call a subset Q (w) ~ W Bayes admissible w. [sent-101, score-0.387]
</p><p>54 w and P w if and only if 'r/xEX:  hw (x)  = BayesQ(w) (x)  . [sent-104, score-0.212]
</p><p>55 Although difficult to achieve in general the following geometrically plausible lemma establishes Bayes-admissibility for the case of interest. [sent-105, score-0.169]
</p><p>56 For uniform measure P w over W each ball Q (w) = {v E W Illw - vlliC :s; r} is Bayes admissible w. [sent-107, score-0.242]
</p><p>57 Please note that by considering a ball Q (w) rather than just w we make use of the fact that w summarises all its neighbouring classifiers v E Q (w). [sent-111, score-0.328]
</p><p>58 Now using a uniform prior P w the normalised margin (8) quantifies the relative volume of classifiers summarised by wand thus allows us to bound its risk. [sent-112, score-1.123]
</p><p>59 Note that in contrast to the classical margin '"Yz (see 3) this normalised margin is a dimensionless quantity and constitutes a measure for the relative size of the version space invariant under rescaling of both weight vectors w and feature vectors Xi. [sent-113, score-1.156]
</p><p>60 4  A PAC-Bayesian Margin Bound  Combining the ideas outlined in the previous section allows us to derive a generalisation error bound for linear classifiers w E V (z) in terms of their normalised margin r z (w). [sent-114, score-1.289]
</p><p>61 Figure 1: Illustration of the volume ratio for the classifier at the north pole. [sent-115, score-0.249]
</p><p>62 Four training points shown as grand circles make up version space - the polyhedron on top of the sphere. [sent-116, score-0.165]
</p><p>63 The radius of the "cap" of the sphere is proportional to the margin r %, which only for constant Ilxill. [sent-117, score-0.348]
</p><p>64 Suppose K ~ f~ is a given feature space of dimensionality n. [sent-120, score-0.135]
</p><p>65 Geometrically the hypothesis space W is the unit sphere in ~n (see Figure 1). [sent-123, score-0.254]
</p><p>66 Let us assume that P w is uniform on the unit sphere as suggested by symmetry. [sent-124, score-0.229]
</p><p>67 Given the training set z and a classifier wall classifiers v E Q (w)  Q (w)  = {v E W I (w, v)K > Vl- r~ (w) }  (10)  are within V (z) (For a proof see [2]). [sent-125, score-0.491]
</p><p>68 Such a set Q (w) is Bayes-admissible by Lemma 2 and hence we can use P w (Q (w» to bound the generalisation error of w. [sent-126, score-0.53]
</p><p>69 Since Pw is uniform, the value -In (P w (Q (w») is simply the logarithm of the volume ratio between the surface of the unit sphere and the surface of all v fulfilling equation (10). [sent-127, score-0.238]
</p><p>70 In [2] it is shown that this ratio is exactly given by  ( In  f;1r sinn - 2 (B) dB rarccos( Vl-r;(w)). [sent-128, score-0.053]
</p><p>71 (B) dB  It can be shown that this ratio is tightly bounded from above by  n In (  1-  1  Vl- r~ (w)  ) + In (2) . [sent-130, score-0.166]
</p><p>72 -- j--j-___ j_ + j -}_++-j--j--I-- --+ l l--} j ·  p  p  (a)  (b)  Figure 2: Generalisation errors of classifiers learned by an SVM with (dashed line) and without (solid line) normalisation of the feature vectors Xi. [sent-133, score-0.543]
</p><p>73 The error bars indicate one standard deviation over 100 random splits of the data sets. [sent-134, score-0.098]
</p><p>74 The two plots are obtained on the (a) thyroid and (b) sonar data set. [sent-135, score-0.121]
</p><p>75 Note that m points maximally span an m- dimensional space and thus we can marginalise over the remaining n - m 0 dimensions of feature space K . [sent-137, score-0.192]
</p><p>76 An appealing feature of equation (9) is that for r z (w) = 1 the bound reduces to ~ (21n (m) - In (8) + 2) with a rapid decay to zero as m increases. [sent-139, score-0.274]
</p><p>77 Furthermore, upper bounding 1/(1- vr=r') by 2/r we see that Theorem 3 is an exponential improvement of Theorem 1 in terms of the attained margins. [sent-144, score-0.04]
</p><p>78 It should be noted, however, that the new bound depends on the dimensionality of the input space via d = min (m, n). [sent-145, score-0.294]
</p><p>79 5  Experimental Study  Theorem 3 suggest the following learning algorithm: given a version space V (z) (through a given training set z) find the classifier w that maximises r z (w). [sent-146, score-0.31]
</p><p>80 This algorithm, however, is given by the SVM only if the training data in feature space K are normalised. [sent-147, score-0.202]
</p><p>81 We investigate the influence of such a normalisation on the generalisation error in the feature space K of all monomials up to the p-th degree (well-known from handwritten digit recognition, see [13]). [sent-148, score-0.564]
</p><p>82 Earlier experiment have shown [13] that without normalisation too large values of p may lead to "overfitting". [sent-150, score-0.095]
</p><p>83 We used the VCI [10] data sets thyroid (d = 5, m = 140, mtest = 75) and sonar (d = 60, m = 124, mtest = 60) and plotted the generalisation error of SVM solutions (estimated over 100 different splits of the data set) as a function of p (see Figure 2). [sent-151, score-0.62]
</p><p>84 As suggested by Theorem 3 in almost all cases the normalisation improved the performance of the support vector machine solution at a statistically significant level. [sent-152, score-0.178]
</p><p>85 As a consequence, we recommend: When training an SVM, always normalise your data in feature space. [sent-153, score-0.145]
</p><p>86 Intuitively, it is only the spatial direction of both weight vector and feature vectors that determines the classification. [sent-154, score-0.134]
</p><p>87 Hence the different lengths of feature vectors in the training set should not enter the SVM optimisation problem. [sent-155, score-0.201]
</p><p>88 6  Conclusion  The PAC-Bayesian framework together with simple geometrical arguments yields the so far tightest margin bound for linear classifiers. [sent-156, score-0.653]
</p><p>89 The role of the normalised margin r % in the new bound suggests that the SVM is theoretically justified only for input vectors of constant length. [sent-157, score-0.657]
</p><p>90 On the uniform convergence of relative frequencies of events to their probabilities. [sent-236, score-0.069]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('generalisation', 0.285), ('classifiers', 0.279), ('margin', 0.272), ('gibbsw', 0.238), ('hw', 0.212), ('bayesw', 0.204), ('bound', 0.196), ('pac', 0.187), ('classifier', 0.145), ('normalised', 0.133), ('risk', 0.128), ('pw', 0.123), ('hypothesis', 0.121), ('luckiness', 0.117), ('svm', 0.117), ('lemma', 0.116), ('theorem', 0.109), ('bayesq', 0.102), ('normalisation', 0.095), ('mcallester', 0.088), ('summarised', 0.088), ('xy', 0.087), ('subset', 0.085), ('bounds', 0.08), ('feature', 0.078), ('sphere', 0.076), ('bayes', 0.074), ('margins', 0.074), ('vapnik', 0.073), ('berlin', 0.071), ('draw', 0.069), ('uniform', 0.069), ('ewiwew', 0.068), ('recommend', 0.068), ('structuring', 0.068), ('succeed', 0.068), ('thyroid', 0.068), ('training', 0.067), ('measure', 0.066), ('svms', 0.064), ('gibbs', 0.064), ('strategy', 0.06), ('admissible', 0.058), ('fulfilling', 0.058), ('mtest', 0.058), ('tightly', 0.058), ('space', 0.057), ('vectors', 0.056), ('bounded', 0.055), ('ratio', 0.053), ('ilwll', 0.053), ('agrees', 0.053), ('geometrically', 0.053), ('sonar', 0.053), ('tightest', 0.053), ('consistent', 0.051), ('volume', 0.051), ('suggested', 0.049), ('error', 0.049), ('boosted', 0.049), ('ball', 0.049), ('graepel', 0.049), ('growth', 0.049), ('splits', 0.049), ('theory', 0.049), ('framework', 0.048), ('bayesian', 0.047), ('hypotheses', 0.047), ('minimising', 0.046), ('spirit', 0.046), ('aims', 0.046), ('constitutes', 0.046), ('herbrich', 0.046), ('maximising', 0.046), ('consequence', 0.045), ('letters', 0.045), ('boosting', 0.044), ('bold', 0.044), ('ez', 0.044), ('geometrical', 0.044), ('vc', 0.044), ('achieving', 0.041), ('version', 0.041), ('min', 0.041), ('complexity', 0.04), ('linear', 0.04), ('quantity', 0.04), ('bounding', 0.04), ('kernel', 0.039), ('classical', 0.039), ('classifying', 0.038), ('acm', 0.038), ('technical', 0.038), ('aim', 0.037), ('labelled', 0.037), ('considerably', 0.037), ('measures', 0.036), ('us', 0.035), ('errors', 0.035), ('machine', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="9-tfidf-1" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: We present a bound on the generalisation error of linear classifiers in terms of a refined margin quantity on the training set. The result is obtained in a PAC- Bayesian framework and is based on geometrical arguments in the space of linear classifiers. The new bound constitutes an exponential improvement of the so far tightest margin bound by Shawe-Taylor et al. [8] and scales logarithmically in the inverse margin. Even in the case of less training examples than input dimensions sufficiently large margins lead to non-trivial bound values and - for maximum margins - to a vanishing complexity term. Furthermore, the classical margin is too coarse a measure for the essential quantity that controls the generalisation error: the volume ratio between the whole hypothesis space and the subset of consistent hypotheses. The practical relevance of the result lies in the fact that the well-known support vector machine is optimal w.r.t. the new bound only if the feature vectors are all of the same length. As a consequence we recommend to use SVMs on normalised feature vectors only - a recommendation that is well supported by our numerical experiments on two benchmark data sets. 1</p><p>2 0.45475784 <a title="9-tfidf-2" href="./nips-2000-From_Margin_to_Sparsity.html">58 nips-2000-From Margin to Sparsity</a></p>
<p>Author: Thore Graepel, Ralf Herbrich, Robert C. Williamson</p><p>Abstract: We present an improvement of Novikoff's perceptron convergence theorem. Reinterpreting this mistake bound as a margin dependent sparsity guarantee allows us to give a PAC-style generalisation error bound for the classifier learned by the perceptron learning algorithm. The bound value crucially depends on the margin a support vector machine would achieve on the same data set using the same kernel. Ironically, the bound yields better guarantees than are currently available for the support vector solution itself. 1</p><p>3 0.42771342 <a title="9-tfidf-3" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: The concept of averaging over classifiers is fundamental to the Bayesian analysis of learning. Based on this viewpoint, it has recently been demonstrated for linear classifiers that the centre of mass of version space (the set of all classifiers consistent with the training set) - also known as the Bayes point - exhibits excellent generalisation abilities. However, the billiard algorithm as presented in [4] is restricted to small sample size because it requires o (m 2 ) of memory and 0 (N . m2 ) computational steps where m is the number of training patterns and N is the number of random draws from the posterior distribution. In this paper we present a method based on the simple perceptron learning algorithm which allows to overcome this algorithmic drawback. The method is algorithmically simple and is easily extended to the multi-class case. We present experimental results on the MNIST data set of handwritten digits which show that Bayes point machines (BPMs) are competitive with the current world champion, the support vector machine. In addition, the computational complexity of BPMs can be tuned by varying the number of samples from the posterior. Finally, rejecting test points on the basis of their (approximative) posterior probability leads to a rapid decrease in generalisation error, e.g. 0.1% generalisation error for a given rejection rate of 10%. 1</p><p>4 0.253598 <a title="9-tfidf-4" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>Author: Thore Graepel, Ralf Herbrich</p><p>Abstract: We present an algorithm that samples the hypothesis space of kernel classifiers. Given a uniform prior over normalised weight vectors and a likelihood based on a model of label noise leads to a piecewise constant posterior that can be sampled by the kernel Gibbs sampler (KGS). The KGS is a Markov Chain Monte Carlo method that chooses a random direction in parameter space and samples from the resulting piecewise constant density along the line chosen. The KGS can be used as an analytical tool for the exploration of Bayesian transduction, Bayes point machines, active learning, and evidence-based model selection on small data sets that are contaminated with label noise. For a simple toy example we demonstrate experimentally how a Bayes point machine based on the KGS outperforms an SVM that is incapable of taking into account label noise. 1</p><p>5 0.21818954 <a title="9-tfidf-5" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>Author: Shie Mannor, Ron Meir</p><p>Abstract: The problem of constructing weak classifiers for boosting algorithms is studied. We present an algorithm that produces a linear classifier that is guaranteed to achieve an error better than random guessing for any distribution on the data. While this weak learner is not useful for learning in general, we show that under reasonable conditions on the distribution it yields an effective weak learner for one-dimensional problems. Preliminary simulations suggest that similar behavior can be expected in higher dimensions, a result which is corroborated by some recent theoretical bounds. Additionally, we provide improved convergence rate bounds for the generalization error in situations where the empirical error can be made small, which is exactly the situation that occurs if weak learners with guaranteed performance that is better than random guessing can be established. 1</p><p>6 0.19187516 <a title="9-tfidf-6" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>7 0.18447265 <a title="9-tfidf-7" href="./nips-2000-Algorithmic_Stability_and_Generalization_Performance.html">21 nips-2000-Algorithmic Stability and Generalization Performance</a></p>
<p>8 0.15639651 <a title="9-tfidf-8" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>9 0.15395647 <a title="9-tfidf-9" href="./nips-2000-Some_New_Bounds_on_the_Generalization_Error_of_Combined_Classifiers.html">119 nips-2000-Some New Bounds on the Generalization Error of Combined Classifiers</a></p>
<p>10 0.15230072 <a title="9-tfidf-10" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>11 0.1333545 <a title="9-tfidf-11" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>12 0.12591006 <a title="9-tfidf-12" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>13 0.12273809 <a title="9-tfidf-13" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<p>14 0.12207279 <a title="9-tfidf-14" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>15 0.11231723 <a title="9-tfidf-15" href="./nips-2000-Efficient_Learning_of_Linear_Perceptrons.html">44 nips-2000-Efficient Learning of Linear Perceptrons</a></p>
<p>16 0.11048046 <a title="9-tfidf-16" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>17 0.10484184 <a title="9-tfidf-17" href="./nips-2000-Active_Support_Vector_Machine_Classification.html">18 nips-2000-Active Support Vector Machine Classification</a></p>
<p>18 0.10263168 <a title="9-tfidf-18" href="./nips-2000-Vicinal_Risk_Minimization.html">144 nips-2000-Vicinal Risk Minimization</a></p>
<p>19 0.092713274 <a title="9-tfidf-19" href="./nips-2000-A_Gradient-Based_Boosting_Algorithm_for_Regression_Problems.html">3 nips-2000-A Gradient-Based Boosting Algorithm for Regression Problems</a></p>
<p>20 0.084813029 <a title="9-tfidf-20" href="./nips-2000-Regularized_Winnow_Methods.html">111 nips-2000-Regularized Winnow Methods</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.328), (1, 0.433), (2, -0.221), (3, 0.02), (4, -0.107), (5, -0.291), (6, 0.084), (7, -0.009), (8, 0.08), (9, -0.068), (10, 0.162), (11, -0.063), (12, -0.06), (13, 0.029), (14, -0.172), (15, 0.027), (16, -0.022), (17, 0.001), (18, 0.018), (19, 0.1), (20, -0.06), (21, 0.045), (22, -0.077), (23, -0.103), (24, 0.054), (25, 0.059), (26, -0.03), (27, 0.114), (28, 0.062), (29, -0.0), (30, 0.006), (31, 0.026), (32, 0.035), (33, -0.076), (34, 0.031), (35, 0.008), (36, -0.002), (37, -0.012), (38, 0.075), (39, -0.048), (40, -0.007), (41, -0.004), (42, 0.0), (43, 0.018), (44, -0.001), (45, 0.024), (46, -0.112), (47, 0.015), (48, 0.011), (49, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97062069 <a title="9-lsi-1" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: We present a bound on the generalisation error of linear classifiers in terms of a refined margin quantity on the training set. The result is obtained in a PAC- Bayesian framework and is based on geometrical arguments in the space of linear classifiers. The new bound constitutes an exponential improvement of the so far tightest margin bound by Shawe-Taylor et al. [8] and scales logarithmically in the inverse margin. Even in the case of less training examples than input dimensions sufficiently large margins lead to non-trivial bound values and - for maximum margins - to a vanishing complexity term. Furthermore, the classical margin is too coarse a measure for the essential quantity that controls the generalisation error: the volume ratio between the whole hypothesis space and the subset of consistent hypotheses. The practical relevance of the result lies in the fact that the well-known support vector machine is optimal w.r.t. the new bound only if the feature vectors are all of the same length. As a consequence we recommend to use SVMs on normalised feature vectors only - a recommendation that is well supported by our numerical experiments on two benchmark data sets. 1</p><p>2 0.8871007 <a title="9-lsi-2" href="./nips-2000-From_Margin_to_Sparsity.html">58 nips-2000-From Margin to Sparsity</a></p>
<p>Author: Thore Graepel, Ralf Herbrich, Robert C. Williamson</p><p>Abstract: We present an improvement of Novikoff's perceptron convergence theorem. Reinterpreting this mistake bound as a margin dependent sparsity guarantee allows us to give a PAC-style generalisation error bound for the classifier learned by the perceptron learning algorithm. The bound value crucially depends on the margin a support vector machine would achieve on the same data set using the same kernel. Ironically, the bound yields better guarantees than are currently available for the support vector solution itself. 1</p><p>3 0.85177028 <a title="9-lsi-3" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: The concept of averaging over classifiers is fundamental to the Bayesian analysis of learning. Based on this viewpoint, it has recently been demonstrated for linear classifiers that the centre of mass of version space (the set of all classifiers consistent with the training set) - also known as the Bayes point - exhibits excellent generalisation abilities. However, the billiard algorithm as presented in [4] is restricted to small sample size because it requires o (m 2 ) of memory and 0 (N . m2 ) computational steps where m is the number of training patterns and N is the number of random draws from the posterior distribution. In this paper we present a method based on the simple perceptron learning algorithm which allows to overcome this algorithmic drawback. The method is algorithmically simple and is easily extended to the multi-class case. We present experimental results on the MNIST data set of handwritten digits which show that Bayes point machines (BPMs) are competitive with the current world champion, the support vector machine. In addition, the computational complexity of BPMs can be tuned by varying the number of samples from the posterior. Finally, rejecting test points on the basis of their (approximative) posterior probability leads to a rapid decrease in generalisation error, e.g. 0.1% generalisation error for a given rejection rate of 10%. 1</p><p>4 0.66411424 <a title="9-lsi-4" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>Author: Thore Graepel, Ralf Herbrich</p><p>Abstract: We present an algorithm that samples the hypothesis space of kernel classifiers. Given a uniform prior over normalised weight vectors and a likelihood based on a model of label noise leads to a piecewise constant posterior that can be sampled by the kernel Gibbs sampler (KGS). The KGS is a Markov Chain Monte Carlo method that chooses a random direction in parameter space and samples from the resulting piecewise constant density along the line chosen. The KGS can be used as an analytical tool for the exploration of Bayesian transduction, Bayes point machines, active learning, and evidence-based model selection on small data sets that are contaminated with label noise. For a simple toy example we demonstrate experimentally how a Bayes point machine based on the KGS outperforms an SVM that is incapable of taking into account label noise. 1</p><p>5 0.53121883 <a title="9-lsi-5" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>Author: Shie Mannor, Ron Meir</p><p>Abstract: The problem of constructing weak classifiers for boosting algorithms is studied. We present an algorithm that produces a linear classifier that is guaranteed to achieve an error better than random guessing for any distribution on the data. While this weak learner is not useful for learning in general, we show that under reasonable conditions on the distribution it yields an effective weak learner for one-dimensional problems. Preliminary simulations suggest that similar behavior can be expected in higher dimensions, a result which is corroborated by some recent theoretical bounds. Additionally, we provide improved convergence rate bounds for the generalization error in situations where the empirical error can be made small, which is exactly the situation that occurs if weak learners with guaranteed performance that is better than random guessing can be established. 1</p><p>6 0.52201223 <a title="9-lsi-6" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>7 0.49406016 <a title="9-lsi-7" href="./nips-2000-Algorithmic_Stability_and_Generalization_Performance.html">21 nips-2000-Algorithmic Stability and Generalization Performance</a></p>
<p>8 0.48458046 <a title="9-lsi-8" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<p>9 0.48209545 <a title="9-lsi-9" href="./nips-2000-Some_New_Bounds_on_the_Generalization_Error_of_Combined_Classifiers.html">119 nips-2000-Some New Bounds on the Generalization Error of Combined Classifiers</a></p>
<p>10 0.45946413 <a title="9-lsi-10" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>11 0.38447335 <a title="9-lsi-11" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>12 0.3819764 <a title="9-lsi-12" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>13 0.37726846 <a title="9-lsi-13" href="./nips-2000-Efficient_Learning_of_Linear_Perceptrons.html">44 nips-2000-Efficient Learning of Linear Perceptrons</a></p>
<p>14 0.35630795 <a title="9-lsi-14" href="./nips-2000-Regularized_Winnow_Methods.html">111 nips-2000-Regularized Winnow Methods</a></p>
<p>15 0.3374435 <a title="9-lsi-15" href="./nips-2000-Active_Support_Vector_Machine_Classification.html">18 nips-2000-Active Support Vector Machine Classification</a></p>
<p>16 0.33129933 <a title="9-lsi-16" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>17 0.32358065 <a title="9-lsi-17" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>18 0.30352968 <a title="9-lsi-18" href="./nips-2000-A_Mathematical_Programming_Approach_to_the_Kernel_Fisher_Algorithm.html">5 nips-2000-A Mathematical Programming Approach to the Kernel Fisher Algorithm</a></p>
<p>19 0.28759527 <a title="9-lsi-19" href="./nips-2000-Minimum_Bayes_Error_Feature_Selection_for_Continuous_Speech_Recognition.html">84 nips-2000-Minimum Bayes Error Feature Selection for Continuous Speech Recognition</a></p>
<p>20 0.28536299 <a title="9-lsi-20" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.266), (17, 0.12), (32, 0.021), (33, 0.125), (36, 0.016), (45, 0.088), (48, 0.011), (55, 0.015), (62, 0.04), (67, 0.05), (76, 0.052), (81, 0.011), (90, 0.074), (97, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92434645 <a title="9-lda-1" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>Author: Dirk Ormoneit, Peter W. Glynn</p><p>Abstract: Many approaches to reinforcement learning combine neural networks or other parametric function approximators with a form of temporal-difference learning to estimate the value function of a Markov Decision Process. A significant disadvantage of those procedures is that the resulting learning algorithms are frequently unstable. In this work, we present a new, kernel-based approach to reinforcement learning which overcomes this difficulty and provably converges to a unique solution. By contrast to existing algorithms, our method can also be shown to be consistent in the sense that its costs converge to the optimal costs asymptotically. Our focus is on learning in an average-cost framework and on a practical application to the optimal portfolio choice problem. 1</p><p>same-paper 2 0.90515614 <a title="9-lda-2" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: We present a bound on the generalisation error of linear classifiers in terms of a refined margin quantity on the training set. The result is obtained in a PAC- Bayesian framework and is based on geometrical arguments in the space of linear classifiers. The new bound constitutes an exponential improvement of the so far tightest margin bound by Shawe-Taylor et al. [8] and scales logarithmically in the inverse margin. Even in the case of less training examples than input dimensions sufficiently large margins lead to non-trivial bound values and - for maximum margins - to a vanishing complexity term. Furthermore, the classical margin is too coarse a measure for the essential quantity that controls the generalisation error: the volume ratio between the whole hypothesis space and the subset of consistent hypotheses. The practical relevance of the result lies in the fact that the well-known support vector machine is optimal w.r.t. the new bound only if the feature vectors are all of the same length. As a consequence we recommend to use SVMs on normalised feature vectors only - a recommendation that is well supported by our numerical experiments on two benchmark data sets. 1</p><p>3 0.90493202 <a title="9-lda-3" href="./nips-2000-Vicinal_Risk_Minimization.html">144 nips-2000-Vicinal Risk Minimization</a></p>
<p>Author: Olivier Chapelle, Jason Weston, Léon Bottou, Vladimir Vapnik</p><p>Abstract: The Vicinal Risk Minimization principle establishes a bridge between generative models and methods derived from the Structural Risk Minimization Principle such as Support Vector Machines or Statistical Regularization. We explain how VRM provides a framework which integrates a number of existing algorithms, such as Parzen windows, Support Vector Machines, Ridge Regression, Constrained Logistic Classifiers and Tangent-Prop. We then show how the approach implies new algorithms for solving problems usually associated with generative models. New algorithms are described for dealing with pattern recognition problems with very different pattern distributions and dealing with unlabeled data. Preliminary empirical results are presented.</p><p>4 0.83763719 <a title="9-lda-4" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>Author: Zhaoping Li, Peter Dayan</p><p>Abstract: Stimulus arrays are inevitably presented at different positions on the retina in visual tasks, even those that nominally require fixation. In particular, this applies to many perceptual learning tasks. We show that perceptual inference or discrimination in the face of positional variance has a structurally different quality from inference about fixed position stimuli, involving a particular, quadratic, non-linearity rather than a purely linear discrimination. We show the advantage taking this non-linearity into account has for discrimination, and suggest it as a role for recurrent connections in area VI, by demonstrating the superior discrimination performance of a recurrent network. We propose that learning the feedforward and recurrent neural connections for these tasks corresponds to the fast and slow components of learning observed in perceptual learning tasks.</p><p>5 0.75269341 <a title="9-lda-5" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: The concept of averaging over classifiers is fundamental to the Bayesian analysis of learning. Based on this viewpoint, it has recently been demonstrated for linear classifiers that the centre of mass of version space (the set of all classifiers consistent with the training set) - also known as the Bayes point - exhibits excellent generalisation abilities. However, the billiard algorithm as presented in [4] is restricted to small sample size because it requires o (m 2 ) of memory and 0 (N . m2 ) computational steps where m is the number of training patterns and N is the number of random draws from the posterior distribution. In this paper we present a method based on the simple perceptron learning algorithm which allows to overcome this algorithmic drawback. The method is algorithmically simple and is easily extended to the multi-class case. We present experimental results on the MNIST data set of handwritten digits which show that Bayes point machines (BPMs) are competitive with the current world champion, the support vector machine. In addition, the computational complexity of BPMs can be tuned by varying the number of samples from the posterior. Finally, rejecting test points on the basis of their (approximative) posterior probability leads to a rapid decrease in generalisation error, e.g. 0.1% generalisation error for a given rejection rate of 10%. 1</p><p>6 0.68512017 <a title="9-lda-6" href="./nips-2000-Keeping_Flexible_Active_Contours_on_Track_using_Metropolis_Updates.html">72 nips-2000-Keeping Flexible Active Contours on Track using Metropolis Updates</a></p>
<p>7 0.6803515 <a title="9-lda-7" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>8 0.67211658 <a title="9-lda-8" href="./nips-2000-Some_New_Bounds_on_the_Generalization_Error_of_Combined_Classifiers.html">119 nips-2000-Some New Bounds on the Generalization Error of Combined Classifiers</a></p>
<p>9 0.66081172 <a title="9-lda-9" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>10 0.64120442 <a title="9-lda-10" href="./nips-2000-From_Margin_to_Sparsity.html">58 nips-2000-From Margin to Sparsity</a></p>
<p>11 0.63797683 <a title="9-lda-11" href="./nips-2000-Algorithmic_Stability_and_Generalization_Performance.html">21 nips-2000-Algorithmic Stability and Generalization Performance</a></p>
<p>12 0.63675833 <a title="9-lda-12" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>13 0.63433665 <a title="9-lda-13" href="./nips-2000-Regularized_Winnow_Methods.html">111 nips-2000-Regularized Winnow Methods</a></p>
<p>14 0.62685096 <a title="9-lda-14" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>15 0.62324041 <a title="9-lda-15" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>16 0.61881334 <a title="9-lda-16" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>17 0.60836017 <a title="9-lda-17" href="./nips-2000-Efficient_Learning_of_Linear_Perceptrons.html">44 nips-2000-Efficient Learning of Linear Perceptrons</a></p>
<p>18 0.60761666 <a title="9-lda-18" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>19 0.6051482 <a title="9-lda-19" href="./nips-2000-%60N-Body%27_Problems_in_Statistical_Learning.html">148 nips-2000-`N-Body' Problems in Statistical Learning</a></p>
<p>20 0.6037274 <a title="9-lda-20" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
