<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>122 nips-2000-Sparse Representation for Gaussian Process Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-122" href="#">nips2000-122</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>122 nips-2000-Sparse Representation for Gaussian Process Models</h1>
<br/><p>Source: <a title="nips-2000-122-pdf" href="http://papers.nips.cc/paper/1893-sparse-representation-for-gaussian-process-models.pdf">pdf</a></p><p>Author: Lehel Csatč´¸, Manfred Opper</p><p>Abstract: We develop an approach for a sparse representation for Gaussian Process (GP) models in order to overcome the limitations of GPs caused by large data sets. The method is based on a combination of a Bayesian online algorithm together with a sequential construction of a relevant subsample of the data which fully specifies the prediction of the model. Experimental results on toy examples and large real-world data sets indicate the efficiency of the approach.</p><p>Reference: <a title="nips-2000-122-reference" href="../nips2000_reference/nips-2000-Sparse_Representation_for_Gaussian_Process_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract We develop an approach for a sparse representation for Gaussian Process (GP) models in order to overcome the limitations of GPs caused by large data sets. [sent-3, score-0.441]
</p><p>2 The method is based on a combination of a Bayesian online algorithm together with a sequential construction of a relevant subsample of the data which fully specifies the prediction of the model. [sent-4, score-0.618]
</p><p>3 Experimental results on toy examples and large real-world data sets indicate the efficiency of the approach. [sent-5, score-0.101]
</p><p>4 1 Introduction Gaussian processes (GP) [1; 15] provide promising non-parametric tools for modelling real-world statistical problems. [sent-6, score-0.219]
</p><p>5 Support Vector Machines (SVMs) [13], they combine a high flexibility ofthe model by working in high (often 00) dimensional feature spaces with the simplicity that all operations are "kernelized" i. [sent-9, score-0.128]
</p><p>6 they are performed in the (lower dimensional) input space using positive definite kernels. [sent-11, score-0.098]
</p><p>7 An important advantage of GPs over other non-Bayesian models is the explicit probabilistic formulation of the model. [sent-12, score-0.128]
</p><p>8 This does not only provide the modeller with (Bayesian) confidence intervals (for regression) or posterior class probabilities (for classification) but also immediately opens the possibility to treat other nonstandard data models (e. [sent-13, score-0.545]
</p><p>9 Unfortunately the drawback of GP models (which was originally apparent in SVMs as well, but has now been overcome [6]) lies in the huge increase of the computational cost with the number of training data. [sent-16, score-0.505]
</p><p>10 This seems to preclude applications of GPs to large datasets. [sent-17, score-0.137]
</p><p>11 This paper presents an approach to overcome this problem. [sent-18, score-0.197]
</p><p>12 It is based on a combination of an online learning approach requiring only a single sweep through the data and a method to reduce the number of parameters representing the model. [sent-19, score-0.477]
</p><p>13 Making use of the proposed parametrisation the method extracts a subset of the examples and the prediction relies only on these basis vectors (BV). [sent-20, score-0.179]
</p><p>14 The memory requirement of the algorithm scales thus only with the size of this set. [sent-21, score-0.054]
</p><p>15 Experiments with real-world datasets confirm the good performance of the proposed method. [sent-22, score-0.161]
</p><p>16 1 1A  different approach for dealing with large datasets was suggested by V. [sent-23, score-0.216]
</p><p>17 His method  2  Gaussian Process Models  GPs belong to Bayesian non-parametric models where likelihoods are parametrised by a Gaussian stochastic process (random field) a(x) which is indexed by the continuous input variable x . [sent-25, score-0.556]
</p><p>18 The prior knowledge about a is expressed in the prior mean and the covariance given by the kernel Ko(x,x') = Cov(a(x), a(x')) [14; 15]. [sent-26, score-0.276]
</p><p>19 In supervised learning the process a(x) is used as a latent variable in the likelihood P(yla(x)) which denotes the probability of output Y given the input x . [sent-28, score-0.333]
</p><p>20 Based on a set of input-output pairs (xn, Yn) with Xn E R m and Yn E R (n = 1, N) the Bayesian learning method computes the posterior distribution of the process a(x) using the prior and likelihood [14; 15; 3]. [sent-29, score-0.556]
</p><p>21 Although the prior is a Gaussian process, the posterior process usually is not Gaussian (except for the special case of regression with Gaussian noise). [sent-30, score-0.587]
</p><p>22 Nevertheless, various approaches have been introduced recently to approximate the posterior averages [11 ; 9]. [sent-31, score-0.257]
</p><p>23 Our approach is based on the idea of approximating the true posterior process p{ a} by a Gaussian process q{a} which is fully specified by a covariance kernel Kt(x,x') and posterior mean (a(x))t, where t is the number of training data processed by the algorithm so far. [sent-32, score-1.215]
</p><p>24 Such an approximation could be formulated within the variational approach, where q is chosen such that the relative entropy D(q,p) == Eq In ~ is minimal [9]. [sent-33, score-0.11]
</p><p>25 However, in this formulation, the expectation is over the approximate process q rather than over p. [sent-34, score-0.293]
</p><p>26 It seems intuitively better to minimise the other KL divergence given by D(p, q) == Ep In ~, because the expectation is over the true distribution. [sent-35, score-0.266]
</p><p>27 The following online approach can be understood as an approximation to this task. [sent-37, score-0.331]
</p><p>28 3  Online learning for Gaussian Processes  In this section we briefly review the main idea of the Bayesian online approach (see e. [sent-38, score-0.269]
</p><p>29 We process the training data sequentially one after the other. [sent-41, score-0.29]
</p><p>30 Assume we have a Gaussian approximation to the posterior process at time t. [sent-42, score-0.506]
</p><p>31 We use the next example t + 1 to update the posterior using Bayes rule via  p(a) = P(Yt+1la(Xt+l))Pt(q) (P(Yt+1la(xt+1)))t Since the resulting posterior p(q) is non-Gaussian, we project it to the closest Gaussian process q which minimises the KL divergence D(p, q). [sent-43, score-0.881]
</p><p>32 Note, that now the new approximation q is on "correct" side of the KL divergence. [sent-44, score-0.062]
</p><p>33 The minimisation can be performed exactly, leading to a match of the means and covariances of p and q. [sent-45, score-0.175]
</p><p>34 Derivatives are is based on splitting the data-set into smaller subsets and training individual GP predictors on each of them. [sent-47, score-0.269]
</p><p>35 The final prediction is achieved by a specific weighting of the individual predictors. [sent-48, score-0.168]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gp', 0.351), ('gps', 0.277), ('process', 0.232), ('kt', 0.223), ('posterior', 0.212), ('online', 0.207), ('gaussian', 0.166), ('predictors', 0.165), ('xt', 0.15), ('kl', 0.14), ('overcome', 0.135), ('yt', 0.112), ('bayesian', 0.11), ('datasets', 0.096), ('yn', 0.093), ('svms', 0.09), ('divergence', 0.085), ('kernel', 0.083), ('minimises', 0.082), ('preclude', 0.082), ('yla', 0.082), ('bv', 0.082), ('huge', 0.082), ('prediction', 0.077), ('regression', 0.075), ('ko', 0.075), ('birmingham', 0.075), ('quantum', 0.075), ('manfred', 0.075), ('tresp', 0.075), ('subsample', 0.075), ('dimensional', 0.074), ('fully', 0.071), ('kingdom', 0.069), ('opens', 0.069), ('cov', 0.069), ('unfortunately', 0.068), ('prior', 0.068), ('models', 0.067), ('sparse', 0.067), ('minimise', 0.065), ('confirm', 0.065), ('eq', 0.065), ('processes', 0.062), ('approximation', 0.062), ('approach', 0.062), ('originally', 0.061), ('minimisation', 0.061), ('united', 0.061), ('opper', 0.061), ('sweep', 0.061), ('covariances', 0.061), ('ep', 0.061), ('formulation', 0.061), ('expectation', 0.061), ('xn', 0.059), ('splitting', 0.058), ('sequentially', 0.058), ('closest', 0.058), ('extracts', 0.058), ('immediately', 0.058), ('dealing', 0.058), ('covariance', 0.057), ('apparent', 0.056), ('limitations', 0.056), ('tools', 0.056), ('moments', 0.056), ('toy', 0.056), ('likelihoods', 0.056), ('seems', 0.055), ('processed', 0.054), ('drawback', 0.054), ('flexibility', 0.054), ('promising', 0.054), ('caused', 0.054), ('requirement', 0.054), ('requiring', 0.054), ('performed', 0.053), ('variable', 0.053), ('belong', 0.052), ('indexed', 0.052), ('nevertheless', 0.052), ('pt', 0.05), ('lies', 0.05), ('specifies', 0.05), ('combination', 0.049), ('formulated', 0.048), ('latent', 0.048), ('confidence', 0.047), ('modelling', 0.047), ('scalar', 0.047), ('treat', 0.047), ('individual', 0.046), ('sequential', 0.045), ('efficiency', 0.045), ('averages', 0.045), ('definite', 0.045), ('weighting', 0.045), ('intervals', 0.045), ('method', 0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="122-tfidf-1" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>Author: Lehel Csatč´¸, Manfred Opper</p><p>Abstract: We develop an approach for a sparse representation for Gaussian Process (GP) models in order to overcome the limitations of GPs caused by large data sets. The method is based on a combination of a Bayesian online algorithm together with a sequential construction of a relevant subsample of the data which fully specifies the prediction of the model. Experimental results on toy examples and large real-world data sets indicate the efficiency of the approach.</p><p>2 0.2718972 <a title="122-tfidf-2" href="./nips-2000-Computing_with_Finite_and_Infinite_Networks.html">35 nips-2000-Computing with Finite and Infinite Networks</a></p>
<p>Author: Ole Winther</p><p>Abstract: Using statistical mechanics results, I calculate learning curves (average generalization error) for Gaussian processes (GPs) and Bayesian neural networks (NNs) used for regression. Applying the results to learning a teacher defined by a two-layer network, I can directly compare GP and Bayesian NN learning. I find that a GP in general requires CJ (d S )-training examples to learn input features of order s (d is the input dimension), whereas a NN can learn the task with order the number of adjustable weights training examples. Since a GP can be considered as an infinite NN, the results show that even in the Bayesian approach, it is important to limit the complexity of the learning machine. The theoretical findings are confirmed in simulations with analytical GP learning and a NN mean field algorithm.</p><p>3 0.19099458 <a title="122-tfidf-3" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: Based on a statistical mechanics approach, we develop a method for approximately computing average case learning curves for Gaussian process regression models. The approximation works well in the large sample size limit and for arbitrary dimensionality of the input space. We explain how the approximation can be systematically improved and argue that similar techniques can be applied to general likelihood models. 1</p><p>4 0.18070056 <a title="122-tfidf-4" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>Author: Alex J. Smola, Peter L. Bartlett</p><p>Abstract: We present a simple sparse greedy technique to approximate the maximum a posteriori estimate of Gaussian Processes with much improved scaling behaviour in the sample size m. In particular, computational requirements are O(n 2 m), storage is O(nm), the cost for prediction is 0 (n) and the cost to compute confidence bounds is O(nm), where n «: m. We show how to compute a stopping criterion, give bounds on the approximation error, and show applications to large scale problems. 1</p><p>5 0.14134574 <a title="122-tfidf-5" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: The concept of averaging over classifiers is fundamental to the Bayesian analysis of learning. Based on this viewpoint, it has recently been demonstrated for linear classifiers that the centre of mass of version space (the set of all classifiers consistent with the training set) - also known as the Bayes point - exhibits excellent generalisation abilities. However, the billiard algorithm as presented in [4] is restricted to small sample size because it requires o (m 2 ) of memory and 0 (N . m2 ) computational steps where m is the number of training patterns and N is the number of random draws from the posterior distribution. In this paper we present a method based on the simple perceptron learning algorithm which allows to overcome this algorithmic drawback. The method is algorithmically simple and is easily extended to the multi-class case. We present experimental results on the MNIST data set of handwritten digits which show that Bayes point machines (BPMs) are competitive with the current world champion, the support vector machine. In addition, the computational complexity of BPMs can be tuned by varying the number of samples from the posterior. Finally, rejecting test points on the basis of their (approximative) posterior probability leads to a rapid decrease in generalisation error, e.g. 0.1% generalisation error for a given rejection rate of 10%. 1</p><p>6 0.13801908 <a title="122-tfidf-6" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>7 0.1122117 <a title="122-tfidf-7" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>8 0.10645082 <a title="122-tfidf-8" href="./nips-2000-Sparse_Kernel_Principal_Component_Analysis.html">121 nips-2000-Sparse Kernel Principal Component Analysis</a></p>
<p>9 0.096735582 <a title="122-tfidf-9" href="./nips-2000-Mixtures_of_Gaussian_Processes.html">85 nips-2000-Mixtures of Gaussian Processes</a></p>
<p>10 0.093308382 <a title="122-tfidf-10" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>11 0.089327425 <a title="122-tfidf-11" href="./nips-2000-Tree-Based_Modeling_and_Estimation_of_Gaussian_Processes_on_Graphs_with_Cycles.html">140 nips-2000-Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles</a></p>
<p>12 0.080139771 <a title="122-tfidf-12" href="./nips-2000-Learning_Continuous_Distributions%3A_Simulations_With_Field_Theoretic_Priors.html">76 nips-2000-Learning Continuous Distributions: Simulations With Field Theoretic Priors</a></p>
<p>13 0.077767558 <a title="122-tfidf-13" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>14 0.076866433 <a title="122-tfidf-14" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>15 0.073539421 <a title="122-tfidf-15" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>16 0.070679806 <a title="122-tfidf-16" href="./nips-2000-The_Unscented_Particle_Filter.html">137 nips-2000-The Unscented Particle Filter</a></p>
<p>17 0.07020513 <a title="122-tfidf-17" href="./nips-2000-The_Kernel_Trick_for_Distances.html">134 nips-2000-The Kernel Trick for Distances</a></p>
<p>18 0.070021547 <a title="122-tfidf-18" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>19 0.069804311 <a title="122-tfidf-19" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>20 0.067625515 <a title="122-tfidf-20" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.27), (1, 0.099), (2, 0.075), (3, 0.009), (4, 0.142), (5, 0.142), (6, -0.097), (7, 0.096), (8, -0.011), (9, -0.271), (10, 0.153), (11, 0.114), (12, 0.012), (13, -0.09), (14, 0.018), (15, 0.1), (16, -0.159), (17, 0.103), (18, 0.061), (19, -0.066), (20, 0.003), (21, -0.176), (22, 0.282), (23, 0.187), (24, -0.013), (25, -0.034), (26, -0.066), (27, -0.006), (28, -0.053), (29, 0.018), (30, -0.035), (31, 0.042), (32, 0.036), (33, -0.022), (34, 0.097), (35, 0.14), (36, -0.017), (37, -0.026), (38, 0.031), (39, -0.08), (40, 0.006), (41, 0.067), (42, -0.045), (43, 0.041), (44, -0.043), (45, -0.014), (46, -0.017), (47, -0.116), (48, -0.026), (49, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97592378 <a title="122-lsi-1" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>Author: Lehel Csatč´¸, Manfred Opper</p><p>Abstract: We develop an approach for a sparse representation for Gaussian Process (GP) models in order to overcome the limitations of GPs caused by large data sets. The method is based on a combination of a Bayesian online algorithm together with a sequential construction of a relevant subsample of the data which fully specifies the prediction of the model. Experimental results on toy examples and large real-world data sets indicate the efficiency of the approach.</p><p>2 0.81771523 <a title="122-lsi-2" href="./nips-2000-Computing_with_Finite_and_Infinite_Networks.html">35 nips-2000-Computing with Finite and Infinite Networks</a></p>
<p>Author: Ole Winther</p><p>Abstract: Using statistical mechanics results, I calculate learning curves (average generalization error) for Gaussian processes (GPs) and Bayesian neural networks (NNs) used for regression. Applying the results to learning a teacher defined by a two-layer network, I can directly compare GP and Bayesian NN learning. I find that a GP in general requires CJ (d S )-training examples to learn input features of order s (d is the input dimension), whereas a NN can learn the task with order the number of adjustable weights training examples. Since a GP can be considered as an infinite NN, the results show that even in the Bayesian approach, it is important to limit the complexity of the learning machine. The theoretical findings are confirmed in simulations with analytical GP learning and a NN mean field algorithm.</p><p>3 0.69499052 <a title="122-lsi-3" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: Based on a statistical mechanics approach, we develop a method for approximately computing average case learning curves for Gaussian process regression models. The approximation works well in the large sample size limit and for arbitrary dimensionality of the input space. We explain how the approximation can be systematically improved and argue that similar techniques can be applied to general likelihood models. 1</p><p>4 0.62464499 <a title="122-lsi-4" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>Author: Alex J. Smola, Peter L. Bartlett</p><p>Abstract: We present a simple sparse greedy technique to approximate the maximum a posteriori estimate of Gaussian Processes with much improved scaling behaviour in the sample size m. In particular, computational requirements are O(n 2 m), storage is O(nm), the cost for prediction is 0 (n) and the cost to compute confidence bounds is O(nm), where n «: m. We show how to compute a stopping criterion, give bounds on the approximation error, and show applications to large scale problems. 1</p><p>5 0.56923932 <a title="122-lsi-5" href="./nips-2000-Mixtures_of_Gaussian_Processes.html">85 nips-2000-Mixtures of Gaussian Processes</a></p>
<p>Author: Volker Tresp</p><p>Abstract: We introduce the mixture of Gaussian processes (MGP) model which is useful for applications in which the optimal bandwidth of a map is input dependent. The MGP is derived from the mixture of experts model and can also be used for modeling general conditional probability densities. We discuss how Gaussian processes -in particular in form of Gaussian process classification, the support vector machine and the MGP modelcan be used for quantifying the dependencies in graphical models.</p><p>6 0.40637574 <a title="122-lsi-6" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>7 0.40280211 <a title="122-lsi-7" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>8 0.39336696 <a title="122-lsi-8" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>9 0.37727976 <a title="122-lsi-9" href="./nips-2000-Tree-Based_Modeling_and_Estimation_of_Gaussian_Processes_on_Graphs_with_Cycles.html">140 nips-2000-Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles</a></p>
<p>10 0.34930748 <a title="122-lsi-10" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>11 0.32446381 <a title="122-lsi-11" href="./nips-2000-Ensemble_Learning_and_Linear_Response_Theory_for_ICA.html">46 nips-2000-Ensemble Learning and Linear Response Theory for ICA</a></p>
<p>12 0.31628934 <a title="122-lsi-12" href="./nips-2000-A_Mathematical_Programming_Approach_to_the_Kernel_Fisher_Algorithm.html">5 nips-2000-A Mathematical Programming Approach to the Kernel Fisher Algorithm</a></p>
<p>13 0.30984917 <a title="122-lsi-13" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>14 0.30652404 <a title="122-lsi-14" href="./nips-2000-Learning_Continuous_Distributions%3A_Simulations_With_Field_Theoretic_Priors.html">76 nips-2000-Learning Continuous Distributions: Simulations With Field Theoretic Priors</a></p>
<p>15 0.29793626 <a title="122-lsi-15" href="./nips-2000-Feature_Correspondence%3A_A_Markov_Chain_Monte_Carlo_Approach.html">53 nips-2000-Feature Correspondence: A Markov Chain Monte Carlo Approach</a></p>
<p>16 0.29703459 <a title="122-lsi-16" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>17 0.28023604 <a title="122-lsi-17" href="./nips-2000-Sparse_Kernel_Principal_Component_Analysis.html">121 nips-2000-Sparse Kernel Principal Component Analysis</a></p>
<p>18 0.27869755 <a title="122-lsi-18" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>19 0.274831 <a title="122-lsi-19" href="./nips-2000-Sequentially_Fitting_%60%60Inclusive%27%27_Trees_for_Inference_in_Noisy-OR_Networks.html">115 nips-2000-Sequentially Fitting ``Inclusive'' Trees for Inference in Noisy-OR Networks</a></p>
<p>20 0.27042878 <a title="122-lsi-20" href="./nips-2000-Factored_Semi-Tied_Covariance_Matrices.html">51 nips-2000-Factored Semi-Tied Covariance Matrices</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.042), (17, 0.144), (33, 0.031), (54, 0.081), (55, 0.045), (62, 0.041), (67, 0.067), (75, 0.025), (76, 0.089), (81, 0.043), (88, 0.212), (90, 0.055), (91, 0.011), (97, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83124763 <a title="122-lda-1" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>Author: Lehel Csatč´¸, Manfred Opper</p><p>Abstract: We develop an approach for a sparse representation for Gaussian Process (GP) models in order to overcome the limitations of GPs caused by large data sets. The method is based on a combination of a Bayesian online algorithm together with a sequential construction of a relevant subsample of the data which fully specifies the prediction of the model. Experimental results on toy examples and large real-world data sets indicate the efficiency of the approach.</p><p>2 0.67801028 <a title="122-lda-2" href="./nips-2000-Tree-Based_Modeling_and_Estimation_of_Gaussian_Processes_on_Graphs_with_Cycles.html">140 nips-2000-Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles</a></p>
<p>Author: Martin J. Wainwright, Erik B. Sudderth, Alan S. Willsky</p><p>Abstract: We present the embedded trees algorithm, an iterative technique for estimation of Gaussian processes defined on arbitrary graphs. By exactly solving a series of modified problems on embedded spanning trees, it computes the conditional means with an efficiency comparable to or better than other techniques. Unlike other methods, the embedded trees algorithm also computes exact error covariances. The error covariance computation is most efficient for graphs in which removing a small number of edges reveals an embedded tree. In this context, we demonstrate that sparse loopy graphs can provide a significant increase in modeling power relative to trees, with only a minor increase in estimation complexity. 1</p><p>3 0.67119902 <a title="122-lda-3" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>Author: Shimon Edelman, Nathan Intrator</p><p>Abstract: We describe a unified framework for the understanding of structure representation in primate vision. A model derived from this framework is shown to be effectively systematic in that it has the ability to interpret and associate together objects that are related through a rearrangement of common</p><p>4 0.65754431 <a title="122-lda-4" href="./nips-2000-Computing_with_Finite_and_Infinite_Networks.html">35 nips-2000-Computing with Finite and Infinite Networks</a></p>
<p>Author: Ole Winther</p><p>Abstract: Using statistical mechanics results, I calculate learning curves (average generalization error) for Gaussian processes (GPs) and Bayesian neural networks (NNs) used for regression. Applying the results to learning a teacher defined by a two-layer network, I can directly compare GP and Bayesian NN learning. I find that a GP in general requires CJ (d S )-training examples to learn input features of order s (d is the input dimension), whereas a NN can learn the task with order the number of adjustable weights training examples. Since a GP can be considered as an infinite NN, the results show that even in the Bayesian approach, it is important to limit the complexity of the learning machine. The theoretical findings are confirmed in simulations with analytical GP learning and a NN mean field algorithm.</p><p>5 0.63024974 <a title="122-lda-5" href="./nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">95 nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>Author: Christopher K. I. Williams</p><p>Abstract: In this paper we show that the kernel peA algorithm of Sch6lkopf et al (1998) can be interpreted as a form of metric multidimensional scaling (MDS) when the kernel function k(x, y) is isotropic, i.e. it depends only on Ilx - yll. This leads to a metric MDS algorithm where the desired configuration of points is found via the solution of an eigenproblem rather than through the iterative optimization of the stress objective function. The question of kernel choice is also discussed. 1</p><p>6 0.62731606 <a title="122-lda-6" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>7 0.62476498 <a title="122-lda-7" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>8 0.61965472 <a title="122-lda-8" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>9 0.61961347 <a title="122-lda-9" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<p>10 0.61936665 <a title="122-lda-10" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>11 0.61498725 <a title="122-lda-11" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>12 0.6146723 <a title="122-lda-12" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>13 0.61161339 <a title="122-lda-13" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>14 0.61104631 <a title="122-lda-14" href="./nips-2000-Algorithmic_Stability_and_Generalization_Performance.html">21 nips-2000-Algorithmic Stability and Generalization Performance</a></p>
<p>15 0.60991418 <a title="122-lda-15" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>16 0.60947102 <a title="122-lda-16" href="./nips-2000-Factored_Semi-Tied_Covariance_Matrices.html">51 nips-2000-Factored Semi-Tied Covariance Matrices</a></p>
<p>17 0.60944724 <a title="122-lda-17" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>18 0.60922718 <a title="122-lda-18" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>19 0.60759318 <a title="122-lda-19" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>20 0.60692686 <a title="122-lda-20" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
