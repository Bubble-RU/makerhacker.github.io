<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>93 nips-2000-On Iterative Krylov-Dogleg Trust-Region Steps for Solving Neural Networks Nonlinear Least Squares Problems</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-93" href="#">nips2000-93</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>93 nips-2000-On Iterative Krylov-Dogleg Trust-Region Steps for Solving Neural Networks Nonlinear Least Squares Problems</h1>
<br/><p>Source: <a title="nips-2000-93-pdf" href="http://papers.nips.cc/paper/1805-on-iterative-krylov-dogleg-trust-region-steps-for-solving-neural-networks-nonlinear-least-squares-problems.pdf">pdf</a></p><p>Author: Eiji Mizutani, James Demmel</p><p>Abstract: This paper describes a method of dogleg trust-region steps, or restricted Levenberg-Marquardt steps, based on a projection process onto the Krylov subspaces for neural networks nonlinear least squares problems. In particular, the linear conjugate gradient (CG) method works as the inner iterative algorithm for solving the linearized Gauss-Newton normal equation, whereas the outer nonlinear algorithm repeatedly takes so-called</p><p>Reference: <a title="nips-2000-93-reference" href="../nips2000_reference/nips-2000-On_Iterative_Krylov-Dogleg_Trust-Region_Steps_for_Solving_Neural_Networks_Nonlinear_Least_Squares_Problems_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 On iterative Krylov-dogleg trust-region steps for solving neural networks nonlinear least squares problems  Eiji Mizutani Department of Computer Science National Tsing Hua University Hsinchu, 30043 TAIWAN R. [sent-1, score-0.485]
</p><p>2 edu  Abstract This paper describes a method of dogleg trust-region steps, or restricted Levenberg-Marquardt steps, based on a projection process onto the Krylov subspaces for neural networks nonlinear least squares problems. [sent-11, score-0.934]
</p><p>3 That is, our iterative dogleg algorithm can reduce both operational counts and memory space by a factor of O(n) (the number of parameters) in comparison with a direct linear-equation solver. [sent-13, score-1.122]
</p><p>4 1  Introduction  We consider the so-called n e ural networks nonlinear least squares problem 1 wherein the objective is to optimize the n weight parameters of neural networks (NN) [e. [sent-15, score-0.273]
</p><p>5 , multilayer perceptrons (MLP)]' denoted by an n-dimensional vector 8 , by minimizing the following: (1)  where a p (8) is the MLP output for the pth training data pattern and tp is the desired output. [sent-17, score-0.04]
</p><p>6 ) Here r(8) denotes the m-dimensional residual vector composed of ri(8) , i = 1, . [sent-19, score-0.064]
</p><p>7 Our algorithm exploits the special structure of the sum of squared error measure in Equation (1); hence , the other objective functions are outside the scope of this paper. [sent-24, score-0.061]
</p><p>8 The gradient vector and Hessian matrix are given by g = g(9) == JT rand H = H( 9) == JT J +S, where J is the m x n Jacobian matrix of r, and S denotes the matrix of second-derivative terms. [sent-25, score-0.148]
</p><p>9 If S is simply omitted based on the "small residual" assumption, then the Hessian matrix reduces to the Gauss-Newton model Hessian: i. [sent-26, score-0.033]
</p><p>10 Furthermore, a family of quasi-Newton methods can be applied to approximate term S alone, leading to the augmented Gauss-Newton model Hessian (see, for example, Mizutani [2] and references therein). [sent-29, score-0.049]
</p><p>11 With any form of the aforementioned Hessian matrices, we can collectively write the following Newton formula to determine the next step lj in the course of the Newton iteration for 9 next = 9 now + lj:  (2)  Hlj = -g. [sent-30, score-0.214]
</p><p>12 This linear system can be solved by a direct solver in conjunction with a suitable matrix factorization. [sent-31, score-0.211]
</p><p>13 However, typical criticisms towards the direct algorithm are: • It is expensive to form and solve the linear equation (2), which requires O(mn 2 ) operations when m > n; • It is expensive to store the (symmetric) Hessian matrix H, which requires n(n2 +1) memory storage. [sent-32, score-0.32]
</p><p>14 In light of the vast literature on the nonlinear optimization, this paper describes how to alleviate these concerns, attempting to solve the Newton formula (2) approximately by iterative methods, which form a family of inexact (or truncated) Newton methods (see Dembo & Steihaug [3], for instance). [sent-34, score-0.365]
</p><p>15 An important subclass ofthe inexact Newton methods are Newton-Krylov methods. [sent-35, score-0.04]
</p><p>16 They are based on a simple direct control ofthe Levenberg-Marquardt parameter J. [sent-38, score-0.135]
</p><p>17 Alternatively, a more efficient dogleg algorithm [10] can be employed that takes, depending on the size of trust region R, the Newton step ljNewton [i. [sent-43, score-0.9]
</p><p>18 (2)], the (restricted) Cauchy step ljCauchy' or an intermediate dogleg step: ljdogleg dcl ljCauchy =  + h ( ljNewton  - ljCauchy ) ,  (4)  which achieves a piecewise linear approximation to a trust-region step, or a restricted Levenberg-Marquardt step. [sent-46, score-0.799]
</p><p>19 Note that ljCauchy is the step that minimizes the local  quadratic model in the steepest descent direction (i. [sent-47, score-0.157]
</p><p>20 For details on Equation (4) , refer to Powell [10]; Mizutani [9 , 2]. [sent-51, score-0.022]
</p><p>21 Among those three direct solvers, approach (1) to Equation (3) is fastest. [sent-53, score-0.135]
</p><p>22 (For more details , refer to Demmel [11], Chapters 2 and 3. [sent-54, score-0.022]
</p><p>23 ) In a highly overdetermined case (with a large data set ; i. [sent-55, score-0.06]
</p><p>24 , m » n) , the dominant cost in approach (1) is the mn 2 operations to form the Gauss-Newton model Hessian by: m  JTJ  = LU;U;'  (5)  ;=1  where uT is the ith row vector of J. [sent-57, score-0.113]
</p><p>25 This cost might be prohibitive even with enough storage for JT J. [sent-58, score-0.025]
</p><p>26 Therefore, to overcome this limitation of direct solvers for Equation (3), we consider an iterative scheme in the next section. [sent-59, score-0.391]
</p><p>27 3  Iterative Krylov-Dogleg Algorithm  The iterative Krylov-dogleg step approximates a trust-region step by iteratively approximating the Levenberg-Marquardt trajectory in the Krylov subspace via linear conjugate gradient iterates until the approximate trajectory hits the trustregion boundary; i. [sent-60, score-0.619]
</p><p>28 In this context, the linear CGNR method is not intended to approximate the full GaussNewton step [i. [sent-63, score-0.105]
</p><p>29 Therefore, the required number of CGNRiterations might be kept small [see Section 4]. [sent-67, score-0.056]
</p><p>30 The iterative process for the linear-equation solution sequence {8 k } is called the inner 2 iteration, whereas the solution sequence {(h} from the Krylov-dogleg algorithm is generated by the outer iteration (or epoch), as shown in Figure 1. [sent-68, score-0.385]
</p><p>31 We now describe the inner iteration algorithm, which is identical to the standard linear CG algorithm (see Demmel [11], pages 311-312) except steps 2, 4, and 5: Algorithm 3. [sent-69, score-0.253]
</p><p>32 1 : The inner iteration of the Krylov-dogleg algorithm (see Figure 1). [sent-70, score-0.172]
</p><p>33 Thus, they ignore the special structure of the nonlinear least squares problem; so does Pearlmutter's method [15] to the Newton formula, although its modification may be possible. [sent-78, score-0.192]
</p><p>34 Q)  "S  o  E( 90 ) ",  IF  ~  E(  90~)  YES algorithm  :. [sent-130, score-0.061]
</p><p>35 :  IF  Vnow  ~ Vsmall  Algorithm for local-model check  Figure 1: The algorithmic flow of an iterative Krylov-dogleg algorithm. [sent-157, score-0.236]
</p><p>36 For detailed procedures in the three dotted rectangular boxes, refer to Mizu tani and Demmel [12} and Algorithm 3. [sent-158, score-0.022]
</p><p>37 Analytical step size:  'fJk  =  rL1rk- l dTz k  4. [sent-161, score-0.082]
</p><p>38 Approximate solution: If Illikll  <  li k = li k - 1 + 'fJk d k. [sent-162, score-0.048]
</p><p>39 R now , then go onto the next step 5; otherwise compute li k li k = Rnowillikll '  (8)  (9)  and terminate. [sent-163, score-0.13]
</p><p>40 If IIrkl1 2 is small enough , then set Rnow  f-  Illikll. [sent-166, score-0.025]
</p><p>41 The first step given by Equation (8) is always the Cauchy step I5Cauchy ' moving 9now to the Cauchy point 9Cauchy when Rnow > III5Cauchyll . [sent-174, score-0.164]
</p><p>42 Then, departing from 9 Cauchy , the linear CG constructs a Krylov-dogleg trajectory (by adding a CG point one by one) towards the Gauss-Newton point 9Newton until the constructed trajectory hits the trust-region boundary (i. [sent-175, score-0.12]
</p><p>43 , Ill5k ll :::: Rnow is satisfied in step 4), or till the linear-system residual becomes small in step 5 (unlikely to occur for small forcing terms; e. [sent-177, score-0.278]
</p><p>44 In this way, the algorithm computes a vector between the steepest descent direction and the Gauss-Newton direction, resulting in an approximate Levenberg-Marquardt step in the Krylov subspace. [sent-181, score-0.241]
</p><p>45 In step 2, the matrix-vector multiplication of Hdk in Equation (7) can be performed with neither the Jacobian nor Hessian matrices explicitly required, keeping only several n-dimensional vectors in memory at the same time , as shown next: Algorithm 3. [sent-182, score-0.191]
</p><p>46 , one sweep of all training data: (a) do forward propagation to compute the MLP output a; (9) for datum i; (b) do backpropagation 3 to obtain the ith row vector u T of matrix J; (c) compute (uT dk)u; and add it to z ;  end for. [sent-186, score-0.085]
</p><p>47 For one sweep of all m data, each of steps (a) and (b) costs at least 2mn (plus additional costs that depend on the MLP architectures) and step (c) [i. [sent-187, score-0.288]
</p><p>48 Hence, the overall cost of the inner iteration (Algorithm 3. [sent-191, score-0.136]
</p><p>49 1) can be kept as O(mn), especially when the number of inner iterations is small owing to our strategy of upper-bounded trust-region radii (e. [sent-192, score-0.153]
</p><p>50 For more details on the algorithm in Figure 1, refer to Mizutani and Demmel [12] . [sent-197, score-0.083]
</p><p>51 4  Experiments and Discussions  In the NN literature, there are numerous algorithmic comparisons available (see, for example , Moller [14] ; Demuth & Beale [6] ; Shepherd [8] ; Mizutani [2 ,9, 16]). [sent-198, score-0.032]
</p><p>52 Due to the space limitation, this section compares typical behaviors of our Krylov-dogleg Gauss-Newton CGNR (or iterative dogleg) algorithm and Powell's dogleg-based algorithm with a direct linear-equation solver (or direct dogleg) for solving highly overdetermined parity problems. [sent-199, score-0.837]
</p><p>53 In our numerical tests, we used a criterion, in which the MLP output for the pth pattern, ap , can be regarded as either "on" (1. [sent-200, score-0.04]
</p><p>54 Figure 2 presents MLP-Iearning curves in RMSE (root mean squared error) for the 20-bit and 14-bit parity problems. [sent-208, score-0.124]
</p><p>55 In (b) and (c), the total execution time [roughly (b) 32 days (500 epochs); (c) two hours (450 epochs), both on 299-MHz UltraSparc] of the direct dogleg algorithm was normalized for comparison purpose. [sent-209, score-1.01]
</p><p>56 Notably, the 3The batch-mode MLP backpropagation can be viewed as an efficient matrix-vector multiplication (2mn operations) for computing the graclient . [sent-210, score-0.107]
</p><p>57 JTr wilhoutfor'ming explicitly the m X n Jacobian matrix or the m-climensional residual vector (with some extra costs) . [sent-211, score-0.097]
</p><p>58 In ( a), (b), the iterative dogleg reduced the number of incorrect patterns down to 21 (nearly RMSE = 0. [sent-245, score-0.898]
</p><p>59 009) at epoch 838, whereas the direct dogleg reached the same error level at epoch 388. [sent-246, score-1.094]
</p><p>60 In (c), the iterative dogleg solved it perfectly at epoch 1,034 and the direct dogleg did so at epoch 401. [sent-247, score-1.992]
</p><p>61 iterative dogleg converged faster to a small RMSE 4 than the direct dogleg at an early stage of learning even with respect to epoch. [sent-248, score-1.799]
</p><p>62 Moreover, the average number of inner CG iterations per epoch in the iterative dogleg algorithm was quite small, 5. [sent-249, score-1.177]
</p><p>63 Thus, the iterative dogleg worked nearly (b) nine times and (c) four times faster than the direct dogleg in terms of the average execution time per epoch. [sent-252, score-1.872]
</p><p>64 Those speed-up ratios b ecame smaller than n mainly due to the aforementioned cost of Algorithm 3. [sent-253, score-0.054]
</p><p>65 Yet, as n increases, the speed-up ratio can be larger especially when the number of inner iterations is reasonably small. [sent-255, score-0.097]
</p><p>66 5  Conclusion and Future Directions  We have compared two batch-mode MLP-Iearning algorithms: iterative and direct dogleg trust-region algorithms. [sent-256, score-1.033]
</p><p>67 Although such a high-dimensional parity problem is very special in the sense that it involves a large data set but the size of MLP can be kept relatively small, the algorithmic features of the two dogleg methods can be well understood from the obtained experimental results. [sent-257, score-0.904]
</p><p>68 That is, the iterative dogleg has the great advantage of reducing the cost of an epoch from O(mn 2 ) to O(mn), and the memory requirements from O(n 2 ) to O(n), a factor of O(n) in both cases. [sent-258, score-1.072]
</p><p>69 It also has the advantage offaster convergence in the early epochs, achieving a lower RMSE after fewer epochs than the direct dogleg. [sent-260, score-0.207]
</p><p>70 Its disadvantage is that it may need more epochs to converge to a very small RMSE than the direct dogleg (although it might work faster in execution time). [sent-261, score-1.045]
</p><p>71 Thus, the iterative dogleg is most attractive when attempting to achieve a reasonably small RMSE on very large problems in a short period of time. [sent-262, score-0.952]
</p><p>72 On dynamic programming-like recursive gradient formula for alleviating hidden-node satuaration in the parity problem. [sent-277, score-0.211]
</p><p>73 In Proceedings of the International Workshop on Intelligent Systems Resolutions - the 8th Bellman Continuum, pages 100- 104, Hsinchu, TAIWAN, 2000. [sent-278, score-0.031]
</p><p>74 Powell's dogleg trust-region steps with the quasi-Newton augmented Hessian for neural nonlinear least-squares learning. [sent-280, score-0.87]
</p><p>75 The conjugate gradient method and trust regions in large scale optimization. [sent-295, score-0.149]
</p><p>76 Computing Powell's dogleg steps for solving adaptive networks nonlinear least-squares problems. [sent-325, score-0.906]
</p><p>77 On generalized dogleg trust-region steps using the Krylov subspace for solving neural networks nonlinear least squares problems. [sent-345, score-1.021]
</p><p>78 In Neuro Fuzzy and Soft Computing, pages 129- 172. [sent-355, score-0.031]
</p><p>79 A scaled conjugate gradient algorithm for fast supervised learning. [sent-365, score-0.17]
</p><p>80 Color device characterization of electronic cameras by solving adaptive networks nonlinear least squares problems. [sent-377, score-0.254]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dogleg', 0.717), ('mizutani', 0.219), ('iterative', 0.181), ('rmse', 0.159), ('hessian', 0.148), ('mlp', 0.14), ('direct', 0.135), ('parity', 0.124), ('epoch', 0.121), ('demmel', 0.119), ('cg', 0.108), ('newton', 0.101), ('eiji', 0.1), ('step', 0.082), ('multiplication', 0.081), ('cauchy', 0.08), ('krylov', 0.08), ('ljcauchy', 0.08), ('powell', 0.08), ('nonlinear', 0.077), ('squares', 0.075), ('inner', 0.073), ('execution', 0.072), ('epochs', 0.072), ('berkeley', 0.067), ('residual', 0.064), ('mn', 0.064), ('algorithm', 0.061), ('conjugate', 0.06), ('cgnr', 0.06), ('demuth', 0.06), ('hsinchu', 0.06), ('overdetermined', 0.06), ('rnow', 0.06), ('taiwan', 0.06), ('jt', 0.054), ('steepest', 0.051), ('solvers', 0.051), ('nn', 0.051), ('jacobian', 0.051), ('steps', 0.05), ('gradient', 0.049), ('costs', 0.045), ('solver', 0.043), ('fuzzy', 0.043), ('james', 0.043), ('trajectory', 0.043), ('dembo', 0.04), ('dreyfus', 0.04), ('inexact', 0.04), ('jang', 0.04), ('ljnewton', 0.04), ('moller', 0.04), ('pth', 0.04), ('steihaug', 0.04), ('trust', 0.04), ('vnow', 0.04), ('least', 0.04), ('equation', 0.039), ('uc', 0.038), ('iteration', 0.038), ('formula', 0.038), ('solving', 0.037), ('siam', 0.036), ('ut', 0.035), ('shepherd', 0.034), ('beale', 0.034), ('hits', 0.034), ('matrix', 0.033), ('outer', 0.032), ('algorithmic', 0.032), ('pages', 0.031), ('kept', 0.031), ('wherein', 0.031), ('attempting', 0.029), ('aforementioned', 0.029), ('memory', 0.028), ('yes', 0.027), ('lj', 0.027), ('nearly', 0.026), ('backpropagation', 0.026), ('sweep', 0.026), ('augmented', 0.026), ('networks', 0.025), ('cost', 0.025), ('small', 0.025), ('normalized', 0.025), ('dk', 0.024), ('limitation', 0.024), ('root', 0.024), ('faster', 0.024), ('iterations', 0.024), ('operations', 0.024), ('li', 0.024), ('direction', 0.024), ('check', 0.023), ('approximate', 0.023), ('decomposition', 0.023), ('iteratively', 0.022), ('refer', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="93-tfidf-1" href="./nips-2000-On_Iterative_Krylov-Dogleg_Trust-Region_Steps_for_Solving_Neural_Networks_Nonlinear_Least_Squares_Problems.html">93 nips-2000-On Iterative Krylov-Dogleg Trust-Region Steps for Solving Neural Networks Nonlinear Least Squares Problems</a></p>
<p>Author: Eiji Mizutani, James Demmel</p><p>Abstract: This paper describes a method of dogleg trust-region steps, or restricted Levenberg-Marquardt steps, based on a projection process onto the Krylov subspaces for neural networks nonlinear least squares problems. In particular, the linear conjugate gradient (CG) method works as the inner iterative algorithm for solving the linearized Gauss-Newton normal equation, whereas the outer nonlinear algorithm repeatedly takes so-called</p><p>2 0.05734729 <a title="93-tfidf-2" href="./nips-2000-Tree-Based_Modeling_and_Estimation_of_Gaussian_Processes_on_Graphs_with_Cycles.html">140 nips-2000-Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles</a></p>
<p>Author: Martin J. Wainwright, Erik B. Sudderth, Alan S. Willsky</p><p>Abstract: We present the embedded trees algorithm, an iterative technique for estimation of Gaussian processes defined on arbitrary graphs. By exactly solving a series of modified problems on embedded spanning trees, it computes the conditional means with an efficiency comparable to or better than other techniques. Unlike other methods, the embedded trees algorithm also computes exact error covariances. The error covariance computation is most efficient for graphs in which removing a small number of edges reveals an embedded tree. In this context, we demonstrate that sparse loopy graphs can provide a significant increase in modeling power relative to trees, with only a minor increase in estimation complexity. 1</p><p>3 0.047886759 <a title="93-tfidf-3" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>Author: Alex J. Smola, Peter L. Bartlett</p><p>Abstract: We present a simple sparse greedy technique to approximate the maximum a posteriori estimate of Gaussian Processes with much improved scaling behaviour in the sample size m. In particular, computational requirements are O(n 2 m), storage is O(nm), the cost for prediction is 0 (n) and the cost to compute confidence bounds is O(nm), where n «: m. We show how to compute a stopping criterion, give bounds on the approximation error, and show applications to large scale problems. 1</p><p>4 0.047767665 <a title="93-tfidf-4" href="./nips-2000-Active_Support_Vector_Machine_Classification.html">18 nips-2000-Active Support Vector Machine Classification</a></p>
<p>Author: Olvi L. Mangasarian, David R. Musicant</p><p>Abstract: An active set strategy is applied to the dual of a simple reformulation of the standard quadratic program of a linear support vector machine. This application generates a fast new dual algorithm that consists of solving a finite number of linear equations, with a typically large dimensionality equal to the number of points to be classified. However, by making novel use of the Sherman-MorrisonWoodbury formula , a much smaller matrix of the order of the original input space is inverted at each step. Thus, a problem with a 32-dimensional input space and 7 million points required inverting positive definite symmetric matrices of size 33 x 33 with a total running time of 96 minutes on a 400 MHz Pentium II. The algorithm requires no specialized quadratic or linear programming code, but merely a linear equation solver which is publicly available. 1</p><p>5 0.043068342 <a title="93-tfidf-5" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: Based on a statistical mechanics approach, we develop a method for approximately computing average case learning curves for Gaussian process regression models. The approximation works well in the large sample size limit and for arbitrary dimensionality of the input space. We explain how the approximation can be systematically improved and argue that similar techniques can be applied to general likelihood models. 1</p><p>6 0.042758118 <a title="93-tfidf-6" href="./nips-2000-Overfitting_in_Neural_Nets%3A_Backpropagation%2C_Conjugate_Gradient%2C_and_Early_Stopping.html">97 nips-2000-Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping</a></p>
<p>7 0.04057508 <a title="93-tfidf-7" href="./nips-2000-Factored_Semi-Tied_Covariance_Matrices.html">51 nips-2000-Factored Semi-Tied Covariance Matrices</a></p>
<p>8 0.04029737 <a title="93-tfidf-8" href="./nips-2000-Fast_Training_of_Support_Vector_Classifiers.html">52 nips-2000-Fast Training of Support Vector Classifiers</a></p>
<p>9 0.039641351 <a title="93-tfidf-9" href="./nips-2000-Sparse_Kernel_Principal_Component_Analysis.html">121 nips-2000-Sparse Kernel Principal Component Analysis</a></p>
<p>10 0.039629549 <a title="93-tfidf-10" href="./nips-2000-Learning_Continuous_Distributions%3A_Simulations_With_Field_Theoretic_Priors.html">76 nips-2000-Learning Continuous Distributions: Simulations With Field Theoretic Priors</a></p>
<p>11 0.03939129 <a title="93-tfidf-11" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>12 0.038718533 <a title="93-tfidf-12" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>13 0.038579881 <a title="93-tfidf-13" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<p>14 0.038438596 <a title="93-tfidf-14" href="./nips-2000-Combining_ICA_and_Top-Down_Attention_for_Robust_Speech_Recognition.html">33 nips-2000-Combining ICA and Top-Down Attention for Robust Speech Recognition</a></p>
<p>15 0.037025269 <a title="93-tfidf-15" href="./nips-2000-Constrained_Independent_Component_Analysis.html">36 nips-2000-Constrained Independent Component Analysis</a></p>
<p>16 0.036506295 <a title="93-tfidf-16" href="./nips-2000-%60N-Body%27_Problems_in_Statistical_Learning.html">148 nips-2000-`N-Body' Problems in Statistical Learning</a></p>
<p>17 0.036421835 <a title="93-tfidf-17" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>18 0.035628229 <a title="93-tfidf-18" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>19 0.035304643 <a title="93-tfidf-19" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>20 0.03420382 <a title="93-tfidf-20" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.122), (1, 0.004), (2, 0.029), (3, -0.024), (4, -0.004), (5, 0.027), (6, -0.016), (7, 0.008), (8, -0.033), (9, 0.019), (10, -0.085), (11, 0.036), (12, 0.021), (13, -0.013), (14, -0.018), (15, -0.012), (16, -0.083), (17, 0.067), (18, 0.047), (19, 0.028), (20, 0.087), (21, -0.055), (22, -0.028), (23, 0.026), (24, -0.111), (25, -0.123), (26, -0.036), (27, 0.1), (28, -0.155), (29, -0.023), (30, -0.042), (31, 0.0), (32, -0.141), (33, -0.031), (34, 0.011), (35, -0.054), (36, -0.235), (37, -0.047), (38, -0.009), (39, 0.094), (40, -0.105), (41, -0.068), (42, 0.056), (43, 0.043), (44, -0.04), (45, 0.284), (46, -0.122), (47, -0.042), (48, -0.022), (49, 0.062)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94169062 <a title="93-lsi-1" href="./nips-2000-On_Iterative_Krylov-Dogleg_Trust-Region_Steps_for_Solving_Neural_Networks_Nonlinear_Least_Squares_Problems.html">93 nips-2000-On Iterative Krylov-Dogleg Trust-Region Steps for Solving Neural Networks Nonlinear Least Squares Problems</a></p>
<p>Author: Eiji Mizutani, James Demmel</p><p>Abstract: This paper describes a method of dogleg trust-region steps, or restricted Levenberg-Marquardt steps, based on a projection process onto the Krylov subspaces for neural networks nonlinear least squares problems. In particular, the linear conjugate gradient (CG) method works as the inner iterative algorithm for solving the linearized Gauss-Newton normal equation, whereas the outer nonlinear algorithm repeatedly takes so-called</p><p>2 0.47434029 <a title="93-lsi-2" href="./nips-2000-Tree-Based_Modeling_and_Estimation_of_Gaussian_Processes_on_Graphs_with_Cycles.html">140 nips-2000-Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles</a></p>
<p>Author: Martin J. Wainwright, Erik B. Sudderth, Alan S. Willsky</p><p>Abstract: We present the embedded trees algorithm, an iterative technique for estimation of Gaussian processes defined on arbitrary graphs. By exactly solving a series of modified problems on embedded spanning trees, it computes the conditional means with an efficiency comparable to or better than other techniques. Unlike other methods, the embedded trees algorithm also computes exact error covariances. The error covariance computation is most efficient for graphs in which removing a small number of edges reveals an embedded tree. In this context, we demonstrate that sparse loopy graphs can provide a significant increase in modeling power relative to trees, with only a minor increase in estimation complexity. 1</p><p>3 0.42522818 <a title="93-lsi-3" href="./nips-2000-Overfitting_in_Neural_Nets%3A_Backpropagation%2C_Conjugate_Gradient%2C_and_Early_Stopping.html">97 nips-2000-Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping</a></p>
<p>Author: Rich Caruana, Steve Lawrence, C. Lee Giles</p><p>Abstract: The conventional wisdom is that backprop nets with excess hidden units generalize poorly. We show that nets with excess capacity generalize well when trained with backprop and early stopping. Experiments suggest two reasons for this: 1) Overfitting can vary significantly in different regions of the model. Excess capacity allows better fit to regions of high non-linearity, and backprop often avoids overfitting the regions of low non-linearity. 2) Regardless of size, nets learn task subcomponents in similar sequence. Big nets pass through stages similar to those learned by smaller nets. Early stopping can stop training the large net when it generalizes comparably to a smaller net. We also show that conjugate gradient can yield worse generalization because it overfits regions of low non-linearity when learning to fit regions of high non-linearity.</p><p>4 0.3780584 <a title="93-lsi-4" href="./nips-2000-%60N-Body%27_Problems_in_Statistical_Learning.html">148 nips-2000-`N-Body' Problems in Statistical Learning</a></p>
<p>Author: Alexander G. Gray, Andrew W. Moore</p><p>Abstract: We present efficient algorithms for all-point-pairs problems , or 'Nbody '-like problems , which are ubiquitous in statistical learning. We focus on six examples, including nearest-neighbor classification, kernel density estimation, outlier detection , and the two-point correlation. These include any problem which abstractly requires a comparison of each of the N points in a dataset with each other point and would naively be solved using N 2 distance computations. In practice N is often large enough to make this infeasible. We present a suite of new geometric t echniques which are applicable in principle to any 'N-body ' computation including large-scale mixtures of Gaussians, RBF neural networks, and HMM 's. Our algorithms exhibit favorable asymptotic scaling and are empirically several orders of magnitude faster than the naive computation, even for small datasets. We are aware of no exact algorithms for these problems which are more efficient either empirically or theoretically. In addition, our framework yields simple and elegant algorithms. It also permits two important generalizations beyond the standard all-point-pairs problems, which are more difficult. These are represented by our final examples, the multiple two-point correlation and the notorious n-point correlation. 1</p><p>5 0.37332886 <a title="93-lsi-5" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>Author: Alex J. Smola, Peter L. Bartlett</p><p>Abstract: We present a simple sparse greedy technique to approximate the maximum a posteriori estimate of Gaussian Processes with much improved scaling behaviour in the sample size m. In particular, computational requirements are O(n 2 m), storage is O(nm), the cost for prediction is 0 (n) and the cost to compute confidence bounds is O(nm), where n «: m. We show how to compute a stopping criterion, give bounds on the approximation error, and show applications to large scale problems. 1</p><p>6 0.35188597 <a title="93-lsi-6" href="./nips-2000-Fast_Training_of_Support_Vector_Classifiers.html">52 nips-2000-Fast Training of Support Vector Classifiers</a></p>
<p>7 0.35181934 <a title="93-lsi-7" href="./nips-2000-Active_Support_Vector_Machine_Classification.html">18 nips-2000-Active Support Vector Machine Classification</a></p>
<p>8 0.35128084 <a title="93-lsi-8" href="./nips-2000-Incremental_and_Decremental_Support_Vector_Machine_Learning.html">70 nips-2000-Incremental and Decremental Support Vector Machine Learning</a></p>
<p>9 0.32962254 <a title="93-lsi-9" href="./nips-2000-Algorithms_for_Non-negative_Matrix_Factorization.html">22 nips-2000-Algorithms for Non-negative Matrix Factorization</a></p>
<p>10 0.30570322 <a title="93-lsi-10" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>11 0.30088192 <a title="93-lsi-11" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>12 0.29083455 <a title="93-lsi-12" href="./nips-2000-Constrained_Independent_Component_Analysis.html">36 nips-2000-Constrained Independent Component Analysis</a></p>
<p>13 0.28882924 <a title="93-lsi-13" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>14 0.27526307 <a title="93-lsi-14" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<p>15 0.27294269 <a title="93-lsi-15" href="./nips-2000-Combining_ICA_and_Top-Down_Attention_for_Robust_Speech_Recognition.html">33 nips-2000-Combining ICA and Top-Down Attention for Robust Speech Recognition</a></p>
<p>16 0.25718692 <a title="93-lsi-16" href="./nips-2000-Factored_Semi-Tied_Covariance_Matrices.html">51 nips-2000-Factored Semi-Tied Covariance Matrices</a></p>
<p>17 0.25332171 <a title="93-lsi-17" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>18 0.23609035 <a title="93-lsi-18" href="./nips-2000-Generalizable_Singular_Value_Decomposition_for_Ill-posed_Datasets.html">61 nips-2000-Generalizable Singular Value Decomposition for Ill-posed Datasets</a></p>
<p>19 0.2286438 <a title="93-lsi-19" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>20 0.22386187 <a title="93-lsi-20" href="./nips-2000-Periodic_Component_Analysis%3A_An_Eigenvalue_Method_for_Representing_Periodic_Structure_in_Speech.html">99 nips-2000-Periodic Component Analysis: An Eigenvalue Method for Representing Periodic Structure in Speech</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.017), (17, 0.112), (32, 0.012), (33, 0.04), (54, 0.026), (55, 0.025), (62, 0.046), (65, 0.025), (67, 0.048), (75, 0.013), (76, 0.045), (79, 0.02), (81, 0.019), (90, 0.057), (97, 0.026), (99, 0.373)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88818824 <a title="93-lda-1" href="./nips-2000-Analysis_of_Bit_Error_Probability_of_Direct-Sequence_CDMA_Multiuser_Demodulators.html">25 nips-2000-Analysis of Bit Error Probability of Direct-Sequence CDMA Multiuser Demodulators</a></p>
<p>Author: Toshiyuki Tanaka</p><p>Abstract: We analyze the bit error probability of multiuser demodulators for directsequence binary phase-shift-keying (DSIBPSK) CDMA channel with additive gaussian noise. The problem of multiuser demodulation is cast into the finite-temperature decoding problem, and replica analysis is applied to evaluate the performance of the resulting MPM (Marginal Posterior Mode) demodulators, which include the optimal demodulator and the MAP demodulator as special cases. An approximate implementation of demodulators is proposed using analog-valued Hopfield model as a naive mean-field approximation to the MPM demodulators, and its performance is also evaluated by the replica analysis. Results of the performance evaluation shows effectiveness of the optimal demodulator and the mean-field demodulator compared with the conventional one, especially in the cases of small information bit rate and low noise level. 1</p><p>2 0.85944545 <a title="93-lda-2" href="./nips-2000-Active_Inference_in_Concept_Learning.html">16 nips-2000-Active Inference in Concept Learning</a></p>
<p>Author: Jonathan D. Nelson, Javier R. Movellan</p><p>Abstract: People are active experimenters, not just passive observers, constantly seeking new information relevant to their goals. A reasonable approach to active information gathering is to ask questions and conduct experiments that maximize the expected information gain, given current beliefs (Fedorov 1972, MacKay 1992, Oaksford & Chater 1994). In this paper we present results on an exploratory experiment designed to study people's active information gathering behavior on a concept learning task (Tenenbaum 2000). The results of the experiment are analyzed in terms of the expected information gain of the questions asked by subjects. In scientific inquiry and in everyday life, people seek out information relevant to perceptual and cognitive tasks. Scientists perform experiments to uncover causal relationships; people saccade to informative areas of visual scenes, turn their head towards surprising sounds, and ask questions to understand the meaning of concepts . Consider a person learning a foreign language, who notices that a particular word,</p><p>same-paper 3 0.79037875 <a title="93-lda-3" href="./nips-2000-On_Iterative_Krylov-Dogleg_Trust-Region_Steps_for_Solving_Neural_Networks_Nonlinear_Least_Squares_Problems.html">93 nips-2000-On Iterative Krylov-Dogleg Trust-Region Steps for Solving Neural Networks Nonlinear Least Squares Problems</a></p>
<p>Author: Eiji Mizutani, James Demmel</p><p>Abstract: This paper describes a method of dogleg trust-region steps, or restricted Levenberg-Marquardt steps, based on a projection process onto the Krylov subspaces for neural networks nonlinear least squares problems. In particular, the linear conjugate gradient (CG) method works as the inner iterative algorithm for solving the linearized Gauss-Newton normal equation, whereas the outer nonlinear algorithm repeatedly takes so-called</p><p>4 0.38456184 <a title="93-lda-4" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>Author: Lehel Csatč´¸, Manfred Opper</p><p>Abstract: We develop an approach for a sparse representation for Gaussian Process (GP) models in order to overcome the limitations of GPs caused by large data sets. The method is based on a combination of a Bayesian online algorithm together with a sequential construction of a relevant subsample of the data which fully specifies the prediction of the model. Experimental results on toy examples and large real-world data sets indicate the efficiency of the approach.</p><p>5 0.38189161 <a title="93-lda-5" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>Author: Martin Szummer, Tommi Jaakkola</p><p>Abstract: Modern classification applications necessitate supplementing the few available labeled examples with unlabeled examples to improve classification performance. We present a new tractable algorithm for exploiting unlabeled examples in discriminative classification. This is achieved essentially by expanding the input vectors into longer feature vectors via both labeled and unlabeled examples. The resulting classification method can be interpreted as a discriminative kernel density estimate and is readily trained via the EM algorithm, which in this case is both discriminative and achieves the optimal solution. We provide, in addition, a purely discriminative formulation of the estimation problem by appealing to the maximum entropy framework. We demonstrate that the proposed approach requires very few labeled examples for high classification accuracy.</p><p>6 0.37933341 <a title="93-lda-6" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>7 0.3763898 <a title="93-lda-7" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>8 0.37381428 <a title="93-lda-8" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>9 0.37169281 <a title="93-lda-9" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>10 0.3714892 <a title="93-lda-10" href="./nips-2000-Fast_Training_of_Support_Vector_Classifiers.html">52 nips-2000-Fast Training of Support Vector Classifiers</a></p>
<p>11 0.37052888 <a title="93-lda-11" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<p>12 0.36955941 <a title="93-lda-12" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>13 0.36905155 <a title="93-lda-13" href="./nips-2000-Regularized_Winnow_Methods.html">111 nips-2000-Regularized Winnow Methods</a></p>
<p>14 0.36753049 <a title="93-lda-14" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>15 0.36587307 <a title="93-lda-15" href="./nips-2000-Algorithmic_Stability_and_Generalization_Performance.html">21 nips-2000-Algorithmic Stability and Generalization Performance</a></p>
<p>16 0.36524045 <a title="93-lda-16" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>17 0.36468408 <a title="93-lda-17" href="./nips-2000-Constrained_Independent_Component_Analysis.html">36 nips-2000-Constrained Independent Component Analysis</a></p>
<p>18 0.36358532 <a title="93-lda-18" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>19 0.36315092 <a title="93-lda-19" href="./nips-2000-Some_New_Bounds_on_the_Generalization_Error_of_Combined_Classifiers.html">119 nips-2000-Some New Bounds on the Generalization Error of Combined Classifiers</a></p>
<p>20 0.36274391 <a title="93-lda-20" href="./nips-2000-Factored_Semi-Tied_Covariance_Matrices.html">51 nips-2000-Factored Semi-Tied Covariance Matrices</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
