<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-86" href="#">nips2000-86</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</h1>
<br/><p>Source: <a title="nips-2000-86-pdf" href="http://papers.nips.cc/paper/1865-model-complexity-goodness-of-fit-and-diminishing-returns.pdf">pdf</a></p><p>Author: Igor V. Cadez, Padhraic Smyth</p><p>Abstract: We investigate a general characteristic of the trade-off in learning problems between goodness-of-fit and model complexity. Specifically we characterize a general class of learning problems where the goodness-of-fit function can be shown to be convex within firstorder as a function of model complexity. This general property of</p><p>Reference: <a title="nips-2000-86-reference" href="../nips2000_reference/nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract We investigate a general characteristic of the trade-off in learning problems between goodness-of-fit and model complexity. [sent-8, score-0.206]
</p><p>2 Specifically we characterize a general class of learning problems where the goodness-of-fit function can be shown to be convex within firstorder as a function of model complexity. [sent-9, score-0.46]
</p><p>3 This general property of "diminishing returns" is illustrated on a number of real data sets and learning problems, including finite mixture modeling and multivariate linear regression. [sent-10, score-0.377]
</p><p>4 1  Introduction, Motivation, and Related Work  Assume we have a data set D = {Xl, X2, . [sent-11, score-0.034]
</p><p>5 , x n }, where the X i could be vectors, sequences, etc. [sent-14, score-0.031]
</p><p>6 We consider modeling the data set D using models indexed by a complexity index k, 1 :::; k :::; kmax • For example, the models could be finite mixture probability density functions (PDFs) for vector Xi'S where model complexity is indexed by the number of components k in the mixture. [sent-15, score-1.307]
</p><p>7 Alternatively, the modeling task could be to fit a conditional regression model y = g(Zk) + e, where now y is one of the variables in the vector X and Z is some subset of size k of the remaining components in the X vector. [sent-16, score-0.577]
</p><p>8 Such learning tasks can typically be characterized by the existence of a model and a loss function. [sent-17, score-0.404]
</p><p>9 A fitted model of complexity k is a function of the data points D and depends on a specific set of fitted parameters B. [sent-18, score-0.533]
</p><p>10 The loss function (goodnessof-fit) is a functional of the model and maps each specific model to a scalar used to evaluate the model, e. [sent-19, score-0.597]
</p><p>11 Figure 1 illustrates a typical empirical curve for loss function versus complexity, for mixtures of Markov models fitted to a large data set of 900,000 sequences. [sent-22, score-0.759]
</p><p>12 The complexity k is the number of Markov models being used in the mixture (see Cadez et al. [sent-23, score-0.496]
</p><p>13 (2000) for further details on the model and the data set). [sent-24, score-0.124]
</p><p>14 The empirical curve has a distinctly concave appearance, with large relative gains in fit for low complexity models and much more modest relative gains for high complexity models. [sent-25, score-0.898]
</p><p>15 A natural question is whether this concavity characteristic can be viewed as a general phenomenon in learning and under what assumptions on model classes and  Nwnber of M Ixture Cmnpo nen1S 1 1]  Figure 1: Log-likelihood scores for a Markov mixtures data set. [sent-26, score-0.536]
</p><p>16 loss functions the concavity can be shown to hold. [sent-27, score-0.563]
</p><p>17 The goal of this paper is to illustrate that in fact it is a natural characteristic for a broad range of problems in mixture modeling and linear regression. [sent-28, score-0.427]
</p><p>18 We note of course that for generalization that using goodness-of-fit alone will lead to the selection of the most complex model under consideration and will not in general select the model which generalizes best to new data. [sent-29, score-0.323]
</p><p>19 Nonetheless our primary focus of interest in this paper is how goodness-of-fit loss functions (such as likelihood and squared error, defined on the training data D) behave in general as a function of model complexity k. [sent-30, score-0.873]
</p><p>20 Our concavity results have a number of interesting implications. [sent-31, score-0.197]
</p><p>21 For example, for model selection methods which add a penalty term to the goodness-of-fit (e. [sent-32, score-0.239]
</p><p>22 , BIC), the resulting score function as a function of model complexity will be unimodal as a function of complexity k within first order. [sent-34, score-0.683]
</p><p>23 Li and Barron (1999) have shown that for finite mixture models the expected value of the log-likelihood for any k is bounded below by a function of the form -C /k where C is a constant which is independent of k. [sent-35, score-0.349]
</p><p>24 The results presented here are complementary in the sense that we show that the actual maximizing log-likelihood itself is concave to first-order as a function of k. [sent-36, score-0.192]
</p><p>25 Furthermore, we obtain a more general principle of "diminishing returns," including both finite mixtures and subset selection in regression. [sent-37, score-0.249]
</p><p>26 2  Notation  We define y = y(x) as a scalar function of x, namely a prediction at x. [sent-38, score-0.071]
</p><p>27 In linear regression y = y(x ) is a linear function of the components in x while in density estimation y = y(x) is the value of the density function at x. [sent-39, score-0.53]
</p><p>28 Although the goals of regression and density estimation are quite different, we can view them both as simply techniques for approximating an unknown true function for different values of x. [sent-40, score-0.304]
</p><p>29 We denote the prediction of a model of complexity k as Yk (xIB) where the subscript indicates the model complexity and B is the associated set of fitted parameters. [sent-41, score-0.692]
</p><p>30 Since different choices of parameters in general yield different models, we will typically abbreviate the notation somewhat and use different letters for different parameterizations of the same functional form (i. [sent-42, score-0.107]
</p><p>31 , we may use Yk(X),gk(X), hk(X) to refer to models of complexity k instead of specifying Yk(xIBd,Yk(xIB 2 ),Yk(xIB3 ), etc. [sent-46, score-0.313]
</p><p>32 Furthermore, since all models under discussion are functions of x, we sometimes omit the explicit dependence on x and use a compact notation Yk, 9k, hk· We focus on classes of models that can be characterized by more complex models having a linear dependence on simpler models within the class. [sent-47, score-0.589]
</p><p>33 More formally, any  model of complexity k can be decomposed as: (1) Yk = a191 + a2h1 + . [sent-48, score-0.356]
</p><p>34 + ak W 1· In PDF mixture modeling we have Yk = p( x) and each model 91, hI, . [sent-51, score-0.359]
</p><p>35 In multivariate linear regression each model 91, hI, . [sent-57, score-0.324]
</p><p>36 ,WI represents a regression on a single variable, e. [sent-60, score-0.224]
</p><p>37 , 91(X) above is 91(X) = 'Ypxp where xp is the p-th variable in the set and 'Yp is the corresponding coefficient one would obtain if regressing on xp alone. [sent-62, score-0.088]
</p><p>38 Note that the total parameters for the model Yk in both cases can be viewed as consisting of both the mixing proportions (the a's) and the parameters for each individual component model. [sent-67, score-0.09]
</p><p>39 The loss function is a functional on models and we write it as E(Yk). [sent-68, score-0.517]
</p><p>40 For simplicity, we use the notation EZ to specify the value of the loss function for the best kcomponent model. [sent-69, score-0.502]
</p><p>41 For example, the loss function in PDF mixture modeling is the negative log likelihood. [sent-71, score-0.654]
</p><p>42 In linear regression we use empirical mean squared error (MSE) as the loss function. [sent-72, score-0.638]
</p><p>43 The loss functions of general interest in this context are those that decompose into a sum of functions over data points in the data set D (equivalently an independence assumption in a likelihood framework), i. [sent-73, score-0.554]
</p><p>44 , n  (2) i=l  For example, in PDF mixture modeling ! [sent-75, score-0.269]
</p><p>45 3  Necessary Conditions on Models and Loss Functions  We consider models that satisfy several conditions that are commonly met in real data analysis applications and are satisfied by both PDF mixture models and linear regression models: 1. [sent-78, score-0.681]
</p><p>46 , each model of complexity  k contains each model of complexity k' < k as a special case (i. [sent-81, score-0.616]
</p><p>47 , it reduces to a simpler model for a special choice of the parameters). [sent-83, score-0.09]
</p><p>48 Any two models of complexities k1 and k2 can be combined as a weighted sum in any proportion to yield a valid model of complexity k = k1 + k 2. [sent-85, score-0.495]
</p><p>49 Each model of complexity k = k1 + k2 can be decomposed into a weighted sum of two valid models of complexities k1 and k2 respectively for each valid choice of k1 and k 2. [sent-87, score-0.579]
</p><p>50 As an example, the standard Gaussian mixture model satisfies all three properties whether the covariance matrices are unconstrained or individually constrained. [sent-89, score-0.344]
</p><p>51 As a counter-example, a Gaussian mixture model where the covariance matrices are constrained to be equal across all components does not satisfy the second property. [sent-90, score-0.373]
</p><p>52 lWe assume the learning task consists of minimization of the loss function. [sent-91, score-0.314]
</p><p>53 If maximization is more appropriate , we can just consider minimization of the negative of the loss function. [sent-92, score-0.35]
</p><p>54 The quantities EZ and EZ±l are the values of the loss function for the best k and k ± I-component models. [sent-94, score-0.427]
</p><p>55 Proal: In the first part of the proof we analyze a general difference of loss functions and write it in a convenient form. [sent-95, score-0.504]
</p><p>56 Consider two arbitrary models, 9 and hand the corresponding loss functions E(g) and E(h) (g and h need not have the same complexity). [sent-96, score-0.366]
</p><p>57 The difference in loss functions can be expressed as: n  E(g) - E(h)  L {I [g(Xi)] - I [h(Xi)]} i=l  n  L {I [h(xi)(1 + Jg ,h(Xi))]- I [h(Xi)]} i=l  n  =  a L h(Xi)! [sent-97, score-0.4]
</p><p>58 (3)  i=l  where the last equation comes from a first order Taylor series expansion around each Jg ,h(Xi) = 0, a is an unknown constant of proportionality (to make the equation exact) and J  g,h  () -=- g(x) - h(x) X  -  (4)  h(x)  represents the relative difference in models 9 and h at point x. [sent-99, score-0.417]
</p><p>59 For example, Equation 3 reduces to a first order Taylor series approximation for a = 1. [sent-100, score-0.066]
</p><p>60 If I (y) is a convex function we also have: n  E(g) - E(h) ~ L h(Xi)! [sent-101, score-0.194]
</p><p>61 (5)  i=l  since the remainder in the Taylor series expansion R2  = I/2f"(h(I + 8J))J 2 ~ O. [sent-103, score-0.073]
</p><p>62 In the second part of the proof we use Equation 5 to derive an appropriate condition on loss functions. [sent-104, score-0.46]
</p><p>63 ' (Y'k(Xi))  [(1 - E)09k ,y;, (Xi) + O~k ,y;' (Xi) + EOy;'_l'Y;' (Xi )]'  (8)  i =l  In the third part of the proof we analyze each of the terms in Equation 8 using Equation 3. [sent-110, score-0.067]
</p><p>64 Consider the first term: n  ilgk ,y;,  = LY'k(xd! [sent-111, score-0.098]
</p><p>65 '(Y'k(Xi))09k ,y;, (Xi )  (9)  i=l  that depends on a relative difference of models gk and y'k at each of the data points Xi . [sent-112, score-0.305]
</p><p>66 According to Equation 3, for small 09k ,Y;' (Xi) (which is presumably true), we can set a: :::::; 1 to get a first order Taylor expansion. [sent-113, score-0.031]
</p><p>67 In other words, we only require that the sign is preserved when making Taylor expansion while the actual value need not be very accurate. [sent-115, score-0.038]
</p><p>68 Similarly, each of the three terms on the right hand side of Equation 8 is first order positive since E(yk) ::; E(gk), E(ek), E(Y'k-1)' This shows that  E k+1 - 2Ek within first order, concluding the proof. [sent-116, score-0.071]
</p><p>69 Each proof consists of merely selecting the appropriate loss function E (y) and model family y. [sent-118, score-0.546]
</p><p>70 1  Concavity of Mixture Model Log-Likelihoods  Theorem 2: In mixture model learning, using log-likelihood as the loss function and using unconstrained mixture components, the in-sample log likelihood is a firstorder concave function of the complexity k. [sent-120, score-1.42]
</p><p>71 Prool: By using I (y) = -In Y in Theorem 1 the loss function E(y) becomes the negative of the in-sample log likelihood, hence it is a first-order convex function of complexity k, i. [sent-121, score-0.797]
</p><p>72 Corollary 1: If a linear or convex penalty term in k is subtracted from the in-sample log likelihood in Theorem 2, using the mixture models as defined in Theorem 2, then the penalized likelihood can have at most one maximum to within first order. [sent-124, score-0.84]
</p><p>73 Prool: We use I(Yk(xi)) = (Yi - ydXi))2 which is a convex function of Yk . [sent-128, score-0.194]
</p><p>74 The corresponding loss function E(Yk) becomes the mean-square-error and is first-order convex as a function of the complexity k by the proof of Theorem 1. [sent-129, score-0.832]
</p><p>75 Corollary 2: If a concave or linear penalty term in k is added to the mean squared error as defined in Theorem 3, then the resulting penalized mean-square-error can have at most one minimum to within first order. [sent-130, score-0.45]
</p><p>76 Such penalty terms include Mallow's Cp criterion, AIC, BIC, predicted squared error, etc. [sent-131, score-0.138]
</p><p>77 6  Experimental Results  In this section we demonstrate empirical evidence of the approximate concavity property on three different data sets with model families and loss functions which satisfy the assumptions stated earlier: 1. [sent-135, score-0.792]
</p><p>78 Figure 2(a) illustrates that the log-likelihood is approximately concave as a function of k. [sent-137, score-0.223]
</p><p>79 com  Web site over a 24-hour period from over 900,000 individuals were fit with mixtures of first-order Markov chains (see Cadez et al. [sent-142, score-0.283]
</p><p>80 Figure 1 again clearly shows a concave characteristic for the log-likelihood as a function of k, the number of Markov components in the model. [sent-144, score-0.337]
</p><p>81 Subset Selection in Linear Regression: Autoregressive (AR) linear models were fit (closed form solutions for the optimal model parameters) to a monthly financial time series with 307 observations, for all possible combinations of lags (all possible  ,. [sent-146, score-0.371]
</p><p>82 '66  '58  Nwnbcr of Mixture Components rt]  Number of Reg ressloo Van ables [It]  Figure 2: (a) In-sample log-likelihood for mixture modeling of the atmospheric data set, (b) mean-squared error for regression using the financial data set. [sent-147, score-0.637]
</p><p>83 For example, the k = 1 model represents the best model with a single predictor from the previous 12 months, not necessarily the AR(l) model. [sent-149, score-0.286]
</p><p>84 Again the goodness-of-fit curve is almost convex in k (Figure 2(b», except at k = 9 where there is a slight non-concavity: this could again be either a numerical estimation effect or a fundamental characteristic indicating that concavity is only true to first-order. [sent-150, score-0.539]
</p><p>85 The main implication is that for at least two common learning scenarios the maximizing/minimizing value of the loss function is strongly constrained as model complexity is varied. [sent-152, score-0.661]
</p><p>86 Thus, for example, when performing model selection using penalized goodness-of-fit (as in the Corollaries above) variants of binary search may be quite useful in problems where k is very large (in the mixtures of Markov chains above it is not necessary to fit the model for all values of k, i. [sent-153, score-0.598]
</p><p>87 Extensions to model selection using loss-functions defined on out-of-sample test data sets can also be derived, and can be carried over under appropriate assumptions to cross-validation. [sent-156, score-0.262]
</p><p>88 Note that the results described here do not have an obvious extension to non-linear models (such as feed-forward neural networks) or loss-functions such as the 0/1 loss for classification. [sent-157, score-0.409]
</p><p>89 White, 'Visualization of navigation patterns on a Web site using model-based clustering,' Technical Report MS-TR-00-18, Microsoft Research, Redmond, WA. [sent-166, score-0.042]
</p><p>90 Ghil, 'Multiple regimes in Northern hemisphere height fields via mixture model clustering,' Journal of the Atmospheric Sciences, vol. [sent-173, score-0.273]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('yk', 0.382), ('loss', 0.314), ('complexity', 0.218), ('xi', 0.213), ('concavity', 0.197), ('regression', 0.192), ('mixture', 0.183), ('convex', 0.155), ('pdf', 0.153), ('concave', 0.153), ('ez', 0.147), ('gk', 0.142), ('cadez', 0.131), ('smyth', 0.131), ('yz', 0.113), ('diminishing', 0.098), ('ilgk', 0.098), ('jg', 0.098), ('mixtures', 0.097), ('models', 0.095), ('model', 0.09), ('taylor', 0.088), ('theorem', 0.087), ('modeling', 0.086), ('characteristic', 0.085), ('penalty', 0.08), ('penalized', 0.077), ('chains', 0.077), ('bic', 0.077), ('oy', 0.077), ('fitted', 0.076), ('equation', 0.076), ('notation', 0.075), ('best', 0.074), ('ek', 0.071), ('selection', 0.069), ('likelihood', 0.068), ('fit', 0.067), ('proof', 0.067), ('atmospheric', 0.066), ('barron', 0.066), ('firstorder', 0.066), ('ghil', 0.066), ('ide', 0.066), ('jyz', 0.066), ('lyz', 0.066), ('prool', 0.066), ('xib', 0.066), ('convexity', 0.063), ('components', 0.06), ('squared', 0.058), ('irvine', 0.056), ('complexities', 0.056), ('markov', 0.056), ('functions', 0.052), ('subset', 0.051), ('mse', 0.051), ('decomposed', 0.048), ('bishop', 0.047), ('returns', 0.046), ('xp', 0.044), ('hi', 0.044), ('density', 0.043), ('condition', 0.043), ('hk', 0.042), ('site', 0.042), ('financial', 0.042), ('web', 0.042), ('linear', 0.042), ('curve', 0.041), ('satisfy', 0.04), ('within', 0.04), ('corollary', 0.04), ('function', 0.039), ('expansion', 0.038), ('ar', 0.037), ('gains', 0.037), ('write', 0.037), ('appropriate', 0.036), ('satisfies', 0.036), ('valid', 0.036), ('indexed', 0.035), ('unconstrained', 0.035), ('series', 0.035), ('difference', 0.034), ('data', 0.034), ('assumptions', 0.033), ('criterion', 0.033), ('log', 0.032), ('represents', 0.032), ('functional', 0.032), ('scalar', 0.032), ('guarantees', 0.032), ('finite', 0.032), ('empirical', 0.032), ('could', 0.031), ('order', 0.031), ('problems', 0.031), ('illustrates', 0.031), ('estimation', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="86-tfidf-1" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>Author: Igor V. Cadez, Padhraic Smyth</p><p>Abstract: We investigate a general characteristic of the trade-off in learning problems between goodness-of-fit and model complexity. Specifically we characterize a general class of learning problems where the goodness-of-fit function can be shown to be convex within firstorder as a function of model complexity. This general property of</p><p>2 0.11230455 <a title="86-tfidf-2" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>Author: Shie Mannor, Ron Meir</p><p>Abstract: The problem of constructing weak classifiers for boosting algorithms is studied. We present an algorithm that produces a linear classifier that is guaranteed to achieve an error better than random guessing for any distribution on the data. While this weak learner is not useful for learning in general, we show that under reasonable conditions on the distribution it yields an effective weak learner for one-dimensional problems. Preliminary simulations suggest that similar behavior can be expected in higher dimensions, a result which is corroborated by some recent theoretical bounds. Additionally, we provide improved convergence rate bounds for the generalization error in situations where the empirical error can be made small, which is exactly the situation that occurs if weak learners with guaranteed performance that is better than random guessing can be established. 1</p><p>3 0.10298157 <a title="86-tfidf-3" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: Based on a statistical mechanics approach, we develop a method for approximately computing average case learning curves for Gaussian process regression models. The approximation works well in the large sample size limit and for arbitrary dimensionality of the input space. We explain how the approximation can be systematically improved and argue that similar techniques can be applied to general likelihood models. 1</p><p>4 0.093819439 <a title="86-tfidf-4" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>Author: Carl Edward Rasmussen, Zoubin Ghahramani</p><p>Abstract: The Bayesian paradigm apparently only sometimes gives rise to Occam's Razor; at other times very large models perform well. We give simple examples of both kinds of behaviour. The two views are reconciled when measuring complexity of functions, rather than of the machinery used to implement them. We analyze the complexity of functions for some linear in the parameter models that are equivalent to Gaussian Processes, and always find Occam's Razor at work.</p><p>5 0.091441073 <a title="86-tfidf-5" href="./nips-2000-Learning_Continuous_Distributions%3A_Simulations_With_Field_Theoretic_Priors.html">76 nips-2000-Learning Continuous Distributions: Simulations With Field Theoretic Priors</a></p>
<p>Author: Ilya Nemenman, William Bialek</p><p>Abstract: Learning of a smooth but nonparametric probability density can be regularized using methods of Quantum Field Theory. We implement a field theoretic prior numerically, test its efficacy, and show that the free parameter of the theory (,smoothness scale') can be determined self consistently by the data; this forms an infinite dimensional generalization of the MDL principle. Finally, we study the implications of one's choice of the prior and the parameterization and conclude that the smoothness scale determination makes density estimation very weakly sensitive to the choice of the prior, and that even wrong choices can be advantageous for small data sets. One of the central problems in learning is to balance 'goodness of fit' criteria against the complexity of models. An important development in the Bayesian approach was thus the realization that there does not need to be any extra penalty for model complexity: if we compute the total probability that data are generated by a model, there is a factor from the volume in parameter space-the 'Occam factor' -that discriminates against models with more parameters [1, 2]. This works remarkably welJ for systems with a finite number of parameters and creates a complexity 'razor' (after 'Occam's razor') that is almost equivalent to the celebrated Minimal Description Length (MDL) principle [3]. In addition, if the a priori distributions involved are strictly Gaussian, the ideas have also been proven to apply to some infinite-dimensional (nonparametric) problems [4]. It is not clear, however, what happens if we leave the finite dimensional setting to consider nonparametric problems which are not Gaussian, such as the estimation of a smooth probability density. A possible route to progress on the nonparametric problem was opened by noticing [5] that a Bayesian prior for density estimation is equivalent to a quantum field theory (QFT). In particular, there are field theoretic methods for computing the infinite dimensional analog of the Occam factor, at least asymptotically for large numbers of examples. These observations have led to a number of papers [6, 7, 8, 9] exploring alternative formulations and their implications for the speed of learning. Here we return to the original formulation of Ref. [5] and use numerical methods to address some of the questions left open by the analytic work [10]: What is the result of balancing the infinite dimensional Occam factor against the goodness of fit? Is the QFT inference optimal in using alJ of the information relevant for learning [II]? What happens if our learning problem is strongly atypical of the prior distribution? Following Ref. [5], if N i. i. d. samples {Xi}, i = 1 ... N, are observed, then the probability that a particular density Q(x) gave rise to these data is given by P[Q(x)l{x.}] P[Q(x)] rr~1 Q(Xi) • - J[dQ(x)]P[Q(x)] rr~1 Q(Xi) , (1) where P[Q(x)] encodes our a priori expectations of Q. Specifying this prior on a space of functions defines a QFf, and the optimal least square estimator is then Q (I{ .}) - (Q(X)Q(Xl)Q(X2) ... Q(XN)}(O) est X X. (Q(Xl)Q(X2) ... Q(XN ))(0) , (2) where ( ... )(0) means averaging with respect to the prior. Since Q(x) ~ 0, it is convenient to define an unconstrained field ¢(x), Q(x) (l/io)exp[-¢(x)]. Other definitions are also possible [6], but we think that most of our results do not depend on this choice. = The next step is to select a prior that regularizes the infinite number of degrees of freedom and allows learning. We want the prior P[¢] to make sense as a continuous theory, independent of discretization of x on small scales. We also require that when we estimate the distribution Q(x) the answer must be everywhere finite. These conditions imply that our field theory must be convergent at small length scales. For x in one dimension, a minimal choice is P[¢(x)] 1 = Z exp [£2 11 - 1 --2- f (8 dx [1 f 11 ¢)2] c5 io 8xll ] dxe-¢(x) -1 , (3) where'T/ > 1/2, Z is the normalization constant, and the c5-function enforces normalization of Q. We refer to i and 'T/ as the smoothness scale and the exponent, respectively. In [5] this theory was solved for large Nand 'T/ = 1: N (II Q(Xi))(O) ~ (4) = (5) + (6) i=1 Seff i8;¢c1 (x) where ¢cl is the 'classical' (maximum likelihood, saddle point) solution. In the effective action [Eq. (5)], it is the square root term that arises from integrating over fluctuations around the classical solution (Occam factors). It was shown that Eq. (4) is nonsingular even at finite N, that the mean value of ¢c1 converges to the negative logarithm of the target distribution P(x) very quickly, and that the variance of fluctuations 'Ij;(x) ¢(x) [- log ioP( x)] falls off as ....., 1/ iN P( x). Finally, it was speculated that if the actual i is unknown one may average over it and hope that, much as in Bayesian model selection [2], the competition between the data and the fluctuations will select the optimal smoothness scale i*. J = At the first glance the theory seems to look almost exactly like a Gaussian Process [4]. This impression is produced by a Gaussian form of the smoothness penalty in Eq. (3), and by the fluctuation determinant that plays against the goodness of fit in the smoothness scale (model) selection. However, both similarities are incomplete. The Gaussian penalty in the prior is amended by the normalization constraint, which gives rise to the exponential term in Eq. (6), and violates many familiar results that hold for Gaussian Processes, the representer theorem [12] being just one of them. In the semi--classical limit of large N, Gaussianity is restored approximately, but the classical solution is extremely non-trivial, and the fluctuation determinant is only the leading term of the Occam's razor, not the complete razor as it is for a Gaussian Process. In addition, it has no data dependence and is thus remarkably different from the usual determinants arising in the literature. The algorithm to implement the discussed density estimation procedure numerically is rather simple. First, to make the problem well posed [10, 11] we confine x to a box a ~ x ~ L with periodic boundary conditions. The boundary value problem Eq. (6) is then solved by a standard 'relaxation' (or Newton) method of iterative improvements to a guessed solution [13] (the target precision is always 10- 5 ). The independent variable x E [0,1] is discretized in equal steps [10 4 for Figs. (l.a-2.b), and 105 for Figs. (3.a, 3.b)]. We use an equally spaced grid to ensure stability of the method, while small step sizes are needed since the scale for variation of ¢el (x) is [5] (7) c5x '</p><p>6 0.09008912 <a title="86-tfidf-6" href="./nips-2000-The_Use_of_MDL_to_Select_among_Computational_Models_of_Cognition.html">139 nips-2000-The Use of MDL to Select among Computational Models of Cognition</a></p>
<p>7 0.08996091 <a title="86-tfidf-7" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>8 0.089477971 <a title="86-tfidf-8" href="./nips-2000-Mixtures_of_Gaussian_Processes.html">85 nips-2000-Mixtures of Gaussian Processes</a></p>
<p>9 0.089206263 <a title="86-tfidf-9" href="./nips-2000-Data_Clustering_by_Markovian_Relaxation_and_the_Information_Bottleneck_Method.html">38 nips-2000-Data Clustering by Markovian Relaxation and the Information Bottleneck Method</a></p>
<p>10 0.088693932 <a title="86-tfidf-10" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>11 0.086036056 <a title="86-tfidf-11" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>12 0.081977375 <a title="86-tfidf-12" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>13 0.077243946 <a title="86-tfidf-13" href="./nips-2000-On_Reversing_Jensen%27s_Inequality.html">94 nips-2000-On Reversing Jensen's Inequality</a></p>
<p>14 0.076733068 <a title="86-tfidf-14" href="./nips-2000-Second_Order_Approximations_for_Probability_Models.html">114 nips-2000-Second Order Approximations for Probability Models</a></p>
<p>15 0.076617442 <a title="86-tfidf-15" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>16 0.074336879 <a title="86-tfidf-16" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>17 0.072954074 <a title="86-tfidf-17" href="./nips-2000-A_Gradient-Based_Boosting_Algorithm_for_Regression_Problems.html">3 nips-2000-A Gradient-Based Boosting Algorithm for Regression Problems</a></p>
<p>18 0.068884298 <a title="86-tfidf-18" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>19 0.067844376 <a title="86-tfidf-19" href="./nips-2000-Some_New_Bounds_on_the_Generalization_Error_of_Combined_Classifiers.html">119 nips-2000-Some New Bounds on the Generalization Error of Combined Classifiers</a></p>
<p>20 0.066433713 <a title="86-tfidf-20" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.243), (1, 0.055), (2, 0.049), (3, -0.022), (4, 0.113), (5, -0.016), (6, -0.051), (7, -0.002), (8, -0.042), (9, -0.069), (10, -0.172), (11, 0.116), (12, 0.119), (13, -0.006), (14, 0.081), (15, 0.102), (16, 0.101), (17, -0.071), (18, -0.057), (19, 0.031), (20, -0.058), (21, -0.0), (22, -0.126), (23, -0.026), (24, 0.013), (25, -0.039), (26, -0.044), (27, -0.01), (28, 0.124), (29, -0.036), (30, -0.028), (31, 0.053), (32, 0.083), (33, 0.052), (34, -0.173), (35, 0.02), (36, 0.05), (37, -0.052), (38, 0.0), (39, -0.074), (40, 0.013), (41, 0.033), (42, 0.009), (43, 0.094), (44, 0.064), (45, -0.036), (46, 0.001), (47, 0.05), (48, 0.119), (49, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96628833 <a title="86-lsi-1" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>Author: Igor V. Cadez, Padhraic Smyth</p><p>Abstract: We investigate a general characteristic of the trade-off in learning problems between goodness-of-fit and model complexity. Specifically we characterize a general class of learning problems where the goodness-of-fit function can be shown to be convex within firstorder as a function of model complexity. This general property of</p><p>2 0.63315755 <a title="86-lsi-2" href="./nips-2000-The_Use_of_MDL_to_Select_among_Computational_Models_of_Cognition.html">139 nips-2000-The Use of MDL to Select among Computational Models of Cognition</a></p>
<p>Author: In Jae Myung, Mark A. Pitt, Shaobo Zhang, Vijay Balasubramanian</p><p>Abstract: How should we decide among competing explanations of a cognitive process given limited observations? The problem of model selection is at the heart of progress in cognitive science. In this paper, Minimum Description Length (MDL) is introduced as a method for selecting among computational models of cognition. We also show that differential geometry provides an intuitive understanding of what drives model selection in MDL. Finally, adequacy of MDL is demonstrated in two areas of cognitive modeling. 1 Model Selection and Model Complexity The development and testing of computational models of cognitive processing are a central focus in cognitive science. A model embodies a solution to a problem whose adequacy is evaluated by its ability to mimic behavior by capturing the regularities underlying observed data. This enterprise of model selection is challenging because of the competing goals that must be satisfied. Traditionally, computational models of cognition have been compared using one of many goodness-of-fit measures. However, use of such a measure can result in the choice of a model that over-fits the data, one that captures idiosyncracies in the particular data set (i.e., noise) over and above the underlying regularities of interest. Such models are considered complex, in that the inherent flexibility in the model enables it to fit diverse patterns of data. As a group, they can be characterized as having many parameters that are combined in a highly nonlinear fashion in the model equation. They do not assume a single structure in the data. Rather, the model contains multiple structures; each obtained by finely tuning the parameter values of the model, and thus can fit a wide range of data patterns. In contrast, simple models, frequently with few parameters, assume a specific structure in the data, which will manifest itself as a narrow range of similar data patterns. Only when one of these patterns occurs will the model fit the data well. The problem of over-fitting data due to model complexity suggests that the goal of model selection should instead be to select the model that generalizes best to all data samples that arise from the same underlying regularity, thus capturing only the regularity, not the noise. To achieve this goal, the selection method must be sensitive to the complexity of a model. There are at least two independent dimensions of model complexity. They are the number of free parameters of a model and its functional form, which refers to the way the parameters are combined in the model equation. For instance, it seems unlikely that two one-parameter models, y = ex and y = x 9, are equally complex in their ability to fit data. The two dimensions of model complexity (number of parameters and functional form) and their interplay can improve a model's fit to the data, without necessarily improving generalizability. The trademark of a good model selection procedure, then, is its ability to satisfy two opposing goals. A model must be sufficiently complex to describe the data sample accurately, but without over-fitting the data and thus losing generalizability. To achieve this end, we need a theoretically well-justified measure of model complexity that takes into account the number of parameters and the functional form of a model. In this paper, we introduce Minimum Description Length (MDL) as an appropriate method of selecting among mathematical models of cognition. We also show that MDL has an elegant geometric interpretation that provides a clear, intuitive understanding of the meaning of complexity in MDL. Finally, application examples of MDL are presented in two areas of cognitive modeling. 1.1 Minimum Description Length The central thesis of model selection is the estimation of a model's generalizability. One approach to assessing generalizability is the Minimum Description Length (MDL) principle [1]. It provides a theoretically well-grounded measure of complexity that is sensitive to both dimensions of complexity and also lends itself to intuitive, geometric interpretations. MDL was developed within algorithmic coding theory to choose the model that permits the greatest compression of data. A model family f with parameters e assigns the likelihood f(yle) to a given set of observed data y . The full form of the MDL measure for such a model family is given below. MDL = -In! (yISA) + ~ln( ; )+ In f dS.jdetl(S) where SA is the parameter that maximizes the likelihood, k is the number of parameters in the model, N is the sample size and I(e) is the Fisher information matrix. MDL is the length in bits of the shortest possible code that describes the data with the help of a model. In the context of cognitive modeling, the model that minimizes MDL uncovers the greatest amount of regularity (i.e., knowledge) underlying the data and therefore should be selected. The first, maximized log likelihood term is the lack-of-fit measure, and the second and third terms constitute the intrinsic complexity of the model. In particular, the third term captures the effects of complexity due to functional form, reflected through I(e). We will call the latter two terms together the geometric complexity of the model, for reasons that will become clear in the remainder of this paper. MDL arises as a finite series of terms in an asymptotic expansion of the Bayesian posterior probability of a model given the data for a special form of the parameter prior density [2] . Hence in essence, minimization of MDL is equivalent to maximization of the Bayesian posterior probability. In this paper we present a geometric interpretation of MDL, as well as Bayesian model selection [3], that provides an elegant and intuitive framework for understanding model complexity, a central concept in model selection. 2 Differential Geometric Interpretation of MDL From a geometric perspective, a parametric model family of probability distributions forms a Riemannian manifold embedded in the space of all probability distributions [4]. Every distribution is a point in this space, and the collection of points created by varying the parameters of the model gives rise to a hyper-surface in which</p><p>3 0.59859514 <a title="86-lsi-3" href="./nips-2000-Mixtures_of_Gaussian_Processes.html">85 nips-2000-Mixtures of Gaussian Processes</a></p>
<p>Author: Volker Tresp</p><p>Abstract: We introduce the mixture of Gaussian processes (MGP) model which is useful for applications in which the optimal bandwidth of a map is input dependent. The MGP is derived from the mixture of experts model and can also be used for modeling general conditional probability densities. We discuss how Gaussian processes -in particular in form of Gaussian process classification, the support vector machine and the MGP modelcan be used for quantifying the dependencies in graphical models.</p><p>4 0.55708712 <a title="86-lsi-4" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>Author: Carl Edward Rasmussen, Zoubin Ghahramani</p><p>Abstract: The Bayesian paradigm apparently only sometimes gives rise to Occam's Razor; at other times very large models perform well. We give simple examples of both kinds of behaviour. The two views are reconciled when measuring complexity of functions, rather than of the machinery used to implement them. We analyze the complexity of functions for some linear in the parameter models that are equivalent to Gaussian Processes, and always find Occam's Razor at work.</p><p>5 0.55275601 <a title="86-lsi-5" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>Author: Sumio Watanabe</p><p>Abstract: Algebraic geometry is essential to learning theory. In hierarchical learning machines such as layered neural networks and gaussian mixtures, the asymptotic normality does not hold , since Fisher information matrices are singular. In this paper , the rigorous asymptotic form of the stochastic complexity is clarified based on resolution of singularities and two different problems are studied. (1) If the prior is positive, then the stochastic complexity is far smaller than BIO, resulting in the smaller generalization error than regular statistical models, even when the true distribution is not contained in the parametric model. (2) If Jeffreys' prior, which is coordinate free and equal to zero at singularities, is employed then the stochastic complexity has the same form as BIO. It is useful for model selection, but not for generalization. 1</p><p>6 0.52375984 <a title="86-lsi-6" href="./nips-2000-Learning_Continuous_Distributions%3A_Simulations_With_Field_Theoretic_Priors.html">76 nips-2000-Learning Continuous Distributions: Simulations With Field Theoretic Priors</a></p>
<p>7 0.52110732 <a title="86-lsi-7" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>8 0.49608442 <a title="86-lsi-8" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>9 0.469311 <a title="86-lsi-9" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>10 0.45969647 <a title="86-lsi-10" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>11 0.44886622 <a title="86-lsi-11" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>12 0.44355661 <a title="86-lsi-12" href="./nips-2000-From_Mixtures_of_Mixtures_to_Adaptive_Transform_Coding.html">59 nips-2000-From Mixtures of Mixtures to Adaptive Transform Coding</a></p>
<p>13 0.4435057 <a title="86-lsi-13" href="./nips-2000-A_Gradient-Based_Boosting_Algorithm_for_Regression_Problems.html">3 nips-2000-A Gradient-Based Boosting Algorithm for Regression Problems</a></p>
<p>14 0.42338961 <a title="86-lsi-14" href="./nips-2000-Beyond_Maximum_Likelihood_and_Density_Estimation%3A_A_Sample-Based_Criterion_for_Unsupervised_Learning_of_Complex_Models.html">31 nips-2000-Beyond Maximum Likelihood and Density Estimation: A Sample-Based Criterion for Unsupervised Learning of Complex Models</a></p>
<p>15 0.40626395 <a title="86-lsi-15" href="./nips-2000-Some_New_Bounds_on_the_Generalization_Error_of_Combined_Classifiers.html">119 nips-2000-Some New Bounds on the Generalization Error of Combined Classifiers</a></p>
<p>16 0.40172696 <a title="86-lsi-16" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>17 0.39741439 <a title="86-lsi-17" href="./nips-2000-Efficient_Learning_of_Linear_Perceptrons.html">44 nips-2000-Efficient Learning of Linear Perceptrons</a></p>
<p>18 0.38833103 <a title="86-lsi-18" href="./nips-2000-Data_Clustering_by_Markovian_Relaxation_and_the_Information_Bottleneck_Method.html">38 nips-2000-Data Clustering by Markovian Relaxation and the Information Bottleneck Method</a></p>
<p>19 0.38497236 <a title="86-lsi-19" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>20 0.3843652 <a title="86-lsi-20" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.021), (17, 0.114), (32, 0.01), (33, 0.035), (55, 0.021), (62, 0.507), (65, 0.02), (67, 0.059), (76, 0.071), (81, 0.015), (90, 0.025), (97, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99348134 <a title="86-lda-1" href="./nips-2000-Hierarchical_Memory-Based_Reinforcement_Learning.html">63 nips-2000-Hierarchical Memory-Based Reinforcement Learning</a></p>
<p>Author: Natalia Hernandez-Gardiol, Sridhar Mahadevan</p><p>Abstract: A key challenge for reinforcement learning is scaling up to large partially observable domains. In this paper, we show how a hierarchy of behaviors can be used to create and select among variable length short-term memories appropriate for a task. At higher levels in the hierarchy, the agent abstracts over lower-level details and looks back over a variable number of high-level decisions in time. We formalize this idea in a framework called Hierarchical Suffix Memory (HSM). HSM uses a memory-based SMDP learning method to rapidly propagate delayed reward across long decision sequences. We describe a detailed experimental study comparing memory vs. hierarchy using the HSM framework on a realistic corridor navigation task. 1</p><p>2 0.9859603 <a title="86-lda-2" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>Author: Geoffrey J. Gordon</p><p>Abstract: Many algorithms for approximate reinforcement learning are not known to converge. In fact, there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point. This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region. The algorithms are SARSA(O) and V(O); the latter algorithm was used in the well-known TD-Gammon program. 1</p><p>same-paper 3 0.95959908 <a title="86-lda-3" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>Author: Igor V. Cadez, Padhraic Smyth</p><p>Abstract: We investigate a general characteristic of the trade-off in learning problems between goodness-of-fit and model complexity. Specifically we characterize a general class of learning problems where the goodness-of-fit function can be shown to be convex within firstorder as a function of model complexity. This general property of</p><p>4 0.79766768 <a title="86-lda-4" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>Author: Anders Jonsson, Andrew G. Barto</p><p>Abstract: Learning a complex task can be significantly facilitated by defining a hierarchy of subtasks. An agent can learn to choose between various temporally abstract actions, each solving an assigned subtask, to accomplish the overall task. In this paper, we study hierarchical learning using the framework of options. We argue that to take full advantage of hierarchical structure, one should perform option-specific state abstraction, and that if this is to scale to larger tasks, state abstraction should be automated. We adapt McCallum's U-Tree algorithm to automatically build option-specific representations of the state feature space, and we illustrate the resulting algorithm using a simple hierarchical task. Results suggest that automated option-specific state abstraction is an attractive approach to making hierarchical learning systems more effective.</p><p>5 0.77371192 <a title="86-lda-5" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>Author: Christian R. Shelton</p><p>Abstract: For many problems which would be natural for reinforcement learning, the reward signal is not a single scalar value but has multiple scalar components. Examples of such problems include agents with multiple goals and agents with multiple users. Creating a single reward value by combining the multiple components can throwaway vital information and can lead to incorrect solutions. We describe the multiple reward source problem and discuss the problems with applying traditional reinforcement learning. We then present an new algorithm for finding a solution and results on simulated environments.</p><p>6 0.75736737 <a title="86-lda-6" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>7 0.71679187 <a title="86-lda-7" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>8 0.70837635 <a title="86-lda-8" href="./nips-2000-Programmable_Reinforcement_Learning_Agents.html">105 nips-2000-Programmable Reinforcement Learning Agents</a></p>
<p>9 0.6495139 <a title="86-lda-9" href="./nips-2000-Learning_Switching_Linear_Models_of_Human_Motion.html">80 nips-2000-Learning Switching Linear Models of Human Motion</a></p>
<p>10 0.64253992 <a title="86-lda-10" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>11 0.60963249 <a title="86-lda-11" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>12 0.609299 <a title="86-lda-12" href="./nips-2000-Robust_Reinforcement_Learning.html">113 nips-2000-Robust Reinforcement Learning</a></p>
<p>13 0.58897072 <a title="86-lda-13" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>14 0.5852493 <a title="86-lda-14" href="./nips-2000-The_Use_of_MDL_to_Select_among_Computational_Models_of_Cognition.html">139 nips-2000-The Use of MDL to Select among Computational Models of Cognition</a></p>
<p>15 0.58032852 <a title="86-lda-15" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>16 0.5769009 <a title="86-lda-16" href="./nips-2000-Recognizing_Hand-written_Digits_Using_Hierarchical_Products_of_Experts.html">108 nips-2000-Recognizing Hand-written Digits Using Hierarchical Products of Experts</a></p>
<p>17 0.56510818 <a title="86-lda-17" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<p>18 0.56348324 <a title="86-lda-18" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>19 0.56086582 <a title="86-lda-19" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>20 0.55250794 <a title="86-lda-20" href="./nips-2000-Algorithms_for_Non-negative_Matrix_Factorization.html">22 nips-2000-Algorithms for Non-negative Matrix Factorization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
