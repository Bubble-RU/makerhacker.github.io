<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-86" href="#">nips2000-86</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</h1>
<br/><p>Source: <a title="nips-2000-86-pdf" href="http://papers.nips.cc/paper/1865-model-complexity-goodness-of-fit-and-diminishing-returns.pdf">pdf</a></p><p>Author: Igor V. Cadez, Padhraic Smyth</p><p>Abstract: We investigate a general characteristic of the trade-off in learning problems between goodness-of-fit and model complexity. Specifically we characterize a general class of learning problems where the goodness-of-fit function can be shown to be convex within firstorder as a function of model complexity. This general property of</p><p>Reference: <a title="nips-2000-86-reference" href="../nips2000_reference/nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('yk', 0.401), ('concav', 0.348), ('loss', 0.322), ('mixt', 0.238), ('xi', 0.223), ('regress', 0.205), ('convex', 0.205), ('pdf', 0.161), ('ez', 0.154), ('gk', 0.149), ('cadez', 0.137), ('smy', 0.137), ('complex', 0.122), ('yz', 0.118), ('model', 0.114), ('dimin', 0.103), ('ilgk', 0.103), ('jg', 0.103), ('fit', 0.096), ('tayl', 0.093)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="86-tfidf-1" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>Author: Igor V. Cadez, Padhraic Smyth</p><p>Abstract: We investigate a general characteristic of the trade-off in learning problems between goodness-of-fit and model complexity. Specifically we characterize a general class of learning problems where the goodness-of-fit function can be shown to be convex within firstorder as a function of model complexity. This general property of</p><p>2 0.12388307 <a title="86-tfidf-2" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>Author: Shie Mannor, Ron Meir</p><p>Abstract: The problem of constructing weak classifiers for boosting algorithms is studied. We present an algorithm that produces a linear classifier that is guaranteed to achieve an error better than random guessing for any distribution on the data. While this weak learner is not useful for learning in general, we show that under reasonable conditions on the distribution it yields an effective weak learner for one-dimensional problems. Preliminary simulations suggest that similar behavior can be expected in higher dimensions, a result which is corroborated by some recent theoretical bounds. Additionally, we provide improved convergence rate bounds for the generalization error in situations where the empirical error can be made small, which is exactly the situation that occurs if weak learners with guaranteed performance that is better than random guessing can be established. 1</p><p>3 0.12008113 <a title="86-tfidf-3" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>Author: DÃ¶rthe Malzahn, Manfred Opper</p><p>Abstract: Based on a statistical mechanics approach, we develop a method for approximately computing average case learning curves for Gaussian process regression models. The approximation works well in the large sample size limit and for arbitrary dimensionality of the input space. We explain how the approximation can be systematically improved and argue that similar techniques can be applied to general likelihood models. 1</p><p>4 0.10412896 <a title="86-tfidf-4" href="./nips-2000-On_Reversing_Jensen%27s_Inequality.html">94 nips-2000-On Reversing Jensen's Inequality</a></p>
<p>Author: Tony Jebara, Alex Pentland</p><p>Abstract: Jensen's inequality is a powerful mathematical tool and one of the workhorses in statistical learning. Its applications therein include the EM algorithm, Bayesian estimation and Bayesian inference. Jensen computes simple lower bounds on otherwise intractable quantities such as products of sums and latent log-likelihoods. This simplification then permits operations like integration and maximization. Quite often (i.e. in discriminative learning) upper bounds are needed as well. We derive and prove an efficient analytic inequality that provides such variational upper bounds. This inequality holds for latent variable mixtures of exponential family distributions and thus spans a wide range of contemporary statistical models. We also discuss applications of the upper bounds including maximum conditional likelihood, large margin discriminative models and conditional Bayesian inference. Convergence, efficiency and prediction results are shown. 1</p><p>5 0.10223608 <a title="86-tfidf-5" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>Author: Zoubin Ghahramani, Matthew J. Beal</p><p>Abstract: Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set. 1</p><p>6 0.096649624 <a title="86-tfidf-6" href="./nips-2000-Data_Clustering_by_Markovian_Relaxation_and_the_Information_Bottleneck_Method.html">38 nips-2000-Data Clustering by Markovian Relaxation and the Information Bottleneck Method</a></p>
<p>7 0.094647691 <a title="86-tfidf-7" href="./nips-2000-Learning_Continuous_Distributions%3A_Simulations_With_Field_Theoretic_Priors.html">76 nips-2000-Learning Continuous Distributions: Simulations With Field Theoretic Priors</a></p>
<p>8 0.093939818 <a title="86-tfidf-8" href="./nips-2000-From_Mixtures_of_Mixtures_to_Adaptive_Transform_Coding.html">59 nips-2000-From Mixtures of Mixtures to Adaptive Transform Coding</a></p>
<p>9 0.091841608 <a title="86-tfidf-9" href="./nips-2000-Mixtures_of_Gaussian_Processes.html">85 nips-2000-Mixtures of Gaussian Processes</a></p>
<p>10 0.087437667 <a title="86-tfidf-10" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>11 0.080950893 <a title="86-tfidf-11" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>12 0.079948835 <a title="86-tfidf-12" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>13 0.078830712 <a title="86-tfidf-13" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>14 0.077969372 <a title="86-tfidf-14" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>15 0.077409714 <a title="86-tfidf-15" href="./nips-2000-A_Gradient-Based_Boosting_Algorithm_for_Regression_Problems.html">3 nips-2000-A Gradient-Based Boosting Algorithm for Regression Problems</a></p>
<p>16 0.075377405 <a title="86-tfidf-16" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<p>17 0.070798814 <a title="86-tfidf-17" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>18 0.068069406 <a title="86-tfidf-18" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>19 0.06797763 <a title="86-tfidf-19" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>20 0.066641673 <a title="86-tfidf-20" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.223), (1, 0.072), (2, 0.003), (3, 0.038), (4, 0.126), (5, 0.056), (6, -0.033), (7, -0.027), (8, 0.006), (9, -0.064), (10, -0.077), (11, 0.023), (12, -0.049), (13, -0.117), (14, -0.04), (15, -0.049), (16, -0.162), (17, -0.001), (18, -0.02), (19, -0.059), (20, 0.153), (21, 0.097), (22, -0.024), (23, -0.101), (24, -0.028), (25, -0.122), (26, 0.061), (27, 0.032), (28, 0.116), (29, 0.052), (30, 0.105), (31, -0.038), (32, -0.128), (33, -0.076), (34, 0.151), (35, -0.128), (36, 0.007), (37, -0.013), (38, 0.126), (39, -0.133), (40, -0.07), (41, 0.065), (42, 0.108), (43, 0.038), (44, -0.017), (45, 0.017), (46, 0.014), (47, -0.072), (48, -0.059), (49, 0.089)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93125087 <a title="86-lsi-1" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>Author: Igor V. Cadez, Padhraic Smyth</p><p>Abstract: We investigate a general characteristic of the trade-off in learning problems between goodness-of-fit and model complexity. Specifically we characterize a general class of learning problems where the goodness-of-fit function can be shown to be convex within firstorder as a function of model complexity. This general property of</p><p>2 0.62660968 <a title="86-lsi-2" href="./nips-2000-Mixtures_of_Gaussian_Processes.html">85 nips-2000-Mixtures of Gaussian Processes</a></p>
<p>Author: Volker Tresp</p><p>Abstract: We introduce the mixture of Gaussian processes (MGP) model which is useful for applications in which the optimal bandwidth of a map is input dependent. The MGP is derived from the mixture of experts model and can also be used for modeling general conditional probability densities. We discuss how Gaussian processes -in particular in form of Gaussian process classification, the support vector machine and the MGP modelcan be used for quantifying the dependencies in graphical models.</p><p>3 0.518879 <a title="86-lsi-3" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>Author: Sumio Watanabe</p><p>Abstract: Algebraic geometry is essential to learning theory. In hierarchical learning machines such as layered neural networks and gaussian mixtures, the asymptotic normality does not hold , since Fisher information matrices are singular. In this paper , the rigorous asymptotic form of the stochastic complexity is clarified based on resolution of singularities and two different problems are studied. (1) If the prior is positive, then the stochastic complexity is far smaller than BIO, resulting in the smaller generalization error than regular statistical models, even when the true distribution is not contained in the parametric model. (2) If Jeffreys' prior, which is coordinate free and equal to zero at singularities, is employed then the stochastic complexity has the same form as BIO. It is useful for model selection, but not for generalization. 1</p><p>4 0.5178715 <a title="86-lsi-4" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>Author: Charles Dugas, Yoshua Bengio, FranÃ§ois BÃ©lisle, Claude Nadeau, RenÃ© Garcia</p><p>Abstract: Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in two of its arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of continuous functions with these and other properties. We apply this new class of functions to the task of modeling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori constraints.</p><p>5 0.50475079 <a title="86-lsi-5" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>Author: Oliver B. Downs</p><p>Abstract: Recent work has exploited boundedness of data in the unsupervised learning of new types of generative model. For nonnegative data it was recently shown that the maximum-entropy generative model is a Nonnegative Boltzmann Distribution not a Gaussian distribution, when the model is constrained to match the first and second order statistics of the data. Learning for practical sized problems is made difficult by the need to compute expectations under the model distribution. The computational cost of Markov chain Monte Carlo methods and low fidelity of naive mean field techniques has led to increasing interest in advanced mean field theories and variational methods. Here I present a secondorder mean-field approximation for the Nonnegative Boltzmann Machine model, obtained using a</p><p>6 0.4685086 <a title="86-lsi-6" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>7 0.45066643 <a title="86-lsi-7" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>8 0.44843814 <a title="86-lsi-8" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>9 0.44826111 <a title="86-lsi-9" href="./nips-2000-The_Use_of_MDL_to_Select_among_Computational_Models_of_Cognition.html">139 nips-2000-The Use of MDL to Select among Computational Models of Cognition</a></p>
<p>10 0.44109768 <a title="86-lsi-10" href="./nips-2000-On_Reversing_Jensen%27s_Inequality.html">94 nips-2000-On Reversing Jensen's Inequality</a></p>
<p>11 0.425964 <a title="86-lsi-11" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>12 0.41990739 <a title="86-lsi-12" href="./nips-2000-Learning_Continuous_Distributions%3A_Simulations_With_Field_Theoretic_Priors.html">76 nips-2000-Learning Continuous Distributions: Simulations With Field Theoretic Priors</a></p>
<p>13 0.4142752 <a title="86-lsi-13" href="./nips-2000-Beyond_Maximum_Likelihood_and_Density_Estimation%3A_A_Sample-Based_Criterion_for_Unsupervised_Learning_of_Complex_Models.html">31 nips-2000-Beyond Maximum Likelihood and Density Estimation: A Sample-Based Criterion for Unsupervised Learning of Complex Models</a></p>
<p>14 0.40933174 <a title="86-lsi-14" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>15 0.40876752 <a title="86-lsi-15" href="./nips-2000-A_Gradient-Based_Boosting_Algorithm_for_Regression_Problems.html">3 nips-2000-A Gradient-Based Boosting Algorithm for Regression Problems</a></p>
<p>16 0.39771095 <a title="86-lsi-16" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>17 0.39272261 <a title="86-lsi-17" href="./nips-2000-Vicinal_Risk_Minimization.html">144 nips-2000-Vicinal Risk Minimization</a></p>
<p>18 0.374677 <a title="86-lsi-18" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>19 0.37329614 <a title="86-lsi-19" href="./nips-2000-Data_Clustering_by_Markovian_Relaxation_and_the_Information_Bottleneck_Method.html">38 nips-2000-Data Clustering by Markovian Relaxation and the Information Bottleneck Method</a></p>
<p>20 0.37248525 <a title="86-lsi-20" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.011), (9, 0.114), (11, 0.028), (16, 0.077), (21, 0.013), (38, 0.038), (54, 0.485), (74, 0.032), (76, 0.019), (84, 0.09)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97648549 <a title="86-lda-1" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>Author: Jakob CarlstrÃ¶m</p><p>Abstract: This paper presents predictive gain scheduling, a technique for simplifying reinforcement learning problems by decomposition. Link admission control of self-similar call traffic is used to demonstrate the technique. The control problem is decomposed into on-line prediction of near-future call arrival rates, and precomputation of policies for Poisson call arrival processes. At decision time, the predictions are used to select among the policies. Simulations show that this technique results in significantly faster learning without any performance loss, compared to a reinforcement learning controller that does not decompose the problem. 1</p><p>2 0.90474313 <a title="86-lda-2" href="./nips-2000-Bayes_Networks_on_Ice%3A_Robotic_Search_for_Antarctic_Meteorites.html">29 nips-2000-Bayes Networks on Ice: Robotic Search for Antarctic Meteorites</a></p>
<p>Author: Liam Pedersen, Dimitrios Apostolopoulos, William Whittaker</p><p>Abstract: A Bayes network based classifier for distinguishing terrestrial rocks from meteorites is implemented onboard the Nomad robot. Equipped with a camera, spectrometer and eddy current sensor, this robot searched the ice sheets of Antarctica and autonomously made the first robotic identification of a meteorite, in January 2000 at the Elephant Moraine. This paper discusses rock classification from a robotic platform, and describes the system onboard Nomad. 1</p><p>same-paper 3 0.88975084 <a title="86-lda-3" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>Author: Igor V. Cadez, Padhraic Smyth</p><p>Abstract: We investigate a general characteristic of the trade-off in learning problems between goodness-of-fit and model complexity. Specifically we characterize a general class of learning problems where the goodness-of-fit function can be shown to be convex within firstorder as a function of model complexity. This general property of</p><p>4 0.82798159 <a title="86-lda-4" href="./nips-2000-Hierarchical_Memory-Based_Reinforcement_Learning.html">63 nips-2000-Hierarchical Memory-Based Reinforcement Learning</a></p>
<p>Author: Natalia Hernandez-Gardiol, Sridhar Mahadevan</p><p>Abstract: A key challenge for reinforcement learning is scaling up to large partially observable domains. In this paper, we show how a hierarchy of behaviors can be used to create and select among variable length short-term memories appropriate for a task. At higher levels in the hierarchy, the agent abstracts over lower-level details and looks back over a variable number of high-level decisions in time. We formalize this idea in a framework called Hierarchical Suffix Memory (HSM). HSM uses a memory-based SMDP learning method to rapidly propagate delayed reward across long decision sequences. We describe a detailed experimental study comparing memory vs. hierarchy using the HSM framework on a realistic corridor navigation task. 1</p><p>5 0.64492941 <a title="86-lda-5" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>Author: Justin A. Boyan, Michael L. Littman</p><p>Abstract: We describe an extension of the Markov decision process model in which a continuous time dimension is included in the state space. This allows for the representation and exact solution of a wide range of problems in which transitions or rewards vary over time. We examine problems based on route planning with public transportation and telescope observation scheduling. 1</p><p>6 0.61597699 <a title="86-lda-6" href="./nips-2000-Hippocampally-Dependent_Consolidation_in_a_Hierarchical_Model_of_Neocortex.html">66 nips-2000-Hippocampally-Dependent Consolidation in a Hierarchical Model of Neocortex</a></p>
<p>7 0.60653514 <a title="86-lda-7" href="./nips-2000-The_Use_of_MDL_to_Select_among_Computational_Models_of_Cognition.html">139 nips-2000-The Use of MDL to Select among Computational Models of Cognition</a></p>
<p>8 0.59910631 <a title="86-lda-8" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>9 0.58916748 <a title="86-lda-9" href="./nips-2000-Robust_Reinforcement_Learning.html">113 nips-2000-Robust Reinforcement Learning</a></p>
<p>10 0.58845145 <a title="86-lda-10" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>11 0.58769298 <a title="86-lda-11" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>12 0.57073522 <a title="86-lda-12" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>13 0.55329037 <a title="86-lda-13" href="./nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">42 nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<p>14 0.55204523 <a title="86-lda-14" href="./nips-2000-The_Missing_Link_-_A_Probabilistic_Model_of_Document_Content_and_Hypertext_Connectivity.html">136 nips-2000-The Missing Link - A Probabilistic Model of Document Content and Hypertext Connectivity</a></p>
<p>15 0.54724401 <a title="86-lda-15" href="./nips-2000-Universality_and_Individuality_in_a_Neural_Code.html">141 nips-2000-Universality and Individuality in a Neural Code</a></p>
<p>16 0.54456544 <a title="86-lda-16" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>17 0.54402757 <a title="86-lda-17" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>18 0.53691846 <a title="86-lda-18" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>19 0.53578687 <a title="86-lda-19" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>20 0.53560936 <a title="86-lda-20" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
