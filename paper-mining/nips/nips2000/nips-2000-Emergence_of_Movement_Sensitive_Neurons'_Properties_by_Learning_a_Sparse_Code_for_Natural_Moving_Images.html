<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-45" href="#">nips2000-45</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</h1>
<br/><p>Source: <a title="nips-2000-45-pdf" href="http://papers.nips.cc/paper/1937-emergence-of-movement-sensitive-neurons-properties-by-learning-a-sparse-code-for-natural-moving-images.pdf">pdf</a></p><p>Author: Rafal Bogacz, Malcolm W. Brown, Christophe G. Giraud-Carrier</p><p>Abstract: Olshausen & Field demonstrated that a learning algorithm that attempts to generate a sparse code for natural scenes develops a complete family of localised, oriented, bandpass receptive fields, similar to those of 'simple cells' in VI. This paper describes an algorithm which finds a sparse code for sequences of images that preserves information about the input. This algorithm when trained on natural video sequences develops bases representing the movement in particular directions with particular speeds, similar to the receptive fields of the movement-sensitive cells observed in cortical visual areas. Furthermore, in contrast to previous approaches to learning direction selectivity, the timing of neuronal activity encodes the phase of the movement, so the precise timing of spikes is crucially important to the information encoding.</p><p>Reference: <a title="nips-2000-45-reference" href="../nips2000_reference/nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Emergence of movement sensitive neurons' properties by learning a sparse code for natural moving images  Rafal Bogacz Dept. [sent-1, score-0.788]
</p><p>2 uk  Abstract Olshausen & Field demonstrated that a learning algorithm that attempts to generate a sparse code for natural scenes develops a complete family of localised, oriented, bandpass receptive fields, similar to those of 'simple cells' in VI. [sent-25, score-0.762]
</p><p>3 This paper describes an algorithm which finds a sparse code for sequences of images that preserves information about the input. [sent-26, score-0.859]
</p><p>4 This algorithm when trained on natural video sequences develops bases representing the movement in particular directions with particular speeds, similar to the receptive fields of the movement-sensitive cells observed in cortical visual areas. [sent-27, score-1.456]
</p><p>5 Furthermore, in contrast to previous approaches to learning direction selectivity, the timing of neuronal activity encodes the phase of the movement, so the precise timing of spikes is crucially important to the information encoding. [sent-28, score-0.616]
</p><p>6 1 Introduction It was suggested by Barlow [3] that the goal of early sensory processing is to reduce  redundancy in sensory information and the activity of sensory neurons encodes independent features. [sent-29, score-0.505]
</p><p>7 Atick & Redlich [1] showed that training a neural network on patches of natural images, aiming to remove pair-wise correlation between neuronal responses, results in neurons having centre-surround receptive fields resembling those of retinal ganglion neurons. [sent-31, score-0.945]
</p><p>8 Olshausen & Field [11,12] demonstrated that a learning algorithm that attempts to generate a sparse code for natural scenes while preserving information about the visual input, develops a complete family of localised, oriented, bandpass receptive fields, similar to those of simple-cells in VI. [sent-32, score-0.762]
</p><p>9 The activities of the neurons implementing this coding signal the presence of edges, which are basic components of natural images. [sent-33, score-0.297]
</p><p>10 Olshausen & Field chose their algorithm to create a sparse representation because it possesses a higher degree of statistical independence among its outputs [11]. [sent-34, score-0.253]
</p><p>11 Similar receptive fields were also obtained by training a neural net so as to make the responses of neurons as independent as possible [4]. [sent-35, score-0.397]
</p><p>12 Other authors [14,16,5] have shown that direction selectivity of the simple-cells may also emerge from unsupervised  learning. [sent-36, score-0.099]
</p><p>13 However, there is no agreed way of how the receptive fields of neurons that encode movements are created. [sent-37, score-0.498]
</p><p>14 This paper describes an algorithm which finds a sparse code for sequences of images that preserves the critical information about the input. [sent-38, score-0.859]
</p><p>15 This algorithm, trained on natural video images, develops bases representing movements in particular directions at particular speeds, similar to the receptive fields of the movement-sensitive cells observed in early visual areas [9,2]. [sent-39, score-1.14]
</p><p>16 The activities of the neurons implementing this encoding signal the presence of edges moving with certain speeds in certain directions, with each neuron having its preferred speed and direction. [sent-40, score-0.515]
</p><p>17 Furthermore, in contrast to all the previous approaches, the timing of neural activity encodes the movement's phase, so the precise timing of spikes is crucially important for information coding. [sent-41, score-0.535]
</p><p>18 The proposed algorithm is an extension of the one proposed by Olshausen & Field. [sent-42, score-0.222]
</p><p>19 Hence it is a high level algorithm, which cannot be directly implemented in a biologically plausible neural network. [sent-43, score-0.088]
</p><p>20 However, a plausible neural network performing a similar task can be developed. [sent-44, score-0.049]
</p><p>21 Finally, Section 5 discusses how the algorithm differs from the previous approaches, and the implications of the presented results. [sent-47, score-0.074]
</p><p>22 2  Description of the algorithm  Since the proposed algorithm is an extension of the one described by Olshausen & Field [11 ,12], this section starts with a brief introduction of the main ideas of their algorithm. [sent-48, score-0.222]
</p><p>23 They assume that an image x can be represented in terms of a linear superposition of basis functions Ai. [sent-49, score-0.372]
</p><p>24 For clarity of notation, let us represent both images and bases as vectors created by concatenating rows of pixels as shown in Figure 1, and let each number in the vector describe the brightness of the corresponding pixel. [sent-50, score-0.683]
</p><p>25 Let the basis functions Ai form the columns of a matrix A . [sent-51, score-0.117]
</p><p>26 Let the weighting of the above mentioned linear superposition (which changes from one image to the next) be given by a vector s: x=As  (1)  The image x may be encoded, for example using the inverted transformation where it exists. [sent-52, score-0.494]
</p><p>27 Hence, the image code s is determined by the choice of basis functions Ai. [sent-53, score-0.442]
</p><p>28 Olshausen & Field [11,12] try to find bases that result in a code s that preserves information about the original image x and that is sparse. [sent-54, score-0.754]
</p><p>29 Let us divide time into intervals (to be able to treat it as discrete) and denote the image observed at time t and the code generated by xt and st, respectively. [sent-56, score-0.394]
</p><p>30 The Olshausen & Field algorithm assumes that image x is a linear superposition (mixture) of s. [sent-57, score-0.329]
</p><p>31 By contrast, our algorithm assumes that images are convolved mixtures of s, i. [sent-58, score-0.337]
</p><p>32 , st depends not only on xt but also on xt-l, xt-2, . [sent-60, score-0.151]
</p><p>33 Therefore, each basis function may also be image  ~  . [sent-66, score-0.288]
</p><p>34 • II  I I I •  I  xT  Figure 1: Representing images as vectors. [sent-72, score-0.225]
</p><p>35 1'26  EE§ EE§ EE§ A/  A/  A IO  ~~~  I  Figure 2: Encoding of an image sequence. [sent-85, score-0.171]
</p><p>36 In the example, there are two basis functions, each described by T = 3 vectors. [sent-86, score-0.117]
</p><p>37 The first basis encodes movement to the right, the second encodes movement down. [sent-87, score-0.659]
</p><p>38 A sequence x of 6 images is shown on the top and the corresponding code s below. [sent-88, score-0.534]
</p><p>39 These vectors create columns of the mixing matrices A 0, A I, . [sent-97, score-0.049]
</p><p>40 1'/ describes how strongly the basis function Ai is present in the last T images. [sent-102, score-0.159]
</p><p>41 A fsf+1  (3)  f=O  In the proposed algorithm, the basis functions A are also found by optimising the cost function of Equation 2. [sent-105, score-0.257]
</p><p>42 The detailed method of this minimisation is described below, and this paragraph gives its overview. [sent-106, score-0.038]
</p><p>43 In each optimisation step, a sequence x of P image patches is selected from a random position in the video sequence (P 2: 2D. [sent-107, score-0.933]
</p><p>44 Each of the optimisation steps consists of two operations. [sent-108, score-0.134]
</p><p>45 Firstly, the sequence of coefficient vectors s which minimises the cost function E for the images x is found. [sent-109, score-0.567]
</p><p>46 Secondly, the basis matrices A are modified in the direction opposite to the gradient of E over A, thus minimising the cost function. [sent-110, score-0.356]
</p><p>47 These two operations are repeated for different sequences of image patches. [sent-111, score-0.315]
</p><p>48 In Equation 2, the term "preserved information in s about x" expresses how weJl x may be reconstructed on the basis of s. [sent-112, score-0.286]
</p><p>49 In particular, it is defined as the negative of the square of the reconstruction error. [sent-113, score-0.06]
</p><p>50 The reconstruction error is the difference between the original image sequence x and the sequence of images r reconstructed from s. [sent-114, score-0.935]
</p><p>51 The sequence r may be reconstructed from s in the foJlowing way: T- I  r' = [. [sent-115, score-0.324]
</p><p>52 A fsf+1  (4)  f=O  The precise definition of the cost function is then given by: . [sent-116, score-0.123]
</p><p>53 , Xl, xP ) may share some bases with images not in the sequence (e. [sent-120, score-0.72]
</p><p>54 To avoid this problem, only the middle images are reconstructed and only for them is the reconstruction error computed in the cost function. [sent-123, score-0.52]
</p><p>55 In particular, only images from T to P-T+l are reconstructed - since the assumed length of the bases is T, those images contain only the bases whose other  parts are also contained in the sequence. [sent-124, score-1.299]
</p><p>56 Since only images from T to P-T+1 are reconstructed, it is clear from Equation 4, that only coefficients ST to sP need to be found. [sent-125, score-0.225]
</p><p>57 These considerations explain the limits of the outer summations in both terms of Equation 5. [sent-126, score-0.038]
</p><p>58 For each image sequence, in the first operation, the coefficients ST, ST+! [sent-127, score-0.171]
</p><p>59 , sP minimising E are found using an optimisation method. [sent-131, score-0.223]
</p><p>60 •i  t  j  J  J  1  (J"  (J"  1  (6)  In the second operation, the bases A are modified so as to minimise E: (7)  In equation 7, 17 denotes the learning rate. [sent-138, score-0.497]
</p><p>61 The vector length of each basis function Ai is adapted over time so as to maintain equal variance on each coefficient s, m  exactly the same way as described in [12]. [sent-139, score-0.2]
</p><p>62 3  Methods of simulations  The proposed algorithm was implemented in Matlab except for finding s minimising E, which was implemented in C++, using the conjugate gradient method for the sake  of speed. [sent-140, score-0.315]
</p><p>63 In the implementation, the original codes of Olshausen & Field were used and modified (downloaded from http://redwood. [sent-141, score-0.044]
</p><p>64 Many parameters of the proposed algorithm were taken from [11]. [sent-145, score-0.148]
</p><p>65 In particular, C(x) = In(1+x2), cris the standard deviation of pixels' colours in the images, A is set up such that A/cr = 0. [sent-146, score-0.038]
</p><p>66 ~A is averaged over 100 image sequences, and hence the bases A are updated with the average of ~A every 100 optimisation steps. [sent-148, score-0.645]
</p><p>67 The length of an image sequence P is set up such that P = 3T. [sent-149, score-0.326]
</p><p>68 The proposed algorithm was tested on two types of video sequences: 'toy' problems and natural video sequences. [sent-150, score-0.536]
</p><p>69 Each of the toy sequences consisted of 10 frames 100x100 pixels. [sent-151, score-0.201]
</p><p>70 Each line was either horizontal or vertical and 1 pixel thick. [sent-153, score-0.039]
</p><p>71 Each line was either black or white, which corresponded to positive or negative values of the elements of x vectors (the grey background corresponded to zero). [sent-154, score-0.152]
</p><p>72 Each horizontal line moved up or down, each vertical - left or right, with the speed of one pixel per frame. [sent-155, score-0.077]
</p><p>73 Then the algorithm was tested on five natural video sequences showing moving people or animals. [sent-156, score-0.528]
</p><p>74 In each optimisation step , a sequence of image patches was selected from a randomly chosen video. [sent-157, score-0.624]
</p><p>75 First, to remove the static aspect of the images, from each frame the previous one was subtracted, i. [sent-159, score-0.106]
</p><p>76 , each image encoded the difference between two successive frames of the video. [sent-161, score-0.321]
</p><p>77 This simple operation reduces redundancy in data since the corresponding pixels in the successive frames tend to have similar colours. [sent-162, score-0.305]
</p><p>78 An analogous operation may be performed by the retina, since the ganglion cells typically respond to the changes in light intensity [10]. [sent-163, score-0.25]
</p><p>79 Then, to remove the pair-wise correlation between pixels of the same frame , Zerophase Component Analysis (ZCA) [4] was applied to each of the patches from the selected sequence, i. [sent-164, score-0.35]
</p><p>80 , W is equal to the inverted square root of the covariance matrix of x. [sent-169, score-0.068]
</p><p>81 The filters in W have centresurround receptive fields resembling those of retinal ganglion neurons [4]. [sent-170, score-0.637]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bases', 0.34), ('olshausen', 0.238), ('images', 0.225), ('bristol', 0.205), ('image', 0.171), ('reconstructed', 0.169), ('movement', 0.161), ('sequence', 0.155), ('video', 0.154), ('code', 0.154), ('receptive', 0.144), ('sequences', 0.144), ('ee', 0.142), ('optimisation', 0.134), ('fields', 0.128), ('neurons', 0.125), ('patches', 0.119), ('basis', 0.117), ('ganglion', 0.113), ('develops', 0.113), ('encodes', 0.11), ('timing', 0.107), ('speeds', 0.103), ('sparse', 0.092), ('minimising', 0.089), ('preserves', 0.089), ('fsf', 0.088), ('lub', 0.088), ('superposition', 0.084), ('coefficient', 0.083), ('st', 0.082), ('pixels', 0.08), ('natural', 0.08), ('moving', 0.076), ('corresponded', 0.076), ('localised', 0.076), ('algorithm', 0.074), ('proposed', 0.074), ('field', 0.072), ('operation', 0.07), ('xt', 0.069), ('remove', 0.068), ('inverted', 0.068), ('resembling', 0.068), ('cells', 0.067), ('cost', 0.066), ('sp', 0.063), ('movements', 0.063), ('reconstruction', 0.06), ('minimise', 0.059), ('retinal', 0.059), ('oriented', 0.059), ('xp', 0.059), ('bandpass', 0.059), ('selectivity', 0.059), ('precise', 0.057), ('frames', 0.057), ('crucially', 0.056), ('sensory', 0.055), ('equation', 0.054), ('redundancy', 0.054), ('preserved', 0.054), ('activity', 0.051), ('directions', 0.051), ('sparseness', 0.051), ('implementing', 0.051), ('encoded', 0.049), ('plausible', 0.049), ('create', 0.049), ('spikes', 0.047), ('spike', 0.047), ('scenes', 0.046), ('selected', 0.045), ('successive', 0.044), ('modified', 0.044), ('encoding', 0.043), ('describes', 0.042), ('activities', 0.041), ('neuronal', 0.041), ('direction', 0.04), ('finds', 0.039), ('horizontal', 0.039), ('implemented', 0.039), ('xl', 0.038), ('speed', 0.038), ('frame', 0.038), ('edges', 0.038), ('convolved', 0.038), ('paragraph', 0.038), ('summations', 0.038), ('colours', 0.038), ('agreed', 0.038), ('ail', 0.038), ('aio', 0.038), ('clarity', 0.038), ('cts', 0.038), ('downloaded', 0.038), ('malcolm', 0.038), ('minimises', 0.038), ('possesses', 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="45-tfidf-1" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>Author: Rafal Bogacz, Malcolm W. Brown, Christophe G. Giraud-Carrier</p><p>Abstract: Olshausen & Field demonstrated that a learning algorithm that attempts to generate a sparse code for natural scenes develops a complete family of localised, oriented, bandpass receptive fields, similar to those of 'simple cells' in VI. This paper describes an algorithm which finds a sparse code for sequences of images that preserves information about the input. This algorithm when trained on natural video sequences develops bases representing the movement in particular directions with particular speeds, similar to the receptive fields of the movement-sensitive cells observed in cortical visual areas. Furthermore, in contrast to previous approaches to learning direction selectivity, the timing of neuronal activity encodes the phase of the movement, so the precise timing of spikes is crucially important to the information encoding.</p><p>2 0.19976963 <a title="45-tfidf-2" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>Author: Michael S. Gray, Terrence J. Sejnowski, Javier R. Movellan</p><p>Abstract: We examine eight different techniques for developing visual representations in machine vision tasks. In particular we compare different versions of principal component and independent component analysis in combination with stepwise regression methods for variable selection. We found that local methods, based on the statistics of image patches, consistently outperformed global methods based on the statistics of entire images. This result is consistent with previous work on emotion and facial expression recognition. In addition, the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance. 1</p><p>3 0.16352159 <a title="45-tfidf-3" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>Author: Te-Won Lee, Thomas Wachtler, Terrence J. Sejnowski</p><p>Abstract: The human visual system encodes the chromatic signals conveyed by the three types of retinal cone photoreceptors in an opponent fashion. This color opponency has been shown to constitute an efficient encoding by spectral decorrelation of the receptor signals. We analyze the spatial and chromatic structure of natural scenes by decomposing the spectral images into a set of linear basis functions such that they constitute a representation with minimal redundancy. Independent component analysis finds the basis functions that transforms the spatiochromatic data such that the outputs (activations) are statistically as independent as possible, i.e. least redundant. The resulting basis functions show strong opponency along an achromatic direction (luminance edges), along a blueyellow direction, and along a red-blue direction. Furthermore, the resulting activations have very sparse distributions, suggesting that the use of color opponency in the human visual system achieves a highly efficient representation of colors. Our findings suggest that color opponency is a result of the properties of natural spectra and not solely a consequence of the overlapping cone spectral sensitivities. 1 Statistical structure of natural scenes Efficient encoding of visual sensory information is an important task for information processing systems and its study may provide insights into coding principles of biological visual systems. An important goal of sensory information processing Electronic version available at www. cnl. salk . edu/</p><p>4 0.13824679 <a title="45-tfidf-4" href="./nips-2000-Machine_Learning_for_Video-Based_Rendering.html">83 nips-2000-Machine Learning for Video-Based Rendering</a></p>
<p>Author: Arno Schödl, Irfan A. Essa</p><p>Abstract: We present techniques for rendering and animation of realistic scenes by analyzing and training on short video sequences. This work extends the new paradigm for computer animation, video textures, which uses recorded video to generate novel animations by replaying the video samples in a new order. Here we concentrate on video sprites, which are a special type of video texture. In video sprites, instead of storing whole images, the object of interest is separated from the background and the video samples are stored as a sequence of alpha-matted sprites with associated velocity information. They can be rendered anywhere on the screen to create a novel animation of the object. We present methods to create such animations by finding a sequence of sprite samples that is both visually smooth and follows a desired path. To estimate visual smoothness, we train a linear classifier to estimate visual similarity between video samples. If the motion path is known in advance, we use beam search to find a good sample sequence. We can specify the motion interactively by precomputing the sequence cost function using Q-Iearning.</p><p>5 0.13344018 <a title="45-tfidf-5" href="./nips-2000-Learning_Joint_Statistical_Models_for_Audio-Visual_Fusion_and_Segregation.html">78 nips-2000-Learning Joint Statistical Models for Audio-Visual Fusion and Segregation</a></p>
<p>Author: John W. Fisher III, Trevor Darrell, William T. Freeman, Paul A. Viola</p><p>Abstract: People can understand complex auditory and visual information, often using one to disambiguate the other. Automated analysis, even at a lowlevel, faces severe challenges, including the lack of accurate statistical models for the signals, and their high-dimensionality and varied sampling rates. Previous approaches [6] assumed simple parametric models for the joint distribution which, while tractable, cannot capture the complex signal relationships. We learn the joint distribution of the visual and auditory signals using a non-parametric approach. First, we project the data into a maximally informative, low-dimensional subspace, suitable for density estimation. We then model the complicated stochastic relationships between the signals using a nonparametric density estimator. These learned densities allow processing across signal modalities. We demonstrate, on synthetic and real signals, localization in video of the face that is speaking in audio, and, conversely, audio enhancement of a particular speaker selected from the video.</p><p>6 0.12733179 <a title="45-tfidf-6" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>7 0.11399791 <a title="45-tfidf-7" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>8 0.10215869 <a title="45-tfidf-8" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>9 0.10105678 <a title="45-tfidf-9" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>10 0.099519089 <a title="45-tfidf-10" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>11 0.098280154 <a title="45-tfidf-11" href="./nips-2000-FaceSync%3A_A_Linear_Operator_for_Measuring_Synchronization_of_Video_Facial_Images_and_Audio_Tracks.html">50 nips-2000-FaceSync: A Linear Operator for Measuring Synchronization of Video Facial Images and Audio Tracks</a></p>
<p>12 0.093483612 <a title="45-tfidf-12" href="./nips-2000-The_Manhattan_World_Assumption%3A_Regularities_in_Scene_Statistics_which_Enable_Bayesian_Inference.html">135 nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</a></p>
<p>13 0.09017086 <a title="45-tfidf-13" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>14 0.087168574 <a title="45-tfidf-14" href="./nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">129 nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<p>15 0.087115265 <a title="45-tfidf-15" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>16 0.079602025 <a title="45-tfidf-16" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>17 0.078735799 <a title="45-tfidf-17" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>18 0.075809754 <a title="45-tfidf-18" href="./nips-2000-Who_Does_What%3F_A_Novel_Algorithm_to_Determine_Function_Localization.html">147 nips-2000-Who Does What? A Novel Algorithm to Determine Function Localization</a></p>
<p>19 0.075366624 <a title="45-tfidf-19" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>20 0.066760249 <a title="45-tfidf-20" href="./nips-2000-Probabilistic_Semantic_Video_Indexing.html">103 nips-2000-Probabilistic Semantic Video Indexing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.242), (1, -0.176), (2, 0.006), (3, 0.172), (4, -0.067), (5, 0.008), (6, 0.244), (7, -0.031), (8, -0.014), (9, -0.069), (10, -0.11), (11, -0.138), (12, -0.079), (13, 0.063), (14, -0.142), (15, 0.083), (16, -0.06), (17, 0.116), (18, -0.077), (19, -0.174), (20, 0.025), (21, -0.17), (22, -0.019), (23, -0.037), (24, -0.029), (25, -0.104), (26, 0.055), (27, -0.008), (28, 0.027), (29, -0.077), (30, 0.037), (31, 0.033), (32, 0.081), (33, -0.002), (34, -0.057), (35, 0.069), (36, 0.001), (37, 0.069), (38, 0.041), (39, 0.007), (40, -0.042), (41, -0.054), (42, -0.003), (43, 0.069), (44, -0.024), (45, -0.113), (46, -0.067), (47, -0.144), (48, 0.025), (49, 0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9781248 <a title="45-lsi-1" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>Author: Rafal Bogacz, Malcolm W. Brown, Christophe G. Giraud-Carrier</p><p>Abstract: Olshausen & Field demonstrated that a learning algorithm that attempts to generate a sparse code for natural scenes develops a complete family of localised, oriented, bandpass receptive fields, similar to those of 'simple cells' in VI. This paper describes an algorithm which finds a sparse code for sequences of images that preserves information about the input. This algorithm when trained on natural video sequences develops bases representing the movement in particular directions with particular speeds, similar to the receptive fields of the movement-sensitive cells observed in cortical visual areas. Furthermore, in contrast to previous approaches to learning direction selectivity, the timing of neuronal activity encodes the phase of the movement, so the precise timing of spikes is crucially important to the information encoding.</p><p>2 0.80622751 <a title="45-lsi-2" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>Author: Te-Won Lee, Thomas Wachtler, Terrence J. Sejnowski</p><p>Abstract: The human visual system encodes the chromatic signals conveyed by the three types of retinal cone photoreceptors in an opponent fashion. This color opponency has been shown to constitute an efficient encoding by spectral decorrelation of the receptor signals. We analyze the spatial and chromatic structure of natural scenes by decomposing the spectral images into a set of linear basis functions such that they constitute a representation with minimal redundancy. Independent component analysis finds the basis functions that transforms the spatiochromatic data such that the outputs (activations) are statistically as independent as possible, i.e. least redundant. The resulting basis functions show strong opponency along an achromatic direction (luminance edges), along a blueyellow direction, and along a red-blue direction. Furthermore, the resulting activations have very sparse distributions, suggesting that the use of color opponency in the human visual system achieves a highly efficient representation of colors. Our findings suggest that color opponency is a result of the properties of natural spectra and not solely a consequence of the overlapping cone spectral sensitivities. 1 Statistical structure of natural scenes Efficient encoding of visual sensory information is an important task for information processing systems and its study may provide insights into coding principles of biological visual systems. An important goal of sensory information processing Electronic version available at www. cnl. salk . edu/</p><p>3 0.70235229 <a title="45-lsi-3" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>Author: Michael S. Gray, Terrence J. Sejnowski, Javier R. Movellan</p><p>Abstract: We examine eight different techniques for developing visual representations in machine vision tasks. In particular we compare different versions of principal component and independent component analysis in combination with stepwise regression methods for variable selection. We found that local methods, based on the statistics of image patches, consistently outperformed global methods based on the statistics of entire images. This result is consistent with previous work on emotion and facial expression recognition. In addition, the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance. 1</p><p>4 0.6385228 <a title="45-lsi-4" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>Author: Penio S. Penev</p><p>Abstract: Low-dimensional representations are key to solving problems in highlevel vision, such as face compression and recognition. Factorial coding strategies for reducing the redundancy present in natural images on the basis of their second-order statistics have been successful in accounting for both psychophysical and neurophysiological properties of early vision. Class-specific representations are presumably formed later, at the higher-level stages of cortical processing. Here we show that when retinotopic factorial codes are derived for ensembles of natural objects, such as human faces, not only redundancy, but also dimensionality is reduced. We also show that objects are built from parts in a non-Gaussian fashion which allows these local-feature codes to have dimensionalities that are substantially lower than the respective Nyquist sampling rates.</p><p>5 0.50325692 <a title="45-lsi-5" href="./nips-2000-The_Manhattan_World_Assumption%3A_Regularities_in_Scene_Statistics_which_Enable_Bayesian_Inference.html">135 nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</a></p>
<p>Author: James M. Coughlan, Alan L. Yuille</p><p>Abstract: Preliminary work by the authors made use of the so-called</p><p>6 0.46990201 <a title="45-lsi-6" href="./nips-2000-Machine_Learning_for_Video-Based_Rendering.html">83 nips-2000-Machine Learning for Video-Based Rendering</a></p>
<p>7 0.41353372 <a title="45-lsi-7" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>8 0.39378965 <a title="45-lsi-8" href="./nips-2000-Learning_Joint_Statistical_Models_for_Audio-Visual_Fusion_and_Segregation.html">78 nips-2000-Learning Joint Statistical Models for Audio-Visual Fusion and Segregation</a></p>
<p>9 0.38836867 <a title="45-lsi-9" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>10 0.38515201 <a title="45-lsi-10" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>11 0.36188915 <a title="45-lsi-11" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>12 0.35789514 <a title="45-lsi-12" href="./nips-2000-FaceSync%3A_A_Linear_Operator_for_Measuring_Synchronization_of_Video_Facial_Images_and_Audio_Tracks.html">50 nips-2000-FaceSync: A Linear Operator for Measuring Synchronization of Video Facial Images and Audio Tracks</a></p>
<p>13 0.33743224 <a title="45-lsi-13" href="./nips-2000-Who_Does_What%3F_A_Novel_Algorithm_to_Determine_Function_Localization.html">147 nips-2000-Who Does What? A Novel Algorithm to Determine Function Localization</a></p>
<p>14 0.32572836 <a title="45-lsi-14" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>15 0.30590445 <a title="45-lsi-15" href="./nips-2000-From_Mixtures_of_Mixtures_to_Adaptive_Transform_Coding.html">59 nips-2000-From Mixtures of Mixtures to Adaptive Transform Coding</a></p>
<p>16 0.3056618 <a title="45-lsi-16" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>17 0.29567596 <a title="45-lsi-17" href="./nips-2000-Error-correcting_Codes_on_a_Bethe-like_Lattice.html">47 nips-2000-Error-correcting Codes on a Bethe-like Lattice</a></p>
<p>18 0.29371545 <a title="45-lsi-18" href="./nips-2000-Bayesian_Video_Shot_Segmentation.html">30 nips-2000-Bayesian Video Shot Segmentation</a></p>
<p>19 0.28780127 <a title="45-lsi-19" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<p>20 0.2833229 <a title="45-lsi-20" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.33), (10, 0.027), (17, 0.212), (33, 0.027), (55, 0.022), (62, 0.043), (65, 0.05), (67, 0.056), (75, 0.019), (76, 0.034), (79, 0.013), (81, 0.05), (90, 0.017), (97, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93610752 <a title="45-lda-1" href="./nips-2000-New_Approaches_Towards_Robust_and_Adaptive_Speech_Recognition.html">90 nips-2000-New Approaches Towards Robust and Adaptive Speech Recognition</a></p>
<p>Author: Hervé Bourlard, Samy Bengio, Katrin Weber</p><p>Abstract: In this paper, we discuss some new research directions in automatic speech recognition (ASR), and which somewhat deviate from the usual approaches. More specifically, we will motivate and briefly describe new approaches based on multi-stream and multi/band ASR. These approaches extend the standard hidden Markov model (HMM) based approach by assuming that the different (frequency) channels representing the speech signal are processed by different (independent)</p><p>same-paper 2 0.87963134 <a title="45-lda-2" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>Author: Rafal Bogacz, Malcolm W. Brown, Christophe G. Giraud-Carrier</p><p>Abstract: Olshausen & Field demonstrated that a learning algorithm that attempts to generate a sparse code for natural scenes develops a complete family of localised, oriented, bandpass receptive fields, similar to those of 'simple cells' in VI. This paper describes an algorithm which finds a sparse code for sequences of images that preserves information about the input. This algorithm when trained on natural video sequences develops bases representing the movement in particular directions with particular speeds, similar to the receptive fields of the movement-sensitive cells observed in cortical visual areas. Furthermore, in contrast to previous approaches to learning direction selectivity, the timing of neuronal activity encodes the phase of the movement, so the precise timing of spikes is crucially important to the information encoding.</p><p>3 0.69018269 <a title="45-lda-3" href="./nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">95 nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>Author: Christopher K. I. Williams</p><p>Abstract: In this paper we show that the kernel peA algorithm of Sch6lkopf et al (1998) can be interpreted as a form of metric multidimensional scaling (MDS) when the kernel function k(x, y) is isotropic, i.e. it depends only on Ilx - yll. This leads to a metric MDS algorithm where the desired configuration of points is found via the solution of an eigenproblem rather than through the iterative optimization of the stress objective function. The question of kernel choice is also discussed. 1</p><p>4 0.56237733 <a title="45-lda-4" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>Author: Michael S. Gray, Terrence J. Sejnowski, Javier R. Movellan</p><p>Abstract: We examine eight different techniques for developing visual representations in machine vision tasks. In particular we compare different versions of principal component and independent component analysis in combination with stepwise regression methods for variable selection. We found that local methods, based on the statistics of image patches, consistently outperformed global methods based on the statistics of entire images. This result is consistent with previous work on emotion and facial expression recognition. In addition, the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance. 1</p><p>5 0.56061095 <a title="45-lda-5" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>Author: Te-Won Lee, Thomas Wachtler, Terrence J. Sejnowski</p><p>Abstract: The human visual system encodes the chromatic signals conveyed by the three types of retinal cone photoreceptors in an opponent fashion. This color opponency has been shown to constitute an efficient encoding by spectral decorrelation of the receptor signals. We analyze the spatial and chromatic structure of natural scenes by decomposing the spectral images into a set of linear basis functions such that they constitute a representation with minimal redundancy. Independent component analysis finds the basis functions that transforms the spatiochromatic data such that the outputs (activations) are statistically as independent as possible, i.e. least redundant. The resulting basis functions show strong opponency along an achromatic direction (luminance edges), along a blueyellow direction, and along a red-blue direction. Furthermore, the resulting activations have very sparse distributions, suggesting that the use of color opponency in the human visual system achieves a highly efficient representation of colors. Our findings suggest that color opponency is a result of the properties of natural spectra and not solely a consequence of the overlapping cone spectral sensitivities. 1 Statistical structure of natural scenes Efficient encoding of visual sensory information is an important task for information processing systems and its study may provide insights into coding principles of biological visual systems. An important goal of sensory information processing Electronic version available at www. cnl. salk . edu/</p><p>6 0.56007499 <a title="45-lda-6" href="./nips-2000-Foundations_for_a_Circuit_Complexity_Theory_of_Sensory_Processing.html">56 nips-2000-Foundations for a Circuit Complexity Theory of Sensory Processing</a></p>
<p>7 0.55544883 <a title="45-lda-7" href="./nips-2000-The_Manhattan_World_Assumption%3A_Regularities_in_Scene_Statistics_which_Enable_Bayesian_Inference.html">135 nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</a></p>
<p>8 0.54739219 <a title="45-lda-8" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>9 0.54582298 <a title="45-lda-9" href="./nips-2000-Factored_Semi-Tied_Covariance_Matrices.html">51 nips-2000-Factored Semi-Tied Covariance Matrices</a></p>
<p>10 0.54575264 <a title="45-lda-10" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>11 0.54483342 <a title="45-lda-11" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>12 0.54146332 <a title="45-lda-12" href="./nips-2000-Sparse_Kernel_Principal_Component_Analysis.html">121 nips-2000-Sparse Kernel Principal Component Analysis</a></p>
<p>13 0.54118937 <a title="45-lda-13" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>14 0.53700405 <a title="45-lda-14" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>15 0.53653222 <a title="45-lda-15" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>16 0.53123033 <a title="45-lda-16" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>17 0.52868223 <a title="45-lda-17" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>18 0.52821654 <a title="45-lda-18" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>19 0.52514309 <a title="45-lda-19" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>20 0.52263731 <a title="45-lda-20" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
