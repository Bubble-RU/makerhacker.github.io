<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-1" href="#">nips2000-1</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</h1>
<br/><p>Source: <a title="nips-2000-1-pdf" href="http://papers.nips.cc/paper/1840-apricodd-approximate-policy-construction-using-decision-diagrams.pdf">pdf</a></p><p>Author: Robert St-Aubin, Jesse Hoey, Craig Boutilier</p><p>Abstract: We propose a method of approximate dynamic programming for Markov decision processes (MDPs) using algebraic decision diagrams (ADDs). We produce near-optimal value functions and policies with much lower time and space requirements than exact dynamic programming. Our method reduces the sizes of the intermediate value functions generated during value iteration by replacing the values at the terminals of the ADD with ranges of values. Our method is demonstrated on a class of large MDPs (with up to 34 billion states), and we compare the results with the optimal value functions.</p><p>Reference: <a title="nips-2000-1-reference" href="../nips2000_reference/nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reord', 0.451), ('diagram', 0.438), ('prun', 0.294), ('shuffl', 0.19), ('policy', 0.179), ('vn', 0.169), ('mdps', 0.149), ('span', 0.149), ('it', 0.14), ('strengths', 0.139), ('hoey', 0.136), ('pjc', 0.136), ('craig', 0.127), ('ad', 0.119), ('sift', 0.109), ('valu', 0.105), ('bool', 0.104), ('decid', 0.104), ('dbn', 0.094), ('reward', 0.087)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="1-tfidf-1" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>Author: Robert St-Aubin, Jesse Hoey, Craig Boutilier</p><p>Abstract: We propose a method of approximate dynamic programming for Markov decision processes (MDPs) using algebraic decision diagrams (ADDs). We produce near-optimal value functions and policies with much lower time and space requirements than exact dynamic programming. Our method reduces the sizes of the intermediate value functions generated during value iteration by replacing the values at the terminals of the ADD with ranges of values. Our method is demonstrated on a class of large MDPs (with up to 34 billion states), and we compare the results with the optimal value functions.</p><p>2 0.13437875 <a title="1-tfidf-2" href="./nips-2000-%60N-Body%27_Problems_in_Statistical_Learning.html">148 nips-2000-`N-Body' Problems in Statistical Learning</a></p>
<p>Author: Alexander G. Gray, Andrew W. Moore</p><p>Abstract: We present efficient algorithms for all-point-pairs problems , or 'Nbody '-like problems , which are ubiquitous in statistical learning. We focus on six examples, including nearest-neighbor classification, kernel density estimation, outlier detection , and the two-point correlation. These include any problem which abstractly requires a comparison of each of the N points in a dataset with each other point and would naively be solved using N 2 distance computations. In practice N is often large enough to make this infeasible. We present a suite of new geometric t echniques which are applicable in principle to any 'N-body ' computation including large-scale mixtures of Gaussians, RBF neural networks, and HMM 's. Our algorithms exhibit favorable asymptotic scaling and are empirically several orders of magnitude faster than the naive computation, even for small datasets. We are aware of no exact algorithms for these problems which are more efficient either empirically or theoretically. In addition, our framework yields simple and elegant algorithms. It also permits two important generalizations beyond the standard all-point-pairs problems, which are more difficult. These are represented by our final examples, the multiple two-point correlation and the notorious n-point correlation. 1</p><p>3 0.12810118 <a title="1-tfidf-3" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>Author: Christian R. Shelton</p><p>Abstract: For many problems which would be natural for reinforcement learning, the reward signal is not a single scalar value but has multiple scalar components. Examples of such problems include agents with multiple goals and agents with multiple users. Creating a single reward value by combining the multiple components can throwaway vital information and can lead to incorrect solutions. We describe the multiple reward source problem and discuss the problems with applying traditional reinforcement learning. We then present an new algorithm for finding a solution and results on simulated environments.</p><p>4 0.12417544 <a title="1-tfidf-4" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>Author: Brian Sallans, Geoffrey E. Hinton</p><p>Abstract: The problem of reinforcement learning in large factored Markov decision processes is explored. The Q-value of a state-action pair is approximated by the free energy of a product of experts network. Network parameters are learned on-line using a modified SARSA algorithm which minimizes the inconsistency of the Q-values of consecutive state-action pairs. Actions are chosen based on the current value estimates by fixing the current state and sampling actions from the network using Gibbs sampling. The algorithm is tested on a co-operative multi-agent task. The product of experts model is found to perform comparably to table-based Q-Iearning for small instances of the task, and continues to perform well when the problem becomes too large for a table-based representation.</p><p>5 0.12349407 <a title="1-tfidf-5" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>Author: Geoffrey J. Gordon</p><p>Abstract: Many algorithms for approximate reinforcement learning are not known to converge. In fact, there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point. This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region. The algorithms are SARSA(O) and V(O); the latter algorithm was used in the well-known TD-Gammon program. 1</p><p>6 0.11711916 <a title="1-tfidf-6" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>7 0.10083389 <a title="1-tfidf-7" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>8 0.093434945 <a title="1-tfidf-8" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>9 0.087892905 <a title="1-tfidf-9" href="./nips-2000-Tree-Based_Modeling_and_Estimation_of_Gaussian_Processes_on_Graphs_with_Cycles.html">140 nips-2000-Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles</a></p>
<p>10 0.085739553 <a title="1-tfidf-10" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>11 0.073199637 <a title="1-tfidf-11" href="./nips-2000-Programmable_Reinforcement_Learning_Agents.html">105 nips-2000-Programmable Reinforcement Learning Agents</a></p>
<p>12 0.068950444 <a title="1-tfidf-12" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>13 0.063981988 <a title="1-tfidf-13" href="./nips-2000-Hierarchical_Memory-Based_Reinforcement_Learning.html">63 nips-2000-Hierarchical Memory-Based Reinforcement Learning</a></p>
<p>14 0.063899279 <a title="1-tfidf-14" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<p>15 0.057592753 <a title="1-tfidf-15" href="./nips-2000-Second_Order_Approximations_for_Probability_Models.html">114 nips-2000-Second Order Approximations for Probability Models</a></p>
<p>16 0.056651816 <a title="1-tfidf-16" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>17 0.055569779 <a title="1-tfidf-17" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>18 0.053944256 <a title="1-tfidf-18" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>19 0.051255293 <a title="1-tfidf-19" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>20 0.050138496 <a title="1-tfidf-20" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.18), (1, 0.01), (2, -0.096), (3, 0.228), (4, -0.102), (5, 0.012), (6, -0.012), (7, 0.07), (8, -0.077), (9, 0.04), (10, -0.055), (11, 0.019), (12, -0.012), (13, -0.074), (14, 0.039), (15, -0.044), (16, 0.041), (17, -0.063), (18, 0.037), (19, -0.108), (20, 0.009), (21, -0.051), (22, 0.066), (23, -0.037), (24, 0.053), (25, -0.034), (26, 0.002), (27, -0.033), (28, -0.058), (29, 0.072), (30, -0.09), (31, 0.054), (32, 0.043), (33, -0.066), (34, -0.09), (35, 0.167), (36, 0.122), (37, -0.06), (38, -0.091), (39, -0.072), (40, 0.204), (41, 0.07), (42, 0.023), (43, 0.046), (44, -0.097), (45, -0.206), (46, 0.05), (47, 0.002), (48, 0.068), (49, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92557615 <a title="1-lsi-1" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>Author: Robert St-Aubin, Jesse Hoey, Craig Boutilier</p><p>Abstract: We propose a method of approximate dynamic programming for Markov decision processes (MDPs) using algebraic decision diagrams (ADDs). We produce near-optimal value functions and policies with much lower time and space requirements than exact dynamic programming. Our method reduces the sizes of the intermediate value functions generated during value iteration by replacing the values at the terminals of the ADD with ranges of values. Our method is demonstrated on a class of large MDPs (with up to 34 billion states), and we compare the results with the optimal value functions.</p><p>2 0.61127865 <a title="1-lsi-2" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>Author: Justin A. Boyan, Michael L. Littman</p><p>Abstract: We describe an extension of the Markov decision process model in which a continuous time dimension is included in the state space. This allows for the representation and exact solution of a wide range of problems in which transitions or rewards vary over time. We examine problems based on route planning with public transportation and telescope observation scheduling. 1</p><p>3 0.59131867 <a title="1-lsi-3" href="./nips-2000-%60N-Body%27_Problems_in_Statistical_Learning.html">148 nips-2000-`N-Body' Problems in Statistical Learning</a></p>
<p>Author: Alexander G. Gray, Andrew W. Moore</p><p>Abstract: We present efficient algorithms for all-point-pairs problems , or 'Nbody '-like problems , which are ubiquitous in statistical learning. We focus on six examples, including nearest-neighbor classification, kernel density estimation, outlier detection , and the two-point correlation. These include any problem which abstractly requires a comparison of each of the N points in a dataset with each other point and would naively be solved using N 2 distance computations. In practice N is often large enough to make this infeasible. We present a suite of new geometric t echniques which are applicable in principle to any 'N-body ' computation including large-scale mixtures of Gaussians, RBF neural networks, and HMM 's. Our algorithms exhibit favorable asymptotic scaling and are empirically several orders of magnitude faster than the naive computation, even for small datasets. We are aware of no exact algorithms for these problems which are more efficient either empirically or theoretically. In addition, our framework yields simple and elegant algorithms. It also permits two important generalizations beyond the standard all-point-pairs problems, which are more difficult. These are represented by our final examples, the multiple two-point correlation and the notorious n-point correlation. 1</p><p>4 0.57236779 <a title="1-lsi-4" href="./nips-2000-On_Iterative_Krylov-Dogleg_Trust-Region_Steps_for_Solving_Neural_Networks_Nonlinear_Least_Squares_Problems.html">93 nips-2000-On Iterative Krylov-Dogleg Trust-Region Steps for Solving Neural Networks Nonlinear Least Squares Problems</a></p>
<p>Author: Eiji Mizutani, James Demmel</p><p>Abstract: This paper describes a method of dogleg trust-region steps, or restricted Levenberg-Marquardt steps, based on a projection process onto the Krylov subspaces for neural networks nonlinear least squares problems. In particular, the linear conjugate gradient (CG) method works as the inner iterative algorithm for solving the linearized Gauss-Newton normal equation, whereas the outer nonlinear algorithm repeatedly takes so-called</p><p>5 0.50235349 <a title="1-lsi-5" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>Author: Geoffrey J. Gordon</p><p>Abstract: Many algorithms for approximate reinforcement learning are not known to converge. In fact, there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point. This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region. The algorithms are SARSA(O) and V(O); the latter algorithm was used in the well-known TD-Gammon program. 1</p><p>6 0.48032033 <a title="1-lsi-6" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>7 0.42286882 <a title="1-lsi-7" href="./nips-2000-Programmable_Reinforcement_Learning_Agents.html">105 nips-2000-Programmable Reinforcement Learning Agents</a></p>
<p>8 0.42212242 <a title="1-lsi-8" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>9 0.42182299 <a title="1-lsi-9" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>10 0.41242576 <a title="1-lsi-10" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>11 0.40412575 <a title="1-lsi-11" href="./nips-2000-Fast_Training_of_Support_Vector_Classifiers.html">52 nips-2000-Fast Training of Support Vector Classifiers</a></p>
<p>12 0.39703968 <a title="1-lsi-12" href="./nips-2000-Tree-Based_Modeling_and_Estimation_of_Gaussian_Processes_on_Graphs_with_Cycles.html">140 nips-2000-Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles</a></p>
<p>13 0.36315066 <a title="1-lsi-13" href="./nips-2000-Sequentially_Fitting_%60%60Inclusive%27%27_Trees_for_Inference_in_Noisy-OR_Networks.html">115 nips-2000-Sequentially Fitting ``Inclusive'' Trees for Inference in Noisy-OR Networks</a></p>
<p>14 0.35882294 <a title="1-lsi-14" href="./nips-2000-Who_Does_What%3F_A_Novel_Algorithm_to_Determine_Function_Localization.html">147 nips-2000-Who Does What? A Novel Algorithm to Determine Function Localization</a></p>
<p>15 0.33451068 <a title="1-lsi-15" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>16 0.32610145 <a title="1-lsi-16" href="./nips-2000-Hierarchical_Memory-Based_Reinforcement_Learning.html">63 nips-2000-Hierarchical Memory-Based Reinforcement Learning</a></p>
<p>17 0.32567525 <a title="1-lsi-17" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>18 0.31448385 <a title="1-lsi-18" href="./nips-2000-Active_Support_Vector_Machine_Classification.html">18 nips-2000-Active Support Vector Machine Classification</a></p>
<p>19 0.29137135 <a title="1-lsi-19" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>20 0.2912097 <a title="1-lsi-20" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(9, 0.105), (11, 0.031), (16, 0.064), (21, 0.028), (26, 0.026), (38, 0.035), (51, 0.383), (54, 0.057), (74, 0.014), (76, 0.04), (84, 0.121)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71718895 <a title="1-lda-1" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>Author: Robert St-Aubin, Jesse Hoey, Craig Boutilier</p><p>Abstract: We propose a method of approximate dynamic programming for Markov decision processes (MDPs) using algebraic decision diagrams (ADDs). We produce near-optimal value functions and policies with much lower time and space requirements than exact dynamic programming. Our method reduces the sizes of the intermediate value functions generated during value iteration by replacing the values at the terminals of the ADD with ranges of values. Our method is demonstrated on a class of large MDPs (with up to 34 billion states), and we compare the results with the optimal value functions.</p><p>2 0.58203644 <a title="1-lda-2" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>Author: Lehel Csatč´¸, Manfred Opper</p><p>Abstract: We develop an approach for a sparse representation for Gaussian Process (GP) models in order to overcome the limitations of GPs caused by large data sets. The method is based on a combination of a Bayesian online algorithm together with a sequential construction of a relevant subsample of the data which fully specifies the prediction of the model. Experimental results on toy examples and large real-world data sets indicate the efficiency of the approach.</p><p>3 0.43568343 <a title="1-lda-3" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>Author: Charles Dugas, Yoshua Bengio, François Bélisle, Claude Nadeau, René Garcia</p><p>Abstract: Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in two of its arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of continuous functions with these and other properties. We apply this new class of functions to the task of modeling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori constraints.</p><p>4 0.43543404 <a title="1-lda-4" href="./nips-2000-Learning_Switching_Linear_Models_of_Human_Motion.html">80 nips-2000-Learning Switching Linear Models of Human Motion</a></p>
<p>Author: Vladimir Pavlovic, James M. Rehg, John MacCormick</p><p>Abstract: The human figure exhibits complex and rich dynamic behavior that is both nonlinear and time-varying. Effective models of human dynamics can be learned from motion capture data using switching linear dynamic system (SLDS) models. We present results for human motion synthesis, classification, and visual tracking using learned SLDS models. Since exact inference in SLDS is intractable, we present three approximate inference algorithms and compare their performance. In particular, a new variational inference algorithm is obtained by casting the SLDS model as a Dynamic Bayesian Network. Classification experiments show the superiority of SLDS over conventional HMM's for our problem domain.</p><p>5 0.43437624 <a title="1-lda-5" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>Author: Anders Jonsson, Andrew G. Barto</p><p>Abstract: Learning a complex task can be significantly facilitated by defining a hierarchy of subtasks. An agent can learn to choose between various temporally abstract actions, each solving an assigned subtask, to accomplish the overall task. In this paper, we study hierarchical learning using the framework of options. We argue that to take full advantage of hierarchical structure, one should perform option-specific state abstraction, and that if this is to scale to larger tasks, state abstraction should be automated. We adapt McCallum's U-Tree algorithm to automatically build option-specific representations of the state feature space, and we illustrate the resulting algorithm using a simple hierarchical task. Results suggest that automated option-specific state abstraction is an attractive approach to making hierarchical learning systems more effective.</p><p>6 0.43352887 <a title="1-lda-6" href="./nips-2000-Tree-Based_Modeling_and_Estimation_of_Gaussian_Processes_on_Graphs_with_Cycles.html">140 nips-2000-Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles</a></p>
<p>7 0.43195468 <a title="1-lda-7" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>8 0.43179673 <a title="1-lda-8" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>9 0.43118912 <a title="1-lda-9" href="./nips-2000-Data_Clustering_by_Markovian_Relaxation_and_the_Information_Bottleneck_Method.html">38 nips-2000-Data Clustering by Markovian Relaxation and the Information Bottleneck Method</a></p>
<p>10 0.43101183 <a title="1-lda-10" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>11 0.43096137 <a title="1-lda-11" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>12 0.42985633 <a title="1-lda-12" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>13 0.42846698 <a title="1-lda-13" href="./nips-2000-Mixtures_of_Gaussian_Processes.html">85 nips-2000-Mixtures of Gaussian Processes</a></p>
<p>14 0.42827249 <a title="1-lda-14" href="./nips-2000-Support_Vector_Novelty_Detection_Applied_to_Jet_Engine_Vibration_Spectra.html">128 nips-2000-Support Vector Novelty Detection Applied to Jet Engine Vibration Spectra</a></p>
<p>15 0.42459658 <a title="1-lda-15" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>16 0.42376211 <a title="1-lda-16" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>17 0.42365742 <a title="1-lda-17" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>18 0.42349967 <a title="1-lda-18" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>19 0.42277765 <a title="1-lda-19" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>20 0.42254096 <a title="1-lda-20" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
