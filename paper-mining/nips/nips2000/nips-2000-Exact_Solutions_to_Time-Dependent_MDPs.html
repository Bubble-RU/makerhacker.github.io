<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>48 nips-2000-Exact Solutions to Time-Dependent MDPs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-48" href="#">nips2000-48</a> knowledge-graph by maker-knowledge-mining</p><h1>48 nips-2000-Exact Solutions to Time-Dependent MDPs</h1>
<br/><p>Source: <a title="nips-2000-48-pdf" href="http://papers.nips.cc/paper/1811-exact-solutions-to-time-dependent-mdps.pdf">pdf</a></p><p>Author: Justin A. Boyan, Michael L. Littman</p><p>Abstract: We describe an extension of the Markov decision process model in which a continuous time dimension is included in the state space. This allows for the representation and exact solution of a wide range of problems in which transitions or rewards vary over time. We examine problems based on route planning with public transportation and telescope observation scheduling. 1</p><p>Reference: <a title="nips-2000-48-reference" href="../nips2000_reference/nips-2000-Exact_Solutions_to_Time-Dependent_MDPs_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tmdp', 0.456), ('piecew', 0.316), ('dur', 0.208), ('driv', 0.18), ('outcom', 0.175), ('highway', 0.171), ('rush', 0.171), ('telescop', 0.171), ('traff', 0.165), ('stochast', 0.16), ('reward', 0.157), ('jl', 0.153), ('dawdl', 0.142), ('mdp', 0.139), ('transport', 0.114), ('tim', 0.109), ('vi', 0.105), ('grid', 0.101), ('tit', 0.097), ('ab', 0.097)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="48-tfidf-1" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>2 0.1784554 <a title="48-tfidf-2" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>3 0.13544987 <a title="48-tfidf-3" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>4 0.12496462 <a title="48-tfidf-4" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>5 0.10083389 <a title="48-tfidf-5" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>6 0.098764889 <a title="48-tfidf-6" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>7 0.095783718 <a title="48-tfidf-7" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>8 0.095390081 <a title="48-tfidf-8" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>9 0.08849971 <a title="48-tfidf-9" href="./nips-2000-Hierarchical_Memory-Based_Reinforcement_Learning.html">63 nips-2000-Hierarchical Memory-Based Reinforcement Learning</a></p>
<p>10 0.082330704 <a title="48-tfidf-10" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>11 0.075750224 <a title="48-tfidf-11" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>12 0.073461488 <a title="48-tfidf-12" href="./nips-2000-Programmable_Reinforcement_Learning_Agents.html">105 nips-2000-Programmable Reinforcement Learning Agents</a></p>
<p>13 0.065974191 <a title="48-tfidf-13" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>14 0.062648416 <a title="48-tfidf-14" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>15 0.061507441 <a title="48-tfidf-15" href="./nips-2000-Learning_Continuous_Distributions%3A_Simulations_With_Field_Theoretic_Priors.html">76 nips-2000-Learning Continuous Distributions: Simulations With Field Theoretic Priors</a></p>
<p>16 0.057671838 <a title="48-tfidf-16" href="./nips-2000-Bayesian_Video_Shot_Segmentation.html">30 nips-2000-Bayesian Video Shot Segmentation</a></p>
<p>17 0.057046302 <a title="48-tfidf-17" href="./nips-2000-From_Mixtures_of_Mixtures_to_Adaptive_Transform_Coding.html">59 nips-2000-From Mixtures of Mixtures to Adaptive Transform Coding</a></p>
<p>18 0.053147819 <a title="48-tfidf-18" href="./nips-2000-Active_Support_Vector_Machine_Classification.html">18 nips-2000-Active Support Vector Machine Classification</a></p>
<p>19 0.052331582 <a title="48-tfidf-19" href="./nips-2000-Data_Clustering_by_Markovian_Relaxation_and_the_Information_Bottleneck_Method.html">38 nips-2000-Data Clustering by Markovian Relaxation and the Information Bottleneck Method</a></p>
<p>20 0.051369902 <a title="48-tfidf-20" href="./nips-2000-Tree-Based_Modeling_and_Estimation_of_Gaussian_Processes_on_Graphs_with_Cycles.html">140 nips-2000-Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.176), (1, -0.019), (2, -0.109), (3, 0.219), (4, -0.157), (5, 0.025), (6, -0.002), (7, 0.074), (8, -0.012), (9, -0.023), (10, -0.047), (11, 0.008), (12, -0.042), (13, -0.083), (14, -0.044), (15, 0.01), (16, 0.007), (17, -0.025), (18, 0.022), (19, 0.028), (20, -0.003), (21, 0.055), (22, 0.011), (23, -0.051), (24, 0.099), (25, 0.032), (26, -0.096), (27, 0.08), (28, 0.182), (29, 0.071), (30, -0.083), (31, 0.024), (32, 0.024), (33, -0.048), (34, -0.136), (35, 0.015), (36, -0.04), (37, -0.083), (38, 0.111), (39, -0.007), (40, 0.138), (41, 0.266), (42, 0.16), (43, -0.136), (44, 0.043), (45, -0.195), (46, 0.097), (47, 0.1), (48, 0.007), (49, -0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92963088 <a title="48-lsi-1" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>2 0.68991727 <a title="48-lsi-2" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>3 0.56767815 <a title="48-lsi-3" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>4 0.44896427 <a title="48-lsi-4" href="./nips-2000-Bayesian_Video_Shot_Segmentation.html">30 nips-2000-Bayesian Video Shot Segmentation</a></p>
<p>5 0.41740188 <a title="48-lsi-5" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>6 0.4016408 <a title="48-lsi-6" href="./nips-2000-Programmable_Reinforcement_Learning_Agents.html">105 nips-2000-Programmable Reinforcement Learning Agents</a></p>
<p>7 0.37853697 <a title="48-lsi-7" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>8 0.36575633 <a title="48-lsi-8" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>9 0.34772965 <a title="48-lsi-9" href="./nips-2000-On_Iterative_Krylov-Dogleg_Trust-Region_Steps_for_Solving_Neural_Networks_Nonlinear_Least_Squares_Problems.html">93 nips-2000-On Iterative Krylov-Dogleg Trust-Region Steps for Solving Neural Networks Nonlinear Least Squares Problems</a></p>
<p>10 0.34085783 <a title="48-lsi-10" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>11 0.3390204 <a title="48-lsi-11" href="./nips-2000-Fast_Training_of_Support_Vector_Classifiers.html">52 nips-2000-Fast Training of Support Vector Classifiers</a></p>
<p>12 0.33891189 <a title="48-lsi-12" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>13 0.33726051 <a title="48-lsi-13" href="./nips-2000-Hierarchical_Memory-Based_Reinforcement_Learning.html">63 nips-2000-Hierarchical Memory-Based Reinforcement Learning</a></p>
<p>14 0.3061409 <a title="48-lsi-14" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>15 0.29058218 <a title="48-lsi-15" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<p>16 0.28996637 <a title="48-lsi-16" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>17 0.2878288 <a title="48-lsi-17" href="./nips-2000-Machine_Learning_for_Video-Based_Rendering.html">83 nips-2000-Machine Learning for Video-Based Rendering</a></p>
<p>18 0.2866461 <a title="48-lsi-18" href="./nips-2000-Competition_and_Arbors_in_Ocular_Dominance.html">34 nips-2000-Competition and Arbors in Ocular Dominance</a></p>
<p>19 0.27110174 <a title="48-lsi-19" href="./nips-2000-Robust_Reinforcement_Learning.html">113 nips-2000-Robust Reinforcement Learning</a></p>
<p>20 0.26300094 <a title="48-lsi-20" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(9, 0.082), (11, 0.028), (15, 0.313), (16, 0.065), (21, 0.041), (38, 0.063), (51, 0.011), (54, 0.113), (74, 0.051), (76, 0.023), (84, 0.09)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73910397 <a title="48-lda-1" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>2 0.55441332 <a title="48-lda-2" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>3 0.50745499 <a title="48-lda-3" href="./nips-2000-Bayes_Networks_on_Ice%3A_Robotic_Search_for_Antarctic_Meteorites.html">29 nips-2000-Bayes Networks on Ice: Robotic Search for Antarctic Meteorites</a></p>
<p>4 0.5038169 <a title="48-lda-4" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>5 0.49919617 <a title="48-lda-5" href="./nips-2000-Hierarchical_Memory-Based_Reinforcement_Learning.html">63 nips-2000-Hierarchical Memory-Based Reinforcement Learning</a></p>
<p>6 0.48207846 <a title="48-lda-6" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>7 0.47852439 <a title="48-lda-7" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>8 0.47275746 <a title="48-lda-8" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>9 0.47236428 <a title="48-lda-9" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>10 0.47234946 <a title="48-lda-10" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>11 0.47146612 <a title="48-lda-11" href="./nips-2000-Universality_and_Individuality_in_a_Neural_Code.html">141 nips-2000-Universality and Individuality in a Neural Code</a></p>
<p>12 0.46993709 <a title="48-lda-12" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>13 0.46501914 <a title="48-lda-13" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>14 0.46407098 <a title="48-lda-14" href="./nips-2000-The_Use_of_MDL_to_Select_among_Computational_Models_of_Cognition.html">139 nips-2000-The Use of MDL to Select among Computational Models of Cognition</a></p>
<p>15 0.4604381 <a title="48-lda-15" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>16 0.45942768 <a title="48-lda-16" href="./nips-2000-Bayesian_Video_Shot_Segmentation.html">30 nips-2000-Bayesian Video Shot Segmentation</a></p>
<p>17 0.45882189 <a title="48-lda-17" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>18 0.45869267 <a title="48-lda-18" href="./nips-2000-Data_Clustering_by_Markovian_Relaxation_and_the_Information_Bottleneck_Method.html">38 nips-2000-Data Clustering by Markovian Relaxation and the Information Bottleneck Method</a></p>
<p>19 0.45712018 <a title="48-lda-19" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>20 0.45629925 <a title="48-lda-20" href="./nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">129 nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
