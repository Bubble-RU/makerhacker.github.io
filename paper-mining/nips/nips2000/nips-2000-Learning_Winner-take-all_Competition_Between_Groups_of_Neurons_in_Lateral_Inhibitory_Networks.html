<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>81 nips-2000-Learning Winner-take-all Competition Between Groups of Neurons in Lateral Inhibitory Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-81" href="#">nips2000-81</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>81 nips-2000-Learning Winner-take-all Competition Between Groups of Neurons in Lateral Inhibitory Networks</h1>
<br/><p>Source: <a title="nips-2000-81-pdf" href="http://papers.nips.cc/paper/1829-learning-winner-take-all-competition-between-groups-of-neurons-in-lateral-inhibitory-networks.pdf">pdf</a></p><p>Author: Xiaohui Xie, Richard H. R. Hahnloser, H. Sebastian Seung</p><p>Abstract: It has long been known that lateral inhibition in neural networks can lead to a winner-take-all competition, so that only a single neuron is active at a steady state. Here we show how to organize lateral inhibition so that groups of neurons compete to be active. Given a collection of potentially overlapping groups, the inhibitory connectivity is set by a formula that can be interpreted as arising from a simple learning rule. Our analysis demonstrates that such inhibition generally results in winner-take-all competition between the given groups, with the exception of some degenerate cases. In a broader context, the network serves as a particular illustration of the general distinction between permitted and forbidden sets, which was introduced recently. From this viewpoint, the computational function of our network is to store and retrieve memories as permitted sets of coactive neurons. In traditional winner-take-all networks, lateral inhibition is used to enforce a localized, or</p><p>Reference: <a title="nips-2000-81-reference" href="../nips2000_reference/nips-2000-Learning_Winner-take-all_Competition_Between_Groups_of_Neurons_in_Lateral_Inhibitory_Networks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Learning winner-take-all competition between groups of neurons in lateral inhibitory networks  Xiaohui Xie, Richard Hahnloser and H. [sent-1, score-1.05]
</p><p>2 edu  Abstract It has long been known that lateral inhibition in neural networks can lead to a winner-take-all competition, so that only a single neuron is active at a steady state. [sent-3, score-0.638]
</p><p>3 Here we show how to organize lateral inhibition so that groups of neurons compete to be active. [sent-4, score-1.025]
</p><p>4 Our analysis demonstrates that such inhibition generally results in winner-take-all competition between the given groups, with the exception of some degenerate cases. [sent-6, score-0.413]
</p><p>5 In a broader context, the network serves as a particular illustration of the general distinction between permitted and forbidden sets, which was introduced recently. [sent-7, score-0.868]
</p><p>6 From this viewpoint, the computational function of our network is to store and retrieve memories as permitted sets of coactive neurons. [sent-8, score-0.795]
</p><p>7 In traditional winner-take-all networks, lateral inhibition is used to enforce a localized, or "grandmother cell" representation in which only a single neuron is active [1, 2, 3, 4]. [sent-9, score-0.481]
</p><p>8 These algorithms lead to networks in which groups of multiple neurons are coactivated to represent an object. [sent-12, score-0.762]
</p><p>9 Therefore, it is of great interest to find ways of using lateral inhibition to mediate winner-take-all competition between groups of neurons, as this could be useful for learning sparsely distributed representations. [sent-13, score-0.94]
</p><p>10 In this paper, we show how winner-take-all competition between groups of neurons can be learned. [sent-14, score-0.841]
</p><p>11 If the strength of inhibition is sufficiently great, and the group organization satisfies certain conditions, we show that the only sets of neurons that can be coactivated at a stable steady state are the given groups and their subsets. [sent-17, score-1.57]
</p><p>12 Because of the competition between groups, only one group can be activated at a time. [sent-18, score-0.448]
</p><p>13 In general, the identity of the winning group depends on the initial conditions of the network dynamics. [sent-19, score-0.45]
</p><p>14 If the groups are ordered by the aggregate input that each receives, the possible winners are those above a cutoff that is set by inequalities to be specified. [sent-20, score-0.438]
</p><p>15 1 Basic definitions Let m groups of neurons be given, where group membership is specified by the matrix  , {I°  fl  =  if the ith neuron is in the ath group otherwise  (1)  We will assume that every neuron belongs to at least one group l, and every group contains at least one neuron. [sent-21, score-2.309]
</p><p>16 A neuron is allowed to belong to more than one group, so that the groups are potentially overlapping. [sent-22, score-0.574]
</p><p>17 The inhibitory synaptic connectivity of the network is defined in terms of the group membership, Ji ' J  = lIm (1 _ ~a ~'! [sent-23, score-0.47]
</p><p>18 j both belong to a group 1 otherwIse  (2)  One can imagine this pattern of connectivity arising by a simple learning mechanism. [sent-25, score-0.439]
</p><p>19 Suppose that all elements of J are initialized to be unity, and the groups are presented sequentially as binary vectors ,~m. [sent-26, score-0.358]
</p><p>20 The ath group is learned through the update  e, . [sent-27, score-0.321]
</p><p>21 (3)  In other words, if neurons i and j both belong to group a, then the connection between them is removed. [sent-30, score-0.681]
</p><p>22 It will be seen that, as inhibitory connections are removed during learning, the competition evolves to mediate competition between groups of neurons rather than individual neurons. [sent-34, score-1.116]
</p><p>23 •  3 Relationship between groups and permitted sets In this section we characterize the conditions under which the lateral inhibition of Eq. [sent-63, score-1.396]
</p><p>24 (4) enforces winner-take-all competition between the groups of neurons. [sent-64, score-0.528]
</p><p>25 That is, the only sets of neurons that can be coactivated at a stable steady state are the groups and their subsets. [sent-65, score-1.051]
</p><p>26 Definition 1 If a set of neurons can be coactivated by some input at an asymptotically stable steady state, it is called permitted. [sent-67, score-0.645]
</p><p>27 Otherwise, it is forbidden Elsewhere we have shown that whether a set is permitted or forbidden depends on the submatrix of synaptic connections between neurons in that set[l]. [sent-68, score-1.349]
</p><p>28 We have also proved that any superset of a forbidden set is forbidden, while any subset of a permitted set is also permitted. [sent-71, score-0.806]
</p><p>29 Our goal in constructing the network (4) is to make the groups and their subsets the only permitted sets of the network. [sent-72, score-1.162]
</p><p>30 Lemma 1 All groups and their subsets are permitted. [sent-77, score-0.426]
</p><p>31 Proof: If a set is contained in a group, then there is no lateral inhibition between the neurons in the set. [sent-78, score-0.703]
</p><p>32 • The answer to the second question, whether all permitted sets are contained in groups, is not necessarily affirmative. [sent-80, score-0.745]
</p><p>33 For example, consider the network defined by the group membership matrix ~ = {(I, 1,0), (0, 1, 1), (1,0,1)}. [sent-81, score-0.46]
</p><p>34 Since every pair of neurons belongs to some group, there is no lateral inhibition (J = 0), which means that there are no forbidden sets. [sent-82, score-0.9]
</p><p>35 As a result, (1,1,1) is a permitted set, but obviously it is not contained in any group. [sent-83, score-0.648]
</p><p>36 Let's define a spurious permitted set to be one that is not contained in any group. [sent-84, score-0.816]
</p><p>37 For example, {I, 1, I} is a spurious permitted set in the above example. [sent-85, score-0.751]
</p><p>38 To eliminate all the spurious permitted sets in the network, certain conditions on the group membership matrix ~ have to be satisfied. [sent-86, score-1.243]
</p><p>39 Definition 2 The membership ~ is degenerate if there exists a set of n ~ 3 neurons that is not contained in any group, but all of its subsets with n - 1 neurons belong to some group. [sent-87, score-1.02]
</p><p>40 (4) with a permitted set if and only if ~ is degenerate. [sent-92, score-0.583]
</p><p>41 Lemma 2 If (3 > 1- a, any set containing two neurons not in the same group isforbidden under the neural dynamics Eq. [sent-94, score-0.667]
</p><p>42 Proof sketch: We will start by analyzing a very simple case, where there are two neurons belonging to two different groups. [sent-96, score-0.313]
</p><p>43 This means that it is impossible for the two neurons to be coactivated at a stable steady state. [sent-102, score-0.587]
</p><p>44 Proof of Theorem 2 (sketch): •  ¢::: If ~ is degenerate, there must exist a set n ~ 3 neurons that is not contained in any group, but all of its subsets with n - 1 neurons belong to some group. [sent-105, score-0.891]
</p><p>45 There is no lateral inhibition between these n neurons, since every pair of neurons belongs to some group. [sent-106, score-0.706]
</p><p>46 Thus the set containing all n neurons is permitted and spurious . [sent-107, score-1.093]
</p><p>47 • =>: If there exists a spurious permitted set P, we need to prove that ~ must be degenerate. [sent-108, score-0.813]
</p><p>48 P must contain at least 2 neurons since anyone neuron subset is permitted and not spurious. [sent-111, score-1.052]
</p><p>49 By Lemma 2, these 2 neurons must be contained in some group, or else it is forbidden. [sent-112, score-0.402]
</p><p>50 Thus P must contain at least 3 neurons to be spurious, and any pair of neurons in P belongs to some group by Lemma 2. [sent-113, score-1.012]
</p><p>51 If P contains at least n neurons and all of its subsets with n - 1 neurons belong to some group, then the set with these n neurons must belong to some group, otherwise ~ is degenerate. [sent-114, score-1.291]
</p><p>52 Thus n must contain at least n + 1 neurons to be spurious, and all its n subsets belong to some group. [sent-115, score-0.54]
</p><p>53 By induction, this implies that P must contain all neurons in the network, in which case, P is either forbidden or nonspurious. [sent-116, score-0.554]
</p><p>54 This contradicts with the assumption P is a spurious permitted set. [sent-117, score-0.751]
</p><p>55 Corollary 1 If every group contains some neuron that does not belong to any other group, then there is no any spurious permitted set. [sent-119, score-1.258]
</p><p>56 4  The potential winners  We have seen that if ~ is nondegenerate, the active set must be contained in a group, provided that lateral inhibition is strong «(3 > 1 - a). [sent-120, score-0.516]
</p><p>57 The group that contains the active set will be called the "winner" of the competition between groups. [sent-121, score-0.523]
</p><p>58 Theorem 3 For nonoverlapping groups, the top c groups with the largest group input could end up the winner depending on the initial conditions of the dynamics, where c is determined by the equation BC 2': (1 - a)(3-1b max > B C+! [sent-130, score-0.882]
</p><p>59 Proof sketch: Suppose the ath group is the winner. [sent-131, score-0.321]
</p><p>60 For all neurons not in this group to be inactive, the self-consistent condition should read  "'[bi J+a 2': ~ ~i i  I-a -(3- max{ [J+ } bj J~a  (5)  If a group containing the neuron with the largest input, this condition can always be satisfied. [sent-132, score-1.104]
</p><p>61 For groups not containing the neuron with the largest input, this condition can be satisfied if and only if they are in the top c groups . [sent-134, score-0.908]
</p><p>62 •  The winner-take-all competition described above holds only for the case of strong inhibition (3 > 1 - a. [sent-135, score-0.353]
</p><p>63 In particular, if (3 < (1 - a) / Amax, where Am ax is the largest eigenvalue of -J, then the set of all neurons is permitted. [sent-137, score-0.363]
</p><p>64 Since every subset of a permitted set is permitted, that means there are no forbidden sets and the network is monostable. [sent-138, score-0.959]
</p><p>65 If (1 - a) / Amax < (3 < 1 - a, the network has forbidden sets, but the possibility of spurious permitted sets cannot be excluded. [sent-140, score-1.098]
</p><p>66 Therefore, the group membership matrix ~ is the identity matrix, and J = 11 T - I, where 1 denotes the vector of all ones. [sent-142, score-0.407]
</p><p>67 According to Corollary 1, only one neuron is permitted to be active at a stable steady state, provided that (3 > 1 - a. [sent-143, score-0.941]
</p><p>68 We refer to the active neuron as the "winner" of the competition mediated by the lateral inhibition. [sent-144, score-0.451]
</p><p>69 According to Theorem 3, any of the neurons 1 to k can be the winner, where k is defined by bk 2': (1 - a)(3-1b1 > bk+! [sent-149, score-0.337]
</p><p>70 Topographic organization Let the N neurons be organized into a ring, and let every set of d contiguous neurons be a group. [sent-153, score-0.676]
</p><p>71 For example, in a network with N = 4 neurons and group width d = 2, then the membership matrix is ~ = {(I, 1,0,0), (0,1,1,0), (0,0,1,1), (1,0,0, I)}. [sent-155, score-0.821]
</p><p>72 Unlike the WTA network where all groups are non-overlapping which implies that ~ is always nondegenerate, in the ring network neurons are shared among different groups, ~ will become degenerate when the width of the group is large. [sent-157, score-1.335]
</p><p>73 To guarantee all permitted sets are the subsets of some group, we have the following corollary, which can be derived from Theorem 2. [sent-158, score-0.732]
</p><p>74 The ring network is comprised of 15 neurons with Q = 0. [sent-160, score-0.502]
</p><p>75 In panels A and D, the 15 groups are represented by columns. [sent-162, score-0.41]
</p><p>76 Black refers to active neurons and white refers to inactive neurons. [sent-163, score-0.42]
</p><p>77 (B) All permitted sets corresponding to the groups in A. [sent-165, score-1.022]
</p><p>78 (C) The 15 permitted sets in B that have no permitted supersets. [sent-166, score-1.247]
</p><p>79 (E) All permitted set corresponding to groups in D. [sent-169, score-0.941]
</p><p>80 (F) There are 20 permitted sets in E that have no permitted supersets. [sent-170, score-1.247]
</p><p>81 Corollary 2 In the ring network with N neurons, no spurious permitted set. [sent-172, score-0.94]
</p><p>82 (1) shows the permitted sets of a ring network with 15 neurons. [sent-174, score-0.853]
</p><p>83 From Corollary 2, we know that if the group width is no larger than 5 neurons, there will not exist any spurious permitted set. [sent-175, score-1.095]
</p><p>84 (1), the group width is 5 and all permitted sets are subsets of these groups. [sent-177, score-1.058]
</p><p>85 However, when the group width is 6 (right three panels), there exists 5 spurious permitted sets as shown in panel F. [sent-178, score-1.179]
</p><p>86 As we have mentioned earlier, the lateral inhibition strength (3 plays a critical role in determining the dynamics of the network. [sent-179, score-0.409]
</p><p>87 (2) shows four types of steady states of a ring network corresponding to different values of (3. [sent-181, score-0.344]
</p><p>88 6  Discussion  We have shown that it is possible to organize lateral inhibition to mediate a winner-take-all competition between potentially overlapping groups of neurons. [sent-182, score-0.99]
</p><p>89 Our construction utilizes the distinction between permitted and forbidden sets of neurons. [sent-183, score-0.877]
</p><p>90 If there is strong lateral inhibition between two neurons, then any set that contains  them is forbidden (Lemma 2). [sent-184, score-0.542]
</p><p>91 Neurons that belong to the same group do not have any mutual inhibition, and so they form a permitted set. [sent-185, score-0.951]
</p><p>92 Because the synaptic connections between neurons in the same group are only composed of self-excitation, their outputs equal their rectified inputs, amplified by the gain factor of 1/(1 - a). [sent-186, score-0.64]
</p><p>93 Hence the neurons in the winning group operate in a purely analog regime. [sent-187, score-0.653]
</p><p>94 It is an interesting open question whether there exists a general way of how to translate arbitrary groups of coactive neurons into permitted sets without involving spurious permitted sets. [sent-193, score-2.152]
</p><p>95 The network is a ring network of 15 neurons with ories as dynamical attracwidth d = 5 and where 0: = 0. [sent-197, score-0.574]
</p><p>96 874, in which the network forms a continuous is to regard permitted sets attractor. [sent-203, score-0.736]
</p><p>97 (C) Forbidden sets exist, and so do spurious permitted sets. [sent-204, score-0.832]
</p><p>98 From this viewpoint, the contribution of the present paper is a method of storing and retrieving memories as permitted sets in neural networks. [sent-207, score-0.717]
</p><p>99 About the piecewise analysis of networks of linear threshold neurons . [sent-235, score-0.332]
</p><p>100 Neurons with graded response have collective properties like those of two-state neurons . [sent-268, score-0.313]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('permitted', 0.583), ('groups', 0.358), ('neurons', 0.313), ('group', 0.278), ('forbidden', 0.194), ('inhibition', 0.183), ('competition', 0.17), ('spurious', 0.168), ('steady', 0.155), ('lateral', 0.142), ('ring', 0.117), ('winner', 0.1), ('membership', 0.09), ('belong', 0.09), ('neuron', 0.087), ('sets', 0.081), ('network', 0.072), ('coactivated', 0.072), ('subsets', 0.068), ('nonnegative', 0.065), ('contained', 0.065), ('degenerate', 0.06), ('active', 0.052), ('panels', 0.052), ('corollary', 0.051), ('largest', 0.05), ('lemma', 0.049), ('inhibitory', 0.048), ('width', 0.048), ('dynamics', 0.047), ('stable', 0.047), ('ath', 0.043), ('connectivity', 0.041), ('sketch', 0.041), ('theorem', 0.04), ('potentially', 0.039), ('mediate', 0.039), ('orthant', 0.039), ('belongs', 0.039), ('strength', 0.037), ('unity', 0.036), ('otherwise', 0.035), ('viewpoint', 0.034), ('bmax', 0.033), ('nondegenerate', 0.033), ('winners', 0.033), ('winning', 0.033), ('synaptic', 0.031), ('stability', 0.03), ('arising', 0.03), ('memories', 0.03), ('overlapping', 0.03), ('every', 0.029), ('analog', 0.029), ('asymptotically', 0.029), ('containing', 0.029), ('input', 0.029), ('bi', 0.029), ('organize', 0.029), ('amax', 0.029), ('coactive', 0.029), ('hahnloser', 0.029), ('superset', 0.029), ('proof', 0.028), ('condition', 0.026), ('characterize', 0.026), ('state', 0.025), ('initial', 0.025), ('must', 0.024), ('great', 0.024), ('bk', 0.024), ('sparsely', 0.024), ('globally', 0.024), ('contain', 0.023), ('contains', 0.023), ('conditions', 0.023), ('suppose', 0.023), ('storing', 0.023), ('inactive', 0.023), ('least', 0.022), ('organization', 0.021), ('exists', 0.021), ('matrix', 0.02), ('richard', 0.02), ('distinction', 0.019), ('networks', 0.019), ('max', 0.019), ('identity', 0.019), ('ordered', 0.018), ('connections', 0.018), ('exist', 0.018), ('prove', 0.017), ('definition', 0.017), ('inputs', 0.017), ('always', 0.017), ('provided', 0.017), ('traditional', 0.017), ('inspired', 0.017), ('whether', 0.016), ('refers', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="81-tfidf-1" href="./nips-2000-Learning_Winner-take-all_Competition_Between_Groups_of_Neurons_in_Lateral_Inhibitory_Networks.html">81 nips-2000-Learning Winner-take-all Competition Between Groups of Neurons in Lateral Inhibitory Networks</a></p>
<p>Author: Xiaohui Xie, Richard H. R. Hahnloser, H. Sebastian Seung</p><p>Abstract: It has long been known that lateral inhibition in neural networks can lead to a winner-take-all competition, so that only a single neuron is active at a steady state. Here we show how to organize lateral inhibition so that groups of neurons compete to be active. Given a collection of potentially overlapping groups, the inhibitory connectivity is set by a formula that can be interpreted as arising from a simple learning rule. Our analysis demonstrates that such inhibition generally results in winner-take-all competition between the given groups, with the exception of some degenerate cases. In a broader context, the network serves as a particular illustration of the general distinction between permitted and forbidden sets, which was introduced recently. From this viewpoint, the computational function of our network is to store and retrieve memories as permitted sets of coactive neurons. In traditional winner-take-all networks, lateral inhibition is used to enforce a localized, or</p><p>2 0.70127285 <a title="81-tfidf-2" href="./nips-2000-Permitted_and_Forbidden_Sets_in_Symmetric_Threshold-Linear_Networks.html">100 nips-2000-Permitted and Forbidden Sets in Symmetric Threshold-Linear Networks</a></p>
<p>Author: Richard H. R. Hahnloser, H. Sebastian Seung</p><p>Abstract: Ascribing computational principles to neural feedback circuits is an important problem in theoretical neuroscience. We study symmetric threshold-linear networks and derive stability results that go beyond the insights that can be gained from Lyapunov theory or energy functions. By applying linear analysis to subnetworks composed of coactive neurons, we determine the stability of potential steady states. We find that stability depends on two types of eigenmodes. One type determines global stability and the other type determines whether or not multistability is possible. We can prove the equivalence of our stability criteria with criteria taken from quadratic programming. Also, we show that there are permitted sets of neurons that can be coactive at a steady state and forbidden sets that cannot. Permitted sets are clustered in the sense that subsets of permitted sets are permitted and supersets of forbidden sets are forbidden. By viewing permitted sets as memories stored in the synaptic connections, we can provide a formulation of longterm memory that is more general than the traditional perspective of fixed point attractor networks. A Lyapunov-function can be used to prove that a given set of differential equations is convergent. For example, if a neural network possesses a Lyapunov-function, then for almost any initial condition, the outputs of the neurons converge to a stable steady state. In the past, this stability-property was used to construct attractor networks that associatively recall memorized patterns. Lyapunov theory applies mainly to symmetric networks in which neurons have monotonic activation functions [1, 2]. Here we show that the restriction of activation functions to threshold-linear ones is not a mere limitation, but can yield new insights into the computational behavior of recurrent networks (for completeness, see also [3]). We present three main theorems about the neural responses to constant inputs. The first theorem provides necessary and sufficient conditions on the synaptic weight matrix for the existence of a globally asymptotically stable set of fixed points. These conditions can be expressed in terms of copositivity, a concept from quadratic programming and linear complementarity theory. Alternatively, they can be expressed in terms of certain eigenvalues and eigenvectors of submatrices of the synaptic weight matrix, making a connection to linear systems theory. The theorem guarantees that the network will produce a steady state response to any constant input. We regard this response as the computational output of the network, and its characterization is the topic of the second and third theorems. In the second theorem, we introduce the idea of permitted and forbidden sets. Under certain conditions on the synaptic weight matrix, we show that there exist sets of neurons that are</p><p>3 0.12953074 <a title="81-tfidf-3" href="./nips-2000-Who_Does_What%3F_A_Novel_Algorithm_to_Determine_Function_Localization.html">147 nips-2000-Who Does What? A Novel Algorithm to Determine Function Localization</a></p>
<p>Author: Ranit Aharonov-Barki, Isaac Meilijson, Eytan Ruppin</p><p>Abstract: We introduce a novel algorithm, termed PPA (Performance Prediction Algorithm), that quantitatively measures the contributions of elements of a neural system to the tasks it performs. The algorithm identifies the neurons or areas which participate in a cognitive or behavioral task, given data about performance decrease in a small set of lesions. It also allows the accurate prediction of performances due to multi-element lesions. The effectiveness of the new algorithm is demonstrated in two models of recurrent neural networks with complex interactions among the elements. The algorithm is scalable and applicable to the analysis of large neural networks. Given the recent advances in reversible inactivation techniques, it has the potential to significantly contribute to the understanding of the organization of biological nervous systems, and to shed light on the long-lasting debate about local versus distributed computation in the brain.</p><p>4 0.11585049 <a title="81-tfidf-4" href="./nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">42 nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<p>Author: Barbara Zenger, Christof Koch</p><p>Abstract: We describe an analogy between psychophysically measured effects in contrast masking, and the behavior of a simple integrate-andfire neuron that receives time-modulated inhibition. In the psychophysical experiments, we tested observers ability to discriminate contrasts of peripheral Gabor patches in the presence of collinear Gabor flankers. The data reveal a complex interaction pattern that we account for by assuming that flankers provide divisive inhibition to the target unit for low target contrasts, but provide subtractive inhibition to the target unit for higher target contrasts. A similar switch from divisive to subtractive inhibition is observed in an integrate-and-fire unit that receives inhibition modulated in time such that the cell spends part of the time in a high-inhibition state and part of the time in a low-inhibition state. The similarity between the effects suggests that one may cause the other. The biophysical model makes testable predictions for physiological single-cell recordings. 1 Psychophysics Visual images of Gabor patches are thought to excite a small and specific subset of neurons in the primary visual cortex and beyond. By measuring psychophysically in humans the contrast detection and discrimination thresholds of peripheral Gabor patches, one can estimate the sensitivity of this subset of neurons. Furthermore, spatial interactions between different neuronal populations can be probed by testing the effects of additional Gabor patches (masks) on performance. Such experiments have revealed a highly configuration-specific pattern of excitatory and inhibitory spatial interactions [1, 2]. 1.1 Methods Two vertical Gabor patches with a spatial frequency of 4cyc/deg were presented at 4 deg eccentricity left and right of fixation, and observers had to report which patch had the higher contrast (spatial 2AFC). In the</p><p>5 0.097202502 <a title="81-tfidf-5" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>Author: Kevin A. Archie, Bartlett W. Mel</p><p>Abstract: Neurons in area V4 have relatively large receptive fields (RFs), so multiple visual features are simultaneously</p><p>6 0.080844253 <a title="81-tfidf-6" href="./nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">129 nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<p>7 0.070419751 <a title="81-tfidf-7" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>8 0.063056313 <a title="81-tfidf-8" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>9 0.059435893 <a title="81-tfidf-9" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>10 0.058007993 <a title="81-tfidf-10" href="./nips-2000-Competition_and_Arbors_in_Ocular_Dominance.html">34 nips-2000-Competition and Arbors in Ocular Dominance</a></p>
<p>11 0.05198475 <a title="81-tfidf-11" href="./nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">88 nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>12 0.050557088 <a title="81-tfidf-12" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>13 0.049858782 <a title="81-tfidf-13" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>14 0.049368143 <a title="81-tfidf-14" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>15 0.048925739 <a title="81-tfidf-15" href="./nips-2000-Foundations_for_a_Circuit_Complexity_Theory_of_Sensory_Processing.html">56 nips-2000-Foundations for a Circuit Complexity Theory of Sensory Processing</a></p>
<p>16 0.047132179 <a title="81-tfidf-16" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>17 0.044969108 <a title="81-tfidf-17" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>18 0.044590242 <a title="81-tfidf-18" href="./nips-2000-Homeostasis_in_a_Silicon_Integrate_and_Fire_Neuron.html">67 nips-2000-Homeostasis in a Silicon Integrate and Fire Neuron</a></p>
<p>19 0.042043351 <a title="81-tfidf-19" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>20 0.038914274 <a title="81-tfidf-20" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.152), (1, -0.13), (2, -0.284), (3, -0.091), (4, 0.138), (5, 0.022), (6, 0.047), (7, -0.614), (8, -0.481), (9, -0.06), (10, 0.175), (11, 0.021), (12, 0.068), (13, 0.076), (14, -0.062), (15, 0.045), (16, -0.051), (17, -0.023), (18, -0.086), (19, -0.054), (20, -0.005), (21, 0.01), (22, 0.04), (23, 0.032), (24, -0.001), (25, -0.02), (26, -0.026), (27, 0.013), (28, 0.012), (29, 0.043), (30, -0.024), (31, -0.027), (32, -0.005), (33, -0.027), (34, 0.003), (35, -0.014), (36, 0.044), (37, -0.015), (38, -0.016), (39, -0.013), (40, 0.002), (41, -0.018), (42, -0.013), (43, -0.005), (44, 0.047), (45, 0.012), (46, -0.011), (47, -0.02), (48, 0.026), (49, -0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98599273 <a title="81-lsi-1" href="./nips-2000-Learning_Winner-take-all_Competition_Between_Groups_of_Neurons_in_Lateral_Inhibitory_Networks.html">81 nips-2000-Learning Winner-take-all Competition Between Groups of Neurons in Lateral Inhibitory Networks</a></p>
<p>Author: Xiaohui Xie, Richard H. R. Hahnloser, H. Sebastian Seung</p><p>Abstract: It has long been known that lateral inhibition in neural networks can lead to a winner-take-all competition, so that only a single neuron is active at a steady state. Here we show how to organize lateral inhibition so that groups of neurons compete to be active. Given a collection of potentially overlapping groups, the inhibitory connectivity is set by a formula that can be interpreted as arising from a simple learning rule. Our analysis demonstrates that such inhibition generally results in winner-take-all competition between the given groups, with the exception of some degenerate cases. In a broader context, the network serves as a particular illustration of the general distinction between permitted and forbidden sets, which was introduced recently. From this viewpoint, the computational function of our network is to store and retrieve memories as permitted sets of coactive neurons. In traditional winner-take-all networks, lateral inhibition is used to enforce a localized, or</p><p>2 0.95484388 <a title="81-lsi-2" href="./nips-2000-Permitted_and_Forbidden_Sets_in_Symmetric_Threshold-Linear_Networks.html">100 nips-2000-Permitted and Forbidden Sets in Symmetric Threshold-Linear Networks</a></p>
<p>Author: Richard H. R. Hahnloser, H. Sebastian Seung</p><p>Abstract: Ascribing computational principles to neural feedback circuits is an important problem in theoretical neuroscience. We study symmetric threshold-linear networks and derive stability results that go beyond the insights that can be gained from Lyapunov theory or energy functions. By applying linear analysis to subnetworks composed of coactive neurons, we determine the stability of potential steady states. We find that stability depends on two types of eigenmodes. One type determines global stability and the other type determines whether or not multistability is possible. We can prove the equivalence of our stability criteria with criteria taken from quadratic programming. Also, we show that there are permitted sets of neurons that can be coactive at a steady state and forbidden sets that cannot. Permitted sets are clustered in the sense that subsets of permitted sets are permitted and supersets of forbidden sets are forbidden. By viewing permitted sets as memories stored in the synaptic connections, we can provide a formulation of longterm memory that is more general than the traditional perspective of fixed point attractor networks. A Lyapunov-function can be used to prove that a given set of differential equations is convergent. For example, if a neural network possesses a Lyapunov-function, then for almost any initial condition, the outputs of the neurons converge to a stable steady state. In the past, this stability-property was used to construct attractor networks that associatively recall memorized patterns. Lyapunov theory applies mainly to symmetric networks in which neurons have monotonic activation functions [1, 2]. Here we show that the restriction of activation functions to threshold-linear ones is not a mere limitation, but can yield new insights into the computational behavior of recurrent networks (for completeness, see also [3]). We present three main theorems about the neural responses to constant inputs. The first theorem provides necessary and sufficient conditions on the synaptic weight matrix for the existence of a globally asymptotically stable set of fixed points. These conditions can be expressed in terms of copositivity, a concept from quadratic programming and linear complementarity theory. Alternatively, they can be expressed in terms of certain eigenvalues and eigenvectors of submatrices of the synaptic weight matrix, making a connection to linear systems theory. The theorem guarantees that the network will produce a steady state response to any constant input. We regard this response as the computational output of the network, and its characterization is the topic of the second and third theorems. In the second theorem, we introduce the idea of permitted and forbidden sets. Under certain conditions on the synaptic weight matrix, we show that there exist sets of neurons that are</p><p>3 0.38544199 <a title="81-lsi-3" href="./nips-2000-Who_Does_What%3F_A_Novel_Algorithm_to_Determine_Function_Localization.html">147 nips-2000-Who Does What? A Novel Algorithm to Determine Function Localization</a></p>
<p>Author: Ranit Aharonov-Barki, Isaac Meilijson, Eytan Ruppin</p><p>Abstract: We introduce a novel algorithm, termed PPA (Performance Prediction Algorithm), that quantitatively measures the contributions of elements of a neural system to the tasks it performs. The algorithm identifies the neurons or areas which participate in a cognitive or behavioral task, given data about performance decrease in a small set of lesions. It also allows the accurate prediction of performances due to multi-element lesions. The effectiveness of the new algorithm is demonstrated in two models of recurrent neural networks with complex interactions among the elements. The algorithm is scalable and applicable to the analysis of large neural networks. Given the recent advances in reversible inactivation techniques, it has the potential to significantly contribute to the understanding of the organization of biological nervous systems, and to shed light on the long-lasting debate about local versus distributed computation in the brain.</p><p>4 0.25914434 <a title="81-lsi-4" href="./nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">42 nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<p>Author: Barbara Zenger, Christof Koch</p><p>Abstract: We describe an analogy between psychophysically measured effects in contrast masking, and the behavior of a simple integrate-andfire neuron that receives time-modulated inhibition. In the psychophysical experiments, we tested observers ability to discriminate contrasts of peripheral Gabor patches in the presence of collinear Gabor flankers. The data reveal a complex interaction pattern that we account for by assuming that flankers provide divisive inhibition to the target unit for low target contrasts, but provide subtractive inhibition to the target unit for higher target contrasts. A similar switch from divisive to subtractive inhibition is observed in an integrate-and-fire unit that receives inhibition modulated in time such that the cell spends part of the time in a high-inhibition state and part of the time in a low-inhibition state. The similarity between the effects suggests that one may cause the other. The biophysical model makes testable predictions for physiological single-cell recordings. 1 Psychophysics Visual images of Gabor patches are thought to excite a small and specific subset of neurons in the primary visual cortex and beyond. By measuring psychophysically in humans the contrast detection and discrimination thresholds of peripheral Gabor patches, one can estimate the sensitivity of this subset of neurons. Furthermore, spatial interactions between different neuronal populations can be probed by testing the effects of additional Gabor patches (masks) on performance. Such experiments have revealed a highly configuration-specific pattern of excitatory and inhibitory spatial interactions [1, 2]. 1.1 Methods Two vertical Gabor patches with a spatial frequency of 4cyc/deg were presented at 4 deg eccentricity left and right of fixation, and observers had to report which patch had the higher contrast (spatial 2AFC). In the</p><p>5 0.20427488 <a title="81-lsi-5" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>Author: Kevin A. Archie, Bartlett W. Mel</p><p>Abstract: Neurons in area V4 have relatively large receptive fields (RFs), so multiple visual features are simultaneously</p><p>6 0.16671351 <a title="81-lsi-6" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>7 0.15985072 <a title="81-lsi-7" href="./nips-2000-Foundations_for_a_Circuit_Complexity_Theory_of_Sensory_Processing.html">56 nips-2000-Foundations for a Circuit Complexity Theory of Sensory Processing</a></p>
<p>8 0.15104643 <a title="81-lsi-8" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>9 0.14949346 <a title="81-lsi-9" href="./nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">129 nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<p>10 0.14244339 <a title="81-lsi-10" href="./nips-2000-Stability_and_Noise_in_Biochemical_Switches.html">125 nips-2000-Stability and Noise in Biochemical Switches</a></p>
<p>11 0.13882266 <a title="81-lsi-11" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>12 0.13249442 <a title="81-lsi-12" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>13 0.13197991 <a title="81-lsi-13" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>14 0.12445102 <a title="81-lsi-14" href="./nips-2000-Competition_and_Arbors_in_Ocular_Dominance.html">34 nips-2000-Competition and Arbors in Ocular Dominance</a></p>
<p>15 0.1200841 <a title="81-lsi-15" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>16 0.10837676 <a title="81-lsi-16" href="./nips-2000-Active_Support_Vector_Machine_Classification.html">18 nips-2000-Active Support Vector Machine Classification</a></p>
<p>17 0.10770691 <a title="81-lsi-17" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>18 0.10369189 <a title="81-lsi-18" href="./nips-2000-Algorithms_for_Non-negative_Matrix_Factorization.html">22 nips-2000-Algorithms for Non-negative Matrix Factorization</a></p>
<p>19 0.10257447 <a title="81-lsi-19" href="./nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">88 nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>20 0.10147436 <a title="81-lsi-20" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.017), (16, 0.018), (17, 0.104), (33, 0.029), (42, 0.033), (55, 0.03), (62, 0.047), (65, 0.012), (67, 0.062), (75, 0.011), (76, 0.037), (77, 0.181), (81, 0.018), (90, 0.259), (97, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93592089 <a title="81-lda-1" href="./nips-2000-Learning_Winner-take-all_Competition_Between_Groups_of_Neurons_in_Lateral_Inhibitory_Networks.html">81 nips-2000-Learning Winner-take-all Competition Between Groups of Neurons in Lateral Inhibitory Networks</a></p>
<p>Author: Xiaohui Xie, Richard H. R. Hahnloser, H. Sebastian Seung</p><p>Abstract: It has long been known that lateral inhibition in neural networks can lead to a winner-take-all competition, so that only a single neuron is active at a steady state. Here we show how to organize lateral inhibition so that groups of neurons compete to be active. Given a collection of potentially overlapping groups, the inhibitory connectivity is set by a formula that can be interpreted as arising from a simple learning rule. Our analysis demonstrates that such inhibition generally results in winner-take-all competition between the given groups, with the exception of some degenerate cases. In a broader context, the network serves as a particular illustration of the general distinction between permitted and forbidden sets, which was introduced recently. From this viewpoint, the computational function of our network is to store and retrieve memories as permitted sets of coactive neurons. In traditional winner-take-all networks, lateral inhibition is used to enforce a localized, or</p><p>2 0.84177953 <a title="81-lda-2" href="./nips-2000-A_Gradient-Based_Boosting_Algorithm_for_Regression_Problems.html">3 nips-2000-A Gradient-Based Boosting Algorithm for Regression Problems</a></p>
<p>Author: Richard S. Zemel, Toniann Pitassi</p><p>Abstract: In adaptive boosting, several weak learners trained sequentially are combined to boost the overall algorithm performance. Recently adaptive boosting methods for classification problems have been derived as gradient descent algorithms. This formulation justifies key elements and parameters in the methods, all chosen to optimize a single common objective function. We propose an analogous formulation for adaptive boosting of regression problems, utilizing a novel objective function that leads to a simple boosting algorithm. We prove that this method reduces training error, and compare its performance to other regression methods. The aim of boosting algorithms is to</p><p>3 0.83431923 <a title="81-lda-3" href="./nips-2000-Active_Support_Vector_Machine_Classification.html">18 nips-2000-Active Support Vector Machine Classification</a></p>
<p>Author: Olvi L. Mangasarian, David R. Musicant</p><p>Abstract: An active set strategy is applied to the dual of a simple reformulation of the standard quadratic program of a linear support vector machine. This application generates a fast new dual algorithm that consists of solving a finite number of linear equations, with a typically large dimensionality equal to the number of points to be classified. However, by making novel use of the Sherman-MorrisonWoodbury formula , a much smaller matrix of the order of the original input space is inverted at each step. Thus, a problem with a 32-dimensional input space and 7 million points required inverting positive definite symmetric matrices of size 33 x 33 with a total running time of 96 minutes on a 400 MHz Pentium II. The algorithm requires no specialized quadratic or linear programming code, but merely a linear equation solver which is publicly available. 1</p><p>4 0.83034784 <a title="81-lda-4" href="./nips-2000-Permitted_and_Forbidden_Sets_in_Symmetric_Threshold-Linear_Networks.html">100 nips-2000-Permitted and Forbidden Sets in Symmetric Threshold-Linear Networks</a></p>
<p>Author: Richard H. R. Hahnloser, H. Sebastian Seung</p><p>Abstract: Ascribing computational principles to neural feedback circuits is an important problem in theoretical neuroscience. We study symmetric threshold-linear networks and derive stability results that go beyond the insights that can be gained from Lyapunov theory or energy functions. By applying linear analysis to subnetworks composed of coactive neurons, we determine the stability of potential steady states. We find that stability depends on two types of eigenmodes. One type determines global stability and the other type determines whether or not multistability is possible. We can prove the equivalence of our stability criteria with criteria taken from quadratic programming. Also, we show that there are permitted sets of neurons that can be coactive at a steady state and forbidden sets that cannot. Permitted sets are clustered in the sense that subsets of permitted sets are permitted and supersets of forbidden sets are forbidden. By viewing permitted sets as memories stored in the synaptic connections, we can provide a formulation of longterm memory that is more general than the traditional perspective of fixed point attractor networks. A Lyapunov-function can be used to prove that a given set of differential equations is convergent. For example, if a neural network possesses a Lyapunov-function, then for almost any initial condition, the outputs of the neurons converge to a stable steady state. In the past, this stability-property was used to construct attractor networks that associatively recall memorized patterns. Lyapunov theory applies mainly to symmetric networks in which neurons have monotonic activation functions [1, 2]. Here we show that the restriction of activation functions to threshold-linear ones is not a mere limitation, but can yield new insights into the computational behavior of recurrent networks (for completeness, see also [3]). We present three main theorems about the neural responses to constant inputs. The first theorem provides necessary and sufficient conditions on the synaptic weight matrix for the existence of a globally asymptotically stable set of fixed points. These conditions can be expressed in terms of copositivity, a concept from quadratic programming and linear complementarity theory. Alternatively, they can be expressed in terms of certain eigenvalues and eigenvectors of submatrices of the synaptic weight matrix, making a connection to linear systems theory. The theorem guarantees that the network will produce a steady state response to any constant input. We regard this response as the computational output of the network, and its characterization is the topic of the second and third theorems. In the second theorem, we introduce the idea of permitted and forbidden sets. Under certain conditions on the synaptic weight matrix, we show that there exist sets of neurons that are</p><p>5 0.8270157 <a title="81-lda-5" href="./nips-2000-Support_Vector_Novelty_Detection_Applied_to_Jet_Engine_Vibration_Spectra.html">128 nips-2000-Support Vector Novelty Detection Applied to Jet Engine Vibration Spectra</a></p>
<p>Author: Paul M. Hayton, Bernhard Schölkopf, Lionel Tarassenko, Paul Anuzis</p><p>Abstract: A system has been developed to extract diagnostic information from jet engine carcass vibration data. Support Vector Machines applied to novelty detection provide a measure of how unusual the shape of a vibration signature is, by learning a representation of normality. We describe a novel method for Support Vector Machines of including information from a second class for novelty detection and give results from the application to Jet Engine vibration analysis.</p><p>6 0.70200145 <a title="81-lda-6" href="./nips-2000-Programmable_Reinforcement_Learning_Agents.html">105 nips-2000-Programmable Reinforcement Learning Agents</a></p>
<p>7 0.69512069 <a title="81-lda-7" href="./nips-2000-Fast_Training_of_Support_Vector_Classifiers.html">52 nips-2000-Fast Training of Support Vector Classifiers</a></p>
<p>8 0.64296246 <a title="81-lda-8" href="./nips-2000-Regularized_Winnow_Methods.html">111 nips-2000-Regularized Winnow Methods</a></p>
<p>9 0.63329685 <a title="81-lda-9" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>10 0.63032198 <a title="81-lda-10" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>11 0.60875452 <a title="81-lda-11" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>12 0.60418034 <a title="81-lda-12" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>13 0.60026586 <a title="81-lda-13" href="./nips-2000-Algorithmic_Stability_and_Generalization_Performance.html">21 nips-2000-Algorithmic Stability and Generalization Performance</a></p>
<p>14 0.57927364 <a title="81-lda-14" href="./nips-2000-Some_New_Bounds_on_the_Generalization_Error_of_Combined_Classifiers.html">119 nips-2000-Some New Bounds on the Generalization Error of Combined Classifiers</a></p>
<p>15 0.57209915 <a title="81-lda-15" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>16 0.56340408 <a title="81-lda-16" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>17 0.56047243 <a title="81-lda-17" href="./nips-2000-Generalizable_Singular_Value_Decomposition_for_Ill-posed_Datasets.html">61 nips-2000-Generalizable Singular Value Decomposition for Ill-posed Datasets</a></p>
<p>18 0.55765676 <a title="81-lda-18" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>19 0.55265808 <a title="81-lda-19" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>20 0.5490315 <a title="81-lda-20" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
