<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-98" href="#">nips2000-98</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</h1>
<br/><p>Source: <a title="nips-2000-98-pdf" href="http://papers.nips.cc/paper/1836-partially-observable-sde-models-for-image-sequence-recognition-tasks.pdf">pdf</a></p><p>Author: Javier R. Movellan, Paul Mineiro, Ruth J. Williams</p><p>Abstract: This paper explores a framework for recognition of image sequences using partially observable stochastic differential equation (SDE) models. Monte-Carlo importance sampling techniques are used for efficient estimation of sequence likelihoods and sequence likelihood gradients. Once the network dynamics are learned, we apply the SDE models to sequence recognition tasks in a manner similar to the way Hidden Markov models (HMMs) are commonly applied. The potential advantage of SDEs over HMMS is the use of continuous state dynamics. We present encouraging results for a video sequence recognition task in which SDE models provided excellent performance when compared to hidden Markov models. 1</p><p>Reference: <a title="nips-2000-98-reference" href="../nips2000_reference/nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Williams Department of Mathematics University of California San Diego  Abstract This paper explores a framework for recognition of image sequences using partially observable stochastic differential equation (SDE) models. [sent-4, score-0.598]
</p><p>2 Monte-Carlo importance sampling techniques are used for efficient estimation of sequence likelihoods and sequence likelihood gradients. [sent-5, score-0.369]
</p><p>3 Once the network dynamics are learned, we apply the SDE models to sequence recognition tasks in a manner similar to the way Hidden Markov models (HMMs) are commonly applied. [sent-6, score-0.471]
</p><p>4 The potential advantage of SDEs over HMMS is the use of continuous state dynamics. [sent-7, score-0.122]
</p><p>5 We present encouraging results for a video sequence recognition task in which SDE models provided excellent performance when compared to hidden Markov models. [sent-8, score-0.42]
</p><p>6 1  Introduction  This paper explores a framework for recognition of image sequences using partially observable stochastic differential equations (SDEs). [sent-9, score-0.563]
</p><p>7 In particular we use SDE models of low-power non-linear RC circuits with a significant thermal noise component. [sent-10, score-0.053]
</p><p>8 A diffusion network consists of a set of n nodes coupled via a vector of adaptive impedance parameters>' which are tuned to optimize the network's behavior. [sent-12, score-0.796]
</p><p>9 The temporal evolution of the n nodes defines a continuous stochastic process X that satisfies the following Ito SDE: dX(t)  = Ji-(X(t), >')dt + a dB(t),  (1)  (2)  X(O) '" v,  where v represents the (stochastic) initial conditions and B is standard Brownian motion. [sent-13, score-0.238]
</p><p>10 The drift is defined by a non-linear RC charging equation Ji-j(X(t),>')  = -1 Kj  ( ~j +Xj(t) - -Xj(t) ) , 1  Pj  for j  = 1,··· ,n,  (3)  where Ji-j is the drift of unit j, i. [sent-14, score-0.225]
</p><p>11 Here Xj is the internal potential at node j, Kj > 0 is the input capacitance, Pj the node resistance, ~j a  Dlstllhutl(lll. [sent-17, score-0.08]
</p><p>12 l  SDE Modeis  ODE Models  t+dt  Hidden Markov Models  Figure 1: An illustration of the differences between stochastic differential equation models (SDE), ordinary differential equation models (ODE) and Hidden Markov Models (HMM). [sent-18, score-0.422]
</p><p>13 In ODEs the the state dynamics are continuous and deterministic. [sent-19, score-0.183]
</p><p>14 In SDEs the state dynamics are continuous and stochastic. [sent-20, score-0.183]
</p><p>15 In HMMs the state dynamics are discrete and probabilistic. [sent-21, score-0.134]
</p><p>16 Intuition for equation (3) can be achieved by thinking of it as the limit of a discrete time stochastic difference equation,  X(t + ~t)  = X(t) + /-£(X(t), A)~t + u-/MZ(t) ,  (6)  where the Z(t) is an n-dimensional vector ofindependent standard Gaussian random variables. [sent-23, score-0.23]
</p><p>17 For a fixed state at time t there are two forces controlling the change in activation: the drift, which is deterministic, and the dispersion which is stochastic (see Figure 1). [sent-24, score-0.218]
</p><p>18 As ~t goes to zero, the solution to the difference equation (6) converges to the diffusion process defined in (3). [sent-26, score-0.66]
</p><p>19 Figures 1 and 2 shows the relationship between SDE models and other approaches in the neural network and the stochastic filtering literature. [sent-27, score-0.337]
</p><p>20 The main difference between ODE models, like standard recurrent neural networks, and SDE models is that the first has deterministic dynamics while the second has probabilistic dynamics. [sent-28, score-0.242]
</p><p>21 The main difference between HMMs and SDEs is that the first have discrete state dynamics while the second have continuous state dynamics. [sent-30, score-0.291]
</p><p>22 If the impedance matrix is symmetric and the network is given enough time to approximate stochastic equilibrium, diffusion network behave like continuous Boltzmann machines (Ackley, Hinton & Sejnowski, 1985). [sent-33, score-1.09]
</p><p>23 If the network is discretized in state and time it becomes a standard HMM. [sent-34, score-0.155]
</p><p>24 Finally, if the dispersion constant is set to zero the network behaves like a deterministic recurrent neural network. [sent-35, score-0.256]
</p><p>25 In order to use of SDE models we need a method for finding the likelihood and the likelihood gradient of observed sequences. [sent-36, score-0.177]
</p><p>26 2  Observed sequence likelihoods  We regard the first d components of an SDE model as observable and denote them by O. [sent-38, score-0.279]
</p><p>27 The last n - d components are denoted by H and named unobservable or hidden. [sent-39, score-0.08]
</p><p>28 Hidden components are included for modeling non-Markovian dependencies in the observable components. [sent-40, score-0.176]
</p><p>29 Let no, nh be the outcome spaces for the observable and hidden processes. [sent-41, score-0.291]
</p><p>30 Here each outcome W is a continuous path w : [0, T] --t IRn. [sent-43, score-0.234]
</p><p>31 For each wEn, we write w = (w o, Wh), where Wo represents the observable dimensions of the path and Wh the hidden dimensions. [sent-44, score-0.366]
</p><p>32 Let Q>'(A) represent the probability that a network with parameter A generates paths in the set A, Q~(Ao) the probability that the observable components generate paths in Ao and Q~(Ah) the probability that the hidden components generate paths in A h . [sent-45, score-1.01]
</p><p>33 To apply the familiar techniques of maximum likelihood and Bayesian estimation we use as reference the probability distribution of a diffusion network with zero drift, Le. [sent-46, score-0.766]
</p><p>34 , the paths generated by this network are Brownian motion scaled by u. [sent-47, score-0.308]
</p><p>35 We denote such reference distribution as R, its observable and hidden components as R o, Rh. [sent-48, score-0.283]
</p><p>36 (8)  The first integral in (8) is an Ito stochastic integral, the second is a standard Lebesgue integral. [sent-56, score-0.128]
</p><p>37 For a fixed path Wo the term L~(wo) is a likelihood function of A that can be used for Maximum likelihood estimation. [sent-58, score-0.238]
</p><p>38 To obtain the likelihood gradient, we differentiate (7) which yields  f  \7>. [sent-59, score-0.062]
</p><p>39 1  = W(t) -  W(O)  -lot  p,(w(u), A) duo  (14)  Importance sampling  The likelihood of observed paths (7), and the gradient of the likelihood (9) require averaging with respect to the distribution of hidden paths Rh. [sent-62, score-0.686]
</p><p>40 We estimate these averages using an importance sampling in the space of sample paths. [sent-63, score-0.179]
</p><p>41 Instead of sampling from Rh we sample from a distribution that weights more heavily regions where L~ h is large. [sent-64, score-0.117]
</p><p>42 Each sample is then weighted by the density of the sampling distributi~n with respect to Rh. [sent-65, score-0.144]
</p><p>43 This weighting function is commonly known as the importance function in the Monte-Carlo literature (Fishman, 1996, p. [sent-66, score-0.062]
</p><p>44 In particular for each observable path Wo we let the sampling distribution S~,wo be the probability distribution generated by a diffusion network with parameter A which has been forced to exhibit the path Wo over the observable units. [sent-68, score-1.324]
</p><p>45 The approach reminiscent of the technique of teacher forcing from deterministic neural networks. [sent-69, score-0.046]
</p><p>46 One interesting property of this approach is that the sampling distributions S~,wo change as learning progresses, since they depend on A. [sent-75, score-0.067]
</p><p>47 Figure 3 shows results of a computer simulation in which a 2 unit network was trained to oscillate. [sent-76, score-0.17]
</p><p>48 We tried an oscillation pattern because of its relevance for the application we explore in a later section, which involves recognizing sequences of lip movements. [sent-77, score-0.137]
</p><p>49 The figure shows the "training" path and a couple of sample paths, one obtained with the u parameter set to 0, and one with the parameter set to 0. [sent-78, score-0.197]
</p><p>50 ,  Figure 3: Training a 2 unit network to maximize the likelihood of a sinusoidal path. [sent-112, score-0.176]
</p><p>51 The center graph shows a sample path obtained after training the network and setting a = 0, i. [sent-115, score-0.338]
</p><p>52 The bottom graph shows a sample path obtained with a = 0. [sent-118, score-0.224]
</p><p>53 3  Recognizing video sequences  In this section we illustrate the use of SDE models on a sequence classification task of reasonable difficulty with a body of realistic data. [sent-120, score-0.221]
</p><p>54 We chose this task since we know of SDE models used for tracking problems but know of no SDE models used for sequence recognition tasks. [sent-121, score-0.324]
</p><p>55 The potential advantage of SDEs over more established approaches such as HMMs is that they enforce continuity constraints, an aspect that may be beneficial when the actual signals are better described using continuous state dynamics. [sent-122, score-0.205]
</p><p>56 We compared a diffusion network approach with classic hidden Markov model approaches. [sent-123, score-0.811]
</p><p>57 For each student two sample utterances were taken for each of the digits "one" through "four". [sent-125, score-0.05]
</p><p>58 We compared the performance of diffusion networks and HMMs using two different image processing techniques (contours and contours plus intensity) in combination with 2 different recognition engines (HMMs and diffusion networks). [sent-129, score-1.416]
</p><p>59 They employ point density models, where each lip contour is represented by a set of points; in this case both the inner and outer lip contour are represented, corresponding to Luettin's double contour model. [sent-131, score-0.426]
</p><p>60 The dimensionality of the representation of the contours is reduced using principal component analysis. [sent-132, score-0.047]
</p><p>61 In this manner a total of 22 parameters were used to represent lip contour information for each still frame. [sent-134, score-0.155]
</p><p>62 These 22 parameters were represented using diffusion networks with 22 observation units, one per parameter value. [sent-135, score-0.659]
</p><p>63 We also tested the performance of a representation that used intensity information in addition to contour shape information. [sent-136, score-0.232]
</p><p>64 This approach used 62 parameters, which were represented using diffusion networks with 62 observation units. [sent-137, score-0.659]
</p><p>65 Approach Best HMM, shape information only Best diffusion network, shape information only Untrained human subjects Best HMM, shape and intensity information Best diffusion network, shape and intensity information Trained human subjects  Correct Generalization 82. [sent-138, score-1.794]
</p><p>66 Shown in order are the performance of the best performing HMM from (Luettin et al. [sent-145, score-0.06]
</p><p>67 We independently trained 4 diffusion networks, to approximate the distributions of lip-contour trajectories of each of the four words to be recognized, i. [sent-147, score-0.646]
</p><p>68 , the first network was trained with examples of the word "one", and the last network with examples of the word "four". [sent-149, score-0.284]
</p><p>69 Each network had the same number of nodes, and the drift of each network was given by (3) with K. [sent-150, score-0.323]
</p><p>70 The number of hidden units was varied from one to 5. [sent-153, score-0.151]
</p><p>71 The initial state of the hidden units was set to (1, . [sent-155, score-0.192]
</p><p>72 The diffusion network dynamics were simulated using a forward-Euler technique, i. [sent-159, score-0.765]
</p><p>73 , equation (1) is approximated in discrete time using (6). [sent-161, score-0.067]
</p><p>74 t = 1/30 seconds, the time between video frame samples. [sent-163, score-0.049]
</p><p>75 Each diffusion network was trained with examples of one of the 4 digits using the cost function  ~(A) = L  log i~(y(i)) -  ~aIAI2,  (16)  i  where {y(i)} are samples from the desired empirical distribution Po and a is the strength of a Gaussian prior on the network parameters. [sent-164, score-0.874]
</p><p>76 Best results were obtained with diffusion networks with 4 hidden units. [sent-165, score-0.799]
</p><p>77 The log-likelihood gradients were estimated using the importance sampling approach with m = 20, i. [sent-166, score-0.129]
</p><p>78 , we generated 20 hidden sample paths per observed path. [sent-168, score-0.351]
</p><p>79 With this number of samples training took about 10 times longer with diffusion networks than with HMMs. [sent-169, score-0.659]
</p><p>80 At test time, computation of the likelihood estimates was very fast and could have been done in real time using a fast Pentium II. [sent-170, score-0.062]
</p><p>81 The generalization performance was estimated using a jacknife (one-out) technique: we trained on all subjects but one, which is used for testing. [sent-171, score-0.111]
</p><p>82 The table includes HMM results reported by Luettin (1997), who tried a variety of HMM architectures and reported the best results obtained with them. [sent-174, score-0.174]
</p><p>83 The only difference between Luettin's approach and our approach is the recognition engine, which was a bank of HMMs in his case and a bank of diffusion networks in our case. [sent-175, score-0.84]
</p><p>84 If anything we were at a disadvantage since the image representations mentioned above were optimized by Luettin to work best with HMMs. [sent-176, score-0.094]
</p><p>85 In all cases the best diffusion networks outperformed the best HMMs reported in the literature using exactly the same visual preprocessing. [sent-177, score-0.867]
</p><p>86 In all cases diffusion net-  works outperformed HMMs. [sent-178, score-0.62]
</p><p>87 4  Discussion  While we presented results for a video sequence recognition task, the same framework can be used for tasks such as sequence recognition, object tracking and sequence generation. [sent-181, score-0.446]
</p><p>88 Our work was inspired by the rich literature on continuous stochastic filtering and stochastic neural networks. [sent-182, score-0.379]
</p><p>89 The idea was to combine the versatility of recurrent neural networks and the well known advantages of stochastic modeling approaches. [sent-183, score-0.244]
</p><p>90 The continuous-time nature of the networks is convenient for data with dropouts or variable sample rates, since the models we use define all the finite dimensional distributions. [sent-184, score-0.172]
</p><p>91 The continuous-state representation is well suited to problems involving inference about continuous unobservable quantities, as in visual tracking tasks. [sent-185, score-0.218]
</p><p>92 Since these networks enforce continuity constraints in the observable paths they may not have the well known problems encountered when HMMs are used as generative models of continuous sequences. [sent-186, score-0.625]
</p><p>93 We have presented encouraging results on a realistic sequence recognition task. [sent-187, score-0.211]
</p><p>94 At this point the main disadvantage of diffusion networks relative to conventional hidden Markov models is training speed. [sent-189, score-0.819]
</p><p>95 The diffusion networks used here were approximately 10 times slower to train than HMMs. [sent-190, score-0.659]
</p><p>96 Moreover, once a network is trained, the computation of the density functions needed in recognition tasks can be done in real time. [sent-192, score-0.256]
</p><p>97 We are exploring applications of diffusion networks to stochastic filtering problems (e. [sent-193, score-0.829]
</p><p>98 , contour tracking) and sequence generation problems, not just sequence recognition problems. [sent-195, score-0.325]
</p><p>99 Our work shows that diffusion networks may be a feasible alternative to HMMs for problems in which state continuity is advantageous. [sent-196, score-0.757]
</p><p>100 The results obtained for the visual speech recognition task are encouraging, and reinforce the possibility that diffusion networks may become a versatile tool for a very wide variety of continuous signal processing tasks. [sent-197, score-0.917]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('diffusion', 0.59), ('sde', 0.39), ('luettin', 0.21), ('paths', 0.194), ('hmms', 0.157), ('observable', 0.145), ('wo', 0.137), ('stochastic', 0.128), ('sdes', 0.122), ('path', 0.114), ('network', 0.114), ('hidden', 0.107), ('drift', 0.095), ('contour', 0.089), ('hmm', 0.086), ('recognition', 0.086), ('movellan', 0.083), ('continuous', 0.081), ('wh', 0.078), ('shape', 0.078), ('sequence', 0.075), ('networks', 0.069), ('sampling', 0.067), ('lip', 0.066), ('intensity', 0.065), ('impedance', 0.063), ('importance', 0.062), ('likelihood', 0.062), ('dynamics', 0.061), ('best', 0.06), ('differential', 0.059), ('continuity', 0.057), ('ode', 0.057), ('brownian', 0.057), ('tracking', 0.057), ('trained', 0.056), ('subjects', 0.055), ('models', 0.053), ('sample', 0.05), ('encouraging', 0.05), ('video', 0.049), ('dispersion', 0.049), ('drh', 0.049), ('fishman', 0.049), ('karatzas', 0.049), ('shreve', 0.049), ('unobservable', 0.049), ('recurrent', 0.047), ('contours', 0.047), ('diego', 0.047), ('rp', 0.047), ('deterministic', 0.046), ('sequences', 0.044), ('units', 0.044), ('california', 0.042), ('filtering', 0.042), ('ackley', 0.042), ('untrained', 0.042), ('markov', 0.041), ('state', 0.041), ('node', 0.04), ('outcome', 0.039), ('database', 0.039), ('explores', 0.038), ('rc', 0.038), ('oh', 0.038), ('forced', 0.035), ('ito', 0.035), ('difference', 0.035), ('equation', 0.035), ('dt', 0.035), ('image', 0.034), ('san', 0.033), ('kj', 0.033), ('obtained', 0.033), ('discrete', 0.032), ('boltzmann', 0.032), ('components', 0.031), ('visual', 0.031), ('rh', 0.031), ('pj', 0.031), ('ao', 0.031), ('human', 0.031), ('cognitive', 0.03), ('xj', 0.03), ('bank', 0.03), ('outperformed', 0.03), ('filters', 0.029), ('partially', 0.029), ('tasks', 0.029), ('nodes', 0.029), ('likelihoods', 0.028), ('io', 0.028), ('tried', 0.027), ('reported', 0.027), ('speech', 0.027), ('graph', 0.027), ('density', 0.027), ('sejnowski', 0.026), ('enforce', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="98-tfidf-1" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>Author: Javier R. Movellan, Paul Mineiro, Ruth J. Williams</p><p>Abstract: This paper explores a framework for recognition of image sequences using partially observable stochastic differential equation (SDE) models. Monte-Carlo importance sampling techniques are used for efficient estimation of sequence likelihoods and sequence likelihood gradients. Once the network dynamics are learned, we apply the SDE models to sequence recognition tasks in a manner similar to the way Hidden Markov models (HMMs) are commonly applied. The potential advantage of SDEs over HMMS is the use of continuous state dynamics. We present encouraging results for a video sequence recognition task in which SDE models provided excellent performance when compared to hidden Markov models. 1</p><p>2 0.11479666 <a title="98-tfidf-2" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>Author: Michael S. Gray, Terrence J. Sejnowski, Javier R. Movellan</p><p>Abstract: We examine eight different techniques for developing visual representations in machine vision tasks. In particular we compare different versions of principal component and independent component analysis in combination with stepwise regression methods for variable selection. We found that local methods, based on the statistics of image patches, consistently outperformed global methods based on the statistics of entire images. This result is consistent with previous work on emotion and facial expression recognition. In addition, the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance. 1</p><p>3 0.11021888 <a title="98-tfidf-3" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>Author: Gal Elidan, Noam Lotner, Nir Friedman, Daphne Koller</p><p>Abstract: A serious problem in learning probabilistic models is the presence of hidden variables. These variables are not observed, yet interact with several of the observed variables. As such, they induce seemingly complex dependencies among the latter. In recent years, much attention has been devoted to the development of algorithms for learning parameters, and in some cases structure, in the presence of hidden variables. In this paper, we address the related problem of detecting hidden variables that interact with the observed variables. This problem is of interest both for improving our understanding of the domain and as a preliminary step that guides the learning procedure towards promising models. A very natural approach is to search for</p><p>4 0.10161912 <a title="98-tfidf-4" href="./nips-2000-New_Approaches_Towards_Robust_and_Adaptive_Speech_Recognition.html">90 nips-2000-New Approaches Towards Robust and Adaptive Speech Recognition</a></p>
<p>Author: Hervé Bourlard, Samy Bengio, Katrin Weber</p><p>Abstract: In this paper, we discuss some new research directions in automatic speech recognition (ASR), and which somewhat deviate from the usual approaches. More specifically, we will motivate and briefly describe new approaches based on multi-stream and multi/band ASR. These approaches extend the standard hidden Markov model (HMM) based approach by assuming that the different (frequency) channels representing the speech signal are processed by different (independent)</p><p>5 0.096085899 <a title="98-tfidf-5" href="./nips-2000-Machine_Learning_for_Video-Based_Rendering.html">83 nips-2000-Machine Learning for Video-Based Rendering</a></p>
<p>Author: Arno Schödl, Irfan A. Essa</p><p>Abstract: We present techniques for rendering and animation of realistic scenes by analyzing and training on short video sequences. This work extends the new paradigm for computer animation, video textures, which uses recorded video to generate novel animations by replaying the video samples in a new order. Here we concentrate on video sprites, which are a special type of video texture. In video sprites, instead of storing whole images, the object of interest is separated from the background and the video samples are stored as a sequence of alpha-matted sprites with associated velocity information. They can be rendered anywhere on the screen to create a novel animation of the object. We present methods to create such animations by finding a sequence of sprite samples that is both visually smooth and follows a desired path. To estimate visual smoothness, we train a linear classifier to estimate visual similarity between video samples. If the motion path is known in advance, we use beam search to find a good sample sequence. We can specify the motion interactively by precomputing the sequence cost function using Q-Iearning.</p><p>6 0.085509755 <a title="98-tfidf-6" href="./nips-2000-Shape_Context%3A_A_New_Descriptor_for_Shape_Matching_and_Object_Recognition.html">117 nips-2000-Shape Context: A New Descriptor for Shape Matching and Object Recognition</a></p>
<p>7 0.084722616 <a title="98-tfidf-7" href="./nips-2000-Stability_and_Noise_in_Biochemical_Switches.html">125 nips-2000-Stability and Noise in Biochemical Switches</a></p>
<p>8 0.084671088 <a title="98-tfidf-8" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>9 0.082836516 <a title="98-tfidf-9" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>10 0.081613235 <a title="98-tfidf-10" href="./nips-2000-Keeping_Flexible_Active_Contours_on_Track_using_Metropolis_Updates.html">72 nips-2000-Keeping Flexible Active Contours on Track using Metropolis Updates</a></p>
<p>11 0.079911098 <a title="98-tfidf-11" href="./nips-2000-Recognizing_Hand-written_Digits_Using_Hierarchical_Products_of_Experts.html">108 nips-2000-Recognizing Hand-written Digits Using Hierarchical Products of Experts</a></p>
<p>12 0.079177096 <a title="98-tfidf-12" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>13 0.078706145 <a title="98-tfidf-13" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>14 0.071607448 <a title="98-tfidf-14" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>15 0.071469374 <a title="98-tfidf-15" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>16 0.069597721 <a title="98-tfidf-16" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>17 0.066867344 <a title="98-tfidf-17" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>18 0.062798776 <a title="98-tfidf-18" href="./nips-2000-A_Neural_Probabilistic_Language_Model.html">6 nips-2000-A Neural Probabilistic Language Model</a></p>
<p>19 0.061412521 <a title="98-tfidf-19" href="./nips-2000-Learning_Joint_Statistical_Models_for_Audio-Visual_Fusion_and_Segregation.html">78 nips-2000-Learning Joint Statistical Models for Audio-Visual Fusion and Segregation</a></p>
<p>20 0.060114507 <a title="98-tfidf-20" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.217), (1, -0.095), (2, 0.116), (3, 0.037), (4, -0.006), (5, -0.009), (6, 0.064), (7, 0.001), (8, -0.015), (9, 0.032), (10, 0.118), (11, -0.08), (12, 0.181), (13, -0.034), (14, -0.002), (15, -0.089), (16, 0.059), (17, -0.001), (18, 0.081), (19, 0.027), (20, 0.161), (21, -0.079), (22, -0.096), (23, -0.118), (24, -0.06), (25, 0.074), (26, 0.007), (27, -0.136), (28, 0.042), (29, -0.058), (30, 0.029), (31, 0.13), (32, -0.023), (33, -0.055), (34, -0.012), (35, -0.002), (36, 0.003), (37, 0.02), (38, 0.08), (39, -0.278), (40, -0.142), (41, 0.044), (42, -0.231), (43, -0.154), (44, -0.029), (45, 0.04), (46, 0.282), (47, 0.083), (48, -0.015), (49, -0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94811636 <a title="98-lsi-1" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>Author: Javier R. Movellan, Paul Mineiro, Ruth J. Williams</p><p>Abstract: This paper explores a framework for recognition of image sequences using partially observable stochastic differential equation (SDE) models. Monte-Carlo importance sampling techniques are used for efficient estimation of sequence likelihoods and sequence likelihood gradients. Once the network dynamics are learned, we apply the SDE models to sequence recognition tasks in a manner similar to the way Hidden Markov models (HMMs) are commonly applied. The potential advantage of SDEs over HMMS is the use of continuous state dynamics. We present encouraging results for a video sequence recognition task in which SDE models provided excellent performance when compared to hidden Markov models. 1</p><p>2 0.46998832 <a title="98-lsi-2" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>Author: Gal Elidan, Noam Lotner, Nir Friedman, Daphne Koller</p><p>Abstract: A serious problem in learning probabilistic models is the presence of hidden variables. These variables are not observed, yet interact with several of the observed variables. As such, they induce seemingly complex dependencies among the latter. In recent years, much attention has been devoted to the development of algorithms for learning parameters, and in some cases structure, in the presence of hidden variables. In this paper, we address the related problem of detecting hidden variables that interact with the observed variables. This problem is of interest both for improving our understanding of the domain and as a preliminary step that guides the learning procedure towards promising models. A very natural approach is to search for</p><p>3 0.44723418 <a title="98-lsi-3" href="./nips-2000-Stability_and_Noise_in_Biochemical_Switches.html">125 nips-2000-Stability and Noise in Biochemical Switches</a></p>
<p>Author: William Bialek</p><p>Abstract: Many processes in biology, from the regulation of gene expression in bacteria to memory in the brain, involve switches constructed from networks of biochemical reactions. Crucial molecules are present in small numbers, raising questions about noise and stability. Analysis of noise in simple reaction schemes indicates that switches stable for years and switchable in milliseconds can be built from fewer than one hundred molecules. Prospects for direct tests of this prediction, as well as implications, are discussed. 1</p><p>4 0.42055297 <a title="98-lsi-4" href="./nips-2000-Shape_Context%3A_A_New_Descriptor_for_Shape_Matching_and_Object_Recognition.html">117 nips-2000-Shape Context: A New Descriptor for Shape Matching and Object Recognition</a></p>
<p>Author: Serge Belongie, Jitendra Malik, Jan Puzicha</p><p>Abstract: We develop an approach to object recognition based on matching shapes and using a resulting measure of similarity in a nearest neighbor classifier. The key algorithmic problem here is that of finding pointwise correspondences between an image shape and a stored prototype shape. We introduce a new shape descriptor, the shape context, which makes this possible, using a simple and robust algorithm. The shape context at a point captures the distribution over relative positions of other shape points and thus summarizes global shape in a rich, local descriptor. We demonstrate that shape contexts greatly simplify recovery of correspondences between points of two given shapes. Once shapes are aligned, shape contexts are used to define a robust score for measuring shape similarity. We have used this score in a nearest-neighbor classifier for recognition of hand written digits as well as 3D objects, using exactly the same distance function. On the benchmark MNIST dataset of handwritten digits, this yields an error rate of 0.63%, outperforming other published techniques. 1</p><p>5 0.40793291 <a title="98-lsi-5" href="./nips-2000-New_Approaches_Towards_Robust_and_Adaptive_Speech_Recognition.html">90 nips-2000-New Approaches Towards Robust and Adaptive Speech Recognition</a></p>
<p>Author: Hervé Bourlard, Samy Bengio, Katrin Weber</p><p>Abstract: In this paper, we discuss some new research directions in automatic speech recognition (ASR), and which somewhat deviate from the usual approaches. More specifically, we will motivate and briefly describe new approaches based on multi-stream and multi/band ASR. These approaches extend the standard hidden Markov model (HMM) based approach by assuming that the different (frequency) channels representing the speech signal are processed by different (independent)</p><p>6 0.36891899 <a title="98-lsi-6" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<p>7 0.35063002 <a title="98-lsi-7" href="./nips-2000-Machine_Learning_for_Video-Based_Rendering.html">83 nips-2000-Machine Learning for Video-Based Rendering</a></p>
<p>8 0.3460111 <a title="98-lsi-8" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>9 0.34344196 <a title="98-lsi-9" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>10 0.33637741 <a title="98-lsi-10" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>11 0.31271893 <a title="98-lsi-11" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>12 0.31226557 <a title="98-lsi-12" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>13 0.29818797 <a title="98-lsi-13" href="./nips-2000-Overfitting_in_Neural_Nets%3A_Backpropagation%2C_Conjugate_Gradient%2C_and_Early_Stopping.html">97 nips-2000-Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping</a></p>
<p>14 0.29121104 <a title="98-lsi-14" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>15 0.28959775 <a title="98-lsi-15" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>16 0.28385562 <a title="98-lsi-16" href="./nips-2000-Keeping_Flexible_Active_Contours_on_Track_using_Metropolis_Updates.html">72 nips-2000-Keeping Flexible Active Contours on Track using Metropolis Updates</a></p>
<p>17 0.28344288 <a title="98-lsi-17" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>18 0.27691224 <a title="98-lsi-18" href="./nips-2000-Mixtures_of_Gaussian_Processes.html">85 nips-2000-Mixtures of Gaussian Processes</a></p>
<p>19 0.27126423 <a title="98-lsi-19" href="./nips-2000-Learning_Switching_Linear_Models_of_Human_Motion.html">80 nips-2000-Learning Switching Linear Models of Human Motion</a></p>
<p>20 0.27052182 <a title="98-lsi-20" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.027), (10, 0.036), (17, 0.112), (32, 0.032), (33, 0.064), (54, 0.016), (55, 0.037), (62, 0.068), (65, 0.02), (67, 0.063), (75, 0.013), (76, 0.032), (79, 0.015), (81, 0.046), (90, 0.038), (91, 0.012), (94, 0.268), (97, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82063138 <a title="98-lda-1" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>Author: Dirk Ormoneit, Hedvig Sidenbladh, Michael J. Black, Trevor Hastie</p><p>Abstract: We present methods for learning and tracking human motion in video. We estimate a statistical model of typical activities from a large set of 3D periodic human motion data by segmenting these data automatically into</p><p>same-paper 2 0.81205535 <a title="98-lda-2" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>Author: Javier R. Movellan, Paul Mineiro, Ruth J. Williams</p><p>Abstract: This paper explores a framework for recognition of image sequences using partially observable stochastic differential equation (SDE) models. Monte-Carlo importance sampling techniques are used for efficient estimation of sequence likelihoods and sequence likelihood gradients. Once the network dynamics are learned, we apply the SDE models to sequence recognition tasks in a manner similar to the way Hidden Markov models (HMMs) are commonly applied. The potential advantage of SDEs over HMMS is the use of continuous state dynamics. We present encouraging results for a video sequence recognition task in which SDE models provided excellent performance when compared to hidden Markov models. 1</p><p>3 0.6184209 <a title="98-lda-3" href="./nips-2000-Learning_Switching_Linear_Models_of_Human_Motion.html">80 nips-2000-Learning Switching Linear Models of Human Motion</a></p>
<p>Author: Vladimir Pavlovic, James M. Rehg, John MacCormick</p><p>Abstract: The human figure exhibits complex and rich dynamic behavior that is both nonlinear and time-varying. Effective models of human dynamics can be learned from motion capture data using switching linear dynamic system (SLDS) models. We present results for human motion synthesis, classification, and visual tracking using learned SLDS models. Since exact inference in SLDS is intractable, we present three approximate inference algorithms and compare their performance. In particular, a new variational inference algorithm is obtained by casting the SLDS model as a Dynamic Bayesian Network. Classification experiments show the superiority of SLDS over conventional HMM's for our problem domain.</p><p>4 0.52017623 <a title="98-lda-4" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>Author: Zoubin Ghahramani, Matthew J. Beal</p><p>Abstract: Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set. 1</p><p>5 0.51795703 <a title="98-lda-5" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass, Eduardo D. Sontag, Anthony M. Zador</p><p>Abstract: Experimental data show that biological synapses behave quite differently from the symbolic synapses in common artificial neural network models. Biological synapses are dynamic, i.e., their</p><p>6 0.51575685 <a title="98-lda-6" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>7 0.50986272 <a title="98-lda-7" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>8 0.50863236 <a title="98-lda-8" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>9 0.50810128 <a title="98-lda-9" href="./nips-2000-Machine_Learning_for_Video-Based_Rendering.html">83 nips-2000-Machine Learning for Video-Based Rendering</a></p>
<p>10 0.50713849 <a title="98-lda-10" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>11 0.50474131 <a title="98-lda-11" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>12 0.50435179 <a title="98-lda-12" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>13 0.50179958 <a title="98-lda-13" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>14 0.50018001 <a title="98-lda-14" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<p>15 0.49842447 <a title="98-lda-15" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>16 0.49813935 <a title="98-lda-16" href="./nips-2000-Regularized_Winnow_Methods.html">111 nips-2000-Regularized Winnow Methods</a></p>
<p>17 0.49727541 <a title="98-lda-17" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>18 0.49534011 <a title="98-lda-18" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>19 0.49412757 <a title="98-lda-19" href="./nips-2000-Algorithms_for_Non-negative_Matrix_Factorization.html">22 nips-2000-Algorithms for Non-negative Matrix Factorization</a></p>
<p>20 0.49302042 <a title="98-lda-20" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
