<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-20" href="#">nips2000-20</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</h1>
<br/><p>Source: <a title="nips-2000-20-pdf" href="http://papers.nips.cc/paper/1826-algebraic-information-geometry-for-learning-machines-with-singularities.pdf">pdf</a></p><p>Author: Sumio Watanabe</p><p>Abstract: Algebraic geometry is essential to learning theory. In hierarchical learning machines such as layered neural networks and gaussian mixtures, the asymptotic normality does not hold , since Fisher information matrices are singular. In this paper , the rigorous asymptotic form of the stochastic complexity is clarified based on resolution of singularities and two different problems are studied. (1) If the prior is positive, then the stochastic complexity is far smaller than BIO, resulting in the smaller generalization error than regular statistical models, even when the true distribution is not contained in the parametric model. (2) If Jeffreys' prior, which is coordinate free and equal to zero at singularities, is employed then the stochastic complexity has the same form as BIO. It is useful for model selection, but not for generalization. 1</p><p>Reference: <a title="nips-2000-20-reference" href="../nips2000_reference/nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp  Abstract Algebraic geometry is essential to learning theory. [sent-4, score-0.114]
</p><p>2 In hierarchical learning machines such as layered neural networks and gaussian mixtures, the asymptotic normality does not hold , since Fisher information matrices are singular. [sent-5, score-0.484]
</p><p>3 In this paper , the rigorous asymptotic form of the stochastic complexity is clarified based on resolution of singularities and two different problems are studied. [sent-6, score-0.855]
</p><p>4 (1) If the prior is positive, then the stochastic complexity is far smaller than BIO, resulting in the smaller generalization error than regular statistical models, even when the true distribution is not contained in the parametric model. [sent-7, score-0.869]
</p><p>5 (2) If Jeffreys' prior, which is coordinate free and equal to zero at singularities, is employed then the stochastic complexity has the same form as BIO. [sent-8, score-0.244]
</p><p>6 1  Introduction  The Fisher information matrix determines a metric of the set of all parameters of a learning machine [2]. [sent-10, score-0.045]
</p><p>7 If it is positive definite, then a learning machine can be understood as a Riemannian manifold. [sent-11, score-0.106]
</p><p>8 However, almost all learning machines such as layered neural networks, gaussian mixtures, and Boltzmann machines have singular Fisher metrics. [sent-12, score-0.329]
</p><p>9 For example, in a three-layer perceptron, the Fisher information matrix J( w) for a parameter w is singular (det J( w) = 0) if and only if w represents a small model which can be realized with the fewer hidden units than the learning model. [sent-13, score-0.178]
</p><p>10 Therefore , when the learning machine is in an almost redundant state, any method in statistics and physics that uses a quadratic approximation of the loss function can not be applied. [sent-14, score-0.141]
</p><p>11 In fact , the maximum likelihood estimator is not subject to the asymptotic normal distribution [4]. [sent-15, score-0.245]
</p><p>12 The Bayesian posterior probability converges to a distribution which is quite different from the normal one [8]. [sent-16, score-0.042]
</p><p>13 To construct a mathematical foundation for such learning machines, we clarified the essential relation between algebraic geometry and Bayesian statistics [9,10]. [sent-17, score-0.415]
</p><p>14 In this  paper, we show that the asymptotic form of the Bayesian stochastic complexity is rigorously obtained by resolution of singularities. [sent-18, score-0.488]
</p><p>15 The Bayesian method gives powerful tools for both generalization and model selection, however, the appropriate prior for each purpose is quite different. [sent-19, score-0.174]
</p><p>16 2  Stochastic Complexity  Let p(xlw) be a learning machine, where x is a pair of an input and an output, and w E Rd is a parameter. [sent-20, score-0.045]
</p><p>17 , Xn) are independently taken from the true distribution q(x), which is not contained in p(x lw) in general. [sent-25, score-0.326]
</p><p>18 The stochastic complexity F(xn) and its average F (n) are defined by  F(xn)  =  - log  J  ITp(Xi lw) 'fJ(w)dw  i=l  and F(n) = Exn{F(xn)}, respectively, where Exn{ . [sent-26, score-0.224]
</p><p>19 The stochastic complexity plays a central role in Bayesian statistics. [sent-28, score-0.151]
</p><p>20 Firstly, F(n+1)-F(n)-S, where S = - J q(x) logq(x)dx, is equal to the average Kullback distance from q(x) to the Bayes predictive distribution p(xlxn), which is called the generalization error denoted by G(n). [sent-29, score-0.162]
</p><p>21 And lastly, if the prior distribution has a hyperparameter (), that is to say, 'fJ(w) = 'fJ(wl()), then it is optimized by minimization of F(xn) [1]. [sent-31, score-0.146]
</p><p>22 We define a function Fo(n) using the Kullback distance H(w),  Fo(n)  =  - log  J  exp( -nH(w))'fJ(w)dw,  H(w)  =  J  q(x) log pf~~2) dx. [sent-32, score-0.196]
</p><p>23 Moreover, we assume that L(x,w) == logq(x) - logp(xlw) is an analytic function from w to the Hilbert space of all square integrable functions with the measure q(x)dx , and that the support of the prior W = supp 'fJ is compact . [sent-34, score-0.423]
</p><p>24 Then H(w) is an analytic function on W, and there exists a constant CI > 0 such that, for an arbitrary n, n  FO("2) -  3  CI ::;  F(n) - Sn ::; Fo(n). [sent-35, score-0.326]
</p><p>25 (1)  General Learning Machines  In this section, we study a case when the true distribution is contained in the parametric model, that is to say, there exists a parameter Wo E W such that q(x) = p(x lwo). [sent-36, score-0.494]
</p><p>26 Let us introduce a zeta function J(z) (z E C) of H(w) and a state density function v(t) by  J(z)  =  J  H(wY'fJ(w)dw,  v(t)  =  J  J(t - H(w))'fJ(w)dw. [sent-37, score-0.073]
</p><p>27 J(z)  =  lh  tZv(t)dt,  Fo(n)  =  - log  lh  exp(-nt)v(t)dt,  where h = maxWEW H(w). [sent-39, score-0.179]
</p><p>28 Moreover, by using the existence of Sato-Bernstein's b-function [6], it can be analytically continued to a meromorphic function on the entire complex plane, whose poles are real, negative, and rational numbers. [sent-42, score-0.095]
</p><p>29 be the poles of J (z) and mk be the order of - Ak. [sent-46, score-0.142]
</p><p>30 Then, by using the inverse Mellin tansform, it follows that v(t) has an asymptotic expansion with coefficients {Ckm}, 00  v(t) ~  mk  LL  Ckm tAk - 1(- logt)m-l  (t  --->  +0). [sent-47, score-0.322]
</p><p>31 k=lm=1 Therefore, also Fo (n) has an asymptotic expansion, by putting A = Al and m = ml,  Fo (n) = A log n - (m - 1) log log n + 0 (1) , which ensures the asymptotic expansion of F(n) by eq. [sent-48, score-0.697]
</p><p>32 (l),  F(n)  =  Sn + Alogn - (m - 1) log log n + 0(1). [sent-49, score-0.146]
</p><p>33 The Kullback distance H(w) depends on the analytic set Wo = {w E W; H(w) = O} , resulting that both A and m depend on Woo Note that, if the Bayes generalization error G(n) = F(n + 1) - F(n) - S has an asymptotic expansion, it should be AI n - (m - 1) I (n log n). [sent-50, score-0.598]
</p><p>34 The following lemma is proven using the definition of Fo(n) and its asymptotic expansion. [sent-51, score-0.3]
</p><p>35 Lemma 1 (1) Let (Ai, mi) (i = 1,2) be constants corresponding to (Hi(W), rpi(W)) (i = 1, 2). [sent-52, score-0.054]
</p><p>36 If H 1(w) :::::: H 2(w) and rpl(W) 2': rp2(W), then 'AI < A2' or 'AI = A2 and ml 2': m2 '. [sent-53, score-0.055]
</p><p>37 (2) Let (Ai , mi) (i = 1, 2) be constants corresponding to (Hi(Wi), rpi(Wi)) (i = 1, 2). [sent-54, score-0.054]
</p><p>38 Then the constants of (H(w) , rp(w)) are A = Al + A2 and m = ml + m2 - 1. [sent-56, score-0.109]
</p><p>39 Let Wi be the open kernel of W (the maximal open set contained in W). [sent-58, score-0.191]
</p><p>40 Theorem 1 (Resolution of Singularities, Hironaka [5}) Let H(w) 2': 0 be a real analytic function on Wi. [sent-59, score-0.202]
</p><p>41 Th en there exist both a real d-dimensional manifold U and a real analytic function g : U ---> Wi such that, in a neighborhood of an arbitrary U E U,  (2) where a( u) > 0 is an analytic function and {sd are non-negative integers. [sent-60, score-0.495]
</p><p>42 M oreover, for arbitrary compact set K c W, g-1 (K) c U is a compact set. [sent-61, score-0.152]
</p><p>43 (2) to the definition of J( z), one can see the integral in J( z) is decomposed into a direct product of the integral of each variable [3]. [sent-65, score-0.088]
</p><p>44 In general it is not so easy to find g(u) that gives the complete resolution of singularities, however , in this paper, we show that even a partial resolution ma pping gives an upper bound of A. [sent-67, score-0.268]
</p><p>45 (1) The prior distribution rp(w) is called positive if rp(w) > 0 for an arbitrary  wE Wi, (W = supp <). [sent-70, score-0.314]
</p><p>46 Ifp(xlw) satisfies the condition of the asymptotic normality, then). [sent-72, score-0.256]
</p><p>47 (Outline of the Proof) (1) In order to examine the poles of J(z), we can divide the parameter space into the sum of neighborhoods. [sent-77, score-0.133]
</p><p>48 Since H( w) is an analytic function, in arbitrary neighborhood of Wo that satisfies H(wo) = 0, we can find a positive definite quadratic form which is smaller than H(w). [sent-78, score-0.562]
</p><p>49 (2) Because Jeffreys' prior is coordinate free, we can study the problem on the parameter space U instead of Wi in eq. [sent-82, score-0.192]
</p><p>50 Hence, there exists an analytic function t(x, u) such that, in each local coordinate,  L(x, u)  =  L(x, g( u))  =  (8t ~Wi UWi  t(x, U)U~l . [sent-84, score-0.282]
</p><p>51 1 (1), in order to prove the latter half of the theorem , it is suthcient to prove that  has a pole z = -d/2 with the order m completes the theorem. [sent-105, score-0.327]
</p><p>52 Direct calculation of integrals in J(z)  Three-Layer Percept ron  In this section, we study some cases when the learner is a three-layer percept ron and the true distribution is contained and not contained. [sent-110, score-0.579]
</p><p>53 We define the three layer  percept ron p(x, vlw) with JII! [sent-111, score-0.174]
</p><p>54 r(x) 1 (27ru 2)N/2 exp(- 2u211v  p(x, vlw)  -  2  fK(x ,w)11 )  K  fK(x,w)  =  Laku(bk路x+Ck) k=l  where w = {(ak' bk, Ck); ak E R N , bk E R M , Ck E Rl}, r(x) is the probability density on the input, and u 2 is the variance of the output (either r(x) or u is not estimated). [sent-113, score-0.173]
</p><p>55 Theorem 3 If the true distribution is represented by the three-layer perceptron with Ko ::; K hidden units, and if positive prior is employed, then 1  A ::; "2 {Ko(M + N  + 1) + (K -  . [sent-114, score-0.355]
</p><p>56 Then da db dc = o:KN-1do: da' db' dc' and there exists an analytic function H1(a' , b' , c') such that H(a , b,c) = 0:2H1(a',b',c'). [sent-125, score-0.642]
</p><p>57 Also by using another blowing-up,  then, da db dc = 0:(M+1)K- 1do: da" db" dc" and there exists an analytic function H2(a l ,bl ,c") such that H(a , b,c) = 0:2H2(a l ,bl ,c"), which shows that J( z) has a pole at z = -K(M + 1)/2. [sent-127, score-0.789]
</p><p>58 By combining both results, we obtain A ::; (K/2) min(M + 1, N) . [sent-128, score-0.041]
</p><p>59 Secondly, we prove the general case, 0 < Ko ::; K. [sent-129, score-0.051]
</p><p>60 If the true regression function g(x) is not contained in the learning model, we assume that, for each 0 ::; k ::; K, there exists a parameter w~k) E W that minimizes the square error  We use notations E(k) k) min(M + 1, N). [sent-136, score-0.49]
</p><p>61 (1/2){k(M + N + 1) + (K -  Theorem 4 If the true regression function is not contained in the learning model and positive prior is applied, then  F(n):,,::: min [n2E (k) +'\(k)lognJ +0(1). [sent-137, score-0.596]
</p><p>62 O~k~K a (Outline of Proof) This theorem can be shown by the same procedure as eq. [sent-138, score-0.078]
</p><p>63 It should be emphasized that the optimal k that minimizes G(n) is smaller than the learning model when n is not so large, and it becomes larger as n increases. [sent-144, score-0.102]
</p><p>64 This fact shows that the positive prior is useful for generalization but not appropriate for model selection. [sent-145, score-0.235]
</p><p>65 Under the condition that the true distribution is contained in the parametric model, Jeffreys' prior may enable us to find the true model with higher probability. [sent-146, score-0.573]
</p><p>66 Theorem 5 If the true regression function is contained in the three-layer perceptron and Jeffrey's prior is applied, then ,\ = d/2 and m = 1, even if the Fisher metric is degenerate at the true parameter. [sent-147, score-0.579]
</p><p>67 (Outline of Proof) For simplicity, we prove the theorem for the case g(x) = O. [sent-148, score-0.129]
</p><p>68 The general cases can be proven by the same method. [sent-149, score-0.043]
</p><p>69 By direct calculation of the Fisher information matrix, there exists an analytic function D(b, e) ~ 0 such that K  N  detI(w) = II(Lakp)2(M+1)D(b,e) k=1 p=1 By using a blowing-up  we obtain H(w) = a 2H 1(a',b',e') same as eq. [sent-150, score-0.282]
</p><p>70 (5), detI(w) ex a 2(M+1)K, and da db de = aN K -1 da da' db de. [sent-151, score-0.508]
</p><p>71 The integral  }(z)  =  1  a 2z a(M+1)K+NK- 1da  1"'1芦'  has a pole at z = -(M + N + 1)K/2. [sent-152, score-0.191]
</p><p>72 By combining this result with Theorem 3, we obtain Theorem. [sent-153, score-0.041]
</p><p>73 5  Discussion  In many applications of neural networks, rather complex machines a re employed compared with the number of training samples. [sent-159, score-0.171]
</p><p>74 In such cases, the set of optimal parameters is not one point but an analytic set with singularities, and the set of almost optimal parameters {Wi H(w ) < E} is not an 'ellipsoid'. [sent-160, score-0.202]
</p><p>75 Hence neither the Kullback distance can be approximat ed by any quadratic form nor the saddle point approximation can be used in integration on the parameter space. [sent-161, score-0.134]
</p><p>76 The zeta function of the Kullback distance clarifies the behavior of the stochastic complexity and resolution of singularities enables us to calculate the learning efficiency. [sent-162, score-0.71]
</p><p>77 6  Conclusion  The relation between algebraic geometry and learning theory is clarified, and two different facts are proven. [sent-163, score-0.255]
</p><p>78 (1) If the true distribution is not contained in a hierarchical learning model, then by using a positive prior, the generalization error is made smaller than the regular statistical models. [sent-164, score-0.613]
</p><p>79 (2) If the true distribution is contained in the learning model and if Jeffreys' prior is used , then the average Bayesian factor has the same form as BIC. [sent-165, score-0.475]
</p><p>80 (1998) On the generalization error by a layered statistical model with Bayesian estimation. [sent-203, score-0.184]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fo', 0.259), ('singularities', 0.257), ('asymptotic', 0.203), ('analytic', 0.202), ('contained', 0.191), ('jeffreys', 0.183), ('kullback', 0.158), ('wi', 0.151), ('da', 0.15), ('pole', 0.147), ('watanabe', 0.147), ('algebraic', 0.141), ('resolution', 0.134), ('fisher', 0.128), ('fq', 0.126), ('layered', 0.114), ('ck', 0.114), ('ko', 0.114), ('clarified', 0.11), ('deti', 0.11), ('rpi', 0.11), ('xlw', 0.11), ('bk', 0.106), ('dc', 0.106), ('prior', 0.104), ('db', 0.104), ('percept', 0.095), ('poles', 0.095), ('rp', 0.094), ('true', 0.093), ('xn', 0.09), ('outline', 0.082), ('wo', 0.082), ('stochastic', 0.081), ('exists', 0.08), ('dw', 0.079), ('ron', 0.079), ('theorem', 0.078), ('sn', 0.075), ('ckm', 0.073), ('exn', 0.073), ('hironaka', 0.073), ('lw', 0.073), ('mellin', 0.073), ('uwi', 0.073), ('vlw', 0.073), ('zeta', 0.073), ('log', 0.073), ('expansion', 0.072), ('generalization', 0.07), ('complexity', 0.07), ('geometry', 0.069), ('bayesian', 0.067), ('ak', 0.067), ('machines', 0.065), ('re', 0.063), ('logq', 0.063), ('supp', 0.063), ('positive', 0.061), ('min', 0.059), ('normality', 0.057), ('firstly', 0.057), ('wl', 0.057), ('smaller', 0.057), ('perceptron', 0.055), ('ml', 0.055), ('units', 0.055), ('compact', 0.054), ('constants', 0.054), ('lemma', 0.054), ('regular', 0.054), ('satisfies', 0.053), ('fk', 0.053), ('lh', 0.053), ('sp', 0.053), ('definite', 0.052), ('prove', 0.051), ('coordinate', 0.05), ('parametric', 0.05), ('proof', 0.05), ('distance', 0.05), ('let', 0.05), ('statistics', 0.05), ('hi', 0.049), ('mk', 0.047), ('sd', 0.047), ('japan', 0.047), ('neighborhood', 0.047), ('quadratic', 0.046), ('learning', 0.045), ('secondly', 0.045), ('integral', 0.044), ('arbitrary', 0.044), ('regression', 0.043), ('employed', 0.043), ('proven', 0.043), ('distribution', 0.042), ('combining', 0.041), ('singular', 0.04), ('parameter', 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="20-tfidf-1" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>Author: Sumio Watanabe</p><p>Abstract: Algebraic geometry is essential to learning theory. In hierarchical learning machines such as layered neural networks and gaussian mixtures, the asymptotic normality does not hold , since Fisher information matrices are singular. In this paper , the rigorous asymptotic form of the stochastic complexity is clarified based on resolution of singularities and two different problems are studied. (1) If the prior is positive, then the stochastic complexity is far smaller than BIO, resulting in the smaller generalization error than regular statistical models, even when the true distribution is not contained in the parametric model. (2) If Jeffreys' prior, which is coordinate free and equal to zero at singularities, is employed then the stochastic complexity has the same form as BIO. It is useful for model selection, but not for generalization. 1</p><p>2 0.11135891 <a title="20-tfidf-2" href="./nips-2000-From_Margin_to_Sparsity.html">58 nips-2000-From Margin to Sparsity</a></p>
<p>Author: Thore Graepel, Ralf Herbrich, Robert C. Williamson</p><p>Abstract: We present an improvement of Novikoff's perceptron convergence theorem. Reinterpreting this mistake bound as a margin dependent sparsity guarantee allows us to give a PAC-style generalisation error bound for the classifier learned by the perceptron learning algorithm. The bound value crucially depends on the margin a support vector machine would achieve on the same data set using the same kernel. Ironically, the bound yields better guarantees than are currently available for the support vector solution itself. 1</p><p>3 0.09403237 <a title="20-tfidf-3" href="./nips-2000-Learning_Continuous_Distributions%3A_Simulations_With_Field_Theoretic_Priors.html">76 nips-2000-Learning Continuous Distributions: Simulations With Field Theoretic Priors</a></p>
<p>Author: Ilya Nemenman, William Bialek</p><p>Abstract: Learning of a smooth but nonparametric probability density can be regularized using methods of Quantum Field Theory. We implement a field theoretic prior numerically, test its efficacy, and show that the free parameter of the theory (,smoothness scale') can be determined self consistently by the data; this forms an infinite dimensional generalization of the MDL principle. Finally, we study the implications of one's choice of the prior and the parameterization and conclude that the smoothness scale determination makes density estimation very weakly sensitive to the choice of the prior, and that even wrong choices can be advantageous for small data sets. One of the central problems in learning is to balance 'goodness of fit' criteria against the complexity of models. An important development in the Bayesian approach was thus the realization that there does not need to be any extra penalty for model complexity: if we compute the total probability that data are generated by a model, there is a factor from the volume in parameter space-the 'Occam factor' -that discriminates against models with more parameters [1, 2]. This works remarkably welJ for systems with a finite number of parameters and creates a complexity 'razor' (after 'Occam's razor') that is almost equivalent to the celebrated Minimal Description Length (MDL) principle [3]. In addition, if the a priori distributions involved are strictly Gaussian, the ideas have also been proven to apply to some infinite-dimensional (nonparametric) problems [4]. It is not clear, however, what happens if we leave the finite dimensional setting to consider nonparametric problems which are not Gaussian, such as the estimation of a smooth probability density. A possible route to progress on the nonparametric problem was opened by noticing [5] that a Bayesian prior for density estimation is equivalent to a quantum field theory (QFT). In particular, there are field theoretic methods for computing the infinite dimensional analog of the Occam factor, at least asymptotically for large numbers of examples. These observations have led to a number of papers [6, 7, 8, 9] exploring alternative formulations and their implications for the speed of learning. Here we return to the original formulation of Ref. [5] and use numerical methods to address some of the questions left open by the analytic work [10]: What is the result of balancing the infinite dimensional Occam factor against the goodness of fit? Is the QFT inference optimal in using alJ of the information relevant for learning [II]? What happens if our learning problem is strongly atypical of the prior distribution? Following Ref. [5], if N i. i. d. samples {Xi}, i = 1 ... N, are observed, then the probability that a particular density Q(x) gave rise to these data is given by P[Q(x)l{x.}] P[Q(x)] rr~1 Q(Xi) • - J[dQ(x)]P[Q(x)] rr~1 Q(Xi) , (1) where P[Q(x)] encodes our a priori expectations of Q. Specifying this prior on a space of functions defines a QFf, and the optimal least square estimator is then Q (I{ .}) - (Q(X)Q(Xl)Q(X2) ... Q(XN)}(O) est X X. (Q(Xl)Q(X2) ... Q(XN ))(0) , (2) where ( ... )(0) means averaging with respect to the prior. Since Q(x) ~ 0, it is convenient to define an unconstrained field ¢(x), Q(x) (l/io)exp[-¢(x)]. Other definitions are also possible [6], but we think that most of our results do not depend on this choice. = The next step is to select a prior that regularizes the infinite number of degrees of freedom and allows learning. We want the prior P[¢] to make sense as a continuous theory, independent of discretization of x on small scales. We also require that when we estimate the distribution Q(x) the answer must be everywhere finite. These conditions imply that our field theory must be convergent at small length scales. For x in one dimension, a minimal choice is P[¢(x)] 1 = Z exp [£2 11 - 1 --2- f (8 dx [1 f 11 ¢)2] c5 io 8xll ] dxe-¢(x) -1 , (3) where'T/ > 1/2, Z is the normalization constant, and the c5-function enforces normalization of Q. We refer to i and 'T/ as the smoothness scale and the exponent, respectively. In [5] this theory was solved for large Nand 'T/ = 1: N (II Q(Xi))(O) ~ (4) = (5) + (6) i=1 Seff i8;¢c1 (x) where ¢cl is the 'classical' (maximum likelihood, saddle point) solution. In the effective action [Eq. (5)], it is the square root term that arises from integrating over fluctuations around the classical solution (Occam factors). It was shown that Eq. (4) is nonsingular even at finite N, that the mean value of ¢c1 converges to the negative logarithm of the target distribution P(x) very quickly, and that the variance of fluctuations 'Ij;(x) ¢(x) [- log ioP( x)] falls off as ....., 1/ iN P( x). Finally, it was speculated that if the actual i is unknown one may average over it and hope that, much as in Bayesian model selection [2], the competition between the data and the fluctuations will select the optimal smoothness scale i*. J = At the first glance the theory seems to look almost exactly like a Gaussian Process [4]. This impression is produced by a Gaussian form of the smoothness penalty in Eq. (3), and by the fluctuation determinant that plays against the goodness of fit in the smoothness scale (model) selection. However, both similarities are incomplete. The Gaussian penalty in the prior is amended by the normalization constraint, which gives rise to the exponential term in Eq. (6), and violates many familiar results that hold for Gaussian Processes, the representer theorem [12] being just one of them. In the semi--classical limit of large N, Gaussianity is restored approximately, but the classical solution is extremely non-trivial, and the fluctuation determinant is only the leading term of the Occam's razor, not the complete razor as it is for a Gaussian Process. In addition, it has no data dependence and is thus remarkably different from the usual determinants arising in the literature. The algorithm to implement the discussed density estimation procedure numerically is rather simple. First, to make the problem well posed [10, 11] we confine x to a box a ~ x ~ L with periodic boundary conditions. The boundary value problem Eq. (6) is then solved by a standard 'relaxation' (or Newton) method of iterative improvements to a guessed solution [13] (the target precision is always 10- 5 ). The independent variable x E [0,1] is discretized in equal steps [10 4 for Figs. (l.a-2.b), and 105 for Figs. (3.a, 3.b)]. We use an equally spaced grid to ensure stability of the method, while small step sizes are needed since the scale for variation of ¢el (x) is [5] (7) c5x '</p><p>4 0.090057373 <a title="20-tfidf-4" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>Author: Thomas P. Minka</p><p>Abstract: A central issue in principal component analysis (PCA) is choosing the number of principal components to be retained. By interpreting PCA as density estimation, we show how to use Bayesian model selection to estimate the true dimensionality of the data. The resulting estimate is simple to compute yet guaranteed to pick the correct dimensionality, given enough data. The estimate involves an integral over the Steifel manifold of k-frames, which is difficult to compute exactly. But after choosing an appropriate parameterization and applying Laplace's method, an accurate and practical estimator is obtained. In simulations, it is convincingly better than cross-validation and other proposed algorithms, plus it runs much faster.</p><p>5 0.089443959 <a title="20-tfidf-5" href="./nips-2000-Sparse_Kernel_Principal_Component_Analysis.html">121 nips-2000-Sparse Kernel Principal Component Analysis</a></p>
<p>Author: Michael E. Tipping</p><p>Abstract: 'Kernel' principal component analysis (PCA) is an elegant nonlinear generalisation of the popular linear data analysis method, where a kernel function implicitly defines a nonlinear transformation into a feature space wherein standard PCA is performed. Unfortunately, the technique is not 'sparse', since the components thus obtained are expressed in terms of kernels associated with every training vector. This paper shows that by approximating the covariance matrix in feature space by a reduced number of example vectors, using a maximum-likelihood approach, we may obtain a highly sparse form of kernel PCA without loss of effectiveness. 1</p><p>6 0.088556491 <a title="20-tfidf-6" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>7 0.086332068 <a title="20-tfidf-7" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>8 0.086036056 <a title="20-tfidf-8" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>9 0.081909336 <a title="20-tfidf-9" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>10 0.073598444 <a title="20-tfidf-10" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>11 0.0734929 <a title="20-tfidf-11" href="./nips-2000-Computing_with_Finite_and_Infinite_Networks.html">35 nips-2000-Computing with Finite and Infinite Networks</a></p>
<p>12 0.071706131 <a title="20-tfidf-12" href="./nips-2000-Regularization_with_Dot-Product_Kernels.html">110 nips-2000-Regularization with Dot-Product Kernels</a></p>
<p>13 0.070155039 <a title="20-tfidf-13" href="./nips-2000-On_Reversing_Jensen%27s_Inequality.html">94 nips-2000-On Reversing Jensen's Inequality</a></p>
<p>14 0.067625515 <a title="20-tfidf-14" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>15 0.06355647 <a title="20-tfidf-15" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>16 0.063227087 <a title="20-tfidf-16" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>17 0.061893173 <a title="20-tfidf-17" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>18 0.060956992 <a title="20-tfidf-18" href="./nips-2000-Periodic_Component_Analysis%3A_An_Eigenvalue_Method_for_Representing_Periodic_Structure_in_Speech.html">99 nips-2000-Periodic Component Analysis: An Eigenvalue Method for Representing Periodic Structure in Speech</a></p>
<p>19 0.060338527 <a title="20-tfidf-19" href="./nips-2000-A_Mathematical_Programming_Approach_to_the_Kernel_Fisher_Algorithm.html">5 nips-2000-A Mathematical Programming Approach to the Kernel Fisher Algorithm</a></p>
<p>20 0.060072295 <a title="20-tfidf-20" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.223), (1, 0.075), (2, -0.008), (3, -0.026), (4, 0.094), (5, 0.022), (6, -0.039), (7, -0.004), (8, -0.035), (9, -0.149), (10, -0.049), (11, -0.002), (12, 0.001), (13, -0.052), (14, 0.078), (15, 0.046), (16, 0.078), (17, -0.073), (18, -0.007), (19, 0.013), (20, 0.024), (21, 0.082), (22, -0.113), (23, 0.019), (24, -0.028), (25, 0.092), (26, 0.113), (27, 0.003), (28, -0.025), (29, 0.013), (30, 0.014), (31, -0.076), (32, 0.064), (33, -0.05), (34, -0.113), (35, -0.043), (36, 0.084), (37, -0.011), (38, 0.192), (39, -0.09), (40, 0.026), (41, 0.005), (42, 0.02), (43, 0.06), (44, 0.08), (45, 0.106), (46, 0.283), (47, -0.042), (48, -0.04), (49, 0.258)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94138682 <a title="20-lsi-1" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>Author: Sumio Watanabe</p><p>Abstract: Algebraic geometry is essential to learning theory. In hierarchical learning machines such as layered neural networks and gaussian mixtures, the asymptotic normality does not hold , since Fisher information matrices are singular. In this paper , the rigorous asymptotic form of the stochastic complexity is clarified based on resolution of singularities and two different problems are studied. (1) If the prior is positive, then the stochastic complexity is far smaller than BIO, resulting in the smaller generalization error than regular statistical models, even when the true distribution is not contained in the parametric model. (2) If Jeffreys' prior, which is coordinate free and equal to zero at singularities, is employed then the stochastic complexity has the same form as BIO. It is useful for model selection, but not for generalization. 1</p><p>2 0.46740928 <a title="20-lsi-2" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>Author: Igor V. Cadez, Padhraic Smyth</p><p>Abstract: We investigate a general characteristic of the trade-off in learning problems between goodness-of-fit and model complexity. Specifically we characterize a general class of learning problems where the goodness-of-fit function can be shown to be convex within firstorder as a function of model complexity. This general property of</p><p>3 0.43720227 <a title="20-lsi-3" href="./nips-2000-The_Use_of_MDL_to_Select_among_Computational_Models_of_Cognition.html">139 nips-2000-The Use of MDL to Select among Computational Models of Cognition</a></p>
<p>Author: In Jae Myung, Mark A. Pitt, Shaobo Zhang, Vijay Balasubramanian</p><p>Abstract: How should we decide among competing explanations of a cognitive process given limited observations? The problem of model selection is at the heart of progress in cognitive science. In this paper, Minimum Description Length (MDL) is introduced as a method for selecting among computational models of cognition. We also show that differential geometry provides an intuitive understanding of what drives model selection in MDL. Finally, adequacy of MDL is demonstrated in two areas of cognitive modeling. 1 Model Selection and Model Complexity The development and testing of computational models of cognitive processing are a central focus in cognitive science. A model embodies a solution to a problem whose adequacy is evaluated by its ability to mimic behavior by capturing the regularities underlying observed data. This enterprise of model selection is challenging because of the competing goals that must be satisfied. Traditionally, computational models of cognition have been compared using one of many goodness-of-fit measures. However, use of such a measure can result in the choice of a model that over-fits the data, one that captures idiosyncracies in the particular data set (i.e., noise) over and above the underlying regularities of interest. Such models are considered complex, in that the inherent flexibility in the model enables it to fit diverse patterns of data. As a group, they can be characterized as having many parameters that are combined in a highly nonlinear fashion in the model equation. They do not assume a single structure in the data. Rather, the model contains multiple structures; each obtained by finely tuning the parameter values of the model, and thus can fit a wide range of data patterns. In contrast, simple models, frequently with few parameters, assume a specific structure in the data, which will manifest itself as a narrow range of similar data patterns. Only when one of these patterns occurs will the model fit the data well. The problem of over-fitting data due to model complexity suggests that the goal of model selection should instead be to select the model that generalizes best to all data samples that arise from the same underlying regularity, thus capturing only the regularity, not the noise. To achieve this goal, the selection method must be sensitive to the complexity of a model. There are at least two independent dimensions of model complexity. They are the number of free parameters of a model and its functional form, which refers to the way the parameters are combined in the model equation. For instance, it seems unlikely that two one-parameter models, y = ex and y = x 9, are equally complex in their ability to fit data. The two dimensions of model complexity (number of parameters and functional form) and their interplay can improve a model's fit to the data, without necessarily improving generalizability. The trademark of a good model selection procedure, then, is its ability to satisfy two opposing goals. A model must be sufficiently complex to describe the data sample accurately, but without over-fitting the data and thus losing generalizability. To achieve this end, we need a theoretically well-justified measure of model complexity that takes into account the number of parameters and the functional form of a model. In this paper, we introduce Minimum Description Length (MDL) as an appropriate method of selecting among mathematical models of cognition. We also show that MDL has an elegant geometric interpretation that provides a clear, intuitive understanding of the meaning of complexity in MDL. Finally, application examples of MDL are presented in two areas of cognitive modeling. 1.1 Minimum Description Length The central thesis of model selection is the estimation of a model's generalizability. One approach to assessing generalizability is the Minimum Description Length (MDL) principle [1]. It provides a theoretically well-grounded measure of complexity that is sensitive to both dimensions of complexity and also lends itself to intuitive, geometric interpretations. MDL was developed within algorithmic coding theory to choose the model that permits the greatest compression of data. A model family f with parameters e assigns the likelihood f(yle) to a given set of observed data y . The full form of the MDL measure for such a model family is given below. MDL = -In! (yISA) + ~ln( ; )+ In f dS.jdetl(S) where SA is the parameter that maximizes the likelihood, k is the number of parameters in the model, N is the sample size and I(e) is the Fisher information matrix. MDL is the length in bits of the shortest possible code that describes the data with the help of a model. In the context of cognitive modeling, the model that minimizes MDL uncovers the greatest amount of regularity (i.e., knowledge) underlying the data and therefore should be selected. The first, maximized log likelihood term is the lack-of-fit measure, and the second and third terms constitute the intrinsic complexity of the model. In particular, the third term captures the effects of complexity due to functional form, reflected through I(e). We will call the latter two terms together the geometric complexity of the model, for reasons that will become clear in the remainder of this paper. MDL arises as a finite series of terms in an asymptotic expansion of the Bayesian posterior probability of a model given the data for a special form of the parameter prior density [2] . Hence in essence, minimization of MDL is equivalent to maximization of the Bayesian posterior probability. In this paper we present a geometric interpretation of MDL, as well as Bayesian model selection [3], that provides an elegant and intuitive framework for understanding model complexity, a central concept in model selection. 2 Differential Geometric Interpretation of MDL From a geometric perspective, a parametric model family of probability distributions forms a Riemannian manifold embedded in the space of all probability distributions [4]. Every distribution is a point in this space, and the collection of points created by varying the parameters of the model gives rise to a hyper-surface in which</p><p>4 0.416861 <a title="20-lsi-4" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>Author: Thomas P. Minka</p><p>Abstract: A central issue in principal component analysis (PCA) is choosing the number of principal components to be retained. By interpreting PCA as density estimation, we show how to use Bayesian model selection to estimate the true dimensionality of the data. The resulting estimate is simple to compute yet guaranteed to pick the correct dimensionality, given enough data. The estimate involves an integral over the Steifel manifold of k-frames, which is difficult to compute exactly. But after choosing an appropriate parameterization and applying Laplace's method, an accurate and practical estimator is obtained. In simulations, it is convincingly better than cross-validation and other proposed algorithms, plus it runs much faster.</p><p>5 0.41578895 <a title="20-lsi-5" href="./nips-2000-Sex_with_Support_Vector_Machines.html">116 nips-2000-Sex with Support Vector Machines</a></p>
<p>Author: Baback Moghaddam, Ming-Hsuan Yang</p><p>Abstract: unkown-abstract</p><p>6 0.39787415 <a title="20-lsi-6" href="./nips-2000-A_Mathematical_Programming_Approach_to_the_Kernel_Fisher_Algorithm.html">5 nips-2000-A Mathematical Programming Approach to the Kernel Fisher Algorithm</a></p>
<p>7 0.37030315 <a title="20-lsi-7" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>8 0.35865515 <a title="20-lsi-8" href="./nips-2000-Algorithms_for_Non-negative_Matrix_Factorization.html">22 nips-2000-Algorithms for Non-negative Matrix Factorization</a></p>
<p>9 0.33964443 <a title="20-lsi-9" href="./nips-2000-Beyond_Maximum_Likelihood_and_Density_Estimation%3A_A_Sample-Based_Criterion_for_Unsupervised_Learning_of_Complex_Models.html">31 nips-2000-Beyond Maximum Likelihood and Density Estimation: A Sample-Based Criterion for Unsupervised Learning of Complex Models</a></p>
<p>10 0.32303262 <a title="20-lsi-10" href="./nips-2000-Computing_with_Finite_and_Infinite_Networks.html">35 nips-2000-Computing with Finite and Infinite Networks</a></p>
<p>11 0.32060552 <a title="20-lsi-11" href="./nips-2000-On_Reversing_Jensen%27s_Inequality.html">94 nips-2000-On Reversing Jensen's Inequality</a></p>
<p>12 0.31986541 <a title="20-lsi-12" href="./nips-2000-Learning_Continuous_Distributions%3A_Simulations_With_Field_Theoretic_Priors.html">76 nips-2000-Learning Continuous Distributions: Simulations With Field Theoretic Priors</a></p>
<p>13 0.31494802 <a title="20-lsi-13" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>14 0.314257 <a title="20-lsi-14" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>15 0.31328249 <a title="20-lsi-15" href="./nips-2000-Regularization_with_Dot-Product_Kernels.html">110 nips-2000-Regularization with Dot-Product Kernels</a></p>
<p>16 0.30931193 <a title="20-lsi-16" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>17 0.29783252 <a title="20-lsi-17" href="./nips-2000-From_Margin_to_Sparsity.html">58 nips-2000-From Margin to Sparsity</a></p>
<p>18 0.2936455 <a title="20-lsi-18" href="./nips-2000-Mixtures_of_Gaussian_Processes.html">85 nips-2000-Mixtures of Gaussian Processes</a></p>
<p>19 0.28734246 <a title="20-lsi-19" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>20 0.28707951 <a title="20-lsi-20" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.353), (10, 0.024), (17, 0.112), (32, 0.014), (33, 0.071), (55, 0.014), (62, 0.05), (65, 0.015), (67, 0.101), (76, 0.076), (79, 0.021), (90, 0.037), (91, 0.021), (97, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80704093 <a title="20-lda-1" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>Author: Sumio Watanabe</p><p>Abstract: Algebraic geometry is essential to learning theory. In hierarchical learning machines such as layered neural networks and gaussian mixtures, the asymptotic normality does not hold , since Fisher information matrices are singular. In this paper , the rigorous asymptotic form of the stochastic complexity is clarified based on resolution of singularities and two different problems are studied. (1) If the prior is positive, then the stochastic complexity is far smaller than BIO, resulting in the smaller generalization error than regular statistical models, even when the true distribution is not contained in the parametric model. (2) If Jeffreys' prior, which is coordinate free and equal to zero at singularities, is employed then the stochastic complexity has the same form as BIO. It is useful for model selection, but not for generalization. 1</p><p>2 0.45682201 <a title="20-lda-2" href="./nips-2000-The_Kernel_Trick_for_Distances.html">134 nips-2000-The Kernel Trick for Distances</a></p>
<p>Author: Bernhard Schölkopf</p><p>Abstract: A method is described which, like the kernel trick in support vector machines (SVMs), lets us generalize distance-based algorithms to operate in feature spaces, usually nonlinearly related to the input space. This is done by identifying a class of kernels which can be represented as norm-based distances in Hilbert spaces. It turns out that common kernel algorithms, such as SVMs and kernel PCA, are actually really distance based algorithms and can be run with that class of kernels, too. As well as providing a useful new insight into how these algorithms work, the present work can form the basis for conceiving new algorithms.</p><p>3 0.44658199 <a title="20-lda-3" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>Author: Tong Zhang</p><p>Abstract: Large margin linear classification methods have been successfully applied to many applications. For a linearly separable problem, it is known that under appropriate assumptions, the expected misclassification error of the computed</p><p>4 0.4391703 <a title="20-lda-4" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>Author: Martin Szummer, Tommi Jaakkola</p><p>Abstract: Modern classification applications necessitate supplementing the few available labeled examples with unlabeled examples to improve classification performance. We present a new tractable algorithm for exploiting unlabeled examples in discriminative classification. This is achieved essentially by expanding the input vectors into longer feature vectors via both labeled and unlabeled examples. The resulting classification method can be interpreted as a discriminative kernel density estimate and is readily trained via the EM algorithm, which in this case is both discriminative and achieves the optimal solution. We provide, in addition, a purely discriminative formulation of the estimation problem by appealing to the maximum entropy framework. We demonstrate that the proposed approach requires very few labeled examples for high classification accuracy.</p><p>5 0.4384239 <a title="20-lda-5" href="./nips-2000-Algorithmic_Stability_and_Generalization_Performance.html">21 nips-2000-Algorithmic Stability and Generalization Performance</a></p>
<p>Author: Olivier Bousquet, Andr茅 Elisseeff</p><p>Abstract: We present a novel way of obtaining PAC-style bounds on the generalization error of learning algorithms, explicitly using their stability properties. A stable learner is one for which the learned solution does not change much with small changes in the training set. The bounds we obtain do not depend on any measure of the complexity of the hypothesis space (e.g. VC dimension) but rather depend on how the learning algorithm searches this space, and can thus be applied even when the VC dimension is infinite. We demonstrate that regularization networks possess the required stability property and apply our method to obtain new bounds on their generalization performance. 1</p><p>6 0.43603167 <a title="20-lda-6" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>7 0.43482044 <a title="20-lda-7" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>8 0.43237242 <a title="20-lda-8" href="./nips-2000-Regularized_Winnow_Methods.html">111 nips-2000-Regularized Winnow Methods</a></p>
<p>9 0.43086645 <a title="20-lda-9" href="./nips-2000-On_Reversing_Jensen%27s_Inequality.html">94 nips-2000-On Reversing Jensen's Inequality</a></p>
<p>10 0.43069309 <a title="20-lda-10" href="./nips-2000-Algorithms_for_Non-negative_Matrix_Factorization.html">22 nips-2000-Algorithms for Non-negative Matrix Factorization</a></p>
<p>11 0.43012324 <a title="20-lda-11" href="./nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">95 nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>12 0.42811078 <a title="20-lda-12" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>13 0.42777658 <a title="20-lda-13" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>14 0.42735633 <a title="20-lda-14" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>15 0.42673987 <a title="20-lda-15" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>16 0.42665774 <a title="20-lda-16" href="./nips-2000-Some_New_Bounds_on_the_Generalization_Error_of_Combined_Classifiers.html">119 nips-2000-Some New Bounds on the Generalization Error of Combined Classifiers</a></p>
<p>17 0.42499685 <a title="20-lda-17" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<p>18 0.42419684 <a title="20-lda-18" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>19 0.4236334 <a title="20-lda-19" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>20 0.42344001 <a title="20-lda-20" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
