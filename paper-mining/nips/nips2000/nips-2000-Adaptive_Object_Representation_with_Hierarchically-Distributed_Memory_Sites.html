<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-19" href="#">nips2000-19</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</h1>
<br/><p>Source: <a title="nips-2000-19-pdf" href="http://papers.nips.cc/paper/1889-adaptive-object-representation-with-hierarchically-distributed-memory-sites.pdf">pdf</a></p><p>Author: Bosco S. Tjan</p><p>Abstract: Theories of object recognition often assume that only one representation scheme is used within one visual-processing pathway. Versatility of the visual system comes from having multiple visual-processing pathways, each specialized in a different category of objects. We propose a theoretically simpler alternative, capable of explaining the same set of data and more. A single primary visual-processing pathway, loosely modular, is assumed. Memory modules are attached to sites along this pathway. Object-identity decision is made independently at each site. A site's response time is a monotonic-decreasing function of its confidence regarding its decision. An observer's response is the first-arriving response from any site. The effective representation(s) of such a system, determined empirically, can appear to be specialized for different tasks and stimuli, consistent with recent clinical and functional-imaging findings. This, however, merely reflects a decision being made at its appropriate level of abstraction. The system itself is intrinsically flexible and adaptive.</p><p>Reference: <a title="nips-2000-19-reference" href="../nips2000_reference/nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Theories of object recognition often assume that only one representation scheme is used within one visual-processing pathway. [sent-3, score-0.357]
</p><p>2 Versatility of the visual system comes from having multiple visual-processing pathways, each specialized in a different category of objects. [sent-4, score-0.272]
</p><p>3 We propose a theoretically simpler alternative, capable of explaining the same set of data and more. [sent-5, score-0.038]
</p><p>4 A single primary visual-processing pathway, loosely modular, is assumed. [sent-6, score-0.06]
</p><p>5 Memory modules are attached to sites along this pathway. [sent-7, score-0.202]
</p><p>6 A site's response time is a monotonic-decreasing function of its confidence regarding its decision. [sent-9, score-0.18]
</p><p>7 An observer's response is the first-arriving response from any site. [sent-10, score-0.19]
</p><p>8 The effective representation(s) of such a system, determined empirically, can appear to be specialized for different tasks and stimuli, consistent with recent clinical and functional-imaging findings. [sent-11, score-0.118]
</p><p>9 This, however, merely reflects a decision being made at its appropriate level of abstraction. [sent-12, score-0.124]
</p><p>10 1 Introduction How does the visual system represent its knowledge about objects so as to identify them? [sent-14, score-0.315]
</p><p>11 A largely unquestioned assumption in the study of object recognition has been that the visual system builds up a representation for an object by sequentially transforming an input image into progressively more abstract representations. [sent-15, score-0.785]
</p><p>12 The final representation is taken to be the representation of an object and is entered into memory. [sent-16, score-0.447]
</p><p>13 Recognition of an object occurs when the representation of the object currently in view matches an item in memory. [sent-17, score-0.575]
</p><p>14 Highly influential proposals for a common representation of objects [1, 2] have failed to show promise of either producing a working artificial system or explaining a gamut of behavioral data. [sent-18, score-0.42]
</p><p>15 This insistence of having a common representation for all objects is also a major cause of the debate on whether the perceptual representation of objects is 2-D appearance-based or 3-D structure-based [3,4]. [sent-19, score-0.524]
</p><p>16 Recently, a convergence of data [5-9], including those from the viewpoint debate itself [10, 11], have been used to suggest that the brain may use multiple  mechanisms or processing pathways to recognize a multitude of objects. [sent-20, score-0.338]
</p><p>17 While insisting on a common representation for all objects seems too restrictive in light of the varying complexity across objects [12], asserting a new pathway for every idiosyncratic data clusters seems unnecessary. [sent-21, score-0.427]
</p><p>18 We propose a parsimonious alternative, which is consistent with existing data but explains them with novel insights. [sent-22, score-0.143]
</p><p>19 Flexibility and self-adaptivity are achieved by having multiple memory and decision sites distributed along the pathway. [sent-24, score-0.554]
</p><p>20 2  Theory and Methods  If the visual system needs to construct an abstract representation of objects for a  certain task (e. [sent-25, score-0.439]
</p><p>21 object categorization), it will have to do so via multiple stages. [sent-27, score-0.167]
</p><p>22 The intermediate result at each stage is itself a representation. [sent-28, score-0.074]
</p><p>23 The entire processing pathway thus provides a hierarchy of representations, ranging from the most imagespecific at the earliest stage to the most abstract at the latest stage. [sent-29, score-0.164]
</p><p>24 The central idea of our proposal is that the visual system can tap this hierarchical collection of representations by attaching memory modules along the processing pathway. [sent-30, score-0.569]
</p><p>25 We further speculate that each memory site makes independent decisions about the identity of an incoming image. [sent-31, score-0.818]
</p><p>26 Each announces its decision after a delay, determined by an amount related to the site's confidence about its own decision and the amount of memory it needs to consult before reaching the decision. [sent-32, score-0.703]
</p><p>27 The homunculus does nothing but takes the first-arriving response as the system's response. [sent-33, score-0.165]
</p><p>28 Figure la depicts this framework, which we shall call the Hierarchically Distributed Decision Theory for object recognition. [sent-34, score-0.167]
</p><p>29 ~  Yy  +  the first-arriving respon se  (a)  (b)  Figure 1: An illustration of the Hierarchically Distributed Decision Theory of object recognition (a) and its implementation in a toy visual system (b). [sent-39, score-0.638]
</p><p>30 1  A toy visual system  We constructed a toy visual system to illustrate various properties of the Hierarchically Distributed Decision Theory . [sent-41, score-0.81]
</p><p>31 The task for this toy system is to identify letters presented at arbitrary position and orientation and corrupted by Gaussian luminance noise. [sent-42, score-0.74]
</p><p>32 This system is not meant to be a model of human vision, but rather a demonstration of the theory. [sent-43, score-0.131]
</p><p>33 Given a stimulus (letter+noise), the position of the target letter is first estimated and centered in the image (position normalization) by computing the centroid of the stimulus' luminance profile. [sent-44, score-0.627]
</p><p>34 Once centered, the principal axis of the luminance profile is determined and the entire image is rotated so that this axis is vertical (orientation normalization). [sent-45, score-0.351]
</p><p>35 The representation at this final stage is both position- and orientation-invariant. [sent-46, score-0.199]
</p><p>36 Traditionally, one would commit only this final representation to memory. [sent-47, score-0.156]
</p><p>37 In contrast, the Hierarchically Distributed Decision Theory stated that the intermediate results are also committed to some form of sensory memory (Figure  Ib). [sent-48, score-0.285]
</p><p>38 For this toy system, a feature vector is a sub-sampled image at the output of each stage. [sent-50, score-0.224]
</p><p>39 I' independently decides the letter's identity L" based on the immediate representation Is available to the site. [sent-52, score-0.193]
</p><p>40 Specifically, (1)  L, = arg max Pr(r II, ) re Letters  where Letters is the set of letter identities. [sent-55, score-0.222]
</p><p>41 A letter identity r is in turn a set of letter images Vat a given luminance, which may be shifted or rotated. [sent-56, score-0.461]
</p><p>42 So we have, Pr(rl/,) =~pr(V II,) =~pr(l, I V) Pr(V) /pr(l, ) VI12 = Eexp(-II/, - 2 ]pr(V)/ 2s  Ve r  E  (2)  Eexp(-III, - 2VI12 ]pr(V)  reLetters VEr  2s  In addition to choosing a response, each site delays sending out its response by an amount 1 s. [sent-57, score-0.695]
</p><p>43 1 s is related to each site's own assessment of its confidence about its decision and the size of memory it needed to consult to make the decision. [sent-58, score-0.515]
</p><p>44 1s is a monotonically decreasing function of confidence (one minus the maximum posterior probability) and a monotonically increasing function of memory size: 1s =  ~  1- max Pr(rl/J +~ 10g(MJ+ho  (3)  reLLtters  ho, h j, and h2' are constants common to all sites. [sent-59, score-0.428]
</p><p>45 Ms is the effective number of items in memory at site . [sent-60, score-0.829]
</p><p>46 1', equal to the number of distinct training views the site saw (or the limit of its memory size, whichever is less). [sent-61, score-1.01]
</p><p>47 In our toy system, M/ is the number of distinct training views presented to the system. [sent-62, score-0.453]
</p><p>48 M2 is approximately the number of training views with distinct orientations (because h is normalized by position), and M3 is effectively one view per letter. [sent-63, score-0.473]
</p><p>49 Relative to the decision time 1" the processing time required to perform normalizations is assumed to be negligible (This assumption can be removed by letting ho depend on site . [sent-65, score-0.761]
</p><p>50 2  Learning and testing  The learning component of the theory has yet to be determined. [sent-68, score-0.03]
</p><p>51 For our toy system, we assumed that the items kept in memory are free of luminance noise but subjected to normalization errors caused by the luminance noise (e. [sent-69, score-1.125]
</p><p>52 the position of a letter may not be perfectly determined). [sent-71, score-0.29]
</p><p>53 We measured performance of the toy system by first exposing it to 5 orientations and 20 positions of each letter at high signal-to-noise ratio (SNR). [sent-72, score-0.721]
</p><p>54 Ten letters from the Times Roman font were used in the simulation (bcdeghnopw). [sent-73, score-0.078]
</p><p>55 The system keeps in memory those studied views (Site 1) and their normalized versions (Sites 2 & 3). [sent-74, score-0.689]
</p><p>56 Since the normalization processes are reliable at high SNR, M2 "" 50, and M3 "" 10. [sent-76, score-0.171]
</p><p>57 We tested the system by presenting it with letters from either the studied views, or views it had not seen before. [sent-77, score-0.542]
</p><p>58 In the latter case, a novel view could be either with novel position alone, or with both novel position and orientation. [sent-78, score-0.682]
</p><p>59 The test stimuli were presented at SNR ranging from 210 to 1800 (Weber contrast of 10-30% at mean luminance of 48 cd/m2 and a noise standard deviation of 10 cd/m2). [sent-79, score-0.41]
</p><p>60 3  Results and Discussions  Figure 2a shows the performance of our toy visual system under different stimulus conditions. [sent-80, score-0.45]
</p><p>61 The numbered thin curves indicate recognition accuracy achieved by each site. [sent-81, score-0.201]
</p><p>62 As expected, Site 1, which kept raw images in memory, achieved the best accuracy when tested with studied views, but it could not generalize to novel views. [sent-82, score-0.379]
</p><p>63 In contrast, Site 3 maintained essentially the same level of performance regardless of view condition - its representation was invariant to position and orientation. [sent-83, score-0.309]
</p><p>64 Familiar views  Novel positions  Familiar views  Novel positions  Novel positions  Novel positions & orientations  & orientations  2,3  i  ~. [sent-84, score-1.086]
</p><p>65 ge  Low contnlst ( 15%)  50  Contrast (%)  (a)  100  50  100  50  100  % Flrst-arnvlng Response  (b)  Figure 2: (a) Accuracy of the system (solid symbols) verses accuracy of each site (numbered curves) under different contrast and view conditions. [sent-92, score-0.815]
</p><p>66 (b) Relative frequency of a site issuing the first-arriving response. [sent-93, score-0.587]
</p><p>67 The thick curves with solid symbols indicated the system's performance based on first-arriving responses. [sent-94, score-0.073]
</p><p>68 Clearly, it tracked the performance of the best-performing site under all conditions. [sent-95, score-0.517]
</p><p>69 The simple delay rule effectively picked out the most reliable response at each trial. [sent-98, score-0.228]
</p><p>70 studied) views were presented at low contrast (low SNR), Site 1, which used raw image as the representation, was responsible for issuing about 60% of the first-arriving responses. [sent-102, score-0.53]
</p><p>71 This is because normalization processes tend to be less reliable at low SNR. [sent-103, score-0.219]
</p><p>72 Whenever an input to Site 2 or 3 cannot be properly normalized, it will match poorly to the normalized views in memory, resulting in lower confidence and longer delay . [sent-104, score-0.429]
</p><p>73 As contrast increased, normalization processes became more accurate, and the first-arriving responses shifted to the higher sites. [sent-105, score-0.221]
</p><p>74 Higher sites encode more invariance, and thus need to consult fewer memory items. [sent-106, score-0.448]
</p><p>75 Lastly, when novel views were presented, Site 3 tended to be the most active, since it was the only site that fully captured all the invariance necessary for this condition. [sent-107, score-0.96]
</p><p>76 3 allows the system as a whole to be selfadaptive. [sent-109, score-0.131]
</p><p>77 Its effective representation, if we can speak of such, is flexible. [sent-110, score-0.067]
</p><p>78 No site is exclusively responsible for any particular kind of stimuli. [sent-111, score-0.552]
</p><p>79 Instead, the decision is always distributed across sites in a trial-by-trial basis. [sent-112, score-0.33]
</p><p>80 What do existing human data on object recognition have to say about this simple framework? [sent-113, score-0.233]
</p><p>81 Wouldn't those data supporting functional specialization or objectcategory-specific representations argue against this framework? [sent-114, score-0.064]
</p><p>82 1  Viewpoint effects  Entry-level object recognition [13] often shows less viewpoint dependence than subordinate-level object recognition. [sent-117, score-0.519]
</p><p>83 This has been taken to suggest that two  different mechanisms or forms of representation may be subserving these two types of object recognition tasks [4]. [sent-118, score-0.388]
</p><p>84 Figure 3a shows our system's overall performance in response time (RT) and error rate when tested with the studied (thus "familiar") and the novel (new positions and orientations) views. [sent-119, score-0.446]
</p><p>85 The difference in RT and error rate between these two conditions (Figure 3b) is a rough measure of the viewpoint effect. [sent-120, score-0.119]
</p><p>86 Even though the system includes a site (Site 3) with viewpoint-invariant representation, the system's overall performance still depends on viewpoint, particularly at low contrast. [sent-121, score-0.696]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('site', 0.517), ('views', 0.227), ('memory', 0.224), ('luminance', 0.219), ('letter', 0.192), ('toy', 0.184), ('object', 0.167), ('pr', 0.153), ('novel', 0.143), ('sites', 0.142), ('system', 0.131), ('representation', 0.124), ('decision', 0.124), ('hierarchically', 0.121), ('viewpoint', 0.119), ('orientations', 0.112), ('positions', 0.102), ('position', 0.098), ('response', 0.095), ('objects', 0.094), ('ho', 0.09), ('visual', 0.09), ('snr', 0.09), ('normalization', 0.086), ('confidence', 0.085), ('pathway', 0.082), ('consult', 0.082), ('delay', 0.082), ('letters', 0.078), ('familiar', 0.076), ('studied', 0.072), ('eexp', 0.07), ('homunculus', 0.07), ('issuing', 0.07), ('recognition', 0.066), ('distributed', 0.064), ('contrast', 0.063), ('modules', 0.06), ('item', 0.06), ('pathways', 0.06), ('view', 0.057), ('debate', 0.055), ('noise', 0.053), ('reliable', 0.051), ('specialized', 0.051), ('delays', 0.051), ('items', 0.051), ('numbered', 0.051), ('low', 0.048), ('raw', 0.047), ('ui', 0.047), ('accuracy', 0.047), ('stimulus', 0.045), ('stage', 0.043), ('invariance', 0.043), ('recognize', 0.043), ('monotonically', 0.043), ('distinct', 0.042), ('image', 0.04), ('ranging', 0.039), ('identity', 0.039), ('shifted', 0.038), ('decisions', 0.038), ('explaining', 0.038), ('rt', 0.038), ('curves', 0.037), ('effective', 0.037), ('kept', 0.036), ('symbols', 0.036), ('deviation', 0.036), ('responsible', 0.035), ('normalized', 0.035), ('processes', 0.034), ('tested', 0.034), ('representations', 0.034), ('centered', 0.033), ('common', 0.033), ('final', 0.032), ('amount', 0.032), ('intermediate', 0.031), ('axis', 0.031), ('mechanisms', 0.031), ('primary', 0.03), ('orientation', 0.03), ('theory', 0.03), ('committed', 0.03), ('profile', 0.03), ('negligible', 0.03), ('speak', 0.03), ('tended', 0.03), ('attaching', 0.03), ('boo', 0.03), ('clinical', 0.03), ('decides', 0.03), ('loosely', 0.03), ('maintained', 0.03), ('multitude', 0.03), ('re', 0.03), ('southern', 0.03), ('specialization', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="19-tfidf-1" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>Author: Bosco S. Tjan</p><p>Abstract: Theories of object recognition often assume that only one representation scheme is used within one visual-processing pathway. Versatility of the visual system comes from having multiple visual-processing pathways, each specialized in a different category of objects. We propose a theoretically simpler alternative, capable of explaining the same set of data and more. A single primary visual-processing pathway, loosely modular, is assumed. Memory modules are attached to sites along this pathway. Object-identity decision is made independently at each site. A site's response time is a monotonic-decreasing function of its confidence regarding its decision. An observer's response is the first-arriving response from any site. The effective representation(s) of such a system, determined empirically, can appear to be specialized for different tasks and stimuli, consistent with recent clinical and functional-imaging findings. This, however, merely reflects a decision being made at its appropriate level of abstraction. The system itself is intrinsically flexible and adaptive.</p><p>2 0.13995817 <a title="19-tfidf-2" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>Author: Shimon Edelman, Nathan Intrator</p><p>Abstract: We describe a unified framework for the understanding of structure representation in primate vision. A model derived from this framework is shown to be effectively systematic in that it has the ability to interpret and associate together objects that are related through a rearrangement of common</p><p>3 0.12677464 <a title="19-tfidf-3" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>Author: Predrag Neskovic, Philip C. Davis, Leon N. Cooper</p><p>Abstract: In this work, we introduce an Interactive Parts (IP) model as an alternative to Hidden Markov Models (HMMs). We t ested both models on a database of on-line cursive script. We show that implementations of HMMs and the IP model, in which all letters are assumed to have the same average width , give comparable results. However , in contrast to HMMs, the IP model can handle duration modeling without an increase in computational complexity. 1</p><p>4 0.093685001 <a title="19-tfidf-4" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>Author: Sophie Denève, Jean-René Duhamel, Alexandre Pouget</p><p>Abstract: Most models of spatial representations in the cortex assume cells with limited receptive fields that are defined in a particular egocentric frame of reference. However, cells outside of primary sensory cortex are either gain modulated by postural input or partially shifting. We show that solving classical spatial tasks, like sensory prediction, multi-sensory integration, sensory-motor transformation and motor control requires more complicated intermediate representations that are not invariant in one frame of reference. We present an iterative basis function map that performs these spatial tasks optimally with gain modulated and partially shifting units, and tests it against neurophysiological and neuropsychological data. In order to perform an action directed toward an object, it is necessary to have a representation of its spatial location. The brain must be able to use spatial cues coming from different modalities (e.g. vision, audition, touch, proprioception), combine them to infer the position of the object, and compute the appropriate movement. These cues are in different frames of reference corresponding to different sensory or motor modalities. Visual inputs are primarily encoded in retinotopic maps, auditory inputs are encoded in head centered maps and tactile cues are encoded in skin-centered maps. Going from one frame of reference to the other might seem easy. For example, the head-centered position of an object can be approximated by the sum of its retinotopic position and the eye position. However, positions are represented by population codes in the brain, and computing a head-centered map from a retinotopic map is a more complex computation than the underlying sum. Moreover, as we get closer to sensory-motor areas it seems reasonable to assume Spksls 150 100 50 o Figure 1: Response of a VIP cell to visual stimuli appearing in different part of the screen, for three different eye positions. The level of grey represent the frequency of discharge (In spikes per seconds). The white cross is the fixation point (the head is fixed). The cell's receptive field is moving with the eyes, but only partially. Here the receptive field shift is 60% of the total gaze shift. Moreover this cell is gain modulated by eye position (adapted from Duhamel et al). that the representations should be useful for sensory-motor transformations, rather than encode an</p><p>5 0.083664365 <a title="19-tfidf-5" href="./nips-2000-An_Adaptive_Metric_Machine_for_Pattern_Classification.html">23 nips-2000-An Adaptive Metric Machine for Pattern Classification</a></p>
<p>Author: Carlotta Domeniconi, Jing Peng, Dimitrios Gunopulos</p><p>Abstract: Nearest neighbor classification assumes locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with finite samples due to the curse of dimensionality. Severe bias can be introduced under these conditions when using the nearest neighbor rule. We propose a locally adaptive nearest neighbor classification method to try to minimize bias. We use a Chi-squared distance analysis to compute a flexible metric for producing neighborhoods that are elongated along less relevant feature dimensions and constricted along most influential ones. As a result, the class conditional probabilities tend to be smoother in the modified neighborhoods, whereby better classification performance can be achieved. The efficacy of our method is validated and compared against other techniques using a variety of real world data. 1</p><p>6 0.083449423 <a title="19-tfidf-6" href="./nips-2000-Hierarchical_Memory-Based_Reinforcement_Learning.html">63 nips-2000-Hierarchical Memory-Based Reinforcement Learning</a></p>
<p>7 0.079667911 <a title="19-tfidf-7" href="./nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">88 nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>8 0.079406798 <a title="19-tfidf-8" href="./nips-2000-Modelling_Spatial_Recall%2C_Mental_Imagery_and_Neglect.html">87 nips-2000-Modelling Spatial Recall, Mental Imagery and Neglect</a></p>
<p>9 0.075687945 <a title="19-tfidf-9" href="./nips-2000-Shape_Context%3A_A_New_Descriptor_for_Shape_Matching_and_Object_Recognition.html">117 nips-2000-Shape Context: A New Descriptor for Shape Matching and Object Recognition</a></p>
<p>10 0.071103543 <a title="19-tfidf-10" href="./nips-2000-Probabilistic_Semantic_Video_Indexing.html">103 nips-2000-Probabilistic Semantic Video Indexing</a></p>
<p>11 0.070303574 <a title="19-tfidf-11" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>12 0.068772726 <a title="19-tfidf-12" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>13 0.067678839 <a title="19-tfidf-13" href="./nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">42 nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<p>14 0.063869022 <a title="19-tfidf-14" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>15 0.062656015 <a title="19-tfidf-15" href="./nips-2000-Noise_Suppression_Based_on_Neurophysiologically-motivated_SNR_Estimation_for_Robust_Speech_Recognition.html">91 nips-2000-Noise Suppression Based on Neurophysiologically-motivated SNR Estimation for Robust Speech Recognition</a></p>
<p>16 0.058519416 <a title="19-tfidf-16" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>17 0.05558975 <a title="19-tfidf-17" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>18 0.054755747 <a title="19-tfidf-18" href="./nips-2000-Error-correcting_Codes_on_a_Bethe-like_Lattice.html">47 nips-2000-Error-correcting Codes on a Bethe-like Lattice</a></p>
<p>19 0.054521695 <a title="19-tfidf-19" href="./nips-2000-The_Manhattan_World_Assumption%3A_Regularities_in_Scene_Statistics_which_Enable_Bayesian_Inference.html">135 nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</a></p>
<p>20 0.05344528 <a title="19-tfidf-20" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.182), (1, -0.112), (2, -0.003), (3, 0.025), (4, -0.08), (5, 0.032), (6, 0.111), (7, -0.046), (8, 0.172), (9, 0.031), (10, 0.076), (11, 0.197), (12, -0.005), (13, 0.129), (14, -0.038), (15, 0.062), (16, 0.011), (17, -0.134), (18, 0.002), (19, 0.051), (20, -0.009), (21, -0.117), (22, 0.094), (23, 0.17), (24, 0.041), (25, 0.198), (26, 0.109), (27, -0.031), (28, -0.026), (29, 0.027), (30, -0.162), (31, 0.093), (32, -0.163), (33, -0.011), (34, -0.119), (35, 0.114), (36, -0.001), (37, -0.087), (38, 0.103), (39, 0.126), (40, 0.047), (41, -0.067), (42, -0.002), (43, 0.038), (44, 0.113), (45, 0.016), (46, -0.026), (47, 0.115), (48, 0.044), (49, -0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98261815 <a title="19-lsi-1" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>Author: Bosco S. Tjan</p><p>Abstract: Theories of object recognition often assume that only one representation scheme is used within one visual-processing pathway. Versatility of the visual system comes from having multiple visual-processing pathways, each specialized in a different category of objects. We propose a theoretically simpler alternative, capable of explaining the same set of data and more. A single primary visual-processing pathway, loosely modular, is assumed. Memory modules are attached to sites along this pathway. Object-identity decision is made independently at each site. A site's response time is a monotonic-decreasing function of its confidence regarding its decision. An observer's response is the first-arriving response from any site. The effective representation(s) of such a system, determined empirically, can appear to be specialized for different tasks and stimuli, consistent with recent clinical and functional-imaging findings. This, however, merely reflects a decision being made at its appropriate level of abstraction. The system itself is intrinsically flexible and adaptive.</p><p>2 0.5545553 <a title="19-lsi-2" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>Author: Predrag Neskovic, Philip C. Davis, Leon N. Cooper</p><p>Abstract: In this work, we introduce an Interactive Parts (IP) model as an alternative to Hidden Markov Models (HMMs). We t ested both models on a database of on-line cursive script. We show that implementations of HMMs and the IP model, in which all letters are assumed to have the same average width , give comparable results. However , in contrast to HMMs, the IP model can handle duration modeling without an increase in computational complexity. 1</p><p>3 0.52281857 <a title="19-lsi-3" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>Author: Shimon Edelman, Nathan Intrator</p><p>Abstract: We describe a unified framework for the understanding of structure representation in primate vision. A model derived from this framework is shown to be effectively systematic in that it has the ability to interpret and associate together objects that are related through a rearrangement of common</p><p>4 0.48150229 <a title="19-lsi-4" href="./nips-2000-An_Adaptive_Metric_Machine_for_Pattern_Classification.html">23 nips-2000-An Adaptive Metric Machine for Pattern Classification</a></p>
<p>Author: Carlotta Domeniconi, Jing Peng, Dimitrios Gunopulos</p><p>Abstract: Nearest neighbor classification assumes locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with finite samples due to the curse of dimensionality. Severe bias can be introduced under these conditions when using the nearest neighbor rule. We propose a locally adaptive nearest neighbor classification method to try to minimize bias. We use a Chi-squared distance analysis to compute a flexible metric for producing neighborhoods that are elongated along less relevant feature dimensions and constricted along most influential ones. As a result, the class conditional probabilities tend to be smoother in the modified neighborhoods, whereby better classification performance can be achieved. The efficacy of our method is validated and compared against other techniques using a variety of real world data. 1</p><p>5 0.39762548 <a title="19-lsi-5" href="./nips-2000-Hierarchical_Memory-Based_Reinforcement_Learning.html">63 nips-2000-Hierarchical Memory-Based Reinforcement Learning</a></p>
<p>Author: Natalia Hernandez-Gardiol, Sridhar Mahadevan</p><p>Abstract: A key challenge for reinforcement learning is scaling up to large partially observable domains. In this paper, we show how a hierarchy of behaviors can be used to create and select among variable length short-term memories appropriate for a task. At higher levels in the hierarchy, the agent abstracts over lower-level details and looks back over a variable number of high-level decisions in time. We formalize this idea in a framework called Hierarchical Suffix Memory (HSM). HSM uses a memory-based SMDP learning method to rapidly propagate delayed reward across long decision sequences. We describe a detailed experimental study comparing memory vs. hierarchy using the HSM framework on a realistic corridor navigation task. 1</p><p>6 0.36644098 <a title="19-lsi-6" href="./nips-2000-The_Interplay_of_Symbolic_and_Subsymbolic_Processes_in_Anagram_Problem_Solving.html">132 nips-2000-The Interplay of Symbolic and Subsymbolic Processes in Anagram Problem Solving</a></p>
<p>7 0.35259351 <a title="19-lsi-7" href="./nips-2000-Modelling_Spatial_Recall%2C_Mental_Imagery_and_Neglect.html">87 nips-2000-Modelling Spatial Recall, Mental Imagery and Neglect</a></p>
<p>8 0.33825585 <a title="19-lsi-8" href="./nips-2000-Shape_Context%3A_A_New_Descriptor_for_Shape_Matching_and_Object_Recognition.html">117 nips-2000-Shape Context: A New Descriptor for Shape Matching and Object Recognition</a></p>
<p>9 0.30495498 <a title="19-lsi-9" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>10 0.30117366 <a title="19-lsi-10" href="./nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">42 nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<p>11 0.29256237 <a title="19-lsi-11" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>12 0.28146619 <a title="19-lsi-12" href="./nips-2000-Probabilistic_Semantic_Video_Indexing.html">103 nips-2000-Probabilistic Semantic Video Indexing</a></p>
<p>13 0.26881468 <a title="19-lsi-13" href="./nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">88 nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>14 0.26158941 <a title="19-lsi-14" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>15 0.25308737 <a title="19-lsi-15" href="./nips-2000-Stagewise_Processing_in_Error-correcting_Codes_and_Image_Restoration.html">126 nips-2000-Stagewise Processing in Error-correcting Codes and Image Restoration</a></p>
<p>16 0.23905884 <a title="19-lsi-16" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>17 0.23726709 <a title="19-lsi-17" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>18 0.23131466 <a title="19-lsi-18" href="./nips-2000-The_Manhattan_World_Assumption%3A_Regularities_in_Scene_Statistics_which_Enable_Bayesian_Inference.html">135 nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</a></p>
<p>19 0.22804275 <a title="19-lsi-19" href="./nips-2000-Programmable_Reinforcement_Learning_Agents.html">105 nips-2000-Programmable Reinforcement Learning Agents</a></p>
<p>20 0.21823598 <a title="19-lsi-20" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.022), (17, 0.086), (33, 0.026), (36, 0.015), (42, 0.016), (55, 0.542), (62, 0.058), (67, 0.028), (75, 0.012), (76, 0.03), (81, 0.023), (90, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97130841 <a title="19-lda-1" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>Author: Bosco S. Tjan</p><p>Abstract: Theories of object recognition often assume that only one representation scheme is used within one visual-processing pathway. Versatility of the visual system comes from having multiple visual-processing pathways, each specialized in a different category of objects. We propose a theoretically simpler alternative, capable of explaining the same set of data and more. A single primary visual-processing pathway, loosely modular, is assumed. Memory modules are attached to sites along this pathway. Object-identity decision is made independently at each site. A site's response time is a monotonic-decreasing function of its confidence regarding its decision. An observer's response is the first-arriving response from any site. The effective representation(s) of such a system, determined empirically, can appear to be specialized for different tasks and stimuli, consistent with recent clinical and functional-imaging findings. This, however, merely reflects a decision being made at its appropriate level of abstraction. The system itself is intrinsically flexible and adaptive.</p><p>2 0.92652261 <a title="19-lda-2" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>Author: Gal Elidan, Noam Lotner, Nir Friedman, Daphne Koller</p><p>Abstract: A serious problem in learning probabilistic models is the presence of hidden variables. These variables are not observed, yet interact with several of the observed variables. As such, they induce seemingly complex dependencies among the latter. In recent years, much attention has been devoted to the development of algorithms for learning parameters, and in some cases structure, in the presence of hidden variables. In this paper, we address the related problem of detecting hidden variables that interact with the observed variables. This problem is of interest both for improving our understanding of the domain and as a preliminary step that guides the learning procedure towards promising models. A very natural approach is to search for</p><p>3 0.51027119 <a title="19-lda-3" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>Author: Shimon Edelman, Nathan Intrator</p><p>Abstract: We describe a unified framework for the understanding of structure representation in primate vision. A model derived from this framework is shown to be effectively systematic in that it has the ability to interpret and associate together objects that are related through a rearrangement of common</p><p>4 0.43027812 <a title="19-lda-4" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>Author: Zoubin Ghahramani, Matthew J. Beal</p><p>Abstract: Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set. 1</p><p>5 0.41646582 <a title="19-lda-5" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass, Eduardo D. Sontag, Anthony M. Zador</p><p>Abstract: Experimental data show that biological synapses behave quite differently from the symbolic synapses in common artificial neural network models. Biological synapses are dynamic, i.e., their</p><p>6 0.41125411 <a title="19-lda-6" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>7 0.4098171 <a title="19-lda-7" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>8 0.40737402 <a title="19-lda-8" href="./nips-2000-Who_Does_What%3F_A_Novel_Algorithm_to_Determine_Function_Localization.html">147 nips-2000-Who Does What? A Novel Algorithm to Determine Function Localization</a></p>
<p>9 0.39811021 <a title="19-lda-9" href="./nips-2000-Recognizing_Hand-written_Digits_Using_Hierarchical_Products_of_Experts.html">108 nips-2000-Recognizing Hand-written Digits Using Hierarchical Products of Experts</a></p>
<p>10 0.39755863 <a title="19-lda-10" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>11 0.39265463 <a title="19-lda-11" href="./nips-2000-The_Early_Word_Catches_the_Weights.html">131 nips-2000-The Early Word Catches the Weights</a></p>
<p>12 0.37435949 <a title="19-lda-12" href="./nips-2000-Accumulator_Networks%3A_Suitors_of_Local_Probability_Propagation.html">15 nips-2000-Accumulator Networks: Suitors of Local Probability Propagation</a></p>
<p>13 0.36605626 <a title="19-lda-13" href="./nips-2000-Overfitting_in_Neural_Nets%3A_Backpropagation%2C_Conjugate_Gradient%2C_and_Early_Stopping.html">97 nips-2000-Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping</a></p>
<p>14 0.36343724 <a title="19-lda-14" href="./nips-2000-Learning_Switching_Linear_Models_of_Human_Motion.html">80 nips-2000-Learning Switching Linear Models of Human Motion</a></p>
<p>15 0.35730273 <a title="19-lda-15" href="./nips-2000-Fast_Training_of_Support_Vector_Classifiers.html">52 nips-2000-Fast Training of Support Vector Classifiers</a></p>
<p>16 0.35570416 <a title="19-lda-16" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>17 0.35273343 <a title="19-lda-17" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>18 0.35266238 <a title="19-lda-18" href="./nips-2000-Modelling_Spatial_Recall%2C_Mental_Imagery_and_Neglect.html">87 nips-2000-Modelling Spatial Recall, Mental Imagery and Neglect</a></p>
<p>19 0.35263994 <a title="19-lda-19" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>20 0.34793422 <a title="19-lda-20" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
