<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-69" href="#">nips2000-69</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</h1>
<br/><p>Source: <a title="nips-2000-69-pdf" href="http://papers.nips.cc/paper/1920-incorporating-second-order-functional-knowledge-for-better-option-pricing.pdf">pdf</a></p><p>Author: Charles Dugas, Yoshua Bengio, François Bélisle, Claude Nadeau, René Garcia</p><p>Abstract: Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in two of its arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of continuous functions with these and other properties. We apply this new class of functions to the task of modeling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori constraints.</p><p>Reference: <a title="nips-2000-69-reference" href="../nips2000_reference/nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Incorporating Second-Order Functional Knowledge for Better Option Pricing  Charles Dugas, Yoshua Bengio, Fran~ois Belisle, Claude Nadeau:Rene Garcia CIRANO, Montreal, Qc, Canada H3A 2A5 {du gas ,beng i o y,beli s lf r ,na de a u c }@ ro . [sent-1, score-0.081]
</p><p>2 ca  Abstract Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. [sent-5, score-0.118]
</p><p>3 We study here a case where we know that the function to be learned is non-decreasing in two of its arguments and convex in one of them. [sent-6, score-0.204]
</p><p>4 For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of continuous functions with these and other properties. [sent-7, score-0.649]
</p><p>5 We apply this new class of functions to the task of modeling the price of call options. [sent-8, score-0.585]
</p><p>6 Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori constraints. [sent-9, score-0.728]
</p><p>7 In this paper we consider prior knowledge on the positivity of some first and second derivatives of the function to be learned. [sent-11, score-0.231]
</p><p>8 In particular such constraints have applications to modeling the price of European stock options. [sent-12, score-0.375]
</p><p>9 Based on the Black-Scholes formula, the price of a call stock option is monotonically increasing in both the "moneyness" and time to maturity of the option, and it is convex in the "moneyness". [sent-13, score-1.201]
</p><p>10 Section 3 better explains these terms and stock options. [sent-14, score-0.088]
</p><p>11 Second, in the main theorem, we extend this result to functions of two or more arguments, with some having the convexity property and all having positive first derivative. [sent-16, score-0.173]
</p><p>12 This result rests on additional properties on cross-derivatives, which we illustrate below for the case of two ·C. [sent-17, score-0.034]
</p><p>13 c a  arguments: (2)  Comparative experiments on these new classes of functions were performed on stock option prices, showing some improvements when using these new classes rather than ordinary feedforward neural networks. [sent-22, score-0.735]
</p><p>14 The improvements appear to be non-stationary but the new class of functions shows the most stable behavior in predicting future prices. [sent-23, score-0.218]
</p><p>15 with a sigmoid activation function h(s)  (3)  j  = l+~-" are universal approximators of continuous functions [1, 2, 5]. [sent-27, score-0.481]
</p><p>16 Since h is monotonically increasing, it is easy to force the first derivatives with respect to x to be positive by forcing the weights to be positive, for example with the exponential function: H  N+  = {f(x) = bo + 2: eWi h(bi + 2: eVii Xj)} i=l  because h'(s)  (4)  j  = h(s)(1- h(s)) > o. [sent-29, score-0.364]
</p><p>17 Since the sigmoid h has a positive first derivative, its primitive, which we call softplus, is convex: ((s) = log(1 + eS ) (5) i. [sent-30, score-0.361]
</p><p>18 Xj~  ~0  (7)  Note that m or p can be 0, so as special cases we find that f is positive, and that it is monotonically increasing w. [sent-50, score-0.093]
</p><p>19 1  Universality of cN++ over ~  Theorem Within the set F ++ of continuous functions from ~n to ~ whose first and second derivatives are non-negative (as specified by equation 7), the class cN++ is a universal approximator. [sent-58, score-0.509]
</p><p>20 Proof For lack of space we only show here a sketch of the proof, and only for the case n = 2 and c = 1 (one convex dimension and one other dimension), but the same principle allows to prove the more general case. [sent-59, score-0.13]
</p><p>21 To perform our approximation we will restrict 9 to the subset of IN++ where the sigmoid becomes a step function B(x) = [x >o and where the softplus becomes the positive part function x+ = max(O, x). [sent-61, score-0.512]
</p><p>22 Let D be the compact domain of interest and t: the desired approximation precision. [sent-62, score-0.035]
</p><p>23 The number of square grids on the Xl axis is Nl and the number on the X2 axis is N 2. [sent-64, score-0.082]
</p><p>24 The number of hidden units is H = (Nl + 1)(N2 + 1). [sent-65, score-0.111]
</p><p>25 Let Xij = (Xi, Xj) = (al + iL, bl + jL) be the grid points, with i = 0,1, . [sent-66, score-0.108]
</p><p>26 With k = i(N2 + 1) + j, we recursively build a series of functions gk(X) as follows :  with increment  for k = 1 to H and with initial approximation go = f(al, bl ). [sent-74, score-0.262]
</p><p>27 The final approximation is g(x) = gH(X), It is exact at every single point on the grid and within t: of the true function value anywhere within D. [sent-75, score-0.126]
</p><p>28 To prove this, we need to show that at every step of the recursive procedure, the necessary increment is nonnegative (since it must be equated with eWk) . [sent-76, score-0.165]
</p><p>29 By the mean value theorem, the third degree finite difference is nonnegative if the corresponding third derivative is nonnegative everywhere over the finite interval which is obtained by constraint 7. [sent-80, score-0.515]
</p><p>30 Finally, the third degree finite difference being nonnegative, the corresponding increment is also nonnegative and this completes the proof. [sent-81, score-0.303]
</p><p>31 Corollary Within the set of positive continuous functions from ~ to ~ whose first and second derivatives are non-negative, the class IN++ is a universal approximator. [sent-82, score-0.588]
</p><p>32 3 Estimating Call Option Prices An option is a contract between two parties that entitles the buyer to a claim at a future date T that depends on the future price, ST of an underlying asset whose price at time t is St. [sent-83, score-0.681]
</p><p>33 In this paper we consider the very common European call options, in which the value of the claim at maturity (time T) is max(O, ST - K), i. [sent-84, score-0.25]
</p><p>34 if the price is above the strike price K, then the seller of the option owes ST - K dollars to the buyer. [sent-86, score-1.019]
</p><p>35 In the no-arbitrage framework, the call function is believed to be a function of the actual market price of the security (St), the strike price (K), the remaining time to maturity (T = T - t), the risk free interest rate (r) , and the volatility of the return (a). [sent-87, score-1.082]
</p><p>36 The challenge is to evaluate the value of the option prior to the expiration date before entering a transaction. [sent-88, score-0.394]
</p><p>37 The risk free interest rate (r) needs to be somehow extracted from the term structure and the volatility (a) needs to be forecasted, this latest task being a field of research in itself. [sent-89, score-0.122]
</p><p>38 We have [3] previously tried to feed in neural networks with estimates of the volatility using historical averages but so far, the gains remained insignificant. [sent-90, score-0.148]
</p><p>39 One more important result is that under mild conditions, the call option function is homogeneous of degree one with respect to the strike price and so our final approximation depends on two variables: the moneyness (M = Stl K) and the time to maturity (T). [sent-92, score-1.185]
</p><p>40 ctl K  =  (8)  f(M, T)  An economic theory yielding to the Black-Scholes formula suggest that f has the properties of (1), so we will evaluate the advantages brought by the function classes of the previous section. [sent-93, score-0.158]
</p><p>41 However, it is not clear whether the constraint on the cross derivatives that are incorporated in IN++ should or not be present in the true price function. [sent-94, score-0.429]
</p><p>42 It is known that the Black-Scholes formula does not adequately represent the market pricing of options, but it might still be a useful guide in designing a learning algorithm for option prices. [sent-95, score-0.713]
</p><p>43 4 Experimental Setup As a reference model, we use a simple multi-layered perceptron with one hidden layer (eq. [sent-96, score-0.038]
</p><p>44 We also compare our results with a recently proposed model [4] that closely resembles the Black-Scholes formula for option pricing (i. [sent-98, score-0.669]
</p><p>45 another way to incorporate possibly useful prior knowledge): nh  yES  a  +M  . [sent-100, score-0.095]
</p><p>46 (9)  i=l  We evaluate two new architectures incorporating some or all of the constraints defined in equation 7. [sent-109, score-0.078]
</p><p>47 We used european call option data from 1988 to 1993. [sent-110, score-0.57]
</p><p>48 A total of 43518 transaction prices on european call options on the S&P500; index were used. [sent-111, score-0.415]
</p><p>49 In each case, we used the first two quarters of 1988 as a training set (3434 examples), the third quarter as a validation set (1642 examples) for model selection and 4 to 20 quarters as a test sets (each with around 1500 examples) for final generalization error estimation. [sent-113, score-0.648]
</p><p>50 In tables 1 and 2, we present results for networks with unconstrained weights on the left-hand side, and weights constrained to positive and monotone functions through exponentiation of parameters on the right-hand side. [sent-114, score-0.888]
</p><p>51 For each model, the number of hidden units varies from one to nine. [sent-115, score-0.111]
</p><p>52 The mean squared error results reported were obtained as follows : first, we randomly sampled the parameter space 1000 times. [sent-116, score-0.054]
</p><p>53 In figure 1, we present tests of the same models on each quarter up to and including 1993 (20 additional test sets) in order to assess the persistence (conversely, the degradation through time) of the trained models. [sent-119, score-0.238]
</p><p>54 5 Forecasting Results Simple Multi-Layered Perceptrons Mean Squared Error Results on Call Option Pricing (x 10- 4 ) Units Unconstrained weights Constrained weights Train Valid Test! [sent-120, score-0.158]
</p><p>55 67 Black-Scholes Similar Networks Mean Squared Error Results on Call Option Pricing (x 10- 4 ) Units Unconstrained weights Constrained weights Train Valid Test! [sent-194, score-0.158]
</p><p>56 Right: parameters are constrained through exponentiation so that the resulting function is both positive and monotone increasing everywhere w. [sent-269, score-0.446]
</p><p>57 Bottom: neural networks with an architecture resembling the Black-Scholes formula as defined in equation 9. [sent-274, score-0.195]
</p><p>58 The number of units varies from 1 to 9 for each network architecture. [sent-275, score-0.073]
</p><p>59 The first two quarters of 1988 were used for training, the third of 1988 for validation and the fourth of 1988 for testing. [sent-276, score-0.304]
</p><p>60 The first quarter of 1989 was used as a second test set to assess the persistence of the models through time (figure 1). [sent-277, score-0.238]
</p><p>61 In bold: test results for models with best validation results. [sent-278, score-0.183]
</p><p>62 As can be seen in tables 1 and 2, the positivity constraints through exponentiation of the weights allow the networks to avoid overfitting. [sent-279, score-0.329]
</p><p>63 The training errors are generally slightly lower for the networks with unconstrained weights, the validation errors are similar but final test errors are disastrous for unconstrained networks, compared to the constrained ones. [sent-280, score-0.899]
</p><p>64 This "liftoff' pattern when looking at training, validation and testing errors has triggered our attention towards the analysis of the evolution of the test error through time. [sent-281, score-0.272]
</p><p>65 97 Sums of SoftPlus and Sigmoid functions Mean Squared Error Results on Call Option Pricing (x 10- 4 ) Units Unconstrained weights Constrained weights Train Valid Testl Test2 Train Valid Test1 Test2 2. [sent-354, score-0.252]
</p><p>66 Top: products of softplus along the convex axis with sigmoid along the monotone axis. [sent-427, score-0.648]
</p><p>67 Bottom: the softplus and sigmoid functions are summed instead of being multiplied. [sent-428, score-0.492]
</p><p>68 The constrained Black-Scholes similar model performs slightly better than other models on the second test set but then fails on latter quarters (figure 1). [sent-432, score-0.309]
</p><p>69 All in all, at the expense of slightly higher initial errors our proposed architecture allows us to forecast with increased stability much farther in the future. [sent-433, score-0.106]
</p><p>70 This is a very welcome property as new derivative products have a tendency to lock in values for much longer durations (up to 10 years) than traditional ones. [sent-434, score-0.058]
</p><p>71 6 Conclusions Motivated by prior knowledge on the derivatives of the function that gives the price of European options, we have introduced new classes of functions similar to multi-layer neural networks that have those properties. [sent-435, score-0.672]
</p><p>72 We have shown one of these classes to be a universal approximator for functions having those properties, and we have shown that using this a priori knowledge can help in improving generalization performance. [sent-436, score-0.536]
</p><p>73 In particular, we have found that the models that incorporate this a priori knowledge generalize in a more stable way over time. [sent-437, score-0.158]
</p><p>74 ,  • • , ,  5  10  15  20  Ouar1Ofusodas lest sel tom3rd01 1988 1 ho11993(llCI) 041  °O~-----'~--~'~O----~,~,----~ro~--~ Quartorusodasleslsel 1 rom3rd01 1988104lho1 1993(Ulci)  Figure 1: Out-of-sample results from the third quarter of 1988 to the fourth of 1993 (incl. [sent-447, score-0.184]
</p><p>75 Left: unconstrained models: results for the BlackScholes similar network. [sent-449, score-0.165]
</p><p>76 Other unconstrained models exhibit similar swinging result patterns and levels of errors. [sent-450, score-0.165]
</p><p>77 Right: constrained models: the fully constrained proposed architecture (solid). [sent-451, score-0.302]
</p><p>78 The model with sums over dimensions obtains similar results. [sent-452, score-0.086]
</p><p>79 The constrained Black-Scholes model obtains very poor results (dashed). [sent-454, score-0.16]
</p><p>80 Continuous valued neural networks with two hidden layers are sufficient. [sent-457, score-0.107]
</p><p>81 Technical Report 1176, Department d'informatique et de Recherche Operationnelle, Universite de Montreal, Montreal, Quebec, Canada, 2000. [sent-468, score-0.09]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('option', 0.353), ('price', 0.287), ('pricing', 0.244), ('softplus', 0.244), ('universal', 0.191), ('unconstrained', 0.165), ('sigmoid', 0.154), ('convex', 0.13), ('call', 0.128), ('constrained', 0.124), ('maturity', 0.122), ('quarter', 0.122), ('quarters', 0.122), ('validation', 0.12), ('options', 0.119), ('derivatives', 0.106), ('montreal', 0.105), ('xij', 0.098), ('functions', 0.094), ('train', 0.093), ('axj', 0.092), ('moneyness', 0.092), ('strike', 0.092), ('european', 0.089), ('valid', 0.089), ('stock', 0.088), ('nonnegative', 0.086), ('canada', 0.083), ('approximator', 0.083), ('st', 0.08), ('exponentiation', 0.079), ('increment', 0.079), ('monotone', 0.079), ('prices', 0.079), ('volatility', 0.079), ('positive', 0.079), ('weights', 0.079), ('incorporating', 0.078), ('class', 0.076), ('xj', 0.075), ('arguments', 0.074), ('units', 0.073), ('formula', 0.072), ('networks', 0.069), ('cn', 0.066), ('knowledge', 0.064), ('test', 0.063), ('third', 0.062), ('cirano', 0.061), ('dugas', 0.061), ('eviixj', 0.061), ('ewi', 0.061), ('garcia', 0.061), ('positivity', 0.061), ('testl', 0.061), ('xlf', 0.061), ('irn', 0.059), ('derivative', 0.058), ('monotonically', 0.056), ('grid', 0.054), ('squared', 0.054), ('architecture', 0.054), ('bl', 0.054), ('persistence', 0.053), ('quebec', 0.053), ('comer', 0.053), ('gh', 0.053), ('nh', 0.053), ('rectangle', 0.053), ('wrt', 0.053), ('errors', 0.052), ('classes', 0.052), ('priori', 0.052), ('dimensions', 0.05), ('improvements', 0.048), ('feedforward', 0.048), ('everywhere', 0.048), ('de', 0.045), ('bo', 0.044), ('market', 0.044), ('risk', 0.043), ('continuous', 0.042), ('incorporate', 0.042), ('tables', 0.041), ('date', 0.041), ('axis', 0.041), ('xl', 0.04), ('degree', 0.039), ('hidden', 0.038), ('final', 0.037), ('nl', 0.037), ('bij', 0.037), ('finite', 0.037), ('testing', 0.037), ('increasing', 0.037), ('obtains', 0.036), ('incorporated', 0.036), ('ro', 0.036), ('approximation', 0.035), ('properties', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="69-tfidf-1" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>Author: Charles Dugas, Yoshua Bengio, François Bélisle, Claude Nadeau, René Garcia</p><p>Abstract: Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in two of its arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of continuous functions with these and other properties. We apply this new class of functions to the task of modeling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori constraints.</p><p>2 0.19797207 <a title="69-tfidf-2" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>Author: Anders Jonsson, Andrew G. Barto</p><p>Abstract: Learning a complex task can be significantly facilitated by defining a hierarchy of subtasks. An agent can learn to choose between various temporally abstract actions, each solving an assigned subtask, to accomplish the overall task. In this paper, we study hierarchical learning using the framework of options. We argue that to take full advantage of hierarchical structure, one should perform option-specific state abstraction, and that if this is to scale to larger tasks, state abstraction should be automated. We adapt McCallum's U-Tree algorithm to automatically build option-specific representations of the state feature space, and we illustrate the resulting algorithm using a simple hierarchical task. Results suggest that automated option-specific state abstraction is an attractive approach to making hierarchical learning systems more effective.</p><p>3 0.099445216 <a title="69-tfidf-3" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>Author: Martijn A. R. Leisink, Hilbert J. Kappen</p><p>Abstract: We present a method to bound the partition function of a Boltzmann machine neural network with any odd order polynomial. This is a direct extension of the mean field bound, which is first order. We show that the third order bound is strictly better than mean field. Additionally we show the rough outline how this bound is applicable to sigmoid belief networks. Numerical experiments indicate that an error reduction of a factor two is easily reached in the region where expansion based approximations are useful. 1</p><p>4 0.093324482 <a title="69-tfidf-4" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>Author: Shie Mannor, Ron Meir</p><p>Abstract: The problem of constructing weak classifiers for boosting algorithms is studied. We present an algorithm that produces a linear classifier that is guaranteed to achieve an error better than random guessing for any distribution on the data. While this weak learner is not useful for learning in general, we show that under reasonable conditions on the distribution it yields an effective weak learner for one-dimensional problems. Preliminary simulations suggest that similar behavior can be expected in higher dimensions, a result which is corroborated by some recent theoretical bounds. Additionally, we provide improved convergence rate bounds for the generalization error in situations where the empirical error can be made small, which is exactly the situation that occurs if weak learners with guaranteed performance that is better than random guessing can be established. 1</p><p>5 0.08996091 <a title="69-tfidf-5" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>Author: Igor V. Cadez, Padhraic Smyth</p><p>Abstract: We investigate a general characteristic of the trade-off in learning problems between goodness-of-fit and model complexity. Specifically we characterize a general class of learning problems where the goodness-of-fit function can be shown to be convex within firstorder as a function of model complexity. This general property of</p><p>6 0.089788347 <a title="69-tfidf-6" href="./nips-2000-Second_Order_Approximations_for_Probability_Models.html">114 nips-2000-Second Order Approximations for Probability Models</a></p>
<p>7 0.089296594 <a title="69-tfidf-7" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>8 0.079760797 <a title="69-tfidf-8" href="./nips-2000-A_Variational_Mean-Field_Theory_for_Sigmoidal_Belief_Networks.html">14 nips-2000-A Variational Mean-Field Theory for Sigmoidal Belief Networks</a></p>
<p>9 0.077099599 <a title="69-tfidf-9" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>10 0.069051191 <a title="69-tfidf-10" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>11 0.067947663 <a title="69-tfidf-11" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>12 0.064723052 <a title="69-tfidf-12" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>13 0.06345427 <a title="69-tfidf-13" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>14 0.062509 <a title="69-tfidf-14" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>15 0.057454426 <a title="69-tfidf-15" href="./nips-2000-A_Neural_Probabilistic_Language_Model.html">6 nips-2000-A Neural Probabilistic Language Model</a></p>
<p>16 0.056515835 <a title="69-tfidf-16" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>17 0.055980407 <a title="69-tfidf-17" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>18 0.055747524 <a title="69-tfidf-18" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>19 0.055579789 <a title="69-tfidf-19" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>20 0.053358711 <a title="69-tfidf-20" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.211), (1, 0.002), (2, 0.044), (3, -0.143), (4, 0.051), (5, -0.004), (6, -0.004), (7, -0.033), (8, -0.012), (9, 0.021), (10, -0.089), (11, -0.047), (12, 0.006), (13, 0.028), (14, 0.086), (15, 0.033), (16, 0.107), (17, -0.074), (18, 0.037), (19, 0.054), (20, 0.001), (21, -0.279), (22, -0.113), (23, -0.084), (24, 0.001), (25, 0.037), (26, -0.109), (27, 0.117), (28, 0.066), (29, -0.293), (30, -0.127), (31, -0.076), (32, 0.111), (33, -0.091), (34, -0.104), (35, -0.044), (36, 0.074), (37, -0.196), (38, -0.212), (39, -0.082), (40, 0.174), (41, 0.058), (42, 0.168), (43, -0.055), (44, -0.128), (45, 0.073), (46, -0.005), (47, -0.042), (48, -0.161), (49, -0.098)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95141172 <a title="69-lsi-1" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>Author: Charles Dugas, Yoshua Bengio, François Bélisle, Claude Nadeau, René Garcia</p><p>Abstract: Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in two of its arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of continuous functions with these and other properties. We apply this new class of functions to the task of modeling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori constraints.</p><p>2 0.57941031 <a title="69-lsi-2" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>Author: Anders Jonsson, Andrew G. Barto</p><p>Abstract: Learning a complex task can be significantly facilitated by defining a hierarchy of subtasks. An agent can learn to choose between various temporally abstract actions, each solving an assigned subtask, to accomplish the overall task. In this paper, we study hierarchical learning using the framework of options. We argue that to take full advantage of hierarchical structure, one should perform option-specific state abstraction, and that if this is to scale to larger tasks, state abstraction should be automated. We adapt McCallum's U-Tree algorithm to automatically build option-specific representations of the state feature space, and we illustrate the resulting algorithm using a simple hierarchical task. Results suggest that automated option-specific state abstraction is an attractive approach to making hierarchical learning systems more effective.</p><p>3 0.34844798 <a title="69-lsi-3" href="./nips-2000-A_Variational_Mean-Field_Theory_for_Sigmoidal_Belief_Networks.html">14 nips-2000-A Variational Mean-Field Theory for Sigmoidal Belief Networks</a></p>
<p>Author: Chiranjib Bhattacharyya, S. Sathiya Keerthi</p><p>Abstract: A variational derivation of Plefka's mean-field theory is presented. This theory is then applied to sigmoidal belief networks with the aid of further approximations. Empirical evaluation on small scale networks show that the proposed approximations are quite competitive. 1</p><p>4 0.34052098 <a title="69-lsi-4" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>Author: Igor V. Cadez, Padhraic Smyth</p><p>Abstract: We investigate a general characteristic of the trade-off in learning problems between goodness-of-fit and model complexity. Specifically we characterize a general class of learning problems where the goodness-of-fit function can be shown to be convex within firstorder as a function of model complexity. This general property of</p><p>5 0.32249355 <a title="69-lsi-5" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>Author: Jakob Carlström</p><p>Abstract: This paper presents predictive gain scheduling, a technique for simplifying reinforcement learning problems by decomposition. Link admission control of self-similar call traffic is used to demonstrate the technique. The control problem is decomposed into on-line prediction of near-future call arrival rates, and precomputation of policies for Poisson call arrival processes. At decision time, the predictions are used to select among the policies. Simulations show that this technique results in significantly faster learning without any performance loss, compared to a reinforcement learning controller that does not decompose the problem. 1</p><p>6 0.31761917 <a title="69-lsi-6" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>7 0.30291322 <a title="69-lsi-7" href="./nips-2000-Second_Order_Approximations_for_Probability_Models.html">114 nips-2000-Second Order Approximations for Probability Models</a></p>
<p>8 0.29584157 <a title="69-lsi-8" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>9 0.27324358 <a title="69-lsi-9" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>10 0.2503753 <a title="69-lsi-10" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>11 0.24659246 <a title="69-lsi-11" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>12 0.23979731 <a title="69-lsi-12" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>13 0.23532932 <a title="69-lsi-13" href="./nips-2000-Algorithmic_Stability_and_Generalization_Performance.html">21 nips-2000-Algorithmic Stability and Generalization Performance</a></p>
<p>14 0.23307949 <a title="69-lsi-14" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>15 0.23256177 <a title="69-lsi-15" href="./nips-2000-Computing_with_Finite_and_Infinite_Networks.html">35 nips-2000-Computing with Finite and Infinite Networks</a></p>
<p>16 0.23146895 <a title="69-lsi-16" href="./nips-2000-Some_New_Bounds_on_the_Generalization_Error_of_Combined_Classifiers.html">119 nips-2000-Some New Bounds on the Generalization Error of Combined Classifiers</a></p>
<p>17 0.22174469 <a title="69-lsi-17" href="./nips-2000-Periodic_Component_Analysis%3A_An_Eigenvalue_Method_for_Representing_Periodic_Structure_in_Speech.html">99 nips-2000-Periodic Component Analysis: An Eigenvalue Method for Representing Periodic Structure in Speech</a></p>
<p>18 0.2201726 <a title="69-lsi-18" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>19 0.2195625 <a title="69-lsi-19" href="./nips-2000-Who_Does_What%3F_A_Novel_Algorithm_to_Determine_Function_Localization.html">147 nips-2000-Who Does What? A Novel Algorithm to Determine Function Localization</a></p>
<p>20 0.21629521 <a title="69-lsi-20" href="./nips-2000-A_Neural_Probabilistic_Language_Model.html">6 nips-2000-A Neural Probabilistic Language Model</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.05), (17, 0.081), (26, 0.011), (32, 0.018), (33, 0.061), (55, 0.031), (62, 0.069), (65, 0.025), (67, 0.09), (68, 0.303), (75, 0.016), (76, 0.041), (79, 0.013), (81, 0.029), (90, 0.045), (91, 0.023), (97, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80298811 <a title="69-lda-1" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>Author: Charles Dugas, Yoshua Bengio, François Bélisle, Claude Nadeau, René Garcia</p><p>Abstract: Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in two of its arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of continuous functions with these and other properties. We apply this new class of functions to the task of modeling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori constraints.</p><p>2 0.79959548 <a title="69-lda-2" href="./nips-2000-Structure_Learning_in_Human_Causal_Induction.html">127 nips-2000-Structure Learning in Human Causal Induction</a></p>
<p>Author: Joshua B. Tenenbaum, Thomas L. Griffiths</p><p>Abstract: We use graphical models to explore the question of how people learn simple causal relationships from data. The two leading psychological theories can both be seen as estimating the parameters of a fixed graph. We argue that a complete account of causal induction should also consider how people learn the underlying causal graph structure, and we propose to model this inductive process as a Bayesian inference. Our argument is supported through the discussion of three data sets.</p><p>3 0.45325363 <a title="69-lda-3" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass, Eduardo D. Sontag, Anthony M. Zador</p><p>Abstract: Experimental data show that biological synapses behave quite differently from the symbolic synapses in common artificial neural network models. Biological synapses are dynamic, i.e., their</p><p>4 0.45266584 <a title="69-lda-4" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>Author: Zoubin Ghahramani, Matthew J. Beal</p><p>Abstract: Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set. 1</p><p>5 0.44818023 <a title="69-lda-5" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>Author: Claudio Gentile</p><p>Abstract: A new incremental learning algorithm is described which approximates the maximal margin hyperplane w.r.t. norm p ~ 2 for a set of linearly separable data. Our algorithm, called ALMAp (Approximate Large Margin algorithm w.r.t. norm p), takes 0 ((P~21;;2) corrections to separate the data with p-norm margin larger than (1 - 0:) ,,(, where,,( is the p-norm margin of the data and X is a bound on the p-norm of the instances. ALMAp avoids quadratic (or higher-order) programming methods. It is very easy to implement and is as fast as on-line algorithms, such as Rosenblatt's perceptron. We report on some experiments comparing ALMAp to two incremental algorithms: Perceptron and Li and Long's ROMMA. Our algorithm seems to perform quite better than both. The accuracy levels achieved by ALMAp are slightly inferior to those obtained by Support vector Machines (SVMs). On the other hand, ALMAp is quite faster and easier to implement than standard SVMs training algorithms.</p><p>6 0.44750279 <a title="69-lda-6" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>7 0.44721904 <a title="69-lda-7" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>8 0.44488004 <a title="69-lda-8" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>9 0.44146258 <a title="69-lda-9" href="./nips-2000-The_Kernel_Trick_for_Distances.html">134 nips-2000-The Kernel Trick for Distances</a></p>
<p>10 0.44133142 <a title="69-lda-10" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>11 0.44086435 <a title="69-lda-11" href="./nips-2000-Algorithms_for_Non-negative_Matrix_Factorization.html">22 nips-2000-Algorithms for Non-negative Matrix Factorization</a></p>
<p>12 0.44045198 <a title="69-lda-12" href="./nips-2000-Regularized_Winnow_Methods.html">111 nips-2000-Regularized Winnow Methods</a></p>
<p>13 0.43924072 <a title="69-lda-13" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>14 0.43845469 <a title="69-lda-14" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>15 0.43799397 <a title="69-lda-15" href="./nips-2000-Algorithmic_Stability_and_Generalization_Performance.html">21 nips-2000-Algorithmic Stability and Generalization Performance</a></p>
<p>16 0.4372271 <a title="69-lda-16" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>17 0.43648523 <a title="69-lda-17" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>18 0.43435863 <a title="69-lda-18" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>19 0.43137151 <a title="69-lda-19" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>20 0.43043885 <a title="69-lda-20" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
