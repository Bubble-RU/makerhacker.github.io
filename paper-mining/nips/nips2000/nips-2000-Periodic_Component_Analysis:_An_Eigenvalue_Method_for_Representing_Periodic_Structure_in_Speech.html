<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>99 nips-2000-Periodic Component Analysis: An Eigenvalue Method for Representing Periodic Structure in Speech</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-99" href="#">nips2000-99</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>99 nips-2000-Periodic Component Analysis: An Eigenvalue Method for Representing Periodic Structure in Speech</h1>
<br/><p>Source: <a title="nips-2000-99-pdf" href="http://papers.nips.cc/paper/1939-periodic-component-analysis-an-eigenvalue-method-for-representing-periodic-structure-in-speech.pdf">pdf</a></p><p>Author: Lawrence K. Saul, Jont B. Allen</p><p>Abstract: An eigenvalue method is developed for analyzing periodic structure in speech. Signals are analyzed by a matrix diagonalization reminiscent of methods for principal component analysis (PCA) and independent component analysis (ICA). Our method-called periodic component analysis (1l</p><p>Reference: <a title="nips-2000-99-reference" href="../nips2000_reference/nips-2000-Periodic_Component_Analysis%3A_An_Eigenvalue_Method_for_Representing_Periodic_Structure_in_Speech_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com AT&T; Labs, 180 Park Ave, Florham Park, NJ 07932  Abstract An eigenvalue method is developed for analyzing periodic structure in speech. [sent-5, score-0.459]
</p><p>2 Our method-called periodic component analysis (1l"CA)-uses constructive interference to enhance periodic components of the frequency spectrum and destructive interference to cancel noise. [sent-7, score-1.091]
</p><p>3 The front end emulates important aspects of auditory processing, such as cochlear filtering, nonlinear compression, and insensitivity to phase, with the aim of approaching the robustness of human listeners. [sent-8, score-0.467]
</p><p>4 The method avoids the inefficiencies of autocorrelation at the pitch period: it does not require long delay lines, and it correlates signals at a clock rate on the order of the actual pitch, as opposed to the original sampling rate. [sent-9, score-1.069]
</p><p>5 At the end of an English sentence, for example, rising versus falling pitch indicates the asking of a question; in tonal languages, such as Chinese, it carries linguistic information. [sent-12, score-0.651]
</p><p>6 In fact, early in the speech chain-prior to the recognition of words or the assignment of meaning-the auditory system divides the frequency spectrum into periodic and non-periodic components. [sent-13, score-0.81]
</p><p>7 Thus, a voiced fricative might be identified by the presence of periodicity in the lower part of the spectrum, but not the upper part. [sent-15, score-0.257]
</p><p>8 In complicated auditory scenes, periodic components of the spectrum are further segregated by their fundamental frequency [3 ]. [sent-16, score-0.736]
</p><p>9 The pitch and voicing of speech signals have been extensively studied[5]. [sent-18, score-0.864]
</p><p>10 The simplest method to analyze periodicity is to compute the autocorrelation function on sliding windows of the speech waveform. [sent-19, score-0.531]
</p><p>11 The peaks in the autocorrelation function provide estimates of the pitch and the degree of voicing. [sent-20, score-0.769]
</p><p>12 In clean wideband speech, the pitch of a speaker can be tracked by combining a peak-picking procedure on the autocorrelation function with some form of smoothing[6], such as dynamic programming. [sent-21, score-0.884]
</p><p>13 This method, however,  does not approach the robustness of human listeners in noise, and at best, it provides an extremely gross picture of the periodic structure in speech. [sent-22, score-0.381]
</p><p>14 It cannot serve as a basis for attacking harder problems in computational auditory scene analysis, such as speaker separation[7], which require decomposing the frequency spectrum into its periodic and non-periodic components. [sent-23, score-0.796]
</p><p>15 The correlogram is a more powerful method for analyzing periodic structure in speech. [sent-24, score-0.412]
</p><p>16 Slaney and Lyon[8] proposed a perceptual pitch detector that autocorrelates multichannel output from a model of the auditory periphery. [sent-26, score-0.842]
</p><p>17 The auditory model includes a cochlear filterbank and periodicity-enhancing nonlinearities. [sent-27, score-0.326]
</p><p>18 The information in the correlogram is summed over channels to produce an estimate of the pitch. [sent-28, score-0.199]
</p><p>19 This method has two compelling features: (i) by measuring autocorrelation, it produces pitch estimates that are insensitive to phase changes across channels; (ii) by working in narrow frequency bands, it produces estimates that are robust to noise. [sent-29, score-1.086]
</p><p>20 To avoid aliasing in upper frequency bands, signals must be correlated at clock rates much higher than the actual pitch. [sent-32, score-0.374]
</p><p>21 From a theoretical point of view, it is unsatisfying that the combination of information across channels is not derived from some principle of optimality. [sent-33, score-0.214]
</p><p>22 Finally, in the absence of conclusive evidence for long delay lines (~1O ms) in the peripheral auditory system, it seems worthwhile-for both scientists and engineers-to study ways of detecting periodicity that do not depend on autocorrelation. [sent-34, score-0.306]
</p><p>23 In this paper, we develop an eigenvalue method for analyzing periodic structure in speech. [sent-35, score-0.459]
</p><p>24 Our method emulates important aspects of auditory processing but avoids the inefficiencies of autocorrelation at the pitch period. [sent-36, score-0.961]
</p><p>25 At the same time, it is highly robust to narrowband noise and insensitive to phase changes across channels. [sent-37, score-0.209]
</p><p>26 1  Cross-correlation of critical bands  Consider the multichannel output of a cochlear filterbank. [sent-42, score-0.235]
</p><p>27 If the input to this filterbank consists of noisy voiced speech, the output will consist of weakly periodic signals from different critical bands. [sent-43, score-0.64]
</p><p>28 Can we combine these signals to enhance the periodic signature of the speaker's pitch? [sent-44, score-0.549]
</p><p>29 Given n real-valued signals, {xi(t)}i=l' what linear combination s(t) = Li WiXi(t) maximizes the periodic structure at some fundamental frequency fa, or equivalently, at some pitch period T = 1/ fa? [sent-46, score-1.24]
</p><p>30 Ideally, the linear combination should use constructive interference to enhance periodic components of the spectrum and destructi ve interference to cancel noise. [sent-47, score-0.595]
</p><p>31 We measure the periodicity of the combined signal by the cost function:  (  c w, T  ) _ Lt Is(t + T) - s(tW Lt Is(t)12  with  s(t) =  L WiXi(t). [sent-48, score-0.297]
</p><p>32 (1)  Here, for simplicity, we have assumed that the signals are discretely sampled and that the period T is an integer multiple of the sampling interval. [sent-49, score-0.247]
</p><p>33 Our methodwhich by analogy we call periodic component analysis (1I'cA)-uses an eigenvalue principle to combine periodicity cues from different parts of the frequency spectrum. [sent-60, score-0.865]
</p><p>34 2 Insensitivity to phase The eigenvalue method in the previous section has one obvious shortcoming: it cannot compensate for phase changes across channels. [sent-62, score-0.331]
</p><p>35 In particular, the real-valued linear combination 8(t) = L i WiX;(t) cannot align the peaks of signals that are (say) 11'/2 radians out of phase, even though such an alignment-prior to combining the signals-would significantly reduce the normalized prediction error in eq. [sent-63, score-0.145]
</p><p>36 The Fourier series of these signals are related by:  X;(t) =  L D:k COS(Wkt + ¢k)  ¢:::::>  x;(t)  =L  k  D:k e;(Wkt+¢k). [sent-67, score-0.145]
</p><p>37 In this setting, moreover, we allow the weights W; to be complex so that they can compensate for phase changes across channels. [sent-70, score-0.17]
</p><p>38 Again, the optimal weights W; are given by the eigenvector corresponding to the smallest eigenvalue of the matrix B- 1 A (7). [sent-73, score-0.17]
</p><p>39 ) Our analysis so far suggests a simple-minded approach to investigating periodic structure in speech. [sent-75, score-0.348]
</p><p>40 In particular, consider the following algorithm for pitch tracking. [sent-76, score-0.606]
</p><p>41 The first step of the algorithm is to pass speech through a cochlear filterbank and compute analytic  signals, Xi (t), via Hilbert transforms. [sent-77, score-0.409]
</p><p>42 The next step is to diagonalize the matrices B- 1A(T) on sliding windows of Xi(t) over a range of pitch periods, T E [Tmin, Tmaxl. [sent-78, score-0.767]
</p><p>43 The final step is to estimate the pitch periods by the values of T that minimize the cost function, eq. [sent-79, score-0.681]
</p><p>44 One might expect such an algorithm to be relatively robust to noise (because it can zero the weights of corrupted channels), as well as insensitive to phase changes across channels (because it can absorb them with complex weights). [sent-81, score-0.375]
</p><p>45 Its worst shortcoming is the amount of computation needed to estimate the pitch period, T. [sent-83, score-0.606]
</p><p>46 This step is unwieldy for three reasons: (i) the burden of recomputing cross-correlations for different values of T, (ii) the high sampling rates required to avoid aliasing in upper frequency bands, and (iii) the poor scaling with the number of channels, n. [sent-85, score-0.247]
</p><p>47 3 Extracting the fundamental Further signal processing is required to create multichannel output whose periodic structure can be analyzed more efficiently. [sent-88, score-0.463]
</p><p>48 1, is designed to analyze voiced speech with fundamental frequencies in the range fa E [fmin, fmax] , where fmax < 2fmin. [sent-90, score-0.382]
</p><p>49 The one-octave restriction on fa can be lifted by considering parallel, overlapping implementations of our front end for different frequency octaves. [sent-91, score-0.38]
</p><p>50 The stages in our front end are inspired by important aspects of auditory processing[lO]. [sent-92, score-0.33]
</p><p>51 Cochlear filtering is modeled by a Bark scale filterbank with contiguous passbands. [sent-93, score-0.194]
</p><p>52 Evidence for comparison of envelopes in the peripheral auditory system comes from experiments on comodulation masking release[ll]. [sent-96, score-0.179]
</p><p>53 Thus, the next stage of our front end creates a multichannel array of signals by pairwise multiplying envelopes from nearby parts of the frequency spectrum. [sent-97, score-0.74]
</p><p>54 The energy at the difference frequency creates a signature of "residue" pitch at fa. [sent-100, score-0.874]
</p><p>55 The energy at the sum frequency is removed by bandpass filtering to frequencies [fmin'! [sent-101, score-0.265]
</p><p>56 In sum, the stages of the front end create an array of bandlimited analytic signals, Xi (t), that-while derived from different parts of the frequency spectrum-have energy concentrated at the fundamental frequency, fa. [sent-104, score-0.625]
</p><p>57 Note that the bandlimiting of these channels to frequencies [fmin, fmax] where fmax <2fmin removes the possibility that a channel contains periodic energy at any harmonic other than the fundamental. [sent-105, score-0.723]
</p><p>58 speech, this has the effect that periodic channels contain noisy sine waves with frequency fa. [sent-107, score-0.765]
</p><p>59 speech waveform  cochlear filterbank  half-wave rectification; cube-root compression ~----~  Q x8. [sent-108, score-0.361]
</p><p>60 /'0,  X  pairwise multiplication  bandlimiting; downsampling  ~----~  Figure 1: Signal processing in the front end. [sent-109, score-0.183]
</p><p>61 =:  compute analytic signals  How can we combine these "baseband" signals to enhance the periodic signature of a speaker's pitch? [sent-110, score-0.746]
</p><p>62 The nature of these signals leads to an important simplification of the problem. [sent-111, score-0.145]
</p><p>63 As opposed to measuring the autocorrelation at lag T, as in eq. [sent-112, score-0.196]
</p><p>64 (1), here we can measure the periodicity of the combined signal by a simple sinusoidalfit. [sent-113, score-0.254]
</p><p>65 denote the phase accumulated per sample by a sine wave with frequency fo at sampling rate f. [sent-115, score-0.434]
</p><p>66 t  For fixed ~, the optimal weights Wi are given by the eigenvector corresponding to the smallest eigenvalue of the matrix B- 1 A( ~). [sent-118, score-0.17]
</p><p>67 (5) over the phase, ~, is equivalent to optimizing over the fundamental frequency, fo, or the pitch period, T. [sent-120, score-0.693]
</p><p>68 The structure of this cost function makes it much easier to optimize than the earlier measure of periodicity in eq. [sent-121, score-0.267]
</p><p>69 Also, the channels Xi(t) appearing in this cost function are sampled at a clock rate on the order of fo, as opposed to the original sampling rate of the speech. [sent-124, score-0.359]
</p><p>70 The improved algorithm, working with baseband signals, estimates the pitch by optimizing eq. [sent-127, score-0.72]
</p><p>71 The recursive algorithm works by constructing and diagonalizing 2 x 2 matrices, as opposed to the n x n matrices required for an exact solution. [sent-134, score-0.154]
</p><p>72 Our approximate algorithm also provides a hierarchical analysis of the frequency spectrum that is interesting in its own right. [sent-135, score-0.302]
</p><p>73 The recursive step of the algorithm takes as input two auditory "substreams", Sl(t) and su(t), derived from "lower" and "upper" parts of the frequency spectrum, and returns as output a single combined stream, s(t) = WISI(t) + wusu(t). [sent-140, score-0.496]
</p><p>74 In the first step  Figure 2: Measures of pitch (fo) and periodicity (e l ) in nested regions of the frequency spectrum. [sent-141, score-1.01]
</p><p>75 The nodes in this tree describe periodic structure in the vowel luI from 4001080 Hz. [sent-142, score-0.437]
</p><p>76 The nodes in the first (bottom) layer describe periodicity cues in individual channels; the nodes in the kth layer measure cues integrated across 2k - l channels. [sent-143, score-0.328]
</p><p>77 of the recursion, the substreams correspond to individual channels Xi (t), while in the kth step, they correspond to weighted combinations of 2k - l channels. [sent-144, score-0.223]
</p><p>78 Associated with the substreams are phases, ~I and ~t" corresponding to estimates of fo from different parts of the frequency spectrum. [sent-145, score-0.396]
</p><p>79 Note that the eigenvalue problem in this case involves only a 2 x 2 matrix, as opposed to an n x n matrix. [sent-148, score-0.146]
</p><p>80 Channels are combined pairwise to form streams, which are in turn combined pairwise to form new streams. [sent-152, score-0.206]
</p><p>81 Each stream has a pitch period and a measure of periodicity computed by optimizing eq. [sent-153, score-0.955]
</p><p>82 We order the channels so that streams are derived from contiguous (or nearly contiguous) parts of the frequency spectrum. [sent-155, score-0.473]
</p><p>83 Note how as one ascends the tree, the combined streams have greater periodicity and less variance in their pitch estimates. [sent-158, score-0.895]
</p><p>84 This shows explicitly how the algorithm integrates information across narrow frequency bands of speech. [sent-159, score-0.355]
</p><p>85 The recursive output also suggests a useful representation for studying problems, such as speaker separation, that depend on grouping different parts of the spectrum by their estimates of fo. [sent-160, score-0.292]
</p><p>86 The pitch contours in these plots were computed by the recursive algorithm in the previous section, with f min = 80 Hz, fmax = 140 Hz, and 60 ms windows shifted in 10 ms intervals. [sent-164, score-0.91]
</p><p>87 The solid curves show the estimated pitch contour for the clean wideband waveform, sampled at 8 kHz. [sent-165, score-0.69]
</p><p>88 The left panel shows results for filtered versions of the vowel, bandlimited to four different frequency octaves. [sent-166, score-0.256]
</p><p>89 These plots show that the algorithm can extract the pitch from different parts of the frequency spectrum. [sent-167, score-0.839]
</p><p>90 The right panel shows the estimated pitch contours for the vowel in 0 dB white noise and four types of -20 dB bandlimited noise. [sent-168, score-0.848]
</p><p>91 The signal-to-noise ratios were computed from the ratio of (wideband) speech energy to noise energy. [sent-169, score-0.198]
</p><p>92 The white noise at 0 dB presents the most difficulty; by contrast, the bandlimited noise leads to relatively few failures , even at -20 dB. [sent-170, score-0.158]
</p><p>93 (Note that the particular frequency octaves used in these experiments had no special relation to the filters in our front end. [sent-172, score-0.284]
</p><p>94 ) The pitch contours could be further improved by some form of smoothing, but this was not done for the plots shown. [sent-173, score-0.642]
</p><p>95 bandhmlted speech  noisy speech  130 l--~--~-r=======il  wide band - 0500 - 1000 - 2000 - 4000  0250 0500 1000 2000  125 120  90  clean  Hz Hz Hz Hz  ~-----=-'-::----=-. [sent-174, score-0.261]
</p><p>96 ~ 06 0~ time (sec)  Figure 3: Tracking the pitch of the vowel lui in corrupted speech. [sent-182, score-0.695]
</p><p>97 Perhaps the most important is the initial filtering into narrow frequency bands. [sent-184, score-0.287]
</p><p>98 While narrow filters have the ability to resolve individual harmonics, overly narrow filters-which reduce all speech input to sine waves~o not adequately differentiate periodic versus noisy excitation. [sent-185, score-0.626]
</p><p>99 Finally, beyond the problem of pitch tracking, we intend to develop the hierarchical representation shown in Fig. [sent-189, score-0.641]
</p><p>100 These harder problems seem to require a method, like ours, that decomposes the frequency spectrum into its periodic and non-periodic components. [sent-191, score-0.618]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pitch', 0.606), ('periodic', 0.316), ('periodicity', 0.192), ('frequency', 0.18), ('channels', 0.166), ('signals', 0.145), ('autocorrelation', 0.13), ('auditory', 0.114), ('filterbank', 0.114), ('fmax', 0.114), ('speech', 0.113), ('front', 0.104), ('hz', 0.099), ('cochlear', 0.098), ('vowel', 0.089), ('spectrum', 0.087), ('phase', 0.081), ('xj', 0.081), ('eigenvalue', 0.08), ('bandlimited', 0.076), ('fmin', 0.076), ('multichannel', 0.076), ('fo', 0.073), ('db', 0.071), ('wi', 0.07), ('period', 0.067), ('opposed', 0.066), ('narrow', 0.066), ('envelopes', 0.065), ('voiced', 0.065), ('sine', 0.065), ('lt', 0.064), ('speaker', 0.064), ('combined', 0.062), ('bands', 0.061), ('sliding', 0.059), ('xi', 0.058), ('diagonalization', 0.057), ('substreams', 0.057), ('recursive', 0.055), ('interference', 0.055), ('parts', 0.053), ('analytic', 0.052), ('fa', 0.051), ('wideband', 0.049), ('clock', 0.049), ('across', 0.048), ('optimizing', 0.048), ('matrix', 0.046), ('perceptual', 0.046), ('channel', 0.045), ('end', 0.045), ('signature', 0.044), ('cues', 0.044), ('aij', 0.044), ('eigenvector', 0.044), ('enhance', 0.044), ('energy', 0.044), ('cost', 0.043), ('stream', 0.042), ('noise', 0.041), ('compensate', 0.041), ('filtering', 0.041), ('separation', 0.041), ('pairwise', 0.041), ('hilbert', 0.039), ('fundamental', 0.039), ('insensitive', 0.039), ('contiguous', 0.039), ('bandlimiting', 0.038), ('constructive', 0.038), ('downsampling', 0.038), ('emulates', 0.038), ('inefficiencies', 0.038), ('waves', 0.038), ('wkt', 0.038), ('windows', 0.037), ('contours', 0.036), ('waveform', 0.036), ('aspects', 0.035), ('sampling', 0.035), ('hierarchical', 0.035), ('harder', 0.035), ('clean', 0.035), ('streams', 0.035), ('estimates', 0.033), ('tracking', 0.033), ('baseband', 0.033), ('diagonalizing', 0.033), ('insensitivity', 0.033), ('listeners', 0.033), ('correlogram', 0.033), ('diagonalize', 0.033), ('structure', 0.032), ('step', 0.032), ('stages', 0.032), ('recursion', 0.032), ('multiplying', 0.031), ('ms', 0.031), ('analyzing', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="99-tfidf-1" href="./nips-2000-Periodic_Component_Analysis%3A_An_Eigenvalue_Method_for_Representing_Periodic_Structure_in_Speech.html">99 nips-2000-Periodic Component Analysis: An Eigenvalue Method for Representing Periodic Structure in Speech</a></p>
<p>Author: Lawrence K. Saul, Jont B. Allen</p><p>Abstract: An eigenvalue method is developed for analyzing periodic structure in speech. Signals are analyzed by a matrix diagonalization reminiscent of methods for principal component analysis (PCA) and independent component analysis (ICA). Our method-called periodic component analysis (1l</p><p>2 0.1691954 <a title="99-tfidf-2" href="./nips-2000-Noise_Suppression_Based_on_Neurophysiologically-motivated_SNR_Estimation_for_Robust_Speech_Recognition.html">91 nips-2000-Noise Suppression Based on Neurophysiologically-motivated SNR Estimation for Robust Speech Recognition</a></p>
<p>Author: Jürgen Tchorz, Michael Kleinschmidt, Birger Kollmeier</p><p>Abstract: A novel noise suppression scheme for speech signals is proposed which is based on a neurophysiologically-motivated estimation of the local signal-to-noise ratio (SNR) in different frequency channels. For SNR-estimation, the input signal is transformed into so-called Amplitude Modulation Spectrograms (AMS), which represent both spectral and temporal characteristics of the respective analysis frame, and which imitate the representation of modulation frequencies in higher stages of the mammalian auditory system. A neural network is used to analyse AMS patterns generated from noisy speech and estimates the local SNR. Noise suppression is achieved by attenuating frequency channels according to their SNR. The noise suppression algorithm is evaluated in speakerindependent digit recognition experiments and compared to noise suppression by Spectral Subtraction. 1</p><p>3 0.15032962 <a title="99-tfidf-3" href="./nips-2000-One_Microphone_Source_Separation.html">96 nips-2000-One Microphone Source Separation</a></p>
<p>Author: Sam T. Roweis</p><p>Abstract: Source separation, or computational auditory scene analysis , attempts to extract individual acoustic objects from input which contains a mixture of sounds from different sources, altered by the acoustic environment. Unmixing algorithms such as lCA and its extensions recover sources by reweighting multiple observation sequences, and thus cannot operate when only a single observation signal is available. I present a technique called refiltering which recovers sources by a nonstationary reweighting (</p><p>4 0.12828471 <a title="99-tfidf-4" href="./nips-2000-Speech_Denoising_and_Dereverberation_Using_Probabilistic_Models.html">123 nips-2000-Speech Denoising and Dereverberation Using Probabilistic Models</a></p>
<p>Author: Hagai Attias, John C. Platt, Alex Acero, Li Deng</p><p>Abstract: This paper presents a unified probabilistic framework for denoising and dereverberation of speech signals. The framework transforms the denoising and dereverberation problems into Bayes-optimal signal estimation. The key idea is to use a strong speech model that is pre-trained on a large data set of clean speech. Computational efficiency is achieved by using variational EM, working in the frequency domain, and employing conjugate priors. The framework covers both single and multiple microphones. We apply this approach to noisy reverberant speech signals and get results substantially better than standard methods.</p><p>5 0.10978433 <a title="99-tfidf-5" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>Author: Lucas C. Parra, Clay Spence, Paul Sajda</p><p>Abstract: We present evidence that several higher-order statistical properties of natural images and signals can be explained by a stochastic model which simply varies scale of an otherwise stationary Gaussian process. We discuss two interesting consequences. The first is that a variety of natural signals can be related through a common model of spherically invariant random processes, which have the attractive property that the joint densities can be constructed from the one dimensional marginal. The second is that in some cases the non-stationarity assumption and only second order methods can be explicitly exploited to find a linear basis that is equivalent to independent components obtained with higher-order methods. This is demonstrated on spectro-temporal components of speech. 1</p><p>6 0.10654561 <a title="99-tfidf-6" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>7 0.10375071 <a title="99-tfidf-7" href="./nips-2000-New_Approaches_Towards_Robust_and_Adaptive_Speech_Recognition.html">90 nips-2000-New Approaches Towards Robust and Adaptive Speech Recognition</a></p>
<p>8 0.080876447 <a title="99-tfidf-8" href="./nips-2000-Learning_Joint_Statistical_Models_for_Audio-Visual_Fusion_and_Segregation.html">78 nips-2000-Learning Joint Statistical Models for Audio-Visual Fusion and Segregation</a></p>
<p>9 0.073005632 <a title="99-tfidf-9" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>10 0.068860762 <a title="99-tfidf-10" href="./nips-2000-Combining_ICA_and_Top-Down_Attention_for_Robust_Speech_Recognition.html">33 nips-2000-Combining ICA and Top-Down Attention for Robust Speech Recognition</a></p>
<p>11 0.06638632 <a title="99-tfidf-11" href="./nips-2000-Factored_Semi-Tied_Covariance_Matrices.html">51 nips-2000-Factored Semi-Tied Covariance Matrices</a></p>
<p>12 0.063864782 <a title="99-tfidf-12" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>13 0.062741742 <a title="99-tfidf-13" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>14 0.060956992 <a title="99-tfidf-14" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>15 0.058626954 <a title="99-tfidf-15" href="./nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">88 nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>16 0.053139016 <a title="99-tfidf-16" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>17 0.050525729 <a title="99-tfidf-17" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>18 0.047748022 <a title="99-tfidf-18" href="./nips-2000-Minimum_Bayes_Error_Feature_Selection_for_Continuous_Speech_Recognition.html">84 nips-2000-Minimum Bayes Error Feature Selection for Continuous Speech Recognition</a></p>
<p>19 0.047136575 <a title="99-tfidf-19" href="./nips-2000-Competition_and_Arbors_in_Ocular_Dominance.html">34 nips-2000-Competition and Arbors in Ocular Dominance</a></p>
<p>20 0.04655695 <a title="99-tfidf-20" href="./nips-2000-Sparse_Kernel_Principal_Component_Analysis.html">121 nips-2000-Sparse Kernel Principal Component Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.189), (1, -0.112), (2, 0.059), (3, 0.157), (4, -0.071), (5, -0.097), (6, -0.235), (7, -0.064), (8, -0.001), (9, 0.042), (10, 0.023), (11, 0.067), (12, -0.029), (13, -0.043), (14, 0.087), (15, -0.053), (16, 0.039), (17, 0.022), (18, 0.054), (19, -0.097), (20, -0.051), (21, 0.073), (22, -0.099), (23, -0.007), (24, -0.013), (25, 0.09), (26, -0.011), (27, 0.024), (28, -0.074), (29, -0.117), (30, 0.073), (31, -0.099), (32, 0.02), (33, -0.074), (34, -0.054), (35, 0.023), (36, -0.09), (37, 0.017), (38, -0.014), (39, 0.008), (40, 0.011), (41, 0.084), (42, 0.146), (43, 0.019), (44, 0.147), (45, 0.092), (46, 0.017), (47, 0.056), (48, -0.126), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96463317 <a title="99-lsi-1" href="./nips-2000-Periodic_Component_Analysis%3A_An_Eigenvalue_Method_for_Representing_Periodic_Structure_in_Speech.html">99 nips-2000-Periodic Component Analysis: An Eigenvalue Method for Representing Periodic Structure in Speech</a></p>
<p>Author: Lawrence K. Saul, Jont B. Allen</p><p>Abstract: An eigenvalue method is developed for analyzing periodic structure in speech. Signals are analyzed by a matrix diagonalization reminiscent of methods for principal component analysis (PCA) and independent component analysis (ICA). Our method-called periodic component analysis (1l</p><p>2 0.73360002 <a title="99-lsi-2" href="./nips-2000-Noise_Suppression_Based_on_Neurophysiologically-motivated_SNR_Estimation_for_Robust_Speech_Recognition.html">91 nips-2000-Noise Suppression Based on Neurophysiologically-motivated SNR Estimation for Robust Speech Recognition</a></p>
<p>Author: Jürgen Tchorz, Michael Kleinschmidt, Birger Kollmeier</p><p>Abstract: A novel noise suppression scheme for speech signals is proposed which is based on a neurophysiologically-motivated estimation of the local signal-to-noise ratio (SNR) in different frequency channels. For SNR-estimation, the input signal is transformed into so-called Amplitude Modulation Spectrograms (AMS), which represent both spectral and temporal characteristics of the respective analysis frame, and which imitate the representation of modulation frequencies in higher stages of the mammalian auditory system. A neural network is used to analyse AMS patterns generated from noisy speech and estimates the local SNR. Noise suppression is achieved by attenuating frequency channels according to their SNR. The noise suppression algorithm is evaluated in speakerindependent digit recognition experiments and compared to noise suppression by Spectral Subtraction. 1</p><p>3 0.62151325 <a title="99-lsi-3" href="./nips-2000-Speech_Denoising_and_Dereverberation_Using_Probabilistic_Models.html">123 nips-2000-Speech Denoising and Dereverberation Using Probabilistic Models</a></p>
<p>Author: Hagai Attias, John C. Platt, Alex Acero, Li Deng</p><p>Abstract: This paper presents a unified probabilistic framework for denoising and dereverberation of speech signals. The framework transforms the denoising and dereverberation problems into Bayes-optimal signal estimation. The key idea is to use a strong speech model that is pre-trained on a large data set of clean speech. Computational efficiency is achieved by using variational EM, working in the frequency domain, and employing conjugate priors. The framework covers both single and multiple microphones. We apply this approach to noisy reverberant speech signals and get results substantially better than standard methods.</p><p>4 0.56443912 <a title="99-lsi-4" href="./nips-2000-One_Microphone_Source_Separation.html">96 nips-2000-One Microphone Source Separation</a></p>
<p>Author: Sam T. Roweis</p><p>Abstract: Source separation, or computational auditory scene analysis , attempts to extract individual acoustic objects from input which contains a mixture of sounds from different sources, altered by the acoustic environment. Unmixing algorithms such as lCA and its extensions recover sources by reweighting multiple observation sequences, and thus cannot operate when only a single observation signal is available. I present a technique called refiltering which recovers sources by a nonstationary reweighting (</p><p>5 0.54984903 <a title="99-lsi-5" href="./nips-2000-New_Approaches_Towards_Robust_and_Adaptive_Speech_Recognition.html">90 nips-2000-New Approaches Towards Robust and Adaptive Speech Recognition</a></p>
<p>Author: Hervé Bourlard, Samy Bengio, Katrin Weber</p><p>Abstract: In this paper, we discuss some new research directions in automatic speech recognition (ASR), and which somewhat deviate from the usual approaches. More specifically, we will motivate and briefly describe new approaches based on multi-stream and multi/band ASR. These approaches extend the standard hidden Markov model (HMM) based approach by assuming that the different (frequency) channels representing the speech signal are processed by different (independent)</p><p>6 0.54009378 <a title="99-lsi-6" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>7 0.43134534 <a title="99-lsi-7" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>8 0.410759 <a title="99-lsi-8" href="./nips-2000-The_Early_Word_Catches_the_Weights.html">131 nips-2000-The Early Word Catches the Weights</a></p>
<p>9 0.3598434 <a title="99-lsi-9" href="./nips-2000-Four-legged_Walking_Gait_Control_Using_a_Neuromorphic_Chip_Interfaced_to_a_Support_Vector_Learning_Algorithm.html">57 nips-2000-Four-legged Walking Gait Control Using a Neuromorphic Chip Interfaced to a Support Vector Learning Algorithm</a></p>
<p>10 0.33565086 <a title="99-lsi-10" href="./nips-2000-Competition_and_Arbors_in_Ocular_Dominance.html">34 nips-2000-Competition and Arbors in Ocular Dominance</a></p>
<p>11 0.30180809 <a title="99-lsi-11" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>12 0.28387743 <a title="99-lsi-12" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>13 0.28252611 <a title="99-lsi-13" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>14 0.27929512 <a title="99-lsi-14" href="./nips-2000-Learning_Joint_Statistical_Models_for_Audio-Visual_Fusion_and_Segregation.html">78 nips-2000-Learning Joint Statistical Models for Audio-Visual Fusion and Segregation</a></p>
<p>15 0.27398503 <a title="99-lsi-15" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>16 0.26632911 <a title="99-lsi-16" href="./nips-2000-Generalizable_Singular_Value_Decomposition_for_Ill-posed_Datasets.html">61 nips-2000-Generalizable Singular Value Decomposition for Ill-posed Datasets</a></p>
<p>17 0.26261145 <a title="99-lsi-17" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>18 0.2592285 <a title="99-lsi-18" href="./nips-2000-Combining_ICA_and_Top-Down_Attention_for_Robust_Speech_Recognition.html">33 nips-2000-Combining ICA and Top-Down Attention for Robust Speech Recognition</a></p>
<p>19 0.25541472 <a title="99-lsi-19" href="./nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">95 nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>20 0.25225225 <a title="99-lsi-20" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.032), (4, 0.021), (10, 0.039), (17, 0.088), (32, 0.022), (33, 0.052), (42, 0.014), (55, 0.02), (62, 0.026), (65, 0.037), (67, 0.042), (75, 0.012), (76, 0.04), (79, 0.027), (81, 0.038), (90, 0.041), (91, 0.03), (93, 0.316), (97, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96089596 <a title="99-lda-1" href="./nips-2000-Homeostasis_in_a_Silicon_Integrate_and_Fire_Neuron.html">67 nips-2000-Homeostasis in a Silicon Integrate and Fire Neuron</a></p>
<p>Author: Shih-Chii Liu, Bradley A. Minch</p><p>Abstract: In this work, we explore homeostasis in a silicon integrate-and-fire neuron. The neuron adapts its firing rate over long time periods on the order of seconds or minutes so that it returns to its spontaneous firing rate after a lasting perturbation. Homeostasis is implemented via two schemes. One scheme looks at the presynaptic activity and adapts the synaptic weight depending on the presynaptic spiking rate. The second scheme adapts the synaptic</p><p>same-paper 2 0.84562325 <a title="99-lda-2" href="./nips-2000-Periodic_Component_Analysis%3A_An_Eigenvalue_Method_for_Representing_Periodic_Structure_in_Speech.html">99 nips-2000-Periodic Component Analysis: An Eigenvalue Method for Representing Periodic Structure in Speech</a></p>
<p>Author: Lawrence K. Saul, Jont B. Allen</p><p>Abstract: An eigenvalue method is developed for analyzing periodic structure in speech. Signals are analyzed by a matrix diagonalization reminiscent of methods for principal component analysis (PCA) and independent component analysis (ICA). Our method-called periodic component analysis (1l</p><p>3 0.47303888 <a title="99-lda-3" href="./nips-2000-A_Silicon_Primitive_for_Competitive_Learning.html">11 nips-2000-A Silicon Primitive for Competitive Learning</a></p>
<p>Author: David Hsu, Miguel Figueroa, Chris Diorio</p><p>Abstract: Competitive learning is a technique for training classification and clustering networks. We have designed and fabricated an 11transistor primitive, that we term an automaximizing bump circuit, that implements competitive learning dynamics. The circuit performs a similarity computation, affords nonvolatile storage, and implements simultaneous local adaptation and computation. We show that our primitive is suitable for implementing competitive learning in VLSI, and demonstrate its effectiveness in a standard clustering task.</p><p>4 0.44939253 <a title="99-lda-4" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>Author: Blaise Agüera y Arcas, Adrienne L. Fairhall, William Bialek</p><p>Abstract: In this paper we formulate a description of the computation performed by a neuron as a combination of dimensional reduction and nonlinearity. We implement this description for the HodgkinHuxley model, identify the most relevant dimensions and find the nonlinearity. A two dimensional description already captures a significant fraction of the information that spikes carry about dynamic inputs. This description also shows that computation in the Hodgkin-Huxley model is more complex than a simple integrateand-fire or perceptron model. 1</p><p>5 0.41520479 <a title="99-lda-5" href="./nips-2000-Noise_Suppression_Based_on_Neurophysiologically-motivated_SNR_Estimation_for_Robust_Speech_Recognition.html">91 nips-2000-Noise Suppression Based on Neurophysiologically-motivated SNR Estimation for Robust Speech Recognition</a></p>
<p>Author: Jürgen Tchorz, Michael Kleinschmidt, Birger Kollmeier</p><p>Abstract: A novel noise suppression scheme for speech signals is proposed which is based on a neurophysiologically-motivated estimation of the local signal-to-noise ratio (SNR) in different frequency channels. For SNR-estimation, the input signal is transformed into so-called Amplitude Modulation Spectrograms (AMS), which represent both spectral and temporal characteristics of the respective analysis frame, and which imitate the representation of modulation frequencies in higher stages of the mammalian auditory system. A neural network is used to analyse AMS patterns generated from noisy speech and estimates the local SNR. Noise suppression is achieved by attenuating frequency channels according to their SNR. The noise suppression algorithm is evaluated in speakerindependent digit recognition experiments and compared to noise suppression by Spectral Subtraction. 1</p><p>6 0.39654377 <a title="99-lda-6" href="./nips-2000-One_Microphone_Source_Separation.html">96 nips-2000-One Microphone Source Separation</a></p>
<p>7 0.39432886 <a title="99-lda-7" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>8 0.38984993 <a title="99-lda-8" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>9 0.38904124 <a title="99-lda-9" href="./nips-2000-Finding_the_Key_to_a_Synapse.html">55 nips-2000-Finding the Key to a Synapse</a></p>
<p>10 0.38763997 <a title="99-lda-10" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>11 0.3854591 <a title="99-lda-11" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>12 0.3796663 <a title="99-lda-12" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>13 0.37866709 <a title="99-lda-13" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>14 0.37815097 <a title="99-lda-14" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>15 0.37757298 <a title="99-lda-15" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>16 0.37622446 <a title="99-lda-16" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>17 0.3760635 <a title="99-lda-17" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>18 0.37531975 <a title="99-lda-18" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>19 0.37508804 <a title="99-lda-19" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>20 0.37464845 <a title="99-lda-20" href="./nips-2000-Speech_Denoising_and_Dereverberation_Using_Probabilistic_Models.html">123 nips-2000-Speech Denoising and Dereverberation Using Probabilistic Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
