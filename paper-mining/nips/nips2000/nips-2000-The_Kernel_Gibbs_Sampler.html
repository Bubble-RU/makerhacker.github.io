<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>133 nips-2000-The Kernel Gibbs Sampler</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-133" href="#">nips2000-133</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>133 nips-2000-The Kernel Gibbs Sampler</h1>
<br/><p>Source: <a title="nips-2000-133-pdf" href="http://papers.nips.cc/paper/1802-the-kernel-gibbs-sampler.pdf">pdf</a></p><p>Author: Thore Graepel, Ralf Herbrich</p><p>Abstract: We present an algorithm that samples the hypothesis space of kernel classifiers. Given a uniform prior over normalised weight vectors and a likelihood based on a model of label noise leads to a piecewise constant posterior that can be sampled by the kernel Gibbs sampler (KGS). The KGS is a Markov Chain Monte Carlo method that chooses a random direction in parameter space and samples from the resulting piecewise constant density along the line chosen. The KGS can be used as an analytical tool for the exploration of Bayesian transduction, Bayes point machines, active learning, and evidence-based model selection on small data sets that are contaminated with label noise. For a simple toy example we demonstrate experimentally how a Bayes point machine based on the KGS outperforms an SVM that is incapable of taking into account label noise. 1</p><p>Reference: <a title="nips-2000-133-reference" href="../nips2000_reference/nips-2000-The_Kernel_Gibbs_Sampler_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract We present an algorithm that samples the hypothesis space of kernel classifiers. [sent-5, score-0.379]
</p><p>2 Given a uniform prior over normalised weight vectors and a likelihood based on a model of label noise leads to a piecewise constant posterior that can be sampled by the kernel Gibbs sampler (KGS). [sent-6, score-1.575]
</p><p>3 The KGS is a Markov Chain Monte Carlo method that chooses a random direction in parameter space and samples from the resulting piecewise constant density along the line chosen. [sent-7, score-0.311]
</p><p>4 The KGS can be used as an analytical tool for the exploration of Bayesian transduction, Bayes point machines, active learning, and evidence-based model selection on small data sets that are contaminated with label noise. [sent-8, score-0.685]
</p><p>5 For a simple toy example we demonstrate experimentally how a Bayes point machine based on the KGS outperforms an SVM that is incapable of taking into account label noise. [sent-9, score-0.473]
</p><p>6 1  Introduction  Two great ideas have dominated recent developments in machine learning: the application of kernel methods and the popularisation of Bayesian inference. [sent-10, score-0.352]
</p><p>7 Focusing on the task of classification, various connections between the two areas exist: kernels have long been a part of Bayesian inference in the disguise of covariance nmctions that characterise priors over functions [9]. [sent-11, score-0.05]
</p><p>8 Also, attempts have been made to re-derive the support vector machine (SVM) [1] - possibly the most prominent representative of kernel methods - as a maximum a-posteriori estimator (MAP) in a Bayesian framework [8] . [sent-12, score-0.309]
</p><p>9 While this work suggests good strategies for evidencebased model selection the MAP estimator is not truly Bayesian in spirit because it is not based on the concept of model averaging which is crucial to Bayesian reasoning. [sent-13, score-0.174]
</p><p>10 As a consequence, the MAP estimator is generally not as robust as a real Bayesian estimator. [sent-14, score-0.063]
</p><p>11 While this drawback is inconsequential in a noise-free setting or in a situation dominated by feature noise, it may have severe consequences when the data is contaminated by label noise that may lead to a multi-modal posterior distribution. [sent-15, score-0.781]
</p><p>12 In order to make use of the full Bayesian posterior distribution it is necessary to generate samples from this distribution. [sent-16, score-0.215]
</p><p>13 This contribution is concerned with the generation of samples from the Bayesian posterior over the hypothesis space of lin-  ear classifiers in arbitrary kernel spaces in the case of label noise. [sent-17, score-1.06]
</p><p>14 In contrast to [8] we consider normalised weight vectors, IIwll. [sent-18, score-0.129]
</p><p>15 ~: = 1, because the classification given by a linear classifier only depends on the spatial direction of the weight vector w and not on its length. [sent-19, score-0.205]
</p><p>16 This point of view leads to a hypothesis space isomorphic to the surface of an n-dimensional sphere which - in the absence of prior information - is naturally equipped with a uniform prior over directions. [sent-20, score-0.272]
</p><p>17 Incorporating the label noise model into the likelihood then leads to a piecewise constant posterior on the surface of the sphere. [sent-21, score-0.898]
</p><p>18 The kernel Gibbs sampler (KGS) is designed to sample from this type of posterior by iteratively choosing a random direction and sampling on the resulting piecewise constant one-dimensional density in the fashion of a hit-and-run algorithm [7]. [sent-22, score-0.847]
</p><p>19 The resulting samples can be used in various ways: i) In Bayesian transduction [3] the decision about the labels of new test points can be inferred by a majority decision of the sampled classifiers. [sent-23, score-0.576]
</p><p>20 ii) The posterior mean - the Bayes point machine (BPM) solution [4] - can be calculated as an approximation to transduction. [sent-24, score-0.205]
</p><p>21 iii) The binary entropy of candidate training points can be calculated to determine their information content for active learning [2]. [sent-25, score-0.204]
</p><p>22 iv) The model evidence [5] can be evaluated for the purpose of model selection. [sent-26, score-0.045]
</p><p>23 We would like to point out, however, that the KGS is limited in practice to a sample size of m ~ 100 and should thus be thought of as an analytical tool to advance our understanding of the interaction of kernel methods and Bayesian reasoning. [sent-27, score-0.391]
</p><p>24 The paper is structured as follows: in Section 2 we introduce the learning scenario and explain our Bayesian approach to linear classifiers in kernel spaces. [sent-28, score-0.304]
</p><p>25 The kernel Gibbs sampler is explained in detail in Section 3. [sent-29, score-0.426]
</p><p>26 Different applications of the KGS are discussed in Section 4 followed by an experimental demonstration of the BPM solution based on using the KGS under label noise conditions. [sent-30, score-0.53]
</p><p>27 X), and vector spaces by calligraphic capitalised letters (e. [sent-37, score-0.11]
</p><p>28 The symbols P, E and I denote a probability measure, the expectation of a random variable and the indicator function, respectively. [sent-40, score-0.093]
</p><p>29 2  Bayesian Learning in Kernel spaces  We consider learning given a sequence x = (Xl, . [sent-41, score-0.068]
</p><p>30 Ym) E {-I, +I} m drawn iid from a fixed distribution P XY = P z over the space X x { -1, + I} = Z of input-output pairs. [sent-47, score-0.093]
</p><p>31 The hypotheses are linear classifiers X I-t (w,ifJ(x))/C =: (w,x)/C in some fixed feature space K ~ £~ where we assume that a mapping ¢ : X -+ K is chosen a priori 1 . [sent-48, score-0.13]
</p><p>32 Since all we need for learning is the real-valued output (w, Xi) /C of the classifier w at the m training points in Xl, . [sent-49, score-0.209]
</p><p>33 This is particularly useful if the dimensionality dim (K) = n of the feature space K is much greater (or possibly infinite) than the number m of training points. [sent-54, score-0.097]
</p><p>34 From (1) we see that all that is needed is the inner product function k (x, x') = (¢ (x) ,¢ (x'))/C also known as the kernel (see [9] for a detailed introduction to the theory of kernels). [sent-55, score-0.252]
</p><p>35 This, however, should not be confused with n-tuple x denoting the training objects. [sent-57, score-0.062]
</p><p>36 (a)  (b)  Figure 1: Illustration of the (log) posterior distribution on the surface of a 3dimensional sphere {w E Il~a IlIwllK = I} resulting from a label noise model with a label flip rate of q = 0. [sent-58, score-1.178]
</p><p>37 The log posterior is plotted over the longitude and latitude, and for small sample size it is multi-modal due to the label noise. [sent-60, score-0.633]
</p><p>38 The classifier w* labelling the data (before label noise) was at (~, 11"). [sent-61, score-0.483]
</p><p>39 In a Bayesian spirit we consider a prior Pw over possible weight vectors w E W of unit length, i. [sent-62, score-0.169]
</p><p>40 Given an iid training set z = (x,y) and a likelihood model PYlx=x,w=w we obtain the posterior PWlz==z using Bayes' formula PY=lx==. [sent-65, score-0.281]
</p><p>41 "w=w (y) By the iid assumption and the independence of the denominator from w we obtain m i=l . [sent-67, score-0.098]
</p><p>42 c[w,z] In the absence of specific prior knowledge symmetry suggests to take Pw uniform on W. [sent-70, score-0.036]
</p><p>43 Furthermore, we choose the likelihood model  PY1X=x,w=w (Y)  ={  if y (w,x)K otherwise  i _q  ::; a  where q specifies the assumed level of label noise. [sent-71, score-0.429]
</p><p>44 Please note the difference to the commonly assumed model of feature noise which essentially assumes noise in the (mapped) input vectors x instead of the labels y and constitutes the basis of the soft-margin SVM [1]. [sent-72, score-0.321]
</p><p>45 Thus the likelihood C[w,z] of the weight vector w is given by C [w, z] = qm. [sent-73, score-0.094]
</p><p>46 Re=p [w,z] (1 _ q)m(l-Re=p [W,Z]) , (3) where the training error Remp [w, z] is defined as  Remp [w,z]  =  1 m  m  L i=l  IYi(W,Xi } K:~O. [sent-74, score-0.062]
</p><p>47 Two data points YIXI and Y2X2 divide the space of normalised weight vectors W into four equivalence classes with different posterior density indicated by the gray shading. [sent-75, score-0.472]
</p><p>48 In each iteration, starting from Wj_l a random direction v with v. [sent-76, score-0.057]
</p><p>49 We sample from the piecewise constant density on the great circle determined by the plane defined by Wj-l and v. [sent-79, score-0.287]
</p><p>50 In order to obtain (*, we calculate the 2m angles (i where the training samples intersect with the circle and keep track of the number m . [sent-80, score-0.257]
</p><p>51 Figure 2: Schematic view of the kernel Gibbs sampling procedure. [sent-82, score-0.267]
</p><p>52 Clearly, the posterior Pw1z==z is piecewise constant for all error Remp [w,z] (see Figure 1). [sent-83, score-0.254]
</p><p>53 3  W  with equal training  The Kernel Gibbs Sampler  In order to sample from PWlz==z on W we suggest a Markov Chain sampling method. [sent-84, score-0.172]
</p><p>54 For a given value of q, the sampling scheme can be decomposed into the following steps (see Figure 2): 1. [sent-85, score-0.058]
</p><p>55 Choose an arbitrary starting point Wo E W and set j  = O. [sent-86, score-0.043]
</p><p>56 Choose a direction v E W in the tangent space {v E W I (v, Wj)K = O}. [sent-88, score-0.092]
</p><p>57 Calculate all m hit points b i E W from W in direction v with the hyperplane having normal YiXi' Before normalisation, this is achieved by [4] b i = Wj -  (Wj,Xi)K ( ) V. [sent-90, score-0.114]
</p><p>58 Calculate the training errors ei of the 2m intervals [(nCi-l),(nCi)] byevaluating ei =  Hemp [cos ((nCHl)2-  (nCi)) Wj - sm ((nCHl)2- (nCi)) v, z ] . [sent-113, score-0.148]
</p><p>59 Sample an angle (* using the piecewise uniform distribution and (3). [sent-115, score-0.165]
</p><p>60 Since the algorithm is carried out in feature space K we can use m W  m  = LCtiXi, i=l  m  v= LViXi,  b  = L. [sent-121, score-0.035]
</p><p>61 i=l  i=l  For the inner products and norms it follows that, e. [sent-123, score-0.043]
</p><p>62 As a consequence the above algorithm can be implemented in arbitrary kernel spaces only making use of k. [sent-126, score-0.277]
</p><p>63 4  Applications of the Kernel Gibbs Sampler  The kernel Gibbs sampler provides samples from the full posterior distribution over the hypothesis space of linear classifiers in kernel space for the case of label noise. [sent-127, score-1.453]
</p><p>64 These samples can be used for various tasks related to learning. [sent-128, score-0.14]
</p><p>65 In the following we will present a selection of these tasks. [sent-129, score-0.047]
</p><p>66 Bayesian Transduction Given a sample from the posterior distribution over hypotheses, a good strategy for prediction is to let the sampled classifiers vote on each new test data point. [sent-130, score-0.352]
</p><p>67 This mode of prediction is closest to the Bayesian spirit and has been shown for the zero-noise case to yield excellent generalisation performance [3]. [sent-131, score-0.286]
</p><p>68 Also the fraction of votes for the majority decision is an excellent indicator for the reliability of the final estimate: Rejection of those test points with the closest decision results in a great reduction of the generalisation error on the remaining test points x. [sent-132, score-0.641]
</p><p>69 Given the posterior PWlz~=% the transductive decision is  BT% (x)  = sign (Ewlz~=% [sign ((W,x)x;)J)  . [sent-133, score-0.253]
</p><p>70 In practice, this estimator is approximated by replacing the expectation by a sum over the sampled weight vectors W j. [sent-134, score-0.288]
</p><p>71 (4) EWlz~=%  Bayes Point Machines For classification, Bayesian Transduction requires the whole collection of sampled weight vectors W in memory. [sent-135, score-0.185]
</p><p>72 Since this may be impractical for large data sets we would like to derive a single classifier W from the Bayesian posterior. [sent-136, score-0.09]
</p><p>73 An excellent approximation of the transductive decision BT% (x) by a single classifier is obtained by exchanging the expectation with the inner sign-function in (4). [sent-137, score-0.362]
</p><p>74 Again, wbp is estimated by replacing the expectation by the mean over samples W j. [sent-139, score-0.256]
</p><p>75 Note that there exists no SVM equivalence WSVM to the Bayes point Wbp in the case of label noise - a fact to be elaborated on in the experimental part in Section 5. [sent-140, score-0.616]
</p><p>76 2  Figure 3: A set of 50 samples Wj of the posterior PWlz~=z for various noise levels q. [sent-144, score-0.441]
</p><p>77 Shown are the resulting decision boundaries in data space X. [sent-145, score-0.144]
</p><p>78 Active Learning The Bayesian posterior can also be employed to determine the usefulness of candidate training points - a task that can be considered as a dual counterpart to Bayesian Transduction. [sent-146, score-0.291]
</p><p>79 This is particularly useful when the label y of a training point x is more expensive to obtain than the training point x itself. [sent-147, score-0.603]
</p><p>80 It was shown in the context of "Query by Committee" [2) that the binary entropy  S (x,z)  = p+ log2P+ + p-Iog2P-  with p± = PWlz~=z (± (W, x) K > 0) is an indicator of the information content of a data point x with regard to the learning task. [sent-148, score-0.134]
</p><p>81 Samples W j from the Bayesian posterior PWlz~=z make it possible to estimate S for a given candidate training points x and the current training set z to decide on the basis of S if it is worthwhile to query the corresponding label y . [sent-149, score-0.801]
</p><p>82 It turns out that in the zero-noise case the margin (the quantity maximised by the SVM) is a measure of the evidence of the model used [4) . [sent-151, score-0.082]
</p><p>83 In the case of label noise the KGS serves to estimate this quantity. [sent-152, score-0.53]
</p><p>84 5  Experiments  In a first experiment we used a surrogate dataset of m = 76 data points x in X = IR2 and the kernel k (x,x') = exp(-t Ilx - x'II~). [sent-153, score-0.266]
</p><p>85 Using the KGS we sampled 50 different classifiers with weight vectors W j for various noise levels q and plotted the resulting decision boundaries {x E IR2 I (w j, x) K = O} in Figure 3 (circles and crosses depict different classes). [sent-154, score-0.615]
</p><p>86 As can be seen form these plots, increasing the noise level q leads to more diverse classifiers on the training set z. [sent-155, score-0.331]
</p><p>87 In a second experiment we investigated the generalisation performance of the Bayes point machine (see (5)) in the case of label noise. [sent-156, score-0.596]
</p><p>88 In IR3 we generated 100 random training and test sets of size mtrain = 100 and mtest = 1000, respectively. [sent-157, score-0.062]
</p><p>89 For each normalised point x E IR3 the longitude and latitude were sampled from a Beta(5, 5) and Beta(O. [sent-158, score-0.32]
</p><p>90 The classes y were obtained by randomly flipping the classes assigned by the classifier w* at (~, 7r) (see also Figure 1) with a true label flip rate of q* = 5%. [sent-161, score-0.609]
</p><p>91 In Figure 4 we plotted the estimated generalisation error for a BPM (trained using 100 samples Wj from the KGS) and  Generalisation errors of BPMs (circled error-bars) and soft-margin SVMs (triangled error-bars) vs. [sent-162, score-0.213]
</p><p>92 assumed noise level q and margin slack penalisation A, respectively. [sent-163, score-0.234]
</p><p>93 The dataset consisted of m = 100 observations with a label noise of 5% (dotted line) and we used k(x,x') = (x,x')x+A·I"=,,,. [sent-164, score-0.53]
</p><p>94 Figure 4: Comparison of BPMs and SVMs on data contaminated by label noise. [sent-166, score-0.474]
</p><p>95 quadratic soft-margin SVM at different label noise levels q and margin slack penalisation A, respectively. [sent-167, score-0.666]
</p><p>96 Clearly, the BPM with the correct noise model outperformed the SVM irrespective of the chosen level of regularisation. [sent-168, score-0.137]
</p><p>97 6  Conclusion and Future Research  The kernel Gibbs sampler provides an analytical tool for the exploration of various Bayesian aspects of learning in kernel spaces. [sent-173, score-0.806]
</p><p>98 It provides a well-founded way for dealing with label noise but suffers from its computational complexity which - so far - makes it inapplicable for large scale applications. [sent-174, score-0.53]
</p><p>99 Therefore it will be an interesting topic for future research to invent new sampling schemes that may be able to trade accuracy for speed and would thus be applicable to large data sets. [sent-175, score-0.058]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('label', 0.393), ('kgs', 0.346), ('pwlz', 0.22), ('sampler', 0.217), ('kernel', 0.209), ('bayesian', 0.188), ('wj', 0.165), ('noise', 0.137), ('bpm', 0.136), ('gibbs', 0.134), ('piecewise', 0.129), ('wbp', 0.126), ('posterior', 0.125), ('generalisation', 0.123), ('transduction', 0.108), ('nci', 0.108), ('classifiers', 0.095), ('samples', 0.09), ('classifier', 0.09), ('svm', 0.084), ('berlin', 0.082), ('contaminated', 0.081), ('remp', 0.081), ('sampled', 0.08), ('bayes', 0.079), ('decision', 0.074), ('normalised', 0.071), ('graepel', 0.068), ('pw', 0.068), ('spaces', 0.068), ('spirit', 0.064), ('herbrich', 0.064), ('estimator', 0.063), ('beta', 0.063), ('ewlz', 0.063), ('latitude', 0.063), ('longitude', 0.063), ('nchl', 0.063), ('penalisation', 0.063), ('training', 0.062), ('great', 0.061), ('excellent', 0.061), ('calculate', 0.06), ('weight', 0.058), ('sampling', 0.058), ('iid', 0.058), ('points', 0.057), ('direction', 0.057), ('query', 0.055), ('transductive', 0.054), ('bpms', 0.054), ('wsvm', 0.054), ('flip', 0.054), ('indicator', 0.053), ('sample', 0.052), ('various', 0.05), ('yixi', 0.049), ('committee', 0.049), ('candidate', 0.047), ('selection', 0.047), ('vectors', 0.047), ('tool', 0.046), ('circle', 0.045), ('py', 0.045), ('dominated', 0.045), ('hypothesis', 0.045), ('evidence', 0.045), ('ei', 0.043), ('inner', 0.043), ('point', 0.043), ('madison', 0.043), ('wisconsin', 0.043), ('majority', 0.043), ('equivalence', 0.043), ('letters', 0.042), ('technical', 0.042), ('surface', 0.041), ('analytical', 0.041), ('denominator', 0.04), ('bt', 0.04), ('bold', 0.04), ('reproducing', 0.04), ('expectation', 0.04), ('levels', 0.039), ('lx', 0.038), ('germany', 0.038), ('closest', 0.038), ('content', 0.038), ('leads', 0.037), ('machine', 0.037), ('quantity', 0.037), ('uniform', 0.036), ('likelihood', 0.036), ('chain', 0.036), ('classes', 0.036), ('boundaries', 0.035), ('sphere', 0.035), ('space', 0.035), ('slack', 0.034), ('exploration', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="133-tfidf-1" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>Author: Thore Graepel, Ralf Herbrich</p><p>Abstract: We present an algorithm that samples the hypothesis space of kernel classifiers. Given a uniform prior over normalised weight vectors and a likelihood based on a model of label noise leads to a piecewise constant posterior that can be sampled by the kernel Gibbs sampler (KGS). The KGS is a Markov Chain Monte Carlo method that chooses a random direction in parameter space and samples from the resulting piecewise constant density along the line chosen. The KGS can be used as an analytical tool for the exploration of Bayesian transduction, Bayes point machines, active learning, and evidence-based model selection on small data sets that are contaminated with label noise. For a simple toy example we demonstrate experimentally how a Bayes point machine based on the KGS outperforms an SVM that is incapable of taking into account label noise. 1</p><p>2 0.35135087 <a title="133-tfidf-2" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: The concept of averaging over classifiers is fundamental to the Bayesian analysis of learning. Based on this viewpoint, it has recently been demonstrated for linear classifiers that the centre of mass of version space (the set of all classifiers consistent with the training set) - also known as the Bayes point - exhibits excellent generalisation abilities. However, the billiard algorithm as presented in [4] is restricted to small sample size because it requires o (m 2 ) of memory and 0 (N . m2 ) computational steps where m is the number of training patterns and N is the number of random draws from the posterior distribution. In this paper we present a method based on the simple perceptron learning algorithm which allows to overcome this algorithmic drawback. The method is algorithmically simple and is easily extended to the multi-class case. We present experimental results on the MNIST data set of handwritten digits which show that Bayes point machines (BPMs) are competitive with the current world champion, the support vector machine. In addition, the computational complexity of BPMs can be tuned by varying the number of samples from the posterior. Finally, rejecting test points on the basis of their (approximative) posterior probability leads to a rapid decrease in generalisation error, e.g. 0.1% generalisation error for a given rejection rate of 10%. 1</p><p>3 0.253598 <a title="133-tfidf-3" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: We present a bound on the generalisation error of linear classifiers in terms of a refined margin quantity on the training set. The result is obtained in a PAC- Bayesian framework and is based on geometrical arguments in the space of linear classifiers. The new bound constitutes an exponential improvement of the so far tightest margin bound by Shawe-Taylor et al. [8] and scales logarithmically in the inverse margin. Even in the case of less training examples than input dimensions sufficiently large margins lead to non-trivial bound values and - for maximum margins - to a vanishing complexity term. Furthermore, the classical margin is too coarse a measure for the essential quantity that controls the generalisation error: the volume ratio between the whole hypothesis space and the subset of consistent hypotheses. The practical relevance of the result lies in the fact that the well-known support vector machine is optimal w.r.t. the new bound only if the feature vectors are all of the same length. As a consequence we recommend to use SVMs on normalised feature vectors only - a recommendation that is well supported by our numerical experiments on two benchmark data sets. 1</p><p>4 0.1842722 <a title="133-tfidf-4" href="./nips-2000-From_Margin_to_Sparsity.html">58 nips-2000-From Margin to Sparsity</a></p>
<p>Author: Thore Graepel, Ralf Herbrich, Robert C. Williamson</p><p>Abstract: We present an improvement of Novikoff's perceptron convergence theorem. Reinterpreting this mistake bound as a margin dependent sparsity guarantee allows us to give a PAC-style generalisation error bound for the classifier learned by the perceptron learning algorithm. The bound value crucially depends on the margin a support vector machine would achieve on the same data set using the same kernel. Ironically, the bound yields better guarantees than are currently available for the support vector solution itself. 1</p><p>5 0.14363964 <a title="133-tfidf-5" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>Author: Huma Lodhi, John Shawe-Taylor, Nello Cristianini, Christopher J. C. H. Watkins</p><p>Abstract: We introduce a novel kernel for comparing two text documents. The kernel is an inner product in the feature space consisting of all subsequences of length k. A subsequence is any ordered sequence of k characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences which are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. A preliminary experimental comparison of the performance of the kernel compared with a standard word feature space kernel [6] is made showing encouraging results. 1</p><p>6 0.1348311 <a title="133-tfidf-6" href="./nips-2000-Sparse_Kernel_Principal_Component_Analysis.html">121 nips-2000-Sparse Kernel Principal Component Analysis</a></p>
<p>7 0.12854181 <a title="133-tfidf-7" href="./nips-2000-The_Kernel_Trick_for_Distances.html">134 nips-2000-The Kernel Trick for Distances</a></p>
<p>8 0.12640984 <a title="133-tfidf-8" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>9 0.11839787 <a title="133-tfidf-9" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>10 0.11708416 <a title="133-tfidf-10" href="./nips-2000-Regularized_Winnow_Methods.html">111 nips-2000-Regularized Winnow Methods</a></p>
<p>11 0.1122117 <a title="133-tfidf-11" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>12 0.11065458 <a title="133-tfidf-12" href="./nips-2000-Feature_Correspondence%3A_A_Markov_Chain_Monte_Carlo_Approach.html">53 nips-2000-Feature Correspondence: A Markov Chain Monte Carlo Approach</a></p>
<p>13 0.11004305 <a title="133-tfidf-13" href="./nips-2000-Computing_with_Finite_and_Infinite_Networks.html">35 nips-2000-Computing with Finite and Infinite Networks</a></p>
<p>14 0.10668965 <a title="133-tfidf-14" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>15 0.10331471 <a title="133-tfidf-15" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>16 0.10314618 <a title="133-tfidf-16" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>17 0.10286661 <a title="133-tfidf-17" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>18 0.099140488 <a title="133-tfidf-18" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>19 0.095521756 <a title="133-tfidf-19" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>20 0.094795763 <a title="133-tfidf-20" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.313), (1, 0.287), (2, -0.077), (3, 0.056), (4, -0.074), (5, 0.025), (6, -0.004), (7, 0.054), (8, 0.057), (9, -0.117), (10, 0.288), (11, -0.037), (12, -0.039), (13, -0.045), (14, -0.145), (15, 0.025), (16, -0.125), (17, -0.051), (18, 0.068), (19, 0.041), (20, -0.17), (21, 0.082), (22, 0.015), (23, -0.069), (24, 0.007), (25, 0.049), (26, -0.016), (27, 0.122), (28, 0.051), (29, -0.013), (30, 0.052), (31, 0.008), (32, -0.028), (33, -0.011), (34, -0.014), (35, -0.023), (36, -0.02), (37, -0.029), (38, 0.046), (39, 0.122), (40, 0.017), (41, -0.035), (42, -0.014), (43, -0.048), (44, -0.007), (45, 0.034), (46, 0.046), (47, 0.077), (48, -0.028), (49, -0.131)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95752907 <a title="133-lsi-1" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>Author: Thore Graepel, Ralf Herbrich</p><p>Abstract: We present an algorithm that samples the hypothesis space of kernel classifiers. Given a uniform prior over normalised weight vectors and a likelihood based on a model of label noise leads to a piecewise constant posterior that can be sampled by the kernel Gibbs sampler (KGS). The KGS is a Markov Chain Monte Carlo method that chooses a random direction in parameter space and samples from the resulting piecewise constant density along the line chosen. The KGS can be used as an analytical tool for the exploration of Bayesian transduction, Bayes point machines, active learning, and evidence-based model selection on small data sets that are contaminated with label noise. For a simple toy example we demonstrate experimentally how a Bayes point machine based on the KGS outperforms an SVM that is incapable of taking into account label noise. 1</p><p>2 0.86894506 <a title="133-lsi-2" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: The concept of averaging over classifiers is fundamental to the Bayesian analysis of learning. Based on this viewpoint, it has recently been demonstrated for linear classifiers that the centre of mass of version space (the set of all classifiers consistent with the training set) - also known as the Bayes point - exhibits excellent generalisation abilities. However, the billiard algorithm as presented in [4] is restricted to small sample size because it requires o (m 2 ) of memory and 0 (N . m2 ) computational steps where m is the number of training patterns and N is the number of random draws from the posterior distribution. In this paper we present a method based on the simple perceptron learning algorithm which allows to overcome this algorithmic drawback. The method is algorithmically simple and is easily extended to the multi-class case. We present experimental results on the MNIST data set of handwritten digits which show that Bayes point machines (BPMs) are competitive with the current world champion, the support vector machine. In addition, the computational complexity of BPMs can be tuned by varying the number of samples from the posterior. Finally, rejecting test points on the basis of their (approximative) posterior probability leads to a rapid decrease in generalisation error, e.g. 0.1% generalisation error for a given rejection rate of 10%. 1</p><p>3 0.6938895 <a title="133-lsi-3" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: We present a bound on the generalisation error of linear classifiers in terms of a refined margin quantity on the training set. The result is obtained in a PAC- Bayesian framework and is based on geometrical arguments in the space of linear classifiers. The new bound constitutes an exponential improvement of the so far tightest margin bound by Shawe-Taylor et al. [8] and scales logarithmically in the inverse margin. Even in the case of less training examples than input dimensions sufficiently large margins lead to non-trivial bound values and - for maximum margins - to a vanishing complexity term. Furthermore, the classical margin is too coarse a measure for the essential quantity that controls the generalisation error: the volume ratio between the whole hypothesis space and the subset of consistent hypotheses. The practical relevance of the result lies in the fact that the well-known support vector machine is optimal w.r.t. the new bound only if the feature vectors are all of the same length. As a consequence we recommend to use SVMs on normalised feature vectors only - a recommendation that is well supported by our numerical experiments on two benchmark data sets. 1</p><p>4 0.60904551 <a title="133-lsi-4" href="./nips-2000-From_Margin_to_Sparsity.html">58 nips-2000-From Margin to Sparsity</a></p>
<p>Author: Thore Graepel, Ralf Herbrich, Robert C. Williamson</p><p>Abstract: We present an improvement of Novikoff's perceptron convergence theorem. Reinterpreting this mistake bound as a margin dependent sparsity guarantee allows us to give a PAC-style generalisation error bound for the classifier learned by the perceptron learning algorithm. The bound value crucially depends on the margin a support vector machine would achieve on the same data set using the same kernel. Ironically, the bound yields better guarantees than are currently available for the support vector solution itself. 1</p><p>5 0.47527534 <a title="133-lsi-5" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>Author: Martin Szummer, Tommi Jaakkola</p><p>Abstract: Modern classification applications necessitate supplementing the few available labeled examples with unlabeled examples to improve classification performance. We present a new tractable algorithm for exploiting unlabeled examples in discriminative classification. This is achieved essentially by expanding the input vectors into longer feature vectors via both labeled and unlabeled examples. The resulting classification method can be interpreted as a discriminative kernel density estimate and is readily trained via the EM algorithm, which in this case is both discriminative and achieves the optimal solution. We provide, in addition, a purely discriminative formulation of the estimation problem by appealing to the maximum entropy framework. We demonstrate that the proposed approach requires very few labeled examples for high classification accuracy.</p><p>6 0.44568428 <a title="133-lsi-6" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>7 0.43640596 <a title="133-lsi-7" href="./nips-2000-Feature_Correspondence%3A_A_Markov_Chain_Monte_Carlo_Approach.html">53 nips-2000-Feature Correspondence: A Markov Chain Monte Carlo Approach</a></p>
<p>8 0.40044504 <a title="133-lsi-8" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>9 0.39040875 <a title="133-lsi-9" href="./nips-2000-Fast_Training_of_Support_Vector_Classifiers.html">52 nips-2000-Fast Training of Support Vector Classifiers</a></p>
<p>10 0.38271594 <a title="133-lsi-10" href="./nips-2000-Sparse_Kernel_Principal_Component_Analysis.html">121 nips-2000-Sparse Kernel Principal Component Analysis</a></p>
<p>11 0.37579605 <a title="133-lsi-11" href="./nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">95 nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>12 0.37359756 <a title="133-lsi-12" href="./nips-2000-The_Kernel_Trick_for_Distances.html">134 nips-2000-The Kernel Trick for Distances</a></p>
<p>13 0.3681677 <a title="133-lsi-13" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>14 0.36602113 <a title="133-lsi-14" href="./nips-2000-Computing_with_Finite_and_Infinite_Networks.html">35 nips-2000-Computing with Finite and Infinite Networks</a></p>
<p>15 0.36498642 <a title="133-lsi-15" href="./nips-2000-A_Mathematical_Programming_Approach_to_the_Kernel_Fisher_Algorithm.html">5 nips-2000-A Mathematical Programming Approach to the Kernel Fisher Algorithm</a></p>
<p>16 0.35432243 <a title="133-lsi-16" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>17 0.35324866 <a title="133-lsi-17" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>18 0.35251626 <a title="133-lsi-18" href="./nips-2000-Incremental_and_Decremental_Support_Vector_Machine_Learning.html">70 nips-2000-Incremental and Decremental Support Vector Machine Learning</a></p>
<p>19 0.34454775 <a title="133-lsi-19" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>20 0.34113356 <a title="133-lsi-20" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.046), (17, 0.151), (32, 0.031), (33, 0.063), (36, 0.02), (45, 0.088), (55, 0.019), (62, 0.037), (65, 0.013), (67, 0.053), (75, 0.011), (76, 0.042), (79, 0.036), (81, 0.011), (90, 0.062), (97, 0.025), (98, 0.231)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8398152 <a title="133-lda-1" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>Author: Thore Graepel, Ralf Herbrich</p><p>Abstract: We present an algorithm that samples the hypothesis space of kernel classifiers. Given a uniform prior over normalised weight vectors and a likelihood based on a model of label noise leads to a piecewise constant posterior that can be sampled by the kernel Gibbs sampler (KGS). The KGS is a Markov Chain Monte Carlo method that chooses a random direction in parameter space and samples from the resulting piecewise constant density along the line chosen. The KGS can be used as an analytical tool for the exploration of Bayesian transduction, Bayes point machines, active learning, and evidence-based model selection on small data sets that are contaminated with label noise. For a simple toy example we demonstrate experimentally how a Bayes point machine based on the KGS outperforms an SVM that is incapable of taking into account label noise. 1</p><p>2 0.77274418 <a title="133-lda-2" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>Author: Marina Meila, Jianbo Shi</p><p>Abstract: We present a new view of image segmentation by pairwise similarities. We interpret the similarities as edge flows in a Markov random walk and study the eigenvalues and eigenvectors of the walk's transition matrix. This interpretation shows that spectral methods for clustering and segmentation have a probabilistic foundation. In particular, we prove that the Normalized Cut method arises naturally from our framework. Finally, the framework provides a principled method for learning the similarity function as a combination of features. 1</p><p>3 0.68539083 <a title="133-lda-3" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: The concept of averaging over classifiers is fundamental to the Bayesian analysis of learning. Based on this viewpoint, it has recently been demonstrated for linear classifiers that the centre of mass of version space (the set of all classifiers consistent with the training set) - also known as the Bayes point - exhibits excellent generalisation abilities. However, the billiard algorithm as presented in [4] is restricted to small sample size because it requires o (m 2 ) of memory and 0 (N . m2 ) computational steps where m is the number of training patterns and N is the number of random draws from the posterior distribution. In this paper we present a method based on the simple perceptron learning algorithm which allows to overcome this algorithmic drawback. The method is algorithmically simple and is easily extended to the multi-class case. We present experimental results on the MNIST data set of handwritten digits which show that Bayes point machines (BPMs) are competitive with the current world champion, the support vector machine. In addition, the computational complexity of BPMs can be tuned by varying the number of samples from the posterior. Finally, rejecting test points on the basis of their (approximative) posterior probability leads to a rapid decrease in generalisation error, e.g. 0.1% generalisation error for a given rejection rate of 10%. 1</p><p>4 0.66678315 <a title="133-lda-4" href="./nips-2000-Keeping_Flexible_Active_Contours_on_Track_using_Metropolis_Updates.html">72 nips-2000-Keeping Flexible Active Contours on Track using Metropolis Updates</a></p>
<p>Author: Trausti T. Kristjansson, Brendan J. Frey</p><p>Abstract: Condensation, a form of likelihood-weighted particle filtering, has been successfully used to infer the shapes of highly constrained</p><p>5 0.62837225 <a title="133-lda-5" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: We present a bound on the generalisation error of linear classifiers in terms of a refined margin quantity on the training set. The result is obtained in a PAC- Bayesian framework and is based on geometrical arguments in the space of linear classifiers. The new bound constitutes an exponential improvement of the so far tightest margin bound by Shawe-Taylor et al. [8] and scales logarithmically in the inverse margin. Even in the case of less training examples than input dimensions sufficiently large margins lead to non-trivial bound values and - for maximum margins - to a vanishing complexity term. Furthermore, the classical margin is too coarse a measure for the essential quantity that controls the generalisation error: the volume ratio between the whole hypothesis space and the subset of consistent hypotheses. The practical relevance of the result lies in the fact that the well-known support vector machine is optimal w.r.t. the new bound only if the feature vectors are all of the same length. As a consequence we recommend to use SVMs on normalised feature vectors only - a recommendation that is well supported by our numerical experiments on two benchmark data sets. 1</p><p>6 0.60323364 <a title="133-lda-6" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>7 0.59861112 <a title="133-lda-7" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>8 0.5934397 <a title="133-lda-8" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>9 0.58583444 <a title="133-lda-9" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>10 0.58503026 <a title="133-lda-10" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>11 0.58469266 <a title="133-lda-11" href="./nips-2000-Regularized_Winnow_Methods.html">111 nips-2000-Regularized Winnow Methods</a></p>
<p>12 0.58408201 <a title="133-lda-12" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>13 0.58032882 <a title="133-lda-13" href="./nips-2000-Some_New_Bounds_on_the_Generalization_Error_of_Combined_Classifiers.html">119 nips-2000-Some New Bounds on the Generalization Error of Combined Classifiers</a></p>
<p>14 0.57835865 <a title="133-lda-14" href="./nips-2000-Algorithmic_Stability_and_Generalization_Performance.html">21 nips-2000-Algorithmic Stability and Generalization Performance</a></p>
<p>15 0.57482946 <a title="133-lda-15" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>16 0.57344121 <a title="133-lda-16" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<p>17 0.57287246 <a title="133-lda-17" href="./nips-2000-Constrained_Independent_Component_Analysis.html">36 nips-2000-Constrained Independent Component Analysis</a></p>
<p>18 0.57073349 <a title="133-lda-18" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>19 0.56928384 <a title="133-lda-19" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>20 0.56777579 <a title="133-lda-20" href="./nips-2000-Fast_Training_of_Support_Vector_Classifiers.html">52 nips-2000-Fast Training of Support Vector Classifiers</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
