<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-101" href="#">nips2000-101</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2000-101-pdf" href="http://papers.nips.cc/paper/1801-place-cells-and-spatial-navigation-based-on-2d-visual-feature-extraction-path-integration-and-reinforcement-learning.pdf">pdf</a></p><p>Author: Angelo Arleo, Fabrizio Smeraldi, Stéphane Hug, Wulfram Gerstner</p><p>Abstract: We model hippocampal place cells and head-direction cells by combining allothetic (visual) and idiothetic (proprioceptive) stimuli. Visual input, provided by a video camera on a miniature robot, is preprocessed by a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. Unsupervised Hebbian learning is employed to incrementally build a population of localized overlapping place fields. Place cells serve as basis functions for reinforcement learning. Experimental results for goal-oriented navigation of a mobile robot are presented.</p><p>Reference: <a title="nips-2000-101-reference" href="../nips2000_reference/nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cel', 0.383), ('robot', 0.381), ('plac', 0.289), ('gab', 0.256), ('hippocamp', 0.235), ('navig', 0.214), ('slec', 0.183), ('smec', 0.183), ('spat', 0.145), ('vis', 0.139), ('idiothet', 0.137), ('snapshot', 0.137), ('snc', 0.137), ('filt', 0.119), ('reinforc', 0.105), ('field', 0.104), ('obstac', 0.103), ('pop', 0.097), ('head', 0.09), ('fir', 0.089)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="101-tfidf-1" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>Author: Angelo Arleo, Fabrizio Smeraldi, Stéphane Hug, Wulfram Gerstner</p><p>Abstract: We model hippocampal place cells and head-direction cells by combining allothetic (visual) and idiothetic (proprioceptive) stimuli. Visual input, provided by a video camera on a miniature robot, is preprocessed by a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. Unsupervised Hebbian learning is employed to incrementally build a population of localized overlapping place fields. Place cells serve as basis functions for reinforcement learning. Experimental results for goal-oriented navigation of a mobile robot are presented.</p><p>2 0.26658612 <a title="101-tfidf-2" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>Author: Sophie Denève, Jean-René Duhamel, Alexandre Pouget</p><p>Abstract: Most models of spatial representations in the cortex assume cells with limited receptive fields that are defined in a particular egocentric frame of reference. However, cells outside of primary sensory cortex are either gain modulated by postural input or partially shifting. We show that solving classical spatial tasks, like sensory prediction, multi-sensory integration, sensory-motor transformation and motor control requires more complicated intermediate representations that are not invariant in one frame of reference. We present an iterative basis function map that performs these spatial tasks optimally with gain modulated and partially shifting units, and tests it against neurophysiological and neuropsychological data. In order to perform an action directed toward an object, it is necessary to have a representation of its spatial location. The brain must be able to use spatial cues coming from different modalities (e.g. vision, audition, touch, proprioception), combine them to infer the position of the object, and compute the appropriate movement. These cues are in different frames of reference corresponding to different sensory or motor modalities. Visual inputs are primarily encoded in retinotopic maps, auditory inputs are encoded in head centered maps and tactile cues are encoded in skin-centered maps. Going from one frame of reference to the other might seem easy. For example, the head-centered position of an object can be approximated by the sum of its retinotopic position and the eye position. However, positions are represented by population codes in the brain, and computing a head-centered map from a retinotopic map is a more complex computation than the underlying sum. Moreover, as we get closer to sensory-motor areas it seems reasonable to assume Spksls 150 100 50 o Figure 1: Response of a VIP cell to visual stimuli appearing in different part of the screen, for three different eye positions. The level of grey represent the frequency of discharge (In spikes per seconds). The white cross is the fixation point (the head is fixed). The cell's receptive field is moving with the eyes, but only partially. Here the receptive field shift is 60% of the total gaze shift. Moreover this cell is gain modulated by eye position (adapted from Duhamel et al). that the representations should be useful for sensory-motor transformations, rather than encode an</p><p>3 0.24408995 <a title="101-tfidf-3" href="./nips-2000-Modelling_Spatial_Recall%2C_Mental_Imagery_and_Neglect.html">87 nips-2000-Modelling Spatial Recall, Mental Imagery and Neglect</a></p>
<p>Author: Suzanna Becker, Neil Burgess</p><p>Abstract: We present a computational model of the neural mechanisms in the parietal and temporal lobes that support spatial navigation, recall of scenes and imagery of the products of recall. Long term representations are stored in the hippocampus, and are associated with local spatial and object-related features in the parahippocampal region. Viewer-centered representations are dynamically generated from long term memory in the parietal part of the model. The model thereby simulates recall and imagery of locations and objects in complex environments. After parietal damage, the model exhibits hemispatial neglect in mental imagery that rotates with the imagined perspective of the observer, as in the famous Milan Square experiment [1]. Our model makes novel predictions for the neural representations in the parahippocampal and parietal regions and for behavior in healthy volunteers and neuropsychological patients.</p><p>4 0.16590698 <a title="101-tfidf-4" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>Author: Kevin A. Archie, Bartlett W. Mel</p><p>Abstract: Neurons in area V4 have relatively large receptive fields (RFs), so multiple visual features are simultaneously</p><p>5 0.15937375 <a title="101-tfidf-5" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>Author: Odelia Schwartz, Eero P. Simoncelli</p><p>Abstract: We explore the statistical properties of natural sound stimuli preprocessed with a bank of linear filters. The responses of such filters exhibit a striking form of statistical dependency, in which the response variance of each filter grows with the response amplitude of filters tuned for nearby frequencies. These dependencies may be substantially reduced using an operation known as divisive normalization, in which the response of each filter is divided by a weighted sum of the rectified responses of other filters. The weights may be chosen to maximize the independence of the normalized responses for an ensemble of natural sounds. We demonstrate that the resulting model accounts for nonlinearities in the response characteristics of the auditory nerve, by comparing model simulations to electrophysiological recordings. In previous work (NIPS, 1998) we demonstrated that an analogous model derived from the statistics of natural images accounts for non-linear properties of neurons in primary visual cortex. Thus, divisive normalization appears to be a generic mechanism for eliminating a type of statistical dependency that is prevalent in natural signals of different modalities. Signals in the real world are highly structured. For example, natural sounds typically contain both harmonic and rythmic structure. It is reasonable to assume that biological auditory systems are designed to represent these structures in an efficient manner [e.g., 1,2]. Specifically, Barlow hypothesized that a role of early sensory processing is to remove redundancy in the sensory input, resulting in a set of neural responses that are statistically independent. Experimentally, one can test this hypothesis by examining the statistical properties of neural responses under natural stimulation conditions [e.g., 3,4], or the statistical dependency of pairs (or groups) of neural responses. Due to their technical difficulty, such multi-cellular experiments are only recently becoming possible, and the earliest reports in vision appear consistent with the hypothesis [e.g., 5]. An alternative approach, which we follow here, is to develop a neural model from the statistics of natural signals and show that response properties of this model are similar to those of biological sensory neurons. A number of researchers have derived linear filter models using statistical criterion. For visual images, this results in linear filters localized in frequency, orientation and phase [6, 7]. Similar work in audition has yielded filters localized in frequency and phase [8]. Although these linear models provide an important starting point for neural modeling, sensory neurons are highly nonlinear. In addition, the statistical properties of natural signals are too complex to expect a linear transformation to result in an independent set of components. Recent results indicate that nonlinear gain control plays an important role in neural processing. Ruderman and Bialek [9] have shown that division by a local estimate of standard deviation can increase the entropy of responses of center-surround filters to natural images. Such a model is consistent with the properties of neurons in the retina and lateral geniculate nucleus. Heeger and colleagues have shown that the nonlinear behaviors of neurons in primary visual cortex may be described using a form of gain control known as divisive normalization [10], in which the response of a linear kernel is rectified and divided by the sum of other rectified kernel responses and a constant. We have recently shown that the responses of oriented linear filters exhibit nonlinear statistical dependencies that may be substantially reduced using a variant of this model, in which the normalization signal is computed from a weighted sum of other rectified kernel responses [11, 12]. The resulting model, with weighting parameters determined from image statistics, accounts qualitatively for physiological nonlinearities observed in primary visual cortex. In this paper, we demonstrate that the responses of bandpass linear filters to natural sounds exhibit striking statistical dependencies, analogous to those found in visual images. A divisive normalization procedure can substantially remove these dependencies. We show that this model, with parameters optimized for a collection of natural sounds, can account for nonlinear behaviors of neurons at the level of the auditory nerve. Specifically, we show that: 1) the shape offrequency tuning curves varies with sound pressure level, even though the underlying linear filters are fixed; and 2) superposition of a non-optimal tone suppresses the response of a linear filter in a divisive fashion, and the amount of suppression depends on the distance between the frequency of the tone and the preferred frequency of the filter. 1 Empirical observations of natural sound statistics The basic statistical properties of natural sounds, as observed through a linear filter, have been previously documented by Attias [13]. In particular, he showed that, as with visual images, the spectral energy falls roughly according to a power law, and that the histograms of filter responses are more kurtotic than a Gaussian (i.e., they have a sharp peak at zero, and very long tails). Here we examine the joint statistical properties of a pair of linear filters tuned for nearby temporal frequencies. We choose a fixed set of filters that have been widely used in modeling the peripheral auditory system [14]. Figure 1 shows joint histograms of the instantaneous responses of a particular pair of linear filters to five different types of natural sound, and white noise. First note that the responses are approximately decorrelated: the expected value of the y-axis value is roughly zero for all values of the x-axis variable. The responses are not, however, statistically independent: the width of the distribution of responses of one filter increases with the response amplitude of the other filter. If the two responses were statistically independent, then the response of the first filter should not provide any information about the distribution of responses of the other filter. We have found that this type of variance dependency (sometimes accompanied by linear correlation) occurs in a wide range of natural sounds, ranging from animal sounds to music. We emphasize that this dependency is a property of natural sounds, and is not due purely to our choice of linear filters. For example, no such dependency is observed when the input consists of white noise (see Fig. 1). The strength of this dependency varies for different pairs of linear filters . In addition, we see this type of dependency between instantaneous responses of a single filter at two Speech o -1 Drums • Monkey Cat White noise Nocturnal nature I~ ~; ~ • Figure 1: Joint conditional histogram of instantaneous linear responses of two bandpass filters with center frequencies 2000 and 2840 Hz. Pixel intensity corresponds to frequency of occurrence of a given pair of values, except that each column has been independently rescaled to fill the full intensity range. For the natural sounds, responses are not independent: the standard deviation of the ordinate is roughly proportional to the magnitude of the abscissa. Natural sounds were recorded from CDs and converted to sampling frequency of 22050 Hz. nearby time instants. Since the dependency involves the variance of the responses, we can substantially reduce it by dividing. In particular, the response of each filter is divided by a weighted sum of responses of other rectified filters and an additive constant. Specifically: L2 Ri = 2: (1) 12 j WjiLj + 0'2 where Li is the instantaneous linear response of filter i, strength of suppression of filter i by filter j. 0' is a constant and Wji controls the We would like to choose the parameters of the model (the weights Wji, and the constant 0') to optimize the independence of the normalized response to an ensemble of natural sounds. Such an optimization is quite computationally expensive. We instead assume a Gaussian form for the underlying conditional distribution, as described in [15]: P (LiILj,j E Ni ) '</p><p>6 0.13984866 <a title="101-tfidf-6" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>7 0.13497379 <a title="101-tfidf-7" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>8 0.13015941 <a title="101-tfidf-8" href="./nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">42 nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<p>9 0.11402465 <a title="101-tfidf-9" href="./nips-2000-Hippocampally-Dependent_Consolidation_in_a_Hierarchical_Model_of_Neocortex.html">66 nips-2000-Hippocampally-Dependent Consolidation in a Hierarchical Model of Neocortex</a></p>
<p>10 0.11308245 <a title="101-tfidf-10" href="./nips-2000-Bayes_Networks_on_Ice%3A_Robotic_Search_for_Antarctic_Meteorites.html">29 nips-2000-Bayes Networks on Ice: Robotic Search for Antarctic Meteorites</a></p>
<p>11 0.11025168 <a title="101-tfidf-11" href="./nips-2000-Four-legged_Walking_Gait_Control_Using_a_Neuromorphic_Chip_Interfaced_to_a_Support_Vector_Learning_Algorithm.html">57 nips-2000-Four-legged Walking Gait Control Using a Neuromorphic Chip Interfaced to a Support Vector Learning Algorithm</a></p>
<p>12 0.10626945 <a title="101-tfidf-12" href="./nips-2000-Smart_Vision_Chip_Fabricated_Using_Three_Dimensional_Integration_Technology.html">118 nips-2000-Smart Vision Chip Fabricated Using Three Dimensional Integration Technology</a></p>
<p>13 0.10107528 <a title="101-tfidf-13" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>14 0.097753681 <a title="101-tfidf-14" href="./nips-2000-Feature_Correspondence%3A_A_Markov_Chain_Monte_Carlo_Approach.html">53 nips-2000-Feature Correspondence: A Markov Chain Monte Carlo Approach</a></p>
<p>15 0.095651254 <a title="101-tfidf-15" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>16 0.094859973 <a title="101-tfidf-16" href="./nips-2000-Machine_Learning_for_Video-Based_Rendering.html">83 nips-2000-Machine Learning for Video-Based Rendering</a></p>
<p>17 0.094364516 <a title="101-tfidf-17" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>18 0.092477344 <a title="101-tfidf-18" href="./nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">129 nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<p>19 0.089227773 <a title="101-tfidf-19" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>20 0.085249305 <a title="101-tfidf-20" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.238), (1, -0.23), (2, -0.059), (3, -0.018), (4, -0.17), (5, -0.207), (6, 0.107), (7, -0.272), (8, 0.144), (9, 0.008), (10, 0.071), (11, -0.074), (12, -0.082), (13, -0.092), (14, 0.124), (15, -0.02), (16, -0.029), (17, -0.097), (18, -0.218), (19, -0.038), (20, -0.079), (21, 0.041), (22, -0.005), (23, -0.078), (24, 0.051), (25, 0.123), (26, 0.138), (27, 0.049), (28, 0.053), (29, -0.018), (30, 0.088), (31, 0.003), (32, 0.059), (33, 0.119), (34, -0.049), (35, -0.037), (36, -0.035), (37, 0.135), (38, -0.002), (39, -0.111), (40, 0.053), (41, -0.046), (42, -0.023), (43, 0.045), (44, 0.066), (45, 0.027), (46, -0.07), (47, -0.059), (48, -0.019), (49, 0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95679772 <a title="101-lsi-1" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>Author: Angelo Arleo, Fabrizio Smeraldi, Stéphane Hug, Wulfram Gerstner</p><p>Abstract: We model hippocampal place cells and head-direction cells by combining allothetic (visual) and idiothetic (proprioceptive) stimuli. Visual input, provided by a video camera on a miniature robot, is preprocessed by a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. Unsupervised Hebbian learning is employed to incrementally build a population of localized overlapping place fields. Place cells serve as basis functions for reinforcement learning. Experimental results for goal-oriented navigation of a mobile robot are presented.</p><p>2 0.77220684 <a title="101-lsi-2" href="./nips-2000-Modelling_Spatial_Recall%2C_Mental_Imagery_and_Neglect.html">87 nips-2000-Modelling Spatial Recall, Mental Imagery and Neglect</a></p>
<p>Author: Suzanna Becker, Neil Burgess</p><p>Abstract: We present a computational model of the neural mechanisms in the parietal and temporal lobes that support spatial navigation, recall of scenes and imagery of the products of recall. Long term representations are stored in the hippocampus, and are associated with local spatial and object-related features in the parahippocampal region. Viewer-centered representations are dynamically generated from long term memory in the parietal part of the model. The model thereby simulates recall and imagery of locations and objects in complex environments. After parietal damage, the model exhibits hemispatial neglect in mental imagery that rotates with the imagined perspective of the observer, as in the famous Milan Square experiment [1]. Our model makes novel predictions for the neural representations in the parahippocampal and parietal regions and for behavior in healthy volunteers and neuropsychological patients.</p><p>3 0.74478626 <a title="101-lsi-3" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>Author: Sophie Denève, Jean-René Duhamel, Alexandre Pouget</p><p>Abstract: Most models of spatial representations in the cortex assume cells with limited receptive fields that are defined in a particular egocentric frame of reference. However, cells outside of primary sensory cortex are either gain modulated by postural input or partially shifting. We show that solving classical spatial tasks, like sensory prediction, multi-sensory integration, sensory-motor transformation and motor control requires more complicated intermediate representations that are not invariant in one frame of reference. We present an iterative basis function map that performs these spatial tasks optimally with gain modulated and partially shifting units, and tests it against neurophysiological and neuropsychological data. In order to perform an action directed toward an object, it is necessary to have a representation of its spatial location. The brain must be able to use spatial cues coming from different modalities (e.g. vision, audition, touch, proprioception), combine them to infer the position of the object, and compute the appropriate movement. These cues are in different frames of reference corresponding to different sensory or motor modalities. Visual inputs are primarily encoded in retinotopic maps, auditory inputs are encoded in head centered maps and tactile cues are encoded in skin-centered maps. Going from one frame of reference to the other might seem easy. For example, the head-centered position of an object can be approximated by the sum of its retinotopic position and the eye position. However, positions are represented by population codes in the brain, and computing a head-centered map from a retinotopic map is a more complex computation than the underlying sum. Moreover, as we get closer to sensory-motor areas it seems reasonable to assume Spksls 150 100 50 o Figure 1: Response of a VIP cell to visual stimuli appearing in different part of the screen, for three different eye positions. The level of grey represent the frequency of discharge (In spikes per seconds). The white cross is the fixation point (the head is fixed). The cell's receptive field is moving with the eyes, but only partially. Here the receptive field shift is 60% of the total gaze shift. Moreover this cell is gain modulated by eye position (adapted from Duhamel et al). that the representations should be useful for sensory-motor transformations, rather than encode an</p><p>4 0.49213901 <a title="101-lsi-4" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>Author: Kevin A. Archie, Bartlett W. Mel</p><p>Abstract: Neurons in area V4 have relatively large receptive fields (RFs), so multiple visual features are simultaneously</p><p>5 0.49177524 <a title="101-lsi-5" href="./nips-2000-Bayes_Networks_on_Ice%3A_Robotic_Search_for_Antarctic_Meteorites.html">29 nips-2000-Bayes Networks on Ice: Robotic Search for Antarctic Meteorites</a></p>
<p>Author: Liam Pedersen, Dimitrios Apostolopoulos, William Whittaker</p><p>Abstract: A Bayes network based classifier for distinguishing terrestrial rocks from meteorites is implemented onboard the Nomad robot. Equipped with a camera, spectrometer and eddy current sensor, this robot searched the ice sheets of Antarctica and autonomously made the first robotic identification of a meteorite, in January 2000 at the Elephant Moraine. This paper discusses rock classification from a robotic platform, and describes the system onboard Nomad. 1</p><p>6 0.44908908 <a title="101-lsi-6" href="./nips-2000-Hippocampally-Dependent_Consolidation_in_a_Hierarchical_Model_of_Neocortex.html">66 nips-2000-Hippocampally-Dependent Consolidation in a Hierarchical Model of Neocortex</a></p>
<p>7 0.41194862 <a title="101-lsi-7" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>8 0.40407789 <a title="101-lsi-8" href="./nips-2000-Smart_Vision_Chip_Fabricated_Using_Three_Dimensional_Integration_Technology.html">118 nips-2000-Smart Vision Chip Fabricated Using Three Dimensional Integration Technology</a></p>
<p>9 0.40301359 <a title="101-lsi-9" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>10 0.3636848 <a title="101-lsi-10" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>11 0.35344562 <a title="101-lsi-11" href="./nips-2000-Four-legged_Walking_Gait_Control_Using_a_Neuromorphic_Chip_Interfaced_to_a_Support_Vector_Learning_Algorithm.html">57 nips-2000-Four-legged Walking Gait Control Using a Neuromorphic Chip Interfaced to a Support Vector Learning Algorithm</a></p>
<p>12 0.34898192 <a title="101-lsi-12" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>13 0.34089407 <a title="101-lsi-13" href="./nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">42 nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<p>14 0.33813217 <a title="101-lsi-14" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>15 0.31854877 <a title="101-lsi-15" href="./nips-2000-Machine_Learning_for_Video-Based_Rendering.html">83 nips-2000-Machine Learning for Video-Based Rendering</a></p>
<p>16 0.29010123 <a title="101-lsi-16" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>17 0.28306833 <a title="101-lsi-17" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>18 0.27492127 <a title="101-lsi-18" href="./nips-2000-Feature_Correspondence%3A_A_Markov_Chain_Monte_Carlo_Approach.html">53 nips-2000-Feature Correspondence: A Markov Chain Monte Carlo Approach</a></p>
<p>19 0.26943699 <a title="101-lsi-19" href="./nips-2000-Robust_Reinforcement_Learning.html">113 nips-2000-Robust Reinforcement Learning</a></p>
<p>20 0.2548832 <a title="101-lsi-20" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(9, 0.088), (11, 0.016), (16, 0.053), (21, 0.037), (38, 0.057), (54, 0.041), (61, 0.094), (74, 0.031), (76, 0.028), (80, 0.044), (84, 0.066), (95, 0.326)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73172116 <a title="101-lda-1" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>Author: Angelo Arleo, Fabrizio Smeraldi, Stéphane Hug, Wulfram Gerstner</p><p>Abstract: We model hippocampal place cells and head-direction cells by combining allothetic (visual) and idiothetic (proprioceptive) stimuli. Visual input, provided by a video camera on a miniature robot, is preprocessed by a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. Unsupervised Hebbian learning is employed to incrementally build a population of localized overlapping place fields. Place cells serve as basis functions for reinforcement learning. Experimental results for goal-oriented navigation of a mobile robot are presented.</p><p>2 0.53895074 <a title="101-lda-2" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>Author: Dirk Ormoneit, Peter W. Glynn</p><p>Abstract: Many approaches to reinforcement learning combine neural networks or other parametric function approximators with a form of temporal-difference learning to estimate the value function of a Markov Decision Process. A significant disadvantage of those procedures is that the resulting learning algorithms are frequently unstable. In this work, we present a new, kernel-based approach to reinforcement learning which overcomes this difficulty and provably converges to a unique solution. By contrast to existing algorithms, our method can also be shown to be consistent in the sense that its costs converge to the optimal costs asymptotically. Our focus is on learning in an average-cost framework and on a practical application to the optimal portfolio choice problem. 1</p><p>3 0.47139454 <a title="101-lda-3" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>Author: Sophie Denève, Jean-René Duhamel, Alexandre Pouget</p><p>Abstract: Most models of spatial representations in the cortex assume cells with limited receptive fields that are defined in a particular egocentric frame of reference. However, cells outside of primary sensory cortex are either gain modulated by postural input or partially shifting. We show that solving classical spatial tasks, like sensory prediction, multi-sensory integration, sensory-motor transformation and motor control requires more complicated intermediate representations that are not invariant in one frame of reference. We present an iterative basis function map that performs these spatial tasks optimally with gain modulated and partially shifting units, and tests it against neurophysiological and neuropsychological data. In order to perform an action directed toward an object, it is necessary to have a representation of its spatial location. The brain must be able to use spatial cues coming from different modalities (e.g. vision, audition, touch, proprioception), combine them to infer the position of the object, and compute the appropriate movement. These cues are in different frames of reference corresponding to different sensory or motor modalities. Visual inputs are primarily encoded in retinotopic maps, auditory inputs are encoded in head centered maps and tactile cues are encoded in skin-centered maps. Going from one frame of reference to the other might seem easy. For example, the head-centered position of an object can be approximated by the sum of its retinotopic position and the eye position. However, positions are represented by population codes in the brain, and computing a head-centered map from a retinotopic map is a more complex computation than the underlying sum. Moreover, as we get closer to sensory-motor areas it seems reasonable to assume Spksls 150 100 50 o Figure 1: Response of a VIP cell to visual stimuli appearing in different part of the screen, for three different eye positions. The level of grey represent the frequency of discharge (In spikes per seconds). The white cross is the fixation point (the head is fixed). The cell's receptive field is moving with the eyes, but only partially. Here the receptive field shift is 60% of the total gaze shift. Moreover this cell is gain modulated by eye position (adapted from Duhamel et al). that the representations should be useful for sensory-motor transformations, rather than encode an</p><p>4 0.43375197 <a title="101-lda-4" href="./nips-2000-Structure_Learning_in_Human_Causal_Induction.html">127 nips-2000-Structure Learning in Human Causal Induction</a></p>
<p>Author: Joshua B. Tenenbaum, Thomas L. Griffiths</p><p>Abstract: We use graphical models to explore the question of how people learn simple causal relationships from data. The two leading psychological theories can both be seen as estimating the parameters of a fixed graph. We argue that a complete account of causal induction should also consider how people learn the underlying causal graph structure, and we propose to model this inductive process as a Bayesian inference. Our argument is supported through the discussion of three data sets.</p><p>5 0.42674679 <a title="101-lda-5" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>Author: Shimon Edelman, Nathan Intrator</p><p>Abstract: We describe a unified framework for the understanding of structure representation in primate vision. A model derived from this framework is shown to be effectively systematic in that it has the ability to interpret and associate together objects that are related through a rearrangement of common</p><p>6 0.42484373 <a title="101-lda-6" href="./nips-2000-Learning_Winner-take-all_Competition_Between_Groups_of_Neurons_in_Lateral_Inhibitory_Networks.html">81 nips-2000-Learning Winner-take-all Competition Between Groups of Neurons in Lateral Inhibitory Networks</a></p>
<p>7 0.4064042 <a title="101-lda-7" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>8 0.40080059 <a title="101-lda-8" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>9 0.3983354 <a title="101-lda-9" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>10 0.39715734 <a title="101-lda-10" href="./nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">129 nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<p>11 0.39603183 <a title="101-lda-11" href="./nips-2000-Universality_and_Individuality_in_a_Neural_Code.html">141 nips-2000-Universality and Individuality in a Neural Code</a></p>
<p>12 0.39574301 <a title="101-lda-12" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>13 0.39390782 <a title="101-lda-13" href="./nips-2000-Foundations_for_a_Circuit_Complexity_Theory_of_Sensory_Processing.html">56 nips-2000-Foundations for a Circuit Complexity Theory of Sensory Processing</a></p>
<p>14 0.38853392 <a title="101-lda-14" href="./nips-2000-Modelling_Spatial_Recall%2C_Mental_Imagery_and_Neglect.html">87 nips-2000-Modelling Spatial Recall, Mental Imagery and Neglect</a></p>
<p>15 0.38553539 <a title="101-lda-15" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>16 0.38430157 <a title="101-lda-16" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>17 0.38321605 <a title="101-lda-17" href="./nips-2000-Who_Does_What%3F_A_Novel_Algorithm_to_Determine_Function_Localization.html">147 nips-2000-Who Does What? A Novel Algorithm to Determine Function Localization</a></p>
<p>18 0.38312775 <a title="101-lda-18" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>19 0.38218999 <a title="101-lda-19" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>20 0.38147834 <a title="101-lda-20" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
