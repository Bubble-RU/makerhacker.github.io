<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-101" href="#">nips2000-101</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2000-101-pdf" href="http://papers.nips.cc/paper/1801-place-cells-and-spatial-navigation-based-on-2d-visual-feature-extraction-path-integration-and-reinforcement-learning.pdf">pdf</a></p><p>Author: Angelo Arleo, Fabrizio Smeraldi, Stéphane Hug, Wulfram Gerstner</p><p>Abstract: We model hippocampal place cells and head-direction cells by combining allothetic (visual) and idiothetic (proprioceptive) stimuli. Visual input, provided by a video camera on a miniature robot, is preprocessed by a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. Unsupervised Hebbian learning is employed to incrementally build a population of localized overlapping place fields. Place cells serve as basis functions for reinforcement learning. Experimental results for goal-oriented navigation of a mobile robot are presented.</p><p>Reference: <a title="nips-2000-101-reference" href="../nips2000_reference/nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Gerstner Centre for Neuro-Mimetic Systems, MANTRA Swiss Federal Institute of Technology Lausanne, CH-1015 Lausanne EPFL, Switzerland  Abstract We model hippocampal place cells and head-direction cells by combining allothetic (visual) and idiothetic (proprioceptive) stimuli. [sent-5, score-1.158]
</p><p>2 Visual input, provided by a video camera on a miniature robot, is preprocessed by a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. [sent-6, score-0.232]
</p><p>3 Unsupervised Hebbian learning is employed to incrementally build a population of localized overlapping place fields. [sent-7, score-0.732]
</p><p>4 Place cells serve as basis functions for reinforcement learning. [sent-8, score-0.32]
</p><p>5 Experimental results for goal-oriented navigation of a mobile robot are presented. [sent-9, score-0.545]
</p><p>6 1 Introduction In order to achieve spatial learning, both animals and artificial agents need to autonomously locate themselves based on available sensory information. [sent-10, score-0.177]
</p><p>7 Neurophysiological findings suggest the spatial self-localization of rodents is supported by place-sensitive and directionsensitive cells. [sent-11, score-0.183]
</p><p>8 Place cells in the rat Hippocampus provide a spatial representation in allocentric coordinates [1]. [sent-12, score-0.472]
</p><p>9 A place cell exhibits a high firing rate only when the animal is in a specific region of the environment, which defines the place field of the cell. [sent-13, score-1.19]
</p><p>10 Head-direction cells observed in the hippocampal formation encode the animal's allocentric heading in the azimuthal plane [2]. [sent-14, score-0.499]
</p><p>11 A directional cell fires maximally only when the animal's heading is equal to the cell's preferred direction, regardless of the orientation of the head relative to the body, of the rat's location, or of the animal's behavior. [sent-15, score-0.358]
</p><p>12 (i) How do we get place fields from visual input [3]? [sent-17, score-0.621]
</p><p>13 This question is non-trivial given that visual input depends on the direction of gaze. [sent-18, score-0.195]
</p><p>14 We present a computational model which is consistent with several neurophysiological findings concerning biological head-direction and place cells. [sent-19, score-0.467]
</p><p>15 Place-coding and directional sense are provided by two coupled neural systems, which interact with each other to form a single substrate for spatial navigation (Fig. [sent-20, score-0.327]
</p><p>16 The resulting representation consists of overlapping place fields with properties similar to those of hippocampal place cells. [sent-27, score-1.117]
</p><p>17 (iiJ What's the use of place cells for navigation ·Corresponding author, angeio. [sent-28, score-0.728]
</p><p>18 (b) A visual scene acquired by the robot during spatial learning. [sent-33, score-0.688]
</p><p>19 The retinotopic sampling grid (white crosses) is employed to sample visual data by means of Gabor decomposition. [sent-35, score-0.263]
</p><p>20 Black circles represent maximally responding Gabor filters (the circle radius varies as a function of the filter's spatial frequency). [sent-36, score-0.227]
</p><p>21 We show that a representation by overlapping place fields is a natural "state space" for reinforcement learning. [sent-38, score-0.689]
</p><p>22 A direct implementation of reinforcement learning on real visual streams would be impossible given the high dimensionality of the visual input space. [sent-39, score-0.451]
</p><p>23 A place field representation extracts the low-dimensional view manifold on which efficient reinforcement learning is possible. [sent-40, score-0.756]
</p><p>24 Eight infrared sensors provide obstacle detection and measure ambient light. [sent-43, score-0.159]
</p><p>25 Since our robot moves on a twodimensional space with a camera pointing in the direction of motion, the high-dimensional visual space is not uniformly filled. [sent-48, score-0.621]
</p><p>26 This low-dimensional description of the visual space is referred to as view manifold [4]. [sent-50, score-0.247]
</p><p>27 2 Extracting the low-dimensional view manifold Hippocampal place fields are determined by a combination of highly-processed multimodal sensory stimuli (e. [sent-51, score-0.593]
</p><p>28 Nevertheless, experiments on rodents suggest that vision plays an eminent role in determining place cell activity [5]. [sent-54, score-0.713]
</p><p>29 Here, we focus on  the visual pathway, and we propose a processing in four steps. [sent-55, score-0.156]
</p><p>30 As a first step, we place a retinotopic sampling grid on the image (Fig. [sent-56, score-0.556]
</p><p>31 In total we have 31 grid points with high resolution only in a localized region of the view field (fovea), whereas peripheral areas are characterized by a low-resolution vision [6]. [sent-58, score-0.215]
</p><p>32 At each point of the grid we place 24 Gabor filters with different orientations and amplitudes. [sent-59, score-0.52]
</p><p>33 Gabor filters [7] provide a suitable mathematical model for biological simple cells [8]. [sent-60, score-0.341]
</p><p>34 As a second step, we take the magnitude of the responses of these Gabor filters for detecting visual properties within video streams. [sent-68, score-0.243]
</p><p>35 While the Gabor filter itself has properties related to simple cells, the amplitude of the complex response does not depend on the exact position within the receptive field and has therefore properties similar to cortical complex cells. [sent-69, score-0.169]
</p><p>36 The third step within the visual pathway of our model, consists of interpreting visual cues by means of neural activity. [sent-73, score-0.435]
</p><p>37 We take a population of hypothetical snapshot cells (SnC in Fig. [sent-74, score-0.463]
</p><p>38 Given a new image I, a snapshot cell S E SnC is created which receives afferents from all /k filters. [sent-77, score-0.388]
</p><p>39 Connections from filters Jk to cell s are initialized according to W s k = rk, Vk E K. [sent-78, score-0.27]
</p><p>40 If, at a later point, the robot sees an image I', the firing activity r s of cell s E SnC is computed by  (4) r s - e - (-k L k h- W skl )2 / 2a where rk are the Gabor filter responses to image I'. [sent-79, score-0.922]
</p><p>41 4 defines a radial basis function in the filter space that measures the similarity of the current image to the image stored in the weights W s k. [sent-81, score-0.172]
</p><p>42 2  As final step, we apply unsupervised Hebbian learning to achieve spatial coding one synapse downstream from the SnC layer (sLEC in Fig. [sent-83, score-0.255]
</p><p>43 Indeed, the snapshot cell activity r s defined in Eq. [sent-85, score-0.384]
</p><p>44 4 depends on the robot's gaze direction, and does not code for a spatial location. [sent-86, score-0.177]
</p><p>45 In order to collect information from several gaze directions, the robot takes four snapshots corresponding to north, east, south, and west views at each location visited during exploration. [sent-87, score-0.545]
</p><p>46 For each visited location the robot creates four SnC snapshot cells, which are bound together to form a place cell in the sLEC layer. [sent-89, score-1.193]
</p><p>47 Thus, sLEC cell activity depends on a combination of several visual cues, which results in non-directional place fields (Fig. [sent-90, score-0.897]
</p><p>48 00  00  Figure 2: (a) A sample of spatial receptive field for a sLEC cell in our model. [sent-92, score-0.43]
</p><p>49 The lighter a region, the higher the cell's firing rate when the robot is in that region of the arena. [sent-93, score-0.517]
</p><p>50 (b) A typical place field in the CA3-CAllayer of the model. [sent-94, score-0.465]
</p><p>51 3 Hippocampal CAI-CA3 place field representation When relying on visual data only, the state space representation encoded by place cells does not fulfill the Markov hypothesis [12]. [sent-95, score-1.352]
</p><p>52 Indeed, distinct areas of the environment may provide identical visual cues and lead to singularities in the view manifold (sensory input aliasing). [sent-96, score-0.37]
</p><p>53 We employ idiothetic signals along with visual information in order to remove such singularities and solve the hidden-state problem. [sent-97, score-0.32]
</p><p>54 A fundamental contribution to build the sMEC idiothetic space representation comes from head-direction cells (projection B in Fig. [sent-100, score-0.445]
</p><p>55 As the robot moves, sMEC cell activity changes according to self-motion signals and to the current heading of the robot as estimated by the directional system. [sent-102, score-1.184]
</p><p>56 The firing activity T m of a cell m E sMEC is given by Tm = exp( - (Sdr - sm)2/2(J2), where Sdr is the robot's current position estimated by dead-reckoning, sm is the center of the receptive field of cell m, and (J is the width of the Gaussian field. [sent-103, score-0.684]
</p><p>57 , sLEC and sMEC place field representations, respectively) converge onto CA3-CAI regions to form a stable spatial representation (Fig. [sent-106, score-0.666]
</p><p>58 On the one hand, unreliable visual data are compensated for by means of path integration. [sent-108, score-0.224]
</p><p>59 On the other hand, reliable visual information can calibrate the path integrator system and maintain the dead-reckoning error bounded over time. [sent-109, score-0.261]
</p><p>60 Correlational learning is applied to combine visual cues and path integration over time. [sent-110, score-0.347]
</p><p>61 CA3-CA 1 cells are recruited incrementally as exploration proceeds. [sent-111, score-0.357]
</p><p>62 For each new location, connections are established from all simultaneously active cells in sLEC and sMEC to newly recruited CA3-CA1 cells. [sent-112, score-0.271]
</p><p>63 After learning, the CA3-CA1 space representation consists of a population of localized overlapping place fields (Fig. [sent-114, score-0.74]
</p><p>64 3(a) shows an example of distribution of CA3-CA1 place cells after learning. [sent-117, score-0.609]
</p><p>65 In this experiment, the robot, starting from an empty population, recruited about 1000 CA3-CA 1 place cells. [sent-118, score-0.45]
</p><p>66 In order to interpret the information represented by the ensemble CA3-CA 1 pattern of activity, we employ population vector coding [13, 14]. [sent-119, score-0.184]
</p><p>67 Let s be the unknown robot's location, Ti (S) the firing activity of a CA3-CA 1 place cell i, and Si the center of its place field . [sent-120, score-1.253]
</p><p>68 The population vector p is given by the center of mass of the network activity: p = Li Si Ti(S)/ L i Ti(S). [sent-121, score-0.164]
</p><p>69 3(a) the center of mass coding for the robot's location is represented by the black cross. [sent-124, score-0.178]
</p><p>70 (b)  (a)  Figure 3: (a) The ensemble activity of approximately 1000 CA3-CAI place cells created by the robot during spatial learning. [sent-239, score-1.329]
</p><p>71 (b) Vector field representation of a navigational map learned after 5 trials. [sent-243, score-0.35]
</p><p>72 4 Action learning: Goal-oriented navigation The above spatial model enables the robot to localize itself within the environment. [sent-246, score-0.651]
</p><p>73 To support cognitive spatial behavior [1], the hippocampal circuit must also allow the robot to learn navigational maps autonomously. [sent-247, score-0.801]
</p><p>74 Our CA3-CAI population provides an incrementally learned coarse coding representation suitable for applying reinforcement learning for continuous high-dimensional state spaces. [sent-248, score-0.422]
</p><p>75 Learning an action-value function over a continuous location space endows the system with spatial generalization capabilities. [sent-249, score-0.209]
</p><p>76 We apply a Q(),) learning scheme [16] to build navigational maps [17, 18]. [sent-250, score-0.203]
</p><p>77 The overlapping localized CA3-CA 1 place fields provide a natural set of basis functions that can be used to learn a parameterized form of the Q(),) function [19]. [sent-251, score-0.586]
</p><p>78 Our representation also solves the problem of ambiguous input or partially hidden states [12], therefore the current state is fully known to the system and reinforcement learning can be applied in a straightforward manner. [sent-254, score-0.2]
</p><p>79 Let ri denote the activation of a CA3-CAI place cell i. [sent-255, score-0.577]
</p><p>80 Each state s is encoded by the ensemble place cell activity vector f(8) = (rl (8) , rds), . [sent-256, score-0.723]
</p><p>81 ,rn (8)), where n is the number of created place cells. [sent-259, score-0.436]
</p><p>82 '1  /  /' _  _ _ _ //_ _____ / / / _  \ ' , _______ / / __ \ , , ______ - - - __  I 1 \ ' , , __ __ ___ - __ _  1",\11,1\",  \ \  \'" \ "  -- ------ -- -- -- -- ------(b)  (a)  Figure 4: Two samples of learned navigational maps. [sent-294, score-0.17]
</p><p>83 The obstacle (dark grey object) is "transparent" with respect to vision, while it is detectable by the robot's infrared sensors. [sent-295, score-0.195]
</p><p>84 (a) The map learned by the robot after 20 training paths. [sent-296, score-0.48]
</p><p>85 (b) The map learned by the robot after 80 training trials. [sent-297, score-0.48]
</p><p>86 The update of the eligibility trace depends on whether the robot selects an exploratory or an exploiting action . [sent-301, score-0.463]
</p><p>87 When the robot reaches the target, a new training path begins at a new random location. [sent-308, score-0.46]
</p><p>88 3(b) shows an example of navigational map learned after 5 training trials . [sent-310, score-0.218]
</p><p>89 4 shows some results obtained by adding an obstacle within the arena after the place field representation has been learned. [sent-312, score-0.663]
</p><p>90 It contains proper goal-oriented information, whereas it does not provide obstacle avoidance accuratelyl . [sent-315, score-0.159]
</p><p>91 4(b) displays a navigational map learned by the robot after 80 training paths. [sent-317, score-0.61]
</p><p>92 Due to longer training, the map provides both appropriate goal-oriented and obstacle avoidance behavior. [sent-318, score-0.207]
</p><p>93 Many of sampled locations were not visited by the robot during training, which confirms the generalization capabilities of the method. [sent-321, score-0.439]
</p><p>94 That is, the robot was able to associate appropriate goal-oriented actions to never experienced spatial positions. [sent-322, score-0.532]
</p><p>95 We have shown that by means of an appropriate state space representation, based on localized overlapping place fields, the robot can learn goal-oriented behavior after only 5 training trials (without obstacles). [sent-324, score-0.907]
</p><p>96 INote that this does not really impair the robot's goal-oriented behavior, since obstacle avoidance is supported by a low-level reactive module driven by infrared sensors. [sent-326, score-0.224]
</p><p>97 Head direction cells recorded from the postsubiculum in freely moving rats. [sent-343, score-0.297]
</p><p>98 Geometric determinants of the place fields of hippocampal neurons. [sent-350, score-0.604]
</p><p>99 Modeling rodent head-direction cells and place cells for spatial learning in bio-mimetic robotics. [sent-396, score-0.998]
</p><p>100 Spatial cognition and neuro-mimetic navigation: A model of hippocampal place cell activity. [sent-409, score-0.716]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('place', 0.394), ('robot', 0.392), ('gabor', 0.243), ('cells', 0.215), ('cell', 0.183), ('slec', 0.173), ('smec', 0.173), ('visual', 0.156), ('spatial', 0.14), ('hippocampal', 0.139), ('idiothetic', 0.13), ('navigational', 0.13), ('snc', 0.13), ('navigation', 0.119), ('snapshot', 0.108), ('reinforcement', 0.105), ('obstacle', 0.094), ('population', 0.093), ('activity', 0.093), ('cues', 0.089), ('filters', 0.087), ('ura', 0.087), ('firing', 0.082), ('fields', 0.071), ('field', 0.071), ('hippocampus', 0.069), ('location', 0.069), ('path', 0.068), ('retinotopic', 0.068), ('directional', 0.068), ('animal', 0.066), ('allothetic', 0.065), ('arleo', 0.065), ('avoidance', 0.065), ('infrared', 0.065), ('smeraldi', 0.065), ('localized', 0.063), ('filter', 0.062), ('representation', 0.061), ('overlapping', 0.058), ('recruited', 0.056), ('allocentric', 0.056), ('heading', 0.056), ('image', 0.055), ('ensemble', 0.053), ('incrementally', 0.051), ('morris', 0.051), ('head', 0.051), ('cp', 0.051), ('manifold', 0.049), ('map', 0.048), ('hypothetical', 0.047), ('visited', 0.047), ('angular', 0.044), ('arena', 0.043), ('cpj', 0.043), ('downstream', 0.043), ('halmstad', 0.043), ('lausanne', 0.043), ('lighter', 0.043), ('miniature', 0.043), ('postsubiculum', 0.043), ('proprioceptive', 0.043), ('rodents', 0.043), ('sdr', 0.043), ('view', 0.042), ('created', 0.042), ('hebbian', 0.042), ('st', 0.04), ('learned', 0.04), ('direction', 0.039), ('build', 0.039), ('grid', 0.039), ('biological', 0.039), ('coding', 0.038), ('gaze', 0.037), ('wilson', 0.037), ('superficial', 0.037), ('entorhinal', 0.037), ('eligibility', 0.037), ('integrator', 0.037), ('sensory', 0.037), ('grey', 0.036), ('center', 0.036), ('receptive', 0.036), ('exploration', 0.035), ('mass', 0.035), ('action', 0.034), ('learning', 0.034), ('camera', 0.034), ('neurophysiological', 0.034), ('pathway', 0.034), ('singularities', 0.034), ('mobile', 0.034), ('nucleus', 0.034), ('qw', 0.034), ('rats', 0.034), ('wy', 0.034), ('translations', 0.034), ('plane', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="101-tfidf-1" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>Author: Angelo Arleo, Fabrizio Smeraldi, Stéphane Hug, Wulfram Gerstner</p><p>Abstract: We model hippocampal place cells and head-direction cells by combining allothetic (visual) and idiothetic (proprioceptive) stimuli. Visual input, provided by a video camera on a miniature robot, is preprocessed by a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. Unsupervised Hebbian learning is employed to incrementally build a population of localized overlapping place fields. Place cells serve as basis functions for reinforcement learning. Experimental results for goal-oriented navigation of a mobile robot are presented.</p><p>2 0.20998621 <a title="101-tfidf-2" href="./nips-2000-Modelling_Spatial_Recall%2C_Mental_Imagery_and_Neglect.html">87 nips-2000-Modelling Spatial Recall, Mental Imagery and Neglect</a></p>
<p>Author: Suzanna Becker, Neil Burgess</p><p>Abstract: We present a computational model of the neural mechanisms in the parietal and temporal lobes that support spatial navigation, recall of scenes and imagery of the products of recall. Long term representations are stored in the hippocampus, and are associated with local spatial and object-related features in the parahippocampal region. Viewer-centered representations are dynamically generated from long term memory in the parietal part of the model. The model thereby simulates recall and imagery of locations and objects in complex environments. After parietal damage, the model exhibits hemispatial neglect in mental imagery that rotates with the imagined perspective of the observer, as in the famous Milan Square experiment [1]. Our model makes novel predictions for the neural representations in the parahippocampal and parietal regions and for behavior in healthy volunteers and neuropsychological patients.</p><p>3 0.20665129 <a title="101-tfidf-3" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>Author: Sophie Denève, Jean-René Duhamel, Alexandre Pouget</p><p>Abstract: Most models of spatial representations in the cortex assume cells with limited receptive fields that are defined in a particular egocentric frame of reference. However, cells outside of primary sensory cortex are either gain modulated by postural input or partially shifting. We show that solving classical spatial tasks, like sensory prediction, multi-sensory integration, sensory-motor transformation and motor control requires more complicated intermediate representations that are not invariant in one frame of reference. We present an iterative basis function map that performs these spatial tasks optimally with gain modulated and partially shifting units, and tests it against neurophysiological and neuropsychological data. In order to perform an action directed toward an object, it is necessary to have a representation of its spatial location. The brain must be able to use spatial cues coming from different modalities (e.g. vision, audition, touch, proprioception), combine them to infer the position of the object, and compute the appropriate movement. These cues are in different frames of reference corresponding to different sensory or motor modalities. Visual inputs are primarily encoded in retinotopic maps, auditory inputs are encoded in head centered maps and tactile cues are encoded in skin-centered maps. Going from one frame of reference to the other might seem easy. For example, the head-centered position of an object can be approximated by the sum of its retinotopic position and the eye position. However, positions are represented by population codes in the brain, and computing a head-centered map from a retinotopic map is a more complex computation than the underlying sum. Moreover, as we get closer to sensory-motor areas it seems reasonable to assume Spksls 150 100 50 o Figure 1: Response of a VIP cell to visual stimuli appearing in different part of the screen, for three different eye positions. The level of grey represent the frequency of discharge (In spikes per seconds). The white cross is the fixation point (the head is fixed). The cell's receptive field is moving with the eyes, but only partially. Here the receptive field shift is 60% of the total gaze shift. Moreover this cell is gain modulated by eye position (adapted from Duhamel et al). that the representations should be useful for sensory-motor transformations, rather than encode an</p><p>4 0.12407416 <a title="101-tfidf-4" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>Author: Shimon Edelman, Nathan Intrator</p><p>Abstract: We describe a unified framework for the understanding of structure representation in primate vision. A model derived from this framework is shown to be effectively systematic in that it has the ability to interpret and associate together objects that are related through a rearrangement of common</p><p>5 0.12345298 <a title="101-tfidf-5" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>Author: Odelia Schwartz, Eero P. Simoncelli</p><p>Abstract: We explore the statistical properties of natural sound stimuli preprocessed with a bank of linear filters. The responses of such filters exhibit a striking form of statistical dependency, in which the response variance of each filter grows with the response amplitude of filters tuned for nearby frequencies. These dependencies may be substantially reduced using an operation known as divisive normalization, in which the response of each filter is divided by a weighted sum of the rectified responses of other filters. The weights may be chosen to maximize the independence of the normalized responses for an ensemble of natural sounds. We demonstrate that the resulting model accounts for nonlinearities in the response characteristics of the auditory nerve, by comparing model simulations to electrophysiological recordings. In previous work (NIPS, 1998) we demonstrated that an analogous model derived from the statistics of natural images accounts for non-linear properties of neurons in primary visual cortex. Thus, divisive normalization appears to be a generic mechanism for eliminating a type of statistical dependency that is prevalent in natural signals of different modalities. Signals in the real world are highly structured. For example, natural sounds typically contain both harmonic and rythmic structure. It is reasonable to assume that biological auditory systems are designed to represent these structures in an efficient manner [e.g., 1,2]. Specifically, Barlow hypothesized that a role of early sensory processing is to remove redundancy in the sensory input, resulting in a set of neural responses that are statistically independent. Experimentally, one can test this hypothesis by examining the statistical properties of neural responses under natural stimulation conditions [e.g., 3,4], or the statistical dependency of pairs (or groups) of neural responses. Due to their technical difficulty, such multi-cellular experiments are only recently becoming possible, and the earliest reports in vision appear consistent with the hypothesis [e.g., 5]. An alternative approach, which we follow here, is to develop a neural model from the statistics of natural signals and show that response properties of this model are similar to those of biological sensory neurons. A number of researchers have derived linear filter models using statistical criterion. For visual images, this results in linear filters localized in frequency, orientation and phase [6, 7]. Similar work in audition has yielded filters localized in frequency and phase [8]. Although these linear models provide an important starting point for neural modeling, sensory neurons are highly nonlinear. In addition, the statistical properties of natural signals are too complex to expect a linear transformation to result in an independent set of components. Recent results indicate that nonlinear gain control plays an important role in neural processing. Ruderman and Bialek [9] have shown that division by a local estimate of standard deviation can increase the entropy of responses of center-surround filters to natural images. Such a model is consistent with the properties of neurons in the retina and lateral geniculate nucleus. Heeger and colleagues have shown that the nonlinear behaviors of neurons in primary visual cortex may be described using a form of gain control known as divisive normalization [10], in which the response of a linear kernel is rectified and divided by the sum of other rectified kernel responses and a constant. We have recently shown that the responses of oriented linear filters exhibit nonlinear statistical dependencies that may be substantially reduced using a variant of this model, in which the normalization signal is computed from a weighted sum of other rectified kernel responses [11, 12]. The resulting model, with weighting parameters determined from image statistics, accounts qualitatively for physiological nonlinearities observed in primary visual cortex. In this paper, we demonstrate that the responses of bandpass linear filters to natural sounds exhibit striking statistical dependencies, analogous to those found in visual images. A divisive normalization procedure can substantially remove these dependencies. We show that this model, with parameters optimized for a collection of natural sounds, can account for nonlinear behaviors of neurons at the level of the auditory nerve. Specifically, we show that: 1) the shape offrequency tuning curves varies with sound pressure level, even though the underlying linear filters are fixed; and 2) superposition of a non-optimal tone suppresses the response of a linear filter in a divisive fashion, and the amount of suppression depends on the distance between the frequency of the tone and the preferred frequency of the filter. 1 Empirical observations of natural sound statistics The basic statistical properties of natural sounds, as observed through a linear filter, have been previously documented by Attias [13]. In particular, he showed that, as with visual images, the spectral energy falls roughly according to a power law, and that the histograms of filter responses are more kurtotic than a Gaussian (i.e., they have a sharp peak at zero, and very long tails). Here we examine the joint statistical properties of a pair of linear filters tuned for nearby temporal frequencies. We choose a fixed set of filters that have been widely used in modeling the peripheral auditory system [14]. Figure 1 shows joint histograms of the instantaneous responses of a particular pair of linear filters to five different types of natural sound, and white noise. First note that the responses are approximately decorrelated: the expected value of the y-axis value is roughly zero for all values of the x-axis variable. The responses are not, however, statistically independent: the width of the distribution of responses of one filter increases with the response amplitude of the other filter. If the two responses were statistically independent, then the response of the first filter should not provide any information about the distribution of responses of the other filter. We have found that this type of variance dependency (sometimes accompanied by linear correlation) occurs in a wide range of natural sounds, ranging from animal sounds to music. We emphasize that this dependency is a property of natural sounds, and is not due purely to our choice of linear filters. For example, no such dependency is observed when the input consists of white noise (see Fig. 1). The strength of this dependency varies for different pairs of linear filters . In addition, we see this type of dependency between instantaneous responses of a single filter at two Speech o -1 Drums • Monkey Cat White noise Nocturnal nature I~ ~; ~ • Figure 1: Joint conditional histogram of instantaneous linear responses of two bandpass filters with center frequencies 2000 and 2840 Hz. Pixel intensity corresponds to frequency of occurrence of a given pair of values, except that each column has been independently rescaled to fill the full intensity range. For the natural sounds, responses are not independent: the standard deviation of the ordinate is roughly proportional to the magnitude of the abscissa. Natural sounds were recorded from CDs and converted to sampling frequency of 22050 Hz. nearby time instants. Since the dependency involves the variance of the responses, we can substantially reduce it by dividing. In particular, the response of each filter is divided by a weighted sum of responses of other rectified filters and an additive constant. Specifically: L2 Ri = 2: (1) 12 j WjiLj + 0'2 where Li is the instantaneous linear response of filter i, strength of suppression of filter i by filter j. 0' is a constant and Wji controls the We would like to choose the parameters of the model (the weights Wji, and the constant 0') to optimize the independence of the normalized response to an ensemble of natural sounds. Such an optimization is quite computationally expensive. We instead assume a Gaussian form for the underlying conditional distribution, as described in [15]: P (LiILj,j E Ni ) '</p><p>6 0.11880311 <a title="101-tfidf-6" href="./nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">42 nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<p>7 0.11508536 <a title="101-tfidf-7" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>8 0.10105678 <a title="101-tfidf-8" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>9 0.093910672 <a title="101-tfidf-9" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>10 0.089918643 <a title="101-tfidf-10" href="./nips-2000-Four-legged_Walking_Gait_Control_Using_a_Neuromorphic_Chip_Interfaced_to_a_Support_Vector_Learning_Algorithm.html">57 nips-2000-Four-legged Walking Gait Control Using a Neuromorphic Chip Interfaced to a Support Vector Learning Algorithm</a></p>
<p>11 0.085178949 <a title="101-tfidf-11" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>12 0.078832567 <a title="101-tfidf-12" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>13 0.077264972 <a title="101-tfidf-13" href="./nips-2000-Bayes_Networks_on_Ice%3A_Robotic_Search_for_Antarctic_Meteorites.html">29 nips-2000-Bayes Networks on Ice: Robotic Search for Antarctic Meteorites</a></p>
<p>14 0.072061323 <a title="101-tfidf-14" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>15 0.068894468 <a title="101-tfidf-15" href="./nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">129 nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<p>16 0.068383515 <a title="101-tfidf-16" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>17 0.066525981 <a title="101-tfidf-17" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>18 0.066312127 <a title="101-tfidf-18" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>19 0.065486513 <a title="101-tfidf-19" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>20 0.06393905 <a title="101-tfidf-20" href="./nips-2000-Programmable_Reinforcement_Learning_Agents.html">105 nips-2000-Programmable Reinforcement Learning Agents</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.21), (1, -0.213), (2, -0.083), (3, 0.0), (4, -0.139), (5, 0.04), (6, 0.199), (7, -0.097), (8, 0.243), (9, -0.103), (10, -0.014), (11, 0.13), (12, -0.159), (13, -0.036), (14, 0.019), (15, -0.018), (16, 0.091), (17, 0.022), (18, 0.063), (19, 0.03), (20, -0.08), (21, 0.025), (22, -0.031), (23, 0.008), (24, -0.179), (25, -0.201), (26, -0.071), (27, -0.001), (28, -0.047), (29, 0.017), (30, 0.058), (31, -0.134), (32, 0.058), (33, 0.054), (34, 0.045), (35, 0.066), (36, 0.166), (37, 0.095), (38, 0.034), (39, -0.053), (40, -0.153), (41, 0.015), (42, 0.033), (43, -0.121), (44, -0.003), (45, -0.002), (46, 0.032), (47, 0.054), (48, -0.042), (49, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97747916 <a title="101-lsi-1" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>Author: Angelo Arleo, Fabrizio Smeraldi, Stéphane Hug, Wulfram Gerstner</p><p>Abstract: We model hippocampal place cells and head-direction cells by combining allothetic (visual) and idiothetic (proprioceptive) stimuli. Visual input, provided by a video camera on a miniature robot, is preprocessed by a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. Unsupervised Hebbian learning is employed to incrementally build a population of localized overlapping place fields. Place cells serve as basis functions for reinforcement learning. Experimental results for goal-oriented navigation of a mobile robot are presented.</p><p>2 0.79806429 <a title="101-lsi-2" href="./nips-2000-Modelling_Spatial_Recall%2C_Mental_Imagery_and_Neglect.html">87 nips-2000-Modelling Spatial Recall, Mental Imagery and Neglect</a></p>
<p>Author: Suzanna Becker, Neil Burgess</p><p>Abstract: We present a computational model of the neural mechanisms in the parietal and temporal lobes that support spatial navigation, recall of scenes and imagery of the products of recall. Long term representations are stored in the hippocampus, and are associated with local spatial and object-related features in the parahippocampal region. Viewer-centered representations are dynamically generated from long term memory in the parietal part of the model. The model thereby simulates recall and imagery of locations and objects in complex environments. After parietal damage, the model exhibits hemispatial neglect in mental imagery that rotates with the imagined perspective of the observer, as in the famous Milan Square experiment [1]. Our model makes novel predictions for the neural representations in the parahippocampal and parietal regions and for behavior in healthy volunteers and neuropsychological patients.</p><p>3 0.79052573 <a title="101-lsi-3" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>Author: Sophie Denève, Jean-René Duhamel, Alexandre Pouget</p><p>Abstract: Most models of spatial representations in the cortex assume cells with limited receptive fields that are defined in a particular egocentric frame of reference. However, cells outside of primary sensory cortex are either gain modulated by postural input or partially shifting. We show that solving classical spatial tasks, like sensory prediction, multi-sensory integration, sensory-motor transformation and motor control requires more complicated intermediate representations that are not invariant in one frame of reference. We present an iterative basis function map that performs these spatial tasks optimally with gain modulated and partially shifting units, and tests it against neurophysiological and neuropsychological data. In order to perform an action directed toward an object, it is necessary to have a representation of its spatial location. The brain must be able to use spatial cues coming from different modalities (e.g. vision, audition, touch, proprioception), combine them to infer the position of the object, and compute the appropriate movement. These cues are in different frames of reference corresponding to different sensory or motor modalities. Visual inputs are primarily encoded in retinotopic maps, auditory inputs are encoded in head centered maps and tactile cues are encoded in skin-centered maps. Going from one frame of reference to the other might seem easy. For example, the head-centered position of an object can be approximated by the sum of its retinotopic position and the eye position. However, positions are represented by population codes in the brain, and computing a head-centered map from a retinotopic map is a more complex computation than the underlying sum. Moreover, as we get closer to sensory-motor areas it seems reasonable to assume Spksls 150 100 50 o Figure 1: Response of a VIP cell to visual stimuli appearing in different part of the screen, for three different eye positions. The level of grey represent the frequency of discharge (In spikes per seconds). The white cross is the fixation point (the head is fixed). The cell's receptive field is moving with the eyes, but only partially. Here the receptive field shift is 60% of the total gaze shift. Moreover this cell is gain modulated by eye position (adapted from Duhamel et al). that the representations should be useful for sensory-motor transformations, rather than encode an</p><p>4 0.42572337 <a title="101-lsi-4" href="./nips-2000-Four-legged_Walking_Gait_Control_Using_a_Neuromorphic_Chip_Interfaced_to_a_Support_Vector_Learning_Algorithm.html">57 nips-2000-Four-legged Walking Gait Control Using a Neuromorphic Chip Interfaced to a Support Vector Learning Algorithm</a></p>
<p>Author: Susanne Still, Bernhard Schölkopf, Klaus Hepp, Rodney J. Douglas</p><p>Abstract: To control the walking gaits of a four-legged robot we present a novel neuromorphic VLSI chip that coordinates the relative phasing of the robot's legs similar to how spinal Central Pattern Generators are believed to control vertebrate locomotion [3]. The chip controls the leg movements by driving motors with time varying voltages which are the outputs of a small network of coupled oscillators. The characteristics of the chip's output voltages depend on a set of input parameters. The relationship between input parameters and output voltages can be computed analytically for an idealized system. In practice, however, this ideal relationship is only approximately true due to transistor mismatch and offsets. Fine tuning of the chip's input parameters is done automatically by the robotic system, using an unsupervised Support Vector (SV) learning algorithm introduced recently [7]. The learning requires only that the description of the desired output is given. The machine learns from (unlabeled) examples how to set the parameters to the chip in order to obtain a desired motor behavior.</p><p>5 0.41532287 <a title="101-lsi-5" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>Author: Shimon Edelman, Nathan Intrator</p><p>Abstract: We describe a unified framework for the understanding of structure representation in primate vision. A model derived from this framework is shown to be effectively systematic in that it has the ability to interpret and associate together objects that are related through a rearrangement of common</p><p>6 0.40584987 <a title="101-lsi-6" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>7 0.38911465 <a title="101-lsi-7" href="./nips-2000-Bayes_Networks_on_Ice%3A_Robotic_Search_for_Antarctic_Meteorites.html">29 nips-2000-Bayes Networks on Ice: Robotic Search for Antarctic Meteorites</a></p>
<p>8 0.38150021 <a title="101-lsi-8" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>9 0.32417011 <a title="101-lsi-9" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>10 0.3203592 <a title="101-lsi-10" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>11 0.31113702 <a title="101-lsi-11" href="./nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">42 nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<p>12 0.28845054 <a title="101-lsi-12" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>13 0.27723676 <a title="101-lsi-13" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>14 0.2758269 <a title="101-lsi-14" href="./nips-2000-Robust_Reinforcement_Learning.html">113 nips-2000-Robust Reinforcement Learning</a></p>
<p>15 0.2685363 <a title="101-lsi-15" href="./nips-2000-Programmable_Reinforcement_Learning_Agents.html">105 nips-2000-Programmable Reinforcement Learning Agents</a></p>
<p>16 0.254583 <a title="101-lsi-16" href="./nips-2000-Hippocampally-Dependent_Consolidation_in_a_Hierarchical_Model_of_Neocortex.html">66 nips-2000-Hippocampally-Dependent Consolidation in a Hierarchical Model of Neocortex</a></p>
<p>17 0.23229125 <a title="101-lsi-17" href="./nips-2000-Machine_Learning_for_Video-Based_Rendering.html">83 nips-2000-Machine Learning for Video-Based Rendering</a></p>
<p>18 0.22710346 <a title="101-lsi-18" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>19 0.22451602 <a title="101-lsi-19" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>20 0.22176491 <a title="101-lsi-20" href="./nips-2000-Smart_Vision_Chip_Fabricated_Using_Three_Dimensional_Integration_Technology.html">118 nips-2000-Smart Vision Chip Fabricated Using Three Dimensional Integration Technology</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.015), (4, 0.024), (10, 0.025), (17, 0.137), (32, 0.014), (33, 0.026), (36, 0.011), (38, 0.352), (42, 0.023), (54, 0.017), (55, 0.032), (62, 0.047), (65, 0.022), (67, 0.045), (76, 0.025), (79, 0.022), (81, 0.046), (90, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87574393 <a title="101-lda-1" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>Author: Angelo Arleo, Fabrizio Smeraldi, Stéphane Hug, Wulfram Gerstner</p><p>Abstract: We model hippocampal place cells and head-direction cells by combining allothetic (visual) and idiothetic (proprioceptive) stimuli. Visual input, provided by a video camera on a miniature robot, is preprocessed by a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. Unsupervised Hebbian learning is employed to incrementally build a population of localized overlapping place fields. Place cells serve as basis functions for reinforcement learning. Experimental results for goal-oriented navigation of a mobile robot are presented.</p><p>2 0.70104831 <a title="101-lda-2" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>Author: Carl Edward Rasmussen, Zoubin Ghahramani</p><p>Abstract: The Bayesian paradigm apparently only sometimes gives rise to Occam's Razor; at other times very large models perform well. We give simple examples of both kinds of behaviour. The two views are reconciled when measuring complexity of functions, rather than of the machinery used to implement them. We analyze the complexity of functions for some linear in the parameter models that are equivalent to Gaussian Processes, and always find Occam's Razor at work.</p><p>3 0.48799491 <a title="101-lda-3" href="./nips-2000-Modelling_Spatial_Recall%2C_Mental_Imagery_and_Neglect.html">87 nips-2000-Modelling Spatial Recall, Mental Imagery and Neglect</a></p>
<p>Author: Suzanna Becker, Neil Burgess</p><p>Abstract: We present a computational model of the neural mechanisms in the parietal and temporal lobes that support spatial navigation, recall of scenes and imagery of the products of recall. Long term representations are stored in the hippocampus, and are associated with local spatial and object-related features in the parahippocampal region. Viewer-centered representations are dynamically generated from long term memory in the parietal part of the model. The model thereby simulates recall and imagery of locations and objects in complex environments. After parietal damage, the model exhibits hemispatial neglect in mental imagery that rotates with the imagined perspective of the observer, as in the famous Milan Square experiment [1]. Our model makes novel predictions for the neural representations in the parahippocampal and parietal regions and for behavior in healthy volunteers and neuropsychological patients.</p><p>4 0.48060459 <a title="101-lda-4" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>Author: Sophie Denève, Jean-René Duhamel, Alexandre Pouget</p><p>Abstract: Most models of spatial representations in the cortex assume cells with limited receptive fields that are defined in a particular egocentric frame of reference. However, cells outside of primary sensory cortex are either gain modulated by postural input or partially shifting. We show that solving classical spatial tasks, like sensory prediction, multi-sensory integration, sensory-motor transformation and motor control requires more complicated intermediate representations that are not invariant in one frame of reference. We present an iterative basis function map that performs these spatial tasks optimally with gain modulated and partially shifting units, and tests it against neurophysiological and neuropsychological data. In order to perform an action directed toward an object, it is necessary to have a representation of its spatial location. The brain must be able to use spatial cues coming from different modalities (e.g. vision, audition, touch, proprioception), combine them to infer the position of the object, and compute the appropriate movement. These cues are in different frames of reference corresponding to different sensory or motor modalities. Visual inputs are primarily encoded in retinotopic maps, auditory inputs are encoded in head centered maps and tactile cues are encoded in skin-centered maps. Going from one frame of reference to the other might seem easy. For example, the head-centered position of an object can be approximated by the sum of its retinotopic position and the eye position. However, positions are represented by population codes in the brain, and computing a head-centered map from a retinotopic map is a more complex computation than the underlying sum. Moreover, as we get closer to sensory-motor areas it seems reasonable to assume Spksls 150 100 50 o Figure 1: Response of a VIP cell to visual stimuli appearing in different part of the screen, for three different eye positions. The level of grey represent the frequency of discharge (In spikes per seconds). The white cross is the fixation point (the head is fixed). The cell's receptive field is moving with the eyes, but only partially. Here the receptive field shift is 60% of the total gaze shift. Moreover this cell is gain modulated by eye position (adapted from Duhamel et al). that the representations should be useful for sensory-motor transformations, rather than encode an</p><p>5 0.436775 <a title="101-lda-5" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>Author: Sham Kakade, Peter Dayan</p><p>Abstract: Substantial data support a temporal difference (TO) model of dopamine (OA) neuron activity in which the cells provide a global error signal for reinforcement learning. However, in certain circumstances, OA activity seems anomalous under the TO model, responding to non-rewarding stimuli. We address these anomalies by suggesting that OA cells multiplex information about reward bonuses, including Sutton's exploration bonuses and Ng et al's non-distorting shaping bonuses. We interpret this additional role for OA in terms of the unconditional attentional and psychomotor effects of dopamine, having the computational role of guiding exploration. 1</p><p>6 0.41020966 <a title="101-lda-6" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>7 0.40989956 <a title="101-lda-7" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>8 0.40939656 <a title="101-lda-8" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>9 0.40895537 <a title="101-lda-9" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>10 0.40841442 <a title="101-lda-10" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>11 0.40704784 <a title="101-lda-11" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>12 0.406023 <a title="101-lda-12" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>13 0.40489748 <a title="101-lda-13" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<p>14 0.40239614 <a title="101-lda-14" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>15 0.39952123 <a title="101-lda-15" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>16 0.39805883 <a title="101-lda-16" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>17 0.39679232 <a title="101-lda-17" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>18 0.39629936 <a title="101-lda-18" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>19 0.39569435 <a title="101-lda-19" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>20 0.39294276 <a title="101-lda-20" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
