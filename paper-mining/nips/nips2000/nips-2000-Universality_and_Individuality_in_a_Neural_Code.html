<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>141 nips-2000-Universality and Individuality in a Neural Code</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-141" href="#">nips2000-141</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>141 nips-2000-Universality and Individuality in a Neural Code</h1>
<br/><p>Source: <a title="nips-2000-141-pdf" href="http://papers.nips.cc/paper/1894-universality-and-individuality-in-a-neural-code.pdf">pdf</a></p><p>Author: Elad Schneidman, Naama Brenner, Naftali Tishby, Robert R. de Ruyter van Steveninck, William Bialek</p><p>Abstract: The problem of neural coding is to understand how sequences of action potentials (spikes) are related to sensory stimuli, motor outputs, or (ultimately) thoughts and intentions. One clear question is whether the same coding rules are used by different neurons, or by corresponding neurons in different individuals. We present a quantitative formulation of this problem using ideas from information theory, and apply this approach to the analysis of experiments in the fly visual system. We find significant individual differences in the structure of the code, particularly in the way that temporal patterns of spikes are used to convey information beyond that available from variations in spike rate. On the other hand, all the flies in our ensemble exhibit a high coding efficiency, so that every spike carries the same amount of information in all the individuals. Thus the neural code has a quantifiable mixture of individuality and universality. 1</p><p>Reference: <a title="nips-2000-141-reference" href="../nips2000_reference/nips-2000-Universality_and_Individuality_in_a_Neural_Code_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Universality and individuality in a neural code Elad Schneidman,1,2 Naama Brenner,3 Naftali Tishby,1,3 Rob R. [sent-1, score-0.17]
</p><p>2 de Ruyter van Steveninck, 3 William Bialek3 ISchool of Computer Science and Engineering, Center for Neural Computation and 2Department of Neurobiology, Hebrew University, Jerusalem 91904, Israel 3NEC Research Institute, 4 Independence Way, Princeton, New Jersey 08540, USA  { elads, tishby} @cs. [sent-2, score-0.079]
</p><p>3 com  Abstract The problem of neural coding is to understand how sequences of action potentials (spikes) are related to sensory stimuli, motor outputs, or (ultimately) thoughts and intentions. [sent-8, score-0.064]
</p><p>4 One clear question is whether the same coding rules are used by different neurons, or by corresponding neurons in different individuals. [sent-9, score-0.094]
</p><p>5 We present a quantitative formulation of this problem using ideas from information theory, and apply this approach to the analysis of experiments in the fly visual system. [sent-10, score-0.578]
</p><p>6 We find significant individual differences in the structure of the code, particularly in the way that temporal patterns of spikes are used to convey information beyond that available from variations in spike rate. [sent-11, score-0.659]
</p><p>7 On the other hand, all the flies in our ensemble exhibit a high coding efficiency, so that every spike carries the same amount of information in all the individuals. [sent-12, score-1.133]
</p><p>8 Thus the neural code has a quantifiable mixture of individuality and universality. [sent-13, score-0.199]
</p><p>9 An accessible version of this question is whether different observers of the same sense data have the same neural representation of these data: how much of the neural code is universal, and how much is individual? [sent-16, score-0.163]
</p><p>10 Differences in the neural codes of different individuals may arise from various sources: First, different individuals may use different 'vocabularies' of coding symbols. [sent-17, score-0.278]
</p><p>11 Second, they may use the same symbols to encode different stimulus features. [sent-18, score-0.205]
</p><p>12 Finally, perhaps the most interesting possibility is that different individuals might encode different features of the stimulus, so that they 'talk about different things'. [sent-20, score-0.123]
</p><p>13 If we are to compare neural codes we must give a quantitative definition of similarity  or divergence among neural responses. [sent-21, score-0.201]
</p><p>14 We shall use ideas from information theory  [1, 2] to quantify the the notions of distinguishability, functional equivalence and content in the neural code. [sent-22, score-0.12]
</p><p>15 This approach does not require a metric either on the space of stimuli or on the space of neural responses (but see [3]); all notions of similarity emerge from the statistical structure of the neural responses. [sent-23, score-0.177]
</p><p>16 We apply these methods to analyze experiments on an identified motion sensitive neuron in the fly's visual system, the cell HI [4]. [sent-24, score-0.154]
</p><p>17 Many invertebrate nervous systems have cells that can be named and numbered [5]; in many cases, including the motion sensitive cells in the fly's lobula plate, a small number of neurons is involved in representing a similarly identifiable portion of the sensory world. [sent-25, score-0.181]
</p><p>18 It might seem that in these cases the question of whether different individuals share the same neural representation of the visual world would have a trivial answer. [sent-26, score-0.224]
</p><p>19 Far from trivial, we shall see that the neural code even for identified neurons in flies has components which are common among flies and significant components which are individual to each fly. [sent-27, score-1.386]
</p><p>20 2  Distinguishing flies according to their spike patterns  Nine different flies are shown precisely the same movie, which is repeated many times for each fly (Figure Ia). [sent-28, score-1.936]
</p><p>21 As we show the movie we record the action potentials from the HI neuron. [sent-29, score-0.072]
</p><p>22 1 The details of the stimulus movie should not have a qualitative impact on the results, provided that the movie is sufficiently long and rich to drive the system through a reasonable and natural range of responses. [sent-30, score-0.31]
</p><p>23 To analyze similarities and differences among the neural codes, we begin by discretizing the neural response into time bins of size I:l. [sent-33, score-0.268]
</p><p>24 At this resolution there are almost never two spikes in a single bin, so we can think of the neural response as a binary string, as in Fig. [sent-35, score-0.203]
</p><p>25 We examine the response in blocks or windows of time having length T, so that an individual neural response becomes a binary 'word' W with T / I:l. [sent-37, score-0.245]
</p><p>26 Figure If shows that different flies 'speak' with similar but distinct vocabularies. [sent-41, score-0.584]
</p><p>27 We quantify the divergence among vocabularies by asking how much information the observation of a single word W provides about the identity of its source, that is about the identity of the fly which generates this word: J(W -+ identity; T) =  8 N  . [sent-42, score-1.107]
</p><p>28 ~ P'(W) log2 pens(w)  bits,  (1)  lThe stimulus presented to the flies is a rigidly moving pattern of vertical bars, randomly dark or bright, with average intensity I ~ 100mW/(m 2 • sr). [sent-44, score-0.795]
</p><p>29 (2) In each Hy we identify the H1 cell as the unique spiking neuron in the lobula plate that has a combination of wide field sensitivity, inward directional selectivity for horizontal motion, and contralateral projection. [sent-48, score-0.093]
</p><p>30 a  Stimulus  o~200~ '0  o  0  a;  >_200 L-----L-----~----~----~----~  b  c  Spike trains  e Fly 1  . [sent-50, score-0.107]
</p><p>31 pFly 1(Wlt=3306)  "  ~~~--~H*~~--+4~~-------­  Fly3 ~~U-  _ _~~_ _~_ _~~~_ _ _ _ _ ___  _~~ ____ ~  Fly4 ~---------*--~--~~~-------­  Fly5 ~---mrr-----,;  Fly 6  d  Fly 6  pFIY'(Wlt=33061  f  Total word distribution  ~---*l¥--­  Fly7 ~---'"""'---------'  Fly8 ~--. [sent-55, score-0.189]
</p><p>32 4  3,5  Time (5)  3306 3318 Time (ms)  20 40 60 binary word value  Figure 1: Different flies' spike trains and word statistics. [sent-57, score-0.823]
</p><p>33 (a) All flies view the same random vertical bar pattern moving across their visual field with a time dependent velocity, part of which is shown. [sent-58, score-0.68]
</p><p>34 (b) A set of 45 response traces to the part of the stimulus shown in (a) from each of the 9 flies . [sent-60, score-0.828]
</p><p>35 The traces are taken from the segment of the experiment where the transient responses have decayed. [sent-61, score-0.078]
</p><p>36 (c) Example of construction of the local word distributions. [sent-62, score-0.189]
</p><p>37 Zooming in on a segment of the repeated responses of fly 1 to the visual stimuli, the fly's spike trains are divided into contiguous 2 ms bins, and the spikes in each of the bins are counted. [sent-63, score-1.187]
</p><p>38 For example, we get the 6 letter words that the fly used at time 3306 ms into the input trace. [sent-64, score-0.601]
</p><p>39 (e) The distributions of words that flies 1 and 6 used at time t = 3306 ms from the beginning of the stimulus. [sent-66, score-0.76]
</p><p>40 , binary word value '17' stands for the word '010001' . [sent-69, score-0.401]
</p><p>41 (f) Collecting the words that each of the flies used through all of the visual stimulus presentations, we get the total word distributions for flies 1 and 6, pI (W) and P6(W} . [sent-70, score-1.68]
</p><p>42 (2)  i=l  The measure J(W -+ identity;T) has been discussed by Lin [11] as the 'JensenShannon divergence' DJS among the distributions pi(W). [sent-73, score-0.068]
</p><p>43 2 2Unlike the Kullback- Leibler divergence [2] (the 'standard' choice for measuring dissimilarity among distributions), the Jensen- Shannon divergence is symmetric, and bounded (see also [12]). [sent-74, score-0.13]
</p><p>44 We find that information about identity is accumulating at more or less constant rate well before the under sampling limits of the experiment are reached (Fig. [sent-76, score-0.266]
</p><p>45 Since the mean spike rate can be measured by counting the number of Is in each word W, this information includes the differences in firing rate among the different flies. [sent-83, score-0.823]
</p><p>46 Even if flies use very similar vocabularies, they may differ substantially in the way that they associate words with particular stimulus features. [sent-84, score-0.794]
</p><p>47 Since we present the stimulus repeatedly to each fly, we can specify the stimulus precisely by noting the time relative to the beginning of the stimulus. [sent-85, score-0.364]
</p><p>48 We can therefore consider the word W that the ith fly will generate at time t. [sent-86, score-0.674]
</p><p>49 This word is drawn from the distribution pi(Wlt) which we can sample, as in Fig. [sent-87, score-0.189]
</p><p>50 lc-e, by looking across multiple presentations of the same stimulus movie. [sent-88, score-0.202]
</p><p>51 In parallel with the discussion above, we can measure the information that the word W observed at known t gives us about the identity of the fly, . [sent-89, score-0.393]
</p><p>52 I(W -+ IdentIty It; T)  N  = ~ 11 ~ p  i  [ pi(Wlt) ] (Wit) log2 pens(Wlt) ,  (3)  where the distribution of words used at time t by the whole ensemble of flies is N  pens(Wlt)  =L  l1Pi(Wlt). [sent-91, score-0.733]
</p><p>53 (4)  i=l  The natural quantity is an average over all times t,  I( {W, t} -+ identity; T)  = (I(W -+ identity It; T)t  bits,  (5)  where (. [sent-92, score-0.188]
</p><p>54 Observing both the spike train and the stimulus together provides 32 ± 1 bits/s about the identity of the fly. [sent-96, score-0.675]
</p><p>55 This is more than six times as much information as we can gain by observing the spike train alone, and corresponds to gaining one bit in ""' 30 ms; correspondingly, a typical pair of flies in our ensemble can be distinguished reliably in ""' 30 ms. [sent-97, score-1.127]
</p><p>56 This is the time scale on which flies actually use their estimates of visual motion to guide their flight during chasing behavior [6], so that the neural codes of different individuals are distinguishable on the time scales relevant to behavior. [sent-98, score-0.976]
</p><p>57 8 '"  ~001  Fly 6 vs mixture  ---=========== Fly 1 vs mixture  % L--~ 5-~ 0 -~~~ 0 ~-2 5~~ 0 1~ 15 2~ ~ 3~  Word length (msec)  5  10  15  20  25  30  Word length (msec)  Figure 2: Distinguishing one fly from others based on spike trains. [sent-107, score-0.888]
</p><p>58 (a) The average rate of information gained about the identity of a fly from its word distribution, as a function of the word size used (middle curve). [sent-108, score-1.165]
</p><p>59 The information rate is saturated even before we reach the maximal word length used. [sent-109, score-0.312]
</p><p>60 Also shown is the average rate of information that the word distribution of fly 1 (and 6) gives about its identity, compared with the word distribution mixture of all of the flies. [sent-110, score-1.028]
</p><p>61 (b) Similar to (a) , we compute the average amount of information that the distribution of words the fly used at a specific point in time gives about its identity. [sent-112, score-0.67]
</p><p>62 Averaging over all times, we show the amount of information gained about the identity of fly 1 (and 6) based on its time dependent word distributions, and the average over the 9 flies (middle curve). [sent-113, score-1.565]
</p><p>63 A "baseline calculation" , where we subdivided the spike trains of one fly into artificial new individuals, and compared their spike trains, gave significantly smaller values (not shown) . [sent-115, score-1.19]
</p><p>64 Figure 3a shows that the flies in our ensemble span a range of information rates from ~ 50 to ~ 150 bits/so This threefold range of information rates is correlated with the range of spike rates, so that each of the cells transmits nearly a constant amount of information per spike, 2. [sent-116, score-1.336]
</p><p>65 This universal efficiency (10% variance over the population, despite three fold variations in total spike rate), reflects that cells with higher firing rates are not generating extra spikes at random, but rather each extra spike is equally informative about the stimulus. [sent-119, score-1.084]
</p><p>66 Although information rates are correlated with spike rates, this does not mean that information is carried by a "rate code" alone. [sent-120, score-0.56]
</p><p>67 To address the rate/timing distinction we compare the total information rate in Fig. [sent-121, score-0.144]
</p><p>68 3a, which includes the detailed structure of the spike train, with the information carried in the temporal modulations of the spike rate. [sent-122, score-0.793]
</p><p>69 For all the flies in our ensemble, the total rate at which the spike train carries information is substantially larger than the 'single spike' information- 2. [sent-125, score-1.13]
</p><p>70 This extra information is carried in the temporal patterns of spikes (Fig. [sent-129, score-0.295]
</p><p>71 Even though flies differ in the structures of their neural responses, distinguishable responses could be functionally equivalent. [sent-132, score-0.703]
</p><p>72 Thus it might be that all flies could be  a 150  ! [sent-133, score-0.584]
</p><p>73 20  40  60  20  Firing rate (spikes/sec)  40  60  Firing rate (spikes/sec)  Figure 3: The information about the stimulus that a fly's spike train carries is correlated with firing rate, and yet a significant part is in the temporal structure. [sent-176, score-0.879]
</p><p>74 (a) The rate at the HI spike train provides information about the visual stimulus is shown as a function of the average spike rate, with each fly providing a single data point The linear fit of the data points for the 9 flies corresponds to a universal rate of 2. [sent-177, score-2.256]
</p><p>75 (b) The extra amount of information that the temporal structure of the spike train of each of the Hies carry about the stimulus, as a function of the average firing rate of the fly (see [10]). [sent-180, score-1.166]
</p><p>76 The average amount of additional information that is carried by the temporal structure of the spike trains, over the population is 45 ± 17%. [sent-181, score-0.558]
</p><p>77 ) with a universal or consensus codebook that allows each individual to make sense of her own spike trains, despite the differences from her conspecifics. [sent-183, score-0.528]
</p><p>78 Thus we want to ask how much information we lose if the identity of the flies is hidden from us, or equivalently how much each fly can gain by knowing its own individual code. [sent-184, score-1.355]
</p><p>79 If we observe the response of a neuron but don't know the identity of the individual generating this response, then we are observing responses drawn from the ensemble distributions defined above, pens(WJt) and pens(w). [sent-185, score-0.452]
</p><p>80 The information that words provide about the visual stimulus then is  IffiiX(W  ~ s(t)j T) = ( ~ pens(WJt) 10g2 [~::~~~)] ) t bits. [sent-186, score-0.335]
</p><p>81 (7)  On the other hand, if we know the identity of the fly to be i, we gain the information that its spike train conveys about the stimulus, Ji(W ~ s(t) j T), Eq. [sent-187, score-1.023]
</p><p>82 The average information loss is then N  I~:~(W ~ s(t)j T) =  L lUi(W ~ s(t)j T) -  IffiiX(W ~ s(t)j T). [sent-189, score-0.106]
</p><p>83 (8)  i= l  After some algebra it can be shown that this average information loss is related to the information that the neural responses give about the identity of the individuals, as defined above:  I( {W, t} ~ identityj T) -I(W ~ identityj T). [sent-190, score-0.466]
</p><p>84 (9)  The result is that, on average, not knowing the identity of the fly limits us to extracting only 64 bits/s of information about the visual stimulus. [sent-191, score-0.764]
</p><p>85 This should be  compared with the average information rate of 92. [sent-192, score-0.168]
</p><p>86 3 bits/s in our ensemble of flies: knowing her own identity allows the average fly to extract 44% more information from Hl. [sent-193, score-0.818]
</p><p>87 Further analysis shows that each individual fly gains approximately the same relative amount of information from knowing its personal codebook. [sent-194, score-0.637]
</p><p>88 5  Discussion  We have found that the flies use similar yet distinct set of 'words' to encode information about the stimulus. [sent-195, score-0.684]
</p><p>89 The main source of this difference is not in the total set of words (or spike rates) but rather in how (i. [sent-196, score-0.38]
</p><p>90 when) these words are used to encode the stimulus; taking this into account the flies are discriminable on time scales of relevance to behavior. [sent-198, score-0.699]
</p><p>91 Using their different codes, the flies' HI spike trains convey very different amounts of information from the same visual inputs. [sent-199, score-0.581]
</p><p>92 Nonetheless, all the flies achieve a high and constant efficiency in their encoding of this information, and the temporal structure of their spike trains adds nearly 50% more information than that carried by the rate. [sent-200, score-1.193]
</p><p>93 So how much is universal and how much is individual? [sent-201, score-0.078]
</p><p>94 We find that each individual fly would lose'" 30% of the visual information carried by this neuron if it 'knew' only the codebook appropriate to the whole ensemble of flies. [sent-202, score-0.831]
</p><p>95 We leave the judgment of whether this is high individuality or not to the reader, but recall that this is the individuality in an identified neuron. [sent-203, score-0.209]
</p><p>96 Hence, we should expect that all neural circuits- both vertebrate and invertebrate-express a degree of universality and a degree of individuality. [sent-204, score-0.064]
</p><p>97 We hope that the methods introduced here will help to explore this issue of individuality more generally. [sent-205, score-0.083]
</p><p>98 Nature and precision of temporal coding in visual cortex: a metric- space analysis, J. [sent-219, score-0.141]
</p><p>99 Bialek, Reproducibility and variability in neural spike trains, Science 275, 1805- 1808, (1997). [sent-246, score-0.35]
</p><p>100 Entropy and information in neural spike trains , Phys. [sent-252, score-0.518]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('flies', 0.584), ('fly', 0.453), ('spike', 0.315), ('word', 0.189), ('wlt', 0.167), ('stimulus', 0.166), ('identity', 0.143), ('pens', 0.134), ('trains', 0.107), ('spikes', 0.09), ('individuals', 0.084), ('individuality', 0.083), ('universal', 0.078), ('ruyter', 0.078), ('ensemble', 0.073), ('movie', 0.072), ('ms', 0.072), ('hies', 0.067), ('steveninck', 0.065), ('visual', 0.064), ('rate', 0.062), ('information', 0.061), ('responses', 0.055), ('response', 0.055), ('firing', 0.054), ('carried', 0.054), ('code', 0.052), ('train', 0.051), ('codebook', 0.05), ('koberie', 0.05), ('bialek', 0.049), ('temporal', 0.048), ('codes', 0.046), ('pi', 0.045), ('average', 0.045), ('divergence', 0.045), ('individual', 0.045), ('rates', 0.045), ('words', 0.044), ('knowing', 0.043), ('extra', 0.042), ('de', 0.041), ('differences', 0.04), ('among', 0.04), ('encode', 0.039), ('van', 0.038), ('motion', 0.037), ('carries', 0.036), ('shannon', 0.036), ('presentations', 0.036), ('neural', 0.035), ('amount', 0.035), ('convey', 0.034), ('chasing', 0.033), ('djs', 0.033), ('identityj', 0.033), ('iffiix', 0.033), ('invertebrates', 0.033), ('lobula', 0.033), ('naama', 0.033), ('synergy', 0.033), ('vocabularies', 0.033), ('wjt', 0.033), ('cells', 0.032), ('time', 0.032), ('neuron', 0.031), ('vs', 0.031), ('bins', 0.031), ('hi', 0.03), ('mixture', 0.029), ('coding', 0.029), ('cts', 0.029), ('universality', 0.029), ('plate', 0.029), ('distinguishable', 0.029), ('distributions', 0.028), ('stimuli', 0.028), ('variations', 0.026), ('comp', 0.026), ('msec', 0.026), ('lose', 0.026), ('bars', 0.025), ('tishby', 0.024), ('notions', 0.024), ('neurons', 0.024), ('efficiency', 0.024), ('correlated', 0.024), ('binary', 0.023), ('traces', 0.023), ('nervous', 0.023), ('gained', 0.023), ('identified', 0.022), ('observing', 0.022), ('things', 0.021), ('reliably', 0.021), ('nonetheless', 0.021), ('total', 0.021), ('whether', 0.021), ('en', 0.021), ('question', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="141-tfidf-1" href="./nips-2000-Universality_and_Individuality_in_a_Neural_Code.html">141 nips-2000-Universality and Individuality in a Neural Code</a></p>
<p>Author: Elad Schneidman, Naama Brenner, Naftali Tishby, Robert R. de Ruyter van Steveninck, William Bialek</p><p>Abstract: The problem of neural coding is to understand how sequences of action potentials (spikes) are related to sensory stimuli, motor outputs, or (ultimately) thoughts and intentions. One clear question is whether the same coding rules are used by different neurons, or by corresponding neurons in different individuals. We present a quantitative formulation of this problem using ideas from information theory, and apply this approach to the analysis of experiments in the fly visual system. We find significant individual differences in the structure of the code, particularly in the way that temporal patterns of spikes are used to convey information beyond that available from variations in spike rate. On the other hand, all the flies in our ensemble exhibit a high coding efficiency, so that every spike carries the same amount of information in all the individuals. Thus the neural code has a quantifiable mixture of individuality and universality. 1</p><p>2 0.29636052 <a title="141-tfidf-2" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>Author: Blaise Agüera y Arcas, Adrienne L. Fairhall, William Bialek</p><p>Abstract: In this paper we formulate a description of the computation performed by a neuron as a combination of dimensional reduction and nonlinearity. We implement this description for the HodgkinHuxley model, identify the most relevant dimensions and find the nonlinearity. A two dimensional description already captures a significant fraction of the information that spikes carry about dynamic inputs. This description also shows that computation in the Hodgkin-Huxley model is more complex than a simple integrateand-fire or perceptron model. 1</p><p>3 0.283288 <a title="141-tfidf-3" href="./nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">88 nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>Author: Adrienne L. Fairhall, Geoffrey D. Lewen, William Bialek, Robert R. de Ruyter van Steveninck</p><p>Abstract: Many neural systems extend their dynamic range by adaptation. We examine the timescales of adaptation in the context of dynamically modulated rapidly-varying stimuli, and demonstrate in the fly visual system that adaptation to the statistical ensemble of the stimulus dynamically maximizes information transmission about the time-dependent stimulus. Further, while the rate response has long transients, the adaptation takes place on timescales consistent with optimal variance estimation.</p><p>4 0.21368811 <a title="141-tfidf-4" href="./nips-2000-Finding_the_Key_to_a_Synapse.html">55 nips-2000-Finding the Key to a Synapse</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass</p><p>Abstract: Experimental data have shown that synapses are heterogeneous: different synapses respond with different sequences of amplitudes of postsynaptic responses to the same spike train. Neither the role of synaptic dynamics itself nor the role of the heterogeneity of synaptic dynamics for computations in neural circuits is well understood. We present in this article methods that make it feasible to compute for a given synapse with known synaptic parameters the spike train that is optimally fitted to the synapse, for example in the sense that it produces the largest sum of postsynaptic responses. To our surprise we find that most of these optimally fitted spike trains match common firing patterns of specific types of neurons that are discussed in the literature.</p><p>5 0.13548362 <a title="141-tfidf-5" href="./nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">129 nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<p>Author: Gal Chechik, Naftali Tishby</p><p>Abstract: The paradigm of Hebbian learning has recently received a novel interpretation with the discovery of synaptic plasticity that depends on the relative timing of pre and post synaptic spikes. This paper derives a temporally dependent learning rule from the basic principle of mutual information maximization and studies its relation to the experimentally observed plasticity. We find that a supervised spike-dependent learning rule sharing similar structure with the experimentally observed plasticity increases mutual information to a stable near optimal level. Moreover, the analysis reveals how the temporal structure of time-dependent learning rules is determined by the temporal filter applied by neurons over their inputs. These results suggest experimental prediction as to the dependency of the learning rule on neuronal biophysical parameters 1</p><p>6 0.11866689 <a title="141-tfidf-6" href="./nips-2000-A_Neural_Probabilistic_Language_Model.html">6 nips-2000-A Neural Probabilistic Language Model</a></p>
<p>7 0.096108668 <a title="141-tfidf-7" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>8 0.072584011 <a title="141-tfidf-8" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>9 0.059303366 <a title="141-tfidf-9" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>10 0.059033453 <a title="141-tfidf-10" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>11 0.057580475 <a title="141-tfidf-11" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>12 0.056403641 <a title="141-tfidf-12" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>13 0.055624098 <a title="141-tfidf-13" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>14 0.054881647 <a title="141-tfidf-14" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>15 0.053807676 <a title="141-tfidf-15" href="./nips-2000-Homeostasis_in_a_Silicon_Integrate_and_Fire_Neuron.html">67 nips-2000-Homeostasis in a Silicon Integrate and Fire Neuron</a></p>
<p>16 0.052757572 <a title="141-tfidf-16" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>17 0.051496208 <a title="141-tfidf-17" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>18 0.048817236 <a title="141-tfidf-18" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>19 0.048773501 <a title="141-tfidf-19" href="./nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">42 nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<p>20 0.047509719 <a title="141-tfidf-20" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.166), (1, -0.227), (2, -0.268), (3, -0.006), (4, 0.008), (5, -0.02), (6, -0.054), (7, 0.302), (8, 0.017), (9, 0.1), (10, 0.012), (11, -0.008), (12, 0.106), (13, 0.32), (14, -0.122), (15, 0.131), (16, 0.003), (17, -0.065), (18, -0.157), (19, -0.066), (20, 0.021), (21, 0.028), (22, 0.083), (23, 0.044), (24, 0.066), (25, -0.069), (26, -0.073), (27, 0.027), (28, -0.06), (29, -0.045), (30, -0.013), (31, -0.196), (32, 0.103), (33, -0.093), (34, -0.016), (35, -0.072), (36, 0.036), (37, -0.012), (38, -0.003), (39, -0.082), (40, 0.014), (41, 0.058), (42, -0.004), (43, 0.076), (44, 0.016), (45, 0.002), (46, 0.055), (47, 0.041), (48, 0.005), (49, -0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97766531 <a title="141-lsi-1" href="./nips-2000-Universality_and_Individuality_in_a_Neural_Code.html">141 nips-2000-Universality and Individuality in a Neural Code</a></p>
<p>Author: Elad Schneidman, Naama Brenner, Naftali Tishby, Robert R. de Ruyter van Steveninck, William Bialek</p><p>Abstract: The problem of neural coding is to understand how sequences of action potentials (spikes) are related to sensory stimuli, motor outputs, or (ultimately) thoughts and intentions. One clear question is whether the same coding rules are used by different neurons, or by corresponding neurons in different individuals. We present a quantitative formulation of this problem using ideas from information theory, and apply this approach to the analysis of experiments in the fly visual system. We find significant individual differences in the structure of the code, particularly in the way that temporal patterns of spikes are used to convey information beyond that available from variations in spike rate. On the other hand, all the flies in our ensemble exhibit a high coding efficiency, so that every spike carries the same amount of information in all the individuals. Thus the neural code has a quantifiable mixture of individuality and universality. 1</p><p>2 0.78594023 <a title="141-lsi-2" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>Author: Blaise Agüera y Arcas, Adrienne L. Fairhall, William Bialek</p><p>Abstract: In this paper we formulate a description of the computation performed by a neuron as a combination of dimensional reduction and nonlinearity. We implement this description for the HodgkinHuxley model, identify the most relevant dimensions and find the nonlinearity. A two dimensional description already captures a significant fraction of the information that spikes carry about dynamic inputs. This description also shows that computation in the Hodgkin-Huxley model is more complex than a simple integrateand-fire or perceptron model. 1</p><p>3 0.75614554 <a title="141-lsi-3" href="./nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">88 nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>Author: Adrienne L. Fairhall, Geoffrey D. Lewen, William Bialek, Robert R. de Ruyter van Steveninck</p><p>Abstract: Many neural systems extend their dynamic range by adaptation. We examine the timescales of adaptation in the context of dynamically modulated rapidly-varying stimuli, and demonstrate in the fly visual system that adaptation to the statistical ensemble of the stimulus dynamically maximizes information transmission about the time-dependent stimulus. Further, while the rate response has long transients, the adaptation takes place on timescales consistent with optimal variance estimation.</p><p>4 0.53067052 <a title="141-lsi-4" href="./nips-2000-Finding_the_Key_to_a_Synapse.html">55 nips-2000-Finding the Key to a Synapse</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass</p><p>Abstract: Experimental data have shown that synapses are heterogeneous: different synapses respond with different sequences of amplitudes of postsynaptic responses to the same spike train. Neither the role of synaptic dynamics itself nor the role of the heterogeneity of synaptic dynamics for computations in neural circuits is well understood. We present in this article methods that make it feasible to compute for a given synapse with known synaptic parameters the spike train that is optimally fitted to the synapse, for example in the sense that it produces the largest sum of postsynaptic responses. To our surprise we find that most of these optimally fitted spike trains match common firing patterns of specific types of neurons that are discussed in the literature.</p><p>5 0.36634761 <a title="141-lsi-5" href="./nips-2000-A_Neural_Probabilistic_Language_Model.html">6 nips-2000-A Neural Probabilistic Language Model</a></p>
<p>Author: Yoshua Bengio, Réjean Ducharme, Pascal Vincent</p><p>Abstract: A goal of statistical language modeling is to learn the joint probability function of sequences of words. This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons. In the proposed approach one learns simultaneously (1) a distributed representation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model.</p><p>6 0.30331969 <a title="141-lsi-6" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>7 0.27218688 <a title="141-lsi-7" href="./nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">129 nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<p>8 0.26933029 <a title="141-lsi-8" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>9 0.26411933 <a title="141-lsi-9" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>10 0.19316345 <a title="141-lsi-10" href="./nips-2000-The_Early_Word_Catches_the_Weights.html">131 nips-2000-The Early Word Catches the Weights</a></p>
<p>11 0.19122289 <a title="141-lsi-11" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>12 0.18392314 <a title="141-lsi-12" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>13 0.17822652 <a title="141-lsi-13" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>14 0.17013018 <a title="141-lsi-14" href="./nips-2000-Analysis_of_Bit_Error_Probability_of_Direct-Sequence_CDMA_Multiuser_Demodulators.html">25 nips-2000-Analysis of Bit Error Probability of Direct-Sequence CDMA Multiuser Demodulators</a></p>
<p>15 0.15599674 <a title="141-lsi-15" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>16 0.14954761 <a title="141-lsi-16" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>17 0.14943141 <a title="141-lsi-17" href="./nips-2000-Homeostasis_in_a_Silicon_Integrate_and_Fire_Neuron.html">67 nips-2000-Homeostasis in a Silicon Integrate and Fire Neuron</a></p>
<p>18 0.14883488 <a title="141-lsi-18" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>19 0.14787275 <a title="141-lsi-19" href="./nips-2000-Stability_and_Noise_in_Biochemical_Switches.html">125 nips-2000-Stability and Noise in Biochemical Switches</a></p>
<p>20 0.14106289 <a title="141-lsi-20" href="./nips-2000-Data_Clustering_by_Markovian_Relaxation_and_the_Information_Bottleneck_Method.html">38 nips-2000-Data Clustering by Markovian Relaxation and the Information Bottleneck Method</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.016), (17, 0.061), (32, 0.021), (33, 0.025), (42, 0.08), (55, 0.022), (62, 0.03), (65, 0.029), (67, 0.048), (75, 0.025), (76, 0.034), (79, 0.015), (81, 0.441), (90, 0.011), (97, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95908064 <a title="141-lda-1" href="./nips-2000-Universality_and_Individuality_in_a_Neural_Code.html">141 nips-2000-Universality and Individuality in a Neural Code</a></p>
<p>Author: Elad Schneidman, Naama Brenner, Naftali Tishby, Robert R. de Ruyter van Steveninck, William Bialek</p><p>Abstract: The problem of neural coding is to understand how sequences of action potentials (spikes) are related to sensory stimuli, motor outputs, or (ultimately) thoughts and intentions. One clear question is whether the same coding rules are used by different neurons, or by corresponding neurons in different individuals. We present a quantitative formulation of this problem using ideas from information theory, and apply this approach to the analysis of experiments in the fly visual system. We find significant individual differences in the structure of the code, particularly in the way that temporal patterns of spikes are used to convey information beyond that available from variations in spike rate. On the other hand, all the flies in our ensemble exhibit a high coding efficiency, so that every spike carries the same amount of information in all the individuals. Thus the neural code has a quantifiable mixture of individuality and universality. 1</p><p>2 0.95095521 <a title="141-lda-2" href="./nips-2000-Hippocampally-Dependent_Consolidation_in_a_Hierarchical_Model_of_Neocortex.html">66 nips-2000-Hippocampally-Dependent Consolidation in a Hierarchical Model of Neocortex</a></p>
<p>Author: Szabolcs KĂĄli, Peter Dayan</p><p>Abstract: In memory consolidation, declarative memories which initially require the hippocampus for their recall, ultimately become independent of it. Consolidation has been the focus of numerous experimental and qualitative modeling studies, but only little quantitative exploration. We present a consolidation model in which hierarchical connections in the cortex, that initially instantiate purely semantic information acquired through probabilistic unsupervised learning, come to instantiate episodic information as well. The hippocampus is responsible for helping complete partial input patterns before consolidation is complete, while also training the cortex to perform appropriate completion by itself.</p><p>3 0.91987401 <a title="141-lda-3" href="./nips-2000-The_Unscented_Particle_Filter.html">137 nips-2000-The Unscented Particle Filter</a></p>
<p>Author: Rudolph van der Merwe, Arnaud Doucet, Nando de Freitas, Eric A. Wan</p><p>Abstract: In this paper, we propose a new particle filter based on sequential importance sampling. The algorithm uses a bank of unscented filters to obtain the importance proposal distribution. This proposal has two very</p><p>4 0.85152 <a title="141-lda-4" href="./nips-2000-Probabilistic_Semantic_Video_Indexing.html">103 nips-2000-Probabilistic Semantic Video Indexing</a></p>
<p>Author: Milind R. Naphade, Igor Kozintsev, Thomas S. Huang</p><p>Abstract: We propose a novel probabilistic framework for semantic video indexing. We define probabilistic multimedia objects (multijects) to map low-level media features to high-level semantic labels. A graphical network of such multijects (multinet) captures scene context by discovering intra-frame as well as inter-frame dependency relations between the concepts. The main contribution is a novel application of a factor graph framework to model this network. We model relations between semantic concepts in terms of their co-occurrence as well as the temporal dependencies between these concepts within video shots. Using the sum-product algorithm [1] for approximate or exact inference in these factor graph multinets, we attempt to correct errors made during isolated concept detection by forcing high-level constraints. This results in a significant improvement in the overall detection performance. 1</p><p>5 0.59674603 <a title="141-lda-5" href="./nips-2000-Finding_the_Key_to_a_Synapse.html">55 nips-2000-Finding the Key to a Synapse</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass</p><p>Abstract: Experimental data have shown that synapses are heterogeneous: different synapses respond with different sequences of amplitudes of postsynaptic responses to the same spike train. Neither the role of synaptic dynamics itself nor the role of the heterogeneity of synaptic dynamics for computations in neural circuits is well understood. We present in this article methods that make it feasible to compute for a given synapse with known synaptic parameters the spike train that is optimally fitted to the synapse, for example in the sense that it produces the largest sum of postsynaptic responses. To our surprise we find that most of these optimally fitted spike trains match common firing patterns of specific types of neurons that are discussed in the literature.</p><p>6 0.57640928 <a title="141-lda-6" href="./nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">88 nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>7 0.55443257 <a title="141-lda-7" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>8 0.49657819 <a title="141-lda-8" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>9 0.49638861 <a title="141-lda-9" href="./nips-2000-The_Early_Word_Catches_the_Weights.html">131 nips-2000-The Early Word Catches the Weights</a></p>
<p>10 0.46351662 <a title="141-lda-10" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>11 0.41310439 <a title="141-lda-11" href="./nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">129 nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<p>12 0.40876555 <a title="141-lda-12" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>13 0.40859607 <a title="141-lda-13" href="./nips-2000-Learning_Switching_Linear_Models_of_Human_Motion.html">80 nips-2000-Learning Switching Linear Models of Human Motion</a></p>
<p>14 0.40353158 <a title="141-lda-14" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>15 0.40054664 <a title="141-lda-15" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>16 0.3989197 <a title="141-lda-16" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>17 0.38956645 <a title="141-lda-17" href="./nips-2000-Stability_and_Noise_in_Biochemical_Switches.html">125 nips-2000-Stability and Noise in Biochemical Switches</a></p>
<p>18 0.38738847 <a title="141-lda-18" href="./nips-2000-Bayesian_Video_Shot_Segmentation.html">30 nips-2000-Bayesian Video Shot Segmentation</a></p>
<p>19 0.3830238 <a title="141-lda-19" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>20 0.38299379 <a title="141-lda-20" href="./nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">42 nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
