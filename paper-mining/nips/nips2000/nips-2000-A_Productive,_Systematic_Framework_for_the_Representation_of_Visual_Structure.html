<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-10" href="#">nips2000-10</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</h1>
<br/><p>Source: <a title="nips-2000-10-pdf" href="http://papers.nips.cc/paper/1827-a-productive-systematic-framework-for-the-representation-of-visual-structure.pdf">pdf</a></p><p>Author: Shimon Edelman, Nathan Intrator</p><p>Abstract: We describe a unified framework for the understanding of structure representation in primate vision. A model derived from this framework is shown to be effectively systematic in that it has the ability to interpret and associate together objects that are related through a rearrangement of common</p><p>Reference: <a title="nips-2000-10-reference" href="../nips2000_reference/nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cof', 0.486), ('frag', 0.392), ('composit', 0.312), ('object', 0.213), ('shap', 0.197), ('vis', 0.162), ('spat', 0.132), ('constitu', 0.122), ('mdl', 0.119), ('stimul', 0.11), ('field', 0.108), ('bind', 0.105), ('at', 0.097), ('gain', 0.097), ('cel', 0.089), ('structure', 0.085), ('rf', 0.082), ('unit', 0.079), ('psycholog', 0.079), ('hum', 0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="10-tfidf-1" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>Author: Shimon Edelman, Nathan Intrator</p><p>Abstract: We describe a unified framework for the understanding of structure representation in primate vision. A model derived from this framework is shown to be effectively systematic in that it has the ability to interpret and associate together objects that are related through a rearrangement of common</p><p>2 0.19847886 <a title="10-tfidf-2" href="./nips-2000-Shape_Context%3A_A_New_Descriptor_for_Shape_Matching_and_Object_Recognition.html">117 nips-2000-Shape Context: A New Descriptor for Shape Matching and Object Recognition</a></p>
<p>Author: Serge Belongie, Jitendra Malik, Jan Puzicha</p><p>Abstract: We develop an approach to object recognition based on matching shapes and using a resulting measure of similarity in a nearest neighbor classifier. The key algorithmic problem here is that of finding pointwise correspondences between an image shape and a stored prototype shape. We introduce a new shape descriptor, the shape context, which makes this possible, using a simple and robust algorithm. The shape context at a point captures the distribution over relative positions of other shape points and thus summarizes global shape in a rich, local descriptor. We demonstrate that shape contexts greatly simplify recovery of correspondences between points of two given shapes. Once shapes are aligned, shape contexts are used to define a robust score for measuring shape similarity. We have used this score in a nearest-neighbor classifier for recognition of hand written digits as well as 3D objects, using exactly the same distance function. On the benchmark MNIST dataset of handwritten digits, this yields an error rate of 0.63%, outperforming other published techniques. 1</p><p>3 0.1835939 <a title="10-tfidf-3" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>Author: Sophie Denève, Jean-René Duhamel, Alexandre Pouget</p><p>Abstract: Most models of spatial representations in the cortex assume cells with limited receptive fields that are defined in a particular egocentric frame of reference. However, cells outside of primary sensory cortex are either gain modulated by postural input or partially shifting. We show that solving classical spatial tasks, like sensory prediction, multi-sensory integration, sensory-motor transformation and motor control requires more complicated intermediate representations that are not invariant in one frame of reference. We present an iterative basis function map that performs these spatial tasks optimally with gain modulated and partially shifting units, and tests it against neurophysiological and neuropsychological data. In order to perform an action directed toward an object, it is necessary to have a representation of its spatial location. The brain must be able to use spatial cues coming from different modalities (e.g. vision, audition, touch, proprioception), combine them to infer the position of the object, and compute the appropriate movement. These cues are in different frames of reference corresponding to different sensory or motor modalities. Visual inputs are primarily encoded in retinotopic maps, auditory inputs are encoded in head centered maps and tactile cues are encoded in skin-centered maps. Going from one frame of reference to the other might seem easy. For example, the head-centered position of an object can be approximated by the sum of its retinotopic position and the eye position. However, positions are represented by population codes in the brain, and computing a head-centered map from a retinotopic map is a more complex computation than the underlying sum. Moreover, as we get closer to sensory-motor areas it seems reasonable to assume Spksls 150 100 50 o Figure 1: Response of a VIP cell to visual stimuli appearing in different part of the screen, for three different eye positions. The level of grey represent the frequency of discharge (In spikes per seconds). The white cross is the fixation point (the head is fixed). The cell's receptive field is moving with the eyes, but only partially. Here the receptive field shift is 60% of the total gaze shift. Moreover this cell is gain modulated by eye position (adapted from Duhamel et al). that the representations should be useful for sensory-motor transformations, rather than encode an</p><p>4 0.16589762 <a title="10-tfidf-4" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>Author: Kevin A. Archie, Bartlett W. Mel</p><p>Abstract: Neurons in area V4 have relatively large receptive fields (RFs), so multiple visual features are simultaneously</p><p>5 0.14040294 <a title="10-tfidf-5" href="./nips-2000-The_Use_of_MDL_to_Select_among_Computational_Models_of_Cognition.html">139 nips-2000-The Use of MDL to Select among Computational Models of Cognition</a></p>
<p>Author: In Jae Myung, Mark A. Pitt, Shaobo Zhang, Vijay Balasubramanian</p><p>Abstract: How should we decide among competing explanations of a cognitive process given limited observations? The problem of model selection is at the heart of progress in cognitive science. In this paper, Minimum Description Length (MDL) is introduced as a method for selecting among computational models of cognition. We also show that differential geometry provides an intuitive understanding of what drives model selection in MDL. Finally, adequacy of MDL is demonstrated in two areas of cognitive modeling. 1 Model Selection and Model Complexity The development and testing of computational models of cognitive processing are a central focus in cognitive science. A model embodies a solution to a problem whose adequacy is evaluated by its ability to mimic behavior by capturing the regularities underlying observed data. This enterprise of model selection is challenging because of the competing goals that must be satisfied. Traditionally, computational models of cognition have been compared using one of many goodness-of-fit measures. However, use of such a measure can result in the choice of a model that over-fits the data, one that captures idiosyncracies in the particular data set (i.e., noise) over and above the underlying regularities of interest. Such models are considered complex, in that the inherent flexibility in the model enables it to fit diverse patterns of data. As a group, they can be characterized as having many parameters that are combined in a highly nonlinear fashion in the model equation. They do not assume a single structure in the data. Rather, the model contains multiple structures; each obtained by finely tuning the parameter values of the model, and thus can fit a wide range of data patterns. In contrast, simple models, frequently with few parameters, assume a specific structure in the data, which will manifest itself as a narrow range of similar data patterns. Only when one of these patterns occurs will the model fit the data well. The problem of over-fitting data due to model complexity suggests that the goal of model selection should instead be to select the model that generalizes best to all data samples that arise from the same underlying regularity, thus capturing only the regularity, not the noise. To achieve this goal, the selection method must be sensitive to the complexity of a model. There are at least two independent dimensions of model complexity. They are the number of free parameters of a model and its functional form, which refers to the way the parameters are combined in the model equation. For instance, it seems unlikely that two one-parameter models, y = ex and y = x 9, are equally complex in their ability to fit data. The two dimensions of model complexity (number of parameters and functional form) and their interplay can improve a model's fit to the data, without necessarily improving generalizability. The trademark of a good model selection procedure, then, is its ability to satisfy two opposing goals. A model must be sufficiently complex to describe the data sample accurately, but without over-fitting the data and thus losing generalizability. To achieve this end, we need a theoretically well-justified measure of model complexity that takes into account the number of parameters and the functional form of a model. In this paper, we introduce Minimum Description Length (MDL) as an appropriate method of selecting among mathematical models of cognition. We also show that MDL has an elegant geometric interpretation that provides a clear, intuitive understanding of the meaning of complexity in MDL. Finally, application examples of MDL are presented in two areas of cognitive modeling. 1.1 Minimum Description Length The central thesis of model selection is the estimation of a model's generalizability. One approach to assessing generalizability is the Minimum Description Length (MDL) principle [1]. It provides a theoretically well-grounded measure of complexity that is sensitive to both dimensions of complexity and also lends itself to intuitive, geometric interpretations. MDL was developed within algorithmic coding theory to choose the model that permits the greatest compression of data. A model family f with parameters e assigns the likelihood f(yle) to a given set of observed data y . The full form of the MDL measure for such a model family is given below. MDL = -In! (yISA) + ~ln( ; )+ In f dS.jdetl(S) where SA is the parameter that maximizes the likelihood, k is the number of parameters in the model, N is the sample size and I(e) is the Fisher information matrix. MDL is the length in bits of the shortest possible code that describes the data with the help of a model. In the context of cognitive modeling, the model that minimizes MDL uncovers the greatest amount of regularity (i.e., knowledge) underlying the data and therefore should be selected. The first, maximized log likelihood term is the lack-of-fit measure, and the second and third terms constitute the intrinsic complexity of the model. In particular, the third term captures the effects of complexity due to functional form, reflected through I(e). We will call the latter two terms together the geometric complexity of the model, for reasons that will become clear in the remainder of this paper. MDL arises as a finite series of terms in an asymptotic expansion of the Bayesian posterior probability of a model given the data for a special form of the parameter prior density [2] . Hence in essence, minimization of MDL is equivalent to maximization of the Bayesian posterior probability. In this paper we present a geometric interpretation of MDL, as well as Bayesian model selection [3], that provides an elegant and intuitive framework for understanding model complexity, a central concept in model selection. 2 Differential Geometric Interpretation of MDL From a geometric perspective, a parametric model family of probability distributions forms a Riemannian manifold embedded in the space of all probability distributions [4]. Every distribution is a point in this space, and the collection of points created by varying the parameters of the model gives rise to a hyper-surface in which</p><p>6 0.13984866 <a title="10-tfidf-6" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>7 0.13594754 <a title="10-tfidf-7" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>8 0.12780154 <a title="10-tfidf-8" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>9 0.11246168 <a title="10-tfidf-9" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>10 0.11140284 <a title="10-tfidf-10" href="./nips-2000-Modelling_Spatial_Recall%2C_Mental_Imagery_and_Neglect.html">87 nips-2000-Modelling Spatial Recall, Mental Imagery and Neglect</a></p>
<p>11 0.10891504 <a title="10-tfidf-11" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>12 0.10099127 <a title="10-tfidf-12" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>13 0.099512748 <a title="10-tfidf-13" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>14 0.095978394 <a title="10-tfidf-14" href="./nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">88 nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>15 0.094978608 <a title="10-tfidf-15" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>16 0.089784093 <a title="10-tfidf-16" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>17 0.082862027 <a title="10-tfidf-17" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>18 0.080206096 <a title="10-tfidf-18" href="./nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">42 nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<p>19 0.079046503 <a title="10-tfidf-19" href="./nips-2000-Feature_Correspondence%3A_A_Markov_Chain_Monte_Carlo_Approach.html">53 nips-2000-Feature Correspondence: A Markov Chain Monte Carlo Approach</a></p>
<p>20 0.074489564 <a title="10-tfidf-20" href="./nips-2000-Universality_and_Individuality_in_a_Neural_Code.html">141 nips-2000-Universality and Individuality in a Neural Code</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.248), (1, -0.143), (2, -0.068), (3, -0.046), (4, -0.078), (5, -0.173), (6, 0.066), (7, -0.28), (8, 0.155), (9, 0.112), (10, 0.004), (11, 0.055), (12, 0.009), (13, -0.087), (14, -0.129), (15, 0.023), (16, -0.041), (17, 0.167), (18, 0.083), (19, -0.082), (20, 0.083), (21, -0.031), (22, 0.057), (23, 0.11), (24, 0.011), (25, 0.045), (26, -0.048), (27, -0.019), (28, -0.035), (29, 0.085), (30, -0.186), (31, -0.061), (32, 0.098), (33, -0.093), (34, -0.021), (35, -0.009), (36, -0.004), (37, -0.0), (38, -0.121), (39, -0.073), (40, -0.194), (41, -0.078), (42, 0.097), (43, -0.0), (44, -0.042), (45, 0.052), (46, -0.034), (47, -0.04), (48, 0.117), (49, 0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94871664 <a title="10-lsi-1" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>Author: Shimon Edelman, Nathan Intrator</p><p>Abstract: We describe a unified framework for the understanding of structure representation in primate vision. A model derived from this framework is shown to be effectively systematic in that it has the ability to interpret and associate together objects that are related through a rearrangement of common</p><p>2 0.67061257 <a title="10-lsi-2" href="./nips-2000-Shape_Context%3A_A_New_Descriptor_for_Shape_Matching_and_Object_Recognition.html">117 nips-2000-Shape Context: A New Descriptor for Shape Matching and Object Recognition</a></p>
<p>Author: Serge Belongie, Jitendra Malik, Jan Puzicha</p><p>Abstract: We develop an approach to object recognition based on matching shapes and using a resulting measure of similarity in a nearest neighbor classifier. The key algorithmic problem here is that of finding pointwise correspondences between an image shape and a stored prototype shape. We introduce a new shape descriptor, the shape context, which makes this possible, using a simple and robust algorithm. The shape context at a point captures the distribution over relative positions of other shape points and thus summarizes global shape in a rich, local descriptor. We demonstrate that shape contexts greatly simplify recovery of correspondences between points of two given shapes. Once shapes are aligned, shape contexts are used to define a robust score for measuring shape similarity. We have used this score in a nearest-neighbor classifier for recognition of hand written digits as well as 3D objects, using exactly the same distance function. On the benchmark MNIST dataset of handwritten digits, this yields an error rate of 0.63%, outperforming other published techniques. 1</p><p>3 0.54673475 <a title="10-lsi-3" href="./nips-2000-The_Use_of_MDL_to_Select_among_Computational_Models_of_Cognition.html">139 nips-2000-The Use of MDL to Select among Computational Models of Cognition</a></p>
<p>Author: In Jae Myung, Mark A. Pitt, Shaobo Zhang, Vijay Balasubramanian</p><p>Abstract: How should we decide among competing explanations of a cognitive process given limited observations? The problem of model selection is at the heart of progress in cognitive science. In this paper, Minimum Description Length (MDL) is introduced as a method for selecting among computational models of cognition. We also show that differential geometry provides an intuitive understanding of what drives model selection in MDL. Finally, adequacy of MDL is demonstrated in two areas of cognitive modeling. 1 Model Selection and Model Complexity The development and testing of computational models of cognitive processing are a central focus in cognitive science. A model embodies a solution to a problem whose adequacy is evaluated by its ability to mimic behavior by capturing the regularities underlying observed data. This enterprise of model selection is challenging because of the competing goals that must be satisfied. Traditionally, computational models of cognition have been compared using one of many goodness-of-fit measures. However, use of such a measure can result in the choice of a model that over-fits the data, one that captures idiosyncracies in the particular data set (i.e., noise) over and above the underlying regularities of interest. Such models are considered complex, in that the inherent flexibility in the model enables it to fit diverse patterns of data. As a group, they can be characterized as having many parameters that are combined in a highly nonlinear fashion in the model equation. They do not assume a single structure in the data. Rather, the model contains multiple structures; each obtained by finely tuning the parameter values of the model, and thus can fit a wide range of data patterns. In contrast, simple models, frequently with few parameters, assume a specific structure in the data, which will manifest itself as a narrow range of similar data patterns. Only when one of these patterns occurs will the model fit the data well. The problem of over-fitting data due to model complexity suggests that the goal of model selection should instead be to select the model that generalizes best to all data samples that arise from the same underlying regularity, thus capturing only the regularity, not the noise. To achieve this goal, the selection method must be sensitive to the complexity of a model. There are at least two independent dimensions of model complexity. They are the number of free parameters of a model and its functional form, which refers to the way the parameters are combined in the model equation. For instance, it seems unlikely that two one-parameter models, y = ex and y = x 9, are equally complex in their ability to fit data. The two dimensions of model complexity (number of parameters and functional form) and their interplay can improve a model's fit to the data, without necessarily improving generalizability. The trademark of a good model selection procedure, then, is its ability to satisfy two opposing goals. A model must be sufficiently complex to describe the data sample accurately, but without over-fitting the data and thus losing generalizability. To achieve this end, we need a theoretically well-justified measure of model complexity that takes into account the number of parameters and the functional form of a model. In this paper, we introduce Minimum Description Length (MDL) as an appropriate method of selecting among mathematical models of cognition. We also show that MDL has an elegant geometric interpretation that provides a clear, intuitive understanding of the meaning of complexity in MDL. Finally, application examples of MDL are presented in two areas of cognitive modeling. 1.1 Minimum Description Length The central thesis of model selection is the estimation of a model's generalizability. One approach to assessing generalizability is the Minimum Description Length (MDL) principle [1]. It provides a theoretically well-grounded measure of complexity that is sensitive to both dimensions of complexity and also lends itself to intuitive, geometric interpretations. MDL was developed within algorithmic coding theory to choose the model that permits the greatest compression of data. A model family f with parameters e assigns the likelihood f(yle) to a given set of observed data y . The full form of the MDL measure for such a model family is given below. MDL = -In! (yISA) + ~ln( ; )+ In f dS.jdetl(S) where SA is the parameter that maximizes the likelihood, k is the number of parameters in the model, N is the sample size and I(e) is the Fisher information matrix. MDL is the length in bits of the shortest possible code that describes the data with the help of a model. In the context of cognitive modeling, the model that minimizes MDL uncovers the greatest amount of regularity (i.e., knowledge) underlying the data and therefore should be selected. The first, maximized log likelihood term is the lack-of-fit measure, and the second and third terms constitute the intrinsic complexity of the model. In particular, the third term captures the effects of complexity due to functional form, reflected through I(e). We will call the latter two terms together the geometric complexity of the model, for reasons that will become clear in the remainder of this paper. MDL arises as a finite series of terms in an asymptotic expansion of the Bayesian posterior probability of a model given the data for a special form of the parameter prior density [2] . Hence in essence, minimization of MDL is equivalent to maximization of the Bayesian posterior probability. In this paper we present a geometric interpretation of MDL, as well as Bayesian model selection [3], that provides an elegant and intuitive framework for understanding model complexity, a central concept in model selection. 2 Differential Geometric Interpretation of MDL From a geometric perspective, a parametric model family of probability distributions forms a Riemannian manifold embedded in the space of all probability distributions [4]. Every distribution is a point in this space, and the collection of points created by varying the parameters of the model gives rise to a hyper-surface in which</p><p>4 0.53110462 <a title="10-lsi-4" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>Author: Kevin A. Archie, Bartlett W. Mel</p><p>Abstract: Neurons in area V4 have relatively large receptive fields (RFs), so multiple visual features are simultaneously</p><p>5 0.51942641 <a title="10-lsi-5" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>Author: Sophie Denève, Jean-René Duhamel, Alexandre Pouget</p><p>Abstract: Most models of spatial representations in the cortex assume cells with limited receptive fields that are defined in a particular egocentric frame of reference. However, cells outside of primary sensory cortex are either gain modulated by postural input or partially shifting. We show that solving classical spatial tasks, like sensory prediction, multi-sensory integration, sensory-motor transformation and motor control requires more complicated intermediate representations that are not invariant in one frame of reference. We present an iterative basis function map that performs these spatial tasks optimally with gain modulated and partially shifting units, and tests it against neurophysiological and neuropsychological data. In order to perform an action directed toward an object, it is necessary to have a representation of its spatial location. The brain must be able to use spatial cues coming from different modalities (e.g. vision, audition, touch, proprioception), combine them to infer the position of the object, and compute the appropriate movement. These cues are in different frames of reference corresponding to different sensory or motor modalities. Visual inputs are primarily encoded in retinotopic maps, auditory inputs are encoded in head centered maps and tactile cues are encoded in skin-centered maps. Going from one frame of reference to the other might seem easy. For example, the head-centered position of an object can be approximated by the sum of its retinotopic position and the eye position. However, positions are represented by population codes in the brain, and computing a head-centered map from a retinotopic map is a more complex computation than the underlying sum. Moreover, as we get closer to sensory-motor areas it seems reasonable to assume Spksls 150 100 50 o Figure 1: Response of a VIP cell to visual stimuli appearing in different part of the screen, for three different eye positions. The level of grey represent the frequency of discharge (In spikes per seconds). The white cross is the fixation point (the head is fixed). The cell's receptive field is moving with the eyes, but only partially. Here the receptive field shift is 60% of the total gaze shift. Moreover this cell is gain modulated by eye position (adapted from Duhamel et al). that the representations should be useful for sensory-motor transformations, rather than encode an</p><p>6 0.51319695 <a title="10-lsi-6" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>7 0.4895041 <a title="10-lsi-7" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>8 0.47256514 <a title="10-lsi-8" href="./nips-2000-The_Interplay_of_Symbolic_and_Subsymbolic_Processes_in_Anagram_Problem_Solving.html">132 nips-2000-The Interplay of Symbolic and Subsymbolic Processes in Anagram Problem Solving</a></p>
<p>9 0.4712494 <a title="10-lsi-9" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>10 0.46861172 <a title="10-lsi-10" href="./nips-2000-Modelling_Spatial_Recall%2C_Mental_Imagery_and_Neglect.html">87 nips-2000-Modelling Spatial Recall, Mental Imagery and Neglect</a></p>
<p>11 0.43138918 <a title="10-lsi-11" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>12 0.36748099 <a title="10-lsi-12" href="./nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">42 nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<p>13 0.34483373 <a title="10-lsi-13" href="./nips-2000-Feature_Correspondence%3A_A_Markov_Chain_Monte_Carlo_Approach.html">53 nips-2000-Feature Correspondence: A Markov Chain Monte Carlo Approach</a></p>
<p>14 0.34201238 <a title="10-lsi-14" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>15 0.31622821 <a title="10-lsi-15" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>16 0.28794795 <a title="10-lsi-16" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>17 0.28643161 <a title="10-lsi-17" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>18 0.28133896 <a title="10-lsi-18" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>19 0.28006575 <a title="10-lsi-19" href="./nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">88 nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>20 0.26552343 <a title="10-lsi-20" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.011), (9, 0.086), (11, 0.034), (16, 0.047), (21, 0.036), (31, 0.248), (38, 0.061), (41, 0.019), (54, 0.046), (57, 0.016), (61, 0.072), (67, 0.016), (74, 0.041), (76, 0.029), (80, 0.031), (84, 0.118)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77538353 <a title="10-lda-1" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>Author: Shimon Edelman, Nathan Intrator</p><p>Abstract: We describe a unified framework for the understanding of structure representation in primate vision. A model derived from this framework is shown to be effectively systematic in that it has the ability to interpret and associate together objects that are related through a rearrangement of common</p><p>2 0.58253866 <a title="10-lda-2" href="./nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">8 nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>Author: Sophie Denève, Jean-René Duhamel, Alexandre Pouget</p><p>Abstract: Most models of spatial representations in the cortex assume cells with limited receptive fields that are defined in a particular egocentric frame of reference. However, cells outside of primary sensory cortex are either gain modulated by postural input or partially shifting. We show that solving classical spatial tasks, like sensory prediction, multi-sensory integration, sensory-motor transformation and motor control requires more complicated intermediate representations that are not invariant in one frame of reference. We present an iterative basis function map that performs these spatial tasks optimally with gain modulated and partially shifting units, and tests it against neurophysiological and neuropsychological data. In order to perform an action directed toward an object, it is necessary to have a representation of its spatial location. The brain must be able to use spatial cues coming from different modalities (e.g. vision, audition, touch, proprioception), combine them to infer the position of the object, and compute the appropriate movement. These cues are in different frames of reference corresponding to different sensory or motor modalities. Visual inputs are primarily encoded in retinotopic maps, auditory inputs are encoded in head centered maps and tactile cues are encoded in skin-centered maps. Going from one frame of reference to the other might seem easy. For example, the head-centered position of an object can be approximated by the sum of its retinotopic position and the eye position. However, positions are represented by population codes in the brain, and computing a head-centered map from a retinotopic map is a more complex computation than the underlying sum. Moreover, as we get closer to sensory-motor areas it seems reasonable to assume Spksls 150 100 50 o Figure 1: Response of a VIP cell to visual stimuli appearing in different part of the screen, for three different eye positions. The level of grey represent the frequency of discharge (In spikes per seconds). The white cross is the fixation point (the head is fixed). The cell's receptive field is moving with the eyes, but only partially. Here the receptive field shift is 60% of the total gaze shift. Moreover this cell is gain modulated by eye position (adapted from Duhamel et al). that the representations should be useful for sensory-motor transformations, rather than encode an</p><p>3 0.57311314 <a title="10-lda-3" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>Author: Angelo Arleo, Fabrizio Smeraldi, Stéphane Hug, Wulfram Gerstner</p><p>Abstract: We model hippocampal place cells and head-direction cells by combining allothetic (visual) and idiothetic (proprioceptive) stimuli. Visual input, provided by a video camera on a miniature robot, is preprocessed by a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. Unsupervised Hebbian learning is employed to incrementally build a population of localized overlapping place fields. Place cells serve as basis functions for reinforcement learning. Experimental results for goal-oriented navigation of a mobile robot are presented.</p><p>4 0.57072663 <a title="10-lda-4" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>Author: Kevin A. Archie, Bartlett W. Mel</p><p>Abstract: Neurons in area V4 have relatively large receptive fields (RFs), so multiple visual features are simultaneously</p><p>5 0.56899816 <a title="10-lda-5" href="./nips-2000-Learning_Winner-take-all_Competition_Between_Groups_of_Neurons_in_Lateral_Inhibitory_Networks.html">81 nips-2000-Learning Winner-take-all Competition Between Groups of Neurons in Lateral Inhibitory Networks</a></p>
<p>Author: Xiaohui Xie, Richard H. R. Hahnloser, H. Sebastian Seung</p><p>Abstract: It has long been known that lateral inhibition in neural networks can lead to a winner-take-all competition, so that only a single neuron is active at a steady state. Here we show how to organize lateral inhibition so that groups of neurons compete to be active. Given a collection of potentially overlapping groups, the inhibitory connectivity is set by a formula that can be interpreted as arising from a simple learning rule. Our analysis demonstrates that such inhibition generally results in winner-take-all competition between the given groups, with the exception of some degenerate cases. In a broader context, the network serves as a particular illustration of the general distinction between permitted and forbidden sets, which was introduced recently. From this viewpoint, the computational function of our network is to store and retrieve memories as permitted sets of coactive neurons. In traditional winner-take-all networks, lateral inhibition is used to enforce a localized, or</p><p>6 0.56426573 <a title="10-lda-6" href="./nips-2000-Structure_Learning_in_Human_Causal_Induction.html">127 nips-2000-Structure Learning in Human Causal Induction</a></p>
<p>7 0.55915141 <a title="10-lda-7" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>8 0.55503631 <a title="10-lda-8" href="./nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">129 nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<p>9 0.54949945 <a title="10-lda-9" href="./nips-2000-Universality_and_Individuality_in_a_Neural_Code.html">141 nips-2000-Universality and Individuality in a Neural Code</a></p>
<p>10 0.54790908 <a title="10-lda-10" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>11 0.54402024 <a title="10-lda-11" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>12 0.54301083 <a title="10-lda-12" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>13 0.54274392 <a title="10-lda-13" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>14 0.54092115 <a title="10-lda-14" href="./nips-2000-Keeping_Flexible_Active_Contours_on_Track_using_Metropolis_Updates.html">72 nips-2000-Keeping Flexible Active Contours on Track using Metropolis Updates</a></p>
<p>15 0.54083312 <a title="10-lda-15" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>16 0.53920752 <a title="10-lda-16" href="./nips-2000-Support_Vector_Novelty_Detection_Applied_to_Jet_Engine_Vibration_Spectra.html">128 nips-2000-Support Vector Novelty Detection Applied to Jet Engine Vibration Spectra</a></p>
<p>17 0.53894037 <a title="10-lda-17" href="./nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">89 nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>18 0.53880835 <a title="10-lda-18" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>19 0.53806049 <a title="10-lda-19" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>20 0.53737873 <a title="10-lda-20" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
