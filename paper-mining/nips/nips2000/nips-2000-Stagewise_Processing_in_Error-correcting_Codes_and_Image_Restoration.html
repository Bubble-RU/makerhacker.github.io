<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>126 nips-2000-Stagewise Processing in Error-correcting Codes and Image Restoration</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-126" href="#">nips2000-126</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>126 nips-2000-Stagewise Processing in Error-correcting Codes and Image Restoration</h1>
<br/><p>Source: <a title="nips-2000-126-pdf" href="http://papers.nips.cc/paper/1858-stagewise-processing-in-error-correcting-codes-and-image-restoration.pdf">pdf</a></p><p>Author: K. Y. Michael Wong, Hidetoshi Nishimori</p><p>Abstract: We introduce stagewise processing in error-correcting codes and image restoration, by extracting information from the former stage and using it selectively to improve the performance of the latter one. Both mean-field analysis using the cavity method and simulations show that it has the advantage of being robust against uncertainties in hyperparameter estimation. 1</p><p>Reference: <a title="nips-2000-126-reference" href="../nips2000_reference/nips-2000-Stagewise_Processing_in_Error-correcting_Codes_and_Image_Restoration_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Stagewise processing in error-correcting codes and image restoration  K. [sent-1, score-0.369]
</p><p>2 jp  Abstract We introduce stagewise processing in error-correcting codes and image restoration, by extracting information from the former stage and using it selectively to improve the performance of the latter one. [sent-8, score-0.428]
</p><p>3 Both mean-field analysis using the cavity method and simulations show that it has the advantage of being robust against uncertainties in hyperparameter estimation. [sent-9, score-0.603]
</p><p>4 1  Introduction  In error-correcting codes [1] and image restoration [2], the choice of the so-called hyperparameters is an important factor in determining their performances. [sent-10, score-0.554]
</p><p>5 Hyperparameters refer to the coefficients weighing the biases and variances of the tasks. [sent-11, score-0.029]
</p><p>6 In error correction, they determine the statistical significance given to the paritychecking terms and the received bits. [sent-12, score-0.029]
</p><p>7 Similarly in image restoration, they determine the statistical weights given to the prior knowledge and the received data. [sent-13, score-0.108]
</p><p>8 It was shown, by the use of inequalities, that the choice of the hyperparameters is optimal when there is a match between the source and model priors [3]. [sent-14, score-0.249]
</p><p>9 Furthermore, from the analytic solution of the infinite-range model and the Monte Carlo simulation of finite-dimensional models, it was shown that an inappropriate choice of the hyperparameters can lead to a rapid degradation of the tasks. [sent-15, score-0.268]
</p><p>10 However, if the prior models the source poorly, no hyperparameters can be reliable [5]. [sent-17, score-0.273]
</p><p>11 Even if they can be estimated accurately through steady-state statistical measurements, they may fluctuate when interfered by bursty noise sources in communication channels. [sent-18, score-0.025]
</p><p>12 Hence it is equally important to devise decoding or restoration procedures which are robust against the uncertainties in hyperparameter estimation. [sent-19, score-0.499]
</p><p>13 Here we introduce selective freezing to increase the tolerance to uncertainties in hy-  perparameter estimation. [sent-20, score-0.564]
</p><p>14 The technique has been studied for pattern reconstruction in neural networks, where it led to an improvement in the retrieval precision, a widening of the basin of attraction, and a boost in the storage capacity [6]. [sent-21, score-0.055]
</p><p>15 The idea is best illustrated for bits or pixels with binary states ±1, though it can be easily generalized to other cases. [sent-22, score-0.152]
</p><p>16 In a finite temperature thermodynamic process, the binary variables keep moving under thermal agitation. [sent-23, score-0.234]
</p><p>17 Some of them have smaller thermal fluctuations than the others, implying that they are more certain to stay in one state than the other. [sent-24, score-0.215]
</p><p>18 This stability implies that they have a higher probability to stay in the correct state for error-correction or image restoration tasks, even when the hyperparameters are not optimally tuned. [sent-25, score-0.556]
</p><p>19 It may thus be interesting to separate the thermodynamic process into two stages. [sent-26, score-0.152]
</p><p>20 In the first stage we select those relatively stable bits or pixels whose time-averaged states have a magnitude exceeding a certain threshold. [sent-27, score-0.432]
</p><p>21 In the second stage we subsequently fix (or freeze) them in the most probable thermodynamic states. [sent-28, score-0.351]
</p><p>22 Thus these selectively frozen bits or pixels are able to provide a more robust assistance to the less stable bits or pixels in their search for the most probable states. [sent-29, score-0.558]
</p><p>23 The two-stage thermodynamic process can be studied analytically in the mean-field model using the cavity method. [sent-30, score-0.59]
</p><p>24 For the more realistic cases of finite dimensions in image restoration, simulation results illustrate the relevance of the infinite-range model in providing qualitative guidance. [sent-31, score-0.084]
</p><p>25 Detailed theory of selective freezing is presented in [7]. [sent-32, score-0.45]
</p><p>26 2  Formulation  Consider an information source which generates data represented by a set of Ising spins {~i}' where ~i = ±1 and i = 1" . [sent-33, score-0.3]
</p><p>27 The data is generated according to the source prior Ps ( {~d ). [sent-36, score-0.088]
</p><p>28 For error-correcting codes transmitting unbiased messages, all sequences are equally probable and Ps({O) = 2-N. [sent-37, score-0.196]
</p><p>29 For images with smooth structures, the prior consists of ferromagnetic Boltzmann factors, which increase the tendencies of the neighboring spins to stay at the same spin states, that is, Ps (  {~}) ex exp (~ 2: ~i~j)  (1)  . [sent-38, score-0.804]
</p><p>30 (ij)  Here (ij) represents pairs of neighboring spins, z is the valency of each site. [sent-39, score-0.104]
</p><p>31 The data is coded by constructing the codewords, which are the products of p spins JR . [sent-40, score-0.236]
</p><p>32 Each spin may appear in a number of p-spin codewords; the number of times of appearance is called the valency zp. [sent-50, score-0.402]
</p><p>33 For conventional image restoration, codewords with only p = 1 are transmitted, corresponding to the pixels in the image. [sent-51, score-0.236]
</p><p>34 When the signal is transmitted through a noisy channel, the output consists of the sets {Jh . [sent-52, score-0.079]
</p><p>35 ip} and {Ti}, which are the corrupted versions of {JR"'i p} and {~i} respectively, and described by the output probability Pout ({ J},  {T} I{0) ex exp  (/ 2: Jil . [sent-55, score-0.052]
</p><p>36 (2)  According to Bayesian statistics, the posterior probability that the source sequence is {(T}, given the outputs {J} and {T}, takes the form P( {(T }I{J}, {T}) ex exp  ((3J 2: Jil . [sent-62, score-0.09]
</p><p>37 One then regards sgn(ai} as the ith spin of the decoding/restoration process. [sent-80, score-0.393]
</p><p>38 The most important quantity in selective freezing is the overlap of the decoded/restored bit sgn(ai} and the original bit ei averaged over the output probability and the spin distribution. [sent-81, score-0.972]
</p><p>39 (7)  ~  Following [3], we can prove that selective freezing cannot outperform the single-stage process if the hyperparameters can be estimated precisely. [sent-83, score-0.688]
</p><p>40 However, the purpose of selective freezing is rather to provide a relatively stable performance when the hyperparameters cannot be estimated precisely. [sent-84, score-0.701]
</p><p>41 3  Modeling error-correcting codes  Let us now suppose that the output of the transmission channel consists of only the set of p-spin interactions {Jh --- ip }. [sent-85, score-0.35]
</p><p>42 Then h = 0 in the Hamiltonian (5), and we set (3m = 0 for the case that all messages are equally probable. [sent-86, score-0.089]
</p><p>43 Analytical solutions are available for the infinite-range model in which the exchange interactions are present for all possible pairs of sites. [sent-87, score-0.119]
</p><p>44 We can apply a gauge transformation a i -+ a iei and J h ---ip -+ Ji1 ---ip ei1 . [sent-93, score-0.052]
</p><p>45 ei p, and arrive at an equivalent p-spin model with a ferromagnetic bias, where  (8) The infinite-range model is exactly solvable using the cavity method [8]. [sent-96, score-0.572]
</p><p>46 The method uses a self-consistency argument to consider what happens when a spin is added or removed from the system. [sent-97, score-0.407]
</p><p>47 The central quantity in this method is the  cavity field, which is the local field of a spin when it is added to the system, assuming that the exchange couplings act only one-way from the system to the new spin (but not from the spin back to the system). [sent-98, score-1.661]
</p><p>48 Since the exchange couplings feeding the new spin have no correlations with the system, the cavity field becomes a Gaussian variable in the limit of large valency. [sent-99, score-0.968]
</p><p>49 N ~(Ui)  i  (10)  i  Applying self-consistently the cavity argument to all terms in Eq. [sent-101, score-0.458]
</p><p>50 Its mean and variance are pjoin p - 1 and pJ 2 ijP-1/2 respectively, where in and ij are the magnetization and EdwardsAnderson order parameter respectively, given by  in  = ~L  [8((}2 - (Ui)2)(Ui)  + 8((Ui)2 - (}2)sgn(ui)] ,  (15)  i  ij  _  ~L  [8((}2 - (Ui)2)(Ui)2  + 8((Ui)2 -  (}2)] . [sent-105, score-0.241]
</p><p>51 Applying self-consistently the same cavity argument to all terms in Eqs. [sent-108, score-0.458]
</p><p>52 (15), (16), (13) and (14), we arrive at the self-consistent equations for in, ij, rand Xtr. [sent-109, score-0.038]
</p><p>53 The performance of selective freezing is measured by Msf  == ~ L [8((}2 - (Ui)2)sgn(ui) + 8((Ui)2 - (}2)sgn(ui)] . [sent-110, score-0.45]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cavity', 0.41), ('spin', 0.334), ('freezing', 0.308), ('ui', 0.304), ('spins', 0.236), ('restoration', 0.236), ('hyperparameters', 0.185), ('ip', 0.167), ('stage', 0.155), ('selective', 0.142), ('sgn', 0.14), ('thermodynamic', 0.124), ('thermal', 0.11), ('codewords', 0.103), ('uncertainties', 0.088), ('hamiltonian', 0.088), ('exchange', 0.088), ('ai', 0.085), ('jh', 0.08), ('stay', 0.08), ('pixels', 0.078), ('codes', 0.078), ('ij', 0.078), ('hyperparameter', 0.074), ('bits', 0.074), ('ferromagnetic', 0.068), ('freeze', 0.068), ('hong', 0.068), ('jil', 0.068), ('kong', 0.068), ('msf', 0.068), ('stagewise', 0.068), ('valency', 0.068), ('xtr', 0.068), ('respectively', 0.068), ('aj', 0.067), ('source', 0.064), ('frozen', 0.059), ('exceeding', 0.059), ('magnetization', 0.059), ('field', 0.056), ('image', 0.055), ('ps', 0.053), ('transmitted', 0.053), ('couplings', 0.053), ('bit', 0.052), ('jr', 0.049), ('tokyo', 0.049), ('probable', 0.048), ('argument', 0.048), ('channel', 0.048), ('messages', 0.046), ('selectively', 0.046), ('pj', 0.044), ('equally', 0.043), ('stable', 0.041), ('stages', 0.038), ('arrive', 0.038), ('uj', 0.036), ('neighboring', 0.036), ('il', 0.035), ('robust', 0.031), ('ti', 0.031), ('ei', 0.031), ('interactions', 0.031), ('ith', 0.03), ('nishimori', 0.029), ('weighing', 0.029), ('assistance', 0.029), ('regards', 0.029), ('wong', 0.029), ('inappropriate', 0.029), ('attraction', 0.029), ('received', 0.029), ('simulation', 0.029), ('studied', 0.028), ('process', 0.028), ('correlations', 0.027), ('quantity', 0.027), ('physics', 0.027), ('boost', 0.027), ('devise', 0.027), ('transmitting', 0.027), ('iei', 0.027), ('variance', 0.026), ('introduce', 0.026), ('ex', 0.026), ('output', 0.026), ('threshold', 0.025), ('added', 0.025), ('estimated', 0.025), ('select', 0.025), ('solvable', 0.025), ('receiver', 0.025), ('gauge', 0.025), ('implying', 0.025), ('degradation', 0.025), ('say', 0.024), ('prior', 0.024), ('second', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="126-tfidf-1" href="./nips-2000-Stagewise_Processing_in_Error-correcting_Codes_and_Image_Restoration.html">126 nips-2000-Stagewise Processing in Error-correcting Codes and Image Restoration</a></p>
<p>Author: K. Y. Michael Wong, Hidetoshi Nishimori</p><p>Abstract: We introduce stagewise processing in error-correcting codes and image restoration, by extracting information from the former stage and using it selectively to improve the performance of the latter one. Both mean-field analysis using the cavity method and simulations show that it has the advantage of being robust against uncertainties in hyperparameter estimation. 1</p><p>2 0.16458525 <a title="126-tfidf-2" href="./nips-2000-Error-correcting_Codes_on_a_Bethe-like_Lattice.html">47 nips-2000-Error-correcting Codes on a Bethe-like Lattice</a></p>
<p>Author: Renato Vicente, David Saad, Yoshiyuki Kabashima</p><p>Abstract: We analyze Gallager codes by employing a simple mean-field approximation that distorts the model geometry and preserves important interactions between sites. The method naturally recovers the probability propagation decoding algorithm as an extremization of a proper free-energy. We find a thermodynamic phase transition that coincides with information theoretical upper-bounds and explain the practical code performance in terms of the free-energy landscape.</p><p>3 0.088187985 <a title="126-tfidf-3" href="./nips-2000-A_Variational_Mean-Field_Theory_for_Sigmoidal_Belief_Networks.html">14 nips-2000-A Variational Mean-Field Theory for Sigmoidal Belief Networks</a></p>
<p>Author: Chiranjib Bhattacharyya, S. Sathiya Keerthi</p><p>Abstract: A variational derivation of Plefka's mean-field theory is presented. This theory is then applied to sigmoidal belief networks with the aid of further approximations. Empirical evaluation on small scale networks show that the proposed approximations are quite competitive. 1</p><p>4 0.073858723 <a title="126-tfidf-4" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>Author: Zoubin Ghahramani, Matthew J. Beal</p><p>Abstract: Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set. 1</p><p>5 0.05881355 <a title="126-tfidf-5" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>Author: Simon Tong, Daphne Koller</p><p>Abstract: Bayesian networks are graphical representations of probability distributions. In virtually all of the work on learning these networks, the assumption is that we are presented with a data set consisting of randomly generated instances from the underlying distribution. In many situations, however, we also have the option of active learning, where we have the possibility of guiding the sampling process by querying for certain types of samples. This paper addresses the problem of estimating the parameters of Bayesian networks in an active learning setting. We provide a theoretical framework for this problem, and an algorithm that chooses which active learning queries to generate based on the model learned so far. We present experimental results showing that our active learning algorithm can significantly reduce the need for training data in many situations.</p><p>6 0.058635999 <a title="126-tfidf-6" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>7 0.051816273 <a title="126-tfidf-7" href="./nips-2000-Analysis_of_Bit_Error_Probability_of_Direct-Sequence_CDMA_Multiuser_Demodulators.html">25 nips-2000-Analysis of Bit Error Probability of Direct-Sequence CDMA Multiuser Demodulators</a></p>
<p>8 0.051705495 <a title="126-tfidf-8" href="./nips-2000-Second_Order_Approximations_for_Probability_Models.html">114 nips-2000-Second Order Approximations for Probability Models</a></p>
<p>9 0.048223864 <a title="126-tfidf-9" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>10 0.047219381 <a title="126-tfidf-10" href="./nips-2000-Ensemble_Learning_and_Linear_Response_Theory_for_ICA.html">46 nips-2000-Ensemble Learning and Linear Response Theory for ICA</a></p>
<p>11 0.046757627 <a title="126-tfidf-11" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>12 0.046245579 <a title="126-tfidf-12" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>13 0.041765522 <a title="126-tfidf-13" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>14 0.04152821 <a title="126-tfidf-14" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>15 0.041269362 <a title="126-tfidf-15" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>16 0.041183349 <a title="126-tfidf-16" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>17 0.039845418 <a title="126-tfidf-17" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>18 0.038329575 <a title="126-tfidf-18" href="./nips-2000-Learning_Continuous_Distributions%3A_Simulations_With_Field_Theoretic_Priors.html">76 nips-2000-Learning Continuous Distributions: Simulations With Field Theoretic Priors</a></p>
<p>19 0.038122751 <a title="126-tfidf-19" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>20 0.038052134 <a title="126-tfidf-20" href="./nips-2000-Combining_ICA_and_Top-Down_Attention_for_Robust_Speech_Recognition.html">33 nips-2000-Combining ICA and Top-Down Attention for Robust Speech Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.134), (1, -0.032), (2, 0.064), (3, -0.017), (4, 0.103), (5, -0.018), (6, -0.007), (7, -0.009), (8, 0.026), (9, 0.058), (10, -0.053), (11, 0.026), (12, -0.144), (13, 0.089), (14, -0.081), (15, 0.032), (16, -0.086), (17, -0.109), (18, 0.182), (19, -0.226), (20, -0.001), (21, -0.023), (22, 0.047), (23, -0.054), (24, -0.041), (25, 0.083), (26, 0.313), (27, -0.099), (28, 0.113), (29, 0.145), (30, 0.02), (31, -0.137), (32, -0.089), (33, -0.055), (34, 0.089), (35, -0.089), (36, 0.006), (37, -0.166), (38, 0.039), (39, 0.119), (40, 0.019), (41, 0.073), (42, -0.046), (43, -0.122), (44, -0.068), (45, -0.058), (46, 0.051), (47, 0.089), (48, 0.043), (49, 0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97135675 <a title="126-lsi-1" href="./nips-2000-Stagewise_Processing_in_Error-correcting_Codes_and_Image_Restoration.html">126 nips-2000-Stagewise Processing in Error-correcting Codes and Image Restoration</a></p>
<p>Author: K. Y. Michael Wong, Hidetoshi Nishimori</p><p>Abstract: We introduce stagewise processing in error-correcting codes and image restoration, by extracting information from the former stage and using it selectively to improve the performance of the latter one. Both mean-field analysis using the cavity method and simulations show that it has the advantage of being robust against uncertainties in hyperparameter estimation. 1</p><p>2 0.67041141 <a title="126-lsi-2" href="./nips-2000-Analysis_of_Bit_Error_Probability_of_Direct-Sequence_CDMA_Multiuser_Demodulators.html">25 nips-2000-Analysis of Bit Error Probability of Direct-Sequence CDMA Multiuser Demodulators</a></p>
<p>Author: Toshiyuki Tanaka</p><p>Abstract: We analyze the bit error probability of multiuser demodulators for directsequence binary phase-shift-keying (DSIBPSK) CDMA channel with additive gaussian noise. The problem of multiuser demodulation is cast into the finite-temperature decoding problem, and replica analysis is applied to evaluate the performance of the resulting MPM (Marginal Posterior Mode) demodulators, which include the optimal demodulator and the MAP demodulator as special cases. An approximate implementation of demodulators is proposed using analog-valued Hopfield model as a naive mean-field approximation to the MPM demodulators, and its performance is also evaluated by the replica analysis. Results of the performance evaluation shows effectiveness of the optimal demodulator and the mean-field demodulator compared with the conventional one, especially in the cases of small information bit rate and low noise level. 1</p><p>3 0.65703261 <a title="126-lsi-3" href="./nips-2000-Error-correcting_Codes_on_a_Bethe-like_Lattice.html">47 nips-2000-Error-correcting Codes on a Bethe-like Lattice</a></p>
<p>Author: Renato Vicente, David Saad, Yoshiyuki Kabashima</p><p>Abstract: We analyze Gallager codes by employing a simple mean-field approximation that distorts the model geometry and preserves important interactions between sites. The method naturally recovers the probability propagation decoding algorithm as an extremization of a proper free-energy. We find a thermodynamic phase transition that coincides with information theoretical upper-bounds and explain the practical code performance in terms of the free-energy landscape.</p><p>4 0.31141636 <a title="126-lsi-4" href="./nips-2000-A_Variational_Mean-Field_Theory_for_Sigmoidal_Belief_Networks.html">14 nips-2000-A Variational Mean-Field Theory for Sigmoidal Belief Networks</a></p>
<p>Author: Chiranjib Bhattacharyya, S. Sathiya Keerthi</p><p>Abstract: A variational derivation of Plefka's mean-field theory is presented. This theory is then applied to sigmoidal belief networks with the aid of further approximations. Empirical evaluation on small scale networks show that the proposed approximations are quite competitive. 1</p><p>5 0.25837991 <a title="126-lsi-5" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>Author: Silvia Scarpetta, Zhaoping Li, John A. Hertz</p><p>Abstract: We apply to oscillatory networks a class of learning rules in which synaptic weights change proportional to pre- and post-synaptic activity, with a kernel A(r) measuring the effect for a postsynaptic spike a time r after the presynaptic one. The resulting synaptic matrices have an outer-product form in which the oscillating patterns are represented as complex vectors. In a simple model, the even part of A(r) enhances the resonant response to learned stimulus by reducing the effective damping, while the odd part determines the frequency of oscillation. We relate our model to the olfactory cortex and hippocampus and their presumed roles in forming associative memories and input representations. 1</p><p>6 0.24325611 <a title="126-lsi-6" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>7 0.23803262 <a title="126-lsi-7" href="./nips-2000-Stability_and_Noise_in_Biochemical_Switches.html">125 nips-2000-Stability and Noise in Biochemical Switches</a></p>
<p>8 0.23713276 <a title="126-lsi-8" href="./nips-2000-Ensemble_Learning_and_Linear_Response_Theory_for_ICA.html">46 nips-2000-Ensemble Learning and Linear Response Theory for ICA</a></p>
<p>9 0.22033848 <a title="126-lsi-9" href="./nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">19 nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>10 0.2177441 <a title="126-lsi-10" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>11 0.21535522 <a title="126-lsi-11" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>12 0.19803077 <a title="126-lsi-12" href="./nips-2000-Active_Support_Vector_Machine_Classification.html">18 nips-2000-Active Support Vector Machine Classification</a></p>
<p>13 0.19558659 <a title="126-lsi-13" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>14 0.19033036 <a title="126-lsi-14" href="./nips-2000-The_Manhattan_World_Assumption%3A_Regularities_in_Scene_Statistics_which_Enable_Bayesian_Inference.html">135 nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</a></p>
<p>15 0.17761403 <a title="126-lsi-15" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>16 0.17262012 <a title="126-lsi-16" href="./nips-2000-Combining_ICA_and_Top-Down_Attention_for_Robust_Speech_Recognition.html">33 nips-2000-Combining ICA and Top-Down Attention for Robust Speech Recognition</a></p>
<p>17 0.16535461 <a title="126-lsi-17" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>18 0.16318449 <a title="126-lsi-18" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>19 0.16175781 <a title="126-lsi-19" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>20 0.16107637 <a title="126-lsi-20" href="./nips-2000-Computing_with_Finite_and_Infinite_Networks.html">35 nips-2000-Computing with Finite and Infinite Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(17, 0.08), (32, 0.016), (33, 0.029), (34, 0.454), (36, 0.011), (54, 0.013), (55, 0.02), (62, 0.035), (65, 0.045), (67, 0.057), (75, 0.01), (76, 0.048), (79, 0.018), (81, 0.017), (90, 0.034), (97, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82554519 <a title="126-lda-1" href="./nips-2000-Stagewise_Processing_in_Error-correcting_Codes_and_Image_Restoration.html">126 nips-2000-Stagewise Processing in Error-correcting Codes and Image Restoration</a></p>
<p>Author: K. Y. Michael Wong, Hidetoshi Nishimori</p><p>Abstract: We introduce stagewise processing in error-correcting codes and image restoration, by extracting information from the former stage and using it selectively to improve the performance of the latter one. Both mean-field analysis using the cavity method and simulations show that it has the advantage of being robust against uncertainties in hyperparameter estimation. 1</p><p>2 0.54450667 <a title="126-lda-2" href="./nips-2000-Regularized_Winnow_Methods.html">111 nips-2000-Regularized Winnow Methods</a></p>
<p>Author: Tong Zhang</p><p>Abstract: In theory, the Winnow multiplicative update has certain advantages over the Perceptron additive update when there are many irrelevant attributes. Recently, there has been much effort on enhancing the Perceptron algorithm by using regularization, leading to a class of linear classification methods called support vector machines. Similarly, it is also possible to apply the regularization idea to the Winnow algorithm, which gives methods we call regularized Winnows. We show that the resulting methods compare with the basic Winnows in a similar way that a support vector machine compares with the Perceptron. We investigate algorithmic issues and learning properties of the derived methods. Some experimental results will also be provided to illustrate different methods.</p><p>3 0.26123261 <a title="126-lda-3" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>Author: Lehel Csatč´¸, Manfred Opper</p><p>Abstract: We develop an approach for a sparse representation for Gaussian Process (GP) models in order to overcome the limitations of GPs caused by large data sets. The method is based on a combination of a Bayesian online algorithm together with a sequential construction of a relevant subsample of the data which fully specifies the prediction of the model. Experimental results on toy examples and large real-world data sets indicate the efficiency of the approach.</p><p>4 0.26117268 <a title="126-lda-4" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>Author: Zoubin Ghahramani, Matthew J. Beal</p><p>Abstract: Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set. 1</p><p>5 0.25790927 <a title="126-lda-5" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>Author: Martin Szummer, Tommi Jaakkola</p><p>Abstract: Modern classification applications necessitate supplementing the few available labeled examples with unlabeled examples to improve classification performance. We present a new tractable algorithm for exploiting unlabeled examples in discriminative classification. This is achieved essentially by expanding the input vectors into longer feature vectors via both labeled and unlabeled examples. The resulting classification method can be interpreted as a discriminative kernel density estimate and is readily trained via the EM algorithm, which in this case is both discriminative and achieves the optimal solution. We provide, in addition, a purely discriminative formulation of the estimation problem by appealing to the maximum entropy framework. We demonstrate that the proposed approach requires very few labeled examples for high classification accuracy.</p><p>6 0.25770384 <a title="126-lda-6" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>7 0.2573466 <a title="126-lda-7" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>8 0.25513008 <a title="126-lda-8" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>9 0.25511909 <a title="126-lda-9" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>10 0.25407502 <a title="126-lda-10" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<p>11 0.25211912 <a title="126-lda-11" href="./nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">95 nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>12 0.25159693 <a title="126-lda-12" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>13 0.2514233 <a title="126-lda-13" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>14 0.2511543 <a title="126-lda-14" href="./nips-2000-The_Kernel_Trick_for_Distances.html">134 nips-2000-The Kernel Trick for Distances</a></p>
<p>15 0.24879663 <a title="126-lda-15" href="./nips-2000-On_Reversing_Jensen%27s_Inequality.html">94 nips-2000-On Reversing Jensen's Inequality</a></p>
<p>16 0.24858582 <a title="126-lda-16" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>17 0.24814215 <a title="126-lda-17" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>18 0.24808472 <a title="126-lda-18" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>19 0.24804345 <a title="126-lda-19" href="./nips-2000-Constrained_Independent_Component_Analysis.html">36 nips-2000-Constrained Independent Component Analysis</a></p>
<p>20 0.24770425 <a title="126-lda-20" href="./nips-2000-Algorithms_for_Non-negative_Matrix_Factorization.html">22 nips-2000-Algorithms for Non-negative Matrix Factorization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
