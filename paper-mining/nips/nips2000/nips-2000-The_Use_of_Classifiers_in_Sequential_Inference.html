<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>138 nips-2000-The Use of Classifiers in Sequential Inference</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-138" href="#">nips2000-138</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>138 nips-2000-The Use of Classifiers in Sequential Inference</h1>
<br/><p>Source: <a title="nips-2000-138-pdf" href="http://papers.nips.cc/paper/1817-the-use-of-classifiers-in-sequential-inference.pdf">pdf</a></p><p>Author: Vasin Punyakanok, Dan Roth</p><p>Abstract: We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.</p><p>Reference: <a title="nips-2000-138-reference" href="../nips2000_reference/nips-2000-The_Use_of_Classifiers_in_Sequential_Inference_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. [sent-5, score-0.559]
</p><p>2 In particular, we develop two general approaches for an important subproblem - identifying phrase structure. [sent-6, score-0.559]
</p><p>3 The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. [sent-7, score-0.444]
</p><p>4 We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing. [sent-9, score-0.214]
</p><p>5 1 Introduction In many situations it is necessary to make decisions that depend on the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints - the sequential nature of the data or other domain specific constraints. [sent-10, score-0.64]
</p><p>6 Consider, for example, the problem of chunking natural language sentences where the goal is to identify several kinds of phrases (e. [sent-11, score-0.632]
</p><p>7 For example, one way to address the problem is to utilize two classifiers for each phrase type, one of which recognizes the beginning of the phrase, and the other its end. [sent-15, score-0.731]
</p><p>8 Clearly, there are constraints over the predictions; for instance, phrases cannot overlap and there are probabilistic constraints over the order of phrases and their lengths. [sent-16, score-1.14]
</p><p>9 The above mentioned problem is an instance of a general class of problems - identifying the phrase structure in sequential data. [sent-17, score-0.525]
</p><p>10 This paper develops two general approaches for this class of problems by utilizing general classifiers and performing inferences with their outcomes. [sent-18, score-0.346]
</p><p>11 Our formalisms directly applies to natural language problems such as shallow parsing [7, 23, 5, 3, 21], computational biology problems such as identifying splice sites [8,4, 15], and problems in information extraction [9]. [sent-19, score-0.55]
</p><p>12 In this case, classifiers are functions of the observation sequence and their outcomes represent states; we study two Markov models that are used as inference procedures and differ in the type of classifiers and the details of the probabilistic modeling. [sent-21, score-1.003]
</p><p>13 The critical shortcoming of this framework is that it attempts to maximize the likelihood of the state sequence - not the true performance measure of interest but only a derivative of it. [sent-22, score-0.127]
</p><p>14 The second approach extends a constraint satisfaction formalism to deal with variables that are associated with costs and shows how to use this to model the classifier combination problem. [sent-23, score-0.25]
</p><p>15 In this approach general constraints can be incorporated flexibly and algorithms can be developed that closely address  the true global optimization criterion of interest. [sent-24, score-0.159]
</p><p>16 For both approaches we develop efficient combination algorithms that use general classifiers to yield the inference. [sent-25, score-0.346]
</p><p>17 The approaches are studied experimentally in the context of shallow parsing - the task of identifying syntactic sequences in sentences [14, 1, 11] - which has been found useful in many large-scale language processing applications including information extraction and text summarization [12, 2]. [sent-26, score-0.664]
</p><p>18 Working within a concrete task allows us to compare the approaches experimentally for phrase types such as base Noun Phrases (NPs) and SubjectVerb phrases (SVs) that differ significantly in their statistical properties, including length and internal dependencies. [sent-27, score-0.986]
</p><p>19 Our two main methods, projection-based Markov Models (PMM) and constraint satisfaction with classifiers (CSCL) are shown to perform very well on the task of predicting NP and SV phrases, with CSCL at least as good as any other method tried on these tasks. [sent-29, score-0.411]
</p><p>20 We attribute it to CSCL's ability to cope better with the length of the phrase and the long term dependencies. [sent-31, score-0.446]
</p><p>21 2  Identifying Phrase Structure  The inference problem considered can be formalized as that of identifying the phrase structure of an input string. [sent-33, score-0.553]
</p><p>22 On >, a phrase is a substring of consecutive input symbols Oi, 0i+l, . [sent-37, score-0.447]
</p><p>23 Some external mechanism is assumed to consistently (or stochastically) annotate substrings as phrases l . [sent-41, score-0.496]
</p><p>24 Our goal is to come up with a mechanism that, given an input string, identifies the phrases in this string. [sent-42, score-0.526]
</p><p>25 The identification mechanism works by using classifiers that attempt to recognize in the input string local signals which are indicative to the existence of a phrase. [sent-43, score-0.618]
</p><p>26 We assume that the outcome of the classifier at input symbol 0 can be represented as a function of the local context of 0 in the input string, perhaps with the aid of some external information inferred from it2 . [sent-44, score-0.216]
</p><p>27 Classifiers can indicate that an input symbol 0 is inside or outside a phrase (10 modeling) or they can indicate that an input symbol 0 opens or closes a phrase (the OC modeling) or some combination of the two. [sent-45, score-1.071]
</p><p>28 Our work here focuses on OC modeling which has been shown to be more robust than the 10, especially with fairly long phrases [21]. [sent-46, score-0.504]
</p><p>29 In any case, the classifiers' outcomes can be combined to determine the phrases in the input string. [sent-47, score-0.582]
</p><p>30 This process, however, needs to satisfy some constraints for the resulting set of phrases to be legitimate. [sent-48, score-0.544]
</p><p>31 The goal is thus two fold: to learn classifiers that recognize the local signals and to combine them in a way that respects the constraints. [sent-50, score-0.427]
</p><p>32 We call the inference algorithm that combines the classifiers and outputs a coherent phrase structure a combinator. [sent-51, score-0.822]
</p><p>33 The performance of this process is measured by how accurately it retrieves the phrase structure of the input string. [sent-52, score-0.447]
</p><p>34 This is quantified in terms of recall - the percentage of phrases that are correctly identified - and precision - the percentage of identified phrases that are indeed correct phrases. [sent-53, score-0.93]
</p><p>35 1 We assume here a single type of phrase, and thus each input symbol is either in a phrase or outside it. [sent-54, score-0.535]
</p><p>36 All the methods can be extended to deal with several kinds of phrases in a string. [sent-55, score-0.465]
</p><p>37 2Jn the case of natural language processing, if the DiS are words in a sentence, additional information might include morphological information, part of speech tags, semantic class information from WordNet, etc. [sent-56, score-0.21]
</p><p>38 3  Markov Modeling  HMM is a probabilistic finite state automaton that models the probabilistic generation of sequential processes. [sent-58, score-0.196]
</p><p>39 It consists of a finite set S of states, a set 0 of observations, an initial state distribution Pl(s), a state-transition distribution P(sls') (s, s' E S) and an observation distribution P(ols) (0 EO, s E S). [sent-59, score-0.157]
</p><p>40 A sequence of observations is generated by first picking an initial state according to PI (s); this state produces an observation according to P(ols) and transits to a new state according to P(sls'). [sent-60, score-0.388]
</p><p>41 This state produces the next observation, and the process goes on until it reaches a designated final state [22]. [sent-61, score-0.114]
</p><p>42 In a supervised learning task, an observation sequence 0 =< 01,02,' . [sent-62, score-0.17]
</p><p>43 On > is supervised by a corresponding state sequence S =< Sl, S2,'" sn >. [sent-64, score-0.127]
</p><p>44 This allows one to estimate the HMM parameters and then, given a new observation sequence, to identify the most likely corresponding state sequence. [sent-65, score-0.191]
</p><p>45 2) using local signals from which the state sequence can be recovered. [sent-67, score-0.156]
</p><p>46 1  A Hidden Markov Model Combinator  To recover the most likely state sequence in HMM, we wish to estimate all the required probability distributions. [sent-71, score-0.161]
</p><p>47 That is, we are given classifiers with states as their outcomes. [sent-74, score-0.351]
</p><p>48 We still need Pt (0) which is harder to approximate but, for each t, can be treated as a constant 'fit because the goal is to find the most likely sequence of states for the given observations, which are the same for all compared sequences. [sent-81, score-0.175]
</p><p>49 With this scheme, we can still combine the classifiers' predictions by finding the most likely sequence for an observation sequence using dynamic programming. [sent-82, score-0.313]
</p><p>50 6 we estimate P(slo) based on a whole observation sequence rather than 0t to significantly improve the performance. [sent-86, score-0.17]
</p><p>51 2 A Projection based Markov Model Combinator In HMMs, observations are allowed to depend only on the current state and long term dependencies are not modeled. [sent-88, score-0.104]
</p><p>52 Equivalently, the constraints structure is restricted by having a stationary probability distribution of a state given the previous one. [sent-89, score-0.136]
</p><p>53 Thus, given an observation sequence 0 we can find the most likely state sequence S given 0 by maximizing n  n  t=2  t=2  Hence, this model generalizes the standard HMM by combining the state-transition probability and the observation probability into one function. [sent-98, score-0.431]
</p><p>54 The most likely state sequence can  still be recovered using the dynamic programming (Viterbi) algorithm if we modify the recursive step: 8t ( s) = maxs'ES 8t - l (s')P( sis', Ot). [sent-99, score-0.161]
</p><p>55 To learn these classifiers we follow the projection approach [26] and separate P( sis', 0) to many functions Ps ' (slo) according to the previous states s'. [sent-101, score-0.38]
</p><p>56 ) Since these are simpler classifiers we hope that the performance will improve. [sent-104, score-0.314]
</p><p>57 6 exhibits the contribution of estimating Ps ' (s 10) using a wider window in the observation sequence. [sent-107, score-0.129]
</p><p>58 In both cases, the attempt to combine classifiers with Markov models is motivated by an attempt to improve the existing Markov models; the belief is that this would yield better generalization than the pure observation probability estimation from the training data. [sent-111, score-0.531]
</p><p>59 The starting point is the existence of general classifiers that provide some local information on the input sequence along with constraints on their outcomes; our goal is to use the classifiers to infer the phrase structure of the sequence in a way that satisfies the constraints. [sent-113, score-1.357]
</p><p>60 Technically, another novelty worth mentioning is that we use a wider range of observations instead of a single observation to predict a state. [sent-115, score-0.176]
</p><p>61 4  Constraints Satisfaction with Classifiers  This section describes a different model that is based on an extension of the Boolean constraint satisfaction (CSP) formalism [17] to handle variables that are the outcome of classifiers. [sent-117, score-0.147]
</p><p>62 On > and local classifiers that, without loss of generality, take two distinct values, one indicating openning a phrase and a second indicating closing it (OC modeling). [sent-121, score-0.76]
</p><p>63 The classifiers provide their output in terms of the probability P(o) and P(c), given the observation. [sent-122, score-0.314]
</p><p>64 We extend the CSP formalism to deal with probabilistic variables (or, more generally, variables with cost) as follows. [sent-123, score-0.102]
</p><p>65 The constraints are encoded as clauses and, as in standard CSP modeling the Boolean CSP becomes a CNF (conjunctive normal form) formula f. [sent-125, score-0.149]
</p><p>66 One efficient way to use this general scheme is by encoding phrases as variables. [sent-128, score-0.465]
</p><p>67 Then, all the non-overlapping constraints can be encoded in: I\e; overlaps ej (-,ei V -,ej). [sent-130, score-0.11]
</p><p>68 This yields a quadratic number of variables, and the constraints are binary, encoding the restriction that phrases do not overlap. [sent-131, score-0.544]
</p><p>69 For the specific case of phrase structure, however, we can find the optimal solution in linear time. [sent-133, score-0.417]
</p><p>70 The solution to the optimization problem corresponds to a shortest path in a directed acyclic graph constructed on the observations symbols, with legitimate phrases (the variables of the CSP) as its edges and their cost as the edges' weights. [sent-134, score-0.579]
</p><p>71 A natural cost function is to use the classifiers probabilities P(o) and P(c) and define, for a phrase e = (0, c), c(e) = 1 - P(o)P(c). [sent-140, score-0.799]
</p><p>72 5  Shallow Parsing  We use shallow parsing tasks in order to evaluate our approaches. [sent-143, score-0.256]
</p><p>73 Shallow parsing involves the identification of phrases or of words that participate in a syntactic relationship. [sent-144, score-0.721]
</p><p>74 The observation that shallow syntactic information can be extracted using local information by examining the pattern itself, its nearby context and the local part-of-speech information - has motivated the use of learning methods to recognize these patterns [7, 23, 3, 5]. [sent-145, score-0.395]
</p><p>75 In this work we study the identification of two types of phrases, base Noun Phrases (NP) and Subject Verb (SV) patterns. [sent-146, score-0.131]
</p><p>76 Consequently, each classifier may output three possible outcomes 0, nOi, nOo (open, not open inside, not open outside) and C, nCi, nCo, resp. [sent-151, score-0.22]
</p><p>77 Figure 1: State-transition diagram for the phrase recognition problem. [sent-154, score-0.417]
</p><p>78 1  Classification  The classifier we use to learn the states as a function of the observation is SNoW [24, 6], a multi-class classifier that is specifically tailored for large scale learning tasks. [sent-156, score-0.283]
</p><p>79 SNoW has already been used successfully for a variety of tasks in natural language and visual processing [10, 25]. [sent-158, score-0.109]
</p><p>80 In the current study we normalize the activation levels of all targets to sum to 1 and output the outcomes for all targets (states). [sent-161, score-0.178]
</p><p>81 6 Experiments We experimented both with NPs and SVs and we show results for two different representations of the observations (that is, different feature sets for the classifiers) - part of speech (PaS) information only and pas with additional lexical information (words). [sent-164, score-0.202]
</p><p>82 For NP, the training and test corpus was prepared from sections 15 to 18 and section 20, respectively; the SV phrase corpus was prepared from sections 1 to 9 for training and section for testing. [sent-169, score-0.571]
</p><p>83 When the observations are in terms of lexical items, the data is too sparse to yield robust estimates and these entries were left empty. [sent-172, score-0.103]
</p><p>84 The NB (naive Bayes) and SNoW classifiers use the same feature set, conjunctions of size 3 of pas tags (PaS and words, resp. [sent-173, score-0.49]
</p><p>85 Table 1: Results (F{3=l) of different methods on NP and SV recognition Method Model Classifier SNoW HMM NB Simple SNoW NB PMM Simple SNoW CSCL NB Simple  POS tags only 90. [sent-175, score-0.112]
</p><p>86 28  The first important observation is that the SV identification task is significantly more difficult than that the NP task. [sent-205, score-0.163]
</p><p>87 What is interesting here is the very significant sensitivity to the feature base of the classifiers used, despite the violation of the probabilistic assumptions. [sent-208, score-0.401]
</p><p>88 For the easier NP task, the HMM model is competitive with the others when the classifiers used are NB or SNoW. [sent-209, score-0.314]
</p><p>89 In particular, the fact that the significant improvements both probabilistic methods achieve when their input is given by SNoW confirms the claim that the output of SNoW can be used reliably as a probabilistic classifier. [sent-210, score-0.134]
</p><p>90 PMM and CSCL perform very well on predicting NP and SV phrases with CSCL at least as good as any other methods tried on these tasks. [sent-211, score-0.465]
</p><p>91 We attribute it to CSCL's ability to cope better with the length of the phrase and the long term dependencies. [sent-213, score-0.446]
</p><p>92 7  Conclusion  We have addressed the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. [sent-214, score-0.526]
</p><p>93 The focus here is on an important subproblem, the identification of phrase structure. [sent-216, score-0.48]
</p><p>94 It seems that the CSP formalisms can support the desired performance measure as well as complex constraints and dependencies more flexibly than the Markovian approach. [sent-219, score-0.148]
</p><p>95 As a side effect, this work exhibits the use of general classifiers within a probabilistic framework. [sent-221, score-0.366]
</p><p>96 Future work includes extensions to deal with more general constraints by exploiting more general probabilistic structures and generalizing the CSP approach. [sent-222, score-0.131]
</p><p>97 A memory-based approach to learning shallow natural language patterns. [sent-248, score-0.253]
</p><p>98 Error-driven pruning of treebanks grammars for base noun phrase identification. [sent-258, score-0.545]
</p><p>99 A stochastic parts program and noun phrase parser for unrestricted text. [sent-270, score-0.51]
</p><p>100 Learning to resolve natural language ambiguities: A unified approach. [sent-366, score-0.109]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('phrases', 0.465), ('phrase', 0.417), ('classifiers', 0.314), ('cscl', 0.186), ('snow', 0.151), ('csp', 0.149), ('shallow', 0.144), ('sls', 0.13), ('slo', 0.128), ('hmm', 0.122), ('parsing', 0.112), ('pmm', 0.112), ('tags', 0.112), ('sv', 0.11), ('pt', 0.106), ('np', 0.102), ('observation', 0.1), ('noun', 0.093), ('ols', 0.093), ('outcomes', 0.087), ('nb', 0.08), ('extraction', 0.08), ('language', 0.079), ('constraints', 0.079), ('pos', 0.074), ('identifying', 0.073), ('classifier', 0.073), ('sequence', 0.07), ('satisfaction', 0.067), ('string', 0.067), ('hmms', 0.065), ('pas', 0.064), ('identification', 0.063), ('sis', 0.058), ('coherent', 0.058), ('state', 0.057), ('lexical', 0.056), ('nps', 0.056), ('svs', 0.056), ('markov', 0.054), ('symbol', 0.054), ('probabilistic', 0.052), ('formalism', 0.05), ('syntactic', 0.048), ('pl', 0.048), ('roth', 0.048), ('corpus', 0.048), ('observations', 0.047), ('recognize', 0.045), ('oc', 0.045), ('incorporated', 0.043), ('combine', 0.039), ('modeling', 0.039), ('attempt', 0.039), ('markovian', 0.038), ('cost', 0.038), ('experimentally', 0.037), ('combinator', 0.037), ('flexibly', 0.037), ('freitag', 0.037), ('otls', 0.037), ('punyakanok', 0.037), ('subproblem', 0.037), ('verb', 0.037), ('states', 0.037), ('boolean', 0.036), ('sequential', 0.035), ('inside', 0.035), ('base', 0.035), ('speech', 0.035), ('likely', 0.034), ('outside', 0.034), ('harder', 0.034), ('satisfies', 0.034), ('inference', 0.033), ('words', 0.033), ('study', 0.033), ('semantic', 0.033), ('linguistics', 0.032), ('penn', 0.032), ('formalisms', 0.032), ('approaches', 0.032), ('mechanism', 0.031), ('encoded', 0.031), ('input', 0.03), ('extends', 0.03), ('text', 0.03), ('natural', 0.03), ('open', 0.03), ('constraint', 0.03), ('local', 0.029), ('projection', 0.029), ('targets', 0.029), ('assignment', 0.029), ('path', 0.029), ('sentences', 0.029), ('attribute', 0.029), ('wider', 0.029), ('chunking', 0.029), ('prepared', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="138-tfidf-1" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<p>Author: Vasin Punyakanok, Dan Roth</p><p>Abstract: We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.</p><p>2 0.12273809 <a title="138-tfidf-2" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: We present a bound on the generalisation error of linear classifiers in terms of a refined margin quantity on the training set. The result is obtained in a PAC- Bayesian framework and is based on geometrical arguments in the space of linear classifiers. The new bound constitutes an exponential improvement of the so far tightest margin bound by Shawe-Taylor et al. [8] and scales logarithmically in the inverse margin. Even in the case of less training examples than input dimensions sufficiently large margins lead to non-trivial bound values and - for maximum margins - to a vanishing complexity term. Furthermore, the classical margin is too coarse a measure for the essential quantity that controls the generalisation error: the volume ratio between the whole hypothesis space and the subset of consistent hypotheses. The practical relevance of the result lies in the fact that the well-known support vector machine is optimal w.r.t. the new bound only if the feature vectors are all of the same length. As a consequence we recommend to use SVMs on normalised feature vectors only - a recommendation that is well supported by our numerical experiments on two benchmark data sets. 1</p><p>3 0.11082613 <a title="138-tfidf-3" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: The concept of averaging over classifiers is fundamental to the Bayesian analysis of learning. Based on this viewpoint, it has recently been demonstrated for linear classifiers that the centre of mass of version space (the set of all classifiers consistent with the training set) - also known as the Bayes point - exhibits excellent generalisation abilities. However, the billiard algorithm as presented in [4] is restricted to small sample size because it requires o (m 2 ) of memory and 0 (N . m2 ) computational steps where m is the number of training patterns and N is the number of random draws from the posterior distribution. In this paper we present a method based on the simple perceptron learning algorithm which allows to overcome this algorithmic drawback. The method is algorithmically simple and is easily extended to the multi-class case. We present experimental results on the MNIST data set of handwritten digits which show that Bayes point machines (BPMs) are competitive with the current world champion, the support vector machine. In addition, the computational complexity of BPMs can be tuned by varying the number of samples from the posterior. Finally, rejecting test points on the basis of their (approximative) posterior probability leads to a rapid decrease in generalisation error, e.g. 0.1% generalisation error for a given rejection rate of 10%. 1</p><p>4 0.105045 <a title="138-tfidf-4" href="./nips-2000-New_Approaches_Towards_Robust_and_Adaptive_Speech_Recognition.html">90 nips-2000-New Approaches Towards Robust and Adaptive Speech Recognition</a></p>
<p>Author: Hervé Bourlard, Samy Bengio, Katrin Weber</p><p>Abstract: In this paper, we discuss some new research directions in automatic speech recognition (ASR), and which somewhat deviate from the usual approaches. More specifically, we will motivate and briefly describe new approaches based on multi-stream and multi/band ASR. These approaches extend the standard hidden Markov model (HMM) based approach by assuming that the different (frequency) channels representing the speech signal are processed by different (independent)</p><p>5 0.10077059 <a title="138-tfidf-5" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>Author: Koby Crammer, Yoram Singer</p><p>Abstract: Output coding is a general method for solving multiclass problems by reducing them to multiple binary classification problems. Previous research on output coding has employed, almost solely, predefined discrete codes. We describe an algorithm that improves the performance of output codes by relaxing them to continuous codes. The relaxation procedure is cast as an optimization problem and is reminiscent of the quadratic program for support vector machines. We describe experiments with the proposed algorithm, comparing it to standard discrete output codes. The experimental results indicate that continuous relaxations of output codes often improve the generalization performance, especially for short codes.</p><p>6 0.098968603 <a title="138-tfidf-6" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>7 0.071528144 <a title="138-tfidf-7" href="./nips-2000-One_Microphone_Source_Separation.html">96 nips-2000-One Microphone Source Separation</a></p>
<p>8 0.066790283 <a title="138-tfidf-8" href="./nips-2000-From_Margin_to_Sparsity.html">58 nips-2000-From Margin to Sparsity</a></p>
<p>9 0.066120997 <a title="138-tfidf-9" href="./nips-2000-A_Neural_Probabilistic_Language_Model.html">6 nips-2000-A Neural Probabilistic Language Model</a></p>
<p>10 0.063577667 <a title="138-tfidf-10" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>11 0.060114507 <a title="138-tfidf-11" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>12 0.060084712 <a title="138-tfidf-12" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>13 0.056970652 <a title="138-tfidf-13" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>14 0.054354012 <a title="138-tfidf-14" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>15 0.053012166 <a title="138-tfidf-15" href="./nips-2000-Data_Clustering_by_Markovian_Relaxation_and_the_Information_Bottleneck_Method.html">38 nips-2000-Data Clustering by Markovian Relaxation and the Information Bottleneck Method</a></p>
<p>16 0.052397456 <a title="138-tfidf-16" href="./nips-2000-Factored_Semi-Tied_Covariance_Matrices.html">51 nips-2000-Factored Semi-Tied Covariance Matrices</a></p>
<p>17 0.051395945 <a title="138-tfidf-17" href="./nips-2000-Speech_Denoising_and_Dereverberation_Using_Probabilistic_Models.html">123 nips-2000-Speech Denoising and Dereverberation Using Probabilistic Models</a></p>
<p>18 0.051232863 <a title="138-tfidf-18" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>19 0.048744123 <a title="138-tfidf-19" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>20 0.047785871 <a title="138-tfidf-20" href="./nips-2000-Tree-Based_Modeling_and_Estimation_of_Gaussian_Processes_on_Graphs_with_Cycles.html">140 nips-2000-Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.195), (1, 0.041), (2, 0.03), (3, 0.017), (4, -0.09), (5, -0.092), (6, -0.038), (7, -0.004), (8, 0.006), (9, 0.126), (10, 0.139), (11, 0.017), (12, 0.064), (13, 0.01), (14, -0.129), (15, 0.069), (16, -0.0), (17, 0.014), (18, 0.123), (19, 0.041), (20, -0.016), (21, -0.03), (22, 0.003), (23, -0.193), (24, 0.006), (25, -0.123), (26, -0.065), (27, 0.041), (28, 0.083), (29, -0.02), (30, -0.019), (31, 0.216), (32, 0.072), (33, 0.046), (34, 0.04), (35, -0.022), (36, 0.017), (37, 0.038), (38, 0.074), (39, -0.201), (40, 0.006), (41, -0.075), (42, -0.008), (43, 0.103), (44, -0.012), (45, -0.047), (46, -0.059), (47, -0.031), (48, 0.076), (49, 0.106)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94152099 <a title="138-lsi-1" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<p>Author: Vasin Punyakanok, Dan Roth</p><p>Abstract: We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.</p><p>2 0.48119774 <a title="138-lsi-2" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>Author: Koby Crammer, Yoram Singer</p><p>Abstract: Output coding is a general method for solving multiclass problems by reducing them to multiple binary classification problems. Previous research on output coding has employed, almost solely, predefined discrete codes. We describe an algorithm that improves the performance of output codes by relaxing them to continuous codes. The relaxation procedure is cast as an optimization problem and is reminiscent of the quadratic program for support vector machines. We describe experiments with the proposed algorithm, comparing it to standard discrete output codes. The experimental results indicate that continuous relaxations of output codes often improve the generalization performance, especially for short codes.</p><p>3 0.47979352 <a title="138-lsi-3" href="./nips-2000-New_Approaches_Towards_Robust_and_Adaptive_Speech_Recognition.html">90 nips-2000-New Approaches Towards Robust and Adaptive Speech Recognition</a></p>
<p>Author: Hervé Bourlard, Samy Bengio, Katrin Weber</p><p>Abstract: In this paper, we discuss some new research directions in automatic speech recognition (ASR), and which somewhat deviate from the usual approaches. More specifically, we will motivate and briefly describe new approaches based on multi-stream and multi/band ASR. These approaches extend the standard hidden Markov model (HMM) based approach by assuming that the different (frequency) channels representing the speech signal are processed by different (independent)</p><p>4 0.43095657 <a title="138-lsi-4" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: We present a bound on the generalisation error of linear classifiers in terms of a refined margin quantity on the training set. The result is obtained in a PAC- Bayesian framework and is based on geometrical arguments in the space of linear classifiers. The new bound constitutes an exponential improvement of the so far tightest margin bound by Shawe-Taylor et al. [8] and scales logarithmically in the inverse margin. Even in the case of less training examples than input dimensions sufficiently large margins lead to non-trivial bound values and - for maximum margins - to a vanishing complexity term. Furthermore, the classical margin is too coarse a measure for the essential quantity that controls the generalisation error: the volume ratio between the whole hypothesis space and the subset of consistent hypotheses. The practical relevance of the result lies in the fact that the well-known support vector machine is optimal w.r.t. the new bound only if the feature vectors are all of the same length. As a consequence we recommend to use SVMs on normalised feature vectors only - a recommendation that is well supported by our numerical experiments on two benchmark data sets. 1</p><p>5 0.39936563 <a title="138-lsi-5" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: The concept of averaging over classifiers is fundamental to the Bayesian analysis of learning. Based on this viewpoint, it has recently been demonstrated for linear classifiers that the centre of mass of version space (the set of all classifiers consistent with the training set) - also known as the Bayes point - exhibits excellent generalisation abilities. However, the billiard algorithm as presented in [4] is restricted to small sample size because it requires o (m 2 ) of memory and 0 (N . m2 ) computational steps where m is the number of training patterns and N is the number of random draws from the posterior distribution. In this paper we present a method based on the simple perceptron learning algorithm which allows to overcome this algorithmic drawback. The method is algorithmically simple and is easily extended to the multi-class case. We present experimental results on the MNIST data set of handwritten digits which show that Bayes point machines (BPMs) are competitive with the current world champion, the support vector machine. In addition, the computational complexity of BPMs can be tuned by varying the number of samples from the posterior. Finally, rejecting test points on the basis of their (approximative) posterior probability leads to a rapid decrease in generalisation error, e.g. 0.1% generalisation error for a given rejection rate of 10%. 1</p><p>6 0.37537754 <a title="138-lsi-6" href="./nips-2000-Learning_Switching_Linear_Models_of_Human_Motion.html">80 nips-2000-Learning Switching Linear Models of Human Motion</a></p>
<p>7 0.36380649 <a title="138-lsi-7" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>8 0.34646675 <a title="138-lsi-8" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>9 0.32674545 <a title="138-lsi-9" href="./nips-2000-One_Microphone_Source_Separation.html">96 nips-2000-One Microphone Source Separation</a></p>
<p>10 0.30469993 <a title="138-lsi-10" href="./nips-2000-The_Missing_Link_-_A_Probabilistic_Model_of_Document_Content_and_Hypertext_Connectivity.html">136 nips-2000-The Missing Link - A Probabilistic Model of Document Content and Hypertext Connectivity</a></p>
<p>11 0.30254486 <a title="138-lsi-11" href="./nips-2000-A_Neural_Probabilistic_Language_Model.html">6 nips-2000-A Neural Probabilistic Language Model</a></p>
<p>12 0.29949608 <a title="138-lsi-12" href="./nips-2000-From_Margin_to_Sparsity.html">58 nips-2000-From Margin to Sparsity</a></p>
<p>13 0.29203236 <a title="138-lsi-13" href="./nips-2000-The_Interplay_of_Symbolic_and_Subsymbolic_Processes_in_Anagram_Problem_Solving.html">132 nips-2000-The Interplay of Symbolic and Subsymbolic Processes in Anagram Problem Solving</a></p>
<p>14 0.28206182 <a title="138-lsi-14" href="./nips-2000-Efficient_Learning_of_Linear_Perceptrons.html">44 nips-2000-Efficient Learning of Linear Perceptrons</a></p>
<p>15 0.27525508 <a title="138-lsi-15" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>16 0.27084681 <a title="138-lsi-16" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>17 0.26721516 <a title="138-lsi-17" href="./nips-2000-Factored_Semi-Tied_Covariance_Matrices.html">51 nips-2000-Factored Semi-Tied Covariance Matrices</a></p>
<p>18 0.26241624 <a title="138-lsi-18" href="./nips-2000-Foundations_for_a_Circuit_Complexity_Theory_of_Sensory_Processing.html">56 nips-2000-Foundations for a Circuit Complexity Theory of Sensory Processing</a></p>
<p>19 0.25374252 <a title="138-lsi-19" href="./nips-2000-Probabilistic_Semantic_Video_Indexing.html">103 nips-2000-Probabilistic Semantic Video Indexing</a></p>
<p>20 0.25299942 <a title="138-lsi-20" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.023), (10, 0.032), (17, 0.081), (26, 0.019), (32, 0.018), (33, 0.057), (55, 0.038), (62, 0.062), (65, 0.035), (67, 0.042), (74, 0.329), (75, 0.011), (76, 0.03), (79, 0.02), (81, 0.038), (90, 0.035), (97, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79329956 <a title="138-lda-1" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<p>Author: Vasin Punyakanok, Dan Roth</p><p>Abstract: We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.</p><p>2 0.38055146 <a title="138-lda-2" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>Author: Zoubin Ghahramani, Matthew J. Beal</p><p>Abstract: Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set. 1</p><p>3 0.38053763 <a title="138-lda-3" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>Author: Javier R. Movellan, Paul Mineiro, Ruth J. Williams</p><p>Abstract: This paper explores a framework for recognition of image sequences using partially observable stochastic differential equation (SDE) models. Monte-Carlo importance sampling techniques are used for efficient estimation of sequence likelihoods and sequence likelihood gradients. Once the network dynamics are learned, we apply the SDE models to sequence recognition tasks in a manner similar to the way Hidden Markov models (HMMs) are commonly applied. The potential advantage of SDEs over HMMS is the use of continuous state dynamics. We present encouraging results for a video sequence recognition task in which SDE models provided excellent performance when compared to hidden Markov models. 1</p><p>4 0.3786054 <a title="138-lda-4" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>Author: Claudio Gentile</p><p>Abstract: A new incremental learning algorithm is described which approximates the maximal margin hyperplane w.r.t. norm p ~ 2 for a set of linearly separable data. Our algorithm, called ALMAp (Approximate Large Margin algorithm w.r.t. norm p), takes 0 ((P~21;;2) corrections to separate the data with p-norm margin larger than (1 - 0:) ,,(, where,,( is the p-norm margin of the data and X is a bound on the p-norm of the instances. ALMAp avoids quadratic (or higher-order) programming methods. It is very easy to implement and is as fast as on-line algorithms, such as Rosenblatt's perceptron. We report on some experiments comparing ALMAp to two incremental algorithms: Perceptron and Li and Long's ROMMA. Our algorithm seems to perform quite better than both. The accuracy levels achieved by ALMAp are slightly inferior to those obtained by Support vector Machines (SVMs). On the other hand, ALMAp is quite faster and easier to implement than standard SVMs training algorithms.</p><p>5 0.37570903 <a title="138-lda-5" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass, Eduardo D. Sontag, Anthony M. Zador</p><p>Abstract: Experimental data show that biological synapses behave quite differently from the symbolic synapses in common artificial neural network models. Biological synapses are dynamic, i.e., their</p><p>6 0.36978111 <a title="138-lda-6" href="./nips-2000-Learning_Switching_Linear_Models_of_Human_Motion.html">80 nips-2000-Learning Switching Linear Models of Human Motion</a></p>
<p>7 0.36824194 <a title="138-lda-7" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>8 0.36506873 <a title="138-lda-8" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>9 0.36451128 <a title="138-lda-9" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>10 0.36298293 <a title="138-lda-10" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>11 0.36127299 <a title="138-lda-11" href="./nips-2000-A_Neural_Probabilistic_Language_Model.html">6 nips-2000-A Neural Probabilistic Language Model</a></p>
<p>12 0.35968566 <a title="138-lda-12" href="./nips-2000-Speech_Denoising_and_Dereverberation_Using_Probabilistic_Models.html">123 nips-2000-Speech Denoising and Dereverberation Using Probabilistic Models</a></p>
<p>13 0.35853216 <a title="138-lda-13" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>14 0.35703534 <a title="138-lda-14" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>15 0.3561908 <a title="138-lda-15" href="./nips-2000-Regularized_Winnow_Methods.html">111 nips-2000-Regularized Winnow Methods</a></p>
<p>16 0.35616305 <a title="138-lda-16" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>17 0.35585222 <a title="138-lda-17" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>18 0.35583761 <a title="138-lda-18" href="./nips-2000-New_Approaches_Towards_Robust_and_Adaptive_Speech_Recognition.html">90 nips-2000-New Approaches Towards Robust and Adaptive Speech Recognition</a></p>
<p>19 0.3550179 <a title="138-lda-19" href="./nips-2000-On_Reversing_Jensen%27s_Inequality.html">94 nips-2000-On Reversing Jensen's Inequality</a></p>
<p>20 0.3549118 <a title="138-lda-20" href="./nips-2000-Structure_Learning_in_Human_Causal_Induction.html">127 nips-2000-Structure Learning in Human Causal Induction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
