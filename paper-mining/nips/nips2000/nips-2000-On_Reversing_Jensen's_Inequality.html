<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>94 nips-2000-On Reversing Jensen's Inequality</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-94" href="#">nips2000-94</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>94 nips-2000-On Reversing Jensen's Inequality</h1>
<br/><p>Source: <a title="nips-2000-94-pdf" href="http://papers.nips.cc/paper/1879-on-reversing-jensens-inequality.pdf">pdf</a></p><p>Author: Tony Jebara, Alex Pentland</p><p>Abstract: Jensen's inequality is a powerful mathematical tool and one of the workhorses in statistical learning. Its applications therein include the EM algorithm, Bayesian estimation and Bayesian inference. Jensen computes simple lower bounds on otherwise intractable quantities such as products of sums and latent log-likelihoods. This simplification then permits operations like integration and maximization. Quite often (i.e. in discriminative learning) upper bounds are needed as well. We derive and prove an efficient analytic inequality that provides such variational upper bounds. This inequality holds for latent variable mixtures of exponential family distributions and thus spans a wide range of contemporary statistical models. We also discuss applications of the upper bounds including maximum conditional likelihood, large margin discriminative models and conditional Bayesian inference. Convergence, efficiency and prediction results are shown. 1</p><p>Reference: <a title="nips-2000-94-reference" href="../nips2000_reference/nips-2000-On_Reversing_Jensen%27s_Inequality_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Jensen's inequality is a powerful mathematical tool and one of the workhorses in statistical learning. [sent-5, score-0.259]
</p><p>2 Its applications therein include the EM algorithm, Bayesian estimation and Bayesian inference. [sent-6, score-0.24]
</p><p>3 Jensen computes simple lower bounds on otherwise intractable quantities such as products of sums and latent log-likelihoods. [sent-7, score-0.769]
</p><p>4 This simplification then permits operations like integration and maximization. [sent-8, score-0.194]
</p><p>5 in discriminative learning) upper bounds are needed as well. [sent-11, score-0.907]
</p><p>6 We derive and prove an efficient analytic inequality that provides such variational upper bounds. [sent-12, score-0.426]
</p><p>7 This inequality holds for latent variable mixtures of exponential family distributions and thus spans a wide range of contemporary statistical models. [sent-13, score-0.811]
</p><p>8 We also discuss applications of the upper bounds including maximum conditional likelihood, large margin discriminative models and conditional Bayesian inference. [sent-14, score-1.195]
</p><p>9 1  1 Introduction Statistical model estimation and inference often require the maximization, evaluation, and integration of complicated mathematical expressions. [sent-16, score-0.26]
</p><p>10 One approach for simplifying the computations is to find and manipulate variational upper and lower bounds instead of the expressions themselves. [sent-17, score-0.883]
</p><p>11 A prominent tool for computing such bounds is Jensen's inequality which subsumes many information-theoretic bounds (cf. [sent-18, score-1.197]
</p><p>12 In maximum likelihood (ML) estimation under incomplete data, Jensen is used to derive an iterative EM algorithm [2]. [sent-20, score-0.332]
</p><p>13 For graphical models, intractable inference and estimation is performed via variational bounds [7]. [sent-21, score-0.766]
</p><p>14 Bayesian integration also uses Jensen and EM-like bounds to compute integrals that are otherwise intractable [9]. [sent-22, score-0.694]
</p><p>15 Recently, however, the learning community has seen the proliferation of conditional or discriminative criteria. [sent-23, score-0.5]
</p><p>16 These include support vector machines, maximum entropy discrimination distributions [4], and discriminative HMMs [3]. [sent-24, score-0.526]
</p><p>17 These criteria allocate resources with the given task (classification or regression) in mind, yielding improved performance. [sent-25, score-0.268]
</p><p>18 In contrast, under canonical ML each density is trained separately to describe observations rather than optimize classification or regression. [sent-26, score-0.108]
</p><p>19 Please download the long version with tighter bounds, detailed proofs, more results, important extensions and sample matlab code from: http://www. [sent-29, score-0.121]
</p><p>20 edu/ "-'jebara/bounds  Computationally, what differentiates these criteria from ML is that they not only require Jensen-type lower bounds but may also utilize the corresponding upper bounds. [sent-32, score-0.856]
</p><p>21 The Jensen bounds only partially simplify their expressions and some intractabilities remain. [sent-33, score-0.519]
</p><p>22 For instance, latent distributions need to be bounded above and below in a discriminative setting [4] [3]. [sent-34, score-0.513]
</p><p>23 Metaphorically, discriminative learning requires lower bounds to cluster positive examples and upper bounds to repel away from negative ones. [sent-35, score-1.502]
</p><p>24 We derive these complementary upper bounds 2 which are useful for discriminative classification and regression. [sent-36, score-1.038]
</p><p>25 These bounds are structurally similar to Jensen bounds, allowing easy migration of ML techniques to discriminative settings. [sent-37, score-0.831]
</p><p>26 This paper is organized as follows: We introduce the probabilistic models we will use: mixtures of the exponential family. [sent-38, score-0.279]
</p><p>27 We then describe some estimation criteria on these models which are intractable. [sent-39, score-0.317]
</p><p>28 One simplification is to lower bound via Jensen's inequality or EM. [sent-40, score-0.37]
</p><p>29 We show implementation and results of the bounds in applications (i. [sent-42, score-0.484]
</p><p>30 Finally, a strict algebraic proof is given to validate the reverse-bound. [sent-45, score-0.144]
</p><p>31 2  The Exponential Family  We restrict the reverse-Jensen bounds to mixtures of the exponential family (e-family). [sent-46, score-0.76]
</p><p>32 In practice this class of densities covers a very large portion of contemporary statistical models. [sent-47, score-0.192]
</p><p>33 Mixtures of the e-family include Gaussians Mixture Models, Multinomials, Poisson, Hidden Markov Models, Sigmoidal Belief Networks, Discrete Bayesian Networks, etc. [sent-48, score-0.056]
</p><p>34 Typically the data vector X is constrained to live in the gradient space of K, i. [sent-51, score-0.054]
</p><p>35 The table above lists example A and K functions for Gaussian and multinomial distributions. [sent-59, score-0.13]
</p><p>36 More generally, though, we will deal with mixtures of the e-family (where m represents the incomplete data? [sent-60, score-0.241]
</p><p>37 :  m  m  These latent probability distributions need to get maximized, integrated, marginalized, conditioned, etc. [sent-63, score-0.181]
</p><p>38 to solve various inference, prediction, and parameter estimation tasks. [sent-64, score-0.102]
</p><p>39 3 Conditional and Discriminative Criteria The combination of ML with EM and Jensen have indeed produced straightforward and monotonically convergent estimation procedures for mixtures of the e-family [2] [1] [7]. [sent-66, score-0.338]
</p><p>40 However, ML criteria are non-discriminative modeling techniques for estimating generative models. [sent-67, score-0.177]
</p><p>41 2 A weaker bound for Gaussian mixture regression appears in [6]. [sent-69, score-0.191]
</p><p>42 3Note we use El to denote an aggregate model encompassing all individual Elm \1m. [sent-71, score-0.041]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('jensen', 0.457), ('bounds', 0.445), ('discriminative', 0.332), ('ml', 0.223), ('criteria', 0.177), ('inequality', 0.16), ('mixtures', 0.16), ('upper', 0.13), ('latent', 0.127), ('contemporary', 0.109), ('estimation', 0.102), ('media', 0.091), ('intractable', 0.09), ('conditional', 0.081), ('multinomial', 0.081), ('simplification', 0.081), ('incomplete', 0.081), ('exponential', 0.081), ('variational', 0.08), ('family', 0.074), ('expressions', 0.074), ('bayesian', 0.073), ('integration', 0.072), ('bound', 0.066), ('em', 0.065), ('lab', 0.064), ('lower', 0.063), ('tool', 0.062), ('derive', 0.056), ('include', 0.056), ('allocate', 0.054), ('proliferation', 0.054), ('multinomials', 0.054), ('strict', 0.054), ('repel', 0.054), ('manipulate', 0.054), ('live', 0.054), ('manipulations', 0.054), ('structurally', 0.054), ('tony', 0.054), ('distributions', 0.054), ('regression', 0.049), ('reversing', 0.049), ('pentland', 0.049), ('marginalized', 0.049), ('lists', 0.049), ('validate', 0.049), ('covers', 0.049), ('maximum', 0.049), ('inference', 0.049), ('spans', 0.046), ('please', 0.046), ('jebara', 0.046), ('matlab', 0.046), ('ek', 0.046), ('subsumes', 0.046), ('otherwise', 0.044), ('likelihood', 0.044), ('integrals', 0.043), ('therein', 0.043), ('linearity', 0.043), ('aggregate', 0.041), ('algebraic', 0.041), ('permits', 0.041), ('canonical', 0.041), ('tighter', 0.041), ('el', 0.041), ('utilize', 0.041), ('convexity', 0.041), ('complementary', 0.041), ('applications', 0.039), ('mind', 0.039), ('monotonically', 0.039), ('prominent', 0.039), ('weaker', 0.039), ('reverse', 0.039), ('models', 0.038), ('mathematical', 0.037), ('mixture', 0.037), ('exploits', 0.037), ('resources', 0.037), ('simplifying', 0.037), ('convergent', 0.037), ('hmms', 0.037), ('maximized', 0.035), ('alex', 0.035), ('sigmoidal', 0.035), ('proofs', 0.035), ('intrinsic', 0.035), ('discrimination', 0.035), ('classification', 0.034), ('portion', 0.034), ('extensions', 0.034), ('cover', 0.034), ('poisson', 0.034), ('prediction', 0.034), ('separately', 0.033), ('cluster', 0.033), ('suffer', 0.033), ('community', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="94-tfidf-1" href="./nips-2000-On_Reversing_Jensen%27s_Inequality.html">94 nips-2000-On Reversing Jensen's Inequality</a></p>
<p>Author: Tony Jebara, Alex Pentland</p><p>Abstract: Jensen's inequality is a powerful mathematical tool and one of the workhorses in statistical learning. Its applications therein include the EM algorithm, Bayesian estimation and Bayesian inference. Jensen computes simple lower bounds on otherwise intractable quantities such as products of sums and latent log-likelihoods. This simplification then permits operations like integration and maximization. Quite often (i.e. in discriminative learning) upper bounds are needed as well. We derive and prove an efficient analytic inequality that provides such variational upper bounds. This inequality holds for latent variable mixtures of exponential family distributions and thus spans a wide range of contemporary statistical models. We also discuss applications of the upper bounds including maximum conditional likelihood, large margin discriminative models and conditional Bayesian inference. Convergence, efficiency and prediction results are shown. 1</p><p>2 0.16755049 <a title="94-tfidf-2" href="./nips-2000-Algorithmic_Stability_and_Generalization_Performance.html">21 nips-2000-Algorithmic Stability and Generalization Performance</a></p>
<p>Author: Olivier Bousquet, Andr茅 Elisseeff</p><p>Abstract: We present a novel way of obtaining PAC-style bounds on the generalization error of learning algorithms, explicitly using their stability properties. A stable learner is one for which the learned solution does not change much with small changes in the training set. The bounds we obtain do not depend on any measure of the complexity of the hypothesis space (e.g. VC dimension) but rather depend on how the learning algorithm searches this space, and can thus be applied even when the VC dimension is infinite. We demonstrate that regularization networks possess the required stability property and apply our method to obtain new bounds on their generalization performance. 1</p><p>3 0.16087064 <a title="94-tfidf-3" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>Author: Zoubin Ghahramani, Matthew J. Beal</p><p>Abstract: Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set. 1</p><p>4 0.13124689 <a title="94-tfidf-4" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>Author: Martin Szummer, Tommi Jaakkola</p><p>Abstract: Modern classification applications necessitate supplementing the few available labeled examples with unlabeled examples to improve classification performance. We present a new tractable algorithm for exploiting unlabeled examples in discriminative classification. This is achieved essentially by expanding the input vectors into longer feature vectors via both labeled and unlabeled examples. The resulting classification method can be interpreted as a discriminative kernel density estimate and is readily trained via the EM algorithm, which in this case is both discriminative and achieves the optimal solution. We provide, in addition, a purely discriminative formulation of the estimation problem by appealing to the maximum entropy framework. We demonstrate that the proposed approach requires very few labeled examples for high classification accuracy.</p><p>5 0.12096026 <a title="94-tfidf-5" href="./nips-2000-Some_New_Bounds_on_the_Generalization_Error_of_Combined_Classifiers.html">119 nips-2000-Some New Bounds on the Generalization Error of Combined Classifiers</a></p>
<p>Author: Vladimir Koltchinskii, Dmitriy Panchenko, Fernando Lozano</p><p>Abstract: In this paper we develop the method of bounding the generalization error of a classifier in terms of its margin distribution which was introduced in the recent papers of Bartlett and Schapire, Freund, Bartlett and Lee. The theory of Gaussian and empirical processes allow us to prove the margin type inequalities for the most general functional classes, the complexity of the class being measured via the so called Gaussian complexity functions. As a simple application of our results, we obtain the bounds of Schapire, Freund, Bartlett and Lee for the generalization error of boosting. We also substantially improve the results of Bartlett on bounding the generalization error of neural networks in terms of h -norms of the weights of neurons. Furthermore, under additional assumptions on the complexity of the class of hypotheses we provide some tighter bounds, which in the case of boosting improve the results of Schapire, Freund, Bartlett and Lee. 1 Introduction and margin type inequalities for general functional classes Let (X, Y) be a random couple, where X is an instance in a space Sand Y E {-I, I} is a label. Let 9 be a set of functions from S into JR. For 9 E g, sign(g(X)) will be used as a predictor (a classifier) of the unknown label Y. If the distribution of (X, Y) is unknown, then the choice of the predictor is based on the training data (Xl, Y l ), ... , (Xn, Y n ) that consists ofn i.i.d. copies of (X, Y). The goal ofleaming is to find a predictor 9 E 9 (based on the training data) whose generalization (classification) error JP'{Yg(X) :::; O} is small enough. We will first introduce some probabilistic bounds for general functional classes and then give several examples of their applications to bounding the generalization error of boosting and neural networks. We omit all the proofs and refer an interested reader to [5]. Let (8, A, P) be a probability space and let F be a class of measurable functions from (8, A) into lR. Let {Xd be a sequence of i.i.d. random variables taking values in (8, A) with common distribution P. Let Pn be the empirical measure based on the sample (Xl,'</p><p>6 0.11363588 <a title="94-tfidf-6" href="./nips-2000-Beyond_Maximum_Likelihood_and_Density_Estimation%3A_A_Sample-Based_Criterion_for_Unsupervised_Learning_of_Complex_Models.html">31 nips-2000-Beyond Maximum Likelihood and Density Estimation: A Sample-Based Criterion for Unsupervised Learning of Complex Models</a></p>
<p>7 0.09670192 <a title="94-tfidf-7" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>8 0.089939088 <a title="94-tfidf-8" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>9 0.089807101 <a title="94-tfidf-9" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>10 0.086309493 <a title="94-tfidf-10" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>11 0.085664697 <a title="94-tfidf-11" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>12 0.081955902 <a title="94-tfidf-12" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>13 0.077243946 <a title="94-tfidf-13" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>14 0.076734945 <a title="94-tfidf-14" href="./nips-2000-From_Margin_to_Sparsity.html">58 nips-2000-From Margin to Sparsity</a></p>
<p>15 0.070155039 <a title="94-tfidf-15" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>16 0.06429369 <a title="94-tfidf-16" href="./nips-2000-Tree-Based_Modeling_and_Estimation_of_Gaussian_Processes_on_Graphs_with_Cycles.html">140 nips-2000-Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles</a></p>
<p>17 0.060270302 <a title="94-tfidf-17" href="./nips-2000-A_Variational_Mean-Field_Theory_for_Sigmoidal_Belief_Networks.html">14 nips-2000-A Variational Mean-Field Theory for Sigmoidal Belief Networks</a></p>
<p>18 0.060225669 <a title="94-tfidf-18" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>19 0.059947595 <a title="94-tfidf-19" href="./nips-2000-Recognizing_Hand-written_Digits_Using_Hierarchical_Products_of_Experts.html">108 nips-2000-Recognizing Hand-written Digits Using Hierarchical Products of Experts</a></p>
<p>20 0.057092819 <a title="94-tfidf-20" href="./nips-2000-From_Mixtures_of_Mixtures_to_Adaptive_Transform_Coding.html">59 nips-2000-From Mixtures of Mixtures to Adaptive Transform Coding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.198), (1, 0.107), (2, 0.058), (3, -0.024), (4, 0.148), (5, -0.164), (6, 0.004), (7, -0.006), (8, 0.021), (9, -0.053), (10, -0.156), (11, 0.009), (12, 0.096), (13, -0.017), (14, 0.007), (15, -0.01), (16, 0.172), (17, 0.031), (18, -0.166), (19, 0.146), (20, -0.025), (21, 0.16), (22, 0.211), (23, 0.047), (24, -0.013), (25, -0.091), (26, 0.143), (27, -0.125), (28, 0.062), (29, 0.107), (30, -0.036), (31, 0.1), (32, 0.227), (33, -0.048), (34, 0.038), (35, -0.013), (36, -0.036), (37, 0.011), (38, 0.145), (39, 0.047), (40, -0.039), (41, -0.148), (42, 0.161), (43, -0.022), (44, 0.032), (45, 0.086), (46, 0.128), (47, -0.114), (48, -0.234), (49, -0.151)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9864397 <a title="94-lsi-1" href="./nips-2000-On_Reversing_Jensen%27s_Inequality.html">94 nips-2000-On Reversing Jensen's Inequality</a></p>
<p>Author: Tony Jebara, Alex Pentland</p><p>Abstract: Jensen's inequality is a powerful mathematical tool and one of the workhorses in statistical learning. Its applications therein include the EM algorithm, Bayesian estimation and Bayesian inference. Jensen computes simple lower bounds on otherwise intractable quantities such as products of sums and latent log-likelihoods. This simplification then permits operations like integration and maximization. Quite often (i.e. in discriminative learning) upper bounds are needed as well. We derive and prove an efficient analytic inequality that provides such variational upper bounds. This inequality holds for latent variable mixtures of exponential family distributions and thus spans a wide range of contemporary statistical models. We also discuss applications of the upper bounds including maximum conditional likelihood, large margin discriminative models and conditional Bayesian inference. Convergence, efficiency and prediction results are shown. 1</p><p>2 0.5310697 <a title="94-lsi-2" href="./nips-2000-Algorithmic_Stability_and_Generalization_Performance.html">21 nips-2000-Algorithmic Stability and Generalization Performance</a></p>
<p>Author: Olivier Bousquet, Andr茅 Elisseeff</p><p>Abstract: We present a novel way of obtaining PAC-style bounds on the generalization error of learning algorithms, explicitly using their stability properties. A stable learner is one for which the learned solution does not change much with small changes in the training set. The bounds we obtain do not depend on any measure of the complexity of the hypothesis space (e.g. VC dimension) but rather depend on how the learning algorithm searches this space, and can thus be applied even when the VC dimension is infinite. We demonstrate that regularization networks possess the required stability property and apply our method to obtain new bounds on their generalization performance. 1</p><p>3 0.46178037 <a title="94-lsi-3" href="./nips-2000-Some_New_Bounds_on_the_Generalization_Error_of_Combined_Classifiers.html">119 nips-2000-Some New Bounds on the Generalization Error of Combined Classifiers</a></p>
<p>Author: Vladimir Koltchinskii, Dmitriy Panchenko, Fernando Lozano</p><p>Abstract: In this paper we develop the method of bounding the generalization error of a classifier in terms of its margin distribution which was introduced in the recent papers of Bartlett and Schapire, Freund, Bartlett and Lee. The theory of Gaussian and empirical processes allow us to prove the margin type inequalities for the most general functional classes, the complexity of the class being measured via the so called Gaussian complexity functions. As a simple application of our results, we obtain the bounds of Schapire, Freund, Bartlett and Lee for the generalization error of boosting. We also substantially improve the results of Bartlett on bounding the generalization error of neural networks in terms of h -norms of the weights of neurons. Furthermore, under additional assumptions on the complexity of the class of hypotheses we provide some tighter bounds, which in the case of boosting improve the results of Schapire, Freund, Bartlett and Lee. 1 Introduction and margin type inequalities for general functional classes Let (X, Y) be a random couple, where X is an instance in a space Sand Y E {-I, I} is a label. Let 9 be a set of functions from S into JR. For 9 E g, sign(g(X)) will be used as a predictor (a classifier) of the unknown label Y. If the distribution of (X, Y) is unknown, then the choice of the predictor is based on the training data (Xl, Y l ), ... , (Xn, Y n ) that consists ofn i.i.d. copies of (X, Y). The goal ofleaming is to find a predictor 9 E 9 (based on the training data) whose generalization (classification) error JP'{Yg(X) :::; O} is small enough. We will first introduce some probabilistic bounds for general functional classes and then give several examples of their applications to bounding the generalization error of boosting and neural networks. We omit all the proofs and refer an interested reader to [5]. Let (8, A, P) be a probability space and let F be a class of measurable functions from (8, A) into lR. Let {Xd be a sequence of i.i.d. random variables taking values in (8, A) with common distribution P. Let Pn be the empirical measure based on the sample (Xl,'</p><p>4 0.40326709 <a title="94-lsi-4" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>Author: Zoubin Ghahramani, Matthew J. Beal</p><p>Abstract: Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set. 1</p><p>5 0.39324623 <a title="94-lsi-5" href="./nips-2000-Beyond_Maximum_Likelihood_and_Density_Estimation%3A_A_Sample-Based_Criterion_for_Unsupervised_Learning_of_Complex_Models.html">31 nips-2000-Beyond Maximum Likelihood and Density Estimation: A Sample-Based Criterion for Unsupervised Learning of Complex Models</a></p>
<p>Author: Sepp Hochreiter, Michael Mozer</p><p>Abstract: The goal of many unsupervised learning procedures is to bring two probability distributions into alignment. Generative models such as Gaussian mixtures and Boltzmann machines can be cast in this light, as can recoding models such as ICA and projection pursuit. We propose a novel sample-based error measure for these classes of models, which applies even in situations where maximum likelihood (ML) and probability density estimation-based formulations cannot be applied, e.g., models that are nonlinear or have intractable posteriors. Furthermore, our sample-based error measure avoids the difficulties of approximating a density function. We prove that with an unconstrained model, (1) our approach converges on the correct solution as the number of samples goes to infinity, and (2) the expected solution of our approach in the generative framework is the ML solution. Finally, we evaluate our approach via simulations of linear and nonlinear models on mixture of Gaussians and ICA problems. The experiments show the broad applicability and generality of our approach. 1</p><p>6 0.3662746 <a title="94-lsi-6" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>7 0.32560879 <a title="94-lsi-7" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>8 0.31471863 <a title="94-lsi-8" href="./nips-2000-Sequentially_Fitting_%60%60Inclusive%27%27_Trees_for_Inference_in_Noisy-OR_Networks.html">115 nips-2000-Sequentially Fitting ``Inclusive'' Trees for Inference in Noisy-OR Networks</a></p>
<p>9 0.3114962 <a title="94-lsi-9" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>10 0.30103663 <a title="94-lsi-10" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>11 0.28811783 <a title="94-lsi-11" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>12 0.23560999 <a title="94-lsi-12" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>13 0.2323124 <a title="94-lsi-13" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>14 0.22312212 <a title="94-lsi-14" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>15 0.21967924 <a title="94-lsi-15" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>16 0.20345503 <a title="94-lsi-16" href="./nips-2000-Tree-Based_Modeling_and_Estimation_of_Gaussian_Processes_on_Graphs_with_Cycles.html">140 nips-2000-Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles</a></p>
<p>17 0.20330516 <a title="94-lsi-17" href="./nips-2000-Bayesian_Video_Shot_Segmentation.html">30 nips-2000-Bayesian Video Shot Segmentation</a></p>
<p>18 0.19931403 <a title="94-lsi-18" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<p>19 0.19377448 <a title="94-lsi-19" href="./nips-2000-From_Margin_to_Sparsity.html">58 nips-2000-From Margin to Sparsity</a></p>
<p>20 0.19057018 <a title="94-lsi-20" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.028), (17, 0.128), (22, 0.24), (26, 0.017), (32, 0.026), (33, 0.071), (54, 0.013), (55, 0.012), (62, 0.063), (65, 0.025), (67, 0.076), (76, 0.043), (79, 0.046), (90, 0.019), (91, 0.019), (97, 0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86133796 <a title="94-lda-1" href="./nips-2000-On_Reversing_Jensen%27s_Inequality.html">94 nips-2000-On Reversing Jensen's Inequality</a></p>
<p>Author: Tony Jebara, Alex Pentland</p><p>Abstract: Jensen's inequality is a powerful mathematical tool and one of the workhorses in statistical learning. Its applications therein include the EM algorithm, Bayesian estimation and Bayesian inference. Jensen computes simple lower bounds on otherwise intractable quantities such as products of sums and latent log-likelihoods. This simplification then permits operations like integration and maximization. Quite often (i.e. in discriminative learning) upper bounds are needed as well. We derive and prove an efficient analytic inequality that provides such variational upper bounds. This inequality holds for latent variable mixtures of exponential family distributions and thus spans a wide range of contemporary statistical models. We also discuss applications of the upper bounds including maximum conditional likelihood, large margin discriminative models and conditional Bayesian inference. Convergence, efficiency and prediction results are shown. 1</p><p>2 0.58740985 <a title="94-lda-2" href="./nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">40 nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>Author: Kevin A. Archie, Bartlett W. Mel</p><p>Abstract: Neurons in area V4 have relatively large receptive fields (RFs), so multiple visual features are simultaneously</p><p>3 0.57960594 <a title="94-lda-3" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>Author: Zoubin Ghahramani, Matthew J. Beal</p><p>Abstract: Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set. 1</p><p>4 0.57497954 <a title="94-lda-4" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>Author: Alex J. Smola, Peter L. Bartlett</p><p>Abstract: We present a simple sparse greedy technique to approximate the maximum a posteriori estimate of Gaussian Processes with much improved scaling behaviour in the sample size m. In particular, computational requirements are O(n 2 m), storage is O(nm), the cost for prediction is 0 (n) and the cost to compute confidence bounds is O(nm), where n «: m. We show how to compute a stopping criterion, give bounds on the approximation error, and show applications to large scale problems. 1</p><p>5 0.56506127 <a title="94-lda-5" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>Author: Tong Zhang</p><p>Abstract: Large margin linear classification methods have been successfully applied to many applications. For a linearly separable problem, it is known that under appropriate assumptions, the expected misclassification error of the computed</p><p>6 0.56325042 <a title="94-lda-6" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<p>7 0.56190521 <a title="94-lda-7" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>8 0.55701572 <a title="94-lda-8" href="./nips-2000-Speech_Denoising_and_Dereverberation_Using_Probabilistic_Models.html">123 nips-2000-Speech Denoising and Dereverberation Using Probabilistic Models</a></p>
<p>9 0.55693477 <a title="94-lda-9" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>10 0.55617267 <a title="94-lda-10" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>11 0.55186772 <a title="94-lda-11" href="./nips-2000-The_Use_of_MDL_to_Select_among_Computational_Models_of_Cognition.html">139 nips-2000-The Use of MDL to Select among Computational Models of Cognition</a></p>
<p>12 0.55012435 <a title="94-lda-12" href="./nips-2000-Algorithms_for_Non-negative_Matrix_Factorization.html">22 nips-2000-Algorithms for Non-negative Matrix Factorization</a></p>
<p>13 0.54830033 <a title="94-lda-13" href="./nips-2000-Algorithmic_Stability_and_Generalization_Performance.html">21 nips-2000-Algorithmic Stability and Generalization Performance</a></p>
<p>14 0.54752332 <a title="94-lda-14" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>15 0.54709035 <a title="94-lda-15" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>16 0.54672903 <a title="94-lda-16" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>17 0.54392827 <a title="94-lda-17" href="./nips-2000-Structure_Learning_in_Human_Causal_Induction.html">127 nips-2000-Structure Learning in Human Causal Induction</a></p>
<p>18 0.54314166 <a title="94-lda-18" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>19 0.54158092 <a title="94-lda-19" href="./nips-2000-Regularized_Winnow_Methods.html">111 nips-2000-Regularized Winnow Methods</a></p>
<p>20 0.54102767 <a title="94-lda-20" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
