<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-77" href="#">nips2000-77</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</h1>
<br/><p>Source: <a title="nips-2000-77-pdf" href="http://papers.nips.cc/paper/1825-learning-curves-for-gaussian-processes-regression-a-framework-for-good-approximations.pdf">pdf</a></p><p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: Based on a statistical mechanics approach, we develop a method for approximately computing average case learning curves for Gaussian process regression models. The approximation works well in the large sample size limit and for arbitrary dimensionality of the input space. We explain how the approximation can be systematically improved and argue that similar techniques can be applied to general likelihood models. 1</p><p>Reference: <a title="nips-2000-77-reference" href="../nips2000_reference/nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Learning curves for Gaussian processes regression: A framework for good approximations  Dorthe Malzahn Manfred Opper Neural Computing Research Group School of Engineering and Applied Science Aston University, Birmingham B4 7ET, United Kingdom. [sent-1, score-0.294]
</p><p>2 uk  Abstract Based on a statistical mechanics approach, we develop a method for approximately computing average case learning curves for Gaussian process regression models. [sent-5, score-0.457]
</p><p>3 The approximation works well in the large sample size limit and for arbitrary dimensionality of the input space. [sent-6, score-0.232]
</p><p>4 We explain how the approximation can be systematically improved and argue that similar techniques can be applied to general likelihood models. [sent-7, score-0.252]
</p><p>5 Being non-parametric models by construction their theoretical understanding seems to be less well developed compared to simpler parametric models like neural networks. [sent-11, score-0.041]
</p><p>6 We are especially interested in developing theoretical approaches which will at least give good approximations to generalization errors when the number of training data is sufficiently large. [sent-12, score-0.391]
</p><p>7 In this paper we present a step in this direction which is based on a statistical mechanics approach. [sent-13, score-0.128]
</p><p>8 In contrast to most previous applications of statistical mechanics to learning theory we are not limited to the so called "thermodynamic" limit which would require a high dimensional input space. [sent-14, score-0.233]
</p><p>9 [5]) who presented a nice approximate treatment of the Bayesian generalization error of G P regression which actually gives good results even in the case of a one dimensional input space. [sent-17, score-0.496]
</p><p>10 His method is based on an exact recursion for the generalization error of the regression problem together with approximations that decouple certain correlations of random variables. [sent-18, score-0.648]
</p><p>11 Unfortunately, the method seems to be limited because the exact recursion is an artifact of the Gaussianity of the regression model and is not available for other cases such as classification models. [sent-19, score-0.261]
</p><p>12 Second, it is not clear how to assess the quality of the approximations made and how one may systematically improve on them. [sent-20, score-0.127]
</p><p>13 Finally, the calculation is (so far) restricted to  a full Bayesian scenario, where a prior average over the unknown data generating function simplifies the analysis. [sent-21, score-0.345]
</p><p>14 It allows us to compute other quantities besides the generalization error. [sent-23, score-0.202]
</p><p>15 Finally, it is possible to compute the corrections to our approximations. [sent-24, score-0.108]
</p><p>16 2  Regression with Gaussian processes  To explain the Gaussian process scenario for regression problems [2], we assume that we observe corrupted values y(x) E R of an unknown function f(x) at input points x E Rd. [sent-25, score-0.437]
</p><p>17 If the corruption is due to independent Gaussian noise with variance u 2, the likelihood for a set of m example data D = (Y(Xl), . [sent-26, score-0.242]
</p><p>18 The available prior information is that f is a realization of a Gaussian process (random field) with zero mean and covariance C(x,x') = E[f(x)f(x')], where E denotes the expectation over the Gaussian process. [sent-35, score-0.145]
</p><p>19 We assume that the prediction at a test point x is given by the posterior expectation of f(x), i. [sent-36, score-0.099]
</p><p>20 by  j(x)  = E{f(x)ID} = Ef(x)P(Dlf) Z  (2)  where the partition function Z normalises the posterior. [sent-38, score-0.157]
</p><p>21 Calling the true data generating function 1* (in order to distinguish it from the functions over which we integrate in the expectations) we are interested in the learning curve, i. [sent-39, score-0.284]
</p><p>22 the generalization (mean square) error averaged over independent draws of example data, i. [sent-41, score-0.342]
</p><p>23 Cg = [((f*(x) - j(x))2}]D as a function of m, the sample size. [sent-43, score-0.068]
</p><p>24 ]D denote averages over example data sets where we assume that the inputs Xi are drawn independently at random from a density p(x). [sent-47, score-0.242]
</p><p>25 ) denotes an average over test inputs drawn from the same density. [sent-51, score-0.149]
</p><p>26 Later, the same brackets will also be used for averages over several different test points and for joint averages over test inputs and test outputs. [sent-52, score-0.31]
</p><p>27 3  The Partition Function  As typical of statistical mechanics approaches, we base our analysis on the averaged "free energy" [-In Z]D where the partition function Z (see Eq. [sent-53, score-0.347]
</p><p>28 (3)  [In Z]D serves as a generating function for suitable posterior averages. [sent-55, score-0.205]
</p><p>29 The computation of [In Z]D is based on the replica trick In Z = limn-+o znn- 1, where we compute [zn]D for integer n and perform the continuation at the end. [sent-57, score-0.146]
</p><p>30 Introducing a set of auxiliary integration variables squares, we get  Zka  in order to decouple the  where En denotes the expectation over the n times replicated GP measure. [sent-58, score-0.314]
</p><p>31 In general, it seems impossible to perform the average over the data. [sent-59, score-0.084]
</p><p>32 Using a cumulant expansion, an infinite series of terms would be created. [sent-60, score-0.211]
</p><p>33 However one may be tempted to try the following heuristic approximation: If (for fixed function I), the distribution of f(Xk) - Yk was a zero mean Gaussian, we would simply end up with only the second cumulant and  JII  dZka 27r exp  k,a  x  En exp  ((72 '" Zka -2 L. [sent-61, score-0.358]
</p><p>34 Although such a reasoning may be justified in cases where the dimensionality of inputs x is large, the assumption of approximate Gaussianity is typically (in the sense of the prior measure over functions I) completely wrong for small dimensions. [sent-65, score-0.253]
</p><p>35 Nevertheless, we will argue in the next section that the expression Eq. [sent-66, score-0.042]
</p><p>36 (5) Uustified by a different reason) is a good approximation for large sample sizes and nonzero noise level. [sent-67, score-0.27]
</p><p>37 (5) following a fairly standard recipe: The high dimensional integrals over Zka are turned into low dimensional integrals by the introduction of" order-parameters" 'T}ab = 2:;;'=1 ZkaZkb so that  ~  ! [sent-69, score-0.356]
</p><p>38 We expect that in the limit of large sample size m, the integrals are well approximated by the saddle-point method. [sent-74, score-0.253]
</p><p>39 To perform the limit n -t 0, we make the assumption that the saddle-point of the matrix 'T} is replica symmetric, i. [sent-75, score-0.159]
</p><p>40 After some calculations we arrive at [lnZJD  =  (72'T}O m m'T} 'T} 0 2 --2- + "2 In ('T}o - 'T}) + 2('T}o _ 'T}) - "2(E f (x)  + In E exp [- 'T}o ; 'T} ((f(x) - y)2)] -  ~ (In(27rm) -  (7) 1)  into which we have to insert the values 'T} and 'T}o that make the right hand side an extremum. [sent-79, score-0.189]
</p><p>41 We have defined a new auxiliary (translated) Gaussian measure over functions by  (8) where ¢ is a functional of f. [sent-80, score-0.128]
</p><p>42 For a given input distribution it is possible to compute the required expectations in terms of sums over eigenvalues and eigenfunctions of the covariance kernel C(x, x'). [sent-81, score-0.288]
</p><p>43 We will give the details as well as the explicit order parameter equations in a full version of the paper. [sent-82, score-0.045]
</p><p>44 4  Generalization error  To relate the generalization error with the order parameters, note that in the replica framework (assuming the approximation Eq. [sent-83, score-0.657]
</p><p>45 (5)) we have  -l~fIId"'ab exp [-~u2L"'aa+G({"'})l a  cg+u 2  x  a~b  aa  X  En exp  "'12  (-~ La,b "'ab((fa(X) - y)(fb(X) - y)))  which by a partial integration and a subsequent saddle point integration yields Cg  = - (  m",  "'0 -",  2  )2 -  (9)  U . [sent-84, score-0.36]
</p><p>46 5  Why (and when) the approximation works  Our intuition behind the approximation Eq. [sent-86, score-0.266]
</p><p>47 This intuition can be checked self consistently by estimating the omitted terms perturbatively. [sent-88, score-0.097]
</p><p>48 We use the following modified partition function [Zn('\)]D  =  f  II d;;a e - u  2 2  Ek,a  z~a En [ exp  k,a  (i'\ Lk,a Zka(fa (Xk) - y)  1 ~ ,\2 L L ZkaZkb((fa(X) - y)(fb(X) - y)))]  a,b  k  (11) D  which for ,\ = 1 becomes the "true" partition function, whereas Eq. [sent-89, score-0.411]
</p><p>49 Expanding in powers of ,\ (the terms with odd powers vanish) is equivalent to generating the cumulant expansion and subsequently expanding the non-quadratic terms down. [sent-91, score-0.694]
</p><p>50 Within the saddle-point approximation, the first nonzero correction to our approximation of [In Z] is given by ,\4  C"'O 2-:n",)2 (u 2(C(X, x)) + (C(x, X)F2(X)) -  (C(x, x')F(x)F(x'))  +",(C(x, x')C(x, x")C(x', x")) - ",(C(x, x)C 2(x, x')))  +~ (-: + ~)  ((C 2(x,x)) - (C 2(x, x'))) ). [sent-92, score-0.227]
</p><p>51 (12)  C(x,x') = E°{f(x)f(x')} denotes the covariance with respect to the auxiliary measure and F(x) == f*(x) - (C(x,XI)f*(X")). [sent-93, score-0.144]
</p><p>52 The significance of the individual terms as m -+ 00 can be estimated from the following scaling. [sent-94, score-0.047]
</p><p>53 (12) remains finite as m -+ 00, whereas the leading approximation Eq. [sent-98, score-0.108]
</p><p>54 We have not (yet) computed the resulting correction to ego However, we have studied the somewhat simpler error measure e' == ~ l:dE{(f*(Xi) - f(Xi))2ID}]D which can be obtained from a derivative of [In Z]D with respect to a 2 . [sent-100, score-0.24]
</p><p>55 It equals the error of a Gibbs algorithm (sampling from the posterior) on the training data. [sent-101, score-0.121]
</p><p>56 We can show that the correction to e' is typically by a factor of O(l/m) smaller than the leading term. [sent-102, score-0.077]
</p><p>57 However, our approximation becomes worse with decreasing noise variance a 2 . [sent-103, score-0.209]
</p><p>58 a = 0 is a singular case for which (at least for some GPs with slowly decreasing eigenvalues) it can be shown that our approximation for eg decays to zero at the wrong rate. [sent-104, score-0.28]
</p><p>59 For small values of a, a -+ 0, we expect that higher order terms in the perturbation expansion will become relevant. [sent-105, score-0.185]
</p><p>60 6  Results  We compare our analytical results for the error measures eg and et with simulations of GP regression. [sent-106, score-0.346]
</p><p>61 For simplicity, we have chosen periodic processes of the form f(x) = v'2l: n(an cos(27rnx) + bn sin(27rnx)) for x E [0,1] where the coefficients an, bn are independent Gaussians with E{ a~} = E{b~} = An. [sent-107, score-0.282]
</p><p>62 This choice is convenient for analytical calculations by the orthogonality of the trigonometric functions when we sample the Xi from a uniform density in [0,1]. [sent-108, score-0.242]
</p><p>63 The An and the translation invariant covariance kernel are related by c(x - y) == C(x,y) = 2l:n An cos(27rn(x - y)) and An = c(x) cos(27rnx) dx. [sent-109, score-0.098]
</p><p>64 We specialise on the (periodic) RBF kernel c(x) = l:~-oo exp [-(x - k)2/2l 2] with l = 0. [sent-110, score-0.138]
</p><p>65 For an illustration we generated learning curves for two target functions f* as displayed in Fig. [sent-112, score-0.16]
</p><p>66 One function is a sine-wave f*(x) = J2Al sin(27rx) while the other is a random realisation from the prior distribution. [sent-114, score-0.042]
</p><p>67 The data points have been obtained by corruption of the target function with Gaussian noise of variance a 2 = 0. [sent-117, score-0.242]
</p><p>68 1 shows the data averaged generalization and training errors eg, et as a function of the number m of example data. [sent-120, score-0.316]
</p><p>69 Solid curves display simulation results while the results of our theory Eqs. [sent-121, score-0.222]
</p><p>70 The training error et converges to the noise level a 2 • As one can see from the pictures our theory is very accurate when the number m of example data is sufficiently large. [sent-123, score-0.29]
</p><p>71 While the generalization error e 9 differs initially, the asymptotic decay is the same. [sent-124, score-0.28]
</p><p>72 J;  7  The Bayes error  We can also apply our method to the Bayesian generalization error (previously approximated by Peter Sollich [5]). [sent-125, score-0.401]
</p><p>73 The Bayes error is obtained by averaging the generalization error over "true" functions f* drawn at random from the prior distribution. [sent-126, score-0.529]
</p><p>74 Within our approach this can be achieved by an average of Eq. [sent-127, score-0.043]
</p><p>75 The resulting order parameter equations and their relation to the Bayes error turn out be identical to Sollich's result. [sent-129, score-0.166]
</p><p>76 Hence, we have managed to re-derive his approximation within a broader framework from which also possible corrections can be obtained. [sent-130, score-0.173]
</p><p>77 Data generating function  Learning curves  10° 10- 1 Ct  f •(x) -1  10- 2 10- 3 10-4  0  0. [sent-131, score-0.271]
</p><p>78 8  1 0  x  150 200 50 100 Number m of example data 10° 10- 1  . [sent-135, score-0.046]
</p><p>79 10- 2  f (x)  10- 3 -1  10-4  -2 1  0 x  Number m of example data  Figure 1: The left panels show two data generating functions f*(x) and example sets of 50 data points. [sent-136, score-0.405]
</p><p>80 The right panels display the corresponding averaged learning curves. [sent-137, score-0.239]
</p><p>81 Solid curves display simulation results for generalization and training errors Cg, Ct. [sent-138, score-0.43]
</p><p>82 8  Future work  At present, we extend our method in the following directions: • The statistical mechanics framework presented in this paper is based on a partition function Z which can be used to generate a variety of other data averages for posterior expectations. [sent-141, score-0.474]
</p><p>83 An obvious interesting quantity is given by the sample fluctuations of the generalization error  which gives confidence intervals on  Cg. [sent-142, score-0.406]
</p><p>84 • Obviously, our method is not restricted to a regression model (in this case however, all resulting integrals are elementary) but can also be directly generalized to other likelihoods such as the classification case [4, 6]. [sent-143, score-0.296]
</p><p>85 • The saddle-point approximation neglects fluctuations of the order parameters. [sent-145, score-0.211]
</p><p>86 This may be well justified when m is sufficiently large. [sent-146, score-0.124]
</p><p>87 It is possible to improve on this by including the quadratic expansion around the saddlepoint. [sent-147, score-0.093]
</p><p>88 • Finally, one may criticise our method as being of minor relevance to practical applications, because our calculations require the knowledge of the unknown function 1* and the density of the inputs x. [sent-148, score-0.215]
</p><p>89 (9) and (10) show that important error measures are solely expressed by the order parameters 'fI and 'flo. [sent-150, score-0.234]
</p><p>90 Hence, estimating some error measures and the posterior variance at the data points empirically would allow us to predict values for the order parameters. [sent-151, score-0.382]
</p><p>91 Those in turn could be used to make predictions for the unknown generalization error. [sent-152, score-0.221]
</p><p>92 Sollich, Learning curves for Gaussian processes, in Neural Information Processing Systems 11, M. [sent-200, score-0.119]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('zka', 0.238), ('fa', 0.226), ('fb', 0.172), ('regression', 0.167), ('cumulant', 0.164), ('sollich', 0.164), ('cg', 0.161), ('generalization', 0.159), ('partition', 0.157), ('generating', 0.152), ('dlf', 0.143), ('zkazkb', 0.143), ('ab', 0.138), ('integrals', 0.129), ('mechanics', 0.128), ('error', 0.121), ('curves', 0.119), ('eg', 0.116), ('processes', 0.109), ('approximation', 0.108), ('replica', 0.103), ('display', 0.103), ('gaussian', 0.102), ('exp', 0.097), ('corruption', 0.095), ('expansion', 0.093), ('calculations', 0.092), ('averages', 0.09), ('auxiliary', 0.087), ('gp', 0.087), ('williams', 0.087), ('mozer', 0.083), ('decouple', 0.082), ('gaussianity', 0.082), ('en', 0.078), ('correction', 0.077), ('cos', 0.077), ('petsche', 0.074), ('panels', 0.074), ('sufficiently', 0.071), ('powers', 0.069), ('brackets', 0.069), ('sample', 0.068), ('measures', 0.068), ('approximations', 0.066), ('zn', 0.065), ('corrections', 0.065), ('averaged', 0.062), ('unknown', 0.062), ('inputs', 0.061), ('systematically', 0.061), ('opper', 0.061), ('periodic', 0.061), ('scenario', 0.058), ('aa', 0.058), ('fluctuations', 0.058), ('expectations', 0.058), ('covariance', 0.057), ('limit', 0.056), ('wrong', 0.056), ('bn', 0.056), ('xi', 0.055), ('integration', 0.054), ('justified', 0.053), ('recursion', 0.053), ('expanding', 0.053), ('posterior', 0.053), ('noise', 0.052), ('panel', 0.051), ('sin', 0.051), ('xk', 0.05), ('intuition', 0.05), ('dimensional', 0.049), ('variance', 0.049), ('errors', 0.049), ('terms', 0.047), ('data', 0.046), ('expectation', 0.046), ('order', 0.045), ('peter', 0.045), ('drawn', 0.045), ('bayes', 0.045), ('bayesian', 0.044), ('average', 0.043), ('compute', 0.043), ('nonzero', 0.042), ('somewhat', 0.042), ('argue', 0.042), ('eigenvalues', 0.042), ('prior', 0.042), ('analytical', 0.041), ('explain', 0.041), ('kernel', 0.041), ('seems', 0.041), ('functions', 0.041), ('jii', 0.041), ('fokoue', 0.041), ('recipe', 0.041), ('schottky', 0.041), ('epsrc', 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="77-tfidf-1" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: Based on a statistical mechanics approach, we develop a method for approximately computing average case learning curves for Gaussian process regression models. The approximation works well in the large sample size limit and for arbitrary dimensionality of the input space. We explain how the approximation can be systematically improved and argue that similar techniques can be applied to general likelihood models. 1</p><p>2 0.19099458 <a title="77-tfidf-2" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>Author: Lehel Csatč´¸, Manfred Opper</p><p>Abstract: We develop an approach for a sparse representation for Gaussian Process (GP) models in order to overcome the limitations of GPs caused by large data sets. The method is based on a combination of a Bayesian online algorithm together with a sequential construction of a relevant subsample of the data which fully specifies the prediction of the model. Experimental results on toy examples and large real-world data sets indicate the efficiency of the approach.</p><p>3 0.15522847 <a title="77-tfidf-3" href="./nips-2000-Computing_with_Finite_and_Infinite_Networks.html">35 nips-2000-Computing with Finite and Infinite Networks</a></p>
<p>Author: Ole Winther</p><p>Abstract: Using statistical mechanics results, I calculate learning curves (average generalization error) for Gaussian processes (GPs) and Bayesian neural networks (NNs) used for regression. Applying the results to learning a teacher defined by a two-layer network, I can directly compare GP and Bayesian NN learning. I find that a GP in general requires CJ (d S )-training examples to learn input features of order s (d is the input dimension), whereas a NN can learn the task with order the number of adjustable weights training examples. Since a GP can be considered as an infinite NN, the results show that even in the Bayesian approach, it is important to limit the complexity of the learning machine. The theoretical findings are confirmed in simulations with analytical GP learning and a NN mean field algorithm.</p><p>4 0.12788956 <a title="77-tfidf-4" href="./nips-2000-Learning_Continuous_Distributions%3A_Simulations_With_Field_Theoretic_Priors.html">76 nips-2000-Learning Continuous Distributions: Simulations With Field Theoretic Priors</a></p>
<p>Author: Ilya Nemenman, William Bialek</p><p>Abstract: Learning of a smooth but nonparametric probability density can be regularized using methods of Quantum Field Theory. We implement a field theoretic prior numerically, test its efficacy, and show that the free parameter of the theory (,smoothness scale') can be determined self consistently by the data; this forms an infinite dimensional generalization of the MDL principle. Finally, we study the implications of one's choice of the prior and the parameterization and conclude that the smoothness scale determination makes density estimation very weakly sensitive to the choice of the prior, and that even wrong choices can be advantageous for small data sets. One of the central problems in learning is to balance 'goodness of fit' criteria against the complexity of models. An important development in the Bayesian approach was thus the realization that there does not need to be any extra penalty for model complexity: if we compute the total probability that data are generated by a model, there is a factor from the volume in parameter space-the 'Occam factor' -that discriminates against models with more parameters [1, 2]. This works remarkably welJ for systems with a finite number of parameters and creates a complexity 'razor' (after 'Occam's razor') that is almost equivalent to the celebrated Minimal Description Length (MDL) principle [3]. In addition, if the a priori distributions involved are strictly Gaussian, the ideas have also been proven to apply to some infinite-dimensional (nonparametric) problems [4]. It is not clear, however, what happens if we leave the finite dimensional setting to consider nonparametric problems which are not Gaussian, such as the estimation of a smooth probability density. A possible route to progress on the nonparametric problem was opened by noticing [5] that a Bayesian prior for density estimation is equivalent to a quantum field theory (QFT). In particular, there are field theoretic methods for computing the infinite dimensional analog of the Occam factor, at least asymptotically for large numbers of examples. These observations have led to a number of papers [6, 7, 8, 9] exploring alternative formulations and their implications for the speed of learning. Here we return to the original formulation of Ref. [5] and use numerical methods to address some of the questions left open by the analytic work [10]: What is the result of balancing the infinite dimensional Occam factor against the goodness of fit? Is the QFT inference optimal in using alJ of the information relevant for learning [II]? What happens if our learning problem is strongly atypical of the prior distribution? Following Ref. [5], if N i. i. d. samples {Xi}, i = 1 ... N, are observed, then the probability that a particular density Q(x) gave rise to these data is given by P[Q(x)l{x.}] P[Q(x)] rr~1 Q(Xi) • - J[dQ(x)]P[Q(x)] rr~1 Q(Xi) , (1) where P[Q(x)] encodes our a priori expectations of Q. Specifying this prior on a space of functions defines a QFf, and the optimal least square estimator is then Q (I{ .}) - (Q(X)Q(Xl)Q(X2) ... Q(XN)}(O) est X X. (Q(Xl)Q(X2) ... Q(XN ))(0) , (2) where ( ... )(0) means averaging with respect to the prior. Since Q(x) ~ 0, it is convenient to define an unconstrained field ¢(x), Q(x) (l/io)exp[-¢(x)]. Other definitions are also possible [6], but we think that most of our results do not depend on this choice. = The next step is to select a prior that regularizes the infinite number of degrees of freedom and allows learning. We want the prior P[¢] to make sense as a continuous theory, independent of discretization of x on small scales. We also require that when we estimate the distribution Q(x) the answer must be everywhere finite. These conditions imply that our field theory must be convergent at small length scales. For x in one dimension, a minimal choice is P[¢(x)] 1 = Z exp [£2 11 - 1 --2- f (8 dx [1 f 11 ¢)2] c5 io 8xll ] dxe-¢(x) -1 , (3) where'T/ > 1/2, Z is the normalization constant, and the c5-function enforces normalization of Q. We refer to i and 'T/ as the smoothness scale and the exponent, respectively. In [5] this theory was solved for large Nand 'T/ = 1: N (II Q(Xi))(O) ~ (4) = (5) + (6) i=1 Seff i8;¢c1 (x) where ¢cl is the 'classical' (maximum likelihood, saddle point) solution. In the effective action [Eq. (5)], it is the square root term that arises from integrating over fluctuations around the classical solution (Occam factors). It was shown that Eq. (4) is nonsingular even at finite N, that the mean value of ¢c1 converges to the negative logarithm of the target distribution P(x) very quickly, and that the variance of fluctuations 'Ij;(x) ¢(x) [- log ioP( x)] falls off as ....., 1/ iN P( x). Finally, it was speculated that if the actual i is unknown one may average over it and hope that, much as in Bayesian model selection [2], the competition between the data and the fluctuations will select the optimal smoothness scale i*. J = At the first glance the theory seems to look almost exactly like a Gaussian Process [4]. This impression is produced by a Gaussian form of the smoothness penalty in Eq. (3), and by the fluctuation determinant that plays against the goodness of fit in the smoothness scale (model) selection. However, both similarities are incomplete. The Gaussian penalty in the prior is amended by the normalization constraint, which gives rise to the exponential term in Eq. (6), and violates many familiar results that hold for Gaussian Processes, the representer theorem [12] being just one of them. In the semi--classical limit of large N, Gaussianity is restored approximately, but the classical solution is extremely non-trivial, and the fluctuation determinant is only the leading term of the Occam's razor, not the complete razor as it is for a Gaussian Process. In addition, it has no data dependence and is thus remarkably different from the usual determinants arising in the literature. The algorithm to implement the discussed density estimation procedure numerically is rather simple. First, to make the problem well posed [10, 11] we confine x to a box a ~ x ~ L with periodic boundary conditions. The boundary value problem Eq. (6) is then solved by a standard 'relaxation' (or Newton) method of iterative improvements to a guessed solution [13] (the target precision is always 10- 5 ). The independent variable x E [0,1] is discretized in equal steps [10 4 for Figs. (l.a-2.b), and 105 for Figs. (3.a, 3.b)]. We use an equally spaced grid to ensure stability of the method, while small step sizes are needed since the scale for variation of ¢el (x) is [5] (7) c5x '</p><p>5 0.1267806 <a title="77-tfidf-5" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>Author: Alex J. Smola, Peter L. Bartlett</p><p>Abstract: We present a simple sparse greedy technique to approximate the maximum a posteriori estimate of Gaussian Processes with much improved scaling behaviour in the sample size m. In particular, computational requirements are O(n 2 m), storage is O(nm), the cost for prediction is 0 (n) and the cost to compute confidence bounds is O(nm), where n «: m. We show how to compute a stopping criterion, give bounds on the approximation error, and show applications to large scale problems. 1</p><p>6 0.11351399 <a title="77-tfidf-6" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>7 0.11211876 <a title="77-tfidf-7" href="./nips-2000-Mixtures_of_Gaussian_Processes.html">85 nips-2000-Mixtures of Gaussian Processes</a></p>
<p>8 0.10947324 <a title="77-tfidf-8" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>9 0.10904473 <a title="77-tfidf-9" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>10 0.10510831 <a title="77-tfidf-10" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>11 0.10298157 <a title="77-tfidf-11" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>12 0.09480717 <a title="77-tfidf-12" href="./nips-2000-Tree-Based_Modeling_and_Estimation_of_Gaussian_Processes_on_Graphs_with_Cycles.html">140 nips-2000-Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles</a></p>
<p>13 0.094373412 <a title="77-tfidf-13" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>14 0.093945086 <a title="77-tfidf-14" href="./nips-2000-Error-correcting_Codes_on_a_Bethe-like_Lattice.html">47 nips-2000-Error-correcting Codes on a Bethe-like Lattice</a></p>
<p>15 0.093883149 <a title="77-tfidf-15" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>16 0.091772668 <a title="77-tfidf-16" href="./nips-2000-Second_Order_Approximations_for_Probability_Models.html">114 nips-2000-Second Order Approximations for Probability Models</a></p>
<p>17 0.08940132 <a title="77-tfidf-17" href="./nips-2000-A_Gradient-Based_Boosting_Algorithm_for_Regression_Problems.html">3 nips-2000-A Gradient-Based Boosting Algorithm for Regression Problems</a></p>
<p>18 0.088717632 <a title="77-tfidf-18" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>19 0.088556491 <a title="77-tfidf-19" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>20 0.088078707 <a title="77-tfidf-20" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.302), (1, 0.085), (2, 0.044), (3, -0.013), (4, 0.183), (5, 0.04), (6, -0.078), (7, 0.065), (8, -0.002), (9, -0.207), (10, -0.047), (11, 0.152), (12, -0.006), (13, -0.013), (14, 0.076), (15, 0.114), (16, -0.029), (17, 0.005), (18, 0.059), (19, -0.17), (20, 0.006), (21, -0.126), (22, 0.09), (23, 0.094), (24, -0.013), (25, 0.065), (26, -0.059), (27, 0.083), (28, -0.038), (29, 0.01), (30, 0.033), (31, 0.016), (32, -0.055), (33, -0.09), (34, 0.019), (35, 0.105), (36, -0.12), (37, 0.102), (38, -0.063), (39, -0.068), (40, 0.008), (41, 0.023), (42, -0.061), (43, -0.001), (44, 0.007), (45, 0.054), (46, -0.038), (47, 0.031), (48, 0.055), (49, 0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9645018 <a title="77-lsi-1" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: Based on a statistical mechanics approach, we develop a method for approximately computing average case learning curves for Gaussian process regression models. The approximation works well in the large sample size limit and for arbitrary dimensionality of the input space. We explain how the approximation can be systematically improved and argue that similar techniques can be applied to general likelihood models. 1</p><p>2 0.767739 <a title="77-lsi-2" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>Author: Lehel Csatč´¸, Manfred Opper</p><p>Abstract: We develop an approach for a sparse representation for Gaussian Process (GP) models in order to overcome the limitations of GPs caused by large data sets. The method is based on a combination of a Bayesian online algorithm together with a sequential construction of a relevant subsample of the data which fully specifies the prediction of the model. Experimental results on toy examples and large real-world data sets indicate the efficiency of the approach.</p><p>3 0.70307189 <a title="77-lsi-3" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>Author: Alex J. Smola, Peter L. Bartlett</p><p>Abstract: We present a simple sparse greedy technique to approximate the maximum a posteriori estimate of Gaussian Processes with much improved scaling behaviour in the sample size m. In particular, computational requirements are O(n 2 m), storage is O(nm), the cost for prediction is 0 (n) and the cost to compute confidence bounds is O(nm), where n «: m. We show how to compute a stopping criterion, give bounds on the approximation error, and show applications to large scale problems. 1</p><p>4 0.67966366 <a title="77-lsi-4" href="./nips-2000-Mixtures_of_Gaussian_Processes.html">85 nips-2000-Mixtures of Gaussian Processes</a></p>
<p>Author: Volker Tresp</p><p>Abstract: We introduce the mixture of Gaussian processes (MGP) model which is useful for applications in which the optimal bandwidth of a map is input dependent. The MGP is derived from the mixture of experts model and can also be used for modeling general conditional probability densities. We discuss how Gaussian processes -in particular in form of Gaussian process classification, the support vector machine and the MGP modelcan be used for quantifying the dependencies in graphical models.</p><p>5 0.63335264 <a title="77-lsi-5" href="./nips-2000-Computing_with_Finite_and_Infinite_Networks.html">35 nips-2000-Computing with Finite and Infinite Networks</a></p>
<p>Author: Ole Winther</p><p>Abstract: Using statistical mechanics results, I calculate learning curves (average generalization error) for Gaussian processes (GPs) and Bayesian neural networks (NNs) used for regression. Applying the results to learning a teacher defined by a two-layer network, I can directly compare GP and Bayesian NN learning. I find that a GP in general requires CJ (d S )-training examples to learn input features of order s (d is the input dimension), whereas a NN can learn the task with order the number of adjustable weights training examples. Since a GP can be considered as an infinite NN, the results show that even in the Bayesian approach, it is important to limit the complexity of the learning machine. The theoretical findings are confirmed in simulations with analytical GP learning and a NN mean field algorithm.</p><p>6 0.56665045 <a title="77-lsi-6" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>7 0.56432897 <a title="77-lsi-7" href="./nips-2000-Learning_Continuous_Distributions%3A_Simulations_With_Field_Theoretic_Priors.html">76 nips-2000-Learning Continuous Distributions: Simulations With Field Theoretic Priors</a></p>
<p>8 0.51020807 <a title="77-lsi-8" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>9 0.49850383 <a title="77-lsi-9" href="./nips-2000-Tree-Based_Modeling_and_Estimation_of_Gaussian_Processes_on_Graphs_with_Cycles.html">140 nips-2000-Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles</a></p>
<p>10 0.47103938 <a title="77-lsi-10" href="./nips-2000-A_Gradient-Based_Boosting_Algorithm_for_Regression_Problems.html">3 nips-2000-A Gradient-Based Boosting Algorithm for Regression Problems</a></p>
<p>11 0.45149288 <a title="77-lsi-11" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>12 0.44216719 <a title="77-lsi-12" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>13 0.43227375 <a title="77-lsi-13" href="./nips-2000-Second_Order_Approximations_for_Probability_Models.html">114 nips-2000-Second Order Approximations for Probability Models</a></p>
<p>14 0.4276033 <a title="77-lsi-14" href="./nips-2000-Analysis_of_Bit_Error_Probability_of_Direct-Sequence_CDMA_Multiuser_Demodulators.html">25 nips-2000-Analysis of Bit Error Probability of Direct-Sequence CDMA Multiuser Demodulators</a></p>
<p>15 0.41594374 <a title="77-lsi-15" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>16 0.41330501 <a title="77-lsi-16" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>17 0.39742836 <a title="77-lsi-17" href="./nips-2000-Some_New_Bounds_on_the_Generalization_Error_of_Combined_Classifiers.html">119 nips-2000-Some New Bounds on the Generalization Error of Combined Classifiers</a></p>
<p>18 0.39635873 <a title="77-lsi-18" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>19 0.39231467 <a title="77-lsi-19" href="./nips-2000-On_Iterative_Krylov-Dogleg_Trust-Region_Steps_for_Solving_Neural_Networks_Nonlinear_Least_Squares_Problems.html">93 nips-2000-On Iterative Krylov-Dogleg Trust-Region Steps for Solving Neural Networks Nonlinear Least Squares Problems</a></p>
<p>20 0.3782368 <a title="77-lsi-20" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.017), (17, 0.103), (32, 0.016), (33, 0.032), (54, 0.022), (55, 0.014), (62, 0.015), (67, 0.066), (76, 0.57), (79, 0.016), (81, 0.012), (90, 0.027), (91, 0.011), (97, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98497528 <a title="77-lda-1" href="./nips-2000-Regularization_with_Dot-Product_Kernels.html">110 nips-2000-Regularization with Dot-Product Kernels</a></p>
<p>Author: Alex J. Smola, Zoltán L. Óvári, Robert C. Williamson</p><p>Abstract: In this paper we give necessary and sufficient conditions under which kernels of dot product type k(x, y) = k(x . y) satisfy Mercer's condition and thus may be used in Support Vector Machines (SVM), Regularization Networks (RN) or Gaussian Processes (GP). In particular, we show that if the kernel is analytic (i.e. can be expanded in a Taylor series), all expansion coefficients have to be nonnegative. We give an explicit functional form for the feature map by calculating its eigenfunctions and eigenvalues. 1</p><p>same-paper 2 0.96943063 <a title="77-lda-2" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: Based on a statistical mechanics approach, we develop a method for approximately computing average case learning curves for Gaussian process regression models. The approximation works well in the large sample size limit and for arbitrary dimensionality of the input space. We explain how the approximation can be systematically improved and argue that similar techniques can be applied to general likelihood models. 1</p><p>3 0.65178251 <a title="77-lda-3" href="./nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">95 nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>Author: Christopher K. I. Williams</p><p>Abstract: In this paper we show that the kernel peA algorithm of Sch6lkopf et al (1998) can be interpreted as a form of metric multidimensional scaling (MDS) when the kernel function k(x, y) is isotropic, i.e. it depends only on Ilx - yll. This leads to a metric MDS algorithm where the desired configuration of points is found via the solution of an eigenproblem rather than through the iterative optimization of the stress objective function. The question of kernel choice is also discussed. 1</p><p>4 0.60788751 <a title="77-lda-4" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>Author: Oliver B. Downs</p><p>Abstract: Recent work has exploited boundedness of data in the unsupervised learning of new types of generative model. For nonnegative data it was recently shown that the maximum-entropy generative model is a Nonnegative Boltzmann Distribution not a Gaussian distribution, when the model is constrained to match the first and second order statistics of the data. Learning for practical sized problems is made difficult by the need to compute expectations under the model distribution. The computational cost of Markov chain Monte Carlo methods and low fidelity of naive mean field techniques has led to increasing interest in advanced mean field theories and variational methods. Here I present a secondorder mean-field approximation for the Nonnegative Boltzmann Machine model, obtained using a</p><p>5 0.56743246 <a title="77-lda-5" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>Author: Martijn A. R. Leisink, Hilbert J. Kappen</p><p>Abstract: We present a method to bound the partition function of a Boltzmann machine neural network with any odd order polynomial. This is a direct extension of the mean field bound, which is first order. We show that the third order bound is strictly better than mean field. Additionally we show the rough outline how this bound is applicable to sigmoid belief networks. Numerical experiments indicate that an error reduction of a factor two is easily reached in the region where expansion based approximations are useful. 1</p><p>6 0.5499649 <a title="77-lda-6" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>7 0.54485917 <a title="77-lda-7" href="./nips-2000-Second_Order_Approximations_for_Probability_Models.html">114 nips-2000-Second Order Approximations for Probability Models</a></p>
<p>8 0.53571856 <a title="77-lda-8" href="./nips-2000-Mixtures_of_Gaussian_Processes.html">85 nips-2000-Mixtures of Gaussian Processes</a></p>
<p>9 0.53107864 <a title="77-lda-9" href="./nips-2000-Some_New_Bounds_on_the_Generalization_Error_of_Combined_Classifiers.html">119 nips-2000-Some New Bounds on the Generalization Error of Combined Classifiers</a></p>
<p>10 0.52455443 <a title="77-lda-10" href="./nips-2000-Computing_with_Finite_and_Infinite_Networks.html">35 nips-2000-Computing with Finite and Infinite Networks</a></p>
<p>11 0.51232016 <a title="77-lda-11" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>12 0.50788367 <a title="77-lda-12" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>13 0.50076985 <a title="77-lda-13" href="./nips-2000-Ensemble_Learning_and_Linear_Response_Theory_for_ICA.html">46 nips-2000-Ensemble Learning and Linear Response Theory for ICA</a></p>
<p>14 0.48776501 <a title="77-lda-14" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>15 0.4858838 <a title="77-lda-15" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>16 0.48568827 <a title="77-lda-16" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>17 0.48192033 <a title="77-lda-17" href="./nips-2000-The_Kernel_Trick_for_Distances.html">134 nips-2000-The Kernel Trick for Distances</a></p>
<p>18 0.47016457 <a title="77-lda-18" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>19 0.46624678 <a title="77-lda-19" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>20 0.46509698 <a title="77-lda-20" href="./nips-2000-Efficient_Learning_of_Linear_Perceptrons.html">44 nips-2000-Efficient Learning of Linear Perceptrons</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
