<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>72 nips-2000-Keeping Flexible Active Contours on Track using Metropolis Updates</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-72" href="#">nips2000-72</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>72 nips-2000-Keeping Flexible Active Contours on Track using Metropolis Updates</h1>
<br/><p>Source: <a title="nips-2000-72-pdf" href="http://papers.nips.cc/paper/1835-keeping-flexible-active-contours-on-track-using-metropolis-updates.pdf">pdf</a></p><p>Author: Trausti T. Kristjansson, Brendan J. Frey</p><p>Abstract: Condensation, a form of likelihood-weighted particle filtering, has been successfully used to infer the shapes of highly constrained</p><p>Reference: <a title="nips-2000-72-reference" href="../nips2000_reference/nips-2000-Keeping_Flexible_Active_Contours_on_Track_using_Metropolis_Updates_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Keeping flexible active contours on track using Metropolis updates  Trausti T. [sent-1, score-0.541]
</p><p>2 Kristjansson University of Waterloo tt kr i s tj @uwa te r l oo . [sent-2, score-0.049]
</p><p>3 ca  Abstract Condensation, a form of likelihood-weighted particle filtering, has been successfully used to infer the shapes of highly constrained "active" contours in video sequences. [sent-5, score-0.91]
</p><p>4 for tracking fingers of a hand), a computationally burdensome number of particles is needed to successfully approximate the contour distribution. [sent-8, score-0.817]
</p><p>5 We show how the Metropolis algorithm can be used to update a particle set representing a distribution over contours at each frame in a video sequence. [sent-9, score-0.918]
</p><p>6 We compare this method to condensation using a video sequence that requires highly flexible contours, and show that the new algorithm performs dramatically better that the condensation algorithm. [sent-10, score-0.776]
</p><p>7 We discuss the incorporation of this method into the "active contour" framework where a shape-subspace is used constrain shape variation. [sent-11, score-0.032]
</p><p>8 1 Introduction Tracking objects with flexible shapes in video sequences is currently an important topic in the vision community. [sent-12, score-0.37]
</p><p>9 Methods include curve fitting [9], layered models [1, 2, 3], Bayesian reconstruction of 3-D models from video[6], and active contour models [10, 14, 15]. [sent-13, score-0.576]
</p><p>10 This problem is amplified when using real data where edge noise can prevent the fit of the contour to the desired object outline. [sent-16, score-0.442]
</p><p>11 [10] introduced a probabilistic framework for curve fitting and tracking. [sent-18, score-0.094]
</p><p>12 Instead of proposing one single best fit for the contour, a probability distribution over contours is found. [sent-19, score-0.223]
</p><p>13 The distribution is represented as a particle set where each particle represents one contour shape. [sent-20, score-1.332]
</p><p>14 Inference in these "active contour" models is accomplished using particle filtering. [sent-21, score-0.497]
</p><p>15 In the "active contour" method, a probabilistic dynamic system is used to model the distribution over the outline of the object (the contour) yt and the observations Zt at time t. [sent-22, score-0.163]
</p><p>16 The outline of an object is tracked through successive frames in a video by using a particle  (a)  (b) . [sent-24, score-0.81]
</p><p>17 Figure 1: (a) Condensation with Gaussian dynamics (result for best a = 2 shown) applied to a video sequence. [sent-46, score-0.113]
</p><p>18 The 200 contours corresponding to 200 particles fail to track the complex outline of the hand. [sent-47, score-0.629]
</p><p>19 The pictures show every 24th frame of a 211-frame sequence. [sent-48, score-0.107]
</p><p>20 (b) Metropolis updates with only 12 particles keep the contours on track. [sent-49, score-0.604]
</p><p>21 At each step, 4 iterations of Metropolis updates are applied with a = 3. [sent-50, score-0.18]
</p><p>22 Each particle Xn represents single contour Y 1 that approximates the outline of the object. [sent-52, score-0.923]
</p><p>23 For any given frame, a set of particles represents the probability distribution over positions and shapes of an object. [sent-53, score-0.426]
</p><p>24 In order to find the likelihood of an observation Zt, given a particle X n , lines perpendicular to the contour are examined and edges are detected. [sent-54, score-0.915]
</p><p>25 A variety of distributions can be used to model the likelihood of the edge positions along each line. [sent-55, score-0.115]
</p><p>26 We assume that the position of the edge belonging to the object is drawn from a Gaussian with mean position at the intersection of the contour and the measurement line Y(Sm) and the positions of the other edges are drawn from a Poisson distribution. [sent-56, score-0.686]
</p><p>27 lNotation: We will use Y to refer to a curve, parameterized by x, and yes) for a particular point on the curve. [sent-60, score-0.028]
</p><p>28 x refers to a particle consisting of subspace parameters, or in our case, control points. [sent-61, score-0.734]
</p><p>29 n indexes a particle in a particle set, i indexes a component of a particle (i. [sent-62, score-1.547]
</p><p>30 a single control point), m indexes measurement lines and t is used as a frame index  where q is the probability of not observing the edge, and A is the rate of the Poisson process. [sent-64, score-0.432]
</p><p>31 A multitude of measurement lines is used along the contour, and (assuming independence) the contour likelihood is  p(Zlxn)  = IIP(ZrnIXn)  (2)  M  where m E M is the set of measurement lines. [sent-66, score-0.589]
</p><p>32 As mentioned, in the condensation algorithm, a particle set is used to represent the distribution of contours. [sent-67, score-0.766]
</p><p>33 Starting from an initial distribution, a new distribution for a successive frame is produced by propagating each particle using the system dynamics P(xtlxt-t} . [sent-68, score-0.627]
</p><p>34 Now the observation likelihood P(Ztlxt) is calculated for each particle, and the particle set is resampled with replacement, using the likelihoods as weights. [sent-69, score-0.531]
</p><p>35 The resulting set of particles approximates the posterior distribution at time t and is then propagated to the next frame. [sent-70, score-0.28]
</p><p>36 Figure l(a) shows the results of using condensation with 200 particles. [sent-71, score-0.269]
</p><p>37 Intuitively, the reason condensation fails is that it is highly unlikely to draw a particle that has raised control points over the four fingers , while keeping the remainder fixed. [sent-73, score-1.109]
</p><p>38 Figure 1(b) shows the result of using Metropolis updates and 12 particles (equivalent amount of computation). [sent-74, score-0.375]
</p><p>39 2  Keeping contours on track using Metropolis updates  To reduce the dimensionality of the inference, a subspace is often used. [sent-75, score-0.557]
</p><p>40 For example, a fixed shape is only allowed horizontal and vertical translation. [sent-76, score-0.032]
</p><p>41 Using a subspace reduces the size of the required particle set, allowing for successful tracking using standard condensation. [sent-77, score-0.763]
</p><p>42 If the object can deform, a subspace that captures the allowed deformations may be used [15]. [sent-78, score-0.226]
</p><p>43 In order to learn such a subspace, a large amount of training samples are used, which are supplied by hand fitting contour shapes to a large number of frames. [sent-80, score-0.504]
</p><p>44 However, even moderately detailed contours (say, the outline of a hand) will have many control points that interact in complex ways, making subspace modeling difficult or impractical. [sent-81, score-0.651]
</p><p>45 A new particle is drawn from a proposal density Q(X'; Xt) , where in our case, Xt is a particle (i. [sent-84, score-1.01]
</p><p>46 a set of control points) at time t, and x' is a tentative new particle produced by perturbing a subset of the control points. [sent-86, score-0.737]
</p><p>47 (3)  (J'  We then calculate  (4) where p(Xt IXt-l)p(Zt IXt) is proportional to the posterior probability of observing the contour in that position. [sent-89, score-0.364]
</p><p>48 Metropolis sampling can be used in the framework of particle propagation in two ways. [sent-93, score-0.475]
</p><p>49 It can either be used to fit splines around contours of a training set that is used to construct a shape subspace, e. [sent-94, score-0.309]
</p><p>50 by PCA, or it can also be used to refine the shapes of the subspace to the actual data during tracking. [sent-96, score-0.219]
</p><p>51 2  B-splines  B-splines or basis function splines are parametric curves, defined as follows:  Y(s)  = B(s)C  (5)  where Y (s) is a two dimensional vector consisting of the 2-D coordinates of a point on the curve, B(s) is a matrix of polynomial basis functions, and C is a vector of control points. [sent-98, score-0.307]
</p><p>52 In other words, a point along the curve Y (s) is a weighted sum of the values of the basis functions B(s) for a particular value of s, where the weights are given by the values of C. [sent-99, score-0.108]
</p><p>53 The basis functions of b-splines have the characteristic that they are non-zero over a limited range of s. [sent-100, score-0.033]
</p><p>54 Thus a particular control point will only affect a portion of the curve. [sent-101, score-0.193]
</p><p>55 For regular b-splines of order 4 (the basis functions are 3rd degree polynomials), a single control point will only affect Y (s) over a range of s of length 4. [sent-102, score-0.226]
</p><p>56 (Xi), where i indexes the component of x that has been altered), Y(Sm) is affected by at most 4 control points (fewer towards the ends). [sent-104, score-0.228]
</p><p>57 As mentioned before, a detailed contour can have a large number of control points, and thus high dimensionality and so it is common to use a subspace. [sent-105, score-0.553]
</p><p>58 In this case C can be written as C = W x + Co where W defines a linear subspace and Co is the template of control points, and x represents perturbations from the template in the subspace. [sent-106, score-0.345]
</p><p>59 In this work we examine unconstrained models, where no prior knowledge about the deformations or dynamics of the object are presumed. [sent-107, score-0.098]
</p><p>60 In this case W is the identity matrix, Co = 0, and x are the actual coordinates of the control points. [sent-108, score-0.164]
</p><p>61 3  Metropolis updates in condensation  The new algorithm consists of two steps: a Metropolis step, followed by a resampling step. [sent-111, score-0.386]
</p><p>62 Iterate over control points: • For one control point at a time, draw a proposal particle by drawing a new control point x~ from a 2-D Gaussian centered at the current control point Xt ,i, Eq. [sent-113, score-1.173]
</p><p>63 • Calculate the observation likelihood for the new control point, Eq. [sent-115, score-0.187]
</p><p>64 Get next image in video If the particle distribution at t - 1 reflects P(xt-lIZl, . [sent-120, score-0.61]
</p><p>65 , Zt-t}, the Metropolis updates will converge to P(XtIZl, . [sent-123, score-0.117]
</p><p>66 As mentioned above, the affect of altering the position of a control point is to change the shape of the contour locally since the basis functions have limited support. [sent-127, score-0.654]
</p><p>67 Thus, when evaluating p(x~lxt-t}p(ZtlxD for a proposed particle, we only need to reexamine measurement lines and evaluate p(zm,t Ix~ ,t) for lines in the effected interval and similarly for p(x~,t IXn,t-l). [sent-128, score-0.179]
</p><p>68 The computation eM required to update a single particle using metropolis, compared to condensation is eM = o· it . [sent-130, score-0.773]
</p><p>69 ec where 0 is the order of the b-spline, it is the number of iterations, and ec is the number of computations required to update a particle using condensation. [sent-131, score-0.556]
</p><p>70 Thus, in the case offourth order splines such as the ones we use, the increase in computation for a single particle is only four for a single iteration, and eight for two iterations. [sent-132, score-0.524]
</p><p>71 However, we have seen that far fewer particles are required. [sent-133, score-0.258]
</p><p>72 Figure 2: The behavior of the algorithm with Metropolis updates is shown at frame 100 (t = 100) as a function of iterations and u. [sent-134, score-0.287]
</p><p>73 the ratio of rejected proposal particles to the total number of proposed particles) is shown as a bar on the right side of each image. [sent-138, score-0.408]
</p><p>74 3 Results We tested our algorithm on the video sequence shown in Figure 1. [sent-139, score-0.113]
</p><p>75 Such high dimensionality is required for the detailed contours required to properly outline the fingers of the hand. [sent-142, score-0.494]
</p><p>76 This allows us to contrast the performance of using Metropolis updates and standard condensation, for the scenarios of interest, i. [sent-146, score-0.117]
</p><p>77 Figure l(b) shows the results for the Metropolis updates for 12 particles, 4 iterations and u = 3. [sent-149, score-0.18]
</p><p>78 The figure shows every 24th frame from frame 1 to frame 211. [sent-150, score-0.321]
</p><p>79 The outline of the splayed fingers is tracked very successfully. [sent-151, score-0.236]
</p><p>80 Figure l(a) shows every 24th frame for the condensation algorithm of equivalent complexity, using 200 particles and u = 2. [sent-152, score-0.634]
</p><p>81 As can be seen, the little finger is tracked moderately well. [sent-154, score-0.193]
</p><p>82 However the other parts of the hand are very poorly tracked. [sent-155, score-0.063]
</p><p>83 For lower values of u the contour distribution did not track  the hand, but stayed in roughly the position of the initial contour distribution. [sent-156, score-0.811]
</p><p>84 For higher values of 0', the contour looped around in the general area of the fingers. [sent-157, score-0.365]
</p><p>85 Figure 2 shows the contour distribution for frame 100 and 12 particles, for different numbers of iterations and values of 0'. [sent-158, score-0.53]
</p><p>86 When 0' = 1 and 2 the contour distribution does not keep up with the deformation. [sent-159, score-0.388]
</p><p>87 For 0' = 4 the contour is correctly tracked except for the case of a single iteration. [sent-160, score-0.396]
</p><p>88 the ratio of rejected proposal particles to the total number of proposed particles) is shown as a bar on the right side of each image. [sent-163, score-0.408]
</p><p>89 Notice that the general trend is that rejection ratio increases as 0' increases, and decreases as the number of iterations is increased (due to a smaller 0' at each step). [sent-164, score-0.172]
</p><p>90 In the case of condensation, Gaussian noise is added to each control point at each time step. [sent-166, score-0.159]
</p><p>91 One particle may be correctly positioned for the little finger and poorly positioned for the forefinger, whereas an other particle may be well positioned around the forefinger and poorly positioned around the little finger. [sent-167, score-1.626]
</p><p>92 In order to track the deformation of the hand, some particles are required that track both the little finger and the forefinger (and all other parts too). [sent-168, score-0.614]
</p><p>93 In contrast the Metropolis updates are likely to reject particles that are locally worse than the current particle, but accept local improvements. [sent-169, score-0.441]
</p><p>94 It should be noted that for lower dimensional problems, the increase in tracking performance is not as dramatic. [sent-170, score-0.131]
</p><p>95 in the case of tracking a rotating head, using a 12 control point b-spline, the two algorithms performed comparably. [sent-173, score-0.29]
</p><p>96 We are also investigating other sequences and groupings of control points for generating proposal particles, and ways of using subspace models in combination with Metropolis updates. [sent-175, score-0.377]
</p><p>97 In this paper we showed how Metropolis updates can be used to keep highly flexible active contours on track, and an efficient implementation strategy was presented. [sent-176, score-0.517]
</p><p>98 For high dimensional problems which are common for detailed shapes, the new algorithm presented produces dramatically better results than standard condensation. [sent-177, score-0.06]
</p><p>99 Blake "A probabilistic exclusion principle for tracking multiple objects" Proc. [sent-243, score-0.131]
</p><p>100 Blake "ICONDENSATION: Unifying low-level and high-level tracking in a stochastic framework" Proc. [sent-248, score-0.131]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('particle', 0.475), ('metropolis', 0.367), ('contour', 0.338), ('condensation', 0.269), ('particles', 0.258), ('contours', 0.201), ('blake', 0.135), ('tracking', 0.131), ('control', 0.131), ('subspace', 0.128), ('updates', 0.117), ('video', 0.113), ('frame', 0.107), ('sm', 0.096), ('shapes', 0.091), ('fingers', 0.09), ('positioned', 0.09), ('outline', 0.088), ('measurement', 0.087), ('track', 0.082), ('active', 0.078), ('vision', 0.077), ('finger', 0.067), ('forefinger', 0.067), ('iterations', 0.063), ('flexible', 0.063), ('xt', 0.061), ('indexes', 0.061), ('zt', 0.061), ('proposal', 0.06), ('tracked', 0.058), ('con', 0.055), ('object', 0.053), ('isard', 0.052), ('rejection', 0.052), ('edge', 0.051), ('splines', 0.049), ('keeping', 0.048), ('curve', 0.047), ('fitting', 0.047), ('lines', 0.046), ('deform', 0.045), ('deformations', 0.045), ('snakes', 0.045), ('waterloo', 0.045), ('intersection', 0.039), ('maccormick', 0.039), ('moderately', 0.039), ('reject', 0.039), ('points', 0.036), ('co', 0.036), ('ratio', 0.035), ('poorly', 0.035), ('kleen', 0.035), ('affect', 0.034), ('freeman', 0.034), ('coordinates', 0.033), ('positions', 0.033), ('basis', 0.033), ('dramatically', 0.032), ('template', 0.032), ('rejected', 0.032), ('shape', 0.032), ('position', 0.031), ('likelihood', 0.031), ('draw', 0.03), ('ixt', 0.03), ('carlo', 0.03), ('monte', 0.03), ('highly', 0.03), ('little', 0.029), ('required', 0.029), ('dimensionality', 0.029), ('point', 0.028), ('detailed', 0.028), ('hand', 0.028), ('keep', 0.028), ('around', 0.027), ('accept', 0.027), ('mentioned', 0.027), ('ec', 0.026), ('tt', 0.026), ('european', 0.026), ('objects', 0.026), ('calculate', 0.026), ('observation', 0.025), ('muller', 0.025), ('poisson', 0.024), ('computer', 0.024), ('line', 0.023), ('oo', 0.023), ('bar', 0.023), ('conference', 0.023), ('inference', 0.023), ('successive', 0.023), ('increases', 0.022), ('models', 0.022), ('distribution', 0.022), ('ieee', 0.022), ('represents', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="72-tfidf-1" href="./nips-2000-Keeping_Flexible_Active_Contours_on_Track_using_Metropolis_Updates.html">72 nips-2000-Keeping Flexible Active Contours on Track using Metropolis Updates</a></p>
<p>Author: Trausti T. Kristjansson, Brendan J. Frey</p><p>Abstract: Condensation, a form of likelihood-weighted particle filtering, has been successfully used to infer the shapes of highly constrained</p><p>2 0.26623157 <a title="72-tfidf-2" href="./nips-2000-The_Unscented_Particle_Filter.html">137 nips-2000-The Unscented Particle Filter</a></p>
<p>Author: Rudolph van der Merwe, Arnaud Doucet, Nando de Freitas, Eric A. Wan</p><p>Abstract: In this paper, we propose a new particle filter based on sequential importance sampling. The algorithm uses a bank of unscented filters to obtain the importance proposal distribution. This proposal has two very</p><p>3 0.12938567 <a title="72-tfidf-3" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>Author: Dirk Ormoneit, Hedvig Sidenbladh, Michael J. Black, Trevor Hastie</p><p>Abstract: We present methods for learning and tracking human motion in video. We estimate a statistical model of typical activities from a large set of 3D periodic human motion data by segmenting these data automatically into</p><p>4 0.1244299 <a title="72-tfidf-4" href="./nips-2000-A_Support_Vector_Method_for_Clustering.html">12 nips-2000-A Support Vector Method for Clustering</a></p>
<p>Author: Asa Ben-Hur, David Horn, Hava T. Siegelmann, Vladimir Vapnik</p><p>Abstract: We present a novel method for clustering using the support vector machine approach. Data points are mapped to a high dimensional feature space, where support vectors are used to define a sphere enclosing them. The boundary of the sphere forms in data space a set of closed contours containing the data. Data points enclosed by each contour are defined as a cluster. As the width parameter of the Gaussian kernel is decreased, these contours fit the data more tightly and splitting of contours occurs. The algorithm works by separating clusters according to valleys in the underlying probability distribution, and thus clusters can take on arbitrary geometrical shapes. As in other SV algorithms, outliers can be dealt with by introducing a soft margin constant leading to smoother cluster boundaries. The structure of the data is explored by varying the two parameters. We investigate the dependence of our method on these parameters and apply it to several data sets.</p><p>5 0.11420581 <a title="72-tfidf-5" href="./nips-2000-Beyond_Maximum_Likelihood_and_Density_Estimation%3A_A_Sample-Based_Criterion_for_Unsupervised_Learning_of_Complex_Models.html">31 nips-2000-Beyond Maximum Likelihood and Density Estimation: A Sample-Based Criterion for Unsupervised Learning of Complex Models</a></p>
<p>Author: Sepp Hochreiter, Michael Mozer</p><p>Abstract: The goal of many unsupervised learning procedures is to bring two probability distributions into alignment. Generative models such as Gaussian mixtures and Boltzmann machines can be cast in this light, as can recoding models such as ICA and projection pursuit. We propose a novel sample-based error measure for these classes of models, which applies even in situations where maximum likelihood (ML) and probability density estimation-based formulations cannot be applied, e.g., models that are nonlinear or have intractable posteriors. Furthermore, our sample-based error measure avoids the difficulties of approximating a density function. We prove that with an unconstrained model, (1) our approach converges on the correct solution as the number of samples goes to infinity, and (2) the expected solution of our approach in the generative framework is the ML solution. Finally, we evaluate our approach via simulations of linear and nonlinear models on mixture of Gaussians and ICA problems. The experiments show the broad applicability and generality of our approach. 1</p><p>6 0.088558897 <a title="72-tfidf-6" href="./nips-2000-Learning_Joint_Statistical_Models_for_Audio-Visual_Fusion_and_Segregation.html">78 nips-2000-Learning Joint Statistical Models for Audio-Visual Fusion and Segregation</a></p>
<p>7 0.085315175 <a title="72-tfidf-7" href="./nips-2000-Machine_Learning_for_Video-Based_Rendering.html">83 nips-2000-Machine Learning for Video-Based Rendering</a></p>
<p>8 0.081613235 <a title="72-tfidf-8" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>9 0.074696802 <a title="72-tfidf-9" href="./nips-2000-Feature_Correspondence%3A_A_Markov_Chain_Monte_Carlo_Approach.html">53 nips-2000-Feature Correspondence: A Markov Chain Monte Carlo Approach</a></p>
<p>10 0.073372595 <a title="72-tfidf-10" href="./nips-2000-Shape_Context%3A_A_New_Descriptor_for_Shape_Matching_and_Object_Recognition.html">117 nips-2000-Shape Context: A New Descriptor for Shape Matching and Object Recognition</a></p>
<p>11 0.058416147 <a title="72-tfidf-11" href="./nips-2000-Probabilistic_Semantic_Video_Indexing.html">103 nips-2000-Probabilistic Semantic Video Indexing</a></p>
<p>12 0.058045782 <a title="72-tfidf-12" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>13 0.056812044 <a title="72-tfidf-13" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>14 0.056453779 <a title="72-tfidf-14" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>15 0.054713801 <a title="72-tfidf-15" href="./nips-2000-Learning_Switching_Linear_Models_of_Human_Motion.html">80 nips-2000-Learning Switching Linear Models of Human Motion</a></p>
<p>16 0.050012577 <a title="72-tfidf-16" href="./nips-2000-Bayesian_Video_Shot_Segmentation.html">30 nips-2000-Bayesian Video Shot Segmentation</a></p>
<p>17 0.049957152 <a title="72-tfidf-17" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>18 0.049520306 <a title="72-tfidf-18" href="./nips-2000-The_Manhattan_World_Assumption%3A_Regularities_in_Scene_Statistics_which_Enable_Bayesian_Inference.html">135 nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</a></p>
<p>19 0.048130028 <a title="72-tfidf-19" href="./nips-2000-Stability_and_Noise_in_Biochemical_Switches.html">125 nips-2000-Stability and Noise in Biochemical Switches</a></p>
<p>20 0.046927311 <a title="72-tfidf-20" href="./nips-2000-Accumulator_Networks%3A_Suitors_of_Local_Probability_Propagation.html">15 nips-2000-Accumulator Networks: Suitors of Local Probability Propagation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.172), (1, -0.062), (2, 0.111), (3, 0.087), (4, -0.04), (5, 0.03), (6, 0.167), (7, 0.095), (8, -0.103), (9, -0.077), (10, 0.07), (11, 0.092), (12, 0.163), (13, -0.075), (14, -0.139), (15, -0.439), (16, 0.064), (17, -0.064), (18, 0.016), (19, -0.08), (20, -0.02), (21, -0.017), (22, -0.088), (23, 0.138), (24, 0.078), (25, -0.017), (26, 0.042), (27, 0.167), (28, 0.096), (29, 0.123), (30, -0.112), (31, -0.043), (32, 0.059), (33, -0.128), (34, 0.04), (35, -0.035), (36, -0.065), (37, -0.071), (38, -0.189), (39, -0.088), (40, -0.111), (41, 0.027), (42, 0.0), (43, 0.125), (44, -0.047), (45, -0.137), (46, 0.07), (47, 0.058), (48, -0.075), (49, -0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97101277 <a title="72-lsi-1" href="./nips-2000-Keeping_Flexible_Active_Contours_on_Track_using_Metropolis_Updates.html">72 nips-2000-Keeping Flexible Active Contours on Track using Metropolis Updates</a></p>
<p>Author: Trausti T. Kristjansson, Brendan J. Frey</p><p>Abstract: Condensation, a form of likelihood-weighted particle filtering, has been successfully used to infer the shapes of highly constrained</p><p>2 0.76388943 <a title="72-lsi-2" href="./nips-2000-The_Unscented_Particle_Filter.html">137 nips-2000-The Unscented Particle Filter</a></p>
<p>Author: Rudolph van der Merwe, Arnaud Doucet, Nando de Freitas, Eric A. Wan</p><p>Abstract: In this paper, we propose a new particle filter based on sequential importance sampling. The algorithm uses a bank of unscented filters to obtain the importance proposal distribution. This proposal has two very</p><p>3 0.38934788 <a title="72-lsi-3" href="./nips-2000-Beyond_Maximum_Likelihood_and_Density_Estimation%3A_A_Sample-Based_Criterion_for_Unsupervised_Learning_of_Complex_Models.html">31 nips-2000-Beyond Maximum Likelihood and Density Estimation: A Sample-Based Criterion for Unsupervised Learning of Complex Models</a></p>
<p>Author: Sepp Hochreiter, Michael Mozer</p><p>Abstract: The goal of many unsupervised learning procedures is to bring two probability distributions into alignment. Generative models such as Gaussian mixtures and Boltzmann machines can be cast in this light, as can recoding models such as ICA and projection pursuit. We propose a novel sample-based error measure for these classes of models, which applies even in situations where maximum likelihood (ML) and probability density estimation-based formulations cannot be applied, e.g., models that are nonlinear or have intractable posteriors. Furthermore, our sample-based error measure avoids the difficulties of approximating a density function. We prove that with an unconstrained model, (1) our approach converges on the correct solution as the number of samples goes to infinity, and (2) the expected solution of our approach in the generative framework is the ML solution. Finally, we evaluate our approach via simulations of linear and nonlinear models on mixture of Gaussians and ICA problems. The experiments show the broad applicability and generality of our approach. 1</p><p>4 0.35757598 <a title="72-lsi-4" href="./nips-2000-A_Support_Vector_Method_for_Clustering.html">12 nips-2000-A Support Vector Method for Clustering</a></p>
<p>Author: Asa Ben-Hur, David Horn, Hava T. Siegelmann, Vladimir Vapnik</p><p>Abstract: We present a novel method for clustering using the support vector machine approach. Data points are mapped to a high dimensional feature space, where support vectors are used to define a sphere enclosing them. The boundary of the sphere forms in data space a set of closed contours containing the data. Data points enclosed by each contour are defined as a cluster. As the width parameter of the Gaussian kernel is decreased, these contours fit the data more tightly and splitting of contours occurs. The algorithm works by separating clusters according to valleys in the underlying probability distribution, and thus clusters can take on arbitrary geometrical shapes. As in other SV algorithms, outliers can be dealt with by introducing a soft margin constant leading to smoother cluster boundaries. The structure of the data is explored by varying the two parameters. We investigate the dependence of our method on these parameters and apply it to several data sets.</p><p>5 0.33971241 <a title="72-lsi-5" href="./nips-2000-Feature_Correspondence%3A_A_Markov_Chain_Monte_Carlo_Approach.html">53 nips-2000-Feature Correspondence: A Markov Chain Monte Carlo Approach</a></p>
<p>Author: Frank Dellaert, Steven M. Seitz, Sebastian Thrun, Charles E. Thorpe</p><p>Abstract: When trying to recover 3D structure from a set of images, the most difficult problem is establishing the correspondence between the measurements. Most existing approaches assume that features can be tracked across frames, whereas methods that exploit rigidity constraints to facilitate matching do so only under restricted camera motion. In this paper we propose a Bayesian approach that avoids the brittleness associated with singling out one</p><p>6 0.32015172 <a title="72-lsi-6" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>7 0.26476133 <a title="72-lsi-7" href="./nips-2000-Bayesian_Video_Shot_Segmentation.html">30 nips-2000-Bayesian Video Shot Segmentation</a></p>
<p>8 0.24208869 <a title="72-lsi-8" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>9 0.23342539 <a title="72-lsi-9" href="./nips-2000-Machine_Learning_for_Video-Based_Rendering.html">83 nips-2000-Machine Learning for Video-Based Rendering</a></p>
<p>10 0.20119436 <a title="72-lsi-10" href="./nips-2000-Probabilistic_Semantic_Video_Indexing.html">103 nips-2000-Probabilistic Semantic Video Indexing</a></p>
<p>11 0.18175602 <a title="72-lsi-11" href="./nips-2000-Shape_Context%3A_A_New_Descriptor_for_Shape_Matching_and_Object_Recognition.html">117 nips-2000-Shape Context: A New Descriptor for Shape Matching and Object Recognition</a></p>
<p>12 0.16792174 <a title="72-lsi-12" href="./nips-2000-Sequentially_Fitting_%60%60Inclusive%27%27_Trees_for_Inference_in_Noisy-OR_Networks.html">115 nips-2000-Sequentially Fitting ``Inclusive'' Trees for Inference in Noisy-OR Networks</a></p>
<p>13 0.16302145 <a title="72-lsi-13" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>14 0.16123201 <a title="72-lsi-14" href="./nips-2000-Learning_Joint_Statistical_Models_for_Audio-Visual_Fusion_and_Segregation.html">78 nips-2000-Learning Joint Statistical Models for Audio-Visual Fusion and Segregation</a></p>
<p>15 0.15907227 <a title="72-lsi-15" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>16 0.15879758 <a title="72-lsi-16" href="./nips-2000-Efficient_Learning_of_Linear_Perceptrons.html">44 nips-2000-Efficient Learning of Linear Perceptrons</a></p>
<p>17 0.15348397 <a title="72-lsi-17" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>18 0.15146025 <a title="72-lsi-18" href="./nips-2000-One_Microphone_Source_Separation.html">96 nips-2000-One Microphone Source Separation</a></p>
<p>19 0.14735401 <a title="72-lsi-19" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>20 0.14470783 <a title="72-lsi-20" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.034), (17, 0.114), (32, 0.041), (33, 0.033), (45, 0.346), (48, 0.014), (55, 0.037), (62, 0.048), (65, 0.025), (67, 0.05), (75, 0.013), (76, 0.032), (79, 0.032), (81, 0.053), (90, 0.032), (91, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81894535 <a title="72-lda-1" href="./nips-2000-Keeping_Flexible_Active_Contours_on_Track_using_Metropolis_Updates.html">72 nips-2000-Keeping Flexible Active Contours on Track using Metropolis Updates</a></p>
<p>Author: Trausti T. Kristjansson, Brendan J. Frey</p><p>Abstract: Condensation, a form of likelihood-weighted particle filtering, has been successfully used to infer the shapes of highly constrained</p><p>2 0.73567361 <a title="72-lda-2" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: The concept of averaging over classifiers is fundamental to the Bayesian analysis of learning. Based on this viewpoint, it has recently been demonstrated for linear classifiers that the centre of mass of version space (the set of all classifiers consistent with the training set) - also known as the Bayes point - exhibits excellent generalisation abilities. However, the billiard algorithm as presented in [4] is restricted to small sample size because it requires o (m 2 ) of memory and 0 (N . m2 ) computational steps where m is the number of training patterns and N is the number of random draws from the posterior distribution. In this paper we present a method based on the simple perceptron learning algorithm which allows to overcome this algorithmic drawback. The method is algorithmically simple and is easily extended to the multi-class case. We present experimental results on the MNIST data set of handwritten digits which show that Bayes point machines (BPMs) are competitive with the current world champion, the support vector machine. In addition, the computational complexity of BPMs can be tuned by varying the number of samples from the posterior. Finally, rejecting test points on the basis of their (approximative) posterior probability leads to a rapid decrease in generalisation error, e.g. 0.1% generalisation error for a given rejection rate of 10%. 1</p><p>3 0.53985423 <a title="72-lda-3" href="./nips-2000-Stability_and_Noise_in_Biochemical_Switches.html">125 nips-2000-Stability and Noise in Biochemical Switches</a></p>
<p>Author: William Bialek</p><p>Abstract: Many processes in biology, from the regulation of gene expression in bacteria to memory in the brain, involve switches constructed from networks of biochemical reactions. Crucial molecules are present in small numbers, raising questions about noise and stability. Analysis of noise in simple reaction schemes indicates that switches stable for years and switchable in milliseconds can be built from fewer than one hundred molecules. Prospects for direct tests of this prediction, as well as implications, are discussed. 1</p><p>4 0.51430833 <a title="72-lda-4" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>Author: Thore Graepel, Ralf Herbrich</p><p>Abstract: We present an algorithm that samples the hypothesis space of kernel classifiers. Given a uniform prior over normalised weight vectors and a likelihood based on a model of label noise leads to a piecewise constant posterior that can be sampled by the kernel Gibbs sampler (KGS). The KGS is a Markov Chain Monte Carlo method that chooses a random direction in parameter space and samples from the resulting piecewise constant density along the line chosen. The KGS can be used as an analytical tool for the exploration of Bayesian transduction, Bayes point machines, active learning, and evidence-based model selection on small data sets that are contaminated with label noise. For a simple toy example we demonstrate experimentally how a Bayes point machine based on the KGS outperforms an SVM that is incapable of taking into account label noise. 1</p><p>5 0.49582383 <a title="72-lda-5" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: We present a bound on the generalisation error of linear classifiers in terms of a refined margin quantity on the training set. The result is obtained in a PAC- Bayesian framework and is based on geometrical arguments in the space of linear classifiers. The new bound constitutes an exponential improvement of the so far tightest margin bound by Shawe-Taylor et al. [8] and scales logarithmically in the inverse margin. Even in the case of less training examples than input dimensions sufficiently large margins lead to non-trivial bound values and - for maximum margins - to a vanishing complexity term. Furthermore, the classical margin is too coarse a measure for the essential quantity that controls the generalisation error: the volume ratio between the whole hypothesis space and the subset of consistent hypotheses. The practical relevance of the result lies in the fact that the well-known support vector machine is optimal w.r.t. the new bound only if the feature vectors are all of the same length. As a consequence we recommend to use SVMs on normalised feature vectors only - a recommendation that is well supported by our numerical experiments on two benchmark data sets. 1</p><p>6 0.40862557 <a title="72-lda-6" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>7 0.40538019 <a title="72-lda-7" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>8 0.39984792 <a title="72-lda-8" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>9 0.3992998 <a title="72-lda-9" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>10 0.39764959 <a title="72-lda-10" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>11 0.39555717 <a title="72-lda-11" href="./nips-2000-Learning_Segmentation_by_Random_Walks.html">79 nips-2000-Learning Segmentation by Random Walks</a></p>
<p>12 0.39537776 <a title="72-lda-12" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>13 0.39409831 <a title="72-lda-13" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>14 0.39042354 <a title="72-lda-14" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>15 0.39004838 <a title="72-lda-15" href="./nips-2000-Learning_Switching_Linear_Models_of_Human_Motion.html">80 nips-2000-Learning Switching Linear Models of Human Motion</a></p>
<p>16 0.390046 <a title="72-lda-16" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>17 0.38810825 <a title="72-lda-17" href="./nips-2000-Text_Classification_using_String_Kernels.html">130 nips-2000-Text Classification using String Kernels</a></p>
<p>18 0.38784894 <a title="72-lda-18" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>19 0.38560283 <a title="72-lda-19" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>20 0.38459077 <a title="72-lda-20" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
