<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>25 nips-2000-Analysis of Bit Error Probability of Direct-Sequence CDMA Multiuser Demodulators</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-25" href="#">nips2000-25</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>25 nips-2000-Analysis of Bit Error Probability of Direct-Sequence CDMA Multiuser Demodulators</h1>
<br/><p>Source: <a title="nips-2000-25-pdf" href="http://papers.nips.cc/paper/1927-analysis-of-bit-error-probability-of-direct-sequence-cdma-multiuser-demodulators.pdf">pdf</a></p><p>Author: Toshiyuki Tanaka</p><p>Abstract: We analyze the bit error probability of multiuser demodulators for directsequence binary phase-shift-keying (DSIBPSK) CDMA channel with additive gaussian noise. The problem of multiuser demodulation is cast into the finite-temperature decoding problem, and replica analysis is applied to evaluate the performance of the resulting MPM (Marginal Posterior Mode) demodulators, which include the optimal demodulator and the MAP demodulator as special cases. An approximate implementation of demodulators is proposed using analog-valued Hopfield model as a naive mean-field approximation to the MPM demodulators, and its performance is also evaluated by the replica analysis. Results of the performance evaluation shows effectiveness of the optimal demodulator and the mean-field demodulator compared with the conventional one, especially in the cases of small information bit rate and low noise level. 1</p><p>Reference: <a title="nips-2000-25-reference" href="../nips2000_reference/nips-2000-Analysis_of_Bit_Error_Probability_of_Direct-Sequence_CDMA_Multiuser_Demodulators_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp  Abstract We analyze the bit error probability of multiuser demodulators for directsequence binary phase-shift-keying (DSIBPSK) CDMA channel with additive gaussian noise. [sent-3, score-0.644]
</p><p>2 The problem of multiuser demodulation is cast into the finite-temperature decoding problem, and replica analysis is applied to evaluate the performance of the resulting MPM (Marginal Posterior Mode) demodulators, which include the optimal demodulator and the MAP demodulator as special cases. [sent-4, score-1.98]
</p><p>3 An approximate implementation of demodulators is proposed using analog-valued Hopfield model as a naive mean-field approximation to the MPM demodulators, and its performance is also evaluated by the replica analysis. [sent-5, score-0.399]
</p><p>4 Results of the performance evaluation shows effectiveness of the optimal demodulator and the mean-field demodulator compared with the conventional one, especially in the cases of small information bit rate and low noise level. [sent-6, score-1.901]
</p><p>5 1  Introduction  The CDMA (Code-Division-Multiple-Access) technique [1] is important as a fundamental technology of digital communications systems, such as cellular phones. [sent-7, score-0.039]
</p><p>6 The important applications include realization of spread-spectrum multipoint-to-point communications systems, in which multiple users share the same communication channel. [sent-8, score-0.119]
</p><p>7 In the multipoint-topoint system, each user modulates his/her own information bit sequence using a spreading code sequence before transmitting it, and the receiver uses the same spreading code sequence for demodulation to obtain the original information bit sequence. [sent-9, score-1.265]
</p><p>8 Different users use different spreading code sequences so that the demodulation procedure randomizes and thus suppresses multiple access interference effects of transmitted signal sequences sent from different users. [sent-10, score-0.667]
</p><p>9 Use of Hopfield-type recurrent neural network has been proposed as an implementation of a multiuser demodulator [2]. [sent-12, score-0.908]
</p><p>10 In this paper, we analyze the bit error probability of the neural multiuser demodulator applied to demodulation of DS/BPSK CDMA channel. [sent-13, score-1.287]
</p><p>11 We also take a simplifying assumption, that all the users are completely synchronized with each other, with respect not only to the chip timing but also to the information bit timing. [sent-15, score-0.32]
</p><p>12 We focus on any of the time intervals corresponding to the duration of one information bit. [sent-16, score-0.028]
</p><p>13 Let ~i E {-I, I} be the information bit to be transmitted by user i (i = 1, . [sent-17, score-0.297]
</p><p>14 , N) during the time interval, and P be the number of the spreading code chips (clocks) per information bit. [sent-20, score-0.283]
</p><p>15 For simplicity, the spreading code sequences for the users are assumed to be random bit sequences {'7:; t = 1, . [sent-21, score-0.633]
</p><p>16 User i modulates the information bit ~i by the spreading code sequence and transmits the modulated sequence {~i '7f; t = 1, . [sent-28, score-0.551]
</p><p>17 Assuming that power control [3] is done perfectly so that every transmitted sequences arrive at the receiver with the same intensity, the received signal sequence (after baseband demodulation) is {yl; t = 1, . [sent-32, score-0.179]
</p><p>18 At the receiver side, one has to estimate the information bits {~i} based on the knowledge of the received signal {i} and the spreading code sequences {'7f} for the users. [sent-41, score-0.455]
</p><p>19 The demodulator refers to the system which does this task. [sent-42, score-0.797]
</p><p>20 Accuracy of the estimation depends on what demodulator one uses. [sent-43, score-0.767]
</p><p>21 3, and analytical results for their performance is derived in Sect. [sent-45, score-0.02]
</p><p>22 1  Conventional demodulator  The conventional demodulator (CD) [1-3] estimates the information bit ~i using the spreading code sequence {11:; t = 1, . [sent-48, score-2.078]
</p><p>23 (2)  1= 1  We can rewrite hi as (3)  The second and third terms of the right-hand side represent the effects of multiple access interference and noise, respectively. [sent-52, score-0.077]
</p><p>24 CD would give the correct information bit in the single-user (N = 1), and no noise (V i == 0) case, but estimation may contain some errors in the multiple-user andlor noisy cases. [sent-53, score-0.245]
</p><p>25 2  MAP demodulator  The accuracy of the estimation would be significantly improved if the demodulator knows the spreading code sequences for all N users and makes full use of them by simultaneously estimating the information bits for aLI the users (the multiuser demodulator). [sent-55, score-2.234]
</p><p>26 A common approach to the multiuser demodulation is to use the MAP decoding, which estimates the information bits lSi = ~;} by maximizing the posterior probability p({~;}I{ y l}). [sent-57, score-0.425]
</p><p>27 We call this kind of multiuser demodulator the MAP demodulator 1. [sent-58, score-1.675]
</p><p>28 When we assume uniform prior for the information bits, the posterior probability is explicitly given by p(sl{i }) = Z - I exp(-flsH(sÂ»), (4) where (5)  fl. [sent-59, score-0.06]
</p><p>29 ==  N fa},  s ==  (Si), h  ==  ==  (wij) is the sample covariance of the spreading  =  (hi), and W  ~ I>:11j. [sent-60, score-0.188]
</p><p>30 code sequences, P  Wij  (6)  1= 1  The problem of MAP demodulation thus reduces to the following minimization problem: A  ~  (MAP)  = arg  min  H(s). [sent-61, score-0.208]
</p><p>31 3  MPM demodulator  Although the MAP demodulator is sometimes referred to as "optimal," actually it is not so in terms of the common measure of performance, i. [sent-63, score-1.534]
</p><p>32 , the bit error probability Ph, which is IThe MAP demodulator refers to the same one as what is frequently called the "maximumlikelihood (ML) demodulator" in the literature. [sent-65, score-1.035]
</p><p>33 Then, we can show that the MPM demodulator with /3 = /3s is the optimal one minimizing  P,B(s)  the bit error probability Pb. [sent-67, score-1.042]
</p><p>34 It is a direct consequence of general argument on optimal decoders [5]. [sent-68, score-0.052]
</p><p>35 Note that the MAP demodulator corresponds to the MPM demodulator in the /3 --* +00 limit (the zero-temperature demodulator). [sent-69, score-1.566]
</p><p>36 1  Analysis Conventional demodulator  In the cases where we can assume that Nand P are both large while a == P / N = 0(1), evaluation of the overlap M, and therefore the bit error probability Pb, for those demodulators are possible. [sent-71, score-1.321]
</p><p>37 For CD, simple application of the central limit theorem yields M  = erf  a) ( 2(1+1//3,,) ,  where erf(x)  ==  2 r . [sent-72, score-0.072]
</p><p>38 rn 10 e-  I  2  (11)  (2)  dt  is the error function. [sent-73, score-0.023]
</p><p>39 2  MPM demodulator  For the MPM demodulator with inverse temperature /3, we have used the replica analysis to evaluate the bit error probability Pb. [sent-75, score-1.879]
</p><p>40 }) denotes averaging over the information bits and the noise. [sent-77, score-0.095]
</p><p>41 Evaluation of the overlap M (within replica-symmetric (RS) ansatz) requires solving saddle-point problem for scalar variables {m, q, E, F}. [sent-78, score-0.039]
</p><p>42 The saddle-point equations are m  =  f  Dz tanh(#z  + E),  E= _ _ _ _ a_/3  1 + /3(1 where Dz  q  =  F-  f  - [l  q)'  == 0/ -J2ir)e- z2 / 2dz  Dz tanh2 (#z  (3)  a/3 2  + /3(1 -  + E)  [  q)]2  1 1 - 2 m -]  + q + /3s  is the gaussian measure. [sent-79, score-0.035]
</p><p>43 The overlap M is then given by  M  =  f  Dzsgn(#z  + E),  from which Pb is evaluated via (8). [sent-80, score-0.039]
</p><p>44 3  MAP demodulator: Zero-temperature limit  Taking the zero-temperature limit f3 --+ +00 of the result for the MPM demodulator yields the result for the MAP demodulator. [sent-83, score-0.831]
</p><p>45 4  Optimal demodulator: The case f3  = f3s  Letting f3 = f3s in the result for the MPM demodulator gives the optimal demodulator minimizing the bit error probability. [sent-86, score-1.791]
</p><p>46 In this case, it can be shown that m = q and E = F hold for the solutions of the saddle-point equations (13). [sent-87, score-0.035]
</p><p>47 5  Demodulator using naive mean-field approximation  Since solving the MAP or MPM demodulation problem is in general NP complete, we have to consider approximate implementations of those demodulators which are sub-optimal. [sent-89, score-0.452]
</p><p>48 A straightforward choice is the mean-field approximation (MFA) demodulator, which uses the analog-valued Hopfield model as the naive mean-field approximation to the finitetemperature demodulation problem 2 . [sent-90, score-0.225]
</p><p>49 The solution {mi} of the mean-field equations mi  = tanh[f3(- LWijmj +h i )]  (16)  j  gives an approximation to {(. [sent-91, score-0.101]
</p><p>50 }, from which we have the mean-field approximation to the MPM estimates, as (MPA) (17) ~i = sgn(mi) . [sent-93, score-0.023]
</p><p>51 A  The macroscopic properties of the MFA demodulator can be derived by the replica analysis as well, along the line proposed by Bray et al. [sent-94, score-0.862]
</p><p>52 [6] We have derived the following saddlepoint equations: m  =  f  Dz fe z ),  af3 E=-1 + f3x'  x= ~ f Dz zf(z),  q  =  f  Dz [f(z)]2  af32 [ 1 - 2 m -] I F- [l+f3X]2 +q+ f3s '  (18)  where fe z ) is the function defined by fe z ) = tanh [ flz - Ef(z ) + E]. [sent-95, score-0.177]
</p><p>53 The overlap M is then calculated by  M  =  f  Dz sgn(t(zÂ»). [sent-97, score-0.039]
</p><p>54 2The proposal by Kechriotis and Manolakos [2] is to use the Hopfield model for an approximation to the MAP demodulation. [sent-99, score-0.04]
</p><p>55 The proposal in this paper goes beyond theirs in that the analog-valued Hopfield model is used to approximate not the MAP demodulator in the zero-temperature limit but the MPM demodulators directly, including the optimal one. [sent-100, score-1.103]
</p><p>56 10  100  = 20  Figure 2: Bit error probability for various demodulators. [sent-146, score-0.041]
</p><p>57 6  AT instability  The AT instability [7] refers to the bifurcation of a saddle-point solution without replica symmetry from the replica-symmetric one. [sent-148, score-0.182]
</p><p>58 In this paper we follow the usual convention and assume that the first such destabilization occurs in the so-called "replicon mode [8] . [sent-149, score-0.026]
</p><p>59 " As the stability condition of the RS saddle-point solution for the MPM demodulator, we obtain (21) a - E2 D z sech4 (flz + E) = O. [sent-150, score-0.036]
</p><p>60 (22)  The RS solution is stable as long as the left-hand side of (21) or (22) is positive. [sent-152, score-0.057]
</p><p>61 5 Performance evaluation The saddle-point equations (13) and (18) can be solved numerically to evaluate the bit error probability Pb of the MPM demodulator and its naive mean-field approximation, respectively. [sent-153, score-1.142]
</p><p>62 We have investigated four demodulators: the optimal one (f3 = f3s), MAP, MFA (with f3 = f3s, i. [sent-154, score-0.053]
</p><p>63 , the naive mean-field approximation to the optimal one), and CD. [sent-156, score-0.098]
</p><p>64 Increasing a corresponds to relatively lowering the information bit rate, so that Pb should become small as a gets larger, which is in consistent with the general trend observed in Fig. [sent-159, score-0.225]
</p><p>65 The optimal demodulator shows consistently better performance than CD, as expected. [sent-161, score-0.824]
</p><p>66 The MAP demodulator marks almost the same performance as the optimal one (indeed the result of the MAP demodulator is nearly the same as that of the optimal demodulator in the case f3s = 1, so they are indistinguishable from each other in Fig. [sent-162, score-2.395]
</p><p>67 We also found that the performance of the optimal, MAP, and MFA demodulators is signifof the noise is small relative icantly improved in the large-a region when the variance to N, the number of the users. [sent-164, score-0.29]
</p><p>68 For example, in order to achieve practical level of bit error probability, Pb '" 10- 5 say, in the f3s = 1 case the optimal and MAP demodulators allow information bit rate 2 times faster than CD does. [sent-165, score-0.765]
</p><p>69 On the other hand, in the f3s = 20 case they allow information bit rate as much as 20 times faster than CD, which demonstrates that significant process gain is achieved by the optimal and MAP demodulators in such cases. [sent-166, score-0.545]
</p><p>70 The MFA demodulator with fl = fls showed the performance competitive with the optimal one for the fls = 1 case. [sent-168, score-0.969]
</p><p>71 Although the MFA demodulator feU behind the optimal and MAP demodulators in the performance for the fls = 20 case, it still had process gain which allows about 10 times faster information bit rate than CD does. [sent-169, score-1.395]
</p><p>72 Moreover, we observed, using (22), that the RS saddle-point solution for the MFA demodulator with fl = fls was stable with respect to replica symmetry breaking (RSB), and thus RS ansatz was indeed valid for the MFA solution. [sent-170, score-1.017]
</p><p>73 It suggests that the free energy landscape is rather simple for these cases, making it easier for the MFA demodulator to find a good solution. [sent-171, score-0.767]
</p><p>74 This argument provides an explanation as to why finite-temperature analog-valued Hopfield models, proposed heuristically by Kechriotis and Manolakos [2], exhibited better performance in their numerical experiments. [sent-172, score-0.035]
</p><p>75 We also found that the RS saddle-point solution for the optimal demodulator was stable with respect to RSB over the whole range investigated, whereas the solution for the MAP demodulator was found to be unstable. [sent-173, score-1.65]
</p><p>76 This observation suggests the possibility to construct efficient near-optimal demodulators using advanced mean-field approximations, such as the TAP approach [9, 10]. [sent-174, score-0.25]
</p><p>77 Manolakos, "Hopfield neural network implementation of the optimal CDMA multiuser detector," IEEE Trans. [sent-190, score-0.178]
</p><p>78 Yu, "On the 'naive' mean-field equations for spin glasses," J. [sent-214, score-0.073]
</p><p>79 Thouless, "Stability of the Sherrington-Kirkpatrick solution ofa spin glass mode," J. [sent-227, score-0.077]
</p><p>80 Palmer, "Solution of 'Solvable model of a spin glass' ," Phil. [sent-246, score-0.038]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('demodulator', 0.767), ('demodulators', 0.25), ('mpm', 0.202), ('bit', 0.197), ('mfa', 0.188), ('spreading', 0.188), ('cdma', 0.141), ('demodulation', 0.141), ('multiuser', 0.141), ('map', 0.12), ('dz', 0.094), ('cd', 0.082), ('users', 0.079), ('hopfield', 0.068), ('pb', 0.068), ('replica', 0.068), ('bits', 0.067), ('code', 0.067), ('dsibpsk', 0.063), ('fls', 0.063), ('rs', 0.053), ('sequences', 0.051), ('kechriotis', 0.047), ('manolakos', 0.047), ('erf', 0.04), ('fe', 0.04), ('overlap', 0.039), ('spin', 0.038), ('naive', 0.038), ('optimal', 0.037), ('transmitted', 0.037), ('user', 0.035), ('equations', 0.035), ('receiver', 0.034), ('limit', 0.032), ('bray', 0.031), ('fez', 0.031), ('flz', 0.031), ('rsb', 0.031), ('refers', 0.03), ('information', 0.028), ('sgn', 0.027), ('evaluation', 0.027), ('ansatz', 0.027), ('glasses', 0.027), ('macroscopic', 0.027), ('mode', 0.026), ('communications', 0.026), ('tanh', 0.026), ('conventional', 0.024), ('spread', 0.024), ('tanaka', 0.024), ('thouless', 0.024), ('sequence', 0.024), ('approximation', 0.023), ('error', 0.023), ('modulates', 0.023), ('tokyo', 0.023), ('instability', 0.023), ('interference', 0.023), ('solution', 0.022), ('evaluate', 0.021), ('hi', 0.021), ('mi', 0.021), ('noise', 0.02), ('performance', 0.02), ('received', 0.02), ('nand', 0.019), ('fl', 0.019), ('wij', 0.019), ('faster', 0.019), ('stable', 0.019), ('probability', 0.018), ('tap', 0.018), ('temperature', 0.018), ('decoding', 0.018), ('access', 0.017), ('glass', 0.017), ('proposal', 0.017), ('side', 0.016), ('investigated', 0.016), ('respect', 0.016), ('estimates', 0.016), ('numerically', 0.016), ('symmetry', 0.016), ('argument', 0.015), ('channel', 0.015), ('vi', 0.014), ('spectrum', 0.014), ('stability', 0.014), ('rate', 0.014), ('posterior', 0.014), ('communication', 0.014), ('assuming', 0.014), ('sent', 0.013), ('carrier', 0.013), ('culture', 0.013), ('baseband', 0.013), ('cellular', 0.013), ('fischer', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="25-tfidf-1" href="./nips-2000-Analysis_of_Bit_Error_Probability_of_Direct-Sequence_CDMA_Multiuser_Demodulators.html">25 nips-2000-Analysis of Bit Error Probability of Direct-Sequence CDMA Multiuser Demodulators</a></p>
<p>Author: Toshiyuki Tanaka</p><p>Abstract: We analyze the bit error probability of multiuser demodulators for directsequence binary phase-shift-keying (DSIBPSK) CDMA channel with additive gaussian noise. The problem of multiuser demodulation is cast into the finite-temperature decoding problem, and replica analysis is applied to evaluate the performance of the resulting MPM (Marginal Posterior Mode) demodulators, which include the optimal demodulator and the MAP demodulator as special cases. An approximate implementation of demodulators is proposed using analog-valued Hopfield model as a naive mean-field approximation to the MPM demodulators, and its performance is also evaluated by the replica analysis. Results of the performance evaluation shows effectiveness of the optimal demodulator and the mean-field demodulator compared with the conventional one, especially in the cases of small information bit rate and low noise level. 1</p><p>2 0.09144529 <a title="25-tfidf-2" href="./nips-2000-Error-correcting_Codes_on_a_Bethe-like_Lattice.html">47 nips-2000-Error-correcting Codes on a Bethe-like Lattice</a></p>
<p>Author: Renato Vicente, David Saad, Yoshiyuki Kabashima</p><p>Abstract: We analyze Gallager codes by employing a simple mean-field approximation that distorts the model geometry and preserves important interactions between sites. The method naturally recovers the probability propagation decoding algorithm as an extremization of a proper free-energy. We find a thermodynamic phase transition that coincides with information theoretical upper-bounds and explain the practical code performance in terms of the free-energy landscape.</p><p>3 0.051816273 <a title="25-tfidf-3" href="./nips-2000-Stagewise_Processing_in_Error-correcting_Codes_and_Image_Restoration.html">126 nips-2000-Stagewise Processing in Error-correcting Codes and Image Restoration</a></p>
<p>Author: K. Y. Michael Wong, Hidetoshi Nishimori</p><p>Abstract: We introduce stagewise processing in error-correcting codes and image restoration, by extracting information from the former stage and using it selectively to improve the performance of the latter one. Both mean-field analysis using the cavity method and simulations show that it has the advantage of being robust against uncertainties in hyperparameter estimation. 1</p><p>4 0.037772812 <a title="25-tfidf-4" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>Author: Koby Crammer, Yoram Singer</p><p>Abstract: Output coding is a general method for solving multiclass problems by reducing them to multiple binary classification problems. Previous research on output coding has employed, almost solely, predefined discrete codes. We describe an algorithm that improves the performance of output codes by relaxing them to continuous codes. The relaxation procedure is cast as an optimization problem and is reminiscent of the quadratic program for support vector machines. We describe experiments with the proposed algorithm, comparing it to standard discrete output codes. The experimental results indicate that continuous relaxations of output codes often improve the generalization performance, especially for short codes.</p><p>5 0.034397382 <a title="25-tfidf-5" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>Author: DÃ¶rthe Malzahn, Manfred Opper</p><p>Abstract: Based on a statistical mechanics approach, we develop a method for approximately computing average case learning curves for Gaussian process regression models. The approximation works well in the large sample size limit and for arbitrary dimensionality of the input space. We explain how the approximation can be systematically improved and argue that similar techniques can be applied to general likelihood models. 1</p><p>6 0.030330706 <a title="25-tfidf-6" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>7 0.029334003 <a title="25-tfidf-7" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>8 0.028545711 <a title="25-tfidf-8" href="./nips-2000-Second_Order_Approximations_for_Probability_Models.html">114 nips-2000-Second Order Approximations for Probability Models</a></p>
<p>9 0.026337961 <a title="25-tfidf-9" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>10 0.024710417 <a title="25-tfidf-10" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>11 0.024337849 <a title="25-tfidf-11" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>12 0.023357244 <a title="25-tfidf-12" href="./nips-2000-A_Variational_Mean-Field_Theory_for_Sigmoidal_Belief_Networks.html">14 nips-2000-A Variational Mean-Field Theory for Sigmoidal Belief Networks</a></p>
<p>13 0.022700276 <a title="25-tfidf-13" href="./nips-2000-Algorithmic_Stability_and_Generalization_Performance.html">21 nips-2000-Algorithmic Stability and Generalization Performance</a></p>
<p>14 0.022462744 <a title="25-tfidf-14" href="./nips-2000-The_Early_Word_Catches_the_Weights.html">131 nips-2000-The Early Word Catches the Weights</a></p>
<p>15 0.022352004 <a title="25-tfidf-15" href="./nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">88 nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>16 0.022322861 <a title="25-tfidf-16" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>17 0.021818802 <a title="25-tfidf-17" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>18 0.021772312 <a title="25-tfidf-18" href="./nips-2000-Feature_Correspondence%3A_A_Markov_Chain_Monte_Carlo_Approach.html">53 nips-2000-Feature Correspondence: A Markov Chain Monte Carlo Approach</a></p>
<p>19 0.021669969 <a title="25-tfidf-19" href="./nips-2000-From_Mixtures_of_Mixtures_to_Adaptive_Transform_Coding.html">59 nips-2000-From Mixtures of Mixtures to Adaptive Transform Coding</a></p>
<p>20 0.020347401 <a title="25-tfidf-20" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.079), (1, -0.018), (2, 0.017), (3, -0.005), (4, 0.038), (5, -0.015), (6, 0.0), (7, 0.014), (8, 0.012), (9, 0.02), (10, -0.026), (11, 0.035), (12, -0.061), (13, 0.057), (14, -0.058), (15, 0.036), (16, -0.029), (17, -0.054), (18, 0.11), (19, -0.208), (20, 0.029), (21, -0.003), (22, 0.039), (23, -0.006), (24, -0.043), (25, -0.02), (26, 0.211), (27, 0.015), (28, 0.141), (29, 0.006), (30, 0.049), (31, -0.069), (32, -0.012), (33, -0.063), (34, 0.138), (35, -0.068), (36, -0.092), (37, 0.048), (38, -0.011), (39, -0.021), (40, 0.012), (41, 0.035), (42, -0.044), (43, -0.132), (44, 0.081), (45, 0.024), (46, 0.031), (47, -0.002), (48, -0.109), (49, -0.005)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95018566 <a title="25-lsi-1" href="./nips-2000-Analysis_of_Bit_Error_Probability_of_Direct-Sequence_CDMA_Multiuser_Demodulators.html">25 nips-2000-Analysis of Bit Error Probability of Direct-Sequence CDMA Multiuser Demodulators</a></p>
<p>Author: Toshiyuki Tanaka</p><p>Abstract: We analyze the bit error probability of multiuser demodulators for directsequence binary phase-shift-keying (DSIBPSK) CDMA channel with additive gaussian noise. The problem of multiuser demodulation is cast into the finite-temperature decoding problem, and replica analysis is applied to evaluate the performance of the resulting MPM (Marginal Posterior Mode) demodulators, which include the optimal demodulator and the MAP demodulator as special cases. An approximate implementation of demodulators is proposed using analog-valued Hopfield model as a naive mean-field approximation to the MPM demodulators, and its performance is also evaluated by the replica analysis. Results of the performance evaluation shows effectiveness of the optimal demodulator and the mean-field demodulator compared with the conventional one, especially in the cases of small information bit rate and low noise level. 1</p><p>2 0.73066634 <a title="25-lsi-2" href="./nips-2000-Error-correcting_Codes_on_a_Bethe-like_Lattice.html">47 nips-2000-Error-correcting Codes on a Bethe-like Lattice</a></p>
<p>Author: Renato Vicente, David Saad, Yoshiyuki Kabashima</p><p>Abstract: We analyze Gallager codes by employing a simple mean-field approximation that distorts the model geometry and preserves important interactions between sites. The method naturally recovers the probability propagation decoding algorithm as an extremization of a proper free-energy. We find a thermodynamic phase transition that coincides with information theoretical upper-bounds and explain the practical code performance in terms of the free-energy landscape.</p><p>3 0.66375804 <a title="25-lsi-3" href="./nips-2000-Stagewise_Processing_in_Error-correcting_Codes_and_Image_Restoration.html">126 nips-2000-Stagewise Processing in Error-correcting Codes and Image Restoration</a></p>
<p>Author: K. Y. Michael Wong, Hidetoshi Nishimori</p><p>Abstract: We introduce stagewise processing in error-correcting codes and image restoration, by extracting information from the former stage and using it selectively to improve the performance of the latter one. Both mean-field analysis using the cavity method and simulations show that it has the advantage of being robust against uncertainties in hyperparameter estimation. 1</p><p>4 0.34209868 <a title="25-lsi-4" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>Author: Koby Crammer, Yoram Singer</p><p>Abstract: Output coding is a general method for solving multiclass problems by reducing them to multiple binary classification problems. Previous research on output coding has employed, almost solely, predefined discrete codes. We describe an algorithm that improves the performance of output codes by relaxing them to continuous codes. The relaxation procedure is cast as an optimization problem and is reminiscent of the quadratic program for support vector machines. We describe experiments with the proposed algorithm, comparing it to standard discrete output codes. The experimental results indicate that continuous relaxations of output codes often improve the generalization performance, especially for short codes.</p><p>5 0.27818131 <a title="25-lsi-5" href="./nips-2000-Stability_and_Noise_in_Biochemical_Switches.html">125 nips-2000-Stability and Noise in Biochemical Switches</a></p>
<p>Author: William Bialek</p><p>Abstract: Many processes in biology, from the regulation of gene expression in bacteria to memory in the brain, involve switches constructed from networks of biochemical reactions. Crucial molecules are present in small numbers, raising questions about noise and stability. Analysis of noise in simple reaction schemes indicates that switches stable for years and switchable in milliseconds can be built from fewer than one hundred molecules. Prospects for direct tests of this prediction, as well as implications, are discussed. 1</p><p>6 0.23546982 <a title="25-lsi-6" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>7 0.2090458 <a title="25-lsi-7" href="./nips-2000-Learning_Continuous_Distributions%3A_Simulations_With_Field_Theoretic_Priors.html">76 nips-2000-Learning Continuous Distributions: Simulations With Field Theoretic Priors</a></p>
<p>8 0.20880367 <a title="25-lsi-8" href="./nips-2000-Algorithmic_Stability_and_Generalization_Performance.html">21 nips-2000-Algorithmic Stability and Generalization Performance</a></p>
<p>9 0.20778586 <a title="25-lsi-9" href="./nips-2000-The_Early_Word_Catches_the_Weights.html">131 nips-2000-The Early Word Catches the Weights</a></p>
<p>10 0.19118762 <a title="25-lsi-10" href="./nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">109 nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>11 0.18853229 <a title="25-lsi-11" href="./nips-2000-Robust_Reinforcement_Learning.html">113 nips-2000-Robust Reinforcement Learning</a></p>
<p>12 0.18452935 <a title="25-lsi-12" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>13 0.17867441 <a title="25-lsi-13" href="./nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">124 nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>14 0.17800049 <a title="25-lsi-14" href="./nips-2000-Smart_Vision_Chip_Fabricated_Using_Three_Dimensional_Integration_Technology.html">118 nips-2000-Smart Vision Chip Fabricated Using Three Dimensional Integration Technology</a></p>
<p>15 0.17791151 <a title="25-lsi-15" href="./nips-2000-Four-legged_Walking_Gait_Control_Using_a_Neuromorphic_Chip_Interfaced_to_a_Support_Vector_Learning_Algorithm.html">57 nips-2000-Four-legged Walking Gait Control Using a Neuromorphic Chip Interfaced to a Support Vector Learning Algorithm</a></p>
<p>16 0.17721541 <a title="25-lsi-16" href="./nips-2000-Periodic_Component_Analysis%3A_An_Eigenvalue_Method_for_Representing_Periodic_Structure_in_Speech.html">99 nips-2000-Periodic Component Analysis: An Eigenvalue Method for Representing Periodic Structure in Speech</a></p>
<p>17 0.17547655 <a title="25-lsi-17" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<p>18 0.16205208 <a title="25-lsi-18" href="./nips-2000-Active_Inference_in_Concept_Learning.html">16 nips-2000-Active Inference in Concept Learning</a></p>
<p>19 0.15741692 <a title="25-lsi-19" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>20 0.14262296 <a title="25-lsi-20" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.024), (17, 0.074), (32, 0.016), (33, 0.037), (42, 0.015), (55, 0.02), (62, 0.036), (65, 0.022), (67, 0.034), (69, 0.017), (76, 0.051), (81, 0.019), (90, 0.019), (91, 0.018), (92, 0.021), (97, 0.019), (99, 0.413)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83939302 <a title="25-lda-1" href="./nips-2000-Analysis_of_Bit_Error_Probability_of_Direct-Sequence_CDMA_Multiuser_Demodulators.html">25 nips-2000-Analysis of Bit Error Probability of Direct-Sequence CDMA Multiuser Demodulators</a></p>
<p>Author: Toshiyuki Tanaka</p><p>Abstract: We analyze the bit error probability of multiuser demodulators for directsequence binary phase-shift-keying (DSIBPSK) CDMA channel with additive gaussian noise. The problem of multiuser demodulation is cast into the finite-temperature decoding problem, and replica analysis is applied to evaluate the performance of the resulting MPM (Marginal Posterior Mode) demodulators, which include the optimal demodulator and the MAP demodulator as special cases. An approximate implementation of demodulators is proposed using analog-valued Hopfield model as a naive mean-field approximation to the MPM demodulators, and its performance is also evaluated by the replica analysis. Results of the performance evaluation shows effectiveness of the optimal demodulator and the mean-field demodulator compared with the conventional one, especially in the cases of small information bit rate and low noise level. 1</p><p>2 0.77832752 <a title="25-lda-2" href="./nips-2000-Active_Inference_in_Concept_Learning.html">16 nips-2000-Active Inference in Concept Learning</a></p>
<p>Author: Jonathan D. Nelson, Javier R. Movellan</p><p>Abstract: People are active experimenters, not just passive observers, constantly seeking new information relevant to their goals. A reasonable approach to active information gathering is to ask questions and conduct experiments that maximize the expected information gain, given current beliefs (Fedorov 1972, MacKay 1992, Oaksford & Chater 1994). In this paper we present results on an exploratory experiment designed to study people's active information gathering behavior on a concept learning task (Tenenbaum 2000). The results of the experiment are analyzed in terms of the expected information gain of the questions asked by subjects. In scientific inquiry and in everyday life, people seek out information relevant to perceptual and cognitive tasks. Scientists perform experiments to uncover causal relationships; people saccade to informative areas of visual scenes, turn their head towards surprising sounds, and ask questions to understand the meaning of concepts . Consider a person learning a foreign language, who notices that a particular word,</p><p>3 0.68435669 <a title="25-lda-3" href="./nips-2000-On_Iterative_Krylov-Dogleg_Trust-Region_Steps_for_Solving_Neural_Networks_Nonlinear_Least_Squares_Problems.html">93 nips-2000-On Iterative Krylov-Dogleg Trust-Region Steps for Solving Neural Networks Nonlinear Least Squares Problems</a></p>
<p>Author: Eiji Mizutani, James Demmel</p><p>Abstract: This paper describes a method of dogleg trust-region steps, or restricted Levenberg-Marquardt steps, based on a projection process onto the Krylov subspaces for neural networks nonlinear least squares problems. In particular, the linear conjugate gradient (CG) method works as the inner iterative algorithm for solving the linearized Gauss-Newton normal equation, whereas the outer nonlinear algorithm repeatedly takes so-called</p><p>4 0.27413395 <a title="25-lda-4" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>Author: Sham Kakade, Peter Dayan</p><p>Abstract: Substantial data support a temporal difference (TO) model of dopamine (OA) neuron activity in which the cells provide a global error signal for reinforcement learning. However, in certain circumstances, OA activity seems anomalous under the TO model, responding to non-rewarding stimuli. We address these anomalies by suggesting that OA cells multiplex information about reward bonuses, including Sutton's exploration bonuses and Ng et al's non-distorting shaping bonuses. We interpret this additional role for OA in terms of the unconditional attentional and psychomotor effects of dopamine, having the computational role of guiding exploration. 1</p><p>5 0.26981163 <a title="25-lda-5" href="./nips-2000-The_Early_Word_Catches_the_Weights.html">131 nips-2000-The Early Word Catches the Weights</a></p>
<p>Author: Mark A. Smith, Garrison W. Cottrell, Karen L. Anderson</p><p>Abstract: The strong correlation between the frequency of words and their naming latency has been well documented. However, as early as 1973, the Age of Acquisition (AoA) of a word was alleged to be the actual variable of interest, but these studies seem to have been ignored in most of the literature. Recently, there has been a resurgence of interest in AoA. While some studies have shown that frequency has no effect when AoA is controlled for, more recent studies have found independent contributions of frequency and AoA. Connectionist models have repeatedly shown strong effects of frequency, but little attention has been paid to whether they can also show AoA effects. Indeed, several researchers have explicitly claimed that they cannot show AoA effects. In this work, we explore these claims using a simple feed forward neural network. We find a significant contribution of AoA to naming latency, as well as conditions under which frequency provides an independent contribution. 1 Background Naming latency is the time between the presentation of a picture or written word and the beginning of the correct utterance of that word. It is undisputed that there are significant differences in the naming latency of many words, even when controlling word length, syllabic complexity, and other structural variants. The cause of differences in naming latency has been the subject of numerous studies. Earlier studies found that the frequency with which a word appears in spoken English is the best determinant of its naming latency (Oldfield & Wingfield, 1965). More recent psychological studies, however, show that the age at which a word is learned, or its Age of Acquisition (AoA), may be a better predictor of naming latency. Further, in many multiple regression analyses, frequency is not found to be significant when AoA is controlled for (Brown & Watson, 1987; Carroll & White, 1973; Morrison et al. 1992; Morrison & Ellis, 1995). These studies show that frequency and AoA are highly correlated (typically r =-.6) explaining the confound of older studies on frequency. However, still more recent studies question this finding and find that both AoA and frequency are significant and contribute independently to naming latency (Ellis & Morrison, 1998; Gerhand & Barry, 1998,1999). Much like their psychological counterparts, connectionist networks also show very strong frequency effects. However, the ability of a connectionist network to show AoA effects has been doubted (Gerhand & Barry, 1998; Morrison & Ellis, 1995). Most of these claims are based on the well known fact that connectionist networks exhibit</p><p>6 0.25792554 <a title="25-lda-6" href="./nips-2000-Error-correcting_Codes_on_a_Bethe-like_Lattice.html">47 nips-2000-Error-correcting Codes on a Bethe-like Lattice</a></p>
<p>7 0.25674844 <a title="25-lda-7" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>8 0.25673786 <a title="25-lda-8" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>9 0.25521165 <a title="25-lda-9" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>10 0.25367817 <a title="25-lda-10" href="./nips-2000-Stability_and_Noise_in_Biochemical_Switches.html">125 nips-2000-Stability and Noise in Biochemical Switches</a></p>
<p>11 0.25291535 <a title="25-lda-11" href="./nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">95 nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>12 0.25277659 <a title="25-lda-12" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>13 0.25228065 <a title="25-lda-13" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>14 0.25092536 <a title="25-lda-14" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>15 0.24959485 <a title="25-lda-15" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>16 0.24959184 <a title="25-lda-16" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>17 0.24896012 <a title="25-lda-17" href="./nips-2000-Mixtures_of_Gaussian_Processes.html">85 nips-2000-Mixtures of Gaussian Processes</a></p>
<p>18 0.24876258 <a title="25-lda-18" href="./nips-2000-Some_New_Bounds_on_the_Generalization_Error_of_Combined_Classifiers.html">119 nips-2000-Some New Bounds on the Generalization Error of Combined Classifiers</a></p>
<p>19 0.24728297 <a title="25-lda-19" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>20 0.24684182 <a title="25-lda-20" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
