<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-39" href="#">nips2000-39</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</h1>
<br/><p>Source: <a title="nips-2000-39-pdf" href="http://papers.nips.cc/paper/1915-decomposition-of-reinforcement-learning-for-admission-control-of-self-similar-call-arrival-processes.pdf">pdf</a></p><p>Author: Jakob Carlström</p><p>Abstract: This paper presents predictive gain scheduling, a technique for simplifying reinforcement learning problems by decomposition. Link admission control of self-similar call traffic is used to demonstrate the technique. The control problem is decomposed into on-line prediction of near-future call arrival rates, and precomputation of policies for Poisson call arrival processes. At decision time, the predictions are used to select among the policies. Simulations show that this technique results in significantly faster learning without any performance loss, compared to a reinforcement learning controller that does not decompose the problem. 1</p><p>Reference: <a title="nips-2000-39-reference" href="../nips2000_reference/nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 il  Abstract This paper presents predictive gain scheduling, a technique for simplifying reinforcement learning problems by decomposition. [sent-4, score-0.243]
</p><p>2 Link admission control of self-similar call traffic is used to demonstrate the technique. [sent-5, score-0.961]
</p><p>3 The control problem is decomposed into on-line prediction of near-future call arrival rates, and precomputation of policies for Poisson call arrival processes. [sent-6, score-1.879]
</p><p>4 Simulations show that this technique results in significantly faster learning without any performance loss, compared to a reinforcement learning controller that does not decompose the problem. [sent-8, score-0.254]
</p><p>5 1  Introduction  In multi-service communications networks, such as Asynchronous Transfer Mode (ATM) networks, resource control is of crucial importance for the network operator as well as for the users. [sent-9, score-0.231]
</p><p>6 At the call level , service quality (Grade of Service) is measured in terms of call blocking probabilities, and the key resource to be controlled is bandwidth. [sent-11, score-0.784]
</p><p>7 Network routing and call admission control (CAC) are two such resource control problems. [sent-12, score-1.087]
</p><p>8 Markov decision processes offer a framework for optimal CAC and routing [1]. [sent-13, score-0.289]
</p><p>9 By modelling the dynamics of the network with traffic and computing control policies using dynamic programming [2], resource control is optimized. [sent-14, score-0.698]
</p><p>10 Although the Poisson assumption is valid for most user-initiated requests in communications networks, a number of studies [3, 4, 5] indicate that many types of arrival processes in wide-area networks as well as in local area networks are statistically selfsimilar. [sent-17, score-0.678]
</p><p>11 Nevertheless, the "fractal" burst structure of self-similar traffic should be possible to exploit in the design of efficient resource control methods. [sent-20, score-0.459]
</p><p>12 We have previously presented a method based on temporal-difference (TD) learning for CAC of self-similar call traffic, which yields higher revenue than a TD-based controller assuming Poisson call arrival processes [7]. [sent-21, score-1.337]
</p><p>13 It decomposes the control problem into two parts: time-series prediction of near-future call arrival rates and precomputation of a set of control policies for Poisson call arrival processes. [sent-24, score-2.057]
</p><p>14 Thus, the self-similar arrival process is approximated by a quasi-stationary Poisson process. [sent-26, score-0.526]
</p><p>15 The policies can be computed using dynamic programming or other reinforcement learning techniques [6]. [sent-28, score-0.258]
</p><p>16 This paper concentrates on the link admission control problem. [sent-29, score-0.596]
</p><p>17 Other recent work on reinforcement learning for CAC and routing includes [10], where Marbach et al. [sent-31, score-0.257]
</p><p>18 apply reinforcement learning to routing subject to Quality of Service constraints. [sent-33, score-0.257]
</p><p>19 2  Self-Similar Call Arrival Processes  The limitations of the traditional Poisson model for network arrival processes have been demonstrated in a number of studies, e. [sent-34, score-0.591]
</p><p>20 [3, 4, 5], which indicate the existence of heavytailed inter-arrival time distributions and long-term correlations in the arrival processes. [sent-36, score-0.528]
</p><p>21 A self-similar arrival process has no "natural" burst length. [sent-38, score-0.554]
</p><p>22 On the contrary, its arrival intensity varies considerably over many time scales. [sent-39, score-0.554]
</p><p>23 This makes the variance of its sample mean decay slowly with the sample size, and its auto-correlation function decay slowly with time, compared to Poisson traffic [4]. [sent-40, score-0.264]
</p><p>24 The complexity of control and prediction of Poisson traffic is reduced by the memory-less property of the Poisson process: its expected future depends on the arrival intensity, but not on the process history. [sent-41, score-0.984]
</p><p>25 On the other hand, the long-range dependence of self-similar traffic makes it possible to improve predictions of the process future by observing the history. [sent-42, score-0.3]
</p><p>26 For self-similar traffic this parameter takes values in the interval (0. [sent-44, score-0.237]
</p><p>27 3  The Link Admission Control Problem  In the link admission control (LAC) problem, a link with capacity C [units/s] is offered calls from K different service classes. [sent-47, score-0.982]
</p><p>28 The per-class call holding times are assumed to be exponentially distributed with mean 1/ftj [s]. [sent-52, score-0.386]
</p><p>29 Access to the link is controlled by a policy:rc that maps states x E X to actions a EA,:rc: X-+ A. [sent-53, score-0.207]
</p><p>30 The set X contains all feasible link states, and the action set is A  =  ((ai, . [sent-54, score-0.217]
</p><p>31 ,aK )  °  :  aj E {O, Il,j E  J),  where a j is for rejecting a presumptive class-j call and 1 for accepting it. [sent-57, score-0.374]
</p><p>32 The set of link states is given by X = N x H, where N is the set of feasible call number tuples, and His the Cartesian product of some representations, '1, of the history of the per-class call arrival processes (needed because of the memory of self-similar arrival processes). [sent-58, score-1.948]
</p><p>33 We assume uniform call charging, which means that the reward rate p(t) at time t is equal to the carried bandwidth: pet) = p(x(t» =  I  (1)  n/t)bj  jEl  Time evolves continuously, with discrete call arrival and departure events, enumerated by k = 0,1,2, . [sent-60, score-1.196]
</p><p>34 By taking optimal actions, the policy controls the probabilities of state transitions so as to increase the probability of reaching states that yield high long-term rewards. [sent-64, score-0.259]
</p><p>35 The objective of link: admission control is to find a policy :rr that maximizes the average reward per stage: R(,,)  ~ )~"! [sent-65, score-0.624]
</p><p>36 xE X X,  (3)  Note that the average reward does not depend on the initial state x, as the contribution from this state to the average reward tends to zero as N -+ 00 (assuming, for example, that the probability of reaching any other state y E X from every state x E X is positive). [sent-68, score-0.384]
</p><p>37 The set of such states X ib C X is given by X ib = Nib X H, where Nib is the set of call number tuples for which the available bandwidth is a multiple of the bandwidth of a wideband call. [sent-71, score-0.606]
</p><p>38 In the states of X ib , the long-term reward may be increased by rejecting narrowband calls to reserve bandwidth for future, expected wideband calls. [sent-72, score-0.355]
</p><p>39 4  Solution by Predictive Gain Scheduling  Gain scheduling is a control theory technique, where the parameters of a controller are changed as a function of operating conditions [12]. [sent-73, score-0.455]
</p><p>40 The approach taken here is to look up policies in a table from predictions of the near-future per-class call arrival rates. [sent-74, score-0.872]
</p><p>41 For Poisson call arrival processes, the optimal policy for the link: admission control problem does not depend on the history, H, of the arrival processes. [sent-75, score-1.852]
</p><p>42 Due to the memory-less property, only the (constant) per-class arrival rates Aj , j E J, matter. [sent-76, score-0.555]
</p><p>43 In our gain scheduled control of self-similar call arrival processes, near-future Aj are predicted from hj- The selfsimilar call arrival processes are approximated by quasi-stationary Poisson processes, by selecting precomputed polices (for Poisson arrival processes) based on predicted A/s. [sent-77, score-2.554]
</p><p>44 One radial-basis function (REF) NN per class is trained to predict its near-future arrival rate. [sent-78, score-0.5]
</p><p>45 1  Solving the Link Admission Control problem for Poisson Traffic  For Poisson call arrival processes, dynamic programming offers well-established techniques for solving the LAC problem [1]. [sent-80, score-0.852]
</p><p>46 It involves two steps: value determination and policy improvement. [sent-82, score-0.168]
</p><p>47 The difference v(x,:rr) - v(y,:rr) between two relative values under a policy :rr is the expected difference in accumulated reward over an infinite time interval, starting in state X instead of state y. [sent-84, score-0.334]
</p><p>48 The dynamics of  the system are characterized by state transition probabilities, given by the policy, the perclass call arrival intensities, (,q, and mean holding times, (1/,ll  J  The policy improvement step consists of finding the action that maximizes the relative value at each state. [sent-86, score-1.067]
</p><p>49 After improving the policy, the value determination and policy improvement steps are iterated until the policy does not change [9]. [sent-87, score-0.296]
</p><p>50 2 Determining The Prediction Horizon Over what future time horizon should we predict the rates used to select policies? [sent-89, score-0.242]
</p><p>51 In this work, the prediction horizon is set to an average of estimated mean first passage times from states back to themselves, in the following referred to as the mean return time. [sent-90, score-0.358]
</p><p>52 The arrival process is approximated by a quasi-stationary Poisson process within this time interval. [sent-91, score-0.58]
</p><p>53 The motivation for this choice of prediction horizon is that the effects of a decision (action) in a state Xd influence the future probabilities of reaching other states and receiving the associated rewards, until the state Xd is reached the next time. [sent-92, score-0.442]
</p><p>54 In accordance with the assumption of quasi-stationarity, the mean return time can be estimated for call tuples n instead of the full state descriptor, x. [sent-94, score-0.495]
</p><p>55 In case of Poisson call arrival processes, the mean first passage times E,. [sent-95, score-0.876]
</p><p>56 The mean return time for the link, T I , is defmed as the average of the individual mean return times of the states of Nib, weighted by their limiting probabilities and normalized: (5)  For ease of implementation, this time window is expressed as a number of call arrivals. [sent-99, score-0.586]
</p><p>57 The window length L j for class j is computed by multiplying the mean return time by the arrival rate, Lj = Aj T[, and rounding off to an integer. [sent-100, score-0.65]
</p><p>58 3  Prediction of Future Call Arrival Rates  The prediction of future arrival call rates is naturally based on measures of recent arrival rates. [sent-102, score-1.443]
</p><p>59 In this work, the following representation of the history of the arrival process is used: for all classes j E J, exponentially weighted running averages h j = (h j ), . [sent-103, score-0.567]
</p><p>60 ,a M } taking values in the interval (0, 1): hik) = a i[t/k) - t/k - 1) 1+ (1 - a;)hik - 1) ,  (6)  where fj(k) is the arrival time of the k-th call from class j. [sent-110, score-0.818]
</p><p>61 After every new call arrival, the prediction error €j(k) is computed: Lj  Elk) =  LI [  t(k  +  i) - t(k  +  i-I)] - y/k). [sent-116, score-0.351]
</p><p>62 (7)  J i~ '  Learning is performed on-line using the least mean squares rule, which means that the upd)lting must be delayed by L j call arrivals. [sent-117, score-0.317]
</p><p>63 The predicted per-class arrival rates A/k) = y(k)-' are used to select a control policy on the arrival of a call request. [sent-118, score-1.646]
</p><p>64 Given the prediction horizon and the arrival rate predictor, ai' . [sent-119, score-0.685]
</p><p>65 ,a M can be tuned by linear search to minimize the prediction error on sample traffic traces. [sent-122, score-0.298]
</p><p>66 5  Numerical study  The performance of the gain scheduled admission controller was evaluated on a simulated link with capacity C = 24 [units/s], that was offered calls from self-similar call arrival processes. [sent-123, score-1.752]
</p><p>67 For comparison, the simulations were repeated with three other link admission controllers: two TD-based controllers, one table-based and one NN based, and a controller using complete sharing, i. [sent-124, score-0.639]
</p><p>68 to accept a call if the free capacity on the link is sufficient. [sent-126, score-0.478]
</p><p>69 The NN based TD controller [7] uses RBF NNs (one per n EN), receiving (h" h2) as input. [sent-127, score-0.194]
</p><p>70 Each NN has 65 hidden units, factorized to 8 units per call class, plus a default activation unit. [sent-128, score-0.348]
</p><p>71 The table-based TD controller assumes Poisson call arrival processes. [sent-130, score-0.956]
</p><p>72 From this, it follows that the call number tuples n E N constitute Markovian states. [sent-131, score-0.341]
</p><p>73 This controller was used for evaluation of the performance loss from incorrectly modelling self-similar call traffic by Poisson traffic. [sent-133, score-0.693]
</p><p>74 1  Synthesis of Call Traffic  Synthetic traffic traces were generated from a Gaussian fractional auto-regressive integrated moving average model, FARIMA (0, d, 0). [sent-135, score-0.237]
</p><p>75 This results in a statistically self-similar arrival process, where the Hurst parameter is easily tuned [7]. [sent-136, score-0.5]
</p><p>76 We generated traces containing arrival/departure pairs from two call classes, characterized by bandwidth requirements bi = 1 (narrow-band) and ~ = 6 (wide-band) [units/s] and call holding times with mean 1/,u1 = 1/,u2= 1 [s]. [sent-137, score-0.765]
</p><p>77 85 was used, and the call arrival rates were scaled to make the expected long-term arrival rates A, and A2 for the two classes fulfill b,A,/,u, + b). [sent-139, score-1.4]
</p><p>78 2 Gain Scheduling For simplicity, a constant prediction horizon was used throughout the simulations. [sent-146, score-0.159]
</p><p>79 A A The table of policies to be used for gain scheduling was computed for predicted A, and A2 ranging from 0. [sent-153, score-0.411]
</p><p>80 3  Numerical results  Both the TD learning controllers and the gain scheduling controller were allowed to adapt to the first 400 000 simulated call arrivals of the traffic traces. [sent-159, score-1.126]
</p><p>81 The throughput obtained by all four methods was measured on the subsequent 400000 call arrivals. [sent-160, score-0.437]
</p><p>82 0 call arrivals x 105 call arrivals (a) Initial weight evolution in neural predictor (b) Long-term weight evolution in neural predictor Throughput [units/s] 11  9  1. [sent-166, score-0.95]
</p><p>83 0 x 105 call arrivals (c) Weight evolution in NN based TD controller  17. [sent-170, score-0.585]
</p><p>84 0  AdA2 (d) Throughput versus arrival rate ratio  Figure 1: Weight evolution for NN predictor (a, b); NN based TD-controller (c). [sent-188, score-0.626]
</p><p>85 Figure 1 (a, b) shows the evolution of the weights of the call arrival rate predictor for class 2, and figure 1 (c) displays nine weights of the RBF NN corresponding to the call number tuple (n! [sent-190, score-1.258]
</p><p>86 The majority of the weights of the gain scheduling RBF NN seems to converge in a few thousand call arrivals, whereas the TD learning controller needs about tOO 000 call arrivals to converge. [sent-193, score-1.134]
</p><p>87 This is not surprising, since the RBF NNs of the TD learning controllers split up the set of training data, so that a single NN is updated much less frequently than a ratepredicting NN in the gain scheduling controller. [sent-194, score-0.348]
</p><p>88 A few of the weights of the gain scheduling NN change considerably even after long training. [sent-196, score-0.329]
</p><p>89 Figure t (d) evaluates performance in terms of throughput versus arrival rate ratio. [sent-198, score-0.673]
</p><p>90 Each data point is the averaged throughput for 10 traffic traces. [sent-199, score-0.384]
</p><p>91 Gain scheduling (GS/RBF) achieves the same throughput as TD learning with RBF NNs (TD/RBF), up to 1. [sent-200, score-0.313]
</p><p>92 The difference in throughput between TD learning and complete sharing is greatest for low arrival rate ratios, since the throughput increase by reserving bandwidth for highrate wideband calls is considerably higher than the loss of throughput from the blocked lowrate narrowband traffic. [sent-203, score-1.246]
</p><p>93 6  Conclusion  We have presented predictive gain scheduling, a technique for decomposing reinforcement learning problems. [sent-204, score-0.243]
</p><p>94 Link admission control, a sub-problem of network routing, was used to demonstrate the technique. [sent-205, score-0.311]
</p><p>95 By predicting near-future call arrival rates from one part of the full state descriptor, precomputed policies for Poisson call arrival processes (computed from the rest of the state descriptor) were selected. [sent-206, score-1.95]
</p><p>96 This increased the on-line convergence rate approximately 50 times, compared to a TD-based admission controller getting the full state descriptor as input. [sent-207, score-0.632]
</p><p>97 The computational complexity of the controller using predictive gain scheduling may reach a computational bottleneck if the size of the state space is increased: the determination of optimal policies for Poisson traffic by policy iteration. [sent-209, score-1.032]
</p><p>98 Further, we have successfully employed gain scheduled link admission control as a building block of network routing [9], where the performance improvement compared to conventional methods is larger than for the link admission control problem. [sent-213, score-1.543]
</p><p>99 The use of gain scheduling to reduce the complexity of reinforcement learning problems is not limited to link admission control. [sent-214, score-0.838]
</p><p>100 In general, the technique should be applicable to problems where parts of the state descriptor can be used, directly or after preprocessing, to select among policies for instances of a simplified version of the original problem. [sent-215, score-0.235]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('arrival', 0.5), ('admission', 0.311), ('call', 0.29), ('traffic', 0.237), ('poisson', 0.203), ('routing', 0.169), ('scheduling', 0.166), ('controller', 0.166), ('link', 0.162), ('throughput', 0.147), ('nn', 0.147), ('policy', 0.128), ('td', 0.126), ('control', 0.123), ('gain', 0.111), ('nns', 0.102), ('horizon', 0.098), ('processes', 0.091), ('bandwidth', 0.089), ('calls', 0.089), ('reinforcement', 0.088), ('rbf', 0.086), ('arrivals', 0.085), ('service', 0.083), ('policies', 0.082), ('resource', 0.071), ('controllers', 0.071), ('descriptor', 0.071), ('scheduled', 0.071), ('cac', 0.066), ('carlstrom', 0.066), ('hurst', 0.066), ('uppsala', 0.066), ('reward', 0.062), ('prediction', 0.061), ('xk', 0.06), ('state', 0.058), ('rr', 0.057), ('aj', 0.056), ('predictor', 0.056), ('rates', 0.055), ('tuples', 0.051), ('nib', 0.049), ('states', 0.045), ('predictive', 0.044), ('evolution', 0.044), ('wideband', 0.042), ('history', 0.041), ('return', 0.041), ('determination', 0.04), ('holding', 0.038), ('april', 0.038), ('future', 0.037), ('communications', 0.037), ('programming', 0.036), ('sharing', 0.033), ('atm', 0.033), ('dziong', 0.033), ('hik', 0.033), ('jakob', 0.033), ('lac', 0.033), ('marbach', 0.033), ('networking', 0.033), ('nordstrom', 0.033), ('precomputation', 0.033), ('tditbl', 0.033), ('teletraffic', 0.033), ('willinger', 0.033), ('times', 0.031), ('units', 0.03), ('decision', 0.029), ('feasible', 0.029), ('time', 0.028), ('receiving', 0.028), ('burst', 0.028), ('default', 0.028), ('passage', 0.028), ('rejecting', 0.028), ('window', 0.028), ('reaching', 0.028), ('mean', 0.027), ('considerably', 0.026), ('process', 0.026), ('predicted', 0.026), ('dynamic', 0.026), ('rate', 0.026), ('weights', 0.026), ('computed', 0.026), ('action', 0.026), ('capacity', 0.026), ('offered', 0.026), ('entering', 0.026), ('blocking', 0.026), ('precomputed', 0.026), ('rc', 0.026), ('networks', 0.025), ('quality', 0.024), ('intelligent', 0.024), ('select', 0.024), ('technion', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="39-tfidf-1" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>Author: Jakob Carlström</p><p>Abstract: This paper presents predictive gain scheduling, a technique for simplifying reinforcement learning problems by decomposition. Link admission control of self-similar call traffic is used to demonstrate the technique. The control problem is decomposed into on-line prediction of near-future call arrival rates, and precomputation of policies for Poisson call arrival processes. At decision time, the predictions are used to select among the policies. Simulations show that this technique results in significantly faster learning without any performance loss, compared to a reinforcement learning controller that does not decompose the problem. 1</p><p>2 0.15822968 <a title="39-tfidf-2" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>Author: Justin A. Boyan, Michael L. Littman</p><p>Abstract: We describe an extension of the Markov decision process model in which a continuous time dimension is included in the state space. This allows for the representation and exact solution of a wide range of problems in which transitions or rewards vary over time. We examine problems based on route planning with public transportation and telescope observation scheduling. 1</p><p>3 0.12127224 <a title="39-tfidf-3" href="./nips-2000-Robust_Reinforcement_Learning.html">113 nips-2000-Robust Reinforcement Learning</a></p>
<p>Author: Jun Morimoto, Kenji Doya</p><p>Abstract: This paper proposes a new reinforcement learning (RL) paradigm that explicitly takes into account input disturbance as well as modeling errors. The use of environmental models in RL is quite popular for both off-line learning by simulations and for on-line action planning. However, the difference between the model and the real environment can lead to unpredictable, often unwanted results. Based on the theory of H oocontrol, we consider a differential game in which a 'disturbing' agent (disturber) tries to make the worst possible disturbance while a 'control' agent (actor) tries to make the best control input. The problem is formulated as finding a minmax solution of a value function that takes into account the norm of the output deviation and the norm of the disturbance. We derive on-line learning algorithms for estimating the value function and for calculating the worst disturbance and the best control in reference to the value function. We tested the paradigm, which we call</p><p>4 0.10180818 <a title="39-tfidf-4" href="./nips-2000-Computing_with_Finite_and_Infinite_Networks.html">35 nips-2000-Computing with Finite and Infinite Networks</a></p>
<p>Author: Ole Winther</p><p>Abstract: Using statistical mechanics results, I calculate learning curves (average generalization error) for Gaussian processes (GPs) and Bayesian neural networks (NNs) used for regression. Applying the results to learning a teacher defined by a two-layer network, I can directly compare GP and Bayesian NN learning. I find that a GP in general requires CJ (d S )-training examples to learn input features of order s (d is the input dimension), whereas a NN can learn the task with order the number of adjustable weights training examples. Since a GP can be considered as an infinite NN, the results show that even in the Bayesian approach, it is important to limit the complexity of the learning machine. The theoretical findings are confirmed in simulations with analytical GP learning and a NN mean field algorithm.</p><p>5 0.096891008 <a title="39-tfidf-5" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>Author: Geoffrey J. Gordon</p><p>Abstract: Many algorithms for approximate reinforcement learning are not known to converge. In fact, there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point. This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region. The algorithms are SARSA(O) and V(O); the latter algorithm was used in the well-known TD-Gammon program. 1</p><p>6 0.096393175 <a title="39-tfidf-6" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>7 0.09628839 <a title="39-tfidf-7" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>8 0.095910676 <a title="39-tfidf-8" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>9 0.073083065 <a title="39-tfidf-9" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>10 0.070265494 <a title="39-tfidf-10" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>11 0.062933266 <a title="39-tfidf-11" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>12 0.062870398 <a title="39-tfidf-12" href="./nips-2000-Programmable_Reinforcement_Learning_Agents.html">105 nips-2000-Programmable Reinforcement Learning Agents</a></p>
<p>13 0.062509 <a title="39-tfidf-13" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>14 0.06239491 <a title="39-tfidf-14" href="./nips-2000-Mixtures_of_Gaussian_Processes.html">85 nips-2000-Mixtures of Gaussian Processes</a></p>
<p>15 0.06039032 <a title="39-tfidf-15" href="./nips-2000-Hierarchical_Memory-Based_Reinforcement_Learning.html">63 nips-2000-Hierarchical Memory-Based Reinforcement Learning</a></p>
<p>16 0.048802845 <a title="39-tfidf-16" href="./nips-2000-Bayesian_Video_Shot_Segmentation.html">30 nips-2000-Bayesian Video Shot Segmentation</a></p>
<p>17 0.048118804 <a title="39-tfidf-17" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>18 0.04401442 <a title="39-tfidf-18" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>19 0.042232033 <a title="39-tfidf-19" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>20 0.039199453 <a title="39-tfidf-20" href="./nips-2000-Finding_the_Key_to_a_Synapse.html">55 nips-2000-Finding the Key to a Synapse</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.157), (1, -0.054), (2, 0.058), (3, -0.233), (4, -0.134), (5, 0.032), (6, -0.022), (7, 0.052), (8, -0.016), (9, -0.094), (10, -0.011), (11, 0.045), (12, 0.023), (13, -0.028), (14, 0.059), (15, 0.043), (16, -0.038), (17, 0.058), (18, 0.032), (19, -0.001), (20, 0.105), (21, -0.085), (22, 0.001), (23, 0.099), (24, -0.026), (25, 0.035), (26, -0.122), (27, -0.109), (28, 0.266), (29, 0.065), (30, 0.273), (31, -0.029), (32, -0.098), (33, -0.138), (34, -0.013), (35, -0.132), (36, 0.073), (37, -0.147), (38, -0.108), (39, -0.074), (40, 0.007), (41, -0.16), (42, 0.177), (43, 0.075), (44, 0.03), (45, 0.024), (46, -0.046), (47, -0.022), (48, -0.012), (49, -0.137)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97508568 <a title="39-lsi-1" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>Author: Jakob Carlström</p><p>Abstract: This paper presents predictive gain scheduling, a technique for simplifying reinforcement learning problems by decomposition. Link admission control of self-similar call traffic is used to demonstrate the technique. The control problem is decomposed into on-line prediction of near-future call arrival rates, and precomputation of policies for Poisson call arrival processes. At decision time, the predictions are used to select among the policies. Simulations show that this technique results in significantly faster learning without any performance loss, compared to a reinforcement learning controller that does not decompose the problem. 1</p><p>2 0.71591872 <a title="39-lsi-2" href="./nips-2000-Robust_Reinforcement_Learning.html">113 nips-2000-Robust Reinforcement Learning</a></p>
<p>Author: Jun Morimoto, Kenji Doya</p><p>Abstract: This paper proposes a new reinforcement learning (RL) paradigm that explicitly takes into account input disturbance as well as modeling errors. The use of environmental models in RL is quite popular for both off-line learning by simulations and for on-line action planning. However, the difference between the model and the real environment can lead to unpredictable, often unwanted results. Based on the theory of H oocontrol, we consider a differential game in which a 'disturbing' agent (disturber) tries to make the worst possible disturbance while a 'control' agent (actor) tries to make the best control input. The problem is formulated as finding a minmax solution of a value function that takes into account the norm of the output deviation and the norm of the disturbance. We derive on-line learning algorithms for estimating the value function and for calculating the worst disturbance and the best control in reference to the value function. We tested the paradigm, which we call</p><p>3 0.64530611 <a title="39-lsi-3" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>Author: Justin A. Boyan, Michael L. Littman</p><p>Abstract: We describe an extension of the Markov decision process model in which a continuous time dimension is included in the state space. This allows for the representation and exact solution of a wide range of problems in which transitions or rewards vary over time. We examine problems based on route planning with public transportation and telescope observation scheduling. 1</p><p>4 0.41942495 <a title="39-lsi-4" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>Author: Dirk Ormoneit, Peter W. Glynn</p><p>Abstract: Many approaches to reinforcement learning combine neural networks or other parametric function approximators with a form of temporal-difference learning to estimate the value function of a Markov Decision Process. A significant disadvantage of those procedures is that the resulting learning algorithms are frequently unstable. In this work, we present a new, kernel-based approach to reinforcement learning which overcomes this difficulty and provably converges to a unique solution. By contrast to existing algorithms, our method can also be shown to be consistent in the sense that its costs converge to the optimal costs asymptotically. Our focus is on learning in an average-cost framework and on a practical application to the optimal portfolio choice problem. 1</p><p>5 0.38816488 <a title="39-lsi-5" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>Author: Robert St-Aubin, Jesse Hoey, Craig Boutilier</p><p>Abstract: We propose a method of approximate dynamic programming for Markov decision processes (MDPs) using algebraic decision diagrams (ADDs). We produce near-optimal value functions and policies with much lower time and space requirements than exact dynamic programming. Our method reduces the sizes of the intermediate value functions generated during value iteration by replacing the values at the terminals of the ADD with ranges of values. Our method is demonstrated on a class of large MDPs (with up to 34 billion states), and we compare the results with the optimal value functions.</p><p>6 0.33829725 <a title="39-lsi-6" href="./nips-2000-Bayesian_Video_Shot_Segmentation.html">30 nips-2000-Bayesian Video Shot Segmentation</a></p>
<p>7 0.32382613 <a title="39-lsi-7" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>8 0.31908211 <a title="39-lsi-8" href="./nips-2000-Dopamine_Bonuses.html">43 nips-2000-Dopamine Bonuses</a></p>
<p>9 0.31116894 <a title="39-lsi-9" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>10 0.30860063 <a title="39-lsi-10" href="./nips-2000-Mixtures_of_Gaussian_Processes.html">85 nips-2000-Mixtures of Gaussian Processes</a></p>
<p>11 0.28506559 <a title="39-lsi-11" href="./nips-2000-Computing_with_Finite_and_Infinite_Networks.html">35 nips-2000-Computing with Finite and Infinite Networks</a></p>
<p>12 0.27046144 <a title="39-lsi-12" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>13 0.26370355 <a title="39-lsi-13" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>14 0.23909491 <a title="39-lsi-14" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>15 0.22614954 <a title="39-lsi-15" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>16 0.22007914 <a title="39-lsi-16" href="./nips-2000-Four-legged_Walking_Gait_Control_Using_a_Neuromorphic_Chip_Interfaced_to_a_Support_Vector_Learning_Algorithm.html">57 nips-2000-Four-legged Walking Gait Control Using a Neuromorphic Chip Interfaced to a Support Vector Learning Algorithm</a></p>
<p>17 0.19818267 <a title="39-lsi-17" href="./nips-2000-Competition_and_Arbors_in_Ocular_Dominance.html">34 nips-2000-Competition and Arbors in Ocular Dominance</a></p>
<p>18 0.19789313 <a title="39-lsi-18" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>19 0.1968635 <a title="39-lsi-19" href="./nips-2000-The_Missing_Link_-_A_Probabilistic_Model_of_Document_Content_and_Hypertext_Connectivity.html">136 nips-2000-The Missing Link - A Probabilistic Model of Document Content and Hypertext Connectivity</a></p>
<p>20 0.1858349 <a title="39-lsi-20" href="./nips-2000-An_Adaptive_Metric_Machine_for_Pattern_Classification.html">23 nips-2000-An Adaptive Metric Machine for Pattern Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.043), (15, 0.384), (17, 0.071), (32, 0.011), (33, 0.031), (42, 0.02), (54, 0.037), (55, 0.02), (62, 0.075), (65, 0.014), (67, 0.04), (75, 0.021), (76, 0.034), (79, 0.031), (81, 0.031), (90, 0.025), (97, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85193503 <a title="39-lda-1" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>Author: Jakob Carlström</p><p>Abstract: This paper presents predictive gain scheduling, a technique for simplifying reinforcement learning problems by decomposition. Link admission control of self-similar call traffic is used to demonstrate the technique. The control problem is decomposed into on-line prediction of near-future call arrival rates, and precomputation of policies for Poisson call arrival processes. At decision time, the predictions are used to select among the policies. Simulations show that this technique results in significantly faster learning without any performance loss, compared to a reinforcement learning controller that does not decompose the problem. 1</p><p>2 0.78725845 <a title="39-lda-2" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>Author: Justin A. Boyan, Michael L. Littman</p><p>Abstract: We describe an extension of the Markov decision process model in which a continuous time dimension is included in the state space. This allows for the representation and exact solution of a wide range of problems in which transitions or rewards vary over time. We examine problems based on route planning with public transportation and telescope observation scheduling. 1</p><p>3 0.31112078 <a title="39-lda-3" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>Author: Zoubin Ghahramani, Matthew J. Beal</p><p>Abstract: Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set. 1</p><p>4 0.30974326 <a title="39-lda-4" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>Author: Anders Jonsson, Andrew G. Barto</p><p>Abstract: Learning a complex task can be significantly facilitated by defining a hierarchy of subtasks. An agent can learn to choose between various temporally abstract actions, each solving an assigned subtask, to accomplish the overall task. In this paper, we study hierarchical learning using the framework of options. We argue that to take full advantage of hierarchical structure, one should perform option-specific state abstraction, and that if this is to scale to larger tasks, state abstraction should be automated. We adapt McCallum's U-Tree algorithm to automatically build option-specific representations of the state feature space, and we illustrate the resulting algorithm using a simple hierarchical task. Results suggest that automated option-specific state abstraction is an attractive approach to making hierarchical learning systems more effective.</p><p>5 0.30606964 <a title="39-lda-5" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>Author: Javier R. Movellan, Paul Mineiro, Ruth J. Williams</p><p>Abstract: This paper explores a framework for recognition of image sequences using partially observable stochastic differential equation (SDE) models. Monte-Carlo importance sampling techniques are used for efficient estimation of sequence likelihoods and sequence likelihood gradients. Once the network dynamics are learned, we apply the SDE models to sequence recognition tasks in a manner similar to the way Hidden Markov models (HMMs) are commonly applied. The potential advantage of SDEs over HMMS is the use of continuous state dynamics. We present encouraging results for a video sequence recognition task in which SDE models provided excellent performance when compared to hidden Markov models. 1</p><p>6 0.30541342 <a title="39-lda-6" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>7 0.30496278 <a title="39-lda-7" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>8 0.30475971 <a title="39-lda-8" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>9 0.30231968 <a title="39-lda-9" href="./nips-2000-Learning_Switching_Linear_Models_of_Human_Motion.html">80 nips-2000-Learning Switching Linear Models of Human Motion</a></p>
<p>10 0.30105403 <a title="39-lda-10" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>11 0.30075553 <a title="39-lda-11" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>12 0.3003751 <a title="39-lda-12" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>13 0.29951942 <a title="39-lda-13" href="./nips-2000-Tree-Based_Modeling_and_Estimation_of_Gaussian_Processes_on_Graphs_with_Cycles.html">140 nips-2000-Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles</a></p>
<p>14 0.29858273 <a title="39-lda-14" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>15 0.298103 <a title="39-lda-15" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>16 0.29756394 <a title="39-lda-16" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>17 0.2974726 <a title="39-lda-17" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>18 0.29576662 <a title="39-lda-18" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>19 0.29367521 <a title="39-lda-19" href="./nips-2000-The_Use_of_MDL_to_Select_among_Computational_Models_of_Cognition.html">139 nips-2000-The Use of MDL to Select among Computational Models of Cognition</a></p>
<p>20 0.29184869 <a title="39-lda-20" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
