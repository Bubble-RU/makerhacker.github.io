<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>115 nips-2000-Sequentially Fitting ``Inclusive'' Trees for Inference in Noisy-OR Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-115" href="#">nips2000-115</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>115 nips-2000-Sequentially Fitting ``Inclusive'' Trees for Inference in Noisy-OR Networks</h1>
<br/><p>Source: <a title="nips-2000-115-pdf" href="http://papers.nips.cc/paper/1815-sequentially-fitting-inclusive-trees-for-inference-in-noisy-or-networks.pdf">pdf</a></p><p>Author: Brendan J. Frey, Relu Patrascu, Tommi Jaakkola, Jodi Moran</p><p>Abstract: An important class of problems can be cast as inference in noisyOR Bayesian networks, where the binary state of each variable is a logical OR of noisy versions of the states of the variable's parents. For example, in medical diagnosis, the presence of a symptom can be expressed as a noisy-OR of the diseases that may cause the symptom - on some occasions, a disease may fail to activate the symptom. Inference in richly-connected noisy-OR networks is intractable, but approximate methods (e .g., variational techniques) are showing increasing promise as practical solutions. One problem with most approximations is that they tend to concentrate on a relatively small number of modes in the true posterior, ignoring other plausible configurations of the hidden variables. We introduce a new sequential variational method for bipartite noisyOR networks, that favors including all modes of the true posterior and models the posterior distribution as a tree. We compare this method with other approximations using an ensemble of networks with network statistics that are comparable to the QMR-DT medical diagnostic network. 1 Inclusive variational approximations Approximate algorithms for probabilistic inference are gaining in popularity and are now even being incorporated into VLSI hardware (T. Richardson, personal communication). Approximate methods include variational techniques (Ghahramani and Jordan 1997; Saul et al. 1996; Frey and Hinton 1999; Jordan et al. 1999), local probability propagation (Gallager 1963; Pearl 1988; Frey 1998; MacKay 1999a; Freeman and Weiss 2001) and Markov chain Monte Carlo (Neal 1993; MacKay 1999b). Many algorithms have been proposed in each of these classes. One problem that most of the above algorithms suffer from is a tendency to concentrate on a relatively small number of modes of the target distribution (the distribution being approximated). In the case of medical diagnosis, different modes correspond to different explanations of the symptoms. Markov chain Monte Carlo methods are usually guaranteed to eventually sample from all the modes, but this may take an extremely long time, even when tempered transitions (Neal 1996) are (a) ,,</p><p>Reference: <a title="nips-2000-115-reference" href="../nips2000_reference/nips-2000-Sequentially_Fitting_%60%60Inclusive%27%27_Trees_for_Inference_in_Noisy-OR_Networks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Sequentially fitting "inclusive" trees for inference in noisy-OR networks  Brendan J. [sent-1, score-0.25]
</p><p>2 For example, in medical diagnosis, the presence of a symptom can be expressed as a noisy-OR of the diseases that may cause the symptom - on some occasions, a disease may fail to activate the symptom. [sent-8, score-1.136]
</p><p>3 Inference in richly-connected noisy-OR networks is intractable, but approximate methods (e . [sent-9, score-0.131]
</p><p>4 , variational techniques) are showing increasing promise as practical solutions. [sent-11, score-0.199]
</p><p>5 One problem with most approximations is that they tend to concentrate on a relatively small number of modes in the true posterior, ignoring other plausible configurations of the hidden variables. [sent-12, score-0.463]
</p><p>6 We introduce a new sequential variational method for bipartite noisyOR networks, that favors including all modes of the true posterior and models the posterior distribution as a tree. [sent-13, score-0.832]
</p><p>7 We compare this method with other approximations using an ensemble of networks with network statistics that are comparable to the QMR-DT medical diagnostic network. [sent-14, score-0.206]
</p><p>8 1  Inclusive variational approximations  Approximate algorithms for probabilistic inference are gaining in popularity and are now even being incorporated into VLSI hardware (T. [sent-15, score-0.402]
</p><p>9 Approximate methods include variational techniques (Ghahramani and Jordan 1997; Saul et al. [sent-17, score-0.243]
</p><p>10 1999), local probability propagation (Gallager 1963; Pearl 1988; Frey 1998; MacKay 1999a; Freeman and Weiss 2001) and Markov chain Monte Carlo (Neal 1993; MacKay 1999b). [sent-19, score-0.179]
</p><p>11 One problem that most of the above algorithms suffer from is a tendency to concentrate on a relatively small number of modes of the target distribution (the distribution being approximated). [sent-21, score-0.35]
</p><p>12 In the case of medical diagnosis, different modes correspond to different explanations of the symptoms. [sent-22, score-0.331]
</p><p>13 Markov chain Monte Carlo methods are usually guaranteed to eventually sample from all the modes, but this may take an extremely long time, even when tempered transitions (Neal 1996) are  (a)  ,,"\  (b)  Q(x)  \  fiE. [sent-23, score-0.071]
</p><p>14 x  Figure 1: We approximate P(x) by adjusting the mean and variance of a Gaussian, Q(x}. [sent-28, score-0.075]
</p><p>15 (a) The result of minimizing D(QIIP) = 2:" Q(x)log(Q(x)/ P(x禄, as is done for most variational methods. [sent-29, score-0.231]
</p><p>16 (b) The result of minimizing D(PIIQ) = 2:" P(x)log(P(x)/Q(x禄. [sent-30, score-0.032]
</p><p>17 Preliminary results on local probability propagation in richly connected networks show that it is sometimes able to oscillate between plausible modes (Murphy et al. [sent-32, score-0.556]
</p><p>18 1999; Frey 2000), but other results also show that it sometimes diverges or oscillates between implausible configurations (McEliece et al. [sent-33, score-0.121]
</p><p>19 Most variational techniques minimize a cost function that favors finding the single, most massive mode, excluding less probable modes of the target distribution (e. [sent-35, score-0.756]
</p><p>20 More sophisticated variational techniques capture multiple modes using substructures (Saul and Jordan 1996) or by leaving part of the original network intact and approximating the remainder (Jaakkola and Jordan 1999). [sent-39, score-0.458]
</p><p>21 However, although these methods increase the number of modes that are captured, they still exclude modes. [sent-40, score-0.349]
</p><p>22 Variational techniques approximate a target distribution P(x) using a simpler, parameterized distribution Q(x) (or a parameterized bound). [sent-41, score-0.165]
</p><p>23 A common approach to variational inference is to minimize a relative entropy,  D(QIIP) =  l: Q(x) log ~~:~. [sent-46, score-0.409]
</p><p>24 Often D(QIIP) can be minimized with respect to the parameters of Q using iterative optimization or even exact optimization. [sent-49, score-0.061]
</p><p>25 To see how minimizing D(QIIP) may exclude modes of the target distribution, suppose Q is a Gaussian and P is bimodal with a region of vanishing density between the two modes, as shown in Fig. [sent-50, score-0.449]
</p><p>26 If we minimize D(QIIP) with respect to the mean and variance of Q, it will cover only one of the two modes, as illustrated in Fig. [sent-52, score-0.044]
</p><p>27 ) This is because D(QIIP) will tend to infinity if Q is nonzero in the region where P has vanishing density. [sent-55, score-0.121]
</p><p>28 In contrast, if we minimize D(PIIQ) = Ex P(x)log(P(x)/Q(x)) with respect to the mean and variance of Q, it will cover all modes, since D(PIIQ) will tend to infinity if Q vanishes in any region where P is nonzero. [sent-56, score-0.127]
</p><p>29 For many problems, including medical diagnosis, it is easy to argue that it is more important that our approximation include all modes than exclude non plausible configurations at the cost of excluding other modes. [sent-59, score-0.66]
</p><p>30 The former leads to a low number of false negatives, whereas the latter may lead to a large number of false negatives (concluding a disease is not present when it is) . [sent-60, score-0.273]
</p><p>31 2 shows a bipartite noisy-OR Bayesian network with N binary hidden variables d = (d 1, . [sent-64, score-0.129]
</p><p>32 , d N ) and K binary observed variables s = (Sl, . [sent-67, score-0.045]
</p><p>33 Later, we present results on medical diagnosis, where dn = 1 indicates a disease is active, dn = 0 indicates a disease is inactive, Sk = 1 indicates a symptom is active and Sk = 0 indicates a symptom is inactive. [sent-71, score-1.582]
</p><p>34 The joint distribution is K  N  k=l  P(d, s) =  n=l  [II P(skl d )] [II P(dn )]. [sent-72, score-0.03]
</p><p>35 (2)  In the case of medical diagnosis, this form assumes the diseases are independent. [sent-73, score-0.407]
</p><p>36 1 Although some diseases probably do depend on other diseases, this form is considered to be a worthwhile representation of the problem (Shwe et al. [sent-74, score-0.338]
</p><p>37 The probability that symptom Sk fails to be activated (Sk = 0) is the product of the probabilities that each active disease fails to activate Sk: N  P(Sk = Old) = PkO  II p~~. [sent-77, score-0.716]
</p><p>38 (3)  n=l Pkn is the probability that an active dn fails to activate Sk. [sent-78, score-0.411]
</p><p>39 1- PkO is the probability that symptom Sk is active when none of the diseases are active. [sent-80, score-0.687]
</p><p>40 Exact inference computes the distribution over d given a subset of observed values in s. [sent-81, score-0.165]
</p><p>41 However, if Sk is not observed, the corresponding likelihood (node plus edges) may be deleted to give a new network that describes the marginal distribution over d and the remaining variables in s. [sent-82, score-0.075]
</p><p>42 So, we assume that we are considering a subnetwork where all the variables in s are observed. [sent-83, score-0.045]
</p><p>43 We reorder the variables in s so that the first J variables are active (Sk = 1, 1 ~ k ~ J) and the remaining variables are inactive (Sk = 0, J + 1 ~ k ~ K). [sent-84, score-0.315]
</p><p>44 The posterior distribution can then be written J  N  k=l  P(dls) ocP(d,s)  n=l  = [II(1-PkoIIp~~)][  K  II  k=J+1  N  N  n=l  n=l  (pkoIIp~~)][IIP(dn)J. [sent-85, score-0.1]
</p><p>45 (4)  Taken together, the two terms in brackets on the right take a simple, product form over the variables in d. [sent-86, score-0.121]
</p><p>46 So, the first step in inference is to "absorb" the inactive 1 However,  the diseases are dependent given that some symptoms are present . [sent-87, score-0.676]
</p><p>47 variables in s by modifying the priors P(dn ) as follows: pI (dn )  K  II  = anP(dn ) (  d  Pkn) n,  (5)  k=J+l where an is a constant that normalizes P/(dn ). [sent-88, score-0.045]
</p><p>48 Assuming the inactive symptoms have been absorbed, we have J  N  N  k=l  P(dls) ex  n=l  n=l  [II (1 - PkO II p~~)] [II P/(dn)]. [sent-89, score-0.247]
</p><p>49 (6)  The term in brackets on the left does not have a product form. [sent-90, score-0.076]
</p><p>50 The entire expression can be multiplied out to give a sum of 2J product forms, and exact "QuickS core" inference can be performed by combining the results of exact inference in each of the 2J product forms (Heckerman 1989). [sent-91, score-0.468]
</p><p>51 3  Sequential inference using inclusive variational trees  As described above, many variational methods minimize D(QIIP), and find approximations that exclude some modes of the posterior distribution. [sent-93, score-1.197]
</p><p>52 We present a method that minimizes D(PIIQ) sequentially - by absorbing one observation at a time - so as to not exclude modes of the posterior. [sent-94, score-0.476]
</p><p>53 Also, we approximate the posterior distribution with a tree. [sent-95, score-0.175]
</p><p>54 (Directed and undirected trees are equivalent we use a directed representation, where each variable has at most one parent. [sent-96, score-0.113]
</p><p>55 ) The algorithm absorbs one active symptom at a time, producing a new tree by searching for the tree that is closest - in the D(PIIQ) sense - to the product of the previous tree and the likelihood for the next symptom. [sent-97, score-0.945]
</p><p>56 This search can be performed efficiently in O(N 2 ) time using probability propagation in two versions of the previous tree to compute weights for edges of a new tree, and then applying a minimum-weight spanning-tree algorithm. [sent-98, score-0.323]
</p><p>57 Let Tk(d) be the tree approximation obtained after absorbing the kth symptom, Sk = 1. [sent-99, score-0.253]
</p><p>58 Initially, we take To(d) to be a tree that decouples the variables and has marginals equal to the marginals obtained by absorbing the inactive symptoms, as described above. [sent-100, score-0.487]
</p><p>59 Interpreting the tree Tk-l (d) from the previous step as the current "prior" over the diseases, we use the likelihood P(Sk = lid) for the next symptom to obtain a new estimate of the posterior: N  A(dls 1 , . [sent-101, score-0.436]
</p><p>60 Let the new tree be Tk(d) = TIn T k (d n ld1rk (n)), where 7rk (n) is the index of the parent of d n in the new tree. [sent-105, score-0.216]
</p><p>61 The parent function 7rk (n) and the conditional probability tables of Tk (d) are found by minimizing (8)  Ignoring constants, we have D(FkIITk) = -  2:Fk(dls1, . [sent-106, score-0.167]
</p><p>62 ,Sk) log Tk (d)  d  = - 2: (Tk- 1 (d) -  TLl(d)) log (II Tk(dnld1fk(n))) n  d  = -  2: (2:(Tk-l(d) n  = - 2:(2: n  -  TLl(d)) 10gTk(dnld1fk(n)))  d  2:  (Tk-l(dn,d1fk(n)) -  T~_l(dn,d1fk(n))) 10gTk(dnld1fk(n))). [sent-109, score-0.062]
</p><p>63 dn d"k(n)  For a given structure (parent function 7l"k(n)), the optimal conditional probability tables are (9)  where f3n is a constant that ensures Ldn T k (dn ld1fk (n)) = 1. [sent-110, score-0.285]
</p><p>64 This table is easily computed using probability propagation in the two trees to compute the two marginals needed in the difference. [sent-111, score-0.258]
</p><p>65 The optimal conditional probability table for a variable is independent of the parentchild relationships in the remainder of the network. [sent-112, score-0.04]
</p><p>66 So, for the current symptom, we compute the optimal conditional probability tables for all N(N - 1)/2 possible parent-child relationships in O(N2) time using probability propagation. [sent-113, score-0.133]
</p><p>67 Then, we use a minimum-weight directed spanning tree algorithm (Bock 1971) to search for the best tree. [sent-114, score-0.266]
</p><p>68 Once all of the symptoms have been absorbed, we use the final tree distribution, TJ(d) to make inferences about d given s. [sent-115, score-0.332]
</p><p>69 The order in which the symptoms are  absorbed will generally affect the quality of the resulting tree (Jaakkola and Jordan 1999), but we used a random ordering in the experiments reported below. [sent-116, score-0.411]
</p><p>70 4  Results on QMR-DT type networks  Using the structural and parameter statistics of the QMR-DT network given in Shwe et al. [sent-117, score-0.1]
</p><p>71 (1991) we simulated 30 QMR-DT type networks with roughly 600 diseases each. [sent-118, score-0.35]
</p><p>72 There were 10 networks in each of 3 groups with 5, 10 and 15 instantiated active symptoms. [sent-119, score-0.147]
</p><p>73 We chose the number of active symptoms to be small enough that we can compare our approximate method with the exact QuickScore method (Heckerman 1989). [sent-120, score-0.385]
</p><p>74 We also tried two other approximate inference methods: local probability propagation (Murphy et al. [sent-121, score-0.403]
</p><p>75 1999) and a variational upper bound (Jaakkola and Jordan 1999). [sent-122, score-0.199]
</p><p>76 For medical diagnosis, an important question is how many most probable diseases n' under the approximate posterior must be examined before the most probable n diseases under the exact posterior are found. [sent-123, score-1.249]
</p><p>77 An exact inference algorithm will give n' = n, whereas an approximate algorithm that mistakenly ranks the most probable disease last will give n' = N. [sent-125, score-0.539]
</p><p>78 For each group of networks and each inference method, we averaged the 10 values of n' for each value of n. [sent-126, score-0.191]
</p><p>79 3 shows the average of n' versus n for 5, 10 and 15 active symptoms. [sent-128, score-0.091]
</p><p>80 The sequential tree-fitting method is closest to optimal (n' = n) in all cases. [sent-129, score-0.082]
</p><p>81 The right column of plots shows the "extra work" caused by the excess number of diseases n' - n that must be examined for the approximate methods. [sent-130, score-0.419]
</p><p>82 5 positive findings  5 positive findings  300  ~  100 , ' ,  ~  r ,  250 200  c  '-:. [sent-131, score-0.158]
</p><p>83 '  'c  I  150 100 ,  50  100  150  200  250  50  300  100  10 positive findings  150  200  250  10 positive findings 350 ,-----~~-~~~-~~-~~_____,  : '~J  300  . [sent-132, score-0.158]
</p><p>84 Approximate methods include the sequential tree-fitting method presented in this paper (tree), local probability propagation (pp) and a variational upper bound (ub). [sent-135, score-0.398]
</p><p>85 5  Summary  Noisy-OR networks can be used to model a variety of problems, including medical diagnosis. [sent-136, score-0.201]
</p><p>86 Exact inference in large, richly connected noisy-OR networks is intractable, and most approximate inference algorithms tend to concentrate on a small number of most probable configurations of the hidden variables under the posterior. [sent-137, score-0.766]
</p><p>87 We presented an "inclusive" variational method for bipartite noisy-OR networks that favors including all probable configurations, at the cost of including some improbable configurations. [sent-138, score-0.593]
</p><p>88 The method fits a tree to the posterior distribution sequentially, i. [sent-139, score-0.274]
</p><p>89 Results on an ensemble of QMR-DT type networks show that the method performs better than local probability propagation and a variational upper bound for ranking most probable diseases. [sent-142, score-0.515]
</p><p>90 An algorithm to construct a minimum directed spanning tree in a directed network. [sent-151, score-0.32]
</p><p>91 Loopy belief propagation for approximate inference: An empirical study. [sent-237, score-0.214]
</p><p>92 Probabilistic diagnosis using a reformulation of the INTERNIST-1/QMR knowledge base I. [sent-276, score-0.133]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sk', 0.307), ('diseases', 0.294), ('symptom', 0.262), ('modes', 0.218), ('variational', 0.199), ('dn', 0.192), ('qiip', 0.181), ('tree', 0.174), ('frey', 0.16), ('symptoms', 0.158), ('disease', 0.157), ('piiq', 0.157), ('pko', 0.157), ('jordan', 0.156), ('inference', 0.135), ('diagnosis', 0.133), ('exclude', 0.131), ('medical', 0.113), ('probable', 0.111), ('propagation', 0.109), ('inclusive', 0.105), ('jaakkola', 0.093), ('active', 0.091), ('inactive', 0.089), ('bipartite', 0.084), ('saul', 0.082), ('findings', 0.079), ('absorbed', 0.079), ('absorbing', 0.079), ('dls', 0.079), ('favors', 0.079), ('shwe', 0.079), ('tll', 0.079), ('configurations', 0.077), ('approximate', 0.075), ('tk', 0.073), ('posterior', 0.07), ('neal', 0.067), ('heckerman', 0.067), ('exact', 0.061), ('trees', 0.059), ('murphy', 0.057), ('ub', 0.057), ('networks', 0.056), ('ghahramani', 0.055), ('directed', 0.054), ('tables', 0.053), ('mackay', 0.053), ('bock', 0.052), ('negatives', 0.052), ('noisyor', 0.052), ('examined', 0.05), ('marginals', 0.05), ('sequential', 0.05), ('sequentially', 0.048), ('activate', 0.048), ('carlo', 0.047), ('monte', 0.047), ('weiss', 0.046), ('tend', 0.045), ('pkn', 0.045), ('richly', 0.045), ('excluding', 0.045), ('gallager', 0.045), ('variables', 0.045), ('plausible', 0.044), ('et', 0.044), ('minimize', 0.044), ('concentrate', 0.042), ('parent', 0.042), ('pp', 0.041), ('tempered', 0.041), ('mceliece', 0.041), ('substructures', 0.041), ('lid', 0.041), ('probability', 0.04), ('fails', 0.04), ('indicates', 0.039), ('graphical', 0.039), ('product', 0.038), ('vanishing', 0.038), ('spanning', 0.038), ('brackets', 0.038), ('infinity', 0.038), ('intractable', 0.037), ('approximations', 0.037), ('attias', 0.036), ('ii', 0.036), ('hinton', 0.034), ('minimizing', 0.032), ('false', 0.032), ('closest', 0.032), ('pearl', 0.032), ('including', 0.032), ('log', 0.031), ('probabilistic', 0.031), ('target', 0.03), ('distribution', 0.03), ('belief', 0.03), ('chain', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="115-tfidf-1" href="./nips-2000-Sequentially_Fitting_%60%60Inclusive%27%27_Trees_for_Inference_in_Noisy-OR_Networks.html">115 nips-2000-Sequentially Fitting ``Inclusive'' Trees for Inference in Noisy-OR Networks</a></p>
<p>Author: Brendan J. Frey, Relu Patrascu, Tommi Jaakkola, Jodi Moran</p><p>Abstract: An important class of problems can be cast as inference in noisyOR Bayesian networks, where the binary state of each variable is a logical OR of noisy versions of the states of the variable's parents. For example, in medical diagnosis, the presence of a symptom can be expressed as a noisy-OR of the diseases that may cause the symptom - on some occasions, a disease may fail to activate the symptom. Inference in richly-connected noisy-OR networks is intractable, but approximate methods (e .g., variational techniques) are showing increasing promise as practical solutions. One problem with most approximations is that they tend to concentrate on a relatively small number of modes in the true posterior, ignoring other plausible configurations of the hidden variables. We introduce a new sequential variational method for bipartite noisyOR networks, that favors including all modes of the true posterior and models the posterior distribution as a tree. We compare this method with other approximations using an ensemble of networks with network statistics that are comparable to the QMR-DT medical diagnostic network. 1 Inclusive variational approximations Approximate algorithms for probabilistic inference are gaining in popularity and are now even being incorporated into VLSI hardware (T. Richardson, personal communication). Approximate methods include variational techniques (Ghahramani and Jordan 1997; Saul et al. 1996; Frey and Hinton 1999; Jordan et al. 1999), local probability propagation (Gallager 1963; Pearl 1988; Frey 1998; MacKay 1999a; Freeman and Weiss 2001) and Markov chain Monte Carlo (Neal 1993; MacKay 1999b). Many algorithms have been proposed in each of these classes. One problem that most of the above algorithms suffer from is a tendency to concentrate on a relatively small number of modes of the target distribution (the distribution being approximated). In the case of medical diagnosis, different modes correspond to different explanations of the symptoms. Markov chain Monte Carlo methods are usually guaranteed to eventually sample from all the modes, but this may take an extremely long time, even when tempered transitions (Neal 1996) are (a) ,,</p><p>2 0.19717777 <a title="115-tfidf-2" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>Author: Zoubin Ghahramani, Matthew J. Beal</p><p>Abstract: Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set. 1</p><p>3 0.1427207 <a title="115-tfidf-3" href="./nips-2000-Accumulator_Networks%3A_Suitors_of_Local_Probability_Propagation.html">15 nips-2000-Accumulator Networks: Suitors of Local Probability Propagation</a></p>
<p>Author: Brendan J. Frey, Anitha Kannan</p><p>Abstract: One way to approximate inference in richly-connected graphical models is to apply the sum-product algorithm (a.k.a. probability propagation algorithm), while ignoring the fact that the graph has cycles. The sum-product algorithm can be directly applied in Gaussian networks and in graphs for coding, but for many conditional probability functions - including the sigmoid function - direct application of the sum-product algorithm is not possible. We introduce</p><p>4 0.13830864 <a title="115-tfidf-4" href="./nips-2000-Tree-Based_Modeling_and_Estimation_of_Gaussian_Processes_on_Graphs_with_Cycles.html">140 nips-2000-Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles</a></p>
<p>Author: Martin J. Wainwright, Erik B. Sudderth, Alan S. Willsky</p><p>Abstract: We present the embedded trees algorithm, an iterative technique for estimation of Gaussian processes defined on arbitrary graphs. By exactly solving a series of modified problems on embedded spanning trees, it computes the conditional means with an efficiency comparable to or better than other techniques. Unlike other methods, the embedded trees algorithm also computes exact error covariances. The error covariance computation is most efficient for graphs in which removing a small number of edges reveals an embedded tree. In this context, we demonstrate that sparse loopy graphs can provide a significant increase in modeling power relative to trees, with only a minor increase in estimation complexity. 1</p><p>5 0.10505985 <a title="115-tfidf-5" href="./nips-2000-Second_Order_Approximations_for_Probability_Models.html">114 nips-2000-Second Order Approximations for Probability Models</a></p>
<p>Author: Hilbert J. Kappen, Wim Wiegerinck</p><p>Abstract: In this paper, we derive a second order mean field theory for directed graphical probability models. By using an information theoretic argument it is shown how this can be done in the absense of a partition function. This method is a direct generalisation of the well-known TAP approximation for Boltzmann Machines. In a numerical example, it is shown that the method greatly improves the first order mean field approximation. For a restricted class of graphical models, so-called single overlap graphs, the second order method has comparable complexity to the first order method. For sigmoid belief networks, the method is shown to be particularly fast and effective.</p><p>6 0.10115688 <a title="115-tfidf-6" href="./nips-2000-A_Variational_Mean-Field_Theory_for_Sigmoidal_Belief_Networks.html">14 nips-2000-A Variational Mean-Field Theory for Sigmoidal Belief Networks</a></p>
<p>7 0.090141006 <a title="115-tfidf-7" href="./nips-2000-Error-correcting_Codes_on_a_Bethe-like_Lattice.html">47 nips-2000-Error-correcting Codes on a Bethe-like Lattice</a></p>
<p>8 0.088115305 <a title="115-tfidf-8" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>9 0.079071842 <a title="115-tfidf-9" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>10 0.076246418 <a title="115-tfidf-10" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>11 0.065933987 <a title="115-tfidf-11" href="./nips-2000-Learning_Switching_Linear_Models_of_Human_Motion.html">80 nips-2000-Learning Switching Linear Models of Human Motion</a></p>
<p>12 0.056011084 <a title="115-tfidf-12" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>13 0.054992255 <a title="115-tfidf-13" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>14 0.054072075 <a title="115-tfidf-14" href="./nips-2000-Generalized_Belief_Propagation.html">62 nips-2000-Generalized Belief Propagation</a></p>
<p>15 0.053506255 <a title="115-tfidf-15" href="./nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">146 nips-2000-What Can a Single Neuron Compute?</a></p>
<p>16 0.05282503 <a title="115-tfidf-16" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>17 0.052334413 <a title="115-tfidf-17" href="./nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>18 0.051635958 <a title="115-tfidf-18" href="./nips-2000-Mixtures_of_Gaussian_Processes.html">85 nips-2000-Mixtures of Gaussian Processes</a></p>
<p>19 0.051052671 <a title="115-tfidf-19" href="./nips-2000-On_Reversing_Jensen%27s_Inequality.html">94 nips-2000-On Reversing Jensen's Inequality</a></p>
<p>20 0.050109375 <a title="115-tfidf-20" href="./nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.185), (1, -0.018), (2, 0.138), (3, -0.084), (4, 0.234), (5, -0.017), (6, 0.039), (7, 0.048), (8, -0.026), (9, 0.1), (10, 0.109), (11, 0.06), (12, -0.142), (13, 0.037), (14, -0.074), (15, -0.085), (16, -0.058), (17, 0.181), (18, -0.063), (19, 0.037), (20, -0.064), (21, 0.085), (22, 0.069), (23, 0.016), (24, -0.008), (25, -0.101), (26, 0.007), (27, -0.048), (28, -0.036), (29, 0.054), (30, -0.066), (31, 0.089), (32, 0.034), (33, 0.043), (34, -0.072), (35, -0.175), (36, -0.01), (37, 0.065), (38, 0.008), (39, 0.046), (40, 0.077), (41, 0.121), (42, 0.125), (43, 0.051), (44, -0.005), (45, -0.073), (46, 0.013), (47, 0.022), (48, 0.018), (49, -0.136)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95799214 <a title="115-lsi-1" href="./nips-2000-Sequentially_Fitting_%60%60Inclusive%27%27_Trees_for_Inference_in_Noisy-OR_Networks.html">115 nips-2000-Sequentially Fitting ``Inclusive'' Trees for Inference in Noisy-OR Networks</a></p>
<p>Author: Brendan J. Frey, Relu Patrascu, Tommi Jaakkola, Jodi Moran</p><p>Abstract: An important class of problems can be cast as inference in noisyOR Bayesian networks, where the binary state of each variable is a logical OR of noisy versions of the states of the variable's parents. For example, in medical diagnosis, the presence of a symptom can be expressed as a noisy-OR of the diseases that may cause the symptom - on some occasions, a disease may fail to activate the symptom. Inference in richly-connected noisy-OR networks is intractable, but approximate methods (e .g., variational techniques) are showing increasing promise as practical solutions. One problem with most approximations is that they tend to concentrate on a relatively small number of modes in the true posterior, ignoring other plausible configurations of the hidden variables. We introduce a new sequential variational method for bipartite noisyOR networks, that favors including all modes of the true posterior and models the posterior distribution as a tree. We compare this method with other approximations using an ensemble of networks with network statistics that are comparable to the QMR-DT medical diagnostic network. 1 Inclusive variational approximations Approximate algorithms for probabilistic inference are gaining in popularity and are now even being incorporated into VLSI hardware (T. Richardson, personal communication). Approximate methods include variational techniques (Ghahramani and Jordan 1997; Saul et al. 1996; Frey and Hinton 1999; Jordan et al. 1999), local probability propagation (Gallager 1963; Pearl 1988; Frey 1998; MacKay 1999a; Freeman and Weiss 2001) and Markov chain Monte Carlo (Neal 1993; MacKay 1999b). Many algorithms have been proposed in each of these classes. One problem that most of the above algorithms suffer from is a tendency to concentrate on a relatively small number of modes of the target distribution (the distribution being approximated). In the case of medical diagnosis, different modes correspond to different explanations of the symptoms. Markov chain Monte Carlo methods are usually guaranteed to eventually sample from all the modes, but this may take an extremely long time, even when tempered transitions (Neal 1996) are (a) ,,</p><p>2 0.6796869 <a title="115-lsi-2" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>Author: Zoubin Ghahramani, Matthew J. Beal</p><p>Abstract: Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set. 1</p><p>3 0.66904992 <a title="115-lsi-3" href="./nips-2000-Accumulator_Networks%3A_Suitors_of_Local_Probability_Propagation.html">15 nips-2000-Accumulator Networks: Suitors of Local Probability Propagation</a></p>
<p>Author: Brendan J. Frey, Anitha Kannan</p><p>Abstract: One way to approximate inference in richly-connected graphical models is to apply the sum-product algorithm (a.k.a. probability propagation algorithm), while ignoring the fact that the graph has cycles. The sum-product algorithm can be directly applied in Gaussian networks and in graphs for coding, but for many conditional probability functions - including the sigmoid function - direct application of the sum-product algorithm is not possible. We introduce</p><p>4 0.52701199 <a title="115-lsi-4" href="./nips-2000-Tree-Based_Modeling_and_Estimation_of_Gaussian_Processes_on_Graphs_with_Cycles.html">140 nips-2000-Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles</a></p>
<p>Author: Martin J. Wainwright, Erik B. Sudderth, Alan S. Willsky</p><p>Abstract: We present the embedded trees algorithm, an iterative technique for estimation of Gaussian processes defined on arbitrary graphs. By exactly solving a series of modified problems on embedded spanning trees, it computes the conditional means with an efficiency comparable to or better than other techniques. Unlike other methods, the embedded trees algorithm also computes exact error covariances. The error covariance computation is most efficient for graphs in which removing a small number of edges reveals an embedded tree. In this context, we demonstrate that sparse loopy graphs can provide a significant increase in modeling power relative to trees, with only a minor increase in estimation complexity. 1</p><p>5 0.42276028 <a title="115-lsi-5" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>Author: Gal Elidan, Noam Lotner, Nir Friedman, Daphne Koller</p><p>Abstract: A serious problem in learning probabilistic models is the presence of hidden variables. These variables are not observed, yet interact with several of the observed variables. As such, they induce seemingly complex dependencies among the latter. In recent years, much attention has been devoted to the development of algorithms for learning parameters, and in some cases structure, in the presence of hidden variables. In this paper, we address the related problem of detecting hidden variables that interact with the observed variables. This problem is of interest both for improving our understanding of the domain and as a preliminary step that guides the learning procedure towards promising models. A very natural approach is to search for</p><p>6 0.41807565 <a title="115-lsi-6" href="./nips-2000-A_Variational_Mean-Field_Theory_for_Sigmoidal_Belief_Networks.html">14 nips-2000-A Variational Mean-Field Theory for Sigmoidal Belief Networks</a></p>
<p>7 0.41530153 <a title="115-lsi-7" href="./nips-2000-Learning_Switching_Linear_Models_of_Human_Motion.html">80 nips-2000-Learning Switching Linear Models of Human Motion</a></p>
<p>8 0.38909891 <a title="115-lsi-8" href="./nips-2000-Second_Order_Approximations_for_Probability_Models.html">114 nips-2000-Second Order Approximations for Probability Models</a></p>
<p>9 0.35784674 <a title="115-lsi-9" href="./nips-2000-Error-correcting_Codes_on_a_Bethe-like_Lattice.html">47 nips-2000-Error-correcting Codes on a Bethe-like Lattice</a></p>
<p>10 0.34295195 <a title="115-lsi-10" href="./nips-2000-Generalized_Belief_Propagation.html">62 nips-2000-Generalized Belief Propagation</a></p>
<p>11 0.3327938 <a title="115-lsi-11" href="./nips-2000-On_Reversing_Jensen%27s_Inequality.html">94 nips-2000-On Reversing Jensen's Inequality</a></p>
<p>12 0.31984416 <a title="115-lsi-12" href="./nips-2000-Feature_Correspondence%3A_A_Markov_Chain_Monte_Carlo_Approach.html">53 nips-2000-Feature Correspondence: A Markov Chain Monte Carlo Approach</a></p>
<p>13 0.30468702 <a title="115-lsi-13" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>14 0.30250382 <a title="115-lsi-14" href="./nips-2000-Mixtures_of_Gaussian_Processes.html">85 nips-2000-Mixtures of Gaussian Processes</a></p>
<p>15 0.29759648 <a title="115-lsi-15" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>16 0.2729111 <a title="115-lsi-16" href="./nips-2000-A_Tighter_Bound_for_Graphical_Models.html">13 nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>17 0.2691057 <a title="115-lsi-17" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>18 0.25109398 <a title="115-lsi-18" href="./nips-2000-The_Interplay_of_Symbolic_and_Subsymbolic_Processes_in_Anagram_Problem_Solving.html">132 nips-2000-The Interplay of Symbolic and Subsymbolic Processes in Anagram Problem Solving</a></p>
<p>19 0.24960561 <a title="115-lsi-19" href="./nips-2000-Probabilistic_Semantic_Video_Indexing.html">103 nips-2000-Probabilistic Semantic Video Indexing</a></p>
<p>20 0.24729681 <a title="115-lsi-20" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.027), (17, 0.094), (32, 0.022), (33, 0.036), (48, 0.388), (54, 0.032), (55, 0.039), (62, 0.022), (65, 0.045), (67, 0.029), (76, 0.057), (79, 0.019), (81, 0.024), (90, 0.035), (91, 0.03), (97, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85955983 <a title="115-lda-1" href="./nips-2000-Sequentially_Fitting_%60%60Inclusive%27%27_Trees_for_Inference_in_Noisy-OR_Networks.html">115 nips-2000-Sequentially Fitting ``Inclusive'' Trees for Inference in Noisy-OR Networks</a></p>
<p>Author: Brendan J. Frey, Relu Patrascu, Tommi Jaakkola, Jodi Moran</p><p>Abstract: An important class of problems can be cast as inference in noisyOR Bayesian networks, where the binary state of each variable is a logical OR of noisy versions of the states of the variable's parents. For example, in medical diagnosis, the presence of a symptom can be expressed as a noisy-OR of the diseases that may cause the symptom - on some occasions, a disease may fail to activate the symptom. Inference in richly-connected noisy-OR networks is intractable, but approximate methods (e .g., variational techniques) are showing increasing promise as practical solutions. One problem with most approximations is that they tend to concentrate on a relatively small number of modes in the true posterior, ignoring other plausible configurations of the hidden variables. We introduce a new sequential variational method for bipartite noisyOR networks, that favors including all modes of the true posterior and models the posterior distribution as a tree. We compare this method with other approximations using an ensemble of networks with network statistics that are comparable to the QMR-DT medical diagnostic network. 1 Inclusive variational approximations Approximate algorithms for probabilistic inference are gaining in popularity and are now even being incorporated into VLSI hardware (T. Richardson, personal communication). Approximate methods include variational techniques (Ghahramani and Jordan 1997; Saul et al. 1996; Frey and Hinton 1999; Jordan et al. 1999), local probability propagation (Gallager 1963; Pearl 1988; Frey 1998; MacKay 1999a; Freeman and Weiss 2001) and Markov chain Monte Carlo (Neal 1993; MacKay 1999b). Many algorithms have been proposed in each of these classes. One problem that most of the above algorithms suffer from is a tendency to concentrate on a relatively small number of modes of the target distribution (the distribution being approximated). In the case of medical diagnosis, different modes correspond to different explanations of the symptoms. Markov chain Monte Carlo methods are usually guaranteed to eventually sample from all the modes, but this may take an extremely long time, even when tempered transitions (Neal 1996) are (a) ,,</p><p>2 0.71276677 <a title="115-lda-2" href="./nips-2000-Efficient_Learning_of_Linear_Perceptrons.html">44 nips-2000-Efficient Learning of Linear Perceptrons</a></p>
<p>Author: Shai Ben-David, Hans-Ulrich Simon</p><p>Abstract: We consider the existence of efficient algorithms for learning the class of half-spaces in ~n in the agnostic learning model (Le., making no prior assumptions on the example-generating distribution). The resulting combinatorial problem - finding the best agreement half-space over an input sample - is NP hard to approximate to within some constant factor. We suggest a way to circumvent this theoretical bound by introducing a new measure of success for such algorithms. An algorithm is IL-margin successful if the agreement ratio of the half-space it outputs is as good as that of any half-space once training points that are inside the IL-margins of its separating hyper-plane are disregarded. We prove crisp computational complexity results with respect to this success measure: On one hand, for every positive IL, there exist efficient (poly-time) IL-margin successful learning algorithms. On the other hand, we prove that unless P=NP, there is no algorithm that runs in time polynomial in the sample size and in 1/ IL that is IL-margin successful for all IL> O. 1</p><p>3 0.35140204 <a title="115-lda-3" href="./nips-2000-Accumulator_Networks%3A_Suitors_of_Local_Probability_Propagation.html">15 nips-2000-Accumulator Networks: Suitors of Local Probability Propagation</a></p>
<p>Author: Brendan J. Frey, Anitha Kannan</p><p>Abstract: One way to approximate inference in richly-connected graphical models is to apply the sum-product algorithm (a.k.a. probability propagation algorithm), while ignoring the fact that the graph has cycles. The sum-product algorithm can be directly applied in Gaussian networks and in graphs for coding, but for many conditional probability functions - including the sigmoid function - direct application of the sum-product algorithm is not possible. We introduce</p><p>4 0.33690345 <a title="115-lda-4" href="./nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">122 nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>Author: Lehel Csatč´¸, Manfred Opper</p><p>Abstract: We develop an approach for a sparse representation for Gaussian Process (GP) models in order to overcome the limitations of GPs caused by large data sets. The method is based on a combination of a Bayesian online algorithm together with a sequential construction of a relevant subsample of the data which fully specifies the prediction of the model. Experimental results on toy examples and large real-world data sets indicate the efficiency of the approach.</p><p>5 0.32859552 <a title="115-lda-5" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>Author: Zoubin Ghahramani, Matthew J. Beal</p><p>Abstract: Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set. 1</p><p>6 0.32634154 <a title="115-lda-6" href="./nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">10 nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>7 0.31625581 <a title="115-lda-7" href="./nips-2000-Mixtures_of_Gaussian_Processes.html">85 nips-2000-Mixtures of Gaussian Processes</a></p>
<p>8 0.31411597 <a title="115-lda-8" href="./nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">74 nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>9 0.31330764 <a title="115-lda-9" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>10 0.31316483 <a title="115-lda-10" href="./nips-2000-Tree-Based_Modeling_and_Estimation_of_Gaussian_Processes_on_Graphs_with_Cycles.html">140 nips-2000-Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles</a></p>
<p>11 0.31140697 <a title="115-lda-11" href="./nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">68 nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>12 0.31100377 <a title="115-lda-12" href="./nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">95 nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>13 0.31009573 <a title="115-lda-13" href="./nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">37 nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>14 0.31004903 <a title="115-lda-14" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>15 0.3093113 <a title="115-lda-15" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>16 0.30835772 <a title="115-lda-16" href="./nips-2000-Gaussianization.html">60 nips-2000-Gaussianization</a></p>
<p>17 0.30832651 <a title="115-lda-17" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>18 0.3068974 <a title="115-lda-18" href="./nips-2000-Some_New_Bounds_on_the_Generalization_Error_of_Combined_Classifiers.html">119 nips-2000-Some New Bounds on the Generalization Error of Combined Classifiers</a></p>
<p>19 0.30669439 <a title="115-lda-19" href="./nips-2000-Constrained_Independent_Component_Analysis.html">36 nips-2000-Constrained Independent Component Analysis</a></p>
<p>20 0.30382058 <a title="115-lda-20" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
