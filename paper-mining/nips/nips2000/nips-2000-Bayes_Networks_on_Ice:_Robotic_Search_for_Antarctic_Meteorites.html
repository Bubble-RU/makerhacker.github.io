<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>29 nips-2000-Bayes Networks on Ice: Robotic Search for Antarctic Meteorites</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-29" href="#">nips2000-29</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>29 nips-2000-Bayes Networks on Ice: Robotic Search for Antarctic Meteorites</h1>
<br/><p>Source: <a title="nips-2000-29-pdf" href="http://papers.nips.cc/paper/1798-bayes-networks-on-ice-robotic-search-for-antarctic-meteorites.pdf">pdf</a></p><p>Author: Liam Pedersen, Dimitrios Apostolopoulos, William Whittaker</p><p>Abstract: A Bayes network based classifier for distinguishing terrestrial rocks from meteorites is implemented onboard the Nomad robot. Equipped with a camera, spectrometer and eddy current sensor, this robot searched the ice sheets of Antarctica and autonomously made the first robotic identification of a meteorite, in January 2000 at the Elephant Moraine. This paper discusses rock classification from a robotic platform, and describes the system onboard Nomad. 1</p><p>Reference: <a title="nips-2000-29-reference" href="../nips2000_reference/nips-2000-Bayes_Networks_on_Ice%3A_Robotic_Search_for_Antarctic_Meteorites_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract  A Bayes network based classifier for distinguishing terrestrial rocks from meteorites is implemented onboard the Nomad robot. [sent-3, score-1.091]
</p><p>2 Equipped with a camera, spectrometer and eddy current sensor, this robot searched the ice sheets of Antarctica and autonomously made the first robotic identification of a meteorite, in January 2000 at the Elephant Moraine. [sent-4, score-0.705]
</p><p>3 This paper discusses rock classification from a robotic platform, and describes the system onboard Nomad. [sent-5, score-0.558]
</p><p>4 1  Introduction  Figure 1 : Human meteorite search with snowmobiles on the Antarctic ice sheets, and on foot in the moraines. [sent-6, score-0.65]
</p><p>5 Antarctica contains the most fertile meteorite hunting grounds on Earth. [sent-7, score-0.357]
</p><p>6 The pristine, dry and cold environment ensures that meteorites deposited there are preserved for long periods. [sent-8, score-0.376]
</p><p>7 Subsequent glacial flow of the ice sheets where they land concentrates them in particular areas. [sent-9, score-0.225]
</p><p>8 To date, most meteorites recovered throughout history have been done so in Antarctica in the last 20 years. [sent-10, score-0.376]
</p><p>9 Furthermore, they are less likely to be contaminated by terrestrial compounds . [sent-11, score-0.151]
</p><p>10 edu/-pedersen  Meteorites are of interest to space scientists because, with the exception of the Apollo lunar samples, they are the sole source of extra-terrestrial material and a window on the early evolution of the solar system. [sent-15, score-0.038]
</p><p>11 The identification of Martian and lunar meteorite samples, and the (controversial) evidence of fossil bacteria in the former underscores the importance of systematically retrieving as many samples as possible. [sent-16, score-0.467]
</p><p>12 Currently, Antarctic meteorite samples are collected by human searchers, either on foot, or on snowmobiles, who systematically search an area and retrieve samples according to strict protocols. [sent-17, score-0.544]
</p><p>13 In certain blue ice fields the only rocks visible are meteorites. [sent-18, score-0.551]
</p><p>14 At other places (moraines - areas where the ice flow brings rocks to the surface) searchers have to contend with many terrestrial rocks (Figure 1). [sent-19, score-1.147]
</p><p>15 1  Robotic search for Antarctic meteorites  color camera  reflectance spectrometer  Figure 2 : Nomad robot, equipped with scientific instruments, investigates a rock in Antarctica. [sent-21, score-1.135]
</p><p>16 With the goal of autonomously search for meteorites in Antarctica, Carnegie Mellon University has built and demonstrated [1] a robot, Nomad (Figure 2), capable of long duration missions in harsh environments. [sent-22, score-0.484]
</p><p>17 Nomad is equipped with a color camera on a pan-tilt platform to survey the ice for rocks and acquire close up images of any candidate objects, and a manipulator arm to place the fiber optic probe of a specially designed visible light reflectance spectrometer over a sample. [sent-23, score-1.148]
</p><p>18 The manipulator arm can also place other sensors, such a metal detector. [sent-24, score-0.12]
</p><p>19 The eventual goal, beyond Antarctic meteorite search, is to develop technologies for extended robotic exploration of remote areas, including planetary surfaces. [sent-25, score-0.52]
</p><p>20 One particular technology is the capacity to carry out autonomous science, including autonomous geology and the ability to recognize a broad range of rock types and note exceptions. [sent-26, score-0.446]
</p><p>21 Identifying meteorites amongst terrestrial rocks is the fundamental engineering problem of robotic meteorite search and is the topic addressed by the rest of this paper. [sent-27, score-1.509]
</p><p>22 2  Bayes network rock and meteorite classifier  Classifying rocks from a mobile robotic vehicle entails several unique issues: •  The classifier must learn from examples. [sent-28, score-1.46]
</p><p>23 In the words of a veteran Antarctic meteorite searcher [2] "First you find a few meteorites, then you know what to look for". [sent-30, score-0.357]
</p><p>24 A complication is the difficulty of acquiring large sets of training data, under realistic field conditions. [sent-31, score-0.128]
</p><p>25 To date this has required two earlier expeditions to Antarctica, as well as visits to the Arctic and the Atacama desert in Chile. [sent-32, score-0.064]
</p><p>26 Therefore, it is necessary to constrain a classifier as much as possible with available prior knowledge, so that training can be accomplished with minimum data. [sent-33, score-0.118]
</p><p>27 •  The classifier must be able to accept incomplete data, and compound evidence for different hypotheses as more information becomes available. [sent-34, score-0.149]
</p><p>28 The robot has multiple sensors, and there is a cost associated with using each one. [sent-35, score-0.121]
</p><p>29 Sensors such as the spectrometer are particularly expensive to use because the robot must be maneuvered to bring the rock sample into the sensor manipulator workspace. [sent-36, score-0.828]
</p><p>30 Therefore, it is desirable that initial classifications be made using data from cheap long range sensors, such as a color camera, before final verification using expensive sensors on promising rock samples. [sent-37, score-0.538]
</p><p>31 A corollary of this is that the classifier should accept prior evidence from other sources, such as an experts knowledge on what to expect in a particular location. [sent-38, score-0.117]
</p><p>32 The classifier must handle this ambiguity, and indicate several likely hypotheses if a definite classification cannot be achieved. [sent-40, score-0.158]
</p><p>33 These requirements for a robotic rock classifier argue strongly in favor of a Bayes network based approach, which can satisfy them all. [sent-41, score-0.627]
</p><p>34 The intuitive graphical structure of a Bayes network makes it easier to encode physical constraints into the network topology, thus reducing the intrinsic dimensionality. [sent-42, score-0.1]
</p><p>35 Bayesian update is a principled way to compound evidence, and prior information is naturally represented by prior probabilities. [sent-43, score-0.032]
</p><p>36 Additionally, with a Bayes network it is simple to compute the likelihood of any new data, and thus conceivably recognize bad sensor readings. [sent-44, score-0.286]
</p><p>37 Furthermore, the network can be queried to estimate the information gain of further sensor readings, enabling active sensor selection. [sent-45, score-0.44]
</p><p>38 1  Network architecture  The (simplified) network architecture for distinguishing rocks from meteorites, using features from sensor data, is shown in Figure 3. [sent-47, score-0.667]
</p><p>39 It is a compromise between a fully connected network (no constraints whatsoever, and computationally intractable) and a naive Bayes classifier (can be efficiently evaluated, but lacks sufficient representational power). [sent-48, score-0.144]
</p><p>40 Sensor features are only weakly (conditionally) dependent on each other because of a careful choice of suitable features, and the intermediate node Rock-type, whose states include all possible rock and meteorite types likely to be encountered by the classifier. [sent-49, score-0.74]
</p><p>41 A complication is that the sensor features are continuous quantities, yet the Bayes network implementation can only handle discrete variables. [sent-50, score-0.344]
</p><p>42 Rock/Meteorite type  True  False  Iron meteorite Sandstone  Figure 3 : Bayes network for discriminating meteorites and rocks based on features computed from sensor data. [sent-53, score-1.4]
</p><p>43 5  strengt of peak (+) or trough (-) at given wavelength  0  400  600 800 wavelength /[nm]  1000  Figure 4 : Example spectrum (with extracted features) and color images of rocks on ice. [sent-63, score-0.728]
</p><p>44 In Antarctica Nomad acquired reflectance spectra and color images (Figure 4) of sample rocks. [sent-65, score-0.41]
</p><p>45 Spectra are obtained by shining white light on the sample and analyzing the reflected light to determine the fraction of light reflected at a series of wavelengths. [sent-66, score-0.152]
</p><p>46 The relevant features in a spectrum, for the purpose of identifying rocks, are the presence, location and size of peaks and troughs in the spectrum (Figure 4), and the average magnitude (albedo) of the spectrum over certain wavelengths. [sent-67, score-0.254]
</p><p>47 Spectral troughs and peaks are detected by computing the correlation of the spectrum with a set of 10 templates over a finite region of support (50 nm). [sent-68, score-0.165]
</p><p>48 Restricting the degree of overlap between templates minimizes statistical dependencies between the resulting spectral features (Figure 3). [sent-69, score-0.103]
</p><p>49 Normalizing the correlation coefficients makes them (conditionally) independent of the average spectral intensity and robust to changes to scale (important, because in practice, when making a field measurement of a spectrum it is difficult to accurately determine the scale). [sent-70, score-0.152]
</p><p>50 A 13 element real valued feature vector (each component corresponding to a sensor feature node in Figure 3) is thus obtained from the original 1000+ element spectrum. [sent-71, score-0.307]
</p><p>51 Color images are harder to interpret (one of the rocks in Figure 4 is a meteorite). [sent-72, score-0.421]
</p><p>52 First the rock needs to be segmented from the background of snow and ice in the  image, using a partially observable Markov model [4]. [sent-73, score-0.489]
</p><p>53 Features of interest are the rock cross sectional area (used as a proxy for size, and requiring that the scaling of the images be known), average color, and simple texture and shape metrics [4]. [sent-74, score-0.409]
</p><p>54 Meteorites tend to be small and dark compared to terrestrial rocks. [sent-75, score-0.151]
</p><p>55 An 8 element real valued feature vector is computed from each image. [sent-76, score-0.065]
</p><p>56 All real valued features are quantized prior to being entered into the Bayes network, which cannot handle continuous quantities. [sent-77, score-0.14]
</p><p>57 If X is a node (with N states) with parent Y, and with CPM Pij = P(X=iIY=j), then each column is represented by a Dirichlet distribution (initially uniform) and assumed independent of the others. [sent-80, score-0.023]
</p><p>58 Furthermore, it is possible to weight each training sample to reflect its frequency of occurrence for the rock type that generated it. [sent-84, score-0.367]
</p><p>59 This is especially important if multiple sensor readings are taken from a single sample 1  -. [sent-85, score-0.312]
</p><p>60 Image ::~~ors  1  Figure 5 : Classifier rate of classification curves using laboratory data for training and testing (25% cross validation), for different sensors. [sent-474, score-0.113]
</p><p>61 The training data (gathered from previous Antarctic expeditions, and from US laboratory collections· of meteorites and Antarctic rocks) is insufficient to fully populate the (quantized) space on which the CPM's are defmed, unless the real valued feature nodes are very coarsely quantized. [sent-475, score-0.441]
</p><p>62 To avoid this, more spectral data was generated from each sample spectra by adding random noise (generated by a • Johnson Space Center, Houston and Ohio State University, Columbus. [sent-476, score-0.154]
</p><p>63 (This is analogous to the approach used by [7] for training neural networks). [sent-478, score-0.024]
</p><p>64 Using meteorite and terrestrial rock data acquired in the lab, partitioned into 75% training, 25% testing cross validation sets, the Rate of Classification (ROC) curves in Figure 5 are generated. [sent-479, score-0.949]
</p><p>65 Note the superior classification with spectra versus classification with color images only. [sent-480, score-0.336]
</p><p>66 In fact, given a spectrum, a color image does not improve classification. [sent-481, score-0.126]
</p><p>67 However, because it is easier to acquire color images than spectra, they are still useful as a sensor for preliminary screening. [sent-482, score-0.389]
</p><p>68 3 Antarctica 2000 field results In January 2000 the Nomad robot was deployed to the Elephant moraine in Antarctica for robotic meteorite searching trials. [sent-483, score-0.681]
</p><p>69 Nomad searched areas known to contain meteorites, autonomously acquiring color images and reflection spectra of both native terrestrial rocks and meteorites, and classifying them. [sent-484, score-0.964]
</p><p>70 On January 22, 2000 Nomad successfully identified a meteorite amongst terrestrial rocks on the ice sheet (http://www. [sent-485, score-1.085]
</p><p>71 5  :::1)  1  false positive rate  Figure 6: Rate of classification curves for the Nomad robot searching for meteorites in Antarctica, 2000 A. [sent-513, score-0.595]
</p><p>72 Overall performance (using spectra only, due to a problem that developed with camera zoom control) is indicated by the ROC performance curves in Figure 6. [sent-515, score-0.173]
</p><p>73 These were generated from a test set of rocks and meteorites (40 and 4 samples respectively, with multiple readings of each) in a particular area of the moraine. [sent-516, score-0.913]
</p><p>74 Figure 6(i) is using the a priori classifier built from the lab data (used to generate Figure 5), acquired prior to arrival in Antarctica. [sent-517, score-0.163]
</p><p>75 There is a notable improvement in (ii), the ROC curve for the same classifier further trained with field data acquired by the robot in the area (from 8 rocks and 2 meteorites not in this test set). [sent-519, score-1.107]
</p><p>76 Even with retraining, classification is systematically bad for a particular class of rocks (hydro-thermally altered dolerites and basalts) that occurred in the Elephant moraine. [sent-520, score-0.52]
</p><p>77 These rocks are stained red with iron oxide (rust) whose spectrum has a very prominent peak at 900 nm, precisely where many meteorite spectra also have a peak. [sent-521, score-1.073]
</p><p>78 This is not surprising, given that most meteorites contain metallic iron, and  therefore can have rust on the surface. [sent-522, score-0.414]
</p><p>79 However, these rocks were absent from the initial training set and not initially expected in this area. [sent-523, score-0.406]
</p><p>80 Performance is much better if these rocks are removed from the test set (iii) and the retrained classifier is used. [sent-524, score-0.476]
</p><p>81 4  Conclusions  With the caveat that training be continued using data acquired by the robot in the field, the Bayes network approach to robotic rock classification is a viable approach to this task. [sent-525, score-0.784]
</p><p>82 However, in areas with hydro-thermally altered rocks (iron-oxide stained) the reflection spectrometer must be supplemented by other sensors, such as metal detectors, magnetometers or more exotic spectrometers (thermal emission or Raman), obviously at greater cost. [sent-527, score-0.605]
</p><p>83 Sensor noise and systematic effects due to autonomous robot placement of sensors on samples in the unstructured and uncontrolled polar environment are significant. [sent-528, score-0.322]
</p><p>84 They are hard to know a priori and need to be learned from data acquired by the robot, and in field conditions, as demonstrated by the significant improvement in classification achieved after field retraining. [sent-529, score-0.186]
</p><p>85 Further work needs to be done in selective sensor selection, active modeling of the local geographical distribution of rocks, and recognizing bad sensor readings, but indications are that this can be done in a principled way with the Bayes network classifier and will be addressed in future papers. [sent-530, score-0.575]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rocks', 0.382), ('meteorites', 0.376), ('meteorite', 0.357), ('rock', 0.32), ('sensor', 0.195), ('antarctic', 0.188), ('antarctica', 0.188), ('nomad', 0.188), ('ice', 0.169), ('robotic', 0.163), ('terrestrial', 0.151), ('color', 0.126), ('robot', 0.121), ('spectrometer', 0.113), ('spectra', 0.097), ('readings', 0.094), ('classifier', 0.094), ('sensors', 0.092), ('spectrum', 0.078), ('bayes', 0.071), ('acquired', 0.069), ('apostolopoulos', 0.056), ('cpm', 0.056), ('elephant', 0.056), ('iron', 0.056), ('manipulator', 0.056), ('reflectance', 0.056), ('sheets', 0.056), ('whittaker', 0.056), ('autonomously', 0.054), ('search', 0.054), ('network', 0.05), ('camera', 0.049), ('roc', 0.049), ('pittsburgh', 0.046), ('dirichlet', 0.044), ('autonomous', 0.044), ('bad', 0.041), ('equipped', 0.041), ('valued', 0.041), ('features', 0.04), ('field', 0.04), ('images', 0.039), ('red', 0.038), ('cassidy', 0.038), ('expeditions', 0.038), ('geology', 0.038), ('lunar', 0.038), ('onboard', 0.038), ('pedersen', 0.038), ('professor', 0.038), ('rust', 0.038), ('searchers', 0.038), ('snowmobiles', 0.038), ('stained', 0.038), ('wavelength', 0.038), ('classification', 0.037), ('systematically', 0.036), ('nm', 0.036), ('january', 0.036), ('samples', 0.036), ('spectral', 0.034), ('false', 0.034), ('carnegie', 0.034), ('mellon', 0.034), ('acquiring', 0.032), ('arm', 0.032), ('complication', 0.032), ('compound', 0.032), ('foot', 0.032), ('metal', 0.032), ('nasa', 0.032), ('quantized', 0.032), ('troughs', 0.032), ('ohio', 0.029), ('searched', 0.029), ('acquire', 0.029), ('platform', 0.029), ('polar', 0.029), ('reflection', 0.029), ('templates', 0.029), ('handle', 0.027), ('peak', 0.027), ('curves', 0.027), ('light', 0.027), ('amongst', 0.026), ('johnson', 0.026), ('peaks', 0.026), ('date', 0.026), ('robotics', 0.026), ('cross', 0.025), ('areas', 0.025), ('area', 0.025), ('element', 0.024), ('altered', 0.024), ('reflected', 0.024), ('training', 0.024), ('sample', 0.023), ('node', 0.023), ('accept', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="29-tfidf-1" href="./nips-2000-Bayes_Networks_on_Ice%3A_Robotic_Search_for_Antarctic_Meteorites.html">29 nips-2000-Bayes Networks on Ice: Robotic Search for Antarctic Meteorites</a></p>
<p>Author: Liam Pedersen, Dimitrios Apostolopoulos, William Whittaker</p><p>Abstract: A Bayes network based classifier for distinguishing terrestrial rocks from meteorites is implemented onboard the Nomad robot. Equipped with a camera, spectrometer and eddy current sensor, this robot searched the ice sheets of Antarctica and autonomously made the first robotic identification of a meteorite, in January 2000 at the Elephant Moraine. This paper discusses rock classification from a robotic platform, and describes the system onboard Nomad. 1</p><p>2 0.077264972 <a title="29-tfidf-2" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>Author: Angelo Arleo, Fabrizio Smeraldi, Stéphane Hug, Wulfram Gerstner</p><p>Abstract: We model hippocampal place cells and head-direction cells by combining allothetic (visual) and idiothetic (proprioceptive) stimuli. Visual input, provided by a video camera on a miniature robot, is preprocessed by a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. Unsupervised Hebbian learning is employed to incrementally build a population of localized overlapping place fields. Place cells serve as basis functions for reinforcement learning. Experimental results for goal-oriented navigation of a mobile robot are presented.</p><p>3 0.069529794 <a title="29-tfidf-3" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>Author: Te-Won Lee, Thomas Wachtler, Terrence J. Sejnowski</p><p>Abstract: The human visual system encodes the chromatic signals conveyed by the three types of retinal cone photoreceptors in an opponent fashion. This color opponency has been shown to constitute an efficient encoding by spectral decorrelation of the receptor signals. We analyze the spatial and chromatic structure of natural scenes by decomposing the spectral images into a set of linear basis functions such that they constitute a representation with minimal redundancy. Independent component analysis finds the basis functions that transforms the spatiochromatic data such that the outputs (activations) are statistically as independent as possible, i.e. least redundant. The resulting basis functions show strong opponency along an achromatic direction (luminance edges), along a blueyellow direction, and along a red-blue direction. Furthermore, the resulting activations have very sparse distributions, suggesting that the use of color opponency in the human visual system achieves a highly efficient representation of colors. Our findings suggest that color opponency is a result of the properties of natural spectra and not solely a consequence of the overlapping cone spectral sensitivities. 1 Statistical structure of natural scenes Efficient encoding of visual sensory information is an important task for information processing systems and its study may provide insights into coding principles of biological visual systems. An important goal of sensory information processing Electronic version available at www. cnl. salk . edu/</p><p>4 0.053461667 <a title="29-tfidf-4" href="./nips-2000-Four-legged_Walking_Gait_Control_Using_a_Neuromorphic_Chip_Interfaced_to_a_Support_Vector_Learning_Algorithm.html">57 nips-2000-Four-legged Walking Gait Control Using a Neuromorphic Chip Interfaced to a Support Vector Learning Algorithm</a></p>
<p>Author: Susanne Still, Bernhard Schölkopf, Klaus Hepp, Rodney J. Douglas</p><p>Abstract: To control the walking gaits of a four-legged robot we present a novel neuromorphic VLSI chip that coordinates the relative phasing of the robot's legs similar to how spinal Central Pattern Generators are believed to control vertebrate locomotion [3]. The chip controls the leg movements by driving motors with time varying voltages which are the outputs of a small network of coupled oscillators. The characteristics of the chip's output voltages depend on a set of input parameters. The relationship between input parameters and output voltages can be computed analytically for an idealized system. In practice, however, this ideal relationship is only approximately true due to transistor mismatch and offsets. Fine tuning of the chip's input parameters is done automatically by the robotic system, using an unsupervised Support Vector (SV) learning algorithm introduced recently [7]. The learning requires only that the description of the desired output is given. The machine learns from (unlabeled) examples how to set the parameters to the chip in order to obtain a desired motor behavior.</p><p>5 0.046216648 <a title="29-tfidf-5" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>Author: Gal Elidan, Noam Lotner, Nir Friedman, Daphne Koller</p><p>Abstract: A serious problem in learning probabilistic models is the presence of hidden variables. These variables are not observed, yet interact with several of the observed variables. As such, they induce seemingly complex dependencies among the latter. In recent years, much attention has been devoted to the development of algorithms for learning parameters, and in some cases structure, in the presence of hidden variables. In this paper, we address the related problem of detecting hidden variables that interact with the observed variables. This problem is of interest both for improving our understanding of the domain and as a preliminary step that guides the learning procedure towards promising models. A very natural approach is to search for</p><p>6 0.043142039 <a title="29-tfidf-6" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>7 0.043077938 <a title="29-tfidf-7" href="./nips-2000-Probabilistic_Semantic_Video_Indexing.html">103 nips-2000-Probabilistic Semantic Video Indexing</a></p>
<p>8 0.042775493 <a title="29-tfidf-8" href="./nips-2000-Feature_Correspondence%3A_A_Markov_Chain_Monte_Carlo_Approach.html">53 nips-2000-Feature Correspondence: A Markov Chain Monte Carlo Approach</a></p>
<p>9 0.042419024 <a title="29-tfidf-9" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>10 0.041970525 <a title="29-tfidf-10" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>11 0.039485749 <a title="29-tfidf-11" href="./nips-2000-Minimum_Bayes_Error_Feature_Selection_for_Continuous_Speech_Recognition.html">84 nips-2000-Minimum Bayes Error Feature Selection for Continuous Speech Recognition</a></p>
<p>12 0.037354305 <a title="29-tfidf-12" href="./nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">107 nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>13 0.03667441 <a title="29-tfidf-13" href="./nips-2000-Learning_Joint_Statistical_Models_for_Audio-Visual_Fusion_and_Segregation.html">78 nips-2000-Learning Joint Statistical Models for Audio-Visual Fusion and Segregation</a></p>
<p>14 0.03455754 <a title="29-tfidf-14" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>15 0.034347873 <a title="29-tfidf-15" href="./nips-2000-Combining_ICA_and_Top-Down_Attention_for_Robust_Speech_Recognition.html">33 nips-2000-Combining ICA and Top-Down Attention for Robust Speech Recognition</a></p>
<p>16 0.033209857 <a title="29-tfidf-16" href="./nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">145 nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>17 0.031383149 <a title="29-tfidf-17" href="./nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">2 nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>18 0.031193359 <a title="29-tfidf-18" href="./nips-2000-Feature_Selection_for_SVMs.html">54 nips-2000-Feature Selection for SVMs</a></p>
<p>19 0.030419562 <a title="29-tfidf-19" href="./nips-2000-An_Adaptive_Metric_Machine_for_Pattern_Classification.html">23 nips-2000-An Adaptive Metric Machine for Pattern Classification</a></p>
<p>20 0.029841037 <a title="29-tfidf-20" href="./nips-2000-%60N-Body%27_Problems_in_Statistical_Learning.html">148 nips-2000-`N-Body' Problems in Statistical Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.116), (1, -0.014), (2, 0.031), (3, 0.029), (4, -0.029), (5, -0.026), (6, 0.064), (7, -0.019), (8, 0.029), (9, 0.014), (10, 0.033), (11, 0.012), (12, -0.003), (13, -0.074), (14, -0.036), (15, 0.05), (16, -0.023), (17, 0.023), (18, -0.002), (19, 0.014), (20, -0.082), (21, -0.103), (22, 0.01), (23, -0.107), (24, -0.03), (25, -0.036), (26, -0.022), (27, 0.054), (28, -0.026), (29, -0.013), (30, 0.081), (31, -0.21), (32, -0.017), (33, -0.079), (34, 0.131), (35, 0.03), (36, 0.264), (37, 0.202), (38, -0.036), (39, 0.16), (40, -0.181), (41, -0.153), (42, 0.124), (43, 0.029), (44, -0.009), (45, 0.036), (46, 0.303), (47, -0.005), (48, 0.245), (49, -0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96040457 <a title="29-lsi-1" href="./nips-2000-Bayes_Networks_on_Ice%3A_Robotic_Search_for_Antarctic_Meteorites.html">29 nips-2000-Bayes Networks on Ice: Robotic Search for Antarctic Meteorites</a></p>
<p>Author: Liam Pedersen, Dimitrios Apostolopoulos, William Whittaker</p><p>Abstract: A Bayes network based classifier for distinguishing terrestrial rocks from meteorites is implemented onboard the Nomad robot. Equipped with a camera, spectrometer and eddy current sensor, this robot searched the ice sheets of Antarctica and autonomously made the first robotic identification of a meteorite, in January 2000 at the Elephant Moraine. This paper discusses rock classification from a robotic platform, and describes the system onboard Nomad. 1</p><p>2 0.43357095 <a title="29-lsi-2" href="./nips-2000-Four-legged_Walking_Gait_Control_Using_a_Neuromorphic_Chip_Interfaced_to_a_Support_Vector_Learning_Algorithm.html">57 nips-2000-Four-legged Walking Gait Control Using a Neuromorphic Chip Interfaced to a Support Vector Learning Algorithm</a></p>
<p>Author: Susanne Still, Bernhard Schölkopf, Klaus Hepp, Rodney J. Douglas</p><p>Abstract: To control the walking gaits of a four-legged robot we present a novel neuromorphic VLSI chip that coordinates the relative phasing of the robot's legs similar to how spinal Central Pattern Generators are believed to control vertebrate locomotion [3]. The chip controls the leg movements by driving motors with time varying voltages which are the outputs of a small network of coupled oscillators. The characteristics of the chip's output voltages depend on a set of input parameters. The relationship between input parameters and output voltages can be computed analytically for an idealized system. In practice, however, this ideal relationship is only approximately true due to transistor mismatch and offsets. Fine tuning of the chip's input parameters is done automatically by the robotic system, using an unsupervised Support Vector (SV) learning algorithm introduced recently [7]. The learning requires only that the description of the desired output is given. The machine learns from (unlabeled) examples how to set the parameters to the chip in order to obtain a desired motor behavior.</p><p>3 0.32953361 <a title="29-lsi-3" href="./nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">32 nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>Author: Te-Won Lee, Thomas Wachtler, Terrence J. Sejnowski</p><p>Abstract: The human visual system encodes the chromatic signals conveyed by the three types of retinal cone photoreceptors in an opponent fashion. This color opponency has been shown to constitute an efficient encoding by spectral decorrelation of the receptor signals. We analyze the spatial and chromatic structure of natural scenes by decomposing the spectral images into a set of linear basis functions such that they constitute a representation with minimal redundancy. Independent component analysis finds the basis functions that transforms the spatiochromatic data such that the outputs (activations) are statistically as independent as possible, i.e. least redundant. The resulting basis functions show strong opponency along an achromatic direction (luminance edges), along a blueyellow direction, and along a red-blue direction. Furthermore, the resulting activations have very sparse distributions, suggesting that the use of color opponency in the human visual system achieves a highly efficient representation of colors. Our findings suggest that color opponency is a result of the properties of natural spectra and not solely a consequence of the overlapping cone spectral sensitivities. 1 Statistical structure of natural scenes Efficient encoding of visual sensory information is an important task for information processing systems and its study may provide insights into coding principles of biological visual systems. An important goal of sensory information processing Electronic version available at www. cnl. salk . edu/</p><p>4 0.32929721 <a title="29-lsi-4" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>Author: Angelo Arleo, Fabrizio Smeraldi, Stéphane Hug, Wulfram Gerstner</p><p>Abstract: We model hippocampal place cells and head-direction cells by combining allothetic (visual) and idiothetic (proprioceptive) stimuli. Visual input, provided by a video camera on a miniature robot, is preprocessed by a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. Unsupervised Hebbian learning is employed to incrementally build a population of localized overlapping place fields. Place cells serve as basis functions for reinforcement learning. Experimental results for goal-oriented navigation of a mobile robot are presented.</p><p>5 0.27612591 <a title="29-lsi-5" href="./nips-2000-Minimum_Bayes_Error_Feature_Selection_for_Continuous_Speech_Recognition.html">84 nips-2000-Minimum Bayes Error Feature Selection for Continuous Speech Recognition</a></p>
<p>Author: George Saon, Mukund Padmanabhan</p><p>Abstract: We consider the problem of designing a linear transformation () E lRPx n, of rank p ~ n, which projects the features of a classifier x E lRn onto y = ()x E lRP such as to achieve minimum Bayes error (or probability of misclassification). Two avenues will be explored: the first is to maximize the ()-average divergence between the class densities and the second is to minimize the union Bhattacharyya bound in the range of (). While both approaches yield similar performance in practice, they outperform standard LDA features and show a 10% relative improvement in the word error rate over state-of-the-art cepstral features on a large vocabulary telephony speech recognition task.</p><p>6 0.23529622 <a title="29-lsi-6" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>7 0.22736214 <a title="29-lsi-7" href="./nips-2000-Feature_Correspondence%3A_A_Markov_Chain_Monte_Carlo_Approach.html">53 nips-2000-Feature Correspondence: A Markov Chain Monte Carlo Approach</a></p>
<p>8 0.21925481 <a title="29-lsi-8" href="./nips-2000-Hippocampally-Dependent_Consolidation_in_a_Hierarchical_Model_of_Neocortex.html">66 nips-2000-Hippocampally-Dependent Consolidation in a Hierarchical Model of Neocortex</a></p>
<p>9 0.21091206 <a title="29-lsi-9" href="./nips-2000-Bayesian_Video_Shot_Segmentation.html">30 nips-2000-Bayesian Video Shot Segmentation</a></p>
<p>10 0.20185427 <a title="29-lsi-10" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>11 0.20121345 <a title="29-lsi-11" href="./nips-2000-Fast_Training_of_Support_Vector_Classifiers.html">52 nips-2000-Fast Training of Support Vector Classifiers</a></p>
<p>12 0.20027547 <a title="29-lsi-12" href="./nips-2000-Combining_ICA_and_Top-Down_Attention_for_Robust_Speech_Recognition.html">33 nips-2000-Combining ICA and Top-Down Attention for Robust Speech Recognition</a></p>
<p>13 0.1997472 <a title="29-lsi-13" href="./nips-2000-An_Adaptive_Metric_Machine_for_Pattern_Classification.html">23 nips-2000-An Adaptive Metric Machine for Pattern Classification</a></p>
<p>14 0.18754283 <a title="29-lsi-14" href="./nips-2000-%60N-Body%27_Problems_in_Statistical_Learning.html">148 nips-2000-`N-Body' Problems in Statistical Learning</a></p>
<p>15 0.1846792 <a title="29-lsi-15" href="./nips-2000-Who_Does_What%3F_A_Novel_Algorithm_to_Determine_Function_Localization.html">147 nips-2000-Who Does What? A Novel Algorithm to Determine Function Localization</a></p>
<p>16 0.18381168 <a title="29-lsi-16" href="./nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">45 nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>17 0.17092146 <a title="29-lsi-17" href="./nips-2000-Probabilistic_Semantic_Video_Indexing.html">103 nips-2000-Probabilistic Semantic Video Indexing</a></p>
<p>18 0.16712382 <a title="29-lsi-18" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>19 0.16157961 <a title="29-lsi-19" href="./nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">20 nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>20 0.15564726 <a title="29-lsi-20" href="./nips-2000-A_Mathematical_Programming_Approach_to_the_Kernel_Fisher_Algorithm.html">5 nips-2000-A Mathematical Programming Approach to the Kernel Fisher Algorithm</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.024), (17, 0.1), (32, 0.011), (33, 0.043), (55, 0.027), (62, 0.019), (65, 0.017), (67, 0.034), (76, 0.018), (79, 0.51), (81, 0.037), (90, 0.027), (91, 0.019), (97, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92303944 <a title="29-lda-1" href="./nips-2000-Bayes_Networks_on_Ice%3A_Robotic_Search_for_Antarctic_Meteorites.html">29 nips-2000-Bayes Networks on Ice: Robotic Search for Antarctic Meteorites</a></p>
<p>Author: Liam Pedersen, Dimitrios Apostolopoulos, William Whittaker</p><p>Abstract: A Bayes network based classifier for distinguishing terrestrial rocks from meteorites is implemented onboard the Nomad robot. Equipped with a camera, spectrometer and eddy current sensor, this robot searched the ice sheets of Antarctica and autonomously made the first robotic identification of a meteorite, in January 2000 at the Elephant Moraine. This paper discusses rock classification from a robotic platform, and describes the system onboard Nomad. 1</p><p>2 0.78988492 <a title="29-lda-2" href="./nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">27 nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>Author: Thomas P. Minka</p><p>Abstract: A central issue in principal component analysis (PCA) is choosing the number of principal components to be retained. By interpreting PCA as density estimation, we show how to use Bayesian model selection to estimate the true dimensionality of the data. The resulting estimate is simple to compute yet guaranteed to pick the correct dimensionality, given enough data. The estimate involves an integral over the Steifel manifold of k-frames, which is difficult to compute exactly. But after choosing an appropriate parameterization and applying Laplace's method, an accurate and practical estimator is obtained. In simulations, it is convincingly better than cross-validation and other proposed algorithms, plus it runs much faster.</p><p>3 0.7795527 <a title="29-lda-3" href="./nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>Author: Simon Tong, Daphne Koller</p><p>Abstract: Bayesian networks are graphical representations of probability distributions. In virtually all of the work on learning these networks, the assumption is that we are presented with a data set consisting of randomly generated instances from the underlying distribution. In many situations, however, we also have the option of active learning, where we have the possibility of guiding the sampling process by querying for certain types of samples. This paper addresses the problem of estimating the parameters of Bayesian networks in an active learning setting. We provide a theoretical framework for this problem, and an algorithm that chooses which active learning queries to generate based on the model learned so far. We present experimental results showing that our active learning algorithm can significantly reduce the need for training data in many situations.</p><p>4 0.44727153 <a title="29-lda-4" href="./nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">106 nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>Author: Zoubin Ghahramani, Matthew J. Beal</p><p>Abstract: Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set. 1</p><p>5 0.42170316 <a title="29-lda-5" href="./nips-2000-Occam%27s_Razor.html">92 nips-2000-Occam's Razor</a></p>
<p>Author: Carl Edward Rasmussen, Zoubin Ghahramani</p><p>Abstract: The Bayesian paradigm apparently only sometimes gives rise to Occam's Razor; at other times very large models perform well. We give simple examples of both kinds of behaviour. The two views are reconciled when measuring complexity of functions, rather than of the machinery used to implement them. We analyze the complexity of functions for some linear in the parameter models that are equivalent to Gaussian Processes, and always find Occam's Razor at work.</p><p>6 0.38487947 <a title="29-lda-6" href="./nips-2000-Speech_Denoising_and_Dereverberation_Using_Probabilistic_Models.html">123 nips-2000-Speech Denoising and Dereverberation Using Probabilistic Models</a></p>
<p>7 0.37068737 <a title="29-lda-7" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>8 0.35152617 <a title="29-lda-8" href="./nips-2000-Four-legged_Walking_Gait_Control_Using_a_Neuromorphic_Chip_Interfaced_to_a_Support_Vector_Learning_Algorithm.html">57 nips-2000-Four-legged Walking Gait Control Using a Neuromorphic Chip Interfaced to a Support Vector Learning Algorithm</a></p>
<p>9 0.34405312 <a title="29-lda-9" href="./nips-2000-Noise_Suppression_Based_on_Neurophysiologically-motivated_SNR_Estimation_for_Robust_Speech_Recognition.html">91 nips-2000-Noise Suppression Based on Neurophysiologically-motivated SNR Estimation for Robust Speech Recognition</a></p>
<p>10 0.34139451 <a title="29-lda-10" href="./nips-2000-On_Reversing_Jensen%27s_Inequality.html">94 nips-2000-On Reversing Jensen's Inequality</a></p>
<p>11 0.33959946 <a title="29-lda-11" href="./nips-2000-Structure_Learning_in_Human_Causal_Induction.html">127 nips-2000-Structure Learning in Human Causal Induction</a></p>
<p>12 0.33790034 <a title="29-lda-12" href="./nips-2000-The_Use_of_MDL_to_Select_among_Computational_Models_of_Cognition.html">139 nips-2000-The Use of MDL to Select among Computational Models of Cognition</a></p>
<p>13 0.32624573 <a title="29-lda-13" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>14 0.32458782 <a title="29-lda-14" href="./nips-2000-Keeping_Flexible_Active_Contours_on_Track_using_Metropolis_Updates.html">72 nips-2000-Keeping Flexible Active Contours on Track using Metropolis Updates</a></p>
<p>15 0.31559581 <a title="29-lda-15" href="./nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">4 nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>16 0.31449932 <a title="29-lda-16" href="./nips-2000-Generalizable_Singular_Value_Decomposition_for_Ill-posed_Datasets.html">61 nips-2000-Generalizable Singular Value Decomposition for Ill-posed Datasets</a></p>
<p>17 0.30729771 <a title="29-lda-17" href="./nips-2000-Learning_Switching_Linear_Models_of_Human_Motion.html">80 nips-2000-Learning Switching Linear Models of Human Motion</a></p>
<p>18 0.30586198 <a title="29-lda-18" href="./nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">82 nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>19 0.3031005 <a title="29-lda-19" href="./nips-2000-Periodic_Component_Analysis%3A_An_Eigenvalue_Method_for_Representing_Periodic_Structure_in_Speech.html">99 nips-2000-Periodic Component Analysis: An Eigenvalue Method for Representing Periodic Structure in Speech</a></p>
<p>20 0.2997787 <a title="29-lda-20" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
