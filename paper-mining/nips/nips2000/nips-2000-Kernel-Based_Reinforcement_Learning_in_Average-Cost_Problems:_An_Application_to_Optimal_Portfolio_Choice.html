<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-73" href="#">nips2000-73</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</h1>
<br/><p>Source: <a title="nips-2000-73-pdf" href="http://papers.nips.cc/paper/1849-kernel-based-reinforcement-learning-in-average-cost-problems-an-application-to-optimal-portfolio-choice.pdf">pdf</a></p><p>Author: Dirk Ormoneit, Peter W. Glynn</p><p>Abstract: Many approaches to reinforcement learning combine neural networks or other parametric function approximators with a form of temporal-difference learning to estimate the value function of a Markov Decision Process. A significant disadvantage of those procedures is that the resulting learning algorithms are frequently unstable. In this work, we present a new, kernel-based approach to reinforcement learning which overcomes this difficulty and provably converges to a unique solution. By contrast to existing algorithms, our method can also be shown to be consistent in the sense that its costs converge to the optimal costs asymptotically. Our focus is on learning in an average-cost framework and on a practical application to the optimal portfolio choice problem. 1</p><p>Reference: <a title="nips-2000-73-reference" href="../nips2000_reference/nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Peter Glynn EESOR Stanford University Stanford, CA 94305-4023  Abstract Many approaches to reinforcement learning combine neural networks or other parametric function approximators with a form of temporal-difference learning to estimate the value function of a Markov Decision Process. [sent-3, score-0.617]
</p><p>2 A significant disadvantage of those procedures is that the resulting learning algorithms are frequently unstable. [sent-4, score-0.064]
</p><p>3 In this work, we present a new, kernel-based approach to reinforcement learning which overcomes this difficulty and provably converges to a unique solution. [sent-5, score-0.704]
</p><p>4 By contrast to existing algorithms, our method can also be shown to be consistent in the sense that its costs converge to the optimal costs asymptotically. [sent-6, score-0.202]
</p><p>5 Our focus is on learning in an average-cost framework and on a practical application to the optimal portfolio choice problem. [sent-7, score-0.471]
</p><p>6 1  Introduction  Temporal-difference (TD) learning has been applied successfully to many real-world applications that can be formulated as discrete state Markov Decision Processes (MDPs) with unknown transition probabilities. [sent-8, score-0.115]
</p><p>7 If the state variables are continuous or high-dimensional, the TD learning rule is typically combined with some sort of function approximator - e. [sent-9, score-0.101]
</p><p>8 Specifically, the algorithm may fail to converge under several circumstances which, in the authors ' opinion, is one of the main obstacles to a more wide-spread use of reinforcement learning (RL) in industrial applications. [sent-12, score-0.549]
</p><p>9 As a remedy, we adopt a non-parametric perspective on reinforcement learning in this work and we suggest a new algorithm that always converges to a unique solution in a finite number of steps. [sent-13, score-0.685]
</p><p>10 In detail, we assign value function estimates to the states in a sample trajectory and we update these estimates in an iterative procedure. [sent-14, score-0.267]
</p><p>11 The  updates are based on local averaging using a so-called "weighting kernel". [sent-15, score-0.146]
</p><p>12 Besides numerical stability, a second crucial advantage of this algorithm is that additional training data always improve the quality of the approximation and eventually lead to optimal performance - that is, our algorithm is consistent in a statistical sense. [sent-16, score-0.215]
</p><p>13 To the authors' best knowledge, this is the first reinforcement learning algorithm for which consistency has been demonstrated in a continuous space framework. [sent-17, score-0.513]
</p><p>14 Specifically, the recently advocated "direct" policy search or perturbation methods can by construction at most be optimal in a local sense [SMSMOO , VRKOOj. [sent-18, score-0.181]
</p><p>15 Relevant earlier work on local averaging in the context of reinforcement learning includes [Rus97j and [Gor99j. [sent-19, score-0.659]
</p><p>16 While these papers pursue related ideas, their approaches differ fundamentally from ours in the assumption that the transition probabilities of the MDP are known and can be used for learning. [sent-20, score-0.051]
</p><p>17 By contrast, kernelbased reinforcement learning only relies on sample trajectories of the MDP and it is therefore much more widely applicable in practice. [sent-21, score-0.559]
</p><p>18 Note that averagecost reinforcement learning has been discussed by several authors (e. [sent-24, score-0.613]
</p><p>19 In Section 2 be provide basic definitions and we describe the kernel-based reinforcement learning algorithm. [sent-28, score-0.513]
</p><p>20 Section 3 focuses on the practical implementation of the algorithm and on theoretical issues. [sent-29, score-0.07]
</p><p>21 , M}, and a family of transition kernels {Pa(x, B)la E A} characterizing the conditional probability of the event X t E B given X t - 1 = x and at-l = a. [sent-34, score-0.051]
</p><p>22 The cost function c(x, a) represents an immediate penalty for applying action a in state x. [sent-35, score-0.064]
</p><p>23 : IRd -+ A, and we let PX,/A denote the probability distribution governing the Markov chain starting from Xo = x associated with the policy J1. [sent-37, score-0.077]
</p><p>24 Several regularity conditions are listed in detail in [OGOOj. [sent-39, score-0.139]
</p><p>25 Our goal is to identify policies that are optimal in that they minimize the long-run  [f  average-cost TJ/A == liIllT-t oo Ex,/A 'L,;=-Ol c(Xt, J1. [sent-40, score-0.06]
</p><p>26 *(x)  + (rah*)(x)}, argmin{c(x, a) + (rah*)(x)} , a  min{c(x, a) a  (1)  (2)  where TJ* is the minimum average-cost and h*(x) has an interpretation as the differential value of starting in x as opposed to drawing a random starting position from the stationary distribution under J1. [sent-44, score-0.142]
</p><p>27 r a denotes the conditional expectation operator (r ah)(X) == Ex,a [h(Xl) ], which is assumed to be unknown so that (1) cannot be solved explicitly. [sent-46, score-0.134]
</p><p>28 Instead, we simulate the MDP using a fixed proposal strategy jl in reinforcement learning to generate a sample trajectory as training data. [sent-47, score-0.694]
</p><p>29 , Zm} denote such an m-step sample trajectory and let  A == {ao, . [sent-51, score-0.115]
</p><p>30 ,am-llas = p,(zs)} and C == {c(zs , as)IO ~ s < m} be the sequences of actions and costs associated with S. [sent-54, score-0.071]
</p><p>31 Then our objective can be reformulated as the approximation of fJ* based on information in S, A, and C. [sent-55, score-0.046]
</p><p>32 In detail, we will construct an approximate expectation operator, l' m,a, based on the training data, S , and use this approximation in place of the true operator rain this work. [sent-56, score-0.356]
</p><p>33 Formally substituting 1'm ,a for rain (1) and (2) gives the Approximate Avemge-Cost Optimality Equation (AACOE): i)m  + hm(x)  (3)  flm(x)  argmjn {c(x , a)  + (1' m,ahm)(X)} . [sent-57, score-0.091]
</p><p>34 (4)  Note that , ifthe solutions i)m and hm to (3) are well-defined, they can be interpreted as statistical estimates of TJ* and h* in equation (1). [sent-58, score-0.205]
</p><p>35 We therefore employ local averaging in this work to construct 1'm,a in a way that guarantees the existence of a unique fixed point of (3). [sent-60, score-0.217]
</p><p>36 For the derivation of the local averaging operator, note that the task of approximating (rah)(x) = Ex,a[h(Xdl can be interpreted alternatively as a regression of the "target" variable h(Xd onto the "input" Xo = x . [sent-61, score-0.146]
</p><p>37 So-called kernel-smoothers address regression tasks of this sort by locally averaging the target values in a small neighborhood of x . [sent-62, score-0.241]
</p><p>38 This gives the following approximation: m-l  L  km ,a(zs, x)h(zs+1)'  (5)  s=o  (6) In detail, we employ the weighting function or weighting kernel km ,a(zs, x) in (6) to determine the weights that are used for averaging in equation (5). [sent-63, score-0.559]
</p><p>39 Here km,a(zs , x) is a multivariate Gaussian, normalized so as to satisfy the constraints km ,. [sent-64, score-0.149]
</p><p>40 Intuitively, (5) assesses the future differential cost of applying action a in state x by looking at all times in the training data where a has been applied previously in a state similar to x , and by averaging the current differential value estimates at the outcomes of these previous transitions. [sent-69, score-0.42]
</p><p>41 (zs , x) are related inversely to the distance Ilzs - xii, transitions originating in the neighborhood of x are most influential in this averaging procedure. [sent-72, score-0.245]
</p><p>42 Practically speaking , this approach is clearly infeasible because in order to assess the value of the simulated successor states we would need to sample recursively, thereby incurring exponentially increasing computational complexity. [sent-74, score-0.191]
</p><p>43 A more realistic alternative is to estimate l' m,ah (x) as a local average of the rewards that were generated in previous transitions originating in the neighborhood of x, where the membership of an observation Z s in the neighborhood of x is quantified using km ,a( zs, x). [sent-75, score-0.438]
</p><p>44 Here the regularization parameter b determines the width of the Gaussian kernel and thereby also the size of the neighborhood used for averaging. [sent-76, score-0.102]
</p><p>45 Depending on the application , it may be advisable to choose b either fixed or as a location-dependent function of the training data. [sent-77, score-0.107]
</p><p>46 3  "Self-Approximating Property"  As we illustrated above, kernel-based reinforcement learning formally amounts to substituting the approximate expectation operator m,a for r a and then applying dynamic programming to derive solutions to the approximate optimality equation (3). [sent-78, score-1.065]
</p><p>47 In this section, we outline a practical implementation of this approach and we present some of our theoretical results. [sent-79, score-0.07]
</p><p>48 In particular, we consider the relative value iteration algorithm for average-cost MDPs that is described , for example, in [Ber95]. [sent-80, score-0.091]
</p><p>49 This procedure iterates a variant of equation (1) to generate a sequence of value function estimates, h~ , that eventually converge to a solution of (1) (or (3), respectively). [sent-81, score-0.122]
</p><p>50 An important practical problem in continuous state MDPs is that the intermediate functions h~ need to be represented explicitly on a computer. [sent-82, score-0.07]
</p><p>51 This requires some form of function approximation which may be numerically undesirable and computationally burdensome in practice. [sent-83, score-0.046]
</p><p>52 In the case of kernel-based reinforcement learning, the so-called "self-approximating" property allows for a much more efficient implementation in vector format (see also [Rus97]). [sent-84, score-0.486]
</p><p>53 Note that (7) is a finite equation system by contrast to (3). [sent-86, score-0.039]
</p><p>54 , m , the relative value iteration algorithm can thus be written conveniently as (for details, see [Ber95, OGOO]): ~k+1 . [sent-99, score-0.091]
</p><p>55 U  (9)  Hence we end up with an algorithm that is analogous to value iteration except that we use the weighting matrix q>a in place ofthe usual transition probabilities and k and C a are vectors of points in the training set S as opposed to vectors of states. [sent-102, score-0.37]
</p><p>56 Intuitively, (9) assigns value estimates to the states in the sample trajectory and updates these estimates in an iterative fashion. [sent-103, score-0.267]
</p><p>57 Here the update of each state is based on a local average over the costs and values of the samples in its neighborhood. [sent-104, score-0.115]
</p><p>58 Since q>a (i, j) > 0 and 2::7=1 q>a(i, j) = 1 we can further exploit the analogy between (9) and the usual value iteration in an "artificial" MDP with transition probabilities q>a to prove the following theorem:  n  Theorem 1 The relative value iteration (9) converges to a unique fixed point. [sent-105, score-0.415]
</p><p>59 Note that Theorem 1 illustrates a rather unique property of kernel-based reinforcement learning by comparison to alternative approaches. [sent-107, score-0.621]
</p><p>60 In addition, we can show that - under suitable regularity conditions - kernel-based reinforcement learning is consistent in the following sense: Theorem 2 The approximate optimal cost TI* in the sense that  E xo , ji. [sent-108, score-0.789]
</p><p>61 1 Tim A  -  Tfm converges to the true optimal cost  TI * 1m-t co 0. [sent-109, score-0.189]
</p><p>62 ---+  Also, the true cost of the approximate strategy  Pm  converges to the optimal cost:  Hence Pm performs as well as fJ* asymptotically and we can also predict the optimal cost TJ* using r,m. [sent-110, score-0.368]
</p><p>63 From a practical standpoint, Theorem 2 asserts that the performance of approximate dynamic programming can be improved by increasing the amount of training data. [sent-111, score-0.326]
</p><p>64 Note, however, that the computational complexity of approximate dynamic programming depends on the sample size m. [sent-112, score-0.198]
</p><p>65 In detail , the complexity of a single application of (9) is O(m2) in a naive implementation and O(mlog m) in a more elaborate nearest neighbor approach. [sent-113, score-0.134]
</p><p>66 As in the case of parametric reinforcement learning, we can of course restrict ourselves to a fixed amount of computational resources simply by discarding observations from the training data or by summarizing clusters of data using "sufficient statistics". [sent-115, score-0.553]
</p><p>67 Note that the convergence property in Theorem 1 remains unaffected by such an approximation. [sent-116, score-0.037]
</p><p>68 4  Optimal Portfolio Choice  In this section , we describe the practical application of kernel-based reinforcement learning to an investment problem where an agent in a financial market decides whether to buy or sell stocks depending on the market situation. [sent-117, score-1.208]
</p><p>69 In the finance and economics literature, this task is known as "optimal portfolio choice" and has created an enormous literature over the past decades. [sent-118, score-0.247]
</p><p>70 Formally, let St symbolize the value of the stock at time t and let the investor choose her portfolio at from the set A == {O , 0. [sent-119, score-0.52]
</p><p>71 , I}, corresponding to the relative amount of wealth invested in stocks as opposed to an alternative riskless asset. [sent-124, score-0.248]
</p><p>72 At time t + 1, the stock price changes from St to St+1, and the portfolio of the investor participates in the price movement depending on her investment choice. [sent-125, score-0.731]
</p><p>73 Formally, if her wealth at time t is W t , it becomes W t +1 = + at St ±~: S, ) W t at time t + 1. [sent-126, score-0.099]
</p><p>74 To render this simulation  (1  as realistic as possible, our investor is assumed to be risk-averse in that her fear of losses dominates her appreciation of gains of equal magnitude. [sent-127, score-0.192]
</p><p>75 A standard way to express these preferences formally is to aim at maximizing the expectation of a concave "utility function", U(z), ofthe final wealth WT. [sent-128, score-0.22]
</p><p>76 Using the choice U(z) = log( z), the investor's utility can be written as U(WT) =  2:,;:01log (1 + at S'±~:S') . [sent-129, score-0.08]
</p><p>77 We present results using simulated and real stock prices. [sent-131, score-0.266]
</p><p>78 With regard to the simulated data, we adopt the common assumption in finance literature that stock prices are driven by an Ito process with stochastic, mean-reverting volatility:  dSt  fJStdt  dVt  ¢(fJ -  + ylv;StdBt, vt)dt + pylv;dBt . [sent-132, score-0.439]
</p><p>79 We  simulated daily data for the period of 13 years using the usual Euler approximation of these equations. [sent-139, score-0.386]
</p><p>80 The resulting stock prices, volatilities, and returns are shown in Figure l. [sent-140, score-0.161]
</p><p>81 Next, we grouped the simulated time series into 10 sets of training and  Figure 1: The simulated time-series of stock prices (left) , volatility (middle) , and daily returns (right; Tt == log(St/St-d) over a period of one year. [sent-141, score-0.73]
</p><p>82 test data such that the last 10 years are used as 10 test sets and the three years preceding each test year are used as training data. [sent-142, score-0.659]
</p><p>83 Table 1 reports the training and test performances on each of these experiments using kernel-based reinforcement learning and a b enchmark buy & hold strategy. [sent-143, score-0.953]
</p><p>84 029886  Table 1: Investment p erformance on the simulated data (initial wealth Wa  = 100). [sent-184, score-0.204]
</p><p>85 the Sharpe-ratio which is a standard measure of risk-adjusted investment performance. [sent-185, score-0.159]
</p><p>86 Note that large values indicate good risk-adjusted performance in years of positive growth , whereas negative values cannot readily b e interpreted. [sent-188, score-0.161]
</p><p>87 We used the root of the volatility (standardized to zero mean and unit variance) as input information and determined a suitable choice for the bandwidth parameter (b = 1) experimentally. [sent-189, score-0.22]
</p><p>88 Our results in Table 1 demonstrate that reinforcement learning dominates buy & hold in eight out of ten years on the training set and in all seven years with positive growth on the test set. [sent-190, score-1.23]
</p><p>89 Table 2 shows the results of an experiment where we replaced the artificial time series with eight years of daily German stock index data (DAX index , 1993-2000). [sent-191, score-0.35]
</p><p>90 We used the years 1996-2000 as t est data and the three years preceding each test year for training. [sent-192, score-0.483]
</p><p>91 As the model input , we computed an approximation of the (root) volatility using a geometric average of historical returns. [sent-193, score-0.183]
</p><p>92 Note that the training performance of reinforcement learning always dominates the buy & hold strategy, and the test results are also superior to the b enchmark except in the year 2000. [sent-194, score-1.177]
</p><p>93 5  Conclusions  We presented a new, kernel-based reinforcement learning method that overcomes several important shortcomings of temporal-difference learning in continuous-state domains. [sent-216, score-0.632]
</p><p>94 In particular, we demonstrated that the new approach always converges to a unique approximation of the optimal policy and that the quality of this approximation improves with the amount of training data. [sent-217, score-0.469]
</p><p>95 Also, we described a financial application where our method consistently outperformed a benchmark model in an artificial and a real market scenario. [sent-218, score-0.151]
</p><p>96 While the optimal portfolio choice problem is relatively simple, it provides an impressive proof of concept by demonstrating the practical feasibility of our method. [sent-219, score-0.366]
</p><p>97 Efficient implementations of local averaging for large-scale problems have been discussed in the data mining community. [sent-220, score-0.146]
</p><p>98 Our work makes these methods applicable to reinforcement learning, which should be valuable to meet the real-time and dimensionality constraints of real-world problems. [sent-221, score-0.486]
</p><p>99 Generalization in reinforcement learning: Safely approximating the value function. [sent-235, score-0.489]
</p><p>100 Policy gradient methods for reinforcement learning with function approximation. [sent-263, score-0.513]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reinforcement', 0.449), ('zs', 0.248), ('portfolio', 0.192), ('buy', 0.191), ('stock', 0.161), ('investment', 0.159), ('year', 0.159), ('km', 0.149), ('volatility', 0.137), ('ormoneit', 0.137), ('investor', 0.127), ('years', 0.115), ('hm', 0.11), ('simulated', 0.105), ('neighborhood', 0.102), ('averaging', 0.102), ('wealth', 0.099), ('mdp', 0.097), ('ogoo', 0.095), ('rah', 0.095), ('detail', 0.093), ('operator', 0.093), ('fj', 0.089), ('stanford', 0.089), ('prices', 0.082), ('formally', 0.08), ('policy', 0.077), ('daily', 0.074), ('costs', 0.071), ('unique', 0.071), ('practical', 0.07), ('market', 0.069), ('trajectory', 0.069), ('training', 0.066), ('converges', 0.065), ('dominates', 0.065), ('cost', 0.064), ('learning', 0.064), ('hold', 0.064), ('aacoe', 0.064), ('averagecost', 0.064), ('dax', 0.064), ('dirk', 0.064), ('enchmark', 0.064), ('smsmoo', 0.064), ('tsitsikiis', 0.064), ('wt', 0.062), ('optimal', 0.06), ('tj', 0.06), ('weighting', 0.06), ('mdps', 0.058), ('estimates', 0.056), ('optimality', 0.056), ('opposed', 0.056), ('test', 0.055), ('stocks', 0.055), ('finance', 0.055), ('overcomes', 0.055), ('rain', 0.055), ('approximate', 0.055), ('xo', 0.051), ('theorem', 0.051), ('dynamic', 0.051), ('iteration', 0.051), ('transition', 0.051), ('table', 0.05), ('st', 0.047), ('usual', 0.046), ('differential', 0.046), ('programming', 0.046), ('approximation', 0.046), ('growth', 0.046), ('price', 0.046), ('regularity', 0.046), ('sample', 0.046), ('local', 0.044), ('choice', 0.044), ('eventually', 0.043), ('application', 0.041), ('nips', 0.041), ('originating', 0.041), ('financial', 0.041), ('td', 0.041), ('expectation', 0.041), ('value', 0.04), ('equation', 0.039), ('preceding', 0.039), ('root', 0.039), ('pm', 0.039), ('amount', 0.038), ('property', 0.037), ('valuable', 0.037), ('vt', 0.037), ('sort', 0.037), ('substituting', 0.036), ('utility', 0.036), ('adopt', 0.036), ('authors', 0.036), ('zi', 0.034), ('reader', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999923 <a title="73-tfidf-1" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>Author: Dirk Ormoneit, Peter W. Glynn</p><p>Abstract: Many approaches to reinforcement learning combine neural networks or other parametric function approximators with a form of temporal-difference learning to estimate the value function of a Markov Decision Process. A significant disadvantage of those procedures is that the resulting learning algorithms are frequently unstable. In this work, we present a new, kernel-based approach to reinforcement learning which overcomes this difficulty and provably converges to a unique solution. By contrast to existing algorithms, our method can also be shown to be consistent in the sense that its costs converge to the optimal costs asymptotically. Our focus is on learning in an average-cost framework and on a practical application to the optimal portfolio choice problem. 1</p><p>2 0.15071057 <a title="73-tfidf-2" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>Author: Geoffrey J. Gordon</p><p>Abstract: Many algorithms for approximate reinforcement learning are not known to converge. In fact, there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point. This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region. The algorithms are SARSA(O) and V(O); the latter algorithm was used in the well-known TD-Gammon program. 1</p><p>3 0.13747495 <a title="73-tfidf-3" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>Author: Brian Sallans, Geoffrey E. Hinton</p><p>Abstract: The problem of reinforcement learning in large factored Markov decision processes is explored. The Q-value of a state-action pair is approximated by the free energy of a product of experts network. Network parameters are learned on-line using a modified SARSA algorithm which minimizes the inconsistency of the Q-values of consecutive state-action pairs. Actions are chosen based on the current value estimates by fixing the current state and sampling actions from the network using Gibbs sampling. The algorithm is tested on a co-operative multi-agent task. The product of experts model is found to perform comparably to table-based Q-Iearning for small instances of the task, and continues to perform well when the problem becomes too large for a table-based representation.</p><p>4 0.11751398 <a title="73-tfidf-4" href="./nips-2000-Robust_Reinforcement_Learning.html">113 nips-2000-Robust Reinforcement Learning</a></p>
<p>Author: Jun Morimoto, Kenji Doya</p><p>Abstract: This paper proposes a new reinforcement learning (RL) paradigm that explicitly takes into account input disturbance as well as modeling errors. The use of environmental models in RL is quite popular for both off-line learning by simulations and for on-line action planning. However, the difference between the model and the real environment can lead to unpredictable, often unwanted results. Based on the theory of H oocontrol, we consider a differential game in which a 'disturbing' agent (disturber) tries to make the worst possible disturbance while a 'control' agent (actor) tries to make the best control input. The problem is formulated as finding a minmax solution of a value function that takes into account the norm of the output deviation and the norm of the disturbance. We derive on-line learning algorithms for estimating the value function and for calculating the worst disturbance and the best control in reference to the value function. We tested the paradigm, which we call</p><p>5 0.11210589 <a title="73-tfidf-5" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>Author: Christian R. Shelton</p><p>Abstract: For many problems which would be natural for reinforcement learning, the reward signal is not a single scalar value but has multiple scalar components. Examples of such problems include agents with multiple goals and agents with multiple users. Creating a single reward value by combining the multiple components can throwaway vital information and can lead to incorrect solutions. We describe the multiple reward source problem and discuss the problems with applying traditional reinforcement learning. We then present an new algorithm for finding a solution and results on simulated environments.</p><p>6 0.11051309 <a title="73-tfidf-6" href="./nips-2000-Programmable_Reinforcement_Learning_Agents.html">105 nips-2000-Programmable Reinforcement Learning Agents</a></p>
<p>7 0.10494055 <a title="73-tfidf-7" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>8 0.097817235 <a title="73-tfidf-8" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>9 0.095910676 <a title="73-tfidf-9" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>10 0.095644504 <a title="73-tfidf-10" href="./nips-2000-Hierarchical_Memory-Based_Reinforcement_Learning.html">63 nips-2000-Hierarchical Memory-Based Reinforcement Learning</a></p>
<p>11 0.089296594 <a title="73-tfidf-11" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>12 0.085178949 <a title="73-tfidf-12" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>13 0.081331864 <a title="73-tfidf-13" href="./nips-2000-An_Adaptive_Metric_Machine_for_Pattern_Classification.html">23 nips-2000-An Adaptive Metric Machine for Pattern Classification</a></p>
<p>14 0.078423075 <a title="73-tfidf-14" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>15 0.07549499 <a title="73-tfidf-15" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>16 0.070689283 <a title="73-tfidf-16" href="./nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">65 nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>17 0.06501174 <a title="73-tfidf-17" href="./nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>18 0.063882902 <a title="73-tfidf-18" href="./nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">86 nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>19 0.063184634 <a title="73-tfidf-19" href="./nips-2000-Learning_Continuous_Distributions%3A_Simulations_With_Field_Theoretic_Priors.html">76 nips-2000-Learning Continuous Distributions: Simulations With Field Theoretic Priors</a></p>
<p>20 0.060931277 <a title="73-tfidf-20" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2000_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.241), (1, -0.008), (2, 0.083), (3, -0.238), (4, -0.173), (5, 0.051), (6, -0.024), (7, 0.008), (8, -0.065), (9, -0.063), (10, -0.068), (11, 0.039), (12, 0.014), (13, -0.0), (14, -0.027), (15, -0.016), (16, 0.075), (17, 0.031), (18, 0.067), (19, -0.015), (20, 0.016), (21, -0.105), (22, -0.11), (23, 0.051), (24, -0.001), (25, -0.113), (26, -0.039), (27, -0.012), (28, 0.011), (29, -0.052), (30, 0.112), (31, -0.031), (32, -0.068), (33, 0.028), (34, -0.005), (35, -0.061), (36, 0.063), (37, 0.097), (38, 0.1), (39, 0.035), (40, 0.093), (41, 0.029), (42, 0.087), (43, -0.084), (44, -0.003), (45, 0.009), (46, -0.134), (47, -0.058), (48, -0.073), (49, 0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94189721 <a title="73-lsi-1" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>Author: Dirk Ormoneit, Peter W. Glynn</p><p>Abstract: Many approaches to reinforcement learning combine neural networks or other parametric function approximators with a form of temporal-difference learning to estimate the value function of a Markov Decision Process. A significant disadvantage of those procedures is that the resulting learning algorithms are frequently unstable. In this work, we present a new, kernel-based approach to reinforcement learning which overcomes this difficulty and provably converges to a unique solution. By contrast to existing algorithms, our method can also be shown to be consistent in the sense that its costs converge to the optimal costs asymptotically. Our focus is on learning in an average-cost framework and on a practical application to the optimal portfolio choice problem. 1</p><p>2 0.77445036 <a title="73-lsi-2" href="./nips-2000-Robust_Reinforcement_Learning.html">113 nips-2000-Robust Reinforcement Learning</a></p>
<p>Author: Jun Morimoto, Kenji Doya</p><p>Abstract: This paper proposes a new reinforcement learning (RL) paradigm that explicitly takes into account input disturbance as well as modeling errors. The use of environmental models in RL is quite popular for both off-line learning by simulations and for on-line action planning. However, the difference between the model and the real environment can lead to unpredictable, often unwanted results. Based on the theory of H oocontrol, we consider a differential game in which a 'disturbing' agent (disturber) tries to make the worst possible disturbance while a 'control' agent (actor) tries to make the best control input. The problem is formulated as finding a minmax solution of a value function that takes into account the norm of the output deviation and the norm of the disturbance. We derive on-line learning algorithms for estimating the value function and for calculating the worst disturbance and the best control in reference to the value function. We tested the paradigm, which we call</p><p>3 0.7085939 <a title="73-lsi-3" href="./nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>Author: Geoffrey J. Gordon</p><p>Abstract: Many algorithms for approximate reinforcement learning are not known to converge. In fact, there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point. This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region. The algorithms are SARSA(O) and V(O); the latter algorithm was used in the well-known TD-Gammon program. 1</p><p>4 0.56633389 <a title="73-lsi-4" href="./nips-2000-Programmable_Reinforcement_Learning_Agents.html">105 nips-2000-Programmable Reinforcement Learning Agents</a></p>
<p>Author: David Andre, Stuart J. Russell</p><p>Abstract: We present an expressive agent design language for reinforcement learning that allows the user to constrain the policies considered by the learning process.The language includes standard features such as parameterized subroutines, temporary interrupts, aborts, and memory variables, but also allows for unspecified choices in the agent program. For learning that which isn't specified, we present provably convergent learning algorithms. We demonstrate by example that agent programs written in the language are concise as well as modular. This facilitates state abstraction and the transferability of learned skills.</p><p>5 0.55293626 <a title="73-lsi-5" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>Author: Robert St-Aubin, Jesse Hoey, Craig Boutilier</p><p>Abstract: We propose a method of approximate dynamic programming for Markov decision processes (MDPs) using algebraic decision diagrams (ADDs). We produce near-optimal value functions and policies with much lower time and space requirements than exact dynamic programming. Our method reduces the sizes of the intermediate value functions generated during value iteration by replacing the values at the terminals of the ADD with ranges of values. Our method is demonstrated on a class of large MDPs (with up to 34 billion states), and we compare the results with the optimal value functions.</p><p>6 0.53674108 <a title="73-lsi-6" href="./nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>7 0.52588665 <a title="73-lsi-7" href="./nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>8 0.48612741 <a title="73-lsi-8" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>9 0.44727069 <a title="73-lsi-9" href="./nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">48 nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>10 0.44208869 <a title="73-lsi-10" href="./nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>11 0.40451252 <a title="73-lsi-11" href="./nips-2000-An_Adaptive_Metric_Machine_for_Pattern_Classification.html">23 nips-2000-An Adaptive Metric Machine for Pattern Classification</a></p>
<p>12 0.38672489 <a title="73-lsi-12" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>13 0.37402636 <a title="73-lsi-13" href="./nips-2000-On_Iterative_Krylov-Dogleg_Trust-Region_Steps_for_Solving_Neural_Networks_Nonlinear_Least_Squares_Problems.html">93 nips-2000-On Iterative Krylov-Dogleg Trust-Region Steps for Solving Neural Networks Nonlinear Least Squares Problems</a></p>
<p>14 0.36338335 <a title="73-lsi-14" href="./nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">104 nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>15 0.35188568 <a title="73-lsi-15" href="./nips-2000-Hierarchical_Memory-Based_Reinforcement_Learning.html">63 nips-2000-Hierarchical Memory-Based Reinforcement Learning</a></p>
<p>16 0.34967718 <a title="73-lsi-16" href="./nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">101 nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>17 0.34651607 <a title="73-lsi-17" href="./nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">120 nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>18 0.32990417 <a title="73-lsi-18" href="./nips-2000-Fast_Training_of_Support_Vector_Classifiers.html">52 nips-2000-Fast Training of Support Vector Classifiers</a></p>
<p>19 0.32753164 <a title="73-lsi-19" href="./nips-2000-Feature_Correspondence%3A_A_Markov_Chain_Monte_Carlo_Approach.html">53 nips-2000-Feature Correspondence: A Markov Chain Monte Carlo Approach</a></p>
<p>20 0.3223446 <a title="73-lsi-20" href="./nips-2000-Algorithms_for_Non-negative_Matrix_Factorization.html">22 nips-2000-Algorithms for Non-negative Matrix Factorization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2000_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.467), (17, 0.084), (32, 0.016), (33, 0.054), (54, 0.012), (55, 0.021), (62, 0.086), (67, 0.054), (75, 0.017), (76, 0.038), (81, 0.018), (90, 0.032), (97, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97593611 <a title="73-lda-1" href="./nips-2000-Vicinal_Risk_Minimization.html">144 nips-2000-Vicinal Risk Minimization</a></p>
<p>Author: Olivier Chapelle, Jason Weston, Léon Bottou, Vladimir Vapnik</p><p>Abstract: The Vicinal Risk Minimization principle establishes a bridge between generative models and methods derived from the Structural Risk Minimization Principle such as Support Vector Machines or Statistical Regularization. We explain how VRM provides a framework which integrates a number of existing algorithms, such as Parzen windows, Support Vector Machines, Ridge Regression, Constrained Logistic Classifiers and Tangent-Prop. We then show how the approach implies new algorithms for solving problems usually associated with generative models. New algorithms are described for dealing with pattern recognition problems with very different pattern distributions and dealing with unlabeled data. Preliminary empirical results are presented.</p><p>same-paper 2 0.94232035 <a title="73-lda-2" href="./nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>Author: Dirk Ormoneit, Peter W. Glynn</p><p>Abstract: Many approaches to reinforcement learning combine neural networks or other parametric function approximators with a form of temporal-difference learning to estimate the value function of a Markov Decision Process. A significant disadvantage of those procedures is that the resulting learning algorithms are frequently unstable. In this work, we present a new, kernel-based approach to reinforcement learning which overcomes this difficulty and provably converges to a unique solution. By contrast to existing algorithms, our method can also be shown to be consistent in the sense that its costs converge to the optimal costs asymptotically. Our focus is on learning in an average-cost framework and on a practical application to the optimal portfolio choice problem. 1</p><p>3 0.76264703 <a title="73-lda-3" href="./nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">102 nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>Author: Zhaoping Li, Peter Dayan</p><p>Abstract: Stimulus arrays are inevitably presented at different positions on the retina in visual tasks, even those that nominally require fixation. In particular, this applies to many perceptual learning tasks. We show that perceptual inference or discrimination in the face of positional variance has a structurally different quality from inference about fixed position stimuli, involving a particular, quadratic, non-linearity rather than a purely linear discrimination. We show the advantage taking this non-linearity into account has for discrimination, and suggest it as a role for recurrent connections in area VI, by demonstrating the superior discrimination performance of a recurrent network. We propose that learning the feedforward and recurrent neural connections for these tasks corresponds to the fast and slow components of learning observed in perceptual learning tasks.</p><p>4 0.70066768 <a title="73-lda-4" href="./nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: We present a bound on the generalisation error of linear classifiers in terms of a refined margin quantity on the training set. The result is obtained in a PAC- Bayesian framework and is based on geometrical arguments in the space of linear classifiers. The new bound constitutes an exponential improvement of the so far tightest margin bound by Shawe-Taylor et al. [8] and scales logarithmically in the inverse margin. Even in the case of less training examples than input dimensions sufficiently large margins lead to non-trivial bound values and - for maximum margins - to a vanishing complexity term. Furthermore, the classical margin is too coarse a measure for the essential quantity that controls the generalisation error: the volume ratio between the whole hypothesis space and the subset of consistent hypotheses. The practical relevance of the result lies in the fact that the well-known support vector machine is optimal w.r.t. the new bound only if the feature vectors are all of the same length. As a consequence we recommend to use SVMs on normalised feature vectors only - a recommendation that is well supported by our numerical experiments on two benchmark data sets. 1</p><p>5 0.49878502 <a title="73-lda-5" href="./nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">1 nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>Author: Robert St-Aubin, Jesse Hoey, Craig Boutilier</p><p>Abstract: We propose a method of approximate dynamic programming for Markov decision processes (MDPs) using algebraic decision diagrams (ADDs). We produce near-optimal value functions and policies with much lower time and space requirements than exact dynamic programming. Our method reduces the sizes of the intermediate value functions generated during value iteration by replacing the values at the terminals of the ADD with ranges of values. Our method is demonstrated on a class of large MDPs (with up to 34 billion states), and we compare the results with the optimal value functions.</p><p>6 0.48039627 <a title="73-lda-6" href="./nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">69 nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>7 0.46332374 <a title="73-lda-7" href="./nips-2000-Some_New_Bounds_on_the_Generalization_Error_of_Combined_Classifiers.html">119 nips-2000-Some New Bounds on the Generalization Error of Combined Classifiers</a></p>
<p>8 0.45278943 <a title="73-lda-8" href="./nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">26 nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>9 0.44978306 <a title="73-lda-9" href="./nips-2000-Explaining_Away_in_Weight_Space.html">49 nips-2000-Explaining Away in Weight Space</a></p>
<p>10 0.44464436 <a title="73-lda-10" href="./nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">7 nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>11 0.43691069 <a title="73-lda-11" href="./nips-2000-Programmable_Reinforcement_Learning_Agents.html">105 nips-2000-Programmable Reinforcement Learning Agents</a></p>
<p>12 0.43663597 <a title="73-lda-12" href="./nips-2000-Competition_and_Arbors_in_Ocular_Dominance.html">34 nips-2000-Competition and Arbors in Ocular Dominance</a></p>
<p>13 0.43482846 <a title="73-lda-13" href="./nips-2000-Large_Scale_Bayes_Point_Machines.html">75 nips-2000-Large Scale Bayes Point Machines</a></p>
<p>14 0.43223995 <a title="73-lda-14" href="./nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">98 nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>15 0.43197027 <a title="73-lda-15" href="./nips-2000-From_Mixtures_of_Mixtures_to_Adaptive_Transform_Coding.html">59 nips-2000-From Mixtures of Mixtures to Adaptive Transform Coding</a></p>
<p>16 0.42548373 <a title="73-lda-16" href="./nips-2000-Who_Does_What%3F_A_Novel_Algorithm_to_Determine_Function_Localization.html">147 nips-2000-Who Does What? A Novel Algorithm to Determine Function Localization</a></p>
<p>17 0.42451987 <a title="73-lda-17" href="./nips-2000-The_Kernel_Gibbs_Sampler.html">133 nips-2000-The Kernel Gibbs Sampler</a></p>
<p>18 0.42298943 <a title="73-lda-18" href="./nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">71 nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>19 0.42005888 <a title="73-lda-19" href="./nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">138 nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<p>20 0.41714972 <a title="73-lda-20" href="./nips-2000-Algorithmic_Stability_and_Generalization_Performance.html">21 nips-2000-Algorithmic Stability and Generalization Performance</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
