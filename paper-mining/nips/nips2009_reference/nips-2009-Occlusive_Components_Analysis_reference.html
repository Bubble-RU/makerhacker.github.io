<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>175 nips-2009-Occlusive Components Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-175" href="../nips2009/nips-2009-Occlusive_Components_Analysis.html">nips2009-175</a> <a title="nips-2009-175-reference" href="#">nips2009-175-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>175 nips-2009-Occlusive Components Analysis</h1>
<br/><p>Source: <a title="nips-2009-175-pdf" href="http://papers.nips.cc/paper/3701-occlusive-components-analysis.pdf">pdf</a></p><p>Author: Jörg Lücke, Richard Turner, Maneesh Sahani, Marc Henniges</p><p>Abstract: We study unsupervised learning in a probabilistic generative model for occlusion. The model uses two types of latent variables: one indicates which objects are present in the image, and the other how they are ordered in depth. This depth order then determines how the positions and appearances of the objects present, speciﬁed in the model parameters, combine to form the image. We show that the object parameters can be learnt from an unlabelled set of images in which objects occlude one another. Exact maximum-likelihood learning is intractable. However, we show that tractable approximations to Expectation Maximization (EM) can be found if the training images each contain only a small number of objects on average. In numerical experiments it is shown that these approximations recover the correct set of object parameters. Experiments on a novel version of the bars test using colored bars, and experiments on more realistic data, show that the algorithm performs well in extracting the generating causes. Experiments based on the standard bars benchmark test for object learning show that the algorithm performs well in comparison to other recent component extraction approaches. The model and the learning algorithm thus connect research on occlusion with the research ﬁeld of multiple-causes component extraction methods. 1</p><br/>
<h2>reference text</h2><p>[1] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. Nature, 381:607 – 609, 1996.</p>
<p>[2] D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative matrix factorization. Nature, 401(6755):788–91, 1999.</p>
<p>[3] N. Jojic and B. Frey. Learning ﬂexible sprites in video layers. Conf. on Computer Vision and Pattern Recognition, 1:199–206, 2001.</p>
<p>[4] C. K. I. Williams and M. K. Titsias. Greedy learning of multiple objects in images using robust statistics and factorial learning. Neural Computation, 16(5):1039–1062, 2004.</p>
<p>[5] K. Fukushima. Restoring partly occluded patterns: a neural network model. Neural Networks, 18(1):33–43, 2005.</p>
<p>[6] C. Eckes, J. Triesch, and C. von der Malsburg. Analysis of cluttered scenes using an elastic matching approach for stereo images. Neural Computation, 18(6):1441–1471, 2006.</p>
<p>[7] R. M. Neal and G. E. Hinton. A view of the EM algorithm that justiﬁes incremental, sparse, and other variants. In M. I. Jordan, editor, Learning in Graphical Models. Kluwer, 1998.</p>
<p>[8] J. L¨ cke and M. Sahani. Maximal causes for non-linear component extraction. Journal of u Machine Learning Research, 9:1227 – 1267, 2008.</p>
<p>[9] N. Ueda and R. Nakano. Deterministic annealing EM algorithm. Neural Networks, 11(2):271– 282, 1998.</p>
<p>[10] M. Sahani. Latent variable models for neural data analysis, 1999. PhD Thesis, Caltech.</p>
<p>[11] P. F¨ ldi´ k. Forming sparse representations by local anti-Hebbian learning. Biol Cybern, 64:165 o a – 170, 1990.</p>
<p>[12] M. W. Spratling. Learning image components for object recognition. Journal of Machine Learning Research, 7:793 – 815, 2006.</p>
<p>[13] S. Hochreiter and J. Schmidhuber. Feature extraction through LOCOCODE. Neural Computation, 11:679 – 714, 1999.</p>
<p>[14] P. O. Hoyer. Non-negative matrix factorization with sparseness constraints. Journal of Machine Learning Research, 5:1457–1469, 2004.</p>
<p>[15] S. A. Nene, S. K. Nayar, and H. Murase. Columbia object image library (COIL-100). Technical report, cucs-006-96, 1996.</p>
<p>[16] H. Wersing and E. K¨ rner. Learning optimized features for hierarchical models of invariant o object recognition. Neural Computation, 15(7):1559–1588, 2003.</p>
<p>[17] U. K¨ ster, J. T. Lindgren, M. Gutmann, and A. Hyv¨ rinen. Learning natural image structure o a with a horizontal product model. In Int. Conf. on Independent Component Analysis and Signal Separation (ICA), pages 507–514, 2009.</p>
<p>[18] P. Wolfrum, C. Wolff, J. L¨ cke, and C. von der Malsburg. A recurrent dynamic model for u correspondence-based face recognition. Journal of Vision, 8(7):1–18, 2008.</p>
<p>[19] D. G. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60(2):91–110, 2004.</p>
<p>[20] J. Eggert, H. Wersing, and E. K¨ rner. Transformation-invariant representation and NMF. In o Int. J. Conf. on Neural Networks (IJCNN), pages 2535–2539, 2004.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
