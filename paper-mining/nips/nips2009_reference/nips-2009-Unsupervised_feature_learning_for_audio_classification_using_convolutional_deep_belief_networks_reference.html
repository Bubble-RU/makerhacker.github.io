<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>253 nips-2009-Unsupervised feature learning for audio classification using convolutional deep belief networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-253" href="../nips2009/nips-2009-Unsupervised_feature_learning_for_audio_classification_using_convolutional_deep_belief_networks.html">nips2009-253</a> <a title="nips-2009-253-reference" href="#">nips2009-253-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>253 nips-2009-Unsupervised feature learning for audio classification using convolutional deep belief networks</h1>
<br/><p>Source: <a title="nips-2009-253-pdf" href="http://papers.nips.cc/paper/3674-unsupervised-feature-learning-for-audio-classification-using-convolutional-deep-belief-networks.pdf">pdf</a></p><p>Author: Honglak Lee, Peter Pham, Yan Largman, Andrew Y. Ng</p><p>Abstract: In recent years, deep learning approaches have gained signiﬁcant interest as a way of building hierarchical representations from unlabeled data. However, to our knowledge, these deep learning approaches have not been extensively studied for auditory data. In this paper, we apply convolutional deep belief networks to audio data and empirically evaluate them on various audio classiﬁcation tasks. In the case of speech data, we show that the learned features correspond to phones/phonemes. In addition, our feature representations learned from unlabeled audio data show very good performance for multiple audio classiﬁcation tasks. We hope that this paper will inspire more research on deep learning approaches applied to a wide range of audio recognition tasks. 1</p><br/>
<h2>reference text</h2><p>[1] E. C. Smith and M. S. Lewicki. Efﬁcient auditory coding. Nature, 439:978–982, 2006.</p>
<p>[2] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. Nature, 381:607–609, 1996.</p>
<p>[3] R. Grosse, R. Raina, H. Kwong, and A.Y. Ng. Shift-invariant sparse coding for audio classiﬁcation. In UAI, 2007.</p>
<p>[4] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18(7):1527–1554, 2006.</p>
<p>[5] M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun. Efﬁcient learning of sparse representations with an energy-based model. In NIPS, 2006.</p>
<p>[6] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In NIPS, 2006.</p>
<p>[7] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In ICML, 2007.</p>
<p>[8] H. Lee, C. Ekanadham, and A. Y. Ng. Sparse deep belief network model for visual area V2. In NIPS, 2008.</p>
<p>[9] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In ICML, 2009.</p>
<p>[10] G. Desjardins and Y. Bengio. Empirical evaluation of convolutional RBMs for vision. Technical report, 2008.</p>
<p>[11] M. Norouzi, M. Ranjbar, and G. Mori. Stacks of convolutional restricted boltzmann machines for shift-invariant feature learning. In CVPR, 2009.</p>
<p>[12] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14:1771–1800, 2002.</p>
<p>[13] W. Fisher, G. Doddington, and K. Goudie-Marshall. The darpa speech recognition research database: Speciﬁcations and status. In DARPA Speech Recognition Workshop, 1986.</p>
<p>[14] R. Raina, A. Battle, H. Lee, B. Packer, and A. Y. Ng. Self-taught learning: Transfer learning from unlabeled data. In ICML, 2007.</p>
<p>[15] P. Clarkson and P. J. Moreno. On the use of support vector machines for phonetic classiﬁcation. In ICASSP99, pages 585–588, 1999.</p>
<p>[16] D. A. Reynolds. Speaker identiﬁcation and veriﬁcation using gaussian mixture speaker models. Speech Commun., 17(1-2):91–108, 1995.</p>
<p>[17] F. Sha and L. K. Saul. Large margin gaussian mixture modeling for phonetic classication and recognition. In ICASSP’06, 2006.</p>
<p>[18] Y.-H. Sung, C. Boulis, C. Manning, and D. Jurafsky. Regularization, adaptation, and nonindependent features improve hidden conditional random ﬁelds for phone classiﬁcation. In IEEE ASRU, 2007.</p>
<p>[19] S. Petrov, A. Pauls, and D. Klein. Learning structured models for phone recognition. In EMNLP-CoNLL, 2007.</p>
<p>[20] D. Yu, L. Deng, and A. Acero. Hidden conditional random ﬁeld with distribution constraints for phone classiﬁcation. In Interspeech, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
