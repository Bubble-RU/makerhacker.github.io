<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-158" href="../nips2009/nips-2009-Multi-Label_Prediction_via_Sparse_Infinite_CCA.html">nips2009-158</a> <a title="nips-2009-158-reference" href="#">nips2009-158-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>158 nips-2009-Multi-Label Prediction via Sparse Infinite CCA</h1>
<br/><p>Source: <a title="nips-2009-158-pdf" href="http://papers.nips.cc/paper/3673-multi-label-prediction-via-sparse-infinite-cca.pdf">pdf</a></p><p>Author: Piyush Rai, Hal Daume</p><p>Abstract: Canonical Correlation Analysis (CCA) is a useful technique for modeling dependencies between two (or more) sets of variables. Building upon the recently suggested probabilistic interpretation of CCA, we propose a nonparametric, fully Bayesian framework that can automatically select the number of correlation components, and effectively capture the sparsity underlying the projections. In addition, given (partially) labeled data, our algorithm can also be used as a (semi)supervised dimensionality reduction technique, and can be applied to learn useful predictive features in the context of learning a set of related tasks. Experimental results demonstrate the efﬁcacy of the proposed approach for both CCA as a stand-alone problem, and when applied to multi-label prediction. 1</p><br/>
<h2>reference text</h2><p>[1] C. Archambeau and F. Bach. Sparse probabilistic projections. In Neural Information Processing Systems 21, 2008.</p>
<p>[2] J. Arenas-Garc´a, K. B. Petersen, and L. K. Hansen. Sparse kernel orthonormalized pls for feature extracı tion in large data sets. In Neural Information Processing Systems 19, 2006.</p>
<p>[3] F. R. Bach and M. I. Jordan. A Probabilistic Interpretation of Canonical Correlation Analysis. In Technical Report 688, Dept. of Statistics. University of California, 2005.</p>
<p>[4] J. Baxter. A Model of Inductive Bias Learning. Journal of Artiﬁcial Intelligence Research, 12:149–198, 2000.</p>
<p>[5] C. M. Bishop. Bayesian PCA. In Neural Information Processing Systems 11, Cambridge, MA, USA, 1999. MIT Press.</p>
<p>[6] R. Caruana. Multitask Learning. Machine Learning, 28(1):41–75, 1997.</p>
<p>[7] H. Daum´ III. Bayesian Multitask Learning with Latent Hierarchies. In Conference on Uncertainty in e Artiﬁcial Intelligence, Montreal, Canada, 2009.</p>
<p>[8] K. Fukumizu, F. R. Bach, and M. I. Jordan. Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces. J. Mach. Learn. Res., 5:73–99, 2004.</p>
<p>[9] Z. Ghahramani, T. L. Grifﬁths, and P. Sollich. Bayesian Nonparametric Latent Feature Models. In Bayesian Statistics 8. Oxford University Press, 2007.</p>
<p>[10] A. Globerson and N. Tishby. Sufﬁcient dimensionality reduction. J. Mach. Learn. Res., 3:1307–1331, 2003.</p>
<p>[11] H. Hotelling. Relations Between Two Sets of Variables. Biometrika, pages 321–377, 1936.</p>
<p>[12] S. Ji, L. Tang, S. Yu, and J. Ye. Extracting Shared Subspace for Multi-label Classiﬁcation. 2008.</p>
<p>[13] S. Ji and J. Ye. Linear dimensionality reduction for multi-label classiﬁcation. In Twenty-ﬁrst International Joint Conference on Artiﬁcial Intelligence, 2009.</p>
<p>[14] M. Kim and V. Pavlovic. Covariance operator based dimensionality reduction with extension to semisupervised settings. In Twelfth International Conference on Artiﬁcial Intelligence and Statistics, Florida USA, 2009.</p>
<p>[15] A. Klami and S. Kaski. Local dependent components. In ICML ’07: Proceedings of the 24th international conference on Machine learning, 2007.</p>
<p>[16] P. Rai and H. Daum´ III. The inﬁnite hierarchical factor regression model. In Neural Information Proe cessing Systems 21, 2008.</p>
<p>[17] D. Hardoon J. Shawe-Taylor. The Double-Barrelled LASSO (Sparse Canonical Correlation Analysis). In Workshop on Learning from Multiple Sources (NIPS), 2008.</p>
<p>[18] B. Sriperumbudur, D. Torres, and G. Lanckriet. The Sparse Eigenvalue Problem. In arXiv:0901.1504v1, 2009.</p>
<p>[19] N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. In Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing, pages 368–377.</p>
<p>[20] N. Ueda and K. Saito. Parametric Mixture Models for Multi-labeled Text. Advances in Neural Information Processing Systems, pages 737–744, 2003.</p>
<p>[21] C. Wang. Variational Bayesian approach to Canonical Correlation Analysis. In IEEE Transactions on Neural Networks, 2007.</p>
<p>[22] A. Wiesel, M. Kliger, and A. Hero. A Greedy Approach to Sparse Canonical Correlation Analysis. In arXiv:0801.2748, 2008.</p>
<p>[23] Y. Xue, X. Liao, L. Carin, and B. Krishnapuram. Multi-task Learning for Classiﬁcation with Dirichlet Process Priors. The Journal of Machine Learning Research, 8:35–63, 2007.</p>
<p>[24] K. Yu, S. Yu, and V. Tresp. Multi-label Informed Latent Semantic Indexing. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258–265. ACM New York, NY, USA, 2005.</p>
<p>[25] S. Yu, K. Yu, V. Tresp, H. Kriegel, and M. Wu. Supervised Probabilistic Principal Component Analysis. In KDD ’06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, 2006.</p>
<p>[26] Y. Zhang Z. H. Zhou. Multi-Label Dimensionality Reduction via Dependence Maximization. In Proceedings of the Twenty-Third AAAI Conference on Artiﬁcial Intelligence, AAAI 2008, pages 1503–1505, 2008.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
