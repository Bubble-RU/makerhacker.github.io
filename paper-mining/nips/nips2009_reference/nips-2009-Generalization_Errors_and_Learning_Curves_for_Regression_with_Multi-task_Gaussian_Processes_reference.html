<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>101 nips-2009-Generalization Errors and Learning Curves for Regression with Multi-task Gaussian Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-101" href="../nips2009/nips-2009-Generalization_Errors_and_Learning_Curves_for_Regression_with_Multi-task_Gaussian_Processes.html">nips2009-101</a> <a title="nips-2009-101-reference" href="#">nips2009-101-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>101 nips-2009-Generalization Errors and Learning Curves for Regression with Multi-task Gaussian Processes</h1>
<br/><p>Source: <a title="nips-2009-101-pdf" href="http://papers.nips.cc/paper/3786-generalization-errors-and-learning-curves-for-regression-with-multi-task-gaussian-processes.pdf">pdf</a></p><p>Author: Kian M. Chai</p><p>Abstract: We provide some insights into how task correlations in multi-task Gaussian process (GP) regression affect the generalization error and the learning curve. We analyze the asymmetric two-tasks case, where a secondary task is to help the learning of a primary task. Within this setting, we give bounds on the generalization error and the learning curve of the primary task. Our approach admits intuitive understandings of the multi-task GP by relating it to single-task GPs. For the case of one-dimensional input-space under optimal sampling with data only for the secondary task, the limitations of multi-task GP can be quantiﬁed explicitly. 1</p><br/>
<h2>reference text</h2><p>[1] Carl E. Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, Cambridge, Massachusetts, 2006.</p>
<p>[2] Yee Whye Teh, Matthias Seeger, and Michael I. Jordan. Semiparametric latent factor models. In Robert G. Cowell and Zoubin Ghahramani, editors, Proceedings of the 10th International Workshop on Artiﬁcial Intelligence and Statistics, pages 333–340. Society for Artiﬁcial Intelligence and Statistics, January 2005.</p>
<p>[3] Edwin V. Bonilla, Felix V. Agakov, and Christopher K. I. Williams. Kernel Multi-task Learning using Task-speciﬁc Features. In Marina Meila and Xiaotong Shen, editors, Proceedings of the 11th International Conference on Artiﬁcial Intelligence and Statistics. Omni Press, March 2007.</p>
<p>[4] Kai Yu, Wei Chu, Shipeng Yu, Volker Tresp, and Zhao Xu. Stochastic Relational Models for Discriminative Link Prediction. In B. Sch¨ lkopf, J. Platt, and T. Hofmann, editors, Advances o in Neural Information Processing Systems 19, Cambridge, MA, 2007. MIT Press.</p>
<p>[5] Edwin V. Bonilla, Kian Ming A. Chai, and Christopher K.I. Williams. Multi-task Gaussian process prediction. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20. MIT Press, Cambridge, MA, 2008.</p>
<p>[6] Jonathan Baxter. A Model of Inductive Bias Learning. Journal of Artiﬁcial Intelligence Research, 12:149–198, March 2000.</p>
<p>[7] Andreas Maurer. Bounds for linear multi-task learning. Journal of Machine Learning Research, 7:117–139, January 2006.</p>
<p>[8] Shai Ben-David and Reba Schuller Borbely. A notion of task relatedness yielding provable multiple-task learning guarantees. Machine Learning, 73(3):273–287, 2008.</p>
<p>[9] Christopher K. I. Williams and Francesco Vivarelli. Upper and lower bounds on the learning curve for Gaussian processes. Machine Learning, 40(1):77–102, 2000.</p>
<p>[10] Ya Xue, Xuejun Liao, Lawrence Carin, and Balaji Krishnapuram. Multi-task learning for classiﬁcation with Dirichlet process prior. Journal of Machine Learning Research, 8:35–63, January 2007.</p>
<p>[11] Klaus Ritter. Average-Case Analysis of Numerical Problems, volume 1733 of Lecture Notes in Mathematics. Springer, 2000.</p>
<p>[12] Christopher T. H. Baker. The Numerical Treatment of Integral Equations. Clarendon Press, 1977.</p>
<p>[13] Peter Sollich and Anason Halees. Learning curves for Gaussian process regression: Approximations and bounds. Neural Computation, 14(6):1393–1428, 2002.</p>
<p>[14] Noel A. Cressie. Statistics for Spatial Data. Wiley, New York, 1993.</p>
<p>[15] Manfred Opper and Francesco Vivarelli. General bounds on Bayes errors for regression with Gaussian processes. In Kearns et al. [18], pages 302–308.</p>
<p>[16] Giancarlo Ferrari Trecate, Christopher K. I. Williams, and Manfred Opper. Finite-dimensional approximation of Gaussian processes. In Kearns et al. [18], pages 218–224.</p>
<p>[17] Huaiyu Zhu, Christopher K. I. Williams, Richard Rohwer, and Michal Morciniec. Gaussian regression and optimal ﬁnite dimensional linear models. In Christopher M. Bishop, editor, Neural Networks and Machine Learning, volume 168 of NATO ASI Series F: Computer and Systems Sciences, pages 167–184. Springer-Verlag, Berlin, 1998.</p>
<p>[18] Michael J. Kearns, Sara A. Solla, and David A. Cohn, editors. Advances in Neural Information Processing Systems 11, 1999. The MIT Press.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
