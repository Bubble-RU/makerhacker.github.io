<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>98 nips-2009-From PAC-Bayes Bounds to KL Regularization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-98" href="../nips2009/nips-2009-From_PAC-Bayes_Bounds_to_KL_Regularization.html">nips2009-98</a> <a title="nips-2009-98-reference" href="#">nips2009-98-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>98 nips-2009-From PAC-Bayes Bounds to KL Regularization</h1>
<br/><p>Source: <a title="nips-2009-98-pdf" href="http://papers.nips.cc/paper/3821-from-pac-bayes-bounds-to-kl-regularization.pdf">pdf</a></p><p>Author: Pascal Germain, Alexandre Lacasse, Mario Marchand, Sara Shanian, François Laviolette</p><p>Abstract: We show that convex KL-regularized objective functions are obtained from a PAC-Bayes risk bound when using convex loss functions for the stochastic Gibbs classiﬁer that upper-bound the standard zero-one loss used for the weighted majority vote. By restricting ourselves to a class of posteriors, that we call quasi uniform, we propose a simple coordinate descent learning algorithm to minimize the proposed KL-regularized cost function. We show that standard p -regularized objective functions currently used, such as ridge regression and p -regularized boosting, are obtained from a relaxation of the KL divergence between the quasi uniform posterior and the uniform prior. We present numerical experiments where the proposed learning algorithm generally outperforms ridge regression and AdaBoost. 1</p><br/>
<h2>reference text</h2><p>[1] Olivier Catoni. PAC-Bayesian surpevised classiﬁcation: the thermodynamics of statistical learning. Monograph series of the Institute of Mathematical Statistics, http://arxiv.org/abs/0712.0248, December 2007.</p>
<p>[2] Pascal Germain, Alexandre Lacasse, Francois Laviolette, and Mario Marchand. A pac-bayes ¸ risk bound for general loss functions. In B. Sch¨ lkopf, J. Platt, and T. Hoffman, editors, Ado vances in Neural Information Processing Systems 19, pages 449–456. MIT Press, Cambridge, MA, 2007.</p>
<p>[3] Pascal Germain, Alexandre Lacasse, Francois Laviolette, and Mario Marchand. PAC-Bayesian ¸ learning of linear classiﬁers. In L´ on Bottou and Michael Littman, editors, Proceedings of e the 26th International Conference on Machine Learning, pages 353–360, Montreal, June 2009. Omnipress.</p>
<p>[4] John Langford. Tutorial on practical prediction theory for classiﬁcation. Journal of Machine Learning Research, 6:273–306, 2005.</p>
<p>[5] John Langford and John Shawe-Taylor. PAC-Bayes & margins. In S. Thrun S. Becker and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 423–430. MIT Press, Cambridge, MA, 2003.</p>
<p>[6] David McAllester. PAC-Bayesian stochastic model selection. Machine Learning, 51:5–21, 2003.</p>
<p>[7] Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. The Annals of Statistics, 26:1651–1686, 1998.</p>
<p>[8] Matthias Seeger. PAC-Bayesian generalization bounds for gaussian processes. Journal of Machine Learning Research, 3:233–269, 2002.</p>
<p>[9] Manfred K. Warmuth, Karen A. Glocer, and S.V.N. Vishwanathan. Entropy regularized LPBoost. In Proceedings of the 2008 conference on Algorithmic Learning Theory, Springer LNAI 5254,, pages 256–271, 2008.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
