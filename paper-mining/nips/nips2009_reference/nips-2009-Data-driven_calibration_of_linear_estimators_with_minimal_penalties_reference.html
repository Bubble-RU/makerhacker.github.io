<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>64 nips-2009-Data-driven calibration of linear estimators with minimal penalties</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-64" href="../nips2009/nips-2009-Data-driven_calibration_of_linear_estimators_with_minimal_penalties.html">nips2009-64</a> <a title="nips-2009-64-reference" href="#">nips2009-64-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>64 nips-2009-Data-driven calibration of linear estimators with minimal penalties</h1>
<br/><p>Source: <a title="nips-2009-64-pdf" href="http://papers.nips.cc/paper/3639-data-driven-calibration-of-linear-estimators-with-minimal-penalties.pdf">pdf</a></p><p>Author: Sylvain Arlot, Francis R. Bach</p><p>Abstract: This paper tackles the problem of selecting among several linear estimators in non-parametric regression; this includes model selection for linear regression, the choice of a regularization parameter in kernel ridge regression or spline smoothing, and the choice of a kernel in multiple kernel learning. We propose a new algorithm which ﬁrst estimates consistently the variance of the noise, based upon the concept of minimal penalty which was previously introduced in the context of model selection. Then, plugging our variance estimate in Mallows’ CL penalty is proved to lead to an algorithm satisfying an oracle inequality. Simulation experiments with kernel ridge regression and multiple kernel learning show that the proposed algorithm often improves signiﬁcantly existing calibration procedures such as 10-fold cross-validation or generalized cross-validation. 1</p><br/>
<h2>reference text</h2><p>[1] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004.</p>
<p>[2] B. Sch¨ lkopf and A. J. Smola. Learning with Kernels. MIT Press, 2001. o</p>
<p>[3] O. Chapelle and V. Vapnik. Model selection for support vector machines. In Advances in Neural Information Processing Systems (NIPS), 1999.</p>
<p>[4] C. E. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.</p>
<p>[5] F. Bach. Consistency of the group Lasso and multiple kernel learning. Journal of Machine Learning Research, 9:1179–1225, 2008.</p>
<p>[6] L. Birg´ and P. Massart. Minimal penalties for Gaussian model selection. Probab. Theory e Related Fields, 138(1-2):33–73, 2007.</p>
<p>[7] S. Arlot and P. Massart. Data-driven calibration of penalties for least-squares regression. J. Mach. Learn. Res., 10:245–279, 2009.</p>
<p>[8] P. Craven and G. Wahba. Smoothing noisy data with spline functions. Estimating the correct degree of smoothing by the method of generalized cross-validation. Numer. Math., 31(4):377– 403, 1978/79.</p>
<p>[9] G. Wahba. Spline Models for Observational Data. SIAM, 1990.</p>
<p>[10] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of The Royal Statistical Society Series B, 68(1):49–67, 2006.</p>
<p>[11] G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. Learning the kernel matrix with semideﬁnite programming. J. Mach. Learn. Res., 5:27–72, 2003/04.</p>
<p>[12] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of The Royal Statistical Society Series B, 58(1):267–288, 1996.</p>
<p>[13] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. SpringerVerlag, 2001.</p>
<p>[14] D. M. Allen. The relationship between variable selection and data augmentation and a method for prediction. Technometrics, 16:125–127, 1974.</p>
<p>[15] M. Stone. Cross-validatory choice and assessment of statistical predictions. J. Roy. Statist. Soc. Ser. B, 36:111–147, 1974.</p>
<p>[16] T. Zhang. Learning bounds for kernel regression using effective data dimensionality. Neural Comput., 17(9):2077–2098, 2005.</p>
<p>[17] C. L. Mallows. Some comments on Cp . Technometrics, 15:661–675, 1973.</p>
<p>[18] B. Efron. How biased is the apparent error rate of a prediction rule? J. Amer. Statist. Assoc., 81(394):461–470, 1986.</p>
<p>[19] Y. Cao and Y. Golubev. On oracle inequalities related to smoothing splines. Math. Methods Statist., 15(4):398–414 (2007), 2006.</p>
<p>[20] S. Arlot and F. Bach. Data-driven calibration of linear estimators with minimal penalties, September 2009. Long version. arXiv:0909.1884v1. ´</p>
<p>[21] E. Lebarbier. Detecting multiple change-points in the mean of a gaussian process by model selection. Signal Proces., 85:717–736, 2005.</p>
<p>[22] C. Maugis and B. Michel. Slope heuristics for variable selection and clustering via gaussian mixtures. Technical Report 6550, INRIA, 2008.</p>
<p>[23] K.-C. Li. Asymptotic optimality for Cp , CL , cross-validation and generalized cross-validation: discrete index set. Ann. Statist., 15(3):958–975, 1987.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
