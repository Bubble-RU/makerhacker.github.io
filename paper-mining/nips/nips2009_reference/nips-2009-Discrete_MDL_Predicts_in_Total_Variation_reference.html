<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>69 nips-2009-Discrete MDL Predicts in Total Variation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-69" href="../nips2009/nips-2009-Discrete_MDL_Predicts_in_Total_Variation.html">nips2009-69</a> <a title="nips-2009-69-reference" href="#">nips2009-69-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>69 nips-2009-Discrete MDL Predicts in Total Variation</h1>
<br/><p>Source: <a title="nips-2009-69-pdf" href="http://papers.nips.cc/paper/3746-discrete-mdl-predicts-in-total-variation.pdf">pdf</a></p><p>Author: Marcus Hutter</p><p>Abstract: The Minimum Description Length (MDL) principle selects the model that has the shortest code for data plus model. We show that for a countable class of models, MDL predictions are close to the true distribution in a strong sense. The result is completely general. No independence, ergodicity, stationarity, identiÔ¨Åability, or other assumption on the model class need to be made. More formally, we show that for any countable class of models, the distributions selected by MDL (or MAP) asymptotically predict (merge with) the true measure in the class in total variation distance. Implications for non-i.i.d. domains like time-series forecasting, discriminative learning, and reinforcement learning are discussed. 1</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
