<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>42 nips-2009-Bayesian Sparse Factor Models and DAGs Inference and Comparison</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-42" href="../nips2009/nips-2009-Bayesian_Sparse_Factor_Models_and_DAGs_Inference_and_Comparison.html">nips2009-42</a> <a title="nips-2009-42-reference" href="#">nips2009-42-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>42 nips-2009-Bayesian Sparse Factor Models and DAGs Inference and Comparison</h1>
<br/><p>Source: <a title="nips-2009-42-pdf" href="http://papers.nips.cc/paper/3867-bayesian-sparse-factor-models-and-dags-inference-and-comparison.pdf">pdf</a></p><p>Author: Ricardo Henao, Ole Winther</p><p>Abstract: In this paper we present a novel approach to learn directed acyclic graphs (DAGs) and factor models within the same framework while also allowing for model comparison between them. For this purpose, we exploit the connection between factor models and DAGs to propose Bayesian hierarchies based on spike and slab priors to promote sparsity, heavy-tailed priors to ensure identiﬁability and predictive densities to perform the model comparison. We require identiﬁability to be able to produce variable orderings leading to valid DAGs and sparsity to learn the structures. The effectiveness of our approach is demonstrated through extensive experiments on artiﬁcial and biological data showing that our approach outperform a number of state of the art methods. 1</p><br/>
<h2>reference text</h2><p>[1] M. West. Bayesian factor regression models in the “large p, small n” paradigm. In J. Bernardo, M. Bayarri, J. Berger, A. Dawid, D. Heckerman, A. Smith, and M. West, editors, Bayesian Statistics 7, pages 723–732. Oxford University Press, 2003.</p>
<p>[2] J. Lucas, C. Carvalho, Q. Wang, A. Bild, J. R. Nevins, and M. West. Bayesian Inference for Gene Expression and Proteomics, chapter Sparse Statistical Modeling in Gene Expression Genomics, pages 155–176. Cambridge University Press, 2006.</p>
<p>[3] S. Shimizu, P. O. Hoyer, A. Hyv¨ rinen, and A. Kerminen. A linear non-Gaussian acyclic model for causal a discovery. Journal of Machine Learning Research, 7:2003–2030, October 2006.</p>
<p>[4] D. M. Chickering. Learning Bayesian networks is NP-complete. In D. Fisher and H.-J. Lenz, editors, Learning from Data: AI and Statistics, pages 121–130. Springer-Verlag, 1996.</p>
<p>[5] I. Tsamardinos, L. E. Brown, and C. F. Aliferis. The max-min hill-climbing Bayesian network structure learning algorithm. Machine Learning, 65(1):31–78, October 2006.</p>
<p>[6] N. Friedman, I. Nachman, and D. Pe’er. Learning Bayesian network structure from massive datasets: The “sparse candidate” algorithm. In K. B. Laskey and H. Prade, editors, UAI, pages 206–215, 1999.</p>
<p>[7] M. Teyssier and D. Koller. Ordering-based search: A simple and effective algorithm for learning Bayesian networks. In UAI, pages 548–549, 2005.</p>
<p>[8] M. W. Schmidt, A. Niculescu-Mizil, and K. P. Murphy. Learning graphical model structure using L1regularization paths. In AAAI, pages 1278–1283, 2007.</p>
<p>[9] D. Heckerman, D. Geiger, and D. M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning, 20(3):197–243, January 1995.</p>
<p>[10] J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, March 2000.</p>
<p>[11] P. Comon. Independent component analysis, a new concept? Signal Processing, 36(3):287–314, December 1994.</p>
<p>[12] C. M. Carvalho, J. Chang, J. E. Lucas, J. R. Nevins, Q. Wang, and M. West. High-dimensional sparse factor modeling: Applications in gene expression genomics. Journal of the American Statistical Association, 103(484):1438–1456, December 2008.</p>
<p>[13] A. Hyv¨ rinen, J. Karhunen, and E. Oja. Independent Component Analysis. Wiley-Interscience, May 2001. a</p>
<p>[14] D. F. Andrews and C. L. Mallows. Scale mixtures of normal distributions. Journal of the Royal Statistical Society: Series B (Methodology), 36(1):99–102, 1974.</p>
<p>[15] T. Park and G. Casella. The Bayesian lasso. 103(482):681–686, June 2008.  Journal of the American Statistical Association,</p>
<p>[16] N. Friedman and D. Koller. Being Bayesian about network structure: A Bayesian approach to structure discovery in Bayesian networks. Machine Learning, 50(1–2):95–125, January 2003.</p>
<p>[17] K. Sachs, O. Perez, D. Pe’er, D. A. Lauffenburger, and G. P. Nolan. Causal protein-signaling networks derived from multiparameter single-cell data. Science, 308(5721):523–529, April 2005.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
