<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>18 nips-2009-A Stochastic approximation method for inference in probabilistic graphical models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-18" href="../nips2009/nips-2009-A_Stochastic_approximation_method_for_inference_in_probabilistic_graphical_models.html">nips2009-18</a> <a title="nips-2009-18-reference" href="#">nips2009-18-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>18 nips-2009-A Stochastic approximation method for inference in probabilistic graphical models</h1>
<br/><p>Source: <a title="nips-2009-18-pdf" href="http://papers.nips.cc/paper/3823-a-stochastic-approximation-method-for-inference-in-probabilistic-graphical-models.pdf">pdf</a></p><p>Author: Peter Carbonetto, Matthew King, Firas Hamze</p><p>Abstract: We describe a new algorithmic framework for inference in probabilistic models, and apply it to inference for latent Dirichlet allocation (LDA). Our framework adopts the methodology of variational inference, but unlike existing variational methods such as mean ﬁeld and expectation propagation it is not restricted to tractable classes of approximating distributions. Our approach can also be viewed as a “population-based” sequential Monte Carlo (SMC) method, but unlike existing SMC methods there is no need to design the artiﬁcial sequence of distributions. Signiﬁcantly, our framework oﬀers a principled means to exchange the variance of an importance sampling estimate for the bias incurred through variational approximation. We conduct experiments on a diﬃcult inference problem in population genetics, a problem that is related to inference for LDA. The results of these experiments suggest that our method can oﬀer improvements in stability and accuracy over existing methods, and at a comparable cost. 1</p><br/>
<h2>reference text</h2><p>[1] C. Andrieu and E. Moulines. On the ergodicity properties of some adaptive MCMC algorithms. Annals of Applied Probability, 16:1462–1505, 2006.</p>
<p>[2] C. Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan. An introduction to MCMC for machine learning. Machine Learning, 50:5–43, 2003.</p>
<p>[3] M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, University College London, 2003.</p>
<p>[4] D. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022, 2003.</p>
<p>[5] P. Carbonetto, M. Schmidt, and N. de Freitas. An interior-point stochastic approximation method and an L1-regularized delta rule. In Advances in Neural Information Processing Systems, volume 21. 2009.</p>
<p>[6] G. Casella and C. P. Robert. Rao-Blackwellisation of sampling schemes. Biometrika, 83:81–94, 1996.</p>
<p>[7] G. Celeux, M. Hurn, and C. P. Robert. Computational and inferential diﬃculties with mixture posterior distributions. Journal of the American Statistical Association, 95:957–970, 2000.</p>
<p>[8] D. W. Coltman. Molecular ecological approaches to studying the evolutionary impact of selective harvesting in wildlife. Molecular Ecology, 17:221–235, 2007.</p>
<p>[9] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley, 1991.</p>
<p>[10] P.-T. de Boer, D. P. Kroese, S. Mannor, and R. Y. Rubinstein. A tutorial on the cross-entropy method. Annals of Operations Research, 134:19–67, 2005.</p>
<p>[11] N. de Freitas, P. Højen-Sørensen, M. I. Jordan, and S. Russell. Variational MCMC. In Proceedings of the 17th Conference on Uncertainty in Artiﬁcial Intelligence, pages 120–127, 2001.</p>
<p>[12] P. Del Moral, A. Doucet, and A. Jasra. Sequential Monte Carlo samplers. Journal of the Royal Statistical Society, 68:411–436, 2006.</p>
<p>[13] A. J. Dobson. An Introduction to Generalized Linear Models. Chapman & Hall/CRC Press, 2002.</p>
<p>[14] A. Doucet, S. Godsill, and C. Andrieu. On sequential Monte Carlo sampling methods for Bayesian ﬁltering. Statistics and Computing, 10:197–208, 2000.</p>
<p>[15] T. L. Griﬃths and M. Steyvers. Finding scientiﬁc topics. Proceedings of the National Academy of Sciences, 101:5228–5235, 2004.</p>
<p>[16] D. L. Hartl and A. G. Clark. Principles of population genetics. Sinauer Associates, 2007.</p>
<p>[17] J. Hein, M. H. Schierup, and C. Wiuf. Gene genealogies, variation and evolution: a primer in coalescent theory. Oxford University Press, 2005.</p>
<p>[18] A. Jasra, D. Stephens, and C. Holmes. On population-based simulation for static inference. Statistics and Computing, 17:263–279, 2007.</p>
<p>[19] M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. An introduction to variational methods for graphical models. In M. Jordan, editor, Learning in Graphical Models, pages 105–161. MIT Press, 1998.</p>
<p>[20] F. Liang, C. Liu, and R. J. Carroll. Stochastic approximation in Monte Carlo computation. Journal of the American Statistical Association, 102:305–320, 2007.</p>
<p>[21] T. Mailund, M. Schierup, C. Pedersen, P. Mechlenborg, J. Madsen, and L. Schauser. CoaSim: a ﬂexible environment for simulating genetic data under coalescent models. BMC Bioinformatics, 6, 2005.</p>
<p>[22] T. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, MIT, 2001.</p>
<p>[23] R. Neal and G. Hinton. A view of the EM algorithm that that justiﬁﬁes incremental, sparse, and other variants. In M. Jordan, editor, Learning in graphical models, pages 355–368. Kluwer Academic, 1998.</p>
<p>[24] R. M. Neal. Annealed importance sampling. Statistics and Computing, 11:125–139, 2001.</p>
<p>[25] M. J. D. Powell. Algorithms for nonlinear constraints that use Lagrangian functions. Mathematical Programming, 14:224–248, 1978.</p>
<p>[26] J. K. Pritchard, M. Stephens, and P. Donnelly. Inference of population structure using multilocus genotype data. Genetics, 155:945–959, 2000.</p>
<p>[27] P. Ravikumar. Approximate Inference, Structure Learning and Feature Estimation in Markov Random Fields. PhD thesis, Carnegie Mellon University, 2007.</p>
<p>[28] H. Robbins and S. Monro. A stochastic approximation method. Annals of Math. Statistics, 22, 1951.</p>
<p>[29] J. C. Spall. Introduction to stochastic search and optimization. Wiley-Interscience, 2003.</p>
<p>[30] Y. W. Teh, D. Newman, and M. Welling. A collapsed variational Bayesian inference algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems, volume 19, 2007.</p>
<p>[31] M. J. Wainwright. Stochastic processes on graphs with cycles: geometric and variational approaches. PhD thesis, Massachusetts Institute of Technology, 2002.</p>
<p>[32] J. S. Yedidia, W. T. Freeman, and Y. Weiss. Constructing free-energy approximations and generalized belief propagation algorithms. IEEE Transactions on Information Theory, 51:2282–2312, 2005.</p>
<p>[33] L. Younes. Stochastic gradient estimation strategies for Markov random ﬁelds. In Proceedings of the Spatial Statistics and Imaging Conference, 1991.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
