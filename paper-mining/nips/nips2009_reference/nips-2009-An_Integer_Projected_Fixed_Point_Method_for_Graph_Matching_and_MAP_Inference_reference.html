<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>30 nips-2009-An Integer Projected Fixed Point Method for Graph Matching and MAP Inference</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-30" href="../nips2009/nips-2009-An_Integer_Projected_Fixed_Point_Method_for_Graph_Matching_and_MAP_Inference.html">nips2009-30</a> <a title="nips-2009-30-reference" href="#">nips2009-30-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>30 nips-2009-An Integer Projected Fixed Point Method for Graph Matching and MAP Inference</h1>
<br/><p>Source: <a title="nips-2009-30-pdf" href="http://papers.nips.cc/paper/3756-an-integer-projected-fixed-point-method-for-graph-matching-and-map-inference.pdf">pdf</a></p><p>Author: Marius Leordeanu, Martial Hebert, Rahul Sukthankar</p><p>Abstract: Graph matching and MAP inference are essential problems in computer vision and machine learning. We introduce a novel algorithm that can accommodate both problems and solve them efﬁciently. Recent graph matching algorithms are based on a general quadratic programming formulation, which takes in consideration both unary and second-order terms reﬂecting the similarities in local appearance as well as in the pairwise geometric relationships between the matched features. This problem is NP-hard, therefore most algorithms ﬁnd approximate solutions by relaxing the original problem. They ﬁnd the optimal continuous solution of the modiﬁed problem, ignoring during optimization the original discrete constraints. Then the continuous solution is quickly binarized at the end, but very little attention is put into this ﬁnal discretization step. In this paper we argue that the stage in which a discrete solution is found is crucial for good performance. We propose an efﬁcient algorithm, with climbing and convergence properties, that optimizes in the discrete domain the quadratic score, and it gives excellent results either by itself or by starting from the solution returned by any graph matching algorithm. In practice it outperforms state-or-the art graph matching algorithms and it also signiﬁcantly improves their performance if used in combination. When applied to MAP inference, the algorithm is a parallel extension of Iterated Conditional Modes (ICM) with climbing and convergence properties that make it a compelling alternative to the sequential ICM. In our experiments on MAP inference our algorithm proved its effectiveness by signiﬁcantly outperforming [13], ICM and Max-Product Belief Propagation. 1</p><br/>
<h2>reference text</h2><p>[1] A. Berg, T. Berg and J. Malik. Shape matching and object recognition using low distortion correspondences. Computer Vision and Pattern Recognition, 2005</p>
<p>[2] R. Zass and A. Shashua. Probabilistic Graph and Hypergraph Matching. Computer Vision and Pattern Recognition, 2008</p>
<p>[3] T. Cour, P. Srinivasan and J. Shi. Balanced Graph Matching. Neural Information Processing Systems, 2006</p>
<p>[4] S. Gold, and A. Rangarajan. A graduated assignment algorithm for graph matching. Pattern Analysis and Machine Intelligence, 1996</p>
<p>[5] M. Leordeanu and M. Hebert. A Spectral Technique for Correspondence Problems using Pairwise Constraints. International Conference on Computer Vision, 2005</p>
<p>[6] M. Leordeanu and M. Hebert. Unsupervised Learning for Graph Matching. Computer Vision and Pattern Recognition, 2009</p>
<p>[7] C. Schellewald and C. Schnorr. Probabilistic subgraph matching based on convex relaxation. EMMCVPR, 2005</p>
<p>[8] P.H.S Torr. Solving markov random ﬁelds using semi deﬁnite programming. Artiﬁcial Intelligence and Statistics, 2003</p>
<p>[9] B. Rainer, M. Dell’Amico and S. Martello. Assignment Problems. SIAM Publications, 2009</p>
<p>[10] J. Besag. On the Statistical Analysis of Dirty Pictures. JRSS, 1986</p>
<p>[11] T. Cour and J. Shi. Solving Markov Random Fields with Spectral Relaxation. International Conference on Artiﬁcial Intelligence and Statistics, 2007</p>
<p>[12] M. Leordeanu and M. Hebert. Efﬁcient MAP approximation for dense energy functions. International Conference on Machine Learning, 2006</p>
<p>[13] P. Ravikumar and J. Lafferty. Quadratic Programming Relaxations for Metric Labeling and Markov Random Field MAP Estimation, International Conference on Machine Learning, 2006</p>
<p>[14] M. Frank and P. Wolfe. An algorithm for quadratic programming, Naval Research Logistics Quarterly, 1956.</p>
<p>[15] N.W. Brixius and K.M. Anstreicher. Solving quadratic assignment problems using convex quadratic programming relaxations, Optimization Methods and Software, 2001</p>
<p>[16] J. Maciel and J.P. Costeira. A global solution to sparse correspondence problems Pattern Analysis and Machine Intelligence, 2003</p>
<p>[17] L. Torresani, V. Kolmogorov and C. Rother. Feature correspondence via graph matching: Models and global optimization. European Conference on Computer Vision, 2008</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
