<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>26 nips-2009-Adaptive Regularization for Transductive Support Vector Machine</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-26" href="../nips2009/nips-2009-Adaptive_Regularization_for_Transductive_Support_Vector_Machine.html">nips2009-26</a> <a title="nips-2009-26-reference" href="#">nips2009-26-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>26 nips-2009-Adaptive Regularization for Transductive Support Vector Machine</h1>
<br/><p>Source: <a title="nips-2009-26-pdf" href="http://papers.nips.cc/paper/3843-adaptive-regularization-for-transductive-support-vector-machine.pdf">pdf</a></p><p>Author: Zenglin Xu, Rong Jin, Jianke Zhu, Irwin King, Michael Lyu, Zhirong Yang</p><p>Abstract: We discuss the framework of Transductive Support Vector Machine (TSVM) from the perspective of the regularization strength induced by the unlabeled data. In this framework, SVM and TSVM can be regarded as a learning machine without regularization and one with full regularization from the unlabeled data, respectively. Therefore, to supplement this framework of the regularization strength, it is necessary to introduce data-dependant partial regularization. To this end, we reformulate TSVM into a form with controllable regularization strength, which includes SVM and TSVM as special cases. Furthermore, we introduce a method of adaptive regularization that is data dependant and is based on the smoothness assumption. Experiments on a set of benchmark data sets indicate the promising results of the proposed work compared with state-of-the-art TSVM algorithms. 1</p><br/>
<h2>reference text</h2><p>[1] Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of Machine Learning Research, 7:2399–2434, 2006.</p>
<p>[2] Avrim Blum and Shuchi Chawla. Learning from labeled and unlabeled data using graph mincuts. In ICML ’01: Proceedings of the 18th international conference on Machine learning, pages 19–26. Morgan Kaufmann, San Francisco, CA, 2001.</p>
<p>[3] O. Chapelle, B. Sch¨lkopf, and A. Zien, editors. Semi-Supervised Learning. MIT Press, Camo bridge, MA, 2006.</p>
<p>[4] O. Chapelle and A. Zien. Semi-supervised classiﬁcation by low density separation. In Proceedings of the Tenth International Workshop on Artiﬁcial Intelligence and Statistics, pages 57–64, 2005.</p>
<p>[5] Olivier Chapelle, Vikas Sindhwani, and Sathiya Keerthi. Branch and bound for semi-supervised support vector machines. In B. Sch¨lkopf, J. Platt, and T. Hoﬀman, editors, Advances in Neural o Information Processing Systems 19. MIT Press, Cambridge, MA, 2007.</p>
<p>[6] Olivier Chapelle, Vikas Sindhwani, and Sathiya S. Keerthi. Optimization techniques for semisupervised support vector machines. Journal of Machine Learning Research, 9:203–233, 2008.</p>
<p>[7] Ke Chen and Shihai Wang. Regularized boost for semi-supervised learning. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 281–288. MIT Press, Cambridge, MA, 2008.</p>
<p>[8] Ronan Collobert, Fabian Sinz, Jason Weston, and L´on Bottou. Large scale transductive e SVMs. Journal of Machine Learning Reseaerch, 7:1687–1712, 2006.</p>
<p>[9] S. C. H. Hoi, M. R. Lyu, and E. Y. Chang. Learning the uniﬁed kernel machines for classiﬁcation. In Proceedings of Twentith International Conference on Knowledge Discovery and Data Mining (KDD-2006), pages 187–196, New York, NY, USA, 2006. ACM Press.</p>
<p>[10] Steven C. H. Hoi, Rong Jin, and Michael R. Lyu. Learning nonparametric kernel matrices from pairwise constraints. In ICML ’07: Proceedings of the 24th international conference on Machine learning, pages 361–368, New York, NY, USA, 2007. ACM.</p>
<p>[11] T. Joachims. Transductive learning via spectral graph partitioning. In ICML ’03: Proceedings of the 20th international conference on Machine learning, pages 290–297, 2003.</p>
<p>[12] Thorsten Joachims. Transductive inference for text classiﬁcation using support vector machines. In ICML ’99: Proceedings of the 16th international conference on Machine learning, pages 200–209, San Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc.</p>
<p>[13] John Laﬀerty and Larry Wasserman. Statistical analysis of semi-supervised regression. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 801–808. MIT Press, Cambridge, MA, 2008.</p>
<p>[14] Hariharan Narayanan, Mikhail Belkin, and Partha Niyogi. On the relation between low density separation, spectral clustering and graph cuts. In B. Sch¨lkopf, J. Platt, and T. Hoﬀman, o editors, Advances in Neural Information Processing Systems 19, pages 1025–1032. MIT Press, Cambridge, MA, 2007.</p>
<p>[15] Vikas Sindhwani, S. Sathiya Keerthi, and Olivier Chapelle. Deterministic annealing for semisupervised kernel machines. In ICML ’06: Proceedings of the 23rd international conference on Machine learning, pages 841–848, New York, NY, USA, 2006. ACM Press.</p>
<p>[16] Aarti Singh, Robert Nowak, and Xiaojin Zhu. Unlabeled data: Now it helps, now it doesn’t. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1513–1520. 2009.</p>
<p>[17] Junhui Wang, Xiaotong Shen, and Wei Pan. On eﬃcient large margin semisupervised learning: Method and theory. Journal of Machine Learning Research, 10:719–742, 2009.</p>
<p>[18] Linli Xu and Dale Schuurmans. Unsupervised and semi-supervised multi-class support vector machines. In AAAI, pages 904–910, 2005.</p>
<p>[19] Zenglin Xu, Rong Jin, Jianke Zhu, Irwin King, and Michael R. Lyu. Eﬃcient convex relaxation for transductive support vector machine. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1641–1648. MIT Press, Cambridge, MA, 2008.</p>
<p>[20] T. Zhang and R. Ando. Analysis of spectral kernel design based semi-supervised learning. In Y. Weiss, B. Sch¨lkopf, and J. Platt, editors, Advances in Neural Information Processing o Systems (NIPS 18), pages 1601–1608. MIT Press, Cambridge, MA, 2006.</p>
<p>[21] Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Sch¨lkopf. o Learning with local and global consistency. In Sebastian Thrun, Lawrence Saul, and Bernhard Sch¨lkopf, editors, Advances in Neural Information Processing Systems 16. MIT Press, o Cambridge, MA, 2004.</p>
<p>[22] X. Zhu, J. Kandola, Z. Ghahramani, and J. Laﬀerty. Nonparametric transforms of graph kernels for semi-supervised learning. In Advances in Neural Information Processing Systems (NIPS 17), pages 1641–1648, Cambridge, MA, 2005. MIT Press.</p>
<p>[23] Xiaojin Zhu. Semi-supervised learning literature survey. Technical report, Computer Sciences, University of Wisconsin-Madison, 2005.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
