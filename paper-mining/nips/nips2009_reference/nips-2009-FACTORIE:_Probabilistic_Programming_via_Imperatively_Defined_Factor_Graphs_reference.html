<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>89 nips-2009-FACTORIE: Probabilistic Programming via Imperatively Defined Factor Graphs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-89" href="../nips2009/nips-2009-FACTORIE%3A_Probabilistic_Programming_via_Imperatively_Defined_Factor_Graphs.html">nips2009-89</a> <a title="nips-2009-89-reference" href="#">nips2009-89-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>89 nips-2009-FACTORIE: Probabilistic Programming via Imperatively Defined Factor Graphs</h1>
<br/><p>Source: <a title="nips-2009-89-pdf" href="http://papers.nips.cc/paper/3654-factorie-probabilistic-programming-via-imperatively-defined-factor-graphs.pdf">pdf</a></p><p>Author: Andrew McCallum, Karl Schultz, Sameer Singh</p><p>Abstract: Discriminatively trained undirected graphical models have had wide empirical success, and there has been increasing interest in toolkits that ease their application to complex relational data. The power in relational models is in their repeated structure and tied parameters; at issue is how to deﬁne these structures in a powerful and ﬂexible way. Rather than using a declarative language, such as SQL or ﬁrst-order logic, we advocate using an imperative language to express various aspects of model structure, inference, and learning. By combining the traditional, declarative, statistical semantics of factor graphs with imperative deﬁnitions of their construction and operation, we allow the user to mix declarative and procedural domain knowledge, and also gain signiﬁcant efﬁciencies. We have implemented such imperatively deﬁned factor graphs in a system we call FACTORIE, a software library for an object-oriented, strongly-typed, functional language. In experimental comparisons to Markov Logic Networks on joint segmentation and coreference, we ﬁnd our approach to be 3-15 times faster while reducing error by 20-25%—achieving a new state of the art. 1</p><br/>
<h2>reference text</h2><p>[1] John D. Lafferty, Andrew McCallum, and Fernando Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In Int Conf on Machine Learning (ICML), 2001.</p>
<p>[2] Charles Sutton and Andrew McCallum. An introduction to conditional random ﬁelds for relational learning. In Introduction to Statistical Relational Learning. 2007.</p>
<p>[3] A. Bernal, K. Crammer, A. Hatzigeorgiou, and F. Pereira. Global discriminative learning for higheraccuracy computation gene prediction. In PloS Computational Biology, 2007.</p>
<p>[4] A. Quottoni, M. Collins, and T. Darrell. Conditional random ﬁelds for object recognition. In NIPS, 2004.</p>
<p>[5] Brian Milch. Probabilistic Models with Unknown Objects. PhD thesis, University of California, Berkeley, 2006.</p>
<p>[6] Avi Pfeffer. IBAL: A probabilistic rational programming language. In IJCAI, pages 733–740, 2001.</p>
<p>[7] Noah D. Goodman, Vikash K. Mansinghka, Daniel Roy, Keith Bonawitz, and Joshua B. Tenenbaum. Church: a language for generative models. In Uncertainty in Artiﬁcial Intelligence (UAI), 2008.</p>
<p>[8] Ben Taskar, Abbeel Pieter, and Daphne Koller. Discriminative probabilistic models for relational data. In Uncertainty in Artiﬁcial Intelligence (UAI), 2002.</p>
<p>[9] Matthew Richardson and Pedro Domingos. Markov logic networks. Machine Learning, 62(1-2), 2006.</p>
<p>[10] David Poole. Probabilistic horn abduction and bayesian networks. Artiﬁcial Intelligence, 64, 1993.</p>
<p>[11] Stephen Muggleton and Luc DeRaedt. Inductive logic programming theory and methods. In Journal of Logic Programming, 1994.</p>
<p>[12] Taisuke Sato and Yoshitaka Kameya. PRISM: a language for symbolic-statistical modeling. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), 1997.</p>
<p>[13] Luc De Raedt and Kristian Kersting. Probabilistic logic learning. SIGKDD Explorations: MultiRelational Data Mining, 2003.</p>
<p>[14] Martin Odersky. An Overview of the Scala Programming Language (second edition). Technical Report IC/2006/001, EPFL Lausanne, Switzerland, 2006.</p>
<p>[15] Aron Culotta and Andrew McCallum. Tractable learning and inference with high-order representations. In ICML WS on Open Problems in Statistical Relational Learning, 2006.</p>
<p>[16] Keith A. Bonawitz. Composable Probabilistic Inference with Blaise. PhD thesis, MIT, 2008.</p>
<p>[17] Aron Culotta. Learning and inference in weighted logic with application to natural language processing. PhD thesis, University of Massachusetts, 2008.</p>
<p>[18] Charles Sutton and Andrew McCallum. Collective segmentation and labeling of distant entities in information extraction. Technical Report TR#04-49, University of Massachusetts, July 2004.</p>
<p>[19] Aron Culotta, Michael Wick, and Andrew McCallum. First-order probabilistic models for coreference resolution. In NAACL: Human Language Technologies (NAACL/HLT), 2007.</p>
<p>[20] Khashayar Rohanimanesh, Michael Wick, and Andrew McCallum. Inference and learning in large factor graphs with a rank based objective. Technical Report UM-CS-2009-08, University of Massachusetts, Amherst, 2009.</p>
<p>[21] Hoifung Poon and Pedro Domingos. Joint inference in information extraction. In AAAI, 2007.</p>
<p>[22] Vasin Punyakanok, Dan Roth, and Wen-tau Yih. The necessity of syntactic parsing for semantic role labeling. In International Joint Conf on Artiﬁcial Intelligence (IJCAI), pages 1117–1123, 2005.</p>
<p>[23] Ben Wellner, Andrew McCallum, Fuchun Peng, and Michael Hay. An integrated, conditional model of information extraction and coreference with application to citation matching. In AUAI, 2004.</p>
<p>[24] Sameer Singh, Karl Schultz, and Andrew McCallum. Bi-directional joint inference for entity resolution and segmentation using imperatively-deﬁned factor graphs. In ECML PKDD, pages 414–429, 2009.</p>
<p>[25] Andrew McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. A machine learning approach to building domain-speciﬁc search engines. In Int Joint Conf on Artiﬁcial Intelligence (IJCAI), 1999.</p>
<p>[26] Hoifung Poon, Pedro Domingos, and Marc Sumner. A general method for reducing the complexity of relational inference and its application to MCMC. In AAAI, 2008.</p>
<p>[27] Kevin Murphy and Matt Dunham. PMTK: Probabilistic modeling toolkit. In Neural Information Processing Systems (NIPS) Workshop on Probabilistic Programming, 2008.</p>
<p>[28] John Winn and Tom Minka. Infer.NET/CSoft, 2008. http://research.microsoft.com/mlp/ml/Infer/Csoft.htm.</p>
<p>[29] Avi Pfeffer. Figaro: An Object-Oriented Probabilistic Programming Language. Technical report, Charles River Analytics, 2009.</p>
<p>[30] Richard Maclin and Jude W. Shavlik. Creating advice-taking reinforcement learners. Machine Learning, 22, 1996.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
