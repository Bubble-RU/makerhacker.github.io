<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>35 nips-2009-Approximating MAP by Compensating for Structural Relaxations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-35" href="../nips2009/nips-2009-Approximating_MAP_by_Compensating_for_Structural_Relaxations.html">nips2009-35</a> <a title="nips-2009-35-reference" href="#">nips2009-35-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>35 nips-2009-Approximating MAP by Compensating for Structural Relaxations</h1>
<br/><p>Source: <a title="nips-2009-35-pdf" href="http://papers.nips.cc/paper/3768-approximating-map-by-compensating-for-structural-relaxations.pdf">pdf</a></p><p>Author: Arthur Choi, Adnan Darwiche</p><p>Abstract: We introduce a new perspective on approximations to the maximum a posteriori (MAP) task in probabilistic graphical models, that is based on simplifying a given instance, and then tightening the approximation. First, we start with a structural relaxation of the original model. We then infer from the relaxation its deﬁciencies, and compensate for them. This perspective allows us to identify two distinct classes of approximations. First, we ﬁnd that max-product belief propagation can be viewed as a way to compensate for a relaxation, based on a particular idealized case for exactness. We identify a second approach to compensation that is based on a more reﬁned idealized case, resulting in a new approximation with distinct properties. We go on to propose a new class of algorithms that, starting with a relaxation, iteratively seeks tighter approximations. 1</p><br/>
<h2>reference text</h2><p>[1] Martin J. Wainwright, Tommi Jaakkola, and Alan S. Willsky. MAP estimation via agreement on trees: message-passing and linear programming. IEEE Transactions on Information Theory, 51(11):3697–3717, 2005.</p>
<p>[2] Amir Globerson and Tommi Jaakkola. Fixing max-product: Convergent message passing algorithms for MAP LP-relaxations. In NIPS, pages 553–560, 2008.</p>
<p>[3] Radu Marinescu, Kalev Kask, and Rina Dechter. Systematic vs. non-systematic algorithms for solving the MPE task. In UAI, pages 394–402, 2003.</p>
<p>[4] Arthur Choi, Mark Chavira, and Adnan Darwiche. Node splitting: A scheme for generating upper bounds in Bayesian networks. In UAI, pages 57–66, 2007.</p>
<p>[5] Rina Dechter and Irina Rish. Mini-buckets: A general scheme for bounded inference. J. ACM, 50(2):107–153, 2003.</p>
<p>[6] Arthur Choi and Adnan Darwiche. An edge deletion semantics for belief propagation and its practical impact on approximation quality. In AAAI, pages 1107–1114, 2006.</p>
<p>[7] Arthur Choi and Adnan Darwiche. Approximating the partition function by deleting and then correcting for model edges. In UAI, pages 79–87, 2008.</p>
<p>[8] Rina Dechter, Kalev Kask, and Robert Mateescu. Iterative join-graph propagation. In UAI, pages 128–136, 2002.</p>
<p>[9] Martin J. Wainwright, Tommi Jaakkola, and Alan S. Willsky. Tree consistency and bounds on the performance of the max-product algorithm and its generalizations. Statistics and Computing, 14:143–166, 2004.</p>
<p>[10] Yair Weiss, Chen Yanover, and Talya Meltzer. MAP estimation, linear programming and belief propagation with convex free energies. In UAI, 2007.</p>
<p>[11] Gal Elidan, Ian McGraw, and Daphne Koller. Residual belief propagation: Informed scheduling for asynchronous message passing. In UAI, 2006.</p>
<p>[12] David Sontag, Talya Meltzer, Amir Globerson, Tommi Jaakkola, and Yair Weiss. Tightening LP relaxations for MAP using message passing. In UAI, pages 503–510, 2008.</p>
<p>[13] Rina Dechter. Mini-buckets: a general scheme for approximation in automated reasoning. In Proc. International Joint Conference on Artiﬁcial Intelligence (IJCAI), pages 1297–1302, 1997.</p>
<p>[14] Jason K. Johnson, Dmitry M. Malioutov, and Alan S. Willsky. Lagrangian relaxation for MAP estimation in graphical models. In Proceedings of the 45th Allerton Conference on Communication, Control and Computing, pages 672–681, 2007.</p>
<p>[15] Arthur Choi, Trevor Standley, and Adnan Darwiche. Approximating weighted Max-SAT problems by compensating for relaxations. In Proceedings of the 15th International Conference on Principles and Practice of Constraint Programming (CP), pages 211–225, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
