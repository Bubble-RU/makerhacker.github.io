<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>210 nips-2009-STDP enables spiking neurons to detect hidden causes of their inputs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-210" href="../nips2009/nips-2009-STDP_enables_spiking_neurons_to_detect_hidden_causes_of_their_inputs.html">nips2009-210</a> <a title="nips-2009-210-reference" href="#">nips2009-210-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>210 nips-2009-STDP enables spiking neurons to detect hidden causes of their inputs</h1>
<br/><p>Source: <a title="nips-2009-210-pdf" href="http://papers.nips.cc/paper/3744-stdp-enables-spiking-neurons-to-detect-hidden-causes-of-their-inputs.pdf">pdf</a></p><p>Author: Bernhard Nessler, Michael Pfeiffer, Wolfgang Maass</p><p>Abstract: The principles by which spiking neurons contribute to the astounding computational power of generic cortical microcircuits, and how spike-timing-dependent plasticity (STDP) of synaptic weights could generate and maintain this computational function, are unknown. We show here that STDP, in conjunction with a stochastic soft winner-take-all (WTA) circuit, induces spiking neurons to generate through their synaptic weights implicit internal models for subclasses (or “causes”) of the high-dimensional spike patterns of hundreds of pre-synaptic neurons. Hence these neurons will ﬁre after learning whenever the current input best matches their internal model. The resulting computational function of soft WTA circuits, a common network motif of cortical microcircuits, could therefore be a drastic dimensionality reduction of information streams, together with the autonomous creation of internal models for the probability distributions of their input patterns. We show that the autonomous generation and maintenance of this computational function can be explained on the basis of rigorous mathematical principles. In particular, we show that STDP is able to approximate a stochastic online Expectation-Maximization (EM) algorithm for modeling the input data. A corresponding result is shown for Hebbian learning in artiﬁcial neural networks. 1</p><br/>
<h2>reference text</h2><p>[1] Y. Dan and M. Poo. Spike timing-dependent plasticity of neural circuits. Neuron, 44:23–30, 2004.</p>
<p>[2] L. F. Abbott and S. B. Nelson. Synaptic plasticity: taming the beast. Nature Neuroscience, 3:1178–1183, 2000.</p>
<p>[3] A. Morrison, A. Aertsen, and M. Diesmann. Spike-timing-dependent plasticity in balanced random networks. Neural Computation, 19:1437–1467, 2007.</p>
<p>[4] R. J. Douglas and K. A. Martin. Neuronal circuits of the neocortex. Annu Rev Neurosci, 27:419–451, 2004.</p>
<p>[5] G. E. Hinton and Z. Ghahramani. Generative models for discovering sparse distributed representations. Philos Trans R Soc Lond B Biol Sci., 352(1358):1177–1190, 1997.</p>
<p>[6] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.</p>
<p>[7] A. Gupta and L. N. Long. Character recognition using spiking neural networks. IJCNN, pages 53–58, 2007.</p>
<p>[8] B. Nessler, M. Pfeiffer, and W. Maass. Spike-timing dependent plasticity performs stochastic expectation maximization to reveal the hidden causes of complex spike inputs. (in preparation).</p>
<p>[9] M. Meil˘ and D. Heckerman. An experimental comparison of model-based clustering methods. Machine a Learning, 42(1):9–29, 2001.</p>
<p>[10] C. M. Bishop. Pattern Recognition and Machine Learning. Springer, New York, 2006.</p>
<p>[11] G. McLachlan and D. Peel. Finite mixture models. Wiley, 2000.</p>
<p>[12] J.H. Kushner and G.G. Yin. Stochastic approximation algorithms and applications. Springer, 1997.</p>
<p>[13] B. Nessler, M. Pfeiffer, and W. Maass. Hebbian learning of bayes optimal decisions. In Advances in Neural Information Processing Systems 21, pages 1169–1176. MIT Press, 2009.</p>
<p>[14] M. Sato. Fast learning of on-line EM algorithm. Rapport Technique, ATR Human Information Processing Research Laboratories, 1999.</p>
<p>[15] Z. Ghahramani and M.I. Jordan. Mixture models for learning from incomplete data. Computational Learning Theory and Natural Learning Systems, 4:67–85, 1997.</p>
<p>[16] V. Vapnik. Universal learning technology: Support vector machines. NEC Journal of Advanced Technology, 2:137–144, 2005.</p>
<p>[17] A. Y. Ng and M. I. Jordan. On discriminative vs. generative classiﬁers: A comparison of logistic regression and naive Bayes. Advances in Neural Information Processing Systems (NIPS), 14:841–848, 2002.</p>
<p>[18] P. J. Sj¨ str¨ m, G. G. Turrigiano, and S. B. Nelson. Rate, timing, and cooperativity jointly determine o o cortical synaptic plasticity. Neuron, 32:1149–1164, 2001.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
