<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>214 nips-2009-Semi-supervised Regression using Hessian energy with an application to semi-supervised dimensionality reduction</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-214" href="../nips2009/nips-2009-Semi-supervised_Regression_using_Hessian_energy_with_an_application_to_semi-supervised_dimensionality_reduction.html">nips2009-214</a> <a title="nips-2009-214-reference" href="#">nips2009-214-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>214 nips-2009-Semi-supervised Regression using Hessian energy with an application to semi-supervised dimensionality reduction</h1>
<br/><p>Source: <a title="nips-2009-214-pdf" href="http://papers.nips.cc/paper/3741-semi-supervised-regression-using-hessian-energy-with-an-application-to-semi-supervised-dimensionality-reduction.pdf">pdf</a></p><p>Author: Kwang I. Kim, Florian Steinke, Matthias Hein</p><p>Abstract: Semi-supervised regression based on the graph Laplacian suffers from the fact that the solution is biased towards a constant and the lack of extrapolating power. Based on these observations, we propose to use the second-order Hessian energy for semi-supervised regression which overcomes both these problems. If the data lies on or close to a low-dimensional submanifold in feature space, the Hessian energy prefers functions whose values vary linearly with respect to geodesic distance. We ﬁrst derive the Hessian energy for smooth manifolds and continue to give a stable estimation procedure for the common case where only samples of the underlying manifold are given. The preference of ‘’linear” functions on manifolds renders the Hessian energy particularly suited for the task of semi-supervised dimensionality reduction, where the goal is to ﬁnd a user-deﬁned embedding function given some labeled points which varies smoothly (and ideally linearly) along the manifold. The experimental results suggest superior performance of our method compared with semi-supervised regression using Laplacian regularization or standard supervised regression techniques applied to this task. 1</p><br/>
<h2>reference text</h2><p>[1] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15(6):1373–1396, 2003. 2</p>
<p>[2] M. Belkin and P. Niyogi. Semi-supervised learning on manifolds. Machine Learning, 56:209– 239, 2004. 1, 3, 5</p>
<p>[3] D. Donoho and C. Grimes. Hessian eigenmaps: Locally linear embedding techniques for highdimensional data. Proc. of the National Academy of Sciences, 100(10):5591–5596, 2003. 2, 3, 6</p>
<p>[4] J. Eells and L. Lemaire. Selected topics in harmonic maps. AMS, Providence, RI, 1983. 3</p>
<p>[5] M. Hein. Uniform convergence of adaptive graph-based regularization. In G. Lugosi and H. Simon, editors, Proc. of the 19th Conf. on Learning Theory (COLT), pages 50–64, Berlin, 2006. Springer. 5</p>
<p>[6] R. Irony, D. Cohen-Or, and D. Lischinski. Colorization by example. In Proc. Eurographics Symposium on Rendering, pages 201–210, 2005. 8</p>
<p>[7] J. M. Lee. Riemannian Manifolds - An introduction to curvature. Springer, New York, 1997. 2</p>
<p>[8] A. Levin, D. Lischinski, and Y. Weiss. Colorization using optimization. In Proc. SIGGRAPH, pages 689–694, 2004. 7, 8</p>
<p>[9] Q. Luan, F. Wen, D. Cohen-Or, L. Liang, Y.-Q. Xu, and H.-Y. Shum. Natural image colorization. In Proc. Eurographics Symposium on Rendering, pages 309–320, 2007. 8</p>
<p>[10] G. Peters. Efﬁcient pose estimation using view-based object representations. Machine Vision and Applications, 16(1):59–63, 2004. 7</p>
<p>[11] F. Steinke and M. Hein. Non-parametric regression between Riemannian manifolds. In Advances in Neural Information Processing Systems, pages 1561–1568, 2009. 2, 4</p>
<p>[12] J. J. Verbeek and N. Vlassis. Gaussian ﬁelds for semi-supervised regression and correspondence learning. Pattern Recognition, 39:1864–1875, 2006. 1, 3</p>
<p>[13] X. Yang, H. Fu, H. Zha, and J. Barlow. Semi-supervised nonlinear dimensionality reduction. In Proc. of the 23rd international conference on Machine learning, pages 1065–1072, New York, NY, USA, 2006. ACM. 1  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
