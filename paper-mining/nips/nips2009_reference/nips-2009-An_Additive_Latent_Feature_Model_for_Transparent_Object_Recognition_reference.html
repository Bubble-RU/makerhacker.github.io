<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>28 nips-2009-An Additive Latent Feature Model for Transparent Object Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-28" href="../nips2009/nips-2009-An_Additive_Latent_Feature_Model_for_Transparent_Object_Recognition.html">nips2009-28</a> <a title="nips-2009-28-reference" href="#">nips2009-28-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>28 nips-2009-An Additive Latent Feature Model for Transparent Object Recognition</h1>
<br/><p>Source: <a title="nips-2009-28-pdf" href="http://papers.nips.cc/paper/3808-an-additive-latent-feature-model-for-transparent-object-recognition.pdf">pdf</a></p><p>Author: Mario Fritz, Gary Bradski, Sergey Karayev, Trevor Darrell, Michael J. Black</p><p>Abstract: Existing methods for visual recognition based on quantized local features can perform poorly when local features exist on transparent surfaces, such as glass or plastic objects. There are characteristic patterns to the local appearance of transparent objects, but they may not be well captured by distances to individual examples or by a local pattern codebook obtained by vector quantization. The appearance of a transparent patch is determined in part by the refraction of a background pattern through a transparent medium: the energy from the background usually dominates the patch appearance. We model transparent local patch appearance using an additive model of latent factors: background factors due to scene content, and factors which capture a local edge energy distribution characteristic of the refraction. We implement our method using a novel LDA-SIFT formulation which performs LDA prior to any vector quantization step; we discover latent topics which are characteristic of particular transparent patches and quantize the SIFT space into transparent visual words according to the latent topic dimensions. No knowledge of the background scene is required at test time; we show examples recognizing transparent glasses in a domestic environment. 1</p><br/>
<h2>reference text</h2><p>[1] E. H. Adelson. On seeing stuff: the perception of materials by humans and machines. In SPIE, 2001.</p>
<p>[2] A. C. Berg, T. L. Berg, and J. Malik. Shape matching and object recognition using low distortion correspondence. In CVPR, pages 26–33, 2005.</p>
<p>[3] D. Blei and J. McAuliffe. Supervised topic models. In NIPS, 2007.</p>
<p>[4] D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. JMLR, 2003.</p>
<p>[5] G. Csurka, C. Dance, L. Fan, J. Willarnowski, and C. Bray. Visual categorization with bags of keypoints. In SLCV’04, pages 59–74, Prague, Czech Republic, May 2004.</p>
<p>[6] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005.</p>
<p>[7] A. DelPozo and S. Savarese. Detecting specular surfaces on natural images. In CVPR, 2007.</p>
<p>[8] M. Everingham, A. Zisserman, C. K. I. Williams, and L. Van Gool. The PASCAL Visual Object Classes Challenge 2005 (VOC2005) Results. http://www.pascalnetwork.org/challenges/VOC/voc2005/results.pdf, 2005.</p>
<p>[9] P. F. Felzenszwalb, D. McAllester, and D. Ramana. A discriminatively trained, multiscale, deformable part model. In CVPR, 2008.</p>
<p>[10] R. Fergus, L. Fei-Fei, P. Perona, and A. Zisserman. Learning object categories from google’s image search. In ICCV, 2005.</p>
<p>[11] R. Fergus, A. Zisserman, and P. Perona. Object class recognition by unsupervised scale-invariant learning. In CVPR, 2003.</p>
<p>[12] R. W. Fleming and H. H. B¨ lthoff. Low-level image cues in the perception of translucent materials. ACM u Trans. Appl. Percept., 2(3):346–382, 2005.</p>
<p>[13] M. Fritz and B. Schiele. Decomposition, discovery and detection of visual categories using topic models. In CVPR, 2008.</p>
<p>[14] K. Grauman and T. Darrell. The pyramid match kernel: Efﬁcient learning with sets of features. JMLR, 8:725–760, 2007.</p>
<p>[15] T. L. Grifﬁths and M. Steyvers. Finding scientiﬁc topics. PNAS USA, 2004.</p>
<p>[16] T. Hofmann. Unsupervised learning by probabilistic latent semantic analysis. Machine Learning, 2001.</p>
<p>[17] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In CVPR, pages 2169–2178, 2006.</p>
<p>[18] D. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60(2):91–110, 2004.</p>
<p>[19] J. Matas, O. Chum, U. Martin, and T. Pajdla. Robust wide baseline stereo from maximally stable extremal regions. In P. L. Rosin and A. D. Marshall, editors, BMVC, pages 384–393, 2002.</p>
<p>[20] K. McHenry and J. Ponce. A geodesic active contour framework for ﬁnding glass. In CVPR, 2006.</p>
<p>[21] K. McHenry, J. Ponce, and D. A. Forsyth. Finding glass. In CVPR, 2005.</p>
<p>[22] J. C. Niebles, H. Wang, and L. Fei-Fei. Unsupervised learning of human action categories using spatialtemporal words. Int. J. Comput. Vision, 79(3):299–318, 2008.</p>
<p>[23] P. Nillius and J.-O. Eklundh. Classifying materials from their reﬂectance properties. In T. Pajdla and J. Matas, editors, ECCV, volume 3021, 2004.</p>
<p>[24] D. Nister and H. Stewenius. Scalable recognition with a vocabulary tree. In CVPR, 2006.</p>
<p>[25] M. Osadchy, D. Jacobs, and R. Ramamoorthi. Using specularities for recognition. In ICCV, 2003.</p>
<p>[26] A. Quattoni, M. Collins, and T. Darrell. Conditional random ﬁelds for object recognition. In NIPS, 2004.</p>
<p>[27] M. Riesenhuber and T. Poggio. Hierarchical models of object recognition in cortex. Nature Neuroscience, 2:1019–1025, 1999.</p>
<p>[28] S. Roth and M. J. Black. Specular ﬂow and the recovery of surface structure. In CVPR, 2006.</p>
<p>[29] E. Shechtman and M. Irani. Matching local self-similarities across images and videos. In CVPR, 2007.</p>
<p>[30] J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T. Freeman. Discovering objects and their locations in images. In ICCV, 2005.</p>
<p>[31] J. Sivic and A. Zisserman. Video Google: A text retrieval approach to object matching in videos. In ICCV, 2003.</p>
<p>[32] E. Sudderth, A. Torralba, W. Freeman, and A. Willsky. Learning hierarchical models of scenes, objects, and parts. In ICCV, 2005.</p>
<p>[33] C. Wang, D. Blei, and L. Fei-Fei. Simultaneous image classiﬁcation and annotation. In CVPR, 2009.</p>
<p>[34] D. Zongker, D. Werner, B. Curless, and D. Salesin. Environment matting and compositing. In SIGGRAPH, pages 205–214, 1999.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
