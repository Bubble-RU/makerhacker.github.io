<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>197 nips-2009-Randomized Pruning: Efficiently Calculating Expectations in Large Dynamic Programs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-197" href="../nips2009/nips-2009-Randomized_Pruning%3A_Efficiently_Calculating_Expectations_in_Large_Dynamic_Programs.html">nips2009-197</a> <a title="nips-2009-197-reference" href="#">nips2009-197-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>197 nips-2009-Randomized Pruning: Efficiently Calculating Expectations in Large Dynamic Programs</h1>
<br/><p>Source: <a title="nips-2009-197-pdf" href="http://papers.nips.cc/paper/3710-randomized-pruning-efficiently-calculating-expectations-in-large-dynamic-programs.pdf">pdf</a></p><p>Author: Alexandre Bouchard-côté, Slav Petrov, Dan Klein</p><p>Abstract: Pruning can massively accelerate the computation of feature expectations in large models. However, any single pruning mask will introduce bias. We present a novel approach which employs a randomized sequence of pruning masks. Formally, we apply auxiliary variable MCMC sampling to generate this sequence of masks, thereby gaining theoretical guarantees about convergence. Because each mask is generally able to skip large portions of an underlying dynamic program, our approach is particularly compelling for high-degree algorithms. Empirically, we demonstrate our method on bilingual parsing, showing decreasing bias as more masks are incorporated, and outperforming ﬁxed tic-tac-toe pruning. 1</p><br/>
<h2>reference text</h2><p>[1] J. Albert and S. Chib. Bayesian analysis of binary and polychotomous response data. JASA, 1993.</p>
<p>[2] C. Andrieu and E. Moulines. On the ergodicity properties of some adaptive MCMC algorithms. Ann. Appl. Probab., 2006.</p>
<p>[3] P. Blunsom, T. Cohn, C. Dyer, and M. Osborne. A Gibbs sampler for phrasal synchronous grammar induction. In EMNLP, 2009.  8</p>
<p>[4] P. Blunsom, T. Cohn, and M. Osborne. A discriminative latent variable model for statistical machine translation. In ACL-HLT, 2008.</p>
<p>[5] P. Blunsom and M. Osborne. Probabilistic inference for machine translation. In EMNLP, 2008.</p>
<p>[6] D. Burkett and D. Klein. Two languages are better than one (for syntactic parsing). In EMNLP ’08, 2008.</p>
<p>[7] E. Charniak and M. Johnson. Coarse-to-ﬁne n-best parsing and maxent discriminative reranking. In ACL, 2005.</p>
<p>[8] D. Chiang. A hierarchical phrase-based model for statistical machine translation. In ACL, 2005.</p>
<p>[9] S. Clark and J. R. Curran. Parsing the WSJ using CCG and log-linear models. In ACL, 2004.</p>
<p>[10] M. Collins. Head-Driven Statistical Models for Natural Language Parsing. PhD thesis, UPenn, 1999.</p>
<p>[11] S. Duane, A. D. Kennedy, B. J. Pendleton, and D. Roweth. Hybrid Monte Carlo. Physics Letters B, 1987.</p>
<p>[12] J. Finkel, A. Kleeman, and C. Manning. Efﬁcient, feature-based, conditional random ﬁeld parsing. In ACL, 2008.</p>
<p>[13] J. R. Finkel, C. D. Manning, and A. Y. Ng. Solving the problem of cascading errors: Approximate Bayesian inference for linguistic annotation pipelines. In EMNLP, 2006.</p>
<p>[14] M. Galley, M. Hopkins, K. Knight, and D. Marcu. What’s in a translation rule? In HLT-NAACL, 2004.</p>
<p>[15] A. Globerson and T. Jaakkola. Approximate inference using planar graph decomposition. In NIPS, 2006.</p>
<p>[16] J. Goodman. Parsing algorithms and metrics. In ACL, 1996.</p>
<p>[17] J. Goodman. Global thresholding and multiple-pass parsing. In EMNLP, 1997.</p>
<p>[18] M. Johnson. Joint and conditional estimation of tagging and parsing models. In ACL, 2001.</p>
<p>[19] M. Johnson, T. L. Grifﬁths, and S. Goldwater. Bayesian inference for PCFGs via Markov Chain Monte Carlo. In ACL, 2007.</p>
<p>[20] P. Liang, M. I. Jordan, and B. Taskar. A permutation-augmented sampler for Dirichlet process mixture models. In ICML, 2007.</p>
<p>[21] P. Liang, B. Taskar, and D. Klein. Alignment by agreement. In NAACL, 2006.</p>
<p>[22] D. J. C. MacKay. Information Theory, Inference and Learning Algorithms. Cambridge U. Press, 2003.</p>
<p>[23] I. W. McKeague and W. Wefelmeyer. Markov chain Monte Carlo and Rao-Blackwellization. Statistical Planning and Inference, 2000.</p>
<p>[24] R. Neal. Slice sampling. Annals of Statistics, 2000.</p>
<p>[25] S. Petrov and D. Klein. Improved inference for unlexicalized parsing. In HLT-NAACL ’07, 2007.</p>
<p>[26] S. Petrov and D. Klein. Discriminative log-linear grammars with latent variables. In NIPS, 2008.</p>
<p>[27] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000.</p>
<p>[28] D. Smith and N. Smith. Bilingual parsing with factored estimation: Using english to parse korean. In EMNLP ’04, 2004.</p>
<p>[29] D. A. Smith and J. Eisner. Dependency parsing by belief propagation. In EMNLP, 2008.</p>
<p>[30] R. H. Swendsen and J. S. Wang. Nonuniversal critical dynamics in MC simulations. Rev. Lett, 1987.</p>
<p>[31] M. A. Tanner and W. H. Wong. The calculation of posterior distributions by data augmentation. JASA, 1987.</p>
<p>[32] B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning. Max-margin parsing. In EMNLP, 2004.</p>
<p>[33] L. Tierney. Markov chains for exploring posterior distributions. The Annals of Statistics, 1994.</p>
<p>[34] I. Titov and J. Henderson. Loss minimization in parse reranking. In EMNLP, 2006.</p>
<p>[35] J. Turian, B. Wellington, and I. D. Melamed. Scalable discriminative learning for natural language parsing and translation. In NIPS, 2006.</p>
<p>[36] J. Van Gael, Y. Saatci, Y. W. Teh, and Z. Ghahramani. Beam sampling for the inﬁnite hidden Markov model. In ICML, 2008.</p>
<p>[37] S. G. Walker. Sampling the Dirichlet mixture model with slices. Communications in Statistics - Simulation and Computation, 2007.</p>
<p>[38] J. Wolfe, A. Haghighi, and D. Klein. Fully distributed em for very large datasets. In ICML ’08, 2008.</p>
<p>[39] N. Xue, F-D Chiou, and M. Palmer. Building a large-scale annotated Chinese corpus. In COLING, 2002.</p>
<p>[40] H. Zhang and D. Gildea. Stochastic lexicalized inversion transduction grammar for alignment. In ACL, 2005.</p>
<p>[41] H. Zhang, C. Quirk, R. C. Moore, and D. Gildea. Bayesian learning of non-compositional phrases with synchronous parsing. In ACL, 2008.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
