<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1 nips-2009-$L 1$-Penalized Robust Estimation for a Class of Inverse Problems Arising in Multiview Geometry</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-1" href="../nips2009/nips-2009-%24L_1%24-Penalized_Robust_Estimation_for_a_Class_of_Inverse_Problems_Arising_in_Multiview_Geometry.html">nips2009-1</a> <a title="nips-2009-1-reference" href="#">nips2009-1-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1 nips-2009-$L 1$-Penalized Robust Estimation for a Class of Inverse Problems Arising in Multiview Geometry</h1>
<br/><p>Source: <a title="nips-2009-1-pdf" href="http://papers.nips.cc/paper/3677-l_1-penalized-robust-estimation-for-a-class-of-inverse-problems-arising-in-multiview-geometry.pdf">pdf</a></p><p>Author: Arnak Dalalyan, Renaud Keriven</p><p>Abstract: We propose a new approach to the problem of robust estimation in multiview geometry. Inspired by recent advances in the sparse recovery problem of statistics, we deﬁne our estimator as a Bayesian maximum a posteriori with multivariate Laplace prior on the vector describing the outliers. This leads to an estimator in which the ﬁdelity to the data is measured by the L∞ -norm while the regularization is done by the L1 -norm. The proposed procedure is fairly fast since the outlier removal is done by solving one linear program (LP). An important difference compared to existing algorithms is that for our estimator it is not necessary to specify neither the number nor the proportion of the outliers. We present strong theoretical results assessing the accuracy of our procedure, as well as a numerical example illustrating its efﬁciency on real data. 1</p><br/>
<h2>reference text</h2><p>[1] F. Bach. Bolasso: model consistent Lasso estimation through the bootstrap. In Twenty-ﬁfth International Conference on Machine Learning (ICML), 2008. 7</p>
<p>[2] P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of lasso and Dantzig selector. Ann. Statist., 37(4):1705–1732, 2009. 2, 6</p>
<p>[3] E. Cand` s and T. Tao. The Dantzig selector: statistical estimation when p is much larger than n. Ann. e Statist., 35(6):2313–2351, 2007. 6</p>
<p>[4] E. J. Cand` s. The restricted isometry property and its implications for compressed sensing. C. R. Math. e Acad. Sci. Paris, 346(9-10):589–592, 2008. 6</p>
<p>[5] E. J. Cand` s and P. A. Randall. Highly robust error correction by convex programming. IEEE Trans. e Inform. Theory, 54(7):2829–2840, 2008. 2</p>
<p>[6] E. J. Cand` s, J. K. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate measuree ments. Comm. Pure Appl. Math., 59(8):1207–1223, 2006. 2, 6</p>
<p>[7] C. Chesneau and M. Hebiri. Some theoretical results on the grouped variables Lasso. Math. Methods Statist., 17(4):317–326, 2008. 7</p>
<p>[8] A. S. Dalalyan, A. Juditsky, and V. Spokoiny. A new algorithm for estimating the effective dimensionreduction subspace. Journal of Machine Learning Research, 9:1647–1678, Aug. 2008. 3</p>
<p>[9] A. S. Dalalyan and A. B. Tsybakov. Aggregation by exponential weighting, sharp PAC-bayesian bounds and sparsity. Machine Learning, 72(1-2):39–61, 2008. 7</p>
<p>[10] D. Donoho, M. Elad, and V. Temlyakov. Stable recovery of sparse overcomplete representations in the presence of noise. IEEE Trans. Inform. Theory, 52(1):6–18, 2006. 2, 6</p>
<p>[11] D. L. Donoho and X. Huo. Uncertainty principles and ideal atomic decomposition. IEEE Trans. Inform. Theory, 47(7):2845–2862, 2001. 6</p>
<p>[12] O. Enqvist and F. Kahl. Robust optimal pose estimation. In ECCV, pages I: 141–153, 2008. 2</p>
<p>[13] R. Hartley and F. Kahl. Optimal algorithms in multiview geometry. In ACCV, volume 1, pages 13 – 34, Nov. 2007. 2</p>
<p>[14] R. Hartley and F. Kahl. Global optimization through rotation space search. IJCV, 2009. 2</p>
<p>[15] R. I. Hartley and F. Schaffalitzky. L∞ minimization in geometric reconstruction problems. In CVPR (1), pages 504–509, 2004. 2, 3</p>
<p>[16] R. I. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, June 2004. 1, 3</p>
<p>[17] F. Kahl. Multiple view geometry and the L∞ -norm. In ICCV, pages 1002–1009. IEEE Computer Society, 2005. 3</p>
<p>[18] F. Kahl and R. I. Hartley. Multiple-view geometry under the L∞ norm. IEEE Trans. Pattern Analysis and Machine Intelligence, 30(9):1603–1617, sep 2008. 2, 3</p>
<p>[19] T. Kanade and Q. Ke. Quasiconvex optimization for robust geometric reconstruction. In ICCV, pages II: 986–993, 2005. 2, 8</p>
<p>[20] Q. Ke and T. Kanade. Uncertainty models in quasiconvex optimization for geometric reconstruction. In CVPR, pages I: 1199–1205, 2006. 4</p>
<p>[21] H. D. Li. A practical algorithm for L∞ triangulation with outliers. In CVPR, pages 1–8, 2007. 2</p>
<p>[22] D. Martinec and T. Pajdla. Robust rotation and translation estimation in multiview reconstruction. In CVPR, pages 1–8, 2007. 2</p>
<p>[23] D. Nist´ r. An efﬁcient solution to the ﬁve-point relative pose problem. IEEE Trans. Pattern Anal. Mach. e Intell, 26(6):756–777, 2004. 1</p>
<p>[24] C. Olsson, A. P. Eriksson, and F. Kahl. Efﬁcient optimization for L∞ problems using pseudoconvexity. In ICCV, pages 1–8, 2007. 2</p>
<p>[25] Y. D. Seo and R. I. Hartley. A fast method to minimize L∞ error norm for geometric vision problems. In ICCV, pages 1–8, 2007. 2</p>
<p>[26] Y. D. Seo, H. J. Lee, and S. W. Lee. Sparse structures in L-inﬁnity norm minimization for structure and motion reconstruction. In ECCV, pages I: 780–793, 2008. 2</p>
<p>[27] K. Sim and R. Hartley. Removing outliers using the L∞ norm. In CVPR, pages I: 485–494, 2006. 2, 8</p>
<p>[28] J. F. Sturm. Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones. Optim. Methods Softw., 11/12(1-4):625–653, 1999. 7</p>
<p>[29] P. Zhao and B. Yu. On model selection consistency of Lasso. J. Mach. Learn. Res., 7:2541–2563, 2006. 6  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
