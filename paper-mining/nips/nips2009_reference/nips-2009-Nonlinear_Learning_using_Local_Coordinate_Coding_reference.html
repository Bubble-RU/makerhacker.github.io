<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>169 nips-2009-Nonlinear Learning using Local Coordinate Coding</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-169" href="../nips2009/nips-2009-Nonlinear_Learning_using_Local_Coordinate_Coding.html">nips2009-169</a> <a title="nips-2009-169-reference" href="#">nips2009-169-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>169 nips-2009-Nonlinear Learning using Local Coordinate Coding</h1>
<br/><p>Source: <a title="nips-2009-169-pdf" href="http://papers.nips.cc/paper/3875-nonlinear-learning-using-local-coordinate-coding.pdf">pdf</a></p><p>Author: Kai Yu, Tong Zhang, Yihong Gong</p><p>Abstract: This paper introduces a new method for semi-supervised learning on high dimensional nonlinear manifolds, which includes a phase of unsupervised basis learning and a phase of supervised function learning. The learned bases provide a set of anchor points to form a local coordinate system, such that each data point x on the manifold can be locally approximated by a linear combination of its nearby anchor points, and the linear weights become its local coordinate coding. We show that a high dimensional nonlinear function can be approximated by a global linear function with respect to this coding scheme, and the approximation quality is ensured by the locality of such coding. The method turns a difﬁcult nonlinear learning problem into a simple global linear learning problem, which overcomes some drawbacks of traditional local learning methods. 1</p><br/>
<h2>reference text</h2><p>[1] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15:1373 – 1396, 2003.</p>
<p>[2] Leon Bottou and Vladimir Vapnik. Local learning algorithms. Neural Computation, 4:888 – 900, 1992.</p>
<p>[3] Robert M. Gray and David L. Neuhoff. Quantization. IEEE Transaction on Information Theory, pages 2325 – 2383, 1998.</p>
<p>[4] Trevor Hastie and Clive Loader. Local regression: Automatic kernel carpentry. Statistical Science, 8:139 – 143, 1993.</p>
<p>[5] Geoffrey E. Hinton and Ruslan R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313:504 – 507, 2006.</p>
<p>[6] Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Y. Ng. Efﬁcient sparse coding algorithms. Neural Information Processing Systems (NIPS) 19, 2007.</p>
<p>[7] Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer, and Andrew Y. Ng. Self-taught learning: Transfer learning from unlabeled data. International Conference on Machine Learning, 2007.</p>
<p>[8] Sam Roweis and Lawrence Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290:2323 – 2326, 2000.</p>
<p>[9] Joshua B. Tenenbaum, Vin De Silva, and John C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290:2319 – 2323, 2000.</p>
<p>[10] Alon Zakai and Ya’acov Ritov. Consistency and localizability. Journal of Machine Learning Research, 10:827 – 856, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
