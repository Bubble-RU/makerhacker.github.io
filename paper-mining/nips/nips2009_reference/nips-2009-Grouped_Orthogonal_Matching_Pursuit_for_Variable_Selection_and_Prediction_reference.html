<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>105 nips-2009-Grouped Orthogonal Matching Pursuit for Variable Selection and Prediction</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-105" href="../nips2009/nips-2009-Grouped_Orthogonal_Matching_Pursuit_for_Variable_Selection_and_Prediction.html">nips2009-105</a> <a title="nips-2009-105-reference" href="#">nips2009-105-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>105 nips-2009-Grouped Orthogonal Matching Pursuit for Variable Selection and Prediction</h1>
<br/><p>Source: <a title="nips-2009-105-pdf" href="http://papers.nips.cc/paper/3878-grouped-orthogonal-matching-pursuit-for-variable-selection-and-prediction.pdf">pdf</a></p><p>Author: Grzegorz Swirszcz, Naoki Abe, Aurelie C. Lozano</p><p>Abstract: We consider the problem of variable group selection for least squares regression, namely, that of selecting groups of variables for best regression performance, leveraging and adhering to a natural grouping structure within the explanatory variables. We show that this problem can be efﬁciently addressed by using a certain greedy style algorithm. More precisely, we propose the Group Orthogonal Matching Pursuit algorithm (Group-OMP), which extends the standard OMP procedure (also referred to as “forward greedy feature selection algorithm” for least squares regression) to perform stage-wise group variable selection. We prove that under certain conditions Group-OMP can identify the correct (groups of) variables. We also provide an upperbound on the l∞ norm of the difference between the estimated regression coefﬁcients and the true coefﬁcients. Experimental results on simulated and real world datasets indicate that Group-OMP compares favorably to Group Lasso, OMP and Lasso, both in terms of variable selection and prediction accuracy. 1</p><br/>
<h2>reference text</h2><p>[1] BACH , F.R., Consistency of the Group Lasso and Multiple Kernel Learning, J. Mach. Learn. Res., 9, 1179-1225, 2008.</p>
<p>[2] BAI D., Y IN Y.Q., Limit of the smallest eigenvalue of a large dimensional sample covariance matrix, Ann. Probab. 21, 1275-1294, 1993.</p>
<p>[3] C HEN J., H UO X., Sparse representations for multiple measurement vectors (MMV) in an overcomplete dictionary, in Proc. of the 2005 IEEE Int. Conf. on Acoustics, Speech, and Signal Proc., 2005.</p>
<p>[4] H UANG J., Z HANG T., M ETAXAS D., Learning with Structured Sparsity, in ICML’09, 2009.</p>
<p>[5] M ALLAT S., Z HANG Z., Matching pursuits with time-frequency dictionaries, IEEE Transactions on Signal Processing, 41, 3397-3415, 1993.</p>
<p>[6] M OORE , E.H, On the reciprocal of the general algebraic matrix, Bulletin of the American Mathematical Society 26, 394-395, 1920.</p>
<p>[7] P ENROSE , R., A generalized inverse for matrices, Proceedings of the Cambridge Philosophical Society 51, 406-413, 1955.</p>
<p>[8] T IBSHIRANI , R., Regression shrinkage and selection via the lasso, J. Royal. Statist. Soc B., 58(1), 267-288, 1996.</p>
<p>[9] T ROPP J.A., Greed is good: Algorithmic results for sparse approximation, IEEE Trans. Info. Theory, 50(10), 2231-2242, 2004.</p>
<p>[10] T ROPP J.A., G ILBERT A.C. , S TRAUSS M.J., Algorithms for simultaneous sparse approximation, Part I: greedy pursuit, Signal Proc. 86 (3), 572-588, 2006.</p>
<p>[11] P EOTTA L., VANDERGHEYNST P., Matching Pursuit with Block Incoherent Dictionaries, Signal Proc. 55 (9), 2007.</p>
<p>[12] Y UAN , M., L IN , Y., Model selection and estimation in regression with grouped variables, J. R. Statist. Soc. B, 68, 4967, 2006.</p>
<p>[13] Z HANG , T., On the consistency of feature selection using greedy least squares regression, J. Machine Learning Research, 2008.</p>
<p>[14] Z HANG , T., Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models, in NIPS08, 2008.</p>
<p>[15] Z HAO , P, ROCHA , G. AND Y U , B., Grouped and hierarchical model selection through composite absolute penalties, Manuscript, 2006.</p>
<p>[16] Z OU , H., H ASTIE T., Regularization and variable selection via the Elastic Net., J. R. Statist. Soc. B, 67(2) 301-320, 2005.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
