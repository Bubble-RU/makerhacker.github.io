<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>145 nips-2009-Manifold Embeddings for Model-Based Reinforcement Learning under Partial Observability</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-145" href="../nips2009/nips-2009-Manifold_Embeddings_for_Model-Based_Reinforcement_Learning_under_Partial_Observability.html">nips2009-145</a> <a title="nips-2009-145-reference" href="#">nips2009-145-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>145 nips-2009-Manifold Embeddings for Model-Based Reinforcement Learning under Partial Observability</h1>
<br/><p>Source: <a title="nips-2009-145-pdf" href="http://papers.nips.cc/paper/3640-manifold-embeddings-for-model-based-reinforcement-learning-under-partial-observability.pdf">pdf</a></p><p>Author: Keith Bush, Joelle Pineau</p><p>Abstract: Interesting real-world datasets often exhibit nonlinear, noisy, continuous-valued states that are unexplorable, are poorly described by ﬁrst principles, and are only partially observable. If partial observability can be overcome, these constraints suggest the use of model-based reinforcement learning. We experiment with manifold embeddings to reconstruct the observable state-space in the context of offline, model-based reinforcement learning. We demonstrate that the embedding of a system can change as a result of learning, and we argue that the best performing embeddings well-represent the dynamics of both the uncontrolled and adaptively controlled system. We apply this approach to learn a neurostimulation policy that suppresses epileptic seizures on animal brain slices. 1</p><br/>
<h2>reference text</h2><p>[1] Christopher G. Atkeson, Andrew W. Moore, and Stefan Schaal. Locally weighted learning for control. Artiﬁcial Intelligence Review, 11:75–113, 1997.</p>
<p>[2] Christopher G. Atkeson and Jun Morimoto. Nonparametric representation of policies and value functions: A trajectory-based approach. In Advances in Neural Information Processing, 2003.</p>
<p>[3] M. Bowling, A. Ghodsi, and D. Wilkinson. Action respecting embedding. In Proceedings of ICML, 2005.</p>
<p>[4] F. Lopes da Silva, W. Blanes, S. Kalitzin, J. Parra, P. Suffczynski, and D. Velis. Dynamical diseases of brain systems: Different routes to epileptic seizures. IEEE Transactions on Biomedical Engineering, 50(5):540–548, 2003.</p>
<p>[5] G. D’Arcangelo, G. Panuccio, V. Tancredi, and M. Avoli. Repetitive low-frequency stimulation reduces epileptiform synchronization in limbic neuronal networks. Neurobiology of Disease, 19:119–128, 2005.</p>
<p>[6] Damien Ernst, Pierre Guerts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research, 6:503–556, 2005.</p>
<p>[7] A. Galka. Topics in Nonlinear Time Series Analysis: with implications for EEG Analysis. World Scientiﬁc, 2000.</p>
<p>[8] J.P. Huke. Embedding nonlinear dynamical systems: A guide to Takens’ Theorem. Technical report, Manchester Institute for Mathematical Sciences, University of Manchester, March, 2006.</p>
<p>[9] K. Jerger and S. Schiff. Periodic pacing and in vitro epileptic focus. Journal of Neurophysiology, 73(2):876–879, 1995.</p>
<p>[10] Nicholas K. Jong and Peter Stone. Model-based function approximation in reinforcement learning. In Proceedings of AAMAS, 2007.</p>
<p>[11] P.W. Keller, S. Mannor, and D. Precup. Automatic basis function construction for approximate dynamic programming and reinforcement learning. In Proceedings of ICML, 2006.</p>
<p>[12] M. Kennel and H. Abarbanel. False neighbors and false strands: A reliable minimum embedding dimension algorithm. Physical Review E, 66:026209, 2002.</p>
<p>[13] S. Mahadevan and M. Maggioni. Proto-value functions: A Laplacian framework for learning representation and control in Markov decision processes. Journal of Machine Learning Research, 8:2169–2231, 2007.</p>
<p>[14] A. K. McCallum. Reinforcement Learning with Selective Perception and Hidden State. PhD thesis, University of Rochester, 1996.</p>
<p>[15] R. Munos and A. Moore. Variable resolution discretization in optimal control. Machine Learning, 49:291– 323, 2002.</p>
<p>[16] U. Parlitz and C. Merkwirth. Prediction of spatiotemporal time series based on reconstructed local states. Physical Review Letters, 84(9):1890–1893, 2000.</p>
<p>[17] Jan Peters, Sethu Vijayakumar, and Stefan Schaal. Natural actor-critic. In Proceedings of ECML, 2005.</p>
<p>[18] Tim Sauer, James A. Yorke, and Martin Casdagli. 65:3/4:579–616, 1991.  Embedology.  Journal of Statistical Physics,</p>
<p>[19] S. Singh, M. L. Littman, N. K. Jong, D. Pardoe, and P. Stone. Learning predictive state representations. In Proceedings of ICML, 2003.</p>
<p>[20] W. Smart. Explicit manifold representations for value-functions in reinforcement learning. In Proceedings of ISAIM, 2004.</p>
<p>[21] J. Stark. Delay embeddings for forced systems. I. Deterministic forcing. Journal of Nonlinear Science, 9:255–332, 1999.</p>
<p>[22] J. Stark, D.S. Broomhead, M.E. Davies, and J. Huke. Delay embeddings for forced systems. II. Stochastic forcing. Journal of Nonlinear Science, 13:519–577, 2003.</p>
<p>[23] R. Sutton and A. Barto. Reinforcement learning: An introduction. The MIT Press, Cambridge, MA, 1998.</p>
<p>[24] F. Takens. Detecting strange attractors in turbulence. In D. A. Rand & L. S. Young, editor, Dynamical Systems and Turbulence, volume 898, pages 366–381. Warwick, 1980.</p>
<p>[25] D. Wingate and S. Singh. On discovery and learning of models with predictive state representations of state for agents with continuous actions and observations. In Proceedings of AAMAS, 2007.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
