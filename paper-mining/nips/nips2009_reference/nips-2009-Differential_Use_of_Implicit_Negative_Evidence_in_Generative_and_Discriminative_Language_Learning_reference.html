<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>66 nips-2009-Differential Use of Implicit Negative Evidence in Generative and Discriminative Language Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-66" href="../nips2009/nips-2009-Differential_Use_of_Implicit_Negative_Evidence_in_Generative_and_Discriminative_Language_Learning.html">nips2009-66</a> <a title="nips-2009-66-reference" href="#">nips2009-66-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>66 nips-2009-Differential Use of Implicit Negative Evidence in Generative and Discriminative Language Learning</h1>
<br/><p>Source: <a title="nips-2009-66-pdf" href="http://papers.nips.cc/paper/3760-differential-use-of-implicit-negative-evidence-in-generative-and-discriminative-language-learning.pdf">pdf</a></p><p>Author: Anne Hsu, Thomas L. Griffiths</p><p>Abstract: A classic debate in cognitive science revolves around understanding how children learn complex linguistic rules, such as those governing restrictions on verb alternations, without negative evidence. Traditionally, formal learnability arguments have been used to claim that such learning is impossible without the aid of innate language-speciﬁc knowledge. However, recently, researchers have shown that statistical models are capable of learning complex rules from only positive evidence. These two kinds of learnability analyses differ in their assumptions about the distribution from which linguistic input is generated. The former analyses assume that learners seek to identify grammatical sentences in a way that is robust to the distribution from which the sentences are generated, analogous to discriminative approaches in machine learning. The latter assume that learners are trying to estimate a generative model, with sentences being sampled from that model. We show that these two learning approaches differ in their use of implicit negative evidence – the absence of a sentence – when learning verb alternations, and demonstrate that human learners can produce results consistent with the predictions of both approaches, depending on how the learning problem is presented. 1</p><br/>
<h2>reference text</h2><p>[1] C. L. Baker. Syntactic theory and the projection problem. Linguistic Inquiry, 10:533–538, 1979.</p>
<p>[2] C. L. Baker and J. J. McCarthy. The logical problem of language acquisition. MIT Press, 1981.</p>
<p>[3] N. Chomsky. Aspects if the theories of syntax. MIT Press, 1965.</p>
<p>[4] S. Pinker. Learnability and Cognition: The acquisition of argument structure. MIT Press, 1989.</p>
<p>[5] M. Bowerman. The ’No Negative Evidence’ Problem: How do children avoid constructing an overly general grammar? In J. Hawkins, editor, Explaining Language Universals, pages 73–101. Blackwell, New York, 1988.</p>
<p>[6] R. Brown and C. Hanlon. Derivational complexity and order of acquisition in child speech. Wiley, 1970.</p>
<p>[7] G. F. Marcus. Negative evidence in language acquisition. Cognition, 46:53–85, 1993.</p>
<p>[8] E. M. Gold. Language identiﬁcation in the limit. Information and Control, 16:447–474, 1967.</p>
<p>[9] M. A. Nowak, N. L. Komarova, and P. Niyogi. Computational and evolutionary aspects of language. Nature, 417:611–617, 2002.</p>
<p>[10] S. Crain and L. D. Martin. An introduction to linguistic theory and language acquisition. Blackwell, 1999.</p>
<p>[11] D. Angluin. Identifying languages from stochastic examples. Technical Report YALEU/DCS/RR-614, Yale University, Department of Computer Science, 1988.</p>
<p>[12] J. J. Horning. A study of grammatical inference. PhD thesis, Stanford University, 1969.</p>
<p>[13] N. Chater and P. Vitanyi. “Ideal learning” of natural language: Positive results about learning from positive evidence. Journal of Mathematical Psychology, 51:135–163, 2007.</p>
<p>[14] M. Dowman. Addressing the learnability of verb subcategorizations with Bayesian inference. In Proceedings of the 22nd Annual Conference of the Cognitive Science Society, 2005.</p>
<p>[15] D. Kemp, A. Perfors, and J. Tenenbaum. Learning overhypothesis with hierarchical Bayesian models. Developmental Science, 10:307–321, 2007.</p>
<p>[16] P. Langley and S. Stromsten. Learning context-free grammars with a simplicity bias. In Proceedings of the 11th European Conference on Machine Learning, 2000.</p>
<p>[17] L. Onnis, M. Roberts, and N. Chater. Simplicity: A cure for overgeneralizations in language acquisition? In Proceedings of the 24th Annual Conference of the Cognitive Science Society, pages 720–725, 2002.</p>
<p>[18] A. Perfors, J. Tenenbaum, and T. Regier. Poverty of the stimulus: A rational approach? In Proceedings of the 28th Annual Conference of the Cognitive Science Society, pages 664–668, 2006.</p>
<p>[19] A. Stolcke. Bayesian learning of probabilistic language models. PhD thesis, UC Berkeley, 1994.</p>
<p>[20] E. Wonnacott, E. Newport, and M. Tanenhaus. Acquiring and processing verb argument structure: Distributional learning in a miniature language. Cognitive Psychology, 56:165–209, 2008.</p>
<p>[21] A. Y. Ng and M. Jordan. On discriminative vs. generative classiﬁers: A comparison of logistic regression and naive Bayes. In Advances in Neural Information Processing Systems 17, 2001.</p>
<p>[22] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. Bayesian data analysis. Chapman Hall, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
