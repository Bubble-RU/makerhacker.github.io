<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 nips-2009-Conditional Neural Fields</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-56" href="../nips2009/nips-2009-Conditional_Neural_Fields.html">nips2009-56</a> <a title="nips-2009-56-reference" href="#">nips2009-56-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>56 nips-2009-Conditional Neural Fields</h1>
<br/><p>Source: <a title="nips-2009-56-pdf" href="http://papers.nips.cc/paper/3869-conditional-neural-fields.pdf">pdf</a></p><p>Author: Jian Peng, Liefeng Bo, Jinbo Xu</p><p>Abstract: Conditional random ﬁelds (CRF) are widely used for sequence labeling such as natural language processing and biological sequence analysis. Most CRF models use a linear potential function to represent the relationship between input features and output. However, in many real-world applications such as protein structure prediction and handwriting recognition, the relationship between input features and output is highly complex and nonlinear, which cannot be accurately modeled by a linear function. To model the nonlinear relationship between input and output we propose a new conditional probabilistic graphical model, Conditional Neural Fields (CNF), for sequence labeling. CNF extends CRF by adding one (or possibly more) middle layer between input and output. The middle layer consists of a number of gate functions, each acting as a local neuron or feature extractor to capture the nonlinear relationship between input and output. Therefore, conceptually CNF is much more expressive than CRF. Experiments on two widely-used benchmarks indicate that CNF performs signiﬁcantly better than a number of popular methods. In particular, CNF is the best among approximately 10 machine learning methods for protein secondary structure prediction and also among a few of the best methods for handwriting recognition.</p><br/>
<h2>reference text</h2><p>[1] Fei Sha and O. Pereira. Shallow parsing with conditional random ﬁelds. In Proceedings of Human Language Technology-NAACL 2003.</p>
<p>[2] D. T. Jones. Protein secondary structure prediction based on position-speciﬁc scoring matrices. Journal of Molecular Biology, 292(2):195–202, September 1999.</p>
<p>[3] Feng Zhao, Shuaicheng Li, Beckett W. Sterner, and Jinbo Xu. Discriminative learning for protein conformation sampling. Proteins, 73(1):228–240, October 2008.</p>
<p>[4] Feng Zhao, Jian Peng, Joe Debartolo, Karl F. Freed, Tobin R. Sosnick, and Jinbo Xu. A probabilistic graphical model for ab initio folding. In RECOMB 2’09: Proceedings of the 13th Annual International Conference on Research in Computational Molecular Biology, pages 59– 73, Berlin, Heidelberg, 2009. Springer-Verlag.</p>
<p>[5] Sy Bor Wang, Ariadna Quattoni, Louis-Philippe Morency, and David Demirdjian. Hidden conditional random ﬁelds for gesture recognition. In CVPR 2006.</p>
<p>[6] Lawrence R. Rabiner. A tutorial on hidden markov models and selected applications in speech recognition. In Proceedings of the IEEE, 1989.</p>
<p>[7] John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In ICML 2001.</p>
<p>[8] Ben Taskar, Carlos Guestrin, and Daphne Koller. Max-margin markov networks. In NIPS 2003. 8</p>
<p>[9] Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. Support vector machine learning for interdependent and structured output spaces. In ICML 2004.</p>
<p>[10] Nam Nguyen and Yunsong Guo. Comparisons of sequence labeling algorithms and extensions. In ICML 2007.</p>
<p>[11] Yan Liu, Jaime Carbonell, Judith Klein-Seetharaman, and Vanathi Gopalakrishnan. Comparison of probabilistic combination methods for protein secondary structure prediction. Bioinformatics, 20(17), November 2004.</p>
<p>[12] D. C. Liu and J. Nocedal. On the limited memory bfgs method for large scale optimization. Mathematical Programming, 45(3), 1989.</p>
<p>[13] Richard H. Byrd, Jorge Nocedal, and Robert B. Schnabel. Representations of quasi-newton matrices and their use in limited memory methods. Mathematical Programming, 63(2), 1994.</p>
<p>[14] David J. C. Mackay. A practical bayesian framework for backpropagation networks. Neural Computation, 4:448–472, 1992.</p>
<p>[15] Christopher M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, November 1995.</p>
<p>[16] John Lafferty, Xiaojin Zhu, and Yan Liu. Kernel conditional random ﬁelds: representation and clique selection. In ICML 2004.</p>
<p>[17] Yoshua Bengio, R´ jean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic e language model. Journal of Machine Learning Research, 3:1137–1155, 2003.</p>
<p>[18] Ilya Sutskever, Geoffrey E Hinton, and Graham Taylor. The recurrent temporal restricted boltzmann machine. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, NIPS 2009.</p>
<p>[19] Barbara Hammer. Recurrent networks for structured data - a unifying approach and its properties. Cognitive Systems Research, 2002.</p>
<p>[20] Alex Graves and Juergen Schmidhuber. Ofﬂine handwriting recognition with multidimensional recurrent neural networks. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, NIPS 2009.</p>
<p>[21] S. F. Altschul, T. L. Madden, A. A. Sch¨ ffer, J. Zhang, Z. Zhang, W. Miller, and D. J. Lipman. a Gapped blast and psi-blast: a new generation of protein database search programs. Nucleic Acids Research, 25, September 1997.</p>
<p>[22] James A. Cuff and Geoffrey J. Barton. Evaluation and improvement of multiple sequence methods for protein secondary structure prediction. Proteins: Structure, Function, and Genetics, 34, 1999.</p>
<p>[23] Wolfgang Kabsch and Christian Sander. Dictionary of protein secondary structure: Pattern recognition of hydrogen-bonded and geometrical features. Biopolymers, 22(12):2577–2637, December 1983.</p>
<p>[24] H. Kim and H. Park. Protein secondary structure prediction based on an improved support vector machines approach. Protein Engineering, 16(8), August 2003.</p>
<p>[25] Wei Chu, Zoubin Ghahramani, and David. A graphical model for protein secondary structure prediction. In ICML 2004.</p>
<p>[26] Sujun Hua and Zhirong Sun. A novel method of protein secondary structure prediction with high segment overlap measure: Support vector machine approach. Journal of Molecular Biology, 308, 2001.</p>
<p>[27] George Karypis. Yasspp: Better kernels and coding schemes lead to improvements in protein secondary structure prediction. Proteins: Structure, Function, and Bioinformatics, 64(3):575– 586, 2006.</p>
<p>[28] O. Dor and Y. Zhou. Achieving 80% ten-fold cross-validated accuracy for secondary structure prediction by large-scale training. Proteins: Structure, Function, and Bioinformatics, 66, March 2007.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
