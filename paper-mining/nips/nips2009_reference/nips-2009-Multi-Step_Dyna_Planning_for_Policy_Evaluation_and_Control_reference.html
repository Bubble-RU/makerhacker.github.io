<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>159 nips-2009-Multi-Step Dyna Planning for Policy Evaluation and Control</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-159" href="../nips2009/nips-2009-Multi-Step_Dyna_Planning_for_Policy_Evaluation_and_Control.html">nips2009-159</a> <a title="nips-2009-159-reference" href="#">nips2009-159-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>159 nips-2009-Multi-Step Dyna Planning for Policy Evaluation and Control</h1>
<br/><p>Source: <a title="nips-2009-159-pdf" href="http://papers.nips.cc/paper/3670-multi-step-dyna-planning-for-policy-evaluation-and-control.pdf">pdf</a></p><p>Author: Hengshuai Yao, Shalabh Bhatnagar, Dongcui Diao, Richard S. Sutton, Csaba Szepesvári</p><p>Abstract: In this paper we introduce a multi-step linear Dyna-style planning algorithm. The key element of the multi-step linear Dyna is a multi-step linear model that enables multi-step projection of a sampled feature and multi-step planning based on the simulated multi-step transition experience. We propose two multi-step linear models. The ﬁrst iterates the one-step linear model, but is generally computationally complex. The second interpolates between the one-step model and the inﬁnite-step model (which turns out to be the LSTD solution), and can be learned efﬁciently online. Policy evaluation on Boyan Chain shows that multi-step linear Dyna learns a policy faster than single-step linear Dyna, and generally learns faster as the number of projection steps increases. Results on Mountain-car show that multi-step linear Dyna leads to much better online performance than single-step linear Dyna and model-free algorithms; however, the performance of multi-step linear Dyna does not always improve as the number of projection steps increases. Our results also suggest that previous attempts on extending LSTD for online control were unsuccessful because LSTD looks inﬁnite steps into the future, and suffers from the model errors in non-stationary (control) environments.</p><br/>
<h2>reference text</h2><p>Bertsekas, D. P., Borkar, V., & Nedich, A. (2004). Improved temporal difference methods with linear function approximation. Learning and Approximate Dynamic Programming (pp. 231–255). IEEE Press. Boyan, J. A. (1999). Least-squares temporal difference learning. ICML-16. Bradtke, S., & Barto, A. G. (1996). Linear least-squares algorithms for temporal difference learning. Machine Learning, 22, 33–57. Li, L., Littman, M. L., & Mansley, C. R. (2009). Online exploration in least-squares policy iteration. AAMAS-8. Peters, J., & Schaal, S. (2008). Natural actor-critic. Neurocomputing, 71, 1180–1190. Sutton, R. S. (1990). Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. ICML-7. Sutton, R. S. (1995). TD models: modeling the world at a mixture of time scales. ICML-12. Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning: An introduction. MIT Press. Sutton, R. S., Szepesv´ ri, C., Geramifard, A., & Bowling, M. (2008). Dyna-style planning with a linear function approximation and prioritized sweeping. UAI-24. Tsitsiklis, J. N., & Van Roy, B. (1997). An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 42, 674–690. Xu, X., He, H., & Hu, D. (2002). Efﬁcient reinforcement learning using recursive least-squares methods. Journal of Artiﬁcial Intelligence Research, 16, 259–292.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
