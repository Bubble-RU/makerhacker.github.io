<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>16 nips-2009-A Smoothed Approximate Linear Program</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-16" href="../nips2009/nips-2009-A_Smoothed_Approximate_Linear_Program.html">nips2009-16</a> <a title="nips-2009-16-reference" href="#">nips2009-16-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>16 nips-2009-A Smoothed Approximate Linear Program</h1>
<br/><p>Source: <a title="nips-2009-16-pdf" href="http://papers.nips.cc/paper/3799-a-smoothed-approximate-linear-program.pdf">pdf</a></p><p>Author: Vijay Desai, Vivek Farias, Ciamac C. Moallemi</p><p>Abstract: We present a novel linear program for the approximation of the dynamic programming cost-to-go function in high-dimensional stochastic control problems. LP approaches to approximate DP naturally restrict attention to approximations that are lower bounds to the optimal cost-to-go function. Our program – the ‘smoothed approximate linear program’ – relaxes this restriction in an appropriate fashion while remaining computationally tractable. Doing so appears to have several advantages: First, we demonstrate superior bounds on the quality of approximation to the optimal cost-to-go function aﬀorded by our approach. Second, experiments with our approach on a challenging problem (the game of Tetris) show that the approach outperforms the existing LP approach (which has previously been shown to be competitive with several ADP algorithms) by an order of magnitude. 1</p><br/>
<h2>reference text</h2><p>[1] D. P. de Farias and B. Van Roy. The linear programming approach to approximate dynamic programming. Operations Research, 51(6):850–865, 2003.</p>
<p>[2] D. P. de Farias and B. Van Roy. On constraint sampling in the linear programming approach to approximate dynamic programming. Mathematics of Operations Research, 293(3):462–478, 2004.</p>
<p>[3] V. F. Farias and B. Van Roy. Tetris: A study of randomized constraint sampling. In Probabilistic and Randomized Methods for Design Under Uncertainty. Springer-Verlag, 2006.</p>
<p>[4] J. Brzustowski. Can you win at Tetris? Master’s thesis, University of British Columbia, 1992.</p>
<p>[5] H. Burgiel. How to lose at Tetris. Mathematical Gazette, page 194, 1997.</p>
<p>[6] E. D. Demaine, S. Hohenberger, and D. Liben-Nowell. Tetris is hard, even to approximate. In Proceedings of the 9th International Computing and Combinatorics Conference, 2003.</p>
<p>[7] D. P. Bertsekas and S. Ioﬀe. Temporal diﬀerences–based policy iteration and applications in neuro–dynamic programming. Technical Report LIDS–P–2349, MIT Laboratory for Information and Decision Systems, 1996.</p>
<p>[8] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc, Belmont, MA, 1996.</p>
<p>[9] S. Kakade. A natural policy gradient. In Advances in Neural Information Processing Systems 14, Cambridge, MA, 2002. MIT Press.</p>
<p>[10] I. Szita and A. L˝rincz. Learning Tetris using the noisy cross-entropy method. Neural o Computation, 18:2936–2941, 2006.</p>
<p>[11] D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning applications. Information and Computation, 100:78–150, 1992.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
