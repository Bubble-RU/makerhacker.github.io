<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>222 nips-2009-Sparse Estimation Using General Likelihoods and Non-Factorial Priors</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-222" href="../nips2009/nips-2009-Sparse_Estimation_Using_General_Likelihoods_and_Non-Factorial_Priors.html">nips2009-222</a> <a title="nips-2009-222-reference" href="#">nips2009-222-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>222 nips-2009-Sparse Estimation Using General Likelihoods and Non-Factorial Priors</h1>
<br/><p>Source: <a title="nips-2009-222-pdf" href="http://papers.nips.cc/paper/3681-sparse-estimation-using-general-likelihoods-and-non-factorial-priors.pdf">pdf</a></p><p>Author: David P. Wipf, Srikantan S. Nagarajan</p><p>Abstract: Finding maximally sparse representations from overcomplete feature dictionaries frequently involves minimizing a cost function composed of a likelihood (or data ﬁt) term and a prior (or penalty function) that favors sparsity. While typically the prior is factorial, here we examine non-factorial alternatives that have a number of desirable properties relevant to sparse estimation and are easily implemented using an efﬁcient and globally-convergent, reweighted 1 -norm minimization procedure. The ﬁrst method under consideration arises from the sparse Bayesian learning (SBL) framework. Although based on a highly non-convex underlying cost function, in the context of canonical sparse estimation problems, we prove uniform superiority of this method over the Lasso in that, (i) it can never do worse, and (ii) for any dictionary and sparsity proﬁle, there will always exist cases where it does better. These results challenge the prevailing reliance on strictly convex penalty functions for ﬁnding sparse solutions. We then derive a new non-factorial variant with similar properties that exhibits further performance improvements in some empirical tests. For both of these methods, as well as traditional factorial analogs, we demonstrate the effectiveness of reweighted 1 -norm algorithms in handling more general sparse estimation problems involving classiﬁcation, group feature selection, and non-negativity constraints. As a byproduct of this development, a rigorous reformulation of sparse Bayesian classiﬁcation (e.g., the relevance vector machine) is derived that, unlike the original, involves no approximation steps and descends a well-deﬁned objective function. 1</p><br/>
<h2>reference text</h2><p>[1] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge University Press, 2004.</p>
<p>[2] A. Bruckstein, M. Elad, and M. Zibulevsky, “A non-negative and sparse enough solution of an underdetermined linear system of equations is unique,” IEEE Trans. Information Theory, vol. 54, no. 11, pp. 4813–4820, Nov. 2008.</p>
<p>[3] E. Cand` s, M. Wakin, and S. Boyd, “Enhancing sparsity by reweighted 1 minimization,” J. e Fourier Anal. Appl., vol. 14, no. 5, pp. 877–905, 2008.</p>
<p>[4] G. Cawley and N. Talbot, “Gene selection in cancer classiﬁcation using sparse logistic regression with Bayesian regularization,” Bioinformatics, vol. 22, no. 19, pp. 2348–2355, 2006.</p>
<p>[5] D. Donoho and M. Elad, “Optimally sparse representation in general (nonorthogonal) dictionaries via 1 minimization,” Proc. Nat. Acad. Sci., vol. 100, no. 5, pp. 2197–2202, 2003.</p>
<p>[6] M. Fazel, H. Hindi, and S. Boyd, “Log-Det heuristic for matrix rank minimization with applications to hankel and Euclidean distance matrices,” Proc. American Control Conf., vol. 3, pp. 2156–2162, June 2003.</p>
<p>[7] B. Krishnapuram, L. Carin, M. Figueiredo, and A. Hartemink, “Sparse multinomial logistic regression: Fast algorithms and generalization bounds,” IEEE Trans. Pattn Anal. Mach. Intell., vol. 27, pp. 957–968, 2005.</p>
<p>[8] D. Luenberger, Linear and Nonlinear Programming, Addison–Wesley, Reading, Massachusetts, second edition, 1984.</p>
<p>[9] D. Malioutov, M. Cetin, and A.S. Willsky, “Optimal sparse representations in general overcom¸ plete bases,” IEEE Int. Conf. Acoust., Speech, and Sig. Proc., vol. 2, pp. II–793–796, 2004.</p>
<p>[10] R. Neal, Bayesian Learning for Neural Networks, Springer-Verlag, New York, 1996.</p>
<p>[11] Y. Qi, T. Minka, R. Picard, and Z. Ghahramani, “Predictive automatic relevance determination by expectation propagation,” Int. Conf. Machine Learning (ICML), pp. 85–92, 2004.</p>
<p>[12] R. Showalter, “Monotone operators in Banach space and nonlinear partial differential equations,” Mathematical Surveys and Monographs 49. AMS, Providence, RI, 1997.</p>
<p>[13] J. Silva, J. Marques, and J. Lemos, “Selecting landmark points for sparse manifold learning,” Advances in Neural Information Processing Systems 18, pp. 1241–1248, 2006.</p>
<p>[14] R. Tibshirani, “Regression shrinkage and selection via the Lasso,” Journal of the Royal Statistical Society, vol. 58, no. 1, pp. 267–288, 1996.</p>
<p>[15] M. Tipping, “Sparse bayesian learning and the relevance vector machine,” J. Machine Learning Research, vol. 1, pp. 211–244, 2001.</p>
<p>[16] M. Tipping and A. Faul, “Fast marginal likelihood maximisation for sparse Bayesian models,” Ninth Int. Workshop. Artiﬁcial Intelligence and Statistics, Jan. 2003.</p>
<p>[17] J. Tropp, “Algorithms for simultaneous sparse approximation. Part II: Convex relaxation,” Signal Processing, vol. 86, pp. 589–602, April 2006.</p>
<p>[18] M. Wakin, M. Duarte, S. Sarvotham, D. Baron, and R. Baraniuk, “Recovery of jointly sparse signals from a few random projections,” Advances in Neural Information Processing Systems 18, pp. 1433–1440, 2006.</p>
<p>[19] D. Wipf and S. Nagarajan, “A new view of automatic relevance determination,” Advances in Neural Information Processing Systems 20, pp. 1625–1632, 2008.</p>
<p>[20] D. Wipf and S. Nagarajan, “Iterative reweighted 1 and 2 methods for ﬁnding sparse solutions,” Submitted, 2009.</p>
<p>[21] D. Wipf and S. Nagarajan, “Latent variable Bayesian models for promoting sparsity,” Submitted, 2009.</p>
<p>[22] D. Wipf, J. Owen, H. Attias, K. Sekihara, and S. Nagarajan, “Robust Bayesian Estimation of the Location, Orientation, and Time Course of Multiple Correlated Neural Sources using MEG,” NeuroImage, vol. 49, no. 1, pp. 641–655, Jan. 2010.</p>
<p>[23] M. Yuan and Y. Lin, “Model selection and estimation in regression with grouped variables,” J. R. Statist. Soc. B, vol. 68, pp. 49–67, 2006.</p>
<p>[24] W. Zangwill, Nonlinear Programming: A Uniﬁed Approach, Prentice Hall, New Jersey, 1969.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
