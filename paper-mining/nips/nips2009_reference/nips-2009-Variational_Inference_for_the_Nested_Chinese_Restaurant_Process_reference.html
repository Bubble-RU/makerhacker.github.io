<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>255 nips-2009-Variational Inference for the Nested Chinese Restaurant Process</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-255" href="../nips2009/nips-2009-Variational_Inference_for_the_Nested_Chinese_Restaurant_Process.html">nips2009-255</a> <a title="nips-2009-255-reference" href="#">nips2009-255-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>255 nips-2009-Variational Inference for the Nested Chinese Restaurant Process</h1>
<br/><p>Source: <a title="nips-2009-255-pdf" href="http://papers.nips.cc/paper/3662-variational-inference-for-the-nested-chinese-restaurant-process.pdf">pdf</a></p><p>Author: Chong Wang, David M. Blei</p><p>Abstract: The nested Chinese restaurant process (nCRP) is a powerful nonparametric Bayesian model for learning tree-based hierarchies from data. Since its posterior distribution is intractable, current inference methods have all relied on MCMC sampling. In this paper, we develop an alternative inference technique based on variational methods. To employ variational methods, we derive a tree-based stick-breaking construction of the nCRP mixture model, and a novel variational algorithm that efﬁciently explores a posterior over a large set of combinatorial structures. We demonstrate the use of this approach for text and hand written digits modeling, where we show we can adapt the nCRP to continuous data as well. 1</p><br/>
<h2>reference text</h2><p>[1] Blei, D. M., T. L. Grifﬁths, M. I. Jordan, et al. Hierarchical topic models and the nested Chinese restaurant process. In NIPS. 2003.</p>
<p>[2] Bart, E., I. Porteous, P. Perona, et al. Unsupervised learning of visual taxonomies. In CVPR. 2008.</p>
<p>[3] Sivic, J., B. C. Russell, A. Zisserman, et al. Unsupervised discovery of visual object class hierarchies. In CVPR. 2008.</p>
<p>[4] Aldous, D. Exchangeability and related topics. In Ecole d’Ete de Probabilities de Saint-Flour XIII 1983, pages 1–198. Springer, 1985.</p>
<p>[5] Ferguson, T. S. A Bayesian analysis of some nonparametric problems. The Annals of Statistics, 1(2):209– 230, 1973.</p>
<p>[6] Neal, R. Probabilistic inference using Markov chain Monte Carlo methods. Tech. Rep. CRG-TR-93-1, Department of Computer Science, University of Toronto, 1993.</p>
<p>[7] Robert, C., G. Casella. Monte Carlo Statistical Methods. Springer-Verlag, New York, NY, 2004.</p>
<p>[8] Jordan, M. I., Z. Ghahramani, T. S. Jaakkola, et al. An introduction to variational methods for graphical models. Learning in Graphical Models, 1999.</p>
<p>[9] Blei, D. M., M. I. Jordan. Variational methods for the Dirichlet process. In ICML. 2004.</p>
<p>[10] Kurihara, K., M. Welling, N. A. Vlassis. Accelerated variational Dirichlet process mixtures. In NIPS. 2006.</p>
<p>[11] Kurihara, K., M. Welling, Y. W. Teh. Collapsed variational Dirichlet process mixture models. In IJCAI. 2007.</p>
<p>[12] Teh, Y. W., K. Kurihara, M. Welling. Collapsed variational inference for HDP. In NIPS. 2008.</p>
<p>[13] Sudderth, E. B., M. I. Jordan. Shared segmentation of natural scenes using dependent Pitman-Yor processes. In NIPS. 2008.</p>
<p>[14] Doshi, F., K. T. Miller, J. Van Gael, et al. Variational inference for the Indian buffet process. In AISTATS, vol. 12. 2009.</p>
<p>[15] Escobar, M. D., M. West. Bayesian density estimation and inference using mixtures. Journal of the American Statistical Association, 90:577–588, 1995.</p>
<p>[16] Tipping, M. E., C. M. Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society, Series B, 61:611–622, 1999.</p>
<p>[17] Bishop, C. M. Variational principal components. In ICANN. 1999.</p>
<p>[18] Collins, M., S. Dasgupta, R. E. Schapire. A generalization of principal components analysis to the exponential family. In NIPS. 2001.</p>
<p>[19] Mohamed, S., K. A. Heller, Z. Ghahramani. Bayesian exponential family PCA. In NIPS. 2008.</p>
<p>[20] Bach, F. R., M. I. Jordan. Beyond independent components: Trees and clusters. JMLR, 4:1205–1233, 2003.</p>
<p>[21] Antoniak, C. E. Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems. The Annals of Statistics, 2(6):1152–1174, 1974.</p>
<p>[22] Sethuraman, J. A constructive deﬁnition of Dirichlet priors. Statistica Sinica, 4:639–650, 1994.</p>
<p>[23] Wainwright, M., M. Jordan. Variational inference in graphical models: The view from the marginal polytope. In Allerton Conference on Control, Communication and Computation. 2003.</p>
<p>[24] Ueda, N., R. Nakano, Z. Ghahramani, et al. SMEM algorithm for mixture models. Neural Computation, 12(9):2109–2128, 2000.</p>
<p>[25] Grifﬁths, T. L., M. Steyvers. Finding scientiﬁc topics. Proc Natl Acad Sci USA, 101 Suppl 1:5228–5235, 2004.</p>
<p>[26] Tipping, M. E., C. M. Bishop. Mixtures of probabilistic principal component analysers. Neural Computation, 11(2):443–482, 1999.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
