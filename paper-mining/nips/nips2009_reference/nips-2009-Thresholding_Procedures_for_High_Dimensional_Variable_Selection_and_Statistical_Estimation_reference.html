<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>245 nips-2009-Thresholding Procedures for High Dimensional Variable Selection and Statistical Estimation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-245" href="../nips2009/nips-2009-Thresholding_Procedures_for_High_Dimensional_Variable_Selection_and_Statistical_Estimation.html">nips2009-245</a> <a title="nips-2009-245-reference" href="#">nips2009-245-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>245 nips-2009-Thresholding Procedures for High Dimensional Variable Selection and Statistical Estimation</h1>
<br/><p>Source: <a title="nips-2009-245-pdf" href="http://papers.nips.cc/paper/3697-thresholding-procedures-for-high-dimensional-variable-selection-and-statistical-estimation.pdf">pdf</a></p><p>Author: Shuheng Zhou</p><p>Abstract: Given n noisy samples with p dimensions, where n ≪ p, we show that the multistep thresholding procedure can accurately estimate a sparse vector β ∈ Rp in a linear model, under the restricted eigenvalue conditions (Bickel-Ritov-Tsybakov 09). Thus our conditions for model selection consistency are considerably weaker than what has been achieved in previous works. More importantly, this method allows very signiﬁcant values of s, which is the number of non-zero elements in the true parameter. For example, it works for cases where the ordinary Lasso would have failed. Finally, we show that if X obeys a uniform uncertainty principle and if the true parameter is sufﬁciently sparse, the Gauss-Dantzig selector (Cand` se Tao 07) achieves the ℓ2 loss within a logarithmic factor of the ideal mean square error one would achieve with an oracle which would supply perfect information about which coordinates are non-zero and which are above the noise level, while selecting a sufﬁciently sparse model. 1</p><br/>
<h2>reference text</h2><p>[1] R. G. Baraniuk, M. Davenport, R. A. DeVore, and M. B. Wakin. A simple proof of the restricted isometry property for random matrices. Constructive Approximation, 28(3):253–263, 2008.</p>
<p>[2] P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. The Annals of Statistics, 37(4):1705–1732, 2009.</p>
<p>[3] E. Cand` s, J. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate measurements. e Communications in Pure and Applied Mathematics, 59(8):1207–1223, August 2006.</p>
<p>[4] E. Cand` s and T. Tao. Decoding by Linear Programming. IEEE Trans. Info. Theory, 51:4203–4215, 2005. e</p>
<p>[5] E. Cand` s and T. Tao. The Dantzig selector: statistical estimation when p is much larger than n. Annals of e Statistics, 35(6):2313–2351, 2007.</p>
<p>[6] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientiﬁc and Statistical Computing, 20:33–61, 1998.</p>
<p>[7] D. L. Donoho and I. M. Johnstone. Ideal spatial adaptation by wavelet shrinkage. Biometrika, 81:425–455, 1994.</p>
<p>[8] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32(2):407– 499, 2004.</p>
<p>[9] E. Greenshtein and Y. Ritov. Persistency in high dimensional linear predictor-selection and the virtue of over-parametrization. Bernoulli, 10:971–988, 2004.</p>
<p>[10] V. Koltchinskii. Dantzig selector and sparsity oracle inequalities. Bernoulli, 15(3):799–828, 2009.</p>
<p>[11] N. Meinshausen and P. B¨ hlmann. High dimensional graphs and variable selection with the Lasso. Annals u of Statistics, 34(3):1436–1462, 2006.</p>
<p>[12] N. Meinshausen and B. Yu. Lasso-type recovery of sparse representations for high-dimensional data. Annals of Statistics, 37(1):246–270, 2009.</p>
<p>[13] S. Mendelson, A. Pajor, and N. Tomczak-Jaegermann. Uniform uncertainty principle for bernoulli and subgaussian ensembles. Constructive Approximation, 28(3):277–289, 2008.</p>
<p>[14] D. Needell and J. A. Tropp. CoSaMP: Iterative signal recovery from incomplete and inaccurate samples. Applied and Computational Harmonic Analysis, 26(3):301–321, 2008.</p>
<p>[15] D. Needell and R. Vershynin. Signal recovery from incomplete and inaccurate measurements via regularized orthogonal matching pursuit. IEEE Journal of Selected Topics in Signal Processing, to appear, 2009.</p>
<p>[16] R. Tibshirani. Regression shrinkage and selection via the Lasso. J. Roy. Statist. Soc. Ser. B, 58(1):267–288, 1996.</p>
<p>[17] S. A. van de Geer. The deterministic Lasso. The JSM Proceedings, American Statistical Association, 2007.</p>
<p>[18] S. A. van de Geer. High-dimensional generalized linear models and the Lasso. The Annals of Statistics, 36:614–645, 2008.</p>
<p>[19] M. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using ℓ1 -constrained quadratic programming. IEEE Trans. Inform. Theory, 2008. to appear, also posted as Technical Report 709, 2006, Department of Statistics, UC Berkeley.</p>
<p>[20] L. Wasserman and K. Roeder. High dimensional variable selection. The Annals of Statistics, 37(5A):2178– 2201, 2009.</p>
<p>[21] P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning Research, 7:2541–2567, 2006.</p>
<p>[22] S. Zhou, S. van de Geer, and P. B¨ hlmann. Adaptive Lasso for high dimensional regression and gaussian u graphical modeling, 2009. arXiv:0903.2515.</p>
<p>[23] H. Zou. The adaptive Lasso and its oracle properties. Journal of the American Statistical Association, 101:1418–1429, 2006.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
