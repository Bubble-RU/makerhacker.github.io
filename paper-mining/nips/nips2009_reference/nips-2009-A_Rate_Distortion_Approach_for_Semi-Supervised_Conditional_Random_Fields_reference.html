<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>15 nips-2009-A Rate Distortion Approach for Semi-Supervised Conditional Random Fields</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-15" href="../nips2009/nips-2009-A_Rate_Distortion_Approach_for_Semi-Supervised_Conditional_Random_Fields.html">nips2009-15</a> <a title="nips-2009-15-reference" href="#">nips2009-15-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>15 nips-2009-A Rate Distortion Approach for Semi-Supervised Conditional Random Fields</h1>
<br/><p>Source: <a title="nips-2009-15-pdf" href="http://papers.nips.cc/paper/3754-a-rate-distortion-approach-for-semi-supervised-conditional-random-fields.pdf">pdf</a></p><p>Author: Yang Wang, Gholamreza Haffari, Shaojun Wang, Greg Mori</p><p>Abstract: We propose a novel information theoretic approach for semi-supervised learning of conditional random ﬁelds that deﬁnes a training objective to combine the conditional likelihood on labeled data and the mutual information on unlabeled data. In contrast to previous minimum conditional entropy semi-supervised discriminative learning methods, our approach is grounded on a more solid foundation, the rate distortion theory in information theory. We analyze the tractability of the framework for structured prediction and present a convergent variational training algorithm to defy the combinatorial explosion of terms in the sum over label conﬁgurations. Our experimental results show the rate distortion approach outperforms standard l2 regularization, minimum conditional entropy regularization as well as maximum conditional entropy regularization on both multi-class classiﬁcation and sequence labeling problems. 1</p><br/>
<h2>reference text</h2><p>[1] S. Abney. Semi-Supervised Learning for Computational Linguistics. Chapman & Hall/CRC, 2007.</p>
<p>[2] Y. Altun, D. McAllester and M. Belkin. Maximum margin semi-supervised learning for structured variables. NIPS 18:33-40, 2005.</p>
<p>[3] S. Arimoto. An algorithm for computing the capacity of arbitrary discrete memoryless channels. IEEE Transactions on Information Theory, 18:1814-1820, 1972.</p>
<p>[4] L. Bahl, P. Brown, P. de Souza and R. Mercer. Maximum mutual information estimation of hidden Markov model parameters for speech recognition. ICASSP, 11:49-52, 1986.</p>
<p>[5] T. Berger and J. Gibson. Lossy source coding. IEEE Transactions on Information Theory, 44(6):26932723, 1998.</p>
<p>[6] R. Blahut. Computation of channel capacity and rate-distortion functions. IEEE Transactions on Information Theory, 18:460-473, 1972.</p>
<p>[7] R. Blahut. Principles and Practice of Information Theory, Addison-Wesley, 1987.</p>
<p>[8] S. Boyd and L. Vandenberghe. Convex Optimization, Cambridge University Press, 2004.</p>
<p>[9] U. Brefeld and T. Scheffer. Semi-supervised learning for structured output variables. ICML, 145-152, 2006.</p>
<p>[10] O. Chapelle, B. Scholk¨ pf and A. Zien. Semi-Supervised Learning, MIT Press, 2006. o</p>
<p>[11] A. Corduneanu and T. Jaakkola. On information regularization. UAI, 151-158, 2003.</p>
<p>[12] A. Corduneanu and T. Jaakkola. Distributed information regularization on graphs. NIPS, 17:297-304, 2004.</p>
<p>[13] A. Corduneanu and T. Jaakkola. Data dependent regularization. In Semi-Supervised Learning, O. Chapelle, B. Scholk¨ pf and A. Zien, (Editors), 163-182, MIT Press, 2006. o</p>
<p>[14] T. Cover and J. Thomas. Elements of Information Theory, Wiley, 1991.</p>
<p>[15] Y. Grandvalet and Y. Bengio. Semi-supervised learning by entropy minimization. NIPS, 17:529-536, 2004.</p>
<p>[16] F. Jiao, S. Wang, C. Lee, R. Greiner and D. Schuurmans. Semi-supervised conditional random ﬁelds for improved sequence segmentation and labeling. COLING/ACL, 209-216, 2006.</p>
<p>[17] M. Jordan, Z. Ghahramani, T. Jaakkola and L. Saul. Introduction to variational methods for graphical models. Machine Learning, 37:183-233, 1999.</p>
<p>[18] D. Jurafsky and J. Martin. Speech and Language Processing, 2nd Edition, Prentice Hall, 2008.</p>
<p>[19] J. Lafferty, A. McCallum and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. ICML, 282-289, 2001.</p>
<p>[20] C. Lee, S. Wang, F. Jiao, D. Schuurmans and R. Greiner. Learning to model spatial dependency: Semisupervised discriminative random ﬁelds. NIPS, 19:793-800, 2006.</p>
<p>[21] G. Mann and A. McCallum. Efﬁcient computation of entropy gradient for semi-supervised conditional random ﬁelds. NAACL/HLT, 109-112, 2007.</p>
<p>[22] G. Mann and A. McCallum. Generalized expectation criteria for semi-supervised learning of conditional random ﬁelds. ACL, 870-878, 2008.</p>
<p>[23] Y. Normandin. Maximum mutual information estimation of hidden Markov models. In Automatic Speech and Speaker Recognition: Advanced Topics, C. Lee, F. Soong and K. Paliwal (Editors), 57-81, Springer, 1996.</p>
<p>[24] N. Oliver and A. Garg. MMIHMM: maximum mutual information hidden Markov models. ICML, 466473, 2002.</p>
<p>[25] K. Rose. Deterministic annealing for clustering, compression, classiﬁcation, regression, and related optimization problems. Proceedings of the IEEE, 80:2210-2239, 1998.</p>
<p>[26] N. Slonim, G. Atwal, G. Tkacik and W. Bialek. Information based clustering. Proceedings of National Academy of Science (PNAS), 102:18297-18302, 2005.</p>
<p>[27] S. Still and W. Bialek. How many clusters? An information theoretic perspective. Neural Computation, 16:2483-2506, 2004.</p>
<p>[28] M. Szummer and T. Jaakkola. Information regularization with partially labeled data. NIPS, 1025-1032, 2002.</p>
<p>[29] B. Taskar, C. Guestrain and D. Koller. Max-margin Markov networks. NIPS, 16:25-32, 2003.</p>
<p>[30] N. Tishby, F. Pereira, and W. Bialek. The information bottleneck method. The 37th Annual Allerton Conference on Communication, Control, and Computing, 368-377, 1999.</p>
<p>[31] L. Zheng, S. Wang, Y. Liu and C. Lee. Information theoretic regularization for semi-supervised boosting. KDD, 1017-1026, 2009.</p>
<p>[32] X. Zhu. Semi-supervised learning literature survey. Computer Sciences TR 1530, University of Wisconsin Madison, 2007.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
