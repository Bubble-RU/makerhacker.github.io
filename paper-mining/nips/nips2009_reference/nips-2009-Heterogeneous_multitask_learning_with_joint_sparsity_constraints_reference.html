<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>108 nips-2009-Heterogeneous multitask learning with joint sparsity constraints</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-108" href="../nips2009/nips-2009-Heterogeneous_multitask_learning_with_joint_sparsity_constraints.html">nips2009-108</a> <a title="nips-2009-108-reference" href="#">nips2009-108-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>108 nips-2009-Heterogeneous multitask learning with joint sparsity constraints</h1>
<br/><p>Source: <a title="nips-2009-108-pdf" href="http://papers.nips.cc/paper/3706-heterogeneous-multitask-learning-with-joint-sparsity-constraints.pdf">pdf</a></p><p>Author: Xiaolin Yang, Seyoung Kim, Eric P. Xing</p><p>Abstract: Multitask learning addresses the problem of learning related tasks that presumably share some commonalities on their input-output mapping functions. Previous approaches to multitask learning usually deal with homogeneous tasks, such as purely regression tasks, or entirely classiﬁcation tasks. In this paper, we consider the problem of learning multiple related tasks of predicting both continuous and discrete outputs from a common set of input variables that lie in a highdimensional feature space. All of the tasks are related in the sense that they share the same set of relevant input variables, but the amount of inﬂuence of each input on different outputs may vary. We formulate this problem as a combination of linear regressions and logistic regressions, and model the joint sparsity as L1 /L∞ or L1 /L2 norm of the model parameters. Among several possible applications, our approach addresses an important open problem in genetic association mapping, where the goal is to discover genetic markers that inﬂuence multiple correlated traits jointly. In our experiments, we demonstrate our method in this setting, using simulated and clinical asthma datasets, and we show that our method can effectively recover the relevant inputs with respect to all of the tasks. 1</p><br/>
<h2>reference text</h2><p>[1] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning, 73(3):243–272, 2008.</p>
<p>[2] B. Bakker and T. Heskes. Task clustering and gating for bayesian multitask learning. Journal of Machine Learning Research, 4:83–99, 2003.</p>
<p>[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.</p>
<p>[4] R. Caruana. Multitask learning. Machine Learning, 28:41–75, 1997.</p>
<p>[5] V. Emilsson, G. Thorleifsson, B. Zhang, A.S. Leonardson, F. Zink, J. Zhu, S. Carlson, A. Helgason, G.B. Walters, S. Gunnarsdottir, et al. Variations in dna elucidate molecular networks that cause disease. Nature, 452(27):423–28, 2008.</p>
<p>[6] J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear models via coordinate descent. Technical Report 703, Department of Statistics, Stanford University, 2009.</p>
<p>[7] S. Kim and E. P. Xing. Statistical estimation of correlated genome associations to a quantitative trait network. PLoS Genetics, 5(8):e1000587, 2009.</p>
<p>[8] K. Koh, S. Kim, and S. Boyd. An interior-point method for large-scale l1-regularized logistic regression. Journal of Machine Learning Research, 8(8):1519–1555, 2007.</p>
<p>[9] G. Obozinski, B. Taskar, and M. Jordan. Joint covariate selection for grouped classiﬁcation. Technical Report 743, Department of Statistics, University of California, Berkeley, 2007.</p>
<p>[10] G. Obozinski, M.J. Wainwright, and M.J. Jordan. High-dimensional union support recovery in multivariate regression. In Advances in Neural Information Processing Systems 21, 2008.</p>
<p>[11] M. Schmidt, G. Fung, and R. Rosales. Fast optimization methods for l1 regularization: a comparative study and two new approaches. In Proceedings of the European Conference on Machine Learning, 2007.</p>
<p>[12] The International HapMap Consortium. A haplotype map of the human genome. Nature, 437:1399–1320, 2005.</p>
<p>[13] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of Royal Statistical Society, Series B, 58(1):267–288, 1996.</p>
<p>[14] K. Yu, V. Tresp, and A. Schwaighofer. Learning gaussian processes from multiple tasks. In Proceedings of the 22nd International Conference on Machine Learning, 2005.</p>
<p>[15] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of Royal Statistical Society, Series B, 68(1):49–67, 2006.</p>
<p>[16] J. Zhang, Z. Ghahramani, and Y. Yang. Flexible latent variable models for multi-task learning. Machine Learning, 73(3):221–242, 2008.</p>
<p>[17] P. Zhao, G. Rocha, and B. Yu. Grouped and hierarchical model selection through composite absolute penalties. Technical Report 703, Department of Statistics, University of California, Berkeley, 2008.</p>
<p>[18] J. Zhu, B. Zhang, E.N. Smith, B. Drees, R.B. Brem, L. Kruglyak, R.E. Bumgarner, and E.E. Schadt. Integrating large-scale functional genomic data to dissect the complexity of yeast regulatory networks. Nature Genetics, 40:854–61, 2008.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
