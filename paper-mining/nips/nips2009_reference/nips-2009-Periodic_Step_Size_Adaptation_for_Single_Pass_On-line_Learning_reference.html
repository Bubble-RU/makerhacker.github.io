<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>189 nips-2009-Periodic Step Size Adaptation for Single Pass On-line Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-189" href="../nips2009/nips-2009-Periodic_Step_Size_Adaptation_for_Single_Pass_On-line_Learning.html">nips2009-189</a> <a title="nips-2009-189-reference" href="#">nips2009-189-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>189 nips-2009-Periodic Step Size Adaptation for Single Pass On-line Learning</h1>
<br/><p>Source: <a title="nips-2009-189-pdf" href="http://papers.nips.cc/paper/3702-periodic-step-size-adaptation-for-single-pass-on-line-learning.pdf">pdf</a></p><p>Author: Chun-nan Hsu, Yu-ming Chang, Hanshen Huang, Yuh-jye Lee</p><p>Abstract: It has been established that the second-order stochastic gradient descent (2SGD) method can potentially achieve generalization performance as well as empirical optimum in a single pass (i.e., epoch) through the training examples. However, 2SGD requires computing the inverse of the Hessian matrix of the loss function, which is prohibitively expensive. This paper presents Periodic Step-size Adaptation (PSA), which approximates the Jacobian matrix of the mapping function and explores a linear relation between the Jacobian and Hessian to approximate the Hessian periodically and achieve near-optimal results in experiments on a wide variety of models and tasks. 1</p><br/>
<h2>reference text</h2><p>[1] S.V.N. Vishwanathan, Nicol N. Schraudolph, Mark W. Schmidt, and Kevin P. Murphy. Accelerated training of conditional random ďŹ elds with stochastic gradient methods. In Proceedings of the 23rd International Conference on Machine Learning (ICMLâ&euro;&trade;06), Pittsburgh, PA, USA, June 2006. 8</p>
<p>[2] Michael Collins, Amir Globerson, Terry Koo, Xavier Carreras, and Peter L. Bartlett. Exponentiated gradient algorithms for conditional random ďŹ elds and max-margin markov networks. Journal of Machine Learning Research, 9:1775â&euro;&ldquo;1822, August 2008.</p>
<p>[3] Noboru Murata and Shun-Ichi Amari. Statistical analysis of learning dynamics. Signal Processing, 74(1):3â&euro;&ldquo;28, April 1999.</p>
<p>[4] LÂ´ on Bottou and Yann LeCun. On-line learning for very large data sets. Applied Stochastic e Models in Business and Industry, 21(2):137â&euro;&ldquo;151, 2005.</p>
<p>[5] Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, 1999.</p>
<p>[6] LÂ´ on Bottou. The tradeoffs of large-scale learning. Tutorial, the 21st Annual Conference e on Neural Information Processing Systems (NIPS 2007), Vancouver, BC, Canada, December 2007. http://leon.bottou.org/talks/largescale.</p>
<p>[7] Albert Benveniste, Michel Metivier, and Pierre Priouret. Adaptive Algorithms and Stochastic Approximations. Springer-Verlag, 1990.</p>
<p>[8] Chun-Nan Hsu, Han-Shen Huang, and Bo-Hou Yang. Global and componentwise extrapolation for accelerating data mining from large incomplete data sets with the EM algorithm. In Proceedings of the Sixth IEEE International Conference on Data Mining (ICDMâ&euro;&trade;06), pages 265â&euro;&ldquo;274, Hong Kong, China, December 2006.</p>
<p>[9] Han-Shen Huang, Bo-Hou Yang, Yu-Ming Chang, and Chun-Nan Hsu. Global and componentwise extrapolations for accelerating training of Bayesian networks and conditional random ďŹ elds. Data Mining and Knowledge Discovery, 19(1):58â&euro;&ldquo;91, 2009.</p>
<p>[10] Fei Sha and Fernando Pereira. Shallow parsing with conditional random ďŹ elds. In Proceedings of Human Language Technology, the North American Chapter of the Association for Computational Linguistics (NAACLâ&euro;&trade;03), pages 213â&euro;&ldquo;220, 2003.</p>
<p>[11] Taku Kudo. CRF++: Yet another CRF toolkit, 2006. Available under LGPL from the following URL: http://crfpp.sourceforge.net/.</p>
<p>[12] Burr Settles. Biomedical named entity recognition using conditional random ďŹ elds and novel feature sets. In Proceedings of the Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA-2004), pages 104â&euro;&ldquo;107, 2004.</p>
<p>[13] Cheng-Ju Kuo, Yu-Ming Chang, Han-Shen Huang, Kuan-Ting Lin, Bo-Hou Yang, Yu-Shi Lin, Chun-Nan Hsu, and I-Fang Chung. Rich feature set, uniďŹ cation of bidirectional parsing and dictionary ďŹ ltering for high f-score gene mention tagging. In Proceedings of the Second BioCreative Challenge Evaluation Workshop, pages 105â&euro;&ldquo;107, 2007.</p>
<p>[14] Yann LeCun and Corinna Cortes. The MNIST database of handwritten digits, 1998. http://yann.lecun.com/exdb/mnist/.</p>
<p>[15] Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal Estimated subGrAdient SOlver for SVM. In ICMLâ&euro;&trade;07: Proceedings of the 24th international conference on Machine learning, pages 807â&euro;&ldquo;814, New York, NY, USA, 2007. ACM Press.</p>
<p>[16] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines, 2001. Software available at http://www.csie.ntu.edu.tw/â&circ;źcjlin/libsvm.</p>
<p>[17] Thorsten Joachims. Training linear SVMs in linear time. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDDâ&euro;&trade;06), pages 217â&euro;&ldquo;226, New York, NY, USA, 2006. ACM.</p>
<p>[18] Yann LeCun, LÂ´ on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning e applied to document recognition. Proceedings of the IEEE, 86(11):2278â&euro;&ldquo;2324, 1998.</p>
<p>[19] Yann LeCun, Leon Bottou, Genevieve B. Orr, and Klaus-Robert Muller. EfďŹ cient backprop. In G. Orr and Muller K., editors, Neural Networks: Tricks of the trade. Springer, 1998.</p>
<p>[20] Nicolas LeRoux, Pierre-Antoine Manzagol, and Yoshua Bengio. Topmoumoute online natural gradient algorithm. In Advances in Neural Information Processing Systems, 20 (NIPS 2007), Cambridge, MA, USA, 2008. MIT Press.</p>
<p>[21] Chun-Nan Hsu, Yu-Ming Chang, Han-Shen Huang, and Yuh-Jye Lee. Periodic step-size adaptation in second-order gradient descent for single-pass on-line structured learning. To appear in Mchine Learning, Special Issue on Structured Prediction. DOI: 10.1007/s10994-009-5142-6, 2009. 9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
