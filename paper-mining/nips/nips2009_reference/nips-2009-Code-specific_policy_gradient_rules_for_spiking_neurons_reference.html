<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>52 nips-2009-Code-specific policy gradient rules for spiking neurons</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-52" href="../nips2009/nips-2009-Code-specific_policy_gradient_rules_for_spiking_neurons.html">nips2009-52</a> <a title="nips-2009-52-reference" href="#">nips2009-52-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>52 nips-2009-Code-specific policy gradient rules for spiking neurons</h1>
<br/><p>Source: <a title="nips-2009-52-pdf" href="http://papers.nips.cc/paper/3733-code-specific-policy-gradient-rules-for-spiking-neurons.pdf">pdf</a></p><p>Author: Henning Sprekeler, Guillaume Hennequin, Wulfram Gerstner</p><p>Abstract: Although it is widely believed that reinforcement learning is a suitable tool for describing behavioral learning, the mechanisms by which it can be implemented in networks of spiking neurons are not fully understood. Here, we show that different learning rules emerge from a policy gradient approach depending on which features of the spike trains are assumed to inﬂuence the reward signals, i.e., depending on which neural code is in effect. We use the framework of Williams (1992) to derive learning rules for arbitrary neural codes. For illustration, we present policy-gradient rules for three different example codes - a spike count code, a spike timing code and the most general “full spike train” code - and test them on simple model problems. In addition to classical synaptic learning, we derive learning rules for intrinsic parameters that control the excitability of the neuron. The spike count learning rule has structural similarities with established Bienenstock-Cooper-Munro rules. If the distribution of the relevant spike train features belongs to the natural exponential family, the learning rules have a characteristic shape that raises interesting prediction problems. 1</p><br/>
<h2>reference text</h2><p>[1] Baxter, J. and Bartlett, P. (2001). Inﬁnite-horizon policy-gradient estimation. Journal of Artiﬁcial Intelligence Research, 15(4):319–350.</p>
<p>[2] Bienenstock, E., Cooper, L., and Munroe, P. (1982). Theory of the development of neuron selectivity: orientation speciﬁcity and binocular interaction in visual cortex. Journal of Neuroscience, 2:32–48. reprinted in Anderson and Rosenfeld, 1990.</p>
<p>[3] Florian, R. V. (2007). Reinforcement learning through modulation of spike-timing-dependent synaptic plasticity. Neural Computation, 19:1468–1502.</p>
<p>[4] Greensmith, E., Bartlett, P., and Baxter, J. (2004). Variance reduction techniques for gradient estimates in reinforcement learning. The Journal of Machine Learning Research, 5:1471–1530.</p>
<p>[5] Pﬁster, J.-P., Toyoizumi, T., Barber, D., and Gerstner, W. (2006). Optimal spike-timing dependent plasticity for precise action potential ﬁring in supervised learning. Neural Computation, 18:1309–1339.</p>
<p>[6] Rao, R. P. and Ballard, D. H. (1999). Predictive coding in the visual cortex: A functional interpretation of some extra-classical receptive-ﬁeld effects. Nature Neuroscience, 2(1):79–87.</p>
<p>[7] Schultz, W., Dayan, P., and Montague, R. (1997). A neural substrate for prediction and reward. Science, 275:1593–1599.</p>
<p>[8] Schwartz, G., Harris, R., Shrom, D., and II, M. (2007). Detection and prediction of periodic patterns by the retina. Nature Neuroscience, 10:552–554.</p>
<p>[9] Sutton, R. and Barto, A. (1998). Reinforcement learning. MIT Press, Cambridge.</p>
<p>[10] Triesch, J. (2007). Synergies between intrinsic and synaptic plasticity mechanisms. Neural computation, 19:885 –909.  8</p>
<p>[11] Urbanczik, R. and Senn, W. (2009). Reinforcement learning in populations of spiking neurons. Nat Neurosci, 12(3):250–252.</p>
<p>[12] Williams, R. (1992). Simple statistical gradient-following methods for connectionist reinforcement learning. Machine Learning, 8:229–256.</p>
<p>[13] Xie, X. and Seung, H. (2004). Learning in neural networks by reinforcement of irregular spiking. Physical Review E, 69(4):41909.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
