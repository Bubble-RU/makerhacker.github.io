<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>127 nips-2009-Learning Label Embeddings for Nearest-Neighbor Multi-class Classification with an Application to Speech Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-127" href="../nips2009/nips-2009-Learning_Label_Embeddings_for_Nearest-Neighbor_Multi-class_Classification_with_an_Application_to_Speech_Recognition.html">nips2009-127</a> <a title="nips-2009-127-reference" href="#">nips2009-127-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>127 nips-2009-Learning Label Embeddings for Nearest-Neighbor Multi-class Classification with an Application to Speech Recognition</h1>
<br/><p>Source: <a title="nips-2009-127-pdf" href="http://papers.nips.cc/paper/3845-learning-label-embeddings-for-nearest-neighbor-multi-class-classification-with-an-application-to-speech-recognition.pdf">pdf</a></p><p>Author: Natasha Singh-miller, Michael Collins</p><p>Abstract: We consider the problem of using nearest neighbor methods to provide a conditional probability estimate, P (y|a), when the number of labels y is large and the labels share some underlying structure. We propose a method for learning label embeddings (similar to error-correcting output codes (ECOCs)) to model the similarity between labels within a nearest neighbor framework. The learned ECOCs and nearest neighbor information are used to provide conditional probability estimates. We apply these estimates to the problem of acoustic modeling for speech recognition. We demonstrate signiﬁcant improvements in terms of word error rate (WER) on a lecture recognition task over a state-of-the-art baseline GMM model. 1</p><br/>
<h2>reference text</h2><p>[1] E. L. Allwein, R. E. Schapire, and Y. Singer. Reducing multiclass to binary: a unifying approach for margin classiﬁers. Journal of Machine Learning Research, 1:113–141, 2000.</p>
<p>[2] K. Crammer and Y. Singer. Improved output coding for classiﬁcation using continuous relaxation. In Advances in Neural Information Processing Systems. MIT Press, 2000.</p>
<p>[3] K. Crammer and Y. Singer. On the learnability and design of output codes for multiclass problems. Machine Learning, 47(2-3):201–233, 2002.</p>
<p>[4] T. G. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting output codes. Journal of Artiﬁcial Intelligence Research, 2:263–286, 1995.</p>
<p>[5] J. Glass. A probabilistic framework for segment-based speech recognition. Computer, Speech, and Language, 17(2-3):137–152, 2003. 8</p>
<p>[6] J. Glass, T. J. Hazen, L. Hetherington, and C. Wang. Analysis and processing of lecture audio data: Preliminary investigations. In HLT-NAACL 2004 Workshop on Interdisciplinary Approaches to Speech Indexing and Retrieval, pages 9–12, 2004.</p>
<p>[7] A. Globerson and S. Roweis. Metric learning by collapsing classes. In Y. Weiss, B. Scholkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 513–520. MIT Press, 2006.</p>
<p>[8] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 513–520. MIT Press, 2005.</p>
<p>[9] A. Klautau, N. Jevtic, and A. Orlitsky. On nearest-neighbor error-correcting output codes with aplication to all-pairs multiclass support vector machines. Journal of Machine Learning Research, 4:1–15, 2003.</p>
<p>[10] W. H. Press, S. A. Teukolsky, W. T. Vetterline, and B. P. Flannery. Numerical recipes: the art of scientiﬁc computing. Cambridge University Press, 3 edition, 2007.</p>
<p>[11] O. Pujol, P. Radeva, and J. Vitria. Discriminant ecoc: a heuristic method for application dependent design of error correcting output codes. IEEE Transactions of Pattern Analysis and Machine Intelligence, 28(6), 2006.</p>
<p>[12] R. Salakhutdinov and G. Hinton. Learning a nonlinear embedding by preserving class neighbourhood structure. AI and Statistics, 2007.</p>
<p>[13] N. Singh-Miller, M. Collins, and T. J. Hazen. Dimensionality reduction for speech recognition using neighborhood components analysis. In Interspeech, 2007.</p>
<p>[14] A. Torralba, R. Fergus, and Y. Weiss. Small codes and large image databases for recognition. IEEE Computer Vision and Pattern Recognition, June 2008.</p>
<p>[15] K. Q. Weinberger, J. Blitzer, and L. K. Saul. Distance metric learning for large margin nearest neighbor classiﬁcation. In Advances in Neural Information Processing Systems. MIT Press, 2006.</p>
<p>[16] G. Zavaliagkos, Y. Zhao, R. Schwartz, and J. Makhoul. A hybrid segmental neural net/hidden markov model system for continuous speech recognition. IEEE Transactions on Speech and Audio Processing, 2(1):151–160, 1994.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
