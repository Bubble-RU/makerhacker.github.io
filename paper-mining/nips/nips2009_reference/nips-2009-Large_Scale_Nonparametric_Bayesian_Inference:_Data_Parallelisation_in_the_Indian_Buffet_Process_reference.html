<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>123 nips-2009-Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-123" href="../nips2009/nips-2009-Large_Scale_Nonparametric_Bayesian_Inference%3A_Data_Parallelisation_in_the_Indian_Buffet_Process.html">nips2009-123</a> <a title="nips-2009-123-reference" href="#">nips2009-123-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>123 nips-2009-Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process</h1>
<br/><p>Source: <a title="nips-2009-123-pdf" href="http://papers.nips.cc/paper/3669-large-scale-nonparametric-bayesian-inference-data-parallelisation-in-the-indian-buffet-process.pdf">pdf</a></p><p>Author: Finale Doshi-velez, Shakir Mohamed, Zoubin Ghahramani, David A. Knowles</p><p>Abstract: Nonparametric Bayesian models provide a framework for ﬂexible probabilistic modelling of complex datasets. Unfortunately, the high-dimensional averages required for Bayesian methods can be slow, especially with the unbounded representations used by nonparametric models. We address the challenge of scaling Bayesian inference to the increasingly large datasets found in real-world applications. We focus on parallelisation of inference in the Indian Buffet Process (IBP), which allows data points to have an unbounded number of sparse latent features. Our novel MCMC sampler divides a large data set between multiple processors and uses message passing to compute the global likelihoods and posteriors. This algorithm, the ﬁrst parallel inference scheme for IBP-based models, scales to datasets orders of magnitude larger than have previously been possible. 1</p><br/>
<h2>reference text</h2><p>[1] C. Chu, S. Kim, Y. Lin, Y. Yu, G. Bradski, A. Ng, and K. Olukotun, “Map-reduce for machine learning on multicore,” in Advances in Neural Information Processing Systems, p. 281, MIT Press, 2007.</p>
<p>[2] A. Asuncion, P. Smyth, and M. Welling, “Asynchronous distributed learning of topic models,” in Advances in Neural Information Processing Systems 21, 2008.</p>
<p>[3] T. Grifﬁths and Z. Ghahramani, “Inﬁnite latent feature models and the Indian buffet process,” in Advances in Neural Information Processing Systems, vol. 16, NIPS, 2006.</p>
<p>[4] F. Doshi-Velez and Z. Ghahramani, “Accelerated inference for the Indian buffet process,” in International Conference on Machine Learning, 2009.</p>
<p>[5] E. Meeds, Z. Ghahramani, R. Neal, and S. Roweis, “Modeling dyadic data with binary latent factors,” in Advances in Neural Information Processing Systems, vol. 19, pp. 977–984, 2007.</p>
<p>[6] Y. W. Teh, D. G¨ r¨ r, and Z. Ghahramani, “Stick-breaking construction for the Indian buffet ou process,” in Proceedings of the Intl. Conf. on Artiﬁcial Intelligence and Statistics, vol. 11, pp. 556–563, 2007.</p>
<p>[7] F. Wood and T. L. Grifﬁths, “Particle ﬁltering for nonparametric Bayesian matrix factorization,” in Advances in Neural Information Processing Systems, vol. 19, pp. 1513–1520, 2007.</p>
<p>[8] F. Doshi-Velez, K. T. Miller, J. Van Gael, and Y. W. Teh, “Variational inference for the Indian buffet process,” in Proceedings of the Intl. Conf. on Artiﬁcial Intelligence and Statistics, vol. 12, pp. 137–144, 2009.</p>
<p>[9] S. P. Brooks and G. O. Roberts, “Convergence assessment techniques for Markov Chain Monte Carlo,” Statistics and Computing, vol. 8, pp. 319–335, 1998.</p>
<p>[10] C. R. Robert and G. Casella, Monte Carlo Statistical Methods. Springer, second ed., 2004.</p>
<p>[11] A. M. Mart’inez and A. C. Kak, “PCA versus LDA,” IEEE Trans. Pattern Anal. Mach. Intelligence, vol. 23, pp. 228–233, 2001.</p>
<p>[12] G. E. Poliner and D. P. W. Ellis, “A discriminative model for polyphonic piano transcription,” EURASIP J. Appl. Signal Process., vol. 2007, no. 1, pp. 154–154, 2007.</p>
<p>[13] T. Kollar and N. Roy, “Utilizing object-object and object-scene context when planning to ﬁnd things.,” in International Conference on Robotics and Automation, 2009.</p>
<p>[14] C. G. Joseph Gonzalez, Yucheng Low, “Residual splash for optimally parallelizing belief propagation,” in Proceedings of the Twelfth International Conference on Artiﬁcial Intelligence and Statistics (D. van Dyk and M. Welling, eds.), vol. 5, pp. 177–184, JMLR, 2009.</p>
<p>[15] D. Stern, R. Herbrich, and T. Graepel, “Matchbox: Large scale online Bayesian recommendations,” in 18th International World Wide Web Conference (WWW2009), April 2009.</p>
<p>[16] R. Nallapati, W. Cohen, and J. Lafferty, “Parallelized variational EM for Latent Dirichlet Allocation: An experimental evaluation of speed and scalability,” in ICDMW ’07: Proceedings of the Seventh IEEE International Conference on Data Mining Workshops, (Washington, DC, USA), pp. 349–354, IEEE Computer Society, 2007.</p>
<p>[17] D. Newman, A. Asuncion, P. Smyth, and M. Welling, “Distributed inference for Latent Dirichlet Allocation,” in Advances in Neural Information Processing Systems 20 (J. Platt, D. Koller, Y. Singer, and S. Roweis, eds.), pp. 1081–1088, Cambridge, MA: MIT Press, 2008.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
