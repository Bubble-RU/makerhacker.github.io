<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>134 nips-2009-Learning to Explore and Exploit in POMDPs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-134" href="../nips2009/nips-2009-Learning_to_Explore_and_Exploit_in_POMDPs.html">nips2009-134</a> <a title="nips-2009-134-reference" href="#">nips2009-134-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>134 nips-2009-Learning to Explore and Exploit in POMDPs</h1>
<br/><p>Source: <a title="nips-2009-134-pdf" href="http://papers.nips.cc/paper/3814-learning-to-explore-and-exploit-in-pomdps.pdf">pdf</a></p><p>Author: Chenghui Cai, Xuejun Liao, Lawrence Carin</p><p>Abstract: A fundamental objective in reinforcement learning is the maintenance of a proper balance between exploration and exploitation. This problem becomes more challenging when the agent can only partially observe the states of its environment. In this paper we propose a dual-policy method for jointly learning the agent behavior and the balance between exploration exploitation, in partially observable environments. The method subsumes traditional exploration, in which the agent takes actions to gather information about the environment, and active learning, in which the agent queries an oracle for optimal actions (with an associated cost for employing the oracle). The form of the employed exploration is dictated by the speciﬁc problem. Theoretical guarantees are provided concerning the optimality of the balancing of exploration and exploitation. The effectiveness of the method is demonstrated by experimental results on benchmark problems.</p><br/>
<h2>reference text</h2><p>[1] M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, Gatsby Computational Neuroscience Unit, Univertisity College London, 2003.</p>
<p>[2] R. I. Brafman and M. Tennenholtz. R-max - a general polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning Research, 3(OCT):213–231, 2002.</p>
<p>[3] F. Doshi, J. Pineau, and N. Roy. Reinforcement learning with limited reinforcement: Using Bayes risk for active learning in POMDPs. In Proceedings of the 25th international conference on Machine learning, pages 256–263. ACM, 2008.</p>
<p>[4] M. Kearns and D. Koller. Efﬁcient reinforcement learning in factored mdps. In Proc. of the Sixteenth International Joint Conference of Artiﬁcial Intelligence, pages 740–747, 1999.</p>
<p>[5] M. Kearns and S. P. Singh. Near-optimal performance for reinforcement learning in polynomial time. In Proc. ICML, pages 260–268, 1998.</p>
<p>[6] H. Li, X. Liao, and L. Carin. Multi-task reinforcement learning in partially observable stochastic environments. Journal of Machine Learning Research, 10:1131–1186, 2009.</p>
<p>[7] M.L. Littman, A.R. Cassandra, and L.P. Kaelbling. Learning policies for partially observable environments: scaling up. In ICML, 1995.</p>
<p>[8] J. Pineau, G. Gordon, and S. Thrun. Point-based value iteration: An anytime algorithm for POMDPs. In Proceedings of the Sixteenth International Joint Conference on Artiﬁcial Intelligence (IJCAI), pages 1025 – 1032, August 2003.</p>
<p>[9] P. Poupart and N. Vlassis. Model-based bayesian reinforcement learning in partially observable domains. In International Symposiu on Artiﬁcial Intelligence and Mathmatics (ISAIM), 2008.</p>
<p>[10] R. Sutton and A. Barto. Reinforcement learning: An introduction. MIT Press, Cambridge, MA, 1998.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
