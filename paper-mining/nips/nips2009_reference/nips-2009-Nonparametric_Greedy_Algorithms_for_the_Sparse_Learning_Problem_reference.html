<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>173 nips-2009-Nonparametric Greedy Algorithms for the Sparse Learning Problem</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-173" href="../nips2009/nips-2009-Nonparametric_Greedy_Algorithms_for_the_Sparse_Learning_Problem.html">nips2009-173</a> <a title="nips-2009-173-reference" href="#">nips2009-173-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>173 nips-2009-Nonparametric Greedy Algorithms for the Sparse Learning Problem</h1>
<br/><p>Source: <a title="nips-2009-173-pdf" href="http://papers.nips.cc/paper/3889-nonparametric-greedy-algorithms-for-the-sparse-learning-problem.pdf">pdf</a></p><p>Author: Han Liu, Xi Chen</p><p>Abstract: This paper studies the forward greedy strategy in sparse nonparametric regression. For additive models, we propose an algorithm called additive forward regression; for general multivariate models, we propose an algorithm called generalized forward regression. Both algorithms simultaneously conduct estimation and variable selection in nonparametric settings for the high dimensional sparse learning problem. Our main emphasis is empirical: on both simulated and real data, these two simple greedy methods can clearly outperform several state-ofthe-art competitors, including LASSO, a nonparametric version of LASSO called the sparse additive model (SpAM) and a recently proposed adaptive parametric forward-backward algorithm called Foba. We also provide some theoretical justiﬁcations of speciﬁc versions of the additive forward regression. 1</p><br/>
<h2>reference text</h2><p>[1] Francis Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine Learning Research, 8:1179–1225, 2008.</p>
<p>[2] Francis Bach. Exploring large feature spaces with hierarchical multiple kernel learning. In Advances in Neural Information Processing Systems 21. MIT Press, 2008.</p>
<p>[3] Andrew R. Barron, Albert Cohen, Wolfgang Dahmen, and Ronald A. DeVore. Approximation and learning by greedy algorithms. The Annals of Statistics, 36:64–94, 2008.</p>
<p>[4] Peter B¨ hlmann and Bin Yu. Sparse boosting. Journal of Machine Learning Research, 7:1001– u 1024, 2006.</p>
<p>[5] Andreas Buja, Trevor Hastie, and Robert Tibshirani. Linear smoothers and additive models. The Annals of Statistics, 17:453–510, 1989.</p>
<p>[6] Emmanuel Candes and Terence Tao. The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35:2313–2351, 2007.</p>
<p>[7] Scott Shaobing Chen, David L. Donoho, and Michael A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientiﬁc and Statistical Computing, 20:33–61, 1998.</p>
<p>[8] Jianqing Fan and Ir` ne Gijbels. Local polynomial modelling and its applications. Chapman e and Hall, 1996.</p>
<p>[9] Jerome H. Friedman. Multivariate adaptive regression splines. The Annals of Statistics, 19:1– 67, 1991.</p>
<p>[10] Trevor Hastie and Robert Tibshirani. Generalized additive models. Chapman & Hall Ltd., 1999.</p>
<p>[11] John Lafferty and Larry Wasserman. Rodeo: Sparse, greedy nonparametric regression. The Annals of Statistics, 36(1):28–63, 2008.</p>
<p>[12] Yi Lin and Hao Helen Zhang. Component selection and smoothing in multivariate nonparametric regression. The Annals of Statistics., 34(5):2272–2297, 2006.</p>
<p>[13] Han Liu and Jian Zhang. On the estimation consistency of the group lasso and its applications. Proceedings of the Twelfth International Conference on Artiﬁcial Intelligence and Statistics, 2009.</p>
<p>[14] S. Mallat and Z. Zhang. Matching pursuit with time-frequency dictionaries. IEEE Transactions on Signal Processing, 41:3397–3415, 1993.</p>
<p>[15] Lukas Meier, Sara van de Geer, and Peter B¨ hlmann. High-dimensional additive modelling. u The Annals of Statistics (to appear), 2009.</p>
<p>[16] Pradeep Ravikumar, John Lafferty, Han Liu, and Larry Wasserman. Sparse additive models. Journal of the Royal Statistical Society, Series B, Methodological, 2009. To appear.</p>
<p>[17] Pradeep Ravikumar, Han Liu, John Lafferty, and Larry Wasserman. Spam: Sparse additive models. In Advances in Neural Information Processing Systems 20, 2007.</p>
<p>[18] B. W. Silverman. Spline smoothing: The equivalent variable kernel method. The Annals of Statistics, 12:898–916, 1984.</p>
<p>[19] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B, Methodological, 58:267–288, 1996.</p>
<p>[20] Joel A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Trans. Inform. Theory, 50(10):2231–2241, October 2004.</p>
<p>[21] Grace Wahba. Spline models for observational data. SIAM [Society for Industrial and Applied Mathematics], 1990.</p>
<p>[22] Tong Zhang. Adaptive forward-backward greedy algorithm for learning sparse representations. Technical report, Rutgers University, 2008.</p>
<p>[23] Tong Zhang. On the consistency of feature selection usinggreedy least squares regression. Journal of Machine Learning Research, 10:555–568, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
