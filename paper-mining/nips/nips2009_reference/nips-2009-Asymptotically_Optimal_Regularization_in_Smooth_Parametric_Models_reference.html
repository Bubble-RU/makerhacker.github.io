<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>37 nips-2009-Asymptotically Optimal Regularization in Smooth Parametric Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-37" href="../nips2009/nips-2009-Asymptotically_Optimal_Regularization_in_Smooth_Parametric_Models.html">nips2009-37</a> <a title="nips-2009-37-reference" href="#">nips2009-37-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>37 nips-2009-Asymptotically Optimal Regularization in Smooth Parametric Models</h1>
<br/><p>Source: <a title="nips-2009-37-pdf" href="http://papers.nips.cc/paper/3693-asymptotically-optimal-regularization-in-smooth-parametric-models.pdf">pdf</a></p><p>Author: Percy Liang, Guillaume Bouchard, Francis R. Bach, Michael I. Jordan</p><p>Abstract: Many types of regularization schemes have been employed in statistical learning, each motivated by some assumption about the problem domain. In this paper, we present a uniﬁed asymptotic analysis of smooth regularizers, which allows us to see how the validity of these assumptions impacts the success of a particular regularizer. In addition, our analysis motivates an algorithm for optimizing regularization parameters, which in turn can be analyzed within our framework. We apply our analysis to several examples, including hybrid generative-discriminative learning and multi-task learning. 1</p><br/>
<h2>reference text</h2><p>[1] H. Akaike. A new look at the statistical model identiﬁcation. IEEE Transactions on Automatic Control, 19:716–723, 1974.</p>
<p>[2] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In Advances in Neural Information Processing Systems (NIPS), pages 41–48, 2007.</p>
<p>[3] B. Bakker and T. Heskes. Task clustering and gating for Bayesian multitask learning. Journal of Machine Learning Research, 4:83–99, 2003.</p>
<p>[4] M. S. Bartlett. Approximate conﬁdence intervals. II. More than one unknown parameter. Biometrika, 40:306–317, 1953.</p>
<p>[5] P. L. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. Annals of Statistics, 33(4):1497–1537, 2005.</p>
<p>[6] G. Bouchard. Bias-variance tradeoff in hybrid generative-discriminative models. In Sixth International Conference on Machine Learning and Applications (ICMLA), pages 124–129, 2007.</p>
<p>[7] G. Bouchard and B. Triggs. The trade-off between generative and discriminative classiﬁers. In International Conference on Computational Statistics, pages 721–728, 2004.</p>
<p>[8] O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research, 2:499–526, 2002.</p>
<p>[9] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press, 2006.</p>
<p>[10] P. Craven and G. Wahba. Smoothing noisy data with spline functions. estimating the correct degree of smoothing by the method of generalized cross-validation. Numerische Mathematik, 31(4):377–403, 1978.</p>
<p>[11] Y. C. Eldar. Generalized SURE for exponential families: Applications to regularization. IEEE Transactions on Signal Processing, 57(2):471–481, 2009.</p>
<p>[12] T. Evgeniou, C. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods. Journal of Machine Learning Research, 6:615–637, 2005.</p>
<p>[13] L. Jacob, F. Bach, and J. Vert. Clustered multi-task learning: A convex formulation. In Advances in Neural Information Processing Systems (NIPS), pages 745–752, 2009.</p>
<p>[14] W. James and C. Stein. Estimation with quadratic loss. In Fourth Berkeley Symposium in Mathematics, Statistics, and Probability, pages 361–380, 1961.</p>
<p>[15] J. A. Lasserre, C. M. Bishop, and T. P. Minka. Principled hybrids of generative and discriminative models. In Computer Vision and Pattern Recognition (CVPR), pages 87–94, 2006.</p>
<p>[16] P. Liang, F. Bach, G. Bouchard, and M. I. Jordan. Asymptotically optimal regularization in smooth parametric models. Technical report, ArXiv, 2010.</p>
<p>[17] P. Liang and M. I. Jordan. An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators. In International Conference on Machine Learning (ICML), 2008.</p>
<p>[18] A. McCallum, C. Pal, G. Druck, and X. Wang. Multi-conditional learning: Generative/discriminative training for clustering and classiﬁcation. In Association for the Advancement of Artiﬁcial Intelligence (AAAI), 2006.</p>
<p>[19] B. Peters, H. Bui, S. Frankild, M. Nielson, C. Lundegaard, E. Kostem, D. Basch, K. Lamberth, M. Harndahl, W. Fleri, S. S. Wilson, J. Sidney, O. Lund, S. Buus, and A. Sette. A community resource benchmarking predictions of peptide binding to MHC-I molecules. PLoS Compututational Biology, 2, 2006.</p>
<p>[20] R. Raina, Y. Shen, A. Ng, and A. McCallum. Classiﬁcation with hybrid generative/discriminative models. In Advances in Neural Information Processing Systems (NIPS), 2004.</p>
<p>[21] C. M. Stein. Estimation of the mean of a multivariate normal distribution. Annals of Statistics, 9(6):1135–1151, 1981.</p>
<p>[22] A. W. van der Vaart. Asymptotic Statistics. Cambridge University Press, 1998.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
