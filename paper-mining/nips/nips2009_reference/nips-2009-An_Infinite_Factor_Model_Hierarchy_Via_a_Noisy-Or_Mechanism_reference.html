<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>29 nips-2009-An Infinite Factor Model Hierarchy Via a Noisy-Or Mechanism</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-29" href="../nips2009/nips-2009-An_Infinite_Factor_Model_Hierarchy_Via_a_Noisy-Or_Mechanism.html">nips2009-29</a> <a title="nips-2009-29-reference" href="#">nips2009-29-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>29 nips-2009-An Infinite Factor Model Hierarchy Via a Noisy-Or Mechanism</h1>
<br/><p>Source: <a title="nips-2009-29-pdf" href="http://papers.nips.cc/paper/3833-an-infinite-factor-model-hierarchy-via-a-noisy-or-mechanism.pdf">pdf</a></p><p>Author: Douglas Eck, Yoshua Bengio, Aaron C. Courville</p><p>Abstract: The Indian Buffet Process is a Bayesian nonparametric approach that models objects as arising from an inﬁnite number of latent factors. Here we extend the latent factor model framework to two or more unbounded layers of latent factors. From a generative perspective, each layer deﬁnes a conditional factorial prior distribution over the binary latent variables of the layer below via a noisy-or mechanism. We explore the properties of the model with two empirical studies, one digit recognition task and one music tag data experiment. 1</p><br/>
<h2>reference text</h2><p>[1] Robert J. Connor and James E. Mosimann. Concepts of independence for proportions with a generalization of the Dirichlet distribution. Journal of the American Statistical Association, 64(325):194–206, 1969.</p>
<p>[2] Aaron C. Courvile, Douglas Eck, and Yoshua Bengio. An inﬁnite factor model hierarchy via a noisy-or mechanism: Supplemental material. Supplement to the NIPS paper.</p>
<p>[3] Finale Doshi-Velez and Zoubin Ghahramni. Correlated nonparametric latent feature models. In Proceedings of the 25 th Conference on Uncertainty in Artiﬁcial Intelligence, 2009.</p>
<p>[4] W. R. Gilks and P. Wild. Adaptive rejection sampling for Gibbs sampling. Applied Statistics, 41(2):337–348, 1992.</p>
<p>[5] Tom Grifﬁths and Zoubin Ghahramani. Inﬁnite latent feature models and the indian buffet process. In Advances in Neural Information Processing Systems 18, Cambridge, MA, 2006. MIT Press.</p>
<p>[6] Max Henrion. Practical issues in constructing a bayes’ belief network. In Proceedings of the Proceedings of the Third Conference Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-87), page 132?139, New York, NY, 1987. Elsevier Science.</p>
<p>[7] Hemant Ishwaran and Lancelot F. James. Gibbs sampling methods for stick-breaking priors. American Statistical Association, 96(453):161–173, 2001.</p>
<p>[8] Michael Kearns and Yishay Mansour. Exact inference of hidden structure from sample data in noisy-or networks. In Proceedings of the 14 th Conference on Uncertainty in Artiﬁcial Intelligence, pages 304–310, 1998.</p>
<p>[9] Piyush Rai and Hal Daum´ III. The inﬁnite hierarchical factor regression model. In Daphne e Koller, Dale Schuurmans, Yoshua Bengio, and L´ on Bottou, editors, Advances in Neural Ine formation Processing Systems 21, 2009.</p>
<p>[10] Yee Whye Teh, Dilan G¨ r¨ r, and Zoubin Ghahramani. Stick-breaking construction for the ou indian buffet process. In Proceedings of the Eleventh International Conference on Artiﬁcal Intelligence and Statistics (AISTAT 2007)., 2007.</p>
<p>[11] Romain Thibaux and Michael I. Jordan. Hierarchical beta process and the indian buffet process. In Proceedings of the Eleventh International Conference on Artiﬁcal Intelligence and Statistics (AISTAT 2007)., 2007.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
