<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>34 nips-2009-Anomaly Detection with Score functions based on Nearest Neighbor Graphs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-34" href="../nips2009/nips-2009-Anomaly_Detection_with_Score_functions_based_on_Nearest_Neighbor_Graphs.html">nips2009-34</a> <a title="nips-2009-34-reference" href="#">nips2009-34-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>34 nips-2009-Anomaly Detection with Score functions based on Nearest Neighbor Graphs</h1>
<br/><p>Source: <a title="nips-2009-34-pdf" href="http://papers.nips.cc/paper/3723-anomaly-detection-with-score-functions-based-on-nearest-neighbor-graphs.pdf">pdf</a></p><p>Author: Manqi Zhao, Venkatesh Saligrama</p><p>Abstract: We propose a novel non-parametric adaptive anomaly detection algorithm for high dimensional data based on score functions derived from nearest neighbor graphs on n-point nominal data. Anomalies are declared whenever the score of a test sample falls below α, which is supposed to be the desired false alarm level. The resulting anomaly detector is shown to be asymptotically optimal in that it is uniformly most powerful for the speciﬁed false alarm level, α, for the case when the anomaly density is a mixture of the nominal and a known density. Our algorithm is computationally efﬁcient, being linear in dimension and quadratic in data size. It does not require choosing complicated tuning parameters or function approximation classes and it can adapt to local structure such as local change in dimensionality. We demonstrate the algorithm on both artiﬁcial and real data sets in high dimensional feature spaces.</p><br/>
<h2>reference text</h2><p>[1] C. Campbell and K. P. Bennett, “A linear programming approach to novelty detection,” in Advances in Neural Information Processing Systems 13. MIT Press, 2001, pp. 395–401.</p>
<p>[2] M. Markou and S. Singh, “Novelty detection: a review – part 1: statistical approaches,” Signal Processing, vol. 83, pp. 2481–2497, 2003.</p>
<p>[3] R. Ramaswamy, R. Rastogi, and K. Shim, “Efﬁcient algorithms for mining outliers from large data sets,” in Proceedings of the ACM SIGMOD Conference, 2000.</p>
<p>[4] R. Vert and J. Vert, “Consistency and convergence rates of one-class svms and related algorithms,” Journal of Machine Learning Research, vol. 7, pp. 817–854, 2006.</p>
<p>[5] D. Tax and K. R. M¨ller, “Feature extraction for one-class classiﬁcation,” in Artiﬁcial neural networks u and neural information processing, Istanbul, TURQUIE, 2003.</p>
<p>[6] R. El-Yaniv and M. Nisenson, “Optimal singl-class classiﬁcation strategies,” in Advances in Neural Information Processing Systems 19. MIT Press, 2007.</p>
<p>[7] I. V. Nikiforov and M. Basseville, Detection of abrupt changes: theory and applications. Prentice-Hall, New Jersey, 1993.</p>
<p>[8] K. Zhang, M. Hutter, and H. Jin, “A new local distance-based outlier detection approach for scattered real-world data,” March 2009, arXiv:0903.3257v1[cs.LG].</p>
<p>[9] B. Sch¨lkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. Williamson, “Estimating the support of a o high-dimensional distribution,” Neural Computation, vol. 13, no. 7, pp. 1443–1471, 2001.</p>
<p>[10] D. Tax, “One-class classiﬁcation: Concept-learning in the absence of counter-examples,” Ph.D. dissertation, Delft University of Technology, June 2001.</p>
<p>[11] G. R. G. Lanckriet, L. E. Ghaoui, and M. I. Jordan, “Robust novelty detection with single-class MPM,” in Neural Information Processing Systems Conference, vol. 18, 2005.</p>
<p>[12] C. Scott and R. D. Nowak, “Learning minimum volume sets,” Journal of Machine Learning Research, vol. 7, pp. 665–704, 2006.</p>
<p>[13] A. O. Hero, “Geometric entropy minimization(GEM) for anomaly detection and localization,” in Neural Information Processing Systems Conference, vol. 19, 2006.</p>
<p>[14] J. B. Tenenbaum, V. de Silva, and J. C. Langford, “A global geometric framework fo nonlinear dimensionality reduction,” Science, vol. 290, pp. 2319–2323, 2000.</p>
<p>[15] M. Bernstein, V. D. Silva, J. C. Langford, and J. B. Tenenbaum, “Graph approximations to geodesics on embedded manifolds,” 2000.</p>
<p>[16] S. T. Roweis and L. K. Saul, “Nonlinear dimensionality reduction by local linear embedding,” Science, vol. 290, pp. 2323–2326, 2000.</p>
<p>[17] C. McDiarmid, “On the method of bounded differences,” in Surveys in Combinatorics. University Press, 1989, pp. 148–188.</p>
<p>[18] L. Devroye, L. Gy¨ rﬁ, and G. Lugosi, A Probabilistic Theory of Pattern Recognition. o New York, Inc., 1996.  Cambridge  Springer Verlag</p>
<p>[19] “Benchmark repository.” [Online]. Available: http://ida.ﬁrst.fhg.de/projects/bench/benchmarks.htm</p>
<p>[20] A. Asuncion and D. J. Newman, “UCI machine learning repository,” 2007. [Online]. Available: http://www.ics.uci.edu/∼mlearn/MLRepository.html  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
