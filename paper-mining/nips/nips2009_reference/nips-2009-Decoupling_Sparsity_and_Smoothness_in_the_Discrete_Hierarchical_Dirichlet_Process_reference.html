<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>65 nips-2009-Decoupling Sparsity and Smoothness in the Discrete Hierarchical Dirichlet Process</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-65" href="../nips2009/nips-2009-Decoupling_Sparsity_and_Smoothness_in_the_Discrete_Hierarchical_Dirichlet_Process.html">nips2009-65</a> <a title="nips-2009-65-reference" href="#">nips2009-65-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>65 nips-2009-Decoupling Sparsity and Smoothness in the Discrete Hierarchical Dirichlet Process</h1>
<br/><p>Source: <a title="nips-2009-65-pdf" href="http://papers.nips.cc/paper/3835-decoupling-sparsity-and-smoothness-in-the-discrete-hierarchical-dirichlet-process.pdf">pdf</a></p><p>Author: Chong Wang, David M. Blei</p><p>Abstract: We present a nonparametric hierarchical Bayesian model of document collections that decouples sparsity and smoothness in the component distributions (i.e., the “topics”). In the sparse topic model (sparseTM), each topic is represented by a bank of selector variables that determine which terms appear in the topic. Thus each topic is associated with a subset of the vocabulary, and topic smoothness is modeled on this subset. We develop an efﬁcient Gibbs sampler for the sparseTM that includes a general-purpose method for sampling from a Dirichlet mixture with a combinatorial number of components. We demonstrate the sparseTM on four real-world datasets. Compared to traditional approaches, the empirical results will show that sparseTMs give better predictive performance with simpler inferred models. 1</p><br/>
<h2>reference text</h2><p>[1] Teh, Y. W., M. I. Jordan, M. J. Beal, et al. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581, 2006.</p>
<p>[2] Blei, D., A. Ng, M. Jordan. Latent Dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, 2003.</p>
<p>[3] Griffths, T., M. Steyvers. Probabilistic topic models. In Latent Semantic Analysis: A Road to Meaning. 2006.</p>
<p>[4] Saund, E. A multiple cause mixture model for unsupervised learning. Neural Comput., 7(1):51–71, 1995.</p>
<p>[5] Kab´ n, A., E. Bingham, T. Hirsim¨ ki. Learning to read between the lines: The aspect Bernoulli model. In a a SDM. 2004.</p>
<p>[6] Ishwaran, H., J. S. Rao. Spike and slab variable selection: Frequentist and Bayesian strategies. The Annals of Statistics, 33(2):730–773, 2005.</p>
<p>[7] Friedman, N., Y. Singer. Efﬁcient Bayesian parameter estimation in large discrete domains. In NIPS. 1999.</p>
<p>[8] Pitman, J. Poisson–Dirichlet and GEM invariant distributions for split-and-merge transformations of an interval partition. Comb. Probab. Comput., 11(5):501–514, 2002.</p>
<p>[9] Sethuraman, J. A constructive deﬁnition of Dirichlet priors. Statistica Sinica, 4:639–650, 1994.</p>
<p>[10] Escobar, M. D., M. West. Bayesian density estimation and inference using mixtures. Journal of the American Statistical Association, 90:577–588, 1995.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
