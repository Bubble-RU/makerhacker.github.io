<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>20 nips-2009-A unified framework for high-dimensional analysis of $M$-estimators with decomposable regularizers</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2009" href="../home/nips2009_home.html">nips2009</a> <a title="nips-2009-20" href="../nips2009/nips-2009-A_unified_framework_for_high-dimensional_analysis_of_%24M%24-estimators_with_decomposable_regularizers.html">nips2009-20</a> <a title="nips-2009-20-reference" href="#">nips2009-20-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>20 nips-2009-A unified framework for high-dimensional analysis of $M$-estimators with decomposable regularizers</h1>
<br/><p>Source: <a title="nips-2009-20-pdf" href="http://papers.nips.cc/paper/3765-a-unified-framework-for-high-dimensional-analysis-of-m-estimators-with-decomposable-regularizers.pdf">pdf</a></p><p>Author: Sahand Negahban, Bin Yu, Martin J. Wainwright, Pradeep K. Ravikumar</p><p>Abstract: High-dimensional statistical inference deals with models in which the the number of parameters p is comparable to or larger than the sample size n. Since it is usually impossible to obtain consistent procedures unless p/n → 0, a line of recent work has studied models with various types of structure (e.g., sparse vectors; block-structured matrices; low-rank matrices; Markov assumptions). In such settings, a general approach to estimation is to solve a regularized convex program (known as a regularized M -estimator) which combines a loss function (measuring how well the model ﬁts the data) with some regularization function that encourages the assumed structure. The goal of this paper is to provide a uniﬁed framework for establishing consistency and convergence rates for such regularized M estimators under high-dimensional scaling. We state one main theorem and show how it can be used to re-derive several existing results, and also to obtain several new results on consistency and convergence rates. Our analysis also identiﬁes two key properties of loss and regularization functions, referred to as restricted strong convexity and decomposability, that ensure the corresponding regularized M -estimators have fast convergence rates. 1</p><br/>
<h2>reference text</h2><p>[1] P. Bickel, Y. Ritov, and A. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Submitted to Annals of Statistics, 2008.</p>
<p>[2] E. Candes and T. Tao. The Dantzig selector: Statistical estimation when p is much larger than n. Annals of Statistics, 35(6):2313–2351, 2007.</p>
<p>[3] S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM J. Sci. Computing, 20(1):33–61, 1998.</p>
<p>[4] K. Lounici, M. Pontil, A. B. Tsybakov, and S. van de Geer. Taking advantage of sparsity in multi-task learning. Arxiv, 2009.</p>
<p>[5] L. Meier, S. Van de Geer, and P. B¨ hlmann. The group lasso for logistic regression. Journal u of the Royal Statistical Society, Series B, 70:53–71, 2008.</p>
<p>[6] N. Meinshausen and P. B¨ hlmann. High-dimensional graphs and variable selection with the u Lasso. Annals of Statistics, 34:1436–1462, 2006.</p>
<p>[7] N. Meinshausen and B. Yu. Lasso-type recovery of sparse representations for high-dimensional data. Annals of Statistics, 37(1):246–270, 2009.</p>
<p>[8] G. Obozinski, M. J. Wainwright, and M. I. Jordan. Union support recovery in high-dimensional multivariate regression. Technical report, Department of Statistics, UC Berkeley, August 2008.</p>
<p>[9] S. Portnoy. Asymptotic behavior of M-estimators of p regression parameters when p2 /n is large: I. consistency. Annals of Statistics, 12(4):1296–1309, 1984.</p>
<p>[10] G. Raskutti, M. J. Wainwright, and B. Yu. Minimax rates of estimation for high-dimensional linear regression over q -balls. Technical Report arXiv:0910.2042, UC Berkeley, Department of Statistics, 2009.</p>
<p>[11] P. Ravikumar, M. J. Wainwright, and J. Lafferty. High-dimensional Ising model selection using 1 -regularized logistic regression. Annals of Statistics, 2008. To appear.</p>
<p>[12] P. Ravikumar, M. J. Wainwright, G. Raskutti, and B. Yu. High-dimensional covariance estimation by minimizing 1 -penalized log-determinant divergence. Technical Report 767, Department of Statistics, UC Berkeley, September 2008.</p>
<p>[13] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. Allerton Conference, 2007.</p>
<p>[14] A.J. Rothman, P.J. Bickel, E. Levina, and J. Zhu. Sparse permutation invariant covariance estimation. Electron. J. Statist., 2:494–515, 2008.</p>
<p>[15] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B, 58(1):267–288, 1996.</p>
<p>[16] J. Tropp. Just relax: Convex programming methods for identifying sparse signals in noise. IEEE Trans. Info Theory, 52(3):1030–1051, March 2006.</p>
<p>[17] B. Turlach, W.N. Venables, and S.J. Wright. Simultaneous variable selection. Technometrics, 27:349–363, 2005.</p>
<p>[18] S. Van de Geer. High-dimensional generalized linear models and the lasso. Annals of Statistics, 36(2):614–645, 2008.</p>
<p>[19] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using 1 constrained quadratic programming (Lasso). IEEE Trans. Information Theory, 55:2183–2202, May 2009.</p>
<p>[20] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society B, 1(68):49, 2006.</p>
<p>[21] C. Zhang and J. Huang. Model selection consistency of the lasso selection in high-dimensional linear regression. Annals of Statistics, 36:1567–1594, 2008.</p>
<p>[22] P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning Research, 7:2541–2567, 2006.</p>
<p>[23] P. Zhao, G. Rocha, and B. Yu. Grouped and hierarchical model selection through composite absolute penalties. Annals of Statistics, 37(6A):3468–3497, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
