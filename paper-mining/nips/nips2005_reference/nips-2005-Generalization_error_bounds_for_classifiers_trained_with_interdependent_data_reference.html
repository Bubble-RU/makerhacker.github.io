<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-83" href="../nips2005/nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">nips2005-83</a> <a title="nips-2005-83-reference" href="#">nips2005-83-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</h1>
<br/><p>Source: <a title="nips-2005-83-pdf" href="http://papers.nips.cc/paper/2860-generalization-error-bounds-for-classifiers-trained-with-interdependent-data.pdf">pdf</a></p><p>Author: Nicolas Usunier, Massih-reza Amini, Patrick Gallinari</p><p>Abstract: In this paper we propose a general framework to study the generalization properties of binary classiﬁers trained with data which may be dependent, but are deterministically generated upon a sample of independent examples. It provides generalization bounds for binary classiﬁcation and some cases of ranking problems, and clariﬁes the relationship between these learning tasks. 1</p><br/>
<h2>reference text</h2><p>[1] Agarwal S., Graepel T., Herbrich R., Har-Peled S., Roth D. (2005) Generalization Error Bounds for the Area Under the ROC curve, Journal of Machine Learning Research.</p>
<p>[2] Agarwal S., Niyogi P. (2005) Stability and generalization of bipartite ranking algorithms, Conference on Learning Theory 18.</p>
<p>[3] Bartlett P., Mendelson S. (2002) Rademacher and Gaussian Complexities: Risk Bounds and Structural Results, Journal of Machine Learning Research 3, pp. 463-482.</p>
<p>[4] Boucheron S., Bousquet O., Lugosi G. (2004) Concentration inequalities, in O. Bousquet, U.v. Luxburg, and G. Rtsch (editors), Advanced Lectures in Machine Learning, Springer, pp. 208-240.</p>
<p>[5] Clemencon S., Lugosi G., Vayatis N. (2005) Ranking and scoring using empirical risk minimiza¸ tion, Conference on Learning Theory 18.</p>
<p>[6] Cortes C., Mohri M. (2004) AUC optimization vs error rate miniminzation NIPS 2003,</p>
<p>[7] Freund Y., Iyer R.D., Schapire R.E., Singer Y. (2003) An Efﬁcient Boosting Algorithm for Combining Preferences, Journal of Machine Learning Research 4, pp. 933-969.</p>
<p>[8] Janson S. (2004) Large deviations for sums of partly dependent random variables, Random Structures and Algorithms 24, pp. 234-248.</p>
<p>[9] McDiarmid C. (1989) On the method of bounded differences, Surveys in Combinatorics.</p>
<p>[10] Meir R., Zhang T. (2003) Generalization Error Bounds for Bayesian Mixture Algorithms, Journal of Machine Learning Research 4, pp. 839-860.</p>
<p>[11] Rudin C., Cortes C., Mohri M., Schapire R.E. (2005) Margin-Based Ranking meets Boosting in the middle, Conference on Learning Theory 18.</p>
<p>[12] Shawe-Taylor J., Cristianini N. (2004) Kernel Methods for Pattern Analysis, Cambridge U. Prs.</p>
<p>[13] Long version of this paper, Available at http://www-connex.lip6.fr/˜usunier/nips05-lv.pdf</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
