<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>175 nips-2005-Sequence and Tree Kernels with Statistical Feature Mining</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-175" href="../nips2005/nips-2005-Sequence_and_Tree_Kernels_with_Statistical_Feature_Mining.html">nips2005-175</a> <a title="nips-2005-175-reference" href="#">nips2005-175-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>175 nips-2005-Sequence and Tree Kernels with Statistical Feature Mining</h1>
<br/><p>Source: <a title="nips-2005-175-pdf" href="http://papers.nips.cc/paper/2816-sequence-and-tree-kernels-with-statistical-feature-mining.pdf">pdf</a></p><p>Author: Jun Suzuki, Hideki Isozaki</p><p>Abstract: This paper proposes a new approach to feature selection based on a statistical feature mining technique for sequence and tree kernels. Since natural language data take discrete structures, convolution kernels, such as sequence and tree kernels, are advantageous for both the concept and accuracy of many natural language processing tasks. However, experiments have shown that the best results can only be achieved when limited small sub-structures are dealt with by these kernels. This paper discusses this issue of convolution kernels and then proposes a statistical feature selection that enable us to use larger sub-structures effectively. The proposed method, in order to execute efﬁciently, can be embedded into an original kernel calculation process by using sub-structure mining algorithms. Experiments on real NLP tasks conﬁrm the problem in the conventional method and compare the performance of a conventional method to that of the proposed method.</p><br/>
<h2>reference text</h2><p>[1] N. Cancedda, E. Gaussier, C. Goutte, and J.-M. Renders. Word-Sequence Kernels. Journal of Machine Learning Research, 3:1059–1082, 2003.</p>
<p>[2] M. Collins and N. Duffy. Convolution kernels for natural language. In Proc. of Neural Information Processing Systems (NIPS’2001), 2001.</p>
<p>[3] D. Haussler. Convolution kernels on discrete structures. In Technical Report UCS-CRL-99-10. UC Santa Cruz, 1999.</p>
<p>[4] T. Joachims. Text Categorization with Support Vector Machines: Learning with Many Relevant Features. In Proc. of European Conference on Machine Learning (ECML ’98), pages 137–142, 1998.</p>
<p>[5] H. Kashima and T. Koyanagi. Kernels for Semi-Structured Data. In Proc. 19th International Conference on Machine Learning (ICML2002), pages 291–298, 2002.</p>
<p>[6] X. Li and D. Roth. Learning Question Classiﬁers. In Proc. of the 19th International Conference on Computational Linguistics (COLING 2002), pages 556–562, 2002.</p>
<p>[7] H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins. Text Classiﬁcation Using String Kernel. Journal of Machine Learning Research, 2:419–444, 2002.</p>
<p>[8] S. Morishita and J. Sese. Traversing Itemset Lattices with Statistical Metric Pruning. In Proc. of ACM SIGACT-SIGMOD-SIGART Symp. on Database Systems (PODS’00), pages 226–236, 2000.</p>
<p>[9] J. Pei, J. Han, B. Mortazavi-Asl, and H. Pinto. PreﬁxSpan: Mining Sequential Patterns Efﬁciently by Preﬁx-Projected Pattern Growth. In Proc. of the 17th International Conference on Data Engineering (ICDE 2001), pages 215–224, 2001.</p>
<p>[10] J. Suzuki, Y. Sasaki, and E. Maeda. Kernels for Structured Natural Language Data. In Proc. of the 17th Annual Conference on Neural Information Processing Systems (NIPS2003), 2003.</p>
<p>[11] C. Watkins. Dynamic alignment kernels. In Technical Report CSD-TR-98-11. Royal Holloway, University of London Computer Science Department, 1999.</p>
<p>[12] M. J. Zaki. Efﬁciently Mining Frequent Trees in a Forest. In Proc. of the 8th International Conference on Knowledge Discovery and Data Mining (KDD’02), pages 71–80, 2002.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
