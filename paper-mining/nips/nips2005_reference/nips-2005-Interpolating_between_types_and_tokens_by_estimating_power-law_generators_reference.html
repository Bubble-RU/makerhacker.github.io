<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>100 nips-2005-Interpolating between types and tokens by estimating power-law generators</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-100" href="../nips2005/nips-2005-Interpolating_between_types_and_tokens_by_estimating_power-law_generators.html">nips2005-100</a> <a title="nips-2005-100-reference" href="#">nips2005-100-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>100 nips-2005-Interpolating between types and tokens by estimating power-law generators</h1>
<br/><p>Source: <a title="nips-2005-100-pdf" href="http://papers.nips.cc/paper/2941-interpolating-between-types-and-tokens-by-estimating-power-law-generators.pdf">pdf</a></p><p>Author: Sharon Goldwater, Mark Johnson, Thomas L. Griffiths</p><p>Abstract: Standard statistical models of language fail to capture one of the most striking properties of natural languages: the power-law distribution in the frequencies of word tokens. We present a framework for developing statistical models that generically produce power-laws, augmenting standard generative models with an adaptor that produces the appropriate pattern of token frequencies. We show that taking a particular stochastic process – the Pitman-Yor process – as an adaptor justiﬁes the appearance of type frequencies in formal analyses of natural language, and improves the performance of a model for unsupervised learning of morphology.</p><br/>
<h2>reference text</h2><p>[1] G. Zipf. Selective Studies and the Principle of Relative Frequency in Language. Harvard University Press, Cambridge, MA, 1932.</p>
<p>[2] M. Mitzenmacher. A brief history of generative models for power law and lognormal distributions. Internet Mathematics, 1(2):226–251, 2003.</p>
<p>[3] H.A. Simon. On a class of skew distribution functions. Biometrika, 42(3/4):425–440, 1955.</p>
<p>[4] J. Pitman. Exchangeable and partially exchangeable random partitions. Probability Theory and Related Fields, 102:145–158, 1995.</p>
<p>[5] J. Pitman and M. Yor. The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator. Annals of Probability, 25:855–900, 1997.</p>
<p>[6] H. Ishwaran and L. F. James. Generalized weighted Chinese restaurant processes for species sampling mixture models. Statistica Sinica, 13:1211–1235, 2003.</p>
<p>[7] J. B. Pierrehumbert. Probabilistic phonology: discrimination and robustness. In R. Bod, J. Hay, and S. Jannedy, editors, Probabilistic linguistics. MIT Press, Cambridge, MA, 2003.</p>
<p>[8] J. Goldsmith. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 27:153–198, 2001.</p>
<p>[9] H. Ney, U. Essen, and R. Kneser. On structuring probabilistic dependences in stochastic language modeling. Computer, Speech, and Language, 8:1–38, 1994.</p>
<p>[10] R. Kneser and H. Ney. Improved backing-off for n-gram language modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 1995.</p>
<p>[11] S. F. Chen and J. Goodman. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Center for Research in Computing Technology, Harvard University, 1998. ´</p>
<p>[12] D. Aldous. Exchangeability and related topics. In Ecole d’´ t´ de probabilit´ s de Saint-Flour, ee e XIII—1983, pages 1–198. Springer, Berlin, 1985.</p>
<p>[13] R. M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics, 9:249–265, 2000.</p>
<p>[14] L. Larkey, L. Ballesteros, and M. Connell. Improving stemming for arabic information retrieval: Light stemming and co-occurrence analysis. In Proceedings of the 25th International Conference on Research and Development in Information Retrieval (SIGIR), 2002.</p>
<p>[15] W.R. Gilks, S. Richardson, and D. J. Spiegelhalter, editors. Markov Chain Monte Carlo in Practice. Chapman and Hall, Suffolk, 1996.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
