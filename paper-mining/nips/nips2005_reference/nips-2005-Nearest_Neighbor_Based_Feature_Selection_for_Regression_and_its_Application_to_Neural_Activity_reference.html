<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-132" href="../nips2005/nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">nips2005-132</a> <a title="nips-2005-132-reference" href="#">nips2005-132-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</h1>
<br/><p>Source: <a title="nips-2005-132-pdf" href="http://papers.nips.cc/paper/2848-nearest-neighbor-based-feature-selection-for-regression-and-its-application-to-neural-activity.pdf">pdf</a></p><p>Author: Amir Navot, Lavi Shpigelman, Naftali Tishby, Eilon Vaadia</p><p>Abstract: We present a non-linear, simple, yet effective, feature subset selection method for regression and use it in analyzing cortical neural activity. Our algorithm involves a feature-weighted version of the k-nearest-neighbor algorithm. It is able to capture complex dependency of the target function on its input and makes use of the leave-one-out error as a natural regularization. We explain the characteristics of our algorithm on synthetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey. By applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data.</p><br/>
<h2>reference text</h2><p>[1] R. Kohavi and G.H. John. Wrapper for feature subset selection. Artiﬁcial Intelligence, 97(12):273–324, 1997.</p>
<p>[2] I. Guyon and A. Elisseeff. An introduction to variable and feature selection. JMLR, 2003.</p>
<p>[3] A.J. Miller. Subset Selection in Regression. Chapman and Hall, 1990.</p>
<p>[4] L. Devroye. The uniform convergence of nearest neighbor regression function estimators and their application in optimization. IEEE transactions in information theory, 24(2), 1978.</p>
<p>[5] C. Atkeson, A. Moore, and S. Schaal. Locally weighted learning. AI Review, 11.</p>
<p>[6] O. Maron and A. Moore. The racing algorithm: Model selection for lazy learners. In Artiﬁcial Intelligence Review, volume 11, pages 193–225, April 1997.</p>
<p>[7] R. Gilad-Bachrach, A. Navot, and N. Tishby. Margin based feature selection - theory and algorithms. In Proc. 21st (ICML), pages 337–344, 2004.</p>
<p>[8] D. M. Taylor, S. I. Tillery, and A. B. Schwartz. Direct cortical control of 3d neuroprosthetic devices. Science, 296(7):1829–1832, 2002.</p>
<p>[9] V. Vapnik. The Nature Of Statistical Learning Theory. Springer-Verlag, 1995.</p>
<p>[10] J. R. Quinlan. Induction of decision trees. In Jude W. Shavlik and Thomas G. Dietterich, editors, Readings in Machine Learning. Morgan Kaufmann, 1990. Originally published in Machine Learning 1:81–106, 1986.</p>
<p>[11] R. Paz, T. Boraud, C. Natan, H. Bergman, and E. Vaadia. Preparatory activity in motor cortex reﬂects learning of local visuomotor skills. Nature Neuroscience, 6(8):882–890, August 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
