<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>145 nips-2005-On Local Rewards and Scaling Distributed Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-145" href="../nips2005/nips-2005-On_Local_Rewards_and_Scaling_Distributed_Reinforcement_Learning.html">nips2005-145</a> <a title="nips-2005-145-reference" href="#">nips2005-145-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>145 nips-2005-On Local Rewards and Scaling Distributed Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2005-145-pdf" href="http://papers.nips.cc/paper/2951-on-local-rewards-and-scaling-distributed-reinforcement-learning.pdf">pdf</a></p><p>Author: Drew Bagnell, Andrew Y. Ng</p><p>Abstract: We consider the scaling of the number of examples necessary to achieve good performance in distributed, cooperative, multi-agent reinforcement learning, as a function of the the number of agents n. We prove a worstcase lower bound showing that algorithms that rely solely on a global reward signal to learn policies confront a fundamental limit: They require a number of real-world examples that scales roughly linearly in the number of agents. For settings of interest with a very large number of agents, this is impractical. We demonstrate, however, that there is a class of algorithms that, by taking advantage of local reward signals in large distributed Markov Decision Processes, are able to ensure good performance with a number of samples that scales as O(log n). This makes them applicable even in settings with a very large number of agents n. 1</p><br/>
<h2>reference text</h2><p>[1]  N. Alon and J. Spencer. The Probabilistic Method. Wiley, 2000.</p>
<p>[2]  C. Boutilier, T. Dean, and S. Hanks. Decision theoretic planning: Structural assumptions and computational leverage. Journal of Artiﬁcial Intelligence Research, 1999.</p>
<p>[3]  Y. Chang, T. Ho, and L. Kaelbling. All learning is local: Multi-agent learning in global reward games. In Advances in NIPS 14, 2004.</p>
<p>[4]  C. Guestrin, D. Koller, and R. Parr. Multi-agent planning with factored MDPs. In NIPS-14, 2002.</p>
<p>[5]</p>
<p>[6]  M. Kearns and D. Koller. Efﬁcient reinforcement learning in factored mdps. In IJCAI 16, 1999.</p>
<p>[7]  M. Kearns, M. Littman, and S. Singh. Graphical models for game theory. In UAI, 2001.</p>
<p>[8]  M. Kearns, Y. Mansour, and A. Ng. Approximate planning in large POMDPs via reusable trajectories. (extended version of paper in NIPS 12), 1999.</p>
<p>[9]  L. Peshkin, K-E. Kim, N. Meleau, and L. Kaelbling. Learning to cooperate via policy search. In UAI 16, 2000.  C. Guestrin, M. Lagoudakis, and R. Parr. Coordinated reinforcement learning. In ICML, 2002.</p>
<p>[10] J. Schneider, W. Wong, A. Moore, and M. Riedmiller. Distributed value functions. In ICML, 1999.</p>
<p>[11] R. Williams and L. Baird. Tight performance bounds on greedy policies based on imperfect value functions. Technical report, Northeastern University, 1993.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
