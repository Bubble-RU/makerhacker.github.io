<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>74 nips-2005-Faster Rates in Regression via Active Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-74" href="../nips2005/nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">nips2005-74</a> <a title="nips-2005-74-reference" href="#">nips2005-74-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>74 nips-2005-Faster Rates in Regression via Active Learning</h1>
<br/><p>Source: <a title="nips-2005-74-pdf" href="http://papers.nips.cc/paper/2831-faster-rates-in-regression-via-active-learning.pdf">pdf</a></p><p>Author: Rebecca Willett, Robert Nowak, Rui M. Castro</p><p>Abstract: This paper presents a rigorous statistical analysis characterizing regimes in which active learning signiﬁcantly outperforms classical passive learning. Active learning algorithms are able to make queries or select sample locations in an online fashion, depending on the results of the previous queries. In some regimes, this extra ﬂexibility leads to signiﬁcantly faster rates of error decay than those possible in classical passive learning settings. The nature of these regimes is explored by studying fundamental performance limits of active and passive learning in two illustrative nonparametric function classes. In addition to examining the theoretical potential of active learning, this paper describes a practical algorithm capable of exploiting the extra ﬂexibility of the active setting and provably improving upon the classical passive techniques. Our active learning theory and methods show promise in a number of applications, including ﬁeld estimation using wireless sensor networks and fault line detection. 1</p><br/>
<h2>reference text</h2><p>[1] D. Cohn, Z. Ghahramani, and M. Jordan, “Active learning with statistical models,” Journal of Artiﬁcial Intelligence Research, pp. 129–145, 1996.</p>
<p>[2] D. J. C. Mackay, “Information-based objective functions for active data selection,” Neural Computation, vol. 4, pp. 698–714, 1991.</p>
<p>[3] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby, “Information, prediction, and query by committee,” Proc. Advances in Neural Information Processing Systems, 1993.</p>
<p>[4] K. Sung and P. Niyogi, “Active learning for function approximation,” Proc. Advances in Neural Information Processing Systems, vol. 7, 1995.</p>
<p>[5] G. Blanchard and D. Geman, “Hierarchical testing designs for pattern recognition,” to appear in Annals of Statistics, 2005.</p>
<p>[6] M. V. Burnashev and K. Sh. Zigangirov, “An interval estimation problem for controlled observations,” Problems in Information Transmission, vol. 10, pp. 223–231, 1974.</p>
<p>[7] P. Hall and I. Molchanov, “Sequential methods for design-adaptive estimation of discontinuities in regression curves and surfaces,” The Annals of Statistics, vol. 31, no. 3, pp. 921–941, 2003.</p>
<p>[8] Alexander Korostelev, “On minimax rates of convergence in image models under sequential design,” Statistics & Probability Letters, vol. 43, pp. 369–375, 1999.</p>
<p>[9] R. Willett, A. Martin, and R. Nowak, “Backcasting: Adaptive sampling for sensor networks,” in Proc. Information Processing in Sensor Networks, 26-27 April, Berkeley, CA, USA, 2004.</p>
<p>[10] Charles J. Stone, “Optimal rates of convergence for nonparametric estimators,” The Annals of Statistics, vol. 8, no. 6, pp. 1348–1360, 1980.</p>
<p>[11] A.P. Korostelev and A.B. Tsybakov, Minimax Theory of Image Reconstruction, Springer Lecture Notes in Statistics, 1993.</p>
<p>[12] R. Castro, R. Willett, and R. Nowak, “Fast rates in regression via active learning,” Tech. Rep., University of Wisconsin, Madison, June 2005, ECE-05-3 Technical Report (available at http://homepages.cae.wisc.edu/ rcastro/ECE-05-3.pdf).</p>
<p>[13] Alexandre B. Tsybakov, Introduction a l’estimation non-param´ trique, Math´ matiques et e e ` Applications, 41. Springer, 2004.</p>
<p>[14] R. Nowak, U. Mitra, and R. Willett, “Estimating inhomogeneous ﬁelds using wireless sensor networks,” IEEE Journal on Selected Areas in Communication, vol. 22, no. 6, pp. 999–1006, 2004.</p>
<p>[15] Andrew R. Barron, “Complexity regularization with application to artiﬁcial neural networks,” in Nonparametric Functional Estimation and Related Topics. 1991, pp. 561–576, Kluwer Academic Publishers.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
