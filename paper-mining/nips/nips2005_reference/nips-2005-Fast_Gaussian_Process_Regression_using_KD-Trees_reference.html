<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>69 nips-2005-Fast Gaussian Process Regression using KD-Trees</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-69" href="../nips2005/nips-2005-Fast_Gaussian_Process_Regression_using_KD-Trees.html">nips2005-69</a> <a title="nips-2005-69-reference" href="#">nips2005-69-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>69 nips-2005-Fast Gaussian Process Regression using KD-Trees</h1>
<br/><p>Source: <a title="nips-2005-69-pdf" href="http://papers.nips.cc/paper/2835-fast-gaussian-process-regression-using-kd-trees.pdf">pdf</a></p><p>Author: Yirong Shen, Matthias Seeger, Andrew Y. Ng</p><p>Abstract: The computation required for Gaussian process regression with n training examples is about O(n3 ) during training and O(n) for each prediction. This makes Gaussian process regression too slow for large datasets. In this paper, we present a fast approximation method, based on kd-trees, that signiﬁcantly reduces both the prediction and the training times of Gaussian process regression.</p><br/>
<h2>reference text</h2><p>[1] Alina Beygelzimer, Sham Kakade, and John Langford. Cover trees for nearest neighbor. (Unpublished manuscript), 2005.</p>
<p>[2] Phil Buonadonna, David Gay, Joseph M. Hellerstein, Wei Hong, and Samuel Madden. Task: Sensor network in a box. In Proceedings of European Workshop on Sensor Networks, 2005.</p>
<p>[3] Lehel Csat´ and Manfred Opper. Sparse on-line Gaussian processes. Neural Computation, o 14:641–668, 2002.</p>
<p>[4] Nando de Freitas, Yang Wang, Maryam Mahdaviani, and Dustin Lang. Fast krylov methods for n-body learning. In Advances in NIPS 18, 2006.</p>
<p>[5] Kan Deng and Andrew Moore. Multiresolution instance-based learning. In Proceedings of the Twelfth International Joint Conference on Artiﬁcial Intellingence, pages 1233–1239. Morgan Kaufmann, 1995.</p>
<p>[6] Mark N. Gibbs. Bayesian Gaussian Processes for Regression and Classiﬁcation. PhD thesis, University of Cambridge, 1997.</p>
<p>[7] Alexander Gray and Andrew Moore. N-body problems in statistical learning. In Advances in NIPS 13, 2001.</p>
<p>[8] N. D. Lawrence, M. Seeger, and R. Herbrich. Fast sparse Gaussian process methods: The informative vector machine. In Advances in NIPS 15, pages 609–616, 2003.</p>
<p>[9] Andrew Moore, Jeff Schneider, and Kan Deng. Efﬁcient locally weighted polynomial regression predictions. In Proceedings of the Fourteenth International Conference on Machine Learning, pages 236–244. Morgan Kaufmann, 1997.</p>
<p>[10] Andrew Y. Ng, Adam Coates, Mark Diel, Varun Ganapathi, Jamie Schulte, Ben Tse, Eric Berger, and Eric Liang. Inverted autonomous helicopter ﬂight via reinforcement learning. In International Symposium on Experimental Robotics, 2004.</p>
<p>[11] Stephen M. Omohundro. Five balltree construction algorithms. Technical Report TR-89-063, International Computer Science Institute, 1989.</p>
<p>[12] R. Kelley Pace and Ronald Barry. Sparse spatial autoregressions. Statistics and Probability Letters, 33(3):291–297, May 5 1997.</p>
<p>[13] F.P. Preparata and M. Shamos. Computational Geometry. Springer-Verlag, 1985.</p>
<p>[14] Nathan Ratliff and J. Andrew Bagnell. Kernel conjugate gradient. Technical Report CMU-RITR-05-30, Robotics Institute, Carnegie Mellon University, June 2005.</p>
<p>[15] Y. Saad. Iterative Methods for Sparse Linear Systems. International Thomson Publishing, 1st edition, 1996.</p>
<p>[16] M. Seeger. Gaussian processes for machine learning. International Journal of Neural Systems, 14(2):69–106, 2004.</p>
<p>[17] M. Stein. Interpolation of Spatial Data: Some Theory for Kriging. Springer, 1999.</p>
<p>[18] Michael Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning Research, 1:211–244, 2001.</p>
<p>[19] C. Williams and C. Rasmussen. Gaussian processes for regression. In Advances in NIPS 8, 1996.</p>
<p>[20] C. Yang, R. Duraiswami, and L. Davis. Efﬁcient kernel machines using the improved fast Gauss transform. In Advances in NIPS 17, pages 1561–1568, 2005.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
