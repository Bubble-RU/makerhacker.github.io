<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>84 nips-2005-Generalization in Clustering with Unobserved Features</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-84" href="../nips2005/nips-2005-Generalization_in_Clustering_with_Unobserved_Features.html">nips2005-84</a> <a title="nips-2005-84-reference" href="#">nips2005-84-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>84 nips-2005-Generalization in Clustering with Unobserved Features</h1>
<br/><p>Source: <a title="nips-2005-84-pdf" href="http://papers.nips.cc/paper/2896-generalization-in-clustering-with-unobserved-features.pdf">pdf</a></p><p>Author: Eyal Krupka, Naftali Tishby</p><p>Abstract: We argue that when objects are characterized by many attributes, clustering them on the basis of a relatively small random subset of these attributes can capture information on the unobserved attributes as well. Moreover, we show that under mild technical conditions, clustering the objects on the basis of such a random subset performs almost as well as clustering with the full attribute set. We prove a ﬁnite sample generalization theorems for this novel learning scheme that extends analogous results from the supervised learning setting. The scheme is demonstrated for collaborative ﬁltering of users with movies rating as attributes. 1</p><br/>
<h2>reference text</h2><p>[1] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering: a review. ACM Computing Surveys, 31(3):264–323, September 1999.</p>
<p>[2] T. M. Cover and J. A. Thomas. Elements Of Information Theory. Wiley Interscience, 1991.</p>
<p>[3] V. N. Vapnik. Statistical Learning Theory. Wiley, 1998.</p>
<p>[4] N. Tishby, F. Pereira, and W. Bialek. The information bottleneck method. Proc. 37th Allerton Conf. on Communication and Computation, 1999.</p>
<p>[5] M. Seeger. Learning with labeled and unlabeled data. Technical report, University of Edinburgh, 2002.</p>
<p>[6] M. Szummer and T. Jaakkola. Information regularization with partially labeled data. In NIPS, 2003.</p>
<p>[7] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58:13–30, 1963.</p>
<p>[8] E. Krupka and N. Tishby. Generalization in clustering with unobserved features. Technical report, Hebrew University, 2005. http://www.cs.huji.ac.il/~tishby/nips2005tr.pdf.</p>
<p>[9] L. Paninski. Estimation of entropy and mutual information. Neural Computation, 15:1101– 1253, 2003.</p>
<p>[10] B. Marlin. Collaborative ﬁltering: A machine learning perspective. Master’s thesis, University of Toronto, 2004.  1  Chinaberries are the fruits of the Melia azedarach tree, and are poisonous.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
