<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>104 nips-2005-Laplacian Score for Feature Selection</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-104" href="../nips2005/nips-2005-Laplacian_Score_for_Feature_Selection.html">nips2005-104</a> <a title="nips-2005-104-reference" href="#">nips2005-104-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>104 nips-2005-Laplacian Score for Feature Selection</h1>
<br/><p>Source: <a title="nips-2005-104-pdf" href="http://papers.nips.cc/paper/2909-laplacian-score-for-feature-selection.pdf">pdf</a></p><p>Author: Xiaofei He, Deng Cai, Partha Niyogi</p><p>Abstract: In supervised learning scenarios, feature selection has been studied widely in the literature. Selecting features in unsupervised learning scenarios is a much harder problem, due to the absence of class labels that would guide the search for relevant information. And, almost all of previous unsupervised feature selection methods are “wrapper” techniques that require a learning algorithm to evaluate the candidate feature subsets. In this paper, we propose a “ﬁlter” method for feature selection which is independent of any learning algorithm. Our method can be performed in either supervised or unsupervised fashion. The proposed method is based on the observation that, in many real world classiﬁcation problems, data from the same class are often close to each other. The importance of a feature is evaluated by its power of locality preserving, or, Laplacian Score. We compare our method with data variance (unsupervised) and Fisher score (supervised) on two data sets. Experimental results demonstrate the effectiveness and efﬁciency of our algorithm. 1</p><br/>
<h2>reference text</h2><p>[1] M. Belkin and P. Niyogi, “Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering,” Advances in Neural Information Processing Systems, Vol. 14, 2001.</p>
<p>[2] Fan R. K. Chung, Spectral Graph Theory, Regional Conference Series in Mathematics, number 92, 1997.</p>
<p>[3] X. He and P. Niyogi, “Locality Preserving Projections,” Advances in Neural Information Processing Systems, Vol. 16, 2003.</p>
<p>[4] R. Kohavi and G. John, “Wrappers for Feature Subset Selection,” Artiﬁcial Intelligence, 97(12):273-324, 1997.</p>
<p>[5] L. Lovasz and M. Plummer, Matching Theory, Akad´miai Kiad´, North Holland, 1986. e o</p>
<p>[6] W. Xu, X. Liu and Y. Gong, “Document Clustering Based on Non-negative Matrix Factorization ,” ACM SIGIR Conference on Information Retrieval, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
