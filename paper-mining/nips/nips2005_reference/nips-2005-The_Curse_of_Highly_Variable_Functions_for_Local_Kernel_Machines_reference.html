<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-190" href="../nips2005/nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">nips2005-190</a> <a title="nips-2005-190-reference" href="#">nips2005-190-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</h1>
<br/><p>Source: <a title="nips-2005-190-pdf" href="http://papers.nips.cc/paper/2810-the-curse-of-highly-variable-functions-for-local-kernel-machines.pdf">pdf</a></p><p>Author: Yoshua Bengio, Olivier Delalleau, Nicolas L. Roux</p><p>Abstract: We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior – with similarity between examples expressed with a local kernel – are sensitive to the curse of dimensionality, or more precisely to the variability of the target. Our discussion covers supervised, semisupervised and unsupervised learning algorithms. These algorithms are found to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set. This makes them sensitive to the curse of dimensionality, well studied for classical non-parametric statistical learning. We show in the case of the Gaussian kernel that when the function to be learned has many variations, these algorithms require a number of training examples proportional to the number of variations, which could be large even though there may exist short descriptions of the target function, i.e. their Kolmogorov complexity may be low. This suggests that there exist non-local learning algorithms that at least have the potential to learn about such structured but apparently complex functions (because locally they have many variations), while not using very speciﬁc prior domain knowledge. 1</p><br/>
<h2>reference text</h2><p>Belkin, M., Matveeva, I., and Niyogi, P. (2004). Regularization and semi-supervised learning on large graphs. In Shawe-Taylor, J. and Singer, Y., editors, COLT’2004. Springer. Belkin, M. and Niyogi, P. (2003). Using manifold structure for partially labeled classiﬁcation. In Becker, S., Thrun, S., and Obermayer, K., editors, Advances in Neural  Information Processing Systems 15, Cambridge, MA. MIT Press. Bengio, Y., Delalleau, O., and Le Roux, N. (2005). The curse of dimensionality for local kernel machines. Technical Report 1258, D´ partement d’informatique et recherche e op´ rationnelle, Universit´ de Montr´ al. e e e Bengio, Y., Delalleau, O., Le Roux, N., Paiement, J.-F., Vincent, P., and Ouimet, M. (2004). Learning eigenfunctions links spectral embedding and kernel PCA. Neural Computation, 16(10):2197–2219. Boser, B., Guyon, I., and Vapnik, V. (1992). A training algorithm for optimal margin classiﬁers. In Fifth Annual Workshop on Computational Learning Theory, pages 144– 152, Pittsburgh. Brand, M. (2003). Charting a manifold. In Becker, S., Thrun, S., and Obermayer, K., editors, Advances in Neural Information Processing Systems 15. MIT Press. Cortes, C. and Vapnik, V. (1995). Support vector networks. Machine Learning, 20:273– 297. Delalleau, O., Bengio, Y., and Le Roux, N. (2005). Efﬁcient non-parametric function induction in semi-supervised learning. In Cowell, R. and Ghahramani, Z., editors, Proceedings of the Tenth International Workshop on Artiﬁcial Intelligence and Statistics, Jan 6-8, 2005, Savannah Hotel, Barbados, pages 96–103. Society for Artiﬁcial Intelligence and Statistics. Duda, R. and Hart, P. (1973). Pattern Classiﬁcation and Scene Analysis. Wiley, New York. H¨ rdle, W., M¨ ller, M., Sperlich, S., and Werwatz, A. (2004). Nonparametric and Semia u parametric Models. Springer, http://www.xplore-stat.de/ebooks/ebooks.html. Micchelli, C. A. (1986). Interpolation of scattered data: distance matrices and conditionally positive deﬁnite functions. Constructive Approximation, 2:11–22. Roweis, S. and Saul, L. (2000). Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326. Schmitt, M. (2002). Descartes’ rule of signs for radial basis function neural networks. Neural Computation, 14(12):2997–3011. Sch¨ lkopf, B., Burges, C. J. C., and Smola, A. J. (1999). Advances in Kernel Methods — o Support Vector Learning. MIT Press, Cambridge, MA. Sch¨ lkopf, B., Smola, A., and M¨ ller, K.-R. (1998). Nonlinear component analysis as a o u kernel eigenvalue problem. Neural Computation, 10:1299–1319. Snapp, R. R. and Venkatesh, S. S. (1998). Asymptotic derivation of the ﬁnite-sample risk of the k nearest neighbor classiﬁer. Technical Report UVM-CS-1998-0101, Department of Computer Science, University of Vermont. Tenenbaum, J., de Silva, V., and Langford, J. (2000). A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323. Weiss, Y. (1999). Segmentation using eigenvectors: a unifying view. In Proceedings IEEE International Conference on Computer Vision, pages 975–982. Williams, C. and Rasmussen, C. (1996). Gaussian processes for regression. In Touretzky, D., Mozer, M., and Hasselmo, M., editors, Advances in Neural Information Processing Systems 8, pages 514–520. MIT Press, Cambridge, MA. ¨ Zhou, D., Bousquet, O., Navin Lal, T., Weston, J., and Scholkopf, B. (2004). Learning ¨ with local and global consistency. In Thrun, S., Saul, L., and Scholkopf, B., editors, Advances in Neural Information Processing Systems 16, Cambridge, MA. MIT Press. Zhu, X., Ghahramani, Z., and Lafferty, J. (2003). Semi-supervised learning using Gaussian ﬁelds and harmonic functions. In ICML’2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
