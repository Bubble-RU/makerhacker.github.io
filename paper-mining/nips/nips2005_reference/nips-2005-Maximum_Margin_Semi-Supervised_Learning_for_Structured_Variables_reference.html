<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>123 nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-123" href="../nips2005/nips-2005-Maximum_Margin_Semi-Supervised_Learning_for_Structured_Variables.html">nips2005-123</a> <a title="nips-2005-123-reference" href="#">nips2005-123-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>123 nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</h1>
<br/><p>Source: <a title="nips-2005-123-pdf" href="http://papers.nips.cc/paper/2850-maximum-margin-semi-supervised-learning-for-structured-variables.pdf">pdf</a></p><p>Author: Y. Altun, D. McAllester, M. Belkin</p><p>Abstract: Many real-world classiﬁcation problems involve the prediction of multiple inter-dependent variables forming some structural dependency. Recent progress in machine learning has mainly focused on supervised classiﬁcation of such structured variables. In this paper, we investigate structured classiﬁcation in a semi-supervised setting. We present a discriminative approach that utilizes the intrinsic geometry of input patterns revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables. Unlike transductive algorithms, our formulation naturally extends to new test points. 1</p><br/>
<h2>reference text</h2><p>[1] Y. Altun, T. Hofmann, and A. Smola. Gaussian process classiﬁcation for segmenting and annotating sequences. In ICML, 2004.</p>
<p>[2] Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden markov support vector machines. In ICML, 2003.</p>
<p>[3] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: a geometric framework for learning from examples. Technical Report 06, UChicago CS, 2004.</p>
<p>[4] Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with cotraining. In COLT, 1998.</p>
<p>[5] U. Brefeld, C. B¨scher, and T. Scheﬀer. Multi-view discriminative sequential learning. u In (ECML), 2005.</p>
<p>[6] O. Chappelle, J. Weston, and B. Scholkopf. Cluster kernels for semi-supervised learning. In (NIPS), 2002.</p>
<p>[7] M. Collins and N.l Duﬀy. Convolution kernels for natural language. In (NIPS), 2001.</p>
<p>[8] Olivier Delalleau, Yoshua Bengio, and Nicolas Le Roux. Eﬃcient non-parametric function induction in semi-supervised learning. In Proceedings of AISTAT, 2005.</p>
<p>[9] Thorsten Joachims. Transductive inference for text classiﬁcation using support vector machines. In (ICML), pages 200–209, 1999.</p>
<p>[10] G. Kimeldorf and G. Wahba. Some results on tchebychean spline functions. Journal of Mathematics Analysis and Applications, 33:82–95, 1971.</p>
<p>[11] John Laﬀerty, Yan Liu, and Xiaojin Zhu. Kernel conditional random ﬁelds: Representation, clique selection, and semi-supervised learning. In (ICML), 2004.</p>
<p>[12] K. Nigam, A. K. McCallum, S. Thrun, and T. M. Mitchell. Learning to classify text from labeled and unlabeled documents. In Proceedings of AAAI-98, pages 792–799, Madison, US, 1998.</p>
<p>[13] V. Sindhwani, P. Niyogi, and M. Belkin. Beyond the point cloud: from transductive to semi-supervised learning. In (ICML), 2005.</p>
<p>[14] B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks. In NIPS, 2004.</p>
<p>[15] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interdependent and structured output spaces. In (ICML), 2004.</p>
<p>[16] T. Zhang. personal communication.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
