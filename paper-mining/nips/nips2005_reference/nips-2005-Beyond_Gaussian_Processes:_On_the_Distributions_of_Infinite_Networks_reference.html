<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>38 nips-2005-Beyond Gaussian Processes: On the Distributions of Infinite Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-38" href="../nips2005/nips-2005-Beyond_Gaussian_Processes%3A_On_the_Distributions_of_Infinite_Networks.html">nips2005-38</a> <a title="nips-2005-38-reference" href="#">nips2005-38-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>38 nips-2005-Beyond Gaussian Processes: On the Distributions of Infinite Networks</h1>
<br/><p>Source: <a title="nips-2005-38-pdf" href="http://papers.nips.cc/paper/2869-beyond-gaussian-processes-on-the-distributions-of-infinite-networks.pdf">pdf</a></p><p>Author: Ricky Der, Daniel D. Lee</p><p>Abstract: A general analysis of the limiting distribution of neural network functions is performed, with emphasis on non-Gaussian limits. We show that with i.i.d. symmetric stable output weights, and more generally with weights distributed from the normal domain of attraction of a stable variable, that the neural functions converge in distribution to stable processes. Conditions are also investigated under which Gaussian limits do occur when the weights are independent but not identically distributed. Some particularly tractable classes of stable distributions are examined, and the possibility of learning with such processes.</p><br/>
<h2>reference text</h2><p>[1] R. Neal, Bayesian Learning for Neural Networks. New York: Springer-Verlag, 1996.</p>
<p>[2] D. MacKay. Introduction to Gaussian Processes. Extended lecture notes, NIPS 1997.</p>
<p>[3] M. Seeger, Gaussian Processes for Machine Learning. International Journal of Neural Systems 14(2), 2004, 69â€“106.</p>
<p>[4] C. Burrill, Measure, Integration and Probability. New York: McGraw-Hill, 1972.</p>
<p>[5] G. Samorodnitsky & M. Taqqu, Stable Non-Gaussian Random Processes. New York: Chapman & Hall, 1994.</p>
<p>[6] W. Feller, An Introduction to Probability Theory and Its Applications, Vol. 2. New York: John Wiley & Sons, 1966.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
