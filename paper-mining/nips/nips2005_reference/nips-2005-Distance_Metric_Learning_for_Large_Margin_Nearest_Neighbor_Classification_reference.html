<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-57" href="../nips2005/nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">nips2005-57</a> <a title="nips-2005-57-reference" href="#">nips2005-57-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</h1>
<br/><p>Source: <a title="nips-2005-57-pdf" href="http://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf">pdf</a></p><p>Author: Kilian Q. Weinberger, John Blitzer, Lawrence K. Saul</p><p>Abstract: We show how to learn a Mahanalobis distance metric for k-nearest neighbor (kNN) classiﬁcation by semideﬁnite programming. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. On seven data sets of varying size and difﬁculty, we ﬁnd that metrics trained in this way lead to signiﬁcant improvements in kNN classiﬁcation—for example, achieving a test error rate of 1.3% on the MNIST handwritten digits. As in support vector machines (SVMs), the learning problem reduces to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our framework requires no modiﬁcation or extension for problems in multiway (as opposed to binary) classiﬁcation. 1</p><br/>
<h2>reference text</h2><p>[1] S. Belongie, J. Malik, and J. Puzicha. Shape matching and object recognition using shape contexts. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 24(4):509– 522, 2002.</p>
<p>[2] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similiarty metric discriminatively, with application to face veriﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR-05), San Diego, CA, 2005.</p>
<p>[3] T. Cover and P. Hart. Nearest neighbor pattern classiﬁcation. In IEEE Transactions in Information Theory, IT-13, pages 21–27, 1967.</p>
<p>[4] K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. Journal of Machine Learning Research, 2:265–292, 2001.</p>
<p>[5] T. G. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting output codes. In Journal of Artiﬁcial Intelligence Research, number 2 in 263-286, 1995.</p>
<p>[6] C. Domeniconi, D. Gunopulos, and J. Peng. Large margin nearest neighbor classiﬁers. IEEE Transactions on Neural Networks, 16(4):899–909, 2005.</p>
<p>[7] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 513–520, Cambridge, MA, 2005. MIT Press.</p>
<p>[8] T. Hastie and R. Tibshirani. Discriminant adaptive nearest neighbor classiﬁcation. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 18:607–616, 1996.</p>
<p>[9] Y. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes, J. Denker, H. Drucker, I. Guyon, U. Muller, E. Sackinger, P. Simard, and V. Vapnik. A comparison of learning algorithms for handwritten digit recognition. In F.Fogelman and P.Gallinari, editors, Proceedings of the 1995 International Conference on Artiﬁcial Neural Networks (ICANN-95), pages 53–60, Paris, 1995.</p>
<p>[10] A. K. McCallum. Bow: A toolkit for statistical language modeling, text retrieval, classiﬁcation and clustering. http://www.cs.cmu.edu/ mccallum/bow, 1996.</p>
<p>[11] B. Sch¨ lkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regularizao tion, Optimization, and Beyond. MIT Press, Cambridge, MA, 2002.</p>
<p>[12] S. Shalev-Shwartz, Y. Singer, and A. Y. Ng. Online and batch learning of pseudo-metrics. In Proceedings of the 21st International Conference on Machine Learning, Banff, Canada, 2004.</p>
<p>[13] N. Shental, T. Hertz, D. Weinshall, and M. Pavel. Adjustment learning and relevant component analysis. In Proceedings of the Seventh European Conference on Computer Vision (ECCV-02), volume 4, pages 776–792, London, UK, 2002. Springer-Verlag.</p>
<p>[14] P. Y. Simard, Y. LeCun, and J. Decker. Efﬁcient pattern recognition using a new transformation distance. In Advances in Neural Information Processing Systems, volume 6, pages 50–58, San Mateo, CA, 1993. Morgan Kaufman.</p>
<p>[15] M. Turk and A. Pentland. Eigenfaces for recognition. Journal of Cognitive Neuroscience, 3(1):71–86, 1991.</p>
<p>[16] L. Vandenberghe and S. P. Boyd. Semideﬁnite programming. SIAM Review, 38(1):49–95, March 1996.</p>
<p>[17] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Distance metric learning, with application to clustering with side-information. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, Cambridge, MA, 2002. MIT Press.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
