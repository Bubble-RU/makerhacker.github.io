<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>53 nips-2005-Cyclic Equilibria in Markov Games</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-53" href="../nips2005/nips-2005-Cyclic_Equilibria_in_Markov_Games.html">nips2005-53</a> <a title="nips-2005-53-reference" href="#">nips2005-53-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>53 nips-2005-Cyclic Equilibria in Markov Games</h1>
<br/><p>Source: <a title="nips-2005-53-pdf" href="http://papers.nips.cc/paper/2834-cyclic-equilibria-in-markov-games.pdf">pdf</a></p><p>Author: Martin Zinkevich, Amy Greenwald, Michael L. Littman</p><p>Abstract: Although variants of value iteration have been proposed for ﬁnding Nash or correlated equilibria in general-sum Markov games, these variants have not been shown to be effective in general. In this paper, we demonstrate by construction that existing variants of value iteration cannot ﬁnd stationary equilibrium policies in arbitrary general-sum Markov games. Instead, we propose an alternative interpretation of the output of value iteration based on a new (non-stationary) equilibrium concept that we call “cyclic equilibria.” We prove that value iteration identiﬁes cyclic equilibria in a class of games in which it fails to ﬁnd stationary equilibria. We also demonstrate empirically that value iteration ﬁnds cyclic equilibria in nearly all examples drawn from a random distribution of Markov games. 1</p><br/>
<h2>reference text</h2><p>Bellman, R. (1957). Dynamic programming. Princeton, NJ: Princeton University Press. Brafman, R. I., & Tennenholtz, M. (2002). R-MAX—a general polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning Research, 3, 213–231. Greenwald, A., & Hall, K. (2003). Correlated Q-learning. Proceedings of the Twentieth International Conference on Machine Learning (pp. 242–249). Hu, J., & Wellman, M. (1998). Multiagent reinforcement learning:theoretical framework and an algorithm. Proceedings of the Fifteenth International Conference on Machine Learning (pp. 242–250). Morgan Kaufman. Littman, M. (2001). Friend-or-foe Q-learning in general-sum games. Proceedings of the Eighteenth International Conference on Machine Learning (pp. 322–328). Morgan Kaufmann. Littman, M. L., & Szepesv´ ri, C. (1996). A generalized reinforcement-learning model: a Convergence and applications. Proceedings of the Thirteenth International Conference on Machine Learning (pp. 310–318). Osborne, M. J., & Rubinstein, A. (1994). A Course in Game Theory. The MIT Press. Puterman, M. (1994). Markov decision processes: Discrete stochastic dynamic programming. Wiley-Interscience. Shapley, L. (1953). Stochastic games. Proceedings of the National Academy of Sciences of the United States of America, 39, 1095–1100. Tesauro, G., & Kephart, J. (1999). Pricing in agent economies using multi-agent Qlearning. Proceedings of Fifth European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty (pp. 71–86).</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
