<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>201 nips-2005-Variational Bayesian Stochastic Complexity of Mixture Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-201" href="../nips2005/nips-2005-Variational_Bayesian_Stochastic_Complexity_of_Mixture_Models.html">nips2005-201</a> <a title="nips-2005-201-reference" href="#">nips2005-201-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>201 nips-2005-Variational Bayesian Stochastic Complexity of Mixture Models</h1>
<br/><p>Source: <a title="nips-2005-201-pdf" href="http://papers.nips.cc/paper/2935-variational-bayesian-stochastic-complexity-of-mixture-models.pdf">pdf</a></p><p>Author: Kazuho Watanabe, Sumio Watanabe</p><p>Abstract: The Variational Bayesian framework has been widely used to approximate the Bayesian learning. In various applications, it has provided computational tractability and good generalization performance. In this paper, we discuss the Variational Bayesian learning of the mixture of exponential families and provide some additional theoretical support by deriving the asymptotic form of the stochastic complexity. The stochastic complexity, which corresponds to the minimum free energy and a lower bound of the marginal likelihood, is a key quantity for model selection. It also enables us to discuss the eﬀect of hyperparameters and the accuracy of the Variational Bayesian approach as an approximation of the true Bayesian learning. 1</p><br/>
<h2>reference text</h2><p>[1] H.Akaike, “Likelihood and Bayes procedure,” Bayesian Statistics, (Bernald J.M. eds.) University Press, Valencia, Spain, pp.143-166, 1980.</p>
<p>[2] H.Attias, ”Inferring parameters and structure of latent variable models by variational bayes,” Proc. of UAI, 1999.</p>
<p>[3] L.D.Brown, “Fundamentals of statistical exponential families,” IMS Lecture NotesMonograph Series, 1986.</p>
<p>[4] Z.Ghahramani, M.J.Beal, “Graphical models and variational methods,” Advanced Mean Field Methods , MIT Press, 2000.</p>
<p>[5] J.A.Hartigan, “A Failure of likelihood asymptotics for normal mixtures,” Proc. of the Berkeley Conference in Honor of J.Neyman and J.Kiefer, Vol.2, 807-810, 1985.</p>
<p>[6] D.J. Mackay, “Bayesian interpolation,” Neural Computation, 4(2), pp.415-447, 1992.</p>
<p>[7] G.McLachlan, D.Peel,”Finite mixture models,” Wiley, 2000.</p>
<p>[8] M.Sato, “Online model selection based on the variational bayes,” Neural Computation, 13(7), pp.1649-1681, 2001.</p>
<p>[9] G.Schwarz, “Estimating the dimension of a model,” Annals of Statistics, 6(2), pp.461464, 1978.</p>
<p>[10] K.Watanabe, S.Watanabe, ”Lower bounds of stochastic complexities in variational bayes learning of gaussian mixture models,” Proc. of IEEE CIS04, pp.99-104, 2004.</p>
<p>[11] K.Watanabe, S.Watanabe, ”Stochastic complexity for mixture of exponential families in variational bayes,” Proc. of ALT05, pp.107-121, 2005.</p>
<p>[12] S.Watanabe,“Algebraic analysis for non-identiﬁable learning machines,” Neural Computation, 13(4), pp.899-933, 2001.</p>
<p>[13] K.Yamazaki, S.Watanabe, ”Singularities in mixture models and upper bounds of stochastic complexity,” Neural Networks, 16, pp.1029-1038, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
