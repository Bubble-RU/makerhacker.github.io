<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>50 nips-2005-Convex Neural Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-50" href="../nips2005/nips-2005-Convex_Neural_Networks.html">nips2005-50</a> <a title="nips-2005-50-reference" href="#">nips2005-50-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>50 nips-2005-Convex Neural Networks</h1>
<br/><p>Source: <a title="nips-2005-50-pdf" href="http://papers.nips.cc/paper/2800-convex-neural-networks.pdf">pdf</a></p><p>Author: Yoshua Bengio, Nicolas L. Roux, Pascal Vincent, Olivier Delalleau, Patrice Marcotte</p><p>Abstract: Convexity has recently received a lot of attention in the machine learning community, and the lack of convexity has been seen as a major disadvantage of many learning algorithms, such as multi-layer artiﬁcial neural networks. We show that training multi-layer neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem. This problem involves an inﬁnite number of variables, but can be solved by incrementally inserting a hidden unit at a time, each time ﬁnding a linear classiﬁer that minimizes a weighted sum of errors. 1</p><br/>
<h2>reference text</h2><p>Chv´ V. (1983). Linear Programming. W.H. Freeman. atal, Delalleau, O., Bengio, Y., and Le Roux, N. (2005). Efﬁcient non-parametric function induction in semi-supervised learning. In Cowell, R. and Ghahramani, Z., editors, Proceedings of AISTATS’2005, pages 96–103. Freund, Y. and Schapire, R. E. (1997). A decision theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Science, 55(1):119–139. Friedman, J. (2001). Greedy function approximation: a gradient boosting machine. Annals of Statistics, 29:1180. Hettich, R. and Kortanek, K. (1993). Semi-inﬁnite programming: theory, methods, and applications. SIAM Review, 35(3):380–429. Marcotte, P. and Savard, G. (1992). Novel approaches to the discrimination problem. Zeitschrift fr Operations Research (Theory), 36:517–545. Mason, L., Baxter, J., Bartlett, P. L., and Frean, M. (2000). Boosting algorithms as gradient descent. In Advances in Neural Information Processing Systems 12, pages 512–518. R¨ tsch, G., Demiriz, A., and Bennett, K. P. (2002). Sparse regression ensembles in inﬁnite and ﬁnite a hypothesis spaces. Machine Learning. Rumelhart, D., Hinton, G., and Williams, R. (1986). Learning representations by back-propagating errors. Nature, 323:533–536.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
