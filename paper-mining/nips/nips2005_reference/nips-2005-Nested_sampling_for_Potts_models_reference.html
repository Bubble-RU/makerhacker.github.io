<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>133 nips-2005-Nested sampling for Potts models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-133" href="../nips2005/nips-2005-Nested_sampling_for_Potts_models.html">nips2005-133</a> <a title="nips-2005-133-reference" href="#">nips2005-133-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>133 nips-2005-Nested sampling for Potts models</h1>
<br/><p>Source: <a title="nips-2005-133-pdf" href="http://papers.nips.cc/paper/2753-nested-sampling-for-potts-models.pdf">pdf</a></p><p>Author: Iain Murray, David MacKay, Zoubin Ghahramani, John Skilling</p><p>Abstract: Nested sampling is a new Monte Carlo method by Skilling [1] intended for general Bayesian computation. Nested sampling provides a robust alternative to annealing-based methods for computing normalizing constants. It can also generate estimates of other quantities such as posterior expectations. The key technical requirement is an ability to draw samples uniformly from the prior subject to a constraint on the likelihood. We provide a demonstration with the Potts model, an undirected graphical model. 1</p><br/>
<h2>reference text</h2><p>[1] John Skilling. Nested sampling. In R. Fischer, R. Preuss, and U. von Toussaint, editors, Bayesian inference and maximum entropy methods in science and engineering, AIP Conference Proceeedings 735, pages 395–405, 2004.</p>
<p>[2] Andrew Gelman and Xiao-Li Meng. Simulating normalizing constants: from importance sampling to bridge sampling to path sampling. Statist. Sci., 13(2):163–185, 1998.</p>
<p>[3] Matthew J. Beal and Zoubin Ghahramani. The variational Bayesian EM algorithm for incomplete data: with application to scoring graphical model structures. Bayesian Statistics, 7:453–464, 2003.</p>
<p>[4] I. R. McDonald and K. Singer. Machine calculation of thermodynamic properties of a simple ﬂuid at supercritical temperatures. J. Chem. Phys., 47(11):4766–4772, 1967.</p>
<p>[5] David J.C. MacKay. Information Theory, Inference, and Learning Algorithms. CUP, 2003. www.inference.phy.cam.ac.uk/mackay/itila/.</p>
<p>[6] Robert G. Edwards and Alan D. Sokal. Generalization of the Fortuin-KasteleynSwendsen-Wang representation and Monte Carlo algorithm. Phys.Rev. D, 38(6), 1988.</p>
<p>[7] C. M. Fortuin and P. W. Kasteleyn. On the random-cluster model. I. Introduction and relation to other models. Physica, 57:536–564, 1972.</p>
<p>[8] R. H. Swendsen and J. S. Wang. Nonuniversal critical dynamics in Monte Carlo simulations. Phys. Rev. Lett., 58(2):86–88, January 1987.</p>
<p>[9] Radford M. Neal. Annealed importance sampling. Statistics and Computing, 11: 125–139, 2001.</p>
<p>[10] Charles H. Bennett. Eﬃcient estimation of free energy diﬀerences from Monte Carlo data. Journal of Computational Physics, 22(2):245–268, October 1976.</p>
<p>[11] Vivek K. Gore and Mark R. Jerrum. The Swendsen-Wang process does not always mix rapidly. In 29th ACM Symposium on Theory of Computing, pages 674–681, 1997.</p>
<p>[12] Bernd A. Berg and Thomas Neuhaus. Multicanonical ensemble: A new approach to simulate ﬁrst-order phase transitions. Phys. Rev. Lett., 68(1):9–12, January 1992.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
