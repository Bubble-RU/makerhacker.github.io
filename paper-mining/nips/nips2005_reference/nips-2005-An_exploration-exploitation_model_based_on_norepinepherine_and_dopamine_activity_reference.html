<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>26 nips-2005-An exploration-exploitation model based on norepinepherine and dopamine activity</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-26" href="../nips2005/nips-2005-An_exploration-exploitation_model_based_on_norepinepherine_and_dopamine_activity.html">nips2005-26</a> <a title="nips-2005-26-reference" href="#">nips2005-26-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>26 nips-2005-An exploration-exploitation model based on norepinepherine and dopamine activity</h1>
<br/><p>Source: <a title="nips-2005-26-pdf" href="http://papers.nips.cc/paper/2950-an-exploration-exploitation-model-based-on-norepinepherine-and-dopamine-activity.pdf">pdf</a></p><p>Author: Samuel M. McClure, Mark S. Gilzenrat, Jonathan D. Cohen</p><p>Abstract: We propose a model by which dopamine (DA) and norepinepherine (NE) combine to alternate behavior between relatively exploratory and exploitative modes. The model is developed for a target detection task for which there is extant single neuron recording data available from locus coeruleus (LC) NE neurons. An exploration-exploitation trade-off is elicited by regularly switching which of the two stimuli are rewarded. DA functions within the model to change synaptic weights according to a reinforcement learning algorithm. Exploration is mediated by the state of LC firing, with higher tonic and lower phasic activity producing greater response variability. The opposite state of LC function, with lower baseline firing rate and greater phasic responses, favors exploitative behavior. Changes in LC firing mode result from combined measures of response conflict and reward rate, where response conflict is monitored using models of anterior cingulate cortex (ACC). Increased long-term response conflict and decreased reward rate, which occurs following reward contingency switch, favors the higher tonic state of LC function and NE release. This increases exploration, and facilitates discovery of the new target. 1 In t rod u ct i on A central problem in reinforcement learning is determining how to adaptively move between exploitative and exploratory behaviors in changing environments. We propose a set of neurophysiologic mechanisms whose interaction may mediate this behavioral shift. Empirical work on the midbrain dopamine (DA) system has suggested that this system is particularly well suited for guiding exploitative behaviors. This hypothesis has been reified by a number of studies showing that a temporal difference (TD) learning algorithm accounts for activity in these neurons in a wide variety of behavioral tasks [1,2]. DA release is believed to encode a reward prediction error signal that acts to change synaptic weights relevant for producing behaviors [3]. Through learning, this allows neural pathways to predict future expected reward through the relative strength of their synaptic connections [1]. Decision-making procedures based on these value estimates are necessarily greedy. Including reward bonuses for exploratory choices supports non-greedy actions [4] and accounts for additional data derived from DA neurons [5]. We show that combining a DA learning algorithm with models of response conflict detection [6] and NE function [7] produces an effective annealing procedure for alternating between exploration and exploitation. NE neurons within the LC alternate between two firing modes [8]. In the first mode, known as the phasic mode, NE neurons fire at a low baseline rate but have relatively robust phasic responses to behaviorally salient stimuli. The second mode, called the tonic mode, is associated with a higher baseline firing and absent or attenuated phasic responses. The effects of NE on efferent areas are modulatory in nature, and are well captured as a change in the gain of efferent inputs so that neuronal responses are potentiated in the presence of NE [9]. Thus, in phasic mode, the LC provides transient facilitation in processing, time-locked to the presence of behaviorally salient information in motor or decision areas. Conversely, in tonic mode, higher overall LC discharge rate increases gain generally and hence increases the probability of arbitrary responding. Consistent with this account, for periods when NE neurons are in the phasic mode, monkey performance is nearly perfect. However, when NE neurons are in the tonic mode, performance is more erratic, with increased response times and error rate [8]. These findings have led to a recent characterization of the LC as a dynamic temporal filter, adjusting the system's relative responsivity to salient and irrelevant information [8]. In this way, the LC is ideally positioned to mediate the shift between exploitative and exploratory behavior. The parameters that underlie changes in LC firing mode remain largely unexplored. Based on data from a target detection task by Aston-Jones and colleagues [10], we propose that LC firing mode is determined in part by measures of response conflict and reward rate as calculated by the ACC and OFC, respectively [8]. Together, the ACC and OFC are the principle sources of cortical input to the LC [8]. Activity in the ACC is known, largely through human neuroimaging experiments, to change in accord with response conflict [6]. In brief, relatively equal activity in competing behavioral responses (reflecting uncertainty) produces high conflict. Low conflict results when one behavioral response predominates. We propose that increased long-term response conflict biases the LC towards a tonic firing mode. Increased conflict necessarily follows changes in reward contingency. As the previously rewarded target no longer produces reward, there will be a relative increase in response ambiguity and hence conflict. This relationship between conflict and LC firing is analogous to other modeling work [11], which proposes that increased tonic firing reflects increased environmental uncertainty. As a final component to our model, we hypothesize that the OFC maintains an ongoing estimate in reward rate, and that this estimate of reward rate also influences LC firing mode. As reward rate increases, we assume that the OFC tends to bias the LC in favor of phasic firing to target stimuli. We have aimed to fix model parameters based on previous work using simpler networks. We use parameters derived primarily from a previous model of the LC by Gilzenrat and colleagues [7]. Integration of response conflict by the ACC and its influence on LC firing was borrowed from unpublished work by Gilzenrat and colleagues in which they fit human behavioral data in a diminishing utilities task. Given this approach, we interpret our observed improvement in model performance with combined NE and DA function as validation of a mechanism for automatically switching between exploitative and exploratory action selection. 2 G o- No- G o Task and Core Mod el We have modeled an experiment in which monkeys performed a target detection task [10]. In the task, monkeys were shown either a vertical bar or a horizontal bar and were required to make or omit a motor response appropriately. Initially, the vertical bar was the target stimulus and correctly responding was rewarded with a squirt of fruit juice (r=1 in the model). Responding to the non-target horizontal stimulus resulted in time out punishment (r=-.1; Figure 1A). No responses to either the target or non-target gave zero reward. After the monkeys had fully acquired the task, the experimenters periodically switched the reward contingency such that the previously rewarded stimulus (target) became the distractor, and vice versa. Following such reversals, LC neurons were observed to change from emitting phasic bursts of firing to the target, to tonic firing following the switch, and slowly back to phasic firing for the new target as the new response criteria was obtained [10]. Figure 1: Task and model design. (A) Responses were required for targets in order to obtain reward. Responses to distractors resulted in a minor punishment. No responses gave zero reward. (B) In the model, vertical and horizontal bar inputs (I1 and I 2 ) fed to integrator neurons (X1 and X2 ) which then drove response units (Y1 and Y2 ). Responses were made if Y 1 or Y2 crossed a threshold while input units were active. We have previously modeled this task [7,12] with a three-layer connectionist network in which two input units, I1 and I 2 , corresponding to the vertical and horizontal bars, drive two mutually inhibitory integrator units, X1 and X2 . The integrator units subsequently feed two response units, Y1 and Y2 (Figure 1B). Responses are made whenever output from Y1 or Y2 crosses a threshold level of activity, θ. Relatively weak cross connections from each input unit to the opposite integrator unit (I1 to X2 and I 2 to X1 ) are intended to model stimulus similarity. Both the integrator and response units were modeled as noisy, leaky accumulators: ˙ X i =</p><br/>
<h2>reference text</h2><p>[1] Montague, P.R. Dayan, P., Sejnowski, T.J. (1996) A framework for mesencephalic dopamine systems based on predictive Hebbian learning. J. Neurosci. 16: 1936-1947.</p>
<p>[2] Schultz, W. Dayan, P. & Montague, P.R. (1997) A neural substrate for prediction and reward. Science 275: 1593-1599.</p>
<p>[3] Reynolds, J.N., Hyland, B.I., Wickens, J.R. (2001) A cellular mechanism of rewardrelated learning. Nature 413: 67-70.</p>
<p>[4] Sutton, R.S. (1990) Integrated architectures for learning, planning, and reacting based on approximated dynamic programming. Mach. Learn., Proc. 7 th International Conf. 216-224.</p>
<p>[5] Kakade, S., Dayan, P. (2002) Dopamine: generalization and bonuses. Neural Networks 15: 549-559.</p>
<p>[6] Botvinick, M.M., Braver, T.S., Barch, D.M., Carter, C.S., Cohen, J.D. (2001) Conflict monitoring and cognitive control. Psychol. Rev. 108: 624-652.</p>
<p>[7] Gilzenrat, M.S., Holmes, B.D., Rajkowski, J., Aston-Jones, G., Cohen, J.D. (2002) Simplified dynamics in a model of noradrenergic modulation of cognitive performance. Neural Networks 15: 647-663.</p>
<p>[8] Aston-Jones, G., Cohen, J.D. (2005) An integrative theory of locus coeruleusnorepinepherine function. Ann. Rev. Neurosci. 28: 403-450.</p>
<p>[9] Servan-Schreiber, D., Printz, H., Cohen, J.D. (1990) A network model of catecholamine effects: gain, signal-to-noise ratio and behavior. Science 249: 892-895.</p>
<p>[10] Aston-Jones, G., Rajkowski, J., Kubiak, P. (1997) Conditioned responses of monkey locus coeruleus neurons anticipate acquisition of discriminative behavior in a vigilance task. Neuroscience 80: 697-715.</p>
<p>[11] Yu, A., Dayan, P. (2005) Uncertainty, neuromodulation and attention. Neuron 46: 68192.</p>
<p>[11] Usher, M., Cohen, J.D., Rajkowski, J., Aston-Jones, G. (1999) The role of the locus coeruleus in the regulation of cognitive performance. Science 283: 549-554.</p>
<p>[12] Schwartz, A. (1993) A reinforcement learning method for maximizing undiscounted rewards. In: Proc. 10th International Conf. Mach. Learn. (pp. 298-305). San Mateo, CA: Morgan Kaufmann.</p>
<p>[13] Daw, N.D., Touretzky, D.S. (2002) Long-term reward prediction in TD models of the dopamine system. Neural Computation 14: 2567-2583.</p>
<p>[14] Goldberg, T.E., Weinberger, D.R., Berman, K.F., Pliskin, N.H., Podd, M.H. (1987) Further evidence for dementia of the prefrontal type in schizophrenia? A controlled study teaching the Wisconsin Card Sorting Test. Arch. Gen. Psychiatry 44: 1008-1014.</p>
<p>[15] Barkley, R.A. (1997) Behavioural inhibition, sustained attention, and executive functions: constructing a unified theory of AD/HD. Psychol. Bull. 121: 65-94.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
