<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-72" href="../nips2005/nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">nips2005-72</a> <a title="nips-2005-72-reference" href="#">nips2005-72-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</h1>
<br/><p>Source: <a title="nips-2005-72-pdf" href="http://papers.nips.cc/paper/2825-fast-online-policy-gradient-learning-with-smd-gain-vector-adaptation.pdf">pdf</a></p><p>Author: Jin Yu, Douglas Aberdeen, Nicol N. Schraudolph</p><p>Abstract: Reinforcement learning by direct policy gradient estimation is attractive in theory but in practice leads to notoriously ill-behaved optimization problems. We improve its robustness and speed of convergence with stochastic meta-descent, a gain vector adaptation method that employs fast Hessian-vector products. In our experiments the resulting algorithms outperform previously employed online stochastic, ofﬂine conjugate, and natural policy gradient methods. 1</p><br/>
<h2>reference text</h2><p>[1] J. Baxter and P. L. Bartlett. Inﬁnite-horizon policy-gradient estimation. Journal of Artiﬁcial Intelligence Research, 15:319–350, 2001.</p>
<p>[2] J. Baxter, P. L. Bartlett, and L. Weaver. Experiments with inﬁnite-horizon, policy-gradient estimation. Journal of Artiﬁcial Intelligence Research, 15:351–381, 2001.</p>
<p>[3] N. N. Schraudolph. Local gain adaptation in stochastic gradient descent. In Proc. Intl. Conf. Artiﬁcial Neural Networks, pages 569–574, Edinburgh, Scotland, 1999. IEE, London.</p>
<p>[4] N. N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. Neural Computation, 14(7):1723–1738, 2002.</p>
<p>[5] R. Jacobs. Increased rates of convergence through learning rate adaptation. Neural Networks, 1:295–307, 1988.</p>
<p>[6] J. Kivinen and M. K. Warmuth. Additive versus exponentiated gradient updates for linear prediction. In Proc. 27th Annual ACM Symposium on Theory of Computing, pages 209–218. ACM Press, New York, NY, 1995.</p>
<p>[7] T. Tollenaere. SuperSAB: Fast adaptive back propagation with good scaling properties. Neural Networks, 3:561–573, 1990.</p>
<p>[8] F. M. Silva and L. B. Almeida. Acceleration techniques for the backpropagation algorithm. In L. B. Almeida and C. J. Wellekens, editors, Neural Networks: Proc. EURASIP Workshop, volume 412 of Lecture Notes in Computer Science, pages 110–119. Springer Verlag, 1990.</p>
<p>[9] M. Riedmiller and H. Braun. A direct adaptive method for faster backpropagation learning: The RPROP algorithm. In Proc. Intl. Conf. Neural Networks, pages 586–591. IEEE, 1993.</p>
<p>[10] L. B. Almeida, T. Langlois, J. D. Amaral, and A. Plakhov. Parameter adaptation in stochastic optimization. In D. Saad, editor, On-Line Learning in Neural Networks, Publications of the Newton Institute, chapter 6, pages 111–134. Cambridge University Press, 1999.</p>
<p>[11] R. S. Sutton. Gain adaptation beats least squares? In Proceedings of the 7th Yale Workshop on Adaptive and Learning Systems, pages 161–166, 1992.</p>
<p>[12] B. A. Pearlmutter. Fast exact multiplication by the Hessian. Neural Comput., 6(1):147–60, 1994.</p>
<p>[13] A. Griewank. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. Frontiers in Applied Mathematics. SIAM, Philadelphia, 2000.</p>
<p>[14] N. N. Schraudolph and T. Graepel. Combining conjugate direction methods with stochastic approximation of gradients. In C. M. Bishop and B. J. Frey, editors, Proc. 9th Intl. Workshop Artiﬁcial Intelligence and Statistics, pages 7–13, Key West, Florida, 2003.</p>
<p>[15] S. Kakade. A natural policy gradient. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 1531–1538. MIT Press, 2002.</p>
<p>[16] S. Amari. Natural gradient works efﬁciently in learning. Neural Comput., 10(2):251–276, 1998.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
