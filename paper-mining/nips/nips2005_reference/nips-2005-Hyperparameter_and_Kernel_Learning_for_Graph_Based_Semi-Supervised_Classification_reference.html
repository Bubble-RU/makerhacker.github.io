<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-92" href="../nips2005/nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">nips2005-92</a> <a title="nips-2005-92-reference" href="#">nips2005-92-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</h1>
<br/><p>Source: <a title="nips-2005-92-pdf" href="http://papers.nips.cc/paper/2932-hyperparameter-and-kernel-learning-for-graph-based-semi-supervised-classification.pdf">pdf</a></p><p>Author: Ashish Kapoor, Hyungil Ahn, Yuan Qi, Rosalind W. Picard</p><p>Abstract: There have been many graph-based approaches for semi-supervised classiﬁcation. One problem is that of hyperparameter learning: performance depends greatly on the hyperparameters of the similarity graph, transformation of the graph Laplacian and the noise model. We present a Bayesian framework for learning hyperparameters for graph-based semisupervised classiﬁcation. Given some labeled data, which can contain inaccurate labels, we pose the semi-supervised classiﬁcation as an inference problem over the unknown labels. Expectation Propagation is used for approximate inference and the mean of the posterior is used for classiﬁcation. The hyperparameters are learned using EM for evidence maximization. We also show that the posterior mean can be written in terms of the kernel matrix, providing a Bayesian classiﬁer to classify new points. Tests on synthetic and real datasets show cases where there are signiﬁcant improvements in performance over the existing approaches. 1</p><br/>
<h2>reference text</h2><p>[1] Csato, L. (2002) Gaussian processes-iterative sparse approximation. PhD Thesis, Aston Univ.</p>
<p>[2] Kim, H. & Ghahramani, Z. (2004) The EM-EP algorithm for Gaussian process classiﬁcation. ECML.</p>
<p>[3] Minka, T. P. (2001) Expectation propagation for approximate Bayesian inference. UAI.</p>
<p>[4] Opper, M. & Winther, O. (1999) Mean ﬁeld methods for classiﬁcation with Gaussian processes. NIPS.</p>
<p>[5] Smola, A. & Kondor, R. (2003) Kernels and regularization on graphs. COLT.</p>
<p>[6] Zhou et al. (2004) Learning with local and global consistency. NIPS.</p>
<p>[7] Zhu, X., Ghahramani, Z. & Lafferty, J. (2003) Semi-supervised learning using Gaussian ﬁelds and harmonic functions. ICML.</p>
<p>[8] Zhu, X., Kandola, J., Ghahramani, Z. & Lafferty, J. (2004) Nonparametric transforms of graph kernels for semi-supervised learning. NIPS.</p>
<p>[9] Zhu, X., Lafferty, J. & Ghahramani, Z. (2003) Semi-supervised learning: From Gaussian ﬁelds to Gaussian processes. CMU Tech Report:CMU-CS-03-175.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
