<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>153 nips-2005-Policy-Gradient Methods for Planning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-153" href="../nips2005/nips-2005-Policy-Gradient_Methods_for_Planning.html">nips2005-153</a> <a title="nips-2005-153-reference" href="#">nips2005-153-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>153 nips-2005-Policy-Gradient Methods for Planning</h1>
<br/><p>Source: <a title="nips-2005-153-pdf" href="http://papers.nips.cc/paper/2910-policy-gradient-methods-for-planning.pdf">pdf</a></p><p>Author: Douglas Aberdeen</p><p>Abstract: Probabilistic temporal planning attempts to ﬁnd good policies for acting in domains with concurrent durative tasks, multiple uncertain outcomes, and limited resources. These domains are typically modelled as Markov decision problems and solved using dynamic programming methods. This paper demonstrates the application of reinforcement learning — in the form of a policy-gradient method — to these domains. Our emphasis is large domains that are infeasible for dynamic programming. Our approach is to construct simple policies, or agents, for each planning task. The result is a general probabilistic temporal planner, named the Factored Policy-Gradient Planner (FPG-Planner), which can handle hundreds of tasks, optimising for probability of success, duration, and resource use. 1</p><br/>
<h2>reference text</h2><p>[1] L. Peshkin, K.-E. Kim, N. Meuleau, and L. P. Kaelbling. Learning to cooperate via policy search. In UAI, 2000.</p>
<p>[2] Nigel Tao, Jonathan Baxter, and Lex Weaver. A multi-agent, policy-gradient approach to network routing. In Proc. ICML’01. Morgan Kaufmann, 2001.</p>
<p>[3] J. Baxter, P. Bartlett, and L. Weaver. Experiments with inﬁnite-horizon, policy-gradient estimation. JAIR, 15:351–381, 2001.</p>
<p>[4] Mausam and Daniel S. Weld. Concurrent probabilistic temporal planning. In Proc. International Conference on Automated Planning and Scheduling, Moneteray, CA, June 2005. AAAI.</p>
<p>[5] I. Little, D. Aberdeen, and S. Thi´ baux. Prottle: A probabilistic temporal planner. In Proc. e AAAI’05, 2005.</p>
<p>[6] Hakan L. S. Younes and Reid G. Simmons. Policy generation for continuous-time stochastic domains with concurrency. In Proc. of ICAPS’04, volume 14, 2005.</p>
<p>[7] Douglas Aberdeen, Sylvie Thi´ baux, and Lin Zhang. Decision-theoretic military operations e planning. In Proc. ICAPS, volume 14, pages 402–411. AAAI, June 2004.</p>
<p>[8] A.G. Barto, S. Bradtke, and S. Singh. Learning to act using real-time dynamic programming. Artiﬁcial Intelligence, 72, 1995.</p>
<p>[9] A.Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Proc. ICML’99, 1999.</p>
<p>[10] Douglas Aberdeen. The factored policy-gradient planner. Technical report, NICTA, 2005.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
