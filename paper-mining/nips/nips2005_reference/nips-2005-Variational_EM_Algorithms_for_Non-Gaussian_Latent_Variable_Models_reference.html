<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>202 nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-202" href="../nips2005/nips-2005-Variational_EM_Algorithms_for_Non-Gaussian_Latent_Variable_Models.html">nips2005-202</a> <a title="nips-2005-202-reference" href="#">nips2005-202-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>202 nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</h1>
<br/><p>Source: <a title="nips-2005-202-pdf" href="http://papers.nips.cc/paper/2933-variational-em-algorithms-for-non-gaussian-latent-variable-models.pdf">pdf</a></p><p>Author: Jason Palmer, Kenneth Kreutz-Delgado, Bhaskar D. Rao, David P. Wipf</p><p>Abstract: We consider criteria for variational representations of non-Gaussian latent variables, and derive variational EM algorithms in general form. We establish a general equivalence among convex bounding methods, evidence based methods, and ensemble learning/Variational Bayes methods, which has previously been demonstrated only for particular cases.</p><br/>
<h2>reference text</h2><p>[1] D. F. Andrews and C. L. Mallows. Scale mixtures of normal distributions. J. Roy. Statist. Soc. Ser. B, 36:99–102, 1974.</p>
<p>[2] H. Attias. Independent factor analysis. Neural Computation, 11:803–851, 1999.</p>
<p>[3] H. Attias. A variational Bayesian framework for graphical models. In Advances in Neural Information Processing Systems 12. MIT Press, 2000.</p>
<p>[4] M. J. Beal and Z. Ghahrarmani. The variational Bayesian EM algorithm for incomplete data: with application to scoring graphical model structures. In Bayesian Statistics 7, pages 453–464. University of Oxford Press, 2002.</p>
<p>[5] A. Benveniste, M. M´ tivier, and P. Priouret. Adaptive algorithms and stochastic approximae tions. Springer-Verlag, 1990.</p>
<p>[6] C. M. Bishop and M. E. Tipping. Variational relevance vector machines. In C. Boutilier and M. Goldszmidt, editors, Proceedings of the 16th Conference on Uncertainty in Artiﬁcial Intelligence, pages 46–53. Morgan Kaufmann, 2000.</p>
<p>[7] S. Bochner. Harmonic analysis and the theory of probability. University of California Press, Berkeley and Los Angeles, 1960.</p>
<p>[8] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.</p>
<p>[9] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39:1–38, 1977.</p>
<p>[10] A. P. Dempster, N. M. Laird, and D. B. Rubin. Iteratively reweighted least squares for linear regression when errors are Normal/Independent distributed. In P. R. Krishnaiah, editor, Multivariate Analysis V, pages 35–57. North Holland Publishing Company, 1980.</p>
<p>[11] M. Figueiredo. Adaptive sparseness using Jeffreys prior. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, Cambridge, MA, 2002. MIT Press.</p>
<p>[12] D. Geman and G. Reynolds. Constrained restoration and the recovery of discontinuities. IEEE Trans. Pattern Analysis and Machine Intelligence, 14(3):367–383, 1992.</p>
<p>[13] Z. Ghahramani and M. J. Beal. Variational inference for Bayesian mixtures of factor analysers. In Advances in Neural Information Processing Systems 12. MIT Press, 2000.</p>
<p>[14] M. Girolami. A variational method for learning sparse and overcomplete representations. Neural Computation, 13:2517–2532, 2001.</p>
<p>[15] A. Honkela and H. Valpola. Unsupervised variational Bayesian learning of nonlinear models. In Advances in Neural Information Processing Systems 17. MIT Press, 2005.</p>
<p>[16] T. S. Jaakkola. Variational Methods for Inference and Estimation in Graphical Models. PhD thesis, Massachusetts Institute of Technology, 1997.</p>
<p>[17] T. S. Jaakkola and M. I. Jordan. A variational approach to Bayesian logistic regression models and their extensions. In Proceedings of the 1997 Conference on Artiﬁcial Intelligence and Statistics, 1997.</p>
<p>[18] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. In M. I. Jordan, editor, Learning in Graphical Models. Kluwer Academic Publishers, 1998.</p>
<p>[19] J. Keilson and F. W. Steutel. Mixtures of distributions, moment inequalities, and measures of exponentiality and Normality. The Annals of Probability, 2:112–130, 1974.</p>
<p>[20] H. Lappalainen. Ensemble learning for independent component analysis. In Proceedings of the First International Workshop on Independent Component Analysis, 1999.</p>
<p>[21] D. J. C. MacKay. Bayesian interpolation. Neural Computation, 4(3):415–447, 1992.</p>
<p>[22] D. J. C. MacKay. Ensemble learning and evidence maximization. Unpublished manuscript, 1995.</p>
<p>[23] D. J. C. Mackay. Comparison of approximate methods for handling hyperparameters. Neural Computation, 11(5):1035–1068, 1999.</p>
<p>[24] R. M. Neal and G. E. Hinton. A view of the EM algorithm that justiﬁes incremental, sparse, and other variants. In M. I. Jordan, editor, Learning in Graphical Models, pages 355–368. Kluwer, 1998.</p>
<p>[25] B. D. Rao, K. Engan, S. F. Cotter, J. Palmer, and K. Kreutz-Delgado. Subset selection in noise based on diversity measure minimization. IEEE Trans. Signal Processing, 51(3), 2003.</p>
<p>[26] B. D. Rao and I. F. Gorodnitsky. Sparse signal reconstruction from limited data using FOCUSS: a re-weighted minimum norm algorithm. IEEE Trans. Signal Processing, 45:600–616, 1997.</p>
<p>[27] R. T. Rockafellar. Convex Analysis. Princeton, 1970.</p>
<p>[28] Sam Roweis and Zoubin Ghahramani. A unifying review of linear gaussian models. Neural Computation, 11(5):305–345, 1999.</p>
<p>[29] L. K. Saul, T. S. Jaakkola, and M. I. Jordan. Mean ﬁeld theory for sigmoid belief networks. Journal of Artiﬁcial Intelligence Research, 4:61–76, 1996.</p>
<p>[30] M. E. Tipping. Sparse Bayesian learning and the Relevance Vector Machine. Journal of Machine Learning Research, 1:211–244, 2001.</p>
<p>[31] D. V. Widder. The Laplace Transform. Princeton University Press, 1946.</p>
<p>[32] D. Wipf, J. Palmer, and B. Rao. Perspectives on sparse bayesian learning. In S. Thrun, L. Saul, and B. Sch¨ lkopf, editors, Advances in Neural Information Processing Systems 16, Cambridge, o MA, 2003. MIT Press.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
