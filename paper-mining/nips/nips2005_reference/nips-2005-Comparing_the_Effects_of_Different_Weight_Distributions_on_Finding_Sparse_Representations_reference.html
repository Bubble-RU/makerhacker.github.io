<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-43" href="../nips2005/nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">nips2005-43</a> <a title="nips-2005-43-reference" href="#">nips2005-43-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</h1>
<br/><p>Source: <a title="nips-2005-43-pdf" href="http://papers.nips.cc/paper/2771-comparing-the-effects-of-different-weight-distributions-on-finding-sparse-representations.pdf">pdf</a></p><p>Author: Bhaskar D. Rao, David P. Wipf</p><p>Abstract: Given a redundant dictionary of basis vectors (or atoms), our goal is to ﬁnd maximally sparse representations of signals. Previously, we have argued that a sparse Bayesian learning (SBL) framework is particularly well-suited for this task, showing that it has far fewer local minima than other Bayesian-inspired strategies. In this paper, we provide further evidence for this claim by proving a restricted equivalence condition, based on the distribution of the nonzero generating model weights, whereby the SBL solution will equal the maximally sparse representation. We also prove that if these nonzero weights are drawn from an approximate Jeffreys prior, then with probability approaching one, our equivalence condition is satisﬁed. Finally, we motivate the worst-case scenario for SBL and demonstrate that it is still better than the most widely used sparse representation algorithms. These include Basis Pursuit (BP), which is based on a convex relaxation of the ℓ0 (quasi)-norm, and Orthogonal Matching Pursuit (OMP), a simple greedy strategy that iteratively selects basis vectors most aligned with the current residual. 1</p><br/>
<h2>reference text</h2><p>[1] D. Donoho and M. Elad, “Optimally sparse representation in general (nonorthogonal) dictionaries via ℓ1 minimization,” Proc. Nat. Acad. Sci., vol. 100, no. 5, pp. 2197–2202, March 2003.</p>
<p>[2] R. Gribonval and M. Nielsen, “Sparse representations in unions of bases,” IEEE Transactions on Information Theory, vol. 49, pp. 3320–3325, Dec. 2003.</p>
<p>[3] D. Donoho, “For most large underdetermined systems of linear equations the minimal ℓ1 -norm solution is also the sparsest solution,” Stanford University Technical Report, September 2004.</p>
<p>[4] J.J. Fuchs, “On sparse representations in arbitrary redundant bases,” IEEE Transactions on Information Theory, vol. 50, no. 6, pp. 1341–1344, June 2004.</p>
<p>[5] J.A. Tropp, “Greed is good: Algorithmic results for sparse approximation,” IEEE Transactions on Information Theory, vol. 50, no. 10, pp. 2231–2242, October 2004.</p>
<p>[6] M.E. Tipping, “Sparse Bayesian learning and the relevance vector machine,” Journal of Machine Learning Research, vol. 1, pp. 211–244, 2001.</p>
<p>[7] I.F. Gorodnitsky and B.D. Rao, “Sparse signal reconstruction from limited data using FOCUSS: A re-weighted minimum norm algorithm,” IEEE Transactions on Signal Processing, vol. 45, no. 3, pp. 600–616, March 1997.</p>
<p>[8] M.A.T. Figueiredo, “Adaptive sparseness using Jeffreys prior,” Advances in Neural Information Processing Systems 14, pp. 697–704, 2002.</p>
<p>[9] D.P. Wipf and B.D. Rao, “ℓ0 -norm minimization for basis selection,” Advances in Neural Information Processing Systems 17, pp. 1513–1520, 2005.</p>
<p>[10] D.P. Wipf and B.D. Rao, “Sparse Bayesian learning for basis selection,” IEEE Transactions on Signal Processing, vol. 52, no. 8, pp. 2153–2164, 2004.</p>
<p>[11] D.P. Wipf, To appear in Bayesian Methods for Sparse Signal Representation, PhD Dissertation, UC San Diego, 2006 (estimated). http://dsp.ucsd.edu/∼dwipf/</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
