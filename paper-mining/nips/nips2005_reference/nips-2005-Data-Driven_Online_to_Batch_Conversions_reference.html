<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>54 nips-2005-Data-Driven Online to Batch Conversions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-54" href="../nips2005/nips-2005-Data-Driven_Online_to_Batch_Conversions.html">nips2005-54</a> <a title="nips-2005-54-reference" href="#">nips2005-54-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>54 nips-2005-Data-Driven Online to Batch Conversions</h1>
<br/><p>Source: <a title="nips-2005-54-pdf" href="http://papers.nips.cc/paper/2775-data-driven-online-to-batch-conversions.pdf">pdf</a></p><p>Author: Ofer Dekel, Yoram Singer</p><p>Abstract: Online learning algorithms are typically fast, memory efﬁcient, and simple to implement. However, many common learning problems ﬁt more naturally in the batch learning setting. The power of online learning algorithms can be exploited in batch settings by using online-to-batch conversions techniques which build a new batch algorithm from an existing online algorithm. We ﬁrst give a uniﬁed overview of three existing online-to-batch conversion techniques which do not use training data in the conversion process. We then build upon these data-independent conversions to derive and analyze data-driven conversions. Our conversions ﬁnd hypotheses with a small risk by explicitly minimizing datadependent generalization bounds. We experimentally demonstrate the usefulness of our approach and in particular show that the data-driven conversions consistently outperform the data-independent conversions.</p><br/>
<h2>reference text</h2><p>[1] K. Azuma. Weighted sums of certain dependent random variables. Tohoku Mathematical Journal, 68:357–367, 1967.</p>
<p>[2] N. Cesa-Bianchi, A. Conconi, and C.Gentile. On the generalization ability of on-line learning algorithms. IEEE Transactions on Information Theory, 2004.</p>
<p>[3] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. Online passive aggressive algorithms. Journal of Machine Learning Research, 2006.</p>
<p>[4] K. Crammer, J. Kandola, and Y. Singer. Online classiﬁcation on a budget. NIPS 16, 2003.</p>
<p>[5] O. Dekel, S. Shalev-Shwartz, and Y. Singer. The Forgetron: A kernel-based perceptron on a ﬁxed budget. NIPS 18, 2005.</p>
<p>[6] Y. Freund and R. E. Schapire. Large margin classiﬁcation using the perceptron algorithm. Machine Learning, 37(3):277–296, 1999.</p>
<p>[7] S. I. Gallant. Optimal linear discriminants. ICPR 8, pages 849–852. IEEE, 1986.</p>
<p>[8] D. P. Helmbold and M. K. Warmuth. On weak learning. Journal of Computer and System Sciences, 50:551–573, 1995.</p>
<p>[9] Y. Li. Selective voting for perceptron-like on-line learning. In ICML 17, 2000.</p>
<p>[10] N. Littlestone. From on-line to batch learning. COLT 2, pages 269–284, July 1989.</p>
<p>[11] N. Littlestone and M. Warmuth. Relating data compression and learnability. Unpublished manuscript, November 1986.</p>
<p>[12] V. N. Vapnik. Statistical Learning Theory. Wiley, 1998.</p>
<p>[13] J. Weston, A. Bordes, and L. Bottou. Online (and ofﬂine) on a tighter budget. AISTAT 10, 2005.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
