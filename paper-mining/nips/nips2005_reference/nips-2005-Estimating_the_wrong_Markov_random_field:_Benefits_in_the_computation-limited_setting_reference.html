<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-65" href="../nips2005/nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">nips2005-65</a> <a title="nips-2005-65-reference" href="#">nips2005-65-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</h1>
<br/><p>Source: <a title="nips-2005-65-pdf" href="http://papers.nips.cc/paper/2925-estimating-the-wrong-markov-random-field-benefits-in-the-computation-limited-setting.pdf">pdf</a></p><p>Author: Martin J. Wainwright</p><p>Abstract: Consider the problem of joint parameter estimation and prediction in a Markov random ﬁeld: i.e., the model parameters are estimated on the basis of an initial set of data, and then the ﬁtted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working in the computation-limited setting, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for ﬁtting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the “wrong” model even in the inﬁnite data limit) is provably beneﬁcial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of variational methods. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product. 1 Keywords: Markov random ﬁelds; variational method; message-passing algorithms; sum-product; belief propagation; parameter estimation; learning. 1</p><br/>
<h2>reference text</h2><p>[1] D. Bertsekas. Nonlinear programming. Athena Scientiﬁc, Belmont, MA, 1995.</p>
<p>[2] M. Crouse, R. Nowak, and R. Baraniuk. Wavelet-based statistical signal processing using hidden Markov models. IEEE Trans. Signal Processing, 46:886–902, April 1998.</p>
<p>[3] A. Ihler, J. Fisher, and A. S. Willsky. Loopy belief propagation: Convergence and effects of message errors. Journal of Machine Learning Research, 6:905–936, May 2005.</p>
<p>[4] M. A. R. Leisink and H. J. Kappen. Learning in higher order Boltzmann machines using linear response. Neural Networks, 13:329–335, 2000.</p>
<p>[5] T. P. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, MIT, January 2001.</p>
<p>[6] S. Tatikonda and M. I. Jordan. Loopy belief propagation and Gibbs measures. In Proc. Uncertainty in Artiﬁcial Intelligence, volume 18, pages 493–500, August 2002.</p>
<p>[7] Y. W. Teh and M. Welling. On improving the efﬁciency of the iterative proportional ﬁtting procedure. In Workshop on Artiﬁcial Intelligence and Statistics, 2003.</p>
<p>[8] A. W. van der Vaart. Asymptotic statistics. Cambridge University Press, Cambridge, UK, 1998.</p>
<p>[9] M. J. Wainwright. Joint estimation and prediction in Markov random ﬁelds: Beneﬁts of inconsistency in the computation-limited regime. Technical Report 690, Department of Statistics, UC Berkeley, 2005.</p>
<p>[10] M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky. Tree-reweighted belief propagation algorithms and approximate ML estimation by pseudomoment matching. In Workshop on Artiﬁcial Intelligence and Statistics, January 2003.</p>
<p>[11] M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky. A new class of upper bounds on the log partition function. IEEE Trans. Info. Theory, 51(7):2313–2335, July 2005.</p>
<p>[12] M. J. Wainwright and M. I. Jordan. A variational principle for graphical models. In New Directions in Statistical Signal Processing. MIT Press, Cambridge, MA, 2005.</p>
<p>[13] W. Wiegerinck. Approximations with reweighted generalized belief propagation. In Workshop on Artiﬁcial Intelligence and Statistics, January 2005.</p>
<p>[14] J. Yedidia, W. T. Freeman, and Y. Weiss. Constructing free energy approximations and generalized belief propagation algorithms. IEEE Trans. Info. Theory, 51(7):2282–2312, July 2005.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
