<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>193 nips-2005-The Role of Top-down and Bottom-up Processes in Guiding Eye Movements during Visual Search</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-193" href="../nips2005/nips-2005-The_Role_of_Top-down_and_Bottom-up_Processes_in_Guiding_Eye_Movements_during_Visual_Search.html">nips2005-193</a> <a title="nips-2005-193-reference" href="#">nips2005-193-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>193 nips-2005-The Role of Top-down and Bottom-up Processes in Guiding Eye Movements during Visual Search</h1>
<br/><p>Source: <a title="nips-2005-193-pdf" href="http://papers.nips.cc/paper/2805-the-role-of-top-down-and-bottom-up-processes-in-guiding-eye-movements-during-visual-search.pdf">pdf</a></p><p>Author: Gregory Zelinsky, Wei Zhang, Bing Yu, Xin Chen, Dimitris Samaras</p><p>Abstract: To investigate how top-down (TD) and bottom-up (BU) information is weighted in the guidance of human search behavior, we manipulated the proportions of BU and TD components in a saliency-based model. The model is biologically plausible and implements an artiﬁcial retina and a neuronal population code. The BU component is based on featurecontrast. The TD component is deﬁned by a feature-template match to a stored target representation. We compared the model’s behavior at different mixtures of TD and BU components to the eye movement behavior of human observers performing the identical search task. We found that a purely TD model provides a much closer match to human behavior than any mixture model using BU information. Only when biological constraints are removed (e.g., eliminating the retina) did a BU/TD mixture model begin to approximate human behavior.</p><br/>
<h2>reference text</h2><p>[1] A. Treisman and G. Gelade. A feature-integration theory of attention. Cognitive Psychology, 12:97–136, 1980.</p>
<p>[2] J. Wolfe, K. Cave, and S. Franzel. Guided search: An alternative to the feature integration model for visual search. Journal of Experimental Psychology: Human Perception and Performance, 15:419–433, 1989.</p>
<p>[3] J. Wolfe. Guided search 2.0: A revised model of visual search. Psychonomic Bulletin and Review, 1:202–238, 1994.</p>
<p>[4] R. Rao, G. Zelinsky, M. Hayhoe, and D. Ballard. Eye movements in iconic visual search. Vision Research, 42:1447–1463, 2002.</p>
<p>[5] C. Koch and S. Ullman. Shifts of selective visual attention: Toward the underlying neural circuitry. Human Neurobiology, 4:219–227, 1985.</p>
<p>[6] L. Itti and C. Koch. Computational modeling of visual attention. Nature Reviews Neuroscience, 2(3):194–203, 2001.</p>
<p>[7] L. Itti and C. Koch. A saliency-based search mechanism for overt and covert shift of visual attention. Vision Research, 40(10-12):1489–1506, 2000.</p>
<p>[8] R. Rao, G. Zelinsky, M. Hayhoe, and D. Ballard. Modeling saccadic targeting in visual search. In NIPS, 1995.</p>
<p>[9] J.S. Perry and W.S. Geisler. Gaze-contingent real-time simulation of arbitrary visual ﬁelds. In SPIE, 2002.</p>
<p>[10] R. M. Klein and W.J. MacInnes. Inhibition of return is a foraging facilitator in visual search. Psychological Science, 10(4):346–352, 1999.</p>
<p>[11] C. A. Dickinson and G. Zelinsky. Marking rejected distractors: A gaze-contingent technique for measuring memory during search. Psychonomic Bulletin and Review, In press.</p>
<p>[12] L. Itti, C. Koch, and E. Niebur. A model of saliency-based visual attention for rapid scene analysis. PAMI, 20(11):1254–1259, 1998.</p>
<p>[13] T. Sejnowski. Neural populations revealed. Nature, 332:308, 1988.</p>
<p>[14] C. Lee, W. Rohrer, and D. Sparks. Population coding of saccadic eye movements by neurons in the superior colliculus. Nature, 332:357–360, 1988.</p>
<p>[15] J. Findlay. Global visual processing for saccadic eye movements. Vision Research, 22:1033– 1045, 1982.</p>
<p>[16] G. Zelinsky, R. Rao, M. Hayhoe, and D. Ballard. Eye movements reveal the spatio-temporal dynamics of visual search. Psychological Science, 8:448–453, 1997.</p>
<p>[17] J. L. Elman. Finding structures in time. Cognitive Science, 14:179–211, 1990.</p>
<p>[18] A. Toet, P. Bijl, F. L. Kooi, and J. M. Valeton. A high-resolution image dataset for testing search and detection models. Technical Report TNO-NM-98-A020, TNO Human Factors Research Institute,, Soesterberg, The Netherlands, 1998.</p>
<p>[19] V. Navalpakkam and L Itti. Modeling the inﬂuence of task on attention. Vision Research, 45:205–231, 2005.</p>
<p>[20] K. A. Turano, D. R. Geruschat, and F. H. Baker. Oculomotor strategies for direction of gaze tested with a real-world activity. Vision Research, 43(3):333–346, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
