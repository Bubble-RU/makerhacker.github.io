<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 nips-2005-Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-56" href="../nips2005/nips-2005-Diffusion_Maps%2C_Spectral_Clustering_and_Eigenfunctions_of_Fokker-Planck_Operators.html">nips2005-56</a> <a title="nips-2005-56-reference" href="#">nips2005-56-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>56 nips-2005-Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators</h1>
<br/><p>Source: <a title="nips-2005-56-pdf" href="http://papers.nips.cc/paper/2942-diffusion-maps-spectral-clustering-and-eigenfunctions-of-fokker-planck-operators.pdf">pdf</a></p><p>Author: Boaz Nadler, Stephane Lafon, Ioannis Kevrekidis, Ronald R. Coifman</p><p>Abstract: This paper presents a diffusion based probabilistic interpretation of spectral clustering and dimensionality reduction algorithms that use the eigenvectors of the normalized graph Laplacian. Given the pairwise adjacency matrix of all points, we deﬁne a diffusion distance between any two data points and show that the low dimensional representation of the data by the ﬁrst few eigenvectors of the corresponding Markov matrix is optimal under a certain mean squared error criterion. Furthermore, assuming that data points are random samples from a density p(x) = e−U (x) we identify these eigenvectors as discrete approximations of eigenfunctions of a Fokker-Planck operator in a potential 2U (x) with reﬂecting boundary conditions. Finally, applying known results regarding the eigenvalues and eigenfunctions of the continuous Fokker-Planck operator, we provide a mathematical justiﬁcation for the success of spectral clustering and dimensional reduction algorithms based on these ﬁrst few eigenvectors. This analysis elucidates, in terms of the characteristics of diffusion processes, many empirical ﬁndings regarding spectral clustering algorithms. Keywords: Algorithms and architectures, learning theory. 1</p><br/>
<h2>reference text</h2><p>[1] B. Sch¨ lkopf, A. Smola and K.R. M¨ ller. Nonlinear component analysis as a kernel eigenvalue o u problem, Neural Computation 10, 1998.</p>
<p>[2] Y. Weiss. Segmentation using eigenvectors: a unifying view. ICCV 1999.</p>
<p>[3] J. Shi and J. Malik. Normalized cuts and image segmentation, PAMI, Vol. 22, 2000.</p>
<p>[4] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering, NIPS Vol. 14, 2002.</p>
<p>[5] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation, Neural Computation 15:1373-1396, 2003.</p>
<p>[6] A.Y. Ng, M. Jordan and Y. Weiss. On spectral clustering, analysis and an algorithm, NIPS Vol. 14, 2002.</p>
<p>[7] X. Zhu, Z. Ghahramani, J. Lafferty, Semi-supervised learning using Gaussian ﬁelds and harmonic functions, Proceedings of the 20th international conference on machine learning, 2003.</p>
<p>[8] M. Saerens, F. Fouss, L. Yen and P. Dupont, The principal component analysis of a graph and its relationships to spectral clustering. ECML 2004.</p>
<p>[9] R.R. Coifman, S. Lafon, Diffusion Maps, to appear in Appl. Comp. Harm. Anal.</p>
<p>[10] R.R. Coifman & al., Geometric diffusion as a tool for harmonic analysis and structure deﬁnition of data, parts I and II, Proc. Nat. Acad. Sci., 102(21):7426-37 (2005).</p>
<p>[11] B. Nadler, S. Lafon, R.R. Coifman, I. G. Kevrekidis, Diffusion maps, spectral clustering, and the reaction coordinates of dynamical systems, to appear in Appl. Comp. Harm. Anal., available at http://arxiv.org/abs/math.NA/0503445.</p>
<p>[12] M. Meila, J. Shi. A random walks view of spectral segmentation, AI and Statistics, 2001.</p>
<p>[13] L. Yen L., Vanvyve D., Wouters F., Fouss F., Verleysen M. and Saerens M. , Clustering using a random-walk based distance measure. ESANN 2005, pp 317-324.</p>
<p>[14] N. Tishby, N. Slonim, Data Clustering by Markovian Relaxation and the information bottleneck method, NIPS, 2000.</p>
<p>[15] S. Yu and J. Shi. Multiclass spectral clustering. ICCV 2003.</p>
<p>[16] Y. Bengio et. al, Learning eigenfunctions links spectral embedding and kernel PCA, Neural Computation, 16:2197-2219 (2004).</p>
<p>[17] U. von Luxburg, O. Bousquet, M. Belkin, On the convergence of spectral clustering on random samples: the normalized case, NIPS, 2004.</p>
<p>[18] S. Lafon, A.B. Lee, Diffusion maps: A uniﬁed framework for dimension reduction, data partitioning and graph subsampling, submitted.</p>
<p>[19] C.W. Gardiner, Handbook of stochastic methods, third edition, Springer NY, 2004.</p>
<p>[20] H. Risken, The Fokker Planck equation, 2nd edition, Springer NY, 1999.</p>
<p>[21] B.J. Matkowsky and Z. Schuss, Eigenvalues of the Fokker-Planck operator and the approach to equilibrium for diffusions in potential ﬁelds, SIAM J. App. Math. 40(2):242-254 (1981).</p>
<p>[22] M. Eckhoff, Precise asymptotics of small eigenvalues of reversible diffusions in the metastable regime, Annals of Prob. 33:244-299, 2005.</p>
<p>[23] M. Belkin and P. Niyogi, Towards a theoeretical foundation for Laplacian-based manifold methods, COLT 2005 (to appear).</p>
<p>[24] M. Hein, J. Audibert, U. von Luxburg, From graphs to manifolds - weak and strong pointwise consistency of graph Laplacians, COLT 2005 (to appear).</p>
<p>[25] W. Huisinga, C. Best, R. Roitzsch, C. Sch¨ tte, F. Cordes, From simulation data to conformau tional ensembles, structure and dynamics based methods, J. Comp. Chem. 20:1760-74, 1999.</p>
<p>[26] L. Zelnik-Manor, P. Perona, Self-Tuning spectral clustering, NIPS, 2004.</p>
<p>[27] A. Singer, Z. Schuss, D. Holcman and R.S. Eisenberg, narrow escape, part I, submitted.</p>
<p>[28] D. Zhou & al., Learning with local and global consistency, NIPS Vol. 16, 2004.</p>
<p>[29] I.G. Kevrekidis, C.W. Gear, G. Hummer, Equation-free: The computer-aided analysis of complex multiscale systems, Aiche J. 50:1346-1355, 2004.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
