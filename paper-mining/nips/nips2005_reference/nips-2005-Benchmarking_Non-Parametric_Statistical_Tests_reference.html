<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>37 nips-2005-Benchmarking Non-Parametric Statistical Tests</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-37" href="../nips2005/nips-2005-Benchmarking_Non-Parametric_Statistical_Tests.html">nips2005-37</a> <a title="nips-2005-37-reference" href="#">nips2005-37-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>37 nips-2005-Benchmarking Non-Parametric Statistical Tests</h1>
<br/><p>Source: <a title="nips-2005-37-pdf" href="http://papers.nips.cc/paper/2846-benchmarking-non-parametric-statistical-tests.pdf">pdf</a></p><p>Author: Mikaela Keller, Samy Bengio, Siew Y. Wong</p><p>Abstract: Although non-parametric tests have already been proposed for that purpose, statistical signiﬁcance tests for non-standard measures (different from the classiﬁcation error) are less often used in the literature. This paper is an attempt at empirically verifying how these tests compare with more classical tests, on various conditions. More precisely, using a very large dataset to estimate the whole “population”, we analyzed the behavior of several statistical test, varying the class unbalance, the compared models, the performance measure, and the sample size. The main result is that providing big enough evaluation sets non-parametric tests are relatively reliable in all conditions. 1</p><br/>
<h2>reference text</h2><p>[1] M. Bisani and H. Ney. Bootstrap estimates for conﬁdence intervals in ASR performance evaluation. In Proceedings of ICASSP, 2004.</p>
<p>[2] R. M. Bolle, N. K. Ratha, and S. Pankanti. Error analysis of pattern recognition systems - the subsets bootstrap. Computer Vision and Image Understanding, 93:1– 33, 2004.  1.0  1.0  0.6  Bootstrap test dF1 McNemar test Proportion test Bootstrap test dCerr  0.0  0.0  0.2  0.4  Power of the test  0.8  0.8 0.6 0.4 0.2  Power of the test  Bootstrap test dF1 McNemar test Proportion test Bootstrap test dCerr  0  1000  2000  3000  4000  5000  6000  0  1000  Evaluation set size  5000  6000  1.0 0.6  0.8  Bootstrap test dF1 McNemar test Proportion test Bootstrap test dCerr  0.0  0.0  0.2  0.4  0.6 0.4 0.2  Power of the test  1.0  4000  (b) Linear SVM vs MLP - Unbalanced data  Bootstrap test dF1 McNemar test Proportion test Bootstrap test dCerr  0.8  3000  Evaluation set size  (a) Linear SVM vs MLP - Balanced data  Power of the test  2000  0  1000  2000  3000  4000  5000  6000  Evaluation set size  (c) Linear vs RBF SVMs - Balanced data  0  1000  2000  3000  4000  5000  6000  Evaluation set size  (d) Linear vs RBF SVMs - Unbalanced data  Figure 2: Power of several statistical tests comparing Linear SVM vs MLP or vs RBF SVM. The power equals -1, in Figures 2(c) and 2(d), when there was not data to compute the proportion (ie H1 was never true).</p>
<p>[3] A. C. Davison and D. V. Hinkley. Bootstrap methods and their application. Cambridge University Press, 1997.</p>
<p>[4] T.G. Dietterich. Approximate statistical tests for comparing supervised classiﬁcation learning algorithms. Neural Computation, 10(7):1895–1924, 1998.</p>
<p>[5] B. Efron and R. Tibshirani. An Introduction to the Bootstrap. Chapman and Hall, 1993.</p>
<p>[6] B. S. Everitt. The analysis of contingency tables. Chapman and Hall, 1977.</p>
<p>[7] C. Goutte and E. Gaussier. A probabilistic interpretation of precision, recall and Fscore, with implication for evaluation. In Proceedings of ECIR, pages 345–359, 2005.</p>
<p>[8] M. Keller, S. Bengio, and S. Y. Wong. Surprising Outcome While Benchmarking Statistical Tests. IDIAP-RR 38, IDIAP, 2005.</p>
<p>[9] Claude Nadeau and Yoshua Bengio. Inference for the generalization error. Machine Learning, 52(3):239–281, 2003.</p>
<p>[10] T.G. Rose, M. Stevenson, and M. Whitehead. The Reuters Corpus Volume 1 - from yesterday’s news to tomorrow’s language resources. In Proceedings of the 3rd Int. Conf. on Language Resources and Evaluation, 2002.</p>
<p>[11] F. Sebastiani. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1–47, 2002.</p>
<p>[12] C. J. van Rijsbergen. Information Retrieval. Butterworths, London, UK, 1975.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
