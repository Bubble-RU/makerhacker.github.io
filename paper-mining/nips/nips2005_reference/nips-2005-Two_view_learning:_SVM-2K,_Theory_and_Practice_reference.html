<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>196 nips-2005-Two view learning: SVM-2K, Theory and Practice</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-196" href="../nips2005/nips-2005-Two_view_learning%3A_SVM-2K%2C_Theory_and_Practice.html">nips2005-196</a> <a title="nips-2005-196-reference" href="#">nips2005-196-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>196 nips-2005-Two view learning: SVM-2K, Theory and Practice</h1>
<br/><p>Source: <a title="nips-2005-196-pdf" href="http://papers.nips.cc/paper/2829-two-view-learning-svm-2k-theory-and-practice.pdf">pdf</a></p><p>Author: Jason Farquhar, David Hardoon, Hongying Meng, John S. Shawe-taylor, Sándor Szedmák</p><p>Abstract: Kernel methods make it relatively easy to deﬁne complex highdimensional feature spaces. This raises the question of how we can identify the relevant subspaces for a particular learning task. When two views of the same phenomenon are available kernel Canonical Correlation Analysis (KCCA) has been shown to be an effective preprocessing step that can improve the performance of classiﬁcation algorithms such as the Support Vector Machine (SVM). This paper takes this observation to its logical conclusion and proposes a method that combines this two stage learning (KCCA followed by SVM) into a single optimisation termed SVM-2K. We present both experimental and theoretical analysis of the approach showing encouraging results and insights. 1</p><br/>
<h2>reference text</h2><p>[1] Francis R. Bach and Michael I. Jordan. Kernel independent component analysis. Journal of Machine Learning Research, 3:1–48, 2002.</p>
<p>[2] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: risk bounds and structural results. Journal of Machine Learning Research, 3:463–482, 2002.</p>
<p>[3] G. Csurka, C. Bray, C. Dance, and L. Fan. Visual categorization with bags of keypoints. In XRCE Research Reports, XEROX. The 8th European Conference on Computer Vision - ECCV, Prague, 2004.</p>
<p>[4] R. Fergus, P. Perona, and A. Zisserman. Object class recognition by unsupervised scale-invariant learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2003.</p>
<p>[5] David Hardoon, Sandor Szedmak, and John Shawe-Taylor. Canonical correlation analysis: An overview with application to learning methods. Neural Computation, 16:2639–2664, 2004.</p>
<p>[6] Yaoyong Li and John Shawe-Taylor. Using kcca for japanese-english cross-language information retrieval and classiﬁcation. to appear in Journal of Intelligent Information Systems, 2005.</p>
<p>[7] S. Mika, B. Sch¨ lkopf, A. Smola, K.-R. M¨ ller, M. Scholz, and G. R¨ tsch. Kernel o u a PCA and de-noising in feature spaces. In Advances in Neural Information Processing Systems 11, 1998.</p>
<p>[8] R. Rosipal and L. J. Trejo. Kernel partial least squares regression in reproducing kernel hilbert space. Journal of Machine Learning Research, 2:97–123, 2001.</p>
<p>[9] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, Cambridge, UK, 2004.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
