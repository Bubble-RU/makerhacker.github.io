<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>71 nips-2005-Fast Krylov Methods for N-Body Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-71" href="../nips2005/nips-2005-Fast_Krylov_Methods_for_N-Body_Learning.html">nips2005-71</a> <a title="nips-2005-71-reference" href="#">nips2005-71-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>71 nips-2005-Fast Krylov Methods for N-Body Learning</h1>
<br/><p>Source: <a title="nips-2005-71-pdf" href="http://papers.nips.cc/paper/2865-fast-krylov-methods-for-n-body-learning.pdf">pdf</a></p><p>Author: Nando D. Freitas, Yang Wang, Maryam Mahdaviani, Dustin Lang</p><p>Abstract: This paper addresses the issue of numerical computation in machine learning domains based on similarity metrics, such as kernel methods, spectral techniques and Gaussian processes. It presents a general solution strategy based on Krylov subspace iteration and fast N-body learning methods. The experiments show signiﬁcant gains in computation and storage on datasets arising in image segmentation, object detection and dimensionality reduction. The paper also presents theoretical bounds on the stability of these methods.</p><br/>
<h2>reference text</h2><p>[1] A Y Ng, M I Jordan, and Y Weiss. On spectral clustering: Analysis and algorithm. In Advances in Neural Information Processing Systems, pages 849–856, 2001.</p>
<p>[2] D Zhou, J Weston, A Gretton, O Bousquet, and B Scholkopf. Ranking on data manifolds. In Advances on Neural Information Processing Systems, 2004.</p>
<p>[3] X Zhu, J Lafferty, and Z Ghahramani. Semi-supervised learning using Gaussian ﬁelds and harmonic functions. In International Conference on Machine Learning, pages 912–919, 2003.</p>
<p>[4] M N Gibbs. Bayesian Gaussian processes for regression and classiﬁcation. In PhD Thesis, University of Cambridge, 1997.</p>
<p>[5] M Belkin and P Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15(6):1373–1396, 2003.</p>
<p>[6] G Hinton and S Roweis. Stochastic neighbor embedding. In Advances in Neural Information Processing Systems, pages 833–840, 2002.</p>
<p>[7] A Smola and R Kondor. Kernels and regularization of graphs. In Computational Learning Theory, pages 144–158, 2003.  True manifold  Sampled data  Embedding of SNE Embedding of SNE withIFGT  S−curve  4  Running time(seconds)  Running time(seconds)  10 SNE SNE with IFGT  3  10  2  10  1  10  Swissroll  4  10  SNE SNE with IFGT 3  10  2  10  1  0  1000  2000  3000 N  4000  5000  10  0  1000  2000  3000  4000  5000  N  Figure 4: Examples of embedding on S-curve and Swiss-roll datasets.</p>
<p>[8] M Mahdaviani, N de Freitas, B Fraser, and F Hamze. Fast computational methods for visually guided robots. In IEEE International Conference on Robotics and Automation, 2004.</p>
<p>[9] L Greengard and V Rokhlin. A fast algorithm for particle simulations. Journal of Computational Physics, 73:325–348, 1987.</p>
<p>[10] C Yang, R Duraiswami, N A Gumerov, and L S Davis. Improved fast Gauss transform and efﬁcient kernel density estimation. In International Conference on Computer Vision, Nice, 2003.</p>
<p>[11] A Gray and A Moore. Rapid evaluation of multiple density models. In Artiﬁcial Iintelligence and Statistics, 2003.</p>
<p>[12] J Shi and J Malik. Normalized cuts and image segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 731–737, 1997.</p>
<p>[13] R K Beatson, J B Cherrie, and C T Mouat. Fast ﬁtting of radial basis functions: Methods based on preconditioned GMRES iteration. Advances in Computational Mathematics, 11:253–270, 1999.</p>
<p>[14] J W Demmel. Applied Numerical Linear Algebra. SIAM, 1997.</p>
<p>[15] Y Saad. Iterative Methods for Sparse Linear Systems. The PWS Publishing Company, 1996.</p>
<p>[16] L Greengard and J Strain. The fast Gauss transform. SIAM Journal of Scientiﬁc Statistical Computing, 12(1):79–94, 1991.</p>
<p>[17] B J C Baxter and G Roussos. A new error estimate of the fast Gauss transform. SIAM Journal of Scientiﬁc Computing, 24(1):257–259, 2002.</p>
<p>[18] D Lang, M Klaas, and N de Freitas. Empirical testing of fast kernel density estimation algorithms. Technical Report TR-2005-03, Department of Computer Science, UBC, 2005.</p>
<p>[19] G H Golub and Q Ye. Inexact preconditioned conjugate gradient method with inner-outer iteration. SIAM Journal of Scientiﬁc Computing, 21:1305–1320, 1999.</p>
<p>[20] G W Stewart. Backward error bounds for approximate Krylov subspaces. Linear Algebra and Applications, 340:81–86, 2002.</p>
<p>[21] V Simoncini and D B Szyld. Theory of inexact Krylov subspace methods and applications to scientiﬁc computing. SIAM Journal on Scientiﬁc Computing, 25:454–477, 2003.</p>
<p>[22] D G Lowe. Object recognition from local scale-invariant features. In ICCV, 1999.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
