<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>172 nips-2005-Selecting Landmark Points for Sparse Manifold Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-172" href="../nips2005/nips-2005-Selecting_Landmark_Points_for_Sparse_Manifold_Learning.html">nips2005-172</a> <a title="nips-2005-172-reference" href="#">nips2005-172-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>172 nips-2005-Selecting Landmark Points for Sparse Manifold Learning</h1>
<br/><p>Source: <a title="nips-2005-172-pdf" href="http://papers.nips.cc/paper/2884-selecting-landmark-points-for-sparse-manifold-learning.pdf">pdf</a></p><p>Author: Jorge Silva, Jorge Marques, João Lemos</p><p>Abstract: There has been a surge of interest in learning non-linear manifold models to approximate high-dimensional data. Both for computational complexity reasons and for generalization capability, sparsity is a desired feature in such models. This usually means dimensionality reduction, which naturally implies estimating the intrinsic dimension, but it can also mean selecting a subset of the data to use as landmarks, which is especially important because many existing algorithms have quadratic complexity in the number of observations. This paper presents an algorithm for selecting landmarks, based on LASSO regression, which is well known to favor sparse approximations because it uses regularization with an l1 norm. As an added beneﬁt, a continuous manifold parameterization, based on the landmarks, is also found. Experimental results with synthetic and real data illustrate the algorithm. 1</p><br/>
<h2>reference text</h2><p>[1] A. Bjorck and G. H. Golub. Numerical methods for computing angles between linear subspaces. Mathematical Computation, 27, 1973.</p>
<p>[2] J. Chen and X. Huo. Sparse representation for multiple measurement vectors (mmv) in an over-complete dictionary. ICASSP, 2005.</p>
<p>[3] V. de Silva and J. B. Tenenbaum. Global versus local methods in nonlinear dimensionality reduction. NIPS, 15, 2002.</p>
<p>[4] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 2003.</p>
<p>[5] T. Hastie, R. Tibshirani, and J. H. Friedman. The Elements of Statistical Learning. Springer, 2001.</p>
<p>[6] H. L¨ desm¨ ki, O. Yli-Harja, W. Zhang, and I. Shmulevich. Intrinsic dimensionality in gene a a expression analysis. GENSIPS, 2005.</p>
<p>[7] T. Poggio and S. Smale. The mathematics of learning: Dealing with data. Notices of the American Mathematical Society, 2003.</p>
<p>[8] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290:2323–2326, 2000.</p>
<p>[9] J. Silva, J. Marques, and J. M. Lemos. Non-linear dimension reduction with tangent bundle approximation. ICASSP, 2005.</p>
<p>[10] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290:2319–2323, 2000.</p>
<p>[11] M. Turk and A. Pentland. Eigenfaces for recognition. Journal of Cognitive Neuroscience, 3:71–86, 1991.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
