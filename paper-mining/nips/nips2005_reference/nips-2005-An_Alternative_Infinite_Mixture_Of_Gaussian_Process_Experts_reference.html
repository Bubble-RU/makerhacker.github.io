<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>21 nips-2005-An Alternative Infinite Mixture Of Gaussian Process Experts</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-21" href="../nips2005/nips-2005-An_Alternative_Infinite_Mixture_Of_Gaussian_Process_Experts.html">nips2005-21</a> <a title="nips-2005-21-reference" href="#">nips2005-21-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>21 nips-2005-An Alternative Infinite Mixture Of Gaussian Process Experts</h1>
<br/><p>Source: <a title="nips-2005-21-pdf" href="http://papers.nips.cc/paper/2768-an-alternative-infinite-mixture-of-gaussian-process-experts.pdf">pdf</a></p><p>Author: Edward Meeds, Simon Osindero</p><p>Abstract: We present an inﬁnite mixture model in which each component comprises a multivariate Gaussian distribution over an input space, and a Gaussian Process model over an output space. Our model is neatly able to deal with non-stationary covariance functions, discontinuities, multimodality and overlapping output signals. The work is similar to that by Rasmussen and Ghahramani [1]; however, we use a full generative model over input and output space rather than just a conditional model. This allows us to deal with incomplete data, to perform inference over inverse functional mappings as well as for regression, and also leads to a more powerful and consistent Bayesian speciﬁcation of the effective ‘gating network’ for the different experts. 1</p><br/>
<h2>reference text</h2><p>[1] C.E. Rasmussen and Z. Ghahramani. Inﬁnite mixtures of Gaussian process experts. In Advances in Neural Information Processing Systems 14, pages 881–888. MIT Press, 2002.</p>
<p>[2] V. Tresp. Mixture of Gaussian processes. In Advances in Neural Information Processing Systems, volume 13. MIT Press, 2001.</p>
<p>[3] Z. Ghahramani and M. I. Jordan. Supervised learning from incomplete data via an EM approach. In Advances in Neural Information Processing Systems 6, pages 120–127. MorganKaufmann, 1995.</p>
<p>[4] L. Xu, M. I. Jordan, and G. E. Hinton. An alternative model for mixtures of experts. In Advances in Neural Information Processing Systems 7, pages 633–640. MIT Press, 1995.</p>
<p>[5] C. E. Rasmussen. The inﬁnite Gaussian mixture model. In Advances in Neural Information Processing Systems, volume 12, pages 554–560. MIT Press, 2000.</p>
<p>[6] R.A. Jacobs, M.I. Jordan, and G.E. Hinton. Adaptive mixture of local experts. Neural Computation, 3, 1991.</p>
<p>[7] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. Bayesian Data Analysis. Chapman and Hall, 2nd edition, 2004.</p>
<p>[8] D. Blackwell and J. B. MacQueen. Ferguson distributions via Polya urn schemes. The Annals of Statistics, 1(2):353–355, 1973.</p>
<p>[9] R. M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics, 9:249–265, 2000.</p>
<p>[10] R. M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. Technical Report CRG-TR-93-1, University of Toronto, 1993.</p>
<p>[11] R. M. Neal. Slice sampling (with discussion). Annals of Statistics, 31:705–767, 2003.</p>
<p>[12] M. Escobar and M. West. Computing Bayesian nonparametric hierarchical models. In Practical Nonparametric and Semiparametric Bayesian Statistics, number 133 in Lecture Notes in Statistics. Springer-Verlag, 1998.</p>
<p>[13] B. W. Silverman. Some aspects of the spline smoothing approach to non-parametric regression curve ﬁtting. J. Royal Stayt Society. Ser. B, 47:1–52, 1985.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
