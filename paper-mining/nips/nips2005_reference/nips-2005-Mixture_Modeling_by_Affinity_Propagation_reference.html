<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>127 nips-2005-Mixture Modeling by Affinity Propagation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-127" href="../nips2005/nips-2005-Mixture_Modeling_by_Affinity_Propagation.html">nips2005-127</a> <a title="nips-2005-127-reference" href="#">nips2005-127-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>127 nips-2005-Mixture Modeling by Affinity Propagation</h1>
<br/><p>Source: <a title="nips-2005-127-pdf" href="http://papers.nips.cc/paper/2870-mixture-modeling-by-affinity-propagation.pdf">pdf</a></p><p>Author: Brendan J. Frey, Delbert Dueck</p><p>Abstract: Clustering is a fundamental problem in machine learning and has been approached in many ways. Two general and quite different approaches include iteratively ﬁtting a mixture model (e.g., using EM) and linking together pairs of training cases that have high afﬁnity (e.g., using spectral methods). Pair-wise clustering algorithms need not compute sufﬁcient statistics and avoid poor solutions by directly placing similar examples in the same cluster. However, many applications require that each cluster of data be accurately described by a prototype or model, so afﬁnity-based clustering – and its beneﬁts – cannot be directly realized. We describe a technique called “afﬁnity propagation”, which combines the advantages of both approaches. The method learns a mixture model of the data by recursively propagating afﬁnity messages. We demonstrate afﬁnity propagation on the problems of clustering image patches for image segmentation and learning mixtures of gene expression models from microarray data. We ﬁnd that afﬁnity propagation obtains better solutions than mixtures of Gaussians, the K-medoids algorithm, spectral clustering and hierarchical clustering, and is both able to ﬁnd a pre-speciﬁed number of clusters and is able to automatically determine the number of clusters. Interestingly, afﬁnity propagation can be viewed as belief propagation in a graphical model that accounts for pairwise training case likelihood functions and the identiﬁcation of cluster centers. 1</p><br/>
<h2>reference text</h2><p>[1]</p>
<p>[2]</p>
<p>[3]</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]  CM Bishop. Neural Networks for Pattern Recognition. Oxford University Press, NY, 1995. KA Heller, Z Ghahramani. Bayesian hierarchical clustering. ICML, 2005. M Meila, J Shi. Learning segmentation by random walks. NIPS 14, 2001. J Shi, J Malik. Normalized cuts and image segmentation. Proc CVPR, 731-737, 1997. A Ng, M Jordan, Y Weiss. On spectral clustering: Analysis and an algorithm. NIPS 14, 2001. N Shental A Zomet T Hertz Y Weiss. Pairwise clustering and graphical models NIPS 16 2003. R Rosales, BJ Frey. Learning generative models of afﬁnity matrices. Proc UAI, 2003. M Charikar, S Guha, A Tardos, DB Shmoys. A constant-factor approximation algorithm for the k-median problem. J Comp and Sys Sci, 65:1, 129-149, 2002. J Malik et al.. Contour and texture analysis for image segmentation. IJCV 43:1, 2001. FR Kschischang, BJ Frey, H-A Loeliger. Factor graphs and the sum-product algorithm. IEEE Trans Info Theory 47:2, 498-519, 2001. BJ Frey, QD Morris, M Robinson, TR Hughes. Finding novel transcripts in high-resolution genome-wide microarray data using the GenRate model. Proc RECOMB 2005, 2005. D. D. Shoemaker et al. Experimental annotation of the human genome using microarray technology. Nature 409, 922-927, 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
