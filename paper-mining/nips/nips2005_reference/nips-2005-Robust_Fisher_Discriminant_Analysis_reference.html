<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>166 nips-2005-Robust Fisher Discriminant Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-166" href="../nips2005/nips-2005-Robust_Fisher_Discriminant_Analysis.html">nips2005-166</a> <a title="nips-2005-166-reference" href="#">nips2005-166-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>166 nips-2005-Robust Fisher Discriminant Analysis</h1>
<br/><p>Source: <a title="nips-2005-166-pdf" href="http://papers.nips.cc/paper/2792-robust-fisher-discriminant-analysis.pdf">pdf</a></p><p>Author: Seung-jean Kim, Alessandro Magnani, Stephen Boyd</p><p>Abstract: Fisher linear discriminant analysis (LDA) can be sensitive to the problem data. Robust Fisher LDA can systematically alleviate the sensitivity problem by explicitly incorporating a model of data uncertainty in a classiﬁcation problem and optimizing for the worst-case scenario under this model. The main contribution of this paper is show that with general convex uncertainty models on the problem data, robust Fisher LDA can be carried out using convex optimization. For a certain type of product form uncertainty model, robust Fisher LDA can be carried out at a cost comparable to standard Fisher LDA. The method is demonstrated with some numerical examples. Finally, we show how to extend these results to robust kernel Fisher discriminant analysis, i.e., robust Fisher LDA in a high dimensional feature space. 1</p><br/>
<h2>reference text</h2><p>[1] D. Bertsekas, A. Nedi´ , and A. Ozdaglar. Convex Analysis and Optimization. Athena Scientiﬁc, c 2003.</p>
<p>[2] C. Bhattacharyya. Second order cone programming formulations for feature selection. Journal of Machine Learning Research, 5:1417–1433, 2004.</p>
<p>[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.</p>
<p>[4] B. Efron and R.J. Tibshirani. An Introduction to Bootstrap. Chapman and Hall, London UK, 1993.</p>
<p>[5] K. Huang, H. Yang, I. King, M. Lyu, and L. Chan. The minimum error minimax probability machine. Journal of Machine Learning Research, 5:1253–1286, 2004.</p>
<p>[6] G. Lanckriet, L. El Ghaoui, C. Bhattacharyya, and M. Jordan. A robust minimax approach to classiﬁcation. Journal of Machine Learning Research, 3:555–582, 2002.</p>
<p>[7] S. Mika, G. R¨ tsch, and K. M¨ ller. A mathematical programming approach to the kernel Fisher a u algorithm, 2001. In Advances in Neural Information Processing Systems, 13, pp. 591-597, MIT Press.</p>
<p>[8] S. Verd´ and H. Poor. On minimax robustness: A general approach and applications. IEEE u Transactions on Information Theory, 30(2):328–340, 1984.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
