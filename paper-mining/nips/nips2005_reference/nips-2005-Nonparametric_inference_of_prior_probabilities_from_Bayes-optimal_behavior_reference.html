<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>140 nips-2005-Nonparametric inference of prior probabilities from Bayes-optimal behavior</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-140" href="../nips2005/nips-2005-Nonparametric_inference_of_prior_probabilities_from_Bayes-optimal_behavior.html">nips2005-140</a> <a title="nips-2005-140-reference" href="#">nips2005-140-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>140 nips-2005-Nonparametric inference of prior probabilities from Bayes-optimal behavior</h1>
<br/><p>Source: <a title="nips-2005-140-pdf" href="http://papers.nips.cc/paper/2900-nonparametric-inference-of-prior-probabilities-from-bayes-optimal-behavior.pdf">pdf</a></p><p>Author: Liam Paninski</p><p>Abstract: We discuss a method for obtaining a subject’s a priori beliefs from his/her behavior in a psychophysics context, under the assumption that the behavior is (nearly) optimal from a Bayesian perspective. The method is nonparametric in the sense that we do not assume that the prior belongs to any ﬁxed class of distributions (e.g., Gaussian). Despite this increased generality, the method is relatively simple to implement, being based in the simplest case on a linear programming algorithm, and more generally on a straightforward maximum likelihood or maximum a posteriori formulation, which turns out to be a convex optimization problem (with no non-global local maxima) in many important cases. In addition, we develop methods for analyzing the uncertainty of these estimates. We demonstrate the accuracy of the method in a simple simulated coin-ﬂipping setting; in particular, the method is able to precisely track the evolution of the subject’s posterior distribution as more and more data are observed. We close by brieﬂy discussing an interesting connection to recent models of neural population coding.</p><br/>
<h2>reference text</h2><p>1. D. Knill, W. Richards, eds., Perception as Bayesian Inference (Cambridge University Press, 1996). 2. Y. Weiss, E. Simoncelli, E. Adelson, Nature Neuroscience 5, 598 (2002). 3. Y. Weiss, D. Fleet, Statistical Theories of the Cortex (MIT Press, 2002), chap. Velocity likelihoods in biological and machine vision, pp. 77–96. 4. D. Kersten, P. Mamassian, A. Yuille, Annual Review of Psychology 55, 271 (2004). 5. K. Koerding, D. Wolpert, Nature 427, 244 (2004). 6. R. Hogarth, Journal of the American Statistical Association 70, 271 (1975). 7. J. Oakley, A. O’Hagan, Biometrika under review (2003). 8. P. Garthwaite, J. Kadane, A. O’Hagan, Handbook of Statistics (2004), chap. Elicitation. 9. A. Ng, S. Russell, ICML-17 (2000). 10. J. Blythe, AAAI02 (2002). 11. G. Strang, Linear algebra and its applications (Harcourt Brace, New York, 1988). 12. Y. Rinott, Annals of Probability 4, 1020 (1976). 13. M. Henrion, et al., Why is diagnosis using belief networks insensitive to imprecision in probabilities?, Tech. Rep. SMI-96-0637, Stanford (1996). 14. R. Zemel, P. Dayan, A. Pouget, Neural Computation 10, 403 (1998). 15. A. Pouget, P. Dayan, R. Zemel, Annual Reviews of Neuroscience 26, 381 (2003). 16. L. Paninski, Network: Computation in Neural Systems 15, 243 (2004). 17. K. Chaloner, I. Verdinelli, Statistical Science 10, 273 (1995). 18. L. Paninski, Advances in Neural Information Processing Systems 16 (2003).</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
