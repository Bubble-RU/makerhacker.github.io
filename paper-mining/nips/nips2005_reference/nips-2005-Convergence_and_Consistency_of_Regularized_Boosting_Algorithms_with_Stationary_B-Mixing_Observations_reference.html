<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-49" href="../nips2005/nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">nips2005-49</a> <a title="nips-2005-49-reference" href="#">nips2005-49-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</h1>
<br/><p>Source: <a title="nips-2005-49-pdf" href="http://papers.nips.cc/paper/2790-convergence-and-consistency-of-regularized-boosting-algorithms-with-stationary-b-mixing-observations.pdf">pdf</a></p><p>Author: Aurelie C. Lozano, Sanjeev R. Kulkarni, Robert E. Schapire</p><p>Abstract: We study the statistical convergence and consistency of regularized Boosting methods, where the samples are not independent and identically distributed (i.i.d.) but come from empirical processes of stationary β-mixing sequences. Utilizing a technique that constructs a sequence of independent blocks close in distribution to the original samples, we prove the consistency of the composite classiﬁers resulting from a regularization achieved by restricting the 1-norm of the base classiﬁers’ weights. When compared to the i.i.d. case, the nature of sampling manifests in the consistency result only through generalization of the original condition on the growth of the regularization parameter.</p><br/>
<h2>reference text</h2><p>[1] Schapire, R.E.: The Boosting Approach to Machine Learning An Overview. In Proc. of the MSRI Workshop on Nonlinear Estimation and Classiﬁcation (2002)</p>
<p>[2] Friedman, J., Hastie T., Tibshirani, R.: Additive logistic regression: A statistical view of boosting. Ann. Statist. 38 (2000) 337–374</p>
<p>[3] Jiang, W.: Does Boosting Overﬁt:Views From an Exact Solution. Technical Report 00-03 Department of Statistics, Northwestern University (2000)</p>
<p>[4] Lugosi, G., Vayatis, N.: On the Bayes-risk consistency of boosting methods. Ann. Statist. 32 (2004) 30–55</p>
<p>[5] Zhang, T.: Statistical Behavior and Consistency of Classiﬁcation Methods based on Convex Risk Minimization. Ann. Statist. 32 (2004) 56–85</p>
<p>[6] Gy¨ rﬁ, L., H¨ rdle, W., Sarda, P., and Vieu, P.: Nonparametric Curve Estimation from Time o a Series. Lecture Notes in Statistics. Springer-Verlag, Berlin. (1989)</p>
<p>[7] Irle, A.: On the consistency in nonparametric estimation under mixing assumptions. J. Multivariate Anal. 60 (1997) 123–147</p>
<p>[8] Meir, R.: Nonparametric Time Series Prediction Through Adaptative Model Selection. Machine Learning 39 (2000) 5–34</p>
<p>[9] Modha, D., Masry, E.: Memory-Universal Prediction of Stationary Random Processes. IEEE Trans. Inform. Theory 44 (1998) 117–133</p>
<p>[10] Roussas, G.G.: Nonparametric estimation in mixing sequences of random variables. J. Statist. Plan. Inference. 18 (1988) 135–149</p>
<p>[11] Vidyasagar, M.: A Theory of Learning and Generalization: With Applications to Neural Networks and Control Systems. Second Edition. Springer-Verlag, London (2002)</p>
<p>[12] Yu, B.: Density estimation in the L∞ norm for dependent data with applications. Ann. Statist. 21 (1993) 711–735</p>
<p>[13] Doukhan, P.: Mixing Properties and Examples. Springer-Verlag, New York (1995)</p>
<p>[14] Yu, B.: Some Results on Empirical Processes and Stochastic Complexity. Ph.D. Thesis, Dept of Statistics, U.C. Berkeley (Apr. 1990)</p>
<p>[15] Yu, B.: Rate of convergence for empirical processes of stationary mixing sequences. Ann. Probab. 22 (1994) 94–116.</p>
<p>[16] Ledoux, M., Talagrand, N.: Probability in Banach Spaces. Springer, New York (1991)</p>
<p>[17] Meir, R., Zhang, T.:Generalization error bounds for Bayesian mixture algorithms. J. Machine Learning Research (2003)</p>
<p>[18] van der Vaart, A.W., Wellner, J.A.: Weak convergence and empirical processes. Springer Series in Statistics. Springer-Verlag, New York (1996)</p>
<p>[19] Devroye, L., Gy¨ rﬁ L., Lugosi, G.: A Probabilistic Theory of Pattern Recognition. Springer, o New York (1996)</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
