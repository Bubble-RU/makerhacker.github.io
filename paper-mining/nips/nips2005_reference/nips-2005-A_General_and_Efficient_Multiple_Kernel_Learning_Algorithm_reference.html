<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-10" href="../nips2005/nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">nips2005-10</a> <a title="nips-2005-10-reference" href="#">nips2005-10-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</h1>
<br/><p>Source: <a title="nips-2005-10-pdf" href="http://papers.nips.cc/paper/2890-a-general-and-efficient-multiple-kernel-learning-algorithm.pdf">pdf</a></p><p>Author: Sören Sonnenburg, Gunnar Rätsch, Christin Schäfer</p><p>Abstract: While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lankriet et al. (2004) considered conic combinations of kernel matrices for classiﬁcation, leading to a convex quadratically constraint quadratic program. We show that it can be rewritten as a semi-inﬁnite linear program that can be efﬁciently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classiﬁcation. Experimental results show that the proposed algorithm helps for automatic model selection, improving the interpretability of the learning result and works for hundred thousands of examples or hundreds of kernels to be combined. 1</p><br/>
<h2>reference text</h2><p>[1] Francis R. Bach, Gert R. G. Lanckriet, and Michael I. Jordan. Multiple kernel learning, conic duality, and the SMO algorithm. In Twenty-ﬁrst international conference on Machine learning. ACM Press, 2004.</p>
<p>[2] Kristin P. Bennett, Michinari Momma, and Mark J. Embrechts. Mark: a boosting algorithm for heterogeneous kernel models. KDD, pages 24–31, 2002.</p>
<p>[3] Jinbo Bi, Tong Zhang, and Kristin P. Bennett. Column-generation boosting methods for mixture of kernels. KDD, pages 521–526, 2004.</p>
<p>[4] O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector machines. Machine Learning, 46(1-3):131–159, 2002.</p>
<p>[5] I. Grandvalet and S. Canu. Adaptive scaling for feature selection in SVMs. In In Advances in Neural Information Processing Systems, 2002.</p>
<p>[6] R. Hettich and K.O. Kortanek. Semi-inﬁnite programming: Theory, methods and applications. SIAM Review, 3:380–429, September 1993.</p>
<p>[7] G.R.G. Lanckriet, T. De Bie, N. Cristianini, M.I. Jordan, and W.S. Noble. A statistical framework for genomic data fusion. Bioinformatics, 2004.</p>
<p>[8] R. Meir and G. R¨ tsch. An introduction to boosting and leveraging. In S. Mendelson and a A. Smola, editors, Proc. of the ﬁrst Machine Learning Summer School in Canberra, LNCS, pages 119–184. Springer, 2003. in press.</p>
<p>[9] C.S. Ong, A.J. Smola, and R.C. Williamson. Hyperkernels. In In Advances in Neural Information Processing Systems, volume 15, pages 495–502, 2003.</p>
<p>[10] J. Platt. Fast training of support vector machines using sequential minimal optimization. In B. Sch¨ lkopf, C.J.C. Burges, and A.J. Smola, editors, Advances in Kernel Methods — Support o Vector Learning, pages 185–208, Cambridge, MA, 1999. MIT Press.</p>
<p>[11] G. R¨ tsch. Robust Boosting via Convex Optimization. PhD thesis, University of Potsdam, a Computer Science Dept., August-Bebel-Str. 89, 14482 Potsdam, Germany, 2001.</p>
<p>[12] G. R¨ tsch, A. Demiriz, and K. Bennett. Sparse regression ensembles in inﬁnite and ﬁnite hya pothesis spaces. Machine Learning, 48(1-3):193–221, 2002. Special Issue on New Methods for Model Selection and Model Combination. Also NeuroCOLT2 Technical Report NC-TR-2000085.</p>
<p>[13] G. R¨ tsch, S. Sonnenburg, and C. Sch¨ fer. Learning interpretable svms for biological sequence a a classiﬁcation. BMC Bioinformatics, Special Issue from NIPS workshop on New Problems and Methods in Computational Biology Whistler, Canada, 18 December 2004, 7(Suppl. 1:S9), February 2006.</p>
<p>[14] G. R¨ tsch and M.K. Warmuth. Marginal boosting. NeuroCOLT2 Technical Report 97, Royal a Holloway College, London, July 2001.</p>
<p>[15] B. Sch¨ lkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002. o</p>
<p>[16] S. Sonnenburg, G. R¨ tsch, and C. Sch¨ fer. Learning interpretable SVMs for biological sea a quence classiﬁcation. In RECOMB 2005, LNBI 3500, pages 389–407. Springer-Verlag Berlin Heidelberg, 2005.</p>
<p>[17] S. Sonnenburg, G. R¨ tsch, S. Sch¨ fer, and B. Sch¨ lkopf. Large scale multiple kernel learning. a a o Journal of Machine Learning Research, 2006. accepted.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
