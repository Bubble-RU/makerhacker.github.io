<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-137" href="../nips2005/nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">nips2005-137</a> <a title="nips-2005-137-reference" href="#">nips2005-137-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</h1>
<br/><p>Source: <a title="nips-2005-137-pdf" href="http://papers.nips.cc/paper/2854-non-gaussian-component-analysis-a-semi-parametric-framework-for-linear-dimension-reduction.pdf">pdf</a></p><p>Author: Gilles Blanchard, Masashi Sugiyama, Motoaki Kawanabe, Vladimir Spokoiny, Klaus-Robert Müller</p><p>Abstract: We propose a new linear method for dimension reduction to identify nonGaussian components in high dimensional data. Our method, NGCA (non-Gaussian component analysis), uses a very general semi-parametric framework. In contrast to existing projection methods we deﬁne what is uninteresting (Gaussian): by projecting out uninterestingness, we can estimate the relevant non-Gaussian subspace. We show that the estimation error of ﬁnding the non-Gaussian components tends to zero at a parametric rate. Once NGCA components are identiﬁed and extracted, various tasks can be applied in the data analysis process, like data visualization, clustering, denoising or classiﬁcation. A numerical study demonstrates the usefulness of our method. 1</p><br/>
<h2>reference text</h2><p>[1] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15(6):1373–1396, 2003.</p>
<p>[2] C.M. Bishop, M. Svensen and C.K.I. Wiliams. GTM: The generative topographic mapping. Neural Computation, 10(1):215–234, 1998. ¨</p>
<p>[3] G. Blanchard, M. Sugiyama, M. Kawanabe, V. Spokoiny, K.-R. M uller. In search of nonGaussian components of a high-dimensional distribution. Technical report of the Weierstrass Institute for Applied Analysis and Stochastics, 2006.</p>
<p>[4] T.F. Cox and M.A.A. Cox. Multidimensional Scaling. Chapman & Hall, London, 2001.</p>
<p>[5] J.H. Friedman and J.W. Tukey. A projection pursuit algorithm for exploratory data analysis. IEEE Transactions on Computers, 23(9):881–890, 1975. ¨</p>
<p>[6] S. Harmeling, A. Ziehe, M. Kawanabe and K.-R. Muller. Kernel-based nonlinear blind source separation. Neural Computation, 15(5):1089–1124, 2003.</p>
<p>[7] P.J. Huber. Projection pursuit. The Annals of Statistics, 13:435–475, 1985.</p>
<p>[8] A. Hyv¨ rinen. Fast and robust ﬁxed-point algorithms for independent component analysis. IEEE a Transactions on Neural Networks, 10(3):626–634, 1999.</p>
<p>[9] A. Hyv¨ rinen, J. Karhunen and E. Oja. Independent component analysis. Wiley, 2001. a</p>
<p>[10] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, 2000. ¨</p>
<p>[11] B. Sch¨ lkopf, A.J. Smola and K.–R. Muller. Nonlinear component analysis as a kernel Eigeno value problem. Neural Computation, 10(5):1299–1319, 1998.</p>
<p>[12] J.B. Tenenbaum, V. de Silva and J.C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
