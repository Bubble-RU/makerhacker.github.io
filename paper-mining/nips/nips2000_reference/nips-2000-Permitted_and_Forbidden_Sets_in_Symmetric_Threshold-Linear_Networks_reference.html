<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>100 nips-2000-Permitted and Forbidden Sets in Symmetric Threshold-Linear Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-100" href="../nips2000/nips-2000-Permitted_and_Forbidden_Sets_in_Symmetric_Threshold-Linear_Networks.html">nips2000-100</a> <a title="nips-2000-100-reference" href="#">nips2000-100-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>100 nips-2000-Permitted and Forbidden Sets in Symmetric Threshold-Linear Networks</h1>
<br/><p>Source: <a title="nips-2000-100-pdf" href="http://papers.nips.cc/paper/1793-permitted-and-forbidden-sets-in-symmetric-threshold-linear-networks.pdf">pdf</a></p><p>Author: Richard H. R. Hahnloser, H. Sebastian Seung</p><p>Abstract: Ascribing computational principles to neural feedback circuits is an important problem in theoretical neuroscience. We study symmetric threshold-linear networks and derive stability results that go beyond the insights that can be gained from Lyapunov theory or energy functions. By applying linear analysis to subnetworks composed of coactive neurons, we determine the stability of potential steady states. We find that stability depends on two types of eigenmodes. One type determines global stability and the other type determines whether or not multistability is possible. We can prove the equivalence of our stability criteria with criteria taken from quadratic programming. Also, we show that there are permitted sets of neurons that can be coactive at a steady state and forbidden sets that cannot. Permitted sets are clustered in the sense that subsets of permitted sets are permitted and supersets of forbidden sets are forbidden. By viewing permitted sets as memories stored in the synaptic connections, we can provide a formulation of longterm memory that is more general than the traditional perspective of fixed point attractor networks. A Lyapunov-function can be used to prove that a given set of differential equations is convergent. For example, if a neural network possesses a Lyapunov-function, then for almost any initial condition, the outputs of the neurons converge to a stable steady state. In the past, this stability-property was used to construct attractor networks that associatively recall memorized patterns. Lyapunov theory applies mainly to symmetric networks in which neurons have monotonic activation functions [1, 2]. Here we show that the restriction of activation functions to threshold-linear ones is not a mere limitation, but can yield new insights into the computational behavior of recurrent networks (for completeness, see also [3]). We present three main theorems about the neural responses to constant inputs. The first theorem provides necessary and sufficient conditions on the synaptic weight matrix for the existence of a globally asymptotically stable set of fixed points. These conditions can be expressed in terms of copositivity, a concept from quadratic programming and linear complementarity theory. Alternatively, they can be expressed in terms of certain eigenvalues and eigenvectors of submatrices of the synaptic weight matrix, making a connection to linear systems theory. The theorem guarantees that the network will produce a steady state response to any constant input. We regard this response as the computational output of the network, and its characterization is the topic of the second and third theorems. In the second theorem, we introduce the idea of permitted and forbidden sets. Under certain conditions on the synaptic weight matrix, we show that there exist sets of neurons that are</p><br/>
<h2>reference text</h2><p>[1] J. J. Hopfield. Neurons with graded response have collective properties like those of two-state neurons. Proc. Natl. Acad. Sci. USA, 81:3088- 3092, 1984.</p>
<p>[2] M.A. Cohen and S. Grossberg. Absolute stability of global pattern formation and</p>
<p>[3]</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]  parallel memory storage by competitive neural networks. IEEE Transactions on Systems, Man and Cybernetics, 13:288- 307,1983. Richard H.R. Hahnloser, Rahul Sarpeshkar, Misha Mahowald, Rodney J . Douglas, and Sebastian Seung. Digital selection and ananlog amplification coexist in a silicon circuit inspired by cortex. Nature, 405:947- 51, 2000. R.A. Horn and C.R. Johnson. Matrix analysis. Cambridge University Press, 1985. J. Feng and K.P. Hadeler. Qualitative behaviour of some simple networks. J . Phys. A:, 29:5019- 5033, 1996. R. Ben-Yishai, R. Lev Bar-Or, and H. Sompolinsky. Theory of orientation tuning in visual cortex. Proc. Natl. Acad. Sci. USA, 92:3844- 3848, 1995. R.J. Douglas, C. Koch, M.A. Mahowald, K.A.C. Martin, and H. Suarez. Recurrent excitation in neocortical circuits. Science, 269:981- 985, 1995. Xie Xiaohui, Richard H.R. Hahnloser, and Sebastian Seung. Learning winnertake-all competition between groups of neurons in lateral inhibitory networks. In Proceedings of NIPS2001 - Neural Information Processing Systems: Natural and Synthetic, 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
