<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>37 nips-2000-Convergence of Large Margin Separable Linear Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-37" href="../nips2000/nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">nips2000-37</a> <a title="nips-2000-37-reference" href="#">nips2000-37-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>37 nips-2000-Convergence of Large Margin Separable Linear Classification</h1>
<br/><p>Source: <a title="nips-2000-37-pdf" href="http://papers.nips.cc/paper/1891-convergence-of-large-margin-separable-linear-classification.pdf">pdf</a></p><p>Author: Tong Zhang</p><p>Abstract: Large margin linear classification methods have been successfully applied to many applications. For a linearly separable problem, it is known that under appropriate assumptions, the expected misclassification error of the computed</p><br/>
<h2>reference text</h2><p>[1] lK. Anlauf and M. Biehl. The AdaTron: an adaptive perceptron algorithm. Europhys. Lett., 10(7):687-692, 1989.</p>
<p>[2] C. Cortes and V.N. Vapnik. Support vector networks. Machine Learning, 20:273-297, 1995.</p>
<p>[3] Nello Cristianini and John Shawe-Taylor. An Introduction to Support Vector Machines and other Kernel-based Learning Methods. Cambridge University Press, 2000.</p>
<p>[4] Harro G. Heuser. Functional analysis. John Wiley & Sons Ltd., Chichester, 1982. Translated from the German by John Horvath, A Wiley-Interscience Publication.</p>
<p>[5] W. Kinzel. Statistical mechanics of the perceptron with maximal stability. In Lecture Notes in Physics, volume 368, pages 175-188. Springer-Verlag, 1990.</p>
<p>[6] 1 Kivinen and M.K. Warmuth. Additive versus exponentiated gradient updates for linear prediction. Journal of Infonnation and Computation, 132:1-64, 1997.</p>
<p>[7] M. Opper. Learning times of neural networks: Exact solution for a perceptron algorithm. Phys. Rev. A, 38(7):3824-3826, 1988.</p>
<p>[8] M. Opper. Learning in neural networks: Solvable dynamics. Europhysics Letters, 8(4):389-392,1989.</p>
<p>[9] R. Tyrrell Rockafellar. Convex analysis. Princeton University Press, Princeton, NJ, 1970.</p>
<p>[10] Dale Schuurmans. Characterizing rational versus exponential learning curves. J. Comput. Syst. Sci., 55:140-160, 1997.</p>
<p>[11] V.N. Vapnik. Statistical learning theory. John Wiley & Sons, New York, 1998.</p>
<p>[12] Robert C. Williamson, Alexander 1 Smola, and Bernhard Scholkopf. Entropy numbers of linear function classes. In COLT'OO, pages 309-319,2000.</p>
<p>[13] Vadim Yurinsky. Sums and Gaussian vectors. Springer-Verlag, Berlin, 1995.</p>
<p>[14] Tong Zhang. Analysis of regularized linear functions for classification problems. Technical Report RC-21572, IBM, 1999. Abstract in NIPS'99, pp. 370-376.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
