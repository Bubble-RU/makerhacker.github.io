<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>97 nips-2000-Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-97" href="../nips2000/nips-2000-Overfitting_in_Neural_Nets%3A_Backpropagation%2C_Conjugate_Gradient%2C_and_Early_Stopping.html">nips2000-97</a> <a title="nips-2000-97-reference" href="#">nips2000-97-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>97 nips-2000-Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping</h1>
<br/><p>Source: <a title="nips-2000-97-pdf" href="http://papers.nips.cc/paper/1895-overfitting-in-neural-nets-backpropagation-conjugate-gradient-and-early-stopping.pdf">pdf</a></p><p>Author: Rich Caruana, Steve Lawrence, C. Lee Giles</p><p>Abstract: The conventional wisdom is that backprop nets with excess hidden units generalize poorly. We show that nets with excess capacity generalize well when trained with backprop and early stopping. Experiments suggest two reasons for this: 1) Overfitting can vary significantly in different regions of the model. Excess capacity allows better fit to regions of high non-linearity, and backprop often avoids overfitting the regions of low non-linearity. 2) Regardless of size, nets learn task subcomponents in similar sequence. Big nets pass through stages similar to those learned by smaller nets. Early stopping can stop training the large net when it generalizes comparably to a smaller net. We also show that conjugate gradient can yield worse generalization because it overfits regions of low non-linearity when learning to fit regions of high non-linearity.</p><br/>
<h2>reference text</h2><p>[1] Peter L. Bartlett. For valid generalization the size of the weights is more important than the size of the network. In Advances in Neural Information Processing Systems, volume 9, page 134. The MIT Press, 1997.</p>
<p>[2] E.B. Baum and D. Haussler. What size net gives valid generalization? Neural Computation, 1(1):151- 160,1989.</p>
<p>[3] C. Darken and J.E. Moody. Note on learning rate schedules for stochastic optimization. In Advances in Neural Information Processing Systems, volume 3, pages 832- 838. Morgan Kaufmann, 1991.</p>
<p>[4] S. Geman et al. Neural networks and the bias/variance dilemma. Neural Computation, 4(1):158,1992.</p>
<p>[5] A Krogh and J.A Hertz. A simple weight decay can improve generalization. In Advances in Neural Information Processing Systems, volume 4, pages 950-957. Morgan Kaufmann, 1992.</p>
<p>[6] Y. Le Cun, J.S. Denker, and S.A Solla. Optimal Brain Damage. In D.S. Touretzky, editor, Advances in Neural Information Processing Systems, volume 2, pages 598-605, San Mateo, 1990. (Denver 1989), Morgan Kaufmann.</p>
<p>[7] G.L. Martin and J.A Pittman. Recognizing hand-printed letters and digits using backpropagation learning. Neural Computation, 3:258-267, 1991.</p>
<p>[8] J.E. Moody. The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. In Advances in Neural Information Processing Systems, volume 4, pages 847-854. Morgan Kaufmann, 1992.</p>
<p>[9] D.A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In D.S. Touretzky, editor, Advances in Neural Information Processing Systems, volume 1, pages 305-313, San Mateo, 1989. (Denver 1988), Morgan Kaufmann.</p>
<p>[10] T. Sejnowski and C. Rosenberg. Parallel networks that learn to pronounce English text. Complex Systems, 1:145-168, 1987.</p>
<p>[11] A Weigend. On overfitting and the effective number of hidden units. In Proceedings of the 1993 Connectionist Models Summer School, pages 335- 342. Lawrence Erlbaum Associates, 1993.</p>
<p>[12] AS. Weigend, D.E. Rumelhart, and B.A Huberman. Generalization by weight-elimination with application to forecasting. In Advances in Neural Information Processing Systems, volume 3, pages 875-882. Morgan Kaufmann, 1991.</p>
<p>[13] D. Wolpert. On bias plus variance. Neural Computation, 9(6):1211-1243, 1997.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
