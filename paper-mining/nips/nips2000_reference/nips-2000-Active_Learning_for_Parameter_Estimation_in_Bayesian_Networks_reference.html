<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-17" href="../nips2000/nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">nips2000-17</a> <a title="nips-2000-17-reference" href="#">nips2000-17-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>17 nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</h1>
<br/><p>Source: <a title="nips-2000-17-pdf" href="http://papers.nips.cc/paper/1795-active-learning-for-parameter-estimation-in-bayesian-networks.pdf">pdf</a></p><p>Author: Simon Tong, Daphne Koller</p><p>Abstract: Bayesian networks are graphical representations of probability distributions. In virtually all of the work on learning these networks, the assumption is that we are presented with a data set consisting of randomly generated instances from the underlying distribution. In many situations, however, we also have the option of active learning, where we have the possibility of guiding the sampling process by querying for certain types of samples. This paper addresses the problem of estimating the parameters of Bayesian networks in an active learning setting. We provide a theoretical framework for this problem, and an algorithm that chooses which active learning queries to generate based on the model learned so far. We present experimental results showing that our active learning algorithm can significantly reduce the need for training data in many situations.</p><br/>
<h2>reference text</h2><p>[1] A.c. Atkinson and A.N. Donev. Optimal Experimental Designs. Oxford University Press, 1992.</p>
<p>[2] D. Cohn, Z. Ghahramani, and M. Jordan. Active learning with statistical models. Journal of Artificial intelligence Research, 4, 1996.</p>
<p>[3] T.M Cover and J.A. Thomas. information Theory. Wiley, 1991.</p>
<p>[4] M. H. DeGroot. Optimal Statistical Decisions. McGraw-Hill, New York, 1970.</p>
<p>[5] D. Heckerman, D. Geiger, and D. M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning, 20: 197-243, 1995.</p>
<p>[6] S. L. Lauritzen and D. J. Spiegelhalter. Local computations with probabilities on graphical structures and their application to expert systems. J. Royal Statistical Society, B 50(2), 1988.</p>
<p>[7] D. MacKay. Information-based objective functions for active data selection. Neural Computation,4:590-604, 1992.</p>
<p>[8] J. Pearl. Causality: Models, Reasoning, and inference. Cambridge University Press, 2000.</p>
<p>[9] H.S. Seung, M. Opper, and H. Sompolinsky. Query by committee. In Froc. COLT, pages 287294,1992.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
