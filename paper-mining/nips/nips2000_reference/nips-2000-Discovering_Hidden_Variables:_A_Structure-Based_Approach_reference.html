<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-41" href="../nips2000/nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">nips2000-41</a> <a title="nips-2000-41-reference" href="#">nips2000-41-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>41 nips-2000-Discovering Hidden Variables: A Structure-Based Approach</h1>
<br/><p>Source: <a title="nips-2000-41-pdf" href="http://papers.nips.cc/paper/1940-discovering-hidden-variables-a-structure-based-approach.pdf">pdf</a></p><p>Author: Gal Elidan, Noam Lotner, Nir Friedman, Daphne Koller</p><p>Abstract: A serious problem in learning probabilistic models is the presence of hidden variables. These variables are not observed, yet interact with several of the observed variables. As such, they induce seemingly complex dependencies among the latter. In recent years, much attention has been devoted to the development of algorithms for learning parameters, and in some cases structure, in the presence of hidden variables. In this paper, we address the related problem of detecting hidden variables that interact with the observed variables. This problem is of interest both for improving our understanding of the domain and as a preliminary step that guides the learning procedure towards promising models. A very natural approach is to search for</p><br/>
<h2>reference text</h2><p>[1] 1. Beinlich, G. Suermondt, R. Chavez, and G. Cooper. The ALARM monitoring system. In Proc. 2 'nd European Conf. on AI and Medicine. , 1989.</p>
<p>[2] J. Binder, D. Koller, S. Russell, and K. Kanazawa. Adaptive probabilistic networks with hidden variables. Machine Learning, 29:213- 244, 1997.</p>
<p>[3] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. J. Royal Stat. Soc., B 39:1- 39, 1977.</p>
<p>[4] N. Friedman. The Bayesian structural EM algorithm. In UAJ, 1998 .</p>
<p>[5] N. Friedman and D. Koller. Being Bayesian about Network Structure. In UAI, 2000.</p>
<p>[6] N. Friedman, M. Goldszmidt, and A. Wyner. Data analysis with Bayesian networks: A bootstrap approach. In UAJ, 1999.</p>
<p>[7] D. Heckerman. A tutorial on learning with Bayesian networks. In Learning in Graphical Models. 1998.</p>
<p>[8] D. Heckerman, D. Geiger, and D. M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning, 20: 197- 243, 1995.</p>
<p>[9] S. L. Lauritzen. The EM algorithm for graphical association models with missing data. Camp. Stat.and Data Ana., 19:191- 201,1995.</p>
<p>[10] J. Martin and K. VanLehn. Discrete factor analysis: Learning hidden variables in Bayesian networks. Technical report, Department of Computer Science, University of Pittsburgh, 1995.</p>
<p>[11] P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction and Search. Springer-Verlag, 1993.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
