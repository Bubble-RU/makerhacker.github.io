<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-39" href="../nips2000/nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">nips2000-39</a> <a title="nips-2000-39-reference" href="#">nips2000-39-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>39 nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</h1>
<br/><p>Source: <a title="nips-2000-39-pdf" href="http://papers.nips.cc/paper/1915-decomposition-of-reinforcement-learning-for-admission-control-of-self-similar-call-arrival-processes.pdf">pdf</a></p><p>Author: Jakob Carlstr√∂m</p><p>Abstract: This paper presents predictive gain scheduling, a technique for simplifying reinforcement learning problems by decomposition. Link admission control of self-similar call traffic is used to demonstrate the technique. The control problem is decomposed into on-line prediction of near-future call arrival rates, and precomputation of policies for Poisson call arrival processes. At decision time, the predictions are used to select among the policies. Simulations show that this technique results in significantly faster learning without any performance loss, compared to a reinforcement learning controller that does not decompose the problem. 1</p><br/>
<h2>reference text</h2><p>[1] Z. Dziong, ATM Network Resource Management, McGraw-Hill, 1997.</p>
<p>[2] D.P. Bertsekas, Dynamic Programming and Optimal Control, Athena Scientific, Belmont, Mass., 1995.</p>
<p>[3] V. Paxson and S. Floyd, </p>
<p>[4] W.E. Leland, M.S. Taqqu, W. Willinger and D.V. Wilson, </p>
<p>[5] A Feldman, AC. Gilbert, W. Willinger and T.G. Kurtz, </p>
<p>[6] R.S. Sutton and AG. Barto, Reinforcement Learning: An Introduction, MIT Press, Cambridge, Mass., 1998.</p>
<p>[7] J. Carlstrom and E. Nordstrom, </p>
<p>[8] Z. Dziong and L. Mason,</p>
<p>[9] J. Carlstrom and E. Nordstrom, </p>
<p>[10] P. Marbach, O. Mihatsch and J.N. Tsitsiklis, </p>
<p>[11] H. Tong and T. Brown, </p>
<p>[12] K.J. Astrom and B. Wittenmark, Adaptive Control, 2 nd ed., Addison-Wesley, 1995.</p>
<p>[13] S. Haykin, Neural Networks: A Comprehensive Foundation, 2nd ed., Macmillan College Publishing Co., Englewood Cliffs, NJ, 1999.</p>
<p>[14] J. Carlstrom, </p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
