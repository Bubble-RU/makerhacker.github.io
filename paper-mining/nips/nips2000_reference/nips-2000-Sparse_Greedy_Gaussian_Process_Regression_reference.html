<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>120 nips-2000-Sparse Greedy Gaussian Process Regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-120" href="../nips2000/nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">nips2000-120</a> <a title="nips-2000-120-reference" href="#">nips2000-120-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>120 nips-2000-Sparse Greedy Gaussian Process Regression</h1>
<br/><p>Source: <a title="nips-2000-120-pdf" href="http://papers.nips.cc/paper/1880-sparse-greedy-gaussian-process-regression.pdf">pdf</a></p><p>Author: Alex J. Smola, Peter L. Bartlett</p><p>Abstract: We present a simple sparse greedy technique to approximate the maximum a posteriori estimate of Gaussian Processes with much improved scaling behaviour in the sample size m. In particular, computational requirements are O(n 2 m), storage is O(nm), the cost for prediction is 0 (n) and the cost to compute confidence bounds is O(nm), where n Â«: m. We show how to compute a stopping criterion, give bounds on the approximation error, and show applications to large scale problems. 1</p><br/>
<h2>reference text</h2><p>[1] S. Fine and K Scheinberg. Efficient SVM training using low-rank kernel representation. Technical report, IBM Watson Research Center, New York, 2000.</p>
<p>[2] M. Gibbs and D .J .C . Mackay. Efficient implementation of gaussian processes. Technical report, Cavendish Laboratory, Cambridge, UK, 1997.</p>
<p>[3] F. Girosi. An equivalence between sparse approximation and support vector machines. Neural Computation, 10(6):1455-1480, 1998.</p>
<p>[4] S. Mallat and Z. Zhang. Matching Pursuit in a time-frequency dictionary. IEEE Transactions on Signal Processing, 41:3397-3415, 1993.</p>
<p>[5] B. K Natarajan. Sparse approximate solutions to linear systems. SIAM Journal of Computing, 25(2) :227-234, 1995.</p>
<p>[6] B. Sch6lkopf, S. Mika, C. Burges, P . Knirsch, K-R. Miiller, G . Ratsch, and A. Smola. Input space vs. feature space in kernel-based methods. IEEE Transactions on Neural Networks, 10(5):1000 - 1017, 1999.</p>
<p>[7] A.J . Smola and B. Sch6lkopf. Sparse greedy matrix approximation for machine learning. In P. Langley, editor, Proceedings of the 17th International Conference on Machine Learning, pages 911 - 918, San Francisco, 2000. Morgan Kaufman.</p>
<p>[8] C .KI. Williams and M. Seeger. The effect of the input density distribution on kernelbased classifiers. In P. Langley, editor, Proceedings of the Seventeenth International Conference on Machine Learning, pages 1159 - 1166, San Francisco, California, 2000. Morgan Kaufmann.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
