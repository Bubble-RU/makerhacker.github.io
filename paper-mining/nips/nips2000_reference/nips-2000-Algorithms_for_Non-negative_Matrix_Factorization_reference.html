<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>22 nips-2000-Algorithms for Non-negative Matrix Factorization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-22" href="../nips2000/nips-2000-Algorithms_for_Non-negative_Matrix_Factorization.html">nips2000-22</a> <a title="nips-2000-22-reference" href="#">nips2000-22-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>22 nips-2000-Algorithms for Non-negative Matrix Factorization</h1>
<br/><p>Source: <a title="nips-2000-22-pdf" href="http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf">pdf</a></p><p>Author: Daniel D. Lee, H. Sebastian Seung</p><p>Abstract: Non-negative matrix factorization (NMF) has previously been shown to be a useful decomposition for multivariate data. Two different multiplicative algorithms for NMF are analyzed. They differ only slightly in the multiplicative factor used in the update rules. One algorithm can be shown to minimize the conventional least squares error while the other minimizes the generalized Kullback-Leibler divergence. The monotonic convergence of both algorithms can be proven using an auxiliary function analogous to that used for proving convergence of the ExpectationMaximization algorithm. The algorithms can also be interpreted as diagonally rescaled gradient descent, where the rescaling factor is optimally chosen to ensure convergence.</p><br/>
<h2>reference text</h2><p>[1] Jolliffe, IT (1986). Principal Component Analysis. New York: Springer-Verlag.</p>
<p>[2] Turk, M & Pentland, A (1991). Eigenfaces for recognition. J. Cogn. Neurosci. 3, 71- 86.</p>
<p>[3] Gersho, A & Gray, RM (1992). Vector Quantization and Signal Compression. Kluwer Acad. Press.</p>
<p>[4] Lee, DD & Seung, HS . Unsupervised learning by convex and conic coding (1997). Proceedings of the Conference on Neural Information Processing Systems 9, 515- 521.</p>
<p>[5] Lee, DD & Seung, HS (1999). Learning the parts of objects by non-negative matrix factorization. Nature 401, 788- 791.</p>
<p>[6] Field, DJ (1994). What is the goal of sensory coding? Neural Comput. 6, 559-601.</p>
<p>[7] Foldiak, P & Young, M (1995). Sparse coding in the primate cortex. The Handbook of Brain Theory and Neural Networks, 895- 898. (MIT Press, Cambridge, MA).</p>
<p>[8] Press, WH, Teukolsky, SA, Vetterling, WT & Flannery, BP (1993). Numerical recipes: the art of scientific computing. (Cambridge University Press, Cambridge, England).</p>
<p>[9] Shepp, LA & Vardi, Y (1982) . Maximum likelihood reconstruction for emission tomography. IEEE Trans . MI-2, 113- 122.</p>
<p>[10] Richardson, WH (1972) . Bayesian-based iterative method of image restoration. 1. Opt. Soc. Am. 62, 55- 59.</p>
<p>[11] Lucy, LB (1974). An iterative technique for the rectification of observed distributions. Astron. J. 74, 745- 754.</p>
<p>[12] Bouman, CA & Sauer, K (1996). A unified approach to statistical tomography using coordinate descent optimization. IEEE Trans. Image Proc. 5, 480--492.</p>
<p>[13] Paatero, P & Tapper, U (1997). Least squares formulation of robust non-negative factor analysis. Chemometr. Intell. Lab. 37, 23- 35.</p>
<p>[14] Kivinen, J & Warmuth, M (1997). Additive versus exponentiated gradient updates for linear prediction. Journal of Tnformation and Computation 132, 1-64.</p>
<p>[15] Dempster, AP, Laird, NM & Rubin, DB (1977). Maximum likelihood from incomplete data via the EM algorithm. J. Royal Stat. Soc. 39, 1-38.</p>
<p>[16] Saul, L & Pereira, F (1997). Aggregate and mixed-order Markov models for statistical language processing. In C. Cardie and R. Weischedel (eds). Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, 81- 89. ACL Press.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
