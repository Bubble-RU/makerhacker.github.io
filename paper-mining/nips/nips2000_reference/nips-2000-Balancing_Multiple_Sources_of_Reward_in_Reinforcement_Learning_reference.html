<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-28" href="../nips2000/nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">nips2000-28</a> <a title="nips-2000-28-reference" href="#">nips2000-28-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>28 nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2000-28-pdf" href="http://papers.nips.cc/paper/1831-balancing-multiple-sources-of-reward-in-reinforcement-learning.pdf">pdf</a></p><p>Author: Christian R. Shelton</p><p>Abstract: For many problems which would be natural for reinforcement learning, the reward signal is not a single scalar value but has multiple scalar components. Examples of such problems include agents with multiple goals and agents with multiple users. Creating a single reward value by combining the multiple components can throwaway vital information and can lead to incorrect solutions. We describe the multiple reward source problem and discuss the problems with applying traditional reinforcement learning. We then present an new algorithm for finding a solution and results on simulated environments.</p><br/>
<h2>reference text</h2><p>[1] 1. Hu and M. P. Wellman. Multiagent reinforcement learning: Theoretical framework and an algorithm. In Froc. of the 15th International Con! on Machine Learning, pages 242- 250, 1998.</p>
<p>[2] C. L. Isbell, C. R. Shelton, M. Kearns, S. Singh, and P. Stone. A social reinforcement learning agent. 2000. submitted to Autonomous Agents 2001.</p>
<p>[3] 1. Karlsson. Learning to Solve Multiple Goals. PhD thesis, University of Rochester, 1997.</p>
<p>[4] M. Kearns, Y. Mansouor, and S. Singh. Fast planning in stochastic games. In Proc. of the 16th Conference on Uncertainty in Artificial Intelligence , 2000.</p>
<p>[5] M. L. Littman. Markov games as a framework for multi-agent reinforcement learning. In Proc. of the 11th International Conference on Machine Learning, pages 157-163, 1994.</p>
<p>[6] G. Owen. Game Theory. Academic Press, UK, 1995.</p>
<p>[7] S. Singh, M. Kearns, and Y. Mansour. Nash convergence of gradient dynamics in general-sum games. In Proc. of the 16th Conference on Uncertainty in Artificial Intelligence , 2000.</p>
<p>[8] S. P. Singh. The efficient learning of multiple task sequences. In NIPS, volume 4, 1992.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
