<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>24 nips-2000-An Information Maximization Approach to Overcomplete and Recurrent Representations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-24" href="../nips2000/nips-2000-An_Information_Maximization_Approach_to_Overcomplete_and_Recurrent_Representations.html">nips2000-24</a> <a title="nips-2000-24-reference" href="#">nips2000-24-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>24 nips-2000-An Information Maximization Approach to Overcomplete and Recurrent Representations</h1>
<br/><p>Source: <a title="nips-2000-24-pdf" href="http://papers.nips.cc/paper/1863-an-information-maximization-approach-to-overcomplete-and-recurrent-representations.pdf">pdf</a></p><p>Author: Oren Shriki, Haim Sompolinsky, Daniel D. Lee</p><p>Abstract: The principle of maximizing mutual information is applied to learning overcomplete and recurrent representations. The underlying model consists of a network of input units driving a larger number of output units with recurrent interactions. In the limit of zero noise, the network is deterministic and the mutual information can be related to the entropy of the output units. Maximizing this entropy with respect to both the feedforward connections as well as the recurrent interactions results in simple learning rules for both sets of parameters. The conventional independent components (ICA) learning algorithm can be recovered as a special case where there is an equal number of output units and no recurrent connections. The application of these new learning rules is illustrated on a simple two-dimensional input example.</p><br/>
<h2>reference text</h2><p>[1] Jolliffe, IT (1986). Principal Component Analysis. New York: Springer-Verlag.</p>
<p>[2] Hayldn, S (1999). Neural networks: a comprehensive foundation. 2nd ed., Prentice-Hall, Upper Saddle River, NJ.</p>
<p>[3] Jutten, C & Herault, J (1991). Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture. Signal Processing 24,1-10.</p>
<p>[4] Hinton, G & Ghahramani, Z (1997). Generative models for discovering sparse distributed representations. Philosophical Transactions Royal Society B 352, 1177-1190.</p>
<p>[5] Pearlmutter, B & Parra, L (1996). A context-sensitive generalization of ICA. In ICONIP'96, 151-157.</p>
<p>[6] Bell, AJ & Sejnowsld, TJ (1995). An information maximization approach to blind separation and blind deconvolution. Neural Comput. 7, 1129- 1159.</p>
<p>[7] Barlow, HB (1989). Unsupervised learning. Neural Comput. 1,295-311.</p>
<p>[8] Linsker, R (1992). Local synaptic learning rules suffice to maximize mutual information in a linear network. Neural Comput. 4,691-702.</p>
<p>[9] Parra, L, Deco, G, & Miesbach, S (1996). Statistical independence and novelty detection with information preserving nonlinear maps. Neural Comput. 8,260-269.</p>
<p>[10] Amari, S, Cichocld, A & Yang, H (1996). A new learning algorithm for blind signal separation. Advances in Neural Information Processing Systems 8, 757-763.</p>
<p>[11] Lewicki, MS & Sejnowsld, TJ (2000). Learning overcomplete representations. Neural Computation 12 337- 365.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
