<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-77" href="../nips2000/nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">nips2000-77</a> <a title="nips-2000-77-reference" href="#">nips2000-77-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>77 nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</h1>
<br/><p>Source: <a title="nips-2000-77-pdf" href="http://papers.nips.cc/paper/1825-learning-curves-for-gaussian-processes-regression-a-framework-for-good-approximations.pdf">pdf</a></p><p>Author: DÃ¶rthe Malzahn, Manfred Opper</p><p>Abstract: Based on a statistical mechanics approach, we develop a method for approximately computing average case learning curves for Gaussian process regression models. The approximation works well in the large sample size limit and for arbitrary dimensionality of the input space. We explain how the approximation can be systematically improved and argue that similar techniques can be applied to general likelihood models. 1</p><br/>
<h2>reference text</h2><p>[1] D. J. C. Mackay, Gaussian Processes, A Replacement for ral Networks, NIPS tutorial 1997, May be obtained http://wol.ra.phy.cam.ac.uk/pub/mackay/.  Neufrom</p>
<p>[2] C. K. I. Williams and C. E. Rasmussen, Gaussian Processes for Regression, in Neural Information Processing Systems 8, D. S. Touretzky, M. C. Mozer and M. E. Hasselmo eds., 514-520, MIT Press (1996).</p>
<p>[3] C. K. I. Williams, Computing with Infinite Networks, in Neural Information Processing Systems 9, M. C. Mozer, M. I. Jordan and T. Petsche, eds., 295-30l. MIT Press (1997).</p>
<p>[4] D. Barber and C. K. I. Williams, Gaussian Processes for Bayesian Classification via Hybrid Monte Carlo, in Neural Information Processing Systems 9, M . C. Mozer, M. I. Jordan and T. Petsche, eds., 340-346. MIT Press (1997).</p>
<p>[5] P. Sollich, Learning curves for Gaussian processes, in Neural Information Processing Systems 11, M. S. Kearns, S. A. Solla and D. A. Cohn, eds. 344 - 350, MIT Press (1999).</p>
<p>[6] L. Csata, E. Fokoue, M. Opper, B. Schottky, and O. Winther. Efficient approaches to Gaussian process classification. In Advances in Neural Information Processing Systems, volume 12, 2000.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
