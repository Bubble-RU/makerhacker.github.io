<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-9" href="../nips2000/nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">nips2000-9</a> <a title="nips-2000-9-reference" href="#">nips2000-9-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>9 nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</h1>
<br/><p>Source: <a title="nips-2000-9-pdf" href="http://papers.nips.cc/paper/1844-a-pac-bayesian-margin-bound-for-linear-classifiers-why-svms-work.pdf">pdf</a></p><p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: We present a bound on the generalisation error of linear classifiers in terms of a refined margin quantity on the training set. The result is obtained in a PAC- Bayesian framework and is based on geometrical arguments in the space of linear classifiers. The new bound constitutes an exponential improvement of the so far tightest margin bound by Shawe-Taylor et al. [8] and scales logarithmically in the inverse margin. Even in the case of less training examples than input dimensions sufficiently large margins lead to non-trivial bound values and - for maximum margins - to a vanishing complexity term. Furthermore, the classical margin is too coarse a measure for the essential quantity that controls the generalisation error: the volume ratio between the whole hypothesis space and the subset of consistent hypotheses. The practical relevance of the result lies in the fact that the well-known support vector machine is optimal w.r.t. the new bound only if the feature vectors are all of the same length. As a consequence we recommend to use SVMs on normalised feature vectors only - a recommendation that is well supported by our numerical experiments on two benchmark data sets. 1</p><br/>
<h2>reference text</h2><p>[1) N. Alon, S. Ben-David, N. Cesa-Bianchi, and D. Haussler. Scale sensitive dimensions, uniform convergence and learnability. Journal of the ACM, 44(4}:615-631, 1997. [2) R. Herbrich. Learning Linear Classifiers - Theory and Algorithms. PhD thesis, Technische Universitat Berlin, 2000. accepted for publication by MIT Press. [3) R. Herbrich, T. Graepel, and C. Campbell. Bayesian learning in reproducing kernel Hilbert spaces. Technical report, Technical University of Berlin, 1999. TR 99-11. [4) M. J. Kearns and R. Schapire. Efficient distribution-free learning of probabilistic concepts. Journal of Computer and System Sciences , 48(2}:464-497, 1993. [5) D. A. McAllester. Some PAC Bayesian theorems. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, pages 230- 234 , Madison, Wisconsin, 1998. [6) N. Sauer. On the density of families of sets. Journal of Combinatorial Theory, Series A, 13:145- 147, 1972. [7) R. E. Schapire, Y. Freund, P. Bartlett, and W. S. Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. In Proceedings of the 14- th International Conference in Machine Learning, 1997. [8) J. Shawe-Taylor, P. L. Bartlett, R. C. Williamson, and M. Anthony. Structural risk minimization over data-dependent hierarchies. IEEE Transactions on Information Theory, 44(5}:1926- 1940, 1998. [9) J. Shawe-Taylor and R. C. Williamson. A PAC analysis of a Bayesian estimator. Technical report, Royal Holloway, University of London, 1997. NC2- TR- 1997- 013. [10) UCI. University of California Irvine: Machine Learning Repository, 1990. [11) L. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11}:11341142, 1984. [12) V. Vapnik. Estimation of Dependences Based on Empirical Data. Springer, 1982. [13) V. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995. [14) V. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Application, 16(2}:264- 281, 1971.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
