<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>133 nips-2000-The Kernel Gibbs Sampler</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-133" href="../nips2000/nips-2000-The_Kernel_Gibbs_Sampler.html">nips2000-133</a> <a title="nips-2000-133-reference" href="#">nips2000-133-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>133 nips-2000-The Kernel Gibbs Sampler</h1>
<br/><p>Source: <a title="nips-2000-133-pdf" href="http://papers.nips.cc/paper/1802-the-kernel-gibbs-sampler.pdf">pdf</a></p><p>Author: Thore Graepel, Ralf Herbrich</p><p>Abstract: We present an algorithm that samples the hypothesis space of kernel classifiers. Given a uniform prior over normalised weight vectors and a likelihood based on a model of label noise leads to a piecewise constant posterior that can be sampled by the kernel Gibbs sampler (KGS). The KGS is a Markov Chain Monte Carlo method that chooses a random direction in parameter space and samples from the resulting piecewise constant density along the line chosen. The KGS can be used as an analytical tool for the exploration of Bayesian transduction, Bayes point machines, active learning, and evidence-based model selection on small data sets that are contaminated with label noise. For a simple toy example we demonstrate experimentally how a Bayes point machine based on the KGS outperforms an SVM that is incapable of taking into account label noise. 1</p><br/>
<h2>reference text</h2><p>[1] C. Cortes and V. Vapnik. Support Vector Networks. Machine Learning, 20:273- 297, 1995.</p>
<p>[2] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. Selective sampling using the query by committee algorithm. Machine Learning, 28:133- 168, 1997.</p>
<p>[3] T. Graepel, R. Herbrich, and K. Obermayer. Bayesian Transduction. In Advances in Neural Information System Processing 12, pages 456-462, 2000.</p>
<p>[4] R. Herbrich, T. Graepel, and C. Campbell. Bayesian learning in reproducing kernel Hilbert spaces. Technical report, Technical University of Berlin, 1999. TR 99-1l.</p>
<p>[5] D. MacKay. The evidence framework applied to classification networks. 4(5):720-736, 1992.  Neural Computation,</p>
<p>[6] D. A. McAllester. Some PAC Bayesian theorems. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, pages 230-234, Madison, Wisconsin, 1998.</p>
<p>[7] R. M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. Technical report, Dept. of Computer Science, University of Toronto, 1993. CRG-TR-93-l.</p>
<p>[8] P. Sollich. Probabilistic methods for Support Vector Machines. In Advances in Neural Information Processing Systems 12, pages 349-355, San Mateo, CA, 2000. Morgan Kaufmann.</p>
<p>[9] G. Wahba. Support Vector Machines, Reproducing Kernel Hilbert Spaces and the randomized GACV. Technical report , Department of Statistics, University of Wisconsin, Madison, 1997. TR- NO- 984.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
