<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-142" href="../nips2000/nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">nips2000-142</a> <a title="nips-2000-142-reference" href="#">nips2000-142-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>142 nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</h1>
<br/><p>Source: <a title="nips-2000-142-pdf" href="http://papers.nips.cc/paper/1888-using-free-energies-to-represent-q-values-in-a-multiagent-reinforcement-learning-task.pdf">pdf</a></p><p>Author: Brian Sallans, Geoffrey E. Hinton</p><p>Abstract: The problem of reinforcement learning in large factored Markov decision processes is explored. The Q-value of a state-action pair is approximated by the free energy of a product of experts network. Network parameters are learned on-line using a modified SARSA algorithm which minimizes the inconsistency of the Q-values of consecutive state-action pairs. Actions are chosen based on the current value estimates by fixing the current state and sampling actions from the network using Gibbs sampling. The algorithm is tested on a co-operative multi-agent task. The product of experts model is found to perform comparably to table-based Q-Iearning for small instances of the task, and continues to perform well when the problem becomes too large for a table-based representation.</p><br/>
<h2>reference text</h2><p>[1] R.S Sutton and A.G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998.</p>
<p>[2] A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive elements that can solve difficult learning control problems. IEEE Transactions on Systems, Man and Cybernetics, 13:835846, 1983.</p>
<p>[3] R. S. Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Proc. International Conference on Machine Learning, 1990.</p>
<p>[4] Tommi Jaakkola, Satinder P. Singh, and Michael 1. Jordan. Reinforcement learning algorithm for partially observable Markov decision problems. In Gerald Tesauro, David S. Touretzky, and Todd K. Leen, editors, Advances in Neural Information Processing Systems, volume 7, pages 345-352. The MIT Press, Cambridge, 1995.</p>
<p>[5] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Technical Report GCNU TR 2000-004, Gatsby Computational Neuroscience Unit, UCL, 2000.</p>
<p>[6] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Ma chine Intelligence, 6:721- 741 , 1984.</p>
<p>[7] G.A. Rummery and M. Niranjan. On-line Q-learning using connectionist systems. Technical Report CUEDIF-INFENGfTR 166, Engineering Department, Cambridge University, 1994.</p>
<p>[8] R.S. Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse coding. In Touretzky et al. [14], pages 1038- 1044.</p>
<p>[9] M.L. Littman, A.R. Cassandra, and L.P. Kaelbling. Learning policies for partially observable environments: Scaling up. In Proc. International Conference on Machine Learning, 1995 .</p>
<p>[10] D.P. Bertsekas and J.N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, Belmont, MA, 1996.</p>
<p>[11] R. M. Neal. Connectionist learning of belief networks. Artificial Intelligence, 56:71- 113, 1992.</p>
<p>[12] T. S. Jaakkola. Variational Methods for Inference and Estimation in Graphical Models . Department of Brain and Cognitive Sciences, MIT, Cambridge, MA, 1997. Ph.D. thesis.</p>
<p>[13] Philip N. Sabes and Michael 1. Jordan. Reinforcement learning by probability matching. In Touretzky et al. [14], pages 1080-1086.</p>
<p>[14] David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, editors. Advances in Neural Information Processing Systems, volume 8. The MIT Press, Cambridge, 1996.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
