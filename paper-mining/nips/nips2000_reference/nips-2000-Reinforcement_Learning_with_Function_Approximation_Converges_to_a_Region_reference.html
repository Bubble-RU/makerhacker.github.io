<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-112" href="../nips2000/nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">nips2000-112</a> <a title="nips-2000-112-reference" href="#">nips2000-112-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>112 nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</h1>
<br/><p>Source: <a title="nips-2000-112-pdf" href="http://papers.nips.cc/paper/1911-reinforcement-learning-with-function-approximation-converges-to-a-region.pdf">pdf</a></p><p>Author: Geoffrey J. Gordon</p><p>Abstract: Many algorithms for approximate reinforcement learning are not known to converge. In fact, there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point. This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region. The algorithms are SARSA(O) and V(O); the latter algorithm was used in the well-known TD-Gammon program. 1</p><br/>
<h2>reference text</h2><p>[1] R. S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3(1):9-44, 1988.</p>
<p>[2] Geoffrey J. Gordon. Stable function approximation in dynamic programming. Technical Report CMU-CS-95-103, Carnegie Mellon University, 1995.</p>
<p>[3] L. C. Baird. Residual algorithms: Reinforcement learning with function approximation. In Machine Learning: proceedings of the twelfth international conference, San Francisco, CA, 1995. Morgan Kaufmann.</p>
<p>[4] Geoffrey J. Gordon. Chattering in SARSA(A). Internal report, 1996. CMU Learning Lab. Available from www.es . emu. edu;-ggordon.</p>
<p>[5] R. S. Sutton. Open theoretical questions in reinforcement learning. In P. Fischer and H. U. Simon, editors, Computational Learning Theory (Proceedings of EuroCOLT'99), pages 11- 17, 1999.</p>
<p>[6] D. P. de Farias and B. Van Roy. On the existence of fixed points for approximate value iteration and temporal-difference learning. Journal of Optimization Theory and Applications, 105(3), 2000.</p>
<p>[7] Gavin A. Rummery and Mahesan Niranjan. On-line Q-Iearning using connectionist systems. Technical Report 166, Cambridge University Engineering Department, 1994.</p>
<p>[8] G. Tesauro. TD-Gammon, a self-teaching backgammon program, achieves master-level play. Neural Computation, 6:215- 219, 1994.</p>
<p>[9] T. Jaakkola, M. 1. Jordan, and S. P. Singh. On the convergence of stochastic iterative dynamic programming algorithms. Neural Computation, 6:1185- 1201, 1994.</p>
<p>[10] B. T. Polyak and Ya. Z. Tsypkin. Pseudogradient adaptation and training algorithms. Automation and Remote Control, 34(3):377- 397, 1973. 'Translated from A vtomatika i Telemekhanika.</p>
<p>[11] J. G. Kemeny and J. L. SnelL Finite Markov Chains. Van Nostrand- Reinhold, New York, 1960.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
