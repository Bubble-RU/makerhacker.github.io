<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>105 nips-2000-Programmable Reinforcement Learning Agents</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-105" href="../nips2000/nips-2000-Programmable_Reinforcement_Learning_Agents.html">nips2000-105</a> <a title="nips-2000-105-reference" href="#">nips2000-105-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>105 nips-2000-Programmable Reinforcement Learning Agents</h1>
<br/><p>Source: <a title="nips-2000-105-pdf" href="http://papers.nips.cc/paper/1936-programmable-reinforcement-learning-agents.pdf">pdf</a></p><p>Author: David Andre, Stuart J. Russell</p><p>Abstract: We present an expressive agent design language for reinforcement learning that allows the user to constrain the policies considered by the learning process.The language includes standard features such as parameterized subroutines, temporary interrupts, aborts, and memory variables, but also allows for unspecified choices in the agent program. For learning that which isn't specified, we present provably convergent learning algorithms. We demonstrate by example that agent programs written in the language are concise as well as modular. This facilitates state abstraction and the transferability of learned skills.</p><br/>
<h2>reference text</h2><p>[1] D. Andre. Programmable HAMs. www.cs.berkeley.edwdandre/pham.ps. 2000.</p>
<p>[2] S. Benson and N. Nilsson. Reacting, planning and learning in an autonomous agent. In K. Furukawa, D. Michie, and S. Muggleton, editors, Machine Intelligence 14. 1995.</p>
<p>[3] G. Berry and G. Gonthier. The Esterel synchronous programming language: Design, semantics, implementation. Science oj Computer Programming, 19(2):87-152, 1992.</p>
<p>[4] T. G. Dietterich. State abstraction in MAXQ hierarchical RL. In NIPS 12, 2000.</p>
<p>[5] R.I. Firby. Modularity issues in reactive planning. In AlPS 96, pages 78-85. AAAI Press, 1996.</p>
<p>[6] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: A survey. lAIR, 4:237-285, 1996.</p>
<p>[7] N. I. Nilsson. Teleo-reactive programs for agent control. lAIR, 1:139-158, 1994.</p>
<p>[8] R. Parr and S. I. Russell. Reinforcement learning with hierarchies of machines. In NIPS 10, 1998.</p>
<p>[9] R. Parr. Hierarchical Control and Learning jor MDPs. PhD thesis, UC Berkeley, 1998.</p>
<p>[10] L. Peshkin, N. Meuleau, and L. Kaelbling. Learning policies with external memory. In ICML, 1999.</p>
<p>[11] R. Sutton. Temporal abstraction in reinforcement learning. In ICML, 1995.</p>
<p>[12] R. Sutton, D. Precup, and S. Singh. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial Intelligence , 112(1):181- 211 , February 1999.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
