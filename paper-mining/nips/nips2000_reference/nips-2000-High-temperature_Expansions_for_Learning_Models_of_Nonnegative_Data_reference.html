<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-64" href="../nips2000/nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">nips2000-64</a> <a title="nips-2000-64-reference" href="#">nips2000-64-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>64 nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</h1>
<br/><p>Source: <a title="nips-2000-64-pdf" href="http://papers.nips.cc/paper/1929-high-temperature-expansions-for-learning-models-of-nonnegative-data.pdf">pdf</a></p><p>Author: Oliver B. Downs</p><p>Abstract: Recent work has exploited boundedness of data in the unsupervised learning of new types of generative model. For nonnegative data it was recently shown that the maximum-entropy generative model is a Nonnegative Boltzmann Distribution not a Gaussian distribution, when the model is constrained to match the first and second order statistics of the data. Learning for practical sized problems is made difficult by the need to compute expectations under the model distribution. The computational cost of Markov chain Monte Carlo methods and low fidelity of naive mean field techniques has led to increasing interest in advanced mean field theories and variational methods. Here I present a secondorder mean-field approximation for the Nonnegative Boltzmann Machine model, obtained using a</p><br/>
<h2>reference text</h2><p>[1] Downs, DB, MacKay, DJC, & Lee, DD (2000). The Nonnegative Boltzmann Machine. Advances in Neural Information Processing Systems 12, 428-434.</p>
<p>[2] Lee, DD, and Seung, HS (1999) Learning the parts of objects by non-negative matrix factorization. Nature 401,788-791.</p>
<p>[3] Socci, ND, Lee, DD, and Seung, HS (1998). The rectified Gaussian distribution. Advances in Neural Information Processing Systems 10, 350-356.</p>
<p>[4] Georges, A, & Yedidia, JS (1991). How to expand around mean-field theory using hightemperature expansions. Journal of Physics A 24, 2173- 2192.</p>
<p>[5] Neal, RM (1997). Markov chain Monte Carlo methods based on 'slicing' the density function. Technical Report 9722, Dept. of Statistics, University of Toronto.</p>
<p>[6] Kappen, HJ & Rodriguez, FB (1998). Efficient learning in Boltzmann Machines using linear response theory. Neural Computation 10, 1137-1156.</p>
<p>[7] Ben-Yishai, R, Bar-Or, RL, & Sompolinsky, H (1995). Theory of orientation tuning in visual cortex. Proc. Nat. Acad. Sci. USA,92(9):3844-3848.</p>
<p>[8] Yedidia, JS , Freeman, WT, & Weiss, Y (2000). Generalized Belief Propagation. Mitsubishi Electric Research Laboratory Technical Report, TR-2000-26.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
