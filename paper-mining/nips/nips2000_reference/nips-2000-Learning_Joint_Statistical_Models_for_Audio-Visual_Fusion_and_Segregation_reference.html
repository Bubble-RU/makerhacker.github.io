<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>78 nips-2000-Learning Joint Statistical Models for Audio-Visual Fusion and Segregation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-78" href="../nips2000/nips-2000-Learning_Joint_Statistical_Models_for_Audio-Visual_Fusion_and_Segregation.html">nips2000-78</a> <a title="nips-2000-78-reference" href="#">nips2000-78-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>78 nips-2000-Learning Joint Statistical Models for Audio-Visual Fusion and Segregation</h1>
<br/><p>Source: <a title="nips-2000-78-pdf" href="http://papers.nips.cc/paper/1898-learning-joint-statistical-models-for-audio-visual-fusion-and-segregation.pdf">pdf</a></p><p>Author: John W. Fisher III, Trevor Darrell, William T. Freeman, Paul A. Viola</p><p>Abstract: People can understand complex auditory and visual information, often using one to disambiguate the other. Automated analysis, even at a lowlevel, faces severe challenges, including the lack of accurate statistical models for the signals, and their high-dimensionality and varied sampling rates. Previous approaches [6] assumed simple parametric models for the joint distribution which, while tractable, cannot capture the complex signal relationships. We learn the joint distribution of the visual and auditory signals using a non-parametric approach. First, we project the data into a maximally informative, low-dimensional subspace, suitable for density estimation. We then model the complicated stochastic relationships between the signals using a nonparametric density estimator. These learned densities allow processing across signal modalities. We demonstrate, on synthetic and real signals, localization in video of the face that is speaking in audio, and, conversely, audio enhancement of a particular speaker selected from the video.</p><br/>
<h2>reference text</h2><p>[1] P. Anandan. A computational framework and an algorithm for the measurement of visual motion. Tnt. J. compo Vision, 2:283-310, 1989.</p>
<p>[2] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons, Inc., New York, 1991.</p>
<p>[3] J. Fisher and J. Principe. Unsupervised learning for nonlinear synthetic discriminant functions. In D. Casasent and T. Chao, editors, Proc. SPIE, Optical Pattern Recognition VII, volume 2752, pages 2-13, 1996.</p>
<p>[4] J. W. Fisher III, A. T. Thier, and P. A. Viola. Learning informative statistics: A nonparametric approach. In S. A. Solla, T. KLeen, and K-R. Mller, editors, Proceedings of 1999 Conference on Advances in Neural Information Processing Systems 12, 1999.</p>
<p>[5] J. W. Fisher III and J. C. Principe. A methodology for information theoretic feature extraction. In A. Stuberud, editor, Proceedings of the IEEE International Joint Conference on Neural Networks, 1998.</p>
<p>[6] J. Hershey andJ. Movellan. Using audio-visual synchrony to locate sounds. In T. K L. S. A. Solla and K-R. Mller, editors, Proceedings of 1999 Conference on Advances in Neural Information Processing Systems 12, 1999.</p>
<p>[7] M. Slaney and M. Covell. Facesync: A linear operator for measuring synchronization of video facial images and audio tracks. In This volume, 2001.</p>
<p>[8] P. Viola, N. Schraudolph, and T. Sejnowski. Empirical entropy manipulation for real-world problems. In Proceedings of 1996 Conference on Advances in Neural Information Processing Systems 8, pages 851- 7,1996.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
