<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="../home/nips2000_home.html">nips2000</a> <a title="nips-2000-73" href="../nips2000/nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">nips2000-73</a> <a title="nips-2000-73-reference" href="#">nips2000-73-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>73 nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</h1>
<br/><p>Source: <a title="nips-2000-73-pdf" href="http://papers.nips.cc/paper/1849-kernel-based-reinforcement-learning-in-average-cost-problems-an-application-to-optimal-portfolio-choice.pdf">pdf</a></p><p>Author: Dirk Ormoneit, Peter W. Glynn</p><p>Abstract: Many approaches to reinforcement learning combine neural networks or other parametric function approximators with a form of temporal-difference learning to estimate the value function of a Markov Decision Process. A significant disadvantage of those procedures is that the resulting learning algorithms are frequently unstable. In this work, we present a new, kernel-based approach to reinforcement learning which overcomes this difficulty and provably converges to a unique solution. By contrast to existing algorithms, our method can also be shown to be consistent in the sense that its costs converge to the optimal costs asymptotically. Our focus is on learning in an average-cost framework and on a practical application to the optimal portfolio choice problem. 1</p><br/>
<h2>reference text</h2><p>[Ber95)  D. P. Bertsekas. Dynamic Programming and Optimal Control, volume 1 and 2. Athena Scientific, 1995.  [BM95)  J. A. Boyan and A. W. Moore. Generalization in reinforcement learning: Safely approximating the value function. In NIPS 7,1995.  [Gor99)  G. Gordon. Approximate Solutions to Markov Decision Processes. PhD thesis, Computer Science Department, Carnegie Mellon University, 1999.  [OGOO)  D. Ormoneit and P. Glynn. Kernel-based reinforcement learning in averagecost problems. Working paper, Stanford University. In preparation.  [O SOO)  D. Ormoneit and S. Sen. Kernel-based reinforcement learning. Machine Learning, 2001. To appear.  [Rus97)  J. Rust. Using randomization to break the curse of dimensionality. Economet</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
