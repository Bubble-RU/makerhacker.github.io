<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>99 nips-2012-Dip-means: an incremental clustering method for estimating the number of clusters</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-99" href="../nips2012/nips-2012-Dip-means%3A_an_incremental_clustering_method_for_estimating_the_number_of_clusters.html">nips2012-99</a> <a title="nips-2012-99-reference" href="#">nips2012-99-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>99 nips-2012-Dip-means: an incremental clustering method for estimating the number of clusters</h1>
<br/><p>Source: <a title="nips-2012-99-pdf" href="http://papers.nips.cc/paper/4795-dip-means-an-incremental-clustering-method-for-estimating-the-number-of-clusters.pdf">pdf</a></p><p>Author: Argyris Kalogeratos, Aristidis Likas</p><p>Abstract: Learning the number of clusters is a key problem in data clustering. We present dip-means, a novel robust incremental method to learn the number of data clusters that can be used as a wrapper around any iterative clustering algorithm of k-means family. In contrast to many popular methods which make assumptions about the underlying cluster distributions, dip-means only assumes a fundamental cluster property: each cluster to admit a unimodal distribution. The proposed algorithm considers each cluster member as an individual ‘viewer’ and applies a univariate statistic hypothesis test for unimodality (dip-test) on the distribution of distances between the viewer and the cluster members. Important advantages are: i) the unimodality test is applied on univariate distance vectors, ii) it can be directly applied with kernel-based methods, since only the pairwise distances are involved in the computations. Experimental results on artiﬁcial and real datasets indicate the eﬀectiveness of our method and its superiority over analogous approaches.</p><br/>
<h2>reference text</h2><p>[1] D. Pelleg and Andrew Moore. X-means: extending k-means with eﬃcient estimation of the number of clusters. International Conference on Machine Learning (ICML), pp. 727-734, 2000.</p>
<p>[2] R.E. Kass and L. Wasserman. A reference Bayesian test for nested hypotheses and its relationship to the Schwarz criterion. Journal of the American Statistical Association, 90(431), pp. 928-934, 1995.</p>
<p>[3] X. Hu and L. Xu. A comparative study of several cluster number selection criteria. In J. Liu et al.(eds.) Intelligent Data Engineering and Automated Learning, pp. 195–202, Springer, 2003.</p>
<p>[4] G. Hamerly and C. Elkan. Learning the k in k-means. Advances in Neural Information Processing Systems (NIPS), pp. 281-288, 2003.</p>
<p>[5] Y. Feng and G. Hamerly. PG-means: learning the number of clusters in data. Advances in Neural Information Processing Systems (NIPS), pp. 393–400, 2006.</p>
<p>[6] K. Kurihara and M. Welling. Bayesian k-means as a maximization-expectation algorithm. Neural Computation, 21(4), pp. 1145–1172, 2009.</p>
<p>[7] R. Tibshirani, G. Walther and T. Hastie. Estimating the number of clusters in a dataset via the Gap statistic. Journal of the Royal Statistical Society B, 63, pp. 411-423, 2001.</p>
<p>[8] L. Zelnik-Manor and P. Perona. Self-tuning spectral clustering. Advances in Neural Information Processing Systems (NIPS), pp. 1601–1608, 2004.</p>
<p>[9] T. Shi, M. Belkin and B. Yu. Data Spectroscopy: eigenspaces of convolution operators and clustering. The Annals of Statistics, 37(6B), pp. 3960–3984, 2009.</p>
<p>[10] E. Levine and E. Domany. Resampling method for unsupervised estimation of cluster validity. Neural Computation, 13(11), pp. 2573-2593, 2001.</p>
<p>[11] Robert Tibshirani and G. Walther. Cluster validation by prediction strength. Computational & Graphical Statistics, 14(3), pp. 511-528, 2005.</p>
<p>[12] T. Lange, V. Roth, Mikio L. Braun, and J.M. Buhmann. Stability-based validation of clustering solutions. Neural Computation, 16(6), pp. 1299-1323, 2004.</p>
<p>[13] I.S. Dhillon, Y. Guan and B. Kulis. Kernel k-means: spectral clustering and normalized cuts. International Conference on Knowledge Discovery and Data Mining (SIGKDD), pp. 551–556, 2004.</p>
<p>[14] B.W. Silverman. Using Kernel density estimates to investigate multimodality. Journal of Royal Statistic Society B, 43(1), pp. 97-99, 1981.</p>
<p>[15] J.A. Hartigan and P. M. Hartigan. The dip test of unimodality. The Annals of Statistics, 13(1), pp. 70-84, 1985.</p>
<p>[16] D.L. Boley. Principal direction divisive partitioning. Data Mining and Knowledge Discovery, 2(4), pp. 344, 1998.</p>
<p>[17] M. Meila. Comparing clusterings – an information based distance. Multivariate Analysis, 98(5), pp. 873-895, 2007.</p>
<p>[18] L. Hubert and P. Arabie. Comparing partitions. Journal of Classiﬁcation, 2(1), pp. 193-218, 1985.</p>
<p>[19] J.J. Verbeek, N. Vlassis, and B. Kro”se. Eﬃcient Greedy Learning of Gaussian Mixture Models. Neural Computation, 15(2), pp. 469-485, 2003.</p>
<p>[20] A. Asuncion and D. Newman. UCI Machine Learning Repository. University of California at Irvine, Irvine, CA, 2007. Available online: http://www.ics.uci.edu/ mlearn/MLRepository.html</p>
<p>[21] S.A. Nene, S.K. Nayar and H. Murase. Columbia Object Image Library (COIL-100). Technical Report CUCS-006-96, February 1996.</p>
<p>[22] D. Lowe. Distinctive image features from scale-invariant keypoints. Journal of Computer Vision, 60, pp. 91-110, 2004.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
