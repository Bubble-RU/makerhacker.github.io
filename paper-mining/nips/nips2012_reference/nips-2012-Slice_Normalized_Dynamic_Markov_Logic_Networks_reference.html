<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>314 nips-2012-Slice Normalized Dynamic Markov Logic Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-314" href="../nips2012/nips-2012-Slice_Normalized_Dynamic_Markov_Logic_Networks.html">nips2012-314</a> <a title="nips-2012-314-reference" href="#">nips2012-314-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>314 nips-2012-Slice Normalized Dynamic Markov Logic Networks</h1>
<br/><p>Source: <a title="nips-2012-314-pdf" href="http://papers.nips.cc/paper/4554-slice-normalized-dynamic-markov-logic-networks.pdf">pdf</a></p><p>Author: Tivadar Papai, Henry Kautz, Daniel Stefankovic</p><p>Abstract: Markov logic is a widely used tool in statistical relational learning, which uses a weighted ﬁrst-order logic knowledge base to specify a Markov random ﬁeld (MRF) or a conditional random ﬁeld (CRF). In many applications, a Markov logic network (MLN) is trained in one domain, but used in a different one. This paper focuses on dynamic Markov logic networks, where the size of the discretized time-domain typically varies between training and testing. It has been previously pointed out that the marginal probabilities of truth assignments to ground atoms can change if one extends or reduces the domains of predicates in an MLN. We show that in addition to this problem, the standard way of unrolling a Markov logic theory into a MRF may result in time-inhomogeneity of the underlying Markov chain. Furthermore, even if these representational problems are not signiﬁcant for a given domain, we show that the more practical problem of generating samples in a sequential conditional random ﬁeld for the next slice relying on the samples from the previous slice has high computational cost in the general case, due to the need to estimate a normalization factor for each sample. We propose a new discriminative model, slice normalized dynamic Markov logic networks (SN-DMLN), that suffers from none of these issues. It supports efﬁcient online inference, and can directly model inﬂuences between variables within a time slice that do not have a causal direction, in contrast with fully directed models (e.g., DBNs). Experimental results show an improvement in accuracy over previous approaches to online inference in dynamic Markov logic networks. 1</p><br/>
<h2>reference text</h2><p>[1] Pedro Domingos and Daniel Lowd. Markov Logic: An Interface Layer for Artiﬁcial Intelligence. Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning. Morgan & Claypool Publishers, 2009.</p>
<p>[2] Thomas Geier and Susanne Biundo. Approximate online inference for dynamic markov logic networks. In Tools with Artiﬁcial Intelligence (ICTAI), 2011 23rd IEEE International Conference on, pages 764–768, 2011.</p>
<p>[3] Shalini Ghosh, Natarajan Shankar, and Sam Owre. Machine reading using markov logic networks for collective probabilistic inference. In In Proceedings of ECML-CoLISD., 2011.</p>
<p>[4] Vibhav Gogate and Rina Dechter. Samplesearch: Importance sampling in presence of determinism. Artif. Intell., 175(2):694–729, 2011.</p>
<p>[5] Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 1990.</p>
<p>[6] Dominik Jain, Andreas Barthels, and Michael Beetz. Adaptive Markov Logic Networks: Learning Statistical Relational Models with Dynamic Parameters. In 19th European Conference on Artiﬁcial Intelligence (ECAI), pages 937–942, 2010. 8</p>
<p>[7] K. Kersting, B. Ahmadi, and S. Natarajan. Counting belief propagation. In J. Bilmes A. Ng, editor, Proceedings of the 25th Conference on Uncertainty in Artiﬁcial Intelligence (UAI–09), Montreal, Canada, June 18–21 2009.</p>
<p>[8] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT Press, 2009.</p>
<p>[9] John Lafferty. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. pages 282–289. Morgan Kaufmann, 2001.</p>
<p>[10] Steffen Lauritzen and Thomas S. Richardson. Chain graph models and their causal interpretations. B, 64:321–361, 2001.</p>
<p>[11] B. Limketkai, D. Fox, and Lin Liao. CRF-Filters: Discriminative Particle Filters for Sequential State Estimation. In Robotics and Automation, 2007 IEEE International Conference on, pages 3142–3147, 2007.</p>
<p>[12] Andrew McCallum, Dayne Freitag, and Fernando C. N. Pereira. Maximum entropy markov models for information extraction and segmentation. In Proceedings of the Seventeenth International Conference on Machine Learning, ICML ’00, pages 591–598, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc.</p>
<p>[13] Kevin Patrick Murphy. Dynamic bayesian networks: representation, inference and learning. PhD thesis, 2002. AAI3082340.</p>
<p>[14] Aniruddh Nath and Pedro Domingos. Efﬁcient belief propagation for utility maximization and repeated inference, 2010.</p>
<p>[15] Hoifung Poon and Pedro Domingos. Sound and efﬁcient inference with probabilistic and deterministic dependencies. In Proceedings of the 21st national conference on Artiﬁcial intelligence - Volume 1, AAAI’06, pages 458–463. AAAI Press, 2006.</p>
<p>[16] G. Potamianos and J. Goutsias. Stochastic approximation algorithms for partition function estimation of Gibbs random ﬁelds. IEEE Transactions on Information Theory, 43(6):1948– 1965, 1997.</p>
<p>[17] Adam Sadilek and Henry Kautz. Recognizing multi-agent activities from GPS data. In TwentyFourth AAAI Conference on Artiﬁcial Intelligence, 2010.</p>
<p>[18] R. Salakhutdinov. Learning and evaluating Boltzmann machines. Technical Report UTML TR 2008-002, Department of Computer Science, University of Toronto, June 2008.</p>
<p>[19] Charles Sutton, Andrew McCallum, and Khashayar Rohanimanesh. Dynamic conditional random ﬁelds: Factorized probabilistic models for labeling and segmenting sequence data. J. Mach. Learn. Res., 8:693–723, May 2007.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
