<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>142 nips-2012-Generalization Bounds for Domain Adaptation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-142" href="../nips2012/nips-2012-Generalization_Bounds_for_Domain_Adaptation.html">nips2012-142</a> <a title="nips-2012-142-reference" href="#">nips2012-142-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>142 nips-2012-Generalization Bounds for Domain Adaptation</h1>
<br/><p>Source: <a title="nips-2012-142-pdf" href="http://papers.nips.cc/paper/4684-generalization-bounds-for-domain-adaptation.pdf">pdf</a></p><p>Author: Chao Zhang, Lei Zhang, Jieping Ye</p><p>Abstract: In this paper, we provide a new framework to study the generalization bound of the learning process for domain adaptation. We consider two kinds of representative domain adaptation settings: one is domain adaptation with multiple sources and the other is domain adaptation combining source and target data. In particular, we use the integral probability metric to measure the difference between two domains. Then, we develop the speciﬁc Hoeffding-type deviation inequality and symmetrization inequality for either kind of domain adaptation to achieve the corresponding generalization bound based on the uniform entropy number. By using the resultant generalization bound, we analyze the asymptotic convergence and the rate of convergence of the learning process for domain adaptation. Meanwhile, we discuss the factors that affect the asymptotic behavior of the learning process. The numerical experiments support our results. 1</p><br/>
<h2>reference text</h2><p>[1] V.N. Vapnik (1999). An overview of statistical learning theory. IEEE Transactions on Neural Networks 10(5):988-999.</p>
<p>[2] O. Bousquet, S. Boucheron, and G. Lugosi (2004). Introduction to Statistical Learning Theory. In O. Bousquet et al. (ed.), Advanced Lectures on Machine Learning, 169-207.</p>
<p>[3] V.N. Vapnik (1998). Statistical Learning Theory. New York: John Wiley and Sons.</p>
<p>[4] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth (1989). Learnability and the VapnikChervonenkis dimension. Journal of the ACM 36(4):929-965.</p>
<p>[5] A. van der Vaart, and J. Wellner (2000). Weak Convergence and Empirical Processes With Applications to Statistics (Hardcover). Springer.</p>
<p>[6] P.L. Bartlett, O. Bousquet, and S. Mendelson (2005). Local Rademacher Complexities. Annals of Statistics 33:1497-1537.</p>
<p>[7] Z. Hussain, and J. Shawe-Taylor (2011). Improved Loss Bounds for Multiple Kernel Learning. Journal of Machine Learning Research - Proceedings Track 15:370-377.  8</p>
<p>[8] J. Jiang, and C. Zhai (2007). Instance Weighting for Domain Adaptation in NLP. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL), 264-271.</p>
<p>[9] J. Blitzer, M. Dredze, and F. Pereira (2007). Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classiﬁcation. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL), 440-447.</p>
<p>[10] S. Bickel, M. Br¨ ckner, and T. Scheffer (2007). Discriminative learning for differing training and test u distributions. Proceedings of the 24th international conference on Machine learning (ICML), 81-88.</p>
<p>[11] P. Wu, and T.G. Dietterich (2004). Improving SVM accuracy by training on auxiliary data sources. Proceedings of the twenty-ﬁrst international conference on Machine learning (ICML), 871-878.</p>
<p>[12] J. Blitzer, R. McDonald, and F. Pereira (2006). Domain adaptation with structural correspondence learning. Conference on Empirical Methods in Natural Language Processing (EMNLP), 120-128.</p>
<p>[13] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. Wortman (2010). A Theory of Learning from Different Domains. Machine Learning 79:151-175.</p>
<p>[14] K. Crammer, M. Kearns, and J. Wortman (2006). Learning from Multiple Sources. Advances in Neural Information Processing Systems (NIPS).</p>
<p>[15] K. Crammer, M. Kearns, and J. Wortman (2008). Learning from Multiple Sources. Journal of Machine Learning Research 9:1757-1774.</p>
<p>[16] Y. Mansour, M. Mohri, and A. Rostamizadeh (2008). Domain adaptation with multiple sources. Advances in Neural Information Processing Systems (NIPS), 1041-1048.</p>
<p>[17] Y. Mansour, M. Mohri, and A. Rostamizadeh (2009). Multiple Source Adaptation and The R´ nyi Divere gence. Proceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence (UAI).</p>
<p>[18] J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. Wortman (2007). Learning Bounds for Domain Adaptation. Advances in Neural Information Processing Systems (NIPS).</p>
<p>[19] S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira, F (2006). Analysis of Representations for Domain Adaptation. Advances in Neural Information Processing Systems (NIPS), 137-144.</p>
<p>[20] Y. Mansour, M. Mohri, and A. Rostamizadeh (2009). Domain Adaptation: Learning Bounds and Algorithms. Conference on Learning Theory (COLT).</p>
<p>[21] W. Hoeffding (1963). Probability Inequalities for Sums of Bounded Random Variables. Journal of the American Statistical Association 58(301):13-30.</p>
<p>[22] S. Mendelson (2003). A Few Notes on Statistical Learning Theory. Lecture Notes in Computer Science 2600:1-40.</p>
<p>[23] V.M. Zolotarev (1984). Probability Metrics. Theory of Probability and its Application 28(1):278-302.</p>
<p>[24] S.T. Rachev (1991). Probability Metrics and the Stability of Stochastic Models. John Wiley and Sons.</p>
<p>[25] A. M¨ ller (1997). Integral Probability Metrics and Their Generating Classes of Functions. Advances in u Applied Probability 29(2):429-443.</p>
<p>[26] M.D. Reid and R.C. Williamson (2011). Information, Divergence and Risk for Binary Experiments. Journal of Machine Learning Research 12:731-817.</p>
<p>[27] B.K. Sriperumbudur, A. Gretton, K. Fukumizu, G.R.G. Lanckriet and B. Sch¨ lkopf (2009). A Note on o Integral Probability Metrics and φ-Divergences. CoRR abs/0901.2698.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
