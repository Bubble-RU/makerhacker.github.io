<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>282 nips-2012-Proximal Newton-type methods for convex optimization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-282" href="../nips2012/nips-2012-Proximal_Newton-type_methods_for_convex_optimization.html">nips2012-282</a> <a title="nips-2012-282-reference" href="#">nips2012-282-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>282 nips-2012-Proximal Newton-type methods for convex optimization</h1>
<br/><p>Source: <a title="nips-2012-282-pdf" href="http://papers.nips.cc/paper/4740-proximal-newton-type-methods-for-convex-optimization.pdf">pdf</a></p><p>Author: Jason Lee, Yuekai Sun, Michael Saunders</p><p>Abstract: We seek to solve convex optimization problems in composite form: minimize f (x) := g(x) + h(x), n x∈R where g is convex and continuously differentiable and h : Rn → R is a convex but not necessarily differentiable function whose proximal mapping can be evaluated efﬁciently. We derive a generalization of Newton-type methods to handle such convex but nonsmooth objective functions. We prove such methods are globally convergent and achieve superlinear rates of convergence in the vicinity of an optimal solution. We also demonstrate the performance of these methods using problems of relevance in machine learning and statistics. 1</p><br/>
<h2>reference text</h2><p>[1] A. Auslender and M. Teboulle, Interior gradient and proximal methods for convex and conic optimization, SIAM J. Optim., 16 (2006), pp. 697–725.</p>
<p>[2] A. Beck and M. Teboulle , A fast iterative shrinkage-thresholding algorithm for linear inverse problems, SIAM J. Imaging Sci., 2 (2009), pp. 183–202.</p>
<p>[3] S. R. Becker, M. J. Cand` s, and M. C. Grant, Templates for convex cone problems with applications to e sparse signal recovery, Math. Program. Comput., 3 (2011), pp. 1–54.</p>
<p>[4] S. Becker and J. Fadili, A quasi-Newton proximal splitting method, NIPS, Lake Tahoe, California, 2012.</p>
<p>[5] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge University Press, Cambridge, 2004.</p>
<p>[6] E. J. Cand` s and B. Recht, Exact matrix completion via convex optimization, Found. Comput. Math, 9 e (2009), pp. 717–772.</p>
<p>[7] J. E. Dennis, Jr. and J. J. Mor´ , A characterization of superlinear convergence and its application to e quasi-Newton methods, Math. Comp., 28, (1974), pp. 549–560.</p>
<p>[8] S. C. Eisenstat and H. F. Walker, Choosing the forcing terms in an inexact Newton method, SIAM J. Sci. Comput., 17 (1996), pp. 16–32.</p>
<p>[9] J. Friedman, T. Hastie, H. Holﬁng, and R. Tibshirani, Pathwise coordinate optimization, Ann. Appl. Stat. (2007), pp. 302–332</p>
<p>[10] M. Fukushima and H. Mine, A generalized proximal point algorithm for certain non-convex minimization problems, Internat. J. Systems Sci., 12 (1981), pp. 989–1000.</p>
<p>[11] C. J. Hsieh, M. A. Sustik, P. Ravikumar, and I. S. Dhillon, Sparse inverse covariance matrix estimation using quadratic approximation, NIPS, Granada, Spain, 2011.</p>
<p>[12] D. Kim, S. Sra, and I. S. Dhillon, A scalable trust-region algorithm with applications to mixed-norm regression, ICML, Haifa, Israel, 2010.</p>
<p>[13] Y. Nesterov, Gradient methods for minimizing composite objective function, CORE discussion paper, 2007.</p>
<p>[14] G. Obozinski, B. Taskar, and M. I. Jordan, Joint covariate selection and joint subspace selection for multiple classiﬁcation problems, Stat. Comput. (2010), pp. 231–252</p>
<p>[15] P. Olsen, F. Oztoprak, J. Nocedal, S. Rennie, Newton-like methods for sparse inverse covariance estimation, NIPS, Lake Tahoe, California, 2012.</p>
<p>[16] M. Patriksson, Nonlinear Programming and Variational Inequality Problems, Kluwer Academic Publishers, The Netherlands, 1999.</p>
<p>[17] P. Ravikumar, M. J. Wainwright and J. D. Lafferty, High-dimensional Ising model selection using 1regularized logistic regression, Ann. Statist. (2010), pp. 1287-1319.</p>
<p>[18] M. Schmidt, Graphical Model Structure Learning with l1-Regularization, Ph.D. Thesis (2010), University of British Columbia</p>
<p>[19] M. Schmidt, E. van den Berg, M. P. Friedlander, and K. Murphy, Optimizing costly functions with simple constraints: a limited-memory projected quasi-Newton algorithm, AISTATS, Clearwater Beach, Florida, 2009.</p>
<p>[20] M. Schmidt, D. Kim, and S. Sra, Projected Newton-type methods in machine learning, in S. Sra, S. Nowozin, and S. Wright, editors, Optimization for Machine Learning, MIT Press (2011).</p>
<p>[21] P. Tseng and S. Yun, A coordinate gradient descent method for nonsmooth separable minimization, Math. Prog. Ser. B, 117 (2009), pp. 387–423.</p>
<p>[22] P. Tseng, On accelerated proximal gradient methods for convex-concave optimization, submitted to SIAM J. Optim. (2008).</p>
<p>[23] R. Tibshirani, Regression shrinkage and selection via the lasso, J. R. Stat. Soc. Ser. B Stat. Methodol., 58 (1996), pp. 267–288.</p>
<p>[24] S. J. Wright, R. D. Nowak, and M. A. T. Figueiredo, Sparse reconstruction by separable approximation, IEEE Trans. Signal Process., 57 (2009), pp. 2479–2493.</p>
<p>[25] J. Yu, S. V. N. Vishwanathan, S. G¨ nter, and N. N. Schraudolph, A Quasi-Newton Approach to Nonsmooth u Convex Optimization, ICML, Helsinki, Finland, 2008.</p>
<p>[26] G. X. Yuan, C. H. Ho and C. J. Lin, An improved GLMNET for 1-regularized logistic regression and support vector machines, National Taiwan University, Tech. Report 2011.</p>
<p>[27] R. H. Zou and T. Hastie, Regularization and variable selection via the elastic net, J. R. Stat. Soc. Ser. B Stat. Methodol., 67 (2005), pp. 301–320.  9</p>
<br/>
<br/><br/><br/></body>
</html>
