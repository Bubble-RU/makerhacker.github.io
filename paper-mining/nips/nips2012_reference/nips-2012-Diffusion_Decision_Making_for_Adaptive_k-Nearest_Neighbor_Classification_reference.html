<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>97 nips-2012-Diffusion Decision Making for Adaptive k-Nearest Neighbor Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-97" href="../nips2012/nips-2012-Diffusion_Decision_Making_for_Adaptive_k-Nearest_Neighbor_Classification.html">nips2012-97</a> <a title="nips-2012-97-reference" href="#">nips2012-97-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>97 nips-2012-Diffusion Decision Making for Adaptive k-Nearest Neighbor Classification</h1>
<br/><p>Source: <a title="nips-2012-97-pdf" href="http://papers.nips.cc/paper/4734-diffusion-decision-making-for-adaptive-k-nearest-neighbor-classification.pdf">pdf</a></p><p>Author: Yung-kyun Noh, Frank Park, Daniel D. Lee</p><p>Abstract: This paper sheds light on some fundamental connections of the diffusion decision making model of neuroscience and cognitive psychology with k-nearest neighbor classiﬁcation. We show that conventional k-nearest neighbor classiﬁcation can be viewed as a special problem of the diffusion decision model in the asymptotic situation. By applying the optimal strategy associated with the diffusion decision model, an adaptive rule is developed for determining appropriate values of k in knearest neighbor classiﬁcation. Making use of the sequential probability ratio test (SPRT) and Bayesian analysis, we propose ﬁve different criteria for adaptively acquiring nearest neighbors. Experiments with both synthetic and real datasets demonstrate the effectiveness of our classiﬁcation criteria. 1</p><br/>
<h2>reference text</h2><p>[1] A. F. Atiya. Estimating the posterior probabilities using the k-nearest neighbor rule. Neural Computation, 17(3):731–740, 2005.</p>
<p>[2] J. M. Beck, W. J. Ma, R. Kiani, T. Hanks, A. K. Churchland, J. Roitman, M. N. Shadlen, P. E. Latham, and A. Pouget. Probabilistic population codes for Bayesian decision making. Neuron, 60(6):1142–1152, 2008.</p>
<p>[3] R. Bogacz, E. Brown, J. Moehlis, P. Holmes, and J. D. Cohen. The physics of optimal decision making: A formal analysis of models of performance in two-alternative forced-choice tasks. Psychological Review, 113(4):700–765, 2006.</p>
<p>[4] T. Cover and P. Hart. Nearest neighbor pattern classiﬁcation. IEEE Transactions on Information Theory, 13(1):21–27, 1967.</p>
<p>[5] L. Devroye, L. Gy¨ rﬁ, and G. Lugosi. A probabilistic theory of pattern recognition. Applicao tions of mathematics. Springer, 1996.</p>
<p>[6] M. A. Girshick. Contributions to the theory of sequential analysis I. The Annuals of Mathematical Statistics, 17:123–143, 1946.</p>
<p>[7] M. Goldstein. kn -Nearest Neighbor Classiﬁcation. IEEE Transactions on Information Theory, IT-18(5):627–630, 1972.</p>
<p>[8] C. C. Holmes and N. M. Adams. A probabilistic nearest neighbour method for statistical pattern recognition. Journal of the Royal Statistical Society Series B, 64(2):295–306, 2002.</p>
<p>[9] M. D. Lee, I. G. Fuss, and D. J. Navarro. A Bayesian approach to diffusion models of decisionmaking and response time. In Advances in Neural Information Processing Systems 19, pages 809–816. 2007.</p>
<p>[10] N. Leonenko, L. Pronzato, and V. Savani. A class of R´ nyi information estimators for multidie mensional densities. Annals of Statistics, 36:2153–2182, 2008.</p>
<p>[11] W. J. Ma, J. M. Beck, P. E. Latham, and A. Pouget. Bayesian inference with probabilistic population codes. Nature Neuroscience, 9(11):1432–1438, 2006.</p>
<p>[12] S. Ougiaroglou, A. Nanopoulos, A. N. Papadopoulos, Y. Manolopoulos, and T. WelzerDruzovec. Adaptive k-nearest-neighbor classiﬁcation using a dynamic number of nearest neighbors. In Proceedings of the 11th East European conference on Advances in databases and information systems, pages 66–82, 2007.</p>
<p>[13] R. Ratcliff and G. Mckoon. The diffusion decision model: theory and data for two-choice decision tasks. Neural Computation, 20(4):873–922, 2008.</p>
<p>[14] R. Ratcliff and J. N. Rouder. A diffusion model account of masking in two-choice letter identiﬁcation. Journal of Experimental Psychology Human Perception and Performance, 26(1):127– 140, 2000.</p>
<p>[15] M. N. Shadlen, A. K. Hanks, A. K. Churchland, R. Kiani, and T. Yang. The speed and accuracy of a simple perceptual decision: a mathematical primer. Bayesian brain: Probabilistic approaches to neural coding, 2006.</p>
<p>[16] M. N. Shadlen and W. T. Newsome. The variable discharge of cortical neurons: Implications for connectivity, computation, and information coding. Journal of Neuroscience, 18:3870– 3896, 1998.</p>
<p>[17] A. Torralba, R. Fergus, and W. T. Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(11):1958–1970, 2008.</p>
<p>[18] A. Wald and J. Wolfowitz. Optimum character of the sequential probability ratio test. Annals of Mathematical Statistics, 19:326–339, 1948.</p>
<p>[19] J. Wang, P. Neskovic, and L. N. Cooper. Neighborhood size selection in the k-nearest-neighbor rule using statistical conﬁdence. Pattern Recognition, 39(3):417–423, 2006.</p>
<p>[20] L. Wasserman. All of Statistics: A Concise Course in Statistical Inference (Springer Texts in Statistics). Springer, December 2003.</p>
<p>[21] J. Zhang and R. Bogacz. Optimal decision making on the basis of evidence represented in spike trains. Neural Computation, 22(5):1113–1148, 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
