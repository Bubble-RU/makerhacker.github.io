<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>217 nips-2012-Mixability in Statistical Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-217" href="../nips2012/nips-2012-Mixability_in_Statistical_Learning.html">nips2012-217</a> <a title="nips-2012-217-reference" href="#">nips2012-217-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>217 nips-2012-Mixability in Statistical Learning</h1>
<br/><p>Source: <a title="nips-2012-217-pdf" href="http://papers.nips.cc/paper/4835-mixability-in-statistical-learning.pdf">pdf</a></p><p>Author: Tim V. Erven, Peter Grünwald, Mark D. Reid, Robert C. Williamson</p><p>Abstract: Statistical learning and sequential prediction are two different but related formalisms to study the quality of predictions. Mapping out their relations and transferring ideas is an active area of investigation. We provide another piece of the puzzle by showing that an important concept in sequential prediction, the mixability of a loss, has a natural counterpart in the statistical setting, which we call stochastic mixability. Just as ordinary mixability characterizes fast rates for the worst-case regret in sequential prediction, stochastic mixability characterizes fast rates in statistical learning. We show that, in the special case of log-loss, stochastic mixability reduces to a well-known (but usually unnamed) martingale condition, which is used in existing convergence theorems for minimum description length and Bayesian inference. In the case of 0/1-loss, it reduces to the margin condition of Mammen and Tsybakov, and in the case that the model under consideration contains all possible predictors, it is equivalent to ordinary mixability. 1</p><br/>
<h2>reference text</h2><p>[1] O. Bousquet, S. Boucheron, and G. Lugosi. Introduction to statistical learning theory. In O. Bousquet, U. von Luxburg, and G. R¨ tsch, editors, Advanced Lectures on Machine Learna ing, volume 3176 of Lecture Notes in Computer Science, pages 169–207. Springer Berlin / Heidelberg, 2004.</p>
<p>[2] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press, 2006.</p>
<p>[3] O. Dekel and Y. Singer. Data-driven online to batch conversions. In Y. Weiss, B. Sch¨ lkopf, o and J. Platt, editors, Advances in Neural Information Processing Systems 18 (NIPS), pages 267–274, Cambridge, MA, 2006. MIT Press.</p>
<p>[4] J. Abernethy, A. Agarwal, P. L. Bartlett, and A. Rakhlin. A stochastic view of optimal regret through minimax duality. In Proceedings of the 22nd Conference on Learning Theory (COLT), 2009.</p>
<p>[5] Y. Kalnishkan and M. V. Vyugin. The weak aggregating algorithm and weak mixability. Journal of Computer and System Sciences, 74:1228–1244, 2008.</p>
<p>[6] V. Vovk. A game of prediction with expert advice. In Proceedings of the 8th Conference on Learning Theory (COLT), pages 51–60. ACM, 1995.</p>
<p>[7] E. Mammen and A. B. Tsybakov. Smooth discrimination analysis. The Annals of Statistics, 27(6):1808–1829, 1999.</p>
<p>[8] A. B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. The Annals of Statistics, 32(1):135–166, 2004.</p>
<p>[9] J. L. Doob. Application of the theory of martingales. In Le Calcul de Probabilit´ s et ses e Applications. Colloques Internationaux du Centre National de la Recherche Scientiﬁque, pages 23–27, Paris, 1949.</p>
<p>[10] A. Barron and T. Cover. Minimum complexity density estimation. IEEE Transactions on Information Theory, 37(4):1034–1054, 1991.</p>
<p>[11] T. Zhang. From ✏-entropy to KL entropy: analysis of minimum information complexity density estimation. Annals of Statistics, 34(5):2180–2210, 2006.</p>
<p>[12] J. Li. Estimation of Mixture Models. PhD thesis, Yale University, 1999.</p>
<p>[13] B. Kleijn and A. van der Vaart. Misspeciﬁcation in inﬁnite-dimensional Bayesian statistics. Annals of Statistics, 34(2), 2006.</p>
<p>[14] P. Gr¨ nwald. Safe learning: bridging the gap between Bayes, MDL and statistical learning u theory via empirical convexity. In Proceedings of the 24th Conference on Learning Theory (COLT), 2011.</p>
<p>[15] A. Chernov, Y. Kalnishkan, F. Zhdanov, and V. Vovk. Supermartingales in prediction with expert advice. Theoretical Computer Science, 411:2647–2669, 2010.</p>
<p>[16] J.-Y. Audibert. Fast learning rates in statistical inference through aggregation. Annals of Statistics, 37(4):1591–1646, 2009.</p>
<p>[17] E. Vernet, R. C. Williamson, and M. D. Reid. Composite multiclass losses. In Advances in Neural Information Processing Systems 24 (NIPS), 2011.</p>
<p>[18] P. Gr¨ nwald. The Minimum Description Length Principle. MIT Press, Cambridge, MA, 2007. u</p>
<p>[19] T. van Erven, M. Reid, and R. Williamson. Mixability is Bayes risk curvature relative to log loss. In Proceedings of the 24th Conference on Learning Theory (COLT), 2011.</p>
<p>[20] S. Arlot and P. L. Bartlett. Margin-adaptive model selection in statistical learning. Bernoulli, 17(2):687–713, 2011.</p>
<p>[21] T. Zhang. Information theoretical upper and lower bounds for statistical estimation. IEEE Transactions on Information Theory, 52(4):1307–1321, 2006.</p>
<p>[22] V. Vapnik. Statistical Learning Theory. Wiley, New York, 1998.</p>
<p>[23] J.-Y. Audibert. PAC-Bayesian statistical learning theory. PhD thesis, Universit´ Paris VI, e 2004.</p>
<p>[24] O. Catoni. PAC-Bayesian Supervised Classiﬁcation. Lecture Notes-Monograph Series. IMS, 2007.</p>
<p>[25] W. Lee, P. Bartlett, and R. Williamson. The importance of convexity in learning with squared loss. IEEE Transactions on Information Theory, 44(5):1974–1980, 1998. Correction, Volume 54(9), 4395 (2008).</p>
<p>[26] A. N. Shiryaev. Probability. Springer-Verlag, 1996.</p>
<p>[27] J.-Y. Audibert. A better variance control for PAC-Bayesian classiﬁcation. Preprint 905, Laboratoire de Probabilit´ s et Mod` les Al´ atoires, Universit´ s Paris 6 and Paris 7, 2004. e e e e 9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
