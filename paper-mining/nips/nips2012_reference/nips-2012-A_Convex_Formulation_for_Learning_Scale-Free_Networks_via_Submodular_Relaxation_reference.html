<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>6 nips-2012-A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-6" href="../nips2012/nips-2012-A_Convex_Formulation_for_Learning_Scale-Free_Networks_via_Submodular_Relaxation.html">nips2012-6</a> <a title="nips-2012-6-reference" href="#">nips2012-6-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>6 nips-2012-A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation</h1>
<br/><p>Source: <a title="nips-2012-6-pdf" href="http://papers.nips.cc/paper/4538-a-convex-formulation-for-learning-scale-free-networks-via-submodular-relaxation.pdf">pdf</a></p><p>Author: Aaron Defazio, Tibério S. Caetano</p><p>Abstract: A key problem in statistics and machine learning is the determination of network structure from data. We consider the case where the structure of the graph to be reconstructed is known to be scale-free. We show that in such cases it is natural to formulate structured sparsity inducing priors using submodular functions, and we use their Lov´ sz extension to obtain a convex relaxation. For tractable classes a such as Gaussian graphical models, this leads to a convex optimization problem that can be efﬁciently solved. We show that our method results in an improvement in the accuracy of reconstructed networks for synthetic data. We also show how our prior encourages scale-free reconstructions on a bioinfomatics dataset.</p><br/>
<h2>reference text</h2><p>[1] Francis Bach. Convex analysis and optimization with submodular functions: a tutorial. Technical report, INRIA, 2010.</p>
<p>[2] Francis Bach. Structured sparsity-inducing norms through submodular functions. NIPS, 2010.</p>
<p>[3] Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski. Optimization with sparsityinducing penalties. Foundations and Trends in Machine Learning, 2012.</p>
<p>[4] Albert-Laszlo Barabasi and Reka Albert. Emergence of scaling in random networks. Science, 286:509– 512, 1999.</p>
<p>[5] Moshen Bayati, Jeong Han Kim, and Amin Saberi. A sequential algorithm for generating random graphs. Algorithmica, 58, 2009.</p>
<p>[6] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3, 2011.</p>
<p>[7] Emmanuel J. Candes, Michael B. Wakin, and Stephen P. Boyd. Enhancing sparsity by reweighted l1 minimization. Journal of Fourier Analysis and Applications, 2008.</p>
<p>[8] A. P. Dempster. Covariance selection. Biometrics, 28:157–175, 1972.</p>
<p>[9] Adrian Dobra, Chris Hans, Beatrix Jones, Joseph R Nevins, and Mike West. Sparse graphical models for exploring gene expression data. Journal of Multivariate Analysis, 2004.</p>
<p>[10] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 2007.</p>
<p>[11] Saruto Fujishige. Submodular Functions and Optimization. Elsevier, 2005.</p>
<p>[12] Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski, and Francis Bach. Proximal methods for sparse hierarchical dictionary learning. ICML, 2010.</p>
<p>[13] Rodolphe Jenatton, Guillaume Obozinski, and Francis Bach. Structured sparse principal component analysis. AISTATS, 2010.</p>
<p>[14] Qiang Liu and Alexander Ihler. Learning scale free networks by reweighted l1 regularization. AISTATS, 2011.</p>
<p>[15] Zhaosong Lu. Smooth optimization approach for sparse covariance selection. SIAM J. Optim., 2009.</p>
<p>[16] Zachary M. Saul and Vladimir Filkov. Exploring biological network structure using exponential random graph models. Bioinformatics, 2007.</p>
<p>[17] Katya Scheinberg and Shiqian Ma. Optimization for Machine Learning, chapter 17. optimization methods for sparse inverse covariance selection. MIT Press, 2011.</p>
<p>[18] Katya Scheinbert, Shiqian Ma, and Donald Goldfarb. Sparse inverse covariance selection via alternating linearization methods. In NIPS, 2010.</p>
<p>[19] T. Snijders. Markov chain monte carlo estimation of exponential random graph models. Journal of Social Structure, 2002.</p>
<p>[20] Tom A.B. Snijders, Philippa E. Pattison, and Mark S. Handcock. New speciﬁcations for exponential random graph models. Technical report, University of Washington, 2004.</p>
<p>[21] Alan Terry. Exponential random graphs. Master’s thesis, University of York, 2005.</p>
<p>[22] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society, 2007.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
