<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>84 nips-2012-Convergence Rate Analysis of MAP Coordinate Minimization Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-84" href="../nips2012/nips-2012-Convergence_Rate_Analysis_of_MAP_Coordinate_Minimization_Algorithms.html">nips2012-84</a> <a title="nips-2012-84-reference" href="#">nips2012-84-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>84 nips-2012-Convergence Rate Analysis of MAP Coordinate Minimization Algorithms</h1>
<br/><p>Source: <a title="nips-2012-84-pdf" href="http://papers.nips.cc/paper/4754-convergence-rate-analysis-of-map-coordinate-minimization-algorithms.pdf">pdf</a></p><p>Author: Ofer Meshi, Amir Globerson, Tommi S. Jaakkola</p><p>Abstract: Finding maximum a posteriori (MAP) assignments in graphical models is an important task in many applications. Since the problem is generally hard, linear programming (LP) relaxations are often used. Solving these relaxations efﬁciently is thus an important practical problem. In recent years, several authors have proposed message passing updates corresponding to coordinate descent in the dual LP. However, these are generally not guaranteed to converge to a global optimum. One approach to remedy this is to smooth the LP, and perform coordinate descent on the smoothed dual. However, little is known about the convergence rate of this procedure. Here we perform a thorough rate analysis of such schemes and derive primal and dual convergence rates. We also provide a simple dual to primal mapping that yields feasible primal solutions with a guaranteed rate of convergence. Empirical evaluation supports our theoretical claims and shows that the method is highly competitive with state of the art approaches that yield global optima. 1</p><br/>
<h2>reference text</h2><p>[1] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM J. Img. Sci., 2(1):183–202, Mar. 2009.</p>
<p>[2] Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy minimization via graph cuts. In Proc. IEEE Conf. Comput. Vision Pattern Recog., 1999.</p>
<p>[3] D. Burshtein. Iterative approximate linear programming decoding of ldpc codes with linear complexity. IEEE Transactions on Information Theory, 55(11):4835–4859, 2009.</p>
<p>[4] G. Elidan, I. Mcgraw, and D. Koller. Residual belief propagation: informed scheduling for asynchronous message passing. In UAI, 2006.</p>
<p>[5] A. Globerson and T. Jaakkola. Fixing max-product: Convergent message passing algorithms for MAP LP-relaxations. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, NIPS 20. MIT Press, 2008.</p>
<p>[6] M. Guignard and S. Kim. Lagrangean decomposition: A model yielding stronger Lagrangean bounds. Mathematical Programming, 39(2):215–228, 1987.</p>
<p>[7] T. Hazan and A. Shashua. Norm-product belief propagation: Primal-dual message-passing for approximate inference. IEEE Transactions on Information Theory, 56(12):6294–6316, 2010.</p>
<p>[8] V. Jojic, S. Gould, and D. Koller. Fast and smooth: Accelerated dual decomposition for MAP inference. In Proceedings of International Conference on Machine Learning (ICML), 2010.</p>
<p>[9] V. Kolmogorov. Convergent tree-reweighted message passing for energy minimization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(10):1568–1583, 2006.</p>
<p>[10] A. L. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar, N. A. Smith, and E. P. Xing. An augmented lagrangian approach to constrained map inference. In ICML, pages 169–176, 2011.</p>
<p>[11] K. Matusita. On the notion of afﬁnity of several distributions and some of its applications. Annals of the Institute of Statistical Mathematics, 19:181–192, 1967. 10.1007/BF02911675.</p>
<p>[12] O. Meshi and A. Globerson. An alternating direction method for dual map lp relaxation. In ECML PKDD, pages 470–483. Springer-Verlag, 2011.</p>
<p>[13] O. Meshi, D. Sontag, T. Jaakkola, and A. Globerson. Learning efﬁciently with approximate inference via dual losses. In ICML, pages 783–790, New York, NY, USA, 2010. ACM.</p>
<p>[14] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course, volume 87. Kluwer Academic Publishers, 2004.</p>
<p>[15] Y. Nesterov. Smooth minimization of non-smooth functions. Math. Prog., 103(1):127–152, May 2005.</p>
<p>[16] Y. Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. Core discussion papers, Universit catholique de Louvain, 2010.</p>
<p>[17] A. Saha and A. Tewari. On the ﬁnite time convergence of cyclic coordinate descent methods, 2010. preprint arXiv:1005.2146.</p>
<p>[18] B. Savchynskyy, S. Schmidt, J. Kappes, and C. Schnorr. A study of Nesterov’s scheme for lagrangian decomposition and map labeling. CVPR, 2011.</p>
<p>[19] B. Savchynskyy, S. Schmidt, J. H. Kappes, and C. Schn¨ rr. Efﬁcient mrf energy minimization via adaptive o diminishing smoothing. In UAI, 2012.</p>
<p>[20] S. Shalev-Shwartz and A. Tewari. Stochastic methods for l1-regularized loss minimization. J. Mach. Learn. Res., 12:1865–1892, July 2011.</p>
<p>[21] D. Sontag, A. Globerson, and T. Jaakkola. Introduction to dual decomposition for inference. In Optimization for Machine Learning, pages 219–254. MIT Press, 2011.</p>
<p>[22] P. Tseng. Convergence of a block coordinate descent method for nondifferentiable minimization 1. Journal of Optimization Theory and Applications, 109(3):475–494, 2001.</p>
<p>[23] M. Wainwright, T. Jaakkola, and A. Willsky. MAP estimation via agreement on trees: message-passing and linear programming. IEEE Transactions on Information Theory, 51(11):3697–3717, 2005.</p>
<p>[24] M. Wainwright and M. I. Jordan. Graphical Models, Exponential Families, and Variational Inference. Now Publishers Inc., Hanover, MA, USA, 2008.</p>
<p>[25] T. Werner. A linear programming approach to max-sum problem: A review. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(7):1165–1179, 2007.</p>
<p>[26] T. Werner. Revisiting the decomposition approach to inference in exponential families and graphical models. Technical Report CTU-CMP-2009-06, Czech Technical University, 2009.</p>
<p>[27] T. Werner. How to compute primal solution from dual one in MAP inference in MRF? In Control Systems and Computers (special issue on Optimal Labeling Problems in Structual Pattern Recognition), 2011.</p>
<p>[28] C. Yanover, T. Meltzer, and Y. Weiss. Linear programming relaxations and belief propagation – an empirical study. Journal of Machine Learning Research, 7:1887–1907, 2006.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
