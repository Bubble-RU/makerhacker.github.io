<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>43 nips-2012-Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-43" href="../nips2012/nips-2012-Approximate_Message_Passing_with_Consistent_Parameter_Estimation_and_Applications_to_Sparse_Learning.html">nips2012-43</a> <a title="nips-2012-43-reference" href="#">nips2012-43-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>43 nips-2012-Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning</h1>
<br/><p>Source: <a title="nips-2012-43-pdf" href="http://papers.nips.cc/paper/4498-approximate-message-passing-with-consistent-parameter-estimation-and-applications-to-sparse-learning.pdf">pdf</a></p><p>Author: Ulugbek Kamilov, Sundeep Rangan, Michael Unser, Alyson K. Fletcher</p><p>Abstract: We consider the estimation of an i.i.d. vector x ∈ Rn from measurements y ∈ Rm obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise (possibly nonlinear) measurement channel. We present a method, called adaptive generalized approximate message passing (Adaptive GAMP), that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector x. Our method can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identiﬁcation of linear-nonlinear cascade models in dynamical systems and neural spiking processes. We prove that for large i.i.d. Gaussian transform matrices the asymptotic componentwise behavior of the adaptive GAMP algorithm is predicted by a simple set of scalar state evolution equations. This analysis shows that the adaptive GAMP method can yield asymptotically consistent parameter estimates, which implies that the algorithm achieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values. The adaptive GAMP methodology thus provides a systematic, general and computationally efﬁcient method applicable to a large range of complex linear-nonlinear models with provable guarantees. 1</p><br/>
<h2>reference text</h2><p>[1] M. Tipping, “Sparse Bayesian learning and the relevance vector machine,” J. Machine Learning Research, vol. 1, pp. 211–244, Sep. 2001.</p>
<p>[2] M. West, “Bayesian factor regressionm models in the “large p, small n” paradigm,” Bayesian Statistics, vol. 7, 2003.  8</p>
<p>[3] D. Wipf and B. Rao, “Sparse Bayesian learning for basis selection,” IEEE Trans. Signal Process., vol. 52, no. 8, pp. 2153–2164, Aug. 2004.</p>
<p>[4] S. Ji, Y. Xue, and L. Carin, “Bayesian compressive sensing,” IEEE Trans. Signal Process., vol. 56, pp. 2346–2356, Jun. 2008.</p>
<p>[5] V. Cevher, “Learning with compressible priors,” in Proc. NIPS, Vancouver, BC, Dec. 2009.</p>
<p>[6] S. Billings and S. Fakhouri, “Identiﬁcation of systems containing linear dynamic and static nonlinear elements,” Automatica, vol. 18, no. 1, pp. 15–26, 1982.</p>
<p>[7] I. W. Hunter and M. J. Korenberg, “The identiﬁcation of nonlinear biological systems: Wiener and Hammerstein cascade models,” Biological Cybernetics, vol. 55, no. 2–3, pp. 135–144, 1986.</p>
<p>[8] O. Schwartz, J. W. Pillow, N. C. Rust, and E. P. Simoncelli, “Spike-triggered neural characterization,” J. Vision, vol. 6, no. 4, pp. 484–507, Jul. 2006.</p>
<p>[9] J. P. Vila and P. Schniter, “Expectation-maximization Bernoulli-Gaussian approximate message passing,” in Conf. Rec. 45th Asilomar Conf. Signals, Syst. & Comput., Paciﬁc Grove, CA, Nov. 2011, pp. 799–803.</p>
<p>[10] ——, “Expectation-maximization Gaussian-mixture approximate message passing,” in Proc. Conf. on Inform. Sci. & Sys., Princeton, NJ, Mar. 2012.</p>
<p>[11] F. Krzakala, M. M´ zard, F. Sausset, Y. Sun, and L. Zdeborov´ , “Statistical physics-based reconstruction e a in compressed sensing,” arXiv:1109.4424, Sep. 2011.</p>
<p>[12] ——, “Probabilistic reconstruction in compressed sensing: Algorithms, phase diagrams, and threshold achieving matrices,” arXiv:1206.3953, Jun. 2012.</p>
<p>[13] S. Rangan, A. K. Fletcher, V. K. Goyal, and P. Schniter, “Hybrid generalized approximation message passing with applications to structured sparsity,” in Proc. IEEE Int. Symp. Inform. Theory, Cambridge, MA, Jul. 2012, pp. 1241–1245.</p>
<p>[14] S. Rangan, “Generalized approximate message passing for estimation with random linear mixing,” in Proc. IEEE Int. Symp. Inform. Theory, Saint Petersburg, Russia, Jul.–Aug. 2011, pp. 2174–2178.</p>
<p>[15] D. Guo and C.-C. Wang, “Asymptotic mean-square optimality of belief propagation for sparse linear systems,” in Proc. IEEE Inform. Theory Workshop, Chengdu, China, Oct. 2006, pp. 194–198.</p>
<p>[16] ——, “Random sparse linear systems observed via arbitrary channels: A decoupling principle,” in Proc. IEEE Int. Symp. Inform. Theory, Nice, France, Jun. 2007, pp. 946–950.</p>
<p>[17] S. Rangan, “Estimation with random linear mixing, belief propagation and compressed sensing,” in Proc. Conf. on Inform. Sci. & Sys., Princeton, NJ, Mar. 2010, pp. 1–6.</p>
<p>[18] M. Bayati and A. Montanari, “The dynamics of message passing on dense graphs, with applications to compressed sensing,” IEEE Trans. Inform. Theory, vol. 57, no. 2, pp. 764–785, Feb. 2011.</p>
<p>[19] U. S. Kamilov, S. Rangan, A. K. Fletcher, and M. Unser, “Approximate message passing with consistent parameter estimation and applications to sparse learning,” arXiv:1207.3859 [cs.IT], Jul. 2012.</p>
<p>[20] J. Boutros and G. Caire, “Iterative multiuser joint decoding: Uniﬁed framework and asymptotic analysis,” IEEE Trans. Inform. Theory, vol. 48, no. 7, pp. 1772–1793, Jul. 2002.</p>
<p>[21] T. Tanaka and M. Okada, “Approximate belief propagation, density evolution, and neurodynamics for CDMA multiuser detection,” IEEE Trans. Inform. Theory, vol. 51, no. 2, pp. 700–706, Feb. 2005.</p>
<p>[22] D. L. Donoho, A. Maleki, and A. Montanari, “Message-passing algorithms for compressed sensing,” Proc. Nat. Acad. Sci., vol. 106, no. 45, pp. 18 914–18 919, Nov. 2009.</p>
<p>[23] T. P. Minka, “A family of algorithms for approximate Bayesian inference,” Ph.D. dissertation, Massachusetts Institute of Technology, Cambridge, MA, 2001.</p>
<p>[24] M. Seeger, “Bayesian inference and optimal design for the sparse linear model,” J. Machine Learning Research, vol. 9, pp. 759–813, Sep. 2008.</p>
<p>[25] E. J. Cand` s and T. Tao, “Near-optimal signal recovery from random projections: Universal encoding e strategies?” IEEE Trans. Inform. Theory, vol. 52, no. 12, pp. 5406–5425, Dec. 2006.</p>
<p>[26] D. Donoho, I. Johnstone, A. Maleki, and A. Montanari, “Compressed sensing over mean square error,” in Proc. ISIT, St. Petersburg, Russia, Jun. 2011.  9  p  -balls: Minimax</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
