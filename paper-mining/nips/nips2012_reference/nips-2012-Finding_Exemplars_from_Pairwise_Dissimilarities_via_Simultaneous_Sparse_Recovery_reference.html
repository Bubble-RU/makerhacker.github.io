<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>133 nips-2012-Finding Exemplars from Pairwise Dissimilarities via Simultaneous Sparse Recovery</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-133" href="../nips2012/nips-2012-Finding_Exemplars_from_Pairwise_Dissimilarities_via_Simultaneous_Sparse_Recovery.html">nips2012-133</a> <a title="nips-2012-133-reference" href="#">nips2012-133-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>133 nips-2012-Finding Exemplars from Pairwise Dissimilarities via Simultaneous Sparse Recovery</h1>
<br/><p>Source: <a title="nips-2012-133-pdf" href="http://papers.nips.cc/paper/4705-finding-exemplars-from-pairwise-dissimilarities-via-simultaneous-sparse-recovery.pdf">pdf</a></p><p>Author: Ehsan Elhamifar, Guillermo Sapiro, René Vidal</p><p>Abstract: Given pairwise dissimilarities between data points, we consider the problem of ﬁnding a subset of data points, called representatives or exemplars, that can efﬁciently describe the data collection. We formulate the problem as a row-sparsity regularized trace minimization problem that can be solved efﬁciently using convex programming. The solution of the proposed optimization program ﬁnds the representatives and the probability that each data point is associated with each one of the representatives. We obtain the range of the regularization parameter for which the solution of the proposed optimization program changes from selecting one representative for all data points to selecting all data points as representatives. When data points are distributed around multiple clusters according to the dissimilarities, we show that the data points in each cluster select representatives only from that cluster. Unlike metric-based methods, our algorithm can be applied to dissimilarities that are asymmetric or violate the triangle inequality, i.e., it does not require that the pairwise dissimilarities come from a metric. We demonstrate the effectiveness of the proposed algorithm on synthetic data as well as real-world image and text data. 1</p><br/>
<h2>reference text</h2><p>[1] S. Garcia, J. Derrac, J. R. Cano, and F. Herrera, “Prototype selection for nearest neighbor classiﬁcation: Taxonomy and empirical study,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 3, pp. 417–435, 2012.</p>
<p>[2] L. Kaufman and P. Rousseeuw, “Clustering by means of medoids,” In Y. Dodge (Ed.), Statistical Data Analysis based on the L1 Norm (North-Holland, Amsterdam), pp. 405–416, 1987.</p>
<p>[3] M. Gu and S. C. Eisenstat, “Efﬁcient algorithms for computing a strong rank-revealing qr factorization,” SIAM Journal on Scientiﬁc Computing, vol. 17, pp. 848–869, 1996.</p>
<p>[4] B. J. Frey and D. Dueck, “Clustering by passing messages between data points,” Science, vol. 315, pp. 972–976, 2007.</p>
<p>[5] J. A. Tropp, “Column subset selection, matrix factorization, and eigenvalue optimization,” ACM-SIAM Symp. Discrete Algorithms (SODA), pp. 978–986, 2009.</p>
<p>[6] C. Boutsidis, M. W. Mahoney, and P. Drineas, “An improved approximation algorithm for the column subset selection problem,” in Proceedings of SODA, 2009, pp. 968–977.</p>
<p>[7] E. Elhamifar, G. Sapiro, and R. Vidal, “See all by looking at a few: Sparse modeling for ﬁnding representative objects,” in IEEE Conference on Computer Vision and Pattern Recognition, 2012.</p>
<p>[8] J. Bien, Y. Xu, and M. W. Mahoney, “CUR from a sparse optimization viewpoint,” NIPS, 2010.</p>
<p>[9] T. Chan, “Rank revealing QR factorizations,” Lin. Alg. and its Appl., vol. 88-89, pp. 67–82, 1987.</p>
<p>[10] L. Balzano, R. Nowak, and W. Bajwa, “Column subset selection with missing data,” in NIPS Workshop on Low-Rank Methods for Large-Scale Machine Learning, 2010.</p>
<p>[11] E. Esser, M. Moller, S. Osher, G. Sapiro, and J. Xin, “A convex model for non-negative matrix factorization and dimensionality reduction on physical space,” IEEE Transactions on Image Processing, vol. 21, no. 7, pp. 3239–3252, 2012.</p>
<p>[12] M. Charikar, S. Guha, A. Tardos, and D. B. Shmoys, “A constant-factor approximation algorithm for the k-median problem,” Journal of Computer System Sciences, vol. 65, no. 1, pp. 129–149, 2002.</p>
<p>[13] B. J. Frey and D. Dueck, “Mixture modeling by afﬁnity propagation,” Neural Information Processing Systems, 2006.</p>
<p>[14] I. E. Givoni, C. Chung, and B. J. Frey, “Hierarchical afﬁnity propagation,” Conference on Uncertainty in Artiﬁcial Intelligence, 2011.</p>
<p>[15] R. Duda, P. Hart, and D. Stork, Pattern Classiﬁcation.  Wiley-Interscience, October 2004.</p>
<p>[16] D. Dueck and B. J. Frey, “Non-metric afﬁnity propagation for unsupervised image categorization,” International Conference in Computer Vision, 2007.</p>
<p>[17] N. Lazic, B. J. Frey, and P. Aarabi, “Solving the uncapacitated facility location problem using message passing algorithms,” International Conference on Artiﬁcial Intelligence and Statistics, 2007.</p>
<p>[18] R. Jenatton, J. Y. Audibert, and F. Bach, “Structured variable selection with sparsity-inducing norms,” Journal of Machine Learning Research, vol. 12, pp. 2777–2824, 2011.</p>
<p>[19] J. A. Tropp., “Algorithms for simultaneous sparse approximation. part ii: Convex relaxation,” Signal Processing, special issue “Sparse approximations in signal and image processing”, vol. 86, pp. 589–602, 2006.</p>
<p>[20] M. Aharon, M. Elad, and A. M. Bruckstein, “K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation,” IEEE Trans. on Signal Processing, vol. 54, no. 11, pp. 4311–4322, 2006.</p>
<p>[21] J. J. Hull, “A database for handwritten text recognition research,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 16, no. 5, pp. 550–554, 1994.</p>
<p>[22] M. Fanty and R. Cole, “Spoken letter recognition,” in Neural Information Processing Systems, 1991.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
