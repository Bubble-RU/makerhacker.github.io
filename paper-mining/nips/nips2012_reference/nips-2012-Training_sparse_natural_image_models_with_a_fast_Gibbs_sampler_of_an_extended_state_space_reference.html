<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>349 nips-2012-Training sparse natural image models with a fast Gibbs sampler of an extended state space</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-349" href="../nips2012/nips-2012-Training_sparse_natural_image_models_with_a_fast_Gibbs_sampler_of_an_extended_state_space.html">nips2012-349</a> <a title="nips-2012-349-reference" href="#">nips2012-349-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>349 nips-2012-Training sparse natural image models with a fast Gibbs sampler of an extended state space</h1>
<br/><p>Source: <a title="nips-2012-349-pdf" href="http://papers.nips.cc/paper/4832-training-sparse-natural-image-models-with-a-fast-gibbs-sampler-of-an-extended-state-space.pdf">pdf</a></p><p>Author: Lucas Theis, Jascha Sohl-dickstein, Matthias Bethge</p><p>Abstract: We present a new learning strategy based on an efﬁcient blocked Gibbs sampler for sparse overcomplete linear models. Particular emphasis is placed on statistical image modeling, where overcomplete models have played an important role in discovering sparse representations. Our Gibbs sampler is faster than general purpose sampling schemes while also requiring no tuning as it is free of parameters. Using the Gibbs sampler and a persistent variant of expectation maximization, we are able to extract highly sparse distributions over latent sources from data. When applied to natural images, our algorithm learns source distributions which resemble spike-and-slab distributions. We evaluate the likelihood and quantitatively compare the performance of the overcomplete linear model to its complete counterpart as well as a product of experts model, which represents another overcomplete generalization of the complete linear model. In contrast to previous claims, we ﬁnd that overcomplete representations lead to signiﬁcant improvements, but that the overcomplete linear model still underperforms other models. 1</p><br/>
<h2>reference text</h2><p>[1] D. F. Andrews and C. L. Mallows. Scale mixtures of normal distributions. Journal of the Royal Statistical Society, Series B, 36(1):99–102, 1974.</p>
<p>[2] P. Berkes, R. Turner, and M. Sahani. On sparsity and overcompleteness in image models. Advances in Neural Information Processing Systems, 20, 2008.</p>
<p>[3] R. H. Byrd, P. Lu, and J. Nocedal. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientiﬁc and Statistical Computing, 16(5):1190–1208, 1995.  8</p>
<p>[4] R.-B. Chen and Y. N. Wu. A null space method for over-complete blind source separation. Computational Statistics & Data Analysis, 51(12):5519–5536, 2007.</p>
<p>[5] T. Cover and J. Thomas. Elements of Information Theory. Wiley, 1991.</p>
<p>[6] B. J. Culpepper, J. Sohl-Dickstein, and B. A. Olshausen. Building a better probabilistic model of images by factorization. Proceedings of the International Conference on Computer Vision, 13, 2011.</p>
<p>[7] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–38, 1977.</p>
<p>[8] A. Doucet. A note on efﬁcient conditional simulation of Gaussian distributions, 2010.</p>
<p>[9] S. Duane, A. D. Kennedy, B. J. Pendleton, and D. Roweth. Hybrid Monte Carlo. Physics Letters B, 195 (2):216–222, 1987.</p>
<p>[10] A. Fischer and C. Igel. Empirical analysis of the divergence of Gibbs sampling based learning algorithms for restricted Boltzmann machines. Proceedings of the 20th International Conference on Artiﬁcial Neural Networks, 2010.</p>
<p>[11] M. Girolami. A variational method for learning sparse and overcomplete representations. Neural Computation, 13(11):2517–2532, 2001.</p>
<p>[12] N. Heess, N. Le Roux, and J. Winn. Weakly supervised learning of foreground-background segmentation using masked rbms. International Conference on Artiﬁcial Neural Networks, 2011.</p>
<p>[13] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771–1800, 2002.</p>
<p>[14] Y. Hoffman and E. Ribak. Constrained realizations of Gaussian ﬁelds: a simple algorithm. The Astrophysical Journal, 380:L5–L8, 1991.</p>
<p>[15] I. Murray and R. Salakhutdinov. Notes on the KL-divergence between a Markov chain and its equilibrium distribution, 2008.</p>
<p>[16] R. M. Neal. Annealed importance sampling. Statistics and Computing, 11(2):125–139, 2001.</p>
<p>[17] R. M. Neal. MCMC using Hamiltonian Dynamics, pages 113–162. Chapman & Hall/CRC Press, 2011.</p>
<p>[18] R. M. Neal and G. E. Hinton. A view of the EM algorithm that justiﬁes incremental, sparse, and other variants, pages 355–368. MIT Press, 1998.</p>
<p>[19] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. Nature, 381:607–609, 1996.</p>
<p>[20] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research, 37(23):3311–3325, 1997.</p>
<p>[21] B. A. Olshausen and K. J. Millman. Learning sparse codes with a mixture-of-Gaussians prior. Advances in Neural Information Processing Systems, 12, 2000.</p>
<p>[22] G. Papandreou and A. L. Yuille. Gaussian sampling by local perturbations. Advances in Neural Information Processing Systems, 23, 2010.</p>
<p>[23] T. Park and G. Casella. The Bayesian lasso. Journal of the American Statistical Association, 103(482): 681–686, 2008.</p>
<p>[24] R. Penrose. A generalized inverse for matrices. Proceedings of the Cambridge Philosophical Society, 51:406–413, 1955.</p>
<p>[25] G. O. Roberts and R. L. Tweedie. Exponential convergence of Langevin diffusions and their discrete approximations. Bernoulli, 2(4):341–363, 1996.</p>
<p>[26] B. Sallans. A hierarchical community of experts. Master’s thesis, University of Toronto, 1998.</p>
<p>[27] U. Schmidt, Q. Gao, and S. Roth. A generative perspective on MRFs in low-level vision. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2010.</p>
<p>[28] M. W. Seeger. Bayesian inference and optimal design for the sparse linear model. Journal of Machine Learning Research, 9:759–813, 2008.</p>
<p>[29] J. Sohl-Dickstein. Persistent minimum probability ﬂow, 2011.</p>
<p>[30] J. Sohl-Dickstein and B. J. Culpepper. Hamiltonian annealed importance sampling for partition function estimation, 2012.</p>
<p>[31] J. Sohl-Dickstein, P. Battaglino, and M. R. DeWeese. Minimum probability ﬂow learning. Proceedings of the 28th International Conference on Machine Learning, 2011.</p>
<p>[32] T. Tieleman. Training restricted Boltzmann machines using approximations to the likelihood gradient. Proceedings of the 25th International Conference on Machine Learning, 2008.</p>
<p>[33] J. H. van Hateren and A. van der Schaaf. Independent component ﬁlters of natural images compared with simple cells in primary visual cortex. Proc. of the Royal Society B: Biological Sciences, 265(1394), 1998.</p>
<p>[34] G. C. G. Wei and M. A. Tanner. A Monte Carlo implementation of the EM algorithm and the poor man’s data augmentation algorithms. Journal of the American Statistical Association, 85(411):699–704, 1990.</p>
<p>[35] M. Welling, G. Hinton, and S. Osindero. Learning sparse topographic representations with products of Student-t distributions. Advances in Neural Information Processing Systems, 15, 2003.</p>
<p>[36] L. Younes. Parametric inference for imperfectly observed Gibbsian ﬁelds. Probability Theory and Related Fields, 1999.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
