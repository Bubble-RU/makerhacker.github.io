<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>278 nips-2012-Probabilistic n-Choose-k Models for Classification and Ranking</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-278" href="../nips2012/nips-2012-Probabilistic_n-Choose-k_Models_for_Classification_and_Ranking.html">nips2012-278</a> <a title="nips-2012-278-reference" href="#">nips2012-278-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>278 nips-2012-Probabilistic n-Choose-k Models for Classification and Ranking</h1>
<br/><p>Source: <a title="nips-2012-278-pdf" href="http://papers.nips.cc/paper/4702-probabilistic-n-choose-k-models-for-classification-and-ranking.pdf">pdf</a></p><p>Author: Kevin Swersky, Brendan J. Frey, Daniel Tarlow, Richard S. Zemel, Ryan P. Adams</p><p>Abstract: In categorical data there is often structure in the number of variables that take on each label. For example, the total number of objects in an image and the number of highly relevant documents per query in web search both tend to follow a structured distribution. In this paper, we study a probabilistic model that explicitly includes a prior distribution over such counts, along with a count-conditional likelihood that deﬁnes probabilities over all subsets of a given size. When labels are binary and the prior over counts is a Poisson-Binomial distribution, a standard logistic regression model is recovered, but for other count distributions, such priors induce global dependencies and combinatorics that appear to complicate learning and inference. However, we demonstrate that simple, efﬁcient learning procedures can be derived for more general forms of this model. We illustrate the utility of the formulation by exploring applications to multi-object classiﬁcation, learning to rank, and top-K classiﬁcation. 1</p><br/>
<h2>reference text</h2><p>[1] S. X. Chen and J. S. Liu. Statistical applications of the Poisson-Binomial and conditional Bernoulli distributions. Statistica Sinica, 7(4), 1997.</p>
<p>[2] X. H. Chen, A. P. Dempster, and J. S. Liu. Weighted ﬁnite population sampling to maximize entropy. Biometrika, 81(3):457–469, 1994.</p>
<p>[3] M. H. Gail, J. H. Lubin, and L. V. Rubinstein. Likelihood calculations for matched case-control studies and survival studies with tied death times. Biometrika, 68:703–707, 1981.</p>
<p>[4] L. Belfore. An O(n) log2(n) algorithm for computing the reliability of k-out-of-n:G and k-tol-out-of-n:G systems. IEEE Transactions on Reliability, 44(1), 1995.</p>
<p>[5] D. Tarlow, K. Swersky, R. Zemel, R.P. Adams, and B. Frey. Fast exact inference for recursive cardinality models. In Uncertainty in Artiﬁcial Intelligence, 2012.</p>
<p>[6] R. Plackett. The analysis of permutations. Applied Statistics, pages 193–202, 1975.</p>
<p>[7] R.D. Luce. Individual Choice Behavior a Theoretical Analysis. Wiley, 1959.</p>
<p>[8] J. Guiver and E. Snelson. Bayesian inference for plackett-luce ranking models. In International Conference on Machine Learning, 2009.</p>
<p>[9] J. Huang, C. Guestrin, and L. Guibas. Efﬁcient inference for distributions on permutations. In Advances in Neural Information Processing Systems, 2007.</p>
<p>[10] T. Qin, T.Y. Liu, J. Xu, and H. Li. LETOR: A benchmark collection for research on learning to rank for information retrieval. Information Retrieval Journal, 2010.</p>
<p>[11] B. Marlin, K. Swersky, B. Chen, and N. de Freitas. Inductive principles for restricted Boltzmann machine learning. In Artiﬁcial Intelligence and Statistics, 2010.</p>
<p>[12] R. Gupta, A. Diwan, and S. Sarawagi. Efﬁcient inference with cardinality-based clique potentials. In International Conference on Machine Learning, 2007.</p>
<p>[13] D. Tarlow, I. Givoni, and R. Zemel. HOP-MAP: Efﬁcient message passing for high order potentials. In Artiﬁcial Intelligence and Statistics, 2010.</p>
<p>[14] A. Delong, A. Osokin, H.N. Isack, and Y. Boykov. Fast approximate energy minimization with label costs. International Journal of Computer Vision, 96(1):127, 2012.</p>
<p>[15] T. Joachims. A support vector method for multivariate performance measures. In International Conference on Machine Learning, 2005.</p>
<p>[16] P. Pletscher and P. Kohli. Learning low-order models for enforcing high-order statistics. In Artiﬁcial Intelligence and Statistics, 2012.</p>
<p>[17] K. Ganchev, J. Graca, J. Gillenwater, and B. Taskar. Posterior regularization for structured ¸ latent variable models. Journal of Machine Learning Research, 11:2001–2049, 2010.</p>
<p>[18] G. Mann and A McCallum. Generalized expectation criteria with application to semisupervised classiﬁcation and sequence modeling. Journal of Machine Learning Research, 11:955–984, 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
