<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>144 nips-2012-Gradient-based kernel method for feature extraction and variable selection</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-144" href="../nips2012/nips-2012-Gradient-based_kernel_method_for_feature_extraction_and_variable_selection.html">nips2012-144</a> <a title="nips-2012-144-reference" href="#">nips2012-144-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>144 nips-2012-Gradient-based kernel method for feature extraction and variable selection</h1>
<br/><p>Source: <a title="nips-2012-144-pdf" href="http://papers.nips.cc/paper/4488-gradient-based-kernel-method-for-feature-extraction-and-variable-selection.pdf">pdf</a></p><p>Author: Kenji Fukumizu, Chenlei Leng</p><p>Abstract: We propose a novel kernel approach to dimension reduction for supervised learning: feature extraction and variable selection; the former constructs a small number of features from predictors, and the latter ﬁnds a subset of predictors. First, a method of linear feature extraction is proposed using the gradient of regression function, based on the recent development of the kernel method. In comparison with other existing methods, the proposed one has wide applicability without strong assumptions on the regressor or type of variables, and uses computationally simple eigendecomposition, thus applicable to large data sets. Second, in combination of a sparse penalty, the method is extended to variable selection, following the approach by Chen et al. [2]. Experimental results show that the proposed methods successfully ﬁnd effective features and variables without parametric models. 1</p><br/>
<h2>reference text</h2><p>[1] L. Breiman and J. Friedman. Estimating optimal transformations for multiple regression and correlation. J. Amer. Stat. Assoc., 80:580–598, 1985.</p>
<p>[2] X. Chen, C. Zou, and R. Dennis Cook. Coordinate-independent sparse sufﬁcient dimension reduction and variable selection. Ann. Stat., 38(6):3696–3723, 2010.</p>
<p>[3] R. Dennis Cook and L. Forzani. Principal ﬁtted components for dimension reduction in regression. Statistical Science, 23(4):485–501, 2008.</p>
<p>[4] R. Dennis Cook and S. Weisberg. Discussion of Li (1991). J. Amer. Stat. Assoc., 86:328–332, 1991.</p>
<p>[5] J. Fan and I. Gijbels. Local Polynomial Modelling and its Applications. Chapman and Hall, 1996.</p>
<p>[6] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. J. Amer. Stat. Assoc., 96(456):1348–1360, 2001.</p>
<p>[7] A. Frank and A. Asuncion. UCI machine learning repository, [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science. 2010.</p>
<p>[8] K. Fukumizu, F.R. Bach, and M.I. Jordan. Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces. JMLR, 5:73–99, 2004.</p>
<p>[9] K. Fukumizu, F.R. Bach, and M.I. Jordan. Kernel dimension reduction in regression. Ann. Stat., 37(4):1871–1905, 2009.</p>
<p>[10] A. Gretton, K. Fukumizu, C.H. Teo, L. Song, B. Sch¨ lkopf, and Alex Smola. A kernel statistical test of o independence. In Advances in NIPS 20, pages 585–592. 2008.</p>
<p>[11] M. Hristache, A. Juditsky, J. Polzehl, and V. Spokoiny. Structure adaptive approach for dimension reduction. Ann. Stat., 29(6):1537–1566, 2001.</p>
<p>[12] B. Li, H. Zha, and F. Chiaromonte. Contour regression: A general approach to dimension reduction. Ann. Stat., 33(4):1580–1616, 2005.</p>
<p>[13] K.-C. Li. Sliced inverse regression for dimension reduction (with discussion). J. Amer. Stat. Assoc., 86:316–342, 1991.</p>
<p>[14] K.-C. Li. On principal Hessian directions for data visualization and dimension reduction: Another application of Stein’s lemma. J. Amer. Stat. Assoc., 87:1025–1039, 1992.</p>
<p>[15] S. Liu, Z. Liu, J. Sun, and L. Liu. Application of synergetic neural network in online writeprint identiﬁcation. Intern. J. Digital Content Technology and its Applications, 5(3):126–135, 2011.</p>
<p>[16] L. Meier, S. Van De Geer, and P. B¨ hlmann. The group lasso for logistic regression. J. Royal Stat. Soc.: u Ser. B, 70(1):53–71, 2008.</p>
<p>[17] A.M. Samarov. Exploring regression structure using nonparametric functional estimation. J. Amer. Stat. Assoc., 88(423):836–847, 1993.</p>
<p>[18] S. K. Shevade and S. S. Keerthi. A simple and efﬁcient algorithm for gene selection using sparse logistic regression. Bioinformatics, 19(17):2246–2253, 2003.</p>
<p>[19] L. Song, J. Huang, A. Smola, and K. Fukumizu. Hilbert space embeddings of conditional distributions with applications to dynamical systems. In Proc. ICML2009, pages 961–968. 2009.</p>
<p>[20] B. K. Sriperumbudur, A. Gretton, K. Fukumizu, B. Sch¨ lkopf, and G.R.G. Lanckriet. Hilbert space o embeddings and metrics on probability measures. JMLR, 11:1517–1561, 2010.</p>
<p>[21] I. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.</p>
<p>[22] G.W. Stewart and J.-Q. Sun. Matrix Perturbation Theory. Academic Press, 1990.</p>
<p>[23] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal Stat. Soc.: Ser. B, 58(1):pp. 267–288, 1996.</p>
<p>[24] H. Wang and C. Leng. Uniﬁed lasso estimation by least squares approximation. J. Amer. Stat. Assoc., 102 (479):1039–1048, 2007.</p>
<p>[25] H. Wang, G. Li, and C.-L. Tsai. Regression coefﬁcient and autoregressive order shrinkage and selection via the lasso. J. Royal Stat. Soc.: Ser. B, 69(1):63–78, 2007.</p>
<p>[26] H. Wang, G. Li, and C.-L. Tsai. On the consistency of SCAD tunign parameter selector. Biometrika, 94: 553–558, 2007.</p>
<p>[27] H. Wang, B. Li, and C. Leng. Shrinkage tuning parameter selection with a diverging number of parameters. J. Royal Stat. Soc.: Ser. B, 71(3):671–683, 2009.</p>
<p>[28] M. Wang, F. Sha, and M. Jordan. Unsupervised kernel dimension reduction. NIPS 23, pages 2379–2387. 2010.</p>
<p>[29] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. J. Royal Stat. Soc.: Ser. B, 68(1):49–67, 2006.</p>
<p>[30] H. Zou. The adaptive lasso and its oracle properties. J. Amer. Stat. Assoc., 101:1418–1429, 2006.</p>
<p>[31] C. Zou and X. Chen. On the consistency of coordinate-independent sparse estimation with BIC. J. Multivariate Analysis, 112:248–255, 2012.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
