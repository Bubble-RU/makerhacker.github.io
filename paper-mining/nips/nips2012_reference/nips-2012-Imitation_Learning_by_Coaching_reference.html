<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>160 nips-2012-Imitation Learning by Coaching</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-160" href="../nips2012/nips-2012-Imitation_Learning_by_Coaching.html">nips2012-160</a> <a title="nips-2012-160-reference" href="#">nips2012-160-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>160 nips-2012-Imitation Learning by Coaching</h1>
<br/><p>Source: <a title="nips-2012-160-pdf" href="http://papers.nips.cc/paper/4545-imitation-learning-by-coaching.pdf">pdf</a></p><p>Author: He He, Jason Eisner, Hal Daume</p><p>Abstract: Imitation Learning has been shown to be successful in solving many challenging real-world problems. Some recent approaches give strong performance guarantees by training the policy iteratively. However, it is important to note that these guarantees depend on how well the policy we found can imitate the oracle on the training data. When there is a substantial difference between the oracle’s ability and the learner’s policy space, we may fail to ﬁnd a policy that has low error on the training set. In such cases, we propose to use a coach that demonstrates easy-to-learn actions for the learner and gradually approaches the oracle. By a reduction of learning by demonstration to online learning, we prove that coaching can yield a lower regret bound than using the oracle. We apply our algorithm to cost-sensitive dynamic feature selection, a hard decision problem that considers a user-speciﬁed accuracy-cost trade-off. Experimental results on UCI datasets show that our method outperforms state-of-the-art imitation learning methods in dynamic feature selection and two static feature selection methods. 1</p><br/>
<h2>reference text</h2><p>[1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In ICML, 2004. 8</p>
<p>[2] M. Veloso B. D. Argall, S. Chernova and B. Browning. A survey of robot learning from demonstration. 2009.</p>
<p>[3] Stéphane. Ross, Geoffrey J. Gordon, and J. Andrew. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS, 2011.</p>
<p>[4] D. McAllester, T. Hazan, and J. Keshet. Direct loss minimization for structured prediction. In NIPS, 2010.</p>
<p>[5] D. Klein P. Liang, A. Bouchard-Ct and B. Taskar. An end-to-end discriminative approach to machine translation. In ACL, 2006.</p>
<p>[6] D. Chiang, Y. Marton, and P. Resnik. Online large-margin training of syntactic and structural translation features. In EMNLP, 2008.</p>
<p>[7] R. Busa-Fekete D. Benbouzid and B. Kégl. Fast classiﬁcation using space decision dags. In ICML, 2012.</p>
<p>[8] P. Preux G. Dulac-Arnold, L. Denoyer and P. Gallinari. Datum-wise classiﬁcation: a sequential approach to sparsity. In ECML, 2011.</p>
<p>[9] Stéphane Ross and J. Andrew Bagnell. Efﬁcient reductions for imitation learning. In AISTATS, 2010.</p>
<p>[10] Kääriäinen. Lower bounds for reductions. In Atomic Learning Workshop, 2006.</p>
<p>[11] Hal Daumé III, John Langford, and Daniel Marcu. Search-based structured prediction. Machine Learning Journal (MLJ), 2009.</p>
<p>[12] Elad Hazan, Adam Kalai, Satyen Kale, and Amit Agarwal. Logarithmic regret algorithms for online convex optimization. In COLT, pages 499–513, 2006.</p>
<p>[13] Sham M. Kakade and Shai Shalev-shwartz. Mind the duality gap: Logarithmic regret algorithms for online optimization. In NIPS, 2008.</p>
<p>[14] Q. Le C. B. Do and C.S. Foo. Proximal regularization for online and batch learning. In ICML, 2009.</p>
<p>[15] H Brendan Mcmahan. Follow-the-regularized-leader and mirror descent : Equivalence theorems and l1 regularization. JMLR, 15:525–533, 2011.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
