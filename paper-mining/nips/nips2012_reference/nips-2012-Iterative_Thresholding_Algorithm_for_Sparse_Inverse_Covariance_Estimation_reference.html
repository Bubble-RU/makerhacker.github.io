<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>164 nips-2012-Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-164" href="../nips2012/nips-2012-Iterative_Thresholding_Algorithm_for_Sparse_Inverse_Covariance_Estimation.html">nips2012-164</a> <a title="nips-2012-164-reference" href="#">nips2012-164-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>164 nips-2012-Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation</h1>
<br/><p>Source: <a title="nips-2012-164-pdf" href="http://papers.nips.cc/paper/4574-iterative-thresholding-algorithm-for-sparse-inverse-covariance-estimation.pdf">pdf</a></p><p>Author: Benjamin Rolfs, Bala Rajaratnam, Dominique Guillot, Ian Wong, Arian Maleki</p><p>Abstract: The 1 -regularized maximum likelihood estimation problem has recently become a topic of great interest within the machine learning, statistics, and optimization communities as a method for producing sparse inverse covariance estimators. In this paper, a proximal gradient method (G-ISTA) for performing 1 -regularized covariance matrix estimation is presented. Although numerous algorithms have been proposed for solving this problem, this simple proximal gradient method is found to have attractive theoretical and numerical properties. G-ISTA has a linear rate of convergence, resulting in an O(log ε) iteration complexity to reach a tolerance of ε. This paper gives eigenvalue bounds for the G-ISTA iterates, providing a closed-form linear convergence rate. The rate is shown to be closely related to the condition number of the optimal point. Numerical convergence results and timing comparisons for the proposed method are presented. G-ISTA is shown to perform very well, especially when the optimal point is well-conditioned. 1</p><br/>
<h2>reference text</h2><p>[1] O. Banerjee, L. El Ghaoui, and A. d’Aspremont. Model selection through sparse maximum likelihood estimation for multivarate gaussian or binary data. Journal of Machine Learning Research, 9:485–516, 2008.</p>
<p>[2] Jonathan Barzilai and Jonathan M. Borwein. Two-Point Step Size Gradient Methods. IMA Journal of Numerical Analysis, 8(1):141–148, 1988.</p>
<p>[3] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2:183–202, 2009. ISSN 1936-4954.</p>
<p>[4] S. Becker, E.J. Candes, and M. Grant. Templates for convex cone problems with applications to sparse signal recovery. Mathematical Programming Computation, 3:165–218, 2010.</p>
<p>[5] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.</p>
<p>[6] P. Brohan, J. J. Kennedy, I. Harris, S. F. B. Tett, and P. D. Jones. Uncertainty estimates in regional and global observed temperature changes: A new data set from 1850. Journal of Geophysical Research, 111, 2006.</p>
<p>[7] George H.G. Chen and R.T. Rockafellar. Convergence rates in forward-backward splitting. Siam Journal on Optimization, 7:421–444, 1997.</p>
<p>[8] Patrick L. Combettes and Val´ rie R. Wajs. Signal recovery by proximal forward-backward splitting. e Multiscale Modeling & Simulation, 4(4):1168–1200, 2005.</p>
<p>[9] Alexandre D’Aspremont, Onureena Banerjee, and Laurent El Ghaoui. First-order methods for sparse covariance selection. SIAM Journal on Matrix Analysis and Applications, 30(1):56–66, 2008.</p>
<p>[10] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9:432–441, 2008.</p>
<p>[11] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming, version 1.21. http://cvxr.com/cvx, April 2011.</p>
<p>[12] Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 1990.</p>
<p>[13] Cho-Jui Hsieh, Matyas A. Sustik, Inderjit S. Dhillon, and Pradeep K. Ravikumar. Sparse inverse covariance matrix estimation using quadratic approximation. In Advances in Neural Information Processing Systems 24, pages 2330–2338. 2011.</p>
<p>[14] S.L. Lauritzen. Graphical models. Oxford Science Publications. Clarendon Press, 1996.</p>
<p>[15] Zhaosong Lu. Smooth optimization approach for sparse covariance selection. SIAM Journal on Optimization, 19(4):1807–1827, 2009. ISSN 1052-6234. doi: http://dx.doi.org/10.1137/070695915.</p>
<p>[16] Zhaosong Lu. Adaptive ﬁrst-order methods for general sparse inverse covariance selection. SIAM Journal on Matrix Analysis and Applications, 31:2000–2016, 2010.</p>
<p>[17] Rahul Mazumder and Deepak K. Agarwal. A ﬂexible, scalable and efﬁcient algorithmic framework for the Primal graphical lasso. Pre-print, 2011.</p>
<p>[18] Rahul Mazumder and Trevor Hastie. The graphical lasso: New insights and alternatives. Pre-print, 2011.</p>
<p>[19] Yurii Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2 ). Soviet Mathematics Doklady, 27(2):372–376, 1983.</p>
<p>[20] Yurii Nesterov. Introductory Lectures on Convex Optimization. Kluwer Academic Publishers, 2004.</p>
<p>[21] Yurii Nesterov. Gradient methods for minimizing composite objective function. CORE discussion papers, Universit´ catholique de Louvain, Center for Operations Research and Econometrics (CORE), 2007. e</p>
<p>[22] Jennifer Pittman, Erich Huang, Holly Dressman, Cheng-Fang F. Horng, Skye H. Cheng, Mei-Hua H. Tsou, Chii-Ming M. Chen, Andrea Bild, Edwin S. Iversen, Andrew T. Huang, Joseph R. Nevins, and Mike West. Integrated modeling of clinical and gene expression information for personalized prediction of disease outcomes. Proceedings of the National Academy of Sciences of the United States of America, 101(22):8431–8436, 2004.</p>
<p>[23] Benjamin T. Rolfs and Bala Rajaratnam. A note on the lack of symmetry in the graphical lasso. Computational Statistics and Data Analysis, 2012.</p>
<p>[24] Katya Scheinberg, Shiqian Ma, and Donald Goldfarb. Sparse inverse covariance selection via alternating linearization methods. In Advances in Neural Information Processing Systems 23, pages 2101–2109. 2010.</p>
<p>[25] Paul Tseng. On accelerated proximal gradient methods for convex-concave optimization. submitted to SIAM Journal on Optimization, 2008.</p>
<p>[26] Lieven Vandenberghe, Stephen Boyd, and Shao-Po Wu. Determinant maximization with linear matrix inequality constraints. SIAM Journal on Matrix Analysis and Applications, 19:499–533, 1996.</p>
<p>[27] J. Whittaker. Graphical Models in Applied Multivariate Statistics. Wiley, 1990.</p>
<p>[28] J. Won, J. Lim, S. Kim, and B. Rajaratnam. Condition number regularized covariance estimation. Journal of the Royal Statistical Society Series B, 2012.</p>
<p>[29] Stephen J. Wright, Robert D. Nowak, and M´ rio A. T. Figueiredo. Sparse reconstruction by separable a approximation. IEE Transactions on Signal Processing, 57(7):2479–2493, 2009.</p>
<p>[30] Ming Yuan and Yi Lin. Model selection and estimation in the gaussian graphical model. Biometrika, 94 (1):19–35, 2007.</p>
<p>[31] X.M. Yuan. Alternating direction method of multipliers for covariance selection models. Journal of Scientiﬁc Computing, pages 1–13, 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
