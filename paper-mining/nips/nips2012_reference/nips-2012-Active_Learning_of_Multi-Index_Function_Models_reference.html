<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>34 nips-2012-Active Learning of Multi-Index Function Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-34" href="../nips2012/nips-2012-Active_Learning_of_Multi-Index_Function_Models.html">nips2012-34</a> <a title="nips-2012-34-reference" href="#">nips2012-34-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>34 nips-2012-Active Learning of Multi-Index Function Models</h1>
<br/><p>Source: <a title="nips-2012-34-pdf" href="http://papers.nips.cc/paper/4820-active-learning-of-multi-index-function-models.pdf">pdf</a></p><p>Author: Tyagi Hemant, Volkan Cevher</p><p>Abstract: We consider the problem of actively learning multi-index functions of the form k f (x) = g(Ax) = i=1 gi (aT x) from point evaluations of f . We assume that i the function f is deﬁned on an 2 -ball in Rd , g is twice continuously differentiable almost everywhere, and A ∈ Rk×d is a rank k matrix, where k d. We propose a randomized, active sampling scheme for estimating such functions with uniform approximation guarantees. Our theoretical developments leverage recent techniques from low rank matrix recovery, which enables us to derive an estimator of the function f along with sample complexity bounds. We also characterize the noise robustness of the scheme, and provide empirical evidence that the highdimensional scaling of our sample complexity bounds are quite accurate. 1</p><br/>
<h2>reference text</h2><p>[1] P. B¨ hlmann and S. Van De Geer. Statistics for High-Dimensional Data: Methods, Theory and u Applications. Springer-Verlag New York Inc, 2011.</p>
<p>[2] L. Carin, R.G. Baraniuk, V. Cevher, D. Dunson, M.I. Jordan, G. Sapiro, and M.B. Wakin. Learning low-dimensional signal models. Signal Processing Magazine, IEEE, 28(2):39–51, 2011.</p>
<p>[3] M. Hristache, A. Juditsky, J. Polzehl, and V. Spokoiny. Structure adaptive approach for dimension reduction. The Annals of Statistics, 29(6):1537–1566, 2001.</p>
<p>[4] K.C. Li. Sliced inverse regression for dimension reduction. Journal of the American Statistical Association, pages 316–327, 1991.</p>
<p>[5] P. Hall and K.C. Li. On almost linearity of low dimensional projections from high dimensional data. The Annals of Statistics, pages 867–889, 1993.</p>
<p>[6] Y. Xia, H. Tong, WK Li, and L.X. Zhu. An adaptive estimation of dimension reduction space. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 64(3):363–410, 2002.</p>
<p>[7] Y. Xia. A multiple-index model and dimension reduction. Journal of the American Statistical Association, 103(484):1631–1640, 2008. 8</p>
<p>[8] Y. Lin and H.H. Zhang. Component selection and smoothing in multivariate nonparametric regression. The Annals of Statistics, 34(5):2272–2297, 2006.</p>
<p>[9] L. Meier, S. Van De Geer, and P. B¨ hlmann. High-dimensional additive modeling. The Annals u of Statistics, 37(6B):3779–3821, 2009.</p>
<p>[10] G. Raskutti, M. J. Wainwright, and B. Yu. Minimax-optimal rates for sparse additive models over kernel classes via convex programming. Technical Report, UC Berkeley, Department of Statistics, August 2010.</p>
<p>[11] P. Ravikumar, J. Lafferty, H. Liu, and L. Wasserman. Sparse additive models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71(5):1009–1030, 2009.</p>
<p>[12] V. Koltchinskii and M. Yuan. Sparsity in multiple kernel learning. The Annals of Statistics, 38(6):3660–3695, 2010.</p>
<p>[13] A. Cohen, I. Daubechies, R. A. DeVore, G. Kerkyacharian, and D. Picard. Capturing ridge functions in high dimensions from point queries. Constr. Approx., pages 1–19, 2011.</p>
<p>[14] M. Fornasier, K. Schnass, and J. Vyb´ral. Learning functions of few arbitrary linear parameters ı in high dimensions. Preprint, 2010.</p>
<p>[15] H. Tyagi and V. Cevher. Learning ridge functions with randomized sampling in high dimensions. In ICASSP, 2011.</p>
<p>[16] J.F. Traub, G.W Wasilkowski, and H. Wozniakowski. Information-based complexity. Academic Press, New York, 1988.</p>
<p>[17] R. DeVore and G.G. Lorentz. Constructive approximation. vol. 303, Grundlehren, Springer Verlag, N.Y., 1993.</p>
<p>[18] E.Novak and H.Woniakowski. Approximation of inﬁnitely differentiable multivariate functions is intractable. J. Complex., 25:398–404, August 2009.</p>
<p>[19] W. Hardle. Applied nonparametric regression, volume 26. Cambridge Univ Press, 1990.</p>
<p>[20] J.H. Friedman and W. Stuetzel. Projection pursuit regression. J. Amer. Statist. Assoc., 76:817– 823, 1981.</p>
<p>[21] D.L. Donoho and I.M. Johnstone. Projection based regression and a duality with kernel methods. Ann. Statist., 17:58–106, 1989.</p>
<p>[22] P.J. Huber. Projection pursuit. Ann. Statist., 13:435–475, 1985.</p>
<p>[23] A. Pinkus. Approximation theory of the MLP model in neural networks. Acta Numerica, 8:143–195, 1999.</p>
<p>[24] E.J Cand` s. Harmonic analysis of neural networks. Appl. Comput. Harmon. Anal., 6(2):197– e 218, 1999.</p>
<p>[25] N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Information-theoretic regret bounds for gaussian process optimization in the bandit setting. To appear in the IEEE Trans. on Information Theory, 2012.</p>
<p>[26] Hemant Tyagi and Volkan Cevher. Learning non-parametric basis independent models from point queries via low-rank methods. Technical Report, Infoscience EPFL, 2012.</p>
<p>[27] E.J. Cand` s and B. Recht. Exact matrix completion via convex optimization. Foundations of e Computational Mathematics, 9(6):717–772, 2009.</p>
<p>[28] E.J. Cand` s and T. Tao. The power of convex relaxation: near-optimal matrix completion. e IEEE Trans. Inf. Theor., 56:2053–2080, May 2010.</p>
<p>[29] E.J. Cand` s and Y. Plan. Tight oracle bounds for low-rank matrix recovery from a minimal e number of random measurements. CoRR, abs/1001.0339, 2010.</p>
<p>[30] B. Recht, M. Fazel, and P.A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM REVIEW, 52:471–501, 2010.</p>
<p>[31] B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. The Annals of Statistics, 28(5):1302–1338.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
