<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>98 nips-2012-Dimensionality Dependent PAC-Bayes Margin Bound</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-98" href="../nips2012/nips-2012-Dimensionality_Dependent_PAC-Bayes_Margin_Bound.html">nips2012-98</a> <a title="nips-2012-98-reference" href="#">nips2012-98-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>98 nips-2012-Dimensionality Dependent PAC-Bayes Margin Bound</h1>
<br/><p>Source: <a title="nips-2012-98-pdf" href="http://papers.nips.cc/paper/4500-dimensionality-dependent-pac-bayes-margin-bound.pdf">pdf</a></p><p>Author: Chi Jin, Liwei Wang</p><p>Abstract: Margin is one of the most important concepts in machine learning. Previous margin bounds, both for SVM and for boosting, are dimensionality independent. A major advantage of this dimensionality independency is that it can explain the excellent performance of SVM whose feature spaces are often of high or inﬁnite dimension. In this paper we address the problem whether such dimensionality independency is intrinsic for the margin bounds. We prove a dimensionality dependent PAC-Bayes margin bound. The bound is monotone increasing with respect to the dimension when keeping all other factors ﬁxed. We show that our bound is strictly sharper than a previously well-known PAC-Bayes margin bound if the feature space is of ﬁnite dimension; and the two bounds tend to be equivalent as the dimension goes to inﬁnity. In addition, we show that the VC bound for linear classiﬁers can be recovered from our bound under mild conditions. We conduct extensive experiments on benchmark datasets and ﬁnd that the new bound is useful for model selection and is usually signiﬁcantly sharper than the dimensionality independent PAC-Bayes margin bound as well as the VC bound for linear classiﬁers.</p><br/>
<h2>reference text</h2><p>[1] John Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony. Structural risk minimization over data-dependent hierarchies. IEEE Transactions on Information Theory, 44(5):1926–1940, 1998.</p>
<p>[2] John Langford and John Shawe-Taylor. PAC-Bayes & Margins. In Advances in Neural Information Processing Systems, pages 423–430, 2002.</p>
<p>[3] David A. McAllester. Simpliﬁed PAC-Bayesian margin bounds. Learning Theory and Kernel Machines, 2777:203–215, 2003.</p>
<p>[4] John Langford. Tutorial on practical prediction theory for classiﬁcation. Journal of Machine Learning Research, 6:273–306, 2005.</p>
<p>[5] Matthias Seeger. PAC-Bayesian generalization error bounds for Gaussian process classiﬁcation. Journal of Machine Learning Research, 3:233–269, 2002.</p>
<p>[6] David A. McAllester. Some PAC-Bayesian theorems. Machine Learning, 37(3):355–363, 1999.</p>
<p>[7] Robert E. Schapire, Yoav Freund, Peter Barlett, and Wee Sun Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. Annals of Statistics, 26(5):1651–1686, 1998.</p>
<p>[8] Vladimir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and bounding the generalization error of combined classiﬁers. Annals of Statistics, 30:1–50, 2002.</p>
<p>[9] Vladimir Koltchinskii and Dmitry Panchenko. Complexities of convex combinations and bounding the generalization error in classiﬁcation. Annals of Statistics, 33:1455–1496, 2005.</p>
<p>[10] John Langford, Matthias Seeger, and Nimrod Megiddo. An improved predictive accuracy bound for averaging classiﬁers. In International Conference on Machine Learning, pages 290–297, 2001.</p>
<p>[11] Sanjoy Dasgupta and Philip M. Long. Boosting with diverse base classiﬁers. In Annual Conference on Learning Theory, pages 273–287, 2003.</p>
<p>[12] Leo Breiman. Prediction games and arcing algorithms. Neural Computation, 11:1493–1518, 1999.</p>
<p>[13] Liwei Wang, Masashi Sugiyama, Zhaoxiang Jing, Cheng Yang, Zhi-Hua Zhou, and Jufu Feng. A reﬁned margin analysis for boosting algorithms via equilibrium margin. Journal of Machine Learning Research, 12:1835–1863, 2011.</p>
<p>[14] Olivier Catoni. PAC-Bayesian supervised classiﬁcation: The thermodynamics of statistical learning. IMS Lecture Notes–Monograph Series, 56, 2007.</p>
<p>[15] Pascal Germain, Alexandre Lacasse, Francois Laviolette, and Mario Marchand. PAC-Bayesian learning ¸ of linear classiﬁers. In International Conference on Machine Learning, page 45, 2009.</p>
<p>[16] Pascal Germain, Alexandre Lacasse, Francois Laviolette, Mario Marchand, and Sara Shanian. From ¸ PAC-Bayes bounds to KL regularization. In Advances in Neural Information Processing Systems, pages 603–610, 2009.</p>
<p>[17] Jean-Francis Roy, Francois Laviolette, and Mario Marchand. From PAC-Bayes bounds to quadratic pro¸ grams for majority votes. In International Conference on Machine Learning, pages 649–656, 2011.</p>
<p>[18] Amiran Ambroladze, Emilio Parrado-Hern´ ndez, and John Shawe-Taylor. Tighter pac-bayes bounds. In a Advances in Neural Information Processing Systems, pages 9–16, 2006.</p>
<p>[19] John Shawe-Taylor, Emilio Parrado-Hern´ ndez, and Amiran Ambroladze. Data dependent priors in PACa Bayes bounds. In International Conference on Computational Statistics, pages 231–240, 2010.</p>
<p>[20] Peter L. Bartlett. The sample complexity of pattern classiﬁcation with neural networks: the size of the weights is more important than the size of the network. IEEE Transactions on Information Theory, 44(2):525–536, 1998.</p>
<p>[21] Ralf Herbrich and Thore Graepel. A PAC-Bayesian margin bound for linear classiﬁers. IEEE Transactions on Information Theory, 48(12):3140–3150, 2002.</p>
<p>[22] Alexandre Lacasse, Francois Laviolette, Mario Marchand, Pascal Germain, and Nicolas Usunier. PAC¸ Bayes bounds for the risk of the majority vote and the variance of the gibbs classiﬁer. In Advances in Neural Information Processing Systems, pages 769–776, 2006.</p>
<p>[23] Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.</p>
<p>[24] Andrew Frank and Arthur Asuncion. UCI machine learning repository, 2010.</p>
<p>[25] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27, 2011.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
