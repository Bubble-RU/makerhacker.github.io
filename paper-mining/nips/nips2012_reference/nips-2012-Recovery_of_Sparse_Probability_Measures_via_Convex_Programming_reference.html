<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>290 nips-2012-Recovery of Sparse Probability Measures via Convex Programming</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-290" href="../nips2012/nips-2012-Recovery_of_Sparse_Probability_Measures_via_Convex_Programming.html">nips2012-290</a> <a title="nips-2012-290-reference" href="#">nips2012-290-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>290 nips-2012-Recovery of Sparse Probability Measures via Convex Programming</h1>
<br/><p>Source: <a title="nips-2012-290-pdf" href="http://papers.nips.cc/paper/4504-recovery-of-sparse-probability-measures-via-convex-programming.pdf">pdf</a></p><p>Author: Mert Pilanci, Laurent E. Ghaoui, Venkat Chandrasekaran</p><p>Abstract: We consider the problem of cardinality penalized optimization of a convex function over the probability simplex with additional convex constraints. The classical 1 regularizer fails to promote sparsity on the probability simplex since 1 norm on the probability simplex is trivially constant. We propose a direct relaxation of the minimum cardinality problem and show that it can be efﬁciently solved using convex programming. As a ﬁrst application we consider recovering a sparse probability measure given moment constraints, in which our formulation becomes linear programming, hence can be solved very efﬁciently. A sufﬁcient condition for exact recovery of the minimum cardinality solution is derived for arbitrary afﬁne constraints. We then develop a penalized version for the noisy setting which can be solved using second order cone programs. The proposed method outperforms known rescaling heuristics based on 1 norm. As a second application we consider convex clustering using a sparse Gaussian mixture and compare our results with the well known soft k-means algorithm. 1</p><br/>
<h2>reference text</h2><p>[1] E.J. Cand´ s, T. Tao, ”Decoding by linear programming”. IEEE Trans. Inform. Theory 51 e (2005), 4203-4215.</p>
<p>[2] S. Chen, D. Donoho, and M. Saunders. Atomic decomposition by basis pursuit” SIAM Review, 43(1):129-159, 2001.</p>
<p>[3] A. Bruckstein, D. Donoho, and M. Elad. ”From sparse solutions of systems of equations to sparse modeling of signals and images”. SIAM Review, 2007.</p>
<p>[4] V. Chandrasekaran, B. Recht, P.A. Parrilo, and A.S. Willsky. ”The convex algebraic geometry of linear inverse problems”. In Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on, pages 699-703, 2010.</p>
<p>[5] S. Boyd and L. Vandenberghe, ”Convex Optimization”. Cambridge, U.K.: Cambridge Univ. Press, 2003.</p>
<p>[6] A. Cohen and A. Yeredor, ”On the use of sparsity for recovering discrete probability distributions from their moments”. Statistical Signal Processing Workshop (SSP), 2011 IEEE</p>
<p>[7] J. Kivinen and M. Warmuth. ”Exponentiated gradient versus gradient descent for linear predictors”. Information and Computation, 132(1):1-63, 1997.</p>
<p>[8] D. Lashkari and P. Golland, ”Convex clustering with exemplar-based models”, in NIPS, 2008.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
