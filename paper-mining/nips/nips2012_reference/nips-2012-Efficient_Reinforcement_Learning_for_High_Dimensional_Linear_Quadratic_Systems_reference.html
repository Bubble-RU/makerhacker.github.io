<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>110 nips-2012-Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-110" href="../nips2012/nips-2012-Efficient_Reinforcement_Learning_for_High_Dimensional_Linear_Quadratic_Systems.html">nips2012-110</a> <a title="nips-2012-110-reference" href="#">nips2012-110-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>110 nips-2012-Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems</h1>
<br/><p>Source: <a title="nips-2012-110-pdf" href="http://papers.nips.cc/paper/4679-efficient-reinforcement-learning-for-high-dimensional-linear-quadratic-systems.pdf">pdf</a></p><p>Author: Morteza Ibrahimi, Adel Javanmard, Benjamin V. Roy</p><p>Abstract: We study the problem of adaptive control of a high dimensional linear quadratic (LQ) system. Previous work established the asymptotic convergence to an optimal controller for various adaptive control schemes. More recently, for the average √ cost LQ problem, a regret bound of O( T ) was shown, apart form logarithmic factors. However, this bound scales exponentially with p, the dimension of the state space. In this work we consider the case where the matrices describing the dynamic of the LQ system are sparse and their dimensions are large. We present √ an adaptive control scheme that achieves a regret bound of O(p T ), apart from logarithmic factors. In particular, our algorithm has an average cost of (1 + ) times the optimum cost after T = polylog(p)O(1/ 2 ). This is in comparison to previous work on the dense dynamics where the algorithm requires time that scales exponentially with dimension in order to achieve regret of times the optimal cost. We believe that our result has prominent applications in the emerging area of computational advertising, in particular targeted online advertising and advertising in social networks. 1</p><br/>
<h2>reference text</h2><p>[1] Y. Abbasi-Yadkori and C. Szepesv´ ri. Regret bounds for the adaptive control of linear quadratic a systems. Proceeding of the 24th Annual Conference on Learning Theory, pages 1–26, 2011.</p>
<p>[2] Y. Bar-Shalom and E. Tse. Dual effect, certainty equivalence, and separation in stochastic control. Automatic Control, IEEE Transactions on, 19(5):494–500, 1974.</p>
<p>[3] J. Bento, M. Ibrahimi, and A. Montanari. Learning networks of stochastic differential equations. Advances in Neural Information Processing Systems 23, pages 172–180, 2010.</p>
<p>[4] D. Bertsekas. Dynamic Programming: Deterministic and Stochastic Models. Prentice-Hall, 1987.</p>
<p>[5] D. P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc, 3rd edition, 2007.</p>
<p>[6] S. Bittanti and M. Campi. Adaptive control of linear time invariant systems: the bet on the best principle. Communications in Information and Systems, 6(4):299–320, 2006.</p>
<p>[7] E. Bradlow, B. Bronnenberg, G. Russell, N. Arora, D. Bell, S. Duvvuri, F. Hofstede, C. Sismeiro, R. Thomadsen, and S. Yang. Spatial models in marketing. Marketing Letters, 16(3):267–278, 2005.</p>
<p>[8] M. Campi. Achieving optimality in adaptive control: the bet on the best approach. In Decision and Control, 1997., Proceedings of the 36th IEEE Conference on, volume 5, pages 4671–4676. IEEE, 1997.</p>
<p>[9] V. Dani, T. Hayes, and S. Kakade. Stochastic linear optimization under bandit feedback. In Proceedings of the 21st Annual Conference on Learning Theory (COLT), 2008.</p>
<p>[10] G. Feichtinger, R. Hartl, and S. Sethi. Dynamic optimal control models in advertising: recent developments. Management Science, pages 195–226, 1994. ˚</p>
<p>[11] L. Guo and H. Chen. The astrom-wittenmark self-tuning regulator revisited and els-based adaptive trackers. Automatic Control, IEEE Transactions on, 36(7):802–812, 1991.</p>
<p>[12] P. Kumar and A. Becker. A new family of optimal adaptive controllers for markov chains. Automatic Control, IEEE Transactions on, 27(1):137–146, 1982.</p>
<p>[13] T. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in applied mathematics, 6(1):4–22, 1985.</p>
<p>[14] T. Lai and C. Wei. Least squares estimates in stochastic regression models with applications to identiﬁcation and control of dynamic systems. The Annals of Statistics, 10(1):154–166, 1982.</p>
<p>[15] C. Marinelli and S. Savin. Optimal distributed dynamic advertising. Journal of Optimization Theory and Applications, 137(3):569–591, 2008.</p>
<p>[16] T. Seidman, S. Sethi, and N. Derzko. Dynamics and optimization of a distributed salesadvertising model. Journal of Optimization Theory and Applications, 52(3):443–462, 1987.</p>
<p>[17] S. Sethi. Dynamic optimal control models in advertising: a survey. SIAM review, pages 685– 725, 1977.</p>
<p>[18] J. Tropp. Just relax: Convex programming methods for identifying sparse signals in noise. Information Theory, IEEE Transactions on, 52(3):1030–1051, 2006.</p>
<p>[19] M. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery usingconstrained quadratic programming (lasso). Information Theory, IEEE Transactions on, 55(5):2183–2202, 2009.</p>
<p>[20] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning Research, 7:2541–2563, 2006.  9</p>
<br/>
<br/><br/><br/></body>
</html>
