<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>171 nips-2012-Latent Coincidence Analysis: A Hidden Variable Model for Distance Metric Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-171" href="../nips2012/nips-2012-Latent_Coincidence_Analysis%3A_A_Hidden_Variable_Model_for_Distance_Metric_Learning.html">nips2012-171</a> <a title="nips-2012-171-reference" href="#">nips2012-171-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>171 nips-2012-Latent Coincidence Analysis: A Hidden Variable Model for Distance Metric Learning</h1>
<br/><p>Source: <a title="nips-2012-171-pdf" href="http://papers.nips.cc/paper/4634-latent-coincidence-analysis-a-hidden-variable-model-for-distance-metric-learning.pdf">pdf</a></p><p>Author: Matthew Der, Lawrence K. Saul</p><p>Abstract: We describe a latent variable model for supervised dimensionality reduction and distance metric learning. The model discovers linear projections of high dimensional data that shrink the distance between similarly labeled inputs and expand the distance between diﬀerently labeled ones. The model’s continuous latent variables locate pairs of examples in a latent space of lower dimensionality. The model diﬀers signiﬁcantly from classical factor analysis in that the posterior distribution over these latent variables is not always multivariate Gaussian. Nevertheless we show that inference is completely tractable and derive an Expectation-Maximization (EM) algorithm for parameter estimation. We also compare the model to other approaches in distance metric learning. The model’s main advantage is its simplicity: at each iteration of the EM algorithm, the distance metric is re-estimated by solving an unconstrained least-squares problem. Experiments show that these simple updates are highly eﬀective. 1</p><br/>
<h2>reference text</h2><p>[1] D. B. Rubin and D. T. Thayer. EM algorithms for ML factor analysis. Psychometrika, 47:69– 76, 1982.</p>
<p>[2] G. McLachlan and K. Basford. Mixture Models: Inference and Applications to Clustering. Marcel Dekker, 1988.</p>
<p>[3] T. Cover and P. Hart. Nearest neighbor pattern classiﬁcation. In IEEE Transactions in Information Theory, IT-13, pages 21–27, 1967.</p>
<p>[4] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 513–520, Cambridge, MA, 2005. MIT Press.</p>
<p>[5] K.Q. Weinberger and L.K. Saul. Distance metric learning for large margin nearest neighbor classiﬁcation. The Journal of Machine Learning Research, 10:207–244, 2009.</p>
<p>[6] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon. Information-theoretic metric learning. In ICML, pages 209–216, Corvalis, Oregon, USA, June 2007.</p>
<p>[7] G. Hinton and S. Roweis. Stochastic neighbor embedding. In S. Thrun S. Becker and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 833–840. MIT Press, Cambridge, MA, 2003.</p>
<p>[8] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20:273–297, 1995.</p>
<p>[9] B. Kulis, M. A. Sustik, and I. S. Dhillon. Learning low-rank kernel matrices. In Proceedings of the Twenty-Third International Conference on Machine Learning (ICML-06), 2006.</p>
<p>[10] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society B, 39:1–37, 1977.</p>
<p>[11] http://yann.lecun.com/exdb/mnist/.</p>
<p>[12] http://mlg.ucd.ie/datasets/bbc.html.</p>
<p>[13] http://www.dataminingresearch.com/index.php/2010/09/classic3-classic4-datasets/.</p>
<p>[14] http://archive.ics.uci.edu/ml/datasets.html.</p>
<p>[15] R A Fisher. The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2):179–188, 1936.</p>
<p>[16] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.</p>
<p>[17] R. Salakhutdinov, S. T. Roweis, and Z. Ghahramani. On the convergence of bound optimization algorithms. In Proceedings of the Nineteenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI-03), pages 509–516, 2003.</p>
<p>[18] S. Shalev-Shwartz, Y. Singer, and A. Y. Ng. Online and batch learning of pseudo-metrics. In Proceedings of the Twenty First International Conference on Machine Learning (ICML-04), pages 94–101, Banﬀ, Canada, 2004.</p>
<p>[19] T. Jaakkola and M. Jordan. A variational approach to bayesian logistic regression models and their extensions. In Proceedings of the Sixth International Workshop on Artiﬁcial Intelligence and Statistics, 1997.</p>
<p>[20] M. Dredze, K. Crammer, and F. Pereira. Conﬁdence-weighted linear classiﬁcation. In Andrew McCallum and Sam Roweis, editors, Proceedings of the 25th Annual International Conference on Machine Learning (ICML 2008), pages 264–271. Omnipress, 2008.</p>
<p>[21] G. Chechik, U. Shalit, V. Sharma, and S. Bengio. An online algorithm for large scale image similarity learning. In Y. Bengio, D. Schuurmans, J. Laﬀerty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 306–314. 2009.  9</p>
<br/>
<br/><br/><br/></body>
</html>
