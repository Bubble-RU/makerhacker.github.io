<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>231 nips-2012-Multiple Operator-valued Kernel Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-231" href="../nips2012/nips-2012-Multiple_Operator-valued_Kernel_Learning.html">nips2012-231</a> <a title="nips-2012-231-reference" href="#">nips2012-231-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>231 nips-2012-Multiple Operator-valued Kernel Learning</h1>
<br/><p>Source: <a title="nips-2012-231-pdf" href="http://papers.nips.cc/paper/4653-multiple-operator-valued-kernel-learning.pdf">pdf</a></p><p>Author: Hachem Kadri, Alain Rakotomamonjy, Philippe Preux, Francis R. Bach</p><p>Abstract: Positive deﬁnite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a ﬁnite linear combination of inﬁnite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an r -norm constraint on the combination coefﬁcients (r ≥ 1). The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure. We experimentally validate our approach on a functional regression task in the context of ﬁnger movement prediction in brain-computer interfaces. 1</p><br/>
<h2>reference text</h2><p>[1] J. Aﬂalo, A. Ben-Tal, C. Bhattacharyya, J. Saketha Nath, and S. Raman. Variable sparsity kernel learning. JMLR, 12:565–592, 2011.</p>
<p>[2] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning, 73(3):243–272, 2008.</p>
<p>[3] F. Bach. Consistency of the group Lasso and multiple kernel learning. JMLR, 9:1179–1225, 2008.</p>
<p>[4] C. Brouard, F. d’Alch´ -Buc, and M. Szafranski. Semi-supervised penalized output kernel regression for e link prediction. In Proc. ICML, 2011.</p>
<p>[5] A. Caponnetto, C. A. Micchelli, M. Pontil, and Y. Ying. Universal multi-task kernels. JMLR, 68:1615– 1646, 2008.</p>
<p>[6] C. Carmeli, E. De Vito, and A. Toigo. Vector valued reproducing kernel Hilbert spaces of integrable functions and mercer theorem. Analysis and Applications, 4:377–408, 2006.</p>
<p>[7] C. Carmeli, E. De Vito, and A. Toigo. Vector valued reproducing kernel Hilbert spaces and universality. Analysis and Applications, 8:19–61, 2010.</p>
<p>[8] C. Cortes, M. Mohri, and A. Rostamizadeh. L2 regularization for learning kernels. In Proc. UAI, 2009.</p>
<p>[9] C. Cortes, M. Mohri, and A. Rostamizadeh. Generalization bounds for learning kernels. In ICML, 2010.</p>
<p>[10] F. Dinuzzo, C. S. Ong, P. Gehler, and G. Pillonetto. Learning output kernels with block coordinate descent. In Proc. ICML, 2011.</p>
<p>[11] T. Evgeniou, C. A. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods. JMLR, 6:615–637, 2005.</p>
<p>[12] H. Kadri, E. Duﬂos, P. Preux, S. Canu, and M. Davy. Nonlinear functional regression: a functional RKHS approach. In Proc. AISTATS, pages 111–125, 2010.</p>
<p>[13] H. Kadri, A. Rabaoui, P. Preux, E. Duﬂos, and A. Rakotomamonjy. Functional regularized least squares classiﬁcation with operator-valued kernels. In Proc. ICML, 2011.</p>
<p>[14] H. Kadri, A. Rakotomamonjy, F. Bach, and P. Preux. Multiple operator-valued kernel learning. Technical Report 00677012, INRIA, 2012.</p>
<p>[15] M. Kloft, U. Brefeld, S. Sonnenburg, and A. Zien. 2011.  p -norm  multiple kernel learning. JMLR, 12:953–997,</p>
<p>[16] S. Kurcyusz. On the existence and nonexistence of lagrange multipliers in Banach spaces. Journal of Optimization Theory and Applications, 20:81–110, 1976.</p>
<p>[17] A. Kurdila and M. Zabarankin. Convex Functional Analysis. Birkhauser Verlag, 2005.</p>
<p>[18] G. Lanckriet, N. Cristianini, L. El Ghaoui, P. Bartlett, and M. Jordan. Learning the kernel matrix with semi-deﬁnite programming. JMLR, 5:27–72, 2004.</p>
<p>[19] H. Lian. Nonlinear functional models for functional responses in reproducing kernel Hilbert spaces. The Canadian Journal of Statistics, 35:597–606, 2007.</p>
<p>[20] C. Micchelli and M. Pontil. Learning the kernel function via regularization. JMLR, 6:1099–1125, 2005.</p>
<p>[21] C. A. Micchelli and M. Pontil. On learning vector-valued functions. Neural Comput., 17:177–204, 2005.</p>
<p>[22] K. J. Miller and G. Schalk. Prediction of ﬁnger ﬂexion: 4th brain-computer interface data competition. BCI Competition IV, 2008.</p>
<p>[23] T. Pistohl, T. Ball, A. Schulze-Bonhage, A. Aertsen, and C. Mehring. Prediction of arm movement trajectories from ECoG-recordings in humans. Journal of Neuroscience Methods, 167(1):105–114, 2008.</p>
<p>[24] A. Rakotomamonjy, F. Bach, Y. Grandvalet, and S. Canu. SimpleMKL. JMLR, 9:2491–2521, 2008.</p>
<p>[25] J. O. Ramsay and B. W. Silverman. Functional Data Analysis, 2nd ed. Springer Verlag, New York, 2005.</p>
<p>[26] John A. Rice and B. W. Silverman. Estimating the mean and covariance structure nonparametrically when the data are curves. Journal of the Royal Statistical Society. Series B, 53(1):233–243, 1991.</p>
<p>[27] G. Schalk, D. J. McFarland, T. Hinterberger, N. Birbaumer, and J. R. Wolpaw. BCI2000: a generalpurpose brain-computer interface system. Biomedical Engineering, IEEE Trans. on, 51:1034–1043, 2004.</p>
<p>[28] B. Sch¨ lkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regularization, Optio mization, and Beyond. MIT Press, Cambridge, MA, USA, 2002.</p>
<p>[29] S. Sonnenburg, G. R¨ tsch, C. Sch¨ fer, and B. Sch¨ lkopf. Large scale multiple kernel learning. JMLR, a a o 7:1531–1565, 2006.</p>
<p>[30] P. Tseng. Convergence of block coordinate descent method for nondifferentiable minimization. J. Optim. Theory Appl., 109:475–494, 2001.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
