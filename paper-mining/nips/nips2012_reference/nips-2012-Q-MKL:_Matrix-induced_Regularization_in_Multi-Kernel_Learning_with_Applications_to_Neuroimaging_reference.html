<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-284" href="../nips2012/nips-2012-Q-MKL%3A_Matrix-induced_Regularization_in_Multi-Kernel_Learning_with_Applications_to_Neuroimaging.html">nips2012-284</a> <a title="nips-2012-284-reference" href="#">nips2012-284-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</h1>
<br/><p>Source: <a title="nips-2012-284-pdf" href="http://papers.nips.cc/paper/4710-q-mkl-matrix-induced-regularization-in-multi-kernel-learning-with-applications-to-neuroimaging.pdf">pdf</a></p><p>Author: Chris Hinrichs, Vikas Singh, Jiming Peng, Sterling Johnson</p><p>Abstract: Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one simultaneously trains a linear classiﬁer and chooses an optimal combination of given base kernels. Model complexity is typically controlled using various norm regularizations on the base kernel mixing coefﬁcients. Existing methods neither regularize nor exploit potentially useful information pertaining to how kernels in the input set ‘interact’; that is, higher order kernel-pair relationships that can be easily obtained via unsupervised (similarity, geodesics), supervised (correlation in errors), or domain knowledge driven mechanisms (which features were used to construct the kernel?). We show that by substituting the norm penalty with an arbitrary quadratic function Q 0, one can impose a desired covariance structure on mixing weights, and use this as an inductive bias when learning the concept. This formulation signiﬁcantly generalizes the widely used 1- and 2-norm MKL objectives. We explore the model’s utility via experiments on a challenging Neuroimaging problem, where the goal is to predict a subject’s conversion to Alzheimer’s Disease (AD) by exploiting aggregate information from many distinct imaging modalities. Here, our new model outperforms the state of the art (p-values 10−3 ). We brieﬂy discuss ramiﬁcations in terms of learning bounds (Rademacher complexity). 1</p><br/>
<h2>reference text</h2><p>[1] I. Guyon and A. Elisseeff. An introduction to variable and feature selection. JMLR, 3:1157–1182, 2003.</p>
<p>[2] P. V. Gehler and S. Nowozin. Let the kernel ﬁgure it out; principled learning of pre-processing for kernel classiﬁers. CVPR, 2009.</p>
<p>[3] C. Hinrichs, V. Singh, G. Xu, and S.C. Johnson. Predictive markers for AD in a multi-modality framework: An analysis of MCI progression in the ADNI population. Neuroimage, 55(2):574–589, 2011.</p>
<p>[4] D. Zhang, Y. Wang, L. Zhou, H. Yuan, and D. Shen. Multimodal Classiﬁcation of Alzheimer’s Disease and Mild Cognitive Impairment. NeuroImage, 55(3):856–867, 2011.</p>
<p>[5] G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. Jordan. Learning the kernel matrix with semideﬁnite programming. JMLR, 5:27–72, 2004.</p>
<p>[6] S. Sonnenburg, G. R¨ tsch, C. Sch¨ fer, and B. Sch¨ lkopf. Large scale multiple kernel learning. JMLR, a a o 7:1531–1565, 2006.</p>
<p>[7] A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet. SimpleMKL. JMLR, 9:2491–2521, 2008.</p>
<p>[8] P. V. Gehler and S. Nowozin. On feature combination for multiclass object classiﬁcation. In ICCV, 2009.</p>
<p>[9] J. Yang, Y. Li, Y. Tian, L. Duan, and W. Gao. Group-sensitive multiple kernel learning for object categorization. In ICCV, 2009.</p>
<p>[10] P. Vemuri, J.L. Gunter, M. L. Senjem, J. L. Whitwell, K. Kantarci, D. S. Knopman, et al. Alzheimer’s disease diagnosis in individual subjects using structural MR images: validation studies. Neuroimage, 39(3):1186–1197, 2008.</p>
<p>[11] M. Szafranski, Y. Grandvalet, and A. Rakotomamonjy. Composite kernel learning. Machine learning, 79(1):73–103, 2010.</p>
<p>[12] F. R. Bach, G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the SMO algorithm. In ICML, 2004.</p>
<p>[13] F. Orabona, L. Jie, and B. Caputo. Online-Batch Strongly Convex Multi Kernel Learning. In CVPR, 2010.</p>
<p>[14] M. Kloft, U. Brefeld, S. Sonnenburg, and A. Zien. p -Norm Multiple Kernel Learning. JMLR, 12:953– 997, 2011.</p>
<p>[15] C.S. Ong, A. Smola, and B. Williamson. Learning the kernel with hyperkernels. JMLR, 6:1045–1071, 2005.</p>
<p>[16] L. Mukherjee, V. Singh, J. Peng, and C. Hinrichs. Learning Kernels for variants of Normalized Cuts: Convex Relaxations and Applications. CVPR, 2010.</p>
<p>[17] P. V. Gehler and S. Nowozin. Inﬁnite kernel learning. Technical Report 178, Max-Planck Institute for Biological Cybernetics, 10 2008.</p>
<p>[18] F. R. Bach. Exploring large feature spaces with hierarchical multiple kernel learning. In NIPS, 2008.</p>
<p>[19] T. Joachims, N. Cristianini, and J. Shawe-Taylor. Composite kernels for hypertext categorisation. In ICML, 2001.</p>
<p>[20] A. Renyi. On measures of entropy and information. In Fourth Berkeley Symposium on Mathematical Statistics and Probability, pages 547–561, 1961.</p>
<p>[21] C. Cortes, M. Mohri, and A. Rostamizadeh. Generalization bounds for learning kernels. In ICML, 2010.</p>
<p>[22] R. Jenssen. Kernel entropy component analysis. IEEE Trans. PAMI, pages 847–860, 2009.</p>
<p>[23] M. Girolami. Orthogonal series density estimation and the kernel eigenvalue problem. Neural Computation, 14(3):669–688, 2002.</p>
<p>[24] D. Erdogmus and J.C. Principe. Generalized information potential criterion for adaptive system training. IEEE Trans. Neural Networks, 13(5):1035–1044, 2002.</p>
<p>[25] M. Kowalski, M. Szafranski, and L. Ralaivola. Multiple indeﬁnite kernel learning with mixed norm regularization. In ICML, 2009.</p>
<p>[26] S. Bergsma, D. Lin, and D. Schuurmans. Improved Natural Language Learning via VarianceRegularization Support Vector Machines. In CoNLL, 2010.</p>
<p>[27] R. Cuingnet, M. Chupin, H. Benali, and O. Colliot. Spatial and anatomical regularization of SVM for brain image analysis. In NIPS, 2010.</p>
<p>[28] P. Shivaswamy and T. Jebara. Maximum relative margin and data-dependent regularization. JMLR, 11:747–788, 2010.</p>
<p>[29] K. Gai, G. Chen, and C. Zhang. Learning kernels with radiuses of minimum enclosing balls. In NIPS, 2010.</p>
<p>[30] S. G. Mueller, M. W. Weiner, et al. Ways toward an early diagnosis in Alzheimers disease: The Alzheimer’s Disease Neuroimaging Initiative. J. of the Alzheimer’s Association, 1(1):55–66, 2005.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
