<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>265 nips-2012-Parametric Local Metric Learning for Nearest Neighbor Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-265" href="../nips2012/nips-2012-Parametric_Local_Metric_Learning_for_Nearest_Neighbor_Classification.html">nips2012-265</a> <a title="nips-2012-265-reference" href="#">nips2012-265-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>265 nips-2012-Parametric Local Metric Learning for Nearest Neighbor Classification</h1>
<br/><p>Source: <a title="nips-2012-265-pdf" href="http://papers.nips.cc/paper/4818-parametric-local-metric-learning-for-nearest-neighbor-classification.pdf">pdf</a></p><p>Author: Jun Wang, Alexandros Kalousis, Adam Woznica</p><p>Abstract: We study the problem of learning local metrics for nearest neighbor classiﬁcation. Most previous works on local metric learning learn a number of local unrelated metrics. While this ”independence” approach delivers an increased ﬂexibility its downside is the considerable risk of overﬁtting. We present a new parametric local metric learning method in which we learn a smooth metric matrix function over the data manifold. Using an approximation error bound of the metric matrix function we learn local metrics as linear combinations of basis metrics deﬁned on anchor points over different regions of the instance space. We constrain the metric matrix function by imposing on the linear combinations manifold regularization which makes the learned metric matrix function vary smoothly along the geodesics of the data manifold. Our metric learning method has excellent performance both in terms of predictive power and scalability. We experimented with several largescale classiﬁcation problems, tens of thousands of instances, and compared it with several state of the art metric learning methods, both global and local, as well as to SVM with automatic kernel selection, all of which it outperforms in a signiﬁcant manner. 1</p><br/>
<h2>reference text</h2><p>[1] F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Convex optimization with sparsity-inducing norms. Optimization for Machine Learning.</p>
<p>[2] A. Beck and M. Teboulle. Gradient-based algorithms with applications to signal-recovery problems. Convex Optimization in Signal Processing and Communications, pages 42–88, 2010.</p>
<p>[3] M. Bilenko, S. Basu, and R.J. Mooney. Integrating constraints and metric learning in semisupervised clustering. In ICML, page 11, 2004.</p>
<p>[4] J.V. Davis, B. Kulis, P. Jain, S. Sra, and I.S. Dhillon. Information-theoretic metric learning. In ICML, 2007.</p>
<p>[5] H. Do, A. Kalousis, J. Wang, and A. Woznica. A metric learning perspective of svm: on the relation of svm and lmnn. AISTATS, 2012.</p>
<p>[6] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efﬁcient projections onto the l 1-ball for learning in high dimensions. In ICML, 2008.</p>
<p>[7] A. Frome, Y. Singer, and J. Malik. Image retrieval and classiﬁcation using local distance functions. In Advances in Neural Information Processing Systems, volume 19, pages 417–424. MIT Press, 2007.</p>
<p>[8] T. Hastie and R. Tibshirani. Discriminant adaptive nearest neighbor classiﬁcation. IEEE Trans. on PAMI, 1996.</p>
<p>[9] P. Jain, B. Kulis, J.V. Davis, and I.S. Dhillon. Metric and kernel learning using a linear transformation. JMLR, 2012.</p>
<p>[10] R. Jin, S. Wang, and Y. Zhou. Regularized distance metric learning: Theory and algorithm. In NIPS, 2009.</p>
<p>[11] Y.K. Noh, B.T. Zhang, and D.D. Lee. Generative local metric learning for nearest neighbor classiﬁcation. NIPS, 2009.</p>
<p>[12] C. Shen, J. Kim, and L. Wang. A scalable dual approach to semideﬁnite metric learning. In CVPR, 2011.</p>
<p>[13] C. Shen, J. Kim, L. Wang, and A. Hengel. Positive semideﬁnite metric learning using boostinglike algorithms. JMLR, 2012.</p>
<p>[14] J. Wang, H. Do, A. Woznica, and A. Kalousis. Metric learning with multiple kernels. In NIPS, 2011.</p>
<p>[15] K.Q. Weinberger and L.K. Saul. Distance metric learning for large margin nearest neighbor classiﬁcation. JMLR, 2009.</p>
<p>[16] D.Y. Yeung and H. Chang. Locally smooth metric learning with application to image retrieval. In ICCV, 2007.</p>
<p>[17] Y. Ying, K. Huang, and C. Campbell. Sparse metric learning via smooth optimization. NIPS, 2009.</p>
<p>[18] K. Yu, T. Zhang, and Y. Gong. Nonlinear learning using local coordinate coding. NIPS, 2009.</p>
<p>[19] L. Zelnik-Manor and P. Perona. Self-tuning spectral clustering. NIPS, 2004.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
