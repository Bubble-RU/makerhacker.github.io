<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>233 nips-2012-Multiresolution Gaussian Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-233" href="../nips2012/nips-2012-Multiresolution_Gaussian_Processes.html">nips2012-233</a> <a title="nips-2012-233-reference" href="#">nips2012-233-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>233 nips-2012-Multiresolution Gaussian Processes</h1>
<br/><p>Source: <a title="nips-2012-233-pdf" href="http://papers.nips.cc/paper/4682-multiresolution-gaussian-processes.pdf">pdf</a></p><p>Author: David B. Dunson, Emily B. Fox</p><p>Abstract: We propose a multiresolution Gaussian process to capture long-range, nonMarkovian dependencies while allowing for abrupt changes and non-stationarity. The multiresolution GP hierarchically couples a collection of smooth GPs, each deﬁned over an element of a random nested partition. Long-range dependencies are captured by the top-level GP while the partition points deﬁne the abrupt changes. Due to the inherent conjugacy of the GPs, one can analytically marginalize the GPs and compute the marginal likelihood of the observations given the partition tree. This property allows for efﬁcient inference of the partition itself, for which we employ graph-theoretic techniques. We apply the multiresolution GP to the analysis of magnetoencephalography (MEG) recordings of brain activity.</p><br/>
<h2>reference text</h2><p>[1] F. Bach. High-dimensional non-linear variable selection through hierarchical kernel learning. Technical Report 0909.0844v1, arXiv, 2009.</p>
<p>[2] J. Beran and Y. Shumeyko. On asymptotically optimal wavelet estimation of trend functions under longrange dependence. Bernoulli, 18(1):137–176, 2012.</p>
<p>[3] C. Blundell, Y. W. Teh, and K. A. Heller. Bayesian rose trees. In Proc. Uncertainty in Artiﬁcial Intelligence, pages 65–72, 2010.</p>
<p>[4] P. Del Moral, A. Doucet, and A. Jasra. Sequential Monte Carlo samplers. Journal of the Royal Statistical Society, Series B, 68(3):411–436, 2006.</p>
<p>[5] F. X. Diebold and G. D. Rudebusch. Long memory and persistence in aggregate output. Journal of Monetary Economics, 24:189–209, 1989.</p>
<p>[6] D. Duvenaud, H. Nickisch, and C. E. Rasmussen. Additive Gaussian processes. In Advances in Neural Information Processing Systems, volume 24, pages 226–234, 2011.</p>
<p>[7] A. Y. Fyshe, E. B. Fox, D. B. Dunson, and T. Mitchell. Hierarchical latent dictionaries for models of brain activation. In Proc. International Conference on Artiﬁcial Intelligence and Statistics, pages 409– 421, 2012.</p>
<p>[8] R. .B. Gramacy and H. K. H. Lee. Bayesian treed Gaussian process models with an application to computer modeling. Journal of the American Statistical Association, 103(483):1119–1130, 2008.</p>
<p>[9] P. Hansen, M. Kringelbach, and R. Salmelin. MEG: An Introduction to Methods. Oxford University Press, USA, 2010. ISBN 0195307232.</p>
<p>[10] R. Henao and J. E. Lucas. Efﬁcient hierarchical clustering for continuous data. Technical Report 1204.4708v1, arXiv, 2012.</p>
<p>[11] N. S. Jones and J. Moriarty. Evolutionary inference for function-valued traits: Gaussian process regression on phylogenies. Technical Report 1004.4668v2, arXiv, 2011.</p>
<p>[12] H. M. Kim, B. K. Mallick, and C. C. Holmes. Analyzing nonstationary spatial data using piecewise Gaussian processes. Journal of the American Statistical Association, 100(470):653–668, 2005.</p>
<p>[13] P. S. Kokoszka and M. S. Taqqu. Parameter estimation for inﬁnite variance fractional ARIMA. The Annals of Statistics, 24(5):1880–1913, 1996.</p>
<p>[14] E. Meeds and S. Osindero. An alternative mixture of Gaussian process experts. In Advances in Neural Information Processing Systems, volume 18, pages 883–890, 2006.</p>
<p>[15] J. S. Morris and R. J. Carroll. Wavelet-based functional mixed models. Journal of the Royal Statistical Society, Series B, 68(2):179–199, 2006.</p>
<p>[16] S. Park and S. Choi. Hierarchical Gaussian process regression. In Proc. Asian Conference on Machine Learning, pages 95–110, 2010.</p>
<p>[17] C. E. Rasmussen and Z. Ghahramani. Inﬁnite mixtures of Gaussian process experts. In Advances in Neural Information Processing Systems, volume 2, pages 881–888, 2002.</p>
<p>[18] C. E. Rasmussen and C. K. .I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.</p>
<p>[19] C. P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer, 2005.</p>
<p>[20] D. M. Roy and Y. W. Teh. The Mondrian process. In Advances in Neural Information Processing Systems, volume 21, pages 1377–1384, 2009.</p>
<p>[21] Y. Saatci, R. Turner, and C. E. Rasmussen. Gausssian process change point models. In Proc. International Conference on Machine Learning, pages 927–934, 2010.</p>
<p>[22] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888–905, 2000.</p>
<p>[23] G. Sudre, D. Pomerleaum, M. Palatucci, L. Wehbe, A. Fyshe, R. Salmelin, and T. Mitchell. Tracking neural coding of perceptual and semantic features of concrete nouns. Neuroimage, 62(1):451–463, 2012.</p>
<p>[24] U. von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395–416, 2007.</p>
<p>[25] A. S. Willsky. Multiresolution Markov models for signal and image processing. Proceedings of the IEEE, 90(8):1396–1458, 2002.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
