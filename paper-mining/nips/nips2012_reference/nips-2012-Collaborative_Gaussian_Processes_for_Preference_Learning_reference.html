<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>74 nips-2012-Collaborative Gaussian Processes for Preference Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-74" href="../nips2012/nips-2012-Collaborative_Gaussian_Processes_for_Preference_Learning.html">nips2012-74</a> <a title="nips-2012-74-reference" href="#">nips2012-74-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>74 nips-2012-Collaborative Gaussian Processes for Preference Learning</h1>
<br/><p>Source: <a title="nips-2012-74-pdf" href="http://papers.nips.cc/paper/4700-collaborative-gaussian-processes-for-preference-learning.pdf">pdf</a></p><p>Author: Neil Houlsby, Ferenc Huszar, Zoubin Ghahramani, Jose M. Hernández-lobato</p><p>Abstract: We present a new model based on Gaussian processes (GPs) for learning pairwise preferences expressed by multiple users. Inference is simpliﬁed by using a preference kernel for GPs which allows us to combine supervised GP learning of user preferences with unsupervised dimensionality reduction for multi-user systems. The model not only exploits collaborative information from the shared structure in user behavior, but may also incorporate user features if they are available. Approximate inference is implemented using a combination of expectation propagation and variational Bayes. Finally, we present an efﬁcient active learning strategy for querying preferences. The proposed technique performs favorably on real-world data against state-of-the-art multi-user preference learning algorithms. 1</p><br/>
<h2>reference text</h2><p>[1] A. Birlutiu, P. Groot, and T. Heskes. Multi-task preference learning with an application to hearing aid personalization. Neurocomputing, 73(79):1177 – 1185, 2010.</p>
<p>[2] Edwin V. Bonilla, Shengbo Guo, and Scott Sanner. Gaussian process preference elicitation. In Advances in Neural Information Processing Systems 23, pages 262–270, 2010.</p>
<p>[3] E. Brochu, N. de Freitas, and A. Ghosh. Active preference learning with discrete choice data. Advances in Neural Information Processing Systems 20, 20:409–416, 2007.</p>
<p>[4] W. Chu and Z. Ghahramani. Preference learning with Gaussian processes. In Proceedings of the 22nd international conference on Machine learning, pages 137–144, 2005.</p>
<p>[5] M. De Gemmis, L. Iaquinta, P. Lops, C. Musto, F. Narducci, and G. Semeraro. Preference learning in recommender systems. In ECML/PKDD-09 Workshop on Preference Learning, 2009.</p>
<p>[6] J. F¨ rnkranz and E. H¨ llermeier. Preference learning. Springer-Verlag New York Inc, 2010. u u</p>
<p>[7] Z. Ghahramani and M. J. Beal. Advanced Mean Field Method—Theory and Practice, chapter Graphical models and variational methods, pages 161–177. 2001.</p>
<p>[8] B. Krishnapuram, D. Williams, Y. Xue, A. Hartemink, L. Carin, and M. Figueiredo. On semisupervised classiﬁcation. In Advances in neural information processing systems 17, pages 721–728, 2004.</p>
<p>[9] N.D. Lawrence, M. Seeger, and R. Herbrich. Fast sparse gaussian process methods: The informative vector machine. Advances in Neural Information Processing Systems 15, 15:609– 616, 2002.</p>
<p>[10] D.V. Lindley. On a measure of the information provided by an experiment. The Annals of Mathematical Statistics, 27(4):986–1005, 1956.</p>
<p>[11] D. J. C. MacKay. Local minima, symmetry-breaking, and model pruning in variational free energy minimization. Available at http://www.inference.phy.cam.ac.uk/mackay/minima.pdf, 2001.</p>
<p>[12] D.J.C. MacKay. Information-based objective functions for active data selection. Neural computation, 4(4):590–604, 1992.</p>
<p>[13] T. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In Proceedings of the Eighteenth conference on Uncertainty in artiﬁcial intelligence, pages 352–359, 2002.</p>
<p>[14] Tom Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, MIT, 2001.</p>
<p>[15] Hannes Nickisch and Carl Edward Rasmussen. Approximations for binary Gaussian process classiﬁcation. The Journal of Machine Learning Research, 9:2035–2078, 2008.</p>
<p>[16] T. Raiko, A. Ilin, and K. Juha. Principal component analysis for large scale problems with lots of missing values. In Joost Kok, Jacek Koronacki, Raomon Mantaras, Stan Matwin, Dunja Mladenic, and Andrzej Skowron, editors, Machine Learning: ECML 2007, volume 4701 of Lecture Notes in Computer Science, pages 691–698. Springer Berlin / Heidelberg, 2007.</p>
<p>[17] P. Sebastiani and H.P. Wynn. Maximum entropy sampling and optimal Bayesian experimental design. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 62(1):145– 157, 2000.</p>
<p>[18] E. Snelson and Z. Ghahramani. Sparse gaussian processes using pseudo-inputs. In Advances in Neural Information Processing Systems 18, 2005.</p>
<p>[19] D. H. Stern, R. Herbrich, and T. Graepel. Matchbox: large scale online bayesian recommendations. In Proceedings of the 18th international conference on World wide web, pages 111–120, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
