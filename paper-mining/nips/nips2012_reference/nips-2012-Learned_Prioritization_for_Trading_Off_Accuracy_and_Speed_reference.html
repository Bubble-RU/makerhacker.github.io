<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-173" href="../nips2012/nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">nips2012-173</a> <a title="nips-2012-173-reference" href="#">nips2012-173-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</h1>
<br/><p>Source: <a title="nips-2012-173-pdf" href="http://papers.nips.cc/paper/4556-learned-prioritization-for-trading-off-accuracy-and-speed.pdf">pdf</a></p><p>Author: Jiarong Jiang, Adam Teichert, Jason Eisner, Hal Daume</p><p>Abstract: Users want inference to be both fast and accurate, but quality often comes at the cost of speed. The ﬁeld has experimented with approximate inference algorithms that make different speed-accuracy tradeoffs (for particular problems and datasets). We aim to explore this space automatically, focusing here on the case of agenda-based syntactic parsing [12]. Unfortunately, off-the-shelf reinforcement learning techniques fail to learn good policies: the state space is simply too large to explore naively. An attempt to counteract this by applying imitation learning algorithms also fails: the “teacher” follows a far better policy than anything in our learner’s policy space, free of the speed-accuracy tradeoff that arises when oracle information is unavailable, and thus largely insensitive to the known reward functﬁon. We propose a hybrid reinforcement/apprenticeship learning algorithm that learns to speed up an initial policy, trading off accuracy for speed according to various settings of a speed term in the loss function. 1</p><br/>
<h2>reference text</h2><p>[1] Pieter Abbeel and Andrew Ng. Apprenticeship learning via inverse reinforcement learning. In ICML, 2004.</p>
<p>[2] J. Andrew Bagnell. Robust supervised learning. In AAAI, 2005.</p>
<p>[3] Nathan Bodenstab, Aaron Dunlop, Keith Hall, and Brian Roark. Beam-width prediction for efﬁcient CYK parsing. In ACL, 2011.</p>
<p>[4] Sharon A. Caraballo and Eugene Charniak. New ﬁgures of merit for best-ﬁrst probabilistic chart parsing. Computational Linguistics, 24(2):275–298, 1998.</p>
<p>[5] Eugene Charniak. Top-down nearly-context-sensitive parsing. In EMNLP, 2010.</p>
<p>[6] Michael Collins and Brian Roark. Incremental parsing with the perceptron algorithm. In ACL, 2004.</p>
<p>[7] Hal Daum´ III, John Langford, and Daniel Marcu. Search-based structured prediction. Machine Learning, e 75(3):297–325, 2009.</p>
<p>[8] Jason Eisner and Hal Daum´ III. Learning speed-accuracy tradeoffs in nondeterministic inference algoe rithms. In COST: NIPS Workshop on Computational Trade-offs in Statistical Learning, 2011.</p>
<p>[9] Joshua Goodman. Semiring parsing. Computational Linguistics, 25(4):573–605, December 1999.</p>
<p>[10] V. Gullapalli and A. G. Barto. Shaping as a method for accelerating reinforcement learning. In Proceedings of the IEEE International Symposium on Intelligent Control, 1992.</p>
<p>[11] R. Kalman. Contributions to the theory of optimal control. Bol. Soc. Mat. Mexicana, 5:558–563, 1968.</p>
<p>[12] Martin Kay. Algorithm schemata and data structures in syntactic processing. In B. J. Grosz, K. Sparck Jones, and B. L. Webber, editors, Readings in Natural Language Processing, pages 35–70. Kaufmann, 1986. First published (1980) as Xerox PARC TR CSL-80-12.</p>
<p>[13] Dan Klein and Chris Manning. A* parsing: Fast exact Viterbi parse selection. In NAACL/HLT, 2003.</p>
<p>[14] Percy Liang, Hal Daum´ III, and Dan Klein. Structure compilation: Trading structure for features. In e ICML, Helsinki, Finland, 2008.</p>
<p>[15] M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini. Building a large annotated corpus of English: The Penn Treebank. Computational linguistics, 19(2):330, 1993.</p>
<p>[16] Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii. Probabilistic CFG with latent annotations. In ACL, 2005.</p>
<p>[17] Andrew Ng and Stuart Russell. Algorithms for inverse reinforcement learning. In ICML, 2000.</p>
<p>[18] A. Pauls and D. Klein. Hierarchical search for parsing. In NAACL/HLT, pages 557–565. Association for Computational Linguistics, 2009.</p>
<p>[19] A. Pauls and D. Klein. Hierarchical A* parsing with bridge outside scores. In ACL, pages 348–352. Association for Computational Linguistics, 2010.</p>
<p>[20] Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural Networks, 21(4), 2008.</p>
<p>[21] S. Petrov and D. Klein. Improved inference for unlexicalized parsing. In NAACL/HLT, pages 404–411, 2007.</p>
<p>[22] B. Roark, K. Hollingshead, and N. Bodenstab. Finite-state chart constraints for reduced complexity context-free parsing pipelines. Computational Linguistics, Early Access:1–35, 2012.</p>
<p>[23] Brian Roark and Kristy Hollingshead. Classifying chart cells for quadratic complexity context-free inference. In COLING, pages 745–752, Manchester, UK, August 2008. Coling 2008 Organizing Committee.</p>
<p>[24] Stephane Ross, Geoff J. Gordon, and J. Andrew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In AI-Stats, 2011.</p>
<p>[25] Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.</p>
<p>[26] Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In NIPS, pages 1057–1063. MIT Press, 2000.</p>
<p>[27] R.J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(23), 1992.</p>
<p>[28] Yuehua Xu and Alan Fern. On learning linear ranking functions for beam search. In ICML, pages 1047– 1054, 2007.</p>
<p>[29] D. H. Younger. Recognition and parsing of context-free languages in time n3 . Information and Control, 10(2):189–208, February 1967.  9</p>
<br/>
<br/><br/><br/></body>
</html>
