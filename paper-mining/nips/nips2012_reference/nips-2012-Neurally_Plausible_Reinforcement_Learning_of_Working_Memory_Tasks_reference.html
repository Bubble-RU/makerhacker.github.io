<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>238 nips-2012-Neurally Plausible Reinforcement Learning of Working Memory Tasks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-238" href="../nips2012/nips-2012-Neurally_Plausible_Reinforcement_Learning_of_Working_Memory_Tasks.html">nips2012-238</a> <a title="nips-2012-238-reference" href="#">nips2012-238-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>238 nips-2012-Neurally Plausible Reinforcement Learning of Working Memory Tasks</h1>
<br/><p>Source: <a title="nips-2012-238-pdf" href="http://papers.nips.cc/paper/4813-neurally-plausible-reinforcement-learning-of-working-memory-tasks.pdf">pdf</a></p><p>Author: Jaldert Rombouts, Pieter Roelfsema, Sander M. Bohte</p><p>Abstract: A key function of brains is undoubtedly the abstraction and maintenance of information from the environment for later use. Neurons in association cortex play an important role in this process: by learning these neurons become tuned to relevant features and represent the information that is required later as a persistent elevation of their activity [1]. It is however not well known how such neurons acquire these task-relevant working memories. Here we introduce a biologically plausible learning scheme grounded in Reinforcement Learning (RL) theory [2] that explains how neurons become selective for relevant information by trial and error learning. The model has memory units which learn useful internal state representations to solve working memory tasks by transforming partially observable Markov decision problems (POMDP) into MDPs. We propose that synaptic plasticity is guided by a combination of attentional feedback signals from the action selection stage to earlier processing levels and a globally released neuromodulatory signal. Feedback signals interact with feedforward signals to form synaptic tags at those connections that are responsible for the stimulus-response mapping. The neuromodulatory signal interacts with tagged synapses to determine the sign and strength of plasticity. The learning scheme is generic because it can train networks in different tasks, simply by varying inputs and rewards. It explains how neurons in association cortex learn to 1) temporarily store task-relevant information in non-linear stimulus-response mapping tasks [1, 3, 4] and 2) learn to optimally integrate probabilistic evidence for perceptual decision making [5, 6]. 1</p><br/>
<h2>reference text</h2><p>[1] Gnadt, J. and Andersen, R. A. Memory Related motor planning activity in posterior parietal cortex of macaque. Experimental brain research., 70(1):216–220, 1988.</p>
<p>[2] Sutton, R. S. and Barto, A. G. Reinforcement learning. MIT Press, Cambridge, MA, 1998.</p>
<p>[3] Gottlieb, J. and Goldberg, M. E. Activity of neurons in the lateral intraparietal area of the monkey during an antisaccade task. Nature neuroscience, 2(10):906–12, 1999.</p>
<p>[4] Bisley, J. W. and Goldberg, M. E. Attention, intention, and priority in the parietal lobe. Annual review of neuroscience, 33:1–21, 2010.</p>
<p>[5] Gold, J. I. and Shadlen, M. N. The neural basis of decision making. Annual review of neuroscience, 30:535–74, 2007.</p>
<p>[6] Yang, T. and Shadlen, M. N. Probabilistic reasoning by neurons. Nature, 447(7148):1075–80, 2007.</p>
<p>[7] O’Reilly, R. C. and Frank, M. J. Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia. Neural computation, 18(2):283–328, 2006.  8</p>
<p>[8] Izhikevich, E. M. Solving the distal reward problem through linkage of STDP and dopamine signaling. Cerebral cortex, 17(10):2443–52, 2007.</p>
<p>[9] Montague, P. R., Hyman, S. E., et al. Computational roles for dopamine in behavioural control. Nature, 431(7010):760–7, 2004.</p>
<p>[10] Rummery, G. A. and Niranjan, M. Online Q-learning using connectionist systems. Technical report, Cambridge University Engineering Department, 1994.</p>
<p>[11] Funahashi, S., Bruce, C. J., et al. Mnemonic Coding of Visual Space in the Monkey’s Dorsolateral Prefrontal Cortex. Journal of Neurophysiology, 6(2):331–349, 1989.</p>
<p>[12] Cassenaer, S. and Laurent, G. Conditional modulation of spike-timing- dependent plasticity for olfactory learning. Nature, 482(7383):47–52, 2012.</p>
<p>[13] Roelfsema, P. R. and van Ooyen, A. Attention-Gated Reinforcement Learning of Internal Representations for Classiﬁcation. Neural Computation, 2214(17):2176–2214, 2005.</p>
<p>[14] Schultz, W. Multiple dopamine functions at different time courses. Annual review of neuroscience, 30:259–88, 2007.</p>
<p>[15] Wiering, M. and Schmidhuber, J. HQ-Learning. Adaptive Behavior, 6(2):219–246, 1997.</p>
<p>[16] Rumelhart, D. E., Hinton, G. E., et al. Learning representations by back-propagating errors. Nature, 323(6088):533–536, 1986.</p>
<p>[17] Krueger, K. A. and Dayan, P. Flexible shaping: how learning in small steps helps. Cognition, 110(3):380– 94, 2009.</p>
<p>[18] Sommer, M. A. and Wurtz, R. H. Frontal Eye Field Sends Delay Activity Related to Movement, Memory, and Vision to the Superior Colliculus. Journal of Neurophysiology, 85(4):1673–1685, 2001.</p>
<p>[19] Soltani, A. and Wang, X.-J. Synaptic computation underlying probabilistic inference. Nature Neuroscience, 13(1):112–119, 2009.</p>
<p>[20] Watkins, C. J. and Dayan, P. Q-learning. Machine learning, 292:279–292, 1992.</p>
<p>[21] Morris, G., Nevet, A., et al. Midbrain dopamine neurons encode decisions for future action. Nature neuroscience, 9(8):1057–63, 2006.</p>
<p>[22] Roesch, M. R., Calu, D. J., et al. Dopamine neurons encode the better option in rats deciding between differently delayed or sized rewards. Nature neuroscience, 10(12):1615–24, 2007.</p>
<p>[23] van Seijen, H., van Hasselt, H., et al. A theoretical and empirical analysis of Expected Sarsa. 2009 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning, pages 177–184, 2009.</p>
<p>[24] Baird, L. Residual algorithms: Reinforcement learning with function approximation. In Proceedings of the 26th International Conference on Machine Learning (ICML), pages 30–37, 1995.</p>
<p>[25] Todd, M. T., Niv, Y., et al. Learning to use working memory in partially observable environments through dopaminic reinforcement. In NIPS, volume 21, pages 1689–1696, 2009.</p>
<p>[26] Zipser, D. Recurrent network model of the neural mechanism of short-term active memory. Neural Computation, 3(2):179–193, 1991.</p>
<p>[27] Moody, S. L., Wise, S. P., et al. A model that accounts for activity in primate frontal cortex during a delayed matching-to-sample task. The journal of Neuroscience, 18(1):399–410, 1998.</p>
<p>[28] Nassi, J. J. and Callaway, E. M. Parallel processing strategies of the primate visual system. Nature reviews. Neuroscience, 10(5):360–72, 2009.</p>
<p>[29] Hikosaka, O., Nakamura, K., et al. Basal ganglia orient eyes to reward. Journal of neurophysiology, 95(2):567–84, 2006.</p>
<p>[30] Samejima, K., Ueda, Y., et al. Representation of action-speciﬁc reward values in the striatum. Science, 310(5752):1337–40, 2005.</p>
<p>[31] Wang, X.-J. Synaptic reverberation underlying mnemonic persistent activity. Trends in neurosciences, 24(8):455–63, 2001.</p>
<p>[32] Roelfsema, P. R., van Ooyen, A., et al. Perceptual learning rules based on reinforcers and attention. Trends in cognitive sciences, 14(2):64–71, 2010.</p>
<p>[33] Deubel, H. and Schneider, W. Saccade target selection and object recognition: Evidence for a common attentional mechanism. Vision Research, 36(12):1827–1837, 1996.</p>
<p>[34] Frey, U. and Morris, R. Synaptic tagging and long-term potentiation. Nature, 385(6616):533–536, 1997.</p>
<p>[35] Moncada, D., Ballarini, F., et al. Identiﬁcation of transmitter systems and learning tag molecules involved in behavioral tagging during memory formation. PNAS, 108(31):12931–6, 2011.</p>
<p>[36] Sajikumar, S. and Korte, M. Metaplasticity governs compartmentalization of synaptic tagging and capture through brain-derived neurotrophic factor (BDNF) and protein kinase Mzeta (PKMzeta). PNAS, 108(6):2551–6, 2011.  9</p>
<br/>
<br/><br/><br/></body>
</html>
