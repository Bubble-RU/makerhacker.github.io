<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>244 nips-2012-Nonconvex Penalization Using Laplace Exponents and Concave Conjugates</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-244" href="../nips2012/nips-2012-Nonconvex_Penalization_Using_Laplace_Exponents_and_Concave_Conjugates.html">nips2012-244</a> <a title="nips-2012-244-reference" href="#">nips2012-244-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>244 nips-2012-Nonconvex Penalization Using Laplace Exponents and Concave Conjugates</h1>
<br/><p>Source: <a title="nips-2012-244-pdf" href="http://papers.nips.cc/paper/4763-nonconvex-penalization-using-laplace-exponents-and-concave-conjugates.pdf">pdf</a></p><p>Author: Zhihua Zhang, Bojun Tu</p><p>Abstract: In this paper we study sparsity-inducing nonconvex penalty functions using L´ vy e processes. We deﬁne such a penalty as the Laplace exponent of a subordinator. Accordingly, we propose a novel approach for the construction of sparsityinducing nonconvex penalties. Particularly, we show that the nonconvex logarithmic (LOG) and exponential (EXP) penalty functions are the Laplace exponents of Gamma and compound Poisson subordinators, respectively. Additionally, we explore the concave conjugate of nonconvex penalties. We ﬁnd that the LOG and EXP penalties are the concave conjugates of negative Kullback-Leiber (KL) distance functions. Furthermore, the relationship between these two penalties is due to asymmetricity of the KL distance. 1</p><br/>
<h2>reference text</h2><p>[1] D. Applebaum. L´ vy Processes and Stochastic Calculus. Cambridge University Press, Cambridge, UK, e 2004.</p>
<p>[2] A. Armagan, D. Dunson, and J. Lee. Generalized double Pareto shrinkage. Technical report, Duke University Department of Statistical Science, February 2011.</p>
<p>[3] P. S. Bradley and O. L. Mangasarian. Feature selection via concave minimization and support vector machines. In The 26th International Conference on Machine Learning, pages 82–90. Morgan Kaufmann Publishers, San Francisco, California, 1998.</p>
<p>[4] F. Caron and A. Doucet. Sparse bayesian nonparametric regression. In Proceedings of the 25th international conference on Machine learning, page 88, 2008.</p>
<p>[5] V. Cevher. Learning with compressible priors. In Advances in Neural Information Processing Systems 22, pages 261–269, 2009.</p>
<p>[6] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its Oracle properties. Journal of the American Statistical Association, 96:1348–1361, 2001.</p>
<p>[7] Osborne B. G., Fearn T., Miller A. R., and Douglas S. Application of near-infrared reﬂectance spectroscopy to compositional analysis of biscuits and biscuit dough. Journal of the Science of Food and Agriculture, 35(1):99–105, 1984.</p>
<p>[8] C. Gao, N. Wang, Q. Yu, and Z. Zhang. A feasible nonconvex relaxation approach to feature selection. In Proceedings of the Twenty-Fifth National Conference on Artiﬁcial Intelligence (AAAI’11), 2011.</p>
<p>[9] P. J. Garrigues and B. A. Olshausen. Group sparse coding with a Laplacian scale mixture prior. In Advances in Neural Information Processing Systems 22, 2010.</p>
<p>[10] Z. Ghahramani, T. Grifﬁths, and P. Sollich. Bayesian nonparametric latent feature models. In World meeting on Bayesian Statistics, 2006.</p>
<p>[11] J. E. Grifﬁn and P. J. Brown. Bayesian adaptive Lassos with non-convex penalization. Technical report, University of Kent, 2010.</p>
<p>[12] A. Lee, F. Caron, A. Doucet, and C. Holmes. A hierarchical Bayesian framework for constructing sparsityinducing priors. Technical report, University of Oxford, UK, 2010.</p>
<p>[13] R. Mazumder, J. Friedman, and T. Hastie. SparseNet: Coordinate descent with nonconvex penalties. Journal of the American Statistical Association, 106(495):1125–1138, 2011.</p>
<p>[14] J. A. Palmer, D. P. Wipf, K. Kreutz-Delgado, and B. D. Rao. Variational EM algorithms for non-Gaussian latent variable models. In Advances in Neural Information Processing Systems 18, 2006.</p>
<p>[15] N. G. Polson and J. G. Scott. Local shrinkage rules, l´ vy processes, and regularized regression. Journal e of the Royal Statistical Society (Series B), 74(2):287–311, 2012.</p>
<p>[16] S.-I. P. Sato. L´ vy Processes and inﬁnitely Divisible Distributions. Cambridge University Press, Came bridge, UK, 1999.</p>
<p>[17] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B, 58:267–288, 1996.</p>
<p>[18] M. K. Titsias. The inﬁnite gamma-poisson feature models. In Advances in Neural Information Processing Systems 20, 2007.</p>
<p>[19] J. Weston, A. Elisseeff, B. Sch¨ lkopf, and M. Tipping. Use of the zero-norm with linear models and o kernel methods. Journal of Machine Learning Research, 3:1439–1461, 2003.</p>
<p>[20] C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, 38:894–942, 2010.</p>
<p>[21] T. Zhang. Analysis of multi-stage convex relaxation for sparse regularization. Journal of Machine Learning Research, 11:1081–1107, 2010.</p>
<p>[22] Z. Zhang, S. Wang, D. Liu, and M. I. Jordan. EP-GIG priors and applications in Bayesian sparse learning. Journal of Machine Learning Research, 13:2031–2061, 2012.</p>
<p>[23] H. Zou and R. Li. One-step sparse estimates in nonconcave penalized likelihood models. The Annals of Statistics, 36(4):1509–1533, 2008.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
