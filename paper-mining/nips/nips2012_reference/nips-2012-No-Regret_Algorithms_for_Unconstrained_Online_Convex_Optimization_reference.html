<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>241 nips-2012-No-Regret Algorithms for Unconstrained Online Convex Optimization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-241" href="../nips2012/nips-2012-No-Regret_Algorithms_for_Unconstrained_Online_Convex_Optimization.html">nips2012-241</a> <a title="nips-2012-241-reference" href="#">nips2012-241-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>241 nips-2012-No-Regret Algorithms for Unconstrained Online Convex Optimization</h1>
<br/><p>Source: <a title="nips-2012-241-pdf" href="http://papers.nips.cc/paper/4709-no-regret-algorithms-for-unconstrained-online-convex-optimization.pdf">pdf</a></p><p>Author: Brendan Mcmahan, Matthew Streeter</p><p>Abstract: Some of the most compelling applications of online convex optimization, including online prediction and classiﬁcation, are unconstrained: the natural feasible set is Rn . Existing algorithms fail to achieve sub-linear regret in this setting unless constraints on the comparator point ˚ are known in advance. We present algox rithms that, without such prior knowledge, offer near-optimal regret bounds with respect to any choice of ˚. In particular, regret with respect to ˚ = 0 is constant. x x We then prove lower bounds showing that our guarantees are near-optimal in this setting. 1</p><br/>
<h2>reference text</h2><p>[1] Jacob Abernethy, Peter L. Bartlett, Alexander Rakhlin, and Ambuj Tewari. Optimal strategies and minimax lower bounds for online convex games. In COLT, 2008.</p>
<p>[2] Amit Agarwal, Elad Hazan, Satyen Kale, and Robert E. Schapire. Algorithms for portfolio management based on the Newton method. In ICML, 2006.</p>
<p>[3] Nicol` Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge Unio versity Press, New York, NY, USA, 2006. ISBN 0521841089.</p>
<p>[4] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. In COLT, 2010.</p>
<p>[5] Elad Hazan and Satyen Kale. Extracting certainty from uncertainty: Regret bounded by variation in costs. In COLT, 2008.</p>
<p>[6] Elad Hazan and Satyen Kale. On stochastic and worst-case models for investing. In Advances in Neural Information Processing Systems 22. 2009.</p>
<p>[7] Robert A. Jacobs. Increased rates of convergence through learning rate adaptation. Neural Networks, 1987.</p>
<p>[8] Jyrki Kivinen and Manfred Warmuth. Exponentiated Gradient Versus Gradient Descent for Linear Predictors. Journal of Information and Computation, 132, 1997.</p>
<p>[9] Todd K. Leen and Genevieve B. Orr. Optimal stochastic search and adaptive momentum. In NIPS, 1993.</p>
<p>[10] H. Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex optimization. In COLT, 2010.</p>
<p>[11] Barak Pearlmutter. Gradient descent: Second order momentum and saturating error. In NIPS, 1991.</p>
<p>[12] Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107–194, 2012.</p>
<p>[13] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In ICML, 2003.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
