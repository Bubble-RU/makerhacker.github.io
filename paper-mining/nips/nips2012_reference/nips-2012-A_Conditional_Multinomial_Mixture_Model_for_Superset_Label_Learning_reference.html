<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 nips-2012-A Conditional Multinomial Mixture Model for Superset Label Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-5" href="../nips2012/nips-2012-A_Conditional_Multinomial_Mixture_Model_for_Superset_Label_Learning.html">nips2012-5</a> <a title="nips-2012-5-reference" href="#">nips2012-5-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>5 nips-2012-A Conditional Multinomial Mixture Model for Superset Label Learning</h1>
<br/><p>Source: <a title="nips-2012-5-pdf" href="http://papers.nips.cc/paper/4597-a-conditional-multinomial-mixture-model-for-superset-label-learning.pdf">pdf</a></p><p>Author: Liping Liu, Thomas G. Dietterich</p><p>Abstract: In the superset label learning problem (SLL), each training instance provides a set of candidate labels of which one is the true label of the instance. As in ordinary regression, the candidate label set is a noisy version of the true label. In this work, we solve the problem by maximizing the likelihood of the candidate label sets of training instances. We propose a probabilistic model, the Logistic StickBreaking Conditional Multinomial Model (LSB-CMM), to do the job. The LSBCMM is derived from the logistic stick-breaking process. It ﬁrst maps data points to mixture components and then assigns to each mixture component a label drawn from a component-speciﬁc multinomial distribution. The mixture components can capture underlying structure in the data, which is very useful when the model is weakly supervised. This advantage comes at little cost, since the model introduces few additional parameters. Experimental tests on several real-world problems with superset labels show results that are competitive or superior to the state of the art. The discovered underlying structures also provide improved explanations of the classiﬁcation predictions. 1</p><br/>
<h2>reference text</h2><p>[1] C. M. Bishop. Pattern recognition and machine learning. Springer, 2006.</p>
<p>[2] F. Briggs & X. F. Fern & R. Raich. Rank-Loss Support Instance Machines for MIML Instance Annotation. In proc. KDD, 2012.</p>
<p>[3] C.-C. Chang & C.-J. Lin. LIBSVM: A Library for Support Vector Machines. ACM Trans. on Intelligent Systems and Technology, 2(3):1-27, 2011.</p>
<p>[4] T. Cour & B. Sapp & C. Jordan & B. Taskar. Learning From Ambiguously Labeled Images. In Proc. CVPR 2009.</p>
<p>[5] T. Cour & B. Sapp & B. Taskar. Learning from Partial Labels. Journal of Machine Learning Research, 12:1225-1261, 2011.</p>
<p>[6] A. Frank & A. Asuncion. UCI Machine Learning Repository [http://archive.ics.uci.edu/ml].</p>
<p>[7] Y. Grandvalet. Logistic Regression for Partial Labels. In Proc. IPMU, 2002.</p>
<p>[8] E. Hullermeier & J. Beringer. Learning from Ambiguously Labeled Examples. In Proc. IDA-05, 6th International Symposium on Intelligent Data Analysis Madrid, 2005.</p>
<p>[9] L. Jie & F. Orabona. Learning from Candidate Labeling Sets. In Proc. NIPS, 2010.</p>
<p>[10] R. Jin & Z. Ghahramani. Learning with Multiple Labels. In Proc. NIPS, 2002.</p>
<p>[11] L-P. Liu & T. Dietterich. A Conditional Multinomial Mixture Model for Superset Label Learning (Supplementary Materials), http://web.engr.oregonstate.edu/˜liuli/pdf/lsb_cmm_supp.pdf .</p>
<p>[12] N. Nguyen & R. Caruana. Classiﬁcation with Partial Labels. In Proc. KDD, 2008.</p>
<p>[13] L. Ren & L. Du & L. Carin & D. B. Dunson. Logistic Stick-Breaking Process. Journal of Machine Learning Research, 12:203-239, 2011.</p>
<p>[14] Y. W. Teh. Dirichlet Processes. Encyclopedia of Machine Learning, to appear. Springer.</p>
<p>[15] Z.-H. Zhou & M.-L. Zhang. Multi-Instance Multi-Label Learning with Application To Scene Classiﬁcation. Advances in Neural Information Processing Systems, 19, 2007</p>
<p>[16] X. Zhu & A. B. Goldberg. Introduction to Semi-Supervised Learning. Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning, 3(1):1-130, 2009.  9</p>
<br/>
<br/><br/><br/></body>
</html>
