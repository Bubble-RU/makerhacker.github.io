<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>191 nips-2012-Learning the Architecture of Sum-Product Networks Using Clustering on Variables</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-191" href="../nips2012/nips-2012-Learning_the_Architecture_of_Sum-Product_Networks_Using_Clustering_on_Variables.html">nips2012-191</a> <a title="nips-2012-191-reference" href="#">nips2012-191-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>191 nips-2012-Learning the Architecture of Sum-Product Networks Using Clustering on Variables</h1>
<br/><p>Source: <a title="nips-2012-191-pdf" href="http://papers.nips.cc/paper/4544-learning-the-architecture-of-sum-product-networks-using-clustering-on-variables.pdf">pdf</a></p><p>Author: Aaron Dennis, Dan Ventura</p><p>Abstract: The sum-product network (SPN) is a recently-proposed deep model consisting of a network of sum and product nodes, and has been shown to be competitive with state-of-the-art deep models on certain difﬁcult tasks such as image completion. Designing an SPN network architecture that is suitable for the task at hand is an open question. We propose an algorithm for learning the SPN architecture from data. The idea is to cluster variables (as opposed to data instances) in order to identify variable subsets that strongly interact with one another. Nodes in the SPN network are then allocated towards explaining these interactions. Experimental evidence shows that learning the SPN architecture signiﬁcantly improves its performance compared to using a previously-proposed static architecture. 1</p><br/>
<h2>reference text</h2><p>[1] Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18:1527–1554, July 2006.</p>
<p>[2] Hoifung Poon and Pedro Domingos. Sum-product networks: A new deep architecture. In Proceedings of the Twenty-Seventh Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-11), pages 337–346, Corvallis, Oregon, 2011. AUAI Press.</p>
<p>[3] Ryan Prescott Adams, Hanna M. Wallach, and Zoubin Ghahramani. Learning the structure of deep sparse graphical models. In Proceedings of the 13th International Conference on Artiﬁcial Intelligence and Statistics, 2010.</p>
<p>[4] Nevin L. Zhang. Hierarchical latent class models for cluster analysis. Journal of Machine Learning Research, 5:697–723, December 2004.</p>
<p>[5] Adnan Darwiche. A differential approach to inference in bayesian networks. Journal of the ACM, 50:280–305, May 2003.</p>
<p>[6] Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks. In Advances in Neural Information Processing Systems 24, pages 666–674. 2011.</p>
<p>[7] Daphne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT Press, 2009.</p>
<p>[8] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in python. Journal of Machine Learning Research, 12:2825–2830, 2011.</p>
<p>[9] F.S. Samaria and A.C. Harter. Parameterisation of a stochastic model for human face identiﬁcation. In Proceedings of the Second IEEE Workshop on Applications of Computer Vision, pages 138 –142, Dec 1994.</p>
<p>[10] Li Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In IEEE CVPR 2004, Workshop on Generative-Model Based Vision, 2004.  9</p>
<br/>
<br/><br/><br/></body>
</html>
