<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>339 nips-2012-The Time-Marginalized Coalescent Prior for Hierarchical Clustering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-339" href="../nips2012/nips-2012-The_Time-Marginalized_Coalescent_Prior_for_Hierarchical_Clustering.html">nips2012-339</a> <a title="nips-2012-339-reference" href="#">nips2012-339-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>339 nips-2012-The Time-Marginalized Coalescent Prior for Hierarchical Clustering</h1>
<br/><p>Source: <a title="nips-2012-339-pdf" href="http://papers.nips.cc/paper/4786-the-time-marginalized-coalescent-prior-for-hierarchical-clustering.pdf">pdf</a></p><p>Author: Levi Boyles, Max Welling</p><p>Abstract: We introduce a new prior for use in Nonparametric Bayesian Hierarchical Clustering. The prior is constructed by marginalizing out the time information of Kingman’s coalescent, providing a prior over tree structures which we call the Time-Marginalized Coalescent (TMC). This allows for models which factorize the tree structure and times, providing two beneﬁts: more ﬂexible priors may be constructed and more efﬁcient Gibbs type inference can be used. We demonstrate this on an example model for density estimation and show the TMC achieves competitive experimental results. 1</p><br/>
<h2>reference text</h2><p>[1] R.P. Adams, Z. Ghahramani, and M.I. Jordan. Tree-structured stick breaking for hierarchical data. Advances in Neural Information Processing Systems, 23:19–27, 2010.</p>
<p>[2] D Aldous. Probability distributions on cladograms. IMA Volumes in Mathematics and its . . . , 1995.</p>
<p>[3] David Blei, Thomas L. Grifﬁths, Michael I. Jordan, and Joshua B. Tenenbaum. Hierarchical topic models and the nested chinese restaurant process. In Sebastian Thrun, Lawrence Saul, and Bernhard Sch¨ lkopf, editors, Advances in Neural Information Processing Systems, o volume 16. MIT Press, Cambridge, MA, 2004.</p>
<p>[4] A. Drummond and A. Rambaut. Beast: Bayesian evolutionary analysis by sampling trees. BMC evolutionary biology, 7(1):214, 2007.</p>
<p>[5] T.S. Ferguson. Bayesian density estimation by mixtures of normal distributions. Recent advances in statistics, pages 287–303, 1983. ˘</p>
<p>[6] D. G¨ r¨ r, L. Boyles, and M. Welling. Scalable inference on kingman2019s coalescent using ou pair similarity. In Proceedings of AISTATS, 2012.</p>
<p>[7] D. G¨ r¨ r and Y. W. Teh. An efﬁcient sequential Monte Carlo algorithm for coalescent clusou tering. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 521–528, 2009.</p>
<p>[8] Katherine Heller and Zoubin Ghahramani. Bayesian hierarchical clustering. In Proceedings of ICML, volume 22, 2005.</p>
<p>[9] J. F. C. Kingman. The coalescent. Stochastic Processes and their Applications, 13:235–248, 1982.</p>
<p>[10] J. F. C. Kingman. On the genealogy of large populations. Journal of Applied Probability, 19:27–43, 1982.</p>
<p>[11] D.A. Knowles and Z. Ghahramani. Pitman-yor diffusion trees. Arxiv preprint arXiv:1106.2494, 2011.</p>
<p>[12] D.A. Knowles, J. Van Gael, and Z. Ghahramani. Message passing algorithms for dirichlet diffusion trees. In Proceedings of the 28th Annual International Conference on Machine Learning, 2011.</p>
<p>[13] A.Y. Lo. On a class of bayesian nonparametric estimates: I. density estimates. The Annals of Statistics, 12(1):351–357, 1984.</p>
<p>[14] Peter McCullagh, Jim Pitman, and Matthias Winkel. Gibbs fragmentation trees. Bernoulli, 14(4):988–1002, November 2008.</p>
<p>[15] R. Neal. Software for ﬂexible bayesian modeling and markov chain sampling. see http://www. cs. toronto. edu/ radford/fbm. software. html, 2003.</p>
<p>[16] R.M. Neal. Density modeling and clustering using dirichlet diffusion trees. Bayesian Statistics, 7:619–629, 2003.</p>
<p>[17] A. Rodriguez, D.B. Dunson, and A.E. Gelfand. The nested dirichlet process. Journal of the American Statistical Association, 103(483):1131–1154, 2008.</p>
<p>[18] R. Salakhutdinov, J. Tenenbaum, and A. Torralba. Learning to learn with compound hd models. In Advances in Neural Information Processing Systems 21, 2012.</p>
<p>[19] J. Steinhardt and Z. Ghahramani. Flexible martingale priors for deep hierarchies. In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), volume 43, pages 61–62, 2012.</p>
<p>[20] Y. W. Teh, H. Daum´ III, and D. M. Roy. Bayesian agglomerative clustering with coalescents. e In Advances in Neural Information Processing Systems, volume 20, 2008.</p>
<p>[21] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. CaltechUCSD Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.</p>
<p>[22] E.J. Yeoh, M.E. Ross, S.A. Shurtleff, W.K. Williams, D. Patel, R. Mahfouz, F.G. Behm, S.C. Raimondi, M.V. Relling, A. Patel, et al. Classiﬁcation, subtype discovery, and prediction of outcome in pediatric acute lymphoblastic leukemia by gene expression proﬁling. Cancer cell, 1(2):133–143, 2002.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
