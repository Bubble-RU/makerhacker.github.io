<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 nips-2012-A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and Tighter Bound</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-17" href="../nips2012/nips-2012-A_Scalable_CUR_Matrix_Decomposition_Algorithm%3A_Lower_Time_Complexity_and_Tighter_Bound.html">nips2012-17</a> <a title="nips-2012-17-reference" href="#">nips2012-17-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>17 nips-2012-A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and Tighter Bound</h1>
<br/><p>Source: <a title="nips-2012-17-pdf" href="http://papers.nips.cc/paper/4745-a-scalable-cur-matrix-decomposition-algorithm-lower-time-complexity-and-tighter-bound.pdf">pdf</a></p><p>Author: Shusen Wang, Zhihua Zhang</p><p>Abstract: The CUR matrix decomposition is an important extension of Nystr¨ m approximao tion to a general matrix. It approximates any data matrix in terms of a small number of its columns and rows. In this paper we propose a novel randomized CUR algorithm with an expected relative-error bound. The proposed algorithm has the advantages over the existing relative-error CUR algorithms that it possesses tighter theoretical bound and lower time complexity, and that it can avoid maintaining the whole data matrix in main memory. Finally, experiments on several real-world datasets demonstrate signiﬁcant improvement over the existing relative-error algorithms. 1</p><br/>
<h2>reference text</h2><p>[1] Adi Ben-Israel and Thomas N.E. Greville. Generalized Inverses: Theory and Applications. Second Edition. Springer, 2003.</p>
<p>[2] Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near-optimal column-based matrix reconstruction. CoRR, abs/1103.0995, 2011.</p>
<p>[3] Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near optimal column-based matrix reconstruction. In Proceedings of the 2011 IEEE 52nd Annual Symposium on Foundations of Computer Science, FOCS ’11, pages 305–314, 2011.</p>
<p>[4] Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. Indexing by latent semantic analysis. Journal of The American Society for Information Science, 41(6):391–407, 1990.</p>
<p>[5] Amit Deshpande and Luis Rademacher. Efﬁcient volume sampling for row/column subset selection. In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, FOCS ’10, pages 329–338, 2010.</p>
<p>[6] Amit Deshpande, Luis Rademacher, Santosh Vempala, and Grant Wang. Matrix approximation and projective clustering via volume sampling. Theory of Computing, 2(2006):225–247, 2006.</p>
<p>[7] Petros Drineas. Pass-efﬁcient algorithms for approximating large matrices. In In Proceeding of the 14th Annual ACM-SIAM Symposium on Dicrete Algorithms, pages 223–232, 2003. 8</p>
<p>[8] Petros Drineas, Ravi Kannan, and Michael W. Mahoney. Fast monte carlo algorithms for matrices iii: Computing a compressed approximate matrix decomposition. SIAM Journal on Computing, 36(1):184–206, 2006.</p>
<p>[9] Petros Drineas and Michael W. Mahoney. On the Nystr¨ m method for approximating a gram o matrix for improved kernel-based learning. Journal of Machine Learning Research, 6:2153– 2175, 2005.</p>
<p>[10] Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Relative-error CUR matrix decompositions. SIAM Journal on Matrix Analysis and Applications, 30(2):844–881, September 2008.</p>
<p>[11] A. Frank and A. Asuncion. UCI machine learning repository, 2010.</p>
<p>[12] S. A. Goreinov, E. E. Tyrtyshnikov, and N. L. Zamarashkin. A theory of pseudoskeleton approximations. Linear Algebra and Its Applications, 261:1–21, 1997.</p>
<p>[13] S. A. Goreinov, N. L. Zamarashkin, and E. E. Tyrtyshnikov. Pseudo-skeleton approximations by matrices of maximal volume. Mathematical Notes, 62(4):619–623, 1997.</p>
<p>[14] Venkatesan Guruswami and Ali Kemal Sinop. Optimal column-based low-rank matrix reconstruction. In Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’12, pages 1207–1214. SIAM, 2012.</p>
<p>[15] Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review, 53(2):217–288, 2011.</p>
<p>[16] John Hopcroft and Ravi Kannan. Computer Science Theory for the Information Age. 2012.</p>
<p>[17] Finny G. Kuruvilla, Peter J. Park, and Stuart L. Schreiber. Vector algebra in the analysis of genome-wide expression data. Genome Biology, 3:research0011–research0011.1, 2002.</p>
<p>[18] Lester Mackey, Ameet Talwalkar, and Michael I. Jordan. Divide-and-conquer matrix factorization. In Advances in Neural Information Processing Systems 24. 2011.</p>
<p>[19] Michael W. Mahoney and Petros Drineas. CUR matrix decompositions for improved data analysis. Proceedings of the National Academy of Sciences, 106(3):697–702, 2009.</p>
<p>[20] L. Sirovich and M. Kirby. Low-dimensional procedure for the characterization of human faces. Journal of the Optical Society of America A, 4(3):519–524, Mar 1987.</p>
<p>[21] Matthew Turk and Alex Pentland. Eigenfaces for recognition. Journal of Cognitive Neuroscience, 3(1):71–86, 1991.</p>
<p>[22] Eugene E. Tyrtyshnikov. Incomplete cross approximation in the mosaic-skeleton method. Computing, 64:367–380, 2000.  9</p>
<br/>
<br/><br/><br/></body>
</html>
