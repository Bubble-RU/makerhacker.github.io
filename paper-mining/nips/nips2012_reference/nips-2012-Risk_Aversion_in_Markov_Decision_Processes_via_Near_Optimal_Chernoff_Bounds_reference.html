<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>296 nips-2012-Risk Aversion in Markov Decision Processes via Near Optimal Chernoff Bounds</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-296" href="../nips2012/nips-2012-Risk_Aversion_in_Markov_Decision_Processes_via_Near_Optimal_Chernoff_Bounds.html">nips2012-296</a> <a title="nips-2012-296-reference" href="#">nips2012-296-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>296 nips-2012-Risk Aversion in Markov Decision Processes via Near Optimal Chernoff Bounds</h1>
<br/><p>Source: <a title="nips-2012-296-pdf" href="http://papers.nips.cc/paper/4514-risk-aversion-in-markov-decision-processes-via-near-optimal-chernoff-bounds.pdf">pdf</a></p><p>Author: Teodor M. Moldovan, Pieter Abbeel</p><p>Abstract: The expected return is a widely used objective in decision making under uncertainty. Many algorithms, such as value iteration, have been proposed to optimize it. In risk-aware settings, however, the expected return is often not an appropriate objective to optimize. We propose a new optimization objective for risk-aware planning and show that it has desirable theoretical properties. We also draw connections to previously proposed objectives for risk-aware planing: minmax, exponential utility, percentile and mean minus variance. Our method applies to an extended class of Markov decision processes: we allow costs to be stochastic as long as they are bounded. Additionally, we present an efﬁcient algorithm for optimizing the proposed objective. Synthetic and real-world experiments illustrate the effectiveness of our method, at scale. 1</p><br/>
<h2>reference text</h2><p>[1] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman, A. Pilchin, S. Sivasubramanian, P. Vosshall, and W. Vogels. Dynamo: amazon’s highly available key-value store. ACM SIGOPS Operating Systems Review, 41(6):205–220, 2007.</p>
<p>[2] Philippe Jorion. Value at risk: the new benchmark for managing ﬁnancial risk, volume 1. McGraw-Hill Professional, 2007.</p>
<p>[3] Shie Mannor and John N. Tsitsiklis. Mean-Variance Optimization in Markov Decision Processes. In Proceedings of the 28 International Conference on Machine Learning, 2011.</p>
<p>[4] Erick Delage and Shie Mannor. Percentile optimization in uncertain Markov decision processes with application to efﬁcient exploration. ICML; Vol. 227, page 225, 2007.</p>
<p>[5] Jay K. Satia and Roy E. Lave Jr. Markovian Decision Processes with Uncertain Transition Probabilities. Operations Research, 21(3):728–740, 1973.</p>
<p>[6] Matthias Heger. Consideration of risk in reinforcement learning. In Proceedings of the 11th International Machine Learning Conference (1994), pages 105–111. Morgan Kaufmann, 1994.</p>
<p>[7] Steve Levitt and Adi Ben-Israel. On Modeling Risk in Markov Decision Processes. Optimization and Related Topics, pages 27–41, 2001.</p>
<p>[8] Erick Delage and Shie Mannor. Percentile Optimization for Markov Decision Processes with Parameter Uncertainty. Operations Research, 58(1):203–213, 2010.</p>
<p>[9] Arnab Nilim and Laurent El Ghaoui. Robust Control of Markov Decision Processes with Uncertain Transition Matrices. Operations Research, 53(5):780–798, 2005.</p>
<p>[10] M. Bouakiz and Y. Kebir. Target-level criterion in Markov decision processes. Journal of Optimization Theory and Applications, 86(1):1–15, July 1995.</p>
<p>[11] Congbin Wu and Yuanlie Lin. Minimizing Risk Models in Markov Decision Processes with Policies Depending on Target Values. Journal of Mathematical Analysis and Applications, 231(1):47–67, 1999.</p>
<p>[12] S.I. Marcus, E. Fern´ ndez-Gaucherand, D. Hern´ ndez-Hernandez, S. Coraluppi, and P. Fard. a a Risk sensitive Markov decision processes. Systems and Control in the Twenty-First Century, 29:263–281, 1997.</p>
<p>[13] VS Borkar and SP Meyn. Risk-sensitive optimal control for Markov decision processes with monotone cost. Mathematics of Operations Research, 27(1):192–209, 2002.</p>
<p>[14] B. Defourny, D. Ernst, and L. Wehenkel. Risk-aware decision making and dynamic programming. In NIPS 2008 Workshop on Model Uncertainty and Risk in RL, 2008.</p>
<p>[15] Yann Le Tallec. Robust, Risk-Sensitive, and Data-driven Control of Markov Decision Processes. PhD thesis, Massachusetts Institute of Technology, 2007.</p>
<p>[16] Richard S. Sutton and Andrew G. Barto. Reinforcement learning: an introduction. MIT Press, 1998.</p>
<p>[17] Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc, October 1996.</p>
<p>[18] J. F. Kenney and E. S. Keeping. Cumulants and the cumulant-generating function, additive property of cumulants, and Sheppard’s correction. In Mathematics of Statistics, chapter 4.104.12, pages 77–82. Van Nostrand, Princeton, NJ, 2 edition, 1951.</p>
<p>[19] Richard Durrett. Probability: Theory and Examples. Cambridge University Press, 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
