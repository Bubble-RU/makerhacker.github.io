<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>192 nips-2012-Learning the Dependency Structure of Latent Factors</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-192" href="../nips2012/nips-2012-Learning_the_Dependency_Structure_of_Latent_Factors.html">nips2012-192</a> <a title="nips-2012-192-reference" href="#">nips2012-192-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>192 nips-2012-Learning the Dependency Structure of Latent Factors</h1>
<br/><p>Source: <a title="nips-2012-192-pdf" href="http://papers.nips.cc/paper/4636-learning-the-dependency-structure-of-latent-factors.pdf">pdf</a></p><p>Author: Yunlong He, Yanjun Qi, Koray Kavukcuoglu, Haesun Park</p><p>Abstract: In this paper, we study latent factor models with dependency structure in the latent space. We propose a general learning framework which induces sparsity on the undirected graphical model imposed on the vector of latent factors. A novel latent factor model SLFA is then proposed as a matrix factorization problem with a special regularization term that encourages collaborative reconstruction. The main beneﬁt (novelty) of the model is that we can simultaneously learn the lowerdimensional representation for data and model the pairwise relationships between latent factors explicitly. An on-line learning algorithm is devised to make the model feasible for large-scale learning problems. Experimental results on two synthetic data and two real-world data sets demonstrate that pairwise relationships and latent factors learned by our model provide a more structured way of exploring high-dimensional data, and the learned representations achieve the state-of-the-art classiﬁcation performance. 1</p><br/>
<h2>reference text</h2><p>[1] Bertsekas, D.: Nonlinear programming. Athena Scientiﬁc Belmont, MA (1999)</p>
<p>[2] Blei, D., Lafferty, J.: Correlated topic models. Advances in Neural Information Processing Systems (2006)</p>
<p>[3] Chandrasekaran, V., Parrilo, P., Willsky, A.: Latent variable graphical model selection via convex optimization. Arxiv preprint arXiv:1008.1290 (2010)</p>
<p>[4] Collins, M., Dasgupta, S., Schapire, R.: A generalization of principal component analysis to the exponential family. Advances in neural information processing systems (2002)</p>
<p>[5] Fan, R., Chang, K., Hsieh, C., Wang, X., Lin, C.: Liblinear: A library for large linear classiﬁcation. JMLR (2008)</p>
<p>[6] Goldfarb, D., Ma, S., Scheinberg, K.: Fast alternating linearization methods for minimizing the sum of two convex functions. Arxiv preprint arXiv:0912.4571 (2009)</p>
<p>[7] Gregor, K., Szlam, A., LeCun, Y.: Structured sparse coding via lateral inhibition. Advances in Neural Information Processing Systems 24 (2011)</p>
<p>[8] Hinton, G., Osindero, S., Bao, K.: Learning causally linked markov random ﬁelds. In: AI & Statistics (2005)</p>
<p>[9] Hsieh, C., Sustik, M., Ravikumar, P., Dhillon, I.: Sparse inverse covariance matrix estimation using quadratic approximation. Advances in Neural Information Processing Systems (NIPS) 24 (2011)</p>
<p>[10] Hyv¨ rinen, A., Hurri, J., Hoyer, P.: Independent component analysis. Natural Image Statistics (2009) a</p>
<p>[11] Jacob, L., Obozinski, G., Vert, J.: Group lasso with overlap and graph lasso. Proceedings of the 26th Annual International Conference on Machine Learning (2009)</p>
<p>[12] Jenatton, R., Mairal, J., Obozinski, G., Bach, F.: Proximal methods for sparse hierarchical dictionary learning. Proceedings of the International Conference on Machine Learning (2010)</p>
<p>[13] Karklin, Y., Lewicki, M.S.: Emergence of complex cell properties by learning to generalize in natural scenes. Nature (2009)</p>
<p>[14] Lee, D., Seung, H.: Learning the parts of objects by non-negative matrix factorization. Nature (1999)</p>
<p>[15] Lee, H., Battle, A., Raina, R., Ng, A.: Efﬁcient sparse coding algorithms. Advances in neural information processing systems (2007)</p>
<p>[16] Lee, H., Raina, R., Teichman, A., Ng, A.: Exponential family sparse coding with applications to selftaught learning. Proceedings of the 21st international jont conference on Artiﬁcal intelligence (2009)</p>
<p>[17] Lyu, S., Simoncelli, E.: Nonlinear extraction of independent components of natural images using radial gaussianization. Neural computation (2009)</p>
<p>[18] Murray, I., Adams, R.: Slice sampling covariance hyperparameters of latent gaussian models. Arxiv preprint arXiv:1006.0868 (2010)</p>
<p>[19] Olshausen, B., et al.: Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. Nature (1996)</p>
<p>[20] Rendl, F., Rinaldi, G., Wiegele, A.: Solving Max-Cut to optimality by intersecting semideﬁnite and polyhedral relaxations. Math. Programming 121(2), 307 (2010)</p>
<p>[21] Schmidt, M.: Graphical model structure learning with l1-regularization. Ph.D. thesis, UNIVERSITY OF BRITISH COLUMBIA (2010)</p>
<p>[22] Schmidt, M., Van Den Berg, E., Friedlander, M., Murphy, K.: Optimizing costly functions with simple constraints: A limited-memory projected quasi-newton algorithm. In: AI & Statistics (2009)</p>
<p>[23] Silva, R., Scheine, R., Glymour, C., Spirtes, P.: Learning the structure of linear latent variable models. The Journal of Machine Learning Research 7, 191–246 (2006)</p>
<p>[24] Stegle, O., Lippert, C., Mooij, J., Lawrence, N., Borgwardt, K.: Efﬁcient inference in matrix-variate gaussian models with iid observation noise. Advances in Neural Information Processing Systems (2011)</p>
<p>[25] Teh, Y., Seeger, M., Jordan, M.: Semiparametric latent factor models. In: AI & Statistics (2005)</p>
<p>[26] Tibshirani, R.: Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological) (1996)</p>
<p>[27] Wainwright, M., Simoncelli, E.: Scale mixtures of gaussians and the statistics of natural images. Advances in neural information processing systems (2000)</p>
<p>[28] Yuan, M., Lin, Y.: Model selection and estimation in the gaussian graphical model. Biometrika (2007)  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
