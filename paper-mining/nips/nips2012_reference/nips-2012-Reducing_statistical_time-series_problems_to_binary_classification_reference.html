<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>291 nips-2012-Reducing statistical time-series problems to binary classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-291" href="../nips2012/nips-2012-Reducing_statistical_time-series_problems_to_binary_classification.html">nips2012-291</a> <a title="nips-2012-291-reference" href="#">nips2012-291-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>291 nips-2012-Reducing statistical time-series problems to binary classification</h1>
<br/><p>Source: <a title="nips-2012-291-pdf" href="http://papers.nips.cc/paper/4510-reducing-statistical-time-series-problems-to-binary-classification.pdf">pdf</a></p><p>Author: Daniil Ryabko, Jeremie Mary</p><p>Abstract: We show how binary classiﬁcation methods developed to work on i.i.d. data can be used for solving statistical problems that are seemingly unrelated to classiﬁcation and concern highly-dependent time series. Speciﬁcally, the problems of time-series clustering, homogeneity testing and the three-sample problem are addressed. The algorithms that we construct for solving these problems are based on a new metric between time-series distributions, which can be evaluated using binary classiﬁcation methods. Universal consistency of the proposed algorithms is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data. 1</p><br/>
<h2>reference text</h2><p>[1] T. M. Adams and A. B. Nobel. Uniform convergence of Vapnik-Chervonenkis classes under ergodic sampling. The Annals of Probability, 38:1345–1367, 2010.</p>
<p>[2] T. M. Adams and A. B. Nobel. Uniform approximation of Vapnik-Chervonenkis classes. Bernoulli, 18(4):1310–1319, 2012.</p>
<p>[3] M.-F. Balcan, N. Bansal, A. Beygelzimer, D. Coppersmith, J. Langford, and G. Sorkin. Robust reductions from ranking to classiﬁcation. In COLT’07, v. 4539 of LNCS, pages 604–619. 2007.</p>
<p>[4] M.F. Balcan, A. Blum, and S. Vempala. A discriminative framework for clustering via similarity functions. In STOC, pp. 671–680. ACM, 2008.</p>
<p>[5] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27, 2011. Software available at http://www.csie.ntu.edu.tw/˜cjlin/libsvm.</p>
<p>[6] C. Cortes and V. Vapnik. Support-vector networks. Mach. Learn., 20(3):273–297, 1995.</p>
<p>[7] R. Fortet and E. Mourier. Convergence de la r´ partition empirique vers la r´ partition e e th´ oretique. Ann. Sci. Ec. Norm. Super., III. Ser, 70(3):267–285, 1953. e</p>
<p>[8] R. Gray. Probability, Random Processes, and Ergodic Properties. Springer Verlag, 1988.</p>
<p>[9] M. Gutman. Asymptotically optimal classiﬁcation for multiple tests with empirically observed statistics. IEEE Transactions on Information Theory, 35(2):402–408, 1989.</p>
<p>[10] Z. Harchaoui, F. Bach, E. Moulines. Kernel change-point analysis. NIPS, pp. 609–616, 2008.</p>
<p>[11] L. V. Kantorovich and G. S. Rubinstein. On a function space in certain extremal problems. Dokl. Akad. Nauk USSR, 115(6):1058–1061, 1957.</p>
<p>[12] R.L. Karandikara and M. Vidyasagar. Rates of uniform convergence of empirical means with mixing processes. Statistics and Probability Letters, 58:297–307, 2002.</p>
<p>[13] A. Khaleghi, D. Ryabko, J. Mary, and P. Preux. Online clustering of processes. In AISTATS, JMLR W&CP; 22, pages 601–609, 2012.</p>
<p>[14] D. Kifer, S. Ben-David, J. Gehrke. Detecting change in data streams. VLDB (v.30): 180–191, 2004.</p>
<p>[15] A.N. Kolmogorov. Sulla determinazione empirica di una legge di distribuzione. G. Inst. Ital. Attuari, pages 83–91, 1933.</p>
<p>[16] John Langford, Roberto Oliveira, and Bianca Zadrozny. Predicting conditional quantiles via reduction to classiﬁcation. In UAI, 2006.</p>
<p>[17] Jos´ del R. Mill´ n. On the need for on-line learning in brain-computer interfaces. In Proc. of e a the Int. Joint Conf. on Neural Networks, 2004.</p>
<p>[18] D. Pollard. Convergence of Stochastic Processes. Springer, 1984.</p>
<p>[19] B. Ryabko. Prediction of random sequences and universal coding. Problems of Information Transmission, 24:87–96, 1988.</p>
<p>[20] B. Ryabko. Compression-based methods for nonparametric prediction and estimation of some characteristics of time series. IEEE Transactions on Information Theory, 55:4309–4315, 2009.</p>
<p>[21] D. Ryabko. Clustering processes. In Proc. ICML 2010, pp. 919–926, Haifa, Israel, 2010.</p>
<p>[22] D. Ryabko. Discrimination between B-processes is impossible. Journal of Theoretical Probability, 23(2):565–575, 2010.</p>
<p>[23] D. Ryabko and B. Ryabko. Nonparametric statistical inference for ergodic processes. IEEE Transactions on Information Theory, 56(3):1430–1435, 2010.</p>
<p>[24] H. Sakoe and S. Chiba. Dynamic programming algorithm optimization for spoken word recognition. IEEE Transactions on Acoustics, Speech and Signal Processing, 26(1):43–49, 1978.</p>
<p>[25] P. Shields. The Ergodic Theory of Discrete Sample Paths. AMS Bookstore, 1996.</p>
<p>[26] V. M. Zolotarev. Metric distances in spaces of random variables and their distributions. Math. USSR-Sb, 30(3):373–401, 1976.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
