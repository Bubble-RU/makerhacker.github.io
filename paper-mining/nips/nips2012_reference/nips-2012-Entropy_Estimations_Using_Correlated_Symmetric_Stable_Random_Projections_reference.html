<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>119 nips-2012-Entropy Estimations Using Correlated Symmetric Stable Random Projections</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-119" href="../nips2012/nips-2012-Entropy_Estimations_Using_Correlated_Symmetric_Stable_Random_Projections.html">nips2012-119</a> <a title="nips-2012-119-reference" href="#">nips2012-119-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>119 nips-2012-Entropy Estimations Using Correlated Symmetric Stable Random Projections</h1>
<br/><p>Source: <a title="nips-2012-119-pdf" href="http://papers.nips.cc/paper/4667-entropy-estimations-using-correlated-symmetric-stable-random-projections.pdf">pdf</a></p><p>Author: Ping Li, Cun-hui Zhang</p><p>Abstract: Methods for efﬁciently estimating Shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e.g., the DDoS attacks). For nonnegative data streams, the method of Compressed Counting (CC) [11, 13] based on maximally-skewed stable random projections can provide accurate estimates of the Shannon entropy using small storage. However, CC is no longer applicable when entries of data streams can be below zero, which is a common scenario when comparing two streams. In this paper, we propose an algorithm for entropy estimation in general data streams which allow negative entries. In our method, the Shannon entropy is approximated by the ﬁnite difference of two correlated frequency moments estimated from correlated samples of symmetric stable random variables. Interestingly, the estimator for the moment we recommend for entropy estimation barely has bounded variance itself, whereas the common geometric mean estimator (which has bounded higher-order moments) is not sufﬁcient for entropy estimation. Our experiments conﬁrm that this method is able to well approximate the Shannon entropy using small storage.</p><br/>
<h2>reference text</h2><p>[1] Brian Babcock, Shivnath Babu, Mayur Datar, Rajeev Motwani, and Jennifer Widom. Models and issues in data stream systems. In PODS, pages 1–16, Madison, WI, 2002.</p>
<p>[2] Daniela Brauckhoff, Bernhard Tellenbach, Arno Wagner, Martin May, and Anukool Lakhina. Impact of packet sampling on anomaly detection metrics. In IMC, pages 159–164, Rio de Janeriro, Brazil, 2006.</p>
<p>[3] John M. Chambers, C. L. Mallows, and B. W. Stuck. A method for simulating stable random variables. Journal of the American Statistical Association, 71(354):340–344, 1976.</p>
<p>[4] Laura Feinstein, Dan Schnackenberg, Ravindra Balupari, and Darrell Kindred. Statistical approaches to DDoS attack detection and response. In DARPA Information Survivability Conference and Exposition, pages 303–314, 2003.</p>
<p>[5] Anupam Gupta, John D. Lafferty, Han Liu, Larry A. Wasserman, and Min Xu. Forest density estimation. In COLT, pages 394–406, Haifa, Israel, 2010.</p>
<p>[6] Nicholas J. A. Harvey, Jelani Nelson, and Krzysztof Onak. Streaming algorithms for estimating entropy. In ITW, 2008.</p>
<p>[7] Piotr Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream computation. Journal of ACM, 53(3):307–323, 2006.</p>
<p>[8] Anukool Lakhina, Mark Crovella, and Christophe Diot. Mining anomalies using trafﬁc feature distributions. In SIGCOMM, pages 217–228, Philadelphia, PA, 2005.</p>
<p>[9] Ashwin Lall, Vyas Sekar, Mitsunori Ogihara, Jun Xu, and Hui Zhang. Data streaming algorithms for estimating entropy of network trafﬁc. In SIGMETRICS, pages 145–156, Saint Malo, France, 2006.</p>
<p>[10] Ping Li. Estimators and tail bounds for dimension reduction in lα (0 < α ≤ 2) using stable random projections. In SODA, pages 10 – 19, San Francisco, CA, 2008.</p>
<p>[11] Ping Li. Compressed counting. In SODA, New York, NY, 2009.</p>
<p>[12] Ping Li and Trevor J. Hastie. A uniﬁed near-optimal estimator for dimension reduction in lα (0 < α ≤ 2) using stable random projections. In NIPS, Vancouver, BC, Canada, 2007.</p>
<p>[13] Ping Li and Cun-Hui Zhang. A new algorithm for compressed counting with applications in shannon entropy estimation in dynamic data. In COLT, 2011.</p>
<p>[14] Qiaozhu Mei and Kenneth Church. Entropy of search logs: How hard is search? with personalization? with backoff? In WSDM, pages 45 – 54, Palo Alto, CA, 2008.</p>
<p>[15] S. Muthukrishnan. Data streams: Algorithms and applications. Foundations and Trends in Theoretical Computer Science, 1:117–236, 2 2005.</p>
<p>[16] Noam Nisan. Pseudorandom generators for space-bounded computations. In Proceedings of the twentysecond annual ACM symposium on Theory of computing, STOC, pages 204–212, 1990.</p>
<p>[17] Liam Paninski. Estimation of entropy and mutual information. Neural Comput., 15(6):1191–1253, 2003.</p>
<p>[18] Constantino Tsallis. Possible generalization of boltzmann-gibbs statistics. Journal of Statistical Physics, 52:479–487, 1988.</p>
<p>[19] Kuai Xu, Zhi-Li Zhang, and Supratik Bhattacharyya. Proﬁling internet backbone trafﬁc: behavior models and applications. In SIGCOMM ’05: Proceedings of the 2005 conference on Applications, technologies, architectures, and protocols for computer communications, pages 169–180, Philadelphia, Pennsylvania, USA, 2005.</p>
<p>[20] Qiang Yang and Xindong Wu. 10 challeng problems in data mining research. International Journal of Information Technology and Decision Making, 5(4):597–604, 2006.</p>
<p>[21] Haiquan Zhao, Ashwin Lall, Mitsunori Ogihara, Oliver Spatscheck, Jia Wang, and Jun Xu. A data streaming algorithm for estimating entropies of od ﬂows. In IMC, San Diego, CA, 2007.</p>
<p>[22] Vladimir M. Zolotarev. One-dimensional Stable Distributions. American Mathematical Society, Providence, RI, 1986.  9</p>
<br/>
<br/><br/><br/></body>
</html>
