<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>123 nips-2012-Exponential Concentration for Mutual Information Estimation with Application to Forests</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-123" href="../nips2012/nips-2012-Exponential_Concentration_for_Mutual_Information_Estimation_with_Application_to_Forests.html">nips2012-123</a> <a title="nips-2012-123-reference" href="#">nips2012-123-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>123 nips-2012-Exponential Concentration for Mutual Information Estimation with Application to Forests</h1>
<br/><p>Source: <a title="nips-2012-123-pdf" href="http://papers.nips.cc/paper/4768-exponential-concentration-for-mutual-information-estimation-with-application-to-forests.pdf">pdf</a></p><p>Author: Han Liu, Larry Wasserman, John D. Lafferty</p><p>Abstract: We prove a new exponential concentration inequality for a plug-in estimator of the Shannon mutual information. Previous results on mutual information estimation only bounded expected error. The advantage of having the exponential inequality is that, combined with the union bound, we can guarantee accurate estimators of the mutual information for many pairs of random variables simultaneously. As an application, we show how to use such a result to optimally estimate the density function and graph of a distribution which is Markov to a forest graph. 1</p><br/>
<h2>reference text</h2><p>[1] Ibrahim A. Ahmad and Pi-Erh Lin. A nonparametric estimation of the entropy for absolutely continuous distributions (corresp.). IEEE Transactions on Information Theory, 22(3):372–375, 1976.</p>
<p>[2] J Beirlant, E J Dudewicz, L Gy¨ rﬁ, and E C Van Der Meulen. Nonparametric entropy estimation: An o overview. International Journal of Mathematical and Statistical Sciences, 6(1):17–39, 1997.</p>
<p>[3] C. Chow and C. Liu. Approximating discrete probability distributions with dependence trees. Information Theory, IEEE Transactions on, 14(3):462–467, 1968.</p>
<p>[4] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. Wiley, 1991.</p>
<p>[5] Paul P. B. Eggermont and Vincent N. LaRiccia. Best asymptotic normality of the kernel density entropy estimator for smooth densities. IEEE Transactions on Information Theory, 45(4):1321–1326, 1999.</p>
<p>[6] A. Gretton, R. Herbrich, and A. J. Smola. The kernel mutual information. In Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP’03). 2003 IEEE International Conference on, volume 4, pages IV–880. IEEE, 2003.</p>
<p>[7] Peter Hall and Sally Morton. On the estimation of entropy. Annals of the Institute of Statistical Mathematics, 45(1):69–88, 1993.</p>
<p>[8] A. O. Hero III, B. Ma, O. J. J. Michel, and J. Gorman. Applications of entropic spanning graphs. Signal Processing Magazine, IEEE, 19(5):85–95, 2002.</p>
<p>[9] Harry Joe. Estimation of entropy and other functionals of a multivariate density. Annals of the Institute of Statistical Mathematics, 41(4):683–697, December 1989.</p>
<p>[10] M. C. Jones, M. C. Linton, and J. P. Nielsen. A simple bias reduction method for density estimation. Biometrika, 82(2):327–338, 1995.</p>
<p>[11] Shiraj Khan, Sharba Bandyopadhyay, Auroop R. Ganguly, Sunil Saigal, David J. Erickson, Vladimir Protopopescu, and George Ostrouchov. Relative performance of mutual information estimation methods for quantifying the dependence among short and noisy data. Phys. Rev. E, 76:026209, Aug 2007.</p>
<p>[12] Alexander Kraskov, Harald St¨ gbauer, and Peter Grassberger. Estimating mutual information. Physical o review. E, Statistical, nonlinear, and soft matter physics, 69(6 Pt 2), June 2004.</p>
<p>[13] Joseph B. Kruskal. On the shortest spanning subtree of a graph and the traveling salesman problem. Proceedings of the American Mathematical Society, 7(1):48–50, 1956.</p>
<p>[14] Han Liu, John Lafferty, and Larry Wasserman. Optimal forest density estimation. Technical Report, 2012.</p>
<p>[15] Han Liu, Min Xu, Haijie Gu, Anupam Gupta, John D. Lafferty, and Larry A. Wasserman. Forest density estimation. Journal of Machine Learning Research, 12:907–951, 2011.</p>
<p>[16] C. McDiarmid. On the method of bounded differences. In Surveys in Combinatorics, number 141 in London Mathematical Society Lecture Note Series, pages 148–188. Cambridge University Press, August 1989.</p>
<p>[17] XuanLong Nguyen, Martin J. Wainwright, and Michael I. Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847–5861, 2010.</p>
<p>[18] D. P´ l, B. P´ czos, and C. Szepesv´ ri. Estimation of R´ nyi entropy and mutual information based on a o a e generalized nearest-neighbor graphs. Arxiv preprint arXiv:1003.1954, 2010.</p>
<p>[19] L. Paninski. Estimation of entropy and mutual information. Neural Computation, 15(6):1191–1253, 2003.</p>
<p>[20] Liam Paninski and Masanao Yajima. Undersmoothed kernel entropy estimators. IEEE Transactions on Information Theory, 54(9):4384–4388, 2008.</p>
<p>[21] Barnab´ s P´ czos and Jeff G. Schneider. Nonparametric estimation of conditional information and divera o gences. Journal of Machine Learning Research - Proceedings Track, 22:914–923, 2012.</p>
<p>[22] B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chapman and Hall. New York, NY, 1986.</p>
<p>[23] A.B. Tsybakov and van den Meulen. Root-n Consistent Estimators of Entropy for Densities with Unbounded Support, volume 23. Universite catholique de Louvain,Institut de statistique, 1994.</p>
<p>[24] Marc M. Van Hulle. Edgeworth approximation of multivariate differential entropy. Neural Comput., 17(9):1903–1910, September 2005.</p>
<p>[25] O Vasicek. A test for normality based on sample entropy. Journal of the Royal Statistical Society Series B, 38(1):54–59, 1976.</p>
<p>[26] Ven Es Bert. Estimating functionals related to a density by a class of statistics based on spacings. Scandinavian Journal of Statistics, 19(1):61–72, 1992.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
