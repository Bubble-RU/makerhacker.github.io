<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>95 nips-2012-Density-Difference Estimation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-95" href="../nips2012/nips-2012-Density-Difference_Estimation.html">nips2012-95</a> <a title="nips-2012-95-reference" href="#">nips2012-95-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>95 nips-2012-Density-Difference Estimation</h1>
<br/><p>Source: <a title="nips-2012-95-pdf" href="http://papers.nips.cc/paper/4641-density-difference-estimation.pdf">pdf</a></p><p>Author: Masashi Sugiyama, Takafumi Kanamori, Taiji Suzuki, Marthinus D. Plessis, Song Liu, Ichiro Takeuchi</p><p>Abstract: We address the problem of estimating the difference between two probability densities. A naive approach is a two-step procedure of ﬁrst estimating two densities separately and then computing their difference. However, such a two-step procedure does not necessarily work well because the ﬁrst step is performed without regard to the second step and thus a small estimation error incurred in the ﬁrst stage can cause a big error in the second stage. In this paper, we propose a single-shot procedure for directly estimating the density difference without separately estimating two densities. We derive a non-parametric ﬁnite-sample error bound for the proposed single-shot density-difference estimator and show that it achieves the optimal convergence rate. We then show how the proposed density-difference estimator can be utilized in L2 -distance approximation. Finally, we experimentally demonstrate the usefulness of the proposed method in robust distribution comparison such as class-prior estimation and change-point detection.</p><br/>
<h2>reference text</h2><p>[1] V. N. Vapnik. Statistical Learning Theory. Wiley, New York, NY, USA, 1998.</p>
<p>[2] M. Sugiyama, T. Suzuki, S. Nakajima, H. Kashima, P. von B¨ nau, and M. Kawanabe. Diu rect importance estimation for covariate shift adaptation. Annals of the Institute of Statistical Mathematics, 60(4):699–746, 2008.</p>
<p>[3] X. Nguyen, M. J. Wainwright, and M. I. Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847–5861, 2010.</p>
<p>[4] C. Cortes, Y. Mansour, and M. Mohri. Learning bounds for importance weighting. In Advances in Neural Information Processing Systems 23, pages 442–450, 2010.</p>
<p>[5] M. Saerens, P. Latinne, and C. Decaestecker. Adjusting the outputs of a classiﬁer to new a priori probabilities: A simple procedure. Neural Computation, 14(1):21–41, 2002.</p>
<p>[6] Y. Kawahara and M. Sugiyama. Sequential change-point detection based on direct densityratio estimation. Statistical Analysis and Data Mining, 5(2):114–127, 2012.</p>
<p>[7] N. Anderson, P. Hall, and D. Titterington. Two-sample test statistics for measuring discrepancies between two multivariate probability density functions using kernel-based density estimates. Journal of Multivariate Analysis, 50(1):41–54, 1994.</p>
<p>[8] A. Basu, I. R. Harris, N. L. Hjort, and M. C. Jones. Robust and efﬁcient estimation by minimising a density power divergence. Biometrika, 85(3):549–559, 1998.</p>
<p>[9] P. Hall and M. P. Wand. On nonparametric discrimination using density differences. Biometrika, 75(3):541–547, 1988.</p>
<p>[10] M. Eberts and I. Steinwart. Optimal learning rates for least squares SVMs using Gaussian kernels. In Advances in Neural Information Processing Systems 24, pages 1539–1547, 2011.</p>
<p>[11] R. H. Farrell. On the best obtainable asymptotic rates of convergence in estimation of a density function at a point. The Annals of Mathematical Statistics, 43(1):170–180, 1972.</p>
<p>[12] B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chapman and Hall, London, UK, 1986.</p>
<p>[13] E. Parzen. On the estimation of a probability density function and mode. The Annals of Mathematical Statistics, 33(3):1065–1076, 1962.</p>
<p>[14] R. T. Rockafellar. Convex Analysis. Princeton University Press, Princeton, NJ, USA, 1970.</p>
<p>[15] W. H¨ rdle, M. M¨ ller, S. Sperlich, and A. Werwatz. Nonparametric and Semiparametric a u Models. Springer, Berlin, Germany, 2004.</p>
<p>[16] O. Chapelle, B. Sch¨ lkopf, and A. Zien, editors. Semi-Supervised Learning. MIT Press, o Cambridge, MA, USA, 2006.</p>
<p>[17] R. Rifkin, G. Yeo, and T. Poggio. Regularized least-squares classiﬁcation. In Advances in Learning Theory: Methods, Models and Applications, pages 131–154. IOS Press, Amsterdam, the Netherlands, 2003.</p>
<p>[18] M. Sugiyama, M. Krauledat, and K.-R. M¨ ller. Covariate shift adaptation by importance u weighted cross validation. Journal of Machine Learning Research, 8:985–1005, May 2007.</p>
<p>[19] Y. Takeuchi and K. Yamanishi. A unifying framework for detecting outliers and change points from non-stationary time series data. IEEE Transactions on Knowledge and Data Engineering, 18(4):482–489, 2006.</p>
<p>[20] Y. Kawahara, T. Yairi, and K. Machida. Change-point detection in time-series data based on subspace identiﬁcation. In Proceedings of the 7th IEEE International Conference on Data Mining, pages 559–564, 2007.</p>
<p>[21] V. Moskvina and A. A. Zhigljavsky. An algorithm based on singular spectrum analysis for change-point detection. Communication in Statistics: Simulation & Computation, 32(2):319– 352, 2003.</p>
<p>[22] F. Desobry, M. Davy, and C. Doncarli. An online kernel change detection algorithm. IEEE Transactions on Signal Processing, 53(8):2961–2974, 2005.</p>
<p>[23] Z. Harchaoui, F. Bach, and E. Moulines. Kernel change-point analysis. In Advances in Neural Information Processing Systems 21, pages 609–616, 2009.</p>
<p>[24] S. Arlot, A. Celisse, and Z. Harchaoui. Kernel change-point detection. Technical Report 1202.3878, arXiv, 2012.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
