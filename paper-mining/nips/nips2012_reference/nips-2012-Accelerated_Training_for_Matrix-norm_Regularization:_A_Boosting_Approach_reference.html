<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>29 nips-2012-Accelerated Training for Matrix-norm Regularization: A Boosting Approach</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-29" href="../nips2012/nips-2012-Accelerated_Training_for_Matrix-norm_Regularization%3A_A_Boosting_Approach.html">nips2012-29</a> <a title="nips-2012-29-reference" href="#">nips2012-29-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>29 nips-2012-Accelerated Training for Matrix-norm Regularization: A Boosting Approach</h1>
<br/><p>Source: <a title="nips-2012-29-pdf" href="http://papers.nips.cc/paper/4663-accelerated-training-for-matrix-norm-regularization-a-boosting-approach.pdf">pdf</a></p><p>Author: Xinhua Zhang, Dale Schuurmans, Yao-liang Yu</p><p>Abstract: Sparse learning models typically combine a smooth loss with a nonsmooth penalty, such as trace norm. Although recent developments in sparse approximation have offered promising solution methods, current approaches either apply only to matrix-norm constrained problems or provide suboptimal convergence rates. In this paper, we propose a boosting method for regularized learning that guarantees accuracy within O(1/ ) iterations. Performance is further accelerated by interlacing boosting with ﬁxed-rank local optimization—exploiting a simpler local objective than previous work. The proposed method yields state-of-the-art performance on large-scale problems. We also demonstrate an application to latent multiview learning for which we provide the ﬁrst efﬁcient weak-oracle. 1</p><br/>
<h2>reference text</h2><p>[1] F. Bach, J. Mairal, and J. Ponce. Convex sparse matrix factorizations. arXiv:0812.1869v1, 2008.</p>
<p>[2] H. Lee, R. Raina, A. Teichman, and A. Ng. Exponential family sparse coding with application to selftaught learning. In IJCAI, 2009.</p>
<p>[3] D. Bradley and J. Bagnell. Convex coding. In UAI, 2009.</p>
<p>[4] X. Zhang, Y-L Yu, M. White, R. Huang, and D. Schuurmans. Convex sparse coding, subspace learning, and semi-supervised extensions. In AAAI, 2011.</p>
<p>[5] T. K. Pong, P. Tseng, S. Ji, and J. Ye. Trace norm regularization: Reformulations, algorithms, and multitask learning. SIAM Journal on Optimization, 20(6):3465–3489, 2010.</p>
<p>[6] K-C Toh and S. Yun. An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems. Paciﬁc Journal of Optimization, 6:615–640, 2010.</p>
<p>[7] J-F Cai, E. J. Cand´ s, and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM e Journal on Optimization, 20(4):1956–1982, 2010.</p>
<p>[8] M. Jaggi and M. Sulovsky. A simple algorithm for nuclear norm regularized problems. In ICML, 2010.</p>
<p>[9] E. Hazan. Sparse approximate solutions to semideﬁnite programs. In LATIN, 2008.</p>
<p>[10] K. L. Clarkson. Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm. In SODA, 2008.</p>
<p>[11] A. Tewari, P. Ravikumar, and I. S. Dhillon. Greedy algorithms for structurally constrained high dimensional problems. In NIPS, 2011.</p>
<p>[12] V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex geometry of linear inverse problems. Foundations of Computational Mathematics, 12(6):805–849, 2012.</p>
<p>[13] Y. Bengio, N.L. Roux, P. Vincent, O. Delalleau, and P. Marcotte. Convex neural networks. In NIPS, 2005.</p>
<p>[14] L. Mason, J. Baxter, P. L. Bartlett, and M. Frean. Functional gradient techniques for combining hypotheses. In Advances in Large Margin Classiﬁers, pages 221–246, Cambridge, MA, 2000. MIT Press.</p>
<p>[15] M. Dudik, Z. Harchaoui, and J. Malick. Lifted coordinate descent for learning with trace-norm regularizations. In AISTATS, 2012.</p>
<p>[16] S. Shalev-Shwartz, N. Srebro, and T. Zhang. Trading accuracy for sparsity in optimization problems with sparsity constraints. SIAM Journal on Optimization, 20:2807–2832, 2010.</p>
<p>[17] X. Yuan and S. Yan. Forward basis selection for sparse approximation over dictionary. In AISTATS, 2012.</p>
<p>[18] T. Zhang. Sequential greedy approximation for certain convex optimization problems. IEEE Trans. Information Theory, 49(3):682–691, 2003.</p>
<p>[19] S. Burer and R. Monteiro. Local minima and convergence in low-rank semideﬁnite programming. Mathematical Programming, 103(3):427–444, 2005.</p>
<p>[20] M. Journee, F. Bach, P.-A. Absil, and R. Sepulchre. Low-rank optimization on the cone of positive semideﬁnite matrices. SIAM Journal on Optimization, 20:2327C–2351, 2010.</p>
<p>[21] S. Laue. A hybrid algorithm for convex semideﬁnite optimization. In ICML, 2012.</p>
<p>[22] B. Mishra, G. Meyer, F. Bach, and R. Sepulchre. Low-rank optimization with trace norm penalty. Technical report, 2011. http://arxiv.org/abs/1112.2318.</p>
<p>[23] S. Shalev-Shwartz, A. Gonen, and O. Shamir. Large-scale convex minimization with a low-rank constraint. In ICML, 2011.</p>
<p>[24] M. White, Y. Yu, X. Zhang, and D. Schuurmans. Convex multi-view subspace learning. In NIPS, 2012.</p>
<p>[25] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning, 73(3): 243–272, 2008.</p>
<p>[26] J-B Hiriart-Urruty and C. Lemar´ chal. Convex Analysis and Minimization Algorithms, I and II, volume e 305 and 306. Springer-Verlag, 1993.</p>
<p>[27] I. Mukherjee, C. Rudin, and R. Schapire. The rate of convergence of Adaboost. In COLT, 2011.</p>
<p>[28] N. Srebro, J. Rennie, and T. Jaakkola. Maximum-margin matrix factorization. In NIPS, 2005.</p>
<p>[29] C. Hillar and L-H Lim. Most tensor problems are NP-hard. arXiv:0911.1393v3, 2012.</p>
<p>[30] W. Ai and S. Zhang. Strong duality for the CDT subproblem: A necessary and sufﬁcient condition. SIAM Journal on Optimization, 19:1735–1756, 2009.</p>
<p>[31] T.S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y.T. Zhang. A real-world web image database from national university of singapore. In International Conference on Image and Video Retrieval, 2009.</p>
<p>[32] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.</p>
<p>[33] K. Bredies, D. Lorenz, and P. Maass. A generalized conditional gradient method and its connection to an iterative shrinkage method. Computational Optimization and Applications, 42:173–193, 2009.</p>
<p>[34] Y. Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341–362, 2012.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
