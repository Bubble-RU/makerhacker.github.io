<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>263 nips-2012-Optimal Regularized Dual Averaging Methods for Stochastic Optimization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-263" href="../nips2012/nips-2012-Optimal_Regularized_Dual_Averaging_Methods_for_Stochastic_Optimization.html">nips2012-263</a> <a title="nips-2012-263-reference" href="#">nips2012-263-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>263 nips-2012-Optimal Regularized Dual Averaging Methods for Stochastic Optimization</h1>
<br/><p>Source: <a title="nips-2012-263-pdf" href="http://papers.nips.cc/paper/4543-optimal-regularized-dual-averaging-methods-for-stochastic-optimization.pdf">pdf</a></p><p>Author: Xi Chen, Qihang Lin, Javier Pena</p><p>Abstract: This paper considers a wide spectrum of regularized stochastic optimization problems where both the loss function and regularizer can be non-smooth. We develop a novel algorithm based on the regularized dual averaging (RDA) method, that can simultaneously achieve the optimal convergence rates for both convex and strongly convex loss. In particular, for strongly convex loss, it achieves the opti1 1 mal rate of O( N + N 2 ) for N iterations, which improves the rate O( log N ) for preN vious regularized dual averaging algorithms. In addition, our method constructs the ﬁnal solution directly from the proximal mapping instead of averaging of all previous iterates. For widely used sparsity-inducing regularizers (e.g., 1 -norm), it has the advantage of encouraging sparser solutions. We further develop a multistage extension using the proposed algorithm as a subroutine, which achieves the 1 uniformly-optimal rate O( N + exp{−N }) for strongly convex loss. 1</p><br/>
<h2>reference text</h2><p>[1] A. Cotter, O. Shamir, N. Srebro, and K. Sridharan. Better mini-batch algorithms via accelerated gradient methods. In Advances in Neural Information Processing Systems (NIPS), 2011.</p>
<p>[2] O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction using mini-batches. Technical report, Microsoft Research, 2011.</p>
<p>[3] J. Duchi, P. L. Bartlett, and M. Wainwright. Randomized smoothing for stochastic optimization. arXiv:1103.4296v1, 2011.</p>
<p>[4] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. In Conference on Learning Theory (COLT), 2010.</p>
<p>[5] J. Duchi, S. Shalev-Shwartz, Y. Singer, and A. Tewari. Composite objective mirror descent. In Conference on Learning Theory (COLT), 2010.</p>
<p>[6] J. Duchi and Y. Singer. Efﬁcient online and batch learning using forward-backward splitting. Journal of Machine Learning Research, 10:2873–2898, 2009.</p>
<p>[7] E. Hazan and S. Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization. In Conference on Learning Theory (COLT), 2011.</p>
<p>[8] C. Hu, J. T. Kwok, and W. Pan. Accelerated gradient methods for stochastic optimization and online learning. In Advances in Neural Information Processing Systems (NIPS), 2009.</p>
<p>[9] A. Juditsky and Y. Nesterov. Primal-dual subgradient methods for minimizing uniformly convex functions. August 2010.</p>
<p>[10] G. Lan and S. Ghadimi. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization, part i: a generic algorithmic framework. Technical report, University of Florida, 2010.</p>
<p>[11] G. Lan and S. Ghadimi. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization, part ii: shrinking procedures and optimal algorithms. Technical report, University of Florida, 2010.</p>
<p>[12] J. Langford, L. Li, and T. Zhang. Sparse online learning via truncated gradient. Journal of Machine Learning Research, 10:777–801, 2009.</p>
<p>[13] S. Lee and S. J. Wright. Manifold identiﬁcation of dual averaging methods for regularized stochastic online learning. In International Conference on Machine Learning (ICML), 2011.</p>
<p>[14] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.</p>
<p>[15] A. Nemirovski and D. Yudin. Problem complexity and method efﬁciency in optimization. John Wiley New York, 1983.</p>
<p>[16] Y. Nesterov. Introductory lectures on convex optimization: a basic course. Kluwer Academic Pub, 2003.</p>
<p>[17] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Programming, 120:221–259, 2009.</p>
<p>[18] A. Rakhlin, O. Shamir, and K. Sridharan. To average or not to average? making stochastic gradient descent optimal for strongly convex problems. In International Conference on Machine Learning (ICML), 2012.</p>
<p>[19] R. Tibshirani. Regression shrinkage and selection via the lasso. J.R.Statist.Soc.B, 58:267–288, 1996.</p>
<p>[20] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. SIAM Journal on Optimization (Submitted), 2008.</p>
<p>[21] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of Machine Learning Research, 11:2543–2596, 2010.</p>
<p>[22] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. J. R. Statist. Soc. B, 67(2):301–320, 2005.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
