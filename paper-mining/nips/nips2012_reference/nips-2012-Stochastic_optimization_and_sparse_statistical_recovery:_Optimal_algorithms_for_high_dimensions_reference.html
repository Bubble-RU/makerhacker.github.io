<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>325 nips-2012-Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-325" href="../nips2012/nips-2012-Stochastic_optimization_and_sparse_statistical_recovery%3A_Optimal_algorithms_for_high_dimensions.html">nips2012-325</a> <a title="nips-2012-325-reference" href="#">nips2012-325-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>325 nips-2012-Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions</h1>
<br/><p>Source: <a title="nips-2012-325-pdf" href="http://papers.nips.cc/paper/4645-stochastic-optimization-and-sparse-statistical-recovery-optimal-algorithms-for-high-dimensions.pdf">pdf</a></p><p>Author: Alekh Agarwal, Sahand Negahban, Martin J. Wainwright</p><p>Abstract: We develop and analyze stochastic optimization algorithms for problems in which the expected loss is strongly convex, and the optimum is (approximately) sparse. Previous approaches are able to exploit only one of these two structures, yielding a O(d/T ) convergence rate for strongly convex objectives in d dimensions and O( s(log d)/T ) convergence rate when the optimum is s-sparse. Our algorithm is based on successively solving a series of ℓ1 -regularized optimization problems using Nesterov’s dual averaging algorithm. We establish that the error of our solution after T iterations is at most O(s(log d)/T ), with natural extensions to approximate sparsity. Our results apply to locally Lipschitz losses including the logistic, exponential, hinge and least-squares losses. By recourse to statistical minimax results, we show that our convergence rates are optimal up to constants. The effectiveness of our approach is also conﬁrmed in numerical simulations where we compare to several baselines on a least-squares regression problem.</p><br/>
<h2>reference text</h2><p>[1] A. Agarwal, S. N. Negahban, and M. J. Wainwright. Fast global convergence rates of gradient methods for high-dimensional statistical recovery. To appear in The Annals of Statistics, 2012. Full-length version http://arxiv.org/pdf/1104.4824v2.</p>
<p>[2] A. Agarwal, S. N. Negahban, and M. J. Wainwright. Stochastic optimization and sparse statistical recovery: An optimal algorithm for high dimensions. 2012. URL http://arxiv.org/abs/1207.4421.</p>
<p>[3] P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Ann. Stat., 37(4):1705–1732, 2009.</p>
<p>[4] L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In NIPS, 2007.</p>
<p>[5] P. B¨ hlmann and S. Van De Geer. Statistics for High-Dimensional Data: Methods, Theory and Applicau tions. Springer Series in Statistics. Springer, 2011.</p>
<p>[6] D. L. Donoho. High-dimensional data analysis: The curses and blessings of dimensionality, 2000.</p>
<p>[7] J. Duchi, S. Shalev-Shwartz, Y. Singer, and A. Tewari. Composite objective mirror descent. In Proceedings of the 23rd Annual Conference on Learning Theory, pages 14–26. Omnipress, 2010.</p>
<p>[8] J. Duchi and Y. Singer. Efﬁcient online and batch learning using forward-backward splitting. Journal of Machine Learning Research, 10:2873–2898, 2009.</p>
<p>[9] E. Hazan, A. Kalai, S. Kale, and A. Agarwal. Logarithmic regret algorithms for online convex optimization. In Proceedings of the Nineteenth Annual Conference on Computational Learning Theory, 2006.</p>
<p>[10] E. Hazan and S. Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization. Journal of Machine Learning Research - Proceedings Track, 19:421–436, 2011.</p>
<p>[11] A. Juditsky and Y. Nesterov. Primal-dual subgradient methods for minimizing uniformly convex functions. Available online http://hal.archives-ouvertes.fr/docs/00/50/89/33/PDF/Strong-hal.pdf, 2010.</p>
<p>[12] G. Lan and S. Ghadimi. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization, part ii: shrinking procedures and optimal algorithms. 2010.</p>
<p>[13] S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A uniﬁed framework for high-dimensional analysis of M-estimators with decomposable regularizers. In NIPS Conference, Vancouver, Canada, December 2009. Full length version arxiv:1010.2731v1.</p>
<p>[14] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.</p>
<p>[15] A. Nemirovski and D. Yudin. Problem Complexity and Method Efﬁciency in Optimization. Wiley, New York, 1983.</p>
<p>[16] Y. Nesterov. Gradient methods for minimizing composite objective function. Technical Report 76, Center for Operations Research and Econometrics (CORE), Catholic University of Louvain (UCL), 2007.</p>
<p>[17] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Programming A, 120(1):261–283, 2009.</p>
<p>[18] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for SVM. In Proceedings of the 24th International Conference on Machine Learning, 2007.</p>
<p>[19] S. Shalev-Shwartz and A. Tewari. Stochastic methods for l1 regularized loss minimization. Journal of Machine Learning Research, 12:1865–1892, June 2011.</p>
<p>[20] N. Srebro, K. Sridharan, and A. Tewari. Smoothness, low noise, and fast rates. In Advances in Neural Information Processing Systems 23, pages 2199–2207, 2010.</p>
<p>[21] S. A. van de Geer. High-dimensional generalized linear models and the lasso. The Annals of Statistics, 36:614–645, 2008.</p>
<p>[22] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of Machine Learning Research, 11:2543–2596, 2010.</p>
<p>[23] L. Xiao and T. Zhang. A proximal-gradient homotopy method for the sparse least-squares problem. ICML, 2012. URL http://arxiv.org/abs/1203.3002.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
