<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>216 nips-2012-Mirror Descent Meets Fixed Share (and feels no regret)</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-216" href="../nips2012/nips-2012-Mirror_Descent_Meets_Fixed_Share_%28and_feels_no_regret%29.html">nips2012-216</a> <a title="nips-2012-216-reference" href="#">nips2012-216-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>216 nips-2012-Mirror Descent Meets Fixed Share (and feels no regret)</h1>
<br/><p>Source: <a title="nips-2012-216-pdf" href="http://papers.nips.cc/paper/4664-mirror-descent-meets-fixed-share-and-feels-no-regret.pdf">pdf</a></p><p>Author: Nicolò Cesa-bianchi, Pierre Gaillard, Gabor Lugosi, Gilles Stoltz</p><p>Abstract: Mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension. This is done using either a carefully designed projection or by a weight sharing technique. Via a novel uniﬁed analysis, we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting, adaptive, discounted, and other related regrets. Our analysis also captures and extends the generalized weight sharing technique of Bousquet and Warmuth, and can be reﬁned in several ways, including improvements for small losses and adaptive tuning of parameters. 1</p><br/>
<h2>reference text</h2><p>[1] M. Herbster and M. Warmuth. Tracking the best linear predictor. Journal of Machine Learning Research, 1:281–309, 2001.</p>
<p>[2] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning, ICML 2003, 2003.</p>
<p>[3] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press, 2006.</p>
<p>[4] M. Herbster and M. Warmuth. Tracking the best expert. Machine Learning, 32:151–178, 1998.</p>
<p>[5] V. Vovk. Derandomizing stochastic prediction strategies. Machine Learning, 35(3):247–282, Jun. 1999.</p>
<p>[6] O. Bousquet and M.K. Warmuth. Tracking a small set of experts by mixing past posteriors. Journal of Machine Learning Research, 3:363–396, 2002.</p>
<p>[7] A. György, T. Linder, and G. Lugosi. Tracking the best of many experts. In Proceedings of the 18th Annual Conference on Learning Theory (COLT), pages 204–216, Bertinoro, Italy, Jun. 2005. Springer.</p>
<p>[8] E. Hazan and C. Seshadhri. Efﬁcient learning algorithms for changing environments. Proceedings of the 26th International Conference of Machine Learning (ICML), 2009.</p>
<p>[9] A. Chernov and F. Zhdanov. Prediction with expert advice under discounted loss. In Proceedings of the 21st International Conference on Algorithmic Learning Theory, ALT 2010, pages 255–269. Springer, 2008.</p>
<p>[10] A. Blum and Y. Mansour. From extermal to internal regret. Journal of Machine Learning Research, 8:1307–1324, 2007.</p>
<p>[11] P. Auer, N. Cesa-Bianchi, and C. Gentile. Adaptive and self-conﬁdent on-line learning algorithms. Journal of Computer and System Sciences, 64:48–75, 2002.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
