<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>259 nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-259" href="../nips2012/nips-2012-Online_Regret_Bounds_for_Undiscounted_Continuous_Reinforcement_Learning.html">nips2012-259</a> <a title="nips-2012-259-reference" href="#">nips2012-259-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>259 nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2012-259-pdf" href="http://papers.nips.cc/paper/4666-online-regret-bounds-for-undiscounted-continuous-reinforcement-learning.pdf">pdf</a></p><p>Author: Ronald Ortner, Daniil Ryabko</p><p>Abstract: We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper conﬁdence bounds for implementing optimism in the face of uncertainty. Beside the existence of an optimal policy which satisﬁes the Poisson equation, the only assumptions made are H¨ lder continuity of rewards and transition o probabilities. 1</p><br/>
<h2>reference text</h2><p>[1] Yasin Abbasi-Yadkori and Csaba Szepesv´ ri. Regret bounds for the adaptive control of linear quadratic a systems. COLT 2011, JMLR Proceedings Track, 19:1–26, 2011.</p>
<p>[2] Peter Auer, Nicol` Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multi-armed bandit probo lem. Mach. Learn., 47:235–256, 2002.</p>
<p>[3] Peter Auer, Nicol` Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed o bandit problem. SIAM J. Comput., 32:48–77, 2002.</p>
<p>[4] Peter Auer, Ronald Ortner, and Csaba Szepesv´ ri. Improved rates for the stochastic continuum-armed a bandit problem. In Learning Theory, 20th Annual Conference on Learning Theory, COLT 2007, pages 454–468, 2007.</p>
<p>[5] Peter L. Bartlett and Ambuj Tewari. REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs. In Proc. 25th Conference on Uncertainty in Artiﬁcial Intelligence, UAI 2009, pages 25–42, 2009.</p>
<p>[6] Andrey Bernstein and Nahum Shimkin. Adaptive-resolution reinforcement learning with polynomial exploration in deterministic domains. Mach. Learn., 81(3):359–397, 2010.</p>
<p>[7] Emma Brunskill, Bethany R. Lefﬂer, Lihong Li, Michael L. Littman, and Nicholas Roy. Provably efﬁcient learning with typed parametric models. J. Mach. Learn. Res., 10:1955–1988, 2009.</p>
<p>[8] S´ bastien Bubeck, R´ mi Munos, Gilles Stoltz, and Csaba Szepesv´ ri. Online optimization of χ-armed e e a bandits. In Advances in Neural Information Processing Systems 22, NIPS 2009, pages 201–208, 2010.</p>
<p>[9] On´ simo Hern´ ndez-Lerma and Jean Bernard Lasserre. Discrete-time Markov control processes, vole a ume 30 of Applications of mathematics. Springer, 1996.</p>
<p>[10] On´ simo Hern´ ndez-Lerma and Jean Bernard Lasserre. Further topics on discrete-time Markov control e a processes, volume 42 of Applications of mathematics. Springer, 1999.</p>
<p>[11] Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. J. Mach. Learn. Res., 11:1563–1600, 2010.</p>
<p>[12] Nicholas K. Jong and Peter Stone. Model-based exploration in continuous state spaces. In Abstraction, Reformulation, and Approximation, 7th International Symposium, SARA 2007, pages 258–272. Springer, 2007.</p>
<p>[13] Sham Kakade, Michael J. Kearns, and John Langford. Exploration in metric state spaces. In Machine Learning, Proc. 20th International Conference, ICML 2003, pages 306–312, 2003.</p>
<p>[14] Michael J. Kearns and Satinder P. Singh. Near-optimal reinforcement learning in polynomial time. Mach. Learn., 49:209–232, 2002.</p>
<p>[15] Robert Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In Advances Neural Information Processing Systems 17, NIPS 2004, pages 697–704, 2005.</p>
<p>[16] Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Multi-armed bandits in metric spaces. In Proc. 40th Annual ACM Symposium on Theory of Computing, STOC 2008, pages 681–690, 2008.</p>
<p>[17] Odalric-Ambrym Maillard, R´ mi Munos, and Daniil Ryabko. Selecting the state-representation in reine forcement learning. In Advances Neural Processing Systems 24, NIPS 2011, pages 2627–2635, 2012.</p>
<p>[18] Odalric-Ambrym Maillard, Phuong Nguyen, Ronald Ortner, and Daniil Ryabko. Optimal regret bounds for selecting the state representation in reinforcement learning. accepted for ICML 2013.</p>
<p>[19] Gerhard Neumann, Michael Pfeiffer, and Wolfgang Maass. Efﬁcient continuous-time reinforcement learning with adaptive state graphs. In Machine Learning: ECML 2007, 18th European Conference on Machine Learning, pages 250–261, 2007.</p>
<p>[20] Ali Nouri and Michael L. Littman. Multi-resolution exploration in continuous spaces. In Advances in Neural Information Processing Systems 21, NIPS 2008, pages 1209–1216, 2009.</p>
<p>[21] Ronald Ortner. Adaptive aggregation for reinforcement learning in average reward Markov decision processes. Ann. Oper. Res., 2012. doi:10.1007/s10479-12-1064-y, to appear.</p>
<p>[22] Ronald Ortner, Daniil Ryabko, Peter Auer, and R´ mi Munos. Regret bounds for restless Markov bandits. e In Proc. 23rd Conference on Algorithmic Learning Theory, ALT 2012, pages 214–228, 2012.</p>
<p>[23] Alexander L. Strehl and Michael L. Littman. Online linear regression and its application to model-based reinforcement learning. In Advances Neural Information Processing Systems 20, NIPS 2007, pages 1417– 1424, 2008.</p>
<p>[24] William T. B. Uther and Manuela M. Veloso. Tree based discretization for continuous state space reinforcement learning. In Proc. 15th National Conference on Artiﬁcial Intelligence and 10th Innovative Applications of Artiﬁcial Intelligence Conference, AAAI 98, IAAI 98, pages 769–774, 1998.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
