<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>12 nips-2012-A Neural Autoregressive Topic Model</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-12" href="../nips2012/nips-2012-A_Neural_Autoregressive_Topic_Model.html">nips2012-12</a> <a title="nips-2012-12-reference" href="#">nips2012-12-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>12 nips-2012-A Neural Autoregressive Topic Model</h1>
<br/><p>Source: <a title="nips-2012-12-pdf" href="http://papers.nips.cc/paper/4613-a-neural-autoregressive-topic-model.pdf">pdf</a></p><p>Author: Hugo Larochelle, Stanislas Lauly</p><p>Abstract: We describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents. This model is inspired by the recently proposed Replicated Softmax, an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations. Speciﬁcally, we take inspiration from the conditional mean-ﬁeld recursive equations of the Replicated Softmax in order to deﬁne a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words. This paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words. The end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the Replicated Softmax. Our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm. 1</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/></body>
</html>
