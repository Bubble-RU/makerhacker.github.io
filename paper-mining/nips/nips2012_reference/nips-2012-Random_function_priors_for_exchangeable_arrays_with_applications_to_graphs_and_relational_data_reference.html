<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>287 nips-2012-Random function priors for exchangeable arrays with applications to graphs and relational data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-287" href="../nips2012/nips-2012-Random_function_priors_for_exchangeable_arrays_with_applications_to_graphs_and_relational_data.html">nips2012-287</a> <a title="nips-2012-287-reference" href="#">nips2012-287-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>287 nips-2012-Random function priors for exchangeable arrays with applications to graphs and relational data</h1>
<br/><p>Source: <a title="nips-2012-287-pdf" href="http://papers.nips.cc/paper/4572-random-function-priors-for-exchangeable-arrays-with-applications-to-graphs-and-relational-data.pdf">pdf</a></p><p>Author: James Lloyd, Peter Orbanz, Zoubin Ghahramani, Daniel M. Roy</p><p>Abstract: A fundamental problem in the analysis of structured relational data like graphs, networks, databases, and matrices is to extract a summary of the common structure underlying relations between individual entities. Relational data are typically encoded in the form of arrays; invariance to the ordering of rows and columns corresponds to exchangeable arrays. Results in probability theory due to Aldous, Hoover and Kallenberg show that exchangeable arrays can be represented in terms of a random measurable function which constitutes the natural model parameter in a Bayesian model. We obtain a ﬂexible yet simple Bayesian nonparametric model by placing a Gaussian process prior on the parameter function. Efﬁcient inference utilises elliptical slice sampling combined with a random sparse approximation to the Gaussian process. We demonstrate applications of the model to network data and clarify its relation to models in the literature, several of which emerge as special cases. 1</p><br/>
<h2>reference text</h2><p>[1] Airoldi, E. M., Blei, D. M., Fienberg, S. E., and Xing, E. P. (2008). Mixed Membership Stochastic Blockmodels. Journal of Machine Learning Research (JMLR), 9, 1981–2014.</p>
<p>[2] Aldous, D. J. (1981). Representations for partially exchangeable arrays of random variables. Journal of Multivariate Analysis, 11(4), 581–598.</p>
<p>[3] Aldous, D. J. (2010). More uses of exchangeability: Representations of complex random structures. In Probability and Mathematical Genetics: Papers in Honour of Sir John Kingman.</p>
<p>[4] Hoff, P. D. (2007). Modeling homophily and stochastic equivalence in symmetric relational data. In Advances in Neural Information Processing Systems (NIPS), volume 20, pages 657–664.</p>
<p>[5] Hoff, P. D., Raftery, A. E., and Handcock, M. S. (2002). Latent Space Approaches to Social Network Analysis. Journal of the American Statistical Association, 97(460), 1090–1098.</p>
<p>[6] Hoover, D. N. (1979). Relations on probability spaces and arrays of random variables. Technical report, Institute for Advanced Study, Princeton.</p>
<p>[7] Kallenberg, O. (1992). Symmetries on random arrays and set-indexed processes. Journal of Theoretical Probability, 5(4), 727–765.</p>
<p>[8] Kallenberg, O. (1999). Multivariate Sampling and the Estimation Problem for Exchangeable Arrays. Journal of Theoretical Probability, 12(3), 859–883.</p>
<p>[9] Kallenberg, O. (2005). Probabilistic Symmetries and Invariance Principles. Springer.</p>
<p>[10] Kemp, C., Tenenbaum, J., Grifﬁths, T., Yamada, T., and Ueda, N. (2006). Learning systems of concepts with an inﬁnite relational model. In Proceedings of the National Conference on Artiﬁcial Intelligence, volume 21.</p>
<p>[11] Lawrence, N. D. (2005). Probabilistic non-linear principal component analysis with Gaussian process latent variable models. Journal of Machine Learning Research (JMLR), 6, 1783–1816.</p>
<p>[12] Lawrence, N. D. and Urtasun, R. (2009). Non-linear matrix factorization with Gaussian processes. In Proceedings of the International Conference on Machine Learning (ICML), pages 1–8. ACM Press.</p>
<p>[13] Lov´ sz, L. and Szegedy, B. (2006). Limits of dense graph sequences. Journal of Combinatorial Theory a Series B, 96, 933–957.</p>
<p>[14] Miller, K. T., Grifﬁths, T. L., and Jordan, M. I. (2009). Nonparametric latent feature models for link prediction. Advances in Neural Information Processing Systems (NIPS), pages 1276–1284.</p>
<p>[15] Murray, I., Adams, R. P., and Mackay, D. J. C. (2010). Elliptical slice sampling. Journal of Machine Learning Research (JMLR), 9, 541–548.</p>
<p>[16] Neal, R. M. (2003). Slice sampling. The Annals of Statistics, 31(3), 705–767. With discussions and a rejoinder by the author.</p>
<p>[17] Palla, K., Knowles, D. A., and Ghahramani, Z. (2012). An Inﬁnite Latent Attribute Model for Network Data. In Proceedings of the International Conference on Machine Learning (ICML).</p>
<p>[18] Qui˜ onero Candela, J. and Rasmussen, C. E. (2005). A unifying view of sparse approximate gaussian n process regression. Journal of Machine Learning Research (JMLR), 6, 1939–1959.</p>
<p>[19] Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT Press.</p>
<p>[20] Roy, D. M. and Teh, Y. W. (2009). The Mondrian process. In Advances in Neural Information Processing Systems (NIPS).</p>
<p>[21] Salakhutdinov, R. (2008). Probabilistic Matrix Factorisation. In Advances in neural information processing systems (NIPS).</p>
<p>[22] Silverman, B. W. (1985). Some aspects of the spline smoothing approach to non-parametric regression curve ﬁtting. Journal of the Royal Statistical Society. Series B (Methodological), 47(1), 1–52.</p>
<p>[23] Smola, A. J. and Bartlett, P. (2001). Sparse greedy gaussian process regression. In Advances in Neural Information Processing Systems (NIPS). MIT Press.</p>
<p>[24] Titsias, M. K. and Lawrence, N. D. (2008). Efﬁcient sampling for Gaussian process inference using control variables. In Advances in Neural Information Processing Systems (NIPS), pages 1681–1688.</p>
<p>[25] Wahba, G., Lin, X., Gao, F., Xiang, D., Klein, R., and Klein, B. (1999). The bias-variance tradeoff and the randomized gacv. In Advances in Neural Information Processing Systems (NIPS).</p>
<p>[26] Wang, Y. J. and Wong, G. Y. (1987). Stochastic Blockmodels for Directed Graphs. Journal of the American Statistical Association, 82(397), 8–19.</p>
<p>[27] Xu, Z., Yan, F., and Qi, Y. (2012). Inﬁnite Tucker Decomposition: Nonparametric Bayesian Models for Multiway Data Analysis. In Proceedings of the International Conference on Machine Learning (ICML).</p>
<p>[28] Yan, F., Xu, Z., and Qi, Y. (2011). Sparse matrix-variate Gaussian process blockmodels for network modeling. In Proceedings of the International Conference on Uncertainty in Artiﬁcial Intelligence (UAI).  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
