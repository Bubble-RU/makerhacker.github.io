<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>200 nips-2012-Local Supervised Learning through Space Partitioning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-200" href="../nips2012/nips-2012-Local_Supervised_Learning_through_Space_Partitioning.html">nips2012-200</a> <a title="nips-2012-200-reference" href="#">nips2012-200-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>200 nips-2012-Local Supervised Learning through Space Partitioning</h1>
<br/><p>Source: <a title="nips-2012-200-pdf" href="http://papers.nips.cc/paper/4725-local-supervised-learning-through-space-partitioning.pdf">pdf</a></p><p>Author: Joseph Wang, Venkatesh Saligrama</p><p>Abstract: We develop a novel approach for supervised learning based on adaptively partitioning the feature space into different regions and learning local region-speciﬁc classiﬁers. We formulate an empirical risk minimization problem that incorporates both partitioning and classiﬁcation in to a single global objective. We show that space partitioning can be equivalently reformulated as a supervised learning problem and consequently any discriminative learning method can be utilized in conjunction with our approach. Nevertheless, we consider locally linear schemes by learning linear partitions and linear region classiﬁers. Locally linear schemes can not only approximate complex decision boundaries and ensure low training error but also provide tight control on over-ﬁtting and generalization error. We train locally linear classiﬁers by using LDA, logistic regression and perceptrons, and so our scheme is scalable to large data sizes and high-dimensions. We present experimental results demonstrating improved performance over state of the art classiﬁcation techniques on benchmark datasets. We also show improved robustness to label noise.</p><br/>
<h2>reference text</h2><p>[1] G. R¨ tsch, T. Onoda, and K.-R. M¨ ller. Soft margins for AdaBoost. Technical Report NC-TRa u 1998-021, Department of Computer Science, Royal Holloway, University of London, Egham, UK, August 1998. Submitted to Machine Learning.</p>
<p>[2] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119 – 139, 1997.</p>
<p>[3] Leo Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classiﬁcation and Regression Trees. Wadsworth, 1984.</p>
<p>[4] Thomas G. Dietterich and Ghulum Bakiri. Solving multiclass learning problems via errorcorrecting output codes. Journal of Artiﬁcial Intelligence Research, 2:263–286, 1995.</p>
<p>[5] Erin L. Allwein, Robert E. Schapire, and Yoram Singer. Reducing multiclass to binary: a unifying approach for margin classiﬁers. J. Mach. Learn. Res., 1:113–141, September 2001.</p>
<p>[6] Koby Crammer and Yoram Singer. On the learnability and design of output codes for multiclass problems. In In Proceedings of the Thirteenth Annual Conference on Computational Learning Theory, pages 35–46, 2000.</p>
<p>[7] Venkatesan Guruswami and Amit Sahai. Multiclass learning, boosting, and error-correcting codes. In Proceedings of the twelfth annual conference on Computational learning theory, COLT ’99, pages 145–155, New York, NY, USA, 1999. ACM.</p>
<p>[8] Yijun Sun, Sinisa Todorovic, Jian Li, and Dapeng Wu. Unifying the error-correcting and output-code adaboost within the margin framework. In Proceedings of the 22nd international conference on Machine learning, ICML ’05, pages 872–879, New York, NY, USA, 2005. ACM.</p>
<p>[9] Trevor Hastie and Robert Tibshirani. Discriminant analysis by gaussian mixtures. Journal of the Royal Statistical Society, Series B, 58:155–176, 1996.</p>
<p>[10] Tae-Kyun Kim and Josef Kittler. Locally linear discriminant analysis for multimodally distributed classes for face recognition with a single model image. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27:318–327, 2005.</p>
<p>[11] Ofer Dekel and Ohad Shamir. There’s a hole in my data space: Piecewise predictors for heterogeneous learning problems. In Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics, volume 15, 2012.</p>
<p>[12] Juan Dai, Shuicheng Yan, Xiaoou Tang, and James T. Kwok. Locally adaptive classiﬁcation piloted by uncertainty. In Proceedings of the 23rd international conference on Machine learning, ICML ’06, pages 225–232, New York, NY, USA, 2006. ACM.</p>
<p>[13] Marc Toussaint and Sethu Vijayakumar. Learning discontinuities with products-of-sigmoids for switching between local models. In Proceedings of the 22nd international conference on Machine Learning, pages 904–911. ACM Press, 2005.</p>
<p>[14] Eduardo D. Sontag. Vc dimension of neural networks. In Neural Networks and Machine Learning, pages 69–95. Springer, 1998.</p>
<p>[15] Yoav Freund and Robert E. Schapire. Large margin classiﬁcation using the perceptron algorithm. Machine Learning, 37:277–296, 1999. 10.1023/A:1007662407062.</p>
<p>[16] A. Frank and A. Asuncion. UCI machine learning repository, 2010.</p>
<p>[17] J. Langford. Tutorial on practical prediction theory for classiﬁcation. Journal of Machine Learning Research, 6(1):273, 2006.</p>
<p>[18] Mohammad J. Saberian and Nuno Vasconcelos. Multiclass boosting: Theory and algorithms. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 2124–2132. 2011.</p>
<p>[19] Ji Zhu, Hui Zou, Saharon Rosset, and Trevor Hastie. Multi-class adaboost, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
