<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>139 nips-2012-Fused sparsity and robust estimation for linear models with unknown variance</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-139" href="../nips2012/nips-2012-Fused_sparsity_and_robust_estimation_for_linear_models_with_unknown_variance.html">nips2012-139</a> <a title="nips-2012-139-reference" href="#">nips2012-139-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>139 nips-2012-Fused sparsity and robust estimation for linear models with unknown variance</h1>
<br/><p>Source: <a title="nips-2012-139-pdf" href="http://papers.nips.cc/paper/4596-fused-sparsity-and-robust-estimation-for-linear-models-with-unknown-variance.pdf">pdf</a></p><p>Author: Arnak Dalalyan, Yin Chen</p><p>Abstract: In this paper, we develop a novel approach to the problem of learning sparse representations in the context of fused sparsity and unknown noise level. We propose an algorithm, termed Scaled Fused Dantzig Selector (SFDS), that accomplishes the aforementioned learning task by means of a second-order cone program. A special emphasize is put on the particular instance of fused sparsity corresponding to the learning in presence of outliers. We establish ﬁnite sample risk bounds and carry out an experimental evaluation on both synthetic and real data. 1</p><br/>
<h2>reference text</h2><p>[1] Stephen Becker, Emmanuel Cand` s, and Michael Grant. Templates for convex cone problems with applie cations to sparse signal recovery. Math. Program. Comput., 3(3):165–218, 2011.</p>
<p>[2] A. Belloni, Victor Chernozhukov, and L. Wang. Square-root lasso: Pivotal recovery of sparse signals via conic programming. Biometrika, to appear, 2012.</p>
<p>[3] Peter J. Bickel, Ya’acov Ritov, and Alexandre B. Tsybakov. Simultaneous analysis of lasso and Dantzig selector. Ann. Statist., 37(4):1705–1732, 2009.</p>
<p>[4] Emmanuel Candes and Terence Tao. The Dantzig selector: statistical estimation when p is much larger than n. Ann. Statist., 35(6):2313–2351, 2007.</p>
<p>[5] Emmanuel J. Cand` s. The restricted isometry property and its implications for compressed sensing. C. e R. Math. Acad. Sci. Paris, 346(9-10):589–592, 2008.</p>
<p>[6] Emmanuel J. Cand` s and Paige A. Randall. Highly robust error correction by convex programming. IEEE e Trans. Inform. Theory, 54(7):2829–2840, 2008.</p>
<p>[7] Arnak S. Dalalyan and Renaud Keriven. L1 -penalized robust estimation for a class of inverse problems arising in multiview geometry. In NIPS, pages 441–449, 2009.</p>
<p>[8] Arnak S. Dalalyan and Renaud Keriven. Robust estimation for an inverse problem arising in multiview geometry. J. Math. Imaging Vision., 43(1):10–23, 2012.</p>
<p>[9] Eric Gautier and Alexandre Tsybakov. High-dimensional instrumental variables regression and conﬁdence sets. Technical Report arxiv:1105.2454, September 2011.</p>
<p>[10] Christophe Giraud, Sylvie Huet, and Nicolas Verzelen. High-dimensional regression with unknown variance. submitted, page arXiv:1109.5587v2 [math.ST].</p>
<p>[11] Z. Harchaoui and C. L´ vy-Leduc. Multiple change-point estimation with a total variation penalty. J. e Amer. Statist. Assoc., 105(492):1480–1493, 2010.</p>
<p>[12] Za¨d Harchaoui and C´ line L´ vy-Leduc. Catching change-points with lasso. In John Platt, Daphne Koller, ı e e Yoram Singer, and Sam Roweis, editors, NIPS. Curran Associates, Inc., 2007.</p>
<p>[13] R. I. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, June 2004.</p>
<p>[14] A. Iouditski, F. Kilinc Karzan, A. S. Nemirovski, and B. T. Polyak. On the accuracy of l1-ﬁltering of signals with block-sparse structure. In NIPS 24, pages 1260–1268. 2011.</p>
<p>[15] S. Lambert-Lacroix and L. Zwald. Robust regression through the Huber’s criterion and adaptive lasso penalty. Electron. J. Stat., 5:1015–1053, 2011.</p>
<p>[16] David G. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60(2):91–110, 2004.</p>
<p>[17] E. Mammen and S. van de Geer. Locally adaptive regression splines. Ann. Statist., 25(1):387–413, 1997.</p>
<p>[18] Nam H. Nguyen, Nasser M. Nasrabadi, and Trac D. Tran. Robust lasso with missing and grossly corrupted observations. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 1881–1889. 2011.</p>
<p>[19] A. Rinaldo. Properties and reﬁnements of the fused lasso. Ann. Statist., 37(5B):2922–2952, 2009.</p>
<p>[20] Nicolas St¨ dler, Peter B¨ hlmann, and Sara van de Geer. 1 -penalization for mixture regression models. a u TEST, 19(2):209–256, 2010.</p>
<p>[21] C. Strecha, W. von Hansen, L. Van Gool, P. Fua, and U. Thoennessen. On benchmarking camera calibration and multi-view stereo for high resolution imagery. In Conference on Computer Vision and Pattern Recognition, pages 1–8, 2009.</p>
<p>[22] J. F. Sturm. Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones. Optim. Methods Softw., 11/12(1-4):625–653, 1999.</p>
<p>[23] T. Sun and C.-H. Zhang. Comments on: 1 -penalization for mixture regression models. TEST, 19(2): 270–275, 2010.</p>
<p>[24] T. Sun and C.-H. Zhang. Scaled sparse linear regression. arXiv:1104.4595, 2011.</p>
<p>[25] R. Szeliski. Computer Vision: Algorithms and Applications. Texts in Computer Science. Springer, 2010.</p>
<p>[26] Robert Tibshirani. Regression shrinkage and selection via the lasso. J. Roy. Statist. Soc. Ser. B, 58(1): 267–288, 1996.</p>
<p>[27] Robert Tibshirani, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith Knight. Sparsity and smoothness via the fused lasso. J. R. Stat. Soc. Ser. B Stat. Methodol., 67(1):91–108, 2005.</p>
<p>[28] Sara A. van de Geer and Peter B¨ hlmann. On the conditions used to prove oracle results for the Lasso. u Electron. J. Stat., 3:1360–1392, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
