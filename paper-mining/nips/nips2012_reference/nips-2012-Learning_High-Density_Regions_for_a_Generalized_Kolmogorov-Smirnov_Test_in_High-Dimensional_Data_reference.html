<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>175 nips-2012-Learning High-Density Regions for a Generalized Kolmogorov-Smirnov Test in High-Dimensional Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-175" href="../nips2012/nips-2012-Learning_High-Density_Regions_for_a_Generalized_Kolmogorov-Smirnov_Test_in_High-Dimensional_Data.html">nips2012-175</a> <a title="nips-2012-175-reference" href="#">nips2012-175-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>175 nips-2012-Learning High-Density Regions for a Generalized Kolmogorov-Smirnov Test in High-Dimensional Data</h1>
<br/><p>Source: <a title="nips-2012-175-pdf" href="http://papers.nips.cc/paper/4553-learning-high-density-regions-for-a-generalized-kolmogorov-smirnov-test-in-high-dimensional-data.pdf">pdf</a></p><p>Author: Assaf Glazer, Michael Lindenbaum, Shaul Markovitch</p><p>Abstract: We propose an efﬁcient, generalized, nonparametric, statistical KolmogorovSmirnov test for detecting distributional change in high-dimensional data. To implement the test, we introduce a novel, hierarchical, minimum-volume sets estimator to represent the distributions to be tested. Our work is motivated by the need to detect changes in data streams, and the test is especially efﬁcient in this context. We provide the theoretical foundations of our test and show its superiority over existing methods. 1</p><br/>
<h2>reference text</h2><p>[1] S. Ben-David and M. Lindenbaum. Learning distributions by their density levels: A paradigm for learning without a teacher. Journal of Computer and System Sciences, 55(1):171–182, 1997.</p>
<p>[2] ZI Botev, JF Grotowski, and DP Kroese. Kernel density estimation via diffusion. The Annals of Statistics, 38(5):2916–2957, 2010.</p>
<p>[3] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines, 2001.</p>
<p>[4] T. Dasu, S. Krishnan, S. Venkatasubramanian, and K. Yi. An information-theoretic approach to detecting changes in multi-dimensional data streams. In INTERFACE, 2006.</p>
<p>[5] F. Desobry, M. Davy, and C. Doncarli. An online kernel change detection algorithm. Signal Processing, Transactions on Information Theory, 53(8):2961–2974, 2005.</p>
<p>[6] Anton Dries and Ulrich R¨ ckert. Adaptive concept drift detection. Statistical Analysis and u Data Mining, 2(5-6):311–327, 2009.</p>
<p>[7] B. Efron and R.J. Tibshirani. An Introduction to the Bootstrap. Chapman and Hall/CRC, 1994.</p>
<p>[8] J.H.J. Einmahl and D.M. Mason. Generalized quantile processes. The Annals of Statistics, pages 1062–1078, 1992.</p>
<p>[9] G. Fasano and A. Franceschini. A multidimensional version of the kolmogorov-smirnov test. Monthly Notices of the Royal Astronomical Society, 225:155–170, 1987.</p>
<p>[10] J.H. Friedman and L.C. Rafsky. Multivariate generalizations of the Wald-Wolfowitz and Smirnov two-sample tests. The Annals of Statistics, 7(4):697–717, 1979.</p>
<p>[11] J. Gama, P. Medas, G. Castillo, and P. Rodrigues. Learning with drift detection. In SBIA, pages 66–112. Springer, 2004.</p>
<p>[12] A. Gretton, K.M. Borgwardt, M. Rasch, B. Scholkopf, and A.J. Smola. A kernel method for the two-sample-problem. Machine Learning, 1:1–10, 2008.</p>
<p>[13] X. Huo and J.C. Lu. A network ﬂow approach in ﬁnding maximum likelihood estimate of high concentration regions. Computational Statistics & Data Analysis, 46(1):33–56, 2004.</p>
<p>[14] D.M. Mason and W. Polonik. Asymptotic normality of plug-in level set estimates. The Annals of Applied Probability, 19(3):1108–1142, 2009.</p>
<p>[15] A. Munoz and J.M. Moguerza. Estimation of high-density regions using one-class neighbor machines. In PAMI, pages 476–480, 2006.</p>
<p>[16] J. Nunez Garcia, Z. Kutalik, K.H. Cho, and O. Wolkenhauer. Level sets and minimum volume sets of probability density functions. International Journal of Approximate Reasoning, 34(1): 25–47, 2003.</p>
<p>[17] JA Peacock. Two-dimensional goodness-of-ﬁt testing in astronomy. Monthly Notices of the Royal Astronomical Society, 202:615–627, 1983.</p>
<p>[18] W. Polonik. Concentration and goodness-of-ﬁt in higher dimensions:(asymptotically) distribution-free methods. The Annals of Statistics, 27(4):1210–1229, 1999.</p>
<p>[19] Bernhard Sch¨ lkopf, John C. Platt, John C. Shawe-Taylor, Alex J. Smola, and Robert C. o Williamson. Estimating the support of a high-dimensional distribution. Neural Computation, 13(7):1443–1471, 2001.</p>
<p>[20] C.D. Scott and R.D. Nowak. Learning minimum volume sets. The Journal of Machine Learning Research, 7:665–704, 2006.</p>
<p>[21] G. Walther. Granulometric smoothing. The Annals of Statistics, pages 2273–2299, 1997.</p>
<p>[22] G. Widmer and M. Kubat. Learning in the presence of concept drift and hidden contexts. Machine Learning, 23(1):69–101, 1996.</p>
<p>[23] R.M. Willett and R.D. Nowak. Minimax optimal level-set estimation. Image Processing, IEEE Transactions on, 16(12):2965–2979, 2007.</p>
<p>[24] John Wright, Yi Ma, Yangyu Tao, Zhouchen Lin, and Heung-Yeung Shum. Classiﬁcation via minimum incremental coding length. SIAM J. Imaging Sciences, 2(2):367–395, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
