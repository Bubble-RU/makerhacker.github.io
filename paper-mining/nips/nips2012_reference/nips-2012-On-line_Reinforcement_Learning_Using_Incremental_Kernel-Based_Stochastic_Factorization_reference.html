<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>250 nips-2012-On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-250" href="../nips2012/nips-2012-On-line_Reinforcement_Learning_Using_Incremental_Kernel-Based_Stochastic_Factorization.html">nips2012-250</a> <a title="nips-2012-250-reference" href="#">nips2012-250-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>250 nips-2012-On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization</h1>
<br/><p>Source: <a title="nips-2012-250-pdf" href="http://papers.nips.cc/paper/4599-on-line-reinforcement-learning-using-incremental-kernel-based-stochastic-factorization.pdf">pdf</a></p><p>Author: Doina Precup, Joelle Pineau, Andre S. Barreto</p><p>Abstract: Kernel-based stochastic factorization (KBSF) is an algorithm for solving reinforcement learning tasks with continuous state spaces which builds a Markov decision process (MDP) based on a set of sample transitions. What sets KBSF apart from other kernel-based approaches is the fact that the size of its MDP is independent of the number of transitions, which makes it possible to control the trade-off between the quality of the resulting approximation and the associated computational cost. However, KBSF’s memory usage grows linearly with the number of transitions, precluding its application in scenarios where a large amount of data must be processed. In this paper we show that it is possible to construct KBSF’s MDP in a fully incremental way, thus freeing the space complexity of this algorithm from its dependence on the number of sample transitions. The incremental version of KBSF is able to process an arbitrary amount of data, which results in a model-based reinforcement learning algorithm that can be used to solve continuous MDPs in both off-line and on-line regimes. We present theoretical results showing that KBSF can approximate the value function that would be computed by conventional kernel-based learning with arbitrary precision. We empirically demonstrate the effectiveness of the proposed algorithm in the challenging threepole balancing task, in which the ability to process a large number of transitions is crucial for success. 1</p><br/>
<h2>reference text</h2><p>[1] D. Ormoneit and S. Sen. Kernel-based reinforcement learning. Machine Learning, 49 (2–3): 161–178, 2002.</p>
<p>[2] D. Ormoneit and P. Glynn. Kernel-based reinforcement learning in average-cost problems. IEEE Transactions on Automatic Control, 47(10):1624–1636, 2002.</p>
<p>[3] N. Jong and P. Stone. Kernel-based models for reinforcement learning in continuous state spaces. In Proceedings of the International Conference on Machine Learning (ICML)— Workshop on Kernel Machines and Reinforcement Learning, 2006.</p>
<p>[4] A. M. S. Barreto, D. Precup, and J. Pineau. Reinforcement learning using kernel-based stochastic factorization. In Advances in Neural Information Processing Systems (NIPS), pages 720– 728, 2011.</p>
<p>[5] B. Kveton and G. Theocharous. Kernel-based reinforcement learning on representative states. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), pages 124–131, 2012.</p>
<p>[6] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.</p>
<p>[7] M. L. Puterman. Markov Decision Processes—Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., 1994.</p>
<p>[8] M. L. Littman, T. L. Dean, and L. P. Kaelbling. On the complexity of solving Markov decision problems. In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pages 394–402, 1995.</p>
<p>[9] A. M. S. Barreto and M. D. Fragoso. Computing the stationary distribution of a ﬁnite Markov chain through stochastic factorization. SIAM Journal on Matrix Analysis and Applications, 32: 1513–1523, 2011.</p>
<p>[10] Y. Engel, S. Mannor, and R. Meir. The kernel recursive least squares algorithm. IEEE Transactions on Signal Processing, 52:2275–2285, 2003.</p>
<p>[11] R. S. Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse coding. In Advances in Neural Information Processing Systems (NIPS), pages 1038– 1044, 1996.</p>
<p>[12] D. Michie and R. Chambers. BOXES: An experiment in adaptive control. Machine Intelligence 2, pages 125–133, 1968.</p>
<p>[13] C. W. Anderson. Learning and Problem Solving with Multilayer Connectionist Systems. PhD thesis, Computer and Information Science, University of Massachusetts, 1986.</p>
<p>[14] A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive elements that can solve difﬁcult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, 13: 834–846, 1983.</p>
<p>[15] F. J. Gomez. Robust non-linear control through neuroevolution. PhD thesis, The University of Texas at Austin, 2003.</p>
<p>[16] A. P. Wieland. Evolving neural network controllers for unstable systems. In Proceedings of the International Joint Conference on Neural Networks (IJCNN), pages 667–673, 1991.</p>
<p>[17] D. Ernst, P. Geurts, and L. Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research, 6:503–556, 2005.</p>
<p>[18] D. Ernst, G. B. Stan, J. Goncalves, and L. Wehenkel. Clinical data based optimal STI strate¸ gies for HIV: a reinforcement learning approach. In Proceedings of the IEEE Conference on Decision and Control (CDC), pages 124–131, 2006.</p>
<p>[19] F. Gomez, J. Schmidhuber, and R. Miikkulainen. Efﬁcient non-linear control through neuroevolution. In Proceedings of the European Conference on Machine Learning (ECML), pages 654–662, 2006.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
