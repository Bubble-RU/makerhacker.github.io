<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>301 nips-2012-Scaled Gradients on Grassmann Manifolds for Matrix Completion</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-301" href="../nips2012/nips-2012-Scaled_Gradients_on_Grassmann_Manifolds_for_Matrix_Completion.html">nips2012-301</a> <a title="nips-2012-301-reference" href="#">nips2012-301-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>301 nips-2012-Scaled Gradients on Grassmann Manifolds for Matrix Completion</h1>
<br/><p>Source: <a title="nips-2012-301-pdf" href="http://papers.nips.cc/paper/4713-scaled-gradients-on-grassmann-manifolds-for-matrix-completion.pdf">pdf</a></p><p>Author: Thanh Ngo, Yousef Saad</p><p>Abstract: This paper describes gradient methods based on a scaled metric on the Grassmann manifold for low-rank matrix completion. The proposed methods signiﬁcantly improve canonical gradient methods, especially on ill-conditioned matrices, while maintaining established global convegence and exact recovery guarantees. A connection between a form of subspace iteration for matrix completion and the scaled gradient descent procedure is also established. The proposed conjugate gradient method based on the scaled gradient outperforms several existing algorithms for matrix completion and is competitive with recently proposed methods. 1</p><br/>
<h2>reference text</h2><p>[1] P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton University Press, Princeton, NJ, 2008.</p>
<p>[2] Y. Amit, M. Fink, N. Srebro, and S. Ullman. Uncovering shared structures in multiclass classiﬁcation. In Proceedings of the 24th international conference on Machine learning, ICML ’07, pages 17–24, 2007.</p>
<p>[3] L. Armijo. Minimization of functions having Lipschitz continuous ﬁrst partial derivatives. Paciﬁc Journal of Mathematics, 16(1):1–3, 1966.</p>
<p>[4] J. Baglama, D. Calvetti, G. H. Golub, and L. Reichel. Adaptively preconditioned GMRES algorithms. SIAM J. Sci. Comput., 20(1):243–269, December 1998.</p>
<p>[5] L. Balzano, R. Nowak, and B. Recht. Online identiﬁcation and tracking of subspaces from highly incomplete information. In Proceedings of Allerton, September 2010.</p>
<p>[6] N. Boumal and P.-A. Absil. Rtrmc: A riemannian trust-region method for low-rank matrix completion. In NIPS, 2011.</p>
<p>[7] J-F. Cai, E. J. Cand` s, and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM e Journal on Optimization, 20(4):1956–1982, 2010.</p>
<p>[8] E. Candes and T. Tao. The power of convex relaxation: Near-optimal matrix completion, 2009.</p>
<p>[9] P. Chen and D. Suter. Recovering the Missing Components in a Large Noisy Low-Rank Matrix: Application to SFM. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(8):1051–1063, 2004.</p>
<p>[10] W. Dai, E. Kerman, and O. Milenkovic. A geometric approach to low-rank matrix completion. IEEE Transactions on Information Theory, 58(1):237–247, 2012.</p>
<p>[11] A. Edelman, T. Arias, and S. T. Smith. The geometry of algorithms with orthogonality constraints. SIAM J. Matrix Anal. Appl, 20:303–353, 1998.</p>
<p>[12] P. Jain, R. Meka, and I. S. Dhillon. Guaranteed rank minimization via singular value projection. In NIPS, pages 937–945, 2010.</p>
<p>[13] R. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 952–960. 2009.</p>
<p>[14] R. H. Keshavan, S. Oh, and A. Montanari. Matrix completion from a few entries. CoRR, abs/0901.3150, 2009.</p>
<p>[15] S. Ma, D. Goldfarb, and L. Chen. Fixed point and bregman iterative methods for matrix rank minimization. Math. Program., 128(1-2):321–353, 2011.</p>
<p>[16] B. Marlin. Collaborative ﬁltering: A machine learning perspective, 2004.</p>
<p>[17] R. Mazumder, T. Hastie, and R. Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. J. Mach. Learn. Res., 11:2287–2322, August 2010.</p>
<p>[18] B. Recht. A simpler approach to matrix completion. CoRR, abs/0910.0651, 2009.</p>
<p>[19] J. D. M. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction. In In Proceedings of the 22nd International Conference on Machine Learning (ICML, pages 713–719. ACM, 2005.</p>
<p>[20] Y. Saad. Numerical Methods for Large Eigenvalue Problems- classics edition. SIAM, Philadelpha, PA, 2011.</p>
<p>[21] N. Srebro and T. Jaakkola. Weighted low-rank approximations. In In 20th International Conference on Machine Learning, pages 720–727. AAAI Press, 2003.</p>
<p>[22] B. Vandereycken. Low-rank matrix completion by riemannian optimization. Technical report, Mathematics Section, Ecole Polytechnique Federale de de Lausanne, 2011.</p>
<p>[23] Z. Wen, W. Yin, and Y. Zhang. Solving a low-rank factorization model for matrix completion using a non-linear successive over-relaxation algorithm. In CAAM Technical Report. Rice University, 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
