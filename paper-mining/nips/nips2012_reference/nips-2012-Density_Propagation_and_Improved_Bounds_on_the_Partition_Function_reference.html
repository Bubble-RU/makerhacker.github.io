<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>96 nips-2012-Density Propagation and Improved Bounds on the Partition Function</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-96" href="../nips2012/nips-2012-Density_Propagation_and_Improved_Bounds_on_the_Partition_Function.html">nips2012-96</a> <a title="nips-2012-96-reference" href="#">nips2012-96-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>96 nips-2012-Density Propagation and Improved Bounds on the Partition Function</h1>
<br/><p>Source: <a title="nips-2012-96-pdf" href="http://papers.nips.cc/paper/4723-density-propagation-and-improved-bounds-on-the-partition-function.pdf">pdf</a></p><p>Author: Stefano Ermon, Ashish Sabharwal, Bart Selman, Carla P. Gomes</p><p>Abstract: Given a probabilistic graphical model, its density of states is a distribution that, for any likelihood value, gives the number of conﬁgurations with that probability. We introduce a novel message-passing algorithm called Density Propagation (DP) for estimating this distribution. We show that DP is exact for tree-structured graphical models and is, in general, a strict generalization of both sum-product and max-product algorithms. Further, we use density of states and tree decomposition to introduce a new family of upper and lower bounds on the partition function. For any tree decomposition, the new upper bound based on ﬁner-grained density of state information is provably at least as tight as previously known bounds based on convexity of the log-partition function, and strictly stronger if a general condition holds. We conclude with empirical evidence of improvement over convex relaxations and mean-ﬁeld based bounds. 1</p><br/>
<h2>reference text</h2><p>[1] M.J. Wainwright and M.I. Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1-2):1–305, 2008.</p>
<p>[2] S. Ermon, C. Gomes, A. Sabharwal, and B. Selman. Accelerated Adaptive Markov Chain for Partition Function Computation. Neural Information Processing Systems, 2011.</p>
<p>[3] F. Wang and DP Landau. Efﬁcient, multiple-range random walk algorithm to calculate the density of states. Physical Review Letters, 86(10):2050–2053, 2001.</p>
<p>[4] M.J. Wainwright. Stochastic processes on graphs with cycles: geometric and Variational approaches. PhD thesis, Massachusetts Institute of Technology, 2002.</p>
<p>[5] M. Wainwright, T. Jaakkola, and A. Willsky. Exact map estimates by (hyper) tree agreement. Advances in neural information processing systems, pages 833–840, 2003.</p>
<p>[6] M.J. Wainwright. Tree-reweighted belief propagation algorithms and approximate ML estimation via pseudo-moment matching. In AISTATS, 2003.</p>
<p>[7] G. Parisi and R. Shankar. Statistical ﬁeld theory. Physics Today, 41:110, 1988.</p>
<p>[8] L.D. Brown. Fundamentals of statistical exponential families: with applications in statistical decision theory. Institute of Mathematical Statistics, 1986.</p>
<p>[9] M. Richardson and P. Domingos. Markov logic networks. Machine Learning, 62(1):107–136, 2006.</p>
<p>[10] Y. Weiss, C. Yanover, and T. Meltzer. MAP estimation, linear programming and belief propagation with convex free energies. In Uncertainty in Artiﬁcial Intelligence, 2007.</p>
<p>[11] T. Hazan and A. Shashua. Norm-product belief propagation: Primal-dual message-passing for approximate inference. Information Theory, IEEE Transactions on, 56(12):6294–6316, 2010.</p>
<p>[12] K.P. Murphy, Y. Weiss, and M.I. Jordan. Loopy belief propagation for approximate inference: An empirical study. In Proceedings of the Fifteenth conference on Uncertainty in artiﬁcial intelligence, pages 467–475. Morgan Kaufmann Publishers Inc., 1999.</p>
<p>[13] J.S. Yedidia, W.T. Freeman, and Y. Weiss. Understanding belief propagation and its generalizations. Exploring artiﬁcial intelligence in the new millennium, 8:236–239, 2003.</p>
<p>[14] S.M. Aji and R.J. McEliece. The generalized distributive law. Information Theory, IEEE Transactions on, 46(2):325–343, 2000.</p>
<p>[15] W.S. Cheung. Generalizations of H¨ lders inequality. International Journal of Mathematics o and Mathematical Sciences, 26:7–10, 2001.</p>
<p>[16] Qiang Liu and Alexander Ihler. Negative tree reweighted belief propagation. In Proceedings of the Twenty-Sixth Conference Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-10), pages 332–339, Corvallis, Oregon, 2010. AUAI Press.</p>
<p>[17] J.M. Mooij. libDAI: A free and open source c++ library for discrete approximate inference in graphical models. The Journal of Machine Learning Research, 11:2169–2173, 2010.</p>
<p>[18] M.J.D. Powell. The BOBYQA algorithm for bound constrained optimization without derivatives. University of Cambridge Technical Report, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
