<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>135 nips-2012-Forging The Graphs: A Low Rank and Positive Semidefinite Graph Learning Approach</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-135" href="../nips2012/nips-2012-Forging_The_Graphs%3A_A_Low_Rank_and_Positive_Semidefinite_Graph_Learning_Approach.html">nips2012-135</a> <a title="nips-2012-135-reference" href="#">nips2012-135-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>135 nips-2012-Forging The Graphs: A Low Rank and Positive Semidefinite Graph Learning Approach</h1>
<br/><p>Source: <a title="nips-2012-135-pdf" href="http://papers.nips.cc/paper/4801-forging-the-graphs-a-low-rank-and-positive-semidefinite-graph-learning-approach.pdf">pdf</a></p><p>Author: Dijun Luo, Heng Huang, Feiping Nie, Chris H. Ding</p><p>Abstract: In many graph-based machine learning and data mining approaches, the quality of the graph is critical. However, in real-world applications, especially in semisupervised learning and unsupervised learning, the evaluation of the quality of a graph is often expensive and sometimes even impossible, due the cost or the unavailability of ground truth. In this paper, we proposed a robust approach with convex optimization to “forge” a graph: with an input of a graph, to learn a graph with higher quality. Our major concern is that an ideal graph shall satisfy all the following constraints: non-negative, symmetric, low rank, and positive semidefinite. We develop a graph learning algorithm by solving a convex optimization problem and further develop an efﬁcient optimization to obtain global optimal solutions with theoretical guarantees. With only one non-sensitive parameter, our method is shown by experimental results to be robust and achieve higher accuracy in semi-supervised learning and clustering under various settings. As a preprocessing of graphs, our method has a wide range of potential applications machine learning and data mining.</p><br/>
<h2>reference text</h2><p>[1] E. Airoldi, D. Blei, E. Xing, and S. Fienberg. A latent mixed membership model for relational data. In Proceedings of the 3rd international workshop on Link discovery, pages 82–89. ACM, 2005.</p>
<p>[2] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural computation, 15(6):1373–1396, 2003.</p>
<p>[3] T. B¨ hler and M. Hein. Spectral Clustering based on the graph p-Laplacian. In Proceedings of u the 26th Annual International Conference on Machine Learning, pages 81–88. ACM, 2009.</p>
<p>[4] J. Cai, E. Candes, and Z. Shen. A singular value thresholding algorithm for matrix completion. IEEE Trans. Inform. Theory, 56(5), 2053-2080, (5):2053–2080, 2008.</p>
<p>[5] E. Candes and Y. Plan. Matrix completion with noise. Proceedings of the IEEE, 98(6):925– 936, 2010.</p>
<p>[6] C. Ding, R. Jin, T. Li, and H. Simon. A learning framework using Green’s function and kernel regularization with application to recommender system. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 260–269. ACM, 2007.</p>
<p>[7] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efﬁcient projections onto the l 1ball for learning in high dimensions. In Proceedings of the 25th international conference on Machine learning, pages 272–279. ACM, 2008.</p>
<p>[8] M. Fazel. Matrix rank minimization with applications. PhD thesis, Stanford University, 2002.</p>
<p>[9] A. Frank and A. Asuncion. UCI machine learning repository, 2010.</p>
<p>[10] M. Gu, H. Zha, C. Ding, X. He, H. Simon, and J. Xia. Spectral relaxation models and structure analysis for k-way graph clustering and bi-clustering. UC Berkeley Math Dept Tech Report, 2001.</p>
<p>[11] L. Hagen and A. Kahng. New spectral methods for ratio cut partitioning and clustering. Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on, 11(9):1074–1085, 2002.</p>
<p>[12] R. Lewis, V. Torczon, and L. R. Center. A globally convergent augmented lagrangian pattern search algorithm for optimization with general constraints and simple bounds. SIAM Journal on Optimization, 12(4):1075–1089, 2002.</p>
<p>[13] W. Liu and S. Chang. Robust multi-class transductive learning with graphs. 2009.</p>
<p>[14] D. Luo, C. Ding, and H. Huang. Graph evolution via social diffusion processes. Machine Learning and Knowledge Discovery in Databases, pages 390–404, 2011.</p>
<p>[15] D. Luo, C. Ding, F. Nie, and H. Huang. Cauchy graph embedding. ICML2011, pages 553–560, 2011.</p>
<p>[16] A. Ng, M. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. Advances in neural information processing systems, 2:849–856, 2002.</p>
<p>[17] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323, 2000.</p>
<p>[18] H. Seung and D. Lee. The manifold ways of perception. Science(Washington), 290(5500):2268–9, 2000.</p>
<p>[19] J. Shi and J. Malik. Normalized cuts and image segmentation. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 22(8):888–905, 2002.</p>
<p>[20] F. Wang, P. Li, and A. K¨ nig. Learning a Bi-Stochastic Data Similarity Matrix. In 2010 IEEE o International Conference on Data Mining, pages 551–560. IEEE, 2010.</p>
<p>[21] F. Wang and C. Zhang. Label propagation through linear neighborhoods. IEEE Transactions on Knowledge and Data Engineering, pages 55–67, 2007.</p>
<p>[22] D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Sch¨ lkopf. Learning with local and global o consistency. In Advances in Neural Information Processing Systems 16: Proceedings of the 2003 Conference, pages 595–602, 2004.</p>
<p>[23] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic functions. In ICML 2003. 9</p>
<br/>
<br/><br/><br/></body>
</html>
