<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>220 nips-2012-Monte Carlo Methods for Maximum Margin Supervised Topic Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-220" href="../nips2012/nips-2012-Monte_Carlo_Methods_for_Maximum_Margin_Supervised_Topic_Models.html">nips2012-220</a> <a title="nips-2012-220-reference" href="#">nips2012-220-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>220 nips-2012-Monte Carlo Methods for Maximum Margin Supervised Topic Models</h1>
<br/><p>Source: <a title="nips-2012-220-pdf" href="http://papers.nips.cc/paper/4614-monte-carlo-methods-for-maximum-margin-supervised-topic-models.pdf">pdf</a></p><p>Author: Qixia Jiang, Jun Zhu, Maosong Sun, Eric P. Xing</p><p>Abstract: An effective strategy to exploit the supervising side information for discovering predictive topic representations is to impose discriminative constraints induced by such information on the posterior distributions under a topic model. This strategy has been adopted by a number of supervised topic models, such as MedLDA, which employs max-margin posterior constraints. However, unlike the likelihoodbased supervised topic models, of which posterior inference can be carried out using the Bayes’ rule, the max-margin posterior constraints have made Monte Carlo methods infeasible or at least not directly applicable, thereby limited the choice of inference algorithms to be based on variational approximation with strict mean ﬁeld assumptions. In this paper, we develop two efﬁcient Monte Carlo methods under much weaker assumptions for max-margin supervised topic models based on an importance sampler and a collapsed Gibbs sampler, respectively, in a convex dual formulation. We report thorough experimental results that compare our approach favorably against existing alternatives in both accuracy and efﬁciency.</p><br/>
<h2>reference text</h2><p>[1] C.M. Bishop. Pattern recognition and machine learning, volume 4. springer New York, 2006.</p>
<p>[2] D.M. Blei and J.D. McAuliffe. Supervised topic models. NIPS, pages 121–128, 2007.</p>
<p>[3] D.M. Blei, A.Y. Ng, and M.I. Jordan. Latent Dirichlet allocation. JMLR, 3:993–1022, 2003.</p>
<p>[4] A. Gelman, J.B. Carlin, H.S. Stern, and D.B. Rubin. Bayesian data analysis. Boca Raton, FL: Chapman and Hall/CRC, 2004.</p>
<p>[5] T.L. Grifﬁths and M. Steyvers. Finding scientiﬁc topics. Proc. of National Academy of Sci., pages 5228–5235, 2004.</p>
<p>[6] T. Joachims, T. Finley, and C.N.J. Yu. Cutting-plane training of structural SVMs. Machine Learning, 77(1):27–59, 2009.</p>
<p>[7] M.I. Jordan, Z. Ghahramani, T.S. Jaakkola, and L.K. Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183–233, 1999.</p>
<p>[8] S. Lacoste-Jullien, F. Sha, and M.I. Jordan. DiscLDA: Discriminative learning for dimensionality reduction and classiﬁcation. NIPS, pages 897–904, 2009.</p>
<p>[9] D. Li, S. Somasundaran, and A. Chakraborty. A combination of topic models with max-margin learning for relation detection. In ACL TextGraphs-6 Workshop, 2011.</p>
<p>[10] R.Y. Rubinstein and D.P. Kroese. Simulation and the Monte Carlo method, volume 707. Wileyinterscience, 2008.</p>
<p>[11] E. Schoﬁeld. Fitting maximum-entropy models on large sample spaces. PhD thesis, Department of Computing, Imperial College London, 2006.</p>
<p>[12] C. Wang, D.M. Blei, and Li F.F. Simultaneous image classiﬁcation and annotation. CVPR, 2009.</p>
<p>[13] Y. Wang and G. Mori. Max-margin latent Dirichlet allocation for image classiﬁcation and annotation. In BMVC, 2011.</p>
<p>[14] S. Yang, J. Bian, and H. Zha. Hybrid generative/discriminative learning for automatic image annotation. In UAI, 2010.</p>
<p>[15] A. Zellner. Optimal information processing and Bayes’s theorem. American Statistician, pages 278–280, 1988.</p>
<p>[16] J. Zhu, A. Ahmed, and E.P. Xing. MedLDA: maximum margin supervised topic models for regression and classiﬁcation. In ICML, pages 1257–1264, 2009.</p>
<p>[17] J. Zhu, N. Chen, and E.P. Xing. Inﬁnite latent SVM for classiﬁcation and multi-task learning. In NIPS, 2011.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
