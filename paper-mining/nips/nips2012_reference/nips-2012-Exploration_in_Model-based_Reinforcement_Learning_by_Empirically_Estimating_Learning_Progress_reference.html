<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>122 nips-2012-Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-122" href="../nips2012/nips-2012-Exploration_in_Model-based_Reinforcement_Learning_by_Empirically_Estimating_Learning_Progress.html">nips2012-122</a> <a title="nips-2012-122-reference" href="#">nips2012-122-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>122 nips-2012-Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress</h1>
<br/><p>Source: <a title="nips-2012-122-pdf" href="http://papers.nips.cc/paper/4642-exploration-in-model-based-reinforcement-learning-by-empirically-estimating-learning-progress.pdf">pdf</a></p><p>Author: Manuel Lopes, Tobias Lang, Marc Toussaint, Pierre-yves Oudeyer</p><p>Abstract: Formal exploration approaches in model-based reinforcement learning estimate the accuracy of the currently learned model without consideration of the empirical prediction error. For example, PAC-MDP approaches such as R- MAX base their model certainty on the amount of collected data, while Bayesian approaches assume a prior over the transition dynamics. We propose extensions to such approaches which drive exploration solely based on empirical estimates of the learner’s accuracy and learning progress. We provide a “sanity check” theoretical analysis, discussing the behavior of our extensions in the standard stationary ﬁnite state-action case. We then provide experimental studies demonstrating the robustness of these exploration measures in cases of non-stationary environments or where original approaches are misled by wrong domain assumptions. 1</p><br/>
<h2>reference text</h2><p>[1] A. Baranes and P.Y. Oudeyer. Active learning of inverse models with intrinsically motivated goal exploration in robots. Robotics and Autonomous Systems, 2012.</p>
<p>[2] Yoshua Bengio and Yves Grandvalet. No unbiased estimator of the variance of k-fold crossvalidation. Journal of Machine Learning Research (JMLR), 5:1089–1105, 2004.</p>
<p>[3] Yoshua Bengio, J´ rˆ me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. eo In Int. Conf. on Machine Learning (ICML), pages 41–48, 2009.</p>
<p>[4] Ronen I. Brafman and Moshe Tennenholtz. R-max - a general polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning Research (JMLR), 3:213– 231, 2002.</p>
<p>[5] Mahdi Milani Fard and Joelle Pineau. Pac-bayesian model selection for reinforcement learning. In Conf. on Neural Information Processing Systems (NIPS). 2010.</p>
<p>[6] Todd Hester and Peter Stone. Intrinsically motivated model learning for a developing curious agent. In AAMAS Workshop on Adaptive Learning Agents (ALA), 2012.</p>
<p>[7] Sham Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, Gatsby Computational Neuroscience Unit, University College London, 2003.</p>
<p>[8] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine Learning Journal, 49(2-3):209–232, 2002.</p>
<p>[9] J. Zico Kolter and Andrew Ng. Near-Bayesian exploration in polynomial time. In Int. Conf. on Machine Learning (ICML), pages 513–520, 2009.</p>
<p>[10] P.Y. Oudeyer, F. Kaplan, and V.V. Hafner. Intrinsic motivation systems for autonomous mental development. IEEE Transactions on Evolutionary Computation, 11(2):265–286, 2007.</p>
<p>[11] Pascal Poupart, Nikos Vlassis, Jesse Hoey, and Kevin Regan. An analytic solution to discrete Bayesian reinforcement learning. In Int. Conf. on Machine Learning (ICML), 2006.</p>
<p>[12] J¨ rgen Schmidhuber. Curious model-building control systems. In Proc. of Int. Joint Conf. on u Neural Networks, volume 2, pages 1458–1463, 1991.</p>
<p>[13] Satinder Singh, Andrew G. Barto, and Nuttapong Chentanez. Intrinsically motivated reinforcement learning. In Conf. on Neural Information Processing Systems (NIPS), pages 1281–1288. 2005.</p>
<p>[14] Alexander L. Strehl, Lihong Li, and Michael Littman. Reinforcement learning in ﬁnite MDPs: PAC analysis. Journal of Machine Learning Research (JMLR), 2009.</p>
<p>[15] Alexander L. Strehl and Michael L. Littman. An analysis of model-based interval estimation for markov decision processes. J. Comput. Syst. Sci., 74(8):1309–1331, 2008.</p>
<p>[16] Christopher M. Vigorito and Andrew G. Barto. Intrinsically motivated hierarchical skill learning in structured environments. IEEE Transactions on Autonomous Mental Development (TAMD), 2(2), 2010.</p>
<p>[17] Marco Wiering and J¨ rgen Schmidhuber. Efﬁcient model-based exploration. In International u Conference on Simulation of Adaptive Behavior: From Animals to Animats 6, 1998.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
