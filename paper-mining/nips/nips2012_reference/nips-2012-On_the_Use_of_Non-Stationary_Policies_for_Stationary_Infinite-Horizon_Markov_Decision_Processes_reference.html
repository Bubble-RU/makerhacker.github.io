<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>255 nips-2012-On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-255" href="../nips2012/nips-2012-On_the_Use_of_Non-Stationary_Policies_for_Stationary_Infinite-Horizon_Markov_Decision_Processes.html">nips2012-255</a> <a title="nips-2012-255-reference" href="#">nips2012-255-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>255 nips-2012-On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes</h1>
<br/><p>Source: <a title="nips-2012-255-pdf" href="http://papers.nips.cc/paper/4800-on-the-use-of-non-stationary-policies-for-stationary-infinite-horizon-markov-decision-processes.pdf">pdf</a></p><p>Author: Bruno Scherrer, Boris Lesner</p><p>Abstract: We consider inﬁnite-horizon stationary γ-discounted Markov Decision Processes, for which it is known that there exists a stationary optimal policy. Using Value and Policy Iteration with some error at each iteration, it is well-known that one 2γ can compute stationary policies that are (1−γ)2 -optimal. After arguing that this guarantee is tight, we develop variations of Value and Policy Iteration for com2γ puting non-stationary policies that can be up to 1−γ -optimal, which constitutes a signiﬁcant improvement in the usual situation when γ is close to 1. Surprisingly, this shows that the problem of “computing near-optimal non-stationary policies” is much simpler than that of “computing near-optimal stationary policies”. 1</p><br/>
<h2>reference text</h2><p>[1] A. Antos, Cs. Szepesv´ ri, and R. Munos. Learning near-optimal policies with Bellmana residual minimization based ﬁtted policy iteration and a single sample path. Machine Learning, 71(1):89–129, 2008.</p>
<p>[2] M. Gheshlaghi Azar, V. Gmez, and H.J. Kappen. Dynamic Policy Programming with Function Approximation. In 14th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), volume 15, Fort Lauderdale, FL, USA, 2011.</p>
<p>[3] D.P. Bertsekas. Approximate policy iteration: a survey and some new methods. Journal of Control Theory and Applications, 9:310–335, 2011.</p>
<p>[4] D.P. Bertsekas and J.N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc, 1996.</p>
<p>[5] L. Busoniu, A. Lazaric, M. Ghavamzadeh, R. Munos, R. Babuska, and B. De Schutter. Leastsquares methods for Policy Iteration. In M. Wiering and M. van Otterlo, editors, Reinforcement Learning: State of the Art. Springer, 2011.</p>
<p>[6] D. Ernst, P. Geurts, and L. Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research (JMLR), 6, 2005. 2  With this choice of m, we have m ≥  1 log 1/γ  and thus  8  2 1−γ m  ≤  2 1−e−1  ≤ 3.164.</p>
<p>[7] E. Even-dar. Planning in pomdps using multiplicity automata. In Uncertainty in Artiﬁcial Intelligence (UAI, pages 185–192, 2005.</p>
<p>[8] A.M. Farahmand, M. Ghavamzadeh, Cs. Szepesv´ ri, and S. Mannor. Regularized policy iteraa tion. Advances in Neural Information Processing Systems, 21:441–448, 2009.</p>
<p>[9] A.M. Farahmand, R. Munos, and Cs. Szepesv´ ri. Error propagation for approximate policy a and value iteration (extended version). In NIPS, December 2010.</p>
<p>[10] V. Gabillon, A. Lazaric, M. Ghavamzadeh, and B. Scherrer. Classiﬁcation-based Policy Iteration with a Critic. In International Conference on Machine Learning (ICML), pages 1049– ´ 1056, Seattle, Etats-Unis, June 2011.</p>
<p>[11] G.J. Gordon. Stable Function Approximation in Dynamic Programming. In ICML, pages 261–268, 1995.</p>
<p>[12] C. Guestrin, D. Koller, and R. Parr. Max-norm projections for factored MDPs. In International Joint Conference on Artiﬁcial Intelligence, volume 17-1, pages 673–682, 2001.</p>
<p>[13] C. Guestrin, D. Koller, R. Parr, and S. Venkataraman. Efﬁcient Solution Algorithms for Factored MDPs. Journal of Artiﬁcial Intelligence Research (JAIR), 19:399–468, 2003.</p>
<p>[14] S.M. Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, University College London, 2003.</p>
<p>[15] S.M. Kakade and J. Langford. Approximately Optimal Approximate Reinforcement Learning. In International Conference on Machine Learning (ICML), pages 267–274, 2002.</p>
<p>[16] M.G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning Research (JMLR), 4:1107–1149, 2003.</p>
<p>[17] A. Lazaric, M. Ghavamzadeh, and R. Munos. Finite-Sample Analysis of Least-Squares Policy Iteration. To appear in Journal of Machine learning Research (JMLR), 2011.</p>
<p>[18] O.A. Maillard, R. Munos, A. Lazaric, and M. Ghavamzadeh. Finite Sample Analysis of Bellman Residual Minimization. In Masashi Sugiyama and Qiang Yang, editors, Asian Conference on Machine Learpning. JMLR: Workshop and Conference Proceedings, volume 13, pages 309– 324, 2010.</p>
<p>[19] R. Munos. Error Bounds for Approximate Policy Iteration. In International Conference on Machine Learning (ICML), pages 560–567, 2003.</p>
<p>[20] R. Munos. Performance Bounds in Lp norm for Approximate Value Iteration. SIAM J. Control and Optimization, 2007.</p>
<p>[21] R. Munos and Cs. Szepesv´ ri. Finite time bounds for sampling based ﬁtted value iteration. a Journal of Machine Learning Research (JMLR), 9:815–857, 2008.</p>
<p>[22] M. Petrik and B. Scherrer. Biasing Approximate Dynamic Programming with a Lower Discount Factor. In Twenty-Second Annual Conference on Neural Information Processing Systems -NIPS 2008, Vancouver, Canada, 2008.</p>
<p>[23] J. Pineau, G.J. Gordon, and S. Thrun. Point-based value iteration: An anytime algorithm for POMDPs. In International Joint Conference on Artiﬁcial Intelligence, volume 18, pages 1025–1032, 2003.</p>
<p>[24] M. Puterman. Markov Decision Processes. Wiley, New York, 1994.</p>
<p>[25] S. Singh and R. Yee. An Upper Bound on the Loss from Approximate Optimal-Value Functions. Machine Learning, 16-3:227–233, 1994.</p>
<p>[26] C. Thiery and B. Scherrer. Least-Squares λ Policy Iteration: Bias-Variance Trade-off in Control Problems. In International Conference on Machine Learning, Haifa, Israel, 2010.</p>
<p>[27] J.N. Tsitsiklis and B. Van Roy. Feature-Based Methods for Large Scale Dynamic Programming. Machine Learning, 22(1-3):59–94, 1996.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
