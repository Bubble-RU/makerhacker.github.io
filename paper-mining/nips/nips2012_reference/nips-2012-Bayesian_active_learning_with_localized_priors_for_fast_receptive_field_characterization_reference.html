<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 nips-2012-Bayesian active learning with localized priors for fast receptive field characterization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-56" href="../nips2012/nips-2012-Bayesian_active_learning_with_localized_priors_for_fast_receptive_field_characterization.html">nips2012-56</a> <a title="nips-2012-56-reference" href="#">nips2012-56-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>56 nips-2012-Bayesian active learning with localized priors for fast receptive field characterization</h1>
<br/><p>Source: <a title="nips-2012-56-pdf" href="http://papers.nips.cc/paper/4552-bayesian-active-learning-with-localized-priors-for-fast-receptive-field-characterization.pdf">pdf</a></p><p>Author: Mijung Park, Jonathan W. Pillow</p><p>Abstract: Active learning methods can dramatically improve the yield of neurophysiology experiments by adaptively selecting stimuli to probe a neuron’s receptive ﬁeld (RF). Bayesian active learning methods specify a posterior distribution over the RF given the data collected so far in the experiment, and select a stimulus on each time step that maximally reduces posterior uncertainty. However, existing methods tend to employ simple Gaussian priors over the RF and do not exploit uncertainty at the level of hyperparameters. Incorporating this uncertainty can substantially speed up active learning, particularly when RFs are smooth, sparse, or local in space and time. Here we describe a novel framework for active learning under hierarchical, conditionally Gaussian priors. Our algorithm uses sequential Markov Chain Monte Carlo sampling (“particle ﬁltering” with MCMC) to construct a mixture-of-Gaussians representation of the RF posterior, and selects optimal stimuli using an approximate infomax criterion. The core elements of this algorithm are parallelizable, making it computationally efﬁcient for real-time experiments. We apply our algorithm to simulated and real neural data, and show that it can provide highly accurate receptive ﬁeld estimates from very limited data, even with a small number of hyperparameter samples. 1</p><br/>
<h2>reference text</h2><p>[1] D. J. C. MacKay. Information-based objective functions for active data selection. Neural Computation, 4(4):590–604, 1992.</p>
<p>[2] K. Chaloner and I. Verdinelli. Bayesian experimental design: a review. Statistical Science, 10:273–304, 1995.</p>
<p>[3] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active learning with statistical models. J. Artif. Intell. Res. (JAIR), 4:129–145, 1996.</p>
<p>[4] A. Watson and D. Pelli. QUEST: a Bayesian adaptive psychophysical method. Perception and Psychophysics, 33:113–120, 1983.</p>
<p>[5] L. Paninski. Asymptotic theory of information-theoretic experimental design. Neural Computation, 17(7):1480–1507, 2005.</p>
<p>[6] J. Lewi, R. Butera, and L. Paninski. Sequential optimal design of neurophysiology experiments. Neural Computation, 21(3):619–687, 2009.</p>
<p>[7] W. Truccolo, U. T. Eden, M. R. Fellows, J. P. Donoghue, and E. N. Brown. A point process framework for relating neural spiking activity to spiking history, neural ensemble and extrinsic covariate effects. J. Neurophysiol, 93(2):1074–1089, 2005.</p>
<p>[8] M. Sahani and J. Linden. Evidence optimization techniques for estimating stimulus-response functions. NIPS, 15, 2003.</p>
<p>[9] M. Park and J. W. Pillow. 7(10):e1002219, 2011.  Receptive ﬁeld inference with localized priors.  PLoS Comput Biol,</p>
<p>[10] N. Houlsby, F. Huszar, Z. Ghahramani, and M. Lengyel. Bayesian active learning for classiﬁcation and preference learning. CoRR, abs/1112.5745, 2011.</p>
<p>[11] L. Paninski. Maximum likelihood estimation of cascade point-process neural encoding models. Network: Computation in Neural Systems, 15:243–262, 2004.</p>
<p>[12] R. Kass and A. Raftery. Bayes factors. Journal of the American Statistical Association, 90:773–795, 1995.</p>
<p>[13] J. W. Pillow, Y. Ahmadian, and L. Paninski. Model-based decoding, information estimation, and changepoint detection techniques for multineuron spike trains. Neural Comput, 23(1):1–45, Jan 2011.</p>
<p>[14] W. R. Gilks and C. Berzuini. Following a moving target – monte carlo inference for dynamic bayesian models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(1):127–146, 2001.</p>
<p>[15] N. C. Rust, Schwartz O., J. A. Movshon, and Simoncelli E.P. Spatiotemporal elements of macaque v1 receptive ﬁelds. Neuron, 46(6):945–956, 2005.  9</p>
<br/>
<br/><br/><br/></body>
</html>
