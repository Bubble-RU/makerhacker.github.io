<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>189 nips-2012-Learning from the Wisdom of Crowds by Minimax Entropy</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-189" href="../nips2012/nips-2012-Learning_from_the_Wisdom_of_Crowds_by_Minimax_Entropy.html">nips2012-189</a> <a title="nips-2012-189-reference" href="#">nips2012-189-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>189 nips-2012-Learning from the Wisdom of Crowds by Minimax Entropy</h1>
<br/><p>Source: <a title="nips-2012-189-pdf" href="http://papers.nips.cc/paper/4490-learning-from-the-wisdom-of-crowds-by-minimax-entropy.pdf">pdf</a></p><p>Author: Dengyong Zhou, Sumit Basu, Yi Mao, John C. Platt</p><p>Abstract: An important way to make large training sets is to gather noisy labels from crowds of nonexperts. We propose a minimax entropy principle to improve the quality of these labels. Our method assumes that labels are generated by a probability distribution over workers, items, and labels. By maximizing the entropy of this distribution, the method naturally infers item confusability and worker expertise. We infer the ground truth by minimizing the entropy of this distribution, which we show minimizes the Kullback-Leibler (KL) divergence between the probability distribution and the unknown truth. We show that a simple coordinate descent scheme can optimize minimax entropy. Empirically, our results are substantially better than previously published methods for the same problem. 1</p><br/>
<h2>reference text</h2><p>[1] Y. Altun and A. Smola. Unifying divergence minimization and statistical inference via convex duality. In Proceedings of the 19th Annual Conference on Learning Theory, 2006.</p>
<p>[2] Amazon Mechanical Turk. https://www.mturk.com/mturk.</p>
<p>[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.</p>
<p>[4] CrowdFlower. http://crowdﬂower.com/.</p>
<p>[5] A. P. Dawid and A. M. Skene. Maximum likeihood estimation of observer error-rates using the EM algorithm. Journal of the Royal Statistical Society, 28(1):20–28, 1979.</p>
<p>[6] O. Dekel and O. Shamir. Vox populi: Collecting high-quality labels from a crowd. In Proceedings of the 22nd Annual Conference on Learning Theory, 2009.</p>
<p>[7] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1):1–38, 1977.</p>
<p>[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255, 2009.</p>
<p>[9] M. Dudik, S. J. Phillips, and R. E. Schapire. Maximum entropy density estimation with generalized regularization and an application to species distribution modeling. Journal of Machine Learning Research, 8:1217–1260, 2007.</p>
<p>[10] S. Ertekin, H. Hirsh, and C. Rudin. Approximating the wisdom of the crowd. In Proceedings of the Workshop on Computational Social Science and the Wisdom of Crowds, 2011.</p>
<p>[11] P. G. Ipeirotis, F. Provost, and J. Wang. Quality management on Amazon Mechanical Turk. In Proceedings of the ACM SIGKDD Workshop on Human Computation, pages 64–67, 2010.</p>
<p>[12] E. Kamar, S. Hacker, and E. Horvitz. Combining human and machine intelligence in largescale crowdsourcing. In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems, pages 467–474, 2012.</p>
<p>[13] D. R. Karger, S. Oh, and D. Shah. Iterative learning for reliable crowdsourcing systems. In Advances in Neural Information Processing Systems 24, pages 1953–1961, 2011.</p>
<p>[14] G. Lebanon and J. Lafferty. Boosting and maximum likelihood for exponential models. In Advances in Neural Information Processing Systems 14, pages 447–454, 2001.</p>
<p>[15] R. M. Neal and G. E. Hinton. A view of the EM algorithm that justiﬁes incremental, sparse, and other variants. In M. I. Jordan, editor, Learning in Graphical Models, pages 355–368. Kluwer Academic, Dordrecht, MA, 1998.</p>
<p>[16] G. Rasch. On general laws and the meaning of measurement in psychology. In Proceedings of the 4th Berkeley Symposium on Mathematical Statistics and Probability, volume 4, pages 321–333, Berkeley, CA, 1961.</p>
<p>[17] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni, and L. Moy. Learning from crowds. Journal of Machine Learning Research, 11:1297–1322, 2010.</p>
<p>[18] P. Smyth, U. Fayyad, M. Burl, P. Perona, and P. Baldi. Inferring ground truth from subjective labelling of venus images. In Advances in neural information processing systems, pages 1085– 1092, 1995.</p>
<p>[19] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng. Cheap and fast—but is it good? Evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 254–263, 2008.</p>
<p>[20] P. Welinder, S. Branson, S. Belongie, and P. Perona. The multidimensional wisdom of crowds. In Advances in Neural Information Processing Systems 23, pages 2424–2432, 2010.</p>
<p>[21] J. Whitehill, P. Ruvolo, T. Wu, J. Bergsma, and J. Movellan. Whose vote should count more: optimal integration of labels from labelers of unknown expertise. In Advances in Neural Information Processing Systems 22, pages 2035–2043, 2009.</p>
<p>[22] S. C. Zhu, Y. N. Wu, and D. B. Mumford. Minimax entropy principle and its applications to texture modeling. Neural Computation, 9:1627–1660, 1997.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
