<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>152 nips-2012-Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-152" href="../nips2012/nips-2012-Homeostatic_plasticity_in_Bayesian_spiking_networks_as_Expectation_Maximization_with_posterior_constraints.html">nips2012-152</a> <a title="nips-2012-152-reference" href="#">nips2012-152-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>152 nips-2012-Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints</h1>
<br/><p>Source: <a title="nips-2012-152-pdf" href="http://papers.nips.cc/paper/4593-homeostatic-plasticity-in-bayesian-spiking-networks-as-expectation-maximization-with-posterior-constraints.pdf">pdf</a></p><p>Author: Stefan Habenschuss, Johannes Bill, Bernhard Nessler</p><p>Abstract: Recent spiking network models of Bayesian inference and unsupervised learning frequently assume either inputs to arrive in a special format or employ complex computations in neuronal activation functions and synaptic plasticity rules. Here we show in a rigorous mathematical treatment how homeostatic processes, which have previously received little attention in this context, can overcome common theoretical limitations and facilitate the neural implementation and performance of existing models. In particular, we show that homeostatic plasticity can be understood as the enforcement of a ’balancing’ posterior constraint during probabilistic inference and learning with Expectation Maximization. We link homeostatic dynamics to the theory of variational inference, and show that nontrivial terms, which typically appear during probabilistic inference in a large class of models, drop out. We demonstrate the feasibility of our approach in a spiking WinnerTake-All architecture of Bayesian inference and learning. Finally, we sketch how the mathematical framework can be extended to richer recurrent network architectures. Altogether, our theory provides a novel perspective on the interplay of homeostatic processes and synaptic plasticity in cortical microcircuits, and points to an essential role of homeostasis during inference and learning in spiking networks. 1</p><br/>
<h2>reference text</h2><p>[1] K. P. K¨ rding and D. M. Wolpert. Bayesian integration in sensorimotor learning. Nature, 427(6971):244– o 247, 2004.</p>
<p>[2] G. Orban, J. Fiser, R.N. Aslin, and M. Lengyel. Bayesian learning of visual chunks by human observers. Proceedings of the National Academy of Sciences, 105(7):2745–2750, 2008.</p>
<p>[3] J. Fiser, P. Berkes, G. Orban, and M. Lengyel. Statistically optimal perception and learning: from behavior to neural representation. Trends in Cogn. Sciences, 14(3):119–130, 2010.</p>
<p>[4] P. Berkes, G. Orban, M. Lengyel, and J. Fiser. Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment. Science, 331:83–87, 2011.</p>
<p>[5] T. L. Grifﬁths and J. B. Tenenbaum. Optimal predictions in everyday cognition. Psychological Science, 17(9):767–773, 2006.</p>
<p>[6] D. E. Angelaki, Y. Gu, and G. C. DeAngelis. Multisensory integration: psychophysics, neurophysiology and computation. Current opinion in neurobiology, 19(4):452–458, 2009.</p>
<p>[7] S. Deneve. Bayesian spiking neurons I: Inference. Neural Computation, 20(1):91–117, 2008.</p>
<p>[8] A. Steimer, W. Maass, and R.J. Douglas. Belief propagation in networks of spiking neurons. Neural Computation, 21:2502–2523, 2009.</p>
<p>[9] L. Buesing, J. Bill, B. Nessler, and W. Maass. Neural dynamics as sampling: A model for stochastic computation in recurrent networks of spiking neurons. PLoS Comput Biol, 7(11):e1002211, 11 2011.</p>
<p>[10] D. Pecevski, L. Buesing, and W. Maass. Probabilistic inference in general graphical models through sampling in stochastic networks of spiking neurons. PLoS Comput Biol, 7(12), 12 2011.</p>
<p>[11] S. Deneve. Bayesian spiking neurons II: Learning. Neural Computation, 20(1):118–145, 2008.</p>
<p>[12] B. Nessler, M. Pfeiffer, and W. Maass. STDP enables spiking neurons to detect hidden causes of their inputs. In Proc. of NIPS 2009, volume 22, pages 1357–1365. MIT Press, 2010.</p>
<p>[13] J. Brea, W. Senn, and J.-P. Pﬁster. Sequence learning with hidden units in spiking neural networks. In Proc. of NIPS 2011, volume 24, pages 1422–1430. MIT Press, 2012.</p>
<p>[14] D. J. Rezende, D. Wierstra, and W. Gerstner. Variational learning for recurrent spiking networks. In Proc. of NIPS 2011, volume 24, pages 136–144. MIT Press, 2012.</p>
<p>[15] C. Keck, C. Savin, and J. L¨ cke. Feedforward inhibition and synaptic scaling–two sides of the same coin? u PLoS Computational Biology, 8(3):e1002432, 2012.</p>
<p>[16] Joshua B. Tenenbaum, Charles Kemp, Thomas L. Grifﬁths, and Noah D. Goodman. How to grow a mind: Statistics, structure, and abstraction. Science, 331(6022):1279–1285, 2011.</p>
<p>[17] J. Schemmel, D. Br¨ derle, A. Gr¨ bl, M. Hock, K. Meier, and S. Millner. A wafer-scale neuromorphic u u hardware system for large-scale neural modeling. Proc. of ISCAS’10, pages 1947–1950, 2010.</p>
<p>[18] N.S. Desai, L.C. Rutherford, and G.G. Turrigiano. Plasticity in the intrinsic excitability of cortical pyramidal neurons. Nature Neuroscience, 2(6):515, 1999.</p>
<p>[19] A. Watt and N. Desai. Homeostatic plasticity and STDP: keeping a neurons cool in a ﬂuctuating world. Frontiers in Synaptic Neuroscience, 2, 2010.</p>
<p>[20] J. Graca, K. Ganchev, and B. Taskar. Expectation maximization and posterior constraints. In Proc. of NIPS 2007, volume 20. MIT Press, 2008.</p>
<p>[21] R. Jolivet, A. Rauch, HR L¨ scher, and W. Gerstner. Predicting spike timing of neocortical pyramidal u neurons by simple threshold models. Journal of Computational Neuroscience, 21:35–49, 2006.</p>
<p>[22] E.P. Simoncelli and D.J. Heeger. A model of neuronal responses in visual area MT. Vision Research, 38(5):743–761, 1998.</p>
<p>[23] C. M. Bishop. Pattern Recognition and Machine Learning. Springer, New York, 2006.</p>
<p>[24] M. Sato. Fast learning of on-line EM algorithm. Rapport Technique, ATR Human Information Processing Research Laboratories, 1999.</p>
<p>[25] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, volume 86, pages 2278–2324, 11 1998.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
