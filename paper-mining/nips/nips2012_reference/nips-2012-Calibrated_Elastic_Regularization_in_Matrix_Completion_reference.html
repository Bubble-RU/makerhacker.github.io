<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>64 nips-2012-Calibrated Elastic Regularization in Matrix Completion</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-64" href="../nips2012/nips-2012-Calibrated_Elastic_Regularization_in_Matrix_Completion.html">nips2012-64</a> <a title="nips-2012-64-reference" href="#">nips2012-64-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>64 nips-2012-Calibrated Elastic Regularization in Matrix Completion</h1>
<br/><p>Source: <a title="nips-2012-64-pdf" href="http://papers.nips.cc/paper/4823-calibrated-elastic-regularization-in-matrix-completion.pdf">pdf</a></p><p>Author: Tingni Sun, Cun-hui Zhang</p><p>Abstract: This paper concerns the problem of matrix completion, which is to estimate a matrix from observations in a small subset of indices. We propose a calibrated spectrum elastic net method with a sum of the nuclear and Frobenius penalties and develop an iterative algorithm to solve the convex minimization problem. The iterative algorithm alternates between imputing the missing entries in the incomplete matrix by the current guess and estimating the matrix by a scaled soft-thresholding singular value decomposition of the imputed matrix until the resulting matrix converges. A calibration step follows to correct the bias caused by the Frobenius penalty. Under proper coherence conditions and for suitable penalties levels, we prove that the proposed estimator achieves an error bound of nearly optimal order and in proportion to the noise level. This provides a uniﬁed analysis of the noisy and noiseless matrix completion problems. Simulation results are presented to compare our proposal with previous ones. 1</p><br/>
<h2>reference text</h2><p>[1] ACM SIGKDD and Netﬂix. Proceedings of KDD Cup and workshop. 2007.</p>
<p>[2] E. Candes and B. Recht. Exact matrix completion via convex optimization. Found. Comput. Math., 9:717–772, 2009.</p>
<p>[3] E. J. Cand` s and T. Tao. The power of convex relaxation: Near-optimal matrix completion. e IEEE Trans. Inform. Theory, 56(5):2053–2080, 2009.</p>
<p>[4] D. Gross. Recovering low-rank matrices from few coefﬁcients in any basis. abs/0910.1879, 2009.  CoRR,</p>
<p>[5] R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries. IEEE Transactions on Information Theory, 56(6):2980–2998, 2010.</p>
<p>[6] R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. Journal of Machine Learning Research, 11:2057–2078, 2010.</p>
<p>[7] V. Koltchinskii, K. Lounici, and A. B. Tsybakov. Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion. The Annals of Statistics, 39:2302–2329, 2011.</p>
<p>[8] R. Mazumder, T. Hastie, and R. Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. Journal of Machine Learning Research, 11:2287–2322, 2010.</p>
<p>[9] S. Negahban and M. J. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. 2010.</p>
<p>[10] R. I. Oliveira. Concentration of the adjacency matrix and of the laplacian in random graphs with independent edges. Technical Report arXiv:0911.0600, arXiv, 2010.</p>
<p>[11] B. Recht. A simpler approach to matrix completion. Journal of Machine Learning Research, 12:3413–3430, 2011.</p>
<p>[12] J. A. Tropp. User-friendly tail bounds for sums of random matrices. Found. Comput. Math. doi:10.1007/s10208-011-9099-z., 2011.</p>
<p>[13] P.-A. Wedin. Perturbation bounds in connection with singular value decomposition. BIT, 12:99–111, 1972.</p>
<p>[14] C.-H. Zhang and T. Zhang. A general framework of dual certiﬁcate analysis for structured sparse recovery problems. Technical report, arXiv: 1201.3302v1, 2012.</p>
<p>[15] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. J. R. Statist. Soc. B, 67:301–320, 2005.  9</p>
<br/>
<br/><br/><br/></body>
</html>
