<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>247 nips-2012-Nonparametric Reduced Rank Regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-247" href="../nips2012/nips-2012-Nonparametric_Reduced_Rank_Regression.html">nips2012-247</a> <a title="nips-2012-247-reference" href="#">nips2012-247-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>247 nips-2012-Nonparametric Reduced Rank Regression</h1>
<br/><p>Source: <a title="nips-2012-247-pdf" href="http://papers.nips.cc/paper/4843-nonparametric-reduced-rank-regression.pdf">pdf</a></p><p>Author: Rina Foygel, Michael Horrell, Mathias Drton, John D. Lafferty</p><p>Abstract: We propose an approach to multivariate nonparametric regression that generalizes reduced rank regression for linear models. An additive model is estimated for each dimension of a q-dimensional response, with a shared p-dimensional predictor variable. To control the complexity of the model, we employ a functional form of the Ky-Fan or nuclear norm, resulting in a set of function estimates that have low rank. Backﬁtting algorithms are derived and justiﬁed using a nonparametric form of the nuclear norm subdifferential. Oracle inequalities on excess risk are derived that exhibit the scaling behavior of the procedure in the high dimensional setting. The methods are illustrated on gene expression data. 1</p><br/>
<h2>reference text</h2><p>[1] Nicol` Cesa-Bianchi and G´ bor Lugosi. On prediction of individual sequences. The Annals of o a Statistics, 27(6):1865–1894, 1999.</p>
<p>[2] Maryam Fazel. Matrix rank minimization with applications. Technical report, Stanford University, 2002. Doctoral Dissertation, Electrical Engineering Department.</p>
<p>[3] D. Marbach, J. C. Costello, R. K¨ ffner, N. Vega, R. J. Prill, D. M. Camacho, K. R. Allison, u the DREAM5 Consortium, M. Kellis, J. J. Collins, and G. Stolovitzky. Wisdom of crowds for robust gene network inference. Nature Methods, 9(8):796–804, 2012.</p>
<p>[4] Sahan Negahban and Martin J. Wainwright. Estimation of (near) low-rank matrices with noise and high-dimensional scaling. Annals of Statistics, 39:1069–1097, 2011.</p>
<p>[5] Garvesh Raskutti, Martin J. Wainwright, and Bin Yu. Minimax-optimal rates for sparse additive models over kernel classes via convex programming. arxiv:1008.3654, 2010.</p>
<p>[6] Pradeep Ravikumar, John Lafferty, Han Liu, and Larry Wasserman. Sparse additive models. Journal of the Royal Statistical Society, Series B, Methodological, 71(5):1009–1030, 2009.</p>
<p>[7] Benjamin Recht, Maryam Fazel, and Pablo A. Parrilo. Guaranteed minimum rank solutions to linear matrix equations via nuclear norm minimization. SIAM Review, 52(3):471–501, 2010.</p>
<p>[8] Roman Vershynin. How close is the sample covariance matrix to the actual covariance matrix? arxiv:1004.3484, 2010.</p>
<p>[9] G. A. Watson. Characterization of the subdifferential of some matrix norms. Linear Algebra and Applications, 170:1039–1053, 1992.</p>
<p>[10] Ming Yuan, Ali Ekici, Zhaosong Lu, and Renato Monteiro. Dimension reduction and coeffcient estimation in multivariate linear regression. J. R. Statist. Soc. B, 69(3):329–346, 2007.</p>
<p>[11] Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49–67, 2006.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
