<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>61 nips-2012-Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-61" href="../nips2012/nips-2012-Best_Arm_Identification%3A_A_Unified_Approach_to_Fixed_Budget_and_Fixed_Confidence.html">nips2012-61</a> <a title="nips-2012-61-reference" href="#">nips2012-61-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>61 nips-2012-Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence</h1>
<br/><p>Source: <a title="nips-2012-61-pdf" href="http://papers.nips.cc/paper/4640-best-arm-identification-a-unified-approach-to-fixed-budget-and-fixed-confidence.pdf">pdf</a></p><p>Author: Victor Gabillon, Mohammad Ghavamzadeh, Alessandro Lazaric</p><p>Abstract: We study the problem of identifying the best arm(s) in the stochastic multi-armed bandit setting. This problem has been studied in the literature from two different perspectives: ﬁxed budget and ﬁxed conﬁdence. We propose a unifying approach that leads to a meta-algorithm called uniﬁed gap-based exploration (UGapE), with a common structure and similar theoretical analysis for these two settings. We prove a performance bound for the two versions of the algorithm showing that the two problems are characterized by the same notion of complexity. We also show how the UGapE algorithm as well as its theoretical analysis can be extended to take into account the variance of the arms and to multiple bandits. Finally, we evaluate the performance of UGapE and compare it with a number of existing ﬁxed budget and ﬁxed conﬁdence algorithms. 1</p><br/>
<h2>reference text</h2><p>[1] J.-Y. Audibert, S. Bubeck, and R. Munos. Best arm identiﬁcation in multi-armed bandits. In Proceedings of the Twenty-Third Annual Conference on Learning Theory, pages 41–53, 2010.</p>
<p>[2] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multi-armed bandit problem. Machine Learning, 47:235–256, 2002.</p>
<p>[3] S. Bubeck, R. Munos, and G. Stoltz. Pure exploration in multi-armed bandit problems. In Proceedings of the Twentieth International Conference on Algorithmic Learning Theory, pages 23–37, 2009.</p>
<p>[4] S. Bubeck, T. Wang, and N. Viswanathan. Multiple identiﬁcations in multi-armed bandits. CoRR, abs/1205.3181, 2012.</p>
<p>[5] K. Deng, J. Pineau, and S. Murphy. Active learning for developing personalized treatment. In Proceedings of the Twenty-Seventh International Conference on Uncertainty in Artiﬁcial Intelligence, pages 161–168, 2011.</p>
<p>[6] E. Even-Dar, S. Mannor, and Y. Mansour. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of Machine Learning Research, 7:1079–1105, 2006.</p>
<p>[7] V. Gabillon, M. Ghavamzadeh, and A. Lazaric. Best Arm Identiﬁcation: A Uniﬁed Approach to Fixed Budget and Fixed Conﬁdence. Technical report 00747005, October 2012.</p>
<p>[8] V. Gabillon, M. Ghavamzadeh, A. Lazaric, and S. Bubeck. Multi-bandit best arm identiﬁcation. In Proceedings of Advances in Neural Information Processing Systems 25, pages 2222–2230, 2011.</p>
<p>[9] S. Kalyanakrishnan. Learning Methods for Sequential Decision Making with Imperfect Representations. PhD thesis, Department of Computer Science, The University of Texas at Austin, Austin, Texas, USA, December 2011. Published as UT Austin Computer Science Technical Report TR-11-41.</p>
<p>[10] S. Kalyanakrishnan and P. Stone. Efﬁcient selection of multiple bandit arms: Theory and practice. In Proceedings of the Twenty-Seventh International Conference on Machine Learning, pages 511–518, 2010.</p>
<p>[11] S. Kalyanakrishnan, A. Tewari, P. Auer, and P. Stone. Pac subset selection in stochastic multiarmed bandits. In Proceedings of the Twentieth International Conference on Machine Learning, 2012.</p>
<p>[12] O. Maron and A. Moore. Hoeffding races: Accelerating model selection search for classiﬁcation and function approximation. In Proceedings of Advances in Neural Information Processing Systems 6, pages 59–66, 1993.</p>
<p>[13] A. Maurer and M. Pontil. Empirical bernstein bounds and sample-variance penalization. In 22th annual conference on learning theory, 2009.</p>
<p>[14] V. Mnih, Cs. Szepesvári, and J.-Y. Audibert. Empirical Bernstein stopping. In Proceedings of the Twenty-Fifth International Conference on Machine Learning, pages 672–679, 2008.</p>
<p>[15] H. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematics Society, 58:527–535, 1952.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
