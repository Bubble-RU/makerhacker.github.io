<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>330 nips-2012-Supervised Learning with Similarity Functions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-330" href="../nips2012/nips-2012-Supervised_Learning_with_Similarity_Functions.html">nips2012-330</a> <a title="nips-2012-330-reference" href="#">nips2012-330-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>330 nips-2012-Supervised Learning with Similarity Functions</h1>
<br/><p>Source: <a title="nips-2012-330-pdf" href="http://papers.nips.cc/paper/4508-supervised-learning-with-similarity-functions.pdf">pdf</a></p><p>Author: Purushottam Kar, Prateek Jain</p><p>Abstract: We address the problem of general supervised learning when data can only be accessed through an (indeﬁnite) similarity function between data points. Existing work on learning with indeﬁnite kernels has concentrated solely on binary/multiclass classiﬁcation problems. We propose a model that is generic enough to handle any supervised learning task and also subsumes the model previously proposed for classiﬁcation. We give a “goodness” criterion for similarity functions w.r.t. a given supervised learning task and then adapt a well-known landmarking technique to provide efﬁcient algorithms for supervised learning using “good” similarity functions. We demonstrate the effectiveness of our model on three important supervised learning problems: a) real-valued regression, b) ordinal regression and c) ranking where we show that our method guarantees bounded generalization error. Furthermore, for the case of real-valued regression, we give a natural goodness deﬁnition that, when used in conjunction with a recent result in sparse vector recovery, guarantees a sparse predictor with bounded generalization error. Finally, we report results of our learning algorithms on regression and ordinal regression tasks using non-PSD similarity functions and demonstrate the effectiveness of our algorithms, especially that of the sparse landmark selection algorithm that achieves signiﬁcantly higher accuracies than the baseline methods while offering reduced computational costs. 1</p><br/>
<h2>reference text</h2><p>[1] Bernhard Sch¨ lkopf and Alex J. Smola. Learning with Kernels : Support Vector Machines, Regularization, o Optimization, and Beyond. MIT Press, 2002.</p>
<p>[2] Bernard Haasdonk. Feature Space Interpretation of SVMs with Indeﬁnite Kernels. IEEE Transactions on Pattern Analysis and Machince Intelligence, 27(4):482–492, 2005.</p>
<p>[3] Cheng Soon Ong, Xavier Mary, St´ phane Canu, and Alexander J. Smola. Learning with non-positive e Kernels. In 21st Annual International Conference on Machine Learning, 2004.</p>
<p>[4] Yihua Chen, Maya R. Gupta, and Benjamin Recht. Learning Kernels from Indeﬁnite Similarities. In 26th Annual International Conference on Machine Learning, pages 145–152, 2009.</p>
<p>[5] Ronny Luss and Alexandre d’Aspremont. Support Vector Machine Classiﬁcation with Indeﬁnite Kernels. In 21st Annual Conference on Neural Information Processing Systems, 2007.</p>
<p>[6] Maria-Florina Balcan and Avrim Blum. On a Theory of Learning with Similarity Functions. In 23rd Annual International Conference on Machine Learning, pages 73–80, 2006.</p>
<p>[7] Liwei Wang, Cheng Yang, and Jufu Feng. On Learning with Dissimilarity Functions. In 24th Annual International Conference on Machine Learning, pages 991–998, 2007.</p>
<p>[8] Purushottam Kar and Prateek Jain. Similarity-based Learning via Data Driven Embeddings. In 25th Annual Conference on Neural Information Processing Systems, 2011.</p>
<p>[9] Nathan Srebro. How Good Is a Kernel When Used as a Similarity Measure? In 20th Annual Conference on Computational Learning Theory, pages 323–335, 2007.</p>
<p>[10] Shai Shalev-Shwartz, Nathan Srebro, and Tong Zhang. Trading Accuracy for Sparsity in Optimization Problems with Sparsity Constraints. SIAM Journal on Optimization, 20(6):2807–2832, 2010.</p>
<p>[11] Nathan Srebro Shai Ben-David, Ali Rahimi. Generalization Bounds for Indeﬁnite Kernel Machines. In NIPS 2008 Workshop: New Challenges in Theoretical Machine Learning, 2008.</p>
<p>[12] Yihua Chen, Eric K. Garcia, Maya R. Gupta, Ali Rahimi, and Luca Cazzanti. Similarity-based Classiﬁcation: Concepts and Algorithms. Journal of Machine Learning Research, 10:747–776, 2009.</p>
<p>[13] Sham M. Kakade, Karthik Sridharan, and Ambuj Tewari. On the Complexity of Linear Prediction : Risk Bounds, Margin Bounds, and Regularization. In 22nd Annual Conference on Neural Information Processing Systems, 2008.</p>
<p>[14] Chia-Hua Ho and Chih-Jen Lin. Large-scale Linear Support Vector Regression. http://www.csie. ntu.edu.tw/˜cjlin/papers/linear-svr.pdf, retrieved on May 18, 2012, 2012.</p>
<p>[15] Maria-Florina Balcan, Avrim Blum, and Nathan Srebro. Improved Guarantees for Learning via Similarity Functions. In 21st Annual Conference on Computational Learning Theory, pages 287–298, 2008.</p>
<p>[16] Wei Chu and S. Sathiya Keerthi. Support Vector Ordinal Regression. Neural Computation, 19(3):792– 815, 2007.</p>
<p>[17] Shivani Agarwal. Generalization Bounds for Some Ordinal Regression Algorithms. In 19th International Conference on Algorithmic Learning Theory, pages 7–21, 2008.</p>
<p>[18] A. Frank and Arthur Asuncion. UCI Machine Learning Repository. http://archive.ics.uci. edu/ml, 2010. University of California, Irvine, School of Information and Computer Sciences.</p>
<p>[19] StatLib Dataset Repository. http://lib.stat.cmu.edu/datasets/. Carnegie Mellon University.</p>
<p>[20] Delve Dataset Repository. http://www.cs.toronto.edu/˜delve/data/datasets.html. University of Toronto.</p>
<p>[21] Kilian Q. Weinberger and Gerald Tesauro. Metric Learning for Kernel Regression. In 11th International Conference on Artiﬁcial Intelligence and Statistics, pages 612–619, 2007.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
