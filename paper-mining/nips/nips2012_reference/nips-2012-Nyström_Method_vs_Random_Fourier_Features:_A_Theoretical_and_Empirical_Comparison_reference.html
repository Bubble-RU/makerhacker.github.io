<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>249 nips-2012-Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-249" href="../nips2012/nips-2012-Nystr%C3%B6m_Method_vs_Random_Fourier_Features%3A_A_Theoretical_and_Empirical_Comparison.html">nips2012-249</a> <a title="nips-2012-249-reference" href="#">nips2012-249-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>249 nips-2012-Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison</h1>
<br/><p>Source: <a title="nips-2012-249-pdf" href="http://papers.nips.cc/paper/4588-nystrom-method-vs-random-fourier-features-a-theoretical-and-empirical-comparison.pdf">pdf</a></p><p>Author: Tianbao Yang, Yu-feng Li, Mehrdad Mahdavi, Rong Jin, Zhi-Hua Zhou</p><p>Abstract: Both random Fourier features and the Nystr¨ m method have been successfully o applied to efﬁcient kernel learning. In this work, we investigate the fundamental difference between these two approaches, and how the difference could affect their generalization performances. Unlike approaches based on random Fourier features where the basis functions (i.e., cosine and sine functions) are sampled from a distribution independent from the training data, basis functions used by the Nystr¨ m method are randomly sampled from the training examples and are o therefore data dependent. By exploring this difference, we show that when there is a large gap in the eigen-spectrum of the kernel matrix, approaches based on the Nystr¨ m method can yield impressively better generalization error bound than o random Fourier features based approach. We empirically verify our theoretical ﬁndings on a wide range of large data sets. 1</p><br/>
<h2>reference text</h2><p>[1] A. Azran and Z. Ghahramani. Spectral methods for automatic multiscale data clustering. In CVPR, pages 190–197, 2006.</p>
<p>[2] F. R. Bach and M. I. Jordan. Learning spectral clustering. Technical Report UCB/CSD-031249, EECS Department, University of California, Berkeley, 2003.</p>
<p>[3] F. R. Bach and M. I. Jordan. Predictive low-rank decomposition for kernel methods. In ICML, pages 33–40, 2005.</p>
<p>[4] P. L. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. Annals of Statistics, pages 44–58, 2002.</p>
<p>[5] C. Chang and C. Lin. Libsvm: a library for support vector machines. TIST, 2(3):27, 2011.</p>
<p>[6] C. Cortes, M. Mohri, and A. Talwalkar. On the impact of kernel approximation on learning accuracy. In AISTAT, pages 113–120, 2010.</p>
<p>[7] O. Dekel, S. Shalev-Shwartz, and Y. Singer. The forgetron: A kernel-based perceptron on a ﬁxed budget. In NIPS, 2005.</p>
<p>[8] P. Drineas and M. W. Mahoney. On the nystrom method for approximating a gram matrix for improved kernel-based learning. JMLR, 6:2153–2175, 2005.</p>
<p>[9] J. Kivinen, A. J. Smola, and R. C. Williamson. Online learning with kernels. IEEE Transactions on Signal Processing, pages 2165–2176, 2004.</p>
<p>[10] V. Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems. Springer, 2011.</p>
<p>[11] S. Kumar, M. Mohri, and A. Talwalkar. Ensemble nystrom method. NIPS, pages 1060–1068, 2009.</p>
<p>[12] U. Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395–416, 2007.</p>
<p>[13] A. Rahimi and B. Recht. Random features for large-scale kernel machines. NIPS, pages 1177– 1184, 2007.</p>
<p>[14] A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. NIPS, pages 1313–1320, 2009.</p>
<p>[15] W. Rudin. Fourier analysis on groups. Wiley-Interscience, 1990.</p>
<p>[16] B. Sch¨ lkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regularizao tion, Optimization, and Beyond. MIT Press, 2001.</p>
<p>[17] T. Shi, M. Belkin, and B. Yu. Data spectroscopy: eigenspace of convolution operators and clustering. The Annals of Statistics, 37(6B):3960–3984, 2009.</p>
<p>[18] S. Smale and D.-X. Zhou. Geometry on probability spaces. Constructive Approximation, 30(3):311–323, 2009.</p>
<p>[19] G. W. Stewart and J. Sun. Matrix Perturbation Theory. Academic Press, 1990.</p>
<p>[20] C. Williams and M. Seeger. Using the nystrom method to speed up kernel machines. NIPS, pages 682–688, 2001.</p>
<p>[21] K. Zhang, I. W. Tsang, and J. T. Kwok. Improved nystrom low-rank approximation and error analysis. In ICML, pages 1232–1239, 2008.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
