<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>105 nips-2012-Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-105" href="../nips2012/nips-2012-Dynamic_Pruning_of_Factor_Graphs_for_Maximum_Marginal_Prediction.html">nips2012-105</a> <a title="nips-2012-105-reference" href="#">nips2012-105-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>105 nips-2012-Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction</h1>
<br/><p>Source: <a title="nips-2012-105-pdf" href="http://papers.nips.cc/paper/4568-dynamic-pruning-of-factor-graphs-for-maximum-marginal-prediction.pdf">pdf</a></p><p>Author: Christoph H. Lampert</p><p>Abstract: We study the problem of maximum marginal prediction (MMP) in probabilistic graphical models, a task that occurs, for example, as the Bayes optimal decision rule under a Hamming loss. MMP is typically performed as a two-stage procedure: one estimates each variable’s marginal probability and then forms a prediction from the states of maximal probability. In this work we propose a simple yet effective technique for accelerating MMP when inference is sampling-based: instead of the above two-stage procedure we directly estimate the posterior probability of each decision variable. This allows us to identify the point of time when we are sufﬁciently certain about any individual decision. Whenever this is the case, we dynamically prune the variables we are conﬁdent about from the underlying factor graph. Consequently, at any time only samples of variables whose decision is still uncertain need to be created. Experiments in two prototypical scenarios, multi-label classiﬁcation and image inpainting, show that adaptive sampling can drastically accelerate MMP without sacriﬁcing prediction accuracy. 1</p><br/>
<h2>reference text</h2><p>[1] N. Ghamrawi and A. McCallum. Collective multi-label classiﬁcation. In CIKM, 2005.</p>
<p>[2] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using Gaussian ﬁelds and harmonic functions. In ICML, 2003.</p>
<p>[3] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. PAMI, 6(6), 1984.</p>
<p>[4] S. Della Pietra, V. Della Pietra, and J. Lafferty. Inducing features of random ﬁelds. PAMI, 19(4), 1997.</p>
<p>[5] C. Yanover and Y. Weiss. Approximate inference and protein folding. In NIPS, volume 15, 2002.</p>
<p>[6] E. Schneidman, M. J. Berry, R. Segev, and W. Bialek. Weak pairwise correlations imply strongly correlated network states in a neural population. Nature, 440(7087), 2006.</p>
<p>[7] M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1-2), 2008.</p>
<p>[8] J. Marroquin, S. Mitter, and T. Poggio. Probabilistic solution of ill-posed problems in computational vision. Journal of the American Statistical Association, 82(397), 1987.</p>
<p>[9] C. Yanover and Y. Weiss. Finding the m most probable conﬁgurations using loopy belief propagation. In NIPS, volume 16, 2004.</p>
<p>[10] M. Jerrum and A. Sinclair. Polynomial-time approximation algorithms for the Ising model. SIAM Journal on Computing, 22, 1993.</p>
<p>[11] R. M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. Technical Report CRGTR-93-1, Department of Computer Science, University of Toronto, 1993.</p>
<p>[12] D. J. C. MacKay. Introduction to Monte Carlo methods. In Proceedings of the NATO Advanced Study Institute on Learning in graphical models, 1998.</p>
<p>[13] A. E. Raftery and S. Lewis. How many iterations in the Gibbs sampler. Bayesian Statistics, 4(2), 1992.</p>
<p>[14] A. G. Schwing, C. Zach, Y. Zheng, and M. Pollefeys. Adaptive random forest – how many ”experts” to ask before making a decision? In CVPR, 2011.</p>
<p>[15] H. Weiler. The use of incomplete beta functions for prior distributions in binomial sampling. Technometrics, 1965.</p>
<p>[16] C. M. Thompson, E. S. Pearson, L. J. Comrie, and H. O. Hartley. Tables of percentage points of the incomplete beta-function. Biometrika, 1941.</p>
<p>[17] R. V. Lenth. Some practical guidelines for effective sample size determination. The American Statistician, 55(3), 2001.</p>
<p>[18] A. Wald. Sequential tests of hypotheses. Annals of Mathematical Statistics, 16, 1945.</p>
<p>[19] D. A. Berry. Bayesian clinical trials. Nature Reviews Drug Discovery, 5(1), 2006.</p>
<p>[20] A. E. Raftery. Bayesian model selection in social research. Sociological Methodology, 25, 1995.</p>
<p>[21] D. Easley, N. M. Kiefer, M. O’hara, and J. B. Paperman. Liquidity, information, and infrequently traded stocks. Journal of Finance, 1996.</p>
<p>[22] C. J. Geyer. Practical Markov chain Monte Carlo. Statistical Science, 7(4), 1992.</p>
<p>[23] C. Chow and C. Liu. Approximating discrete probability distributions with dependence trees. IEEE Transactions on Information Theory, 14(3), 1968.</p>
<p>[24] F. Bach and M. I. Jordan. Thin junction trees. In NIPS, volume 14, 2002.</p>
<p>[25] M. J. Collins. A new statistical parser based on bigram lexical dependencies. In ACL, 1996.</p>
<p>[26] J. A. Shelton, J. Bornschein, A. S. Sheikh, P. Berkes, and J. L¨ cke. Select and sample – a model of u efﬁcient neural inference and learning. In NIPS, volume 24, 2011.</p>
<p>[27] Y. Guo and S. Gu. Multi-label classiﬁcation using conditional dependency networks. In IJCAI, 2011.</p>
<p>[28] G. Madjarov, D. Kocev, D. Gjorgjevikj, and S. Dzeroski. An extensive experimental comparison of methods for multi-label learning. Pattern Recognition, 2012.</p>
<p>[29] T. Finley and T. Joachims. Training structural SVMs when exact inference is intractable. In ICML, 2008.</p>
<p>[30] C. H. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In CVPR, 2009.</p>
<p>[31] S. Nowozin, C. Rother, S. Bagon, T. Sharp, B. Yao, and P. Kohli. Decision tree ﬁelds. In ICCV, 2011.</p>
<p>[32] D. Chafai and D. Concordet. Conﬁdence regions for the multinomial parameter with small sample size. Journal of the American Statistical Association, 104(487), 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
