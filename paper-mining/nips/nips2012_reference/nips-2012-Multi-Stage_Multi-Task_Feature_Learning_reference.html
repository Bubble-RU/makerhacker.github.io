<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>221 nips-2012-Multi-Stage Multi-Task Feature Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-221" href="../nips2012/nips-2012-Multi-Stage_Multi-Task_Feature_Learning.html">nips2012-221</a> <a title="nips-2012-221-reference" href="#">nips2012-221-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>221 nips-2012-Multi-Stage Multi-Task Feature Learning</h1>
<br/><p>Source: <a title="nips-2012-221-pdf" href="http://papers.nips.cc/paper/4729-multi-stage-multi-task-feature-learning.pdf">pdf</a></p><p>Author: Pinghua Gong, Jieping Ye, Chang-shui Zhang</p><p>Abstract: Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex sparse regularization problem, which is usually suboptimal, due to its looseness for approximating an ℓ0 -type regularizer. In this paper, we propose a non-convex formulation for multi-task sparse feature learning based on a novel regularizer. To solve the non-convex optimization problem, we propose a MultiStage Multi-Task Feature Learning (MSMTFL) algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms. 1</p><br/>
<h2>reference text</h2><p>[1] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning, 73(3):243–272, 2008.</p>
<p>[2] J. Bi, T. Xiong, S. Yu, M. Dundar, and R. Rao. An improved multi-task learning approach with applications in medical diagnosis. Machine Learning and Knowledge Discovery in Databases, pages 117–132, 2008.</p>
<p>[3] E. Candes and T. Tao. Decoding by linear programming. IEEE Transactions on Information Theory, 51(12):4203–4215, 2005.</p>
<p>[4] J. Chen, J. Liu, and J. Ye. Learning incoherent sparse and low-rank patterns from multiple tasks. In SIGKDD, pages 1179–1188, 2010.</p>
<p>[5] D. Donoho, M. Elad, and V. Temlyakov. Stable recovery of sparse overcomplete representations in the presence of noise. IEEE Transactions on Information Theory, 52(1):6–18, 2006.</p>
<p>[6] T. Evgeniou and M. Pontil. Regularized multi–task learning. In SIGKDD, pages 109–117, 2004.</p>
<p>[7] P. Gong, J. Ye, and C. Zhang. Multi-stage multi-task feature learning. arXiv:1210.5806, 2012.</p>
<p>[8] P. Gong, J. Ye, and C. Zhang. Robust multi-task feature learning. In SIGKDD, pages 895–903, 2012.</p>
<p>[9] A. Jalali, P. Ravikumar, S. Sanghavi, and C. Ruan. A dirty model for multi-task learning. In NIPS, pages 964–972, 2010.</p>
<p>[10] S. Kim and E. Xing. Tree-guided group lasso for multi-task regression with structured sparsity. In ICML, pages 543–550, 2009.</p>
<p>[11] K. Lounici, M. Pontil, A. Tsybakov, and S. Van De Geer. Taking advantage of sparsity in multi-task learning. In COLT, pages 73–82, 2009.</p>
<p>[12] S. Negahban and M. Wainwright. Joint support recovery under high-dimensional scaling: Beneﬁts and perils of ℓ1,∞ -regularization. In NIPS, pages 1161–1168, 2008.</p>
<p>[13] S. Negahban and M. Wainwright. Estimation of (near) low-rank matrices with noise and high-dimensional scaling. The Annals of Statistics, 39(2):1069–1097, 2011.</p>
<p>[14] G. Obozinski, B. Taskar, and M. Jordan. Multi-task feature selection. Statistics Department, UC Berkeley, Tech. Rep, 2006.</p>
<p>[15] G. Obozinski, M. Wainwright, and M. Jordan. Support union recovery in high-dimensional multivariate regression. Annals of statistics, 39(1):1–47, 2011.</p>
<p>[16] S. Parameswaran and K. Weinberger. Large margin multi-task metric learning. In NIPS, pages 1867– 1875, 2010.</p>
<p>[17] N. Quadrianto, A. Smola, T. Caetano, S. Vishwanathan, and J. Petterson. Multitask learning without label correspondences. In NIPS, pages 1957–1965, 2010.</p>
<p>[18] A. Schwaighofer, V. Tresp, and K. Yu. Learning gaussian process kernels via hierarchical bayes. In NIPS, pages 1209–1216, 2005.</p>
<p>[19] S. Van De Geer and P. B¨ hlmann. On the conditions used to prove oracle results for the lasso. Electronic u Journal of Statistics, 3:1360–1392, 2009.</p>
<p>[20] X. Yang, S. Kim, and E. Xing. Heterogeneous multitask learning with joint sparsity constraints. In NIPS, pages 2151–2159, 2009.</p>
<p>[21] K. Yu, V. Tresp, and A. Schwaighofer. Learning gaussian processes from multiple tasks. In ICML, pages 1012–1019, 2005.</p>
<p>[22] C. Zhang and J. Huang. The sparsity and bias of the lasso selection in high-dimensional linear regression. The Annals of Statistics, 36(4):1567–1594, 2008.</p>
<p>[23] C. Zhang and T. Zhang. A general theory of concave regularization for high dimensional sparse estimation problems. Statistical Science, 2012.</p>
<p>[24] J. Zhang, Z. Ghahramani, and Y. Yang. Learning multiple related tasks using latent independent component analysis. In NIPS, pages 1585–1592, 2006.</p>
<p>[25] T. Zhang. Some sharp performance bounds for least squares regression with ℓ1 regularization. The Annals of Statistics, 37:2109–2144, 2009.</p>
<p>[26] T. Zhang. Analysis of multi-stage convex relaxation for sparse regularization. JMLR, 11:1081–1107, 2010.</p>
<p>[27] T. Zhang. Multi-stage convex relaxation for feature selection. Bernoulli, 2012.</p>
<p>[28] Y. Zhang and D. Yeung. Multi-task learning using generalized t process. In AISTATS, 2010.</p>
<p>[29] J. Zhou, J. Chen, and J. Ye. Clustered multi-task learning via alternating structure optimization. In NIPS, pages 702–710, 2011.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
