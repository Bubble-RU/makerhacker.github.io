<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>136 nips-2012-Forward-Backward Activation Algorithm for Hierarchical Hidden Markov Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-136" href="../nips2012/nips-2012-Forward-Backward_Activation_Algorithm_for_Hierarchical_Hidden_Markov_Models.html">nips2012-136</a> <a title="nips-2012-136-reference" href="#">nips2012-136-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>136 nips-2012-Forward-Backward Activation Algorithm for Hierarchical Hidden Markov Models</h1>
<br/><p>Source: <a title="nips-2012-136-pdf" href="http://papers.nips.cc/paper/4628-forward-backward-activation-algorithm-for-hierarchical-hidden-markov-models.pdf">pdf</a></p><p>Author: Kei Wakabayashi, Takao Miura</p><p>Abstract: Hierarchical Hidden Markov Models (HHMMs) are sophisticated stochastic models that enable us to capture a hierarchical context characterization of sequence data. However, existing HHMM parameter estimation methods require large computations of time complexity O(T N 2D ) at least for model inference, where D is the depth of the hierarchy, N is the number of states in each level, and T is the sequence length. In this paper, we propose a new inference method of HHMMs for which the time complexity is O(T N D+1 ). A key idea of our algorithm is application of the forward-backward algorithm to state activation probabilities. The notion of a state activation, which offers a simple formalization of the hierarchical transition behavior of HHMMs, enables us to conduct model inference efﬁciently. We present some experiments to demonstrate that our proposed method works more efﬁciently to estimate HHMM parameters than do some existing methods such as the ﬂattening method and Gibbs sampling method. 1</p><br/>
<h2>reference text</h2><p>[1] C. Bishop. Pattern Recognition and Machine Learning. Springer, 2007.</p>
<p>[2] S. Chib. Calculating posterior distributions and modal estimates in markov mixture models. Journal of Econometrics, 1996.</p>
<p>[3] S. Fine, Y. Singer, and N. Tishby. The hierarchical hidden markov model: Analysis and applications. Machine Learning, 1998. 8</p>
<p>[4] T. Grifﬁths and M. Steyvers. Finding scientiﬁc topics. Proc. the National Academy of Sciences of the United States of America, 2004.</p>
<p>[5] K. Heller, Y. Teh, and D. Gorur. Inﬁnite hierarchical hidden markov models. In Proc. International Conference on Artiﬁcial Intelligence and Statistics, 2009.</p>
<p>[6] S. Luhr, H. Bui, S. Venkatesh, and G. West. Recognition of human activity through hierarchical stochastic learning. In Proc. Pervasive Computing and Communication, 2003.</p>
<p>[7] K. Murphy and M. Paskin. Linear time inference in hierarchical hmms. In Proc. Neural Information Processing Systems, 2001.</p>
<p>[8] N. Nguyen, D. Phung, and S. Venkatesh. Learning and detecting activities from movement trajectories using the hierarchical hidden markov models. In Proc. Computer Vision and Pattern Recognition, 2005.</p>
<p>[9] L. Rabiner. A tutorial on hidden markov models and selected applications in speech recognition. Proc. IEEE, 1989.</p>
<p>[10] S. Scott. Bayesian methods for hidden markov models: Recursive computing in the 21st century. Journal of the American Statistical Association, 2002.</p>
<p>[11] M. Skounakis, M. Craven, and S. Ray. Hierarchical hidden markov models for information extraction. In Proc. International Joint Conference on Artiﬁcial Intelligence, 2003.</p>
<p>[12] M. Weiland, A. Smaill, and P. Nelson. Learning musical pitch structures with hierarchical hidden markov models. In Proc. Journees Informatiques Musicales, 2005.</p>
<p>[13] L. Xie, S. Chang, A. Divakaran, and H. Sun. Learning hierarchical hidden markov models for video structure discovery. Technical report, Columbia University, 2002.  9</p>
<br/>
<br/><br/><br/></body>
</html>
