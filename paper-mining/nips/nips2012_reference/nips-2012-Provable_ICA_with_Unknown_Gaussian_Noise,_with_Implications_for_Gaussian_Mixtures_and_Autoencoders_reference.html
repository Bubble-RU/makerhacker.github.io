<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>281 nips-2012-Provable ICA with Unknown Gaussian Noise, with Implications for Gaussian Mixtures and Autoencoders</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-281" href="../nips2012/nips-2012-Provable_ICA_with_Unknown_Gaussian_Noise%2C_with_Implications_for_Gaussian_Mixtures_and_Autoencoders.html">nips2012-281</a> <a title="nips-2012-281-reference" href="#">nips2012-281-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>281 nips-2012-Provable ICA with Unknown Gaussian Noise, with Implications for Gaussian Mixtures and Autoencoders</h1>
<br/><p>Source: <a title="nips-2012-281-pdf" href="http://papers.nips.cc/paper/4603-provable-ica-with-unknown-gaussian-noise-with-implications-for-gaussian-mixtures-and-autoencoders.pdf">pdf</a></p><p>Author: Sanjeev Arora, Rong Ge, Ankur Moitra, Sushant Sachdeva</p><p>Abstract: We present a new algorithm for Independent Component Analysis (ICA) which has provable performance guarantees. In particular, suppose we are given samples of the form y = Ax + η where A is an unknown n × n matrix and x is a random variable whose components are independent and have a fourth moment strictly less than that of a standard Gaussian random variable and η is an n-dimensional Gaussian random variable with unknown covariance Σ: We give an algorithm that provable recovers A and Σ up to an additive and whose running time and sample complexity are polynomial in n and 1/ . To accomplish this, we introduce a novel “quasi-whitening” step that may be useful in other contexts in which the covariance of Gaussian noise is not known in advance. We also give a general framework for ﬁnding all local optima of a function (given an oracle for approximately ﬁnding just one) and this is a crucial step in our algorithm, one that has been overlooked in previous attempts, and allows us to control the accumulation of error when we ﬁnd the columns of A one by one via local search. 1</p><br/>
<h2>reference text</h2><p>[1] P. Comon. Independent component analysis: a new concept? Signal Processing, pp. 287–314, 1994. 1, 1.1</p>
<p>[2] A. Hyvarinen, J. Karhunen, E. Oja. Independent Component Analysis. Wiley: New York, 2001. 1</p>
<p>[3] A. Hyvarinen, E. Oja. Independent component analysis: algorithms and applications. Neural Networks, pp. 411–430, 2000. 1</p>
<p>[4] P. J. Huber. Projection pursuit. Annals of Statistics pp. 435–475, 1985. 1</p>
<p>[5] A. Frieze, M. Jerrum, R. Kannan. Learning linear transformations. FOCS, pp. 359–368, 1996. 1, 1.1, 4.1, 4.4, 5, 5.1</p>
<p>[6] A. Anandkumar, D. Foster, D. Hsu, S. Kakade, Y. Liu. Two SVDs sufﬁce: spectral decompositions for probabilistic topic modeling and latent Dirichlet allocation. Arxiv:abs/1203.0697, 2012. 1</p>
<p>[7] D. Hsu, S. Kakade. Learning mixtures of spherical Gaussians: moment methods and spectral decompositions. Arxiv:abs/1206.5766, 2012. 1</p>
<p>[8] L. De Lathauwer; J. Castaing; J.-F. Cardoso, Fourth-Order Cumulant-Based Blind Identiﬁcation of Underdetermined Mixtures, Signal Processing, IEEE Transactions on, vol.55, no.6, pp.2965-2973, June 2007 1</p>
<p>[9] A.P. Dempster, N.M. Laird, and D.B. Rubin. Maximum likelihood from incomplete data via the EM Algorithm. Journal of the Royal Statistical Society Series B, pp. 1–38, 1977. 1</p>
<p>[10] S. Dasgupta. Learning mixtures of Gaussians. FOCS pp. 634–644, 1999. 1</p>
<p>[11] S. Arora and R. Kannan. Learning mixtures of separated nonspherical Gaussians. Annals of Applied Probability, pp. 69-92, 2005. 1</p>
<p>[12] M. Belkin and K. Sinha. Polynomial learning of distribution families. FOCS pp. 103–112, 2010. 1</p>
<p>[13] A. T. Kalai, A. Moitra, and G. Valiant. Efﬁciently learning mixtures of two Gaussians. STOC pp. 553-562, 2010. 1</p>
<p>[14] A. Moitra and G. Valiant. Setting the polynomial learnability of mixtures of Gaussians. FOCS pp. 93–102, 2010. 1</p>
<p>[15] G. Hinton, R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science pp. 504–507, 2006. 1</p>
<p>[16] Y. Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning, pp. 1–127, 2009. 1</p>
<p>[17] G. E. Hinton. A Practical Guide to Training Restricted Boltzmann Machines, Version 1, UTML TR 2010-003, Department of Computer Science, University of Toronto, August 2010 1</p>
<p>[18] Y. Freund , D. Haussler. Unsupervised Learning of Distributions on Binary Vectors using Two Layer Networks University of California at Santa Cruz, Santa Cruz, CA, 1994 1</p>
<p>[19] S. Cruces, L. Castedo, A. Cichocki, Robust blind source separation algorithms using cumulants, Neurocomputing, Volume 49, Issues 14, pp 87-118, 2002. 1.1</p>
<p>[20] L., De Lathauwer; B., De Moor; J. Vandewalle. Independent component analysis based on higher-order statistics only Proceedings of 8th IEEE Signal Processing Workshop on Statistical Signal and Array Processing, 1996. 1.1</p>
<p>[21] S. Vempala, Y. Xiao. Structure from local optima: learning subspace juntas via higher order PCA. Arxiv:abs/1108.3329, 2011. 1.1</p>
<p>[22] M. Kendall, A. Stuart. The Advanced Theory of Statistics Charles Grifﬁn and Company, 1958. 2  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
