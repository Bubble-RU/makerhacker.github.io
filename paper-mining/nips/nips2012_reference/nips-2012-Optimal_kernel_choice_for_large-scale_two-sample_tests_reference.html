<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>264 nips-2012-Optimal kernel choice for large-scale two-sample tests</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-264" href="../nips2012/nips-2012-Optimal_kernel_choice_for_large-scale_two-sample_tests.html">nips2012-264</a> <a title="nips-2012-264-reference" href="#">nips2012-264-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>264 nips-2012-Optimal kernel choice for large-scale two-sample tests</h1>
<br/><p>Source: <a title="nips-2012-264-pdf" href="http://papers.nips.cc/paper/4727-optimal-kernel-choice-for-large-scale-two-sample-tests.pdf">pdf</a></p><p>Author: Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano Pontil, Kenji Fukumizu, Bharath K. Sriperumbudur</p><p>Abstract: Given samples from distributions p and q, a two-sample test determines whether to reject the null hypothesis that p = q, based on the value of a test statistic measuring the distance between the samples. One choice of test statistic is the maximum mean discrepancy (MMD), which is a distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. The kernel used in obtaining these embeddings is critical in ensuring the test has high power, and correctly distinguishes unlike distributions with high probability. A means of parameter selection for the two-sample test based on the MMD is proposed. For a given test level (an upper bound on the probability of making a Type I error), the kernel is chosen so as to maximize the test power, and minimize the probability of making a Type II error. The test statistic, test threshold, and optimization over the kernel parameters are obtained with cost linear in the sample size. These properties make the kernel selection and test procedures suited to data streams, where the observations cannot all be stored in memory. In experiments, the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics.</p><br/>
<h2>reference text</h2><p>[1] R. Adler and J. Taylor. Random Fields and Geometry. Springer, 2007.</p>
<p>[2] Andreas Argyriou, Raphael Hauser, Charles A. Micchelli, and Massimiliano Pontil. A dcprogramming algorithm for kernel selection. In ICML, pages 41–48, 2006.</p>
<p>[3] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463–482, 2002.</p>
<p>[4] A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and Statistics. Kluwer, 2004.</p>
<p>[5] Magnetic Fields. 69 love songs. Merge, MRG169, 1999.</p>
<p>[6] P. Gehler and S. Nowozin. Inﬁnite kernel learning. Technical Report TR-178, Max Planck Institute for Biological Cybernetics, 2008.</p>
<p>[7] A. Gretton, K. Borgwardt, M. Rasch, B. Schoelkopf, and A. Smola. A kernel two-sample test. JMLR, 13:723–773, 2012. 8</p>
<p>[8] A. Gretton, K. Borgwardt, M. Rasch, B. Sch¨ lkopf, and A. J. Smola. A kernel method for o the two-sample problem. In Advances in Neural Information Processing Systems 15, pages 513–520, Cambridge, MA, 2007. MIT Press.</p>
<p>[9] A. Gretton, K. Fukumizu, Z. Harchaoui, and B. Sriperumbudur. A fast, consistent kernel twosample test. In Advances in Neural Information Processing Systems 22, Red Hook, NY, 2009. Curran Associates Inc.</p>
<p>[10] Z. Harchaoui, F. Bach, and E. Moulines. Testing for homogeneity with kernel Fisher discriminant analysis. In Advances in Neural Information Processing Systems 20, pages 609–616. MIT Press, Cambridge, MA, 2008.</p>
<p>[11] R. A. Horn and C. R. Johnson. Matrix analysis. Cambridge Univ Press, 1990.</p>
<p>[12] C. McDiarmid. On the method of bounded differences. In Survey in Combinatorics, pages 148–188. Cambridge University Press, 1989.</p>
<p>[13] R. Serﬂing. Approximation Theorems of Mathematical Statistics. Wiley, New York, 1980.</p>
<p>[14] A. J. Smola, A. Gretton, L. Song, and B. Sch¨ lkopf. A Hilbert space embedding for diso tributions. In Proceedings of the International Conference on Algorithmic Learning Theory, volume 4754, pages 13–31. Springer, 2007.</p>
<p>[15] B. Sriperumbudur, K. Fukumizu, A. Gretton, G. Lanckriet, and B. Schoelkopf. Kernel choice and classiﬁability for RKHS embeddings of probability distributions. In Advances in Neural Information Processing Systems 22, Red Hook, NY, 2009. Curran Associates Inc.</p>
<p>[16] B. Sriperumbudur, A. Gretton, K. Fukumizu, G. Lanckriet, and B. Sch¨ lkopf. Hilbert space o embeddings and metrics on probability measures. Journal of Machine Learning Research, 11:1517–1561, 2010.</p>
<p>[17] M. Sugiyama, T. Suzuki, Y. Itoh, T. Kanamori, and M. Kimura. Least-squares two-sample test. Neural Networks, 24(7):735–751, 2011.</p>
<p>[18] A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes. Springer, 1996.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
