<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>228 nips-2012-Multilabel Classification using Bayesian Compressed Sensing</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-228" href="../nips2012/nips-2012-Multilabel_Classification_using_Bayesian_Compressed_Sensing.html">nips2012-228</a> <a title="nips-2012-228-reference" href="#">nips2012-228-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>228 nips-2012-Multilabel Classification using Bayesian Compressed Sensing</h1>
<br/><p>Source: <a title="nips-2012-228-pdf" href="http://papers.nips.cc/paper/4591-multilabel-classification-using-bayesian-compressed-sensing.pdf">pdf</a></p><p>Author: Ashish Kapoor, Raajay Viswanathan, Prateek Jain</p><p>Abstract: In this paper, we present a Bayesian framework for multilabel classiďŹ cation using compressed sensing. The key idea in compressed sensing for multilabel classiďŹ cation is to ďŹ rst project the label vector to a lower dimensional space using a random transformation and then learn regression functions over these projections. Our approach considers both of these components in a single probabilistic model, thereby jointly optimizing over compression as well as learning tasks. We then derive an efďŹ cient variational inference scheme that provides joint posterior distribution over all the unobserved labels. The two key beneďŹ ts of the model are that a) it can naturally handle datasets that have missing labels and b) it can also measure uncertainty in prediction. The uncertainty estimate provided by the model allows for active learning paradigms where an oracle provides information about labels that promise to be maximally informative for the prediction task. Our experiments show signiďŹ cant boost over prior methods in terms of prediction performance over benchmark datasets, both in the fully labeled and the missing labels case. Finally, we also highlight various useful active learning scenarios that are enabled by the probabilistic model. 1</p><br/>
<h2>reference text</h2><p>[1] D. Hsu, S. Kakade, J. Langford, and T. Zhang. Multi-label prediction via compressed sensing. In NIPS, pages 772â&euro;&ldquo;780, 2009.</p>
<p>[2] B. Hariharan, L. Zelnik-Manor, S. V. N. Vishwanathan, and M. Varma. Large scale max-margin multilabel classiďŹ cation with priors. In ICML, pages 423â&euro;&ldquo;430, 2010.</p>
<p>[3] G. Tsoumakas and I. Katakis. Multi-label classiďŹ cation: An overview. IJDWM, 3(3):1â&euro;&ldquo;13, 2007.</p>
<p>[4] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6:1453â&euro;&ldquo;1484, 2005.</p>
<p>[5] M. R. Boutell, J. Luo, X. Shen, and C. M. Brown. Learning multi-label scene classiďŹ cation. Pattern Recognition, 37(9):1757â&euro;&ldquo;1771, 2004.</p>
<p>[6] B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks. In NIPS, 2003.</p>
<p>[7] R. M. Rifkin and A. Klautau. In defense of one-vs-all classiďŹ cation. Journal of Machine Learning Research, 5:101â&euro;&ldquo;141, 2004.</p>
<p>[8] D. Needell and J. A. Tropp. Cosamp: Iterative signal recovery from incomplete and inaccurate samples. Applied and Computational Harmonic Analysis, 26(3):301 â&euro;&ldquo; 321, 2009.</p>
<p>[9] S. Foucart. Hard thresholding pursuit: an algorithm for compressive sensing, 2010. preprint.</p>
<p>[10] D. Baron, S. S. Sarvotham, and R. G. Baraniuk. Bayesian compressive sensing via belief propagation. IEEE Transactions on Signal Processing, 58(1), 2010.</p>
<p>[11] S. Ji, Y. Xue, and L. Carin. Bayesian compressive sensing. IEEE Transactions on Signal Processing, 56(6), 2008.</p>
<p>[12] N. Cesa-Bianchi, A Conconi, and C. Gentile. Learning probabilistic linear-threshold classiďŹ ers via selective sampling. In COLT, 2003.</p>
<p>[13] N. Lawrence, M. Seeger, and R. Herbrich. Fast sparse Gaussian Process method: Informative vector machines. NIPS, 2002.</p>
<p>[14] D. MacKay. Information-based objective functions for active data selection. Neural Computation, 4(4), 1992.</p>
<p>[15] S. Tong and D. Koller. Support vector machine active learning with applications to text classiďŹ cation. In ICML, 2000.</p>
<p>[16] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. Selective sampling using the query by committee algorithm. Machine Learning, 28(2-3), 1997.</p>
<p>[17] B. Yang, J.-Tao Sun, T. Wang, and Z. Chen. Effective multi-label active learning for text classiďŹ cation. In KDD, pages 917â&euro;&ldquo;926, 2009.</p>
<p>[18] J. Weston, S. Bengio, and N. Usunier. Large scale image annotation: learning to rank with joint wordimage embeddings. Machine Learning, 81(1):21â&euro;&ldquo;35, 2010.</p>
<p>[19] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). The MIT Press, 2005.</p>
<p>[20] M. E. Tipping. Sparse bayesian learning and the relevance vector machine. Journal of Machine Learning Research, 1:211â&euro;&ldquo;244, 2001.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
