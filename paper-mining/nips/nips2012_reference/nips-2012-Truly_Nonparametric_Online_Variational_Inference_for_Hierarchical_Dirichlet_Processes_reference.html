<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-354" href="../nips2012/nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">nips2012-354</a> <a title="nips-2012-354-reference" href="#">nips2012-354-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</h1>
<br/><p>Source: <a title="nips-2012-354-pdf" href="http://papers.nips.cc/paper/4606-truly-nonparametric-online-variational-inference-for-hierarchical-dirichlet-processes.pdf">pdf</a></p><p>Author: Michael Bryant, Erik B. Sudderth</p><p>Abstract: Variational methods provide a computationally scalable alternative to Monte Carlo methods for large-scale, Bayesian nonparametric learning. In practice, however, conventional batch and online variational methods quickly become trapped in local optima. In this paper, we consider a nonparametric topic model based on the hierarchical Dirichlet process (HDP), and develop a novel online variational inference algorithm based on split-merge topic updates. We derive a simpler and faster variational approximation of the HDP, and show that by intelligently splitting and merging components of the variational posterior, we can achieve substantially better predictions of test data than conventional online and batch variational algorithms. For streaming analysis of large datasets where batch analysis is infeasible, we show that our split-merge updates better capture the nonparametric properties of the underlying model, allowing continual learning of new topics.</p><br/>
<h2>reference text</h2><p>[1] Y.W. Teh, M. Jordan, and M. Beal. Hierarchical Dirichlet processes. JASA, 2006.</p>
<p>[2] D. Blei and M. Jordan. Variational methods for Dirichlet process mixtures. Bayesian Analysis, 1:121–144, 2005.</p>
<p>[3] Y.W. Teh, K. Kurihara, and M. Welling. Collapsed variational inference for HDP. NIPS, 2008.</p>
<p>[4] C. Wang, J. Paisley, and D. Blei. Online variational inference for the hierarchical Dirichlet process. AISTATS, 2011.</p>
<p>[5] M. Hoffman, D. Blei, and F. Bach. Online learning for latent Dirichlet allocation. NIPS, 2010.</p>
<p>[6] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. JMLR, 2003.</p>
<p>[7] S. Jain and R. Neal. A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model. Journal of Computational and Graphical Statistics, 13:158–182, 2004.</p>
<p>[8] D.B. Dahl. Sequentially-allocated merge-split sampler for conjugate and nonconjugate Dirichlet process mixture models. Technical report, Texas A&M; University, 2005.</p>
<p>[9] C. Wang and D. Blei. A split-merge MCMC algorithm for the hierarchical Dirichlet process. ArXiv e-prints, January 2012.</p>
<p>[10] N. Ueda, R. Nakano, Z. Ghahramani, and G. Hinton. SMEM algorithm for mixture models. Neural Computation, 2000.</p>
<p>[11] K. Kurihara and M. Welling. Bayesian K-means as a ’Maximization-Expectation’ algorithm. SIAM conference on data mining SDM06, 2006.</p>
<p>[12] N. Ueda and Z. Ghahramani. Bayesian model search for mixture models based on optimizing variational bounds. Neural Networks, 15, 2002.</p>
<p>[13] Z. Ghahramani and M. Beal. Variational inference for Bayesian mixtures of factor analysers. NIPS, 2000.</p>
<p>[14] M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. Machine Learning, 1999.</p>
<p>[15] P. Liang, S. Petrov, D. Klein, and M. Jordan. The inﬁnite PCFG using hierarchical Dirichlet processes. Empirical Methods in Natural Language Processing, 2007.</p>
<p>[16] K. Kurihara, M. Welling, and N. Vlassis. Accelerated variational Dirichlet process mixtures. NIPS, 2007.</p>
<p>[17] D. Blei and C. Wang. Variational inference for the nested Chinese restaurant process. NIPS, 2009.</p>
<p>[18] T. L. Grifﬁths and M. Steyvers. Finding scientiﬁc topics. PNAS, 101:5228–5235, 2004.</p>
<p>[19] A. Asuncion, M. Welling, P. Smyth, and Y.W. Teh. On smoothing and inference for topic models. UAI, 2009.</p>
<p>[20] G. Doyle and C. Elkan. Accounting for word burstiness in topic models. ICML, 2009.</p>
<p>[21] M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1:1–305, 2008.  9</p>
<br/>
<br/><br/><br/></body>
</html>
