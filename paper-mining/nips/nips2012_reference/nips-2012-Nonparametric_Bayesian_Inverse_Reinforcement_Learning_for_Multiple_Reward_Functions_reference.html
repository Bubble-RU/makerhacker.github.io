<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>245 nips-2012-Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-245" href="../nips2012/nips-2012-Nonparametric_Bayesian_Inverse_Reinforcement_Learning_for_Multiple_Reward_Functions.html">nips2012-245</a> <a title="nips-2012-245-reference" href="#">nips2012-245-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>245 nips-2012-Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions</h1>
<br/><p>Source: <a title="nips-2012-245-pdf" href="http://papers.nips.cc/paper/4737-nonparametric-bayesian-inverse-reinforcement-learning-for-multiple-reward-functions.pdf">pdf</a></p><p>Author: Jaedeug Choi, Kee-eung Kim</p><p>Abstract: We present a nonparametric Bayesian approach to inverse reinforcement learning (IRL) for multiple reward functions. Most previous IRL algorithms assume that the behaviour data is obtained from an agent who is optimizing a single reward function, but this assumption is hard to guarantee in practice. Our approach is based on integrating the Dirichlet process mixture model into Bayesian IRL. We provide an efﬁcient Metropolis-Hastings sampling algorithm utilizing the gradient of the posterior to estimate the underlying reward functions, and demonstrate that our approach outperforms previous ones via experiments on a number of problem domains. 1</p><br/>
<h2>reference text</h2><p>[1] Stuart Russell. Learning agents for uncertain environments (extended abstract). In Proceedings of COLT, 1998.</p>
<p>[2] Andrew Y. Ng and Stuart Russell. Algorithms for inverse reinforcement learning. In Proceedings of ICML, 2000.</p>
<p>[3] Gergely Neu and Csaba Szepesv´ ri. Apprenticeship learning using inverse reinforcement learning and a gradient methods. In Proceedings of UAI, 2007.</p>
<p>[4] Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In Proceedings of IJCAI, 2007.</p>
<p>[5] Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse reinforcement learning. In Proceedings of AAAI, 2008.</p>
<p>[6] Brian D. Ziebart, Andrew L. Maas, Anind K. Dey, and J. Andrew Bagnell. Navigate like a cabbie: probabilistic reasoning from observed context-aware behavior. In Proceedings of the international conference on Ubiquitous computing, 2008.</p>
<p>[7] Zeynep Erkin, Matthew D. Bailey, Lisa M. Maillart, Andrew J. Schaefer, and Mark S. Roberts. Eliciting patients’ revealed preferences: An inverse Markov decision process approach. Decision Analysis, 7(4), 2010.</p>
<p>[8] Senthilkumar Chandramohan, Matthieu Geist, Fabrice Lefevre, and Olivier Pietquin. User simulation in dialogue systems using inverse reinforcement learning. In Proceedings of Interspeech, 2011.</p>
<p>[9] Christos Dimitrakakis and Constantin A. Rothkopf. Bayesian multitask inverse reinforcement learning. In Proceedings of the European Workshop on Reinforcement Learning, 2011.</p>
<p>[10] Monica Babes-Vroman, Vukosi Marivate, Kaushik Subramanian, and Michael Littman. Apprenticeship ¸ learning about multiple intentions. In Proceedings of ICML, 2011.</p>
<p>[11] Peter Dayan and Geoffrey E. Hinton. Using expectation-maximization for reinforcement learning. Neural Computation, 9(2), 1997.</p>
<p>[12] Jaedeug Choi and Kee-Eung Kim. MAP inference for Bayesian inverse reinforcement learning. In Proceedings of NIPS, 2011.</p>
<p>[13] Radford M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics, 9(2), 2000.</p>
<p>[14] Gareth O. Roberts and Jeffrey S. Rosenthal. Optimal scaling of discrete approximations to langevin diffusions. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 60(1), 1998.</p>
<p>[15] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of ICML, 2004.</p>
<p>[16] Abdeslam Boularias, Jens Kober, and Jan Peters. Relative entropy inverse reinforcement learning. In Proceedings of AISTATS, 2011.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
