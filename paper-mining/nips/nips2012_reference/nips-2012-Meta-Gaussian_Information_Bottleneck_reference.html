<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>211 nips-2012-Meta-Gaussian Information Bottleneck</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-211" href="../nips2012/nips-2012-Meta-Gaussian_Information_Bottleneck.html">nips2012-211</a> <a title="nips-2012-211-reference" href="#">nips2012-211-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>211 nips-2012-Meta-Gaussian Information Bottleneck</h1>
<br/><p>Source: <a title="nips-2012-211-pdf" href="http://papers.nips.cc/paper/4517-meta-gaussian-information-bottleneck.pdf">pdf</a></p><p>Author: Melanie Rey, Volker Roth</p><p>Abstract: We present a reformulation of the information bottleneck (IB) problem in terms of copula, using the equivalence between mutual information and negative copula entropy. Focusing on the Gaussian copula we extend the analytical IB solution available for the multivariate Gaussian case to distributions with a Gaussian dependence structure but arbitrary marginal densities, also called meta-Gaussian distributions. This opens new possibles applications of IB to continuous data and provides a solution more robust to outliers. 1</p><br/>
<h2>reference text</h2><p>[1] N. Tishby, F.C. Pereira, and W. Bialek. The information bottleneck method. The 37th annual Allerton Conference on Communication, Control, and Computing, (29-30):368–377, 1999.</p>
<p>[2] O. Shamir, S. Sabato, and N. Tishby. Learning and generalization with the information bottleneck. Theor. Comput. Sci., 411(29-30):2696–2711, 2010.</p>
<p>[3] G. Chechik, A. Globerson, N. Tishby, and Y. Weiss. Information bottleneck for Gaussian variables. Journal of Machine Learning Research, 6:165–188, 2005.</p>
<p>[4] A. Globerson and N. Tishby. On the optimality of the Gaussian information bottleneck curve. Hebrew University Technical Report, 2004.</p>
<p>[5] R.M. Hecht, E. Noor, and N. Tishby. Speaker recognition by Gaussian information bottleneck. INTERSPEECH, pages 1567–1570, 2009.</p>
<p>[6] J. Ma and Z. Sun. Mutual information is copula entropy. arXiv:0808.0845v1, 2008. `</p>
<p>[7] A. Sklar. Fonctions de r´ partition a n dimensions et leurs marges. Publications de l’Institut de Statistique e de l’Universit´ de Paris, 8:229–231, 1959. e</p>
<p>[8] A. J. McNeil, R. Frey, and P. Embrechts. Quantitative Risk Management. Princeton Series in Finance. Princeton University Press, 2005.</p>
<p>[9] G. Elidan. Copula bayesian networks. Proceedings of the Neural Information Processing Systems (NIPS), 2010.</p>
<p>[10] C. Genest, K. Ghoudhi, and L.P. Rivet. A semiparametric estimation procedure of dependence parameters in multivariate families of distributions. Biometrika, 82(3):543–552, 1995.</p>
<p>[11] H. Tsukahara. Semiparametric estimation in copula models. 33(3):357–375, 2005.  The Canadian Journal of Statistics,</p>
<p>[12] Peter D. Hoff. Extending the rank likelihood for semiparametric copula estimation. Annals of Applied Statistics, 1(1):273, 2007.</p>
<p>[13] K. Boudt, J. Cornelissen, and C. Croux. The gaussian rank correlation estimator: Robustness properties. Statistics and Computing, 22:471–483, 2012.</p>
<p>[14] D. P´ l, B. P´ czos, and C. Szepesv´ ri. Estimation of R´ nyi entropy and mutual information based on a o a e generalized nearest-neighbor graphs. Proceedings of the Neural Information Processing Systems (NIPS), 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
