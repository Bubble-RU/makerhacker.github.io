<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>163 nips-2012-Isotropic Hashing</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-163" href="../nips2012/nips-2012-Isotropic_Hashing.html">nips2012-163</a> <a title="nips-2012-163-reference" href="#">nips2012-163-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>163 nips-2012-Isotropic Hashing</h1>
<br/><p>Source: <a title="nips-2012-163-pdf" href="http://papers.nips.cc/paper/4846-isotropic-hashing.pdf">pdf</a></p><p>Author: Weihao Kong, Wu-jun Li</p><p>Abstract: Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values, and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically, the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers, it is still not veriﬁed by either theory or experiment because no methods have been proposed to ﬁnd a projection with equal variances for different dimensions. In this paper, we propose a novel method, called isotropic hashing (IsoHash), to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions, which veriﬁes the viewpoint that projections with isotropic variances will be better than those with anisotropic variances. 1</p><br/>
<h2>reference text</h2><p>[1] A. Andoni and P. Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. Commun. ACM, 51(1):117–122, 2008.</p>
<p>[2] M.T. Chu. Constructing a Hermitian matrix from its diagonal entries and eigenvalues. SIAM Journal on Matrix Analysis and Applications, 16(1):207–217, 1995.</p>
<p>[3] M.T. Chu and K.R. Driessel. The projected gradient method for least squares matrix approximations with spectral constraints. SIAM Journal on Numerical Analysis, pages 1050–1060, 1990.</p>
<p>[4] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the ACM Symposium on Computational Geometry, 2004.</p>
<p>[5] A. Gionis, P. Indyk, and R. Motwani. Similarity search in high dimensions via hashing. In VLDB, 1999.</p>
<p>[6] Y. Gong, S. Kumar, V. Verma, and S. Lazebnik. Angular quantization based binary codes for fast similarity search. In NIPS, 2012.</p>
<p>[7] Y. Gong and S. Lazebnik. Iterative quantization: A Procrustean approach to learning binary codes. In CVPR, 2011.</p>
<p>[8] Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin. Iterative quantization: A Procrustean approach to learning binary codes for large-scale image retrieval. In IEEE Trans. Pattern Anal. Mach. Intell., 2012.</p>
<p>[9] J. He, W. Liu, and S.-F. Chang. Scalable similarity search with optimized kernel hashing. In KDD, 2010.</p>
<p>[10] J.-P. Heo, Y. Lee, J. He, S.-F. Chang, and S.-E. Yoon. Spherical hashing. In CVPR, 2012.</p>
<p>[11] A. Horn. Doubly stochastic matrices and the diagonal of a rotation matrix. American Journal of Mathematics, 76(3):620–630, 1954.</p>
<p>[12] H. Jegou, M. Douze, C. Schmid, and P. P´ rez. Aggregating local descriptors into a compact image e representation. In CVPR, 2010.</p>
<p>[13] I. Jolliffe. Principal Component Analysis. Springer, 2002.</p>
<p>[14] W. Kong and W.-J. Li. Double-bit quantization for hashing. In AAAI, 2012.</p>
<p>[15] W. Kong, W.-J. Li, and M. Guo. Manhattan hashing for large-scale image retrieval. In SIGIR, 2012.</p>
<p>[16] A. Krizhevsky. Learning multiple layers of features from tiny images. Tech report, University of Toronto, 2009.</p>
<p>[17] B. Kulis and T. Darrell. Learning to hash with binary reconstructive embeddings. In NIPS, 2009.</p>
<p>[18] B. Kulis and K. Grauman. Kernelized locality-sensitive hashing for scalable image search. In ICCV, 2009.</p>
<p>[19] B. Kulis, P. Jain, and K. Grauman. Fast similarity search for learned metrics. IEEE Trans. Pattern Anal. Mach. Intell., 31(12):2143–2157, 2009.</p>
<p>[20] W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang. Supervised hashing with kernels. In CVPR, 2012.</p>
<p>[21] W. Liu, J. Wang, S. Kumar, and S.-F. Chang. Hashing with graphs. In ICML, 2011.</p>
<p>[22] Y. Mu and S. Yan. Non-metric locality-sensitive hashing. In AAAI, 2010.</p>
<p>[23] M. Norouzi and D. J. Fleet. Minimal loss hashing for compact binary codes. In ICML, 2011.</p>
<p>[24] A. Oliva and A. Torralba. Modeling the shape of the scene: A holistic representation of the spatial envelope. International Journal of Computer Vision, 42(3):145–175, 2001.</p>
<p>[25] M. Raginsky and S. Lazebnik. Locality-sensitive binary codes from shift-invariant kernels. In NIPS, 2009.</p>
<p>[26] R. Salakhutdinov and G. E. Hinton. Semantic hashing. Int. J. Approx. Reasoning, 50(7):969–978, 2009.</p>
<p>[27] L.F. Shampine and M.K. Gordon. Computer solution of ordinary differential equations: the initial value problem. Freeman, San Francisco, California, 1975.</p>
<p>[28] A. Torralba, R. Fergus, and Y. Weiss. Small codes and large image databases for recognition. In CVPR, 2008.</p>
<p>[29] J. Wang, S. Kumar, and S.-F. Chang. Sequential projection learning for hashing with compact codes. In ICML, 2010.</p>
<p>[30] J. Wang, S. Kumar, and S.-F. Chang. Semi-supervised hashing for large-scale search. IEEE Trans. Pattern Anal. Mach. Intell., 34(12):2393–2406, 2012.</p>
<p>[31] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In NIPS, 2008.</p>
<p>[32] H. Xu, J. Wang, Z. Li, G. Zeng, S. Li, and N. Yu. Complementary hashing for approximate nearest neighbor search. In ICCV, 2011.</p>
<p>[33] D. Zhang, F. Wang, and L. Si. Composite hashing with multiple information sources. In SIGIR, 2011.</p>
<p>[34] Y. Zhen and D.-Y. Yeung. A probabilistic model for multimodal hash function learning. In KDD, 2012.  9</p>
<br/>
<br/><br/><br/></body>
</html>
