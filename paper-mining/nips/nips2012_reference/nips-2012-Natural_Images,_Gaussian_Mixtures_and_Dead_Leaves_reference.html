<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>235 nips-2012-Natural Images, Gaussian Mixtures and Dead Leaves</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-235" href="../nips2012/nips-2012-Natural_Images%2C_Gaussian_Mixtures_and_Dead_Leaves.html">nips2012-235</a> <a title="nips-2012-235-reference" href="#">nips2012-235-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>235 nips-2012-Natural Images, Gaussian Mixtures and Dead Leaves</h1>
<br/><p>Source: <a title="nips-2012-235-pdf" href="http://papers.nips.cc/paper/4758-natural-images-gaussian-mixtures-and-dead-leaves.pdf">pdf</a></p><p>Author: Daniel Zoran, Yair Weiss</p><p>Abstract: Simple Gaussian Mixture Models (GMMs) learned from pixels of natural image patches have been recently shown to be surprisingly strong performers in modeling the statistics of natural images. Here we provide an in depth analysis of this simple yet rich model. We show that such a GMM model is able to compete with even the most successful models of natural images in log likelihood scores, denoising performance and sample quality. We provide an analysis of what such a model learns from natural images as a function of number of mixture components including covariance structure, contrast variation and intricate structures such as textures, boundaries and more. Finally, we show that the salient properties of the GMM learned from natural images can be derived from a simplified Dead Leaves model which explicitly models occlusion, explaining its surprising success relative to other models. 1 GMMs and natural image statistics models Many models for the statistics of natural image patches have been suggested in recent years. Finding good models for natural images is important to many different research areas - computer vision, biological vision and neuroscience among others. Recently, there has been a growing interest in comparing different aspects of models for natural images such as log-likelihood and multi-information reduction performance, and much progress has been achieved [1,2, 3,4,5, 6]. Out of these results there is one which is particularly interesting: simple, unconstrained Gaussian Mixture Models (GMMs) with a relatively small number of mixture components learned from image patches are extraordinarily good in modeling image statistics [6, 4]. This is a surprising result due to the simplicity of GMMs and their ubiquity. Another surprising aspect of this result is that many of the current models may be thought of as GMMs with an exponential or infinite number of components, having different constraints on the covariance structure of the mixture components. In this work we study the nature of GMMs learned from natural image patches. We start with a thorough comparison to some popular and cutting edge image models. We show that indeed, GMMs are excellent performers in modeling natural image patches. We then analyze what properties of natural images these GMMs capture, their dependence on the number of components in the mixture and their relation to the structure of the world around us. Finally, we show that the learned GMM suggests a strong connection between natural image statistics and a simple variant of the dead leaves model [7, 8] , explicitly modeling occlusions and explaining some of the success of GMMs in modeling natural images. 1 3.5 .,...- ••.......-.-.. -..---'-. 1 ~~6\8161·· -.. .-.. --...--.-- ---..-.- -. --------------MII+··+ilIl ..... .. . . ~ '[25 . . . ---- ] B'II 1_ -- ~2 ;t:: fI 1 - --- ,---- ._.. : 61.5 ..... '</p><br/>
<h2>reference text</h2><p>[1] M. Bethge, </p>
<p>[2] P. Berkes, R. Turner, and M. Sahani, </p>
<p>[3] S. Lyu and E. P. Simoncelli, </p>
<p>[4] D. Zoran and Y. Weiss, </p>
<p>[5] B. Culpepper, J. Sohl-Dickstein, and B. Olshausen, </p>
<p>[6] L. Theis, S. Gerwinn, F. Sinz, and M. Bethge, </p>
<p>[7] G. Matheron, Random sets and integral geometry.  Wiley New York, 1975, vol. 1.</p>
<p>[8] X. Pitkow, </p>
<p>[9] B. 01shausen et al., </p>
<p>[10] A. J. Bell and T. J. Sejnowski, </p>
<p>[11] A. Hyvarinen and E. Oja, </p>
<p>[12] Y. Karklin and M. Lewicki, </p>
<p>[13] J. Sohl-Dickstein and B. Culpepper, </p>
<p>[14] M. Lewicki and B. Olshausen, </p>
<p>[15] A. Lee, D. Mumford, and J. Huang, </p>
<p>[16] C. Zetzsche, E. Barth, and B. Wegmann, </p>
<p>[17] E . Simoncelli, </p>
<p>[18] D. Field, </p>
<p>[19] J. Lucke, R. Turner, M. Sahani, and M. Henniges, </p>
<p>[20] G. Puertas, J. Bornschein, and 1. Lucke, </p>
<p>[21] N. Le Roux, N. Heess, J. Shotton, and J. Winn, </p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
