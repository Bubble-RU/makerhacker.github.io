<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>32 nips-2012-Active Comparison of Prediction Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-32" href="../nips2012/nips-2012-Active_Comparison_of_Prediction_Models.html">nips2012-32</a> <a title="nips-2012-32-reference" href="#">nips2012-32-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>32 nips-2012-Active Comparison of Prediction Models</h1>
<br/><p>Source: <a title="nips-2012-32-pdf" href="http://papers.nips.cc/paper/4495-active-comparison-of-prediction-models.pdf">pdf</a></p><p>Author: Christoph Sawade, Niels Landwehr, Tobias Scheffer</p><p>Abstract: We address the problem of comparing the risks of two given predictive models—for instance, a baseline model and a challenger—as conﬁdently as possible on a ﬁxed labeling budget. This problem occurs whenever models cannot be compared on held-out training data, possibly because the training data are unavailable or do not reﬂect the desired test distribution. In this case, new test instances have to be drawn and labeled at a cost. We devise an active comparison method that selects instances according to an instrumental sampling distribution. We derive the sampling distribution that maximizes the power of a statistical test applied to the observed empirical risks, and thereby minimizes the likelihood of choosing the inferior model. Empirically, we investigate model selection problems on several classiﬁcation and regression tasks and study the accuracy of the resulting p-values. 1</p><br/>
<h2>reference text</h2><p>[1] M. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In Proceedings of the 23rd International Conference on Machine Learning, 2006.</p>
<p>[2] A. Beygelzimer, S. Dasgupta, and J. Langford. Importance weighted active learning. In Proceedings of the 26th International Conference on Machine Learning, 2009.</p>
<p>[3] J. Geweke. Bayesian inference in econometric models using monte carlo integration. Econometrica, 57(6):1317–1339, 1989.</p>
<p>[4] J. S. Liu. Monte Carlo Strategies in Scientiﬁc Computing. Springer, 2001.</p>
<p>[5] O. Madani, D. J. Lizotte, and R. Greiner. Active model selection. In Proceedings of the 20th Conference on Uncertainty in Artiﬁcial Intelligence, 2004.</p>
<p>[6] O. Maron and A. W. Moore. Hoeffding races: Accelerating model selection search for classiﬁcation and function approximation. In Proceedings of the 6th Annual Conference on Neural Information Processing Systems, 1993.</p>
<p>[7] Carl Edward Rasmussen and Christopher Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.</p>
<p>[8] C. Sawade, N. Landwehr, S. Bickel, and T. Scheffer. Active risk estimation. In Proceedings of the 27th International Conference on Machine Learning, 2010.</p>
<p>[9] C. Sawade, N. Landwehr, and T. Scheffer. Active estimation of f-measures. In Proceedings of the 23rd Annual Conference on Neural Information Processing Systems, 2010.</p>
<p>[10] T. Scheffer and S. Wrobel. Finding the most interesting patterns in a database quickly by using sequential sampling. Journal of Machine Learning Research, 3:833–862, 2003.</p>
<p>[11] D. Sheskin. Handbook of Parametric and Nonparametric Statistical Procedures. Chapman & Hall, 2004.</p>
<p>[12] L. Wasserman. All of Statistics: a Concise Course in Statistical Inference. Springer, 2004.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
