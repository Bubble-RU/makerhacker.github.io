<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>66 nips-2012-Causal discovery with scale-mixture model for spatiotemporal variance dependencies</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-66" href="../nips2012/nips-2012-Causal_discovery_with_scale-mixture_model_for_spatiotemporal_variance_dependencies.html">nips2012-66</a> <a title="nips-2012-66-reference" href="#">nips2012-66-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>66 nips-2012-Causal discovery with scale-mixture model for spatiotemporal variance dependencies</h1>
<br/><p>Source: <a title="nips-2012-66-pdf" href="http://papers.nips.cc/paper/4541-causal-discovery-with-scale-mixture-model-for-spatiotemporal-variance-dependencies.pdf">pdf</a></p><p>Author: Zhitang Chen, Kun Zhang, Laiwan Chan</p><p>Abstract: In conventional causal discovery, structural equation models (SEM) are directly applied to the observed variables, meaning that the causal effect can be represented as a function of the direct causes themselves. However, in many real world problems, there are signiﬁcant dependencies in the variances or energies, which indicates that causality may possibly take place at the level of variances or energies. In this paper, we propose a probabilistic causal scale-mixture model with spatiotemporal variance dependencies to represent a speciﬁc type of generating mechanism of the observations. In particular, the causal mechanism including contemporaneous and temporal causal relations in variances or energies is represented by a Structural Vector AutoRegressive model (SVAR). We prove the identiﬁability of this model under the non-Gaussian assumption on the innovation processes. We also propose algorithms to estimate the involved parameters and discover the contemporaneous causal structure. Experiments on synthetic and real world data are conducted to show the applicability of the proposed model and algorithms.</p><br/>
<h2>reference text</h2><p>[1] T. Bollerslev. Generalized autoregressive conditional heteroskedasticity. 31(3):307–327, 1986.  Journal of econometrics,</p>
<p>[2] Z. Chen and L. Chan. Causal discovery for linear non-gaussian acyclic models in the presence of latent gaussian confounders. In Proceedings of the 10th international conference on Latent Variable Analysis and Signal Separation, pages 17–24. Springer-Verlag, 2012.</p>
<p>[3] P. Comon. Independent component analysis, a new concept? Signal processing, 36(3):287–314, 1994.</p>
<p>[4] R. Henao and O. Winther. Sparse linear identiﬁable multivariate modeling. Journal of Machine Learning Research, 12:863–905, 2011.</p>
<p>[5] J. Hirayama and A. Hyv¨ rinen. Structural equations and divisive normalization for energy-dependent a component analysis. Advances in Neural Information Processing Systems (NIPS2011), 24, 2012.</p>
<p>[6] P.O. Hoyer, S. Shimizu, A.J. Kerminen, and M. Palviainen. Estimation of causal effects using linear non-gaussian causal models with hidden variables. International Journal of Approximate Reasoning, 49(2):362–378, 2008.</p>
<p>[7] A. Hyv¨ rinen. Pairwise measures of causal direction in linear non-gaussian acyclic models. In JMLR a Workshop and Conference Proceedings (Proc. 2nd Asian Conference on Machine Learning), ACML2010, volume 13, pages 1–16, 2010.</p>
<p>[8] A. Hyv¨ rinen, P. O. Hoyer, and M. Inki. Topographic independent component analysis. Neural Compua tation, 13(7):1527–1558, 2001.</p>
<p>[9] A. Hyv¨ rinen and J. Hurri. Blind separation of sources that have spatiotemporal variance dependencies. a Signal Processing, 84(2):247–254, 2004.</p>
<p>[10] A. Hyv¨ rinen and E. Oja. Independent component analysis: algorithms and applications. Neural neta works, 13(4-5):411–430, 2000.</p>
<p>[11] A. Hyv¨ rinen, K. Zhang, S. Shimizu, and P. O. Hoyer. Estimation of a structural vector autoregression a model using non-gaussianity. Journal of Machine Learning Research, 11:1709–1731, 2010.</p>
<p>[12] D. Janzing, J. Mooij, K. Zhang, J. Lemeire, J. Zscheischler, P. Daniuˇis, B. Steudel, and B. Sch¨ lkopf. s o Information-geometric approach to inferring causal directions. Artiﬁcial Intelligence, 2012.</p>
<p>[13] Y. Kawahara, S. Shimizu, and T. Washio. Analyzing relationships among arma processes based on nongaussianity of external inﬂuences. Neurocomputing, 2011.</p>
<p>[14] A. Moneta, D. Entner, PO Hoyer, and A. Coad. Causal inference by independent component analysis with applications to micro-and macroeconomic data. Jena Economic Research Papers, 2010:031, 2010.</p>
<p>[15] J. Pearl. Causality: models, reasoning, and inference. Cambridge Univ Pr, 2000.</p>
<p>[16] S. Shimizu, P.O. Hoyer, A. Hyv¨ rinen, and A. Kerminen. A linear non-gaussian acyclic model for causal a discovery. Journal of Machine Learning Research, 7:2003–2030, 2006.</p>
<p>[17] S. Shimizu, T. Inazumi, Y. Sogawa, A. Hyv¨ rinen, Y. Kawahara, T. Washio, P.O. Hoyer, and K. Bollen. a Directlingam: A direct method for learning a linear non-gaussian structural equation model. Journal of Machine Learning Research, 12:1225–1248, 2011.</p>
<p>[18] Y. Sogawa, S. Shimizu, T. Shimamura, A. Hyv¨ rinen, T. Washio, and S. Imoto. Estimating exogenous a variables in data with more variables than observations. Neural Networks, 2011.</p>
<p>[19] P. Spirtes, C.N. Glymour, and R. Scheines. Causation, prediction, and search. The MIT Press, 2000.</p>
<p>[20] K. Zhang and L. Chan. Efﬁcient factor garch models and factor-dcc models. Quantitative Finance, 9(1):71–91, 2009.</p>
<p>[21] K. Zhang and L.W. Chan. Extensions of ica for causality discovery in the hong kong stock market. In Proc. of the 13th international conference on Neural information processing-Volume Part III, pages 400–409. Springer-Verlag, 2006.</p>
<p>[22] K. Zhang and A. Hyv¨ rinen. On the identiﬁability of the post-nonlinear causal model. In Proceedings of a the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence, pages 647–655, 2009.</p>
<p>[23] K. Zhang and A. Hyv¨ rinen. Source separation and higher-order causal analysis of meg and eeg. In a Proceedings of the Twenty-Sixth Conference on Uncertainty in Artiﬁcial Intelligence, pages 709–716, 2010.</p>
<p>[24] K. Zhang and A. Hyv¨ rinen. A general linear non-gaussian state-space model: Identiﬁability, identiﬁcaa tion, and applications. In Proceedings of Asian Conference on Machine Learning, JMLR W&CP;, pages 113–128, 2011.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
