<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-174" href="../nips2010/nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">nips2010-174</a> <a title="nips-2010-174-reference" href="#">nips2010-174-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</h1>
<br/><p>Source: <a title="nips-2010-174-pdf" href="http://papers.nips.cc/paper/4177-multi-label-multiple-kernel-learning-by-stochastic-approximation-application-to-visual-object-recognition.pdf">pdf</a></p><p>Author: Serhat Bucak, Rong Jin, Anil K. Jain</p><p>Abstract: Recent studies have shown that multiple kernel learning is very effective for object recognition, leading to the popularity of kernel learning in computer vision problems. In this work, we develop an efﬁcient algorithm for multi-label multiple kernel learning (ML-MKL). We assume that all the classes under consideration share the same combination of kernel functions, and the objective is to ﬁnd the optimal kernel combination that beneﬁts all the classes. Although several algorithms have been developed for ML-MKL, their computational cost is linear in the number of classes, making them unscalable when the number of classes is large, a challenge frequently encountered in visual object recognition. We address this computational challenge by developing a framework for ML-MKL that combines the worst-case analysis with stochastic approximation. Our analysis √ shows that the complexity of our algorithm is O(m1/3 lnm), where m is the number of classes. Empirical studies with object recognition show that while achieving similar classiﬁcation accuracy, the proposed method is signiﬁcantly more efﬁcient than the state-of-the-art algorithms for ML-MKL. 1</p><br/>
<h2>reference text</h2><p>[1] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The PASCAL Visual Object Classes Challenge 2009 (VOC2009) Results.” http://www.pascal-network.org/challenges/VOC/voc2009/workshop/index.html.</p>
<p>[2] G. Lanckriet, T. De Bie, N. Cristianini, M. Jordan, and W. Noble, “A statistical framework for genomic data fusion,” Bioinformatics, vol. 20, pp. 2626–2635, 2004.</p>
<p>[3] S. Ji, L. Sun, R. Jin, and J. Ye, “Multi-label multiple kernel learning,” in Proceedings of Neural Information Processings Systems, 2008.</p>
<p>[4] G. Lanckriet, N. Cristianini, P. Bartlett, L. Ghaoui, and M. Jordan, “Learning the kernel matrix with semideﬁnite programming,” Journal of Machine Learning Research, vol. 5, pp. 27–72, 2004.</p>
<p>[5] O. Chapelle and A. Rakotomamonjy, “Second order optimization of kernel parameters,” in NIPS Workshop on Kernel Learning: Automatic Selection of Optimal Kernels, 2008.</p>
<p>[6] P. Gehler and S. Nowozin, “On feature combination for multiclass object classiﬁcation,” in Proceedings of the IEEE International Conference on Computer Vision, 2009.</p>
<p>[7] P. Gehler and S. Nowozin, “Let the kernel ﬁgure it out: Principled learning of pre-processing for kernel classiﬁers,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2009.</p>
<p>[8] F. Bach, G. Lanckriet, and M. Jordan, “Multiple kernel learning, conic duality, and the smo algorithm,” in Proceedings of the 21st International Conference on Machine Learning, 2004.</p>
<p>[9] S. Sonnenburg, G. Ratsch, and C. Schafer, “A general and efﬁcient multiple kernel learning algorithm,” in Proceedings of Neural Information Processings Systems, pp. 1273–1280, 2006.</p>
<p>[10] A. Rakotomamonjy, F. Bach, Y. Grandvalet, and S. Canu, “SimpleMKL,” Journal of Machine Learning Research, vol. 9, pp. 2491–2521, 2008.  8</p>
<p>[11] Z. Xu, R. Jin, I. King, and M. R. Lyu, “An extended level method for efﬁcient multiple kernel learning,” in Proceedings of Neural Information Processings Systems, pp. 1825–1832, 2008.</p>
<p>[12] Z. Xu, R. Jin, H. Yang, I. King, and M. R. Lyu, “Simple and efﬁcient multiple kernel learning by group lasso,” in Proceedings of the 27th International Conference on Machine Learning, 2010.</p>
<p>[13] F. Bach, “Consistency of the group lasso and multiple kernel learning,” Journal of Machine Learning Research, vol. 9, pp. 1179–1225, 2008.</p>
<p>[14] Z. Xu, R. Jin, S. Zhu, M. R. Lyu, and I. King, “Smooth optimization for effective multiple kernel learning,” in Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2010.</p>
<p>[15] A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet, “More efﬁciency in multiple kernel learning,” in Proceedings of the 24th International Conference on Machine Learning, 2007.</p>
<p>[16] M. Kloft, U. Brefeld, A. Sonnenburg, and A. Zien, “Comparing sparse and non-sparse multiple kernel learning,” in NIPS Workshop on Understanding Multiple Kernel Learning Methods, 2009.</p>
<p>[17] M. Kloft, U. Brefeld, A. Sonnenburg, P. Laskov, K.-R. Muller, and A. Zien, “Efﬁcient and accurate lp-norm multiple kernel learning,” in Proceedings of Neural Information Processings Systems, 2009.</p>
<p>[18] S. Hoi, M. Lyu, and E. Chang, “Learning the uniﬁed kernel machines for classiﬁcation,” in Proceedings of the Conference on Knowledge Discovery and Data Mining, p. 187196, 2006.</p>
<p>[19] J. Ye, J. Chen, and J. S., “Discriminant kernel and regularization parameter learning via semideﬁnite programming,” in Proceedings of the International Conference on Machine Learning, p. 10951102, 2007.</p>
<p>[20] A. Zien and S. Cheng, “Multiclass multiple kernel learning,” in Proceedings of the 24th International Conference on Machine Learning, 2007.</p>
<p>[21] L. Tang, J. Chen, and J. Ye, “On multiple kernel learning with multiple labels,” in Proceedings of the 21st International Jont Conference on Artiﬁcal Intelligence, 2009.</p>
<p>[22] J. Yang, Y. Li, Y. Tian, L. Duan, and W. Gao, “Group-sensitive multiple kernel learning for object categorization,” in Proceedings of the IEEE International Conference on Computer Vision, 2009.</p>
<p>[23] F. Orabona, L. Jie, and B. Caputo, “Online-batch strongly convex multi kernel learning,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2010.</p>
<p>[24] A. Nemirovski, “Prox-method with rate of convergence o(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems,” SIAM Journal on Optimization, vol. 15, pp. 229–251, 2004.</p>
<p>[25] M. Varma and D. Ray, “Learning the discriminative power-invariance trade-off,” in Proceedings of the IEEE International Conference on Computer Vision, October 2007.</p>
<p>[26] M. Everingham, A. Zisserman, C. K. I. Williams, and L. Van Gool, “The PASCAL Visual Object Classes Challenge 2006 (VOC2006) Results.” http://www.pascal-network.org/challenges/VOC/voc2006/results.pdf.</p>
<p>[27] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results.” http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html.</p>
<p>[28] A. Vedaldi and B. Fulkerson, “VLFeat: An open and portable library of computer vision algorithms.” http://www. vlfeat.org/, 2008.</p>
<p>[29] A. Berg, T. Berg, and J. Malik, “Shape matching and object recognition using low distortion correspondences,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2005.</p>
<p>[30] S. Lazebnik, C. Schmid, and P. Ponce, “Beyond bag of features: Spatial pyramid matching for recognizing natural scene categories,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2006.</p>
<p>[31] E. Shechtman and I. M., “Matching local self-similarities across images and videos,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2007.</p>
<p>[32] K. Mikolajczyk and C. Schmid, “Distinctive image features from scale-invariant keypoints,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, no. 10, pp. 1615–1630, 2005.</p>
<p>[33] D. Lowe, “Distinctive image features from scale-invariant keypoints,” International Journal of Computer Vision, vol. 2, no. 60, pp. 91–110, 2004.</p>
<p>[34] S. Lazebnik, C. Schmid, and P. Ponce, “Sparse texture representation using afﬁne-invariant neighborhoods,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2003.</p>
<p>[35] M. Muja and D. G. Lowe, “Fast approximate nearest neighbors with automatic algorithm conﬁguration,” in Proceedings of the International Conference on Computer Vision Theory and Application, pp. 331–340, INSTICC Press, 2009.</p>
<p>[36] J. Saketha Nath, G. Dinesh, S. Raman, C. Bhattacharyya, A. Ben-Tal, and K. Ramakrishan, “On the algorithmics and applications of a mixed-norm based kernel learning formulation,” in Proceedings of Neural Information Processings Systems, 2009.  9</p>
<br/>
<br/><br/><br/></body>
</html>
