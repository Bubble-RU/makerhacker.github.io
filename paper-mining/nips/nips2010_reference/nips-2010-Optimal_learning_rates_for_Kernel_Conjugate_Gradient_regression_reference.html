<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-199" href="../nips2010/nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">nips2010-199</a> <a title="nips-2010-199-reference" href="#">nips2010-199-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</h1>
<br/><p>Source: <a title="nips-2010-199-pdf" href="http://papers.nips.cc/paper/4077-optimal-learning-rates-for-kernel-conjugate-gradient-regression.pdf">pdf</a></p><p>Author: Gilles Blanchard, Nicole Krämer</p><p>Abstract: We prove rates of convergence in the statistical sense for kernel-based least squares regression using a conjugate gradient algorithm, where regularization against overﬁtting is obtained by early stopping. This method is directly related to Kernel Partial Least Squares, a regression method that combines supervised dimensionality reduction with least squares projection. The rates depend on two key quantities: ﬁrst, on the regularity of the target regression function and second, on the effective dimensionality of the data mapped into the kernel space. Lower bounds on attainable rates depending on these two quantities were established in earlier literature, and we obtain upper bounds for the considered method that match these lower bounds (up to a log factor) if the true regression function belongs to the reproducing kernel Hilbert space. If this assumption is not fulﬁlled, we obtain similar convergence rates provided additional unlabeled data are available. The order of the learning rates match state-of-the-art results that were recently obtained for least squares support vector machines and for linear regularization operators. 1</p><br/>
<h2>reference text</h2><p>[1] F. Bauer, S. Pereverzev, and L. Rosasco. On Regularization Algorithms in Learning Theory. Journal of Complexity, 23:52–72, 2007.</p>
<p>[2] N. Bissantz, T. Hohage, A. Munk, and F. Ruymgaart. Convergence Rates of General Regularization Methods for Statistical Inverse Problems and Applications. SIAM Journal on Numerical Analysis, 45(6):2610–2636, 2007.</p>
<p>[3] G. Blanchard and N. Kr¨ mer. Kernel Partial Least Squares is Universally Consistent. Proa ceedings of the 13th International Conference on Artiﬁcial Intelligence and Statistics, JMLR Workshop & Conference Proceedings, 9:57–64, 2010.</p>
<p>[4] G. Blanchard and P. Massart. Discussion of V. Koltchinskii’s ”Local Rademacher complexities and oracle inequalities in risk minimization”. Annals of Statistics, 34(6):2664–2671, 2006.</p>
<p>[5] A. Caponnetto. Optimal Rates for Regularization Operators in Learning Theory. Technical Report CBCL Paper 264/ CSAIL-TR 2006-062, Massachusetts Institute of Technology, 2006.</p>
<p>[6] A. Caponnetto and E. De Vito. Optimal Rates for Regularized Least-squares Algorithm. Foundations of Computational Mathematics, 7(3):331–368, 2007.</p>
<p>[7] A. Caponnetto and Y. Yao. Cross-validation based Adaptation for Regularization Operators in Learning Theory. Analysis and Applications, 8(2):161–183, 2010.</p>
<p>[8] H. Chun and S. Keles. Sparse Partial Least Squares for Simultaneous Dimension Reduction and Variable Selection. Journal of the Royal Statistical Society B, 72(1):3–25, 2010.</p>
<p>[9] E. De Vito, L. Rosasco, A. Caponnetto, U. De Giovannini, and F. Odone. Learning from Examples as an Inverse Problem. Journal of Machine Learning Research, 6(1):883, 2006.</p>
<p>[10] L. Gy¨ rﬁ, M. Kohler, A. Krzyzak, and H. Walk. A Distribution-Free Theory of Nonparametric o Regression. Springer, 2002.</p>
<p>[11] M. Hanke. Conjugate Gradient Type Methods for Linear Ill-posed Problems. Pitman Research Notes in Mathematics Series, 327, 1995.</p>
<p>[12] L. Lo Gerfo, L. Rosasco, E. Odone, F.and De Vito, and A. Verri. Spectral Algorithms for Supervised Learning. Neural Computation, 20:1873–1897, 2008.</p>
<p>[13] S. Mendelson and J. Neeman. Regularization in Kernel Learning. The Annals of Statistics, 38(1):526–565, 2010.</p>
<p>[14] P. Naik and C.L. Tsai. Partial Least Squares Estimator for Single-index Models. Journal of the Royal Statistical Society B, 62(4):763–771, 2000.</p>
<p>[15] A. S. Nemirovskii. The Regularizing Properties of the Adjoint Gradient Method in Ill-posed Problems. USSR Computational Mathematics and Mathematical Physics, 26(2):7–16, 1986.</p>
<p>[16] C. S. Ong. Kernels: Regularization and Optimization. Doctoral dissertation, Australian National University, 2005.</p>
<p>[17] C. S. Ong, X. Mary, S. Canu, and A. J. Smola. Learning with Non-positive Kernels. In Proceedings of the 21st International Conference on Machine Learning, pages 639 – 646, 2004.</p>
<p>[18] R. Rosipal and L.J. Trejo. Kernel Partial Least Squares Regression in Reproducing Kernel Hilbert Spaces. Journal of Machine Learning Research, 2:97–123, 2001.</p>
<p>[19] R. Rosipal, L.J. Trejo, and B. Matthews. Kernel PLS-SVC for Linear and Nonlinear Classiﬁcation. In Proceedings of the Twentieth International Conference on Machine Learning, pages 640–647, Washington, DC, 2003.</p>
<p>[20] I. Steinwart, D. Hush, and C. Scovel. Optimal Rates for Regularized Least Squares Regression. In Proceedings of the 22nd Annual Conference on Learning Theory, pages 79–93, 2009.</p>
<p>[21] S. Wold, H. Ruhe, H. Wold, and W.J. Dunn III. The Collinearity Problem in Linear Regression. The Partial Least Squares (PLS) Approach to Generalized Inverses. SIAM Journal of Scientiﬁc and Statistical Computations, 5:735–743, 1984.</p>
<p>[22] T. Zhang. Learning bounds for kernel regression using effective data dimensionality. Neural Computation, 17(9):2077–2098, 2005.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
