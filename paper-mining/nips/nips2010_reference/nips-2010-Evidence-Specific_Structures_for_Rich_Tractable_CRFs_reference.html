<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>83 nips-2010-Evidence-Specific Structures for Rich Tractable CRFs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-83" href="../nips2010/nips-2010-Evidence-Specific_Structures_for_Rich_Tractable_CRFs.html">nips2010-83</a> <a title="nips-2010-83-reference" href="#">nips2010-83-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>83 nips-2010-Evidence-Specific Structures for Rich Tractable CRFs</h1>
<br/><p>Source: <a title="nips-2010-83-pdf" href="http://papers.nips.cc/paper/4002-evidence-specific-structures-for-rich-tractable-crfs.pdf">pdf</a></p><p>Author: Anton Chechetka, Carlos Guestrin</p><p>Abstract: We present a simple and effective approach to learning tractable conditional random ﬁelds with structure that depends on the evidence. Our approach retains the advantages of tractable discriminative models, namely efﬁcient exact inference and arbitrarily accurate parameter learning in polynomial time. At the same time, our algorithm does not suffer a large expressive power penalty inherent to ﬁxed tractable structures. On real-life relational datasets, our approach matches or exceeds state of the art accuracy of the dense models, and at the same time provides an order of magnitude speedup. 1</p><br/>
<h2>reference text</h2><p>[1] J. D. Lafferty, A. McCallum, and F. C. N. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In ICML, 2001.</p>
<p>[2] M. Schmidt, K. Murphy, G. Fung, and R. Rosales. Structure learning in random ﬁelds for heart motion abnormality detection. In CVPR, 2008.</p>
<p>[3] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. 2009.</p>
<p>[4] J. K. Bradley and C. Guestrin. Learning tree conditional random ﬁelds. In ICML, to appear, 2010.</p>
<p>[5] D. Roth. On the hardness of approximate reasoning. Artiﬁcial Intelligence, 82(1-2), 1996.</p>
<p>[6] C. Sutton and A. McCallum. Piecewise pseudolikelihood for efﬁcient CRF training. In ICML, 2007.</p>
<p>[7] C. Chow and C. Liu. Approximating discrete probability distributions with dependence trees. IEEE Trans. on Inf. Theory, 14(3), 1968.</p>
<p>[8] D. Karger and N. Srebro. Learning Markov networks: Maximum bounded tree-width graphs. In SODA’01.</p>
<p>[9] D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(3), 1989.</p>
<p>[10] J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. 1988.</p>
<p>[11] J. S. Yedidia, W. T. Freeman, and Y. Weiss. Generalized belief propagation. In NIPS, 2000.</p>
<p>[12] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. Pattern Analysis and Machine Intelligence, IEEE Transactions on, PAMI-6(6), 1984.</p>
<p>[13] S. Arnborg, D. G. Corneil, and A. Proskurowski. Complexity of ﬁnding embeddings in a k-tree. SIAM Journal on Algebraic and Discrete Methods, 8(2), 1987.</p>
<p>[14] W. H¨ rdle, M. M¨ ller, S. Sperlich, and A. Werwatz. Nonparametric and Semiparametric Models. 2004. a u</p>
<p>[15] D. Shahaf, A. Chechetka, and C. Guestrin. Learning thin junction trees via graph cuts. In AISTATS, 2009.</p>
<p>[16] L. Getoor and B. Taskar. Introduction to Statistical Relational Learning. The MIT Press, 2007.</p>
<p>[17] A. Deshpande, C. Guestrin, S. Madden, J. Hellerstein, and W. Hong. Model-driven data acquisition in sensor networks. In VLDB, 2004.</p>
<p>[18] A. Krause and C. Guestrin. Near-optimal nonmyopic value of information in graphical models. In UAI’05.</p>
<p>[19] A. Chechetka, D. Dash, and M. Philipose. Relational learning for collective classiﬁcation of entities in images. In AAAI Workshop on Statistical Relational AI, 2010.</p>
<p>[20] M. Richardson and P. Domingos. Markov logic networks. Machine Learning, 62(1-2), 2006.</p>
<p>[21] S. Kok, M. Sumner, M. Richardson, P. Singla, H. Poon, D. Lowd, and P. Domingos. The alchemy system for statistical relational AI. Technical report, University of Washington, Seattle, WA., 2009.</p>
<p>[22] J. Gonzalez, Y. Low, and C. Guestrin. Residual splash for optimally parallelizing belief propagation. In AISTATS, 2009.</p>
<p>[23] B. Taskar, P. Abbeel, and D. Koller. Discriminative probabilistic models for relational data. In UAI, 2002.</p>
<p>[24] B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks. In NIPS, 2003.</p>
<p>[25] C. Boutilier, N. Friedman, M. Goldszmidt, and D. Koller. Context-speciﬁc independence in Bayesian networks. In UAI, 1996.</p>
<p>[26] M. desJardins, P. Rathod, and L. Getoor. Bayesian network learning with abstraction hierarchies and context-speciﬁc independence. In ECML, 2005.</p>
<p>[27] B. Thiesson, C. Meek, D. Chickering, and D. Heckerman. Learning mixtures of DAG models. In UAI’97.</p>
<p>[28] M. Meil˘ and M. I. Jordan. Learning with mixtures of trees. JMLR, 1, 2001. a</p>
<p>[29] A. J. Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Transactions on Information Theory, IT-13, 1967.</p>
<p>[30] S. L. Lauritzen. The EM algorithm for graphical association models with missing data. Computational Statistics & Data Analysis, 19(2), 1995.</p>
<p>[31] A. Torralba, K. P. Murphy, and W. T. Freeman. Contextual models for object detection using boosted random ﬁelds. In NIPS, 2004.</p>
<p>[32] C. Sutton and A. McCallum. Collective segmentation and labeling of distant entities in information extraction. In ICML Workshop on Statistical Relational Learning and Its Connections, 2004.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
