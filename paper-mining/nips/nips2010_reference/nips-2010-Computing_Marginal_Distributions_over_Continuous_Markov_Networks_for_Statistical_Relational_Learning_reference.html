<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-49" href="../nips2010/nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">nips2010-49</a> <a title="nips-2010-49-reference" href="#">nips2010-49-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</h1>
<br/><p>Source: <a title="nips-2010-49-pdf" href="http://papers.nips.cc/paper/3942-computing-marginal-distributions-over-continuous-markov-networks-for-statistical-relational-learning.pdf">pdf</a></p><p>Author: Matthias Broecheler, Lise Getoor</p><p>Abstract: Continuous Markov random ﬁelds are a general formalism to model joint probability distributions over events with continuous outcomes. We prove that marginal computation for constrained continuous MRFs is #P-hard in general and present a polynomial-time approximation scheme under mild assumptions on the structure of the random ﬁeld. Moreover, we introduce a sampling algorithm to compute marginal distributions and develop novel techniques to increase its efﬁciency. Continuous MRFs are a general purpose probabilistic modeling tool and we demonstrate how they can be applied to statistical relational learning. On the problem of collective classiﬁcation, we evaluate our algorithm and show that the standard deviation of marginals serves as a useful measure of conﬁdence. 1</p><br/>
<h2>reference text</h2><p>[1] E. B Sudderth. Graphical models for visual object recognition and tracking. Ph.D. thesis, Massachusetts Institute of Technology, 2006.</p>
<p>[2] L. Lovasz and S. Vempala. Hit-and-run from a corner. In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 310–314, Chicago, IL, USA, 2004. ACM.</p>
<p>[3] M. Broecheler, L. Mihalkova, and L. Getoor. Probabilistic similarity logic. In Conference on Uncertainty in Artiﬁcial Intelligence, 2010.</p>
<p>[4] M. Richardson and P. Domingos. Markov logic networks. Machine Learning, 62(1):107–136, 2006.</p>
<p>[5] K. Kersting and L. De Raedt. Bayesian logic programs. Technical report, Albert-Ludwigs University, 2001.</p>
<p>[6] B. Taskar, P. Abbeel, and D. Koller. Discriminative probabilistic models for relational data. In Proceedings of UAI-02, 2002.</p>
<p>[7] M. Broecheler, G. Simari, and VS. Subrahmanian. Using histograms to better answer queries to probabilistic logic programs. Logic Programming, page 4054, 2009.</p>
<p>[8] M. E. Dyer and A. M. Frieze. On the complexity of computing the volume of a polyhedron. SIAM Journal on Computing, 17(5):967–974, October 1988.</p>
<p>[9] R. Kannan, L. Lovasz, and M. Simonovits. Random walks and an o*(n5) volume algorithm for convex bodies. Random structures and algorithms, 11(1):150, 1997.</p>
<p>[10] R. L. Smith. Efﬁcient monte carlo procedures for generating points uniformly distributed over bounded regions. Operations Research, 32(6):1296–1308, 1984.</p>
<p>[11] S. Agmon. The relaxation method for linear inequalities. Canadian Journal of Mathematics, 6(3):382392, 1954.</p>
<p>[12] T. S. Motzkin and I. J. Schoenberg. The relaxation method for linear inequalities. IJ Schoenberg: Selected Papers, page 75, 1988.</p>
<p>[13] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classiﬁcation in network data. AI Magazine, 29(3):93, 2008.  9</p>
<br/>
<br/><br/><br/></body>
</html>
