<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>208 nips-2010-Policy gradients in linearly-solvable MDPs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-208" href="../nips2010/nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">nips2010-208</a> <a title="nips-2010-208-reference" href="#">nips2010-208-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>208 nips-2010-Policy gradients in linearly-solvable MDPs</h1>
<br/><p>Source: <a title="nips-2010-208-pdf" href="http://papers.nips.cc/paper/4013-policy-gradients-in-linearly-solvable-mdps.pdf">pdf</a></p><p>Author: Emanuel Todorov</p><p>Abstract: We present policy gradient results within the framework of linearly-solvable MDPs. For the ﬁrst time, compatible function approximators and natural policy gradients are obtained by estimating the cost-to-go function, rather than the (much larger) state-action advantage function as is necessary in traditional MDPs. We also develop the ﬁrst compatible function approximators and natural policy gradients for continuous-time stochastic systems.</p><br/>
<h2>reference text</h2><p>[1] S. Amari. Natural gradient works efﬁciently in learning. Neural Computation, 10:251–276, 1998.</p>
<p>[2] J. Bagnell and J. Schneider. Covariant policy search. In International Joint Conference on Artiﬁcial Intelligence, 2003.</p>
<p>[3] J. Boyan. Least-squares temporal difference learning. In International Conference on Machine Learning, 1999.</p>
<p>[4] W. Fleming and S. Mitter. Optimal control and nonlinear ﬁltering for nondegenerate diffusion processes. Stochastics, 8:226–261, 1982.</p>
<p>[5] S. Kakade. A natural policy gradient. In Advances in Neural Information Processing Systems, 2002.</p>
<p>[6] S. Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, University College London, 2003.</p>
<p>[7] H. Kappen. Linear theory for control of nonlinear stochastic systems. Physical Review Letters, 95, 2005.</p>
<p>[8] V. Konda and J. Tsitsiklis. Actor-critic algorithms. SIAM Journal on Control and Optimization, pages 1008–1014, 2001.</p>
<p>[9] R. Munos. Policy gradient in continuous time. The Journal of Machine Learning Research, 7:771–791, 2006.</p>
<p>[10] B. Oksendal. Stochastic Differential Equations (4th Ed). Springer-Verlag, Berlin, 1995.</p>
<p>[11] J. Peters and S. Schaal. Natural actor-critic. Neurocomputing, 71:1180–1190, 2008.</p>
<p>[12] M. Schmidt. minfunc. online material, 2005.</p>
<p>[13] R. Stengel. Optimal Control and Estimation. Dover, New York, 1994.</p>
<p>[14] R. Sutton, D. Mcallester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems, 2000.</p>
<p>[15] E. Todorov. Linearly-solvable Markov decision problems. Advances in Neural Information Processing Systems, 2006.</p>
<p>[16] E. Todorov. Efﬁcient computation of optimal actions. PNAS, 106:11478–11483, 2009.</p>
<p>[17] E. Todorov. Eigen-function approximation methods for linearly-solvable optimal control problems. IEEE ADPRL, 2009.</p>
<p>[18] R. Williams. Simple statistical gradient following algorithms for connectionist reinforcement learning. Machine Learning, pages 229–256, 1992.  9</p>
<br/>
<br/><br/><br/></body>
</html>
