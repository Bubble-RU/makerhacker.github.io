<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>265 nips-2010-The LASSO risk: asymptotic results and real world examples</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-265" href="../nips2010/nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">nips2010-265</a> <a title="nips-2010-265-reference" href="#">nips2010-265-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>265 nips-2010-The LASSO risk: asymptotic results and real world examples</h1>
<br/><p>Source: <a title="nips-2010-265-pdf" href="http://papers.nips.cc/paper/4096-the-lasso-risk-asymptotic-results-and-real-world-examples.pdf">pdf</a></p><p>Author: Mohsen Bayati, José Pereira, Andrea Montanari</p><p>Abstract: We consider the problem of learning a coefﬁcient vector x0 ∈ RN from noisy linear observation y = Ax0 + w ∈ Rn . In many contexts (ranging from model selection to image processing) it is desirable to construct a sparse estimator x. In this case, a popular approach consists in solving an ℓ1 -penalized least squares problem known as the LASSO or Basis Pursuit DeNoising (BPDN). For sequences of matrices A of increasing dimensions, with independent gaussian entries, we prove that the normalized risk of the LASSO converges to a limit, and we obtain an explicit expression for this limit. Our result is the ﬁrst rigorous derivation of an explicit formula for the asymptotic mean square error of the LASSO for random instances. The proof technique is based on the analysis of AMP, a recently developed efﬁcient algorithm, that is inspired from graphical models ideas. Through simulations on real data matrices (gene expression data and hospital medical records) we observe that these results can be relevant in a broad array of practical applications.</p><br/>
<h2>reference text</h2><p>[AJ07]  G. Andrew and G. Jianfeng, Scalable training of l1 -regularized log-linear models, Proceedings of the 24th international conference on Machine learning, 2007, pp. 33–40.  [BBM10] M. Bayati, J .A. Bento, and A. Montanari, The LASSO risk: asymptotic results and real world examples, Long version (in preparation), 2010. [BM10a]  [BM10b]  M. Bayati and A. Montanari, The dynamics of message passing on dense graphs, with applications to compressed sensing, Proceedings of IEEE International Symposium on Inform. Theory (ISIT), 2010, Longer version in http://arxiv.org/abs/1001.3448. , The LASSO risk for gaussian matrices, 2010, preprint available in http://arxiv.org/abs/1008.2581.  [CD95]  S.S. Chen and D.L. Donoho, Examples of basis pursuit, Proceedings of Wavelet Applications in Signal and Image Processing III (San Diego, CA), 1995.  [CRT06]  E. Candes, J. K. Romberg, and T. Tao, Stable signal recovery from incomplete and inaccurate measurements, Communications on Pure and Applied Mathematics 59 (2006), 1207–1223.  [CT07]  E. Candes and T. Tao, The Dantzig selector: statistical estimation when p is much larger than n, Annals of Statistics 35 (2007), 2313–2351.  [DMM09] D. L. Donoho, A. Maleki, and A. Montanari, Message Passing Algorithms for Compressed Sensing, Proceedings of the National Academy of Sciences 106 (2009), 18914– 18919. [DMM10] D.L. Donoho, A. Maleki, and A. Montanari, The Noise Sensitivity Phase Transition in Compressed Sensing, Preprint, 2010. [GBS09]  D. Guo, D. Baron, and S. Shamai, A single-letter characterization of optimal noisy compressed sensing, 47th Annual Allerton Conference (Monticello, IL), September 2009.  [Joh06]  I. Johnstone, High Dimensional Statistical Inference and Random Matrices, Proc. International Congress of Mathematicians (Madrid), 2006.  [KKL+ 07] S. J. Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinevsky, A method for large-scale ℓ1 -regularized least squares., IEEE Journal on Selected Topics in Signal Processing 1 (2007), 606–617. [KM10]  S. Korada and A. Montanari, Applications of Lindeberg Principle in Communications and Statistical Learning, preprint available in http://arxiv.org/abs/1004.0557, 2010.  [KWT09] Y. Kabashima, T. Wadayama, and T. Tanaka, A typical reconstruction limit for compressed sensing based on lp-norm minimization, J.Stat. Mech. (2009), L09003. [MM09] M. M´ zard and A. Montanari, Information, Physics and Computation, Oxford Univere sity Press, Oxford, 2009. [RFG09] S. Rangan, A. K. Fletcher, and V. K. Goyal, Asymptotic analysis of map estimation via the replica method and applications to compressed sensing, PUT NIPS REF, 2009. [Tel99] E. Telatar, Capacity of Multi-antenna Gaussian Channels, European Transactions on Telecommunications 10 (1999), 585–595. [Tib96] R. Tibshirani, Regression shrinkage and selection with the lasso, J. Royal. Statist. Soc B 58 (1996), 267–288. [ZY06]  P. Zhao and B. Yu, On model selection consistency of Lasso, The Journal of Machine Learning Research 7 (2006), 2541–2563.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
