<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>4 nips-2010-A Computational Decision Theory for Interactive Assistants</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-4" href="../nips2010/nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">nips2010-4</a> <a title="nips-2010-4-reference" href="#">nips2010-4-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>4 nips-2010-A Computational Decision Theory for Interactive Assistants</h1>
<br/><p>Source: <a title="nips-2010-4-pdf" href="http://papers.nips.cc/paper/4052-a-computational-decision-theory-for-interactive-assistants.pdf">pdf</a></p><p>Author: Alan Fern, Prasad Tadepalli</p><p>Abstract: We study several classes of interactive assistants from the points of view of decision theory and computational complexity. We ﬁrst introduce a class of POMDPs called hidden-goal MDPs (HGMDPs), which formalize the problem of interactively assisting an agent whose goal is hidden and whose actions are observable. In spite of its restricted nature, we show that optimal action selection in ﬁnite horizon HGMDPs is PSPACE-complete even in domains with deterministic dynamics. We then introduce a more restricted model called helper action MDPs (HAMDPs), where the assistant’s action is accepted by the agent when it is helpful, and can be easily ignored by the agent otherwise. We show classes of HAMDPs that are complete for PSPACE and NP along with a polynomial time class. Furthermore, we show that for general HAMDPs a simple myopic policy achieves a regret, compared to an omniscient assistant, that is bounded by the entropy of the initial goal distribution. A variation of this policy is shown to achieve worst-case regret that is logarithmic in the number of goals for any goal distribution. 1</p><br/>
<h2>reference text</h2><p>[1] Xinlong Bao, Jonathan L. Herlocker, and Thomas G. Dietterich. Fewer clicks and less frustration: reducing the cost of reaching the right folder. In IUI, pages 178–185, 2006.</p>
<p>[2] J. Boger, P. Poupart, J. Hoey, C. Boutilier, G. Fernie, and A. Mihailidis. A decision-theoretic approach to task assistance for persons with dementia. In IJCAI, 2005.</p>
<p>[3] Anton N. Dragunov, Thomas G. Dietterich, Kevin Johnsrude, Matt McLaughlin, Lida Li, and Jon L. Herlocker. Tasktracer: A desktop environment to support multi-tasking knowledge workers. In Proceedings of IUI, 2005.</p>
<p>[4] Andrzej Ehrenfeucht and David Haussler. Learning decision trees from random examples. Information and Computation, 82(3):231–246, September 1989.</p>
<p>[5] A. Fern, S. Natarajan, K. Judah, and P. Tadepalli. A decision-theoretic model of assistance. In Proceedings of the International Joint Conference in AI, 2007.</p>
<p>[6] Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in partially observable stochastic domains. Artiﬁcial Intelligence, 101:99–134, 1998.</p>
<p>[7] Tessa A. Lau, Steven A. Wolfman, Pedro Domingos, and Daniel S. Weld. Programming by demonstration using version space algebra. Machine Learning, 53(1-2):111–156, 2003.</p>
<p>[8] H. Lieberman. User interface goals, AI opportunities. AI Magazine, 30(2), 2009.</p>
<p>[9] M. L . Littman. Algorithms for Sequential Decision Making. PhD thesis, Brown University, Providence, RI, 1996.</p>
<p>[10] Martin Mundhenk. The complexity of planning with partially-observable Markov Decision Processes. PhD thesis, Friedrich-Schiller-Universitdt, 2001.</p>
<p>[11] K. Myers, P. Berry, J. Blythe, K. Conley, M. Gervasio, D. McGuinness, D. Morley, A. Pfeffer, M. Pollack, and M. Tambe. An intelligent personal assistant for task and time management. AI Magazine, 28(2):47– 61, 2007.</p>
<p>[12] C. Papadimitriou and J. Tsitsiklis. The complexity of Markov Decision Processes. Mathematics of Operations Research, 12(3):441–450, 1987.</p>
<p>[13] M. Tambe. Electric Elves: What went wrong and why. AI Magazine, 29(2), 2008.  9</p>
<br/>
<br/><br/><br/></body>
</html>
