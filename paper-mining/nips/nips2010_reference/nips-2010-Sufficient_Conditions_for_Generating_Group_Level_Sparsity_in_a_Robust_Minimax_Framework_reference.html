<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-260" href="../nips2010/nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">nips2010-260</a> <a title="nips-2010-260-reference" href="#">nips2010-260-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</h1>
<br/><p>Source: <a title="nips-2010-260-pdf" href="http://papers.nips.cc/paper/4000-sufficient-conditions-for-generating-group-level-sparsity-in-a-robust-minimax-framework.pdf">pdf</a></p><p>Author: Hongbo Zhou, Qiang Cheng</p><p>Abstract: Regularization technique has become a principled tool for statistics and machine learning research and practice. However, in most situations, these regularization terms are not well interpreted, especially on how they are related to the loss function and data. In this paper, we propose a robust minimax framework to interpret the relationship between data and regularization terms for a large class of loss functions. We show that various regularization terms are essentially corresponding to different distortions to the original data matrix. This minimax framework includes ridge regression, lasso, elastic net, fused lasso, group lasso, local coordinate coding, multiple kernel learning, etc., as special cases. Within this minimax framework, we further give mathematically exact deﬁnition for a novel representation called sparse grouping representation (SGR), and prove a set of sufﬁcient conditions for generating such group level sparsity. Under these sufﬁcient conditions, a large set of consistent regularization terms can be designed. This SGR is essentially different from group lasso in the way of using class or group information, and it outperforms group lasso when there appears group label noise. We also provide some generalization bounds in a classiﬁcation setting. 1</p><br/>
<h2>reference text</h2><p>[1] A. Antoniadis and J. Fan. Regularitation of wavelets approximations. J. the American Statistical Association, 96:939–967, 2001.</p>
<p>[2] F. Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine Learning Research, 9:1179–1225, 2008.</p>
<p>[3] F. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the smo algorithm. In Proceedings of the Twenty-ﬁrst International Conference on Machine Learning, 2004.</p>
<p>[4] P. N. Bellhumer, J. Hespanha, and D. Kriegman. Eigenfaces vs. ﬁsherfaces: Recognition using class speciﬁc linear projection. IEEE Trans. Pattern Anal. Mach. Intelligence, 17(7):711–720, 1997.</p>
<p>[5] L. Breiman. Heuristics of instability and stabilization in model selection. Ann. Statist., 24:2350–2383, 1996.</p>
<p>[6] E. Cand´ s, J. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate e measurements. Comm. on Pure and Applied Math, 59(8):1207–1233, 2006.</p>
<p>[7] E. Cand´ s and T. Tao. Near-optimal signal recovery from random projections: Universal ene coding strategies? IEEE Trans. Information Theory, 52(12):5406–5425, 2006.</p>
<p>[8] D. Donoho. For most large underdetermined systems of linear equations the minimum l1 nom solution is also the sparsest solution. Comm. on Pure and Applied Math, 59(6):797–829, 2006.</p>
<p>[9] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. J. Am. Statist. Ass., 96:1348–1360, 2001.</p>
<p>[10] I. Frank and J. Friedman. A statistical view of some chemometrics regression tools. Technometrics, 35:109–148, 1993.</p>
<p>[11] L. El Ghaoui and H. Lebret. Robust solutions to least-squares problems with uncertain data. SIAM Journal Matrix Analysis and Applications, 18:1035–1064, 1997.</p>
<p>[12] G.H. Golub and C.F. Van Loan. Matrix computations. Johns Hopkins Univ Pr, 1996. 8</p>
<p>[13] M. Grant and S. Boyd. Graph implementations for nonsmooth convex programs, recent advances in learning and control. Lecture Notes in Control and Information Sciences, pages 95–110, 2008.</p>
<p>[14] M. Grant and S. Boyd. UCI machine learning repositorycvx: Matlab software for disciplined convex programming, 2009.</p>
<p>[15] A. Hoerl and R. Kennard. Ridge regression. Encyclpedia of Statistical Science, 8:129–136, 1988.</p>
<p>[16] L. Jacob, G. Obozinski, and J.-P. Vert. Group lasso with overlap and graph lasso. In Proceedings of the Twenty-six International Conference on Machine Learning, pages 433–440, 2009.</p>
<p>[17] J. Maletic and A. Marcus. Data cleansing: Beyond integrity analysis. In Proceedings of the Conference on Information Quality, 2000.</p>
<p>[18] K. Orr. Data quality and systems theory. Communications of the ACM, 41(2):66–71, 1998.</p>
<p>[19] R. Tibshirani. Regression shrinkage and selection via the lasso. J. R. Statist. Soc. B, 58:267– 288, 1996.</p>
<p>[20] R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight. Sparsity and smoothness via the fused lasso. J.R.Statist.Soc.B, 67:91–108, 2005.</p>
<p>[21] J. Wright, A.Y. Yang, A. Ganesh, S.S. Sastry, and Y. Ma. Robust face recognition via sparse representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 210– 227, 2009.</p>
<p>[22] X. Wu. Knowledge Acquisition from Databases. Ablex Pulishing Corp, Greenwich, CT, USA, 1995.</p>
<p>[23] H. Xu, C. Caramanis, and S. Mannor. Robust regression and lasso. In NIPS, 2008.</p>
<p>[24] K. Yu, T. Zhang, and Y. Gong. Nonlinear learning using local coordinate coding. In Advances in Neural Information Processing Systems, volume 22, 2009.</p>
<p>[25] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of The Royal Statistical Society Series B, 68(1):49–67, 2006.</p>
<p>[26] X. Zhu, X. Wu, and S. Chen. Eliminating class noise in large datasets. In Proceedings of the 20th ICML International Conference on Machine Learning, Washington D.C., USA, March 2003.</p>
<p>[27] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. J. R. Statist. Soc. B, 67(2):301–320, 2005.  9</p>
<br/>
<br/><br/><br/></body>
</html>
