<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-272" href="../nips2010/nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">nips2010-272</a> <a title="nips-2010-272-reference" href="#">nips2010-272-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</h1>
<br/><p>Source: <a title="nips-2010-272-pdf" href="http://papers.nips.cc/paper/4003-towards-holistic-scene-understanding-feedback-enabled-cascaded-classification-models.pdf">pdf</a></p><p>Author: Congcong Li, Adarsh Kowdle, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: In many machine learning domains (such as scene understanding), several related sub-tasks (such as scene categorization, depth estimation, object detection) operate on the same raw data and provide correlated outputs. Each of these tasks is often notoriously hard, and state-of-the-art classiﬁers already exist for many subtasks. It is desirable to have an algorithm that can capture such correlation without requiring to make any changes to the inner workings of any classiﬁer. We propose Feedback Enabled Cascaded Classiﬁcation Models (FE-CCM), that maximizes the joint likelihood of the sub-tasks, while requiring only a ‘black-box’ interface to the original classiﬁer for each sub-task. We use a two-layer cascade of classiﬁers, which are repeated instantiations of the original ones, with the output of the ﬁrst layer fed into the second layer as input. Our training method involves a feedback step that allows later classiﬁers to provide earlier classiﬁers information about what error modes to focus on. We show that our method signiﬁcantly improves performance in all the sub-tasks in two different domains: (i) scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection, and (ii) robotic grasping, where we consider grasp point detection and object classiﬁcation. 1</p><br/>
<h2>reference text</h2><p>[1] R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk. Frequency-tuned Salient Region Detection. In CVPR, 2009.</p>
<p>[2] A. Agarwal and B. Triggs. Monocular human motion capture with a mixture of regressors. In IEEE Workshop Vision for HCI, CVPR, 2005.</p>
<p>[3] Y. Bengio and Y. LeCun. Scaling learning algorithms towards ai. In Large-Scale Kernel Machines, 2007.</p>
<p>[4] S. C. Brubaker, J. Wu, J. Sun, M. D. Mullin, and J. M. Rehg. On the design of cascades of boosted ensembles for face detection. IJCV, 77(1-3):65–86, 2008.</p>
<p>[5] R. Caruana. Multitask learning. Machine Learning, 28:41–75, 1997.</p>
<p>[6] R. Collobert and J. Weston. A uniﬁed architecture for natural language processing: Deep neural networks with multitask learning. In ICML, 2008.</p>
<p>[7] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. CVPR, 2005.</p>
<p>[8] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. J of Royal Stat. Soc., Series B, 39(1):1–38, 1977.</p>
<p>[9] M. Everingham, A. Zisserman, C. K. I. Williams, and L. Van Gool. The pascal voc2006 results.</p>
<p>[10] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Discriminatively trained deformable part models, release 3. http://people.cs.uchicago.edu/∼pff/latent-release3/.</p>
<p>[11] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models. PAMI, 2009.</p>
<p>[12] Y. Freund and R. E. Schapire. Cascaded neural networks based image classiﬁer. In ICASSP, 1993.</p>
<p>[13] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. In EuroCOLT, 1995.</p>
<p>[14] M. Gibbs and D. Mackay. Variational gaussian process classiﬁers. Neural Networks, IEEE Trans, 2000.</p>
<p>[15] I. Goodfellow, Q. Le, A. Saxena, H. Lee, and A. Ng. Measuring invariances in deep networks. In NIPS, 2009.</p>
<p>[16] L. Hansen and P. Salamon. Neural network ensembles. PAMI, 12(10):993–1001, 1990.</p>
<p>[17] G. Heitz, S. Gould, A. Saxena, and D. Koller. Cascaded classiﬁcation models: Combining models for holistic scene understanding. In NIPS, 2008.</p>
<p>[18] G. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. In N. Comp, 2006.</p>
<p>[19] D. Hoiem, A. A. Efros, and M. Hebert. Closing the loop on scene interpretation. In CVPR, 2008.</p>
<p>[20] D. Hoiem, A. A. Efros, and M. Hebert. Putting objects in perspective. IJCV, 2008.</p>
<p>[21] J. Kittler, M. Hatef, R. P. Duin, and J. Matas. On combining classiﬁers. PAMI, 20:226–239, 1998.</p>
<p>[22] A. Kowdle, C. Li, A. Saxena, and T. Chen. A generic model to compose vision modules for holistic scene understanding. In Workshop on Parts and Attributes, ECCV, 2010.</p>
<p>[23] S. Kumar and M. Hebert. A hierarchical ﬁeld framework for uniﬁed context-based classiﬁcation. In ICCV, 2005.</p>
<p>[24] L. Li and L. Fei-Fei. What, where and who? classifying event by scene and object recognition. In ICCV, 2007.</p>
<p>[25] L.-J. Li, R. Socher, and L. Fei-Fei. Towards total scene understanding: Classiﬁcation, annotation and segmentation in an automatic framework. In CVPR, 2009.</p>
<p>[26] R. Neal and G. Hinton. A view of the EM algorithm that justiﬁes incremental, sparse, and other variants. Learning in graphical models, 89:355–368, 1998.</p>
<p>[27] A. Oliva and A. Torralba. Mit outdoor scene dataset. http://people.csail.mit.edu/torralba/code/spatialenvelope/.</p>
<p>[28] A. Oliva and A. Torralba. Modeling the shape of the scene: A holistic representation of the spatial envelope. IJCV, 42:145–175, 2001.</p>
<p>[29] D. Parikh, C. Zitnick, and T. Chen. From appearance to context-based recognition: Dense labeling in small images. CVPR, 2008.</p>
<p>[30] A. Saxena, S. H. Chung, and A. Y. Ng. Learning depth from single monocular images. In NIPS, 2005.</p>
<p>[31] A. Saxena, S. H. Chung, and A. Y. Ng. 3-d depth reconstruction from a single still image. IJCV, 76, 2007.</p>
<p>[32] A. Saxena, J. Driemeyer, J. Kearns, and A. Y. Ng. Robotic grasping of novel objects. In NIPS, 2006.</p>
<p>[33] A. Saxena, J. Schulte, and A. Y. Ng. Depth estimation using monocular and stereo cues. In IJCAI, 2007.</p>
<p>[34] A. Saxena, M. Sun, and A. Y. Ng. Make3d: Learning 3d scene structure from a single still image. IEEE PAMI, 30(5), 2009.</p>
<p>[35] E. B. Sudderth, A. Torralba, W. T. Freeman, and A. S. Willsky. Depth from familiar objects: A hierarchical model for 3d scenes. In CVPR, 2006.</p>
<p>[36] C. Sutton and A. McCallum. Joint parsing and semantic role labeling. In CoNLL, 2005.</p>
<p>[37] A. Toshev, B. Taskar, and K. Daniilidis. Object detection via boundary structure segmentation. In CVPR, 2010.</p>
<p>[38] I. Tsochantaridis, T. Hofmann, and T. Joachims. Support vector machine learning for interdependent and structured output spaces. In ICML, 2004.</p>
<p>[39] Z. Tu. Auto-context and its application to high-level vision tasks. In CVPR, 2008.</p>
<p>[40] P. Viola and M. J. Jones. Robust real-time face detection. IJCV, 57(2):137–154, 2004.</p>
<p>[41] M. Zeiler, D. Krishnan, G. Taylor, and R. Fergus. Deconvolutional networks. In CVPR, 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
