<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>93 nips-2010-Feature Construction for Inverse Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-93" href="../nips2010/nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">nips2010-93</a> <a title="nips-2010-93-reference" href="#">nips2010-93-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>93 nips-2010-Feature Construction for Inverse Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2010-93-pdf" href="http://papers.nips.cc/paper/3918-feature-construction-for-inverse-reinforcement-learning.pdf">pdf</a></p><p>Author: Sergey Levine, Zoran Popovic, Vladlen Koltun</p><p>Abstract: The goal of inverse reinforcement learning is to ﬁnd a reward function for a Markov decision process, given example traces from its optimal policy. Current IRL techniques generally rely on user-supplied features that form a concise basis for the reward. We present an algorithm that instead constructs reward features from a large collection of component features, by building logical conjunctions of those component features that are relevant to the example policy. Given example traces, the algorithm returns a reward function as well as the constructed features. The reward function can be used to recover a full, deterministic, stationary policy, and the features can be used to transplant the reward function into any novel environment on which the component features are well deﬁned. 1</p><br/>
<h2>reference text</h2><p>[1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In ICML ’04: Proceedings of the 21st International Conference on Machine Learning. ACM, 2004.</p>
<p>[2] C. L. Baker, J. B. Tenenbaum, and R. R. Saxe. Goal inference as inverse planning. In Proceedings of the 29th Annual Conference of the Cognitive Science Society, 2007.</p>
<p>[3] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classiﬁcation and Regression Trees. Wadsworth and Brooks, Monterey, CA, 1984.</p>
<p>[4] P. Dayan and B. W. Balleine. Reward, motivation, and reinforcement learning. Neuron, 36(2):285–298, 2002.</p>
<p>[5] D. P. de Farias and B. Van Roy. The linear programming approach to approximate dynamic programming. Operations Research, 51(6):850–865, 2003.</p>
<p>[6] M. Grant and S. Boyd. CVX: Matlab Software for Disciplined Convex Programming (web page and software), 2008. http://stanford.edu/˜boyd/cvx.</p>
<p>[7] A. Y. Ng and S. J. Russell. Algorithms for inverse reinforcement learning. In ICML ’00: Proceedings of the 17th International Conference on Machine Learning, pages 663–670. Morgan Kaufmann Publishers Inc., 2000.</p>
<p>[8] D. Ramachandran and E. Amir. Bayesian inverse reinforcement learning. In IJCAI’07: Proceedings of the 20th International Joint Conference on Artiﬁcal Intelligence, pages 2586–2591. Morgan Kaufmann Publishers Inc., 2007.</p>
<p>[9] N. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich. Maximum margin planning. In ICML ’06: Proceedings of the 23rd International Conference on Machine Learning, pages 729–736. ACM, 2006.</p>
<p>[10] U. Syed, M. Bowling, and R. E. Schapire. Apprenticeship learning using linear programming. In ICML ’08: Proceedings of the 25th International Conference on Machine Learning, pages 1032–1039. ACM, 2008.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
