<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>50 nips-2010-Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-50" href="../nips2010/nips-2010-Constructing_Skill_Trees_for_Reinforcement_Learning_Agents_from_Demonstration_Trajectories.html">nips2010-50</a> <a title="nips-2010-50-reference" href="#">nips2010-50-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>50 nips-2010-Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories</h1>
<br/><p>Source: <a title="nips-2010-50-pdf" href="http://papers.nips.cc/paper/3903-constructing-skill-trees-for-reinforcement-learning-agents-from-demonstration-trajectories.pdf">pdf</a></p><p>Author: George Konidaris, Scott Kuindersma, Roderic Grupen, Andre S. Barreto</p><p>Abstract: We introduce CST, an algorithm for constructing skill trees from demonstration trajectories in continuous reinforcement learning domains. CST uses a changepoint detection method to segment each trajectory into a skill chain by detecting a change of appropriate abstraction, or that a segment is too complex to model as a single skill. The skill chains from each trajectory are then merged to form a skill tree. We demonstrate that CST constructs an appropriate skill tree that can be further reﬁned through learning in a challenging continuous domain, and that it can be used to segment demonstration trajectories on a mobile manipulator into chains of skills where each skill is assigned an appropriate abstraction. 1</p><br/>
<h2>reference text</h2><p>[1] A.G. Barto and S. Mahadevan. Recent advances in hierarchical reinforcement learning. Discrete Event Dynamic Systems, 13:41–77, 2003. Special Issue on Reinforcement Learning.</p>
<p>[2] R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998.</p>
<p>[3] G.D. Konidaris and A.G. Barto. Skill discovery in continuous reinforcement learning domains using skill chaining. In Advances in Neural Information Processing Systems 22, pages 1015–1023, 2009.</p>
<p>[4] G.D. Konidaris and A.G. Barto. Efﬁcient skill learning using abstraction selection. In Proceedings of the Twenty First International Joint Conference on Artiﬁcial Intelligence, July 2009.</p>
<p>[5] B. Argall, S. Chernova, M. Veloso, and B. Browning. A survey of robot learning from demonstration. Robotics and Autonomous Systems, 57:469–483, 2009.</p>
<p>[6] M. Huber and R.A. Grupen. A feedback control structure for on-line learning tasks. Robotics and Autonomous Systems, 22(3-4):303–315, 1997.</p>
<p>[7] M. Rosenstein and A.G. Barto. Supervised actor-critic reinforcement learning. In J. Si, A.G. Barto, A. Powell, and D. Wunsch, editors, Learning and Approximate Dynamic Programming: Scaling up the Real World, pages 359–380. John Wiley & Sons, Inc., New York, 2004.</p>
<p>[8] P. Fearnhead and Z. Liu. On-line inference for multiple changepoint problems. Journal of the Royal Statistical Society B, 69:589–605, 2007.</p>
<p>[9] R.S. Sutton, D. Precup, and S.P. Singh. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artiﬁcial Intelligence, 112(1-2):181–211, 1999.</p>
<p>[10] T. Lozano-Perez, M.T. Mason, and R.H. Taylor. Automatic synthesis of ﬁne-motion strategies for robots. The International Journal of Robotics Research, 3(1):3–24, 1984.</p>
<p>[11] R.R. Burridge, A.A. Rizzi, and D.E. Koditschek. Sequential composition of dynamically dextrous robot behaviors. International Journal of Robotics Research, 18(6):534–555, 1999.</p>
<p>[12] S. Boccaletti, A. Farini, E.J. Kostelich, and F.T. Arecchi. Adaptive targeting of chaos. Physical Review E, 55(5):4845–4848, 1997.</p>
<p>[13] G.D. Konidaris and S. Osentoski. Value function approximation in reinforcement learning using the Fourier basis. Technical Report UM-CS-2008-19, Department of Computer Science, University of Massachusetts Amherst, June 2008.</p>
<p>[14] N. Mehta, S. Ray, P. Tadepalli, and T. Dietterich. Automatic discovery and transfer of MAXQ hierarchies. In Proceedings of the Twenty Fifth International Conference on Machine Learning, pages 648–655, 2008.</p>
<p>[15] J. Mugan and B. Kuipers. Autonomously learning an action hierarchy using a learned qualitative state representation. In Proceedings of the 21st International Joint Conference on Artiﬁcial Intelligence, 2009.</p>
<p>[16] G. Neumann, W. Maass, and J. Peters. Learning complex motions by sequencing simpler motion templates. In Proceedings of the 26th International Conference on Machine Learning, 2009.</p>
<p>[17] R. Tedrake. LQR-Trees: Feedback motion planning on sparse randomized trees. In Proceedings of Robotics: Science and Systems, pages 18–24, 2009.</p>
<p>[18] X. Xuan and K. Murphy. Modeling changing dependency structure in multivariate time series. In Proceedings of the Twenty-Fourth International Conference on Machine Learning, 2007.</p>
<p>[19] E.B. Fox, E.B. Sudderth, M.I. Jordan, and A.S. Willsky. Nonparametric Bayesian learning of switching linear dynamical systems. In Advances in Neural Information Processing Systems 21, 2008.</p>
<p>[20] O.C. Jenkins and M. Matari´ . Performance-derived behavior vocabularies: data-driven acquisition of c skills from motion. International Journal of Humanoid Robotics, 1(2):237–288, 2004.</p>
<p>[21] D.H. Grollman and O.C. Jenkins. Incremental learning of subtasks from unsegmented demonstration. In International Conference on Intelligent Robots and Systems, 2010.</p>
<p>[22] J. Butterﬁeld, S. Osentoski, G. Jay, and O.C. Jenkins. Learning from demonstration using a multi-valued function regressor for time-series data. In Proceedings of the Tenth IEEE-RAS International Conference on Humanoid Robots, 2010.</p>
<p>[23] J. Peters and S. Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180–1190, 2008.</p>
<p>[24] P. Abbeel and A.Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the 21st International Conference on Machine Learning, 2004.</p>
<p>[25] S. Chernova and M. Veloso. Conﬁdence-based policy learning from demonstration using Gaussian mixture models. In Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems, 2007.</p>
<p>[26] G.D. Konidaris and A.G. Barto. Building portable options: Skill transfer in reinforcement learning. In Proceedings of the Twentieth International Joint Conference on Artiﬁcial Intelligence, 2007.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
