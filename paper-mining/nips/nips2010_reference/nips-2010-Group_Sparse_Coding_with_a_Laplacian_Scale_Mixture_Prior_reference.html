<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-109" href="../nips2010/nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">nips2010-109</a> <a title="nips-2010-109-reference" href="#">nips2010-109-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</h1>
<br/><p>Source: <a title="nips-2010-109-pdf" href="http://papers.nips.cc/paper/3997-group-sparse-coding-with-a-laplacian-scale-mixture-prior.pdf">pdf</a></p><p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefﬁcients. Each coefﬁcient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efﬁcient inference procedures for both the coefﬁcients and the scale parameter. When the scale parameters of a group of coefﬁcients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude ﬂuctuations among coefﬁcients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. We show that, as a consequence of this group sparse coding, the resulting inference of the coefﬁcients follows a divisive normalization rule, and that this may be efﬁciently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model. 1</p><br/>
<h2>reference text</h2><p>[1] S. Lyu and E. P. Simoncelli. Statistical modeling of images with ﬁelds of gaussian scale mixtures. In Advances in Neural Computation Systems (NIPS), Vancouver, Canada, 2006.</p>
<p>[2] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientiﬁc Computing, 20(1):33–61, 1999.</p>
<p>[3] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B, 58(1):267–288, 1996.</p>
<p>[4] Y. Tsaig and D.L. Donoho. Extensions of compressed sensing. Signal Processing, 86(3):549–571, 2006.</p>
<p>[5] R. Raina, A. Battle, H. Lee, B. Packer, and A.Y. Ng. Self-taught learning: Transfer learning from unlabeled data. Proceedings of the Twenty-fourth International Conference on Machine Learning, 2007.  8</p>
<p>[6] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32(2):407–499, 2004.</p>
<p>[7] C.J. Rozell, D.H Johnson, R.G. Baraniuk, and B.A. Olshausen. Sparse coding via thresholding and local competition in neural circuits. Neural Computation, 20(10):2526–2563, October 2008.</p>
<p>[8] J. Friedman, T. Hastie, H. Hoeﬂing, and R. Tibshirani. Pathwise coordinate optimization. The Annals of Applied Statistics, 1(2):302–332, 2007.</p>
<p>[9] M. Figueiredo, R. Nowak, and S. Wright. Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems. IEEE Journal of Selected Topics in Signal Processing, 1(4):586–597, 2007.</p>
<p>[10] M. Elad, P. Milanfar, and R. Rubinstein. Analysis vs synthesis in signal priors. Inverse Problems, 23(3):947–968, June 2007.</p>
<p>[11] C. Zetzsche, G. Krieger, and B. Wegmann. The atoms of vision: Cartesian or polar? Journal of the Optical Society of America A, 16(7):1554–1565, 1999.</p>
<p>[12] M.J. Wainwright, E.P. Simoncelli, and A.S. Willsky. Random cascades on wavelet trees and their use in modeling and analyzing natural imagery. Applied and Computational Harmonic Analysis, 11(1), July 2001.</p>
<p>[13] A. Hyv¨ rinen, P.O. Hoyer, and M. Inki. Topographic independent component analysis. Neural Computaa tion, 13(7):1527–1558, 2001.</p>
<p>[14] Y. Karklin and M.S. Lewicki. A hierarchical bayesian model for learning nonlinear statistical regularities in nonstationary natural signals. Neural Computation, 17(2):397–423, February 2005.</p>
<p>[15] P. Hoyer and A. Hyv¨ rinen. A multi-layer sparse coding network learns contour coding from natural a images. Vision Research, 42:1593–1605, 2002.</p>
<p>[16] P.J. Garrigues and B.A. Olshausen. Learning horizontal connections in a sparse coding model of natural images. In Advances in Neural Computation Systems (NIPS), Vancouver, Canada, 2007.</p>
<p>[17] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49–67, February 2006.</p>
<p>[18] L. Jacob, G. Obozinski, and J.-P. Vert. Group lasso with overlap and graph lasso. In International Conference on Machine Learning (ICML), 2009.</p>
<p>[19] R.G. Baraniuk, V. Cevher, M.F. Duarte, and C. Hegde. Model-based compressive sensing. Preprint, August 2008.</p>
<p>[20] I. Ramirez, F. Lecumberry, and G. Sapiro. Universal priors for sparse modeling. CAMPSAP, December 2009.</p>
<p>[21] E.J. Cand` s, M.B. Wakin, and S.P. Boyd. Enhancing sparsity by reweighted l1 minimization. J. Fourier e Anal. Appl., to appear, 2008.</p>
<p>[22] D. Wipf and S. Nagarajan. A new view of automatic relevance determination. In Advances in Neural Information Processing Systems 20, 2008.</p>
<p>[23] J.A. Tropp. Just relax: convex programming methods for identifying sparse signals in noise. IEEE Transactions on Information Theory, 52(3):1030–1051, 2006.</p>
<p>[24] B.A. Olshausen and D.J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. Nature, 381(6583):607–609, June 1996.</p>
<p>[25] M.J. Wainwright, O. Schwartz, and E.P. Simoncelli. Natural image statistics and divisive normalization: Modeling nonlinearity and adaptation in cortical neurons. In R. Rao, B.A. Olshausen, and M.S. Lewicki, editors, Statistical Theories of the Brain. MIT Press, 2001.</p>
<p>[26] J. Portilla, V. Strela, M.J Wainwright, and E.P. Simoncelli. Image denoising using scale mixtures of gaussians in the wavelet domain. IEEE Transactions on Image Processing, 12(11):1338–1351, 2003.</p>
<p>[27] R.M. Figueras and E.P. Simoncelli. Statistically driven sparse image representation. In Proc 14th IEEE Int’l Conf on Image Proc, volume 6, pages 29–32, September 2007.</p>
<p>[28] E. Cand` s. Compressive sampling. Proceedings of the International Congress of Mathematicians, 2006. e</p>
<p>[29] V. Cevher, , M. F. Duarte, C. Hegde, and R. G. Baraniuk. Sparse signal recovery using markov random ﬁelds. In Advances in Neural Computation Systems (NIPS), Vancouver, B.C., Canada, 2008.</p>
<p>[30] D. Donoho and Y. Tsaig. Fast solution of l 1-norm minimization problems when the solution may be sparse. preprint, 2006.</p>
<p>[31] S. Osindero, M. Welling, and G.E. Hinton. Topographic product models applied to natural scene statistics. Neural Computation, 18(2):381–414, 2006.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
