<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-92" href="../nips2010/nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">nips2010-92</a> <a title="nips-2010-92-reference" href="#">nips2010-92-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</h1>
<br/><p>Source: <a title="nips-2010-92-pdf" href="http://papers.nips.cc/paper/3984-fast-global-convergence-rates-of-gradient-methods-for-high-dimensional-statistical-recovery.pdf">pdf</a></p><p>Author: Alekh Agarwal, Sahand Negahban, Martin J. Wainwright</p><p>Abstract: Many statistical M -estimators are based on convex optimization problems formed by the weighted sum of a loss function with a norm-based regularizer. We analyze the convergence rates of ﬁrst-order gradient methods for solving such problems within a high-dimensional framework that allows the data dimension d to grow with (and possibly exceed) the sample size n. This high-dimensional structure precludes the usual global assumptions— namely, strong convexity and smoothness conditions—that underlie classical optimization analysis. We deﬁne appropriately restricted versions of these conditions, and show that they are satisﬁed with high probability for various statistical models. Under these conditions, our theory guarantees that Nesterov’s ﬁrst-order method [12] has a globally geometric rate of convergence up to the statistical precision of the model, meaning the typical Euclidean distance between the true unknown parameter θ∗ and the optimal solution θ. This globally linear rate is substantially faster than previous analyses of global convergence for speciﬁc methods that yielded only sublinear rates. Our analysis applies to a wide range of M -estimators and statistical models, including sparse linear regression using Lasso ( 1 regularized regression), group Lasso, block sparsity, and low-rank matrix recovery using nuclear norm regularization. Overall, this result reveals an interesting connection between statistical precision and computational eﬃciency in high-dimensional estimation. 1</p><br/>
<h2>reference text</h2><p>[1] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.</p>
<p>[2] S. Becker, J. Bobin, and E. J. Candes. Nesta: a fast and accurate ﬁrst-order method for sparse recovery. Technical report, Stanford University, 2009.</p>
<p>[3] K. Bredies and D. A. Lorenz. Linear convergence of iterative soft-thresholding. Journal of Fourier Analysis and Applications, 14:813–837, 2008.</p>
<p>[4] R. Garg and R. Khandekar. Gradient descent with sparsiﬁcation: an iterative algorithm for sparse recovery with restricted isometry property. In ICML, New York, NY, USA, 2009. ACM.</p>
<p>[5] E. T. Hale, Y. Wotao, and Y. Zhang. Fixed-point continuation for 1 -minimization: Methodology and convergence. SIAM J. on Optimization, 19(3):1107–1130, 2008.</p>
<p>[6] S. Ji and J. Ye. An accelerated gradient method for trace norm minimization. In International Conference on Machine Learning, New York, NY, USA, 2009. ACM.</p>
<p>[7] Z. Lin, A. Ganesh, J. Wright, L. Wu, M. Chen, and Y. Ma. Fast convex optimization algorithms for exact recovery of a corrupted low-rank matrix. Technical Report UILUENG-09-2214, Univ. Illinois, Urbana-Champaign, July 2009.</p>
<p>[8] S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A uniﬁed framework for high-dimensional analysis of M-estimators with decomposable regularizers. In NIPS Conference, Vancouver, Canada, December 2009. Full length version arxiv:1010.2731v1.</p>
<p>[9] S. Negahban and M. J. Wainwright. Restricted strong convexity and (weighted) matrix completion: Optimal bounds with noise. Technical report, UC Berkeley, August 2010. arxiv:1009.2118.</p>
<p>[10] S. Negahban and M. J. Wainwright. Estimation of (near) low-rank matrices with noise and high-dimensional scaling. Annals of Statistics, To appear. Originally posted as arxiv:0912.5100.</p>
<p>[11] Y. Nesterov. Introductory Lectures on Convex Optimization. Kluwer Academic Publishers, New York, 2004.</p>
<p>[12] Y. Nesterov. Gradient methods for minimizing composite objective function. Technical Report 76, Center for Operations Research and Econometrics (CORE), Catholic University of Louvain (UCL), 2007.</p>
<p>[13] G. Raskutti, M. J. Wainwright, and B. Yu. Minimax rates of estimation for highdimensional linear regression over q -balls. Technical Report arXiv:0910.2042, UC Berkeley, Department of Statistics, 2009.</p>
<p>[14] G. Raskutti, M. J. Wainwright, and B. Yu. Restricted eigenvalue conditions for correlated Gaussian designs. Journal of Machine Learning Research, 11:2241–2259, August 2010.</p>
<p>[15] B. Recht. A simpler approach to matrix completion. Journal of Machine Learning Research, 2010. Posted as arXiv:0910.0651v2.</p>
<p>[16] B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM Review, Vol 52(3):471–501, 2010.</p>
<p>[17] A. Rohde and A. Tsybakov. Estimation of high-dimensional low-rank matrices. Technical Report arXiv:0912.5338v2, Universite de Paris, January 2010.</p>
<p>[18] J. A. Tropp and A. C. Gilbert. Signal recovery from random measurements via orthogonal matching pursuit. IEEE Transactions on Information Theory, 53(12):4655–4666, December 2007.</p>
<p>[19] M. Yuan, A. Ekici, Z. Lu, and R. Monteiro. Dimension reduction and coeﬃcient estimation in multivariate linear regression. Journal Of The Royal Statistical Society Series B, 69(3):329–346, 2007.  9</p>
<br/>
<br/><br/><br/></body>
</html>
