<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>256 nips-2010-Structural epitome: a way to summarize one’s visual experience</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-256" href="../nips2010/nips-2010-Structural_epitome%3A_a_way_to_summarize_one%E2%80%99s_visual_experience.html">nips2010-256</a> <a title="nips-2010-256-reference" href="#">nips2010-256-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>256 nips-2010-Structural epitome: a way to summarize one’s visual experience</h1>
<br/><p>Source: <a title="nips-2010-256-pdf" href="http://papers.nips.cc/paper/4092-structural-epitome-a-way-to-summarize-ones-visual-experience.pdf">pdf</a></p><p>Author: Nebojsa Jojic, Alessandro Perina, Vittorio Murino</p><p>Abstract: In order to study the properties of total visual input in humans, a single subject wore a camera for two weeks capturing, on average, an image every 20 seconds. The resulting new dataset contains a mix of indoor and outdoor scenes as well as numerous foreground objects. Our ﬁrst goal is to create a visual summary of the subject’s two weeks of life using unsupervised algorithms that would automatically discover recurrent scenes, familiar faces or common actions. Direct application of existing algorithms, such as panoramic stitching (e.g., Photosynth) or appearance-based clustering models (e.g., the epitome), is impractical due to either the large dataset size or the dramatic variations in the lighting conditions. As a remedy to these problems, we introduce a novel image representation, the ”structural element (stel) epitome,” and an associated efﬁcient learning algorithm. In our model, each image or image patch is characterized by a hidden mapping T which, as in previous epitome models, deﬁnes a mapping between the image coordinates and the coordinates in the large ”all-I-have-seen” epitome matrix. The limited epitome real-estate forces the mappings of different images to overlap which indicates image similarity. However, the image similarity no longer depends on direct pixel-to-pixel intensity/color/feature comparisons as in previous epitome models, but on spatial conﬁguration of scene or object parts, as the model is based on the palette-invariant stel models. As a result, stel epitomes capture structure that is invariant to non-structural changes, such as illumination changes, that tend to uniformly affect pixels belonging to a single scene or object part. 1</p><br/>
<h2>reference text</h2><p>[1] B. Frey and N. Jojic, “Transformation-invariant clustering using the EM algorithm ”, TPAMI 2003, vol. 25, no. 1, pp. 1-17.</p>
<p>[2] N. Jojic, B. Frey, A. Kannan, “Epitomic analysis of appearance and shape”, ICCV 2003.</p>
<p>[3] D. Lowe, “Distinctive Image Features from Scale-Invariant Keypoints,” IJCV, 2004, vol. 60, no. 2, pp. 91-110.</p>
<p>[4] L. Fei-Fei, P. Perona, “A Bayesian Hierarchical Model for Learning Natural Scene Categories,” IEEE CVPR 2005, pp. 524-531.</p>
<p>[5] S. Lazebnik, C. Schmid, J. Ponce, “Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories,” IEEE CVPR, 2006, pp. 2169-2178.</p>
<p>[6] N. Jojic and C. Caspi, “Capturing image structure with probabilistic index maps,” IEEE CVPR 2004, pp. 212-219.</p>
<p>[7] J. Winn and N. Jojic, “LOCUS: Learning Object Classes with Unsupervised Segmentation” ICCV 2005.</p>
<p>[8] N. Jojic, A.Perina, M.Cristani, V.Murino and B. Frey, “Stel component analysis: modeling spatial correlation in image class structure,” IEEE CVPR 2009.</p>
<p>[9] K. Ni, A. Kannan, A. Criminisi and J. Winn, “Epitomic Location Recognition,” IEEE CVPR 2008.</p>
<p>[10] A. Perina, M. Cristani, U. Castellani, V. Murino and N. Jojic, “Free energy score-space,” NIPS 2009.</p>
<p>[11] A. Torralba, K.P. Murphy, W.T. Freeman and M.A. Rubin, “Context-based vision system for place and object recognition,” ICCV 2003, pp. 273-280.</p>
<p>[12] C. Stauffer, E. Miller, and K. Tieu, “Transform invariant image decomposition with similarity templates,” NIPS 2003.</p>
<p>[13] V. Ferrari , A. Zisserman, “Learning Visual Attributes,” NIPS 2007.</p>
<p>[14] B. Russell, A. Efros, J. Sivic, B. Freeman, A. Zisserman “Segmenting Scenes by Matching Image Composites,” NIPS 2009.</p>
<p>[15] G. Bell and J. Gemmell, Total Recall. Dutton Adult 2009.  9</p>
<br/>
<br/><br/><br/></body>
</html>
