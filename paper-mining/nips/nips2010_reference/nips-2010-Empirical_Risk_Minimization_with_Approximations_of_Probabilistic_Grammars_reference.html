<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>75 nips-2010-Empirical Risk Minimization with Approximations of Probabilistic Grammars</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-75" href="../nips2010/nips-2010-Empirical_Risk_Minimization_with_Approximations_of_Probabilistic_Grammars.html">nips2010-75</a> <a title="nips-2010-75-reference" href="#">nips2010-75-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>75 nips-2010-Empirical Risk Minimization with Approximations of Probabilistic Grammars</h1>
<br/><p>Source: <a title="nips-2010-75-pdf" href="http://papers.nips.cc/paper/3975-empirical-risk-minimization-with-approximations-of-probabilistic-grammars.pdf">pdf</a></p><p>Author: Noah A. Smith, Shay B. Cohen</p><p>Abstract: Probabilistic grammars are generative statistical models that are useful for compositional and sequential structures. We present a framework, reminiscent of structural risk minimization, for empirical risk minimization of the parameters of a ﬁxed probabilistic grammar using the log-loss. We derive sample complexity bounds in this framework that apply both to the supervised setting and the unsupervised setting. 1</p><br/>
<h2>reference text</h2><p>[1] N. Abe, J. Takeuchi, and M. Warmuth. Polynomial learnability of probabilistic concepts with respect to the Kullback-Leiber divergence. In ACM Conference on Computational Learning Theory, 1990. 8</p>
<p>[2] N. Abe and M. Warmuth. On the computational complexity of approximating distributions by probabilistic automata. Machine Learning, 2:205–260, 1992.</p>
<p>[3] M. Anthony and P. L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, 1999.</p>
<p>[4] E. Charniak and M. Johnson. Coarse-to-ﬁne n-best parsing and maxent discriminative reranking. In Proc. of ACL, 2005.</p>
<p>[5] S. B. Cohen and N. A. Smith. Viterbi training for PCFGs: Hardness results and competitiveness of uniform initialization. In Proceedings of ACL, 2010.</p>
<p>[6] S. B. Cohen and N. A. Smith. Empirical risk minimization for probabilistic grammars: Sample complexity and hardness of learning, in preparation.</p>
<p>[7] M. Collins. Head-driven statistical models for natural language processing. Computational Linguistics, 29:589–637, 2003.</p>
<p>[8] M. Collins. Parameter estimation for statistical parsing models: theory and practice of distribution-free methods. Text, Speech and Language Technology (new developments in parsing technology), pages 19–55, 2004.</p>
<p>[9] S. Dasgupta. The sample complexity of learning ﬁxed-structure Bayesian networks. Machine Learning, 29(2-3):165–180, 1997.</p>
<p>[10] J. Eisner. Three new probabilistic models for dependency parsing: An exploration. In Proc. of COLING, 1996.</p>
<p>[11] E. M. Gold. Language identiﬁcation in the limit. Information and Control, 10(5):447–474, 1967.</p>
<p>[12] G. Guerra and Y. Aloimonos. Discovering a language for human activity. In AAAI Workshop on Anticipation in Cognitive Systems, 2005.</p>
<p>[13] D. Haussler. Decision-theoretic generalizations of the PAC model for neural net and other learning applications. Information and Computation, 100:78–150, 1992.</p>
<p>[14] D. Klein and C. D. Manning. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proc. of ACL, 2004.</p>
<p>[15] V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization. The Annals of Statistics, 34(6):2593–2656, 2006.</p>
<p>[16] L. Lin, T. Wu, J. Porway, and Z. Xu. A stochastic graph grammar for compositional object representation and recognition. Pattern Recognition, 8, 2009.</p>
<p>[17] S. Petrov and D. Klein. Improved inference for unlexicalized parsing. In Proc. of HLT-NAACL, 2007.</p>
<p>[18] D. Pollard. Convergence of Stochastic Processes. New York: Springer-Verlag, 1984.</p>
<p>[19] Y. Sakakibara, M. Brown, R. Hughey, S. Mian, K. Sj¨ lander, R. C. Underwood, and D. Hauso sler. Stochastic context-free grammars for tRNA modeling. Nucleic Acids Research, 22, 1994.</p>
<p>[20] A. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. The Annals of Statistics, 32(1):135–166, 2004.</p>
<p>[21] V. N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.</p>
<p>[22] D. Weiss and B. Taskar. Structured prediction cascades. In Proceedings of AISTATS, 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
