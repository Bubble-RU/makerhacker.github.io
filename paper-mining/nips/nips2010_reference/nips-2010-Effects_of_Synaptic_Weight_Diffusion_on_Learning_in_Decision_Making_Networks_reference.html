<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-68" href="../nips2010/nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">nips2010-68</a> <a title="nips-2010-68-reference" href="#">nips2010-68-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</h1>
<br/><p>Source: <a title="nips-2010-68-pdf" href="http://papers.nips.cc/paper/4029-effects-of-synaptic-weight-diffusion-on-learning-in-decision-making-networks.pdf">pdf</a></p><p>Author: Kentaro Katahira, Kazuo Okanoya, Masato Okada</p><p>Abstract: When animals repeatedly choose actions from multiple alternatives, they can allocate their choices stochastically depending on past actions and outcomes. It is commonly assumed that this ability is achieved by modiﬁcations in synaptic weights related to decision making. Choice behavior has been empirically found to follow Herrnstein’s matching law. Loewenstein & Seung (2006) demonstrated that matching behavior is a steady state of learning in neural networks if the synaptic weights change proportionally to the covariance between reward and neural activities. However, their proof did not take into account the change in entire synaptic distributions. In this study, we show that matching behavior is not necessarily a steady state of the covariance-based learning rule when the synaptic strength is sufﬁciently strong so that the ﬂuctuations in input from individual sensory neurons inﬂuence the net input to output neurons. This is caused by the increasing variance in the input potential due to the diffusion of synaptic weights. This effect causes an undermatching phenomenon, which has been observed in many behavioral experiments. We suggest that the synaptic diffusion effects provide a robust neural mechanism for stochastic choice behavior.</p><br/>
<h2>reference text</h2><p>[1] R. J. Herrnstein, H. Rachlin, and D. I. Laibson. The Matching Law. Russell Sage Foundation New York, 1997.</p>
<p>[2] L. P. Sugrue, G. S. Corrado, and W. T. Newsome. Matching behavior and the representation of value in the parietal cortex. Science, 304(5678):1782–1787, 2004.</p>
<p>[3] Y. Loewenstein and H. S. Seung. Operant matching is a generic outcome of synaptic plasticity based on the covariance between reward and neural activity. Proceedings of the National Academy of Sciences, 103(41):15224–15229, 2006.</p>
<p>[4] A. Soltani and X. J. Wang. A biophysically based neural model of matching law behavior: melioration by stochastic synapses. Journal of Neuroscience, 26(14):3731–3744, 2006.</p>
<p>[5] Y. Sakai and T. Fukai. The actor-critic learning is behind the matching law: Matching versus optimal behaviors. Neural Computation, 20(1):227–251, 2008.</p>
<p>[6] Y. Sakai and T. Fukai. When does reward maximization lead to matching law? PLoS ONE, 3(11):e3795, 2008.</p>
<p>[7] Y. Loewenstein. Robustness of learning that is based on covariance-driven synaptic plasticity. PLoS Computational Biology, 4(3):e1000007, 2008.</p>
<p>[8] W. Kinzel and P. Rujan. Improving a network generalization ability by selecting examples. Europhysics Letters, 13(5):473–477, 1990.</p>
<p>[9] D. Saad. On-line learning in neural networks. Cambridge University Press, 1998.</p>
<p>[10] G. Reents and R. Urbanczik. Self-averaging and on-line learning. Physical Review Letters, 80(24):5445–5448, 1998.</p>
<p>[11] M. Biehl, N. Caticha, and P. Riegler. Statistical mechanics of on-line learning. SimilarityBased Clustering, pages 1–22, 2009.</p>
<p>[12] K. Katahira, K. Okanoya, and M. Okada. Statistical mechanics of reward-modulated learning in decision making networks. under review.</p>
<p>[13] X. J. Wang. Probabilistic decision making by slow reverberation in cortical circuits. Neuron, 36(5):955–968, 2002.</p>
<p>[14] C. van Vreeswijk and H. Sompolinsky. Chaotic balanced state in a model of cortical circuits. Neural Computation, 10(6):1321–1371, 1998.</p>
<p>[15] A. Soltani, D. Lee, and X. J. Wang. Neural mechanism for stochastic behaviour during a competitive game. Neural Networks, 19(8):1075–1090, 2006.</p>
<p>[16] S. Fusi, W. F. Asaad, E. K. Miller, and X. J. Wang. A neural circuit model of ﬂexible sensorimotor mapping: learning and forgetting on multiple timescales. Neuron, 54(2):319–333, 2007.</p>
<p>[17] E. M. Izhikevich. Solving the distal reward problem through linkage of STDP and dopamine signaling. Cerebral Cortex, 17:2443–2452, 2007.</p>
<p>[18] R. V. Florian. Reinforcement learning through modulation of spike-timing-dependent synaptic plasticity. Neural Computation, 19(6):1468–1502, 2007.</p>
<p>[19] M. A. Farries and A. L. Fairhall. Reinforcement Learning With Modulated Spike Timing Dependent Synaptic Plasticity. Journal of Neurophysiology, 98(6):3648–3665, 2007.</p>
<p>[20] R. Legenstein, D. Pecevski, and W. Maass. A learning theory for reward-modulated spiketiming-dependent plasticity with application to biofeedback. PLoS Computational Biology, 4(10):e1000180, 2008.</p>
<p>[21] C. T. Law and J. I. Gold. Reinforcement learning can account for associative and perceptual learning on a visual-decision task. Nature Neuroscience, 12(5):655–663, 2009.</p>
<p>[22] M. Biehl. An exactly solvable model of unsupervised learning. Europhysics Letters, 25(5):391–396, 1994.</p>
<p>[23] W. M. Baum. On two types of deviation from the matching law: Bias and undermatching. Journal of the Experimental Analysis of Behavior, 22(1):231–242, 1974.</p>
<p>[24] B. Barbour, N. Brunel, V. Hakim, and J. P. Nadal. What can we learn from synaptic weight distributions? TRENDS in Neurosciences, 30(12):622–629, 2007.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
