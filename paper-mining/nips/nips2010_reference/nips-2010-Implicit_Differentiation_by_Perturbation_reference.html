<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>118 nips-2010-Implicit Differentiation by Perturbation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-118" href="../nips2010/nips-2010-Implicit_Differentiation_by_Perturbation.html">nips2010-118</a> <a title="nips-2010-118-reference" href="#">nips2010-118-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>118 nips-2010-Implicit Differentiation by Perturbation</h1>
<br/><p>Source: <a title="nips-2010-118-pdf" href="http://papers.nips.cc/paper/4107-implicit-differentiation-by-perturbation.pdf">pdf</a></p><p>Author: Justin Domke</p><p>Abstract: This paper proposes a simple and eﬃcient ﬁnite diﬀerence method for implicit diﬀerentiation of marginal inference results in discrete graphical models. Given an arbitrary loss function, deﬁned on marginals, we show that the derivatives of this loss with respect to model parameters can be obtained by running the inference procedure twice, on slightly perturbed model parameters. This method can be used with approximate inference, with a loss function over approximate marginals. Convenient choices of loss functions make it practical to ﬁt graphical models with hidden variables, high treewidth and/or model misspeciﬁcation. 1</p><br/>
<h2>reference text</h2><p>[1] Percy Liang and Michael Jordan. An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators. In ICML, 2008.</p>
<p>[2] Samuel Gross, Olga Russakovsky, Chuong Do, and Seraﬁm Batzoglou. Training conditional random ﬁelds for maximum labelwise accuracy. In NIPS. 2006.</p>
<p>[3] Sham Kakade, Yee Whye Teh, and Sam Roweis. An alternate objective function for Markovian ﬁelds. In ICML, 2002.</p>
<p>[4] Martin Wainwright. Estimating the </p>
<p>[5] Justin Domke. Learning convex inference of marginals. In UAI, 2008.</p>
<p>[6] Frederik Eaton and Zoubin Ghahramani. Choosing a variable to clamp. In AISTATS, 2009.</p>
<p>[7] Max Welling and Yee Whye Teh. Belief optimization for binary networks: A stable alternative to loopy belief propagation. In UAI, 2001.</p>
<p>[8] Tom Heskes, Kees Albers, and Bert Kappen. Approximate inference and constrained optimization. In UAI, 2003.</p>
<p>[9] Alan Yuille. CCCP algorithms to minimize the Bethe and Kikuchi free energies: Convergent alternatives to belief propagation. Neural Computation, 14:2002, 2002.</p>
<p>[10] Martin Wainwright and Michael Jordan. Graphical models, exponential families, and variational inference. Found. Trends Mach. Learn., 1(1-2):1–305, 2008.</p>
<p>[11] Jonathan Yedidia, William Freeman, and Yair Weiss. Constructing free energy approximations and generalized belief propagation algorithms. IEEE Transactions on Information Theory, 51:2282–2312, 2005.</p>
<p>[12] Martin Wainwright, Tommi Jaakkola, and Alan Willsky. A new class of upper bounds on the log partition function. IEEE Transactions on Information Theory, 51(7):2313–2335, 2005.</p>
<p>[13] Ofer Meshi, Ariel Jaimovich, Amir Globerson, and Nir Friedman. Convexifying the bethe free energy. In UAI, 2009.</p>
<p>[14] Tom Heskes. Convexity arguments for eﬃcient minimization of the bethe and kikuchi free energies. J. Artif. Intell. Res. (JAIR), 26:153–190, 2006.</p>
<p>[15] Varun Ganapathi, David Vickrey, John Duchi, and Daphne Koller. Constrained approximate maximum entropy learning of markov random ﬁelds. In UAI, 2008.</p>
<p>[16] Tamir Hazan and Amnon Shashua. Convergent message-passing algorithms for inference over general graphs with convex free energies. In UAI, pages 264–273, 2008.</p>
<p>[17] Neculai Andrei. Accelerated conjugate gradient algorithm with ﬁnite diﬀerence hessian/vector product approximation for unconstrained optimization. J. Comput. Appl. Math., 230(2):570– 582, 2009.</p>
<p>[18] Talya Meltzer, Amir Globerson, and Yair Weiss. Convergent message passing algorithms - a unifying view, 2009.</p>
<p>[19] Joris M. Mooij et al. libDAI 0.2.4: A free/open source C++ library for Discrete Approximate Inference. http://www.libdai.org/, 2010.</p>
<p>[20] Sanjiv Kumar, Jonas August, and Martial Hebert. Exploiting inference for approximate parameter learning in discriminative ﬁelds: An empirical study. In EMMCVPR, 2005.</p>
<p>[21] Sanjiv Kumar and Martial Hebert. Discriminative random ﬁelds. International Journal of Computer Vision, 68(2):179–201, 2006.</p>
<p>[22] S. V. N. Vishwanathan, Nicol Schraudolph, Mark Schmidt, and Kevin Murphy. Accelerated training of conditional random ﬁelds with stochastic gradient methods. In ICML, 2006.</p>
<p>[23] Patrick Pletscher, Cheng Soon Ong, and Joachim Buhmann. Spanning tree approximations for conditional random ﬁelds. In AISTATS, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
