<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>11 nips-2010-A POMDP Extension with Belief-dependent Rewards</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-11" href="../nips2010/nips-2010-A_POMDP_Extension_with_Belief-dependent_Rewards.html">nips2010-11</a> <a title="nips-2010-11-reference" href="#">nips2010-11-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>11 nips-2010-A POMDP Extension with Belief-dependent Rewards</h1>
<br/><p>Source: <a title="nips-2010-11-pdf" href="http://papers.nips.cc/paper/3971-a-pomdp-extension-with-belief-dependent-rewards.pdf">pdf</a></p><p>Author: Mauricio Araya, Olivier Buffet, Vincent Thomas, Françcois Charpillet</p><p>Abstract: Partially Observable Markov Decision Processes (POMDPs) model sequential decision-making problems under uncertainty and partial observability. Unfortunately, some problems cannot be modeled with state-dependent reward functions, e.g., problems whose objective explicitly implies reducing the uncertainty on the state. To that end, we introduce ρPOMDPs, an extension of POMDPs where the reward function ρ depends on the belief state. We show that, under the common assumption that ρ is convex, the value function is also convex, what makes it possible to (1) approximate ρ arbitrarily well with a piecewise linear and convex (PWLC) function, and (2) use state-of-the-art exact or approximate solving algorithms with limited changes. 1</p><br/>
<h2>reference text</h2><p>[1] R. Smallwood and E. Sondik. The optimal control of partially observable Markov decision processes over a ﬁnite horizon. Operation Research, 21:1071–1088, 1973.</p>
<p>[2] S. Thrun. Probabilistic algorithms in robotics. AI Magazine, 21(4):93–109, 2000.</p>
<p>[3] L. Mihaylova, T. Lefebvre, H. Bruyninckx, K. Gadeyne, and J. De Schutter. Active sensing for robotics - a survey. In Proc. 5th Intl. Conf. On Numerical Methods and Applications, 2002.</p>
<p>[4] S. Ji and L. Carin. Cost-sensitive feature acquisition and classiﬁcation. Pattern Recogn., 40(5):1474–1485, 2007.</p>
<p>[5] A. Hero, D. Castan, D. Cochran, and K. Kastella. Foundations and Applications of Sensor Management. Springer Publishing Company, Incorporated, 2007.</p>
<p>[6] A. Cassandra. Exact and approximate algorithms for partially observable Markov decision processes. PhD thesis, Providence, RI, USA, 1998.</p>
<p>[7] R. Bellman. The theory of dynamic programming. Bull. Amer. Math. Soc., 60:503–516, 1954.</p>
<p>[8] G. Monahan. A survey of partially observable Markov decision processes. Management Science, 28:1–16, 1982. 8</p>
<p>[9] M. Hauskrecht. Value-function approximations for partially observable Markov decision processes. Journal of Artiﬁcial Intelligence Research, 13:33–94.</p>
<p>[10] W. Lovejoy. Computationally feasible bounds for partially observed Markov decision processes. Operations Research, 39(1):162–175.</p>
<p>[11] J. Pineau, G. Gordon, and S. Thrun. Anytime point-based approximations for large POMDPs. Journal of Artiﬁcial Intelligence Research (JAIR), 27:335–380, 2006.</p>
<p>[12] M. Spaan and N. Vlassis. Perseus: Randomized point-based value iteration for POMDPs. Journal of Artiﬁcial Intelligence Research, 24:195–220, 2005.</p>
<p>[13] T. Smith and R. Simmons. Point-based POMDP algorithms: Improved analysis and implementation. In Proc. of the Int. Conf. on Uncertainty in Artiﬁcial Intelligence (UAI), 2005.</p>
<p>[14] H. Kurniawati, D. Hsu, and W. Lee. SARSOP: Efﬁcient point-based POMDP planning by approximating optimally reachable belief spaces. In Robotics: Science and Systems IV, 2008.</p>
<p>[15] R. Kaplow. Point-based POMDP solvers: Survey and comparative analysis. Master’s thesis, Montreal, Quebec, Canada, 2010.</p>
<p>[16] T. Cover and J. Thomas. Elements of Information Theory. Wiley-Interscience, 1991.</p>
<p>[17] M. Araya-L´ pez, O. Buffet, V. Thomas, and F. Charpillet. A POMDP extension with beliefo dependent rewards – extended version. Technical Report RR-7433, INRIA, Oct 2010. (See also NIPS supplementary material).</p>
<p>[18] R. Saigal. On piecewise linear approximations to smooth mappings. Mathematics of Operations Research, 4(2):153–161, 1979.</p>
<p>[19] M. Spaan. Cooperative active perception using POMDPs. In AAAI 2008 Workshop on Advancements in POMDP Solvers, July 2008.</p>
<p>[20] S. Ross, J. Pineau, S. Paquet, and B. Chaib-draa. Online planning algorithms for POMDPs. Journal of Artiﬁcial Intelligence Research (JAIR), 32:663–704, 2008.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
