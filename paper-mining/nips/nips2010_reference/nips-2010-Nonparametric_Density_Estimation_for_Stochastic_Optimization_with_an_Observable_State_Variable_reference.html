<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-185" href="../nips2010/nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">nips2010-185</a> <a title="nips-2010-185-reference" href="#">nips2010-185-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</h1>
<br/><p>Source: <a title="nips-2010-185-pdf" href="http://papers.nips.cc/paper/4098-nonparametric-density-estimation-for-stochastic-optimization-with-an-observable-state-variable.pdf">pdf</a></p><p>Author: Lauren Hannah, Warren Powell, David M. Blei</p><p>Abstract: In this paper we study convex stochastic optimization problems where a noisy objective function value is observed after a decision is made. There are many stochastic optimization problems whose behavior depends on an exogenous state variable which affects the shape of the objective function. Currently, there is no general purpose algorithm to solve this class of problems. We use nonparametric density estimation to take observations from the joint state-outcome distribution and use them to infer the optimal decision for a given query state s. We propose two solution methods that depend on the problem characteristics: function-based and gradient-based optimization. We examine two weighting schemes, kernel based weights and Dirichlet process based weights, for use with the solution methods. The weights and solution methods are tested on a synthetic multi-product newsvendor problem and the hour ahead wind commitment problem. Our results show that in some cases Dirichlet process weights offer substantial beneﬁts over kernel based weights and more generally that nonparametric estimation methods provide good solutions to otherwise intractable problems. 1</p><br/>
<h2>reference text</h2><p>[1] Antoniak, C. E. [1974], ‘Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems’, The Annals of Statistics 2(6), 1152–1174.</p>
<p>[2] Bennett, K. P. and Parrado-Hern´ ndez, E. [2006], ‘The interplay of optimization and machine a learning research’, The Journal of Machine Learning Research 7, 1265–1281.</p>
<p>[3] Blackwell, D. and MacQueen, J. B. [1973], ‘Ferguson distributions via Polya urn schemes’, The Annals Statistics 1(2), 353–355.</p>
<p>[4] Cheung, R. K. and Powell, W. B. [2000], ‘SHAPE-A stochastic hybrid approximation procedure for two-stage stochastic programs’, Operations Research 48(1), 73–79.</p>
<p>[5] Fan, J. and Gijbels, I. [1996], Local Polynomial Modelling and Its Applications, Chapman & Hall/CRC.</p>
<p>[6] Ferguson, T. S. [1973], ‘A Bayesian analysis of some nonparametric problems’, The Annals of Statistics 1(2), 209–230.</p>
<p>[7] Hayﬁeld, T. and Racine, J. S. [2008], ‘Nonparametric econometrics: The np package’, Journal of Statistical Software 27(5), 1–32.</p>
<p>[8] Ishwaran, H. and James, L. F. [2003], ‘Generalized weighted Chinese restaurant processes for species sampling mixture models’, Statistica Sinica 13(4), 1211–1236.</p>
<p>[9] Kiefer, J. and Wolfowitz, J. [1952], ‘Stochastic estimation of the maximum of a regression function’, The Annals of Mathematical Statistics 23(3), 462–466.</p>
<p>[10] Nadaraya, E. A. [1964], ‘On estimating regression’, Theory of Probability and its Applications 9(1), 141–142.</p>
<p>[11] Neal, R. M. [2000], ‘Markov chain sampling methods for Dirichlet process mixture models’, Journal of Computational and Graphical Statistics 9(2), 249–265.</p>
<p>[12] Parr, R., Painter-Wakeﬁeld, C., Li, L. and Littman, M. [2007], Analyzing feature generation for value-function approximation, in ‘Proceedings of the 24th international conference on Machine learning’, ACM, p. 744.</p>
<p>[13] Pitman, J. [1996], ‘Some developments of the Blackwell-MacQueen urn scheme’, Lecture Notes-Monograph Series 30, 245–267.</p>
<p>[14] Powell, W. B. [2007], Approximate Dynamic Programming: Solving the curses of dimensionality, Wiley-Blackwell.</p>
<p>[15] Powell, W. B., Ruszczy´ ski, A. and Topaloglu, H. [2004], ‘Learning algorithms for separan ble approximations of discrete stochastic optimization problems’, Mathematics of Operations Research 29(4), 814–836.</p>
<p>[16] Puterman, M. L. [1994], Markov decision processes: Discrete stochastic dynamic programming, John Wiley & Sons, Inc. New York, NY, USA.</p>
<p>[17] Robbins, H. and Monro, S. [1951], ‘A stochastic approximation method’, The Annals of Mathematical Statistics 22(3), 400–407.</p>
<p>[18] Schwartz, E. S. [1997], ‘The stochastic behavior of commodity prices: Implications for valuation and hedging’, The Journal of Finance 52(3), 923–973.</p>
<p>[19] Shapiro, A., Homem-de Mello, T. and Kim, J. [2002], ‘Conditioning of convex piecewise linear stochastic programs’, Mathematical Programming 94(1), 1–19.</p>
<p>[20] Spall, J. C. [2003], Introduction to stochastic search and optimization: estimation, simulation, and control, John Wiley and Sons.</p>
<p>[21] Sutton, R. S. and Barto, A. G. [1998], Introduction to reinforcement learning, MIT Press Cambridge, MA, USA.</p>
<p>[22] Tsitsiklis, J. N. and Van Roy, B. [2001], ‘Regression methods for pricing complex Americanstyle options’, IEEE Transactions on Neural Networks 12(4), 694–703.</p>
<p>[23] Watson, G. S. [1964], ‘Smooth regression analysis’, Sankhy¯ : The Indian Journal of Statistics, a Series A 26(4), 359–372.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
