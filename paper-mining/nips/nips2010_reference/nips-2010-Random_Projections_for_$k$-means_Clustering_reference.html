<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>221 nips-2010-Random Projections for $k$-means Clustering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-221" href="../nips2010/nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">nips2010-221</a> <a title="nips-2010-221-reference" href="#">nips2010-221-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>221 nips-2010-Random Projections for $k$-means Clustering</h1>
<br/><p>Source: <a title="nips-2010-221-pdf" href="http://papers.nips.cc/paper/3901-random-projections-for-k-means-clustering.pdf">pdf</a></p><p>Author: Christos Boutsidis, Anastasios Zouzias, Petros Drineas</p><p>Abstract: This paper discusses the topic of dimensionality reduction for k-means clustering. We prove that any set of n points in d dimensions (rows in a matrix A ∈ Rn×d ) can be projected into t = Ω(k/ε2 ) dimensions, for any ε ∈ (0, 1/3), in O(nd⌈ε−2 k/ log(d)⌉) time, such that with constant probability the optimal k-partition of the point set is preserved within a factor of 2 + √ The projection is done ε. √ by post-multiplying A with a d × t random matrix R having entries +1/ t or −1/ t with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.</p><br/>
<h2>reference text</h2><p>[1] D. Achlioptas. Database-friendly random projections: Johnson-Lindenstrauss with binary coins. Journal of Computer and System Science, 66(4):671–687, 2003.</p>
<p>[2] N. Ailon and B. Chazelle. Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform. In ACM Symposium on Theory of Computing (STOC), pages 557–563, 2006.</p>
<p>[3] D. Aloise, A. Deshpande, P. Hansen, and P. Popat. NP-hardness of Euclidean sum-of-squares clustering. Machine Learning, 75(2):245–248, 2009.</p>
<p>[4] E. Bingham and H. Mannila. Random projection in dimensionality reduction: applications to image and text data. In ACM SIGKDD international conference on Knowledge discovery and data mining (KDD), pages 245– 250, 2001.</p>
<p>[5] C. Boutsidis, M. W. Mahoney, and P. Drineas. Unsupervised feature selection for the k-means clustering problem. In Advances in Neural Information Processing Systems (NIPS), 2009.</p>
<p>[6] P. Drineas, A. Frieze, R. Kannan, S. Vempala, and V. Vinay. Clustering in large graphs and matrices. In ACMSIAM Symposium on Discrete Algorithms (SODA), pages 291–299, 1999.</p>
<p>[7] D. Foley and J. Sammon. An optimal set of discriminant vectors. IEEE Transactions on Computers, C-24(3):281– 289, March 1975.</p>
<p>[8] I. Guyon and A. Elisseeff. An introduction to variable and feature selection. Journal of Machine Learning Research, 3:1157–1182, 2003.</p>
<p>[9] I. Guyon, S. Gunn, A. Ben-Hur, and G. Dror. Result analysis of the NIPS 2003 feature selection challenge. In Advances in Neural Information Processing Systems (NIPS), pages 545–552. 2005.</p>
<p>[10] X. He, D. Cai, and P. Niyogi. Laplacian score for feature selection. In Advances in Neural Information Processing Systems (NIPS) 18, pages 507–514. 2006.</p>
<p>[11] P. Indyk and R. Motwani Approximate nearest neighbors: towards removing the curse of dimensionality. In ACM Symposium on Theory of Computing (STOC), pages 604–613, 1998.</p>
<p>[12] W. Johnson and J. Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space. Contemporary mathematics, 26(189-206):1–1, 1984.</p>
<p>[13] E. Kokiopoulou, J. Chen and Y. Saad. Trace optimization and eigenproblems in dimension reduction methods. Numerical Linear Algebra with Applications, to appear.</p>
<p>[14] A. Kumar, Y. Sabharwal, and S. Sen. A simple linear time (1+ε)-approximation algorithm for k-means clustering in any dimensions. In IEEE Symposium on Foundations of Computer Science (FOCS), pages 454–462, 2004.</p>
<p>[15] E. Liberty and S. Zucker. The Mailman algorithm: A note on matrix-vector multiplication. Information Processing Letters, 109(3):179–182, 2009.</p>
<p>[16] S. Lloyd. Least squares quantization in PCM. IEEE Transactions on Information Theory, 28(2):129–137, 1982.</p>
<p>[17] R. Ostrovsky, Y. Rabani, L. J. Schulman, and C. Swamy. The effectiveness of Lloyd-type methods for the k-means problem. In IEEE Symposium on Foundations of Computer Science (FOCS), pages 165–176, 2006.</p>
<p>[18] S. Roweis, and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290:5500, pages 2323-2326, 2000.</p>
<p>[19] T. Sarlos. Improved approximation algorithms for large matrices via random projections. In IEEE Symposium on Foundations of Computer Science (FOCS), pages 329–337, 2006.</p>
<p>[20] X. Wu et al. Top 10 algorithms in data mining. Knowledge and Information Systems, 14(1):1–37, 2008.</p>
<p>[21] http://www.cs.uiuc.edu/˜dengcai2/Data/FaceData.html</p>
<p>[22] http://www.cs.uiuc.edu/˜dengcai2/Data/data.html</p>
<p>[23] http://www.cs.nyu.edu/˜roweis/lle/  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
