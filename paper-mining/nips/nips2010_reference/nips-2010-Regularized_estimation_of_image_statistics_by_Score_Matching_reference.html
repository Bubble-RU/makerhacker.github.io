<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>224 nips-2010-Regularized estimation of image statistics by Score Matching</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-224" href="../nips2010/nips-2010-Regularized_estimation_of_image_statistics_by_Score_Matching.html">nips2010-224</a> <a title="nips-2010-224-reference" href="#">nips2010-224-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>224 nips-2010-Regularized estimation of image statistics by Score Matching</h1>
<br/><p>Source: <a title="nips-2010-224-pdf" href="http://papers.nips.cc/paper/4060-regularized-estimation-of-image-statistics-by-score-matching.pdf">pdf</a></p><p>Author: Diederik P. Kingma, Yann L. Cun</p><p>Abstract: Score Matching is a recently-proposed criterion for training high-dimensional density models for which maximum likelihood training is intractable. It has been applied to learning natural image statistics but has so-far been limited to simple models due to the difﬁculty of differentiating the loss with respect to the model parameters. We show how this differentiation can be automated with an extended version of the double-backpropagation algorithm. In addition, we introduce a regularization term for the Score Matching loss that enables its use for a broader range of problem by suppressing instabilities that occur with ﬁnite training sample sizes and quantized input values. Results are reported for image denoising and super-resolution.</p><br/>
<h2>reference text</h2><p>[1] S. Becker and Y. LeCun. Improving the convergence of back-propagation learning with second-order methods. In D. Touretzky, G. Hinton, and T. Sejnowski, editors, Proc. of the 1988 Connectionist Models Summer School, pages 29–37, San Mateo, 1989. Morgan Kaufman.</p>
<p>[2] C. M. Bishop. Neural networks for pattern recognition. Oxford University Press, Oxford, UK, 1996.</p>
<p>[3] A. E. Bryson and Y. C. Ho. Applied optimal control; optimization, estimation, and control. Blaisdell Pub. Co. Waltham, Massachusetts, 1969.</p>
<p>[4] M. A. Carreira-Perpinan and G. E. Hinton. On contrastive divergence learning. In Artiﬁcial Intelligence and Statistics, 2005.</p>
<p>[5] H. Drucker and Y. LeCun. Improving generalization performance using double backpropagation. IEEE Transactions on Neural Networks, 3(6):991–997, 1992.</p>
<p>[6] M. Gutmann and A. Hyv¨ rinen. Noise-contrastive estimation: A new estimation principle for unnora malized statistical models. In Proc. Int. Conf. on Artiﬁcial Intelligence and Statistics (AISTATS2010), 2010.</p>
<p>[7] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14:2002, 2000.</p>
<p>[8] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18(7):1527–1554, 2006.</p>
<p>[9] G. E. Hinton, S. Osindero, M. Welling, and Y. W. Teh. Unsupervised discovery of non-linear structure using contrastive backpropagation. Cognitive Science, 30(4):725–731, 2006.</p>
<p>[10] A. Hyv¨ rinen. Estimation of non-normalized statistical models by score matching. Journal of Machine a Learning Research, 6:695–709, 2005.</p>
<p>[11] A. Hyv¨ rinen. Some extensions of score matching. Computational Statistics & Data Analysis, a 51(5):2499–2512, 2007.</p>
<p>[12] A. Hyv¨ rinen. Optimal approximation of signal priors. Neural Computation, 20:3087–3110, 2008. a</p>
<p>[13] U. K¨ ster and A. Hyv¨ rinen. A two-layer ica-like model estimated by score matching. In J. M. de S´ , o a a L. A. Alexandre, W. Duch, and D. P. Mandic, editors, ICANN (2), volume 4669 of Lecture Notes in Computer Science, pages 798–807. Springer, 2007.</p>
<p>[14] U. Koster, J. T. Lindgren, and A. Hyv¨ rinen. Estimating markov random ﬁeld potentials for natural a images. Proc. Int. Conf. on Independent Component Analysis and Blind Source Separation (ICA2009), 2009.</p>
<p>[15] U. K¨ ster, J. T. Lindgren, and A. Hyv¨ rinen. Estimating markov random ﬁeld potentials for natural o a images. In T. Adali, C. Jutten, J. M. T. Romano, and A. K. Barros, editors, ICA, volume 5441 of Lecture Notes in Computer Science, pages 515–522. Springer, 2009.</p>
<p>[16] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, pages 2278–2324, 1998.</p>
<p>[17] Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. Huang. A tutorial on energy-based learning. In G. Bakir, T. Hofman, B. Sch¨ lkopf, A. Smola, and B. Taskar, editors, Predicting Structured Data. MIT o Press, 2006.</p>
<p>[18] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proc. 8th Int’l Conf. Computer Vision, volume 2, pages 416–423, July 2001.</p>
<p>[19] S. Osindero and G. E. Hinton. Modeling image patches with a directed hierarchy of markov random ﬁelds. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1121–1128. MIT Press, Cambridge, MA, 2008.</p>
<p>[20] M. Ranzato, Y. Boureau, and Y. LeCun. Sparse feature learning for deep belief networks. In Advances in Neural Information Processing Systems (NIPS 2007), 2007.</p>
<p>[21] S. Roth and M. J. Black. Fields of experts. International Journal of Computer Vision, 82(2):205–229, 2009.</p>
<p>[22] A. N. Tikhonov. On the stability of inverse problems. Dokl. Akad. Nauk SSSR, (39):176–179, 1943.</p>
<p>[23] Y. Weiss and W. T. Freeman. What makes a good model of natural images. In CVPR 2007: Proceedings of the 2007 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, IEEE Computer Society, pages 1–8, 2007.</p>
<p>[24] M. Welling, G. E. Hinton, and S. Osindero. Learning sparse topographic representations with products of student-t distributions. In S. T. S. Becker and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 1359–1366. MIT Press, Cambridge, MA, 2003.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
