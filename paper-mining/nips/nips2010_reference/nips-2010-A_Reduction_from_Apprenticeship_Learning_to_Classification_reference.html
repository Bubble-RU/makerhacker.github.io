<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>14 nips-2010-A Reduction from Apprenticeship Learning to Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-14" href="../nips2010/nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">nips2010-14</a> <a title="nips-2010-14-reference" href="#">nips2010-14-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>14 nips-2010-A Reduction from Apprenticeship Learning to Classification</h1>
<br/><p>Source: <a title="nips-2010-14-pdf" href="http://papers.nips.cc/paper/4180-a-reduction-from-apprenticeship-learning-to-classification.pdf">pdf</a></p><p>Author: Umar Syed, Robert E. Schapire</p><p>Abstract: We provide new theoretical results for apprenticeship learning, a variant of reinforcement learning in which the true reward function is unknown, and the goal is to perform well relative to an observed expert. We study a common approach to learning from expert demonstrations: using a classiﬁcation algorithm to learn to imitate the expert’s behavior. Although this straightforward learning strategy is widely-used in practice, it has been subject to very little formal analysis. We prove that, if the learned classiﬁer has error rate ǫ, the difference between the √ value of the apprentice’s policy and the expert’s policy is O( ǫ). Further, we prove that this difference is only O(ǫ) when the expert’s policy is close to optimal. This latter result has an important practical consequence: Not only does imitating a near-optimal expert result in a better policy, but far fewer demonstrations are required to successfully imitate such an expert. This suggests an opportunity for substantial savings whenever the expert is known to be good, but demonstrations are expensive or difﬁcult to obtain. 1</p><br/>
<h2>reference text</h2><p>[1] Pieter Abbeel and Andrew Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the 21st International Conference on Machine Learning, 2004.</p>
<p>[2] P Abbeel and A Y Ng. Exploration and apprenticeship learning in reinforcement learning. In Proceedings of the 22nd International Conference on Machine Learning, 2005.</p>
<p>[3] Nathan D. Ratliff, J. Andrew Bagnell, and Martin A. Zinkevich. Maximum margin planning. In Proceedings of the 23rd International Conference on Machine Learning, 2006.</p>
<p>[4] Umar Syed and Robert E. Schapire. A game-theoretic approach to apprenticeship learning. In Advances in Neural Information Processing Systems 20, 2008.</p>
<p>[5] J. Zico Kolter, Pieter Abbeel, and Andrew Ng. Hierarchical apprenticeship learning with application to quadruped locomotion. In Advances in Neural Information Processing Systems 20, 2008.</p>
<p>[6] Umar Syed and Robert E. Schapire. Apprenticeship learning using linear programming. In Proceedings of the 25th International Conference on Machine Learning, 2008.</p>
<p>[7] Brenna D. Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from demonstration. Robotics and Autonomous Systems, 57(5):469–483, 2009.</p>
<p>[8] St´ phane Ross and J. Andrew Bagnell. Efﬁcient reduction for imitation learning. In AISTATS, e 2010.</p>
<p>[9] Bianca Zadrozny, John Langford, and Naoki Abe. Cost-sensitive learning by costproportionate example weighting. In Proceedings of the Third IEEE International Conference on Data Mining, 2003.</p>
<p>[10] J. Andrew Bagnell, Sham Kakade, Andrew Y. Ng, and Jeff Schneider. Policy search by dynamic programming. In Advances in Neural Information Processing Systems 15, 2003.</p>
<p>[11] John Langford and Bianca Zadrozny. Relating reinforcement learning performance to classiﬁcation performance. In Proceedings of the 22nd International Conference on Machine Learning, 2005.</p>
<p>[12] Doron Blatt and Alfred Hero. From weighted classiﬁcation to policy search. In Advances in Neural Information Processing Systems 18, pages 139–146, 2006.</p>
<p>[13] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Proceedings 19th International Conference on Machine Learning, 2002.</p>
<p>[14] V. N. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and Its Applications, 16:264–280, 1971.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
