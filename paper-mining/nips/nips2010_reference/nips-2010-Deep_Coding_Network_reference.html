<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>59 nips-2010-Deep Coding Network</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-59" href="../nips2010/nips-2010-Deep_Coding_Network.html">nips2010-59</a> <a title="nips-2010-59-reference" href="#">nips2010-59-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>59 nips-2010-Deep Coding Network</h1>
<br/><p>Source: <a title="nips-2010-59-pdf" href="http://papers.nips.cc/paper/3929-deep-coding-network.pdf">pdf</a></p><p>Author: Yuanqing Lin, Zhang Tong, Shenghuo Zhu, Kai Yu</p><p>Abstract: This paper proposes a principled extension of the traditional single-layer ﬂat sparse coding scheme, where a two-layer coding scheme is derived based on theoretical analysis of nonlinear functional approximation that extends recent results for local coordinate coding. The two-layer approach can be easily generalized to deeper structures in a hierarchical multiple-layer manner. Empirically, it is shown that the deep coding approach yields improved performance in benchmark datasets.</p><br/>
<h2>reference text</h2><p>[1] http://yann.lecun.com/exdb/mnist/.</p>
<p>[2] Samy Bengio, Fernando Pereira, Yoram Singer, and Dennis Strelow. Group sparse coding. In NIPS’ 09, 2009.</p>
<p>[3] D P. Bertsekas. Projected newton methods for optimization problems with simple constraints. SIAM J. Control Optim., 20(2):221–246, 1982.</p>
<p>[4] Dimitri P. Bertsekas. Nonlinear programming. Athena Scientiﬁc, 2003.</p>
<p>[5] David Bradley and J. Andrew (Drew) Bagnell. Differentiable sparse coding. In Proceedings of Neural Information Processing Systems 22, December 2008.</p>
<p>[6] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. http://www.pascalnetwork.org/challenges/VOC/voc2007/workshop/index.html.</p>
<p>[7] Mark Everingham. Overview and results of the classiﬁcation challenge. The PASCAL Visual Object Classes Challenge Workshop at ICCV, 2009.</p>
<p>[8] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504 – 507, July 2006. 8</p>
<p>[9] Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Y. Ng. Efﬁcient sparse coding algorithms. In Proceedings of the Neural Information Processing Systems (NIPS) 19, 2007.</p>
<p>[10] Michael S. Lewicki and Terrence J. Sejnowski. Learning overcomplete representations. Neural Computation, 12:337–365, 2000.</p>
<p>[11] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Supervised dictionary learning. In NIPS’ 08, 2008.</p>
<p>[12] B.A. Olshausen and D.J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for nature images. Nature, 381:607–609, 1996.</p>
<p>[13] Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer, and Andrew Y. Ng. Self-taught learning: Transfer learning from unlabeled data. International Conference on Machine Learning, 2007.</p>
<p>[14] Marc Aurelio Ranzato, Y-Lan Boureau, and Yann LeCun. Sparse feature learning for deep belief networks. In NIPS’ 07, 2007.</p>
<p>[15] Jianchao Yang, Kai Yu, Yihong Gong, and Thomas Huang. Linear spatial pyramid matching using sparse coding for image classiﬁcation. In IEEE Conference on Computer Vision and Pattern Recognition, 2009.</p>
<p>[16] Kai Yu and Tong Zhang. Improved local coordinate coding using local tangents. In ICML’ 09, 2010.</p>
<p>[17] Kai Yu, Tong Zhang, and Yihong Gong. Nonlinear learning using local coordinate coding. In NIPS’ 09, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
