<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>71 nips-2010-Efficient Relational Learning with Hidden Variable Detection</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-71" href="../nips2010/nips-2010-Efficient_Relational_Learning_with_Hidden_Variable_Detection.html">nips2010-71</a> <a title="nips-2010-71-reference" href="#">nips2010-71-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>71 nips-2010-Efficient Relational Learning with Hidden Variable Detection</h1>
<br/><p>Source: <a title="nips-2010-71-pdf" href="http://papers.nips.cc/paper/4175-efficient-relational-learning-with-hidden-variable-detection.pdf">pdf</a></p><p>Author: Ni Lao, Jun Zhu, Liu Xinwang, Yandong Liu, William W. Cohen</p><p>Abstract: Markov networks (MNs) can incorporate arbitrarily complex features in modeling relational data. However, this ﬂexibility comes at a sharp price of training an exponentially complex model. To address this challenge, we propose a novel relational learning approach, which consists of a restricted class of relational MNs (RMNs) called relation tree-based RMN (treeRMN), and an efﬁcient Hidden Variable Detection algorithm called Contrastive Variable Induction (CVI). On one hand, the restricted treeRMN only considers simple (e.g., unary and pairwise) features in relational data and thus achieves computational efﬁciency; and on the other hand, the CVI algorithm efﬁciently detects hidden variables which can capture long range dependencies. Therefore, the resultant approach is highly efﬁcient yet does not sacriﬁce its expressive power. Empirical results on four real datasets show that the proposed relational learning method can achieve similar prediction quality as the state-of-the-art approaches, but is signiﬁcantly more efﬁcient in training; and the induced hidden variables are semantically meaningful and crucial to improve the training speed and prediction qualities of treeRMNs.</p><br/>
<h2>reference text</h2><p>[1] Galen Andrew and Jianfeng Gao. Scalable training of ℓ1 -regularized log-linear models. In ICML, 2007.</p>
<p>[2] Razvan C. Bunescu and Raymond J. Mooney. Collective information extraction with relational Markov networks. In ACL, 2004.</p>
<p>[3] Miguel A. Carreira-Perpinan and Geoffrey E. Hinton. On contrastive divergence learning. In AISTATS, 2005.</p>
<p>[4] Gal Elidan and Nir Friedman. The information bottleneck em algorithm. In UAI, 2003.</p>
<p>[5] Gal Elidan, Noam Lotner, Nir Friedman, and Daphne Koller. Discovering hidden variables: A structure-based approach. In NIPS, 2000.</p>
<p>[6] Nir Friedman, Lise Getoor, Daphne Koller, and Avi Pfeffer. Learning probabilistic relational models. In IJCAI, 1999.</p>
<p>[7] Yi Huang, Volker Tresp, and Stefan Hagen Weber. Predictive modeling using features derived from paths in relational graphs. In Technical report, 2007.</p>
<p>[8] Ariel Jaimovich, Ofer Meshi, and Nir Friedman. Template-based inference in symmetric relational Markov random ﬁelds. In UAI, 2007.</p>
<p>[9] Charles Kemp, Joshua B. Tenenbaum, Thomas L. Grifﬁths, Takeshi Yamada, and Naonori Ueda. Learning systems of concepts with an inﬁnite relational model. In AAAI, 2006.</p>
<p>[10] Stanley Kok and Pedro Domingos. Learning the structure of Markov logic networks. In ICML, 2005.</p>
<p>[11] Stanley Kok and Pedro Domingos. Statistical predicate invention. In ICML, 2007.</p>
<p>[12] Stanley Kok and Pedro Domingos. Learning Markov logic networks using structural motifs. In ICML, 2010.</p>
<p>[13] Su-In Lee, Varun Ganapathi, and Daphne Koller. Efﬁcient structure learning of Markov networks using ℓ1 -regularization. In NIPS, 2006.</p>
<p>[14] Kurt T. Miller, Thomas L. Grifﬁths, and Michael I. Jordan. Nonparametric latent feature models for link prediction. In NIPS, 2009.</p>
<p>[15] Kevin P. Murphy, Yair Weiss, and Michael I. Jordan. Loopy belief propagation for approximate inference: An empirical study. In UAI, 1999.</p>
<p>[16] Iftach Nachman, Gal Elidan, and Nir Friedman. “Ideal parent” structure learning for continuous variable networks. In UAI, 2004.</p>
<p>[17] Simon Perkins, Kevin Lacker, and James Theiler. Grafting: Fast, incremental feature selection by gradient descent in function spaces. In JMLR, 2003.</p>
<p>[18] Hoifung Poon and Pedro Domingos. Joint inference in information extraction. In AAAI, 2007.</p>
<p>[19] Karen Sachs, Omar Perez, Dana Peer, Douglas A. Lauffenburger, and Garry P. Nolan. Causal protein-signaling networks derived from multiparameter single-cell data. In Science, 2005.</p>
<p>[20] Ilya Sutskever, Ruslan Salakhutdinov, and Josh Tenenbaum. Modelling relational data using Bayesian clustered tensor factorization. In NIPS, 2009.</p>
<p>[21] Benjamin Taskar, Pieter Abbeel, and Daphne Koller. Discriminative probabilistic models for relational data. In UAI, 2002.</p>
<p>[22] Benjamin Taskar, Eran Segal, and Daphne Koller. Probabilistic classiﬁcation and clustering in relational data. In IJCAI, 2001.</p>
<p>[23] Max Welling and Geoffrey E. Hinton. A new learning algorithm for mean ﬁeld Boltzmann machines. In ICANN, 2001.</p>
<p>[24] Zhao Xu, Volker Tresp, Kai Yu, and Hans-Peter Kriegel. Inﬁnite hidden relational models. In UAI, 2006.</p>
<p>[25] Alan Yuille. The convergence of contrastive divergence. In NIPS, 2004.</p>
<p>[26] Jun Zhu, Ni Lao, and Eric P. Xing. Grafting-light: Fast, incremental feature selection and structure learning of Markov random ﬁelds. In KDD, 2010.</p>
<p>[27] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. In Journal Of The Royal Statistical Society Series B, 2005. 9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
