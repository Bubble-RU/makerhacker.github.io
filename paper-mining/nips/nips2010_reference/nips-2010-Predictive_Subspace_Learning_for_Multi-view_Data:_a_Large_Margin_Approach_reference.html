<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>213 nips-2010-Predictive Subspace Learning for Multi-view Data: a Large Margin Approach</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-213" href="../nips2010/nips-2010-Predictive_Subspace_Learning_for_Multi-view_Data%3A_a_Large_Margin_Approach.html">nips2010-213</a> <a title="nips-2010-213-reference" href="#">nips2010-213-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>213 nips-2010-Predictive Subspace Learning for Multi-view Data: a Large Margin Approach</h1>
<br/><p>Source: <a title="nips-2010-213-pdf" href="http://papers.nips.cc/paper/4128-predictive-subspace-learning-for-multi-view-data-a-large-margin-approach.pdf">pdf</a></p><p>Author: Ning Chen, Jun Zhu, Eric P. Xing</p><p>Abstract: Learning from multi-view data is important in many applications, such as image classiﬁcation and annotation. In this paper, we present a large-margin learning framework to discover a predictive latent subspace representation shared by multiple views. Our approach is based on an undirected latent space Markov network that fulﬁlls a weak conditional independence assumption that multi-view observations and response variables are independent given a set of latent variables. We provide efﬁcient inference and parameter estimation methods for the latent subspace model. Finally, we demonstrate the advantages of large-margin learning on real video and web image data for discovering predictive latent representations and improving the performance on image classiﬁcation, annotation and retrieval.</p><br/>
<h2>reference text</h2><p>[1] S. Akaho. A kernel method for canonical correlation analysis. In IMPS, 2001.</p>
<p>[2] K. Ando and T. Zhang. Two-view feature generation model for semi-supervised learning. In ICML, 2007.</p>
<p>[3] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical report, Technical Report 688, Dept. of Statistics. University of California, 2005.</p>
<p>[4] D. M. Blei and M. I. Jordan. Modeling annotated data. In ACM SIGIR, pages 127–134, 2003.</p>
<p>[5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. JMLR, 3:993–1022, 2003.</p>
<p>[6] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-trainnig. In COLT, 1998.</p>
<p>[7] U. Brefeld and T. Scheffer. Co-EM support vector learning. In ICML, 2004.</p>
<p>[8] K. Chaudhuri, S. M. Kakade, K. Livescu, and K. Sridharan. Multi-view clustering via canonical correlation analysis. In ICML, 2009.</p>
<p>[9] C. M. Christoudias, R. Urtasun, and T. Darrell. Multi-view learning in the presence of view disagreement. In UAI, 2008.</p>
<p>[10] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y.-T. Zheng. NUS-WIDE: A real-world web image database from national university of singapore. In CIVR, 2009.</p>
<p>[11] K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. JMLR, (2):265–292, 2001.</p>
<p>[12] M. Culp, G. Michailidis, and K. Johnson. On multi-view learning with additive models. Annals of Applied Statistics, 3(1):292–318, 2009.</p>
<p>[13] T. Diethe, D. R. Hardoon, and J. Shawe-Taylor. Multiview ﬁsher discriminant analysis. In NIPS Workshop on Learning from Multiple Sources, 2008.</p>
<p>[14] D. Foster, S. Kakade, and T. Zhang. Multi-view dimensionality reduction via canonical correlation analysis. Technical report, Technical Report TR-2008-4, TTI-Chicago, 2008.</p>
<p>[15] D. G¨kalp and S. Aksoy. Scene classiﬁcation using bag-of-regions representations. In CVPR, 2007. o</p>
<p>[16] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771–1800, 2002.</p>
<p>[17] H. Hotelling. Relations between two sets of variates. Biometrika, 28(3/4):321–377, 1936.</p>
<p>[18] J. Lafferty, A. McCallum, and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In ICML, 2001.</p>
<p>[19] D. G. Lowe. Object recognition from local scale-invariant features. In CVPR, 1999.</p>
<p>[20] R. Salakhutdinov and G. E. Hinton. Replicated softmax: an undirected topic model. In NIPS, 2009.</p>
<p>[21] L. van der Maaten and G. Hinton. Visualizing data using t-SNE. JMLR, 9:2579–2605, 2008.</p>
<p>[22] M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1–2):1–305, 2008.</p>
<p>[23] H. M. Wallach. Topic modeling: Beyond bag-of-words. In ICML, 2006.</p>
<p>[24] C. Wang, D. M. Blei, and L. Fei-Fei. Simultaneous image classiﬁcation and annotation. In CVPR, 2009.</p>
<p>[25] M. Welling and G. E. Hinton. A new learning algorithm for mean ﬁeld boltzmann machines. In ICANN, 2001.</p>
<p>[26] M. Welling, M. Rosen-Zvi, and G. E. Hinton. Exponential family harmoniums with an application to information retrieval. In NIPS, pages 1481–1488, 2004.</p>
<p>[27] E. P. Xing, M. I. Jordan, and S. Russell. A generalized mean ﬁeld algorithm for variational inference in exponential families. In UAI, 2003.</p>
<p>[28] E. P. Xing, R. Yan, and A. G. Hauptmann. Mining associated text and images with dual-wing harmoniums. In UAI, 2005.</p>
<p>[29] J. Yang, Y. Liu, E. P. Xing, and A. G. Hauptmann. Harmonium models for semantic video representation and classiﬁcation. In SDM, 2007.</p>
<p>[30] J. Zhang, Z. Ghahramani, and Y. Yang. Flexible latent variable models for multi-task learning. Machine Learning, 73(3):221–242, 2008.</p>
<p>[31] J. Zhu, A. Ahmed, and E. P. Xing. MedLDA: Maximum margin supervised topic models for regression and classiﬁcation. In ICML, 2009.  9</p>
<br/>
<br/><br/><br/></body>
</html>
