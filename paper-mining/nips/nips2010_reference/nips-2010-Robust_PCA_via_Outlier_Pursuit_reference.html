<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>231 nips-2010-Robust PCA via Outlier Pursuit</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-231" href="../nips2010/nips-2010-Robust_PCA_via_Outlier_Pursuit.html">nips2010-231</a> <a title="nips-2010-231-reference" href="#">nips2010-231-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>231 nips-2010-Robust PCA via Outlier Pursuit</h1>
<br/><p>Source: <a title="nips-2010-231-pdf" href="http://papers.nips.cc/paper/4005-robust-pca-via-outlier-pursuit.pdf">pdf</a></p><p>Author: Huan Xu, Constantine Caramanis, Sujay Sanghavi</p><p>Abstract: Singular Value Decomposition (and Principal Component Analysis) is one of the most widely used techniques for dimensionality reduction: successful and efﬁciently computable, it is nevertheless plagued by a well-known, well-documented sensitivity to outliers. Recent work has considered the setting where each point has a few arbitrarily corrupted components. Yet, in applications of SVD or PCA such as robust collaborative ﬁltering or bioinformatics, malicious agents, defective genes, or simply corrupted or contaminated experiments may effectively yield entire points that are completely corrupted. We present an efﬁcient convex optimization-based algorithm we call Outlier Pursuit, that under some mild assumptions on the uncorrupted points (satisﬁed, e.g., by the standard generative assumption in PCA problems) recovers the exact optimal low-dimensional subspace, and identiﬁes the corrupted points. Such identiﬁcation of corrupted points that do not conform to the low-dimensional approximation, is of paramount interest in bioinformatics and ﬁnancial applications, and beyond. Our techniques involve matrix decomposition using nuclear norm minimization, however, our results, setup, and approach, necessarily differ considerably from the existing line of work in matrix completion and matrix decomposition, since we develop an approach to recover the correct column space of the uncorrupted matrix, rather than the exact matrix itself.</p><br/>
<h2>reference text</h2><p>[1] I. T. Jolliffe. Principal Component Analysis. Springer Series in Statistics, Berlin: Springer, 1986.</p>
<p>[2] P. J. Huber. Robust Statistics. John Wiley & Sons, New York, 1981.</p>
<p>[3] L. Xu and A. L. Yuille. Robust principal component analysis by self-organizing rules based on statistical physics approach. IEEE Tran. on Neural Networks, 6(1):131–143, 1995.</p>
<p>[4] E. Cand` s, X. Li, Y. Ma, and J. Wright. Robust pricinpal component analysis? ArXiv:0912.3599, 2009. e</p>
<p>[5] S. J. Devlin, R. Gnanadesikan, and J. R. Kettenring. Robust estimation of dispersion matrices and principal components. Journal of the American Statistical Association, 76(374):354–362, 1981.</p>
<p>[6] T. N. Yang and S. D. Wang. Robust algorithms for principal component analysis. Pattern Recognition Letters, 20(9):927–933, 1999.</p>
<p>[7] C. Croux and G. Hasebroeck. Principal component analysis based on robust estimators of the covariance or correlation matrix: Inﬂuence functions and efﬁciencies. Biometrika, 87(3):603–618, 2000.</p>
<p>[8] F. De la Torre and M. J. Black. Robust principal component analysis for computer vision. In ICCV’01, pages 362–369, 2001.</p>
<p>[9] F. De la Torre and M. J. Black. A framework for robust subspace learning. International Journal of Computer Vision, 54(1/2/3):117–142, 2003.</p>
<p>[10] C. Croux, P. Filzmoser, and M. Oliveira. Algorithms for Projection−Pursuit robust principal component analysis. Chemometrics and Intelligent Laboratory Systems, 87(2):218–225, 2007.</p>
<p>[11] S. C. Brubaker. Robust PCA and clustering on noisy mixtures. In SODA’09, pages 1078–1087, 2009.</p>
<p>[12] H. Xu, C. Caramanis, and S. Mannor. Principal component analysis with contaminated data: The high dimensional case. In COLT’10, pages 490–502, 2010.</p>
<p>[13] E. J. Cand` s, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from e highly incomplete frequency information. IEEE Tran. on Information Theory, 52(2):489–509, 2006.</p>
<p>[14] B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum rank solutions to linear matrix equations via nuclear norm minimization. To appear in SIAM Review, 2010.</p>
<p>[15] V. Chandrasekaran, S. Sanghavi, P. Parrilo, and A. Willsky. Rank-sparsity incoherence for matrix decomposition. ArXiv:0906.2220, 2009.</p>
<p>[16] E. Cand` s and B. Recht. Exact matrix completion via convex optimization. Foundations of Computational e Mathematics, 9:717–772, 2009.</p>
<p>[17] E. Cand` s and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Tran. on e Information Theory, 56(2053-2080), 2010.</p>
<p>[18] D. L. Donoho. Breakdown properties of multivariate location estimators. Qualifying paper, Harvard University, 1982.</p>
<p>[19] R. Maronna. Robust M-estimators of multivariate location and scatter. The Annals of Statistics, 4:51–67, 1976.</p>
<p>[20] V. Barnett. The ordering of multivariate data. Journal of Royal Statistics Society, A, 138:318–344, 1976.</p>
<p>[21] D. Titterington. Estimation of correlation coefﬁcients by ellipsoidal trimming. Applied Statistics, 27:227– 234, 1978.</p>
<p>[22] V. Barnett and T. Lewis. Outliers in Statistical Data. Wiley, New York, 1978.</p>
<p>[23] A. Dempster and M. Gasko-Green. New tools for residual analysis. The Annals of Statistics, 9(5):945– 959, 1981.</p>
<p>[24] S. J. Devlin, R. Gnanadesikan, and J. R. Kettenring. Robust estimation and outlier detection with correlation coefﬁcients. Biometrika, 62:531–545, 1975.</p>
<p>[25] G. Li and Z. Chen. Projection-pursuit approach to robust dispersion matrices and principal components: Primary theory and monte carlo. Journal of the American Statistical Association, 80(391):759–766, 1985.</p>
<p>[26] H. Xu, C. Caramanis, and S. Sanghavi. Robust PCA via outlier pursuit. http://arxiv.org/abs/1010.4237, 2010.</p>
<p>[27] Y. Nesterov. A method of solving a convex programming problem with convergence rate o(1/k2 ). Soviet Mathematics Doklady, 27(372-376), 1983.</p>
<p>[28] J-F. Cai, E. Cand` s, and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM e Journal on Optimization, 20:1956–1982, 2008.</p>
<p>[29] C. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. the MIT Press, 2006.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
