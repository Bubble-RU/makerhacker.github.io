<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>203 nips-2010-Parametric Bandits: The Generalized Linear Case</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-203" href="../nips2010/nips-2010-Parametric_Bandits%3A_The_Generalized_Linear_Case.html">nips2010-203</a> <a title="nips-2010-203-reference" href="#">nips2010-203-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>203 nips-2010-Parametric Bandits: The Generalized Linear Case</h1>
<br/><p>Source: <a title="nips-2010-203-pdf" href="http://papers.nips.cc/paper/4166-parametric-bandits-the-generalized-linear-case.pdf">pdf</a></p><p>Author: Sarah Filippi, Olivier Cappe, Aurélien Garivier, Csaba Szepesvári</p><p>Abstract: We consider structured multi-armed bandit problems based on the Generalized Linear Model (GLM) framework of statistics. For these bandits, we propose a new algorithm, called GLM-UCB. We derive ﬁnite time, high probability bounds on the regret of the algorithm, extending previous analyses developed for the linear bandits to the non-linear case. The analysis highlights a key difﬁculty in generalizing linear bandit algorithms to the non-linear case, which is solved in GLM-UCB by focusing on the reward space rather than on the parameter space. Moreover, as the actual effectiveness of current parameterized bandit algorithms is often poor in practice, we provide a tuning method based on asymptotic arguments, which leads to signiﬁcantly better practical performance. We present two numerical experiments on real-world data that illustrate the potential of the GLM-UCB approach. Keywords: multi-armed bandit, parametric bandits, generalized linear models, UCB, regret minimization. 1</p><br/>
<h2>reference text</h2><p>[1] T.L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4–22, 1985.</p>
<p>[2] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2):235–256, 2002.</p>
<p>[3] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge Univ Pr, 2006.</p>
<p>[4] J. Audibert, R. Munos, and Cs. Szepesv´ ri. Tuning bandit algorithms in stochastic environa ments. Lecture Notes in Computer Science, 4754:150, 2007.</p>
<p>[5] C.C. Wang, S.R. Kulkarni, and H.V. Poor. Bandit problems with side observations. IEEE Transactions on Automatic Control, 50(3):338–355, 2005.</p>
<p>[6] J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. Advances in Neural Information Processing Systems, pages 817–824, 2008.</p>
<p>[7] S. Pandey, D. Chakrabarti, and D. Agarwal. Multi-armed bandit problems with dependent arms. International Conference on Machine learning, pages 721–728, 2007.</p>
<p>[8] V. Dani, T.P. Hayes, and S.M. Kakade. Stochastic linear optimization under bandit feedback. Conference on Learning Theory, 2008.</p>
<p>[9] S.M. Kakade, S. Shalev-Shwartz, and A. Tewari. Efﬁcient bandit algorithms for online multiclass prediction. In Proceedings of the 25th International Conference on Machine learning, pages 440–447. ACM, 2008.</p>
<p>[10] P. Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3:397–422, 2002.</p>
<p>[11] Y. Abbasi-Yadkori, A. Antos, and Cs. Szepesv´ ri. Forced-exploration based algorithms for a playing in stochastic linear bandits. In COLT Workshop on On-line Learning with Limited Feedback, 2009.</p>
<p>[12] P. Rusmevichientong and J.N. Tsitsiklis. Linearly parameterized bandits. Mathematics of Operations Research, 35(2):395–411, 2010.</p>
<p>[13] P. McCullagh and J. A. Nelder. Generalized Linear Models. Chapman and Hall, 1989.</p>
<p>[14] K. Chen, I. Hu, and Z. Ying. Strong consistency of maximum quasi-likelihood estimators in generalized linear models with ﬁxed and adaptive designs. Annals of Statistics, 27(4):1155–1163, 1999.</p>
<p>[15] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent Dirichlet allocation. Advances in Neural Information Processing Systems, 14:601–608, 2002.</p>
<p>[16] V.H. De La Pena, M.J. Klass, and T.L. Lai. Self-normalized processes: exponential inequalities, moment bounds and iterated logarithm laws. Annals of Probability, 32(3):1902–1933, 2004.</p>
<p>[17] P. Rusmevichientong and J.N. Tsitsiklis. Linearly parameterized bandits. Arxiv preprint arXiv:0812.3465v2, 2008.  9</p>
<br/>
<br/><br/><br/></body>
</html>
