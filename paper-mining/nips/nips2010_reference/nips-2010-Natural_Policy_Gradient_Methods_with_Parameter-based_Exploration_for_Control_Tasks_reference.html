<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-179" href="../nips2010/nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">nips2010-179</a> <a title="nips-2010-179-reference" href="#">nips2010-179-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</h1>
<br/><p>Source: <a title="nips-2010-179-pdf" href="http://papers.nips.cc/paper/3987-natural-policy-gradient-methods-with-parameter-based-exploration-for-control-tasks.pdf">pdf</a></p><p>Author: Atsushi Miyamae, Yuichi Nagata, Isao Ono, Shigenobu Kobayashi</p><p>Abstract: In this paper, we propose an efﬁcient algorithm for estimating the natural policy gradient using parameter-based exploration; this algorithm samples directly in the parameter space. Unlike previous methods based on natural gradients, our algorithm calculates the natural policy gradient using the inverse of the exact Fisher information matrix. The computational cost of this algorithm is equal to that of conventional policy gradients whereas previous natural policy gradient methods have a prohibitive computational cost. Experimental results show that the proposed method outperforms several policy gradient methods. 1</p><br/>
<h2>reference text</h2><p>[1] Youhei Akimoto, Yuichi Nagata, Isao Ono, and Shigenobu Kobayashi. Bidirectional Relation between CMA Evolution Strategies and Natural Evolution Strategies. Parallel Problem Solving from Nature XI, pages 154–163, 2010.</p>
<p>[2] S. Amari. Natural Gradient Works Efﬁciently in Learning. Neural Computation, 10(2):251– 276, 1998.</p>
<p>[3] S. Amari and H. Nagaoka. Methods of Information Geometry. American Mathematical Society, 2007.</p>
<p>[4] J. Andrew Bagnell and Jeff Schneider. Covariant policy search. In IJCAI’03: Proceedings of the 18th international joint conference on Artiﬁcial intelligence, pages 1019–1024, 2003.</p>
<p>[5] Jonathan Baxter and Peter L. Bartlett. Inﬁnite-horizon policy-gradient estimation. Journal of Artiﬁcial Intelligence Research, 15:319–350, 2001.</p>
<p>[6] Evan Greensmith, Peter L. Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient estimates in reinforcement learning. The Journal of Machine Learning Research, 5:1471–1530, 2004.</p>
<p>[7] V. Heidrich-Meisner and C. Igel. Variable metric reinforcement learning methods applied to the noisy mountain car problem. In EWRL 2008, pages 136–150, 2008.</p>
<p>[8] Antti Honkela, Matti Tornio, Tapani Raiko, and Juha Karhunen. Natural conjugate gradient in variational inference. In ICONIP 2007, pages 305–314, 2008.</p>
<p>[9] S. A. Kakade. A natural policy gradient. In In Advances in Neural Information Processing Systems, pages 1531–1538, 2001.</p>
<p>[10] H. Kimura and S. Kobayashi. Reinforcement learning for continuous action using stochastic gradient ascent. In Intelligent Autonomous Systems (IAS-5), pages 288–295, 1998.</p>
<p>[11] Hajime Kimura, Kazuteru Miyazaki, and Shigenobu Kobayashi. Reinforcement learning in pomdps with function approximation. In ICML ’97: Proceedings of the Fourteenth International Conference on Machine Learning, pages 152–160, 1997.</p>
<p>[12] Hajime Kimura, Masayuki Yamamura, and Shigenobu Kobayashi. Reinforcement learning by stochastic hill climbing on discounted reward. In ICML, pages 295–303, 1995.</p>
<p>[13] Jens Kober and Jan Peters. Policy search for motor primitives in robotics. In Advances in Neural Information Processing Systems 21, pages 849–856, 2009.</p>
<p>[14] Jan Peters and Stefan Schaal. Policy Gradient Methods for Robotics. In 2006 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 2219–2225, 2006.</p>
<p>[15] Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7–9):1180–1190, 2008.</p>
<p>[16] Silvia Richter, Douglas Aberdeen, and Jin Yu. Natural actor-critic for road trafﬁc optimisation. In Advances in Neural Information Processing Systems 19, pages 1169–1176. MIT Press, Cambridge, MA, 2007.</p>
<p>[17] Thomas R¨ ckstieß, Martin Felder, and J¨ rgen Schmidhuber. State-dependent exploration for u u policy gradient methods. In ECML PKDD ’08: Proceedings of the European conference on Machine Learning and Knowledge Discovery in Databases - Part II, pages 234–249, 2008.</p>
<p>[18] Frank Sehnke, C Osendorfer, T Rueckstiess, A. Graves, J. Peters, and J. Schmidhuber. Policy gradients with parameter-based exploration for control. In Proceedings of the International Conference on Artiﬁcial Neural Networks (ICANN), pages 387–396, 2008.</p>
<p>[19] Yi Sun, Daan Wierstra, Tom Schaul, and Juergen Schmidhuber. Efﬁcient natural evolution strategies. In GECCO ’09: Proceedings of the 11th Annual conference on Genetic and evolutionary computation, pages 539–546, 2009.</p>
<p>[20] R. S. Sutton. Policy gradient method for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems, volume 12, pages 1057–1063, 2000.</p>
<p>[21] Daan Wierstra, Er Foerster, Jan Peters, and Juergen Schmidhuber. Solving deep memory pomdps with recurrent policy gradients. In In International Conference on Artiﬁcial Neural Networks, 2007.</p>
<p>[22] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Machine Learning, pages 229–256, 1992.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
