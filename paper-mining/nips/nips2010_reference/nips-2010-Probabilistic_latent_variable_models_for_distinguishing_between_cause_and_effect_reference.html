<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>218 nips-2010-Probabilistic latent variable models for distinguishing between cause and effect</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-218" href="../nips2010/nips-2010-Probabilistic_latent_variable_models_for_distinguishing_between_cause_and_effect.html">nips2010-218</a> <a title="nips-2010-218-reference" href="#">nips2010-218-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>218 nips-2010-Probabilistic latent variable models for distinguishing between cause and effect</h1>
<br/><p>Source: <a title="nips-2010-218-pdf" href="http://papers.nips.cc/paper/4173-probabilistic-latent-variable-models-for-distinguishing-between-cause-and-effect.pdf">pdf</a></p><p>Author: Oliver Stegle, Dominik Janzing, Kun Zhang, Joris M. Mooij, Bernhard Schölkopf</p><p>Abstract: We propose a novel method for inferring whether X causes Y or vice versa from joint observations of X and Y . The basic idea is to model the observed data using probabilistic latent variable models, which incorporate the effects of unobserved noise. To this end, we consider the hypothetical effect variable to be a function of the hypothetical cause variable and an independent noise term (not necessarily additive). An important novel aspect of our work is that we do not restrict the model class, but instead put general non-parametric priors on this function and on the distribution of the cause. The causal direction can then be inferred by using standard Bayesian model selection. We evaluate our approach on synthetic data and real-world data and report encouraging results. 1</p><br/>
<h2>reference text</h2><p>[1] N. Friedman and I. Nachman. Gaussian process networks. In Proc. of the 16th Annual Conference on Uncertainty in Artiﬁcial Intelligence, pages 211–219, 2000.</p>
<p>[2] S. Shimizu, P. O. Hoyer, A. Hyv¨ rinen, and A. J. Kerminen. A linear non-Gaussian acyclic model for a causal discovery. Journal of Machine Learning Research, 7:2003–2030, 2006.</p>
<p>[3] X. Sun, D. Janzing, and B. Sch¨ lkopf. Causal inference by choosing graphs with most plausible Markov o kernels. In Proceeding of the 9th Int. Symp. Art. Int. and Math., Fort Lauderdale, Florida, 2006.</p>
<p>[4] X. Sun, D. Janzing, and B. Sch¨ lkopf. Distinguishing between cause and effect via kernel-based como plexity measures for conditional probability densities. Neurocomputing, pages 1248–1256, 2008.</p>
<p>[5] P. O. Hoyer, D. Janzing, J. M. Mooij, J. Peters, and B. Sch¨ lkopf. Nonlinear causal discovery with o additive noise models. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21 (NIPS*2008), pages 689–696, 2009.</p>
<p>[6] K. Zhang and A. Hyv¨ rinen. On the identiﬁability of the post-nonlinear causal model. In Proceedings of a the 25th Conference on Uncertainty in Artiﬁcial Intelligence, Montreal, Canada, 2009.</p>
<p>[7] D. Janzing, P. Hoyer, and B. Sch¨ lkopf. Telling cause from effect based on high-dimensional observations. o In Proceedings of the International Conference on Machine Learning (ICML 2010), pages 479–486, 2010.</p>
<p>[8] J. M. Mooij and D. Janzing. Distinguishing between cause and effect. In Journal of Machine Learning Research Workshop and Conference Proceedings, volume 6, pages 147–156, 2010.</p>
<p>[9] P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. Springer-Verlag, 1993. (2nd ed. MIT Press 2000).</p>
<p>[10] J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, 2000.</p>
<p>[11] J. Lemeire and E. Dirkx. Causal models as minimal descriptions of multivariate systems. http://parallel.vub.ac.be/∼jan/, 2006.</p>
<p>[12] D. Janzing and B. Sch¨ lkopf. Causal inference using the algorithmic Markov condition. IEEE Transaco tions on Information Theory, 56(10):5168–5194, 2010.</p>
<p>[13] P. Daniuˇis, D. Janzing, J. M. Mooij, J. Zscheischler, B. Steudel, K. Zhang, and B. Sch¨ lkopf. Inferring s o deterministic causal relations. In Proceedings of the 26th Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-10), 2010.</p>
<p>[14] A. Hyv¨ rinen and P. Pajunen. Nonlinear independent component analysis: Existence and uniqueness a results. Neural Networks, 12(3):429–439, 1999.</p>
<p>[15] M. A. T. Figueiredo and A. K. Jain. Unsupervised learning of ﬁnite mixture models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(3):381–396, March 2002.</p>
<p>[16] N. D. Lawrence. Gaussian process latent variable models for visualisation of high dimensional data. In Advances in Neural Information Processing Systems 16: Proceedings of the 2003 Conference, page 329. The MIT Press, 2004.</p>
<p>[17] A. Gretton, R. Herbrich, A. Smola, O. Bousquet, and B. Sch¨ lkopf. Kernel methods for measuring o independence. Journal of Machine Learning Research, 6:2075–2129, 2005.</p>
<p>[18] C. E. Rasmussen and H. Nickisch. Gaussian Processes for Machine Learning (GPML) Toolbox. Journal of Machine Learning Research, accepted, 2010.  9</p>
<br/>
<br/><br/><br/></body>
</html>
