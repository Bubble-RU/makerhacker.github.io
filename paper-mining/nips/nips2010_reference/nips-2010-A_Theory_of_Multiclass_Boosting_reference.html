<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>15 nips-2010-A Theory of Multiclass Boosting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-15" href="../nips2010/nips-2010-A_Theory_of_Multiclass_Boosting.html">nips2010-15</a> <a title="nips-2010-15-reference" href="#">nips2010-15-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>15 nips-2010-A Theory of Multiclass Boosting</h1>
<br/><p>Source: <a title="nips-2010-15-pdf" href="http://papers.nips.cc/paper/4135-a-theory-of-multiclass-boosting.pdf">pdf</a></p><p>Author: Indraneel Mukherjee, Robert E. Schapire</p><p>Abstract: Boosting combines weak classiﬁers to form highly accurate predictors. Although the case of binary classiﬁcation is well understood, in the multiclass setting, the “correct” requirements on the weak classiﬁer, or the notion of the most efﬁcient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classiﬁer, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements. 1</p><br/>
<h2>reference text</h2><p>[1] Jacob Abernethy, Peter L. Bartlett, Alexander Rakhlin, and Ambuj Tewari. Optimal stragies and minimax lower bounds for online convex games. In Proceedings of the Nineteenth Annual Conference on Computational Learning Theory, pages 415–424, 2008.</p>
<p>[2] Erin L. Allwein, Robert E. Schapire, and Yoram Singer. Reducing multiclass to binary: A unifying approach for margin classiﬁers. Journal of Machine Learning Research, 1:113–141, 2000.</p>
<p>[3] Alina Beygelzimer, John Langford, and Pradeep Ravikumar. Error-correcting tournaments. In Algorithmic Learning Theory: 20th International Conference, pages 247–262, 2009.</p>
<p>[4] Thomas G. Dietterich and Ghulum Bakiri. Solving multiclass learning problems via error-correcting output codes. Journal of Artiﬁcial Intelligence Research, 2:263–286, January 1995.</p>
<p>[5] G¨ nther Eibl and Karl-Peter Pfeiffer. Multiclass boosting for weak classiﬁers. Journal of Machine Learnu ing Research, 6:189–210, 2005.</p>
<p>[6] Yoav Freund. Boosting a weak learning algorithm by majority. 121(2):256–285, 1995.  Information and Computation,</p>
<p>[7] Yoav Freund. An adaptive version of the boost by majority algorithm. Machine Learning, 43(3):293–318, June 2001.</p>
<p>[8] Yoav Freund and Manfred Opper. Continuous drifting games. Journal of Computer and System Sciences, pages 113–132, 2002.</p>
<p>[9] Yoav Freund and Robert E. Schapire. Experiments with a new boosting algorithm. In Machine Learning: Proceedings of the Thirteenth International Conference, pages 148–156, 1996.</p>
<p>[10] Yoav Freund and Robert E. Schapire. Game theory, on-line prediction and boosting. In Proceedings of the Ninth Annual Conference on Computational Learning Theory, pages 325–332, 1996.</p>
<p>[11] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119–139, August 1997.</p>
<p>[12] Trevor Hastie and Robert Tibshirani. Classiﬁcation by pairwise coupling. Annals of Statistics, 26(2):451– 471, 1998.</p>
<p>[13] V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization error of combined classiﬁers. Annals of Statistics, 30(1), February 2002.</p>
<p>[14] Gunnar R¨ tsch and Manfred K. Warmuth. Efﬁcient margin maximizing with boosting. Journal of Machine a Learning Research, 6:2131–2152, 2005.</p>
<p>[15] Robert E. Schapire. The strength of weak learnability. Machine Learning, 5(2):197–227, 1990.</p>
<p>[16] Robert E. Schapire. Drifting games. Machine Learning, 43(3):265–291, June 2001.</p>
<p>[17] Robert E. Schapire. The boosting approach to machine learning: An overview. In MSRI Workshop on Nonlinear Estimation and Classiﬁcation, 2002.</p>
<p>[18] Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. Annals of Statistics, 26(5):1651–1686, October 1998.</p>
<p>[19] Robert E. Schapire and Yoram Singer. Improved boosting algorithms using conﬁdence-rated predictions. Machine Learning, 37(3):297–336, December 1999.</p>
<p>[20] Robert E. Schapire and Yoram Singer. BoosTexter: A boosting-based system for text categorization. Machine Learning, 39(2/3):135–168, May/June 2000.</p>
<p>[21] Ji Zhu, Hui Zou, Saharon Rosset, and Trevor Hastie. Multi-class AdaBoost. Statistics and Its Interface, 2:349360, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
