<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-73" href="../nips2010/nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">nips2010-73</a> <a title="nips-2010-73-reference" href="#">nips2010-73-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</h1>
<br/><p>Source: <a title="nips-2010-73-pdf" href="http://papers.nips.cc/paper/3988-efficient-and-robust-feature-selection-via-joint-l21-norms-minimization.pdf">pdf</a></p><p>Author: Feiping Nie, Heng Huang, Xiao Cai, Chris H. Ding</p><p>Abstract: Feature selection is an important component of many machine learning applications. Especially in many bioinformatics tasks, efﬁcient and robust feature selection methods are desired to extract meaningful features and eliminate noisy ones. In this paper, we propose a new robust feature selection method with emphasizing joint 2,1 -norm minimization on both loss function and regularization. The 2,1 -norm based loss function is robust to outliers in data points and the 2,1 norm regularization selects features across all data points with joint sparsity. An efﬁcient algorithm is introduced with proved convergence. Our regression based objective makes the feature selection process more efﬁcient. Our method has been applied into both genomic and proteomic biomarkers discovery. Extensive empirical studies are performed on six data sets to demonstrate the performance of our feature selection method. 1</p><br/>
<h2>reference text</h2><p>[1] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. NIPS, pages 41–48, 2007.</p>
<p>[2] A. Bhattacharjee, W. G. Richards, and et. al. Classiﬁcation of human lung carcinomas by mRNA expression proﬁling reveals distinct adenocarcinoma subclasses. Proceedings of the National Academy of Sciences, 98(24):13790–13795, 2001.</p>
<p>[3] P. Bradley and O. Mangasarian. Feature selection via concave minimization and support vector machines. ICML, 1998.</p>
<p>[4] C. Ding and H. Peng. Minimum redundancy feature selection from microarray gene expression data. Proceedings of the Computational Systems Bioinformatics, 2003.</p>
<p>[5] C. Ding, D. Zhou, X. He, and H. Zha. R1-PCA: Rotational invariant L1-norm principal component analysis for robust subspace factorization. Proc. Int’l Conf. Machine Learning (ICML), June 2006.</p>
<p>[6] S. P. Fodor. DNA SEQUENCING: Massively Parallel Genomics. Science, 277(5324):393–395, 1997.</p>
<p>[7] I. Guyon and A. Elisseeff. An introduction to variable and feature selection. J. Machine Learning Research, 2003.</p>
<p>[8] I. Guyon, J.Weston, S. Barnhill, and V. Vapnik. Gene selection for cancer classiﬁcation using support vector machines. Machine Learning, 46(1):389, 2002.</p>
<p>[9] M. A. Hall and L. A. Smith. Feature selection for machine learning: Comparing a correlation-based ﬁlter approach to the wrapper. 1999.</p>
<p>[10] H. Huang and C. Ding. Robust tensor factorization using r1 norm. CVPR 2008, pages 1–8, 2008.</p>
<p>[11] K. Kira and L. A. Rendell. A practical approach to feature selection. In A Practical Approach to Feature Selection, pages 249–256, 1992.</p>
<p>[12] R. Kohavi and G. H. John. Wrappers for feature subset selection. Artiﬁcial Intelligence, 97(1-2):273–324, 1997.</p>
<p>[13] I. Kononenko. Estimating attributes: Analysis and extensions of RELIEF. In European Conference on Machine Learning, pages 171–182, 1994.</p>
<p>[14] P. Langley. Selection of relevant features in machine learning. In AAAI Fall Symposium on Relevance, pages 140–144, 1994.</p>
<p>[15] H. Liu and H. Motoda. Feature Selection for Knowledge Discovery and Data Mining. Springer, 1998.</p>
<p>[16] D. Luo, C. Ding, and H. Huang. Towards structural sparsity: An explicit 2 / 0 approach. ICDM, 2010.</p>
<p>[17] C. L. Nutt, D. R. Mani, R. A. Betensky, P. Tamayo, J. G. Cairncross, C. Ladd, U. Pohl, C. Hartmann, and M. E. Mclaughlin. Gene expression-based classiﬁcation of malignant gliomas correlates better with survival than histological classiﬁcation. Cancer Res., 63:1602–1607, 2003.</p>
<p>[18] G. Obozinski, B. Taskar, and M. Jordan. Multi-task feature selection. Technical report, Department of Statistics, University of California, Berkeley, 2006.</p>
<p>[19] H. Peng, F. Long, and C. Ding. Feature selection based on mutual information: Criteria of max-depe ndency, max-relevance, and min-redundancy. IEEE Trans. Pattern Analysis and Machine Intelligence, 27, 2005.</p>
<p>[20] P. C. Petricoin EF, Ornstein DK. Serum proteomic patterns for detection of prostate cancer. J Natl Cancer Inst., 94(20):1576–8, 2002.</p>
<p>[21] L. E. Raileanu and K. Stoffel. Theoretical comparison between the gini index and information gain criteria. Univeristy of Neuchatel, 2000.</p>
<p>[22] Y. Saeys, I. Inza, and P. Larranaga. A review of feature selection techniques in bioinformatics. Bioinformatics, 23(19):2507–2517, 2007.</p>
<p>[23] D. Singh, P. Febbo, K. Ross, and et al. Gene expression correlates of clinical prostate cancer behavior. Cancer Cell, pages 203–209, 2002.</p>
<p>[24] A. I. Su, J. B. Welsh, L. M. Sapinoso, and et al. Molecular classiﬁcation of human carcinomas by use of gene expression signatures. Cancer Research, 61:7388–7393, 2001.</p>
<p>[25] L. Sun, J. Liu, J. Chen, and J. Ye. Efﬁcient recovery of jointly sparse vectors. In Neural Information Processing Systems, 2009.</p>
<p>[26] L. Wang, J. Zhu, and H. Zou. Hybrid huberized support vector machines for microarray classiﬁcation. ICML, 2007.</p>
<p>[27] K. Yang, Z. Cai, J. Li, and G. Lin. A stable gene selection in microarray data analysis. BMC Bioinformatics, 7:228, 2006.</p>
<p>[28] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B, 68:49–67, 2005.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
