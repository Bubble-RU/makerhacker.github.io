<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>144 nips-2010-Learning Efficient Markov Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-144" href="../nips2010/nips-2010-Learning_Efficient_Markov_Networks.html">nips2010-144</a> <a title="nips-2010-144-reference" href="#">nips2010-144-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>144 nips-2010-Learning Efficient Markov Networks</h1>
<br/><p>Source: <a title="nips-2010-144-pdf" href="http://papers.nips.cc/paper/4010-learning-efficient-markov-networks.pdf">pdf</a></p><p>Author: Vibhav Gogate, William Webb, Pedro Domingos</p><p>Abstract: We present an algorithm for learning high-treewidth Markov networks where inference is still tractable. This is made possible by exploiting context-speciﬁc independence and determinism in the domain. The class of models our algorithm can learn has the same desirable properties as thin junction trees: polynomial inference, closed-form weight learning, etc., but is much broader. Our algorithm searches for a feature that divides the state space into subspaces where the remaining variables decompose into independent subsets (conditioned on the feature and its negation) and recurses on each subspace/subset of variables until no useful new features can be found. We provide probabilistic performance guarantees for our algorithm under the assumption that the maximum feature length is bounded by a constant k (the treewidth can be much larger) and dependences are of bounded strength. We also propose a greedy version of the algorithm that, while forgoing these guarantees, is much more efﬁcient. Experiments on a variety of domains show that our approach outperforms many state-of-the-art Markov network structure learners. 1</p><br/>
<h2>reference text</h2><p>[1] G. Andrew and J. Gao. Scalable training of L1-regularized log-linear models. In Proceedings of the Twenty-Fourth International Conference (ICML), pages 33–40, 2007.</p>
<p>[2] F. R. Bach and M. I. Jordan. Thin junction trees. In Advances in Neural Information Processing Systems, pages 569–576, 2001.</p>
<p>[3] I. Beinlich, J. Suermondt, M. Chavez, and G. Cooper. The alarm monitoring system: A case study with two probablistic inference techniques for belief networks. In European Conference on AI in Medicine, 1988.</p>
<p>[4] J. Besag. Statistical analysis of non-lattice data. The Statistician, 24:179–195, 1975.</p>
<p>[5] C. Blake and C. J. Merz. UCI repository of machine learning databases. Machine-readable data repository, Department of Information and Computer Science, University of California at Irvine, Irvine, CA, 2000. http://www.ics.uci.edu/∼mlearn/MLRepository.html.</p>
<p>[6] C. Boutilier. Context-speciﬁc independence in Bayesian networks. In Proceedings of the Twelfth Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pages 115–123, 1996.</p>
<p>[7] M. Chavira and A. Darwiche. On probabilistic inference by weighted model counting. Artiﬁcial Intelligence, 172(6–7):772–799, April 2008.</p>
<p>[8] A. Chechetka and C. Guestrin. Efﬁcient principled learning of thin junction trees. In Advances in Neural Information Processing Systems (NIPS), December 2007.</p>
<p>[9] D.M. Chickering, D. Geiger, and D. Heckerman. Learning Bayesian networks: Search methods and experimental results. In Proceedings of the Fifth International Workshop on Artiﬁcial Intelligence and Statistics (AISTATS), pages 112–128, 1995.</p>
<p>[10] A. Darwiche. A differential approach to inference in Bayesian networks. Journal of the ACM, 50(3):280– 305, 2003.</p>
<p>[11] R. Dechter and R. Mateescu. AND/OR search spaces for graphical models. Artiﬁcial Intelligence, 171(23):73–106, 2007.</p>
<p>[12] S. Della Pietra, V. Della Pietra, and J. Lafferty. Inducing features of random ﬁelds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19:380–392, 1997.</p>
<p>[13] S. Goldman. Efﬁcient methods for calculating maximum entropy distributions. Master’s thesis, Massachusetts Institute of Technology, 1987.</p>
<p>[14] K. H¨ ffgen. Learning and robust learning of product distributions. In Proceedings of the Sixth Annual o ACM Conference on Computational Learning Theory (COLT), pages 77–83, 1993.</p>
<p>[15] D. R. Karger and N. Srebro. Learning Markov networks: maximum bounded tree-width graphs. In Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 392–401, 2001.</p>
<p>[16] S. Lee, V. Ganapathi, and D. Koller. Efﬁcient structure learning of Markov networks using L1regularization. In Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems (NIPS), pages 817–824, 2006.</p>
<p>[17] D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(3):503–528, 1989.</p>
<p>[18] D. Lowd and P. Domingos. Learning arithmetic circuits. In Proceedings of the Twenty Fourth Conference in Uncertainty in Artiﬁcial Intelligence, pages 383–392, 2008.</p>
<p>[19] A. W. Moore and M. S. Lee. Cached sufﬁcient statistics for efﬁcient machine learning with large datasets. Journal of Artiﬁcial Intelligence Research, 8:67–91, 1997.</p>
<p>[20] K. P. Murphy, Y. Weiss, and M. I. Jordan. Loopy belief propagation for approximate inference: An empirical study. In Proceedings of the Fifteenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pages 467–475, 1999.</p>
<p>[21] M. Narasimhan and J. Bilmes. PAC-learning bounded tree-width graphical models. In Proceedings of the Twentieth Conference in Uncertainty in Artiﬁcial Intelligence (UAI), pages 410–417, 2004.</p>
<p>[22] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, San Francisco, CA, 1988.</p>
<p>[23] M. Queyranne. Minimizing symmetric submodular functions. Mathematical Programming, 82(1):3–12, 1998.</p>
<p>[24] P. Ravikumar, M. J. Wainwright, and J. Lafferty. High-dimensional Ising model selection using L1regularized logistic regression. Annals of Statistics, 38(3):1287–1319, 2010.</p>
<p>[25] D. Roth. On the hardness of approximate reasoning. Artiﬁcial Intelligence, 82:273–302, 1996.</p>
<p>[26] T. Sang, P. Beame, and H. Kautz. Performing Bayesian inference by weighted model counting. In Proceedings of The Twentieth National Conference on Artiﬁcial Intelligence (AAAI), pages 475–482, 2005.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
