<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>172 nips-2010-Multi-Stage Dantzig Selector</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-172" href="../nips2010/nips-2010-Multi-Stage_Dantzig_Selector.html">nips2010-172</a> <a title="nips-2010-172-reference" href="#">nips2010-172-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>172 nips-2010-Multi-Stage Dantzig Selector</h1>
<br/><p>Source: <a title="nips-2010-172-pdf" href="http://papers.nips.cc/paper/4015-multi-stage-dantzig-selector.pdf">pdf</a></p><p>Author: Ji Liu, Peter Wonka, Jieping Ye</p><p>Abstract: We consider the following sparse signal recovery (or feature selection) problem: given a design matrix X ∈ Rn×m (m n) and a noisy observation vector y ∈ Rn satisfying y = Xβ ∗ + where is the noise vector following a Gaussian distribution N (0, σ 2 I), how to recover the signal (or parameter vector) β ∗ when the signal is sparse? The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively reﬁnes the target signal β ∗ . We show that if X obeys a certain condition, then with a large probability the difference between the solution ˆ β estimated by the proposed method and the true solution β ∗ measured in terms of the lp norm (p ≥ 1) is bounded as ˆ β − β∗ p ≤ C(s − N )1/p log m + ∆ σ, where C is a constant, s is the number of nonzero entries in β ∗ , ∆ is independent of m and is much smaller than the ﬁrst term, and N is the number of entries of √ β ∗ larger than a certain value in the order of O(σ log m). The proposed method improves the estimation bound of the standard Dantzig selector approximately √ √ from Cs1/p log mσ to C(s − N )1/p log mσ where the value N depends on the number of large entries in β ∗ . When N = s, the proposed algorithm achieves the oracle solution with a high probability. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector. 1</p><br/>
<h2>reference text</h2><p>[1] P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Annals of Statistics, 37:1705–1732, 2009.</p>
<p>[2] F. Bunea, A. Tsybakov, and M. Wegkamp. Sparsity oracle inequalities for the Lasso. Electronic Journal of Statistics, 2007.</p>
<p>[3] T. Cai and L. Wang. Orthogonal matching pursuit for sparse signal reconvery. Technical Report, 2010.</p>
<p>[4] T. Cai, G. Xu, and J. Zhang. On recovery of sparse signals via l1 minimization. IEEE Transactions on Information Theory, 55(7):3388–3397, 2009.</p>
<p>[5] E. J. Candes and Y. Plan. Near-ideal model selection by l1 minimization. Annals of Statistics, 37:2145–2177, 2006.</p>
<p>[6] E. J. Candes and T. Tao. Decoding by linear programming. IEEE Transactions on Information Theory, 51(12):4203–4215, 2005.</p>
<p>[7] E. J. Candes and T. Tao. The Dantzig selector: Statistical estimation when p is much larger than n. Annals of Statistics, 35:2313, 2007.</p>
<p>[8] D. L. Donoho, M. Elad, and V. N. Temlyakov. Stable recovery of sparse overcomplete representations in the presence of noise. IEEE Transactions on Information Theory, pages 6–18, 2006.</p>
<p>[9] G. M. James, P. Radchenko, and J. Lv. DASSO: connections between the Dantzig selector and Lasso. Journal of The Royal Statistical Society Series B, 71(1):127–142, 2009.</p>
<p>[10] V. Koltchinskii and M. Yuan. Sparse recovery in large ensembles of kernel machines on-line learning and bandits. COLT, pages 229–238, 2008.</p>
<p>[11] K. Lounici. Sup-norm convergence rate and sign concentration property of Lasso and Dantzig esti mators. Electronic Journal of Statistics, 2:90–102, 2008.</p>
<p>[12] N. Meinshausen, P. Bhlmann, and E. Zrich. High dimensional graphs and variable selection with the Lasso. Annals of Statistics, 34:1436–1462, 2006.</p>
<p>[13] P. Ravikumar, G. Raskutti, M. J. Wainwright, and B. Yu. Model selection in gaussian graphical models: High-dimensional consistency of l1 -regularized MLE. pages 1329–1336, 2008.</p>
<p>[14] J. Romberg. The Dantzig selector and generalized thresholding. CISS, pages 22–25, 2008.</p>
<p>[15] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society: Series B, 58(1):267–288, 1996.</p>
<p>[16] J. A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Information Theory, 50:2231–2242, 2004.</p>
<p>[17] M. J. Wainwright. Sharp thresholds for noisy and high-dimensional recovery of sparsity using l1 -constrained quadratic programming (Lasso). IEEE Transactions on Information Theory, pages 2183–2202, 2009.</p>
<p>[18] T. Zhang. Adaptive forward-backward greedy algorithm for sparse learning with linear models. NIPS, pages 1921–1928, 2008.</p>
<p>[19] T. Zhang. On the consistency of feature selection using greedy least squares regression. Journal of Machine Learning Reserch, 10:555–568, 2009.</p>
<p>[20] T. Zhang. Some sharp performance bounds for least squares regression with l1 regularization. Annals of Statistics, 37:2109, 2009.</p>
<p>[21] T. Zhang. Sparse recovery with orthogonal matching pursuit under RIP. arXiv:1005.2249, 2010.</p>
<p>[22] P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning Reserch, 7:2541–2563, 2006.</p>
<p>[23] S. Zhou. Thresholding procedures for high dimensional variable selection and statistical estimation. NIPS, pages 2304–2312, 2009.</p>
<p>[24] H. Zou. The adaptive Lasso and its oracle properties. Journal of the American Statistical Association, 101:1418–1429, 2006.  9</p>
<br/>
<br/><br/><br/></body>
</html>
