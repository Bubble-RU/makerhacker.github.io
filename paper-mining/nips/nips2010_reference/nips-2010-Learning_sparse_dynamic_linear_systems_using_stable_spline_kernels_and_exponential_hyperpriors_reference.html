<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-154" href="../nips2010/nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">nips2010-154</a> <a title="nips-2010-154-reference" href="#">nips2010-154-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</h1>
<br/><p>Source: <a title="nips-2010-154-pdf" href="http://papers.nips.cc/paper/4174-learning-sparse-dynamic-linear-systems-using-stable-spline-kernels-and-exponential-hyperpriors.pdf">pdf</a></p><p>Author: Alessandro Chiuso, Gianluigi Pillonetto</p><p>Abstract: We introduce a new Bayesian nonparametric approach to identiﬁcation of sparse dynamic linear systems. The impulse responses are modeled as Gaussian processes whose autocovariances encode the BIBO stability constraint, as deﬁned by the recently introduced “Stable Spline kernel”. Sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels. Numerical experiments regarding estimation of ARMAX models show that this technique provides a deﬁnite advantage over a group LAR algorithm and state-of-the-art parametric identiﬁcation techniques based on prediction error minimization. 1</p><br/>
<h2>reference text</h2><p>[1] L. Ljung. System Identiﬁcation - Theory For the User. Prentice Hall, 1999.</p>
<p>[2] J. Mohammadpour and K.M. Grigoriadis. Springer, 2010.  Efﬁcient Modeling and Control of Large-scale Systems.</p>
<p>[3] T. J. Hastie and R. J. Tibshirani. Generalized additive models. In Monographs on Statistics and Applied Probability, volume 43. Chapman and Hall, London, UK, 1990.</p>
<p>[4] D. Donoho. Compressed sensing. IEEE Trans. on Information Theory, 52(4):1289–1306, 2006.</p>
<p>[5] H. Akaike. A new look at the statistical model identiﬁcation. IEEE Transactions on Automatic Control, 19:716–723, 1974.</p>
<p>[6] G. Schwarz. Estimating the dimension of a model. The Annals of Statistics, 6:461–464, 1978.</p>
<p>[7] R. Tibshirani. Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society, Series B., 58, 1996.</p>
<p>[8] B. Efron, T. Hastie, L. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32:407– 499, 2004.</p>
<p>[9] P. Zhao and B. Yu. On model selection consistency of lasso. Journal of Machine Learning Research, 7:2541–2563, 2006.</p>
<p>[10] H. Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical Association, 101:1418–1429, 2006.</p>
<p>[11] Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society, Series B, 68:49–67, 2006.</p>
<p>[12] F.R. Bach. Consistency of the group lasso and multiple kernel learning. J. Mach. Learn. Res., 9:1179– 1225, 2008.</p>
<p>[13] C. A. Micchelli and M. Pontil. Learning the kernel function via regularization. Journal of Machine Learning Research, 6:1099–1125, 2005.</p>
<p>[14] H. Wang, G. Li, and C.L. Tsai. Regression coefﬁcient and autoregressive order shrinkage and selection via the lasso. Journal Of The Royal Statistical Society Series B, 69(1):63–78, 2007.</p>
<p>[15] Nan-Jung Hsu, Hung-Lin Hung, and Ya-Mei Chang. Subset selection for vector autoregressive processes using lasso. Computational Statistics and Data Analysis, 52:3645–3657, 2008.</p>
<p>[16] G. Pillonetto and G. De Nicolao. A new kernel-based approach for linear system identiﬁcation. Automatica, 46(1):81–93, 2010.</p>
<p>[17] G. Pillonetto, A. Chiuso, and G. De Nicolao. Prediction error identiﬁcation of linear systems: a nonparametric Gaussian regression approach. Automatica (in press), 2011.</p>
<p>[18] C.E. Rasmussen and C.K.I. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2006.</p>
<p>[19] N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical Society, 68:337–404, 1950.</p>
<p>[20] G. Wahba. Support vector machines, reproducing kernel Hilbert spaces and randomized GACV. Technical Report 984, Department of Statistics, University of Wisconsin, 1998.</p>
<p>[21] G. Kimeldorf and G. Wahba. Some results on Tchebychefﬁan spline functions. Journal of Mathematical Analysis and Applications, 33(1):82–95, 1971.</p>
<p>[22] A. J. Smola and B. Sch¨ lkopf. Bayesian kernel methods. In S. Mendelson and A. J. Smola, editors, o Machine Learning, Proceedings of the Summer School, Australian National University, pages 65–117, Berlin, Germany, 2003. Springer-Verlag.</p>
<p>[23] G. Wahba. Spline models for observational data. SIAM, Philadelphia, 1990.</p>
<p>[24] G.C. Goodwin, M. Gevers, and B. Ninness. Quantifying the error in estimated transfer functions with application to model order selection. IEEE Transactions on Automatic Control, 37(7):913–928, 1992.</p>
<p>[25] F. Dinuzzo. Kernel machines with two layers and multiple kernel learning. Technical report, Preprint arXiv:1001.2709, 2010. Available at http://www-dimat.unipv.it/ dinuzzo.</p>
<p>[26] L. Ljung. System Identiﬁcation Toolbox V7.1 for Matlab. Natick, MA: The MathWorks, Inc., 2007.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
