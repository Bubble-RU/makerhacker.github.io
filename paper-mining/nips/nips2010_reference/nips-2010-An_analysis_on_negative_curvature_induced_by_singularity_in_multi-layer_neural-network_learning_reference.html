<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-31" href="../nips2010/nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">nips2010-31</a> <a title="nips-2010-31-reference" href="#">nips2010-31-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</h1>
<br/><p>Source: <a title="nips-2010-31-pdf" href="http://papers.nips.cc/paper/4046-an-analysis-on-negative-curvature-induced-by-singularity-in-multi-layer-neural-network-learning.pdf">pdf</a></p><p>Author: Eiji Mizutani, Stuart Dreyfus</p><p>Abstract: In the neural-network parameter space, an attractive ﬁeld is likely to be induced by singularities. In such a singularity region, ﬁrst-order gradient learning typically causes a long plateau with very little change in the objective function value E (hence, a ﬂat region). Therefore, it may be confused with “attractive” local minima. Our analysis shows that the Hessian matrix of E tends to be indeﬁnite in the vicinity of (perturbed) singular points, suggesting a promising strategy that exploits negative curvature so as to escape from the singularity plateaus. For numerical evidence, we limit the scope to small examples (some of which are found in journal papers) that allow us to conﬁrm singularities and the eigenvalues of the Hessian matrix, and for which computation using a descent direction of negative curvature encounters no plateau. Even for those small problems, no efﬁcient methods have been previously developed that avoided plateaus. 1</p><br/>
<h2>reference text</h2><p>[1] Amari, S.-I., Park,H. & Fukumizu, K. Adaptive Method of Realizing Natural Gradient Learning for Multilayer Perceptrons. Neural Computation, 12:1399-1409, 2000.</p>
<p>[2] Amari, S.-I., Park, H. & Ozeki, T. Singularities affect dynamics of learning in neuro-manifolds. Neural Computation, 18(5):1007-1065, 2006.</p>
<p>[3] Wei, H., Zhang, J., Cousseau, F., Ozeki, T., & Amari, S.-I. Dynamics of Learning Near Singularities in Layered Networks. Neural Computation, 20(3):813-843, 2008.</p>
<p>[4] Fukumizu, K. & Amari, S.-I. Local Minima and Plateaus in Hierarchical Structures of Multilayer Perceptrons. Neural Networks, 13(3):317–327, 2000.</p>
<p>[5] Jennrich, R.I. & Sampson, P.F. Application of Stepwise Regression to Non-Linear Estimation. Technometrics, 10(1):63–72, 1968.</p>
<p>[6] Cousseau, F., Ozeki, T., & Amari, S.-I. Dynamics of Learning in Multilayer Perceptrons near Singularities IEEE Trans. on Neural Networks, 19(8):1313-1328, 2008.</p>
<p>[7] Powell, M.J.D. A hybrid method for nonlinear equations. In Numerical Methods for Nonlinear Algebraic Equations, Ed. by P.Rabinowitz, Gordon & Breach, London, pp.87–114, 1970.</p>
<p>[8] Dennis, J.E., Jr. Nonlinear least squares and equations. In The state of the art in numerical analysis, Ed. by D. Jacobs, Academic Press, London, pp.269–312, 1977.</p>
<p>[9] Parlett, B.N. The Symmetric Eigenvalue Problem. SIAM, 1998.</p>
<p>[10] Blum, E.K. Approximation of Boolean Functions by Sigmoidal Networks: Part I: XOR and other twovariable functions. Neural Computation, 1:532-540, 1989.</p>
<p>[11] Sprinkhuizen-Kuyper, I.G. & Boers, E.J.W. A Local Minimum for the 2-3-1 XOR Network. IEEE Transactions on Neural Networks, 10(4):968–971, 1999.</p>
<p>[12] Mizutani, E. & Dreyfus, S.E. Second-order stagewise backpropagation for Hessian-matrix analyses and investigation of negative curvature. Neural Networks, vol.21 (issues 2–3):193-203, 2008. (See its Corrigendum in vol.21, issue 9, page 1418).</p>
<p>[13] Sprinkhuizen-Kuyper, I.G. & Boers, E.J.W. The error surface of the 2-2-1 XOR network: The ﬁnite stationary points. Neural Networks, 11:683–690, 1998.</p>
<p>[14] Tsaih, R.-H. An Improved Back Propagation Neural Network Learning Algorithm. Ph.D thesis at the Department of Industrial Engineering and Operations Research, University of California at Berkeley, pp.67–70, 1991.</p>
<p>[15] Sprinkhuizen-Kuyper, I.G. & Boers, E.J.W. A comment on a paper of Blum: Blum’s local minima are saddle points. Tech. Rep. No. 94-34, Leiden University, Department of Computer Science, Leiden, The Netherlands, 1994.</p>
<p>[16] Gori, M. & Tesi, A. Some examples of local minima during learning with backpropagation. Third Italian Workshop on Parallel Architectures and Neural Networks. (Ed. by E.R. Caianiello), World Scientiﬁc Publishing Co., pp. 87–94, 1990.</p>
<p>[17] Gori, M. & Tesi, A. On the Problem of Local Minima in Backpropagation. IEEE Trans. on Pattern Analysis and Machine Intelligence, 14(1):76-86, 1992.</p>
<p>[18] Nocedal, J & Wright, S.J. Numerical Optimization. Springer Verlag, 1999.</p>
<p>[19] Byrd, R.H., Schnabel, R.B. & Schultz, G.A. Approximate solution of the trust region problems by minimization over two-dimensional subspaces. Mathematical Programming, 40:247–263, 1988.</p>
<p>[20] Mizutani, E. & Demmel, J.W. Iterative scaled trust-region learning in Krylov subspaces via Pearlmutter’s ¨ implicit sparse Hessian-vector multiply. In S. Thrun, L. Saul, and B. Sch olkopf, editors, Advances in Neural Information Processing Systems, MIT Press, 16:209–216, 2004.</p>
<p>[21] Gould, N.I.M., Lucidi, S., Roma, M. & Toint, Ph.L. Solving the trust-region subproblem using the Lanczos method. SIAM Journal on Optimization, 9(2):504–525, 1999.</p>
<p>[22] Rojas, M., Santos, S.A. & Sorensen, D.C. A New Matrix-Free Algorithm for the Large-Scale TrustRegion Subproblem. SIAM Journal on Optimization, 11(3):611–646, 2000.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
