<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-193" href="../nips2010/nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">nips2010-193</a> <a title="nips-2010-193-reference" href="#">nips2010-193-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</h1>
<br/><p>Source: <a title="nips-2010-193-pdf" href="http://papers.nips.cc/paper/4116-online-learning-random-averages-combinatorial-parameters-and-learnability.pdf">pdf</a></p><p>Author: Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: We develop a theory of online learning by deﬁning several complexity measures. Among them are analogues of Rademacher complexity, covering numbers and fatshattering dimension from statistical learning theory. Relationship among these complexity measures, their connection to online learning, and tools for bounding them are provided. We apply these results to various learning problems. We provide a complete characterization of online learnability in the supervised setting. 1</p><br/>
<h2>reference text</h2><p>[1] J. Abernethy, A. Agarwal, P. Bartlett, and A. Rakhlin. A stochastic view of optimal regret through minimax duality. In Proceedings of the 22nd Annual Conference on Learning Theory, 2009.</p>
<p>[2] J. Abernethy, P. L. Bartlett, A. Rakhlin, and A. Tewari. Optimal strategies and minimax lower bounds for online convex games. In COLT, pages 414–424, 2008.</p>
<p>[3] N. Alon, S. Ben-David, N. Cesa-Bianchi, and D. Haussler. Scale-sensitive dimensions, uniform convergence, and learnability. Journal of the ACM, 44:615–631, 1997.</p>
<p>[4] N. Alon and J. Spencer. The Probabilistic Method. John Wiley & Sons, 2nd edition, 2000.</p>
<p>[5] P. L. Bartlett, P. M. Long, and R. C. Williamson. Fat-shattering and the learnability of real-valued functions. Journal of Computer and System Sciences, 52(3):434–452, 1996. (special issue on COLT‘94).</p>
<p>[6] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: risk bounds and structural results. J. Mach. Learn. Res., 3:463–482, 2003.</p>
<p>[7] S. Ben-David, D. Pal, and S. Shalev-Shwartz. Agnostic online learning. In COLT, 2009.</p>
<p>[8] N. Cesa-Bianchi and G. Lugosi. On prediction of individual sequences. A. of S., pages 1865–1895, 1999.</p>
<p>[9] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.</p>
<p>[10] R. M. Dudley. The sizes of compact subsets of Hilbert space and continuity of Gaussian processes. Journal of Functional Analysis, 1(3):290–330, 1967.</p>
<p>[11] R. M. Dudley. Uniform Central Limit Theorems. Cambridge University Press, 1999.</p>
<p>[12] E. Gin´ and J. Zinn. Some limit theorems for empirical processes. Ann. of Prob., 12(4):929–989, 1984. e</p>
<p>[13] J. Hannan. Approximation to Bayes risk in repeated play. Contr. to Theo. of Games, 3:97–139, 1957.</p>
<p>[14] D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning applications. Information and Computation, 100(1):78–150, 1992.</p>
<p>[15] S. M. Kakade and A. Kalai. From batch to transductive online learning. In NIPS, 2005.</p>
<p>[16] A. Tauman Kalai and R. Sastry. The isotron algorithm: High-dimensional isotonic regression. In Proceedings of the 22th Annual Conference on Learning Theory, 2009.</p>
<p>[17] M. J. Kearns and R. E. Schapire. Efﬁcient distribution-free learning of probabilistic concepts. Journal of Computer and System Sciences, 48(3):464–497, 1994.</p>
<p>[18] V. Koltchinskii and D. Panchenko. Rademacher processes and bounding the risk of function learning. High Dimensional Probability II, 47:443–459, 2000.</p>
<p>[19] N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning, 2(4):285–318, 04 1988.</p>
<p>[20] P. Massart. Some applications of concentration inequalities to statistics. Annales de la Facult´ des Scie ences de Toulouse, IX(2):245–303, 2000.</p>
<p>[21] S. Mendelson. A few notes on statistical learning theory. In MLSS 2002, pages 1–40. 2003.</p>
<p>[22] S. Mendelson and R. Vershynin. Entropy and the combinatorial dimension. Inventiones mathematicae, 152:37–55, 2003.</p>
<p>[23] D. Pollard. Empirical Processes: Theory and Applications, volume 2. Hayward, CA, 1990.</p>
<p>[24] N. Sauer. On the density of families of sets. J. Combinatorial Theory, 13:145–147, 1972.</p>
<p>[25] S. Shalev-Shwartz and Y. Singer. Convex repeated games and fenchel duality. In NIPS, pages 1265–1272. MIT Press, Cambridge, MA, 2007.</p>
<p>[26] S. Shelah. A combinatorial problem: Stability and order for models and theories in inﬁnitary languages. Pac. J. Math, 4:247–261, 1972.</p>
<p>[27] K. Sridharan and A. Tewari. Convex games in banach spaces. In COLT, 2010.</p>
<p>[28] A. W. Van Der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes : With Applications to Statistics. Springer Series, March 1996.</p>
<p>[29] L. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142, 1984.</p>
<p>[30] S.A. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press, 2000.</p>
<p>[31] V. N. Vapnik. Estimation of Dependences Based on Empirical Data (Springer Series in Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA, 1982.</p>
<p>[32] V. N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications, 16(2):264–280, 1971.</p>
<p>[33] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In ICML, pages 928–936, 2003.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
