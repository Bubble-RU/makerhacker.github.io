<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-238" href="../nips2010/nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">nips2010-238</a> <a title="nips-2010-238-reference" href="#">nips2010-238-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</h1>
<br/><p>Source: <a title="nips-2010-238-pdf" href="http://papers.nips.cc/paper/3980-short-term-memory-in-neuronal-networks-through-dynamical-compressed-sensing.pdf">pdf</a></p><p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><br/>
<h2>reference text</h2><p>[1] J.J. Hopﬁeld. Neural networks and physical systems with emergent collective computational abilities. PNAS, 79(8):2554, 1982.</p>
<p>[2] W. Maass, T. Natschlager, and H. Markram. Real-time computing without stable states: A new framework for neural computation based on perturbations. Neural computation, 14(11):2531– 2560, 2002.</p>
<p>[3] H. Jaeger and H. Haas. Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication. Science, 304(5667):78, 2004.</p>
<p>[4] H. Jaeger. Short term memory in echo state networks. GMD Report 152 German National Research Center for Information Technology, 2001.</p>
<p>[5] O.L. White, D.D. Lee, and H. Sompolinsky. Short-term memory in orthogonal neural networks. Phys. Rev. Lett., 92(14):148102, 2004.</p>
<p>[6] S. Ganguli, D. Huh, and H. Sompolinsky. Memory traces in dynamical systems. Proc. Natl. Acad. Sci., 105(48):18970, 2008.</p>
<p>[7] A.M. Bruckstein, D.L. Donoho, and M. Elad. From sparse solutions of systems of equations to sparse modeling of signals and images. Siam Review, 51(1):34–81, 2009.</p>
<p>[8] E. Candes and M. Wakin. An introduction to compressive sampling. IEEE Sig. Proc. Mag., 25(2):21–30, 2008.</p>
<p>[9] D.L. Donoho and M. Elad. Optimally sparse representation in general (non-orthogonal) dictionaries via l1 minimization. PNAS, 100:2197–2202, 2003.</p>
<p>[10] E. Candes, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE Trans. Inf. Theory, 52(2):489–509, 2006.</p>
<p>[11] E. Candes and T. Tao. Decoding by linear programming. IEEE Trans. Inf. Theory, 51:4203– 4215, 2005.</p>
<p>[12] D.L. Donoho and J. Tanner. Sparse nonnegative solution of underdetermined linear equations by linear programming. PNAS, 102:9446–51, 2005.</p>
<p>[13] D.L. Donoho and J. Tanner. Neighborliness of randomly projected simplices in high dimensions. PNAS, 102:9452–7, 2005.</p>
<p>[14] Y. Kabashima, T. Wadayama, and T. Tanaka. A typical reconstruction limit for compressed sensing based on l p-norm minimization. J. Stat. Mech., page L09003, 2009.</p>
<p>[15] S. Ganguli and H. Sompolinsky. Statistical mechanics of compressed sensing. Phys. Rev. Lett., 104(18):188701, 2010.</p>
<p>[16] M. Mezard, G. Parisi, and M.A. Virasoro. Spin glass theory and beyond. World scientiﬁc Singapore, 1987.</p>
<p>[17] S. Rangan, A.K. Fletcher, and Goyal V.K. Asymptotic analysis of map estimation via the replica method and applications to compressed sensing. CoRR, abs/0906.3234, 2009.</p>
<p>[18] D.L. Donoho, A. Maleki, and A. Montanari. Message-passing algorithms for compressed sensing. Proc. Natl. Acad. Sci., 106(45):18914, 2009.</p>
<p>[19] Y. Xia and M.S. Kamel. A cooperative recurrent neural network for solving l 1 estimation problems with general linear constraints. Neural computation, 20(3):844–872, 2008.</p>
<p>[20] C.J. Rozell, D.H. Johnson, R.G. Baraniuk, and B.A. Olshausen. Sparse coding via thresholding and local competition in neural circuits. Neural computation, 20(10):2526–2563, 2008.  9</p>
<br/>
<br/><br/><br/></body>
</html>
