<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-12" href="../nips2010/nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">nips2010-12</a> <a title="nips-2010-12-reference" href="#">nips2010-12-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</h1>
<br/><p>Source: <a title="nips-2010-12-pdf" href="http://papers.nips.cc/paper/3974-a-primal-dual-algorithm-for-group-sparse-regularization-with-overlapping-groups.pdf">pdf</a></p><p>Author: Sofia Mosci, Silvia Villa, Alessandro Verri, Lorenzo Rosasco</p><p>Abstract: We deal with the problem of variable selection when variables must be selected group-wise, with possibly overlapping groups deﬁned a priori. In particular we propose a new optimization procedure for solving the regularized algorithm presented in [12], where the group lasso penalty is generalized to overlapping groups of variables. While in [12] the proposed implementation requires explicit replication of the variables belonging to more than one group, our iterative procedure is based on a combination of proximal methods in the primal space and projected Newton method in a reduced dual space, corresponding to the active groups. This procedure provides a scalable alternative with no need for data duplication, and allows to deal with high dimensional problems without pre-processing for dimensionality reduction. The computational advantages of our scheme with respect to state-of-the-art algorithms using data duplication are shown empirically with numerical simulations. 1</p><br/>
<h2>reference text</h2><p>[1] F. Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine Learning Research, 9:1179–1225, 2008.</p>
<p>[2] F. Bach. High-dimensional non-linear variable selection through hierarchical kernel learning. Technical Report HAL 00413473, INRIA, 2009.</p>
<p>[3] F. R. Bach, G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the smo algorithm. In ICML, volume 69 of ACM International Conference Proceeding Series, 2004.</p>
<p>[4] A. Beck and Teboulle. M. Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems. IEEE Transactions on Image Processing, 18(11):2419– 2434, 2009.</p>
<p>[5] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM J. Imaging Sci., 2(1):183–202, 2009.</p>
<p>[6] S. Becker, J. Bobin, and E. Candes. Nesta: A fast and accurate ﬁrst-order method for sparse recovery, 2009.</p>
<p>[7] D. Bertsekas. Projected newton methods for optimization problems with simple constraints. SIAM Journal on Control and Optimization, 20(2):221–246, 1982.</p>
<p>[8] R. Brayton and J. Cullum. An algorithm for minimizing a differentiable function subject to. J. Opt. Th. Appl., 29:521–558, 1979.</p>
<p>[9] J. Duchi and Y. Singer. Efﬁcient online and batch learning using forward backward splitting. Journal of Machine Learning Research, 10:28992934, December 2009.</p>
<p>[10] O. Guler. New proximal point algorithm for convex minimization. SIAM J. on Optimization, 2(4):649–664, 1992.</p>
<p>[11] E. T. Hale, W. Yin, and Y. Zhang. Fixed-point continuation for l1-minimization: Methodology and convergence. SIOPT, 19(3):1107–1130, 2008.</p>
<p>[12] L. Jacob, G. Obozinski, and J.-P. Vert. Group lasso with overlap and graph lasso. In ICML, page 55, 2009.</p>
<p>[13] R. Jenatton, J.-Y . Audibert, and F. Bach. Structured variable selection with sparsity-inducing norms. Technical report, INRIA, 2009.</p>
<p>[14] R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for sparse hierarchical dictionary learning. In Proceeding of ICML 2010, 2010.</p>
<p>[15] I. Loris. On the performance of algorithms for the minimization of l1 -penalized functionals. Inverse Problems, 25(3):035008, 16, 2009.</p>
<p>[16] L. Meier, S. van de Geer, and P. Buhlmann. The group lasso for logistic regression. J. R. Statist. Soc, B(70):53–71, 2008.</p>
<p>[17] S. Mosci, S. Villa, Verri A., and L. Rosasco. A fast algorithm for structured gene selection. presented at MLSB 2010, Edinburgh.</p>
<p>[18] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o(1/k 2 ). Doklady AN SSSR, 269(3):543–547, 1983.</p>
<p>[19] Y. Nesterov. Smooth minimization of non-smooth functions. 103(1):127–152, 2005.  Math. Prog. Series A,</p>
<p>[20] M. Y. Park and T. Hastie. L1-regularization path algorithm for generalized linear models. J. R. Statist. Soc. B, 69:659–677, 2007.</p>
<p>[21] L. Rosasco, M. Mosci, S. Santoro, A. Verri, and S. Villa. Iterative projection methods for structured sparsity regularization. Technical Report MIT-CSAIL-TR-2009-050, MIT, 2009.</p>
<p>[22] J. Rosen. The gradient projection method for nonlinear programming, part i: linear constraints. J. Soc. Ind. Appl. Math., 8:181–217, 1960.</p>
<p>[23] V. Roth and B. Fischer. The group-lasso for generalized linear models: uniqueness of solutions and efﬁcient algorithms. In Proceedings of 25th ICML, 2008.</p>
<p>[24] P. Zhao, G. Rocha, and B. Yu. The composite absolute penalties family for grouped and hierarchical variable selection. Annals of Statistics, 37(6A):3468–3497, 2009. 9</p>
<br/>
<br/><br/><br/></body>
</html>
