<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-85" href="../nips2010/nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">nips2010-85</a> <a title="nips-2010-85-reference" href="#">nips2010-85-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</h1>
<br/><p>Source: <a title="nips-2010-85-pdf" href="http://papers.nips.cc/paper/3981-exact-learning-curves-for-gaussian-process-regression-on-large-random-graphs.pdf">pdf</a></p><p>Author: Matthew Urry, Peter Sollich</p><p>Abstract: We study learning curves for Gaussian process regression which characterise performance in terms of the Bayes error averaged over datasets of a given size. Whilst learning curves are in general very difﬁcult to calculate we show that for discrete input domains, where similarity between input points is characterised in terms of a graph, accurate predictions can be obtained. These should in fact become exact for large graphs drawn from a broad range of random graph ensembles with arbitrary degree distributions where each input (node) is connected only to a ﬁnite number of others. Our approach is based on translating the appropriate belief propagation equations to the graph ensemble. We demonstrate the accuracy of the predictions for Poisson (Erdos-Renyi) and regular random graphs, and discuss when and why previous approximations of the learning curve fail. 1</p><br/>
<h2>reference text</h2><p>[1] Carl E. Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). MIT Press, December 2005.</p>
<p>[2] Shun-ichi Amari, Naotake Fujita, and Shigeru Shinomoto. Four types of learning curves. Neural Computation, 4(4):605–618, 1992.</p>
<p>[3] M. Opper. Regression with Gaussian processes: Average case performance. Theoretical Aspects of Neural Computation: A Multidisciplinary Perspective. Springer-Verlag, pages 17–23, 1997.</p>
<p>[4] P. Sollich. Learning curves for Gaussian processes. In Advances in Neural Information Processing Systems 11, pages 344–350. MIT Press, 1999.</p>
<p>[5] F. Vivarelli and M. Opper. General bounds on Bayes errors for regression with Gaussian processes. In Advances in Neural Information Processing Systems 11, pages 302–308. MIT Press, 1999.</p>
<p>[6] C. K. I. Williams and F. Vivarelli. Upper and lower bounds on the learning curve for Gaussian processes. Machine Learning, 40(1):77–102, 2000.</p>
<p>[7] M. Opper and D. Malzahn. Learning curves for gaussian processes regression: A framework for good approximations. In Advances in Neural Information Processing Systems 14, pages 273–279. MIT Press, 2001.  8</p>
<p>[8] M. Opper and D. Malzahn. A variational approach to learning curves. In Advances in Neural Information Processing Systems 14, pages 463–469. MIT Press, 2002.</p>
<p>[9] P. Sollich and A. Halees. Learning curves for Gaussian process regression: Approximations and bounds. Neural Computation, 14(6):1393–1428, 2002.</p>
<p>[10] P. Sollich. Gaussian process regression with mismatched models. In Advances in Neural Information Processing Systems 14, pages 519–526. MIT Press, 2002.</p>
<p>[11] P. Sollich. Can Gaussian process regression be made robust against model mismatch? In N Lawrence J Winkler and M Niranjan, editors, Deterministic and Statistical Methods in Machine Learning, pages 211–228, Berlin, 2005. Springer.</p>
<p>[12] P. Sollich, M. J. Urry, and C. Coti. Kernels and learning curves for Gaussian process regression on random graphs. In Advances in Neural Information Processing Systems 22, pages 1723–1731. Curran Associates, Inc., 2009.</p>
<p>[13] M. Herbster, M. Pontil, and L. Wainer. Online learning over graphs. In ICML ’05: Proceedings of the 22nd international conference on Machine learning, pages 305–312, New York, NY, USA, 2005. ACM.</p>
<p>[14] M. Herbster and M. Pontil. Prediction on a graph with a perceptron. In Advances in Neural Information Processing Systems 19, pages 577–584. MIT Press, 2007.</p>
<p>[15] M. Herbster. Exploiting cluster-structure to predict the labeling of a graph. In Proceedings of the 19th international conference on Algorithmic Learning Theory, pages 54–69. Springer, 2008.</p>
<p>[16] M. Belkin, I. Matveeva, and P. Niyogi. Regularization and semi-supervised learning on large graphs. Learning theory, 3120:624–638, 2004.</p>
<p>[17] Tim Rogers, Koujin Takeda, Issac P´ rez Castillo, and Reimer K¨ hn. Cavity approach to the spectral e u density of sparse symmetric random matricies. Physical Review E, 78(3):31116–31121, 2008.</p>
<p>[18] M. Mezard, G. Parisi, and M. A. Virasoro. Random free energies in spin glasses. Le journal de physique - lettres, 46(6):217–222, 1985.</p>
<p>[19] M. T. Farrell and A. Correa. Gaussian process regression models for predicting stock trends. Relation, 10:3414, 2007.</p>
<p>[20] B. Ferris, D. Haehnel, and D. Fox. Gaussian processes for signal strength-based location estimation. In Proceedings of Robotics: Science and Systems, Philadelphia, USA, August 2006.</p>
<p>[21] Sunho Park and Seungjin Choi. Gaussian process regression for voice activity detection and speech enhancement. In International Joint Conference on Neural Networks, pages 2879–2882, Hong Kong, China, 2008. Institute of Electrical and Electronics Engineers (IEEE).</p>
<p>[22] A. J. Smola and R. Kondor. Kernels and regularization on graphs. In M. Warmuth and B. Scholkopf, editors, Learning theory and Kernel machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop (COLT), pages 144–158, Heidelberg, 2003. Springer.</p>
<p>[23] M. Opper and D. Saad. Advanced mean ﬁeld methods: Theory and practice. MIT Press, 2001.</p>
<p>[24] Reimer K¨ hn. Finitely coordinated models for low-temperature phases of amorphous systems. Journal u of Physics A, 40(31):9227, 2007.</p>
<p>[25] M. M´ zard and G. Parisi. The Bethe lattice spin glass revisited. The European Physical Journal B, e 20(2):217–233, 2001.</p>
<p>[26] P. Erd¨ s and A. R´ nyi. On random graphs, I. Publicationes Mathematicae (Debrecen), 6:290–297, 1959. o e</p>
<p>[27] Tim Rogers, Conrad P´ rez Vicente, Koujin Takeda, and Isaac P´ rez Castillo. Spectral density of random e e graphs with topological constraints. Journal of Physics A, 43(19):195002, 2010.</p>
<p>[28] Kian Ming Chai. Generalization errors and learning curves for regression with multi-task Gaussian processes. In Advances in Neural Information Processing Systems 22, pages 279–287. Curran Associates, Inc., 2009.</p>
<p>[29] M. Alvarez, D. Luengo, and N. D. Lawrence. Latent force models. In D. van Dyk and M. Welling, editors, Proceedings of the Twelfth International Workshop on Artiﬁcial Intelligence and Statistics, pages 9–16, Clearwater Beach, FL, USA, 2009. MIT Press.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
