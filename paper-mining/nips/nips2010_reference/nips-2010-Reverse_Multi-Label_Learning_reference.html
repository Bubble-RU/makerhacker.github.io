<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>228 nips-2010-Reverse Multi-Label Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-228" href="../nips2010/nips-2010-Reverse_Multi-Label_Learning.html">nips2010-228</a> <a title="nips-2010-228-reference" href="#">nips2010-228-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>228 nips-2010-Reverse Multi-Label Learning</h1>
<br/><p>Source: <a title="nips-2010-228-pdf" href="http://papers.nips.cc/paper/3920-reverse-multi-label-learning.pdf">pdf</a></p><p>Author: James Petterson, Tibério S. Caetano</p><p>Abstract: Multi-label classiﬁcation is the task of predicting potentially multiple labels for a given instance. This is common in several applications such as image annotation, document classiﬁcation and gene function prediction. In this paper we present a formulation for this problem based on reverse prediction: we predict sets of instances given the labels. By viewing the problem from this perspective, the most popular quality measures for assessing the performance of multi-label classiﬁcation admit relaxations that can be efﬁciently optimised. We optimise these relaxations with standard algorithms and compare our results with several stateof-the-art methods, showing excellent performance. 1</p><br/>
<h2>reference text</h2><p>[1] Krzysztof Dembczynski, Weiwei Cheng, and Eyke H¨ llermeier. Bayes Optimal Multilabel u Classiﬁcation via Probabilistic Classiﬁer Chains. In Proc. Intl. Conf. Machine Learning, 2010.</p>
<p>[2] Xinhua Zhang, T. Graepel, and Ralf Herbrich. Bayesian Online Learning for Multi-label and Multi-variate Performance Measures. In Proc. Intl. Conf. on Artiﬁcial Intelligence and Statistics, volume 9, pages 956–963, 2010.</p>
<p>[3] Piyush Rai and Hal Daume. Multi-Label Prediction via Sparse Inﬁnite CCA. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 1518–1526. 2009.</p>
<p>[4] Jesse Read, Bernhard Pfahringer, Geoffrey Holmes, and Eibe Frank. Classiﬁer chains for multi-label classiﬁcation. In Wray L. Buntine, Marko Grobelnik, Dunja Mladenic, and John Shawe-Taylor, editors, ECML/PKDD (2), volume 5782 of Lecture Notes in Computer Science, pages 254–269. Springer, 2009.</p>
<p>[5] Andr´ Elisseeff and Jason Weston. A kernel method for multi-labelled classiﬁcation. In Annual e ACM Conference on Research and Development in Information Retrieval, pages 274–281, 2005.</p>
<p>[6] Matthieu Guillaumin, Thomas Mensink, Jakob Verbeek, and Cordelia Schmid. TagProp: Discriminative Metric Learning in Nearest Neighbor Models for Image Auto-Annotation. In Proc. Intl. Conf. Computer Vision, 2009.</p>
<p>[7] Douglas W. Oard and Jason R. Baron. Overview of the TREC 2008 Legal Track.</p>
<p>[8] Linli Xu, Martha White, and Dale Schuurmans. Optimal reverse prediction. Proc. Intl. Conf. Machine Learning, pages 1–8, 2009.</p>
<p>[9] Grigorios Tsoumakas, Ioannis Katakis, and Ioannis P. Vlahavas. Mining Multi-label Data. Springer, 2009.</p>
<p>[10] Grigorios Tsoumakas and Ioannis P. Vlahavas. Random k-labelsets: An ensemble method for multilabel classiﬁcation. In Proceedings of the 18th European Conference on Machine Learning (ECML 2007), pages 406–417, Warsaw, Poland, 2007.</p>
<p>[11] Jesse Read, Bernhard Pfahringer, and Geoff Holmes. Multi-label classiﬁcation using ensembles of pruned sets. In ICDM ’08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 995–1000, Washington, DC, USA, 2008. IEEE Computer Society.</p>
<p>[12] Shantanu Godbole and Sunita Sarawagi. Discriminative methods for multi-labeled classiﬁcation. In Proceedings of the 8th Paciﬁc-Asia Conference on Knowledge Discovery and Data Mining, pages 22–30. Springer, 2004.</p>
<p>[13] Martin Jansche. Maximum expected F-measure training of logistic regression models. HLT, pages 692–699, 2005.</p>
<p>[14] T. Joachims. A support vector method for multivariate performance measures. In Proc. Intl. Conf. Machine Learning, pages 377–384, San Francisco, California, 2005. Morgan Kaufmann Publishers.</p>
<p>[15] V. Vapnik. Statistical Learning Theory. John Wiley and Sons, New York, 1998.</p>
<p>[16] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and interdependent output variables. J. Mach. Learn. Res., 6:1453–1484, 2005.</p>
<p>[17] D. E. Knuth. The Art of Computer Programming: Fundamental Algorithms, volume 1. Addison-Wesley, Reading, Massachusetts, second edition, 1998.</p>
<p>[18] Choon Hui Teo, S. V. N. Vishwanathan, Alex J. Smola, and Quoc V. Le. Bundle methods for regularized risk minimization. Journal of Machine Learning Research, 11:311–365, 2010.</p>
<p>[19] Robert E. Schapire and Y. Singer. Improved boosting algorithms using conﬁdence-rated predictions. Machine Learning, 37(3):297–336, 1999.</p>
<p>[20] Min-Ling Zhang and Zhi-Hua Zhou. ML-KNN: A lazy learning approach to multi-label learning. Pattern Recognition, 40(7):2038–2048, July 2007.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
