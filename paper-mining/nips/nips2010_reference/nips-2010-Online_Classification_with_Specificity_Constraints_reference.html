<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>192 nips-2010-Online Classification with Specificity Constraints</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-192" href="../nips2010/nips-2010-Online_Classification_with_Specificity_Constraints.html">nips2010-192</a> <a title="nips-2010-192-reference" href="#">nips2010-192-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>192 nips-2010-Online Classification with Specificity Constraints</h1>
<br/><p>Source: <a title="nips-2010-192-pdf" href="http://papers.nips.cc/paper/3896-online-classification-with-specificity-constraints.pdf">pdf</a></p><p>Author: Andrey Bernstein, Shie Mannor, Nahum Shimkin</p><p>Abstract: We consider the online binary classiﬁcation problem, where we are given m classiﬁers. At each stage, the classiﬁers map the input to the probability that the input belongs to the positive class. An online classiﬁcation meta-algorithm is an algorithm that combines the outputs of the classiﬁers in order to attain a certain goal, without having prior knowledge on the form and statistics of the input, and without prior knowledge on the performance of the given classiﬁers. In this paper, we use sensitivity and speciﬁcity as the performance metrics of the meta-algorithm. In particular, our goal is to design an algorithm that satisﬁes the following two properties (asymptotically): (i) its average false positive rate (fp-rate) is under some given threshold; and (ii) its average true positive rate (tp-rate) is not worse than the tp-rate of the best convex combination of the m given classiﬁers that satisﬁes fprate constraint, in hindsight. We show that this problem is in fact a special case of the regret minimization problem with constraints, and therefore the above goal is not attainable. Hence, we pose a relaxed goal and propose a corresponding practical online learning meta-algorithm that attains it. In the case of two classiﬁers, we show that this algorithm takes a very simple form. To our best knowledge, this is the ﬁrst algorithm that addresses the problem of the average tp-rate maximization under average fp-rate constraints in the online setting. 1</p><br/>
<h2>reference text</h2><p>[1] Y. Amit, S. Shalev-Shwartz, and Y. Singer. Online classiﬁcation for complex problems using simultaneous projections. In NIPS 2006.</p>
<p>[2] D. Blackwell. Controlled random walks. In Proceedings of the International Congress of Mathematicians, volume III, pages 335–338, 1954.</p>
<p>[3] D. Blackwell. An analog of the minimax theorem for vector payoffs. Paciﬁc Journal of Mathematics, 6:1–8, 1956.</p>
<p>[4] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, New York, NY, USA, 2006.</p>
<p>[5] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551–585, 2006.</p>
<p>[6] T. Fawcett. An introduction to ROC analysis. Pattern Recognition Letters, 27(8):861–874, 2006.</p>
<p>[7] Y. Freund and R.E. Schapire. Adaptive game playing using multiplicative weights. Games and Economic Behavior, 29(12):79–103, 1999.</p>
<p>[8] J. Hannan. Approximation to Bayes risk in repeated play. Contributions to the Theory of Games, 3:97–139, 1957.</p>
<p>[9] E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2-3):169–192, 2007.</p>
<p>[10] N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Computation, 108(2):212–261, 1994.</p>
<p>[11] S. Mannor, J. N. Tsitsiklis, and J. Y. Yu. Online learning with sample path constraints. Journal of Machine Learning Research, 10:569–590, 2009.</p>
<p>[12] N. Shimkin. Stochastic games with average cost constraints. Annals of the International Society of Dynamic Games, Vol. 1: Advances in Dynamic Games and Applications, 1994.</p>
<p>[13] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML ’03), pages 928–936, 2003.  9</p>
<br/>
<br/><br/><br/></body>
</html>
