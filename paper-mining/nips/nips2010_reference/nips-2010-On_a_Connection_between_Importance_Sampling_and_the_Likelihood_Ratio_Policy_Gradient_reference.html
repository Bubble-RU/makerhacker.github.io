<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-189" href="../nips2010/nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">nips2010-189</a> <a title="nips-2010-189-reference" href="#">nips2010-189-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</h1>
<br/><p>Source: <a title="nips-2010-189-pdf" href="http://papers.nips.cc/paper/3922-on-a-connection-between-importance-sampling-and-the-likelihood-ratio-policy-gradient.pdf">pdf</a></p><p>Author: Tang Jie, Pieter Abbeel</p><p>Abstract: Likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems. We describe how the likelihood ratio policy gradient can be derived from an importance sampling perspective. This derivation highlights how likelihood ratio methods under-use past experience by (i) using the past experience to estimate only the gradient of the expected return U (θ) at the current policy parameterization θ, rather than to obtain a more complete estimate of U (θ), and (ii) using past experience under the current policy only rather than using all past experience to improve the estimates. We present a new policy search method, which leverages both of these observations as well as generalized baselines—a new technique which generalizes commonly used baseline techniques for policy gradient methods. Our algorithm outperforms standard likelihood ratio policy gradient algorithms on several testbeds. 1</p><br/>
<h2>reference text</h2><p>[1] J. Peters, S. Vijayakumar, and S. Schaal. Natural actor-critic. In Proceedings of the European Machine Learning Conference (ECML), 2005.</p>
<p>[2] T. Mori, Y. Nakamura, M. Sato, and S. Ishii. Reinforcement learning for cpg-driven biped robot. In AAAI, 2004.</p>
<p>[3] R. Tedrake, T. W. Zhang, and H.S. Seung. Learning to walk in 20 minutes. In Proceedings of the Fourteenth Yale Workshop on Adaptive and Learning Systems, 2005.</p>
<p>[4] N. Kohl and P. Stone. Policy gradient reinforcement learning for fast quadrupedal locomotion, 2004.</p>
<p>[5] J. Zico Kolter and Andrew Y. Ng. Learning omnidirectional path following using dimensionality reduction. RSS, 2007.</p>
<p>[6] P. Glynn. Likelihood Ratio Gradient Estimation: An Overview”. In Proceedings of the 1987 Winter Simulation Conference, Atlanta, GA, 1987.</p>
<p>[7] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:23, 1992.</p>
<p>[8] J. Baxter and P. Bartlett. Direct gradient-based reinforcement learning. Journal of Artiﬁcial Intelligence Research, 1999.</p>
<p>[9] E. Greensmith, P. Bartlett, and J. Baxter. Variance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learning Research, 2004.</p>
<p>[10] Leonid Peshkin and Christian R. Shelton. Learning from scarce experience. In Proceedings of the Nineteenth International Conference on Machine Learning, 2002.</p>
<p>[11] R. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning. In NIPS 13, 2000.</p>
<p>[12] J. Peters and S. Schaal. Policy gradient methods for robotics. In Proceedings of the IEEE International Conference on Intelligent Robotics Systems, 2006.</p>
<p>[13] A. Kong, J. S. Liu, and W. H. Wong. Sequential imputations and Bayesian missing data problems. Journal of American Statistics Association, 89:278–288, 1994.</p>
<p>[14] Jens Kober and Jan Peters. Policy search for motor primitives in robotics. NIPS, 2008.</p>
<p>[15] Dimitri P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 2004.</p>
<p>[16] Richard S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting, 1991.</p>
<p>[17] Xi-Ren Cao. A basic formula for on-line policy-gradient algorithms. IEEE Transactions on Automatic Control, 50:696–699, 2005.</p>
<p>[18] Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational space control. In In: Proceedings of the International Conference on Machine Learning (ICML, pages 745–750, 2007.</p>
<p>[19] Andrew Ng and Michael Jordan. Pegasus: A policy search method for large mdps and pomdps. In In Proceedings of the Sixteenth Conference on Uncertainty in Artiﬁcial Intelligence, pages 406–415, 2000.</p>
<p>[20] S. Amari. Natural gradient works efﬁciently in learning. Neural Computation, 10, 1998.</p>
<p>[21] S. Kakade. A natural policy gradient. In Advances in Neural Information Processing Systems, volume 14 of 26, 2001.</p>
<p>[22] Nicolas Le Roux, Pierre-Antoine Manzagol, and Yoshua Bengio. Topmoumoute online natural gradient algorithm. NIPS, 2007.</p>
<p>[23] Jan Peters. Machine Learning of Motor Skills for Robotics. PhD thesis, University of Southern California, 2007.</p>
<p>[24] M. Riedmiller, J. Peters, and S. Schaal. Evaluation of policy gradient methods and variants on the cart-pole benchmark. In IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning, 2007.</p>
<p>[25] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
