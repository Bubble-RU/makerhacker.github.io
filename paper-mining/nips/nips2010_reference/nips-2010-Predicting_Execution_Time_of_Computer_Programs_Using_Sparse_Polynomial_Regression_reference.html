<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>211 nips-2010-Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-211" href="../nips2010/nips-2010-Predicting_Execution_Time_of_Computer_Programs_Using_Sparse_Polynomial_Regression.html">nips2010-211</a> <a title="nips-2010-211-reference" href="#">nips2010-211-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>211 nips-2010-Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</h1>
<br/><p>Source: <a title="nips-2010-211-pdf" href="http://papers.nips.cc/paper/4145-predicting-execution-time-of-computer-programs-using-sparse-polynomial-regression.pdf">pdf</a></p><p>Author: Ling Huang, Jinzhu Jia, Bin Yu, Byung-gon Chun, Petros Maniatis, Mayur Naik</p><p>Abstract: Predicting the execution time of computer programs is an important but challenging problem in the community of computer systems. Existing methods require experts to perform detailed analysis of program code in order to construct predictors or select important features. We recently developed a new system to automatically extract a large number of features from program execution on sample inputs, on which prediction models can be constructed without expert knowledge. In this paper we study the construction of predictive models for this problem. We propose the SPORE (Sparse POlynomial REgression) methodology to build accurate prediction models of program performance using feature data collected from program execution on sample inputs. Our two SPORE algorithms are able to build relationships between responses (e.g., the execution time of a computer program) and features, and select a few from hundreds of the retrieved features to construct an explicitly sparse and non-linear model to predict the response variable. The compact and explicitly polynomial form of the estimated model could reveal important insights into the computer program (e.g., features and their non-linear combinations that dominate the execution time), enabling a better understanding of the program’s behavior. Our evaluation on three widely used computer programs shows that SPORE methods can give accurate prediction with relative error less than 7% by using a moderate number of training data samples. In addition, we compare SPORE algorithms to state-of-the-art sparse regression algorithms, and show that SPORE methods, motivated by real applications, outperform the other methods in terms of both interpretability and prediction accuracy.</p><br/>
<h2>reference text</h2><p>[1] Caltech 101 Object Categories. http://www.vision.caltech.edu/Image_Datasets/ Caltech101/Caltech101.html.</p>
<p>[2] Event Dataset. http://vision.stanford.edu/lijiali/event_dataset/.</p>
<p>[3] ImageJ. http://rsbweb.nih.gov/ij/.</p>
<p>[4] Mahout. lucene.apache.org/mahout.</p>
<p>[5] Visual Object Classes Challenge 2008. http://pascallin.ecs.soton.ac.uk/challenges/ VOC/voc2008/.</p>
<p>[6] S. Chen, K. Joshi, M. A. Hiltunen, W. H. Sanders, and R. D. Schlichting. Link gradients: Predicting the impact of network latency on multitier applications. In INFOCOM, 2009.</p>
<p>[7] B.-G. Chun, L. Huang, S. Lee, P. Maniatis, and M. Naik. Mantis: Predicting system performance through program analysis and modeling. Technical Report, 2010. arXiv:1010.0019v1 [cs.PF].</p>
<p>[8] D. Donoho. For most large underdetermined systems of equations, the minimal 1-norm solution is the sparsest solution. Communications on Pure and Applied Mathematics, 59:797829, 2006.</p>
<p>[9] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32(2):407–499, 2002.</p>
<p>[10] J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1), 2010.</p>
<p>[11] A. Ganapathi, H. Kuno, U. Dayal, J. L. Wiener, A. Fox, M. Jordan, and D. Patterson. Predicting multiple metrics for queries: Better decisions enabled by machine learning. In ICDE, 2009.</p>
<p>[12] S. Goldsmith, A. Aiken, and D. Wilkerson. Measuring empirical computational complexity. In FSE, 2007.</p>
<p>[13] C. Gupta, A. Mehta, and U. Dayal. PQR: Predicting query execution times for autonomous workload management. In ICAC, 2008.</p>
<p>[14] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer, 2009.</p>
<p>[15] M. Isard, V. Prabhakaran, J. Currey, U. Wieder, K. Talwar, and A. Goldberg. Quincy: fair scheduling for distributed computing clusters. In Proceedings of SOSP’09, 2009.</p>
<p>[16] S.-J. Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinevsky. An interior-point method for large-scale l1-regularized least squares. IEEE Journal on Selected Topics in Signal Processing, 1(4):606–617, 2007.</p>
<p>[17] Z. Li, M. Zhang, Z. Zhu, Y. Chen, A. Greenberg, and Y.-M. Wang. WebProphet: Automating performance prediction for web services. In NSDI, 2010.</p>
<p>[18] H. Liu and X. Chen. Nonparametric greedy algorithm for the sparse learning problems. In NIPS 22, 2009.</p>
<p>[19] M. Osborne, B. Presnell, and B. Turlach. On the lasso and its dual. Journal of Computational and Graphical Statistics, 9(2):319–337, 2000.</p>
<p>[20] P. Ravikumar, J. Lafferty, H. Liu, and L. Wasserman. Sparse additive models. Journal of the Royal Statistical Society: Series B(Statistical Methodology), 71(5):1009–1030, 2009.</p>
<p>[21] P. Ravikumar, V. Vu, B. Yu, T. Naselaris, K. Kay, J. Gallant, and C. Berkeley. Nonparametric sparse hierarchical models describe v1 fmri responses to natural images. Advances in Neural Information Processing Systems (NIPS), 21, 2008.</p>
<p>[22] S. A. Seshia and A. Rakhlin. Game-theoretic timing analysis. In Proceedings of the IEEE/ACM International Conference on Computer-Aided Design (ICCAD), pages 575–582. IEEE Press, Nov. 2008.</p>
<p>[23] S. A. Seshia and A. Rakhlin. Quantitative analysis of systems using game-theoretic learning. ACM Transactions on Embedded Computing Systems (TECS), 2010. To appear.</p>
<p>[24] M. Tariq, A. Zeitoun, V. Valancius, N. Feamster, and M. Ammar. Answering what-if deployment and conﬁguration questions with wise. In ACM SIGCOMM, 2008.</p>
<p>[25] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal. Statist. Soc B., 1996.</p>
<p>[26] M. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using l1-constrained quadratic programming (Lasso). IEEE Trans. Information Theory, 55:2183–2202, 2009.</p>
<p>[27] T. Zhang. Adaptive forward-backward greedy algorithm for sparse learning with linear models. Advances in Neural Information Processing Systems, 22, 2008.</p>
<p>[28] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning Research, 7:2563, 2006.</p>
<p>[29] H. Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical Association, 101(476):1418–1429, 2006.  9</p>
<br/>
<br/><br/><br/></body>
</html>
