<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-130" href="../nips2010/nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">nips2010-130</a> <a title="nips-2010-130-reference" href="#">nips2010-130-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</h1>
<br/><p>Source: <a title="nips-2010-130-pdf" href="http://papers.nips.cc/paper/4090-interval-estimation-for-reinforcement-learning-algorithms-in-continuous-state-domains.pdf">pdf</a></p><p>Author: Martha White, Adam White</p><p>Abstract: The reinforcement learning community has explored many approaches to obtaining value estimates and models to guide decision making; these approaches, however, do not usually provide a measure of conﬁdence in the estimate. Accurate estimates of an agent’s conﬁdence are useful for many applications, such as biasing exploration and automatically adjusting parameters to reduce dependence on parameter-tuning. Computing conﬁdence intervals on reinforcement learning value estimates, however, is challenging because data generated by the agentenvironment interaction rarely satisﬁes traditional assumptions. Samples of valueestimates are dependent, likely non-normally distributed and often limited, particularly in early learning when conﬁdence estimates are pivotal. In this work, we investigate how to compute robust conﬁdences for value estimates in continuous Markov decision processes. We illustrate how to use bootstrapping to compute conﬁdence intervals online under a changing policy (previously not possible) and prove validity under a few reasonable assumptions. We demonstrate the applicability of our conﬁdence estimation algorithms with experiments on exploration, parameter estimation and tracking. 1</p><br/>
<h2>reference text</h2><p>[1] D.W.K. Andrews. The block-block bootstrap: Improved asymptotic reﬁnements. 72(3):673–700, 2004.  Econometrica,</p>
<p>[2] G.E.P. Box, G.M. Jenkins, and G.C. Reinsel. Time series analysis: forecasting and control. Holden-day San Francisco, 1976.</p>
<p>[3] R. I. Brafman and M. Tennenholtz. R-max - a general polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning Research, 3:213–231, 2002.</p>
<p>[4] A.C. Davison and DV Hinkley. Bootstrap methods and their application. Cambridge Univ Pr, 1997.</p>
<p>[5] E. Delage and S. Mannor. Percentile optimization for Markov decision processes with parameter uncertainty. Operations Research, 58(1):203, 2010.</p>
<p>[6] Y. Engel, S. Mannor, and R. Meir. Reinforcement learning with Gaussian processes. In Proceedings of the 22nd international conference on Machine learning, page 208. ACM, 2005.</p>
<p>[7] P Hall. The bootstrap and Edgeworth expansion. Springer Series in Statistics, Jan 1997.</p>
<p>[8] Peter Hall, Joel L. Horowitz, and Bing-Yi Jing. On blocking rules for the bootstrap with dependent data. Biometrika, 82(3):561–74, 1995.</p>
<p>[9] Todd Hester and Peter Stone. Generalized model learning for reinforcement learning in factored domains. In The Eighth International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2009.</p>
<p>[10] J.L. Horowitz. Bootstrap methods for Markov processes. Econometrica, 71(4):1049–1082, 2003.</p>
<p>[11] Leslie P. Kaelbling. Learning in Embedded Systems (Bradford Books). The MIT Press, May 1993.</p>
<p>[12] SN Lahiri. Edgeworth correction by moving blockbootstrap for stationary and nonstationary data. Exploring the Limits of Bootstrap, pages 183–214, 1992.</p>
<p>[13] S. Lee and PY Lai. Double block bootstrap conﬁdence intervals for dependent data. Biometrika, 2009.</p>
<p>[14] L. Li, M.L. Littman, and C.R. Mansley. Online exploration in least-squares policy iteration. In Proc. of The 8th Int. Conf. on Autonomous Agents and Multiagent Systems, volume 2, pages 733–739, 2009.</p>
<p>[15] H.R. Maei, C. Szepesv´ ri, S. Bhatnagar, and R.S. Sutton. Toward off-policy learning control with function a approximation. ICM (2010), 50, 2010.</p>
<p>[16] S. Mannor, D. Simester, P. Sun, and J.N. Tsitsiklis. Bias and variance in value function estimation. In Proceedings of the twenty-ﬁrst international conference on Machine learning, page 72. ACM, 2004.</p>
<p>[17] Lilyana Mihalkova and Raymond J. Mooney. Using active relocation to aid reinforcement learning. In FLAIRS Conference, pages 580–585, 2006.</p>
<p>[18] Peter Stone Nicholas K. Jong. Model-based exploration in continuous state spaces. In The 7th Symposium on Abstraction, Reformulation, and Approximation, July 2007.</p>
<p>[19] A. Nouri and M.L. Littman. Multi-resolution exploration in continuous spaces. In NIPS, pages 1209– 1216, 2008.</p>
<p>[20] Francois Rivest and Doina Precup. Combining td-learning with cascade-correlation networks. In ICML, ¸ pages 632–639, 2003.</p>
<p>[21] J. Shao and D. Tu. The jackknife and bootstrap. Springer, 1995.</p>
<p>[22] A.L. Strehl and M.L. Littman. An empirical evaluation of interval estimation for markov decision processes. In Proc. of the 16th Int. Conf. on Tools with Artiﬁcial Intelligence (ICTAI04), 2004.</p>
<p>[23] Alexander L. Strehl and Michael L. Littman. Online linear regression and its application to model-based reinforcement learning. In NIPS, 2007.</p>
<p>[24] R.S. Sutton. Adapting bias by gradient descent: An incremental version of delta-bar-delta. In Proceedings of the National Conference on Artiﬁcial Intelligence, pages 171–171, 1992.</p>
<p>[25] R.S. Sutton and A.G. Barto. Introduction to reinforcement learning. MIT Press Cambridge, USA, 1998.</p>
<p>[26] R.S. Sutton, A. Koop, and D. Silver. On the role of tracking in stationary environments. In Proceedings of the 24th international conference on Machine learning, page 878. ACM, 2007.</p>
<p>[27] Brian Tanner and Adam White. RL-Glue : Language-independent software for reinforcement-learning experiments. JMLR, 10:2133–2136, September 2009.</p>
<p>[28] B A. Turlach. Bandwidth selection in kernel density estimation: A review. In CORE and Institut de Statistique, 1993.</p>
<p>[29] J. Zvingelis. On bootstrap coverage probability with dependent data. Computer-Aided Econ., 2001.  9</p>
<br/>
<br/><br/><br/></body>
</html>
