<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>151 nips-2010-Learning from Candidate Labeling Sets</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-151" href="../nips2010/nips-2010-Learning_from_Candidate_Labeling_Sets.html">nips2010-151</a> <a title="nips-2010-151-reference" href="#">nips2010-151-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>151 nips-2010-Learning from Candidate Labeling Sets</h1>
<br/><p>Source: <a title="nips-2010-151-pdf" href="http://papers.nips.cc/paper/4041-learning-from-candidate-labeling-sets.pdf">pdf</a></p><p>Author: Jie Luo, Francesco Orabona</p><p>Abstract: In many real world applications we do not have access to fully-labeled training data, but only to a list of possible labels. This is the case, e.g., when learning visual classiﬁers from images downloaded from the web, using just their text captions or tags as learning oracles. In general, these problems can be very difﬁcult. However most of the time there exist different implicit sources of information, coming from the relations between instances and labels, which are usually dismissed. In this paper, we propose a semi-supervised framework to model this kind of problems. Each training sample is a bag containing multi-instances, associated with a set of candidate labeling vectors. Each labeling vector encodes the possible labels for the instances in the bag, with only one being fully correct. The use of the labeling vectors provides a principled way not to exclude any information. We propose a large margin discriminative formulation, and an efﬁcient algorithm to solve it. Experiments conducted on artiﬁcial datasets and a real-world images and captions dataset show that our approach achieves performance comparable to an SVM trained with the ground-truth labels, and outperforms other baselines.</p><br/>
<h2>reference text</h2><p>[1] S. Andrews, I. Tsochantaridis, and T. Hofmann. Support vector machines for multiple-instance learning. In Proc. NIPS, 2003.</p>
<p>[2] K. Barnard, P. Duygulu, D. Forsyth, N. de Freitas, D. Blei, and M. Jordan. Matching words and pictures. JMLR, 3:1107–1135, 2003.</p>
<p>[3] T. Berg, A. Berg, J. Edwards, and D. Forsyth. Who’s in the picture? In Proc. NIPS, 2004.</p>
<p>[4] D. P. Bertsekas. Convex Analysis and Optimization. Athena Scientiﬁc, 2003.</p>
<p>[5] R. C. Bunescu and R. J. Mooney. Multiple instance learning for sparse positive bags. In Proc. ICML, 2007.</p>
<p>[6] C. C. Chang and C. J. Lin. LIBSVM: A Library for Support Vector Machines, 2001. Software available at http://www.csie.ntu.edu.tw/˜cjlin/libsvm.</p>
<p>[7] O. Chapelle, A. Zien, and B. Sch¨ lkopf (Eds.). Semi-supervised Learning. MIT Press, 2006. o</p>
<p>[8] T. Cour, B. Sapp, C. Jordan, and B. Taskar. Learning from ambiguously labeled images. In Proc. CVPR, 2009.</p>
<p>[9] K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. JMLR, 2:265–292, 2001.</p>
<p>[10] T. G. Dietterich, R. H. Lathrop, T. Lozano-Perez, and A. Pharmaceutical. Solving the multipleinstance problem with axis-parallel rectangles. Artiﬁcial Intelligence, 39:31–71, 1997.</p>
<p>[11] R.-E. Fan, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and S. Sundarajan. LIBLINEAR: A library for large linear classiﬁcation. JMLR, 9:1871–1874, 2008.</p>
<p>[12] Y. Grandvalet. Logistic regression for partial labels. In Proc. IPMU, 2002.</p>
<p>[13] M. Guillaumin, J. Verbeek, and C. Schmid. Multiple instance metric learning from automatically labeled bags of faces. In Proc. ECCV, 2010.</p>
<p>[14] A. Gupta and L. Davis. Beyond nouns: Exploiting prepositions and comparative adjectives for learning visual classiﬁers. In Proc. ECCV, 2008.</p>
<p>[15] E. H¨ llermeier and J. Beringe. Learning from ambiguously labelled example. Intelligent Data u Analysis, 10:419–439, 2006.</p>
<p>[16] L. Jie, B. Caputo, and V. Ferrari. Who’s doing what: Joint modeling of names and verbs for simultaneous face and pose annotation. In Proc. NIPS, 2009.</p>
<p>[17] R. Jin and Z. Ghahramani. Learning with multiple labels. In Proc. NIPS, 2002.</p>
<p>[18] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal Estimated sub-GrAdient SOlver for SVM. In Proc. ICML, 2007.</p>
<p>[19] A. J. Smola, S. V. N. Vishwanathan, and T. Hofmann. Kernel methods for missing variables. In Proc. AISTAT, 2005.</p>
<p>[20] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and interdependent output variables. JMLR, 6:1453–1484, 2005.</p>
<p>[21] E.P Xing, A.Y. Ng, M.I. Jordan, and S. Russell. Distance metric learning with application to clustering with side-information. In Proc. NIPS, 2002.</p>
<p>[22] C.-N. Yu and T. Joachims. Learning structural svms with latent variables. In Proc. ICML, 2009.</p>
<p>[23] A. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15:915– 936, 2003.</p>
<p>[24] M.-L. Zhang and Z.-H. Zhou. M3 MIML: A maximum margin method for multi-instance multilabel learning. In Proc. ICDM, 2008.</p>
<p>[25] Z.-H. Zhou and M.-L. Zhang. Multi-instance multi-label learning with application to scene classiﬁcation. In Proc. NIPS, 2006.</p>
<p>[26] X. Zhu. Semi-supervised learning literature survey. Technical Report 1530, Computer Sciences, University of Wisconsin-Madison, 2005.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
