<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-200" href="../nips2010/nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">nips2010-200</a> <a title="nips-2010-200-reference" href="#">nips2010-200-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</h1>
<br/><p>Source: <a title="nips-2010-200-pdf" href="http://papers.nips.cc/paper/4020-over-complete-representations-on-recurrent-neural-networks-can-support-persistent-percepts.pdf">pdf</a></p><p>Author: Shaul Druckmann, Dmitri B. Chklovskii</p><p>Abstract: A striking aspect of cortical neural networks is the divergence of a relatively small number of input channels from the peripheral sensory apparatus into a large number of cortical neurons, an over-complete representation strategy. Cortical neurons are then connected by a sparse network of lateral synapses. Here we propose that such architecture may increase the persistence of the representation of an incoming stimulus, or a percept. We demonstrate that for a family of networks in which the receptive ﬁeld of each neuron is re-expressed by its outgoing connections, a represented percept can remain constant despite changing activity. We term this choice of connectivity REceptive FIeld REcombination (REFIRE) networks. The sparse REFIRE network may serve as a high-dimensional integrator and a biologically plausible model of the local cortical circuit. 1</p><br/>
<h2>reference text</h2><p>[1] B. A. Olshausen and D. J. Field, “Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images,” Nature, vol. 381, pp. 607–9, Jun 1996.</p>
<p>[2] M. Rehn and F. Sommer, “A network that uses few active neurones to code visual input predicts the diverse shapes of cortical receptive ﬁelds,” Journal of Computational Neuroscience, vol. 22, pp. 135–146, 2007. 10.1007/s10827-006-0003-9.</p>
<p>[3] P. J. Garrigues and B. A. Olshausen, “Learning horizontal connections in a sparse coding model of natural images,” Advances in Neural Information Processing Systems, vol. 20, pp. 505–512, 2008. 8</p>
<p>[4] O. Shriki, H. Sompolinsky, and D. D. Lee, “An information maximization approach to overcomplete and recurrent representations,” Advances in Neural Information Processing Systems, vol. 12, pp. 87–93, 2000.</p>
<p>[5] D. B. Chklovskii and A. A. Koulakov, “Maps in the brain: What can we learn from them?,” Annual Review of Neuroscience, vol. 27, no. 1, pp. 369–392, 2004.</p>
<p>[6] G. Major and D. Tank, “Persistent neural activity: prevalence and mechanisms,” Current opinion in neurobiology, vol. 14, no. 6, pp. 675–684, 2004.</p>
<p>[7] M. Goldman, “Memory without feedback in a neural network,” Neuron, vol. 61, no. 4, pp. 621– 634, 2009.</p>
<p>[8] A. Hyvarinen, J. Hurri, and P. O. Hoyer, Natural Image Statistics: A Probabilistic Approach to Early Computational Vision. Springer Publishing Company, Incorporated, 2009.</p>
<p>[9] O. Christensen, An Introduction to Frames and Riesz Bases. birkhauser, 2003.</p>
<p>[10] S. Mallat and Z. Zhang, “Matching pursuits with time-frequency dictionaries,” Signal Processing, IEEE Transactions on, vol. 41, pp. 3397 –3415, dec 1993.</p>
<p>[11] S. Chen, D. Donoho, and M. Saunders, “Atomic decomposition by basis pursuit,” SIAM review, vol. 43, no. 1, pp. 129–159, 2001.</p>
<p>[12] R. Tibshirani, “Regression shrinkage and selection via the lasso,” Journal of the Royal Statistical Society (Series B), vol. 58, pp. 267–288, 1996.</p>
<p>[13] C. J. Rozell, D. H. Johnson, R. G. Baraniuk, and B. A. Olshausen, “Sparse coding via thresholding and local competition in neural circuits,” Neural Comput, vol. 20, pp. 2526–63, 2008.</p>
<p>[14] V. Braitenberg and A. Sch¨ z, Cortex: Statistics and Geometry of Neuronal Connectivity. u Berlin, Germany: Springer, 1998. ISBN: 3-540-63816-4.</p>
<p>[15] S. Cannon, D. Robinson, and S. Shamma, “A proposed neural network for the integrator of the oculomotor system,” Biological Cybernetics, vol. 49, no. 2, pp. 127–136, 1983.</p>
<p>[16] H. Seung, “How the brain keeps the eyes still,” Proceedings of the National Academy of Sciences, vol. 93, no. 23, p. 13339, 1996.</p>
<p>[17] J. Kovavcevic and A. Chebira, “An introduction to frames,” Found. Trends Signal Process., vol. 2, no. 1, pp. 1–94, 2008.</p>
<p>[18] Y. Mishchenko, T. Hu, J. Spacek, J. Mendenhall, K. M. Harris, and D. B. Chklovskii, “Ultrastructural analysis of hippocampal neuropil from the connectomics perspective,” Neuron, vol. 67, no. 6, pp. 1009–1020, 2010.</p>
<p>[19] L. R. Varshney, P. J. Sj¨ str¨ m, and D. B. Chklovskii, “Optimal information storage in noisy o o synapses under resource constraints,” Neuron, vol. 52, no. 3, pp. 409 – 423, 2006.</p>
<p>[20] B. Cheng, J. Yang, S. Yan, Y. Fu, and T. Huang, “Learning with L1-Graph for Image Analysis,” IEEE Transactions on Image Processing, p. 1, 2010.</p>
<p>[21] E. Elhamifar and R. Vidal, “Sparse subspace clustering,” in CVPR, pp. 2790 –2797, 2009.</p>
<p>[22] R. Jenatton, J. Mairal, G. Obozinski, and F. Bach, “Proximal Methods for Sparse Hierarchical Dictionary Learning,” Proc. ICML, 2010.</p>
<p>[23] D. Zoran and Y. Weiss, “The” Tree-Dependent Components” of Natural Images are Edge Filters,” Advances in Neural Information Processing Systems, 2009.</p>
<p>[24] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online learning for matrix factorization and sparse coding,” Journal of Machine Learning Research, vol. 11, pp. 19–60, 2010.</p>
<p>[25] S. Song, P. J. Sj¨ str¨ m, M. Reigl, S. Nelson, and D. B. Chklovskii, “Highly nonrandom features o o of synaptic connectivity in local cortical circuits,” PLoS Biol, vol. 3, p. e68, Mar 2005.</p>
<p>[26] G. Yu, G. Sapiro, and S. Mallat, “Image modeling and enhancement via structured sparse model selection,” 2010.</p>
<p>[27] K. Kavukcuoglu, M. Ranzato, R. Fergus, and Y. LeCun, “Learning invariant features through topographic ﬁlter maps,” in Proc. International Conference on Computer Vision and Pattern Recognition (CVPR’09), IEEE, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
