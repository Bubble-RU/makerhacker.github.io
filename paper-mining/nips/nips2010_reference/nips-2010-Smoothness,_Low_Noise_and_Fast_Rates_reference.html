<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>243 nips-2010-Smoothness, Low Noise and Fast Rates</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-243" href="../nips2010/nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">nips2010-243</a> <a title="nips-2010-243-reference" href="#">nips2010-243-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>243 nips-2010-Smoothness, Low Noise and Fast Rates</h1>
<br/><p>Source: <a title="nips-2010-243-pdf" href="http://papers.nips.cc/paper/3894-smoothness-low-noise-and-fast-rates.pdf">pdf</a></p><p>Author: Nathan Srebro, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: √ ˜ We establish an excess risk bound of O HR2 + HL∗ Rn for ERM with an H-smooth loss n function and a hypothesis class with Rademacher complexity Rn , where L∗ is the best risk achievable by the hypothesis class. For typical hypothesis classes where Rn = R/n, this translates to ˜ ˜ a learning rate of O (RH/n) in the separable (L∗ = 0) case and O RH/n + L∗ RH/n more generally. We also provide similar guarantees for online and stochastic convex optimization of a smooth non-negative objective. 1</p><br/>
<h2>reference text</h2><p>[1] N. Alon, S. Ben-David, N. Cesa-Bianchi, and D. Haussler. Scale-sensitive dimensions, uniform convergence, and learnability. FOCS, 0:292–301, 1993.</p>
<p>[2] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. JMLR, 3:463– 482, 2002.</p>
<p>[3] P.L. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. Annals of Statistics, 33(4):1497–1537, 2005.</p>
<p>[4] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31:167–175, 2003.</p>
<p>[5] P.J. Bickel, Y. Ritov, and A.B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. The Annals of Statistics, 37(4):1705–1732, 2009.</p>
<p>[6] O. Bousquet. Concentration Inequalities and Empirical Processes Theory Applied to the Analysis of Learning Algorithms. PhD thesis, Ecole Polytechnique, 2002.</p>
<p>[7] Olivier Bousquet and Andr´ Elisseeff. Stability and generalization. J. Mach. Learn. Res., 2:499–526, 2002. e</p>
<p>[8] N. Cesa-Bianchi, A. Conconi, and C.Gentile. On the generalization ability of on-line learning algorithms. In NIPS, pages 359–366, 2002.</p>
<p>[9] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press, 2006.</p>
<p>[10] Christophe Chesneau and Guillaume Lecu. Adapting to unknown smoothness by aggregation of thresholded wavelet estimators. 2006.</p>
<p>[11] S.M. Kakade, K. Sridharan, and A. Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In NIPS, 2008.</p>
<p>[12] V. Koltchinskii. Sparsity in penalized empirical risk minimization. Ann. Inst, H. Poincar´ Probab. Statist., 45(1):7–57, 2009. e</p>
<p>[13] V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization error of combined classiﬁers. Ann. of Stats., 30(1):1–50, 2002.</p>
<p>[14] G. Lan. Convex Optimization Under Inexact First-order Information. PhD thesis, Georgia Institute of Technology, 2009.</p>
<p>[15] J. Langford and J. Shawe-Taylor. PAC-Bayes & margins. In Advances in Neural Information Processing Systems 15, pages 423–430, 2003.</p>
<p>[16] Wee Sun Lee, Peter L. Bartlett, and Robert C. Williamson. The importance of convexity in learning with squared loss. IEEE Trans. on Information Theory, 1998.</p>
<p>[17] P. Liang, F. Bach, G. Bouchard, and M. I. Jordan. Asymptotically optimal regularization in smooth parametric models. In NIPS, 2010.</p>
<p>[18] P. Liang and N. Srebro. On the interaction between norm and dimensionality: Multiple regimes in learning. In ICML, 2010.</p>
<p>[19] D. A. McAllester. Simpliﬁed PAC-Bayesian margin bounds. In COLT, pages 203–215, 2003.</p>
<p>[20] Shahar Mendelson. Rademacher averages and phase transitions in glivenko-cantelli classes. IEEE Trans. On Information Theory, 48(1):251–263, 2002.</p>
<p>[21] A. Nemirovski and D. Yudin. Problem complexity and method efﬁciency in optimization. Nauka Publishers, Moscow, 1978.</p>
<p>[22] D. Panchenko. Some extensions of an inequality of vapnik and chervonenkis. Electronic Communications in Probability, 7:55–65, 2002.</p>
<p>[23] David Pollard. Convergence of Stochastic Processes. Springer-Verlag, 1984.</p>
<p>[24] S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Stochastic convex optimization. In COLT, 2009.</p>
<p>[25] S. Shalev-Shwartz, N. Srebro, and T. Zhang. Trading accuracy for sparsity. Technical report, TTI-C, 2009. Available at ttic.uchicago.edu/∼shai.</p>
<p>[26] S.Shalev-Shwartz. Online Learning: Theory, Algorithms, and Applications. PhD thesis, Hebrew University of Jerusalem, 2007.</p>
<p>[27] I. Steinwart and C. Scovel. Fast rates for support vector machines using gaussian kernels. ANNALS OF STATISTICS, 35:575, 2007.</p>
<p>[28] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal. Statist. Soc B., 58(1):267–288, 1996.</p>
<p>[29] A. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Annals of Statistics, 32:135–166, 2004.</p>
<p>[30] S. A. van de Geer. High-dimensional generalized linear models and the lasso. Annals of Statistics, 36(2):614–645, 2008.</p>
<p>[31] C. Zalinescu. Convex analysis in general vector spaces. World Scientiﬁc Publishing Co. Inc., River Edge, NJ, 2002.</p>
<p>[32] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In ICML, 2003.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
