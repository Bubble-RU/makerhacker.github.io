<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>80 nips-2010-Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-80" href="../nips2010/nips-2010-Estimation_of_Renyi_Entropy_and_Mutual_Information_Based_on_Generalized_Nearest-Neighbor_Graphs.html">nips2010-80</a> <a title="nips-2010-80-reference" href="#">nips2010-80-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>80 nips-2010-Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs</h1>
<br/><p>Source: <a title="nips-2010-80-pdf" href="http://papers.nips.cc/paper/4072-estimation-of-renyi-entropy-and-mutual-information-based-on-generalized-nearest-neighbor-graphs.pdf">pdf</a></p><p>Author: Barnabás Póczos, Csaba Szepesvári, David Tax</p><p>Abstract: We present simple and computationally efﬁcient nonparametric estimators of R´ nyi entropy and mutual information based on an i.i.d. sample drawn from an e unknown, absolutely continuous distribution over Rd . The estimators are calculated as the sum of p-th powers of the Euclidean lengths of the edges of the ‘generalized nearest-neighbor’ graph of the sample and the empirical copula of the sample respectively. For the ﬁrst time, we prove the almost sure consistency of these estimators and upper bounds on their rates of convergence, the latter of which under the assumption that the density underlying the sample is Lipschitz continuous. Experiments demonstrate their usefulness in independent subspace analysis. 1</p><br/>
<h2>reference text</h2><p>C. Adami. Information theory in molecular biology. Physics of Life Reviews, 1:3–22, 2004. M. Aghagolzadeh, H. Soltanian-Zadeh, B. Araabi, and A. Aghagolzadeh. A hierarchical clustering based on mutual information maximization. In in IEEE ICIP, pages 277–280, 2007. P. A. Alemany and D. H. Zanette. Fractal random walks from a variational formalism for Tsallis entropies. Phys. Rev. E, 49(2):R956–R958, Feb 1994. J. Cardoso. Multidimensional independent component analysis. Proc. ICASSP’98, Seattle, WA., 1998. B. Chai, D. B. Walther, D. M. Beck, and L. Fei-Fei. Exploring functional connectivity of the human brain using multivariate information analysis. In NIPS, 2009. J. A. Costa and A. O. Hero. Entropic graphs for manifold learning. In IEEE Asilomar Conf. on Signals, Systems, and Computers, 2003. J. Dedecker, P. Doukhan, G. Lang, J.R. Leon, S. Louhichi, and C Prieur. Weak Dependence: With Examples and Applications, volume 190 of Lecture notes in Statistics. Springer, 2007. M. N. Goria, N. N. Leonenko, V. V. Mergel, and P. L. Novi Inverardi. A new class of random vector entropy estimators and its applications in testing statistical hypotheses. Journal of Nonparametric Statistics, 17: 277–297, 2005. A. O. Hero and O. J. Michel. Asymptotic theory of greedy approximations to minimal k-point random graphs. IEEE Trans. on Information Theory, 45(6):1921–1938, 1999. A. O. Hero, B. Ma, O. Michel, and J. Gorman. Alpha-divergence for classiﬁcation, indexing and retrieval, 2002a. Communications and Signal Processing Laboratory Technical Report CSPL-328. A. O. Hero, B. Ma, O. Michel, and J. Gorman. Applications of entropic spanning graphs. IEEE Signal Processing Magazine, 19(5):85–95, 2002b.  8  K. Hlav´ ckova-Schindler, M. Paluˆb, M. Vejmelkab, and J. Bhattacharya. Causality detection based on a s information-theoretic approaches in time series analysis. Physics Reports, 441:1–46, 2007. M. M. Van Hulle. Constrained subspace ICA based on mutual information optimization directly. Neural Computation, 20:964–973, 2008. A. Hyv¨ rinen, J. Karhunen, and E. Oja. Independent Component Analysis. John Wiley, New York, 2001. a Y. Koo and S. Lee. Rates of convergence of means of Euclidean functionals. Journal of Theoretical Probability, 20(4):821–841, 2007. L. F. Kozachenko and N. N. Leonenko. A statistical estimate for the entropy of a random vector. Problems of Information Transmission, 23:9–16, 1987. A. Kraskov, H. St¨ gbauer, and P. Grassberger. Estimating mutual information. Phys. Rev. E, 69:066138, 2004. o J. Kybic. Incremental updating of nearest neighbor-based high-dimensional entropy estimation. In Proc. Acoustics, Speech and Signal Processing, 2006. E. Learned-Miller and J. W. Fisher. ICA using spacings estimates of entropy. Journal of Machine Learning Research, 4:1271–1295, 2003. N. Leonenko, L. Pronzato, and V. Savani. A class of R´ nyi information estimators for multidimensional densie ties. Annals of Statistics, 36(5):2153–2182, 2008. J. Lewi, R. Butera, and L. Paninski. Real-time adaptive information-theoretic optimization of neurophysiology experiments. In Advances in Neural Information Processing Systems, volume 19, 2007. D. P´ l, Cs. Szepesv´ ri, and B. P´ czos. Estimation of R´ nyi entropy and mutual information based on generala a o e ized nearest-neighbor graphs, 2010. http://arxiv.org/abs/1003.1954. H. Peng and C. Ding. Feature selection based on mutual information: Criteria of max-dependency, maxrelevance, and min-redundancy. IEEE Trans On Pattern Analysis and Machine Intelligence, 27, 2005. B. P´ czos and A. L˝ rincz. Independent subspace analysis using geodesic spanning trees. In ICML, pages o o 673–680, 2005. B. P´ czos and A. L˝ rincz. Identiﬁcation of recurrent neural networks by Bayesian interrogation techniques. o o Journal of Machine Learning Research, 10:515–554, 2009. B. P´ czos, S. Kirshner, and Cs. Szepesv´ ri. REGO: Rank-based estimation of R´ nyi information using Euo a e clidean graph optimization. In AISTATS 2010, 2010. C. Redmond and J. E. Yukich. Asymptotics for Euclidean functionals with power-weighted edges. Stochastic processes and their applications, 61(2):289–304, 1996. D. W. Scott. On optimal and data-based histograms. Biometrika, 66:605–610, 1979. C. Shan, S. Gong, and P. W. Mcowan. Conditional mutual information based boosting for facial expression recognition. In British Machine Vision Conference (BMVC), 2005. J. M. Steele. Probability Theory and Combinatorial Optimization. Society for Industrial and Applied Mathematics, 1997. Z. Szab´ , B. P´ czos, and A. L˝ rincz. Undercomplete blind subspace deconvolution. Journal of Machine o o o Learning Research, 8:1063–1095, 2007. A. B. Tsybakov and E. C. van der Meulen. Root-n consistent estimators of entropy for densities with unbounded support. Scandinavian Journal of Statistics, 23:75–83, 1996. O. Vasicek. A test for normality based on sample entropy. Journal of the Royal Statistical Society, Series B, 38:54–59, 1976. Q. Wang, S. R. Kulkarni, and S. Verd´ . Universal estimation of information measures for analog sources. u Foundations and Trends in Communications and Information Theory, 5(3):265–352, 2009a. Q. Wang, S. R. Kulkarni, and S. Verd´ . Divergence estimation for multidimensional densities via k-nearestu neighbor distances. IEEE Transactions on Information Theory, 55(5):2392–2405, 2009b. E. Wolsztynski, E. Thierry, and L. Pronzato. Minimum-entropy estimation in semi-parametric models. Signal Process., 85(5):937–949, 2005. J. E. Yukich. Probability Theory of Classical Euclidean Optimization Problems. Springer, 1998.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
