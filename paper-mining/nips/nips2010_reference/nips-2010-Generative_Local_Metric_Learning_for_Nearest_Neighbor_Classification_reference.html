<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>104 nips-2010-Generative Local Metric Learning for Nearest Neighbor Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-104" href="../nips2010/nips-2010-Generative_Local_Metric_Learning_for_Nearest_Neighbor_Classification.html">nips2010-104</a> <a title="nips-2010-104-reference" href="#">nips2010-104-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>104 nips-2010-Generative Local Metric Learning for Nearest Neighbor Classification</h1>
<br/><p>Source: <a title="nips-2010-104-pdf" href="http://papers.nips.cc/paper/4040-generative-local-metric-learning-for-nearest-neighbor-classification.pdf">pdf</a></p><p>Author: Yung-kyun Noh, Byoung-tak Zhang, Daniel D. Lee</p><p>Abstract: We consider the problem of learning a local metric to enhance the performance of nearest neighbor classiﬁcation. Conventional metric learning methods attempt to separate data distributions in a purely discriminative manner; here we show how to take advantage of information from parametric generative models. We focus on the bias in the information-theoretic error arising from ﬁnite sampling effects, and ﬁnd an appropriate local metric that maximally reduces the bias based upon knowledge from generative models. As a byproduct, the asymptotic theoretical analysis in this work relates metric learning with dimensionality reduction, which was not understood from previous discriminative approaches. Empirical experiments show that this learned local metric enhances the discriminative nearest neighbor performance on various datasets using simple class conditional generative models. 1</p><br/>
<h2>reference text</h2><p>[1] B. Alipanahi, M. Biggs, and A. Ghodsi. Distance metric learning vs. Fisher discriminant analysis. In Proceedings of the 23rd national conference on Artiﬁcial intelligence, pages 598–603, 2008.  8</p>
<p>[2] K. Das and Z. Nenadic. Approximate information discriminant analysis: A computationally simple heteroscedastic feature extraction technique. Pattern Recognition, 41(5):1548–1557, 2008.</p>
<p>[3] J.V. Davis, B. Kulis, P. Jain, S. Sra, and I.S. Dhillon. Information-theoretic metric learning. In Proceedings of the 24th International Conference on Machine Learning, pages 209–216, 2007.</p>
<p>[4] R.O. Duda, P.E. Hart, and D.G. Stork. Pattern Classiﬁcation (2nd Edition). Wiley-Interscience, 2000.</p>
<p>[5] A. Frome, Y. Singer, and J. Malik. Image retrieval and classiﬁcation using local distance functions. In Advances in Neural Information Processing Systems 18, pages 417–424, 2006.</p>
<p>[6] K. Fukunaga. Introduction to Statistical Pattern Recognition. Academic Press, San Diego, CA, 1990.</p>
<p>[7] K. Fukunaga and T.E. Flick. The optimal distance measure for nearest neighbour classiﬁcation. IEEE Transactions on Information Theory, 27(5):622–627, 1981.</p>
<p>[8] K. Fukunaga and T.E. Flick. An optimal global nearest neighbour measure. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6(3):314–318, 1984.</p>
<p>[9] A. Globerson and S. Roweis. Metric learning by collapsing classes. In Advances in Neural Information Processing Systems 18, pages 451–458. 2006.</p>
<p>[10] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. In Advances in Neural Information Processing Systems 17, pages 513–520. 2005.</p>
<p>[11] M. N. Goria, N. N. Leonenko, V. V. Mergel, and P. Inverardi. A new class of random vector entropy estimators and its applications in testing statistical hypotheses. Journal of Nonparametric Statistics, 17(3):277–297, 2005.</p>
<p>[12] T. Jaakkola and D. Haussler. Exploiting generative models in discriminative classiﬁers. In Advances in Neural Information Processing Systems 11, pages 487–493, 1998.</p>
<p>[13] J.N. Kapur. Measures of Information and Their applications. John Wiley & Sons, New York, NY, 1994.</p>
<p>[14] S. Lacoste-Julien, F. Sha, and M. Jordan. DiscLDA: Discriminative learning for dimensionality reduction and classiﬁcation. In Advances in Neural Information Processing Systems 21, pages 897–904. 2009.</p>
<p>[15] J.A. Lasserre, C.M. Bishop, and T.P. Minka. Principled hybrids of generative and discriminative models. In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 87–94, 2006.</p>
<p>[16] M. Loog and R.P.W. Duin. Linear dimensionality reduction via a heteroscedastic extension of LDA: The chernoff criterion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(6):732–739, 2004.</p>
<p>[17] Z. Nenadic. Information discriminant analysis: Feature extraction with an information-theoretic objective. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(8):1394–1407, 2007.</p>
<p>[18] A.Y. Ng and M.I. Jordan. On discriminative vs. generative classiﬁers: A comparison of logistic regression and naive Bayes. In Advances in Neural Information Processing Systems 14, pages 841–848, 2001.</p>
<p>[19] F. Perez-Cruz. Estimation of information theoretic measures for continuous random variables. In Advances in Neural Information Processing Systems 21, pages 1257–1264. 2009.</p>
<p>[20] R. Raina, Y. Shen, A.Y. Ng, and A. McCallum. Classiﬁcation with hybrid generative/discriminative models. In Advances in Neural Information Processing Systems 16, pages 545–552. 2004.</p>
<p>[21] C. Shen, J. Kim, L. Wang, and A. van den Hengel. Positive semideﬁnite metric learning with boosting. In Advances in Neural Information Processing Systems 22, pages 1651–1659. 2009.</p>
<p>[22] N. Singh-Miller and M. Collins. Learning label embeddings for nearest-neighbor multi-class classiﬁcation with an application to speech recognition. In Advances in Neural Information Processing Systems 22, pages 1678–1686. 2009.</p>
<p>[23] D. Tran and A. Sorokin. Human activity recognition with metric learning. In Proceedings of the 10th European Conference on Computer Vision, pages 548–561, 2008.</p>
<p>[24] Q. Wang, S. R. Kulkarni, and S. Verd´ . A nearest-neighbor approach to estimating divergence between u continuous random vectors. In Proceedings of IEEE International Symposium on Information Theory, pages 242–246, 2006.</p>
<p>[25] Q. Wang, S. R. Kulkarni, and S. Verd´ . Divergence estimation for multidimensional densities via ku nearest-neighbor distances. IEEE Transactions on Information Theory, 55(5):2392–2405, 2009.</p>
<p>[26] K. Weinberger, J. Blitzer, and L. Saul. Distance metric learning for large margin nearest neighbor classiﬁcation. In Advances in Neural Information Processing Systems 18, pages 1473–1480. 2006.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
