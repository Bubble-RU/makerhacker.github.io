<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>223 nips-2010-Rates of convergence for the cluster tree</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-223" href="../nips2010/nips-2010-Rates_of_convergence_for_the_cluster_tree.html">nips2010-223</a> <a title="nips-2010-223-reference" href="#">nips2010-223-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>223 nips-2010-Rates of convergence for the cluster tree</h1>
<br/><p>Source: <a title="nips-2010-223-pdf" href="http://papers.nips.cc/paper/4068-rates-of-convergence-for-the-cluster-tree.pdf">pdf</a></p><p>Author: Kamalika Chaudhuri, Sanjoy Dasgupta</p><p>Abstract: For a density f on Rd , a high-density cluster is any connected component of {x : f (x) ≥ λ}, for some λ > 0. The set of all high-density clusters form a hierarchy called the cluster tree of f . We present a procedure for estimating the cluster tree given samples from f . We give ﬁnite-sample convergence rates for our algorithm, as well as lower bounds on the sample complexity of this estimation problem. 1</p><br/>
<h2>reference text</h2><p>[1] O. Bousquet, S. Boucheron, and G. Lugosi. Introduction to statistical learning theory. Lecture Notes in Artiﬁcial Intelligence, 3176:169–207, 2004.</p>
<p>[2] T. Cover and J. Thomas. Elements of Information Theory. Wiley, 2005.</p>
<p>[3] S. Dasgupta and Y. Freund. Random projection trees for vector quantization. IEEE Transactions on Information Theory, 55(7):3229–3242, 2009.</p>
<p>[4] S. Dasgupta, A. Kalai, and C. Monteleoni. Analysis of perceptron-based active learning. Journal of Machine Learning Research, 10:281–299, 2009.</p>
<p>[5] J.A. Hartigan. Consistency of single linkage for high-density clusters. Journal of the American Statistical Association, 76(374):388–394, 1981.</p>
<p>[6] M. Maier, M. Hein, and U. von Luxburg. Optimal construction of k-nearest neighbor graphs for identifying noisy clusters. Theoretical Computer Science, 410:1749–1764, 2009.</p>
<p>[7] M. Penrose. Single linkage clustering and continuum percolation. Journal of Multivariate Analysis, 53:94–109, 1995.</p>
<p>[8] D. Pollard. Strong consistency of k-means clustering. Annals of Statistics, 9(1):135–140, 1981.</p>
<p>[9] P. Rigollet and R. Vert. Fast rates for plug-in estimators of density level sets. Bernoulli, 15(4):1154–1178, 2009.</p>
<p>[10] A. Rinaldo and L. Wasserman. 38(5):2678–2722, 2010.  Generalized density clustering.  Annals of Statistics,</p>
<p>[11] A. Singh, C. Scott, and R. Nowak. Adaptive hausdorff estimation of density level sets. Annals of Statistics, 37(5B):2760–2782, 2009.</p>
<p>[12] W. Stuetzle and R. Nugent. A generalized single linkage method for estimating the cluster tree of a density. Journal of Computational and Graphical Statistics, 19(2):397–418, 2010.</p>
<p>[13] D. Wishart. Mode analysis: a generalization of nearest neighbor which reduces chaining effects. In Proceedings of the Colloquium on Numerical Taxonomy held in the University of St. Andrews, pages 282–308, 1969.</p>
<p>[14] M.A. Wong and T. Lane. A kth nearest neighbour clustering procedure. Journal of the Royal Statistical Society Series B, 45(3):362–368, 1983.</p>
<p>[15] B. Yu. Assouad, Fano and Le Cam. Festschrift for Lucien Le Cam, pages 423–435, 1997.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
