<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-254" href="../nips2010/nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">nips2010-254</a> <a title="nips-2010-254-reference" href="#">nips2010-254-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</h1>
<br/><p>Source: <a title="nips-2010-254-pdf" href="http://papers.nips.cc/paper/3966-stability-approach-to-regularization-selection-stars-for-high-dimensional-graphical-models.pdf">pdf</a></p><p>Author: Han Liu, Kathryn Roeder, Larry Wasserman</p><p>Abstract: A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include K-fold cross-validation (K-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size diverges with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including K-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all these competing procedures.</p><br/>
<h2>reference text</h2><p>[1] Hirotsugu Akaike. Information theory and an extension of the maximum likelihood principle. Second International Symposium on Information Theory, (2):267–281, 1973.</p>
<p>[2] Onureena Banerjee, Laurent El Ghaoui, and Alexandre d’Aspremont. Model selection through sparse maximum likelihood estimation. Journal of Machine Learning Research, 9:485–516, March 2008.</p>
<p>[3] Shai Ben-david, Ulrike Von Luxburg, and David Pal. A sober look at clustering stability. In Proceedings of the Conference of Learning Theory, pages 5–19. Springer, 2006.</p>
<p>[4] Arthur P. Dempster. Covariance selection. Biometrics, 28:157–175, 1972.</p>
<p>[5] David Edwards. Introduction to graphical modelling. Springer-Verlag Inc, 1995.</p>
<p>[6] Bradley Efron. The jackknife, the bootstrap and other resampling plans. SIAM [Society for Industrial and Applied Mathematics], 1982.</p>
<p>[7] Jerome H. Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9(3):432–441, 2007.</p>
<p>[8] Tilman Lange, Volker Roth, Mikio L. Braun, and Joachim M. Buhmann. Stability-based validation of clustering solutions. Neural Computation, 16(6):1299–1323, 2004.</p>
<p>[9] Steffen L. Lauritzen. Graphical Models. Oxford University Press, 1996.</p>
<p>[10] Han Liu, John Lafferty, and J. Wainwright. The nonparanormal: Semiparametric estimation of high dimensional undirected graphs. Journal of Machine Learning Research, 10:2295–2328, 2009.</p>
<p>[11] Nicolai Meinshausen and Peter B¨ hlmann. High dimensional graphs and variable selection u with the Lasso. The Annals of Statistics, 34:1436–1462, 2006.</p>
<p>[12] Nicolai Meinshausen and Peter B¨ hlmann. Stability selection. To Appear in Journal of the u Royal Statistical Society, Series B, Methodological, 2010.</p>
<p>[13] Renuka R. Nayak, Michael Kearns, Richard S. Spielman, and Vivian G. Cheung. Coexpression network based on natural variation in human gene expression reveals gene interactions and functions. Genome Research, 19(11):1953–1962, November 2009.</p>
<p>[14] Jie Peng, Pei Wang, Nengfeng Zhou, and Ji Zhu. Partial correlation estimation by joint sparse regression models. Journal of the American Statistical Association, 104(486):735–746, 2009.</p>
<p>[15] Dimitris N. Politis, Joseph P. Romano, and Michael Wolf. Subsampling (Springer Series in Statistics). Springer, 1 edition, August 1999.</p>
<p>[16] Pradeep Ravikumar, Martin Wainwright, Garvesh Raskutti, and Bin Yu. Model selection in Gaussian graphical models: High-dimensional consistency of ℓ1 -regularized MLE. In Advances in Neural Information Processing Systems 22, Cambridge, MA, 2009. MIT Press.</p>
<p>[17] Alessandro Rinaldo and Larry Wasserman. Generalized density clustering. arXiv/0907.3454, 2009.</p>
<p>[18] Adam J. Rothman, Peter J. Bickel, Elizaveta Levina, and Ji Zhu. Sparse permutation invariant covariance estimation. Electronic Journal of Statistics, 2:494–515, 2008.</p>
<p>[19] Gideon Schwarz. Estimating the dimension of a model. The Annals of Statistics, 6:461–464, 1978.</p>
<p>[20] Robert J. Serﬂing. Approximation theorems of mathematical statistics. John Wiley and Sons, 1980.</p>
<p>[21] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B, Methodological, 58:267–288, 1996.</p>
<p>[22] Larry Wasserman and Kathryn Roeder. High dimensional variable selection. Annals of statistics, 37(5A):2178–2201, January 2009.</p>
<p>[23] Joe Whittaker. Graphical Models in Applied Multivariate Statistics. Wiley, 1990.</p>
<p>[24] Ming Yuan and Yi Lin. Model selection and estimation in the Gaussian graphical model. Biometrika, 94(1):19–35, 2007.  9</p>
<br/>
<br/><br/><br/></body>
</html>
