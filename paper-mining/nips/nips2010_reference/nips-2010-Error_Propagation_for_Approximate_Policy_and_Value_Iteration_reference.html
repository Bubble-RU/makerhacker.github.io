<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-78" href="../nips2010/nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">nips2010-78</a> <a title="nips-2010-78-reference" href="#">nips2010-78-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</h1>
<br/><p>Source: <a title="nips-2010-78-pdf" href="http://papers.nips.cc/paper/4181-error-propagation-for-approximate-policy-and-value-iteration.pdf">pdf</a></p><p>Author: Amir-massoud Farahmand, Csaba Szepesvári, Rémi Munos</p><p>Abstract: We address the question of how the approximation error/Bellman residual at each iteration of the Approximate Policy/Value Iteration algorithms inﬂuences the quality of the resulted policy. We quantify the performance loss as the Lp norm of the approximation error/Bellman residual at each iteration. Moreover, we show that the performance loss depends on the expectation of the squared Radon-Nikodym derivative of a certain distribution rather than its supremum – as opposed to what has been suggested by the previous results. Also our results indicate that the contribution of the approximation/Bellman error to the performance loss is more prominent in the later iterations of API/AVI, and the effect of an error term in the earlier iterations decays exponentially fast. 1</p><br/>
<h2>reference text</h2><p>[1] Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research, 6:503–556, 2005. 8</p>
<p>[2] Martin Riedmiller. Neural ﬁtted Q iteration – ﬁrst experiences with a data efﬁcient neural reinforcement learning method. In 16th European Conference on Machine Learning, pages 317–328, 2005.</p>
<p>[3] Amir-massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesv´ ri, and Shie Mannor. a Regularized ﬁtted Q-iteration for planning in continuous-space markovian decision problems. In Proceedings of American Control Conference (ACC), pages 725–730, June 2009.</p>
<p>[4] R´ mi Munos and Csaba Szepesv´ ri. Finite-time bounds for ﬁtted value iteration. Journal of e a Machine Learning Research, 9:815–857, 2008.</p>
<p>[5] Michail G. Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine Learning Research, 4:1107–1149, 2003.</p>
<p>[6] Steven J. Bradtke and Andrew G. Barto. Linear least-squares algorithms for temporal difference learning. Machine Learning, 22:33–57, 1996.</p>
<p>[7] Andr´ s Antos, Csaba Szepesv´ ri, and R´ mi Munos. Learning near-optimal policies with a a e Bellman-residual minimization based ﬁtted policy iteration and a single sample path. Machine Learning, 71:89–129, 2008.</p>
<p>[8] Odalric Maillard, R´ mi Munos, Alessandro Lazaric, and Mohammad Ghavamzadeh. Finitee sample analysis of bellman residual minimization. In Proceedings of the Second Asian Conference on Machine Learning (ACML), 2010.</p>
<p>[9] Amir-massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesv´ ri, and Shie Mannor. a Regularized policy iteration. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 441–448. MIT Press, 2009.</p>
<p>[10] J. Zico Kolter and Andrew Y. Ng. Regularization and feature selection in least-squares temporal difference learning. In ICML ’09: Proceedings of the 26th Annual International Conference on Machine Learning, pages 521–528, New York, NY, USA, 2009. ACM.</p>
<p>[11] Xin Xu, Dewen Hu, and Xicheng Lu. Kernel-based least squares policy iteration for reinforcement learning. IEEE Trans. on Neural Networks, 18:973–992, 2007.</p>
<p>[12] Tobias Jung and Daniel Polani. Least squares SVM for least squares TD learning. In In Proc. 17th European Conference on Artiﬁcial Intelligence, pages 499–503, 2006.</p>
<p>[13] Gavin Taylor and Ronald Parr. Kernelized value function approximation for reinforcement learning. In ICML ’09: Proceedings of the 26th Annual International Conference on Machine Learning, pages 1017–1024, New York, NY, USA, 2009. ACM.</p>
<p>[14] Sridhar Mahadevan and Mauro Maggioni. Proto-value functions: A Laplacian framework for learning representation and control in markov decision processes. Journal of Machine Learning Research, 8:2169–2231, 2007.</p>
<p>[15] Alborz Geramifard, Michael Bowling, Michael Zinkevich, and Richard S. Sutton. iLSTD: Eligibility traces and convergence analysis. In B. Sch¨ lkopf, J. Platt, and T. Hoffman, editors, o Advances in Neural Information Processing Systems 19, pages 441–448. MIT Press, Cambridge, MA, 2007.</p>
<p>[16] Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming (Optimization and Neural Computation Series, 3). Athena Scientiﬁc, 1996.</p>
<p>[17] R´ mi Munos. Performance bounds in lp norm for approximate value iteration. SIAM Journal e on Control and Optimization, 2007.</p>
<p>[18] R´ mi Munos. Error bounds for approximate policy iteration. In ICML 2003: Proceedings of e the 20th Annual International Conference on Machine Learning, 2003.</p>
<p>[19] Dimitri P. Bertsekas and Steven E. Shreve. Stochastic Optimal Control: The Discrete-Time Case. Academic Press, 1978.</p>
<p>[20] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning). The MIT Press, 1998.</p>
<p>[21] Csaba Szepesv´ ri. Algorithms for Reinforcement Learning. Morgan Claypool Publishers, a 2010.</p>
<p>[22] L´ szl´ Gy¨ rﬁ, Michael Kohler, Adam Krzy˙ ak, and Harro Walk. A Distribution-Free Theory a o o z of Nonparametric Regression. Springer Verlag, New York, 2002. 9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
