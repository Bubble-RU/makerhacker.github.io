<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-30" href="../nips2010/nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">nips2010-30</a> <a title="nips-2010-30-reference" href="#">nips2010-30-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</h1>
<br/><p>Source: <a title="nips-2010-30-pdf" href="http://papers.nips.cc/paper/4110-an-inverse-power-method-for-nonlinear-eigenproblems-with-applications-in-1-spectral-clustering-and-sparse-pca.pdf">pdf</a></p><p>Author: Matthias Hein, Thomas Bühler</p><p>Abstract: Many problems in machine learning and statistics can be formulated as (generalized) eigenproblems. In terms of the associated optimization problem, computing linear eigenvectors amounts to ﬁnding critical points of a quadratic function subject to quadratic constraints. In this paper we show that a certain class of constrained optimization problems with nonquadratic objective and constraints can be understood as nonlinear eigenproblems. We derive a generalization of the inverse power method which is guaranteed to converge to a nonlinear eigenvector. We apply the inverse power method to 1-spectral clustering and sparse PCA which can naturally be formulated as nonlinear eigenproblems. In both applications we achieve state-of-the-art results in terms of solution quality and runtime. Moving beyond the standard eigenproblem should be useful also in many other applications and our inverse power method can be easily adapted to new problems. 1</p><br/>
<h2>reference text</h2><p>[1] http://www.stat.ucla.edu/˜wxl/research/microarray/DBC/index.htm.</p>
<p>[2] A. Beck and M. Teboulle. Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems. IEEE Transactions on Image Processing, 18(11):2419–2434, 2009.</p>
<p>[3] D.P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 1999.</p>
<p>[4] R.J. Biezuner, G. Ercole, and E.M. Martins. Computing the ﬁrst eigenvalue of the p-Laplacian via the inverse power method. Journal of Functional Analysis, 257:243–270, 2009.</p>
<p>[5] T. B¨ hler and M. Hein. Spectral Clustering based on the graph p-Laplacian. In Proceedings of the 26th u International Conference on Machine Learning, pages 81–88. Omnipress, 2009.</p>
<p>[6] J. Cadima and I.T. Jolliffe. Loading and correlations in the interpretation of principal components. Journal of Applied Statistics, 22:203–214, 1995.</p>
<p>[7] K.-C. Chang. Variational methods for non-differentiable functionals and their applications to partial differential equations. Journal of Mathematical Analysis and Applications, 80:102–129, 1981.</p>
<p>[8] F.R.K. Chung. Spectral Graph Theory. AMS, 1997.</p>
<p>[9] F.H. Clarke. Optimization and Nonsmooth Analysis. Wiley New York, 1983.</p>
<p>[10] A. d’Aspremont, F. Bach, and L. El Ghaoui. Optimal solutions for sparse principal component analysis. Journal of Machine Learning Research, 9:1269–1294, 2008.</p>
<p>[11] T. Goldstein and S. Osher. The Split Bregman method for L1-Regularized Problems. SIAM Journal on Imaging Sciences, 2(2):323–343, 2009.</p>
<p>[12] G.H. Golub and C.F. Van Loan. Matrix Computations. Johns Hopkins University Press, 3rd edition, 1996.</p>
<p>[13] I.T. Jolliffe. Principal Component Analysis. Springer, 2nd edition, 2002.</p>
<p>[14] I.T. Jolliffe, N. Trendaﬁlov, and M. Uddin. A modiﬁed principal component technique based on the LASSO. Journal of Computational and Graphical Statistics, 12:531–547, 2003.</p>
<p>[15] M. Journ´ e, Y. Nesterov, P. Richt´ rik, and R. Sepulchre. Generalized Power Method for Sparse Principal e a Component Analysis. Journal of Machine Learning Research, 11:517–553, 2010.</p>
<p>[16] B. Moghaddam, Y. Weiss, and S. Avidan. Spectral bounds for sparse PCA: Exact and greedy algorithms. In Advances in Neural Information Processing Systems, pages 915–922. MIT Press, 2006.</p>
<p>[17] C.D. Sigg and J.M. Buhmann. Expectation-maximization for sparse and non-negative PCA. In Proceedings of the 25th International Conference on Machine Learning, pages 960–967. ACM, 2008.</p>
<p>[18] B.K. Sriperumbudur, D.A. Torres, and G.R.G. Lanckriet. Sparse eigen methods by D.C. programming. In Proceedings of the 24th International Conference on Machine Learning, pages 831–838. ACM, 2007.</p>
<p>[19] A. Szlam and X. Bresson. Total variation and Cheeger cuts. In Proceedings of the 27th International Conference on Machine Learning, pages 1039–1046. Omnipress, 2010.</p>
<p>[20] U. von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17:395–416, 2007.</p>
<p>[21] F. Yang and Z. Wei. Generalized Euler identity for subdifferentials of homogeneous functions and applications. Mathematical Analysis and Applications, 337:516–523, 2008.</p>
<p>[22] H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis. Journal of Computational and Graphical Statistics, 15:265–286, 2006.  9</p>
<br/>
<br/><br/><br/></body>
</html>
