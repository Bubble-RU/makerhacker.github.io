<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>191 nips-2010-On the Theory of Learnining with Privileged Information</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-191" href="../nips2010/nips-2010-On_the_Theory_of_Learnining_with_Privileged_Information.html">nips2010-191</a> <a title="nips-2010-191-reference" href="#">nips2010-191-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>191 nips-2010-On the Theory of Learnining with Privileged Information</h1>
<br/><p>Source: <a title="nips-2010-191-pdf" href="http://papers.nips.cc/paper/3960-on-the-theory-of-learnining-with-privileged-information.pdf">pdf</a></p><p>Author: Dmitry Pechyony, Vladimir Vapnik</p><p>Abstract: In Learning Using Privileged Information (LUPI) paradigm, along with the standard training data in the decision space, a teacher supplies a learner with the privileged information in the correcting space. The goal of the learner is to ﬁnd a classiﬁer with a low generalization error in the decision space. We consider an empirical risk minimization algorithm, called Privileged ERM, that takes into account the privileged information in order to ﬁnd a good function in the decision space. We outline the conditions on the correcting space that, if satisﬁed, allow Privileged ERM to have much faster learning rate in the decision space than the one of the regular empirical risk minimization. 1</p><br/>
<h2>reference text</h2><p>[1] S. Boucheron, O. Bousquet, and G. Lugosi. Theory of classiﬁcation: a survey of some recent advances. ESAIM: Probability and Statistics, 9:329–375, 2005.</p>
<p>[2] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, 1995.</p>
<p>[3] L. Devroye and G. Lugosi. Lower bounds in pattern recognition and learning. Pattern Recognition, 28(7):1011–1018, 1995.</p>
<p>[4] E. Gine and V. Koltchinskii. Concentration inequalities and asymptotic resutls for ratio type empirical processes. Annals of Probability, 34(3):1143–1216, 2006.</p>
<p>[5] V. Koltchinskii. 2008 Saint Flour lectures: Oracle inequalities in empirical risk minimization and sparse recovery problems, 2008. Available at fodava.gatech.edu/ﬁles/reports/FODAVA09-17.pdf.</p>
<p>[6] P. Massart and E. Nedelec. Risk bounds for statistical learning. Annals of Statistics, 34(5):2326–2366, 2006.</p>
<p>[7] A. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Annals of Statistics, 32(1):135–166, 2004.</p>
<p>[8] V. Vapnik. Estimation of dependencies based on empirical data. Springer–Verlag, 2nd edition, 2006.</p>
<p>[9] V. Vapnik and A. Vashist. A new learning paradigm: Learning using privileged information. Neural Networks, 22(5-6):544–557, 2009.</p>
<p>[10] V. Vapnik, A. Vashist, and N. Pavlovich. Learning using hidden information: Master class learning. In Proceedings of NATO workshop on Mining Massive Data Sets for Security, pages 3–14. 2008.  9</p>
<br/>
<br/><br/><br/></body>
</html>
