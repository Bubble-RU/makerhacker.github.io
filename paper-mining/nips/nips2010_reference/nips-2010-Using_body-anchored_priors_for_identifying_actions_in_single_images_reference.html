<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>281 nips-2010-Using body-anchored priors for identifying actions in single images</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-281" href="../nips2010/nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">nips2010-281</a> <a title="nips-2010-281-reference" href="#">nips2010-281-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>281 nips-2010-Using body-anchored priors for identifying actions in single images</h1>
<br/><p>Source: <a title="nips-2010-281-pdf" href="http://papers.nips.cc/paper/4012-using-body-anchored-priors-for-identifying-actions-in-single-images.pdf">pdf</a></p><p>Author: Leonid Karlinsky, Michael Dinerstein, Shimon Ullman</p><p>Abstract: This paper presents an approach to the visual recognition of human actions using only single images as input. The task is easy for humans but difficult for current approaches to object recognition, because instances of different actions may be similar in terms of body pose, and often require detailed examination of relations between participating objects and body parts in order to be recognized. The proposed approach applies a two-stage interpretation procedure to each training and test image. The first stage produces accurate detection of the relevant body parts of the actor, forming a prior for the local evidence needed to be considered for identifying the action. The second stage extracts features that are anchored to the detected body parts, and uses these features and their feature-to-part relations in order to recognize the action. The body anchored priors we propose apply to a large range of human actions. These priors allow focusing on the relevant regions and relations, thereby significantly simplifying the learning process and increasing recognition performance. 1</p><br/>
<h2>reference text</h2><p>[1] Jackendoff, R.: Semantic interpretation in generative grammar. The MIT Press (1972)</p>
<p>[2] Laptev, I., Marszalek, M., Schmid, C., Rozenfeld, B.: Learning realistic human actions from movies. In: CVPR. (2008) 1–8</p>
<p>[3] Iacoboni, M., Mazziotta, J.C.: Mirror neuron system: basic findings and clinical applications. Annals of Neurology (2007)</p>
<p>[4] Kim, J., Biederman, I.: Where do objects become scenes? Journal of Vision (2009)</p>
<p>[5] Helbig, H., Graf, M., Kiefer, M.: The role of action representations in visual object recognition. Experimental Brain Research (2006)</p>
<p>[6] Sakata, H., Taira, M., Kusunoki, M., Murata, A., Tanaka, Y., Tsutsui, K.: Neural coding of 3d features of objects for hand action in the parietal cortex of the monkey. Philos Trans R Soc Lond B Biol Sci. (1998)</p>
<p>[7] Wang, Y., Jiang, H., Drew, M.S., nian Li, Z., Mori, G.: Unsupervised discovery of action classes. In: CVPR. (2006) 5</p>
<p>[8] Li, L., Fei-Fei, L.: What, where and who? classifying events by scene and object recognition. In: ICCV. (2007) 1–8</p>
<p>[9] Thurau, C., Hlavac, V.: Pose primitive based human action recognition in videos or still images. In: CVPR. (2008) 1–8</p>
<p>[10] Yao, B., Fei-Fei, L.: Grouplet: A structured image representation for recognizing human and object interactions. CVPR (2010)</p>
<p>[11] Gupta, A., Kembhavi, A., Davis, L.: Observing human-object interactions: Using spatial and functional compatibility for recognition. PAMI (2009)</p>
<p>[12] Yao, B., Fei-Fei, L.: Modeling mutual context of object and human pose in human-object interaction activities. CVPR (2010)</p>
<p>[13] Blake, A., Rother, C., Brown, M., Perez, P., Torr, P.: Interactive image segmentation using an adaptive gmmrf model. ECCV (2004)</p>
<p>[14] Karlinsky, L., Dinerstein, M., Harari, D., Ullman, S.: The chains model for detecting parts by their context. CVPR (2010)</p>
<p>[15] Ferrari, V., Marin, M., Zisserman, A.: Progressive search space reduction for human pose estimation. CVPR (2008)</p>
<p>[16] Andriluka, M., Roth, S., Schiele, B.: Pictorial structures revisited: People detection and articulated pose estimation. CVPR (2009)</p>
<p>[17] Felzenszwalb, P., Huttenlocher, D.: Pictorial structures for object recognition. IJCV 61 (2005) 55–79</p>
<p>[18] Ramanan, D., Forsyth, D.A., Barnard, K.: Building models of animals from video. PAMI (2006)</p>
<p>[19] Maouene, J., Hidaka, S., Smith, L.B.: Body parts and early-learned verbs. Cognitive Science (2008)</p>
<p>[20] Lowe, D.: Distinctive image features from scale-invariant keypoints. IJCV (2004)</p>
<p>[21] Duda, R., Hart, P.: Pattern classification and scene analysis. Wiley (1973)</p>
<p>[22] Mount, D., Arya, S.: Ann: A library for approximate nearest neighbor searching. CGC 2nd Annual Workshop on Comp. Geometry (1997)</p>
<p>[23] Felzenszwalb, P., McAllester, D., Ramanan, D.: A discriminatively trained, multiscale, deformable part model. CVPR (2008) 1–8</p>
<p>[24] Zhang, J., Marszalek, M., Lazebnik, S., Schmid, C.: Local features and kernels for classification of texture and object categories: A comprehensive study. IJCV (2007)</p>
<p>[25] Everingham, M., Van Gool, L., Williams, C., Winn, J., Zisserman, A.: The pascal visual object classes challenge 2007 results. http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007 (2007) 9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
