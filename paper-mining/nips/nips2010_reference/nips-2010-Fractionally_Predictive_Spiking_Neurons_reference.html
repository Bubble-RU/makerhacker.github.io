<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>96 nips-2010-Fractionally Predictive Spiking Neurons</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-96" href="../nips2010/nips-2010-Fractionally_Predictive_Spiking_Neurons.html">nips2010-96</a> <a title="nips-2010-96-reference" href="#">nips2010-96-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>96 nips-2010-Fractionally Predictive Spiking Neurons</h1>
<br/><p>Source: <a title="nips-2010-96-pdf" href="http://papers.nips.cc/paper/3983-fractionally-predictive-spiking-neurons.pdf">pdf</a></p><p>Author: Jaldert Rombouts, Sander M. Bohte</p><p>Abstract: Recent experimental work has suggested that the neural ﬁring rate can be interpreted as a fractional derivative, at least when signal variation induces neural adaptation. Here, we show that the actual neural spike-train itself can be considered as the fractional derivative, provided that the neural signal is approximated by a sum of power-law kernels. A simple standard thresholding spiking neuron sufﬁces to carry out such an approximation, given a suitable refractory response. Empirically, we ﬁnd that the online approximation of signals with a sum of powerlaw kernels is beneﬁcial for encoding signals with slowly varying components, like long-memory self-similar signals. For such signals, the online power-law kernel approximation typically required less than half the number of spikes for similar SNR as compared to sums of similar but exponentially decaying kernels. As power-law kernels can be accurately approximated using sums or cascades of weighted exponentials, we demonstrate that the corresponding decoding of spiketrains by a receiving neuron allows for natural and transparent temporal signal ﬁltering by tuning the weights of the decoding kernel. 1</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/></body>
</html>
