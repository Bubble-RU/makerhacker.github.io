<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>163 nips-2010-Lower Bounds on Rate of Convergence of Cutting Plane Methods</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-163" href="../nips2010/nips-2010-Lower_Bounds_on_Rate_of_Convergence_of_Cutting_Plane_Methods.html">nips2010-163</a> <a title="nips-2010-163-reference" href="#">nips2010-163-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>163 nips-2010-Lower Bounds on Rate of Convergence of Cutting Plane Methods</h1>
<br/><p>Source: <a title="nips-2010-163-pdf" href="http://papers.nips.cc/paper/4144-lower-bounds-on-rate-of-convergence-of-cutting-plane-methods.pdf">pdf</a></p><p>Author: Xinhua Zhang, Ankan Saha, S.v.n. Vishwanathan</p><p>Abstract: In a recent paper Joachims [1] presented SVM-Perf, a cutting plane method (CPM) for training linear Support Vector Machines (SVMs) which converges to an accurate solution in O(1/ 2 ) iterations. By tightening the analysis, Teo et al. [2] showed that O(1/ ) iterations sufﬁce. Given the impressive convergence speed of CPM on a number of practical problems, it was conjectured that these rates could be further improved. In this paper we disprove this conjecture. We present counter examples which are not only applicable for training linear SVMs with hinge loss, but also hold for support vector methods which optimize a multivariate performance score. However, surprisingly, these problems are not inherently hard. By exploiting the structure of the objective function we can devise an algo√ rithm that converges in O(1/ ) iterations. 1</p><br/>
<h2>reference text</h2><p>[1] T. Joachims. Training linear SVMs in linear time. In Proc. ACM Conf. Knowledge Discovery and Data Mining (KDD), pages 217–226, 2006.</p>
<p>[2] C. H. Teo, S. V. N. Vishwanthan, A. J. Smola, and Q. V. Le. Bundle methods for regularized risk minimization. J. Mach. Learn. Res., 11:311–365, January 2010.</p>
<p>[3] T. Joachims. A support vector method for multivariate performance measures. In Proc. Intl. Conf. Machine Learning, pages 377–384, 2005.</p>
<p>[4] Y. Nesterov. Introductory Lectures On Convex Optimization: A Basic Course. Springer, 2003.</p>
<p>[5] A. Nemirovski and D. Yudin. Problem Complexity and Method Efﬁciency in Optimization. John Wiley and Sons, 1983.</p>
<p>[6] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence O(1/k 2 ). Soviet Math. Docl., 269:543–547, 1983.</p>
<p>[7] Xinhua Zhang, Ankan Saha, and S.V.N. Vishwanathan. Lower bounds on rate of convergence of cutting plane methods (long version). Technical report, 2010. http://www.stat.purdue.edu/∼vishy/papers/ZhaSahVis10 long.pdf.</p>
<p>[8] J. M. Borwein and A. S. Lewis. Convex Analysis and Nonlinear Optimization: Theory and Examples. CMS books in Mathematics. Canadian Mathematical Society, 2000.</p>
<p>[9] Y. Nesterov. Excessive gap technique in nonsmooth convex minimization. SIAM Journal on Optimization, 16(1):235–249, 2005. ISSN 1052-6234.</p>
<p>[10] Y. Nesterov. Gradient methods for minimizing composite objective function. Technical Report 76, CORE Discussion Paper, UCL, 2007.</p>
<p>[11] Xinhua Zhang, Ankan Saha, and S.V.N. Vishwanathan. Regularized risk minimization by Nesterov’s accelerated gradient methods: Algorithmic extensions and empirical studies. Technical report arXiv:1011.0472, 2010. http://arxiv.org/abs/1011.0472.</p>
<p>[12] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and interdependent output variables. J. Mach. Learn. Res., 6:1453–1484, 2005. ˜</p>
<p>[13] T. Joachims, T. Finley, and C.N.J. Yu. Cutting-plane training of structural SVMs. Machine Learning Journal, 77(1):27–59, 2009.</p>
<p>[14] P. M. Pardalos and N. Kovoor. An algorithm for singly constrained class of quadratic programs subject to upper and lower bounds. Mathematical Programming, 46:321–328, 1990.</p>
<p>[15] Y.-H. Dai and R. Fletcher. New algorithms for singly linearly constrained quadratic programs subject to lower and upper bounds. Mathematical Programming: Series A and B archive, 106 (3):403–421, 2006.</p>
<p>[16] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efﬁcient projections onto the 1 -ball for learning in high dimensions. In Proc. Intl. Conf. Machine Learning, pages 272–279, 2008.</p>
<p>[17] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for SVM. In Proc. Intl. Conf. Machine Learning, pages 807–814, 2007.</p>
<p>[18] A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. Wainwright. Information-theoretic lower bounds on the oracle complexity of convex optimization. In Neural Information Processing Systems, pages 1–9, 2009.</p>
<p>[19] J. C. Platt. Sequential minimal optimization: A fast algorithm for training support vector machines. Technical Report MSR-TR-98-14, Microsoft Research, 1998.</p>
<p>[20] N. List and H. U. Simon. SVM-optimization and steepest-descent line search. In S. Dasgupta and A. Klivans, editors, Proc. Annual Conf. Computational Learning Theory, 2009.</p>
<p>[21] M. C. Ferris and T. S. Munson. Interior-point methods for massive support vector machines. SIAM Journal on Optimization, 13(3):783–804, 2002.  9</p>
<br/>
<br/><br/><br/></body>
</html>
