<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-88" href="../nips2010/nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">nips2010-88</a> <a title="nips-2010-88-reference" href="#">nips2010-88-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</h1>
<br/><p>Source: <a title="nips-2010-88-pdf" href="http://papers.nips.cc/paper/4103-extensions-of-generalized-binary-search-to-group-identification-and-exponential-costs.pdf">pdf</a></p><p>Author: Gowtham Bellala, Suresh Bhavnani, Clayton Scott</p><p>Abstract: Generalized Binary Search (GBS) is a well known greedy algorithm for identifying an unknown object while minimizing the number of “yes” or “no” questions posed about that object, and arises in problems such as active learning and active diagnosis. Here, we provide a coding-theoretic interpretation for GBS and show that GBS can be viewed as a top-down algorithm that greedily minimizes the expected number of queries required to identify an object. This interpretation is then used to extend GBS in two ways. First, we consider the case where the objects are partitioned into groups, and the objective is to identify only the group to which the object belongs. Then, we consider the case where the cost of identifying an object grows exponentially in the number of queries. In each case, we present an exact formula for the objective function involving Shannon or R´ nyi entropy, and e develop a greedy algorithm for minimizing it. 1</p><br/>
<h2>reference text</h2><p>[1] S. Dasgupta, “Analysis of a greedy active learning strategy,” Advances in Neural Information Processing Systems, 2004.</p>
<p>[2] R. Nowak, “Generalized binary search,” Proceedings of the 46th Allerton Conference on Communications, Control and Computing, pp. 568–574, 2008.</p>
<p>[3] ——, “Noisy generalized binary search,” Advances in Neural Information Processing Systems, vol. 22, pp. 1366–1374, 2009.</p>
<p>[4] D. Golovin and A. Krause, “Adaptive Submodularity: A new approach to active learning and stochastic optimization,” In Proceedings of International Conference on Learning Theory (COLT), 2010.</p>
<p>[5] D. W. Loveland, “Performance bounds for binary testing with arbitrary weights,” Acta Informatica, 1985.</p>
<p>[6] F. Yu, F. Tu, H. Tu, and K. Pattipati, “Multiple disease (fault) diagnosis with applications to the QMR-DT problem,” Proceedings of IEEE International Conference on Systems, Man and Cybernetics, vol. 2, pp. 1187–1192, October 2003.</p>
<p>[7] J. Shiozaki, H. Matsuyama, E. O’Shima, and M. Iri, “An improved algorithm for diagnosis of system failures in the chemical process,” Computational Chemical Engineering, vol. 9, no. 3, pp. 285–293, 1985.</p>
<p>[8] S. Bhavnani, A. Abraham, C. Demeniuk, M. Gebrekristos, A. Gong, S. Nainwal, G. Vallabha, and R. Richardson, “Network analysis of toxic chemicals and symptoms: Implications for designing ﬁrstresponder systems,” Proceedings of American Medical Informatics Association, 2007.</p>
<p>[9] D. Geman and B. Jedynak, “An active testing model for tracking roads in satellite images,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 18, no. 1, pp. 1–14, 1996.</p>
<p>[10] M. J. Swain and M. A. Stricker, “Promising directions in active vision,” International Journal of Computer Vision, vol. 11, no. 2, pp. 109–126, 1993.</p>
<p>[11] A. Gupta, R. Krishnaswamy, V. Nagarajan, and R. Ravi, “Approximation algorithms for optimal decision trees and adaptive TSP problems,” 2010, available online at arXiv.org:1003.0722.</p>
<p>[12] M. Garey, “Optimal binary identiﬁcation procedures,” SIAM Journal on Applied Mathematics, vol. 23(2), pp. 173–186, 1972.</p>
<p>[13] L. Hyaﬁl and R. Rivest, “Constructing optimal binary decision trees is NP-complete,” Information Processing Letters, vol. 5(1), pp. 15–17, 1976.</p>
<p>[14] S. R. Kosaraju, T. M. Przytycka, and R. S. Borgstrom, “On an optimal split tree problem,” Proceedings of 6th International Workshop on Algorithms and Data Structures, WADS, pp. 11–14, 1999.</p>
<p>[15] R. M. Goodman and P. Smyth, “Decision tree design from a communication theory standpoint,” IEEE Transactions on Information Theory, vol. 34, no. 5, 1988.</p>
<p>[16] P. A. Humblet, “Generalization of Huffman coding to minimize the probability of buffer overﬂow,” IEEE Transactions on Information Theory, vol. IT-27, no. 2, pp. 230–232, March 1981.</p>
<p>[17] F. Schulz, “Trees with exponentially growing costs,” Information and Computation, vol. 206, 2008.</p>
<p>[18] M. B. Baer, “R´ nyi to R´ nyi - source coding under seige,” Proceedings of IEEE International Symposium e e on Information Theory, pp. 1258–1262, July 2006.</p>
<p>[19] T. M. Cover and J. A. Thomas, Elements of Information Theory.  John Wiley, 1991.</p>
<p>[20] D. A. Huffman, “A method for the construction of minimum-redundancy codes,” Proceedings of the Institute of Radio Engineers, 1952.</p>
<p>[21] C. E. Shannon, “A mathematical theory of communication,” Bell Systems Technical Journal, vol. 27, pp. 379 – 423, July 1948.</p>
<p>[22] R. M. Fano, Transmission of Information.  MIT Press, 1961.</p>
<p>[23] D. Golovin, D. Ray, and A. Krause, “Near-optimal Bayesian active learning with noisy observations,” to appear in the Proceedings of the Neural Information Processing Systems (NIPS), 2010.</p>
<p>[24] S. Dasgupta, “Coarse sample complexity bounds for active learning,” Advances in Neural Information Processing Systems, 2006.</p>
<p>[25] G. Bellala, S. Bhavnani, and C. Scott, “Group-based query learning for rapid diagnosis in time-critical situations,” Tech. Rep., 2009, available online at arXiv.org:0911.4511.</p>
<p>[26] L. L. Campbell, “A coding problem and R´ nyi’s entropy,” Information and Control, vol. 8, no. 4, pp. e 423–429, August 1965.</p>
<p>[27] G. Bellala, S. Bhavnani, and C. Scott, “Query learning with exponential query costs,” Tech. Rep., 2010, available online at arXiv.org:1002.4019.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
