<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>269 nips-2010-Throttling Poisson Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-269" href="../nips2010/nips-2010-Throttling_Poisson_Processes.html">nips2010-269</a> <a title="nips-2010-269-reference" href="#">nips2010-269-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>269 nips-2010-Throttling Poisson Processes</h1>
<br/><p>Source: <a title="nips-2010-269-pdf" href="http://papers.nips.cc/paper/4025-throttling-poisson-processes.pdf">pdf</a></p><p>Author: Uwe Dick, Peter Haider, Thomas Vanck, Michael Brückner, Tobias Scheffer</p><p>Abstract: We study a setting in which Poisson processes generate sequences of decisionmaking events. The optimization goal is allowed to depend on the rate of decision outcomes; the rate may depend on a potentially long backlog of events and decisions. We model the problem as a Poisson process with a throttling policy that enforces a data-dependent rate limit and reduce the learning problem to a convex optimization problem that can be solved efﬁciently. This problem setting matches applications in which damage caused by an attacker grows as a function of the rate of unsuppressed hostile events. We report on experiments on abuse detection for an email service. 1</p><br/>
<h2>reference text</h2><p>[1] J.A. Bagnell, S. Kakade, A. Ng, and J. Schneider. Policy search by dynamic programming. Advances in Neural Information Processing Systems, 16, 2004.</p>
<p>[2] D. Blatt and A.O. Hero. From weighted classiﬁcation to policy search. Advances in Neural Information Processing Systems, 18, 2006.</p>
<p>[3] C. Dimitrakakis and M.G. Lagoudakis. Rollout sampling approximate policy iteration. Machine Learning, 72(3):157–171, 2008. 8</p>
<p>[4] M. Ghavamzadeh and Y. Engel. Bayesian policy gradient algorithms. Advances in Neural Information Processing Systems, 19, 2007.</p>
<p>[5] D.L. Jagerman, B. Melamed, and W. Willinger. Stochastic modeling of trafﬁc processes. Frontiers in queueing: models, methods and problems, pages 271–370, 1996.</p>
<p>[6] M.G. Lagoudakis and R. Parr. Reinforcement learning as classiﬁcation: Leveraging modern classiﬁers. In Proceedings of the 20th International Conference on Machine Learning, 2003.</p>
<p>[7] J. Langford and B. Zadrozny. Relating reinforcement learning performance to classiﬁcation performance. In Proceedings of the 22nd International Conference on Machine learning, 2005.</p>
<p>[8] R.S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in Neural Information Processing Systems, 12, 2000.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
