<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-184" href="../nips2010/nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">nips2010-184</a> <a title="nips-2010-184-reference" href="#">nips2010-184-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2010-184-pdf" href="http://papers.nips.cc/paper/3992-nonparametric-bayesian-policy-priors-for-reinforcement-learning.pdf">pdf</a></p><p>Author: Finale Doshi-velez, David Wingate, Nicholas Roy, Joshua B. Tenenbaum</p><p>Abstract: We consider reinforcement learning in partially observable domains where the agent can query an expert for demonstrations. Our nonparametric Bayesian approach combines model knowledge, inferred from expert information and independent exploration, with policy knowledge inferred from expert trajectories. We introduce priors that bias the agent towards models with both simple representations and simple policies, resulting in improved policy and model learning. 1</p><br/>
<h2>reference text</h2><p>[1] R. Jaulmes, J. Pineau, and D. Precup. Learning in non-stationary partially observable Markov decision processes. ECML Workshop, 2005.</p>
<p>[2] Stephane Ross, Brahim Chaib-draa, and Joelle Pineau. Bayes-adaptive POMDPs. In Neural Information Processing Systems (NIPS), 2008.</p>
<p>[3] Stephane Ross, Brahim Chaib-draa, and Joelle Pineau. Bayesian reinforcement learning in continuous POMDPs with application to robot navigation. In ICRA, 2008.</p>
<p>[4] Finale Doshi, Joelle Pineau, and Nicholas Roy. Reinforcement learning with limited reinforcement: Using Bayes risk for active learning in POMDPs. In International Conference on Machine Learning, volume 25, 2008.</p>
<p>[5] Pieter Abbeel, Morgan Quigley, and Andrew Y. Ng. Using inaccurate models in reinforcement learning. In In International Conference on Machine Learning (ICML) Pittsburgh, pages 1–8. ACM Press, 2006.</p>
<p>[6] Nathan Ratliff, Brian Ziebart, Kevin Peterson, J. Andrew Bagnell, Martial Hebert, Anind K. Dey, and Siddhartha Srinivasa. Inverse optimal heuristic control for imitation learning. In Proc. AISTATS, pages 424–431, 2009.</p>
<p>[7] P. Poupart and N. Vlassis. Model-based Bayesian reinforcement learning in partially observable domains. In ISAIM, 2008.</p>
<p>[8] M. Strens. A Bayesian framework for reinforcement learning. In ICML, 2000.</p>
<p>[9] John Asmuth, Lihong Li, Michael Littman, Ali Nouri, and David Wingate. A Bayesian sampling approach to exploration in reinforcement learning. In Uncertainty in Artiﬁcial Intelligence (UAI), 2009.</p>
<p>[10] R. Dearden, N. Friedman, and D. Andre. Model based Bayesian exploration. pages 150–159, 1999.</p>
<p>[11] E. J. Sondik. The Optimial Control of Partially Observable Markov Processes. PhD thesis, Stanford University, 1971.</p>
<p>[12] Finale Doshi-Velez. The inﬁnite partially observable Markov decision process. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 477–485. 2009.</p>
<p>[13] Matthew J. Beal, Zoubin Ghahramani, and Carl E. Rasmussen. The inﬁnite hidden Markov model. In Machine Learning, pages 29–245. MIT Press, 2002.</p>
<p>[14] Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101:1566–1581, 2006.</p>
<p>[15] Tao Wang, Daniel Lizotte, Michael Bowling, and Dale Schuurmans. Bayesian sparse sampling for on-line reward optimization. In International Conference on Machine Learning (ICML), 2005.</p>
<p>[16] J. Zico Kolter and Andrew Ng. Near-Bayesian exploration in polynomial time. In International Conference on Machine Learning (ICML), 2009.</p>
<p>[17] J. van Gael, Y. Saatci, Y. W. Teh, and Z. Ghahramani. Beam sampling for the inﬁnite hidden Markov model. In ICML, volume 25, 2008.</p>
<p>[18] J. Pineau, G. Gordon, and S. Thrun. Point-based value iteration: An anytime algorithm for POMDPs. IJCAI, 2003.</p>
<p>[19] Pascal Poupart and Craig Boutilier. Bounded ﬁnite state controllers. In Neural Information Processing Systems, 2003.</p>
<p>[20] M. L. Littman, A. R. Cassandra, and L. P. Kaelbling. Learning policies for partially observable environments: scaling up. ICML, 1995.</p>
<p>[21] Lonnie Chrisman. Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. In In Proceedings of the Tenth National Conference on Artiﬁcial Intelligence, pages 183–188. AAAI Press, 1992.</p>
<p>[22] T. Smith and R. Simmons. Heuristic search value iteration for POMDPs. In Proc. of UAI 2004, Banff, Alberta, 2004.  9</p>
<br/>
<br/><br/><br/></body>
</html>
