<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1 nips-2010-(RF)^2 -- Random Forest Random Field</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-1" href="../nips2010/nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">nips2010-1</a> <a title="nips-2010-1-reference" href="#">nips2010-1-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1 nips-2010-(RF)^2 -- Random Forest Random Field</h1>
<br/><p>Source: <a title="nips-2010-1-pdf" href="http://papers.nips.cc/paper/4140-rf2-random-forest-random-field.pdf">pdf</a></p><p>Author: Nadia Payet, Sinisa Todorovic</p><p>Abstract: We combine random forest (RF) and conditional random ﬁeld (CRF) into a new computational framework, called random forest random ﬁeld (RF)2 . Inference of (RF)2 uses the Swendsen-Wang cut algorithm, characterized by MetropolisHastings jumps. A jump from one state to another depends on the ratio of the proposal distributions, and on the ratio of the posterior distributions of the two states. Prior work typically resorts to a parametric estimation of these four distributions, and then computes their ratio. Our key idea is to instead directly estimate these ratios using RF. RF collects in leaf nodes of each decision tree the class histograms of training examples. We use these class histograms for a nonparametric estimation of the distribution ratios. We derive the theoretical error bounds of a two-class (RF)2 . (RF)2 is applied to a challenging task of multiclass object recognition and segmentation over a random ﬁeld of input image regions. In our empirical evaluation, we use only the visual information provided by image regions (e.g., color, texture, spatial layout), whereas the competing methods additionally use higher-level cues about the horizon location and 3D layout of surfaces in the scene. Nevertheless, (RF)2 outperforms the state of the art on benchmark datasets, in terms of accuracy and computation time.</p><br/>
<h2>reference text</h2><p>[1] L.-J. Li, R. Socher, and L. Fei-Fei, “Towards total scene understanding: Classiﬁcation, annotation and segmentation in an automatic framework,” in CVPR, 2009.</p>
<p>[2] X. He, R. S. Zemel, and M. A. Carreira-Perpinan, “Multiscale Conditional Random Fields for image labeling,” in CVPR, 2004, pp. 695–702.</p>
<p>[3] J. Shotton, J. Winn, C. Rother, and A. Criminisi, “Textonboost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation,” in ECCV, 2006, pp. 1–15.</p>
<p>[4] J. Verbeek and B. Triggs, “Scene segmentation with CRFs learned from partially labeled images,” in NIPS, 2007.</p>
<p>[5] A. Torralba, K. P. Murphy, and W. T. Freeman, “Contextual models for object detection using boosted random ﬁelds,” in NIPS, 2004.</p>
<p>[6] S. Gould, T. Gao, and D. Koller, “Region-based segmentation and object detection,” in NIPS, 2009.</p>
<p>[7] A. Rabinovich, A. Vedaldi, C. Galleguillos, E. Wiewiora, and S. Belongie, “Objects in context,” in ICCV, 2007.</p>
<p>[8] N. Payet and S. Todorovic, “From a set of shapes to object discovery,” in ECCV, 2010.</p>
<p>[9] S. Todorovic and N. Ahuja, “Unsupervised category modeling, recognition, and segmentation in images,” IEEE TPAMI, vol. 30, no. 12, pp. 1–17, 2008.</p>
<p>[10] J. J. Lim, P. Arbelaez, C. Gu, and J. Malik, “Context by region ancestry,” in ICCV, 2009.</p>
<p>[11] J. Sivic, B. C. Russell, A. Zisserman, W. T. Freeman, and A. A. Efros, “Unsupervised discovery of visual object class hierarchies,” in CVPR, 2008.</p>
<p>[12] J. Lafferty, A. McCallum, and F. Pereira, “Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data,” in ICML, 2001, pp. 282–289.</p>
<p>[13] L. Breiman, “Random forests,” Mach. Learn., vol. 45, no. 1, pp. 5–32, 2001.</p>
<p>[14] J. Gall and V. Lempitsky, “Class-speciﬁc hough forests for object detection,” in CVPR, 2009.</p>
<p>[15] G. Martinez-Munoz, N. Larios, E. Mortensen, W. Zhang, A. Yamamuro, R. Paasch, N. Payet, D. Lytle, L. Shapiro, S. Todorovic, A. Moldenke, and T. Dietterich, “Dictionary-free categorization of very similar objects via stacked evidence trees,” in CVPR, 2009.</p>
<p>[16] Y. Lin and Y. Jeon, “Random forests and adaptive nearest neighbors,” Journal of the American Statistical Association, pp. 101–474, 2006.</p>
<p>[17] C. F. P. Arbelaez, M. Maire and J. Malik, “From contours to regions: An empirical evaluation,” in CVPR, 2009.</p>
<p>[18] A. Barbu and S.-C. Zhu, “Graph partition by Swendsen-Wang cuts,” in ICCV, 2003, p. 320.</p>
<p>[19] S. Bileschi and L. Wolf, “A uniﬁed system for object detection, texture recognition, and context analysis based on the standard model feature set,” in BMVC, 2005.</p>
<p>[20] C. Galleguillos, B. McFee, S. Belongie, and G. R. G. Lanckriet, “Multi-class object localization by combining local contextual interactions,” in CVPR, 2010.</p>
<p>[21] S. Gould, R. Fulton, and D. Koller, “Decomposing a scene into geometric and semantically consistent regions,” in ICCV, 2009.</p>
<p>[22] J. Shotton, M. Johnson, and R. Cipolla, “Semantic texton forests for image categorization and segmentation,” in CVPR, 2008.</p>
<p>[23] Z. Tu and X. Bai, “Auto-context and its application to high-level vision tasks and 3D brain image segmentation,” IEEE TPAMI, vol. 99, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
