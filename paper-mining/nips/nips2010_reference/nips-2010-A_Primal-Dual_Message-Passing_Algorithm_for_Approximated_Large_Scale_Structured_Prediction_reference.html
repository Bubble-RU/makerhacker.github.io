<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-13" href="../nips2010/nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">nips2010-13</a> <a title="nips-2010-13-reference" href="#">nips2010-13-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</h1>
<br/><p>Source: <a title="nips-2010-13-pdf" href="http://papers.nips.cc/paper/3913-a-primal-dual-message-passing-algorithm-for-approximated-large-scale-structured-prediction.pdf">pdf</a></p><p>Author: Tamir Hazan, Raquel Urtasun</p><p>Abstract: In this paper we propose an approximated structured prediction framework for large scale graphical models and derive message-passing algorithms for learning their parameters efﬁciently. We ﬁrst relate CRFs and structured SVMs and show that in CRFs a variant of the log-partition function, known as the soft-max, smoothly approximates the hinge loss function of structured SVMs. We then propose an intuitive approximation for the structured prediction problem, using duality, based on a local entropy approximation and derive an efﬁcient messagepassing algorithm that is guaranteed to converge. Unlike existing approaches, this allows us to learn efﬁciently graphical models with cycles and very large number of parameters. 1</p><br/>
<h2>reference text</h2><p>[1] M. Collins, A. Globerson, T. Koo, X. Carreras, and P.L. Bartlett. Exponentiated gradient algorithms for conditional random ﬁelds and max-margin markov networks. JMLR, 9:1775–1822, 2008.</p>
<p>[2] T. Finley and T. Joachims. Training structural SVMs when exact inference is intractable. In ICML, pages 304–311. ACM, 2008.</p>
<p>[3] V. Ganapathi, D. Vickrey, J. Duchi, and D. Koller. Constrained approximate maximum entropy learning of markov random ﬁelds. In UAI, 2008.</p>
<p>[4] K. Gimpel and N.A. Smith. Softmax-margin CRFs: Training log-linear models with cost functions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 733–736. Association for Computational Linguistics, 2010.</p>
<p>[5] T. Hazan and A. Shashua. Norm-Product Belief Propagation: Primal-Dual Message-Passing for Approximate Inference. Arxiv preprint arXiv:0903.3127, 2009.</p>
<p>[6] T. Hazan and R. Urtasun. Approximated Structured Prediction for Learning Large Scale Graphical Models. Arxiv preprint arXiv:1006.2899, 2010.</p>
<p>[7] T. Heskes. Convexity arguments for efﬁcient minimization of the Bethe and Kikuchi free energies. Journal of Artiﬁcial Intelligence Research, 26(1):153–190, 2006.</p>
<p>[8] J.K. Johnson, D.M. Malioutov, and A.S. Willsky. Lagrangian relaxation for MAP estimation in graphical models. In Proceedings of the Allerton Conference on Control, Communication and Computing. Citeseer, 2007.</p>
<p>[9] S. Kumar and M. Hebert. Discriminative Fields for Modeling Spatial Dependencies in Natural Images. In Neural Information Processing Systems. MIT Press, Cambridge, MA, 2003.</p>
<p>[10] J. Lafferty, A. McCallum, and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In ICML, pages 282–289, 2001.</p>
<p>[11] G. Lebanon and J. Lafferty. Boosting and maximum likelihood for exponential models. NIPS, 1:447–454, 2002.</p>
<p>[12] A. Levin and Y. Weiss. Learning to Combine Bottom-Up and Top-Down Segmentation. In European Conference on Computer Vision, 2006.</p>
<p>[13] T. Meltzer, A. Globerson, and Y. Weiss. Convergent message passing algorithms-a unifying view. In UAI, 2009.</p>
<p>[14] O. Meshi, D. Sontag, T. Jaakkola, and A. Globerson. Learning Efﬁciently with Approximate Inference via Dual Losses. In Proc. ICML. Citeseer, 2010.</p>
<p>[15] P. Pletscher, C. Ong, and J. Buhmann. Entropy and Margin Maximization for Structured Output Learning. Machine Learning and Knowledge Discovery in Databases, pages 83–98, 2010.</p>
<p>[16] N. Ratliff, J.A. Bagnell, and M. Zinkevich. Subgradient methods for maximum margin structured learning. In ICML Workshop on Learning in Structured Output Spaces, 2006.</p>
<p>[17] F. Sha and L.K. Saul. Large margin hidden Markov models for automatic speech recognition. Advances in neural information processing systems, 19:1249, 2007.</p>
<p>[18] C. Sutton and A. McCallum. Piecewise training for structured prediction. Machine Learning, 77(2):165– 194, 2009.</p>
<p>[19] B. Taskar. Learning structured prediction models: a large margin approach. PhD thesis, Stanford, CA, USA, 2005. Adviser-Koller, Daphne.</p>
<p>[20] B. Taskar, P. Abbeel, and D. Koller. Discriminative probabilistic models for relational data. In UAI, pages 895–902. Citeseer, 2002.</p>
<p>[21] B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. NIPS, 16:51, 2004.</p>
<p>[22] B. Taskar, S. Lacoste-Julien, and M. I. Jordan. Structured prediction, dual extragradient and Bregman projections. JMLR, 7:1653–1684, 2006.</p>
<p>[23] S. Vishwanathan, N. Schraudolph, M. Schmidt, and K. Murphy. Accelerated Training of Conditional Random Fields with Stochastic Meta-Descent . In International Conference in Machine Learning, 2006.</p>
<p>[24] M.J. Wainwright. Estimating the Wrong Graphical Model: Beneﬁts in the Computation-Limited Setting. JMLR, 7:1859, 2006.</p>
<p>[25] M.J. Wainwright and M.I. Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends R in Machine Learning, 1(1-2):1–305, 2008.</p>
<p>[26] J.S. Yedidia, W. T. Freeman, and Y. Weiss. Constructing free-energy approximations and generalized belief propagation algorithms. Transactions on Information Theory, 51(7):2282–2312, 2005.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
