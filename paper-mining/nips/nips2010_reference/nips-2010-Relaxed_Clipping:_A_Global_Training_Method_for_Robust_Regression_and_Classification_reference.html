<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-225" href="../nips2010/nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification.html">nips2010-225</a> <a title="nips-2010-225-reference" href="#">nips2010-225-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</h1>
<br/><p>Source: <a title="nips-2010-225-pdf" href="http://papers.nips.cc/paper/3936-relaxed-clipping-a-global-training-method-for-robust-regression-and-classification.pdf">pdf</a></p><p>Author: Min Yang, Linli Xu, Martha White, Dale Schuurmans, Yao-liang Yu</p><p>Abstract: Robust regression and classiﬁcation are often thought to require non-convex loss functions that prevent scalable, global training. However, such a view neglects the possibility of reformulated training methods that can yield practically solvable alternatives. A natural way to make a loss function more robust to outliers is to truncate loss values that exceed a maximum threshold. We demonstrate that a relaxation of this form of “loss clipping” can be made globally solvable and applicable to any standard loss while guaranteeing robustness against outliers. We present a generic procedure that can be applied to standard loss functions and demonstrate improved robustness in regression and classiﬁcation problems. 1</p><br/>
<h2>reference text</h2><p>[1] A. Banerjee, S. Merugu, I. Dhillon, and J. Ghosh. Clustering with Bregman divergences. Journal of Machine Learning Research, 6:1705–1749, 2005.</p>
<p>[2] M. Black and A. Rangarajan. On the uniﬁcation of line processes, outlier rejection, and robust statistics with applications in early vision. International Journal of Computer Vision, 19(1):57–91, 1996.</p>
<p>[3] O. Bousquet and A. Elisseeff. Stability and generalization. J. of Machine Learning Research, 2, 2002.</p>
<p>[4] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge U. Press, 2004.</p>
<p>[5] A. Christmann and I. Steinwart. On robustness properties of convex risk minimization methods for pattern recognition. Journal of Machine Learning Research, 5:1007–1034, 2004.</p>
<p>[6] A. Christmann and I. Steinwart. Consistency and robustness of kernel-based regression in convex risk minimization. Bernoulli, 13(3):799–819, 2007.</p>
<p>[7] M. Collins, R. Schapire, and Y. Singer. Logistic regression, AdaBoost and Bregman distances. Machine Learning, 48, 2002.</p>
<p>[8] Y. Freund. A more robust boosting algorithm, 2009. arXiv.org:0905.2138.</p>
<p>[9] Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and Systems Sciences, 55(1):119–139, 1997.</p>
<p>[10] G. Golub and C. Van Loan. Matrix Computations. Johns Hopkins U. Press, 1996.</p>
<p>[11] F. Hampel, E. Ronchetti, P. Rousseeuw, and W. Stahel. Robust Statistics: The Approach Based on Inﬂuence Functions. Wiley, 1986.</p>
<p>[12] T. Hastie, R. Tibshirani, and J. Friedman. Elements of Statistical Learning. Springer, 2nd edition, 2009.</p>
<p>[13] P. Huber and E. Ronchetti. Robust Statistics. Wiley, 2nd edition, 2009.</p>
<p>[14] J. Kivinen and M. Warmuth. Relative loss bounds for multidimensional regression problems. Machine Learning, 45:301–329, 2001.</p>
<p>[15] N. Krause and Y. Singer. Leveraging the margin more carefully. In Proceedings of the International Conference on Machine Learning (ICML), 2004.</p>
<p>[16] P. Long and R. Servedio. Random classiﬁcation noise defeats all convex potential boosters. Machine Learning, 78:287–304, 2010.</p>
<p>[17] R. Maronna, R.D. Martin, and V. Yohai. Robust Statistics: Theory and Methods. Wiley, 2006.</p>
<p>[18] H. Masnadi-Shirazi and N. Vasconcelos. On the design of loss functions for classiﬁcation: theory, robustness to outliers, and SavageBoost. In Advances in Neural Information Processing Systems (NIPS), volume 21, pages 1049–1056, 2008.</p>
<p>[19] L. Mason, J. Baxter, P. Bartlett, and M. Frean. Functional gradient techniques for combining hypotheses. In Advances in Large Margin Classiﬁers. MIT Press, 2000.</p>
<p>[20] Y. Nesterov. Introductory Lectures on Convex Optimization. Kluwer, 1994.</p>
<p>[21] M. Overton and R. Womersley. Optimality conditions and duality theory for minimizing sums of the largest eigenvalues of symmetric matrices. Mathematical Programming, 62(2):321–357, 1993.</p>
<p>[22] R. Rockafellar. Convex Analysis. Princeton U. Press, 1970.</p>
<p>[23] P. Rousseeuw and A. Leroy. Robust Regression and Outlier Detection. Wiley, 1987.</p>
<p>[24] B. Schoelkopf and A. Smola. Learning with Kernels. MIT Press, 2002.</p>
<p>[25] X. Shen, G. Tseng, X. Zhang, and W.-H. Wong. On ψ-learning. Journal of the American Statistical Association, 98(463):724–734, 2003.</p>
<p>[26] C. Stewart. Robust parameter estimation in computer vision. SIAM Review, 41(3), 1999.</p>
<p>[27] Y. Wu and Y. Liu. Robust truncated hinge loss support vector machines. Journal of the American Statistical Association, 102(479):974–983, 2007.</p>
<p>[28] H. Xu, C. Caramanis, and S. Mannor. Robust regression and Lasso. In Advances in Neural Information Processing Systems (NIPS), volume 21, pages 1801–1808, 2008.</p>
<p>[29] H. Xu, C. Caramanis, and S. Mannor. Robustness and regularization of support vector machines. Journal of Machine Learning Research, 10:1485–1510, 2009.</p>
<p>[30] L. Xu, K. Crammer, and D. Schuurmans. Robust support vector machine training via convex outlier ablation. In Proceedings of the National Conference on Artiﬁcial Intelligence (AAAI), 2006.</p>
<p>[31] J. Yu, S. Vishwanathan, S. G¨ nter, and N. Schraudolph. A quasi-Newton approach to nonsmooth convex u optimization problems in machine learning. J. of Machine Learning Research, 11:1145–1200, 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
