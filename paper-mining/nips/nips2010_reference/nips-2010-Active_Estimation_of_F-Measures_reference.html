<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>22 nips-2010-Active Estimation of F-Measures</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-22" href="../nips2010/nips-2010-Active_Estimation_of_F-Measures.html">nips2010-22</a> <a title="nips-2010-22-reference" href="#">nips2010-22-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>22 nips-2010-Active Estimation of F-Measures</h1>
<br/><p>Source: <a title="nips-2010-22-pdf" href="http://papers.nips.cc/paper/3999-active-estimation-of-f-measures.pdf">pdf</a></p><p>Author: Christoph Sawade, Niels Landwehr, Tobias Scheffer</p><p>Abstract: We address the problem of estimating the Fα -measure of a given model as accurately as possible on a ﬁxed labeling budget. This problem occurs whenever an estimate cannot be obtained from held-out training data; for instance, when data that have been used to train the model are held back for reasons of privacy or do not reﬂect the test distribution. In this case, new test instances have to be drawn and labeled at a cost. An active estimation procedure selects instances according to an instrumental sampling distribution. An analysis of the sources of estimation error leads to an optimal sampling distribution that minimizes estimator variance. We explore conditions under which active estimates of Fα -measures are more accurate than estimates based on instances sampled from the test distribution. 1</p><br/>
<h2>reference text</h2><p>[1] F. Bach. Active learning for misspeciﬁed generalized linear models. In Advances in Neural Information Processing Systems, 2007.</p>
<p>[2] A. Beygelzimer, S. Dasgupta, and J. Langford. Importance weighted active learning. In Proceedings of the International Conference on Machine Learning, 2009.</p>
<p>[3] H Cram´ r. Mathematical Methods of Statistics, chapter 20. Princeton University Press, 1946. e</p>
<p>[4] A. Frank and A. Asuncion. UCI machine learning repository, 2010.</p>
<p>[5] S. Geman, E. Bienenstock, and R. Doursat. Neural networks and the bias/variance dilemma. Neural Computation, 4:1–58, 1992.</p>
<p>[6] J. Hammersley and D. Handscomb. Monte carlo methods. Taylor & Francis, 1964.</p>
<p>[7] C. Sawade, N. Landwehr, S. Bickel, and T. Scheffer. Active risk estimation. In Proceedings of the 27th International Conference on Machine Learning, 2010.</p>
<p>[8] M. Sugiyama. Active learning in approximately linear regression based on conditional expectation of generalization error. Journal of Machine Learning Research, 7:141–166, 2006.</p>
<p>[9] S. Tong and D. Koller. Support vector machine active learning with applications to text classiﬁcation. Journal of Machine Learning Research, pages 45–66, 2002.</p>
<p>[10] C. van Rijsbergen. Information Retrieval. Butterworths, 2nd edition, 1979.</p>
<p>[11] M. Yamada, M. Sugiyama, and T. Matsui. Semi-supervised speaker identiﬁcation under covariate shift. Signal Processing, 90(8):2353–2361, 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
