<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>nips 2012 knowledge graph</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="#">nips2012</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>nips 2012 knowledge graph</h1>
<br/><h3>similar papers computed by tfidf model</h3><br/><h3>similar papers computed by <a title="lsi-model" href="./nips2012_lsi.html">lsi model</a></h3><br/><h3>similar papers computed by <a title="lda-model" href="./nips2012_lda.html">lda model</a></h3><br/><h2>papers list:</h2><p>1 <a title="nips-2012-1" href="../nips2012/nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>Author: Sanja Fidler, Sven Dickinson, Raquel Urtasun</p><p>Abstract: This paper addresses the problem of category-level 3D object detection. Given a monocular image, our aim is to localize the objects in 3D by enclosing them with tight oriented 3D bounding boxes. We propose a novel approach that extends the well-acclaimed deformable part-based model [1] to reason in 3D. Our model represents an object class as a deformable 3D cuboid composed of faces and parts, which are both allowed to deform with respect to their anchors on the 3D box. We model the appearance of each face in fronto-parallel coordinates, thus effectively factoring out the appearance variation induced by viewpoint. Our model reasons about face visibility patters called aspects. We train the cuboid model jointly and discriminatively and share weights across all aspects to attain efﬁciency. Inference then entails sliding and rotating the box in 3D and scoring object hypotheses. While for inference we discretize the search space, the variables are continuous in our model. We demonstrate the effectiveness of our approach in indoor and outdoor scenarios, and show that our approach signiﬁcantly outperforms the stateof-the-art in both 2D [1] and 3D object detection [2]. 1</p><p>2 <a title="nips-2012-2" href="../nips2012/nips-2012-3D_Social_Saliency_from_Head-mounted_Cameras.html">nips-2012-3D Social Saliency from Head-mounted Cameras</a></p>
<p>Author: Hyun S. Park, Eakta Jain, Yaser Sheikh</p><p>Abstract: A gaze concurrence is a point in 3D where the gaze directions of two or more people intersect. It is a strong indicator of social saliency because the attention of the participating group is focused on that point. In scenes occupied by large groups of people, multiple concurrences may occur and transition over time. In this paper, we present a method to construct a 3D social saliency ďŹ eld and locate multiple gaze concurrences that occur in a social scene from videos taken by head-mounted cameras. We model the gaze as a cone-shaped distribution emanating from the center of the eyes, capturing the variation of eye-in-head motion. We calibrate the parameters of this distribution by exploiting the ďŹ xed relationship between the primary gaze ray and the head-mounted camera pose. The resulting gaze model enables us to build a social saliency ďŹ eld in 3D. We estimate the number and 3D locations of the gaze concurrences via provably convergent modeseeking in the social saliency ďŹ eld. Our algorithm is applied to reconstruct multiple gaze concurrences in several real world scenes and evaluated quantitatively against motion-captured ground truth. 1</p><p>3 <a title="nips-2012-3" href="../nips2012/nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>Author: Aaron Wilson, Alan Fern, Prasad Tadepalli</p><p>Abstract: We consider the problem of learning control policies via trajectory preference queries to an expert. In particular, the agent presents an expert with short runs of a pair of policies originating from the same state and the expert indicates which trajectory is preferred. The agent’s goal is to elicit a latent target policy from the expert with as few queries as possible. To tackle this problem we propose a novel Bayesian model of the querying process and introduce two methods that exploit this model to actively select expert queries. Experimental results on four benchmark problems indicate that our model can effectively learn policies from trajectory preference queries and that active query selection can be substantially more efﬁcient than random selection. 1</p><p>4 <a title="nips-2012-4" href="../nips2012/nips-2012-A_Better_Way_to_Pretrain_Deep_Boltzmann_Machines.html">nips-2012-A Better Way to Pretrain Deep Boltzmann Machines</a></p>
<p>Author: Geoffrey E. Hinton, Ruslan Salakhutdinov</p><p>Abstract: We describe how the pretraining algorithm for Deep Boltzmann Machines (DBMs) is related to the pretraining algorithm for Deep Belief Networks and we show that under certain conditions, the pretraining procedure improves the variational lower bound of a two-hidden-layer DBM. Based on this analysis, we develop a different method of pretraining DBMs that distributes the modelling work more evenly over the hidden layers. Our results on the MNIST and NORB datasets demonstrate that the new pretraining algorithm allows us to learn better generative models. 1</p><p>5 <a title="nips-2012-5" href="../nips2012/nips-2012-A_Conditional_Multinomial_Mixture_Model_for_Superset_Label_Learning.html">nips-2012-A Conditional Multinomial Mixture Model for Superset Label Learning</a></p>
<p>Author: Liping Liu, Thomas G. Dietterich</p><p>Abstract: In the superset label learning problem (SLL), each training instance provides a set of candidate labels of which one is the true label of the instance. As in ordinary regression, the candidate label set is a noisy version of the true label. In this work, we solve the problem by maximizing the likelihood of the candidate label sets of training instances. We propose a probabilistic model, the Logistic StickBreaking Conditional Multinomial Model (LSB-CMM), to do the job. The LSBCMM is derived from the logistic stick-breaking process. It ﬁrst maps data points to mixture components and then assigns to each mixture component a label drawn from a component-speciﬁc multinomial distribution. The mixture components can capture underlying structure in the data, which is very useful when the model is weakly supervised. This advantage comes at little cost, since the model introduces few additional parameters. Experimental tests on several real-world problems with superset labels show results that are competitive or superior to the state of the art. The discovered underlying structures also provide improved explanations of the classiﬁcation predictions. 1</p><p>6 <a title="nips-2012-6" href="../nips2012/nips-2012-A_Convex_Formulation_for_Learning_Scale-Free_Networks_via_Submodular_Relaxation.html">nips-2012-A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation</a></p>
<p>Author: Aaron Defazio, Tibério S. Caetano</p><p>Abstract: A key problem in statistics and machine learning is the determination of network structure from data. We consider the case where the structure of the graph to be reconstructed is known to be scale-free. We show that in such cases it is natural to formulate structured sparsity inducing priors using submodular functions, and we use their Lov´ sz extension to obtain a convex relaxation. For tractable classes a such as Gaussian graphical models, this leads to a convex optimization problem that can be efﬁciently solved. We show that our method results in an improvement in the accuracy of reconstructed networks for synthetic data. We also show how our prior encourages scale-free reconstructions on a bioinfomatics dataset.</p><p>7 <a title="nips-2012-7" href="../nips2012/nips-2012-A_Divide-and-Conquer_Method_for_Sparse_Inverse_Covariance_Estimation.html">nips-2012-A Divide-and-Conquer Method for Sparse Inverse Covariance Estimation</a></p>
<p>Author: Cho-jui Hsieh, Arindam Banerjee, Inderjit S. Dhillon, Pradeep K. Ravikumar</p><p>Abstract: We consider the composite log-determinant optimization problem, arising from the 1 regularized Gaussian maximum likelihood estimator of a sparse inverse covariance matrix, in a high-dimensional setting with a very large number of variables. Recent work has shown this estimator to have strong statistical guarantees in recovering the true structure of the sparse inverse covariance matrix, or alternatively the underlying graph structure of the corresponding Gaussian Markov Random Field, even in very high-dimensional regimes with a limited number of samples. In this paper, we are concerned with the computational cost in solving the above optimization problem. Our proposed algorithm partitions the problem into smaller sub-problems, and uses the solutions of the sub-problems to build a good approximation for the original problem. Our key idea for the divide step to obtain a sub-problem partition is as follows: we ﬁrst derive a tractable bound on the quality of the approximate solution obtained from solving the corresponding sub-divided problems. Based on this bound, we propose a clustering algorithm that attempts to minimize this bound, in order to ﬁnd effective partitions of the variables. For the conquer step, we use the approximate solution, i.e., solution resulting from solving the sub-problems, as an initial point to solve the original problem, and thereby achieve a much faster computational procedure. 1</p><p>8 <a title="nips-2012-8" href="../nips2012/nips-2012-A_Generative_Model_for_Parts-based_Object_Segmentation.html">nips-2012-A Generative Model for Parts-based Object Segmentation</a></p>
<p>Author: S. Eslami, Christopher Williams</p><p>Abstract: The Shape Boltzmann Machine (SBM) [1] has recently been introduced as a stateof-the-art model of foreground/background object shape. We extend the SBM to account for the foreground object’s parts. Our new model, the Multinomial SBM (MSBM), can capture both local and global statistics of part shapes accurately. We combine the MSBM with an appearance model to form a fully generative model of images of objects. Parts-based object segmentations are obtained simply by performing probabilistic inference in the model. We apply the model to two challenging datasets which exhibit signiﬁcant shape and appearance variability, and ﬁnd that it obtains results that are comparable to the state-of-the-art. There has been signiﬁcant focus in computer vision on object recognition and detection e.g. [2], but a strong desire remains to obtain richer descriptions of objects than just their bounding boxes. One such description is a parts-based object segmentation, in which an image is partitioned into multiple sets of pixels, each belonging to either a part of the object of interest, or its background. The signiﬁcance of parts in computer vision has been recognized since the earliest days of the ﬁeld (e.g. [3, 4, 5]), and there exists a rich history of work on probabilistic models for parts-based segmentation e.g. [6, 7]. Many such models only consider local neighborhood statistics, however several models have recently been proposed that aim to increase the accuracy of segmentations by also incorporating prior knowledge about the foreground object’s shape [8, 9, 10, 11]. In such cases, probabilistic techniques often mainly differ in how accurately they represent and learn about the variability exhibited by the shapes of the object’s parts. Accurate models of the shapes and appearances of parts can be necessary to perform inference in datasets that exhibit large amounts of variability. In general, the stronger the models of these two components, the more performance is improved. A generative model has the added beneﬁt of being able to generate samples, which allows us to visually inspect the quality of its understanding of the data and the problem. Recently, a generative probabilistic model known as the Shape Boltzmann Machine (SBM) has been used to model binary object shapes [1]. The SBM has been shown to constitute the state-of-the-art and it possesses several highly desirable characteristics: samples from the model look realistic, and it generalizes to generate samples that differ from the limited number of examples it is trained on. The main contributions of this paper are as follows: 1) In order to account for object parts we extend the SBM to use multinomial visible units instead of binary ones, resulting in the Multinomial Shape Boltzmann Machine (MSBM), and we demonstrate that the MSBM constitutes a strong model of parts-based object shape. 2) We combine the MSBM with an appearance model to form a fully generative model of images of objects (see Fig. 1). We show how parts-based object segmentations can be obtained simply by performing probabilistic inference in the model. We apply our model to two challenging datasets and ﬁnd that in addition to being principled and fully generative, the model’s performance is comparable to the state-of-the-art. 1 Train labels Train images Test image Appearance model Joint Model Shape model Parsing Figure 1: Overview. Using annotated images separate models of shape and appearance are trained. Given an unseen test image, its parsing is obtained via inference in the proposed joint model. In Secs. 1 and 2 we present the model and propose efﬁcient inference and learning schemes. In Sec. 3 we compare and contrast the resulting joint model with existing work in the literature. We describe our experimental results in Sec. 4 and conclude with a discussion in Sec. 5. 1 Model We consider datasets of cropped images of an object class. We assume that the images are constructed through some combination of a ﬁxed number of parts. Given a dataset D = {Xd }, d = 1...n of such images X, each consisting of P pixels {xi }, i = 1...P , we wish to infer a segmentation S for the image. S consists of a labeling si for every pixel, where si is a 1-of-(L+1) encoded variable, and L is the ﬁxed number of parts that combine to generate the foreground. In other words, si = (sli ), P l = 0...L, sli 2 {0, 1} and l sli = 1. Note that the background is also treated as a ‘part’ (l = 0). Accurate inference of S is driven by models for 1) part shapes and 2) part appearances. Part shapes: Several types of models can be used to deﬁne probabilistic distributions over segmentations S. The simplest approach is to model each pixel si independently with categorical variables whose parameters are speciﬁed by the object’s mean shape (Fig. 2(a)). Markov Random Fields (MRFs, Fig. 2(b)) additionally model interactions between nearby pixels using pairwise potential functions that efﬁciently capture local properties of images like smoothness and continuity. Restricted Boltzmann Machines (RBMs) and their multi-layered counterparts Deep Boltzmann Machines (DBMs, Fig. 2(c)) make heavy use of hidden variables to efﬁciently deﬁne higher-order potentials that take into account the conﬁguration of larger groups of image pixels. The introduction of such hidden variables provides a way to efﬁciently capture complex, global properties of image pixels. RBMs and DBMs are powerful generative models, but they also have many parameters. Segmented images, however, are expensive to obtain and datasets are typically small (hundreds of examples). In order to learn a model that accurately captures the properties of part shapes we use DBMs but also impose carefully chosen connectivity and capacity constraints, following the structure of the Shape Boltzmann Machine (SBM) [1]. We further extend the model to account for multi-part shapes to obtain the Multinomial Shape Boltzmann Machine (MSBM). The MSBM has two layers of latent variables: h1 and h2 (collectively H = {h1 , h2 }), and deﬁnes a P Boltzmann distribution over segmentations p(S) = h1 ,h2 exp{ E(S, h1 , h2 |✓s )}/Z(✓s ) where X X X X X 1 2 E(S, h1 , h2 |✓s ) = bli sli + wlij sli h1 + c 1 h1 + wjk h1 h2 + c2 h2 , (1) j j j j k k k i,l j i,j,l j,k k where j and k range over the ﬁrst and second layer hidden variables, and ✓s = {W 1 , W 2 , b, c1 , c2 } are the shape model parameters. In the ﬁrst layer, local receptive ﬁelds are enforced by connecting each hidden unit in h1 only to a subset of the visible units, corresponding to one of four patches, as shown in Fig. 2(d,e). Each patch overlaps its neighbor by b pixels, which allows boundary continuity to be learned at the lowest layer. We share weights between the four sets of ﬁrst-layer hidden units and patches, and purposely restrict the number of units in h2 . These modiﬁcations signiﬁcantly reduce the number of parameters whilst taking into account an important property of shapes, namely that the strongest dependencies between pixels are typically local. 2 h2 1 1 h S S (a) Mean h S (b) MRF h2 h2 h1 S S (c) DBM b (d) SBM (e) 2D SBM Figure 2: Models of shape. Object shape is modeled with undirected graphical models. (a) 1D slice of a mean model. (b) Markov Random Field in 1D. (c) Deep Boltzmann Machine in 1D. (d) 1D slice of a Shape Boltzmann Machine. (e) Shape Boltzmann Machine in 2D. In all models latent units h are binary and visible units S are multinomial random variables. Based on Fig. 2 of [1]. k=1 k=2 k=3 k=1 k=2 k=3 k=1 k=2 k=3 ⇡ l=0 l=1 l=2 Figure 3: A model of appearances. Left: An exemplar dataset. Here we assume one background (l = 0) and two foreground (l = 1, non-body; l = 2, body) parts. Right: The corresponding appearance model. In this example, L = 2, K = 3 and W = 6. Best viewed in color. Part appearances: Pixels in a given image are assumed to have been generated by W ﬁxed Gaussians in RGB space. During pre-training, the means {µw } and covariances {⌃w } of these Gaussians are extracted by training a mixture model with W components on every pixel in the dataset, ignoring image and part structure. It is also assumed that each of the L parts can have different appearances in different images, and that these appearances can be clustered into K classes. The classes differ in how likely they are to use each of the W components when ‘coloring in’ the part. The generative process is as follows. For part l in an image, one of the K classes is chosen (represented by a 1-of-K indicator variable al ). Given al , the probability distribution deﬁned on pixels associated with part l is given by a Gaussian mixture model with means {µw } and covariances {⌃w } and mixing proportions { lkw }. The prior on A = {al } speciﬁes the probability ⇡lk of appearance class k being chosen for part l. Therefore appearance parameters ✓a = {⇡lk , lkw } (see Fig. 3) and: a p(xi |A, si , ✓ ) = p(A|✓a ) = Y l Y l a sli p(xi |al , ✓ ) p(al |✓a ) = = Y Y X YY l l k w lkw N (xi |µw , ⌃w ) !alk !sli (⇡lk )alk . , (2) (3) k Combining shapes and appearances: To summarize, the latent variables for X are A, S, H, and the model’s active parameters ✓ include shape parameters ✓s and appearance parameters ✓a , so that p(X, A, S, H|✓) = Y 1 p(A|✓a )p(S, H|✓s ) p(xi |A, si , ✓a ) , Z( ) i (4) where the parameter adjusts the relative contributions of the shape and appearance components. See Fig. 4 for an illustration of the complete graphical model. During learning, we ﬁnd the values of ✓ that maximize the likelihood of the training data D, and segmentation is performed on a previously-unseen image by querying the marginal distribution p(S|Xtest , ✓). Note that Z( ) is constant throughout the execution of the algorithms. We set via trial and error in our experiments. 3 n H ✓a si al H xi L+1 ✓s S X A P Figure 4: A model of shape and appearance. Left: The joint model. Pixels xi are modeled via appearance variables al . The model’s belief about each layer’s shape is captured by shape variables H. Segmentation variables si assign each pixel to a layer. Right: Schematic for an image X. 2 Inference and learning Inference: We approximate p(A, S, H|X, ✓) by drawing samples of A, S and H using block-Gibbs Markov Chain Monte Carlo (MCMC). The desired distribution p(S|X, ✓) can then be obtained by considering only the samples for S (see Algorithm 1). In order to sample p(A|S, H, X, ✓) we consider the conditional distribution of appearance class k being chosen for part l which is given by: Q P ·s ⇡lk i ( w lkw N (xi |µw , ⌃w )) li h Q P i. p(alk = 1|S, X, ✓) = P (5) K ·sli r=1 ⇡lr i( w lrw N (xi |µw , ⌃w )) Since the MSBM only has edges between each pair of adjacent layers, all hidden units within a layer are conditionally independent given the units in the other two layers. This property can be exploited to make inference in the shape model exact and efﬁcient. The conditional probabilities are: X X 1 2 p(h1 = 1|s, h2 , ✓) = ( wlij sli + wjk h2 + c1 ), (6) j k j i,l p(h2 k 1 = 1|h , ✓) = ( X k 2 wjk h1 j + c2 ), j (7) j where (y) = 1/(1 + exp( y)) is the sigmoid function. To sample from p(H|S, X, ✓) we iterate between Eqns. 6 and 7 multiple times and keep only the ﬁnal values of h1 and h2 . Finally, we draw samples for the pixels in p(S|A, H, X, ✓) independently: P 1 exp( j wlij h1 + bli ) p(xi |A, sli = 1, ✓) j p(sli = 1|A, H, X, ✓) = PL . (8) P 1 1 m=1 exp( j wmij hj + bmi ) p(xi |A, smi = 1, ✓) Seeding: Since the latent-space is extremely high-dimensional, in practice we ﬁnd it helpful to run several inference chains, each initializing S(1) to a different value. The ‘best’ inference is retained and the others are discarded. The computation of the likelihood p(X|✓) of image X is intractable, so we approximate the quality of each inference using a scoring function: 1X Score(X|✓) = p(X, A(t) , S(t) , H(t) |✓), (9) T t where {A(t) , S(t) , H(t) }, t = 1...T are the samples obtained from the posterior p(A, S, H|X, ✓). If the samples were drawn from the prior p(A, S, H|✓) the scoring function would be an unbiased estimator of p(X|✓), but would be wildly inaccurate due to the high probability of missing the important regions of latent space (see e.g. [12, p. 107-109] for further discussion of this issue). Learning: Learning of the model involves maximizing the log likelihood log p(D|✓a , ✓s ) of the training dataset D with respect to the model parameters ✓a and ✓s . Since training is partially supervised, in that for each image X its corresponding segmentation S is also given, we can learn the parameters of the shape and appearance components separately. For appearances, the learning of the mixing coefﬁcients and the histogram parameters decomposes into standard mixture updates independently for each part. For shapes, we follow the standard deep 4 Algorithm 1 MCMC inference algorithm. 1: procedure I NFER(X, ✓) 2: Initialize S(1) , H(1) 3: for t 2 : chain length do 4: A(t) ⇠ p(A|S(t 1) , H(t 1) , X, ✓) 5: S(t) ⇠ p(S|A(t) , H(t 1) , X, ✓) 6: H(t) ⇠ p(H|S(t) , ✓) 7: return {S(t) }t=burnin:chain length learning literature closely [13, 1]. In the pre-training phase we greedily train the model bottom up, one layer at a time. We begin by training an RBM on the observed data using stochastic maximum likelihood learning (SML; also referred to as ‘persistent CD’; [14, 13]). Once this RBM is trained, we infer the conditional mean of the hidden units for each training image. The resulting vectors then serve as the training data for a second RBM which is again trained using SML. We use the parameters of these two RBMs to initialize the parameters of the full MSBM model. In the second phase we perform approximate stochastic gradient ascent in the likelihood of the full model to ﬁnetune the parameters in an EM-like scheme as described in [13]. 3 Related work Existing probabilistic models of images can be categorized by the amount of variability they expect to encounter in the data and by how they model this variability. A signiﬁcant portion of the literature models images using only two parts: a foreground object and its background e.g. [15, 16, 17, 18, 19]. Models that account for the parts within the foreground object mainly differ in how accurately they learn about and represent the variability of the shapes of the object’s parts. In Probabilistic Index Maps (PIMs) [8] a mean partitioning is learned, and the deformable PIM [9] additionally allows for local deformations of this mean partitioning. Stel Component Analysis [10] accounts for larger amounts of shape variability by learning a number of different template means for the object that are blended together on a pixel-by-pixel basis. Factored Shapes and Appearances [11] models global properties of shape using a factor analysis-like model, and ‘masked’ RBMs have been used to model more local properties of shape [20]. However, none of these models constitute a strong model of shape in terms of realism of samples and generalization capabilities [1]. We demonstrate in Sec. 4 that, like the SBM, the MSBM does in fact possess these properties. The closest works to ours in terms of ability to deal with datasets that exhibit signiﬁcant variability in both shape and appearance are the works of Bo and Fowlkes [21] and Thomas et al. [22]. Bo and Fowlkes [21] present an algorithm for pedestrian segmentation that models the shapes of the parts using several template means. The different parts are composed using hand coded geometric constraints, which means that the model cannot be automatically extended to other application domains. The Implicit Shape Model (ISM) used in [22] is reliant on interest point detectors and deﬁnes distributions over segmentations only in the posterior, and therefore is not fully generative. The model presented here is entirely learned from data and fully generative, therefore it can be applied to new datasets and diagnosed with relative ease. Due to its modular structure, we also expect it to rapidly absorb future developments in shape and appearance models. 4 Experiments Penn-Fudan pedestrians: The ﬁrst dataset that we considered is Penn-Fudan pedestrians [23], consisting of 169 images of pedestrians (Fig. 6(a)). The images are annotated with ground-truth segmentations for L = 7 different parts (hair, face, upper and lower clothes, shoes, legs, arms; Fig. 6(d)). We compare the performance of the model with the algorithm of Bo and Fowlkes [21]. For the shape component, we trained an MSBM on the 684 images of a labeled version of the HumanEva dataset [24] (at 48 ⇥ 24 pixels; also ﬂipped horizontally) with overlap b = 4, and 400 and 50 hidden units in the ﬁrst and second layers respectively. Each layer was pre-trained for 3000 epochs (iterations). After pre-training, joint training was performed for 1000 epochs. 5 (c) Completion (a) Sampling (b) Diffs ! ! ! Figure 5: Learned shape model. (a) A chain of samples (1000 samples between frames). The apparent ‘blurriness’ of samples is not due to averaging or resizing. We display the probability of each pixel belonging to different parts. If, for example, there is a 50-50 chance that a pixel belongs to the red or blue parts, we display that pixel in purple. (b) Differences between the samples and their most similar counterparts in the training dataset. (c) Completion of occlusions (pink). To assess the realism and generalization characteristics of the learned MSBM we sample from it. In Fig. 5(a) we show a chain of unconstrained samples from an MSBM generated via block-Gibbs MCMC (1000 samples between frames). The model captures highly non-linear correlations in the data whilst preserving the object’s details (e.g. face and arms). To demonstrate that the model has not simply memorized the training data, in Fig. 5(b) we show the difference between the sampled shapes in Fig. 5(a) and their closest images in the training set (based on per-pixel label agreement). We see that the model generalizes in non-trivial ways to generate realistic shapes that it had not encountered during training. In Fig. 5(c) we show how the MSBM completes rectangular occlusions. The samples highlight the variability in possible completions captured by the model. Note how, e.g. the length of the person’s trousers on one leg affects the model’s predictions for the other, demonstrating the model’s knowledge about long-range dependencies. An interactive M ATLAB GUI for sampling from this MSBM has been included in the supplementary material. The Penn-Fudan dataset (at 200 ⇥ 100 pixels) was then split into 10 train/test cross-validation splits without replacement. We used the training images in each split to train the appearance component with a vocabulary of size W = 50 and K = 100 mixture components1 . We additionally constrained the model by sharing the appearance models for the arms and legs with that of the face. We assess the quality of the appearance model by performing the following experiment: for each test image, we used the scoring function described in Eq. 9 to evaluate a number of different proposal segmentations for that image. We considered 10 randomly chosen segmentations from the training dataset as well as the ground-truth segmentation for the test image, and found that the appearance model correctly assigns the highest score to the ground-truth 95% of the time. During inference, the shape and appearance models (which are deﬁned on images of different sizes), were combined at 200 ⇥ 100 pixels via M ATLAB’s imresize function, and we set = 0.8 (Eq. 8) via trial and error. Inference chains were seeded at 100 exemplar segmentations from the HumanEva dataset (obtained using the K-medoids algorithm with K = 100), and were run for 20 Gibbs iterations each (with 5 iterations of Eqs. 6 and 7 per Gibbs iteration). Our unoptimized M ATLAB implementation completed inference for each chain in around 7 seconds. We compute the conditional probability of each pixel belonging to different parts given the last set of samples obtained from the highest scoring chain, assign each pixel independently to the most likely part at that pixel, and report the percentage of correctly labeled pixels (see Table 1). We ﬁnd that accuracy can be improved using superpixels (SP) computed on X (pixels within a superpixel are all assigned the most common label within it; as with [21] we use gPb-OWT-UCM [25]). We also report the accuracy obtained, had the top scoring seed segmentation been returned as the ﬁnal segmentation for each image. Here the quality of the seed is determined solely by the appearance model. We observe that the model has comparable performance to the state-of-the-art but pedestrianspeciﬁc algorithm of [21], and that inference in the model signiﬁcantly improves the accuracy of the segmentations over the baseline (top seed+SP). Qualitative results can be seen in Fig. 6(c). 1 We obtained the best quantitative results with these settings. The appearances exhibited by the parts in the dataset are highly varied, and the complexity of the appearance model reﬂects this fact. 6 Table 1: Penn-Fudan pedestrians. We report the percentage of correctly labeled pixels. The ﬁnal column is an average of the background, upper and lower body scores (as reported in [21]). FG BG Upper Body Lower Body Head Average Bo and Fowlkes [21] 73.3% 81.1% 73.6% 71.6% 51.8% 69.5% MSBM MSBM + SP 70.7% 71.6% 72.8% 73.8% 68.6% 69.9% 66.7% 68.5% 53.0% 54.1% 65.3% 66.6% Top seed Top seed + SP 59.0% 61.6% 61.8% 67.3% 56.8% 60.8% 49.8% 54.1% 45.5% 43.5% 53.5% 56.4% Table 2: ETHZ cars. We report the percentage of pixels belonging to each part that are labeled correctly. The ﬁnal column is an average weighted by the frequency of occurrence of each label. BG Body Wheel Window Bumper License Light Average ISM [22] 93.2% 72.2% 63.6% 80.5% 73.8% 56.2% 34.8% 86.8% MSBM 94.6% 72.7% 36.8% 74.4% 64.9% 17.9% 19.9% 86.0% Top seed 92.2% 68.4% 28.3% 63.8% 45.4% 11.2% 15.1% 81.8% ETHZ cars: The second dataset that we considered is the ETHZ labeled cars dataset [22], which itself is a subset of the LabelMe dataset [23], consisting of 139 images of cars, all in the same semiproﬁle view (Fig. 7(a)). The images are annotated with ground-truth segmentations for L = 6 parts (body, wheel, window, bumper, license plate, headlight; Fig. 7(d)). We compare the performance of the model with the ISM of Thomas et al. [22], who also report their results on this dataset. The dataset was split into 10 train/test cross-validation splits without replacement. We used the training images in each split to train both the shape and appearance components. For the shape component, we trained an MSBM at 50 ⇥ 50 pixels with overlap b = 4, and 2000 and 100 hidden units in the ﬁrst and second layers respectively. Each layer was pre-trained for 3000 epochs and joint training was performed for 1000 epochs. The appearance model was trained with a vocabulary of size W = 50 and K = 100 mixture components and we set = 0.7. Inference chains were seeded at 50 exemplar segmentations (obtained using K-medoids). We ﬁnd that the use of superpixels does not help with this dataset (due to the poor quality of superpixels obtained for these images). Qualitative and quantitative results that show the performance of model to be comparable to the state-of-the-art ISM can be seen in Fig. 7(c) and Table 2. We believe the discrepancy in accuracy between the MSBM and ISM on the ‘license’ and ‘light’ labels to mainly be due to ISM’s use of interest-points, as they are able to locate such ﬁne structures accurately. By incorporating better models of part appearance into the generative model, we expect to see this discrepancy decrease. 5 Conclusions and future work In this paper we have shown how the SBM can be extended to obtain the MSBM, and presented a principled probabilistic model of images of objects that exploits the MSBM as its model for part shapes. We demonstrated how object segmentations can be obtained simply by performing MCMC inference in the model. The model can also be treated as a probabilistic evaluator of segmentations: given a proposal segmentation it can be used to estimate its likelihood. This leads us to believe that the combination of a generative model such as ours, with a discriminative, bottom-up segmentation algorithm could be highly effective. We are currently investigating how textured appearance models, which take into account the spatial structure of pixels, affect the learning and inference algorithms and the performance of the model. Acknowledgments Thanks to Charless Fowlkes and Vittorio Ferrari for access to datasets, and to Pushmeet Kohli and John Winn for valuable discussions. AE has received funding from the Carnegie Trust, the SORSAS scheme, and the IST Programme under the PASCAL2 Network of Excellence (IST-2007-216886). 7 (a) Test (c) MSBM (b) Bo and Fowlkes (d) Ground truth Background Hair Face Upper Shoes Legs Lower Arms (d) Ground truth (c) MSBM (b) Thomas et al. (a) Test Figure 6: Penn-Fudan pedestrians. (a) Test images. (b) Results reported by Bo and Fowlkes [21]. (c) Output of the joint model. (d) Ground-truth images. Images shown are those selected by [21]. Background Body Wheel Window Bumper License Headlight Figure 7: ETHZ cars. (a) Test images. (b) Results reported by Thomas et al. [22]. (c) Output of the joint model. (d) Ground-truth images. Images shown are those selected by [22]. 8 References [1] S. M. Ali Eslami, Nicolas Heess, and John Winn. The Shape Boltzmann Machine: a Strong Model of Object Shape. In IEEE CVPR, 2012. [2] Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman. The PASCAL Visual Object Classes (VOC) Challenge. International Journal of Computer Vision, 88:303–338, 2010. [3] Martin Fischler and Robert Elschlager. The Representation and Matching of Pictorial Structures. IEEE Transactions on Computers, 22(1):67–92, 1973. [4] David Marr. Vision: A Computational Investigation into the Human Representation and Processing of Visual Information. Freeman, 1982. [5] Irving Biederman. Recognition-by-components: A theory of human image understanding. Psychological Review, 94:115–147, 1987. [6] Ashish Kapoor and John Winn. Located Hidden Random Fields: Learning Discriminative Parts for Object Detection. In ECCV, pages 302–315, 2006. [7] John Winn and Jamie Shotton. The Layout Consistent Random Field for Recognizing and Segmenting Partially Occluded Objects. In IEEE CVPR, pages 37–44, 2006. [8] Nebojsa Jojic and Yaron Caspi. Capturing Image Structure with Probabilistic Index Maps. In IEEE CVPR, pages 212–219, 2004. [9] John Winn and Nebojsa Jojic. LOCUS: Learning object classes with unsupervised segmentation. In ICCV, pages 756–763, 2005. [10] Nebojsa Jojic, Alessandro Perina, Marco Cristani, Vittorio Murino, and Brendan Frey. Stel component analysis. In IEEE CVPR, pages 2044–2051, 2009. [11] S. M. Ali Eslami and Christopher K. I. Williams. Factored Shapes and Appearances for Partsbased Object Understanding. In BMVC, pages 18.1–18.12, 2011. [12] Nicolas Heess. Learning generative models of mid-level structure in natural images. PhD thesis, University of Edinburgh, 2011. [13] Ruslan Salakhutdinov and Geoffrey Hinton. Deep Boltzmann Machines. In AISTATS, volume 5, pages 448–455, 2009. [14] Tijmen Tieleman. Training restricted Boltzmann machines using approximations to the likelihood gradient. In ICML, pages 1064–1071, 2008. [15] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. “GrabCut”: interactive foreground extraction using iterated graph cuts. ACM SIGGRAPH, 23:309–314, 2004. [16] Eran Borenstein, Eitan Sharon, and Shimon Ullman. Combining Top-Down and Bottom-Up Segmentation. In CVPR Workshop on Perceptual Organization in Computer Vision, 2004. [17] Himanshu Arora, Nicolas Loeff, David Forsyth, and Narendra Ahuja. Unsupervised Segmentation of Objects using Efﬁcient Learning. IEEE CVPR, pages 1–7, 2007. [18] Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari. ClassCut for unsupervised class segmentation. In ECCV, pages 380–393, 2010. [19] Nicolas Heess, Nicolas Le Roux, and John Winn. Weakly Supervised Learning of ForegroundBackground Segmentation using Masked RBMs. In ICANN, 2011. [20] Nicolas Le Roux, Nicolas Heess, Jamie Shotton, and John Winn. Learning a Generative Model of Images by Factoring Appearance and Shape. Neural Computation, 23(3):593–650, 2011. [21] Yihang Bo and Charless Fowlkes. Shape-based Pedestrian Parsing. In IEEE CVPR, 2011. [22] Alexander Thomas, Vittorio Ferrari, Bastian Leibe, Tinne Tuytelaars, and Luc Van Gool. Using Recognition and Annotation to Guide a Robot’s Attention. IJRR, 28(8):976–998, 2009. [23] Bryan Russell, Antonio Torralba, Kevin Murphy, and William Freeman. LabelMe: A Database and Tool for Image Annotation. International Journal of Computer Vision, 77:157–173, 2008. [24] Leonid Sigal, Alexandru Balan, and Michael Black. HumanEva. International Journal of Computer Vision, 87(1-2):4–27, 2010. [25] Pablo Arbelaez, Michael Maire, Charless C. Fowlkes, and Jitendra Malik. From Contours to Regions: An Empirical Evaluation. In IEEE CVPR, 2009. 9</p><p>9 <a title="nips-2012-9" href="../nips2012/nips-2012-A_Geometric_take_on_Metric_Learning.html">nips-2012-A Geometric take on Metric Learning</a></p>
<p>Author: Søren Hauberg, Oren Freifeld, Michael J. Black</p><p>Abstract: Multi-metric learning techniques learn local metric tensors in different parts of a feature space. With such an approach, even simple classiﬁers can be competitive with the state-of-the-art because the distance measure locally adapts to the structure of the data. The learned distance measure is, however, non-metric, which has prevented multi-metric learning from generalizing to tasks such as dimensionality reduction and regression in a principled way. We prove that, with appropriate changes, multi-metric learning corresponds to learning the structure of a Riemannian manifold. We then show that this structure gives us a principled way to perform dimensionality reduction and regression according to the learned metrics. Algorithmically, we provide the ﬁrst practical algorithm for computing geodesics according to the learned metrics, as well as algorithms for computing exponential and logarithmic maps on the Riemannian manifold. Together, these tools let many Euclidean algorithms take advantage of multi-metric learning. We illustrate the approach on regression and dimensionality reduction tasks that involve predicting measurements of the human body from shape data. 1 Learning and Computing Distances Statistics relies on measuring distances. When the Euclidean metric is insufﬁcient, as is the case in many real problems, standard methods break down. This is a key motivation behind metric learning, which strives to learn good distance measures from data. In the most simple scenarios a single metric tensor is learned, but in recent years, several methods have proposed learning multiple metric tensors, such that different distance measures are applied in different parts of the feature space. This has proven to be a very powerful approach for classiﬁcation tasks [1, 2], but the approach has not generalized to other tasks. Here we consider the generalization of Principal Component Analysis (PCA) and linear regression; see Fig. 1 for an illustration of our approach. The main problem with generalizing multi-metric learning is that it is based on assumptions that make the feature space both non-smooth and non-metric. Speciﬁcally, it is often assumed that straight lines form geodesic curves and that the metric tensor stays constant along these lines. These assumptions are made because it is believed that computing the actual geodesics is intractable, requiring a discretization of the entire feature space [3]. We solve these problems by smoothing the transitions between different metric tensors, which ensures a metric space where geodesics can be computed. In this paper, we consider the scenario where the metric tensor at a given point in feature space is deﬁned as the weighted average of a set of learned metric tensors. In this model, we prove that the feature space becomes a chart for a Riemannian manifold. This ensures a metric feature space, i.e. dist(x, y) = 0 ⇔ x = y , dist(x, y) = dist(y, x) (symmetry), (1) dist(x, z) ≤ dist(x, y) + dist(y, z) (triangle inequality). To compute statistics according to the learned metric, we need to be able to compute distances, which implies that we need to compute geodesics. Based on the observation that geodesics are 1 (a) Local Metrics & Geodesics (b) Tangent Space Representation (c) First Principal Geodesic Figure 1: Illustration of Principal Geodesic Analysis. (a) Geodesics are computed between the mean and each data point. (b) Data is mapped to the Euclidean tangent space and the ﬁrst principal component is computed. (c) The principal component is mapped back to the feature space. smooth curves in Riemannian spaces, we derive an algorithm for computing geodesics that only requires a discretization of the geodesic rather than the entire feature space. Furthermore, we show how to compute the exponential and logarithmic maps of the manifold. With this we can map any point back and forth between a Euclidean tangent space and the manifold. This gives us a general strategy for incorporating the learned metric tensors in many Euclidean algorithms: map the data to the tangent of the manifold, perform the Euclidean analysis and map the results back to the manifold. Before deriving the algorithms (Sec. 3) we set the scene by an analysis of the shortcomings of current state-of-the-art methods (Sec. 2), which motivate our ﬁnal model. The model is general and can be used for many problems. Here we illustrate it with several challenging problems in 3D body shape modeling and analysis (Sec. 4). All proofs can be found in the supplementary material along with algorithmic details and further experimental results. 2 Background and Related Work Single-metric learning learns a metric tensor, M, such that distances are measured as dist2 (xi , xj ) = xi − xj 2 M ≡ (xi − xj )T M(xi − xj ) , (2) where M is a symmetric and positive deﬁnite D × D matrix. Classic approaches for ﬁnding such a metric tensor include PCA, where the metric is given by the inverse covariance matrix of the training data; and linear discriminant analysis (LDA), where the metric tensor is M = S−1 SB S−1 , with Sw W W and SB being the within class scatter and the between class scatter respectively [9]. A more recent approach tries to learn a metric tensor from triplets of data points (xi , xj , xk ), where the metric should obey the constraint that dist(xi , xj ) < dist(xi , xk ). Here the constraints are often chosen such that xi and xj belong to the same class, while xi and xk do not. Various relaxed versions of this idea have been suggested such that the metric can be learned by solving a semi-deﬁnite or a quadratic program [1, 2, 4–8]. Among the most popular approaches is the Large Margin Nearest Neighbor (LMNN) classiﬁer [5], which ﬁnds a linear transformation that satisﬁes local distance constraints, making the approach suitable for multi-modal classes. For many problems, a single global metric tensor is not enough, which motivates learning several local metric tensors. The classic work by Hastie and Tibshirani [9] advocates locally learning metric tensors according to LDA and using these as part of a kNN classiﬁer. In a somewhat similar fashion, Weinberger and Saul [5] cluster the training data and learn a separate metric tensor for each cluster using LMNN. A more extreme point of view was taken by Frome et al. [1, 2], who learn a diagonal metric tensor for every point in the training set, such that distance rankings are preserved. Similarly, Malisiewicz and Efros [6] ﬁnd a diagonal metric tensor for each training point such that the distance to a subset of the training data from the same class is kept small. Once a set of metric tensors {M1 , . . . , MR } has been learned, the distance dist(a, b) is measured according to (2) where “the nearest” metric tensor is used, i.e. R M(x) = r=1 wr (x) ˜ Mr , where wr (x) = ˜ ˜ j wj (x) 1 0 x − xr 2 r ≤ x − xj M otherwise 2 Mj , ∀j , (3) where x is either a or b depending on the algorithm. Note that this gives a non-metric distance function as it is not symmetric. To derive this equation, it is necessary to assume that 1) geodesics 2 −8 −8 Assumed Geodesics Location of Metric Tensors Test Points −6 −8 Actual Geodesics Location of Metric Tensors Test Points −6 Riemannian Geodesics Location of Metric Tensors Test Points −6 −4 −4 −4 −2 −2 −2 0 0 0 2 2 2 4 4 4 6 −8 6 −8 −6 −4 −2 0 (a) 2 4 6 −6 −4 −2 0 2 4 6 6 −8 −6 (b) −4 −2 (c) 0 2 4 6 (d) Figure 2: (a)–(b) An illustrative example where straight lines do not form geodesics and where the metric tensor does not stay constant along lines; see text for details. The background color is proportional to the trace of the metric tensor, such that light grey corresponds to regions where paths are short (M1 ), and dark grey corresponds to regions they are long (M2 ). (c) The suggested geometric model along with the geodesics. Again, background colour is proportional to the trace of the metric tensor; the colour scale is the same is used in (a) and (b). (d) An illustration of the exponential and logarithmic maps. form straight lines, and 2) the metric tensor stays constant along these lines [3]. Both assumptions are problematic, which we illustrate with a simple example in Fig. 2a–c. Assume we are given two metric tensors M1 = 2I and M2 = I positioned at x1 = (2, 2)T and x2 = (4, 4)T respectively. This gives rise to two regions in feature space in which x1 is nearest in the ﬁrst and x2 is nearest in the second, according to (3). This is illustrated in Fig. 2a. In the same ﬁgure, we also show the assumed straight-line geodesics between selected points in space. As can be seen, two of the lines goes through both regions, such that the assumption of constant metric tensors along the line is violated. Hence, it would seem natural to measure the length of the line, by adding the length of the line segments which pass through the different regions of feature space. This was suggested by Ramanan and Baker [3] who also proposed a polynomial time algorithm for measuring these line lengths. This gives a symmetric distance function. Properly computing line lengths according to the local metrics is, however, not enough to ensure that the distance function is metric. As can be seen in Fig. 2a the straight line does not form a geodesic as a shorter path can be found by circumventing the region with the “expensive” metric tensor M1 as illustrated in Fig. 2b. This issue makes it trivial to construct cases where the triangle inequality is violated, which again makes the line length measure non-metric. In summary, if we want a metric feature space, we can neither assume that geodesics are straight lines nor that the metric tensor stays constant along such lines. In practice, good results have been reported using (3) [1,3,5], so it seems obvious to ask: is metricity required? For kNN classiﬁers this does not appear to be the case, with many successes based on dissimilarities rather than distances [10]. We, however, want to generalize PCA and linear regression, which both seek to minimize the reconstruction error of points projected onto a subspace. As the notion of projection is hard to deﬁne sensibly in non-metric spaces, we consider metricity essential. In order to build a model with a metric feature space, we change the weights in (3) to be smooth functions. This impose a well-behaved geometric structure on the feature space, which we take advantage of in order to perform statistical analysis according to the learned metrics. However, ﬁrst we review the basics of Riemannian geometry as this provides the theoretical foundation of our work. 2.1 Geodesics and Riemannian Geometry We start by deﬁning Riemannian manifolds, which intuitively are smoothly curved spaces equipped with an inner product. Formally, they are smooth manifolds endowed with a Riemannian metric [11]: Deﬁnition A Riemannian metric M on a manifold M is a smoothly varying inner product < a, b >x = aT M(x)b in the tangent space Tx M of each point x ∈ M . 3 Often Riemannian manifolds are represented by a chart; i.e. a parameter space for the curved surface. An example chart is the spherical coordinate system often used to represent spheres. While such charts are often ﬂat spaces, the curvature of the manifold arises from the smooth changes in the metric. On a Riemannian manifold M, the length of a smooth curve c : [0, 1] → M is deﬁned as the integral of the norm of the tangent vector (interpreted as speed) along the curve: 1 Length(c) = 1 c (λ) M(c(λ)) dλ c (λ)T M(c(λ))c (λ)dλ , = (4) 0 0 where c denotes the derivative of c and M(c(λ)) is the metric tensor at c(λ). A geodesic curve is then a length-minimizing curve connecting two given points x and y, i.e. (5) cgeo = arg min Length(c) with c(0) = x and c(1) = y . c The distance between x and y is deﬁned as the length of the geodesic. Given a tangent vector v ∈ Tx M, there exists a unique geodesic cv (t) with initial velocity v at x. The Riemannian exponential map, Expx , maps v to a point on the manifold along the geodesic cv at t = 1. This mapping preserves distances such that dist(cv (0), cv (1)) = v . The inverse of the exponential map is the Riemannian logarithmic map denoted Logx . Informally, the exponential and logarithmic maps move points back and forth between the manifold and the tangent space while preserving distances (see Fig. 2d for an illustration). This provides a general strategy for generalizing many Euclidean techniques to Riemannian domains: data points are mapped to the tangent space, where ordinary Euclidean techniques are applied and the results are mapped back to the manifold. 3 A Metric Feature Space With the preliminaries settled we deﬁne the new model. Let C = RD denote the feature space. We endow C with a metric tensor in every point x, which we deﬁne akin to (3), R M(x) = wr (x)Mr , where wr (x) = r=1 wr (x) ˜ R ˜ j=1 wj (x) , (6) with wr > 0. The only difference from (3) is that we shall not restrict ourselves to binary weight ˜ functions wr . We assume the metric tensors Mr have already been learned; Sec. 4 contain examples ˜ where they have been learned using LMNN [5] and LDA [9]. From the deﬁnition of a Riemannian metric, we trivially have the following result: Lemma 1 The space C = RD endowed with the metric tensor from (6) is a chart of a Riemannian manifold, iff the weights wr (x) change smoothly with x. Hence, by only considering smooth weight functions wr we get a well-studied geometric structure ˜ on the feature space, which ensures us that it is metric. To illustrate the implications we return to the example in Fig. 2. We change the weight functions from binary to squared exponentials, which gives the feature space shown in Fig. 2c. As can be seen, the metric tensor now changes smoothly, which also makes the geodesics smooth curves (a property we will use when computing the geodesics). It is worth noting that Ramanan and Baker [3] also consider the idea of smoothly averaging the metric tensor. They, however, only evaluate the metric tensor at the test point of their classiﬁer and then assume straight line geodesics with a constant metric tensor. Such assumptions violate the premise of a smoothly changing metric tensor and, again, the distance measure becomes non-metric. Lemma 1 shows that metric learning can be viewed as manifold learning. The main difference between our approach and techniques such as Isomap [12] is that, while Isomap learns an embedding of the data points, we learn the actual manifold structure. This gives us the beneﬁt that we can compute geodesics as well as the exponential and logarithmic maps. These provide us with mappings back and forth between the manifold and Euclidean representation of the data, which preserve distances as well as possible. The availability of such mappings is in stark contrast to e.g. Isomap. In the next section we will derive a system of ordinary differential equations (ODE’s) that geodesics in C have to satisfy, which provides us with algorithms for computing geodesics as well as exponential and logarithmic maps. With these we can generalize many Euclidean techniques. 4 3.1 Computing Geodesics, Maps and Statistics At minima of (4) we know that the Euler-Lagrange equation must hold [11], i.e. ∂L d ∂L , where L(λ, c, c ) = c (λ)T M(c(λ))c (λ) . = ∂c dλ ∂c As we have an explicit expression for the metric tensor we can compute (7) in closed form: (7) Theorem 2 Geodesic curves in C satisfy the following system of 2nd order ODE’s M(c(λ))c (λ) = − 1 ∂vec [M(c(λ))] 2 ∂c(λ) T (c (λ) ⊗ c (λ)) , (8) where ⊗ denotes the Kronecker product and vec [·] stacks the columns of a matrix into a vector [13]. Proof See supplementary material. This result holds for any smooth weight functions wr . We, however, still need to compute ∂vec[M] , ˜ ∂c which depends on the speciﬁc choice of wr . Any smooth weighting scheme is applicable, but we ˜ restrict ourselves to the obvious smooth generalization of (3) and use squared exponentials. From this assumption, we get the following result Theorem 3 For wr (x) = exp − ρ x − xr ˜ 2 ∂vec [M(c)] = ∂c the derivative of the metric tensor from (6) is R ρ R j=1 2 Mr R 2 wj ˜ T r=1 T wj (c − xj ) Mj − (c − xr ) Mr ˜ wr vec [Mr ] ˜ . (9) j=1 Proof See supplementary material. Computing Geodesics. Any geodesic curve must be a solution to (8). Hence, to compute a geodesic between x and y, we can solve (8) subject to the constraints c(0) = x and c(1) = y . (10) This is a boundary value problem, which has a smooth solution. This allows us to solve the problem numerically using a standard three-stage Lobatto IIIa formula, which provides a fourth-order accurate C 1 –continuous solution [14]. Ramanan and Baker [3] discuss the possibility of computing geodesics, but arrive at the conclusion that this is intractable based on the assumption that it requires discretizing the entire feature space. Our solution avoids discretizing the feature space by discretizing the geodesic curve instead. As this is always one-dimensional the approach remains tractable in high-dimensional feature spaces. Computing Logarithmic Maps. Once a geodesic c is found, it follows from the deﬁnition of the logarithmic map, Logx (y), that it can be computed as v = Logx (y) = c (0) Length(c) . c (0) (11) In practice, we solve (8) by rewriting it as a system of ﬁrst order ODE’s, such that we compute both c and c simultaneously (see supplementary material for details). Computing Exponential Maps. Given a starting point x on the manifold and a vector v in the tangent space, the exponential map, Expx (v), ﬁnds the unique geodesic starting at x with initial velocity v. As the geodesic must fulﬁll (8), we can compute the exponential map by solving this system of ODE’s with the initial conditions c(0) = x and c (0) = v . (12) This initial value problem has a unique solution, which we ﬁnd numerically using a standard RungeKutta scheme [15]. 5 3.1.1 Generalizing PCA and Regression At this stage, we know that the feature space is Riemannian and we know how to compute geodesics and exponential and logarithmic maps. We now seek to generalize PCA and linear regression, which becomes straightforward since solutions are available in Riemannian spaces [16, 17]. These generalizations can be summarized as mapping the data to the tangent space at the mean, performing standard Euclidean analysis in the tangent and mapping the results back. The ﬁrst step is to compute the mean value on the manifold, which is deﬁned as the point that minimizes the sum-of-squares distances to the data points. Pennec [18] provides an efﬁcient gradient descent approach for computing this point, which we also summarize in the supplementary material. The empirical covariance of a set of points is deﬁned as the ordinary Euclidean covariance in the tangent space at the mean value [18]. With this in mind, it is not surprising that the principal components of a dataset have been generalized as the geodesics starting at the mean with initial velocity corresponding to the eigenvectors of the covariance [16], γvd (t) = Expµ (tvd ) , (13) th where vd denotes the d eigenvector of the covariance. This approach is called Principal Geodesic Analysis (PGA), and the geodesic curve γvd is called the principal geodesic. An illustration of the approach can be seen in Fig. 1 and more algorithmic details are in the supplementary material. Linear regression has been generalized in a similar way [17] by performing regression in the tangent of the mean and mapping the resulting line back to the manifold using the exponential map. The idea of working in the tangent space is both efﬁcient and convenient, but comes with an element of approximation as the logarithmic map is only guarantied to preserve distances to the origin of the tangent and not between all pairs of data points. Practical experience, however, indicates that this is a good tradeoff; see [19] for a more in-depth discussion of when the approximation is suitable. 4 Experiments To illustrate the framework1 we consider an example in human body analysis, and then we analyze the scalability of the approach. But ﬁrst, to build intuition, Fig. 3a show synthetically generated data samples from two classes. We sample random points xr and learn a local LDA metric [9] by considering all data points within a radius; this locally pushes the two classes apart. We combine the local metrics using (6) and Fig. 3b show the data in the tangent space of the resulting manifold. As can be seen the two classes are now globally further apart, which shows the effect of local metrics. 4.1 Human Body Shape We consider a regression example concerning human body shape analysis. We study 986 female body laser scans from the CAESAR [20] data set; each shape is represented using the leading 35 principal components of the data learned using a SCAPE-like model [21, 22]. Each shape is associated with anthropometric measurements such as body height, shoe size, etc. We show results for shoulder to wrist distance and shoulder breadth, but results for more measurements are in the supplementary material. To predict the measurements from shape coefﬁcients, we learn local metrics and perform linear regression according to these. As a further experiment, we use PGA to reduce the dimensionality of the shape coefﬁcients according to the local metrics, and measure the quality of the reduction by performing linear regression to predict the measurements. As a baseline we use the corresponding Euclidean techniques. To learn the local metric we do the following. First we whiten the data such that the variance captured by PGA will only be due to the change of metric; this allows easy visualization of the impact of the learned metrics. We then cluster the body shapes into equal-sized clusters according to the measurement and learn a LMNN metric for each cluster [5], which we associate with the mean of each class. These push the clusters apart, which introduces variance along the directions where the measurement changes. From this we construct a Riemannian manifold according to (6), 1 Our software implementation for computing geodesics and performing manifold statistics is available at http://ps.is.tue.mpg.de/project/Smooth Metric Learning 6 30 Euclidean Model Riemannian Model 24 20 18 16 20 15 10 5 14 12 0 (a) 25 22 Running Time (sec.) Average Prediction Error 26 10 (b) 20 Dimensionality 0 0 30 50 (c) 100 Dimensionality 150 (d) 4 3 3 2 2 1 1 0 −1 −2 −3 −4 −4 −3 −2 −1 0 1 2 3 4 Shoulder breadth 20 −2 −3 Euclidean Model Riemannian Model 0 −1 25 Prediction Error 4 15 10 0 −4 −5 0 4 10 15 20 Dimensionality 16 25 30 35 17 3 3 5 5 Euclidean Model Riemannian Model 2 15 2 1 1 Prediction Error Shoulder to wrist distance Figure 3: Left panels: Synthetic data. (a) Samples from two classes along with illustratively sampled metric tensors from (6). (b) The data represented in the tangent of a manifold constructed from local LDA metrics learned at random positions. Right panels: Real data. (c) Average error of linearly predicted body measurements (mm). (d) Running time (sec) of the geodesic computation as a function of dimensionality. 0 0 −1 −2 −1 −3 14 13 12 11 −2 −4 −3 −4 −4 10 −5 −3 −2 −1 0 1 Euclidean PCA 2 3 −6 −4 9 0 −2 0 2 4 Tangent Space PCA (PGA) 6 5 10 15 20 Dimensionality 25 30 35 Regression Error Figure 4: Left: body shape data in the ﬁrst two principal components according to the Euclidean metric. Point color indicates cluster membership. Center: As on the left, but according to the Riemannian model. Right: regression error as a function of the dimensionality of the shape space; again the Euclidean metric and the Riemannian metric are compared. compute the mean value on the manifold, map the data to the tangent space at the mean and perform linear regression in the tangent space. As a ﬁrst visualization we plot the data expressed in the leading two dimensions of PGA in Fig. 4; as can be seen the learned metrics provide principal geodesics, which are more strongly related with the measurements than the Euclidean model. In order to predict the measurements from the body shape, we perform linear regression, both directly in the shape space according to the Euclidean metric and in the tangent space of the manifold corresponding to the learned metrics (using the logarithmic map from (11)). We measure the prediction error using leave-one-out cross-validation. To further illustrate the power of the PGA model, we repeat this experiment for different dimensionalities of the data. The results are plotted in Fig. 4, showing that regression according to the learned metrics outperforms the Euclidean model. To verify that the learned metrics improve accuracy, we average the prediction errors over all millimeter measurements. The result in Fig. 3c shows that much can be gained in lower dimensions by using the local metrics. To provide visual insights into the behavior of the learned metrics, we uniformly sample body shape along the ﬁrst principal geodesic (in the range ±7 times the standard deviation) according to the different metrics. The results are available as a movie in the supplementary material, but are also shown in Fig. 5. As can be seen, the learned metrics pick up intuitive relationships between body shape and the measurements, e.g. shoulder to wrist distance is related to overall body size, while shoulder breadth is related to body weight. 7 Shoulder to wrist distance Shoulder breadth Figure 5: Shapes corresponding to the mean (center) and ±7 times the standard deviations along the principal geodesics (left and right). Movies are available in the supplementary material. 4.2 Scalability The human body data set is small enough (986 samples in 35 dimensions) that computing a geodesic only takes a few seconds. To show that the current unoptimized Matlab implementation can handle somewhat larger datasets, we brieﬂy consider a dimensionality reduction task on the classic MNIST handwritten digit data set. We use the preprocessed data available with [3] where the original 28×28 gray scale images were deskewed and projected onto their leading 164 Euclidean principal components (which captures 95% of the variance in the original data). We learn one diagonal LMNN metric per class, which we associate with the mean of the class. From this we construct a Riemannian manifold from (6), compute the mean value on the manifold and compute geodesics between the mean and each data point; this is the computationally expensive part of performing PGA. Fig. 3d plots the average running time (sec) for the computation of geodesics as a function of the dimensionality of the training data. A geodesic can be computed in 100 dimensions in approximately 5 sec., whereas in 150 dimensions it takes about 30 sec. In this experiment, we train a PGA model on 60,000 data points, and test a nearest neighbor classiﬁer in the tangent space as we decrease the dimensionality of the model. Compared to a Euclidean model, this gives a modest improvement in classiﬁcation accuracy of 2.3 percent, when averaged across different dimensionalities. Plots of the results can be found in the supplementary material. 5 Discussion This work shows that multi-metric learning techniques are indeed applicable outside the realm of kNN classiﬁers. The idea of deﬁning the metric tensor at any given point as the weighted average of a ﬁnite set of learned metrics is quite natural from a modeling point of view, which is also validated by the Riemannian structure of the resulting space. This opens both a theoretical and a practical toolbox for analyzing and developing algorithms that use local metric tensors. Speciﬁcally, we show how to use local metric tensors for both regression and dimensionality reduction tasks. Others have attempted to solve non-classiﬁcation problems using local metrics, but we feel that our approach is the ﬁrst to have a solid theoretical backing. For example, Hastie and Tibshirani [9] use local LDA metrics for dimensionality reduction by averaging the local metrics and using the resulting metric as part of a Euclidean PCA, which essentially is a linear approach. Another approach was suggested by Hong et al. [23] who simply compute the principal components according to each metric separately, such that one low dimensional model is learned per metric. The suggested approach is, however, not difﬁculty-free in its current implementation. Currently, we are using off-the-shelf numerical solvers for computing geodesics, which can be computationally demanding. While we managed to analyze medium-sized datasets, we believe that the run-time can be drastically improved by developing specialized numerical solvers. In the experiments, we learned local metrics using techniques specialized for classiﬁcation tasks as this is all the current literature provides. We expect improvements by learning the metrics speciﬁcally for regression and dimensionality reduction, but doing so is currently an open problem. Acknowledgments: Søren Hauberg is supported in part by the Villum Foundation, and Oren Freifeld is supported in part by NIH-NINDS EUREKA (R01-NS066311). 8 References [1] Andrea Frome, Yoram Singer, and Jitendra Malik. Image retrieval and classiﬁcation using local distance functions. In B. Sch¨ lkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing o Systems 19 (NIPS), pages 417–424, Cambridge, MA, 2007. MIT Press. [2] Andrea Frome, Fei Sha, Yoram Singer, and Jitendra Malik. Learning globally-consistent local distance functions for shape-based image retrieval and classiﬁcation. In International Conference on Computer Vision (ICCV), pages 1–8, 2007. [3] Deva Ramanan and Simon Baker. Local distance functions: A taxonomy, new algorithms, and an evaluation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(4):794–806, 2011. [4] Shai Shalev-Shwartz, Yoram Singer, and Andrew Y. Ng. Online and batch learning of pseudo-metrics. In Proceedings of the twenty-ﬁrst international conference on Machine learning, ICML ’04, pages 94–101. ACM, 2004. [5] Kilian Q. Weinberger and Lawrence K. Saul. Distance metric learning for large margin nearest neighbor classiﬁcation. The Journal of Machine Learning Research, 10:207–244, 2009. [6] Tomasz Malisiewicz and Alexei A. Efros. Recognition by association via learning per-exemplar distances. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1–8, 2008. [7] Yiming Ying and Peng Li. Distance metric learning with eigenvalue optimization. The Journal of Machine Learning Research, 13:1–26, 2012. [8] Matthew Schultz and Thorsten Joachims. Learning a distance metric from relative comparisons. In Advances in Neural Information Processing Systems 16 (NIPS), 2004. [9] Trevor Hastie and Robert Tibshirani. Discriminant adaptive nearest neighbor classiﬁcation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 18(6):607–616, June 1996. [10] Elzbieta Pekalska, Pavel Paclik, and Robert P. W. Duin. A generalized kernel approach to dissimilaritybased classiﬁcation. Journal of Machine Learning Research, 2:175–211, 2002. [11] Manfredo Perdigao do Carmo. Riemannian Geometry. Birkh¨ user Boston, January 1992. a [12] Joshua B. Tenenbaum, Vin De Silva, and John C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. [13] Jan R. Magnus and Heinz Neudecker. Matrix Differential Calculus with Applications in Statistics and Econometrics. John Wiley & Sons, 2007. [14] Jacek Kierzenka and Lawrence F. Shampine. A BVP solver based on residual control and the Matlab PSE. ACM Transactions on Mathematical Software, 27(3):299–316, 2001. [15] John R. Dormand and P. J. Prince. A family of embedded Runge-Kutta formulae. Journal of Computational and Applied Mathematics, 6:19–26, 1980. [16] P. Thomas Fletcher, Conglin Lu, Stephen M. Pizer, and Sarang Joshi. Principal Geodesic Analysis for the study of Nonlinear Statistics of Shape. IEEE Transactions on Medical Imaging, 23(8):995–1005, 2004. [17] Peter E. Jupp and John T. Kent. Fitting smooth paths to spherical data. Applied Statistics, 36(1):34–46, 1987. [18] Xavier Pennec. Probabilities and statistics on Riemannian manifolds: Basic tools for geometric measurements. In Proceedings of Nonlinear Signal and Image Processing, pages 194–198, 1999. [19] Stefan Sommer, Francois Lauze, Søren Hauberg, and Mads Nielsen. Manifold valued statistics, exact ¸ principal geodesic analysis and the effect of linear approximations. In European Conference on Computer Vision (ECCV), pages 43–56, 2010. [20] Kathleen M. Robinette, Hein Daanen, and Eric Paquet. The CAESAR project: a 3-D surface anthropometry survey. In 3-D Digital Imaging and Modeling, pages 380–386, 1999. [21] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Sebastian Thrun, Jim Rodgers, and James Davis. Scape: shape completion and animation of people. ACM Transactions on Graphics, 24(3):408–416, 2005. [22] Oren Freifeld and Michael J. Black. Lie bodies: A manifold representation of 3D human shape. In A. Fitzgibbon et al. (Eds.), editor, European Conference on Computer Vision (ECCV), Part I, LNCS 7572, pages 1–14. Springer-Verlag, oct 2012. [23] Yi Hong, Quannan Li, Jiayan Jiang, and Zhuowen Tu. Learning a mixture of sparse distance metrics for classiﬁcation and dimensionality reduction. In International Conference on Computer Vision (ICCV), pages 906–913, 2011. 9</p><p>10 <a title="nips-2012-10" href="../nips2012/nips-2012-A_Linear_Time_Active_Learning_Algorithm_for_Link_Classification.html">nips-2012-A Linear Time Active Learning Algorithm for Link Classification</a></p>
<p>Author: Nicolò Cesa-bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella</p><p>Abstract: We present very eﬃcient active learning algorithms for link classiﬁcation in signed networks. Our algorithms are motivated by a stochastic model in which edge labels are obtained through perturbations of a initial sign assignment consistent with a two-clustering of the nodes. We provide a theoretical analysis within this model, showing that we can achieve an optimal (to whithin a constant factor) number of mistakes on any graph G = (V, E) such that |E| = Ω(|V |3/2 ) by querying O(|V |3/2 ) edge labels. More generally, we show an algorithm that achieves optimality to within a factor of O(k) by querying at most order of |V | + (|V |/k)3/2 edge labels. The running time of this algorithm is at most of order |E| + |V | log |V |. 1</p><p>11 <a title="nips-2012-11" href="../nips2012/nips-2012-A_Marginalized_Particle_Gaussian_Process_Regression.html">nips-2012-A Marginalized Particle Gaussian Process Regression</a></p>
<p>Author: Yali Wang, Brahim Chaib-draa</p><p>Abstract: We present a novel marginalized particle Gaussian process (MPGP) regression, which provides a fast, accurate online Bayesian ﬁltering framework to model the latent function. Using a state space model established by the data construction procedure, our MPGP recursively ﬁlters out the estimation of hidden function values by a Gaussian mixture. Meanwhile, it provides a new online method for training hyperparameters with a number of weighted particles. We demonstrate the estimated performance of our MPGP on both simulated and real large data sets. The results show that our MPGP is a robust estimation algorithm with high computational efﬁciency, which outperforms other state-of-art sparse GP methods. 1</p><p>12 <a title="nips-2012-12" href="../nips2012/nips-2012-A_Neural_Autoregressive_Topic_Model.html">nips-2012-A Neural Autoregressive Topic Model</a></p>
<p>Author: Hugo Larochelle, Stanislas Lauly</p><p>Abstract: We describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents. This model is inspired by the recently proposed Replicated Softmax, an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations. Speciﬁcally, we take inspiration from the conditional mean-ﬁeld recursive equations of the Replicated Softmax in order to deﬁne a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words. This paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words. The end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the Replicated Softmax. Our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm. 1</p><p>13 <a title="nips-2012-13" href="../nips2012/nips-2012-A_Nonparametric_Conjugate_Prior_Distribution_for_the_Maximizing_Argument_of_a_Noisy_Function.html">nips-2012-A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function</a></p>
<p>Author: Pedro Ortega, Jordi Grau-moya, Tim Genewein, David Balduzzi, Daniel Braun</p><p>Abstract: We propose a novel Bayesian approach to solve stochastic optimization problems that involve ﬁnding extrema of noisy, nonlinear functions. Previous work has focused on representing possible functions explicitly, which leads to a two-step procedure of ﬁrst, doing inference over the function space and second, ﬁnding the extrema of these functions. Here we skip the representation step and directly model the distribution over extrema. To this end, we devise a non-parametric conjugate prior based on a kernel regressor. The resulting posterior distribution directly captures the uncertainty over the maximum of the unknown function. Given t observations of the function, the posterior can be evaluated efﬁciently in time O(t2 ) up to a multiplicative constant. Finally, we show how to apply our model to optimize a noisy, non-convex, high-dimensional objective function.</p><p>14 <a title="nips-2012-14" href="../nips2012/nips-2012-A_P300_BCI_for_the_Masses%3A_Prior_Information_Enables_Instant_Unsupervised_Spelling.html">nips-2012-A P300 BCI for the Masses: Prior Information Enables Instant Unsupervised Spelling</a></p>
<p>Author: Pieter-jan Kindermans, Hannes Verschore, David Verstraeten, Benjamin Schrauwen</p><p>Abstract: The usability of Brain Computer Interfaces (BCI) based on the P300 speller is severely hindered by the need for long training times and many repetitions of the same stimulus. In this contribution we introduce a set of unsupervised hierarchical probabilistic models that tackle both problems simultaneously by incorporating prior knowledge from two sources: information from other training subjects (through transfer learning) and information about the words being spelled (through language models). We show, that due to this prior knowledge, the performance of the unsupervised models parallels and in some cases even surpasses that of supervised models, while eliminating the tedious training session. 1</p><p>15 <a title="nips-2012-15" href="../nips2012/nips-2012-A_Polylog_Pivot_Steps_Simplex_Algorithm_for_Classification.html">nips-2012-A Polylog Pivot Steps Simplex Algorithm for Classification</a></p>
<p>Author: Elad Hazan, Zohar Karnin</p><p>Abstract: We present a simplex algorithm for linear programming in a linear classiﬁcation formulation. The paramount complexity parameter in linear classiﬁcation problems is called the margin. We prove that for margin values of practical interest our simplex variant performs a polylogarithmic number of pivot steps in the worst case, and its overall running time is near linear. This is in contrast to general linear programming, for which no sub-polynomial pivot rule is known. 1</p><p>16 <a title="nips-2012-16" href="../nips2012/nips-2012-A_Polynomial-time_Form_of_Robust_Regression.html">nips-2012-A Polynomial-time Form of Robust Regression</a></p>
<p>Author: Ozlem Aslan, Dale Schuurmans, Yao-liang Yu</p><p>Abstract: Despite the variety of robust regression methods that have been developed, current regression formulations are either NP-hard, or allow unbounded response to even a single leverage point. We present a general formulation for robust regression—Variational M-estimation—that uniﬁes a number of robust regression methods while allowing a tractable approximation strategy. We develop an estimator that requires only polynomial-time, while achieving certain robustness and consistency guarantees. An experimental evaluation demonstrates the effectiveness of the new estimation approach compared to standard methods. 1</p><p>17 <a title="nips-2012-17" href="../nips2012/nips-2012-A_Scalable_CUR_Matrix_Decomposition_Algorithm%3A_Lower_Time_Complexity_and_Tighter_Bound.html">nips-2012-A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and Tighter Bound</a></p>
<p>Author: Shusen Wang, Zhihua Zhang</p><p>Abstract: The CUR matrix decomposition is an important extension of Nystr¨ m approximao tion to a general matrix. It approximates any data matrix in terms of a small number of its columns and rows. In this paper we propose a novel randomized CUR algorithm with an expected relative-error bound. The proposed algorithm has the advantages over the existing relative-error CUR algorithms that it possesses tighter theoretical bound and lower time complexity, and that it can avoid maintaining the whole data matrix in main memory. Finally, experiments on several real-world datasets demonstrate signiﬁcant improvement over the existing relative-error algorithms. 1</p><p>18 <a title="nips-2012-18" href="../nips2012/nips-2012-A_Simple_and_Practical_Algorithm_for_Differentially_Private_Data_Release.html">nips-2012-A Simple and Practical Algorithm for Differentially Private Data Release</a></p>
<p>Author: Moritz Hardt, Katrina Ligett, Frank Mcsherry</p><p>Abstract: We present a new algorithm for differentially private data release, based on a simple combination of the Multiplicative Weights update rule with the Exponential Mechanism. Our MWEM algorithm achieves what are the best known and nearly optimal theoretical guarantees, while at the same time being simple to implement and experimentally more accurate on actual data sets than existing techniques. 1</p><p>19 <a title="nips-2012-19" href="../nips2012/nips-2012-A_Spectral_Algorithm_for_Latent_Dirichlet_Allocation.html">nips-2012-A Spectral Algorithm for Latent Dirichlet Allocation</a></p>
<p>Author: Anima Anandkumar, Yi-kai Liu, Daniel J. Hsu, Dean P. Foster, Sham M. Kakade</p><p>Abstract: Topic modeling is a generalization of clustering that posits that observations (words in a document) are generated by multiple latent factors (topics), as opposed to just one. This increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic-word distributions when only words are observed, and the topics are hidden. This work provides a simple and efﬁcient learning procedure that is guaranteed to recover the parameters for a wide class of topic models, including Latent Dirichlet Allocation (LDA). For LDA, the procedure correctly recovers both the topic-word distributions and the parameters of the Dirichlet prior over the topic mixtures, using only trigram statistics (i.e., third order moments, which may be estimated with documents containing just three words). The method, called Excess Correlation Analysis, is based on a spectral decomposition of low-order moments via two singular value decompositions (SVDs). Moreover, the algorithm is scalable, since the SVDs are carried out only on k × k matrices, where k is the number of latent factors (topics) and is typically much smaller than the dimension of the observation (word) space. 1</p><p>20 <a title="nips-2012-20" href="../nips2012/nips-2012-A_Stochastic_Gradient_Method_with_an_Exponential_Convergence__Rate_for_Finite_Training_Sets.html">nips-2012-A Stochastic Gradient Method with an Exponential Convergence  Rate for Finite Training Sets</a></p>
<p>Author: Nicolas L. Roux, Mark Schmidt, Francis R. Bach</p><p>Abstract: We propose a new stochastic gradient method for optimizing the sum of a ﬁnite set of smooth functions, where the sum is strongly convex. While standard stochastic gradient methods converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence rate. In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard algorithms, both in terms of optimizing the training error and reducing the test error quickly. 1</p><p>21 <a title="nips-2012-21" href="../nips2012/nips-2012-A_Unifying_Perspective_of_Parametric_Policy_Search_Methods_for_Markov_Decision_Processes.html">nips-2012-A Unifying Perspective of Parametric Policy Search Methods for Markov Decision Processes</a></p>
<p>22 <a title="nips-2012-22" href="../nips2012/nips-2012-A_latent_factor_model_for_highly_multi-relational_data.html">nips-2012-A latent factor model for highly multi-relational data</a></p>
<p>23 <a title="nips-2012-23" href="../nips2012/nips-2012-A_lattice_filter_model_of_the_visual_pathway.html">nips-2012-A lattice filter model of the visual pathway</a></p>
<p>24 <a title="nips-2012-24" href="../nips2012/nips-2012-A_mechanistic_model_of_early_sensory_processing_based_on_subtracting_sparse_representations.html">nips-2012-A mechanistic model of early sensory processing based on subtracting sparse representations</a></p>
<p>25 <a title="nips-2012-25" href="../nips2012/nips-2012-A_new_metric_on_the_manifold_of_kernel_matrices_with_application_to_matrix_geometric_means.html">nips-2012-A new metric on the manifold of kernel matrices with application to matrix geometric means</a></p>
<p>26 <a title="nips-2012-26" href="../nips2012/nips-2012-A_nonparametric_variable_clustering_model.html">nips-2012-A nonparametric variable clustering model</a></p>
<p>27 <a title="nips-2012-27" href="../nips2012/nips-2012-A_quasi-Newton_proximal_splitting_method.html">nips-2012-A quasi-Newton proximal splitting method</a></p>
<p>28 <a title="nips-2012-28" href="../nips2012/nips-2012-A_systematic_approach_to_extracting_semantic_information_from_functional_MRI_data.html">nips-2012-A systematic approach to extracting semantic information from functional MRI data</a></p>
<p>29 <a title="nips-2012-29" href="../nips2012/nips-2012-Accelerated_Training_for_Matrix-norm_Regularization%3A_A_Boosting_Approach.html">nips-2012-Accelerated Training for Matrix-norm Regularization: A Boosting Approach</a></p>
<p>30 <a title="nips-2012-30" href="../nips2012/nips-2012-Accuracy_at_the_Top.html">nips-2012-Accuracy at the Top</a></p>
<p>31 <a title="nips-2012-31" href="../nips2012/nips-2012-Action-Model_Based_Multi-agent_Plan_Recognition.html">nips-2012-Action-Model Based Multi-agent Plan Recognition</a></p>
<p>32 <a title="nips-2012-32" href="../nips2012/nips-2012-Active_Comparison_of_Prediction_Models.html">nips-2012-Active Comparison of Prediction Models</a></p>
<p>33 <a title="nips-2012-33" href="../nips2012/nips-2012-Active_Learning_of_Model_Evidence_Using_Bayesian_Quadrature.html">nips-2012-Active Learning of Model Evidence Using Bayesian Quadrature</a></p>
<p>34 <a title="nips-2012-34" href="../nips2012/nips-2012-Active_Learning_of_Multi-Index_Function_Models.html">nips-2012-Active Learning of Multi-Index Function Models</a></p>
<p>35 <a title="nips-2012-35" href="../nips2012/nips-2012-Adaptive_Learning_of_Smoothing_Functions%3A_Application_to_Electricity_Load_Forecasting.html">nips-2012-Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting</a></p>
<p>36 <a title="nips-2012-36" href="../nips2012/nips-2012-Adaptive_Stratified_Sampling_for_Monte-Carlo_integration_of_Differentiable_functions.html">nips-2012-Adaptive Stratified Sampling for Monte-Carlo integration of Differentiable functions</a></p>
<p>37 <a title="nips-2012-37" href="../nips2012/nips-2012-Affine_Independent_Variational_Inference.html">nips-2012-Affine Independent Variational Inference</a></p>
<p>38 <a title="nips-2012-38" href="../nips2012/nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<p>39 <a title="nips-2012-39" href="../nips2012/nips-2012-Analog_readout_for_optical_reservoir_computers.html">nips-2012-Analog readout for optical reservoir computers</a></p>
<p>40 <a title="nips-2012-40" href="../nips2012/nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>41 <a title="nips-2012-41" href="../nips2012/nips-2012-Ancestor_Sampling_for_Particle_Gibbs.html">nips-2012-Ancestor Sampling for Particle Gibbs</a></p>
<p>42 <a title="nips-2012-42" href="../nips2012/nips-2012-Angular_Quantization-based_Binary_Codes_for_Fast_Similarity_Search.html">nips-2012-Angular Quantization-based Binary Codes for Fast Similarity Search</a></p>
<p>43 <a title="nips-2012-43" href="../nips2012/nips-2012-Approximate_Message_Passing_with_Consistent_Parameter_Estimation_and_Applications_to_Sparse_Learning.html">nips-2012-Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning</a></p>
<p>44 <a title="nips-2012-44" href="../nips2012/nips-2012-Approximating_Concavely_Parameterized_Optimization_Problems.html">nips-2012-Approximating Concavely Parameterized Optimization Problems</a></p>
<p>45 <a title="nips-2012-45" href="../nips2012/nips-2012-Approximating_Equilibria_in_Sequential_Auctions_with_Incomplete_Information_and_Multi-Unit_Demand.html">nips-2012-Approximating Equilibria in Sequential Auctions with Incomplete Information and Multi-Unit Demand</a></p>
<p>46 <a title="nips-2012-46" href="../nips2012/nips-2012-Assessing_Blinding_in_Clinical_Trials.html">nips-2012-Assessing Blinding in Clinical Trials</a></p>
<p>47 <a title="nips-2012-47" href="../nips2012/nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>48 <a title="nips-2012-48" href="../nips2012/nips-2012-Augmented-SVM%3A_Automatic_space_partitioning_for_combining_multiple_non-linear_dynamics.html">nips-2012-Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics</a></p>
<p>49 <a title="nips-2012-49" href="../nips2012/nips-2012-Automatic_Feature_Induction_for_Stagewise_Collaborative_Filtering.html">nips-2012-Automatic Feature Induction for Stagewise Collaborative Filtering</a></p>
<p>50 <a title="nips-2012-50" href="../nips2012/nips-2012-Bandit_Algorithms_boost_Brain_Computer_Interfaces_for_motor-task_selection_of_a_brain-controlled_button.html">nips-2012-Bandit Algorithms boost Brain Computer Interfaces for motor-task selection of a brain-controlled button</a></p>
<p>51 <a title="nips-2012-51" href="../nips2012/nips-2012-Bayesian_Hierarchical_Reinforcement_Learning.html">nips-2012-Bayesian Hierarchical Reinforcement Learning</a></p>
<p>52 <a title="nips-2012-52" href="../nips2012/nips-2012-Bayesian_Nonparametric_Modeling_of_Suicide_Attempts.html">nips-2012-Bayesian Nonparametric Modeling of Suicide Attempts</a></p>
<p>53 <a title="nips-2012-53" href="../nips2012/nips-2012-Bayesian_Pedigree_Analysis_using_Measure_Factorization.html">nips-2012-Bayesian Pedigree Analysis using Measure Factorization</a></p>
<p>54 <a title="nips-2012-54" href="../nips2012/nips-2012-Bayesian_Probabilistic_Co-Subspace_Addition.html">nips-2012-Bayesian Probabilistic Co-Subspace Addition</a></p>
<p>55 <a title="nips-2012-55" href="../nips2012/nips-2012-Bayesian_Warped_Gaussian_Processes.html">nips-2012-Bayesian Warped Gaussian Processes</a></p>
<p>56 <a title="nips-2012-56" href="../nips2012/nips-2012-Bayesian_active_learning_with_localized_priors_for_fast_receptive_field_characterization.html">nips-2012-Bayesian active learning with localized priors for fast receptive field characterization</a></p>
<p>57 <a title="nips-2012-57" href="../nips2012/nips-2012-Bayesian_estimation_of_discrete_entropy_with_mixtures_of_stick-breaking_priors.html">nips-2012-Bayesian estimation of discrete entropy with mixtures of stick-breaking priors</a></p>
<p>58 <a title="nips-2012-58" href="../nips2012/nips-2012-Bayesian_models_for_Large-scale_Hierarchical_Classification.html">nips-2012-Bayesian models for Large-scale Hierarchical Classification</a></p>
<p>59 <a title="nips-2012-59" href="../nips2012/nips-2012-Bayesian_nonparametric_models_for_bipartite_graphs.html">nips-2012-Bayesian nonparametric models for bipartite graphs</a></p>
<p>60 <a title="nips-2012-60" href="../nips2012/nips-2012-Bayesian_nonparametric_models_for_ranked_data.html">nips-2012-Bayesian nonparametric models for ranked data</a></p>
<p>61 <a title="nips-2012-61" href="../nips2012/nips-2012-Best_Arm_Identification%3A_A_Unified_Approach_to_Fixed_Budget_and_Fixed_Confidence.html">nips-2012-Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence</a></p>
<p>62 <a title="nips-2012-62" href="../nips2012/nips-2012-Burn-in%2C_bias%2C_and_the_rationality_of_anchoring.html">nips-2012-Burn-in, bias, and the rationality of anchoring</a></p>
<p>63 <a title="nips-2012-63" href="../nips2012/nips-2012-CPRL_--_An_Extension_of_Compressive_Sensing_to_the_Phase_Retrieval_Problem.html">nips-2012-CPRL -- An Extension of Compressive Sensing to the Phase Retrieval Problem</a></p>
<p>64 <a title="nips-2012-64" href="../nips2012/nips-2012-Calibrated_Elastic_Regularization_in_Matrix_Completion.html">nips-2012-Calibrated Elastic Regularization in Matrix Completion</a></p>
<p>65 <a title="nips-2012-65" href="../nips2012/nips-2012-Cardinality_Restricted_Boltzmann_Machines.html">nips-2012-Cardinality Restricted Boltzmann Machines</a></p>
<p>66 <a title="nips-2012-66" href="../nips2012/nips-2012-Causal_discovery_with_scale-mixture_model_for_spatiotemporal_variance_dependencies.html">nips-2012-Causal discovery with scale-mixture model for spatiotemporal variance dependencies</a></p>
<p>67 <a title="nips-2012-67" href="../nips2012/nips-2012-Classification_Calibration_Dimension_for_General_Multiclass_Losses.html">nips-2012-Classification Calibration Dimension for General Multiclass Losses</a></p>
<p>68 <a title="nips-2012-68" href="../nips2012/nips-2012-Clustering_Aggregation_as_Maximum-Weight_Independent_Set.html">nips-2012-Clustering Aggregation as Maximum-Weight Independent Set</a></p>
<p>69 <a title="nips-2012-69" href="../nips2012/nips-2012-Clustering_Sparse_Graphs.html">nips-2012-Clustering Sparse Graphs</a></p>
<p>70 <a title="nips-2012-70" href="../nips2012/nips-2012-Clustering_by_Nonnegative_Matrix_Factorization_Using_Graph_Random_Walk.html">nips-2012-Clustering by Nonnegative Matrix Factorization Using Graph Random Walk</a></p>
<p>71 <a title="nips-2012-71" href="../nips2012/nips-2012-Co-Regularized_Hashing_for_Multimodal_Data.html">nips-2012-Co-Regularized Hashing for Multimodal Data</a></p>
<p>72 <a title="nips-2012-72" href="../nips2012/nips-2012-Cocktail_Party_Processing_via_Structured_Prediction.html">nips-2012-Cocktail Party Processing via Structured Prediction</a></p>
<p>73 <a title="nips-2012-73" href="../nips2012/nips-2012-Coding_efficiency_and_detectability_of_rate_fluctuations_with_non-Poisson_neuronal_firing.html">nips-2012-Coding efficiency and detectability of rate fluctuations with non-Poisson neuronal firing</a></p>
<p>74 <a title="nips-2012-74" href="../nips2012/nips-2012-Collaborative_Gaussian_Processes_for_Preference_Learning.html">nips-2012-Collaborative Gaussian Processes for Preference Learning</a></p>
<p>75 <a title="nips-2012-75" href="../nips2012/nips-2012-Collaborative_Ranking_With_17_Parameters.html">nips-2012-Collaborative Ranking With 17 Parameters</a></p>
<p>76 <a title="nips-2012-76" href="../nips2012/nips-2012-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">nips-2012-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>77 <a title="nips-2012-77" href="../nips2012/nips-2012-Complex_Inference_in_Neural_Circuits_with_Probabilistic_Population_Codes_and_Topic_Models.html">nips-2012-Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models</a></p>
<p>78 <a title="nips-2012-78" href="../nips2012/nips-2012-Compressive_Sensing_MRI_with_Wavelet_Tree_Sparsity.html">nips-2012-Compressive Sensing MRI with Wavelet Tree Sparsity</a></p>
<p>79 <a title="nips-2012-79" href="../nips2012/nips-2012-Compressive_neural_representation_of_sparse%2C_high-dimensional_probabilities.html">nips-2012-Compressive neural representation of sparse, high-dimensional probabilities</a></p>
<p>80 <a title="nips-2012-80" href="../nips2012/nips-2012-Confusion-Based_Online_Learning_and_a_Passive-Aggressive_Scheme.html">nips-2012-Confusion-Based Online Learning and a Passive-Aggressive Scheme</a></p>
<p>81 <a title="nips-2012-81" href="../nips2012/nips-2012-Context-Sensitive_Decision_Forests_for_Object_Detection.html">nips-2012-Context-Sensitive Decision Forests for Object Detection</a></p>
<p>82 <a title="nips-2012-82" href="../nips2012/nips-2012-Continuous_Relaxations_for_Discrete_Hamiltonian_Monte_Carlo.html">nips-2012-Continuous Relaxations for Discrete Hamiltonian Monte Carlo</a></p>
<p>83 <a title="nips-2012-83" href="../nips2012/nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>84 <a title="nips-2012-84" href="../nips2012/nips-2012-Convergence_Rate_Analysis_of_MAP_Coordinate_Minimization_Algorithms.html">nips-2012-Convergence Rate Analysis of MAP Coordinate Minimization Algorithms</a></p>
<p>85 <a title="nips-2012-85" href="../nips2012/nips-2012-Convergence_and_Energy_Landscape_for_Cheeger_Cut_Clustering.html">nips-2012-Convergence and Energy Landscape for Cheeger Cut Clustering</a></p>
<p>86 <a title="nips-2012-86" href="../nips2012/nips-2012-Convex_Multi-view_Subspace_Learning.html">nips-2012-Convex Multi-view Subspace Learning</a></p>
<p>87 <a title="nips-2012-87" href="../nips2012/nips-2012-Convolutional-Recursive_Deep_Learning_for_3D_Object_Classification.html">nips-2012-Convolutional-Recursive Deep Learning for 3D Object Classification</a></p>
<p>88 <a title="nips-2012-88" href="../nips2012/nips-2012-Cost-Sensitive_Exploration_in_Bayesian_Reinforcement_Learning.html">nips-2012-Cost-Sensitive Exploration in Bayesian Reinforcement Learning</a></p>
<p>89 <a title="nips-2012-89" href="../nips2012/nips-2012-Coupling_Nonparametric_Mixtures_via_Latent_Dirichlet_Processes.html">nips-2012-Coupling Nonparametric Mixtures via Latent Dirichlet Processes</a></p>
<p>90 <a title="nips-2012-90" href="../nips2012/nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>91 <a title="nips-2012-91" href="../nips2012/nips-2012-Deep_Neural_Networks_Segment_Neuronal_Membranes_in_Electron_Microscopy_Images.html">nips-2012-Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images</a></p>
<p>92 <a title="nips-2012-92" href="../nips2012/nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>93 <a title="nips-2012-93" href="../nips2012/nips-2012-Deep_Spatio-Temporal_Architectures_and_Learning_for_Protein_Structure_Prediction.html">nips-2012-Deep Spatio-Temporal Architectures and Learning for Protein Structure Prediction</a></p>
<p>94 <a title="nips-2012-94" href="../nips2012/nips-2012-Delay_Compensation_with_Dynamical_Synapses.html">nips-2012-Delay Compensation with Dynamical Synapses</a></p>
<p>95 <a title="nips-2012-95" href="../nips2012/nips-2012-Density-Difference_Estimation.html">nips-2012-Density-Difference Estimation</a></p>
<p>96 <a title="nips-2012-96" href="../nips2012/nips-2012-Density_Propagation_and_Improved_Bounds_on_the_Partition_Function.html">nips-2012-Density Propagation and Improved Bounds on the Partition Function</a></p>
<p>97 <a title="nips-2012-97" href="../nips2012/nips-2012-Diffusion_Decision_Making_for_Adaptive_k-Nearest_Neighbor_Classification.html">nips-2012-Diffusion Decision Making for Adaptive k-Nearest Neighbor Classification</a></p>
<p>98 <a title="nips-2012-98" href="../nips2012/nips-2012-Dimensionality_Dependent_PAC-Bayes_Margin_Bound.html">nips-2012-Dimensionality Dependent PAC-Bayes Margin Bound</a></p>
<p>99 <a title="nips-2012-99" href="../nips2012/nips-2012-Dip-means%3A_an_incremental_clustering_method_for_estimating_the_number_of_clusters.html">nips-2012-Dip-means: an incremental clustering method for estimating the number of clusters</a></p>
<p>100 <a title="nips-2012-100" href="../nips2012/nips-2012-Discriminative_Learning_of_Sum-Product_Networks.html">nips-2012-Discriminative Learning of Sum-Product Networks</a></p>
<p>101 <a title="nips-2012-101" href="../nips2012/nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>102 <a title="nips-2012-102" href="../nips2012/nips-2012-Distributed_Non-Stochastic_Experts.html">nips-2012-Distributed Non-Stochastic Experts</a></p>
<p>103 <a title="nips-2012-103" href="../nips2012/nips-2012-Distributed_Probabilistic_Learning_for_Camera_Networks_with_Missing_Data.html">nips-2012-Distributed Probabilistic Learning for Camera Networks with Missing Data</a></p>
<p>104 <a title="nips-2012-104" href="../nips2012/nips-2012-Dual-Space_Analysis_of_the_Sparse_Linear_Model.html">nips-2012-Dual-Space Analysis of the Sparse Linear Model</a></p>
<p>105 <a title="nips-2012-105" href="../nips2012/nips-2012-Dynamic_Pruning_of_Factor_Graphs_for_Maximum_Marginal_Prediction.html">nips-2012-Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction</a></p>
<p>106 <a title="nips-2012-106" href="../nips2012/nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>107 <a title="nips-2012-107" href="../nips2012/nips-2012-Effective_Split-Merge_Monte_Carlo_Methods_for_Nonparametric_Models_of_Sequential_Data.html">nips-2012-Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data</a></p>
<p>108 <a title="nips-2012-108" href="../nips2012/nips-2012-Efficient_Bayes-Adaptive_Reinforcement_Learning_using_Sample-Based_Search.html">nips-2012-Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search</a></p>
<p>109 <a title="nips-2012-109" href="../nips2012/nips-2012-Efficient_Monte_Carlo_Counterfactual_Regret_Minimization_in_Games_with_Many_Player_Actions.html">nips-2012-Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions</a></p>
<p>110 <a title="nips-2012-110" href="../nips2012/nips-2012-Efficient_Reinforcement_Learning_for_High_Dimensional_Linear_Quadratic_Systems.html">nips-2012-Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems</a></p>
<p>111 <a title="nips-2012-111" href="../nips2012/nips-2012-Efficient_Sampling_for_Bipartite_Matching_Problems.html">nips-2012-Efficient Sampling for Bipartite Matching Problems</a></p>
<p>112 <a title="nips-2012-112" href="../nips2012/nips-2012-Efficient_Spike-Coding_with_Multiplicative_Adaptation_in_a_Spike_Response_Model.html">nips-2012-Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model</a></p>
<p>113 <a title="nips-2012-113" href="../nips2012/nips-2012-Efficient_and_direct_estimation_of_a_neural_subunit_model_for_sensory_coding.html">nips-2012-Efficient and direct estimation of a neural subunit model for sensory coding</a></p>
<p>114 <a title="nips-2012-114" href="../nips2012/nips-2012-Efficient_coding_provides_a_direct_link_between_prior_and_likelihood_in_perceptual_Bayesian_inference.html">nips-2012-Efficient coding provides a direct link between prior and likelihood in perceptual Bayesian inference</a></p>
<p>115 <a title="nips-2012-115" href="../nips2012/nips-2012-Efficient_high_dimensional_maximum_entropy_modeling_via_symmetric_partition_functions.html">nips-2012-Efficient high dimensional maximum entropy modeling via symmetric partition functions</a></p>
<p>116 <a title="nips-2012-116" href="../nips2012/nips-2012-Emergence_of_Object-Selective_Features_in_Unsupervised_Feature_Learning.html">nips-2012-Emergence of Object-Selective Features in Unsupervised Feature Learning</a></p>
<p>117 <a title="nips-2012-117" href="../nips2012/nips-2012-Ensemble_weighted_kernel_estimators_for_multivariate_entropy_estimation.html">nips-2012-Ensemble weighted kernel estimators for multivariate entropy estimation</a></p>
<p>118 <a title="nips-2012-118" href="../nips2012/nips-2012-Entangled_Monte_Carlo.html">nips-2012-Entangled Monte Carlo</a></p>
<p>119 <a title="nips-2012-119" href="../nips2012/nips-2012-Entropy_Estimations_Using_Correlated_Symmetric_Stable_Random_Projections.html">nips-2012-Entropy Estimations Using Correlated Symmetric Stable Random Projections</a></p>
<p>120 <a title="nips-2012-120" href="../nips2012/nips-2012-Exact_and_Stable_Recovery_of_Sequences_of_Signals_with_Sparse_Increments_via_Differential__1-Minimization.html">nips-2012-Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential  1-Minimization</a></p>
<p>121 <a title="nips-2012-121" href="../nips2012/nips-2012-Expectation_Propagation_in_Gaussian_Process_Dynamical_Systems.html">nips-2012-Expectation Propagation in Gaussian Process Dynamical Systems</a></p>
<p>122 <a title="nips-2012-122" href="../nips2012/nips-2012-Exploration_in_Model-based_Reinforcement_Learning_by_Empirically_Estimating_Learning_Progress.html">nips-2012-Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress</a></p>
<p>123 <a title="nips-2012-123" href="../nips2012/nips-2012-Exponential_Concentration_for_Mutual_Information_Estimation_with_Application_to_Forests.html">nips-2012-Exponential Concentration for Mutual Information Estimation with Application to Forests</a></p>
<p>124 <a title="nips-2012-124" href="../nips2012/nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>125 <a title="nips-2012-125" href="../nips2012/nips-2012-Factoring_nonnegative_matrices_with_linear_programs.html">nips-2012-Factoring nonnegative matrices with linear programs</a></p>
<p>126 <a title="nips-2012-126" href="../nips2012/nips-2012-FastEx%3A_Hash_Clustering_with_Exponential_Families.html">nips-2012-FastEx: Hash Clustering with Exponential Families</a></p>
<p>127 <a title="nips-2012-127" href="../nips2012/nips-2012-Fast_Bayesian_Inference_for_Non-Conjugate_Gaussian_Process_Regression.html">nips-2012-Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression</a></p>
<p>128 <a title="nips-2012-128" href="../nips2012/nips-2012-Fast_Resampling_Weighted_v-Statistics.html">nips-2012-Fast Resampling Weighted v-Statistics</a></p>
<p>129 <a title="nips-2012-129" href="../nips2012/nips-2012-Fast_Variational_Inference_in_the_Conjugate_Exponential_Family.html">nips-2012-Fast Variational Inference in the Conjugate Exponential Family</a></p>
<p>130 <a title="nips-2012-130" href="../nips2012/nips-2012-Feature-aware_Label_Space_Dimension_Reduction_for_Multi-label_Classification.html">nips-2012-Feature-aware Label Space Dimension Reduction for Multi-label Classification</a></p>
<p>131 <a title="nips-2012-131" href="../nips2012/nips-2012-Feature_Clustering_for_Accelerating_Parallel_Coordinate_Descent.html">nips-2012-Feature Clustering for Accelerating Parallel Coordinate Descent</a></p>
<p>132 <a title="nips-2012-132" href="../nips2012/nips-2012-Fiedler_Random_Fields%3A_A_Large-Scale_Spectral_Approach_to_Statistical_Network_Modeling.html">nips-2012-Fiedler Random Fields: A Large-Scale Spectral Approach to Statistical Network Modeling</a></p>
<p>133 <a title="nips-2012-133" href="../nips2012/nips-2012-Finding_Exemplars_from_Pairwise_Dissimilarities_via_Simultaneous_Sparse_Recovery.html">nips-2012-Finding Exemplars from Pairwise Dissimilarities via Simultaneous Sparse Recovery</a></p>
<p>134 <a title="nips-2012-134" href="../nips2012/nips-2012-Finite_Sample_Convergence_Rates_of_Zero-Order_Stochastic_Optimization_Methods.html">nips-2012-Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods</a></p>
<p>135 <a title="nips-2012-135" href="../nips2012/nips-2012-Forging_The_Graphs%3A_A_Low_Rank_and_Positive_Semidefinite_Graph_Learning_Approach.html">nips-2012-Forging The Graphs: A Low Rank and Positive Semidefinite Graph Learning Approach</a></p>
<p>136 <a title="nips-2012-136" href="../nips2012/nips-2012-Forward-Backward_Activation_Algorithm_for_Hierarchical_Hidden_Markov_Models.html">nips-2012-Forward-Backward Activation Algorithm for Hierarchical Hidden Markov Models</a></p>
<p>137 <a title="nips-2012-137" href="../nips2012/nips-2012-From_Deformations_to_Parts%3A_Motion-based_Segmentation_of_3D_Objects.html">nips-2012-From Deformations to Parts: Motion-based Segmentation of 3D Objects</a></p>
<p>138 <a title="nips-2012-138" href="../nips2012/nips-2012-Fully_Bayesian_inference_for_neural_models_with_negative-binomial_spiking.html">nips-2012-Fully Bayesian inference for neural models with negative-binomial spiking</a></p>
<p>139 <a title="nips-2012-139" href="../nips2012/nips-2012-Fused_sparsity_and_robust_estimation_for_linear_models_with_unknown_variance.html">nips-2012-Fused sparsity and robust estimation for linear models with unknown variance</a></p>
<p>140 <a title="nips-2012-140" href="../nips2012/nips-2012-Fusion_with_Diffusion_for_Robust_Visual_Tracking.html">nips-2012-Fusion with Diffusion for Robust Visual Tracking</a></p>
<p>141 <a title="nips-2012-141" href="../nips2012/nips-2012-GenDeR%3A_A_Generic_Diversified_Ranking_Algorithm.html">nips-2012-GenDeR: A Generic Diversified Ranking Algorithm</a></p>
<p>142 <a title="nips-2012-142" href="../nips2012/nips-2012-Generalization_Bounds_for_Domain_Adaptation.html">nips-2012-Generalization Bounds for Domain Adaptation</a></p>
<p>143 <a title="nips-2012-143" href="../nips2012/nips-2012-Globally_Convergent_Dual_MAP_LP_Relaxation_Solvers_using_Fenchel-Young_Margins.html">nips-2012-Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins</a></p>
<p>144 <a title="nips-2012-144" href="../nips2012/nips-2012-Gradient-based_kernel_method_for_feature_extraction_and_variable_selection.html">nips-2012-Gradient-based kernel method for feature extraction and variable selection</a></p>
<p>145 <a title="nips-2012-145" href="../nips2012/nips-2012-Gradient_Weights_help_Nonparametric_Regressors.html">nips-2012-Gradient Weights help Nonparametric Regressors</a></p>
<p>146 <a title="nips-2012-146" href="../nips2012/nips-2012-Graphical_Gaussian_Vector_for_Image_Categorization.html">nips-2012-Graphical Gaussian Vector for Image Categorization</a></p>
<p>147 <a title="nips-2012-147" href="../nips2012/nips-2012-Graphical_Models_via_Generalized_Linear_Models.html">nips-2012-Graphical Models via Generalized Linear Models</a></p>
<p>148 <a title="nips-2012-148" href="../nips2012/nips-2012-Hamming_Distance_Metric_Learning.html">nips-2012-Hamming Distance Metric Learning</a></p>
<p>149 <a title="nips-2012-149" href="../nips2012/nips-2012-Hierarchical_Optimistic_Region_Selection_driven_by_Curiosity.html">nips-2012-Hierarchical Optimistic Region Selection driven by Curiosity</a></p>
<p>150 <a title="nips-2012-150" href="../nips2012/nips-2012-Hierarchical_spike_coding_of_sound.html">nips-2012-Hierarchical spike coding of sound</a></p>
<p>151 <a title="nips-2012-151" href="../nips2012/nips-2012-High-Order_Multi-Task_Feature_Learning_to_Identify_Longitudinal_Phenotypic_Markers_for_Alzheimer%27s_Disease_Progression_Prediction.html">nips-2012-High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer's Disease Progression Prediction</a></p>
<p>152 <a title="nips-2012-152" href="../nips2012/nips-2012-Homeostatic_plasticity_in_Bayesian_spiking_networks_as_Expectation_Maximization_with_posterior_constraints.html">nips-2012-Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints</a></p>
<p>153 <a title="nips-2012-153" href="../nips2012/nips-2012-How_Prior_Probability_Influences_Decision_Making%3A_A_Unifying_Probabilistic_Model.html">nips-2012-How Prior Probability Influences Decision Making: A Unifying Probabilistic Model</a></p>
<p>154 <a title="nips-2012-154" href="../nips2012/nips-2012-How_They_Vote%3A_Issue-Adjusted_Models_of_Legislative_Behavior.html">nips-2012-How They Vote: Issue-Adjusted Models of Legislative Behavior</a></p>
<p>155 <a title="nips-2012-155" href="../nips2012/nips-2012-Human_memory_search_as_a_random_walk_in_a_semantic_network.html">nips-2012-Human memory search as a random walk in a semantic network</a></p>
<p>156 <a title="nips-2012-156" href="../nips2012/nips-2012-Identifiability_and_Unmixing_of_Latent_Parse_Trees.html">nips-2012-Identifiability and Unmixing of Latent Parse Trees</a></p>
<p>157 <a title="nips-2012-157" href="../nips2012/nips-2012-Identification_of_Recurrent_Patterns_in_the_Activation_of_Brain_Networks.html">nips-2012-Identification of Recurrent Patterns in the Activation of Brain Networks</a></p>
<p>158 <a title="nips-2012-158" href="../nips2012/nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>159 <a title="nips-2012-159" href="../nips2012/nips-2012-Image_Denoising_and_Inpainting_with_Deep_Neural_Networks.html">nips-2012-Image Denoising and Inpainting with Deep Neural Networks</a></p>
<p>160 <a title="nips-2012-160" href="../nips2012/nips-2012-Imitation_Learning_by_Coaching.html">nips-2012-Imitation Learning by Coaching</a></p>
<p>161 <a title="nips-2012-161" href="../nips2012/nips-2012-Interpreting_prediction_markets%3A_a_stochastic_approach.html">nips-2012-Interpreting prediction markets: a stochastic approach</a></p>
<p>162 <a title="nips-2012-162" href="../nips2012/nips-2012-Inverse_Reinforcement_Learning_through_Structured_Classification.html">nips-2012-Inverse Reinforcement Learning through Structured Classification</a></p>
<p>163 <a title="nips-2012-163" href="../nips2012/nips-2012-Isotropic_Hashing.html">nips-2012-Isotropic Hashing</a></p>
<p>164 <a title="nips-2012-164" href="../nips2012/nips-2012-Iterative_Thresholding_Algorithm_for_Sparse_Inverse_Covariance_Estimation.html">nips-2012-Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation</a></p>
<p>165 <a title="nips-2012-165" href="../nips2012/nips-2012-Iterative_ranking_from_pair-wise_comparisons.html">nips-2012-Iterative ranking from pair-wise comparisons</a></p>
<p>166 <a title="nips-2012-166" href="../nips2012/nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>167 <a title="nips-2012-167" href="../nips2012/nips-2012-Kernel_Hyperalignment.html">nips-2012-Kernel Hyperalignment</a></p>
<p>168 <a title="nips-2012-168" href="../nips2012/nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>169 <a title="nips-2012-169" href="../nips2012/nips-2012-Label_Ranking_with_Partial_Abstention_based_on_Thresholded_Probabilistic_Models.html">nips-2012-Label Ranking with Partial Abstention based on Thresholded Probabilistic Models</a></p>
<p>170 <a title="nips-2012-170" href="../nips2012/nips-2012-Large_Scale_Distributed_Deep_Networks.html">nips-2012-Large Scale Distributed Deep Networks</a></p>
<p>171 <a title="nips-2012-171" href="../nips2012/nips-2012-Latent_Coincidence_Analysis%3A_A_Hidden_Variable_Model_for_Distance_Metric_Learning.html">nips-2012-Latent Coincidence Analysis: A Hidden Variable Model for Distance Metric Learning</a></p>
<p>172 <a title="nips-2012-172" href="../nips2012/nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>173 <a title="nips-2012-173" href="../nips2012/nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<p>174 <a title="nips-2012-174" href="../nips2012/nips-2012-Learning_Halfspaces_with_the_Zero-One_Loss%3A_Time-Accuracy_Tradeoffs.html">nips-2012-Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs</a></p>
<p>175 <a title="nips-2012-175" href="../nips2012/nips-2012-Learning_High-Density_Regions_for_a_Generalized_Kolmogorov-Smirnov_Test_in_High-Dimensional_Data.html">nips-2012-Learning High-Density Regions for a Generalized Kolmogorov-Smirnov Test in High-Dimensional Data</a></p>
<p>176 <a title="nips-2012-176" href="../nips2012/nips-2012-Learning_Image_Descriptors_with_the_Boosting-Trick.html">nips-2012-Learning Image Descriptors with the Boosting-Trick</a></p>
<p>177 <a title="nips-2012-177" href="../nips2012/nips-2012-Learning_Invariant_Representations_of_Molecules_for_Atomization_Energy_Prediction.html">nips-2012-Learning Invariant Representations of Molecules for Atomization Energy Prediction</a></p>
<p>178 <a title="nips-2012-178" href="../nips2012/nips-2012-Learning_Label_Trees_for_Probabilistic_Modelling_of_Implicit_Feedback.html">nips-2012-Learning Label Trees for Probabilistic Modelling of Implicit Feedback</a></p>
<p>179 <a title="nips-2012-179" href="../nips2012/nips-2012-Learning_Manifolds_with_K-Means_and_K-Flats.html">nips-2012-Learning Manifolds with K-Means and K-Flats</a></p>
<p>180 <a title="nips-2012-180" href="../nips2012/nips-2012-Learning_Mixtures_of_Tree_Graphical_Models.html">nips-2012-Learning Mixtures of Tree Graphical Models</a></p>
<p>181 <a title="nips-2012-181" href="../nips2012/nips-2012-Learning_Multiple_Tasks_using_Shared_Hypotheses.html">nips-2012-Learning Multiple Tasks using Shared Hypotheses</a></p>
<p>182 <a title="nips-2012-182" href="../nips2012/nips-2012-Learning_Networks_of_Heterogeneous_Influence.html">nips-2012-Learning Networks of Heterogeneous Influence</a></p>
<p>183 <a title="nips-2012-183" href="../nips2012/nips-2012-Learning_Partially_Observable_Models_Using_Temporally_Abstract_Decision_Trees.html">nips-2012-Learning Partially Observable Models Using Temporally Abstract Decision Trees</a></p>
<p>184 <a title="nips-2012-184" href="../nips2012/nips-2012-Learning_Probability_Measures_with_respect_to_Optimal_Transport_Metrics.html">nips-2012-Learning Probability Measures with respect to Optimal Transport Metrics</a></p>
<p>185 <a title="nips-2012-185" href="../nips2012/nips-2012-Learning_about_Canonical_Views_from_Internet_Image_Collections.html">nips-2012-Learning about Canonical Views from Internet Image Collections</a></p>
<p>186 <a title="nips-2012-186" href="../nips2012/nips-2012-Learning_as_MAP_Inference_in_Discrete_Graphical_Models.html">nips-2012-Learning as MAP Inference in Discrete Graphical Models</a></p>
<p>187 <a title="nips-2012-187" href="../nips2012/nips-2012-Learning_curves_for_multi-task_Gaussian_process_regression.html">nips-2012-Learning curves for multi-task Gaussian process regression</a></p>
<p>188 <a title="nips-2012-188" href="../nips2012/nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>189 <a title="nips-2012-189" href="../nips2012/nips-2012-Learning_from_the_Wisdom_of_Crowds_by_Minimax_Entropy.html">nips-2012-Learning from the Wisdom of Crowds by Minimax Entropy</a></p>
<p>190 <a title="nips-2012-190" href="../nips2012/nips-2012-Learning_optimal_spike-based_representations.html">nips-2012-Learning optimal spike-based representations</a></p>
<p>191 <a title="nips-2012-191" href="../nips2012/nips-2012-Learning_the_Architecture_of_Sum-Product_Networks_Using_Clustering_on_Variables.html">nips-2012-Learning the Architecture of Sum-Product Networks Using Clustering on Variables</a></p>
<p>192 <a title="nips-2012-192" href="../nips2012/nips-2012-Learning_the_Dependency_Structure_of_Latent_Factors.html">nips-2012-Learning the Dependency Structure of Latent Factors</a></p>
<p>193 <a title="nips-2012-193" href="../nips2012/nips-2012-Learning_to_Align_from_Scratch.html">nips-2012-Learning to Align from Scratch</a></p>
<p>194 <a title="nips-2012-194" href="../nips2012/nips-2012-Learning_to_Discover_Social_Circles_in_Ego_Networks.html">nips-2012-Learning to Discover Social Circles in Ego Networks</a></p>
<p>195 <a title="nips-2012-195" href="../nips2012/nips-2012-Learning_visual_motion_in_recurrent_neural_networks.html">nips-2012-Learning visual motion in recurrent neural networks</a></p>
<p>196 <a title="nips-2012-196" href="../nips2012/nips-2012-Learning_with_Partially_Absorbing_Random_Walks.html">nips-2012-Learning with Partially Absorbing Random Walks</a></p>
<p>197 <a title="nips-2012-197" href="../nips2012/nips-2012-Learning_with_Recursive_Perceptual_Representations.html">nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>198 <a title="nips-2012-198" href="../nips2012/nips-2012-Learning_with_Target_Prior.html">nips-2012-Learning with Target Prior</a></p>
<p>199 <a title="nips-2012-199" href="../nips2012/nips-2012-Link_Prediction_in_Graphs_with_Autoregressive_Features.html">nips-2012-Link Prediction in Graphs with Autoregressive Features</a></p>
<p>200 <a title="nips-2012-200" href="../nips2012/nips-2012-Local_Supervised_Learning_through_Space_Partitioning.html">nips-2012-Local Supervised Learning through Space Partitioning</a></p>
<p>201 <a title="nips-2012-201" href="../nips2012/nips-2012-Localizing_3D_cuboids_in_single-view_images.html">nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>202 <a title="nips-2012-202" href="../nips2012/nips-2012-Locally_Uniform_Comparison_Image_Descriptor.html">nips-2012-Locally Uniform Comparison Image Descriptor</a></p>
<p>203 <a title="nips-2012-203" href="../nips2012/nips-2012-Locating_Changes_in_Highly_Dependent_Data_with_Unknown_Number_of_Change_Points.html">nips-2012-Locating Changes in Highly Dependent Data with Unknown Number of Change Points</a></p>
<p>204 <a title="nips-2012-204" href="../nips2012/nips-2012-MAP_Inference_in_Chains_using_Column_Generation.html">nips-2012-MAP Inference in Chains using Column Generation</a></p>
<p>205 <a title="nips-2012-205" href="../nips2012/nips-2012-MCMC_for_continuous-time_discrete-state_systems.html">nips-2012-MCMC for continuous-time discrete-state systems</a></p>
<p>206 <a title="nips-2012-206" href="../nips2012/nips-2012-Majorization_for_CRFs_and_Latent_Likelihoods.html">nips-2012-Majorization for CRFs and Latent Likelihoods</a></p>
<p>207 <a title="nips-2012-207" href="../nips2012/nips-2012-Mandatory_Leaf_Node_Prediction_in_Hierarchical_Multilabel_Classification.html">nips-2012-Mandatory Leaf Node Prediction in Hierarchical Multilabel Classification</a></p>
<p>208 <a title="nips-2012-208" href="../nips2012/nips-2012-Matrix_reconstruction_with_the_local_max_norm.html">nips-2012-Matrix reconstruction with the local max norm</a></p>
<p>209 <a title="nips-2012-209" href="../nips2012/nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>210 <a title="nips-2012-210" href="../nips2012/nips-2012-Memorability_of_Image_Regions.html">nips-2012-Memorability of Image Regions</a></p>
<p>211 <a title="nips-2012-211" href="../nips2012/nips-2012-Meta-Gaussian_Information_Bottleneck.html">nips-2012-Meta-Gaussian Information Bottleneck</a></p>
<p>212 <a title="nips-2012-212" href="../nips2012/nips-2012-Minimax_Multi-Task_Learning_and_a_Generalized_Loss-Compositional_Paradigm_for_MTL.html">nips-2012-Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL</a></p>
<p>213 <a title="nips-2012-213" href="../nips2012/nips-2012-Minimization_of_Continuous_Bethe_Approximations%3A_A_Positive_Variation.html">nips-2012-Minimization of Continuous Bethe Approximations: A Positive Variation</a></p>
<p>214 <a title="nips-2012-214" href="../nips2012/nips-2012-Minimizing_Sparse_High-Order_Energies_by_Submodular_Vertex-Cover.html">nips-2012-Minimizing Sparse High-Order Energies by Submodular Vertex-Cover</a></p>
<p>215 <a title="nips-2012-215" href="../nips2012/nips-2012-Minimizing_Uncertainty_in_Pipelines.html">nips-2012-Minimizing Uncertainty in Pipelines</a></p>
<p>216 <a title="nips-2012-216" href="../nips2012/nips-2012-Mirror_Descent_Meets_Fixed_Share_%28and_feels_no_regret%29.html">nips-2012-Mirror Descent Meets Fixed Share (and feels no regret)</a></p>
<p>217 <a title="nips-2012-217" href="../nips2012/nips-2012-Mixability_in_Statistical_Learning.html">nips-2012-Mixability in Statistical Learning</a></p>
<p>218 <a title="nips-2012-218" href="../nips2012/nips-2012-Mixing_Properties_of_Conditional_Markov_Chains_with_Unbounded_Feature_Functions.html">nips-2012-Mixing Properties of Conditional Markov Chains with Unbounded Feature Functions</a></p>
<p>219 <a title="nips-2012-219" href="../nips2012/nips-2012-Modelling_Reciprocating_Relationships_with_Hawkes_Processes.html">nips-2012-Modelling Reciprocating Relationships with Hawkes Processes</a></p>
<p>220 <a title="nips-2012-220" href="../nips2012/nips-2012-Monte_Carlo_Methods_for_Maximum_Margin_Supervised_Topic_Models.html">nips-2012-Monte Carlo Methods for Maximum Margin Supervised Topic Models</a></p>
<p>221 <a title="nips-2012-221" href="../nips2012/nips-2012-Multi-Stage_Multi-Task_Feature_Learning.html">nips-2012-Multi-Stage Multi-Task Feature Learning</a></p>
<p>222 <a title="nips-2012-222" href="../nips2012/nips-2012-Multi-Task_Averaging.html">nips-2012-Multi-Task Averaging</a></p>
<p>223 <a title="nips-2012-223" href="../nips2012/nips-2012-Multi-criteria_Anomaly_Detection_using_Pareto_Depth_Analysis.html">nips-2012-Multi-criteria Anomaly Detection using Pareto Depth Analysis</a></p>
<p>224 <a title="nips-2012-224" href="../nips2012/nips-2012-Multi-scale_Hyper-time_Hardware_Emulation_of_Human_Motor_Nervous_System_Based_on_Spiking_Neurons_using_FPGA.html">nips-2012-Multi-scale Hyper-time Hardware Emulation of Human Motor Nervous System Based on Spiking Neurons using FPGA</a></p>
<p>225 <a title="nips-2012-225" href="../nips2012/nips-2012-Multi-task_Vector_Field_Learning.html">nips-2012-Multi-task Vector Field Learning</a></p>
<p>226 <a title="nips-2012-226" href="../nips2012/nips-2012-Multiclass_Learning_Approaches%3A_A_Theoretical_Comparison_with_Implications.html">nips-2012-Multiclass Learning Approaches: A Theoretical Comparison with Implications</a></p>
<p>227 <a title="nips-2012-227" href="../nips2012/nips-2012-Multiclass_Learning_with_Simplex_Coding.html">nips-2012-Multiclass Learning with Simplex Coding</a></p>
<p>228 <a title="nips-2012-228" href="../nips2012/nips-2012-Multilabel_Classification_using_Bayesian_Compressed_Sensing.html">nips-2012-Multilabel Classification using Bayesian Compressed Sensing</a></p>
<p>229 <a title="nips-2012-229" href="../nips2012/nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines.html">nips-2012-Multimodal Learning with Deep Boltzmann Machines</a></p>
<p>230 <a title="nips-2012-230" href="../nips2012/nips-2012-Multiple_Choice_Learning%3A_Learning_to_Produce_Multiple_Structured_Outputs.html">nips-2012-Multiple Choice Learning: Learning to Produce Multiple Structured Outputs</a></p>
<p>231 <a title="nips-2012-231" href="../nips2012/nips-2012-Multiple_Operator-valued_Kernel_Learning.html">nips-2012-Multiple Operator-valued Kernel Learning</a></p>
<p>232 <a title="nips-2012-232" href="../nips2012/nips-2012-Multiplicative_Forests_for_Continuous-Time_Processes.html">nips-2012-Multiplicative Forests for Continuous-Time Processes</a></p>
<p>233 <a title="nips-2012-233" href="../nips2012/nips-2012-Multiresolution_Gaussian_Processes.html">nips-2012-Multiresolution Gaussian Processes</a></p>
<p>234 <a title="nips-2012-234" href="../nips2012/nips-2012-Multiresolution_analysis_on_the_symmetric_group.html">nips-2012-Multiresolution analysis on the symmetric group</a></p>
<p>235 <a title="nips-2012-235" href="../nips2012/nips-2012-Natural_Images%2C_Gaussian_Mixtures_and_Dead_Leaves.html">nips-2012-Natural Images, Gaussian Mixtures and Dead Leaves</a></p>
<p>236 <a title="nips-2012-236" href="../nips2012/nips-2012-Near-Optimal_MAP_Inference_for_Determinantal_Point_Processes.html">nips-2012-Near-Optimal MAP Inference for Determinantal Point Processes</a></p>
<p>237 <a title="nips-2012-237" href="../nips2012/nips-2012-Near-optimal_Differentially_Private_Principal_Components.html">nips-2012-Near-optimal Differentially Private Principal Components</a></p>
<p>238 <a title="nips-2012-238" href="../nips2012/nips-2012-Neurally_Plausible_Reinforcement_Learning_of_Working_Memory_Tasks.html">nips-2012-Neurally Plausible Reinforcement Learning of Working Memory Tasks</a></p>
<p>239 <a title="nips-2012-239" href="../nips2012/nips-2012-Neuronal_Spike_Generation_Mechanism_as_an_Oversampling%2C_Noise-shaping_A-to-D_converter.html">nips-2012-Neuronal Spike Generation Mechanism as an Oversampling, Noise-shaping A-to-D converter</a></p>
<p>240 <a title="nips-2012-240" href="../nips2012/nips-2012-Newton-Like_Methods_for_Sparse_Inverse_Covariance_Estimation.html">nips-2012-Newton-Like Methods for Sparse Inverse Covariance Estimation</a></p>
<p>241 <a title="nips-2012-241" href="../nips2012/nips-2012-No-Regret_Algorithms_for_Unconstrained_Online_Convex_Optimization.html">nips-2012-No-Regret Algorithms for Unconstrained Online Convex Optimization</a></p>
<p>242 <a title="nips-2012-242" href="../nips2012/nips-2012-Non-linear_Metric_Learning.html">nips-2012-Non-linear Metric Learning</a></p>
<p>243 <a title="nips-2012-243" href="../nips2012/nips-2012-Non-parametric_Approximate_Dynamic_Programming_via_the_Kernel_Method.html">nips-2012-Non-parametric Approximate Dynamic Programming via the Kernel Method</a></p>
<p>244 <a title="nips-2012-244" href="../nips2012/nips-2012-Nonconvex_Penalization_Using_Laplace_Exponents_and_Concave_Conjugates.html">nips-2012-Nonconvex Penalization Using Laplace Exponents and Concave Conjugates</a></p>
<p>245 <a title="nips-2012-245" href="../nips2012/nips-2012-Nonparametric_Bayesian_Inverse_Reinforcement_Learning_for_Multiple_Reward_Functions.html">nips-2012-Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions</a></p>
<p>246 <a title="nips-2012-246" href="../nips2012/nips-2012-Nonparametric_Max-Margin_Matrix_Factorization_for_Collaborative_Prediction.html">nips-2012-Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction</a></p>
<p>247 <a title="nips-2012-247" href="../nips2012/nips-2012-Nonparametric_Reduced_Rank_Regression.html">nips-2012-Nonparametric Reduced Rank Regression</a></p>
<p>248 <a title="nips-2012-248" href="../nips2012/nips-2012-Nonparanormal_Belief_Propagation_%28NPNBP%29.html">nips-2012-Nonparanormal Belief Propagation (NPNBP)</a></p>
<p>249 <a title="nips-2012-249" href="../nips2012/nips-2012-Nystr%C3%B6m_Method_vs_Random_Fourier_Features%3A_A_Theoretical_and_Empirical_Comparison.html">nips-2012-Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison</a></p>
<p>250 <a title="nips-2012-250" href="../nips2012/nips-2012-On-line_Reinforcement_Learning_Using_Incremental_Kernel-Based_Stochastic_Factorization.html">nips-2012-On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization</a></p>
<p>251 <a title="nips-2012-251" href="../nips2012/nips-2012-On_Lifting_the_Gibbs_Sampling_Algorithm.html">nips-2012-On Lifting the Gibbs Sampling Algorithm</a></p>
<p>252 <a title="nips-2012-252" href="../nips2012/nips-2012-On_Multilabel_Classification_and_Ranking_with_Partial_Feedback.html">nips-2012-On Multilabel Classification and Ranking with Partial Feedback</a></p>
<p>253 <a title="nips-2012-253" href="../nips2012/nips-2012-On_Triangular_versus_Edge_Representations_---_Towards_Scalable_Modeling_of_Networks.html">nips-2012-On Triangular versus Edge Representations --- Towards Scalable Modeling of Networks</a></p>
<p>254 <a title="nips-2012-254" href="../nips2012/nips-2012-On_the_Sample_Complexity_of_Robust_PCA.html">nips-2012-On the Sample Complexity of Robust PCA</a></p>
<p>255 <a title="nips-2012-255" href="../nips2012/nips-2012-On_the_Use_of_Non-Stationary_Policies_for_Stationary_Infinite-Horizon_Markov_Decision_Processes.html">nips-2012-On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes</a></p>
<p>256 <a title="nips-2012-256" href="../nips2012/nips-2012-On_the_connections_between_saliency_and_tracking.html">nips-2012-On the connections between saliency and tracking</a></p>
<p>257 <a title="nips-2012-257" href="../nips2012/nips-2012-One_Permutation_Hashing.html">nips-2012-One Permutation Hashing</a></p>
<p>258 <a title="nips-2012-258" href="../nips2012/nips-2012-Online_L1-Dictionary_Learning_with_Application_to_Novel_Document_Detection.html">nips-2012-Online L1-Dictionary Learning with Application to Novel Document Detection</a></p>
<p>259 <a title="nips-2012-259" href="../nips2012/nips-2012-Online_Regret_Bounds_for_Undiscounted_Continuous_Reinforcement_Learning.html">nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</a></p>
<p>260 <a title="nips-2012-260" href="../nips2012/nips-2012-Online_Sum-Product_Computation_Over_Trees.html">nips-2012-Online Sum-Product Computation Over Trees</a></p>
<p>261 <a title="nips-2012-261" href="../nips2012/nips-2012-Online_allocation_and_homogeneous_partitioning_for_piecewise_constant_mean-approximation.html">nips-2012-Online allocation and homogeneous partitioning for piecewise constant mean-approximation</a></p>
<p>262 <a title="nips-2012-262" href="../nips2012/nips-2012-Optimal_Neural_Tuning_Curves_for_Arbitrary_Stimulus_Distributions%3A_Discrimax%2C_Infomax_and_Minimum_%24L_p%24_Loss.html">nips-2012-Optimal Neural Tuning Curves for Arbitrary Stimulus Distributions: Discrimax, Infomax and Minimum $L p$ Loss</a></p>
<p>263 <a title="nips-2012-263" href="../nips2012/nips-2012-Optimal_Regularized_Dual_Averaging_Methods_for_Stochastic_Optimization.html">nips-2012-Optimal Regularized Dual Averaging Methods for Stochastic Optimization</a></p>
<p>264 <a title="nips-2012-264" href="../nips2012/nips-2012-Optimal_kernel_choice_for_large-scale_two-sample_tests.html">nips-2012-Optimal kernel choice for large-scale two-sample tests</a></p>
<p>265 <a title="nips-2012-265" href="../nips2012/nips-2012-Parametric_Local_Metric_Learning_for_Nearest_Neighbor_Classification.html">nips-2012-Parametric Local Metric Learning for Nearest Neighbor Classification</a></p>
<p>266 <a title="nips-2012-266" href="../nips2012/nips-2012-Patient_Risk_Stratification_for_Hospital-Associated_C._diff_as_a_Time-Series_Classification_Task.html">nips-2012-Patient Risk Stratification for Hospital-Associated C. diff as a Time-Series Classification Task</a></p>
<p>267 <a title="nips-2012-267" href="../nips2012/nips-2012-Perceptron_Learning_of_SAT.html">nips-2012-Perceptron Learning of SAT</a></p>
<p>268 <a title="nips-2012-268" href="../nips2012/nips-2012-Perfect_Dimensionality_Recovery_by_Variational_Bayesian_PCA.html">nips-2012-Perfect Dimensionality Recovery by Variational Bayesian PCA</a></p>
<p>269 <a title="nips-2012-269" href="../nips2012/nips-2012-Persistent_Homology_for_Learning_Densities_with_Bounded_Support.html">nips-2012-Persistent Homology for Learning Densities with Bounded Support</a></p>
<p>270 <a title="nips-2012-270" href="../nips2012/nips-2012-Phoneme_Classification_using_Constrained_Variational_Gaussian_Process_Dynamical_System.html">nips-2012-Phoneme Classification using Constrained Variational Gaussian Process Dynamical System</a></p>
<p>271 <a title="nips-2012-271" href="../nips2012/nips-2012-Pointwise_Tracking_the_Optimal_Regression_Function.html">nips-2012-Pointwise Tracking the Optimal Regression Function</a></p>
<p>272 <a title="nips-2012-272" href="../nips2012/nips-2012-Practical_Bayesian_Optimization_of_Machine_Learning_Algorithms.html">nips-2012-Practical Bayesian Optimization of Machine Learning Algorithms</a></p>
<p>273 <a title="nips-2012-273" href="../nips2012/nips-2012-Predicting_Action_Content_On-Line_and_in_Real_Time_before_Action_Onset_%E2%80%93_an_Intracranial_Human_Study.html">nips-2012-Predicting Action Content On-Line and in Real Time before Action Onset – an Intracranial Human Study</a></p>
<p>274 <a title="nips-2012-274" href="../nips2012/nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>275 <a title="nips-2012-275" href="../nips2012/nips-2012-Privacy_Aware_Learning.html">nips-2012-Privacy Aware Learning</a></p>
<p>276 <a title="nips-2012-276" href="../nips2012/nips-2012-Probabilistic_Event_Cascades_for_Alzheimer%27s_disease.html">nips-2012-Probabilistic Event Cascades for Alzheimer's disease</a></p>
<p>277 <a title="nips-2012-277" href="../nips2012/nips-2012-Probabilistic_Low-Rank_Subspace_Clustering.html">nips-2012-Probabilistic Low-Rank Subspace Clustering</a></p>
<p>278 <a title="nips-2012-278" href="../nips2012/nips-2012-Probabilistic_n-Choose-k_Models_for_Classification_and_Ranking.html">nips-2012-Probabilistic n-Choose-k Models for Classification and Ranking</a></p>
<p>279 <a title="nips-2012-279" href="../nips2012/nips-2012-Projection_Retrieval_for_Classification.html">nips-2012-Projection Retrieval for Classification</a></p>
<p>280 <a title="nips-2012-280" href="../nips2012/nips-2012-Proper_losses_for_learning_from_partial_labels.html">nips-2012-Proper losses for learning from partial labels</a></p>
<p>281 <a title="nips-2012-281" href="../nips2012/nips-2012-Provable_ICA_with_Unknown_Gaussian_Noise%2C_with_Implications_for_Gaussian_Mixtures_and_Autoencoders.html">nips-2012-Provable ICA with Unknown Gaussian Noise, with Implications for Gaussian Mixtures and Autoencoders</a></p>
<p>282 <a title="nips-2012-282" href="../nips2012/nips-2012-Proximal_Newton-type_methods_for_convex_optimization.html">nips-2012-Proximal Newton-type methods for convex optimization</a></p>
<p>283 <a title="nips-2012-283" href="../nips2012/nips-2012-Putting_Bayes_to_sleep.html">nips-2012-Putting Bayes to sleep</a></p>
<p>284 <a title="nips-2012-284" href="../nips2012/nips-2012-Q-MKL%3A_Matrix-induced_Regularization_in_Multi-Kernel_Learning_with_Applications_to_Neuroimaging.html">nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</a></p>
<p>285 <a title="nips-2012-285" href="../nips2012/nips-2012-Query_Complexity_of_Derivative-Free_Optimization.html">nips-2012-Query Complexity of Derivative-Free Optimization</a></p>
<p>286 <a title="nips-2012-286" href="../nips2012/nips-2012-Random_Utility_Theory_for_Social_Choice.html">nips-2012-Random Utility Theory for Social Choice</a></p>
<p>287 <a title="nips-2012-287" href="../nips2012/nips-2012-Random_function_priors_for_exchangeable_arrays_with_applications_to_graphs_and_relational_data.html">nips-2012-Random function priors for exchangeable arrays with applications to graphs and relational data</a></p>
<p>288 <a title="nips-2012-288" href="../nips2012/nips-2012-Rational_inference_of_relative_preferences.html">nips-2012-Rational inference of relative preferences</a></p>
<p>289 <a title="nips-2012-289" href="../nips2012/nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>290 <a title="nips-2012-290" href="../nips2012/nips-2012-Recovery_of_Sparse_Probability_Measures_via_Convex_Programming.html">nips-2012-Recovery of Sparse Probability Measures via Convex Programming</a></p>
<p>291 <a title="nips-2012-291" href="../nips2012/nips-2012-Reducing_statistical_time-series_problems_to_binary_classification.html">nips-2012-Reducing statistical time-series problems to binary classification</a></p>
<p>292 <a title="nips-2012-292" href="../nips2012/nips-2012-Regularized_Off-Policy_TD-Learning.html">nips-2012-Regularized Off-Policy TD-Learning</a></p>
<p>293 <a title="nips-2012-293" href="../nips2012/nips-2012-Relax_and_Randomize_%3A_From_Value_to_Algorithms.html">nips-2012-Relax and Randomize : From Value to Algorithms</a></p>
<p>294 <a title="nips-2012-294" href="../nips2012/nips-2012-Repulsive_Mixtures.html">nips-2012-Repulsive Mixtures</a></p>
<p>295 <a title="nips-2012-295" href="../nips2012/nips-2012-Risk-Aversion_in_Multi-armed_Bandits.html">nips-2012-Risk-Aversion in Multi-armed Bandits</a></p>
<p>296 <a title="nips-2012-296" href="../nips2012/nips-2012-Risk_Aversion_in_Markov_Decision_Processes_via_Near_Optimal_Chernoff_Bounds.html">nips-2012-Risk Aversion in Markov Decision Processes via Near Optimal Chernoff Bounds</a></p>
<p>297 <a title="nips-2012-297" href="../nips2012/nips-2012-Robustness_and_risk-sensitivity_in_Markov_decision_processes.html">nips-2012-Robustness and risk-sensitivity in Markov decision processes</a></p>
<p>298 <a title="nips-2012-298" href="../nips2012/nips-2012-Scalable_Inference_of_Overlapping_Communities.html">nips-2012-Scalable Inference of Overlapping Communities</a></p>
<p>299 <a title="nips-2012-299" href="../nips2012/nips-2012-Scalable_imputation_of_genetic_data_with_a_discrete_fragmentation-coagulation_process.html">nips-2012-Scalable imputation of genetic data with a discrete fragmentation-coagulation process</a></p>
<p>300 <a title="nips-2012-300" href="../nips2012/nips-2012-Scalable_nonconvex_inexact_proximal_splitting.html">nips-2012-Scalable nonconvex inexact proximal splitting</a></p>
<p>301 <a title="nips-2012-301" href="../nips2012/nips-2012-Scaled_Gradients_on_Grassmann_Manifolds_for_Matrix_Completion.html">nips-2012-Scaled Gradients on Grassmann Manifolds for Matrix Completion</a></p>
<p>302 <a title="nips-2012-302" href="../nips2012/nips-2012-Scaling_MPE_Inference_for_Constrained_Continuous_Markov_Random_Fields_with_Consensus_Optimization.html">nips-2012-Scaling MPE Inference for Constrained Continuous Markov Random Fields with Consensus Optimization</a></p>
<p>303 <a title="nips-2012-303" href="../nips2012/nips-2012-Searching_for_objects_driven_by_context.html">nips-2012-Searching for objects driven by context</a></p>
<p>304 <a title="nips-2012-304" href="../nips2012/nips-2012-Selecting_Diverse_Features_via_Spectral_Regularization.html">nips-2012-Selecting Diverse Features via Spectral Regularization</a></p>
<p>305 <a title="nips-2012-305" href="../nips2012/nips-2012-Selective_Labeling_via_Error_Bound_Minimization.html">nips-2012-Selective Labeling via Error Bound Minimization</a></p>
<p>306 <a title="nips-2012-306" href="../nips2012/nips-2012-Semantic_Kernel_Forests_from_Multiple_Taxonomies.html">nips-2012-Semantic Kernel Forests from Multiple Taxonomies</a></p>
<p>307 <a title="nips-2012-307" href="../nips2012/nips-2012-Semi-Crowdsourced_Clustering%3A_Generalizing_Crowd_Labeling_by_Robust_Distance_Metric_Learning.html">nips-2012-Semi-Crowdsourced Clustering: Generalizing Crowd Labeling by Robust Distance Metric Learning</a></p>
<p>308 <a title="nips-2012-308" href="../nips2012/nips-2012-Semi-Supervised_Domain_Adaptation_with_Non-Parametric_Copulas.html">nips-2012-Semi-Supervised Domain Adaptation with Non-Parametric Copulas</a></p>
<p>309 <a title="nips-2012-309" href="../nips2012/nips-2012-Semi-supervised_Eigenvectors_for_Locally-biased_Learning.html">nips-2012-Semi-supervised Eigenvectors for Locally-biased Learning</a></p>
<p>310 <a title="nips-2012-310" href="../nips2012/nips-2012-Semiparametric_Principal_Component_Analysis.html">nips-2012-Semiparametric Principal Component Analysis</a></p>
<p>311 <a title="nips-2012-311" href="../nips2012/nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>312 <a title="nips-2012-312" href="../nips2012/nips-2012-Simultaneously_Leveraging_Output_and_Task_Structures_for_Multiple-Output_Regression.html">nips-2012-Simultaneously Leveraging Output and Task Structures for Multiple-Output Regression</a></p>
<p>313 <a title="nips-2012-313" href="../nips2012/nips-2012-Sketch-Based_Linear_Value_Function_Approximation.html">nips-2012-Sketch-Based Linear Value Function Approximation</a></p>
<p>314 <a title="nips-2012-314" href="../nips2012/nips-2012-Slice_Normalized_Dynamic_Markov_Logic_Networks.html">nips-2012-Slice Normalized Dynamic Markov Logic Networks</a></p>
<p>315 <a title="nips-2012-315" href="../nips2012/nips-2012-Slice_sampling_normalized_kernel-weighted_completely_random_measure_mixture_models.html">nips-2012-Slice sampling normalized kernel-weighted completely random measure mixture models</a></p>
<p>316 <a title="nips-2012-316" href="../nips2012/nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>317 <a title="nips-2012-317" href="../nips2012/nips-2012-Smooth-projected_Neighborhood_Pursuit_for_High-dimensional_Nonparanormal_Graph_Estimation.html">nips-2012-Smooth-projected Neighborhood Pursuit for High-dimensional Nonparanormal Graph Estimation</a></p>
<p>318 <a title="nips-2012-318" href="../nips2012/nips-2012-Sparse_Approximate_Manifolds_for_Differential_Geometric_MCMC.html">nips-2012-Sparse Approximate Manifolds for Differential Geometric MCMC</a></p>
<p>319 <a title="nips-2012-319" href="../nips2012/nips-2012-Sparse_Prediction_with_the_%24k%24-Support_Norm.html">nips-2012-Sparse Prediction with the $k$-Support Norm</a></p>
<p>320 <a title="nips-2012-320" href="../nips2012/nips-2012-Spectral_Learning_of_General_Weighted_Automata_via_Constrained_Matrix_Completion.html">nips-2012-Spectral Learning of General Weighted Automata via Constrained Matrix Completion</a></p>
<p>321 <a title="nips-2012-321" href="../nips2012/nips-2012-Spectral_learning_of_linear_dynamics_from_generalised-linear_observations_with_application_to_neural_population_data.html">nips-2012-Spectral learning of linear dynamics from generalised-linear observations with application to neural population data</a></p>
<p>322 <a title="nips-2012-322" href="../nips2012/nips-2012-Spiking_and_saturating_dendrites_differentially_expand_single_neuron_computation_capacity.html">nips-2012-Spiking and saturating dendrites differentially expand single neuron computation capacity</a></p>
<p>323 <a title="nips-2012-323" href="../nips2012/nips-2012-Statistical_Consistency_of_Ranking_Methods_in_A_Rank-Differentiable_Probability_Space.html">nips-2012-Statistical Consistency of Ranking Methods in A Rank-Differentiable Probability Space</a></p>
<p>324 <a title="nips-2012-324" href="../nips2012/nips-2012-Stochastic_Gradient_Descent_with_Only_One_Projection.html">nips-2012-Stochastic Gradient Descent with Only One Projection</a></p>
<p>325 <a title="nips-2012-325" href="../nips2012/nips-2012-Stochastic_optimization_and_sparse_statistical_recovery%3A_Optimal_algorithms_for_high_dimensions.html">nips-2012-Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions</a></p>
<p>326 <a title="nips-2012-326" href="../nips2012/nips-2012-Structure_estimation_for_discrete_graphical_models%3A_Generalized_covariance_matrices_and_their_inverses.html">nips-2012-Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses</a></p>
<p>327 <a title="nips-2012-327" href="../nips2012/nips-2012-Structured_Learning_of_Gaussian_Graphical_Models.html">nips-2012-Structured Learning of Gaussian Graphical Models</a></p>
<p>328 <a title="nips-2012-328" href="../nips2012/nips-2012-Submodular-Bregman_and_the_Lov%C3%A1sz-Bregman_Divergences_with_Applications.html">nips-2012-Submodular-Bregman and the Lovász-Bregman Divergences with Applications</a></p>
<p>329 <a title="nips-2012-329" href="../nips2012/nips-2012-Super-Bit_Locality-Sensitive_Hashing.html">nips-2012-Super-Bit Locality-Sensitive Hashing</a></p>
<p>330 <a title="nips-2012-330" href="../nips2012/nips-2012-Supervised_Learning_with_Similarity_Functions.html">nips-2012-Supervised Learning with Similarity Functions</a></p>
<p>331 <a title="nips-2012-331" href="../nips2012/nips-2012-Symbolic_Dynamic_Programming_for_Continuous_State_and_Observation_POMDPs.html">nips-2012-Symbolic Dynamic Programming for Continuous State and Observation POMDPs</a></p>
<p>332 <a title="nips-2012-332" href="../nips2012/nips-2012-Symmetric_Correspondence_Topic_Models_for_Multilingual_Text_Analysis.html">nips-2012-Symmetric Correspondence Topic Models for Multilingual Text Analysis</a></p>
<p>333 <a title="nips-2012-333" href="../nips2012/nips-2012-Synchronization_can_Control_Regularization_in_Neural_Systems_via_Correlated_Noise_Processes.html">nips-2012-Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes</a></p>
<p>334 <a title="nips-2012-334" href="../nips2012/nips-2012-Tensor_Decomposition_for_Fast_Parsing_with_Latent-Variable_PCFGs.html">nips-2012-Tensor Decomposition for Fast Parsing with Latent-Variable PCFGs</a></p>
<p>335 <a title="nips-2012-335" href="../nips2012/nips-2012-The_Bethe_Partition_Function_of_Log-supermodular_Graphical_Models.html">nips-2012-The Bethe Partition Function of Log-supermodular Graphical Models</a></p>
<p>336 <a title="nips-2012-336" href="../nips2012/nips-2012-The_Coloured_Noise_Expansion_and_Parameter_Estimation_of_Diffusion_Processes.html">nips-2012-The Coloured Noise Expansion and Parameter Estimation of Diffusion Processes</a></p>
<p>337 <a title="nips-2012-337" href="../nips2012/nips-2012-The_Lov%C3%A1sz_%CF%91_function%2C_SVMs_and_finding_large_dense_subgraphs.html">nips-2012-The Lovász ϑ function, SVMs and finding large dense subgraphs</a></p>
<p>338 <a title="nips-2012-338" href="../nips2012/nips-2012-The_Perturbed_Variation.html">nips-2012-The Perturbed Variation</a></p>
<p>339 <a title="nips-2012-339" href="../nips2012/nips-2012-The_Time-Marginalized_Coalescent_Prior_for_Hierarchical_Clustering.html">nips-2012-The Time-Marginalized Coalescent Prior for Hierarchical Clustering</a></p>
<p>340 <a title="nips-2012-340" href="../nips2012/nips-2012-The_representer_theorem_for_Hilbert_spaces%3A_a_necessary_and_sufficient_condition.html">nips-2012-The representer theorem for Hilbert spaces: a necessary and sufficient condition</a></p>
<p>341 <a title="nips-2012-341" href="../nips2012/nips-2012-The_topographic_unsupervised_learning_of_natural_sounds_in_the_auditory_cortex.html">nips-2012-The topographic unsupervised learning of natural sounds in the auditory cortex</a></p>
<p>342 <a title="nips-2012-342" href="../nips2012/nips-2012-The_variational_hierarchical_EM_algorithm_for_clustering_hidden_Markov_models.html">nips-2012-The variational hierarchical EM algorithm for clustering hidden Markov models</a></p>
<p>343 <a title="nips-2012-343" href="../nips2012/nips-2012-Tight_Bounds_on_Profile_Redundancy_and_Distinguishability.html">nips-2012-Tight Bounds on Profile Redundancy and Distinguishability</a></p>
<p>344 <a title="nips-2012-344" href="../nips2012/nips-2012-Timely_Object_Recognition.html">nips-2012-Timely Object Recognition</a></p>
<p>345 <a title="nips-2012-345" href="../nips2012/nips-2012-Topic-Partitioned_Multinetwork_Embeddings.html">nips-2012-Topic-Partitioned Multinetwork Embeddings</a></p>
<p>346 <a title="nips-2012-346" href="../nips2012/nips-2012-Topology_Constraints_in_Graphical_Models.html">nips-2012-Topology Constraints in Graphical Models</a></p>
<p>347 <a title="nips-2012-347" href="../nips2012/nips-2012-Towards_a_learning-theoretic_analysis_of_spike-timing_dependent_plasticity.html">nips-2012-Towards a learning-theoretic analysis of spike-timing dependent plasticity</a></p>
<p>348 <a title="nips-2012-348" href="../nips2012/nips-2012-Tractable_Objectives_for_Robust_Policy_Optimization.html">nips-2012-Tractable Objectives for Robust Policy Optimization</a></p>
<p>349 <a title="nips-2012-349" href="../nips2012/nips-2012-Training_sparse_natural_image_models_with_a_fast_Gibbs_sampler_of_an_extended_state_space.html">nips-2012-Training sparse natural image models with a fast Gibbs sampler of an extended state space</a></p>
<p>350 <a title="nips-2012-350" href="../nips2012/nips-2012-Trajectory-Based_Short-Sighted_Probabilistic_Planning.html">nips-2012-Trajectory-Based Short-Sighted Probabilistic Planning</a></p>
<p>351 <a title="nips-2012-351" href="../nips2012/nips-2012-Transelliptical_Component_Analysis.html">nips-2012-Transelliptical Component Analysis</a></p>
<p>352 <a title="nips-2012-352" href="../nips2012/nips-2012-Transelliptical_Graphical_Models.html">nips-2012-Transelliptical Graphical Models</a></p>
<p>353 <a title="nips-2012-353" href="../nips2012/nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<p>354 <a title="nips-2012-354" href="../nips2012/nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>355 <a title="nips-2012-355" href="../nips2012/nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>356 <a title="nips-2012-356" href="../nips2012/nips-2012-Unsupervised_Structure_Discovery_for_Semantic_Analysis_of_Audio.html">nips-2012-Unsupervised Structure Discovery for Semantic Analysis of Audio</a></p>
<p>357 <a title="nips-2012-357" href="../nips2012/nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>358 <a title="nips-2012-358" href="../nips2012/nips-2012-Value_Pursuit_Iteration.html">nips-2012-Value Pursuit Iteration</a></p>
<p>359 <a title="nips-2012-359" href="../nips2012/nips-2012-Variational_Inference_for_Crowdsourcing.html">nips-2012-Variational Inference for Crowdsourcing</a></p>
<p>360 <a title="nips-2012-360" href="../nips2012/nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>361 <a title="nips-2012-361" href="../nips2012/nips-2012-Volume_Regularization_for_Binary_Classification.html">nips-2012-Volume Regularization for Binary Classification</a></p>
<p>362 <a title="nips-2012-362" href="../nips2012/nips-2012-Waveform_Driven_Plasticity_in_BiFeO3_Memristive_Devices%3A_Model_and_Implementation.html">nips-2012-Waveform Driven Plasticity in BiFeO3 Memristive Devices: Model and Implementation</a></p>
<p>363 <a title="nips-2012-363" href="../nips2012/nips-2012-Wavelet_based_multi-scale_shape_features_on_arbitrary_surfaces_for_cortical_thickness_discrimination.html">nips-2012-Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination</a></p>
<p>364 <a title="nips-2012-364" href="../nips2012/nips-2012-Weighted_Likelihood_Policy_Search_with_Model_Selection.html">nips-2012-Weighted Likelihood Policy Search with Model Selection</a></p>
<p>365 <a title="nips-2012-365" href="../nips2012/nips-2012-Why_MCA%3F_Nonlinear_sparse_coding_with_spike-and-slab_prior_for_neurally_plausible_image_encoding.html">nips-2012-Why MCA? Nonlinear sparse coding with spike-and-slab prior for neurally plausible image encoding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
