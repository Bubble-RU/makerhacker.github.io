<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>nips 2000 knowledge graph</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2000" href="#">nips2000</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>nips 2000 knowledge graph</h1>
<br/><h3>similar papers computed by tfidf model</h3><br/><h3>similar papers computed by <a title="lsi-model" href="./nips2000_lsi.html">lsi model</a></h3><br/><h3>similar papers computed by <a title="lda-model" href="./nips2000_lda.html">lda model</a></h3><br/><h2>papers list:</h2><p>1 <a title="nips-2000-1" href="../nips2000/nips-2000-APRICODD%3A_Approximate_Policy_Construction_Using_Decision_Diagrams.html">nips-2000-APRICODD: Approximate Policy Construction Using Decision Diagrams</a></p>
<p>Author: Robert St-Aubin, Jesse Hoey, Craig Boutilier</p><p>Abstract: We propose a method of approximate dynamic programming for Markov decision processes (MDPs) using algebraic decision diagrams (ADDs). We produce near-optimal value functions and policies with much lower time and space requirements than exact dynamic programming. Our method reduces the sizes of the intermediate value functions generated during value iteration by replacing the values at the terminals of the ADD with ranges of values. Our method is demonstrated on a class of large MDPs (with up to 34 billion states), and we compare the results with the optimal value functions.</p><p>2 <a title="nips-2000-2" href="../nips2000/nips-2000-A_Comparison_of_Image_Processing_Techniques_for_Visual_Speech_Recognition_Applications.html">nips-2000-A Comparison of Image Processing Techniques for Visual Speech Recognition Applications</a></p>
<p>Author: Michael S. Gray, Terrence J. Sejnowski, Javier R. Movellan</p><p>Abstract: We examine eight different techniques for developing visual representations in machine vision tasks. In particular we compare different versions of principal component and independent component analysis in combination with stepwise regression methods for variable selection. We found that local methods, based on the statistics of image patches, consistently outperformed global methods based on the statistics of entire images. This result is consistent with previous work on emotion and facial expression recognition. In addition, the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance. 1</p><p>3 <a title="nips-2000-3" href="../nips2000/nips-2000-A_Gradient-Based_Boosting_Algorithm_for_Regression_Problems.html">nips-2000-A Gradient-Based Boosting Algorithm for Regression Problems</a></p>
<p>Author: Richard S. Zemel, Toniann Pitassi</p><p>Abstract: In adaptive boosting, several weak learners trained sequentially are combined to boost the overall algorithm performance. Recently adaptive boosting methods for classification problems have been derived as gradient descent algorithms. This formulation justifies key elements and parameters in the methods, all chosen to optimize a single common objective function. We propose an analogous formulation for adaptive boosting of regression problems, utilizing a novel objective function that leads to a simple boosting algorithm. We prove that this method reduces training error, and compare its performance to other regression methods. The aim of boosting algorithms is to</p><p>4 <a title="nips-2000-4" href="../nips2000/nips-2000-A_Linear_Programming_Approach_to_Novelty_Detection.html">nips-2000-A Linear Programming Approach to Novelty Detection</a></p>
<p>Author: Colin Campbell, Kristin P. Bennett</p><p>Abstract: Novelty detection involves modeling the normal behaviour of a system hence enabling detection of any divergence from normality. It has potential applications in many areas such as detection of machine damage or highlighting abnormal features in medical data. One approach is to build a hypothesis estimating the support of the normal data i. e. constructing a function which is positive in the region where the data is located and negative elsewhere. Recently kernel methods have been proposed for estimating the support of a distribution and they have performed well in practice - training involves solution of a quadratic programming problem. In this paper we propose a simpler kernel method for estimating the support based on linear programming. The method is easy to implement and can learn large datasets rapidly. We demonstrate the method on medical and fault detection datasets. 1 Introduction. An important classification task is the ability to distinguish b etween new instances similar to m embers of the training set and all other instances that can occur. For example, we may want to learn the normal running behaviour of a machine and highlight any significant divergence from normality which may indicate onset of damage or faults. This issue is a generic problem in many fields. For example, an abnormal event or feature in medical diagnostic data typically leads to further investigation. Novel events can be highlighted by constructing a real-valued density estimation function. However, here we will consider the simpler task of modelling the support of a data distribution i.e. creating a binary-valued function which is positive in those regions of input space where the data predominantly lies and negative elsewhere. Recently kernel methods have been applied to this problem [4]. In this approach data is implicitly mapped to a high-dimensional space called feature space [13]. Suppose the data points in input space are X i (with i = 1, . . . , m) and the mapping is Xi --+ ¢;(Xi) then in the span of {¢;(Xi)}, we can expand a vector w = Lj cr.j¢;(Xj). Hence we can define separating hyperplanes in feature space by w . ¢;(x;) + b = O. We will refer to w . ¢;(Xi) + b as the margin which will be positive on one side of the separating hyperplane and negative on the other. Thus we can also define a decision function: (1) where z is a new data point. The data appears in the form of an inner product in feature space so we can implicitly define feature space by our choice of kernel function: (2) A number of choices for the kernel are possible, for example, RBF kernels: (3) With the given kernel the decision function is therefore given by: (4) One approach to novelty detection is to find a hypersphere in feature space with a minimal radius R and centre a which contains most of the data: novel test points lie outside the boundary of this hypersphere [3 , 12] . This approach to novelty detection was proposed by Tax and Duin [10] and successfully used on real life applications [11] . The effect of outliers is reduced by using slack variables to allow for datapoints outside the sphere and the task is to minimise the volume of the sphere and number of datapoints outside i.e. e i mIll s.t. [R2 + oX L i ei 1 (Xi - a) . (Xi - a) S R2 + e ei i, ~ a (5) Since the data appears in the form of inner products kernel substitution can be applied and the learning task can be reduced to a quadratic programming problem. An alternative approach has been developed by Scholkopf et al. [7]. Suppose we restricted our attention to RBF kernels (3) then the data lies on the surface of a hypersphere in feature space since ¢;(x) . ¢;(x) = K(x , x) = l. The objective is therefore to separate off the surface region constaining data from the region containing no data. This is achieved by constructing a hyperplane which is maximally distant from the origin with all datapoints lying on the opposite side from the origin and such that the margin is positive. The learning task in dual form involves minimisation of: mIll s.t. W(cr.) = t L7,'k=l cr.icr.jK(Xi, Xj) a S cr.i S C, L::1 cr.i = l. (6) However, the origin plays a special role in this model. As the authors point out [9] this is a disadvantage since the origin effectively acts as a prior for where the class of abnormal instances is assumed to lie. In this paper we avoid this problem: rather than repelling the hyperplane away from an arbitrary point outside the data distribution we instead try and attract the hyperplane towards the centre of the data distribution. In this paper we will outline a new algorithm for novelty detection which can be easily implemented using linear programming (LP) techniques. As we illustrate in section 3 it performs well in practice on datasets involving the detection of abnormalities in medical data and fault detection in condition monitoring. 2 The Algorithm For the hard margin case (see Figure 1) the objective is to find a surface in input space which wraps around the data clusters: anything outside this surface is viewed as abnormal. This surface is defined as the level set, J(z) = 0, of some nonlinear function. In feature space, J(z) = L; O'.;K(z, x;) + b, this corresponds to a hyperplane which is pulled onto the mapped datapoints with the restriction that the margin always remains positive or zero. We make the fit of this nonlinear function or hyperplane as tight as possible by minimizing the mean value of the output of the function, i.e., Li J(x;). This is achieved by minimising: (7) subject to: m LO'.jK(x;,Xj) + b 2:: 0 (8) j=l m L 0'.; = 1, 0'.; 2:: 0 (9) ;=1 The bias b is just treated as an additional parameter in the minimisation process though unrestricted in sign. The added constraints (9) on 0'. bound the class of models to be considered - we don't want to consider simple linear rescalings of the model. These constraints amount to a choice of scale for the weight vector normal to the hyperplane in feature space and hence do not impose a restriction on the model. Also, these constraints ensure that the problem is well-posed and that an optimal solution with 0'. i- 0 exists. Other constraints on the class of functions are possible, e.g. 110'.111 = 1 with no restriction on the sign of O'.i. Many real-life datasets contain noise and outliers. To handle these we can introduce a soft margin in analogy to the usual approach used with support vector machines. In this case we minimise: (10) subject to: m LO:jJ{(Xi , Xj)+b~-ei' ei~O (11) j=l and constraints (9). The parameter). controls the extent of margin errors (larger ). means fewer outliers are ignored: ). -+ 00 corresponds to the hard margin limit). The above problem can be easily solved for problems with thousands of points using standard simplex or interior point algorithms for linear programming. With the addition of column generation techniques, these same approaches can be adopted for very large problems in which the kernel matrix exceeds the capacity of main memory. Column generation algorithms incrementally add and drop columns each corresponding to a single kernel function until optimality is reached. Such approaches have been successfully applied to other support vector problems [6 , 2]. Basic simplex algorithms were sufficient for the problems considered in this paper, so we defer a listing of the code for column generation to a later paper together with experiments on large datasets [1]. 3 Experiments Artificial datasets. Before considering experiments on real-life data we will first illustrate the performance of the algorithm on some artificial datasets. In Figure 1 the algorithm places a boundary around two data clusters in input space: a hard margin was used with RBF kernels and (J</p><p>5 <a title="nips-2000-5" href="../nips2000/nips-2000-A_Mathematical_Programming_Approach_to_the_Kernel_Fisher_Algorithm.html">nips-2000-A Mathematical Programming Approach to the Kernel Fisher Algorithm</a></p>
<p>Author: Sebastian Mika, Gunnar R채tsch, Klaus-Robert M체ller</p><p>Abstract: We investigate a new kernel-based classifier: the Kernel Fisher Discriminant (KFD). A mathematical programming formulation based on the observation that KFD maximizes the average margin permits an interesting modification of the original KFD algorithm yielding the sparse KFD. We find that both, KFD and the proposed sparse KFD, can be understood in an unifying probabilistic context. Furthermore, we show connections to Support Vector Machines and Relevance Vector Machines. From this understanding, we are able to outline an interesting kernel-regression technique based upon the KFD algorithm. Simulations support the usefulness of our approach.</p><p>6 <a title="nips-2000-6" href="../nips2000/nips-2000-A_Neural_Probabilistic_Language_Model.html">nips-2000-A Neural Probabilistic Language Model</a></p>
<p>Author: Yoshua Bengio, Réjean Ducharme, Pascal Vincent</p><p>Abstract: A goal of statistical language modeling is to learn the joint probability function of sequences of words. This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons. In the proposed approach one learns simultaneously (1) a distributed representation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model.</p><p>7 <a title="nips-2000-7" href="../nips2000/nips-2000-A_New_Approximate_Maximal_Margin_Classification_Algorithm.html">nips-2000-A New Approximate Maximal Margin Classification Algorithm</a></p>
<p>Author: Claudio Gentile</p><p>Abstract: A new incremental learning algorithm is described which approximates the maximal margin hyperplane w.r.t. norm p ~ 2 for a set of linearly separable data. Our algorithm, called ALMAp (Approximate Large Margin algorithm w.r.t. norm p), takes 0 ((P~21;;2) corrections to separate the data with p-norm margin larger than (1 - 0:) ,,(, where,,( is the p-norm margin of the data and X is a bound on the p-norm of the instances. ALMAp avoids quadratic (or higher-order) programming methods. It is very easy to implement and is as fast as on-line algorithms, such as Rosenblatt's perceptron. We report on some experiments comparing ALMAp to two incremental algorithms: Perceptron and Li and Long's ROMMA. Our algorithm seems to perform quite better than both. The accuracy levels achieved by ALMAp are slightly inferior to those obtained by Support vector Machines (SVMs). On the other hand, ALMAp is quite faster and easier to implement than standard SVMs training algorithms.</p><p>8 <a title="nips-2000-8" href="../nips2000/nips-2000-A_New_Model_of_Spatial_Representation_in_Multimodal_Brain_Areas.html">nips-2000-A New Model of Spatial Representation in Multimodal Brain Areas</a></p>
<p>Author: Sophie Denève, Jean-René Duhamel, Alexandre Pouget</p><p>Abstract: Most models of spatial representations in the cortex assume cells with limited receptive fields that are defined in a particular egocentric frame of reference. However, cells outside of primary sensory cortex are either gain modulated by postural input or partially shifting. We show that solving classical spatial tasks, like sensory prediction, multi-sensory integration, sensory-motor transformation and motor control requires more complicated intermediate representations that are not invariant in one frame of reference. We present an iterative basis function map that performs these spatial tasks optimally with gain modulated and partially shifting units, and tests it against neurophysiological and neuropsychological data. In order to perform an action directed toward an object, it is necessary to have a representation of its spatial location. The brain must be able to use spatial cues coming from different modalities (e.g. vision, audition, touch, proprioception), combine them to infer the position of the object, and compute the appropriate movement. These cues are in different frames of reference corresponding to different sensory or motor modalities. Visual inputs are primarily encoded in retinotopic maps, auditory inputs are encoded in head centered maps and tactile cues are encoded in skin-centered maps. Going from one frame of reference to the other might seem easy. For example, the head-centered position of an object can be approximated by the sum of its retinotopic position and the eye position. However, positions are represented by population codes in the brain, and computing a head-centered map from a retinotopic map is a more complex computation than the underlying sum. Moreover, as we get closer to sensory-motor areas it seems reasonable to assume Spksls 150 100 50 o Figure 1: Response of a VIP cell to visual stimuli appearing in different part of the screen, for three different eye positions. The level of grey represent the frequency of discharge (In spikes per seconds). The white cross is the fixation point (the head is fixed). The cell's receptive field is moving with the eyes, but only partially. Here the receptive field shift is 60% of the total gaze shift. Moreover this cell is gain modulated by eye position (adapted from Duhamel et al). that the representations should be useful for sensory-motor transformations, rather than encode an</p><p>9 <a title="nips-2000-9" href="../nips2000/nips-2000-A_PAC-Bayesian_Margin_Bound_for_Linear_Classifiers%3A_Why_SVMs_work.html">nips-2000-A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work</a></p>
<p>Author: Ralf Herbrich, Thore Graepel</p><p>Abstract: We present a bound on the generalisation error of linear classifiers in terms of a refined margin quantity on the training set. The result is obtained in a PAC- Bayesian framework and is based on geometrical arguments in the space of linear classifiers. The new bound constitutes an exponential improvement of the so far tightest margin bound by Shawe-Taylor et al. [8] and scales logarithmically in the inverse margin. Even in the case of less training examples than input dimensions sufficiently large margins lead to non-trivial bound values and - for maximum margins - to a vanishing complexity term. Furthermore, the classical margin is too coarse a measure for the essential quantity that controls the generalisation error: the volume ratio between the whole hypothesis space and the subset of consistent hypotheses. The practical relevance of the result lies in the fact that the well-known support vector machine is optimal w.r.t. the new bound only if the feature vectors are all of the same length. As a consequence we recommend to use SVMs on normalised feature vectors only - a recommendation that is well supported by our numerical experiments on two benchmark data sets. 1</p><p>10 <a title="nips-2000-10" href="../nips2000/nips-2000-A_Productive%2C_Systematic_Framework_for_the_Representation_of_Visual_Structure.html">nips-2000-A Productive, Systematic Framework for the Representation of Visual Structure</a></p>
<p>Author: Shimon Edelman, Nathan Intrator</p><p>Abstract: We describe a unified framework for the understanding of structure representation in primate vision. A model derived from this framework is shown to be effectively systematic in that it has the ability to interpret and associate together objects that are related through a rearrangement of common</p><p>11 <a title="nips-2000-11" href="../nips2000/nips-2000-A_Silicon_Primitive_for_Competitive_Learning.html">nips-2000-A Silicon Primitive for Competitive Learning</a></p>
<p>Author: David Hsu, Miguel Figueroa, Chris Diorio</p><p>Abstract: Competitive learning is a technique for training classification and clustering networks. We have designed and fabricated an 11transistor primitive, that we term an automaximizing bump circuit, that implements competitive learning dynamics. The circuit performs a similarity computation, affords nonvolatile storage, and implements simultaneous local adaptation and computation. We show that our primitive is suitable for implementing competitive learning in VLSI, and demonstrate its effectiveness in a standard clustering task.</p><p>12 <a title="nips-2000-12" href="../nips2000/nips-2000-A_Support_Vector_Method_for_Clustering.html">nips-2000-A Support Vector Method for Clustering</a></p>
<p>Author: Asa Ben-Hur, David Horn, Hava T. Siegelmann, Vladimir Vapnik</p><p>Abstract: We present a novel method for clustering using the support vector machine approach. Data points are mapped to a high dimensional feature space, where support vectors are used to define a sphere enclosing them. The boundary of the sphere forms in data space a set of closed contours containing the data. Data points enclosed by each contour are defined as a cluster. As the width parameter of the Gaussian kernel is decreased, these contours fit the data more tightly and splitting of contours occurs. The algorithm works by separating clusters according to valleys in the underlying probability distribution, and thus clusters can take on arbitrary geometrical shapes. As in other SV algorithms, outliers can be dealt with by introducing a soft margin constant leading to smoother cluster boundaries. The structure of the data is explored by varying the two parameters. We investigate the dependence of our method on these parameters and apply it to several data sets.</p><p>13 <a title="nips-2000-13" href="../nips2000/nips-2000-A_Tighter_Bound_for_Graphical_Models.html">nips-2000-A Tighter Bound for Graphical Models</a></p>
<p>Author: Martijn A. R. Leisink, Hilbert J. Kappen</p><p>Abstract: We present a method to bound the partition function of a Boltzmann machine neural network with any odd order polynomial. This is a direct extension of the mean field bound, which is first order. We show that the third order bound is strictly better than mean field. Additionally we show the rough outline how this bound is applicable to sigmoid belief networks. Numerical experiments indicate that an error reduction of a factor two is easily reached in the region where expansion based approximations are useful. 1</p><p>14 <a title="nips-2000-14" href="../nips2000/nips-2000-A_Variational_Mean-Field_Theory_for_Sigmoidal_Belief_Networks.html">nips-2000-A Variational Mean-Field Theory for Sigmoidal Belief Networks</a></p>
<p>Author: Chiranjib Bhattacharyya, S. Sathiya Keerthi</p><p>Abstract: A variational derivation of Plefka's mean-field theory is presented. This theory is then applied to sigmoidal belief networks with the aid of further approximations. Empirical evaluation on small scale networks show that the proposed approximations are quite competitive. 1</p><p>15 <a title="nips-2000-15" href="../nips2000/nips-2000-Accumulator_Networks%3A_Suitors_of_Local_Probability_Propagation.html">nips-2000-Accumulator Networks: Suitors of Local Probability Propagation</a></p>
<p>Author: Brendan J. Frey, Anitha Kannan</p><p>Abstract: One way to approximate inference in richly-connected graphical models is to apply the sum-product algorithm (a.k.a. probability propagation algorithm), while ignoring the fact that the graph has cycles. The sum-product algorithm can be directly applied in Gaussian networks and in graphs for coding, but for many conditional probability functions - including the sigmoid function - direct application of the sum-product algorithm is not possible. We introduce</p><p>16 <a title="nips-2000-16" href="../nips2000/nips-2000-Active_Inference_in_Concept_Learning.html">nips-2000-Active Inference in Concept Learning</a></p>
<p>Author: Jonathan D. Nelson, Javier R. Movellan</p><p>Abstract: People are active experimenters, not just passive observers, constantly seeking new information relevant to their goals. A reasonable approach to active information gathering is to ask questions and conduct experiments that maximize the expected information gain, given current beliefs (Fedorov 1972, MacKay 1992, Oaksford & Chater 1994). In this paper we present results on an exploratory experiment designed to study people's active information gathering behavior on a concept learning task (Tenenbaum 2000). The results of the experiment are analyzed in terms of the expected information gain of the questions asked by subjects. In scientific inquiry and in everyday life, people seek out information relevant to perceptual and cognitive tasks. Scientists perform experiments to uncover causal relationships; people saccade to informative areas of visual scenes, turn their head towards surprising sounds, and ask questions to understand the meaning of concepts . Consider a person learning a foreign language, who notices that a particular word,</p><p>17 <a title="nips-2000-17" href="../nips2000/nips-2000-Active_Learning_for_Parameter_Estimation_in_Bayesian_Networks.html">nips-2000-Active Learning for Parameter Estimation in Bayesian Networks</a></p>
<p>Author: Simon Tong, Daphne Koller</p><p>Abstract: Bayesian networks are graphical representations of probability distributions. In virtually all of the work on learning these networks, the assumption is that we are presented with a data set consisting of randomly generated instances from the underlying distribution. In many situations, however, we also have the option of active learning, where we have the possibility of guiding the sampling process by querying for certain types of samples. This paper addresses the problem of estimating the parameters of Bayesian networks in an active learning setting. We provide a theoretical framework for this problem, and an algorithm that chooses which active learning queries to generate based on the model learned so far. We present experimental results showing that our active learning algorithm can significantly reduce the need for training data in many situations.</p><p>18 <a title="nips-2000-18" href="../nips2000/nips-2000-Active_Support_Vector_Machine_Classification.html">nips-2000-Active Support Vector Machine Classification</a></p>
<p>Author: Olvi L. Mangasarian, David R. Musicant</p><p>Abstract: An active set strategy is applied to the dual of a simple reformulation of the standard quadratic program of a linear support vector machine. This application generates a fast new dual algorithm that consists of solving a finite number of linear equations, with a typically large dimensionality equal to the number of points to be classified. However, by making novel use of the Sherman-MorrisonWoodbury formula , a much smaller matrix of the order of the original input space is inverted at each step. Thus, a problem with a 32-dimensional input space and 7 million points required inverting positive definite symmetric matrices of size 33 x 33 with a total running time of 96 minutes on a 400 MHz Pentium II. The algorithm requires no specialized quadratic or linear programming code, but merely a linear equation solver which is publicly available. 1</p><p>19 <a title="nips-2000-19" href="../nips2000/nips-2000-Adaptive_Object_Representation_with_Hierarchically-Distributed_Memory_Sites.html">nips-2000-Adaptive Object Representation with Hierarchically-Distributed Memory Sites</a></p>
<p>Author: Bosco S. Tjan</p><p>Abstract: Theories of object recognition often assume that only one representation scheme is used within one visual-processing pathway. Versatility of the visual system comes from having multiple visual-processing pathways, each specialized in a different category of objects. We propose a theoretically simpler alternative, capable of explaining the same set of data and more. A single primary visual-processing pathway, loosely modular, is assumed. Memory modules are attached to sites along this pathway. Object-identity decision is made independently at each site. A site's response time is a monotonic-decreasing function of its confidence regarding its decision. An observer's response is the first-arriving response from any site. The effective representation(s) of such a system, determined empirically, can appear to be specialized for different tasks and stimuli, consistent with recent clinical and functional-imaging findings. This, however, merely reflects a decision being made at its appropriate level of abstraction. The system itself is intrinsically flexible and adaptive.</p><p>20 <a title="nips-2000-20" href="../nips2000/nips-2000-Algebraic_Information_Geometry_for_Learning_Machines_with_Singularities.html">nips-2000-Algebraic Information Geometry for Learning Machines with Singularities</a></p>
<p>Author: Sumio Watanabe</p><p>Abstract: Algebraic geometry is essential to learning theory. In hierarchical learning machines such as layered neural networks and gaussian mixtures, the asymptotic normality does not hold , since Fisher information matrices are singular. In this paper , the rigorous asymptotic form of the stochastic complexity is clarified based on resolution of singularities and two different problems are studied. (1) If the prior is positive, then the stochastic complexity is far smaller than BIO, resulting in the smaller generalization error than regular statistical models, even when the true distribution is not contained in the parametric model. (2) If Jeffreys' prior, which is coordinate free and equal to zero at singularities, is employed then the stochastic complexity has the same form as BIO. It is useful for model selection, but not for generalization. 1</p><p>21 <a title="nips-2000-21" href="../nips2000/nips-2000-Algorithmic_Stability_and_Generalization_Performance.html">nips-2000-Algorithmic Stability and Generalization Performance</a></p>
<p>22 <a title="nips-2000-22" href="../nips2000/nips-2000-Algorithms_for_Non-negative_Matrix_Factorization.html">nips-2000-Algorithms for Non-negative Matrix Factorization</a></p>
<p>23 <a title="nips-2000-23" href="../nips2000/nips-2000-An_Adaptive_Metric_Machine_for_Pattern_Classification.html">nips-2000-An Adaptive Metric Machine for Pattern Classification</a></p>
<p>24 <a title="nips-2000-24" href="../nips2000/nips-2000-An_Information_Maximization_Approach_to_Overcomplete_and_Recurrent_Representations.html">nips-2000-An Information Maximization Approach to Overcomplete and Recurrent Representations</a></p>
<p>25 <a title="nips-2000-25" href="../nips2000/nips-2000-Analysis_of_Bit_Error_Probability_of_Direct-Sequence_CDMA_Multiuser_Demodulators.html">nips-2000-Analysis of Bit Error Probability of Direct-Sequence CDMA Multiuser Demodulators</a></p>
<p>26 <a title="nips-2000-26" href="../nips2000/nips-2000-Automated_State_Abstraction_for_Options_using_the_U-Tree_Algorithm.html">nips-2000-Automated State Abstraction for Options using the U-Tree Algorithm</a></p>
<p>27 <a title="nips-2000-27" href="../nips2000/nips-2000-Automatic_Choice_of_Dimensionality_for_PCA.html">nips-2000-Automatic Choice of Dimensionality for PCA</a></p>
<p>28 <a title="nips-2000-28" href="../nips2000/nips-2000-Balancing_Multiple_Sources_of_Reward_in_Reinforcement_Learning.html">nips-2000-Balancing Multiple Sources of Reward in Reinforcement Learning</a></p>
<p>29 <a title="nips-2000-29" href="../nips2000/nips-2000-Bayes_Networks_on_Ice%3A_Robotic_Search_for_Antarctic_Meteorites.html">nips-2000-Bayes Networks on Ice: Robotic Search for Antarctic Meteorites</a></p>
<p>30 <a title="nips-2000-30" href="../nips2000/nips-2000-Bayesian_Video_Shot_Segmentation.html">nips-2000-Bayesian Video Shot Segmentation</a></p>
<p>31 <a title="nips-2000-31" href="../nips2000/nips-2000-Beyond_Maximum_Likelihood_and_Density_Estimation%3A_A_Sample-Based_Criterion_for_Unsupervised_Learning_of_Complex_Models.html">nips-2000-Beyond Maximum Likelihood and Density Estimation: A Sample-Based Criterion for Unsupervised Learning of Complex Models</a></p>
<p>32 <a title="nips-2000-32" href="../nips2000/nips-2000-Color_Opponency_Constitutes_a_Sparse_Representation_for_the_Chromatic_Structure_of_Natural_Scenes.html">nips-2000-Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes</a></p>
<p>33 <a title="nips-2000-33" href="../nips2000/nips-2000-Combining_ICA_and_Top-Down_Attention_for_Robust_Speech_Recognition.html">nips-2000-Combining ICA and Top-Down Attention for Robust Speech Recognition</a></p>
<p>34 <a title="nips-2000-34" href="../nips2000/nips-2000-Competition_and_Arbors_in_Ocular_Dominance.html">nips-2000-Competition and Arbors in Ocular Dominance</a></p>
<p>35 <a title="nips-2000-35" href="../nips2000/nips-2000-Computing_with_Finite_and_Infinite_Networks.html">nips-2000-Computing with Finite and Infinite Networks</a></p>
<p>36 <a title="nips-2000-36" href="../nips2000/nips-2000-Constrained_Independent_Component_Analysis.html">nips-2000-Constrained Independent Component Analysis</a></p>
<p>37 <a title="nips-2000-37" href="../nips2000/nips-2000-Convergence_of_Large_Margin_Separable_Linear_Classification.html">nips-2000-Convergence of Large Margin Separable Linear Classification</a></p>
<p>38 <a title="nips-2000-38" href="../nips2000/nips-2000-Data_Clustering_by_Markovian_Relaxation_and_the_Information_Bottleneck_Method.html">nips-2000-Data Clustering by Markovian Relaxation and the Information Bottleneck Method</a></p>
<p>39 <a title="nips-2000-39" href="../nips2000/nips-2000-Decomposition_of_Reinforcement_Learning_for_Admission_Control_of_Self-Similar_Call_Arrival_Processes.html">nips-2000-Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes</a></p>
<p>40 <a title="nips-2000-40" href="../nips2000/nips-2000-Dendritic_Compartmentalization_Could_Underlie_Competition_and_Attentional_Biasing_of_Simultaneous_Visual_Stimuli.html">nips-2000-Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli</a></p>
<p>41 <a title="nips-2000-41" href="../nips2000/nips-2000-Discovering_Hidden_Variables%3A_A_Structure-Based_Approach.html">nips-2000-Discovering Hidden Variables: A Structure-Based Approach</a></p>
<p>42 <a title="nips-2000-42" href="../nips2000/nips-2000-Divisive_and_Subtractive_Mask_Effects%3A_Linking_Psychophysics_and_Biophysics.html">nips-2000-Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics</a></p>
<p>43 <a title="nips-2000-43" href="../nips2000/nips-2000-Dopamine_Bonuses.html">nips-2000-Dopamine Bonuses</a></p>
<p>44 <a title="nips-2000-44" href="../nips2000/nips-2000-Efficient_Learning_of_Linear_Perceptrons.html">nips-2000-Efficient Learning of Linear Perceptrons</a></p>
<p>45 <a title="nips-2000-45" href="../nips2000/nips-2000-Emergence_of_Movement_Sensitive_Neurons%27_Properties_by_Learning_a_Sparse_Code_for_Natural_Moving_Images.html">nips-2000-Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images</a></p>
<p>46 <a title="nips-2000-46" href="../nips2000/nips-2000-Ensemble_Learning_and_Linear_Response_Theory_for_ICA.html">nips-2000-Ensemble Learning and Linear Response Theory for ICA</a></p>
<p>47 <a title="nips-2000-47" href="../nips2000/nips-2000-Error-correcting_Codes_on_a_Bethe-like_Lattice.html">nips-2000-Error-correcting Codes on a Bethe-like Lattice</a></p>
<p>48 <a title="nips-2000-48" href="../nips2000/nips-2000-Exact_Solutions_to_Time-Dependent_MDPs.html">nips-2000-Exact Solutions to Time-Dependent MDPs</a></p>
<p>49 <a title="nips-2000-49" href="../nips2000/nips-2000-Explaining_Away_in_Weight_Space.html">nips-2000-Explaining Away in Weight Space</a></p>
<p>50 <a title="nips-2000-50" href="../nips2000/nips-2000-FaceSync%3A_A_Linear_Operator_for_Measuring_Synchronization_of_Video_Facial_Images_and_Audio_Tracks.html">nips-2000-FaceSync: A Linear Operator for Measuring Synchronization of Video Facial Images and Audio Tracks</a></p>
<p>51 <a title="nips-2000-51" href="../nips2000/nips-2000-Factored_Semi-Tied_Covariance_Matrices.html">nips-2000-Factored Semi-Tied Covariance Matrices</a></p>
<p>52 <a title="nips-2000-52" href="../nips2000/nips-2000-Fast_Training_of_Support_Vector_Classifiers.html">nips-2000-Fast Training of Support Vector Classifiers</a></p>
<p>53 <a title="nips-2000-53" href="../nips2000/nips-2000-Feature_Correspondence%3A_A_Markov_Chain_Monte_Carlo_Approach.html">nips-2000-Feature Correspondence: A Markov Chain Monte Carlo Approach</a></p>
<p>54 <a title="nips-2000-54" href="../nips2000/nips-2000-Feature_Selection_for_SVMs.html">nips-2000-Feature Selection for SVMs</a></p>
<p>55 <a title="nips-2000-55" href="../nips2000/nips-2000-Finding_the_Key_to_a_Synapse.html">nips-2000-Finding the Key to a Synapse</a></p>
<p>56 <a title="nips-2000-56" href="../nips2000/nips-2000-Foundations_for_a_Circuit_Complexity_Theory_of_Sensory_Processing.html">nips-2000-Foundations for a Circuit Complexity Theory of Sensory Processing</a></p>
<p>57 <a title="nips-2000-57" href="../nips2000/nips-2000-Four-legged_Walking_Gait_Control_Using_a_Neuromorphic_Chip_Interfaced_to_a_Support_Vector_Learning_Algorithm.html">nips-2000-Four-legged Walking Gait Control Using a Neuromorphic Chip Interfaced to a Support Vector Learning Algorithm</a></p>
<p>58 <a title="nips-2000-58" href="../nips2000/nips-2000-From_Margin_to_Sparsity.html">nips-2000-From Margin to Sparsity</a></p>
<p>59 <a title="nips-2000-59" href="../nips2000/nips-2000-From_Mixtures_of_Mixtures_to_Adaptive_Transform_Coding.html">nips-2000-From Mixtures of Mixtures to Adaptive Transform Coding</a></p>
<p>60 <a title="nips-2000-60" href="../nips2000/nips-2000-Gaussianization.html">nips-2000-Gaussianization</a></p>
<p>61 <a title="nips-2000-61" href="../nips2000/nips-2000-Generalizable_Singular_Value_Decomposition_for_Ill-posed_Datasets.html">nips-2000-Generalizable Singular Value Decomposition for Ill-posed Datasets</a></p>
<p>62 <a title="nips-2000-62" href="../nips2000/nips-2000-Generalized_Belief_Propagation.html">nips-2000-Generalized Belief Propagation</a></p>
<p>63 <a title="nips-2000-63" href="../nips2000/nips-2000-Hierarchical_Memory-Based_Reinforcement_Learning.html">nips-2000-Hierarchical Memory-Based Reinforcement Learning</a></p>
<p>64 <a title="nips-2000-64" href="../nips2000/nips-2000-High-temperature_Expansions_for_Learning_Models_of_Nonnegative_Data.html">nips-2000-High-temperature Expansions for Learning Models of Nonnegative Data</a></p>
<p>65 <a title="nips-2000-65" href="../nips2000/nips-2000-Higher-Order_Statistical_Properties_Arising_from_the_Non-Stationarity_of_Natural_Signals.html">nips-2000-Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals</a></p>
<p>66 <a title="nips-2000-66" href="../nips2000/nips-2000-Hippocampally-Dependent_Consolidation_in_a_Hierarchical_Model_of_Neocortex.html">nips-2000-Hippocampally-Dependent Consolidation in a Hierarchical Model of Neocortex</a></p>
<p>67 <a title="nips-2000-67" href="../nips2000/nips-2000-Homeostasis_in_a_Silicon_Integrate_and_Fire_Neuron.html">nips-2000-Homeostasis in a Silicon Integrate and Fire Neuron</a></p>
<p>68 <a title="nips-2000-68" href="../nips2000/nips-2000-Improved_Output_Coding_for_Classification_Using_Continuous_Relaxation.html">nips-2000-Improved Output Coding for Classification Using Continuous Relaxation</a></p>
<p>69 <a title="nips-2000-69" href="../nips2000/nips-2000-Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing.html">nips-2000-Incorporating Second-Order Functional Knowledge for Better Option Pricing</a></p>
<p>70 <a title="nips-2000-70" href="../nips2000/nips-2000-Incremental_and_Decremental_Support_Vector_Machine_Learning.html">nips-2000-Incremental and Decremental Support Vector Machine Learning</a></p>
<p>71 <a title="nips-2000-71" href="../nips2000/nips-2000-Interactive_Parts_Model%3A_An_Application_to_Recognition_of_On-line_Cursive_Script.html">nips-2000-Interactive Parts Model: An Application to Recognition of On-line Cursive Script</a></p>
<p>72 <a title="nips-2000-72" href="../nips2000/nips-2000-Keeping_Flexible_Active_Contours_on_Track_using_Metropolis_Updates.html">nips-2000-Keeping Flexible Active Contours on Track using Metropolis Updates</a></p>
<p>73 <a title="nips-2000-73" href="../nips2000/nips-2000-Kernel-Based_Reinforcement_Learning_in_Average-Cost_Problems%3A_An_Application_to_Optimal_Portfolio_Choice.html">nips-2000-Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice</a></p>
<p>74 <a title="nips-2000-74" href="../nips2000/nips-2000-Kernel_Expansions_with_Unlabeled_Examples.html">nips-2000-Kernel Expansions with Unlabeled Examples</a></p>
<p>75 <a title="nips-2000-75" href="../nips2000/nips-2000-Large_Scale_Bayes_Point_Machines.html">nips-2000-Large Scale Bayes Point Machines</a></p>
<p>76 <a title="nips-2000-76" href="../nips2000/nips-2000-Learning_Continuous_Distributions%3A_Simulations_With_Field_Theoretic_Priors.html">nips-2000-Learning Continuous Distributions: Simulations With Field Theoretic Priors</a></p>
<p>77 <a title="nips-2000-77" href="../nips2000/nips-2000-Learning_Curves_for_Gaussian_Processes_Regression%3A_A_Framework_for_Good_Approximations.html">nips-2000-Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations</a></p>
<p>78 <a title="nips-2000-78" href="../nips2000/nips-2000-Learning_Joint_Statistical_Models_for_Audio-Visual_Fusion_and_Segregation.html">nips-2000-Learning Joint Statistical Models for Audio-Visual Fusion and Segregation</a></p>
<p>79 <a title="nips-2000-79" href="../nips2000/nips-2000-Learning_Segmentation_by_Random_Walks.html">nips-2000-Learning Segmentation by Random Walks</a></p>
<p>80 <a title="nips-2000-80" href="../nips2000/nips-2000-Learning_Switching_Linear_Models_of_Human_Motion.html">nips-2000-Learning Switching Linear Models of Human Motion</a></p>
<p>81 <a title="nips-2000-81" href="../nips2000/nips-2000-Learning_Winner-take-all_Competition_Between_Groups_of_Neurons_in_Lateral_Inhibitory_Networks.html">nips-2000-Learning Winner-take-all Competition Between Groups of Neurons in Lateral Inhibitory Networks</a></p>
<p>82 <a title="nips-2000-82" href="../nips2000/nips-2000-Learning_and_Tracking_Cyclic_Human_Motion.html">nips-2000-Learning and Tracking Cyclic Human Motion</a></p>
<p>83 <a title="nips-2000-83" href="../nips2000/nips-2000-Machine_Learning_for_Video-Based_Rendering.html">nips-2000-Machine Learning for Video-Based Rendering</a></p>
<p>84 <a title="nips-2000-84" href="../nips2000/nips-2000-Minimum_Bayes_Error_Feature_Selection_for_Continuous_Speech_Recognition.html">nips-2000-Minimum Bayes Error Feature Selection for Continuous Speech Recognition</a></p>
<p>85 <a title="nips-2000-85" href="../nips2000/nips-2000-Mixtures_of_Gaussian_Processes.html">nips-2000-Mixtures of Gaussian Processes</a></p>
<p>86 <a title="nips-2000-86" href="../nips2000/nips-2000-Model_Complexity%2C_Goodness_of_Fit_and_Diminishing_Returns.html">nips-2000-Model Complexity, Goodness of Fit and Diminishing Returns</a></p>
<p>87 <a title="nips-2000-87" href="../nips2000/nips-2000-Modelling_Spatial_Recall%2C_Mental_Imagery_and_Neglect.html">nips-2000-Modelling Spatial Recall, Mental Imagery and Neglect</a></p>
<p>88 <a title="nips-2000-88" href="../nips2000/nips-2000-Multiple_Timescales_of_Adaptation_in_a_Neural_Code.html">nips-2000-Multiple Timescales of Adaptation in a Neural Code</a></p>
<p>89 <a title="nips-2000-89" href="../nips2000/nips-2000-Natural_Sound_Statistics_and_Divisive_Normalization_in_the_Auditory_System.html">nips-2000-Natural Sound Statistics and Divisive Normalization in the Auditory System</a></p>
<p>90 <a title="nips-2000-90" href="../nips2000/nips-2000-New_Approaches_Towards_Robust_and_Adaptive_Speech_Recognition.html">nips-2000-New Approaches Towards Robust and Adaptive Speech Recognition</a></p>
<p>91 <a title="nips-2000-91" href="../nips2000/nips-2000-Noise_Suppression_Based_on_Neurophysiologically-motivated_SNR_Estimation_for_Robust_Speech_Recognition.html">nips-2000-Noise Suppression Based on Neurophysiologically-motivated SNR Estimation for Robust Speech Recognition</a></p>
<p>92 <a title="nips-2000-92" href="../nips2000/nips-2000-Occam%27s_Razor.html">nips-2000-Occam's Razor</a></p>
<p>93 <a title="nips-2000-93" href="../nips2000/nips-2000-On_Iterative_Krylov-Dogleg_Trust-Region_Steps_for_Solving_Neural_Networks_Nonlinear_Least_Squares_Problems.html">nips-2000-On Iterative Krylov-Dogleg Trust-Region Steps for Solving Neural Networks Nonlinear Least Squares Problems</a></p>
<p>94 <a title="nips-2000-94" href="../nips2000/nips-2000-On_Reversing_Jensen%27s_Inequality.html">nips-2000-On Reversing Jensen's Inequality</a></p>
<p>95 <a title="nips-2000-95" href="../nips2000/nips-2000-On_a_Connection_between_Kernel_PCA_and_Metric_Multidimensional_Scaling.html">nips-2000-On a Connection between Kernel PCA and Metric Multidimensional Scaling</a></p>
<p>96 <a title="nips-2000-96" href="../nips2000/nips-2000-One_Microphone_Source_Separation.html">nips-2000-One Microphone Source Separation</a></p>
<p>97 <a title="nips-2000-97" href="../nips2000/nips-2000-Overfitting_in_Neural_Nets%3A_Backpropagation%2C_Conjugate_Gradient%2C_and_Early_Stopping.html">nips-2000-Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping</a></p>
<p>98 <a title="nips-2000-98" href="../nips2000/nips-2000-Partially_Observable_SDE_Models_for_Image_Sequence_Recognition_Tasks.html">nips-2000-Partially Observable SDE Models for Image Sequence Recognition Tasks</a></p>
<p>99 <a title="nips-2000-99" href="../nips2000/nips-2000-Periodic_Component_Analysis%3A_An_Eigenvalue_Method_for_Representing_Periodic_Structure_in_Speech.html">nips-2000-Periodic Component Analysis: An Eigenvalue Method for Representing Periodic Structure in Speech</a></p>
<p>100 <a title="nips-2000-100" href="../nips2000/nips-2000-Permitted_and_Forbidden_Sets_in_Symmetric_Threshold-Linear_Networks.html">nips-2000-Permitted and Forbidden Sets in Symmetric Threshold-Linear Networks</a></p>
<p>101 <a title="nips-2000-101" href="../nips2000/nips-2000-Place_Cells_and_Spatial_Navigation_Based_on_2D_Visual_Feature_Extraction%2C_Path_Integration%2C_and_Reinforcement_Learning.html">nips-2000-Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning</a></p>
<p>102 <a title="nips-2000-102" href="../nips2000/nips-2000-Position_Variance%2C_Recurrence_and_Perceptual_Learning.html">nips-2000-Position Variance, Recurrence and Perceptual Learning</a></p>
<p>103 <a title="nips-2000-103" href="../nips2000/nips-2000-Probabilistic_Semantic_Video_Indexing.html">nips-2000-Probabilistic Semantic Video Indexing</a></p>
<p>104 <a title="nips-2000-104" href="../nips2000/nips-2000-Processing_of_Time_Series_by_Neural_Circuits_with_Biologically_Realistic_Synaptic_Dynamics.html">nips-2000-Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics</a></p>
<p>105 <a title="nips-2000-105" href="../nips2000/nips-2000-Programmable_Reinforcement_Learning_Agents.html">nips-2000-Programmable Reinforcement Learning Agents</a></p>
<p>106 <a title="nips-2000-106" href="../nips2000/nips-2000-Propagation_Algorithms_for_Variational_Bayesian_Learning.html">nips-2000-Propagation Algorithms for Variational Bayesian Learning</a></p>
<p>107 <a title="nips-2000-107" href="../nips2000/nips-2000-Rate-coded_Restricted_Boltzmann_Machines_for_Face_Recognition.html">nips-2000-Rate-coded Restricted Boltzmann Machines for Face Recognition</a></p>
<p>108 <a title="nips-2000-108" href="../nips2000/nips-2000-Recognizing_Hand-written_Digits_Using_Hierarchical_Products_of_Experts.html">nips-2000-Recognizing Hand-written Digits Using Hierarchical Products of Experts</a></p>
<p>109 <a title="nips-2000-109" href="../nips2000/nips-2000-Redundancy_and_Dimensionality_Reduction_in_Sparse-Distributed_Representations_of_Natural_Objects_in_Terms_of_Their_Local_Features.html">nips-2000-Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features</a></p>
<p>110 <a title="nips-2000-110" href="../nips2000/nips-2000-Regularization_with_Dot-Product_Kernels.html">nips-2000-Regularization with Dot-Product Kernels</a></p>
<p>111 <a title="nips-2000-111" href="../nips2000/nips-2000-Regularized_Winnow_Methods.html">nips-2000-Regularized Winnow Methods</a></p>
<p>112 <a title="nips-2000-112" href="../nips2000/nips-2000-Reinforcement_Learning_with_Function_Approximation_Converges_to_a_Region.html">nips-2000-Reinforcement Learning with Function Approximation Converges to a Region</a></p>
<p>113 <a title="nips-2000-113" href="../nips2000/nips-2000-Robust_Reinforcement_Learning.html">nips-2000-Robust Reinforcement Learning</a></p>
<p>114 <a title="nips-2000-114" href="../nips2000/nips-2000-Second_Order_Approximations_for_Probability_Models.html">nips-2000-Second Order Approximations for Probability Models</a></p>
<p>115 <a title="nips-2000-115" href="../nips2000/nips-2000-Sequentially_Fitting_%60%60Inclusive%27%27_Trees_for_Inference_in_Noisy-OR_Networks.html">nips-2000-Sequentially Fitting ``Inclusive'' Trees for Inference in Noisy-OR Networks</a></p>
<p>116 <a title="nips-2000-116" href="../nips2000/nips-2000-Sex_with_Support_Vector_Machines.html">nips-2000-Sex with Support Vector Machines</a></p>
<p>117 <a title="nips-2000-117" href="../nips2000/nips-2000-Shape_Context%3A_A_New_Descriptor_for_Shape_Matching_and_Object_Recognition.html">nips-2000-Shape Context: A New Descriptor for Shape Matching and Object Recognition</a></p>
<p>118 <a title="nips-2000-118" href="../nips2000/nips-2000-Smart_Vision_Chip_Fabricated_Using_Three_Dimensional_Integration_Technology.html">nips-2000-Smart Vision Chip Fabricated Using Three Dimensional Integration Technology</a></p>
<p>119 <a title="nips-2000-119" href="../nips2000/nips-2000-Some_New_Bounds_on_the_Generalization_Error_of_Combined_Classifiers.html">nips-2000-Some New Bounds on the Generalization Error of Combined Classifiers</a></p>
<p>120 <a title="nips-2000-120" href="../nips2000/nips-2000-Sparse_Greedy_Gaussian_Process_Regression.html">nips-2000-Sparse Greedy Gaussian Process Regression</a></p>
<p>121 <a title="nips-2000-121" href="../nips2000/nips-2000-Sparse_Kernel_Principal_Component_Analysis.html">nips-2000-Sparse Kernel Principal Component Analysis</a></p>
<p>122 <a title="nips-2000-122" href="../nips2000/nips-2000-Sparse_Representation_for_Gaussian_Process_Models.html">nips-2000-Sparse Representation for Gaussian Process Models</a></p>
<p>123 <a title="nips-2000-123" href="../nips2000/nips-2000-Speech_Denoising_and_Dereverberation_Using_Probabilistic_Models.html">nips-2000-Speech Denoising and Dereverberation Using Probabilistic Models</a></p>
<p>124 <a title="nips-2000-124" href="../nips2000/nips-2000-Spike-Timing-Dependent_Learning_for_Oscillatory_Networks.html">nips-2000-Spike-Timing-Dependent Learning for Oscillatory Networks</a></p>
<p>125 <a title="nips-2000-125" href="../nips2000/nips-2000-Stability_and_Noise_in_Biochemical_Switches.html">nips-2000-Stability and Noise in Biochemical Switches</a></p>
<p>126 <a title="nips-2000-126" href="../nips2000/nips-2000-Stagewise_Processing_in_Error-correcting_Codes_and_Image_Restoration.html">nips-2000-Stagewise Processing in Error-correcting Codes and Image Restoration</a></p>
<p>127 <a title="nips-2000-127" href="../nips2000/nips-2000-Structure_Learning_in_Human_Causal_Induction.html">nips-2000-Structure Learning in Human Causal Induction</a></p>
<p>128 <a title="nips-2000-128" href="../nips2000/nips-2000-Support_Vector_Novelty_Detection_Applied_to_Jet_Engine_Vibration_Spectra.html">nips-2000-Support Vector Novelty Detection Applied to Jet Engine Vibration Spectra</a></p>
<p>129 <a title="nips-2000-129" href="../nips2000/nips-2000-Temporally_Dependent_Plasticity%3A_An_Information_Theoretic_Account.html">nips-2000-Temporally Dependent Plasticity: An Information Theoretic Account</a></p>
<p>130 <a title="nips-2000-130" href="../nips2000/nips-2000-Text_Classification_using_String_Kernels.html">nips-2000-Text Classification using String Kernels</a></p>
<p>131 <a title="nips-2000-131" href="../nips2000/nips-2000-The_Early_Word_Catches_the_Weights.html">nips-2000-The Early Word Catches the Weights</a></p>
<p>132 <a title="nips-2000-132" href="../nips2000/nips-2000-The_Interplay_of_Symbolic_and_Subsymbolic_Processes_in_Anagram_Problem_Solving.html">nips-2000-The Interplay of Symbolic and Subsymbolic Processes in Anagram Problem Solving</a></p>
<p>133 <a title="nips-2000-133" href="../nips2000/nips-2000-The_Kernel_Gibbs_Sampler.html">nips-2000-The Kernel Gibbs Sampler</a></p>
<p>134 <a title="nips-2000-134" href="../nips2000/nips-2000-The_Kernel_Trick_for_Distances.html">nips-2000-The Kernel Trick for Distances</a></p>
<p>135 <a title="nips-2000-135" href="../nips2000/nips-2000-The_Manhattan_World_Assumption%3A_Regularities_in_Scene_Statistics_which_Enable_Bayesian_Inference.html">nips-2000-The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference</a></p>
<p>136 <a title="nips-2000-136" href="../nips2000/nips-2000-The_Missing_Link_-_A_Probabilistic_Model_of_Document_Content_and_Hypertext_Connectivity.html">nips-2000-The Missing Link - A Probabilistic Model of Document Content and Hypertext Connectivity</a></p>
<p>137 <a title="nips-2000-137" href="../nips2000/nips-2000-The_Unscented_Particle_Filter.html">nips-2000-The Unscented Particle Filter</a></p>
<p>138 <a title="nips-2000-138" href="../nips2000/nips-2000-The_Use_of_Classifiers_in_Sequential_Inference.html">nips-2000-The Use of Classifiers in Sequential Inference</a></p>
<p>139 <a title="nips-2000-139" href="../nips2000/nips-2000-The_Use_of_MDL_to_Select_among_Computational_Models_of_Cognition.html">nips-2000-The Use of MDL to Select among Computational Models of Cognition</a></p>
<p>140 <a title="nips-2000-140" href="../nips2000/nips-2000-Tree-Based_Modeling_and_Estimation_of_Gaussian_Processes_on_Graphs_with_Cycles.html">nips-2000-Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles</a></p>
<p>141 <a title="nips-2000-141" href="../nips2000/nips-2000-Universality_and_Individuality_in_a_Neural_Code.html">nips-2000-Universality and Individuality in a Neural Code</a></p>
<p>142 <a title="nips-2000-142" href="../nips2000/nips-2000-Using_Free_Energies_to_Represent_Q-values_in_a_Multiagent_Reinforcement_Learning_Task.html">nips-2000-Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task</a></p>
<p>143 <a title="nips-2000-143" href="../nips2000/nips-2000-Using_the_Nystr%C3%B6m_Method_to_Speed_Up_Kernel_Machines.html">nips-2000-Using the Nyström Method to Speed Up Kernel Machines</a></p>
<p>144 <a title="nips-2000-144" href="../nips2000/nips-2000-Vicinal_Risk_Minimization.html">nips-2000-Vicinal Risk Minimization</a></p>
<p>145 <a title="nips-2000-145" href="../nips2000/nips-2000-Weak_Learners_and_Improved_Rates_of_Convergence_in_Boosting.html">nips-2000-Weak Learners and Improved Rates of Convergence in Boosting</a></p>
<p>146 <a title="nips-2000-146" href="../nips2000/nips-2000-What_Can_a_Single_Neuron_Compute%3F.html">nips-2000-What Can a Single Neuron Compute?</a></p>
<p>147 <a title="nips-2000-147" href="../nips2000/nips-2000-Who_Does_What%3F_A_Novel_Algorithm_to_Determine_Function_Localization.html">nips-2000-Who Does What? A Novel Algorithm to Determine Function Localization</a></p>
<p>148 <a title="nips-2000-148" href="../nips2000/nips-2000-%60N-Body%27_Problems_in_Statistical_Learning.html">nips-2000-`N-Body' Problems in Statistical Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
