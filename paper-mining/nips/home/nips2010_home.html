<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>nips 2010 knowledge graph</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="#">nips2010</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>nips 2010 knowledge graph</h1>
<br/><h3>similar papers computed by tfidf model</h3><br/><h3>similar papers computed by <a title="lsi-model" href="./nips2010_lsi.html">lsi model</a></h3><br/><h3>similar papers computed by <a title="lda-model" href="./nips2010_lda.html">lda model</a></h3><br/><h2>papers list:</h2><p>1 <a title="nips-2010-1" href="../nips2010/nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>Author: Nadia Payet, Sinisa Todorovic</p><p>Abstract: We combine random forest (RF) and conditional random ﬁeld (CRF) into a new computational framework, called random forest random ﬁeld (RF)2 . Inference of (RF)2 uses the Swendsen-Wang cut algorithm, characterized by MetropolisHastings jumps. A jump from one state to another depends on the ratio of the proposal distributions, and on the ratio of the posterior distributions of the two states. Prior work typically resorts to a parametric estimation of these four distributions, and then computes their ratio. Our key idea is to instead directly estimate these ratios using RF. RF collects in leaf nodes of each decision tree the class histograms of training examples. We use these class histograms for a nonparametric estimation of the distribution ratios. We derive the theoretical error bounds of a two-class (RF)2 . (RF)2 is applied to a challenging task of multiclass object recognition and segmentation over a random ﬁeld of input image regions. In our empirical evaluation, we use only the visual information provided by image regions (e.g., color, texture, spatial layout), whereas the competing methods additionally use higher-level cues about the horizon location and 3D layout of surfaces in the scene. Nevertheless, (RF)2 outperforms the state of the art on benchmark datasets, in terms of accuracy and computation time.</p><p>2 <a title="nips-2010-2" href="../nips2010/nips-2010-A_Bayesian_Approach_to_Concept_Drift.html">nips-2010-A Bayesian Approach to Concept Drift</a></p>
<p>Author: Stephen Bach, Mark Maloof</p><p>Abstract: To cope with concept drift, we placed a probability distribution over the location of the most-recent drift point. We used Bayesian model comparison to update this distribution from the predictions of models trained on blocks of consecutive observations and pruned potential drift points with low probability. We compare our approach to a non-probabilistic method for drift and a probabilistic method for change-point detection. In our experiments, our approach generally yielded improved accuracy and/or speed over these other methods. 1</p><p>3 <a title="nips-2010-3" href="../nips2010/nips-2010-A_Bayesian_Framework_for_Figure-Ground_Interpretation.html">nips-2010-A Bayesian Framework for Figure-Ground Interpretation</a></p>
<p>Author: Vicky Froyen, Jacob Feldman, Manish Singh</p><p>Abstract: Figure/ground assignment, in which the visual image is divided into nearer (ﬁgural) and farther (ground) surfaces, is an essential step in visual processing, but its underlying computational mechanisms are poorly understood. Figural assignment (often referred to as border ownership) can vary along a contour, suggesting a spatially distributed process whereby local and global cues are combined to yield local estimates of border ownership. In this paper we model ﬁgure/ground estimation in a Bayesian belief network, attempting to capture the propagation of border ownership across the image as local cues (contour curvature and T-junctions) interact with more global cues to yield a ﬁgure/ground assignment. Our network includes as a nonlocal factor skeletal (medial axis) structure, under the hypothesis that medial structure “draws” border ownership so that borders are owned by the skeletal hypothesis that best explains them. We also brieﬂy present a psychophysical experiment in which we measured local border ownership along a contour at various distances from an inducing cue (a T-junction). Both the human subjects and the network show similar patterns of performance, converging rapidly to a similar pattern of spatial variation in border ownership along contours. Figure/ground assignment (further referred to as f/g), in which the visual image is divided into nearer (ﬁgural) and farther (ground) surfaces, is an essential step in visual processing. A number of factors are known to affect f/g assignment, including region size [9], convexity [7, 16], and symmetry [1, 7, 11]. Figural assignment (often referred to as border ownership, under the assumption that the ﬁgural side “owns” the border) is usually studied globally, meaning that entire surfaces and their enclosing boundaries are assumed to receive a globally consistent ﬁgural status. But recent psychophysical ﬁndings [8] have suggested that border ownership can vary locally along a boundary, even leading to a globally inconsistent ﬁgure/ground assignment—broadly consistent with electrophysiological evidence showing local coding for border ownership in area V2 as early as 68 msec after image onset [20]. This suggests a spatially distributed and potentially competitive process of ﬁgural assignment [15], in which adjacent surfaces compete to own their common boundary, with ﬁgural status propagating across the image as this competition proceeds. But both the principles and computational mechanisms underlying this process are poorly understood. ∗ V.F. was supported by a Fullbright Honorary fellowship and by the Rutgers NSF IGERT program in Perceptual Science, NSF DGE 0549115, J.F. by NIH R01 EY15888, and M.S. by NSF CCF-0541185 1 In this paper we consider how border ownership might propagate over both space and time—that is, across the image as well as over the progression of computation. Following Weiss et al. [18] we adopt a Bayesian belief network architecture, with nodes along boundaries representing estimated border ownership, and connections arranged so that both neighboring nodes and nonlocal integrating nodes combine to inﬂuence local estimates of border ownership. Our model is novel in two particular respects: (a) we combine both local and global inﬂuences on border ownership in an integrated and principled way; and (b) we include as a nonlocal factor skeletal (medial axis) inﬂuences on f/g assignment. Skeletal structure has not been previously considered as a factor on border ownership, but its relevance follows from a model [4] in which shapes are conceived of as generated by or “grown” from an internal skeleton, with the consequence that their boundaries are perceptually “owned” by the skeletal side. We also briey present a psychophysical experiment in which we measured local border ownership along a contour, at several distances from a strong local f/g inducing cue, and at several time delays after the onset of the cue. The results show measurable spatial differences in judged border ownership, with judgments varying with distance from the inducer; but no temporal effect, with essentially asymptotic judgments even after very brief exposures. Both results are consistent with the behavior of the network, which converges quickly to an asymptotic but spatially nonuniform f/g assignment. 1 The Model The Network. For simplicity, we take an edge map as input for the model, assuming that edges and T-junctions have already been detected. From this edge map we then create a Bayesian belief network consisting of four hierarchical levels. At the input level the model receives evidence E from the image, consisting of local contour curvature and T-junctions. The nodes for this level are placed at equidistant locations along the contour. At the ﬁrst level the model estimates local border ownership. The border ownership, or B-nodes at this level are at the same locations as the E-nodes, but are connected to their nearest neighbors, and are the parent of the E-node at their location. (As a simplifying assumption, such connections are broken at T-junctions in such a way that the occluded contour is disconnected from the occluder.) The highest level has skeletal nodes, S, whose positions are deﬁned by the circumcenters of the Delaunay triangulation on all the E-nodes, creating a coarse medial axis skeleton [13]. Because of the structure of the Delaunay, each S-node is connected to exactly three E-nodes from which they receive information about the position and the local tangent of the contour. In the current state of the model the S-nodes are “passive”, meaning their posteriors are computed before the model is initiated. Between the S nodes and the B nodes are the grouping nodes G. They have the same positions as the S-nodes and the same Delaunay connections, but to B-nodes that have the same image positions as the E-nodes. They will integrate information from distant B-nodes, applying an interiority cue that is inﬂuenced by the local strength of skeletal axes as computed by the S-nodes (Fig. 1). Although this is a multiply connected network, we have found that given reasonable parameters the model converges to intuitive posteriors for a variety of shapes (see below). Updating. Our goal is to compute the posterior p(Bi |I), where I is the whole image. Bi is a binary variable coding for the local direction of border ownership, that is, the side that owns the border. In order for border ownership estimates to be inﬂuenced by image structure elsewhere in the image, information has to propagate throughout the network. To achieve this propagation, we use standard equations for node updating [14, 12]. However while to all other connections being directed, connections at the B-node level are undirected, causing each node to be child and parent node at the same time. Considering only the B-node level, a node Bi is only separated from the rest of the network by its two neighbors. Hence the Markovian property applies, in that Bi only needs to get iterative information from its neighbors to eventually compute p(Bi |I). So considering the whole network, at each iteration t, Bi receives information from both its child, Ei and from its parents—that is neigbouring nodes (Bi+1 and Bi−1 )—as well as all grouping nodes connected to it (Gj , ..., Gm ). The latter encode for interiority versus exteriority, interiority meaning that the B-node’s estimated gural direction points towards the G-node in question, exteriority meaning that it points away. Integrating all this information creates a multidimensional likelihood function: p(Bi |Bi−1 , Bi+1 , Gj , ..., Gm ). Because of its complexity we choose to approximate it (assuming all nodes are marginally independent of each other when conditioned on Bi ) by 2 Figure 1: Basic network structure of the model. Both skeletal (S-nodes) and border-ownerhsip nodes (B-nodes) get evidence from E-nodes, though different types. S-nodes receive mere positional information, while B-nodes receive information about local curvature and the presence of T-junctions. Because of the structure of the Delaunay triangulation S-nodes and G-nodes (grouping nodes) always get input from exactly three nodes, respectively E and B-nodes. The gray color depicts the fact that this part of the network is computed before the model is initiated and does not thereafter interact with the dynamics of the model. m p(Bi |Pj , ..., Pm ) ∝ p(Bi |Pj ) (1) j where the Pj ’s are the parents of Bi . Given this, at each iteration, each node Bi performs the following computation: Bel(Bi ) ← cλ(Bi )π(Bi )α(Bi )β(Bi ) (2) where conceptually λ stands for bottom-up information, π for top down information and α and β for information received from within the same level. More formally, λ(Bi ) ← p(E|Bi ) (3) m π(Bi ) ← p(Bi |Gj )πGj (Bi ) j (4) Gj and analogously to equation 4 for α(Bi ) and β(Bi ), which compute information coming from Bi−1 and Bi+1 respectively. For these πBi−1 (Bi ), πBi+1 (Bi ), and πGj (Bi ): πGj (Bi ) ← c π(G) λBk (Gj ) (5) k=i πBi−1 (Bi ) ← c β(Bi−1 )λ(Bi−1 )π(Bi−1 ) 3 (6) and πBi+1 (Bi ) is analogous to πBi−1 (Bi ), with c and c being normalization constants. Finally for the G-nodes: Bel(Gi ) ← cλ(Gi )π(Gi ) λ(Gi ) ← (7) λBj (Gi ) (8) j m λBj (Gi ) ← λ(Bj )p(Bi |Gj )[α(Bj )β(Bj ) Bj p(Bi |Gk )πGk (Bi )] (9) k=i Gk The posteriors of the S-nodes are used to compute the π(Gi ). This posterior computes how well the S-node at each position explains the contour—that is, how well it accounts for the cues ﬂowing from the E-nodes it is connected to. Each Delaunay connection between S- and E-nodes can be seen as a rib that sprouts from the skeleton. More speciﬁcally each rib sprouts in a direction that is normal (perpendicular) to the tangent of the contour at the E-node plus a random error φi chosen independently for each rib from a von Mises distribution centered on zero, i.e. φi ∼ V (0, κS ) with spread parameter κS [4]. The rib lengths are drawn from an exponential decreasing density function p(ρi ) ∝ e−λS ρi [4]. We can now express how well this node “explains” the three E-nodes it is connected to via the probability that this S-node deserves to be a skeletal node or not, p(S = true|E1 , E2 , E3 ) ∝ p(ρi )p(φi ) (10) i with S = true depicting that this S-node deserves to be a skeletal node. From this we then compute the prior π(Gi ) in such a way that good (high posterior) skeletal nodes induce a high interiority bias, hence a stronger tendency to induce ﬁgural status. Conversely, bad (low posterior) skeletal nodes create a prior close to indifferent (uniform) and thus have less (or no) inﬂuence on ﬁgural status. Likelihood functions Finally we need to express the likelihood function necessary for the updating rules described above. The ﬁrst two likelihood functions are part of p(Ei |Bi ), one for each of the local cues. The ﬁrst one, reﬂecting local curvature, gives the probability of the orientations of the two vectors inherent to Ei (α1 and α2 ) given both direction of ﬁgure (θ) encoded in Bi as a von Mises density centered on θ, i.e. αi ∼ V (θ, κEB ). The second likelihood function, reﬂecting the presence of a T-junction, simply assumes a ﬁxed likelihood when a T-junction is present—that is p(T-junction = true|Bi ) = θT , where Bi places the direction of ﬁgure in the direction of the occluder. This likelihood function is only in effect when a T-junction is present, replacing the curvature cue at that node. The third likelihood function serves to keep consistency between nodes of the ﬁrst level. This function p(Bi |Bi−1 ) or p(Bi |Bi+1 ) is used to compute α(B) and β(B) and is deﬁned 2x2 conditional probability matrix with a single free parameter, θBB (the probability that ﬁgural direction at both B-nodes are the same). A fourth and ﬁnal likelihood function p(Bi |Gj ) serves to propagate information between level one and two. This likelihood function is 2x2 conditional probability matrix matrix with one free parameter, θBG . In this case θBG encodes the probability that the ﬁgural direction of the B-node is in the direction of the exterior or interior preference of the G-node. In total this brings us to six free parameters in the model: κS , λS , κEB , θT , θBB , and θBG . 2 Basic Simulations To evaluate the performance of the model, we ﬁrst tested it on several basic stimulus conﬁgurations in which the desired outcome is intuitively clear: a convex shape, a concave shape, a pair of overlapping shapes, and a pair of non-overlapping shapes (Fig. 2,3). The convex shape is the simplest in that curvature never changes sign. The concave shape includes a region with oppositely signed curvature. (The shape is naturally described as predominantly positively curved with a region of negative curvature, i.e. a concavity. But note that it can also be interpreted as predominantly negatively curved “window” with a region of positive curvature, although this is not the intuitive interpretation.) 4 The overlapping pair of shapes consists of two convex shapes with one partly occluding the other, creating a competition between the two shapes for the ownership of the common borderline. Finally the non-overlapping shapes comprise two simple convex shapes that do not touch—again setting up a competition for ownership of the two inner boundaries (i.e. between each shape and the ground space between them). Fig. 2 shows the network structures for each of these four cases. Figure 2: Network structure for the four shape categories (left to right: convex, concave, overlapping, non-overlapping shapes). Blue depict the locations of the B-nodes (and also the E-nodes), the red connections are the connections between B-nodes, the green connections are connections between B-nodes and G-nodes, and the G-nodes (and also the S-nodes) go from orange to dark red. This colour code depicts low (orange) to high (dark red) probability that this is a skeletal node, and hence the strength of the interiority cue. Running our model with hand-estimated parameter values yields highly intuitive posteriors (Fig. 3), an essential “sanity check” to ensure that the network approximates human judgments in simple cases. For the convex shape the model assigns ﬁgure to the interior just as one would expect even based solely on local curvature (Fig. 3A). In the concave ﬁgure (Fig. 3B), estimated border ownership begins to reverse inside the deep concavity. This may seem surprising, but actually closely matches empirical results obtained when local border ownership is probed psychophysically inside a similarly deep concavity, i.e. a “negative part” in which f/g seems to partly reverse [8]. For the overlapping shapes posteriors were also intuitive, with the occluding shape interpreted as in front and owning the common border (Fig. 3C). Finally, for the two non-overlapping shapes the model computed border-ownership just as one would expect if each shape were run separately, with each shape treated as ﬁgural along its entire boundary (Fig. 3D). That is, even though there is skeletal structure in the ground-region between the two shapes (see Fig. 2D), its posterior is weak compared to the skeletal structure inside the shapes, which thus loses the competition to own the boundary between them. For all these conﬁgurations, the model not only converged to intuitive estimates but did so rapidly (Fig. 4), always in fewer cycles than would be expected by pure lateral propagation, niterations < Nnodes [18] (with these parameters, typically about ﬁve times faster). Figure 3: Posteriors after convergence for the four shape categories (left to right: convex, concave, overlapping, non-overlapping). Arrows indicate estimated border ownership, with direction pointing to the perceived ﬁgural side, and length proportional to the magnitude of the posterior. All four simulations used the same parameters. 5 Figure 4: Convergence of the model for the basic shape categories. The vertical lines represent the point of convergence for each of the three shape categories. The posterior change is calculated as |p(Bi = 1|I)t − p(Bi = 1|I)t−1 | at each iteration. 3 Comparison to human data Beyond the simple cases reviewed above, we wished to submit our network to a more ﬁne-grained comparison with human data. To this end we compared its performance to that of human subjects in an experiment we conducted (to be presented in more detail in a future paper). Brieﬂy, our experiment involved ﬁnding evidence for propagation of f/g signals across the image. Subjects were ﬁrst shown a stimulus in which the f/g conﬁguration was globally and locally unambiguous and consistent: a smaller rectangle partly occluding a larger one (Fig. 5A), meaning that the smaller (front) one owns the common border. Then this conﬁguration was perturbed by adding two bars, of which one induced a local f/g reversal—making it now appear locally that the larger rectangle owned the border (Fig. 5B). (The other bar in the display does not alter f/g interpretation, but was included to control for the attentional affects of introducing a bar in the image.) The inducing bar creates T-junctions that serve as strong local f/g cues, in this case tending to reverse the prior global interpretation of the ﬁgure. We then measured subjective border ownership along the central contour at various distances from the inducing bar, and at different times after the onset of the bar (25ms, 100ms and 250ms). We measured border ownership locally using a method introduced in [8] in which a local motion probe is introduced at a point on the boundary between two color regions of different colors, and the subject is asked which color appeared to move. Because the ﬁgural side “owns” the border, the response reﬂects perceived ﬁgural status. The goal of the experiment was to actually measure the progression of the inﬂuence of the inducing T-junction as it (hypothetically) propagated along the boundary. Brieﬂy, we found no evidence of temporal differences, meaning that f/g judgments were essentially constant over time, suggesting rapid convergence of local f/g assignment. (This is consistent with the very rapid convergence of our network, which would suggest a lack of measurable temporal differences except at much shorter time scales than we measured.) But we did ﬁnd a progressive reduction of f/g reversal with increasing distance from the inducer—that is, the inﬂuence of the T-junction decayed with distance. Mean responses aggregated over subjects (shortest delay only) are shown in Fig. 6. In order to run our model on this stimulus (which has a much more complex structure than the simple ﬁgures tested above) we had to make some adjustments. We removed the bars from the edge map, leaving only the T-junctions as underlying cues. This was a necessary ﬁrst step because our model is not yet able to cope with skeletons that are split up by occluders. (The larger rectangle’s skeleton has been split up by the lower bar.) In this way all contours except those created by the bars were used to create the network (Fig. 7). Given this network we ran the model using hand-picked parameters that 6 Figure 5: Stimuli used in the experiment. A. Initial stimulus with locally and globally consistent and unambiguous f/g. B. Subsequently bars were added of which one (the top bar in this case) created a local reversal of f/g. C. Positions at which local f/g judgments of subjects were probed. Figure 6: Results from our experiment aggregated for all 7 subjects (shortest delay only) are shown in red. The x-axis shows distance from the inducing bar at which f/g judgment was probed. The y-axis shows the proportion of trials on which subjects judged the smaller rectangle to own the boundary. As can be seen, the further from the T-junction, the lower the f/g reversal. The ﬁtted model (green curve) shows very similar pattern. Horizontal black line indicates chance performance (ambiguous f/g). gave us the best possible qualitative similarity to the human data. The parameters used never entailed total elimination of the inﬂuence of any likelihood function (κS = 16, λS = .025, κEB = .5, θT = .9, θBB = .9, and θBG = .6). As can be seen in Fig. 6 the border-ownership estimates at the locations where we had data show compelling similarities to human judgments. Furthermore along the entire contour the model converged to intuitive border-ownership estimates (Fig. 7) very rapidly (within 36 iterations). The fact that our model yielded intuitive estimates for the current network in which not all contours were completed shows another strength of our model. Because our model included grouping nodes, it did not require contours to be amodally completed [6] in order for information to propagate. 4 Conclusion In this paper we proposed a model rooted in Bayesian belief networks to compute ﬁgure/ground. The model uses both local and global cues, combined in a principled way, to achieve a stable and apparently psychologically reasonable estimate of border ownership. Local cues included local curvature and T-junctions, both well-established cues to f/g. Global cues included skeletal structure, 7 Figure 7: (left) Node structure for the experimental stimulus. (right) The model’s local borderownership estimates after convergence. a novel cue motivated by the idea that strongly axial shapes tend to be ﬁgural and thus own their boundaries. We successfully tested this model on both simple displays, in which it gave intuitive results, and on a more complex experimental stimulus, in which it gave a close match to the pattern of f/g propagation found in our subjects. Speciﬁcally, the model, like the human subjects rapidly converged to a stable local f/g interpretation. Our model’s structure shows several interesting parallels to properties of neural coding of border ownership in visual cortex. Some cortical cells (end-stopped cells) appear to code for local curvature [3] and T-junctions [5]. The B-nodes in our model could be seen as corresponding to cells that code for border ownership [20]. Furthermore, some authors [2] have suggested that recurrent feedback loops between border ownership cells in V2 and cells in V4 (corresponding to G-nodes in our model) play a role in the rapid computation of border ownership. The very rapid convergence we observed in our model likewise appears to be due to the connections between B-nodes and G-nodes. Finally scale-invariant shape representations (such as, speculatively, those based on skeletons) are thought to be present in higher cortical regions such as IT [17], which project down to earlier areas in ways that are not yet understood. A number of parallels to past models of f/g should be mentioned. Weiss [18] pioneered the application of belief networks to the f/g problem, though their network only considered a more restricted set of local cues and no global ones, such that information only propagated along the contour. Furthermore it has not been systematically compared to human judgments. Kogo et al. [10] proposed an exponential decay of f/g signals as they spread throughout the image. Our model has a similar decay for information going through the G-nodes, though it is also inﬂuenced by an angular factor deﬁned by the position of the skeletal node. Like the model by Li Zhaoping [19], our model includes horizontal propagation between B-nodes, analogous to border-ownership cells in her model. A neurophysiological model by Craft et al. [2] deﬁnes grouping cells coding for an interiority preference that decays with the size of the receptive ﬁelds of these grouping cells. Our model takes this a step further by including shape (skeletal) structure as a factor in interiority estimates, rather than simply size of receptive ﬁelds (which is similar to the rib lengths in our model). Currently, our use of skeletons as shape representations is still limited to medial axis skeletons and surfaces that are not split up by occluders. Our future goals including integrating skeletons in a more robust way following the probabilistic account suggested by Feldman and Singh [4]. Eventually, we hope to fully integrate skeleton computation with f/g computation so that the more general problem of shape and surface estimation can be approached in a coherent and uniﬁed fashion. 8 References [1] P. Bahnsen. Eine untersuchung uber symmetrie und assymmetrie bei visuellen wahrnehmungen. Zeitschrift fur psychology, 108:129–154, 1928. [2] E. Craft, H. Sch¨ tze, E. Niebur, and R. von der Heydt. A neural model of ﬁgure-ground u organization. Journal of Neurophysiology, 97:4310–4326, 2007. [3] A. Dobbins, S. W. Zucker, and M. S. Cyander. Endstopping and curvature. Vision Research, 29:1371–1387, 1989. [4] J. Feldman and M. Singh. Bayesian estimation of the shape skeleton. Proceedings of the National Academy of Sciences, 103:18014–18019, 2006. [5] B. Heider, V. Meskenaite, and E. Peterhans. Anatomy and physiology of a neural mechanism deﬁning depth order and contrast polarity at illusory contours. European Journal of Neuroscience, 12:4117–4130, 2000. [6] G. Kanizsa. Organization inVision. New York: Praeger, 1979. [7] G. Kanizsa and W. Gerbino. Vision and Artifact, chapter Convexity and symmetry in ﬁgureground organisation, pages 25–32. New York: Springer, 1976. [8] S. Kim and J. Feldman. Globally inconsistent ﬁgure/ground relations induced by a negative part. Journal of Vision, 9:1534–7362, 2009. [9] K. Koffka. Principles of Gestalt Psychology. Lund Humphries, London, 1935. [10] N. Kogo, C. Strecha, L. Van Gool, and J. Wagemans. Surface construction by a 2-d differentiation-integration process: a neurocomputational model for perceived border ownership, depth, and lightness in kanizsa ﬁgures. Psychological Review, 117:406–439, 2010. [11] B. Machielsen, M. Pauwels, and J. Wagemans. The role of vertical mirror-symmetry in visual shape detection. Journal of Vision, 9:1–11, 2009. [12] K. Murphy, Y. Weiss, and M.I. Jordan. Loopy belief propagation for approximate inference: an empirical study. Proceedings of Uncertainty in AI, pages 467–475, 1999. [13] R. L. Ogniewicz and O. K¨ bler. Hierarchic Voronoi skeletons. Pattern Recognition, 28:343– u 359, 1995. [14] J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann, 1988. [15] M. A. Peterson and E. Skow. Inhibitory competition between shape properties in ﬁgureground perception. Journal of Experimental Psychology: Human Perception and Performance, 34:251–267, 2008. [16] K. A. Stevens and A. Brookes. The concave cusp as a determiner of ﬁgure-ground. Perception, 17:35–42, 1988. [17] K. Tanaka, H. Saito, Y. Fukada, and M. Moriya. Coding visual images of object in the inferotemporal cortex of the macaque monkey. Journal of Neurophysiology, 66:170–189, 1991. [18] Y. Weiss. Interpreting images by propagating Bayesian beliefs. Adv. in Neural Information Processing Systems, 9:908915, 1997. [19] L. Zhaoping. Border ownership from intracortical interactions in visual area V2. Neuron, 47(1):143–153, Jul 2005. [20] H. Zhou, H. S. Friedman, and R. von der Heydt. Coding of border ownerschip in monkey visual cortex. The Journal of Neuroscience, 20:6594–6611, 2000. 9</p><p>4 <a title="nips-2010-4" href="../nips2010/nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>Author: Alan Fern, Prasad Tadepalli</p><p>Abstract: We study several classes of interactive assistants from the points of view of decision theory and computational complexity. We ﬁrst introduce a class of POMDPs called hidden-goal MDPs (HGMDPs), which formalize the problem of interactively assisting an agent whose goal is hidden and whose actions are observable. In spite of its restricted nature, we show that optimal action selection in ﬁnite horizon HGMDPs is PSPACE-complete even in domains with deterministic dynamics. We then introduce a more restricted model called helper action MDPs (HAMDPs), where the assistant’s action is accepted by the agent when it is helpful, and can be easily ignored by the agent otherwise. We show classes of HAMDPs that are complete for PSPACE and NP along with a polynomial time class. Furthermore, we show that for general HAMDPs a simple myopic policy achieves a regret, compared to an omniscient assistant, that is bounded by the entropy of the initial goal distribution. A variation of this policy is shown to achieve worst-case regret that is logarithmic in the number of goals for any goal distribution. 1</p><p>5 <a title="nips-2010-5" href="../nips2010/nips-2010-A_Dirty_Model_for_Multi-task_Learning.html">nips-2010-A Dirty Model for Multi-task Learning</a></p>
<p>Author: Ali Jalali, Sujay Sanghavi, Chao Ruan, Pradeep K. Ravikumar</p><p>Abstract: We consider multi-task learning in the setting of multiple linear regression, and where some relevant features could be shared across the tasks. Recent research has studied the use of ℓ1 /ℓq norm block-regularizations with q > 1 for such blocksparse structured problems, establishing strong guarantees on recovery even under high-dimensional scaling where the number of features scale with the number of observations. However, these papers also caution that the performance of such block-regularized methods are very dependent on the extent to which the features are shared across tasks. Indeed they show [8] that if the extent of overlap is less than a threshold, or even if parameter values in the shared features are highly uneven, then block ℓ1 /ℓq regularization could actually perform worse than simple separate elementwise ℓ1 regularization. Since these caveats depend on the unknown true parameters, we might not know when and which method to apply. Even otherwise, we are far away from a realistic multi-task setting: not only do the set of relevant features have to be exactly the same across tasks, but their values have to as well. Here, we ask the question: can we leverage parameter overlap when it exists, but not pay a penalty when it does not ? Indeed, this falls under a more general question of whether we can model such dirty data which may not fall into a single neat structural bracket (all block-sparse, or all low-rank and so on). With the explosion of such dirty high-dimensional data in modern settings, it is vital to develop tools – dirty models – to perform biased statistical estimation tailored to such data. Here, we take a ﬁrst step, focusing on developing a dirty model for the multiple regression problem. Our method uses a very simple idea: we estimate a superposition of two sets of parameters and regularize them differently. We show both theoretically and empirically, our method strictly and noticeably outperforms both ℓ1 or ℓ1 /ℓq methods, under high-dimensional scaling and over the entire range of possible overlaps (except at boundary cases, where we match the best method). 1 Introduction: Motivation and Setup High-dimensional scaling. In ﬁelds across science and engineering, we are increasingly faced with problems where the number of variables or features p is larger than the number of observations n. Under such high-dimensional scaling, for any hope of statistically consistent estimation, it becomes vital to leverage any potential structure in the problem such as sparsity (e.g. in compressed sensing [3] and LASSO [14]), low-rank structure [13, 9], or sparse graphical model structure [12]. It is in such high-dimensional contexts in particular that multi-task learning [4] could be most useful. Here, 1 multiple tasks share some common structure such as sparsity, and estimating these tasks jointly by leveraging this common structure could be more statistically efﬁcient. Block-sparse Multiple Regression. A common multiple task learning setting, and which is the focus of this paper, is that of multiple regression, where we have r > 1 response variables, and a common set of p features or covariates. The r tasks could share certain aspects of their underlying distributions, such as common variance, but the setting we focus on in this paper is where the response variables have simultaneously sparse structure: the index set of relevant features for each task is sparse; and there is a large overlap of these relevant features across the different regression problems. Such “simultaneous sparsity” arises in a variety of contexts [15]; indeed, most applications of sparse signal recovery in contexts ranging from graphical model learning, kernel learning, and function estimation have natural extensions to the simultaneous-sparse setting [12, 2, 11]. It is useful to represent the multiple regression parameters via a matrix, where each column corresponds to a task, and each row to a feature. Having simultaneous sparse structure then corresponds to the matrix being largely “block-sparse” – where each row is either all zero or mostly non-zero, and the number of non-zero rows is small. A lot of recent research in this setting has focused on ℓ1 /ℓq norm regularizations, for q > 1, that encourage the parameter matrix to have such blocksparse structure. Particular examples include results using the ℓ1 /ℓ∞ norm [16, 5, 8], and the ℓ1 /ℓ2 norm [7, 10]. Dirty Models. Block-regularization is “heavy-handed” in two ways. By strictly encouraging sharedsparsity, it assumes that all relevant features are shared, and hence suffers under settings, arguably more realistic, where each task depends on features speciﬁc to itself in addition to the ones that are common. The second concern with such block-sparse regularizers is that the ℓ1 /ℓq norms can be shown to encourage the entries in the non-sparse rows taking nearly identical values. Thus we are far away from the original goal of multitask learning: not only do the set of relevant features have to be exactly the same, but their values have to as well. Indeed recent research into such regularized methods [8, 10] caution against the use of block-regularization in regimes where the supports and values of the parameters for each task can vary widely. Since the true parameter values are unknown, that would be a worrisome caveat. We thus ask the question: can we learn multiple regression models by leveraging whatever overlap of features there exist, and without requiring the parameter values to be near identical? Indeed this is an instance of a more general question on whether we can estimate statistical models where the data may not fall cleanly into any one structural bracket (sparse, block-sparse and so on). With the explosion of dirty high-dimensional data in modern settings, it is vital to investigate estimation of corresponding dirty models, which might require new approaches to biased high-dimensional estimation. In this paper we take a ﬁrst step, focusing on such dirty models for a speciﬁc problem: simultaneously sparse multiple regression. Our approach uses a simple idea: while any one structure might not capture the data, a superposition of structural classes might. Our method thus searches for a parameter matrix that can be decomposed into a row-sparse matrix (corresponding to the overlapping or shared features) and an elementwise sparse matrix (corresponding to the non-shared features). As we show both theoretically and empirically, with this simple ﬁx we are able to leverage any extent of shared features, while allowing disparities in support and values of the parameters, so that we are always better than both the Lasso or block-sparse regularizers (at times remarkably so). The rest of the paper is organized as follows: In Sec 2. basic deﬁnitions and setup of the problem are presented. Main results of the paper is discussed in sec 3. Experimental results and simulations are demonstrated in Sec 4. Notation: For any matrix M , we denote its j th row as Mj , and its k-th column as M (k) . The set of all non-zero rows (i.e. all rows with at least one non-zero element) is denoted by RowSupp(M ) (k) and its support by Supp(M ). Also, for any matrix M , let M 1,1 := j,k |Mj |, i.e. the sums of absolute values of the elements, and M 1,∞ := j 2 Mj ∞ where, Mj ∞ (k) := maxk |Mj |. 2 Problem Set-up and Our Method Multiple regression. We consider the following standard multiple linear regression model: ¯ y (k) = X (k) θ(k) + w(k) , k = 1, . . . , r, where y (k) ∈ Rn is the response for the k-th task, regressed on the design matrix X (k) ∈ Rn×p (possibly different across tasks), while w(k) ∈ Rn is the noise vector. We assume each w(k) is drawn independently from N (0, σ 2 ). The total number of tasks or target variables is r, the number of features is p, while the number of samples we have for each task is n. For notational convenience, ¯ we collate these quantities into matrices Y ∈ Rn×r for the responses, Θ ∈ Rp×r for the regression n×r parameters and W ∈ R for the noise. ¯ Dirty Model. In this paper we are interested in estimating the true parameter Θ from data by lever¯ aging any (unknown) extent of simultaneous-sparsity. In particular, certain rows of Θ would have many non-zero entries, corresponding to features shared by several tasks (“shared” rows), while certain rows would be elementwise sparse, corresponding to those features which are relevant for some tasks but not all (“non-shared rows”), while certain rows would have all zero entries, corresponding to those features that are not relevant to any task. We are interested in estimators Θ that automatically adapt to different levels of sharedness, and yet enjoy the following guarantees: Support recovery: We say an estimator Θ successfully recovers the true signed support if ¯ sign(Supp(Θ)) = sign(Supp(Θ)). We are interested in deriving sufﬁcient conditions under which ¯ the estimator succeeds. We note that this is stronger than merely recovering the row-support of Θ, which is union of its supports for the different tasks. In particular, denoting Uk for the support of the ¯ k-th column of Θ, and U = k Uk . Error bounds: We are also interested in providing bounds on the elementwise ℓ∞ norm error of the estimator Θ, ¯ Θ−Θ 2.1 ∞ = max max j=1,...,p k=1,...,r (k) Θj (k) ¯ − Θj . Our Method Our method explicitly models the dirty block-sparse structure. We estimate a sum of two parameter matrices B and S with different regularizations for each: encouraging block-structured row-sparsity in B and elementwise sparsity in S. The corresponding “clean” models would either just use blocksparse regularizations [8, 10] or just elementwise sparsity regularizations [14, 18], so that either method would perform better in certain suited regimes. Interestingly, as we will see in the main results, by explicitly allowing to have both block-sparse and elementwise sparse component, we are ¯ able to outperform both classes of these “clean models”, for all regimes Θ. Algorithm 1 Dirty Block Sparse Solve the following convex optimization problem: (S, B) ∈ arg min S,B 1 2n r k=1 y (k) − X (k) S (k) + B (k) 2 2 + λs S 1,1 + λb B 1,∞ . (1) Then output Θ = B + S. 3 Main Results and Their Consequences We now provide precise statements of our main results. A number of recent results have shown that the Lasso [14, 18] and ℓ1 /ℓ∞ block-regularization [8] methods succeed in recovering signed supports with controlled error bounds under high-dimensional scaling regimes. Our ﬁrst two theorems extend these results to our dirty model setting. In Theorem 1, we consider the case of deterministic design matrices X (k) , and provide sufﬁcient conditions guaranteeing signed support recovery, and elementwise ℓ∞ norm error bounds. In Theorem 2, we specialize this theorem to the case where the 3 rows of the design matrices are random from a general zero mean Gaussian distribution: this allows us to provide scaling on the number of observations required in order to guarantee signed support recovery and bounded elementwise ℓ∞ norm error. Our third result is the most interesting in that it explicitly quantiﬁes the performance gains of our method vis-a-vis Lasso and the ℓ1 /ℓ∞ block-regularization method. Since this entailed ﬁnding the precise constants underlying earlier theorems, and a correspondingly more delicate analysis, we follow Negahban and Wainwright [8] and focus on the case where there are two-tasks (i.e. r = 2), and where we have standard Gaussian design matrices as in Theorem 2. Further, while each of two tasks depends on s features, only a fraction α of these are common. It is then interesting to see how the behaviors of the different regularization methods vary with the extent of overlap α. Comparisons. Negahban and Wainwright [8] show that there is actually a “phase transition” in the scaling of the probability of successful signed support-recovery with the number of observations. n Denote a particular rescaling of the sample-size θLasso (n, p, α) = s log(p−s) . Then as Wainwright [18] show, when the rescaled number of samples scales as θLasso > 2 + δ for any δ > 0, Lasso succeeds in recovering the signed support of all columns with probability converging to one. But when the sample size scales as θLasso < 2−δ for any δ > 0, Lasso fails with probability converging to one. For the ℓ1 /ℓ∞ -reguralized multiple linear regression, deﬁne a similar rescaled sample size n θ1,∞ (n, p, α) = s log(p−(2−α)s) . Then as Negahban and Wainwright [8] show there is again a transition in probability of success from near zero to near one, at the rescaled sample size of θ1,∞ = (4 − 3α). Thus, for α < 2/3 (“less sharing”) Lasso would perform better since its transition is at a smaller sample size, while for α > 2/3 (“more sharing”) the ℓ1 /ℓ∞ regularized method would perform better. As we show in our third theorem, the phase transition for our method occurs at the rescaled sample size of θ1,∞ = (2 − α), which is strictly before either the Lasso or the ℓ1 /ℓ∞ regularized method except for the boundary cases: α = 0, i.e. the case of no sharing, where we match Lasso, and for α = 1, i.e. full sharing, where we match ℓ1 /ℓ∞ . Everywhere else, we strictly outperform both methods. Figure 3 shows the empirical performance of each of the three methods; as can be seen, they agree very well with the theoretical analysis. (Further details in the experiments Section 4). 3.1 Sufﬁcient Conditions for Deterministic Designs We ﬁrst consider the case where the design matrices X (k) for k = 1, · · ·, r are deterministic, and start by specifying the assumptions we impose on the model. We note that similar sufﬁcient conditions for the deterministic X (k) ’s case were imposed in papers analyzing Lasso [18] and block-regularization methods [8, 10]. (k) A0 Column Normalization Xj 2 ≤ √ 2n for all j = 1, . . . , p, k = 1, . . . , r. ¯ Let Uk denote the support of the k-th column of Θ, and U = supports for each task. Then we require that k r A1 Incoherence Condition γb := 1 − max c j∈U (k) (k) Xj , XUk (k) (k) XUk , XUk Uk denote the union of −1 c We will also ﬁnd it useful to deﬁne γs := 1−max1≤k≤r maxj∈Uk (k) > 0. 1 k=1 (k) Xj , XUk Note that by the incoherence condition A1, we have γs > 0. A2 Eigenvalue Condition Cmin := min λmin 1≤k≤r A3 Boundedness Condition Dmax := max 1≤k≤r 1 (k) (k) XUk , XUk n 1 (k) (k) XUk , XUk n (k) (k) XUk , XUk −1 . 1 > 0. −1 ∞,1 < ∞. Further, we require the regularization penalties be set as λs > 2(2 − γs )σ log(pr) √ γs n and 4 λb > 2(2 − γb )σ log(pr) √ . γb n (2) 1 0.9 0.8 0.8 Dirty Model L1/Linf Reguralizer Probability of Success Probability of Success 1 0.9 0.7 0.6 0.5 0.4 LASSO 0.3 0.2 0 0.5 1 1.5 1.7 2 2.5 Control Parameter θ 3 3.1 3.5 0.6 0.5 0.4 L1/Linf Reguralizer 0.3 LASSO 0.2 p=128 p=256 p=512 0.1 Dirty Model 0.7 p=128 p=256 p=512 0.1 0 0.5 4 1 1.333 (a) α = 0.3 1.5 2 Control Parameter θ (b) α = 2.5 3 2 3 1 0.9 Dirty Model Probability of Success 0.8 0.7 L1/Linf Reguralizer 0.6 0.5 LASSO 0.4 0.3 0.2 p=128 p=256 p=512 0.1 0 0.5 1 1.2 1.5 1.6 2 Control Parameter θ 2.5 (c) α = 0.8 Figure 1: Probability of success in recovering the true signed support using dirty model, Lasso and ℓ1 /ℓ∞ regularizer. For a 2-task problem, the probability of success for different values of feature-overlap fraction α is plotted. As we can see in the regimes that Lasso is better than, as good as and worse than ℓ1 /ℓ∞ regularizer ((a), (b) and (c) respectively), the dirty model outperforms both of the methods, i.e., it requires less number of observations for successful recovery of the true signed support compared to Lasso and ℓ1 /ℓ∞ regularizer. Here p s = ⌊ 10 ⌋ always. Theorem 1. Suppose A0-A3 hold, and that we obtain estimate Θ from our algorithm with regularization parameters chosen according to (2). Then, with probability at least 1 − c1 exp(−c2 n) → 1, we are guaranteed that the convex program (1) has a unique optimum and (a) The estimate Θ has no false inclusions, and has bounded ℓ∞ norm error so that ¯ Supp(Θ) ⊆ Supp(Θ), and ¯ Θ−Θ ∞,∞ 4σ 2 log (pr) + λs Dmax . n Cmin ≤ bmin ¯ (b) sign(Supp(Θ)) = sign Supp(Θ) provided that min ¯ (j,k)∈Supp(Θ) ¯(k) θj > bmin . Here the positive constants c1 , c2 depend only on γs , γb , λs , λb and σ, but are otherwise independent of n, p, r, the problem dimensions of interest. Remark: Condition (a) guarantees that the estimate will have no false inclusions; i.e. all included features will be relevant. If in addition, we require that it have no false exclusions and that recover the support exactly, we need to impose the assumption in (b) that the non-zero elements are large enough to be detectable above the noise. 3.2 General Gaussian Designs Often the design matrices consist of samples from a Gaussian ensemble. Suppose that for each task (k) k = 1, . . . , r the design matrix X (k) ∈ Rn×p is such that each row Xi ∈ Rp is a zero-mean Gaussian random vector with covariance matrix Σ(k) ∈ Rp×p , and is independent of every other (k) row. Let ΣV,U ∈ R|V|×|U | be the submatrix of Σ(k) with rows corresponding to V and columns to U . We require these covariance matrices to satisfy the following conditions: r C1 Incoherence Condition γb := 1 − max c j∈U (k) (k) Σj,Uk , ΣUk ,Uk k=1 5 −1 >0 1 C2 Eigenvalue Condition Cmin := min λmin Σ(k),Uk Uk > 0 so that the minimum eigenvalue 1≤k≤r is bounded away from zero. C3 Boundedness Condition Dmax := (k) ΣUk ,Uk −1 ∞,1 < ∞. These conditions are analogues of the conditions for deterministic designs; they are now imposed on the covariance matrix of the (randomly generated) rows of the design matrix. Further, deﬁning s := maxk |Uk |, we require the regularization penalties be set as 1/2 λs > 1/2 4σ 2 Cmin log(pr) √ γs nCmin − 2s log(pr) and λb > 4σ 2 Cmin r(r log(2) + log(p)) . √ γb nCmin − 2sr(r log(2) + log(p)) (3) Theorem 2. Suppose assumptions C1-C3 hold, and that the number of samples scale as n > max 2s log(pr) 2sr r log(2)+log(p) 2 2 Cmin γs , Cmin γb . Suppose we obtain estimate Θ from algorithm (3). Then, with probability at least 1 − c1 exp (−c2 (r log(2) + log(p))) − c3 exp(−c4 log(rs)) → 1 for some positive numbers c1 − c4 , we are guaranteed that the algorithm estimate Θ is unique and satisﬁes the following conditions: (a) the estimate Θ has no false inclusions, and has bounded ℓ∞ norm error so that ¯ Supp(Θ) ⊆ Supp(Θ), and ¯ Θ−Θ ∞,∞ ≤ 50σ 2 log(rs) + λs nCmin 4s √ + Dmax . Cmin n gmin ¯ (b) sign(Supp(Θ)) = sign Supp(Θ) provided that 3.3 min ¯ (j,k)∈Supp(Θ) ¯(k) θj > gmin . Sharp Transition for 2-Task Gaussian Designs This is one of the most important results of this paper. Here, we perform a more delicate and ﬁner analysis to establish precise quantitative gains of our method. We focus on the special case where r = 2 and the design matrix has rows generated from the standard Gaussian distribution N (0, In×n ), so that C1 − C3 hold, with Cmin = Dmax = 1. As we will see both analytically and experimentally, our method strictly outperforms both Lasso and ℓ1 /ℓ∞ -block-regularization over for all cases, except at the extreme endpoints of no support sharing (where it matches that of Lasso) and full support sharing (where it matches that of ℓ1 /ℓ∞ ). We now present our analytical results; the empirical comparisons are presented next in Section 4. The results will be in terms of a particular rescaling of the sample size n as θ(n, p, s, α) := n . (2 − α)s log (p − (2 − α)s) We will also require the assumptions that 4σ 2 (1 − F1 λs > F2 λb > s/n)(log(r) + log(p − (2 − α)s)) 1/2 (n)1/2 − (s)1/2 − ((2 − α) s (log(r) + log(p − (2 − α)s)))1/2 4σ 2 (1 − s/n)r(r log(2) + log(p − (2 − α)s)) , 1/2 (n)1/2 − (s)1/2 − ((1 − α/2) sr (r log(2) + log(p − (2 − α)s)))1/2 . Theorem 3. Consider a 2-task regression problem (n, p, s, α), where the design matrix has rows generated from the standard Gaussian distribution N (0, In×n ). 6 Suppose maxj∈B∗ ∗(1) Θj − ∗(2) Θj = o(λs ), where B ∗ is the submatrix of Θ∗ with rows where both entries are non-zero. Then the estimate Θ of the problem (1) satisﬁes the following: (Success) Suppose the regularization coefﬁcients satisfy F1 − F2. Further, assume that the number of samples scales as θ(n, p, s, α) > 1. Then, with probability at least 1 − c1 exp(−c2 n) for some positive numbers c1 and c2 , we are guaranteed that Θ satisﬁes the support-recovery and ℓ∞ error bound conditions (a-b) in Theorem 2. ˆ ˆ (Failure) If θ(n, p, s, α) < 1 there is no solution (B, S) for any choices of λs and λb such that ¯ sign Supp(Θ) = sign Supp(Θ) . We note that we require the gap ∗(1) Θj ∗(2) − Θj to be small only on rows where both entries are non-zero. As we show in a more general theorem in the appendix, even in the case where the gap is large, the dependence of the sample scaling on the gap is quite weak. 4 Empirical Results In this section, we investigate the performance of our dirty block sparse estimator on synthetic and real-world data. The synthetic experiments explore the accuracy of Theorem 3, and compare our estimator with LASSO and the ℓ1 /ℓ∞ regularizer. We see that Theorem 3 is very accurate indeed. Next, we apply our method to a real world datasets containing hand-written digits for classiﬁcation. Again we compare against LASSO and the ℓ1 /ℓ∞ . (a multi-task regression dataset) with r = 2 tasks. In both of this real world dataset, we show that dirty model outperforms both LASSO and ℓ1 /ℓ∞ practically. For each method, the parameters are chosen via cross-validation; see supplemental material for more details. 4.1 Synthetic Data Simulation We consider a r = 2-task regression problem as discussed in Theorem 3, for a range of parameters (n, p, s, α). The design matrices X have each entry being i.i.d. Gaussian with mean 0 and variance 1. For each ﬁxed set of (n, s, p, α), we generate 100 instances of the problem. In each instance, ¯ given p, s, α, the locations of the non-zero entries of the true Θ are chosen at randomly; each nonzero entry is then chosen to be i.i.d. Gaussian with mean 0 and variance 1. n samples are then generated from this. We then attempt to estimate using three methods: our dirty model, ℓ1 /ℓ∞ regularizer and LASSO. In each case, and for each instance, the penalty regularizer coefﬁcients are found by cross validation. After solving the three problems, we compare the signed support of the solution with the true signed support and decide whether or not the program was successful in signed support recovery. We describe these process in more details in this section. Performance Analysis: We ran the algorithm for ﬁve different values of the overlap ratio α ∈ 2 {0.3, 3 , 0.8} with three different number of features p ∈ {128, 256, 512}. For any instance of the ˆ ¯ problem (n, p, s, α), if the recovered matrix Θ has the same sign support as the true Θ, then we count it as success, otherwise failure (even if one element has different sign, we count it as failure). As Theorem 3 predicts and Fig 3 shows, the right scaling for the number of oservations is n s log(p−(2−α)s) , where all curves stack on the top of each other at 2 − α. Also, the number of observations required by dirty model for true signed support recovery is always less than both LASSO and ℓ1 /ℓ∞ regularizer. Fig 1(a) shows the probability of success for the case α = 0.3 (when LASSO is better than ℓ1 /ℓ∞ regularizer) and that dirty model outperforms both methods. When α = 2 3 (see Fig 1(b)), LASSO and ℓ1 /ℓ∞ regularizer performs the same; but dirty model require almost 33% less observations for the same performance. As α grows toward 1, e.g. α = 0.8 as shown in Fig 1(c), ℓ1 /ℓ∞ performs better than LASSO. Still, dirty model performs better than both methods in this case as well. 7 4 p=128 p=256 p=512 Phase Transition Threshold 3.5 L1/Linf Regularizer 3 2.5 LASSO 2 Dirty Model 1.5 1 0 0.1 0.2 0.3 0.4 0.5 0.6 Shared Support Parameter α 0.7 0.8 0.9 1 Figure 2: Veriﬁcation of the result of the Theorem 3 on the behavior of phase transition threshold by changing the parameter α in a 2-task (n, p, s, α) problem for dirty model, LASSO and ℓ1 /ℓ∞ regularizer. The y-axis p n is s log(p−(2−α)s) , where n is the number of samples at which threshold was observed. Here s = ⌊ 10 ⌋. Our dirty model method shows a gain in sample complexity over the entire range of sharing α. The pre-constant in Theorem 3 is also validated. n 10 20 40 Average Classiﬁcation Error Variance of Error Average Row Support Size Average Support Size Average Classiﬁcation Error Variance of Error Average Row Support Size Average Support Size Average Classiﬁcation Error Variance of Error Average Row Support Size Average Support Size Our Model 8.6% 0.53% B:165 B + S:171 S:18 B + S:1651 3.0% 0.56% B:211 B + S:226 S:34 B + S:2118 2.2% 0.57% B:270 B + S:299 S:67 B + S:2761 ℓ1 /ℓ∞ 9.9% 0.64% 170 1700 3.5% 0.62% 217 2165 3.2% 0.68% 368 3669 LASSO 10.8% 0.51% 123 539 4.1% 0.68% 173 821 2.8% 0.85% 354 2053 Table 1: Handwriting Classiﬁcation Results for our model, ℓ1 /ℓ∞ and LASSO Scaling Veriﬁcation: To verify that the phase transition threshold changes linearly with α as predicted by Theorem 3, we plot the phase transition threshold versus α. For ﬁve different values of 2 α ∈ {0.05, 0.3, 3 , 0.8, 0.95} and three different values of p ∈ {128, 256, 512}, we ﬁnd the phase transition threshold for dirty model, LASSO and ℓ1 /ℓ∞ regularizer. We consider the point where the probability of success in recovery of signed support exceeds 50% as the phase transition threshold. We ﬁnd this point by interpolation on the closest two points. Fig 2 shows that phase transition threshold for dirty model is always lower than the phase transition for LASSO and ℓ1 /ℓ∞ regularizer. 4.2 Handwritten Digits Dataset We use the handwritten digit dataset [1], containing features of handwritten numerals (0-9) extracted from a collection of Dutch utility maps. This dataset has been used by a number of papers [17, 6] as a reliable dataset for handwritten recognition algorithms. There are thus r = 10 tasks, and each handwritten sample consists of p = 649 features. Table 1 shows the results of our analysis for different sizes n of the training set . We measure the classiﬁcation error for each digit to get the 10-vector of errors. Then, we ﬁnd the average error and the variance of the error vector to show how the error is distributed over all tasks. We compare our method with ℓ1 /ℓ∞ reguralizer method and LASSO. Again, in all methods, parameters are chosen via cross-validation. For our method we separate out the B and S matrices that our method ﬁnds, so as to illustrate how many features it identiﬁes as “shared” and how many as “non-shared”. For the other methods we just report the straight row and support numbers, since they do not make such a separation. Acknowledgements We acknowledge support from NSF grant IIS-101842, and NSF CAREER program, Grant 0954059. 8 References [1] A. Asuncion and D.J. Newman. UCI Machine Learning Repository, http://www.ics.uci.edu/ mlearn/MLRepository.html. University of California, School of Information and Computer Science, Irvine, CA, 2007. [2] F. Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine Learning Research, 9:1179–1225, 2008. [3] R. Baraniuk. Compressive sensing. IEEE Signal Processing Magazine, 24(4):118–121, 2007. [4] R. Caruana. Multitask learning. Machine Learning, 28:41–75, 1997. [5] C.Zhang and J.Huang. Model selection consistency of the lasso selection in high-dimensional linear regression. Annals of Statistics, 36:1567–1594, 2008. [6] X. He and P. Niyogi. Locality preserving projections. In NIPS, 2003. [7] K. Lounici, A. B. Tsybakov, M. Pontil, and S. A. van de Geer. Taking advantage of sparsity in multi-task learning. In 22nd Conference On Learning Theory (COLT), 2009. [8] S. Negahban and M. J. Wainwright. Joint support recovery under high-dimensional scaling: Beneﬁts and perils of ℓ1,∞ -regularization. In Advances in Neural Information Processing Systems (NIPS), 2008. [9] S. Negahban and M. J. Wainwright. Estimation of (near) low-rank matrices with noise and high-dimensional scaling. In ICML, 2010. [10] G. Obozinski, M. J. Wainwright, and M. I. Jordan. Support union recovery in high-dimensional multivariate regression. Annals of Statistics, 2010. [11] P. Ravikumar, H. Liu, J. Lafferty, and L. Wasserman. Sparse additive models. Journal of the Royal Statistical Society, Series B. [12] P. Ravikumar, M. J. Wainwright, and J. Lafferty. High-dimensional ising model selection using ℓ1 -regularized logistic regression. Annals of Statistics, 2009. [13] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. In Allerton Conference, Allerton House, Illinois, 2007. [14] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B, 58(1):267–288, 1996. [15] J. A. Tropp, A. C. Gilbert, and M. J. Strauss. Algorithms for simultaneous sparse approximation. Signal Processing, Special issue on “Sparse approximations in signal and image processing”, 86:572–602, 2006. [16] B. Turlach, W.N. Venables, and S.J. Wright. Simultaneous variable selection. Techno- metrics, 27:349–363, 2005. [17] M. van Breukelen, R.P.W. Duin, D.M.J. Tax, and J.E. den Hartog. Handwritten digit recognition by combined classiﬁers. Kybernetika, 34(4):381–386, 1998. [18] M. J. Wainwright. Sharp thresholds for noisy and high-dimensional recovery of sparsity using ℓ1 -constrained quadratic programming (lasso). IEEE Transactions on Information Theory, 55: 2183–2202, 2009. 9</p><p>6 <a title="nips-2010-6" href="../nips2010/nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>Author: Yang Wang, Greg Mori</p><p>Abstract: We propose a discriminative latent model for annotating images with unaligned object-level textual annotations. Instead of using the bag-of-words image representation currently popular in the computer vision community, our model explicitly captures more intricate relationships underlying visual and textual information. In particular, we model the mapping that translates image regions to annotations. This mapping allows us to relate image regions to their corresponding annotation terms. We also model the overall scene label as latent information. This allows us to cluster test images. Our training data consist of images and their associated annotations. But we do not have access to the ground-truth regionto-annotation mapping or the overall scene label. We develop a novel variant of the latent SVM framework to model them as latent variables. Our experimental results demonstrate the effectiveness of the proposed model compared with other baseline methods.</p><p>7 <a title="nips-2010-7" href="../nips2010/nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>Author: Jean Morales, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We study the problem of learning a sparse linear regression vector under additional conditions on the structure of its sparsity pattern. We present a family of convex penalty functions, which encode this prior knowledge by means of a set of constraints on the absolute values of the regression coefﬁcients. This family subsumes the ℓ1 norm and is ﬂexible enough to include different models of sparsity patterns, which are of practical and theoretical importance. We establish some important properties of these functions and discuss some examples where they can be computed explicitly. Moreover, we present a convergent optimization algorithm for solving regularized least squares with these penalty functions. Numerical simulations highlight the beneﬁt of structured sparsity and the advantage offered by our approach over the Lasso and other related methods.</p><p>8 <a title="nips-2010-8" href="../nips2010/nips-2010-A_Log-Domain_Implementation_of_the_Diffusion_Network_in_Very_Large_Scale_Integration.html">nips-2010-A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration</a></p>
<p>Author: Yi-da Wu, Shi-jie Lin, Hsin Chen</p><p>Abstract: The Diffusion Network(DN) is a stochastic recurrent network which has been shown capable of modeling the distributions of continuous-valued, continuoustime paths. However, the dynamics of the DN are governed by stochastic differential equations, making the DN unfavourable for simulation in a digital computer. This paper presents the implementation of the DN in analogue Very Large Scale Integration, enabling the DN to be simulated in real time. Moreover, the logdomain representation is applied to the DN, allowing the supply voltage and thus the power consumption to be reduced without limiting the dynamic ranges for diffusion processes. A VLSI chip containing a DN with two stochastic units has been designed and fabricated. The design of component circuits will be described, so will the simulation of the full system be presented. The simulation results demonstrate that the DN in VLSI is able to regenerate various types of continuous paths in real-time. 1</p><p>9 <a title="nips-2010-9" href="../nips2010/nips-2010-A_New_Probabilistic_Model_for_Rank_Aggregation.html">nips-2010-A New Probabilistic Model for Rank Aggregation</a></p>
<p>Author: Tao Qin, Xiubo Geng, Tie-yan Liu</p><p>Abstract: This paper is concerned with rank aggregation, which aims to combine multiple input rankings to get a better ranking. A popular approach to rank aggregation is based on probabilistic models on permutations, e.g., the Luce model and the Mallows model. However, these models have their limitations in either poor expressiveness or high computational complexity. To avoid these limitations, in this paper, we propose a new model, which is deﬁned with a coset-permutation distance, and models the generation of a permutation as a stagewise process. We refer to the new model as coset-permutation distance based stagewise (CPS) model. The CPS model has rich expressiveness and can therefore be used in versatile applications, because many different permutation distances can be used to induce the coset-permutation distance. The complexity of the CPS model is low because of the stagewise decomposition of the permutation probability and the efﬁcient computation of most coset-permutation distances. We apply the CPS model to supervised rank aggregation, derive the learning and inference algorithms, and empirically study their effectiveness and efﬁciency. Experiments on public datasets show that the derived algorithms based on the CPS model can achieve state-ofthe-art ranking accuracy, and are much more efﬁcient than previous algorithms.</p><p>10 <a title="nips-2010-10" href="../nips2010/nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>11 <a title="nips-2010-11" href="../nips2010/nips-2010-A_POMDP_Extension_with_Belief-dependent_Rewards.html">nips-2010-A POMDP Extension with Belief-dependent Rewards</a></p>
<p>Author: Mauricio Araya, Olivier Buffet, Vincent Thomas, Françcois Charpillet</p><p>Abstract: Partially Observable Markov Decision Processes (POMDPs) model sequential decision-making problems under uncertainty and partial observability. Unfortunately, some problems cannot be modeled with state-dependent reward functions, e.g., problems whose objective explicitly implies reducing the uncertainty on the state. To that end, we introduce ρPOMDPs, an extension of POMDPs where the reward function ρ depends on the belief state. We show that, under the common assumption that ρ is convex, the value function is also convex, what makes it possible to (1) approximate ρ arbitrarily well with a piecewise linear and convex (PWLC) function, and (2) use state-of-the-art exact or approximate solving algorithms with limited changes. 1</p><p>12 <a title="nips-2010-12" href="../nips2010/nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>Author: Sofia Mosci, Silvia Villa, Alessandro Verri, Lorenzo Rosasco</p><p>Abstract: We deal with the problem of variable selection when variables must be selected group-wise, with possibly overlapping groups deﬁned a priori. In particular we propose a new optimization procedure for solving the regularized algorithm presented in [12], where the group lasso penalty is generalized to overlapping groups of variables. While in [12] the proposed implementation requires explicit replication of the variables belonging to more than one group, our iterative procedure is based on a combination of proximal methods in the primal space and projected Newton method in a reduced dual space, corresponding to the active groups. This procedure provides a scalable alternative with no need for data duplication, and allows to deal with high dimensional problems without pre-processing for dimensionality reduction. The computational advantages of our scheme with respect to state-of-the-art algorithms using data duplication are shown empirically with numerical simulations. 1</p><p>13 <a title="nips-2010-13" href="../nips2010/nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</a></p>
<p>Author: Tamir Hazan, Raquel Urtasun</p><p>Abstract: In this paper we propose an approximated structured prediction framework for large scale graphical models and derive message-passing algorithms for learning their parameters efﬁciently. We ﬁrst relate CRFs and structured SVMs and show that in CRFs a variant of the log-partition function, known as the soft-max, smoothly approximates the hinge loss function of structured SVMs. We then propose an intuitive approximation for the structured prediction problem, using duality, based on a local entropy approximation and derive an efﬁcient messagepassing algorithm that is guaranteed to converge. Unlike existing approaches, this allows us to learn efﬁciently graphical models with cycles and very large number of parameters. 1</p><p>14 <a title="nips-2010-14" href="../nips2010/nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>Author: Umar Syed, Robert E. Schapire</p><p>Abstract: We provide new theoretical results for apprenticeship learning, a variant of reinforcement learning in which the true reward function is unknown, and the goal is to perform well relative to an observed expert. We study a common approach to learning from expert demonstrations: using a classiﬁcation algorithm to learn to imitate the expert’s behavior. Although this straightforward learning strategy is widely-used in practice, it has been subject to very little formal analysis. We prove that, if the learned classiﬁer has error rate ǫ, the difference between the √ value of the apprentice’s policy and the expert’s policy is O( ǫ). Further, we prove that this difference is only O(ǫ) when the expert’s policy is close to optimal. This latter result has an important practical consequence: Not only does imitating a near-optimal expert result in a better policy, but far fewer demonstrations are required to successfully imitate such an expert. This suggests an opportunity for substantial savings whenever the expert is known to be good, but demonstrations are expensive or difﬁcult to obtain. 1</p><p>15 <a title="nips-2010-15" href="../nips2010/nips-2010-A_Theory_of_Multiclass_Boosting.html">nips-2010-A Theory of Multiclass Boosting</a></p>
<p>Author: Indraneel Mukherjee, Robert E. Schapire</p><p>Abstract: Boosting combines weak classiﬁers to form highly accurate predictors. Although the case of binary classiﬁcation is well understood, in the multiclass setting, the “correct” requirements on the weak classiﬁer, or the notion of the most efﬁcient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classiﬁer, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements. 1</p><p>16 <a title="nips-2010-16" href="../nips2010/nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>Author: Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz</p><p>Abstract: We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. 1</p><p>17 <a title="nips-2010-17" href="../nips2010/nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>Author: Kritika Muralidharan, Nuno Vasconcelos</p><p>Abstract: The determination of dominant orientation at a given image location is formulated as a decision-theoretic question. This leads to a novel measure for the dominance of a given orientation θ, which is similar to that used by SIFT. It is then shown that the new measure can be computed with a network that implements the sequence of operations of the standard neurophysiological model of V1. The measure can thus be seen as a biologically plausible version of SIFT, and is denoted as bioSIFT. The network units are shown to exhibit trademark properties of V1 neurons, such as cross-orientation suppression, sparseness and independence. The connection between SIFT and biological vision provides a justiﬁcation for the success of SIFT-like features and reinforces the importance of contrast normalization in computer vision. We illustrate this by replacing the Gabor units of an HMAX network with the new bioSIFT units. This is shown to lead to significant gains for classiﬁcation tasks, leading to state-of-the-art performance among biologically inspired network models and performance competitive with the best non-biological object recognition systems. 1</p><p>18 <a title="nips-2010-18" href="../nips2010/nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>Author: Sohan Seth, Park Il, Austin Brockmeier, Mulugeta Semework, John Choi, Joseph Francis, Jose Principe</p><p>Abstract: Hypothesis testing on point processes has several applications such as model ﬁtting, plasticity detection, and non-stationarity detection. Standard tools for hypothesis testing include tests on mean ﬁring rate and time varying rate function. However, these statistics do not fully describe a point process, and therefore, the conclusions drawn by these tests can be misleading. In this paper, we introduce a family of non-parametric divergence measures for hypothesis testing. A divergence measure compares the full probability structure and, therefore, leads to a more robust test of hypothesis. We extend the traditional Kolmogorov–Smirnov and Cram´ r–von-Mises tests to the space of spike trains via stratiﬁcation, and e show that these statistics can be consistently estimated from data without any free parameter. We demonstrate an application of the proposed divergences as a cost function to ﬁnd optimally matched point processes. 1</p><p>19 <a title="nips-2010-19" href="../nips2010/nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>Author: Pradeep Shenoy, Angela J. Yu, Rajesh P. Rao</p><p>Abstract: Intelligent agents are often faced with the need to choose actions with uncertain consequences, and to modify those actions according to ongoing sensory processing and changing task demands. The requisite ability to dynamically modify or cancel planned actions is known as inhibitory control in psychology. We formalize inhibitory control as a rational decision-making problem, and apply to it to the classical stop-signal task. Using Bayesian inference and stochastic control tools, we show that the optimal policy systematically depends on various parameters of the problem, such as the relative costs of different action choices, the noise level of sensory inputs, and the dynamics of changing environmental demands. Our normative model accounts for a range of behavioral data in humans and animals in the stop-signal task, suggesting that the brain implements statistically optimal, dynamically adaptive, and reward-sensitive decision-making in the context of inhibitory control problems. 1</p><p>20 <a title="nips-2010-20" href="../nips2010/nips-2010-A_unified_model_of_short-range_and_long-range_motion_perception.html">nips-2010-A unified model of short-range and long-range motion perception</a></p>
<p>Author: Shuang Wu, Xuming He, Hongjing Lu, Alan L. Yuille</p><p>Abstract: The human vision system is able to effortlessly perceive both short-range and long-range motion patterns in complex dynamic scenes. Previous work has assumed that two different mechanisms are involved in processing these two types of motion. In this paper, we propose a hierarchical model as a uniﬁed framework for modeling both short-range and long-range motion perception. Our model consists of two key components: a data likelihood that proposes multiple motion hypotheses using nonlinear matching, and a hierarchical prior that imposes slowness and spatial smoothness constraints on the motion ﬁeld at multiple scales. We tested our model on two types of stimuli, random dot kinematograms and multiple-aperture stimuli, both commonly used in human vision research. We demonstrate that the hierarchical model adequately accounts for human performance in psychophysical experiments.</p><p>21 <a title="nips-2010-21" href="../nips2010/nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>22 <a title="nips-2010-22" href="../nips2010/nips-2010-Active_Estimation_of_F-Measures.html">nips-2010-Active Estimation of F-Measures</a></p>
<p>23 <a title="nips-2010-23" href="../nips2010/nips-2010-Active_Instance_Sampling_via_Matrix_Partition.html">nips-2010-Active Instance Sampling via Matrix Partition</a></p>
<p>24 <a title="nips-2010-24" href="../nips2010/nips-2010-Active_Learning_Applied_to_Patient-Adaptive_Heartbeat_Classification.html">nips-2010-Active Learning Applied to Patient-Adaptive Heartbeat Classification</a></p>
<p>25 <a title="nips-2010-25" href="../nips2010/nips-2010-Active_Learning_by_Querying_Informative_and_Representative_Examples.html">nips-2010-Active Learning by Querying Informative and Representative Examples</a></p>
<p>26 <a title="nips-2010-26" href="../nips2010/nips-2010-Adaptive_Multi-Task_Lasso%3A_with_Application_to_eQTL_Detection.html">nips-2010-Adaptive Multi-Task Lasso: with Application to eQTL Detection</a></p>
<p>27 <a title="nips-2010-27" href="../nips2010/nips-2010-Agnostic_Active_Learning_Without_Constraints.html">nips-2010-Agnostic Active Learning Without Constraints</a></p>
<p>28 <a title="nips-2010-28" href="../nips2010/nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<p>29 <a title="nips-2010-29" href="../nips2010/nips-2010-An_Approximate_Inference_Approach_to_Temporal_Optimization_in_Optimal_Control.html">nips-2010-An Approximate Inference Approach to Temporal Optimization in Optimal Control</a></p>
<p>30 <a title="nips-2010-30" href="../nips2010/nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>31 <a title="nips-2010-31" href="../nips2010/nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>32 <a title="nips-2010-32" href="../nips2010/nips-2010-Approximate_Inference_by_Compilation_to_Arithmetic_Circuits.html">nips-2010-Approximate Inference by Compilation to Arithmetic Circuits</a></p>
<p>33 <a title="nips-2010-33" href="../nips2010/nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>34 <a title="nips-2010-34" href="../nips2010/nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>35 <a title="nips-2010-35" href="../nips2010/nips-2010-Auto-Regressive_HMM_Inference_with_Incomplete_Data_for_Short-Horizon_Wind_Forecasting.html">nips-2010-Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting</a></p>
<p>36 <a title="nips-2010-36" href="../nips2010/nips-2010-Avoiding_False_Positive_in_Multi-Instance_Learning.html">nips-2010-Avoiding False Positive in Multi-Instance Learning</a></p>
<p>37 <a title="nips-2010-37" href="../nips2010/nips-2010-Basis_Construction_from_Power_Series_Expansions_of_Value_Functions.html">nips-2010-Basis Construction from Power Series Expansions of Value Functions</a></p>
<p>38 <a title="nips-2010-38" href="../nips2010/nips-2010-Batch_Bayesian_Optimization_via_Simulation_Matching.html">nips-2010-Batch Bayesian Optimization via Simulation Matching</a></p>
<p>39 <a title="nips-2010-39" href="../nips2010/nips-2010-Bayesian_Action-Graph_Games.html">nips-2010-Bayesian Action-Graph Games</a></p>
<p>40 <a title="nips-2010-40" href="../nips2010/nips-2010-Beyond_Actions%3A_Discriminative_Models_for_Contextual_Group_Activities.html">nips-2010-Beyond Actions: Discriminative Models for Contextual Group Activities</a></p>
<p>41 <a title="nips-2010-41" href="../nips2010/nips-2010-Block_Variable_Selection_in_Multivariate_Regression_and_High-dimensional_Causal_Inference.html">nips-2010-Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference</a></p>
<p>42 <a title="nips-2010-42" href="../nips2010/nips-2010-Boosting_Classifier_Cascades.html">nips-2010-Boosting Classifier Cascades</a></p>
<p>43 <a title="nips-2010-43" href="../nips2010/nips-2010-Bootstrapping_Apprenticeship_Learning.html">nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>44 <a title="nips-2010-44" href="../nips2010/nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>45 <a title="nips-2010-45" href="../nips2010/nips-2010-CUR_from_a_Sparse_Optimization_Viewpoint.html">nips-2010-CUR from a Sparse Optimization Viewpoint</a></p>
<p>46 <a title="nips-2010-46" href="../nips2010/nips-2010-Causal_discovery_in_multiple_models_from_different_experiments.html">nips-2010-Causal discovery in multiple models from different experiments</a></p>
<p>47 <a title="nips-2010-47" href="../nips2010/nips-2010-Co-regularization_Based_Semi-supervised_Domain_Adaptation.html">nips-2010-Co-regularization Based Semi-supervised Domain Adaptation</a></p>
<p>48 <a title="nips-2010-48" href="../nips2010/nips-2010-Collaborative_Filtering_in_a_Non-Uniform_World%3A_Learning_with_the_Weighted_Trace_Norm.html">nips-2010-Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm</a></p>
<p>49 <a title="nips-2010-49" href="../nips2010/nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>50 <a title="nips-2010-50" href="../nips2010/nips-2010-Constructing_Skill_Trees_for_Reinforcement_Learning_Agents_from_Demonstration_Trajectories.html">nips-2010-Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories</a></p>
<p>51 <a title="nips-2010-51" href="../nips2010/nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>52 <a title="nips-2010-52" href="../nips2010/nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<p>53 <a title="nips-2010-53" href="../nips2010/nips-2010-Copula_Bayesian_Networks.html">nips-2010-Copula Bayesian Networks</a></p>
<p>54 <a title="nips-2010-54" href="../nips2010/nips-2010-Copula_Processes.html">nips-2010-Copula Processes</a></p>
<p>55 <a title="nips-2010-55" href="../nips2010/nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>56 <a title="nips-2010-56" href="../nips2010/nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>57 <a title="nips-2010-57" href="../nips2010/nips-2010-Decoding_Ipsilateral_Finger_Movements_from_ECoG_Signals_in_Humans.html">nips-2010-Decoding Ipsilateral Finger Movements from ECoG Signals in Humans</a></p>
<p>58 <a title="nips-2010-58" href="../nips2010/nips-2010-Decomposing_Isotonic_Regression_for_Efficiently_Solving_Large_Problems.html">nips-2010-Decomposing Isotonic Regression for Efficiently Solving Large Problems</a></p>
<p>59 <a title="nips-2010-59" href="../nips2010/nips-2010-Deep_Coding_Network.html">nips-2010-Deep Coding Network</a></p>
<p>60 <a title="nips-2010-60" href="../nips2010/nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>61 <a title="nips-2010-61" href="../nips2010/nips-2010-Direct_Loss_Minimization_for_Structured_Prediction.html">nips-2010-Direct Loss Minimization for Structured Prediction</a></p>
<p>62 <a title="nips-2010-62" href="../nips2010/nips-2010-Discriminative_Clustering_by_Regularized_Information_Maximization.html">nips-2010-Discriminative Clustering by Regularized Information Maximization</a></p>
<p>63 <a title="nips-2010-63" href="../nips2010/nips-2010-Distributed_Dual_Averaging_In_Networks.html">nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>64 <a title="nips-2010-64" href="../nips2010/nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>65 <a title="nips-2010-65" href="../nips2010/nips-2010-Divisive_Normalization%3A_Justification_and_Effectiveness_as_Efficient_Coding_Transform.html">nips-2010-Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform</a></p>
<p>66 <a title="nips-2010-66" href="../nips2010/nips-2010-Double_Q-learning.html">nips-2010-Double Q-learning</a></p>
<p>67 <a title="nips-2010-67" href="../nips2010/nips-2010-Dynamic_Infinite_Relational_Model_for_Time-varying_Relational_Data_Analysis.html">nips-2010-Dynamic Infinite Relational Model for Time-varying Relational Data Analysis</a></p>
<p>68 <a title="nips-2010-68" href="../nips2010/nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>69 <a title="nips-2010-69" href="../nips2010/nips-2010-Efficient_Minimization_of_Decomposable_Submodular_Functions.html">nips-2010-Efficient Minimization of Decomposable Submodular Functions</a></p>
<p>70 <a title="nips-2010-70" href="../nips2010/nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>71 <a title="nips-2010-71" href="../nips2010/nips-2010-Efficient_Relational_Learning_with_Hidden_Variable_Detection.html">nips-2010-Efficient Relational Learning with Hidden Variable Detection</a></p>
<p>72 <a title="nips-2010-72" href="../nips2010/nips-2010-Efficient_algorithms_for_learning_kernels_from_multiple_similarity_matrices_with_general_convex_loss_functions.html">nips-2010-Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions</a></p>
<p>73 <a title="nips-2010-73" href="../nips2010/nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>74 <a title="nips-2010-74" href="../nips2010/nips-2010-Empirical_Bernstein_Inequalities_for_U-Statistics.html">nips-2010-Empirical Bernstein Inequalities for U-Statistics</a></p>
<p>75 <a title="nips-2010-75" href="../nips2010/nips-2010-Empirical_Risk_Minimization_with_Approximations_of_Probabilistic_Grammars.html">nips-2010-Empirical Risk Minimization with Approximations of Probabilistic Grammars</a></p>
<p>76 <a title="nips-2010-76" href="../nips2010/nips-2010-Energy_Disaggregation_via_Discriminative_Sparse_Coding.html">nips-2010-Energy Disaggregation via Discriminative Sparse Coding</a></p>
<p>77 <a title="nips-2010-77" href="../nips2010/nips-2010-Epitome_driven_3-D_Diffusion_Tensor_image_segmentation%3A_on_extracting_specific_structures.html">nips-2010-Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures</a></p>
<p>78 <a title="nips-2010-78" href="../nips2010/nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">nips-2010-Error Propagation for Approximate Policy and Value Iteration</a></p>
<p>79 <a title="nips-2010-79" href="../nips2010/nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>80 <a title="nips-2010-80" href="../nips2010/nips-2010-Estimation_of_Renyi_Entropy_and_Mutual_Information_Based_on_Generalized_Nearest-Neighbor_Graphs.html">nips-2010-Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs</a></p>
<p>81 <a title="nips-2010-81" href="../nips2010/nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>82 <a title="nips-2010-82" href="../nips2010/nips-2010-Evaluation_of_Rarity_of_Fingerprints_in_Forensics.html">nips-2010-Evaluation of Rarity of Fingerprints in Forensics</a></p>
<p>83 <a title="nips-2010-83" href="../nips2010/nips-2010-Evidence-Specific_Structures_for_Rich_Tractable_CRFs.html">nips-2010-Evidence-Specific Structures for Rich Tractable CRFs</a></p>
<p>84 <a title="nips-2010-84" href="../nips2010/nips-2010-Exact_inference_and_learning_for_cumulative_distribution_functions_on_loopy_graphs.html">nips-2010-Exact inference and learning for cumulative distribution functions on loopy graphs</a></p>
<p>85 <a title="nips-2010-85" href="../nips2010/nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>86 <a title="nips-2010-86" href="../nips2010/nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>87 <a title="nips-2010-87" href="../nips2010/nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>88 <a title="nips-2010-88" href="../nips2010/nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>89 <a title="nips-2010-89" href="../nips2010/nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>90 <a title="nips-2010-90" href="../nips2010/nips-2010-Fast_Large-scale_Mixture_Modeling_with_Component-specific_Data_Partitions.html">nips-2010-Fast Large-scale Mixture Modeling with Component-specific Data Partitions</a></p>
<p>91 <a title="nips-2010-91" href="../nips2010/nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS.html">nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</a></p>
<p>92 <a title="nips-2010-92" href="../nips2010/nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>93 <a title="nips-2010-93" href="../nips2010/nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>94 <a title="nips-2010-94" href="../nips2010/nips-2010-Feature_Set_Embedding_for_Incomplete_Data.html">nips-2010-Feature Set Embedding for Incomplete Data</a></p>
<p>95 <a title="nips-2010-95" href="../nips2010/nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>96 <a title="nips-2010-96" href="../nips2010/nips-2010-Fractionally_Predictive_Spiking_Neurons.html">nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>97 <a title="nips-2010-97" href="../nips2010/nips-2010-Functional_Geometry_Alignment_and_Localization_of_Brain_Areas.html">nips-2010-Functional Geometry Alignment and Localization of Brain Areas</a></p>
<p>98 <a title="nips-2010-98" href="../nips2010/nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>99 <a title="nips-2010-99" href="../nips2010/nips-2010-Gated_Softmax_Classification.html">nips-2010-Gated Softmax Classification</a></p>
<p>100 <a title="nips-2010-100" href="../nips2010/nips-2010-Gaussian_Process_Preference_Elicitation.html">nips-2010-Gaussian Process Preference Elicitation</a></p>
<p>101 <a title="nips-2010-101" href="../nips2010/nips-2010-Gaussian_sampling_by_local_perturbations.html">nips-2010-Gaussian sampling by local perturbations</a></p>
<p>102 <a title="nips-2010-102" href="../nips2010/nips-2010-Generalized_roof_duality_and_bisubmodular_functions.html">nips-2010-Generalized roof duality and bisubmodular functions</a></p>
<p>103 <a title="nips-2010-103" href="../nips2010/nips-2010-Generating_more_realistic_images_using_gated_MRF%27s.html">nips-2010-Generating more realistic images using gated MRF's</a></p>
<p>104 <a title="nips-2010-104" href="../nips2010/nips-2010-Generative_Local_Metric_Learning_for_Nearest_Neighbor_Classification.html">nips-2010-Generative Local Metric Learning for Nearest Neighbor Classification</a></p>
<p>105 <a title="nips-2010-105" href="../nips2010/nips-2010-Getting_lost_in_space%3A_Large_sample_analysis_of_the_resistance_distance.html">nips-2010-Getting lost in space: Large sample analysis of the resistance distance</a></p>
<p>106 <a title="nips-2010-106" href="../nips2010/nips-2010-Global_Analytic_Solution_for_Variational_Bayesian_Matrix_Factorization.html">nips-2010-Global Analytic Solution for Variational Bayesian Matrix Factorization</a></p>
<p>107 <a title="nips-2010-107" href="../nips2010/nips-2010-Global_seismic_monitoring_as_probabilistic_inference.html">nips-2010-Global seismic monitoring as probabilistic inference</a></p>
<p>108 <a title="nips-2010-108" href="../nips2010/nips-2010-Graph-Valued_Regression.html">nips-2010-Graph-Valued Regression</a></p>
<p>109 <a title="nips-2010-109" href="../nips2010/nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>110 <a title="nips-2010-110" href="../nips2010/nips-2010-Guaranteed_Rank_Minimization_via_Singular_Value_Projection.html">nips-2010-Guaranteed Rank Minimization via Singular Value Projection</a></p>
<p>111 <a title="nips-2010-111" href="../nips2010/nips-2010-Hallucinations_in_Charles_Bonnet_Syndrome_Induced_by_Homeostasis%3A_a_Deep_Boltzmann_Machine_Model.html">nips-2010-Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model</a></p>
<p>112 <a title="nips-2010-112" href="../nips2010/nips-2010-Hashing_Hyperplane_Queries_to_Near_Points_with_Applications_to_Large-Scale_Active_Learning.html">nips-2010-Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning</a></p>
<p>113 <a title="nips-2010-113" href="../nips2010/nips-2010-Heavy-Tailed_Process_Priors_for_Selective_Shrinkage.html">nips-2010-Heavy-Tailed Process Priors for Selective Shrinkage</a></p>
<p>114 <a title="nips-2010-114" href="../nips2010/nips-2010-Humans_Learn_Using_Manifolds%2C_Reluctantly.html">nips-2010-Humans Learn Using Manifolds, Reluctantly</a></p>
<p>115 <a title="nips-2010-115" href="../nips2010/nips-2010-Identifying_Dendritic_Processing.html">nips-2010-Identifying Dendritic Processing</a></p>
<p>116 <a title="nips-2010-116" href="../nips2010/nips-2010-Identifying_Patients_at_Risk_of_Major_Adverse_Cardiovascular_Events_Using_Symbolic_Mismatch.html">nips-2010-Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch</a></p>
<p>117 <a title="nips-2010-117" href="../nips2010/nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>118 <a title="nips-2010-118" href="../nips2010/nips-2010-Implicit_Differentiation_by_Perturbation.html">nips-2010-Implicit Differentiation by Perturbation</a></p>
<p>119 <a title="nips-2010-119" href="../nips2010/nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>120 <a title="nips-2010-120" href="../nips2010/nips-2010-Improvements_to_the_Sequence_Memoizer.html">nips-2010-Improvements to the Sequence Memoizer</a></p>
<p>121 <a title="nips-2010-121" href="../nips2010/nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>122 <a title="nips-2010-122" href="../nips2010/nips-2010-Improving_the_Asymptotic_Performance_of_Markov_Chain_Monte-Carlo_by_Inserting_Vortices.html">nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</a></p>
<p>123 <a title="nips-2010-123" href="../nips2010/nips-2010-Individualized_ROI_Optimization_via_Maximization_of_Group-wise_Consistency_of_Structural_and_Functional_Profiles.html">nips-2010-Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles</a></p>
<p>124 <a title="nips-2010-124" href="../nips2010/nips-2010-Inductive_Regularized_Learning_of_Kernel_Functions.html">nips-2010-Inductive Regularized Learning of Kernel Functions</a></p>
<p>125 <a title="nips-2010-125" href="../nips2010/nips-2010-Inference_and_communication_in_the_game_of_Password.html">nips-2010-Inference and communication in the game of Password</a></p>
<p>126 <a title="nips-2010-126" href="../nips2010/nips-2010-Inference_with_Multivariate_Heavy-Tails_in_Linear_Models.html">nips-2010-Inference with Multivariate Heavy-Tails in Linear Models</a></p>
<p>127 <a title="nips-2010-127" href="../nips2010/nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>128 <a title="nips-2010-128" href="../nips2010/nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>129 <a title="nips-2010-129" href="../nips2010/nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>130 <a title="nips-2010-130" href="../nips2010/nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>131 <a title="nips-2010-131" href="../nips2010/nips-2010-Joint_Analysis_of_Time-Evolving_Binary_Matrices_and_Associated_Documents.html">nips-2010-Joint Analysis of Time-Evolving Binary Matrices and Associated Documents</a></p>
<p>132 <a title="nips-2010-132" href="../nips2010/nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>133 <a title="nips-2010-133" href="../nips2010/nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>134 <a title="nips-2010-134" href="../nips2010/nips-2010-LSTD_with_Random_Projections.html">nips-2010-LSTD with Random Projections</a></p>
<p>135 <a title="nips-2010-135" href="../nips2010/nips-2010-Label_Embedding_Trees_for_Large_Multi-Class_Tasks.html">nips-2010-Label Embedding Trees for Large Multi-Class Tasks</a></p>
<p>136 <a title="nips-2010-136" href="../nips2010/nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>137 <a title="nips-2010-137" href="../nips2010/nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>138 <a title="nips-2010-138" href="../nips2010/nips-2010-Large_Margin_Multi-Task_Metric_Learning.html">nips-2010-Large Margin Multi-Task Metric Learning</a></p>
<p>139 <a title="nips-2010-139" href="../nips2010/nips-2010-Latent_Variable_Models_for_Predicting_File_Dependencies_in_Large-Scale_Software_Development.html">nips-2010-Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development</a></p>
<p>140 <a title="nips-2010-140" href="../nips2010/nips-2010-Layer-wise_analysis_of_deep_networks_with_Gaussian_kernels.html">nips-2010-Layer-wise analysis of deep networks with Gaussian kernels</a></p>
<p>141 <a title="nips-2010-141" href="../nips2010/nips-2010-Layered_image_motion_with_explicit_occlusions%2C_temporal_consistency%2C_and_depth_ordering.html">nips-2010-Layered image motion with explicit occlusions, temporal consistency, and depth ordering</a></p>
<p>142 <a title="nips-2010-142" href="../nips2010/nips-2010-Learning_Bounds_for_Importance_Weighting.html">nips-2010-Learning Bounds for Importance Weighting</a></p>
<p>143 <a title="nips-2010-143" href="../nips2010/nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>144 <a title="nips-2010-144" href="../nips2010/nips-2010-Learning_Efficient_Markov_Networks.html">nips-2010-Learning Efficient Markov Networks</a></p>
<p>145 <a title="nips-2010-145" href="../nips2010/nips-2010-Learning_Kernels_with_Radiuses_of_Minimum_Enclosing_Balls.html">nips-2010-Learning Kernels with Radiuses of Minimum Enclosing Balls</a></p>
<p>146 <a title="nips-2010-146" href="../nips2010/nips-2010-Learning_Multiple_Tasks_using_Manifold_Regularization.html">nips-2010-Learning Multiple Tasks using Manifold Regularization</a></p>
<p>147 <a title="nips-2010-147" href="../nips2010/nips-2010-Learning_Multiple_Tasks_with_a_Sparse_Matrix-Normal_Penalty.html">nips-2010-Learning Multiple Tasks with a Sparse Matrix-Normal Penalty</a></p>
<p>148 <a title="nips-2010-148" href="../nips2010/nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>149 <a title="nips-2010-149" href="../nips2010/nips-2010-Learning_To_Count_Objects_in_Images.html">nips-2010-Learning To Count Objects in Images</a></p>
<p>150 <a title="nips-2010-150" href="../nips2010/nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>151 <a title="nips-2010-151" href="../nips2010/nips-2010-Learning_from_Candidate_Labeling_Sets.html">nips-2010-Learning from Candidate Labeling Sets</a></p>
<p>152 <a title="nips-2010-152" href="../nips2010/nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>153 <a title="nips-2010-153" href="../nips2010/nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>154 <a title="nips-2010-154" href="../nips2010/nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</a></p>
<p>155 <a title="nips-2010-155" href="../nips2010/nips-2010-Learning_the_context_of_a_category.html">nips-2010-Learning the context of a category</a></p>
<p>156 <a title="nips-2010-156" href="../nips2010/nips-2010-Learning_to_combine_foveal_glimpses_with_a_third-order_Boltzmann_machine.html">nips-2010-Learning to combine foveal glimpses with a third-order Boltzmann machine</a></p>
<p>157 <a title="nips-2010-157" href="../nips2010/nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>158 <a title="nips-2010-158" href="../nips2010/nips-2010-Learning_via_Gaussian_Herding.html">nips-2010-Learning via Gaussian Herding</a></p>
<p>159 <a title="nips-2010-159" href="../nips2010/nips-2010-Lifted_Inference_Seen_from_the_Other_Side_%3A_The_Tractable_Features.html">nips-2010-Lifted Inference Seen from the Other Side : The Tractable Features</a></p>
<p>160 <a title="nips-2010-160" href="../nips2010/nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>161 <a title="nips-2010-161" href="../nips2010/nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>162 <a title="nips-2010-162" href="../nips2010/nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>163 <a title="nips-2010-163" href="../nips2010/nips-2010-Lower_Bounds_on_Rate_of_Convergence_of_Cutting_Plane_Methods.html">nips-2010-Lower Bounds on Rate of Convergence of Cutting Plane Methods</a></p>
<p>164 <a title="nips-2010-164" href="../nips2010/nips-2010-MAP_Estimation_for_Graphical_Models_by_Likelihood_Maximization.html">nips-2010-MAP Estimation for Graphical Models by Likelihood Maximization</a></p>
<p>165 <a title="nips-2010-165" href="../nips2010/nips-2010-MAP_estimation_in_Binary_MRFs_via_Bipartite_Multi-cuts.html">nips-2010-MAP estimation in Binary MRFs via Bipartite Multi-cuts</a></p>
<p>166 <a title="nips-2010-166" href="../nips2010/nips-2010-Minimum_Average_Cost_Clustering.html">nips-2010-Minimum Average Cost Clustering</a></p>
<p>167 <a title="nips-2010-167" href="../nips2010/nips-2010-Mixture_of_time-warped_trajectory_models_for_movement_decoding.html">nips-2010-Mixture of time-warped trajectory models for movement decoding</a></p>
<p>168 <a title="nips-2010-168" href="../nips2010/nips-2010-Monte-Carlo_Planning_in_Large_POMDPs.html">nips-2010-Monte-Carlo Planning in Large POMDPs</a></p>
<p>169 <a title="nips-2010-169" href="../nips2010/nips-2010-More_data_means_less_inference%3A_A_pseudo-max_approach_to_structured_learning.html">nips-2010-More data means less inference: A pseudo-max approach to structured learning</a></p>
<p>170 <a title="nips-2010-170" href="../nips2010/nips-2010-Moreau-Yosida_Regularization_for_Grouped_Tree_Structure_Learning.html">nips-2010-Moreau-Yosida Regularization for Grouped Tree Structure Learning</a></p>
<p>171 <a title="nips-2010-171" href="../nips2010/nips-2010-Movement_extraction_by_detecting_dynamics_switches_and_repetitions.html">nips-2010-Movement extraction by detecting dynamics switches and repetitions</a></p>
<p>172 <a title="nips-2010-172" href="../nips2010/nips-2010-Multi-Stage_Dantzig_Selector.html">nips-2010-Multi-Stage Dantzig Selector</a></p>
<p>173 <a title="nips-2010-173" href="../nips2010/nips-2010-Multi-View_Active_Learning_in_the_Non-Realizable_Case.html">nips-2010-Multi-View Active Learning in the Non-Realizable Case</a></p>
<p>174 <a title="nips-2010-174" href="../nips2010/nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>175 <a title="nips-2010-175" href="../nips2010/nips-2010-Multiparty_Differential_Privacy_via_Aggregation_of_Locally_Trained_Classifiers.html">nips-2010-Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers</a></p>
<p>176 <a title="nips-2010-176" href="../nips2010/nips-2010-Multiple_Kernel_Learning_and_the_SMO_Algorithm.html">nips-2010-Multiple Kernel Learning and the SMO Algorithm</a></p>
<p>177 <a title="nips-2010-177" href="../nips2010/nips-2010-Multitask_Learning_without_Label_Correspondences.html">nips-2010-Multitask Learning without Label Correspondences</a></p>
<p>178 <a title="nips-2010-178" href="../nips2010/nips-2010-Multivariate_Dyadic_Regression_Trees_for_Sparse_Learning_Problems.html">nips-2010-Multivariate Dyadic Regression Trees for Sparse Learning Problems</a></p>
<p>179 <a title="nips-2010-179" href="../nips2010/nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>180 <a title="nips-2010-180" href="../nips2010/nips-2010-Near-Optimal_Bayesian_Active_Learning_with_Noisy_Observations.html">nips-2010-Near-Optimal Bayesian Active Learning with Noisy Observations</a></p>
<p>181 <a title="nips-2010-181" href="../nips2010/nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>182 <a title="nips-2010-182" href="../nips2010/nips-2010-New_Adaptive_Algorithms_for_Online_Classification.html">nips-2010-New Adaptive Algorithms for Online Classification</a></p>
<p>183 <a title="nips-2010-183" href="../nips2010/nips-2010-Non-Stochastic_Bandit_Slate_Problems.html">nips-2010-Non-Stochastic Bandit Slate Problems</a></p>
<p>184 <a title="nips-2010-184" href="../nips2010/nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>185 <a title="nips-2010-185" href="../nips2010/nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>186 <a title="nips-2010-186" href="../nips2010/nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>187 <a title="nips-2010-187" href="../nips2010/nips-2010-Occlusion_Detection_and_Motion_Estimation_with_Convex_Optimization.html">nips-2010-Occlusion Detection and Motion Estimation with Convex Optimization</a></p>
<p>188 <a title="nips-2010-188" href="../nips2010/nips-2010-On_Herding_and_the_Perceptron_Cycling_Theorem.html">nips-2010-On Herding and the Perceptron Cycling Theorem</a></p>
<p>189 <a title="nips-2010-189" href="../nips2010/nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>190 <a title="nips-2010-190" href="../nips2010/nips-2010-On_the_Convexity_of_Latent_Social_Network_Inference.html">nips-2010-On the Convexity of Latent Social Network Inference</a></p>
<p>191 <a title="nips-2010-191" href="../nips2010/nips-2010-On_the_Theory_of_Learnining_with_Privileged_Information.html">nips-2010-On the Theory of Learnining with Privileged Information</a></p>
<p>192 <a title="nips-2010-192" href="../nips2010/nips-2010-Online_Classification_with_Specificity_Constraints.html">nips-2010-Online Classification with Specificity Constraints</a></p>
<p>193 <a title="nips-2010-193" href="../nips2010/nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>194 <a title="nips-2010-194" href="../nips2010/nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>195 <a title="nips-2010-195" href="../nips2010/nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>196 <a title="nips-2010-196" href="../nips2010/nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>197 <a title="nips-2010-197" href="../nips2010/nips-2010-Optimal_Bayesian_Recommendation_Sets_and_Myopically_Optimal_Choice_Query_Sets.html">nips-2010-Optimal Bayesian Recommendation Sets and Myopically Optimal Choice Query Sets</a></p>
<p>198 <a title="nips-2010-198" href="../nips2010/nips-2010-Optimal_Web-Scale_Tiering_as_a_Flow_Problem.html">nips-2010-Optimal Web-Scale Tiering as a Flow Problem</a></p>
<p>199 <a title="nips-2010-199" href="../nips2010/nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>200 <a title="nips-2010-200" href="../nips2010/nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>201 <a title="nips-2010-201" href="../nips2010/nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>202 <a title="nips-2010-202" href="../nips2010/nips-2010-Parallelized_Stochastic_Gradient_Descent.html">nips-2010-Parallelized Stochastic Gradient Descent</a></p>
<p>203 <a title="nips-2010-203" href="../nips2010/nips-2010-Parametric_Bandits%3A_The_Generalized_Linear_Case.html">nips-2010-Parametric Bandits: The Generalized Linear Case</a></p>
<p>204 <a title="nips-2010-204" href="../nips2010/nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>205 <a title="nips-2010-205" href="../nips2010/nips-2010-Permutation_Complexity_Bound_on_Out-Sample_Error.html">nips-2010-Permutation Complexity Bound on Out-Sample Error</a></p>
<p>206 <a title="nips-2010-206" href="../nips2010/nips-2010-Phone_Recognition_with_the_Mean-Covariance_Restricted_Boltzmann_Machine.html">nips-2010-Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</a></p>
<p>207 <a title="nips-2010-207" href="../nips2010/nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>208 <a title="nips-2010-208" href="../nips2010/nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>209 <a title="nips-2010-209" href="../nips2010/nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<p>210 <a title="nips-2010-210" href="../nips2010/nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>211 <a title="nips-2010-211" href="../nips2010/nips-2010-Predicting_Execution_Time_of_Computer_Programs_Using_Sparse_Polynomial_Regression.html">nips-2010-Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</a></p>
<p>212 <a title="nips-2010-212" href="../nips2010/nips-2010-Predictive_State_Temporal_Difference_Learning.html">nips-2010-Predictive State Temporal Difference Learning</a></p>
<p>213 <a title="nips-2010-213" href="../nips2010/nips-2010-Predictive_Subspace_Learning_for_Multi-view_Data%3A_a_Large_Margin_Approach.html">nips-2010-Predictive Subspace Learning for Multi-view Data: a Large Margin Approach</a></p>
<p>214 <a title="nips-2010-214" href="../nips2010/nips-2010-Probabilistic_Belief_Revision_with_Structural_Constraints.html">nips-2010-Probabilistic Belief Revision with Structural Constraints</a></p>
<p>215 <a title="nips-2010-215" href="../nips2010/nips-2010-Probabilistic_Deterministic_Infinite_Automata.html">nips-2010-Probabilistic Deterministic Infinite Automata</a></p>
<p>216 <a title="nips-2010-216" href="../nips2010/nips-2010-Probabilistic_Inference_and_Differential_Privacy.html">nips-2010-Probabilistic Inference and Differential Privacy</a></p>
<p>217 <a title="nips-2010-217" href="../nips2010/nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>218 <a title="nips-2010-218" href="../nips2010/nips-2010-Probabilistic_latent_variable_models_for_distinguishing_between_cause_and_effect.html">nips-2010-Probabilistic latent variable models for distinguishing between cause and effect</a></p>
<p>219 <a title="nips-2010-219" href="../nips2010/nips-2010-Random_Conic_Pursuit_for_Semidefinite_Programming.html">nips-2010-Random Conic Pursuit for Semidefinite Programming</a></p>
<p>220 <a title="nips-2010-220" href="../nips2010/nips-2010-Random_Projection_Trees_Revisited.html">nips-2010-Random Projection Trees Revisited</a></p>
<p>221 <a title="nips-2010-221" href="../nips2010/nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>222 <a title="nips-2010-222" href="../nips2010/nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>223 <a title="nips-2010-223" href="../nips2010/nips-2010-Rates_of_convergence_for_the_cluster_tree.html">nips-2010-Rates of convergence for the cluster tree</a></p>
<p>224 <a title="nips-2010-224" href="../nips2010/nips-2010-Regularized_estimation_of_image_statistics_by_Score_Matching.html">nips-2010-Regularized estimation of image statistics by Score Matching</a></p>
<p>225 <a title="nips-2010-225" href="../nips2010/nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification.html">nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</a></p>
<p>226 <a title="nips-2010-226" href="../nips2010/nips-2010-Repeated_Games_against_Budgeted_Adversaries.html">nips-2010-Repeated Games against Budgeted Adversaries</a></p>
<p>227 <a title="nips-2010-227" href="../nips2010/nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>228 <a title="nips-2010-228" href="../nips2010/nips-2010-Reverse_Multi-Label_Learning.html">nips-2010-Reverse Multi-Label Learning</a></p>
<p>229 <a title="nips-2010-229" href="../nips2010/nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>230 <a title="nips-2010-230" href="../nips2010/nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>231 <a title="nips-2010-231" href="../nips2010/nips-2010-Robust_PCA_via_Outlier_Pursuit.html">nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>232 <a title="nips-2010-232" href="../nips2010/nips-2010-Sample_Complexity_of_Testing_the_Manifold_Hypothesis.html">nips-2010-Sample Complexity of Testing the Manifold Hypothesis</a></p>
<p>233 <a title="nips-2010-233" href="../nips2010/nips-2010-Scrambled_Objects_for_Least-Squares_Regression.html">nips-2010-Scrambled Objects for Least-Squares Regression</a></p>
<p>234 <a title="nips-2010-234" href="../nips2010/nips-2010-Segmentation_as_Maximum-Weight_Independent_Set.html">nips-2010-Segmentation as Maximum-Weight Independent Set</a></p>
<p>235 <a title="nips-2010-235" href="../nips2010/nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<p>236 <a title="nips-2010-236" href="../nips2010/nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>237 <a title="nips-2010-237" href="../nips2010/nips-2010-Shadow_Dirichlet_for_Restricted_Probability_Modeling.html">nips-2010-Shadow Dirichlet for Restricted Probability Modeling</a></p>
<p>238 <a title="nips-2010-238" href="../nips2010/nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>239 <a title="nips-2010-239" href="../nips2010/nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<p>240 <a title="nips-2010-240" href="../nips2010/nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>241 <a title="nips-2010-241" href="../nips2010/nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>242 <a title="nips-2010-242" href="../nips2010/nips-2010-Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models.html">nips-2010-Slice sampling covariance hyperparameters of latent Gaussian models</a></p>
<p>243 <a title="nips-2010-243" href="../nips2010/nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>244 <a title="nips-2010-244" href="../nips2010/nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>245 <a title="nips-2010-245" href="../nips2010/nips-2010-Space-Variant_Single-Image_Blind_Deconvolution_for_Removing_Camera_Shake.html">nips-2010-Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake</a></p>
<p>246 <a title="nips-2010-246" href="../nips2010/nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives.html">nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</a></p>
<p>247 <a title="nips-2010-247" href="../nips2010/nips-2010-Sparse_Instrumental_Variables_%28SPIV%29_for_Genome-Wide_Studies.html">nips-2010-Sparse Instrumental Variables (SPIV) for Genome-Wide Studies</a></p>
<p>248 <a title="nips-2010-248" href="../nips2010/nips-2010-Sparse_Inverse_Covariance_Selection_via_Alternating_Linearization_Methods.html">nips-2010-Sparse Inverse Covariance Selection via Alternating Linearization Methods</a></p>
<p>249 <a title="nips-2010-249" href="../nips2010/nips-2010-Spatial_and_anatomical_regularization_of_SVM_for_brain_image_analysis.html">nips-2010-Spatial and anatomical regularization of SVM for brain image analysis</a></p>
<p>250 <a title="nips-2010-250" href="../nips2010/nips-2010-Spectral_Regularization_for_Support_Estimation.html">nips-2010-Spectral Regularization for Support Estimation</a></p>
<p>251 <a title="nips-2010-251" href="../nips2010/nips-2010-Sphere_Embedding%3A_An_Application_to_Part-of-Speech_Induction.html">nips-2010-Sphere Embedding: An Application to Part-of-Speech Induction</a></p>
<p>252 <a title="nips-2010-252" href="../nips2010/nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>253 <a title="nips-2010-253" href="../nips2010/nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>254 <a title="nips-2010-254" href="../nips2010/nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<p>255 <a title="nips-2010-255" href="../nips2010/nips-2010-Static_Analysis_of_Binary_Executables_Using_Structural_SVMs.html">nips-2010-Static Analysis of Binary Executables Using Structural SVMs</a></p>
<p>256 <a title="nips-2010-256" href="../nips2010/nips-2010-Structural_epitome%3A_a_way_to_summarize_one%E2%80%99s_visual_experience.html">nips-2010-Structural epitome: a way to summarize one’s visual experience</a></p>
<p>257 <a title="nips-2010-257" href="../nips2010/nips-2010-Structured_Determinantal_Point_Processes.html">nips-2010-Structured Determinantal Point Processes</a></p>
<p>258 <a title="nips-2010-258" href="../nips2010/nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>259 <a title="nips-2010-259" href="../nips2010/nips-2010-Subgraph_Detection_Using_Eigenvector_L1_Norms.html">nips-2010-Subgraph Detection Using Eigenvector L1 Norms</a></p>
<p>260 <a title="nips-2010-260" href="../nips2010/nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>261 <a title="nips-2010-261" href="../nips2010/nips-2010-Supervised_Clustering.html">nips-2010-Supervised Clustering</a></p>
<p>262 <a title="nips-2010-262" href="../nips2010/nips-2010-Switched_Latent_Force_Models_for_Movement_Segmentation.html">nips-2010-Switched Latent Force Models for Movement Segmentation</a></p>
<p>263 <a title="nips-2010-263" href="../nips2010/nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>264 <a title="nips-2010-264" href="../nips2010/nips-2010-Synergies_in_learning_words_and_their_referents.html">nips-2010-Synergies in learning words and their referents</a></p>
<p>265 <a title="nips-2010-265" href="../nips2010/nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>266 <a title="nips-2010-266" href="../nips2010/nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>267 <a title="nips-2010-267" href="../nips2010/nips-2010-The_Multidimensional_Wisdom_of_Crowds.html">nips-2010-The Multidimensional Wisdom of Crowds</a></p>
<p>268 <a title="nips-2010-268" href="../nips2010/nips-2010-The_Neural_Costs_of_Optimal_Control.html">nips-2010-The Neural Costs of Optimal Control</a></p>
<p>269 <a title="nips-2010-269" href="../nips2010/nips-2010-Throttling_Poisson_Processes.html">nips-2010-Throttling Poisson Processes</a></p>
<p>270 <a title="nips-2010-270" href="../nips2010/nips-2010-Tight_Sample_Complexity_of_Large-Margin_Learning.html">nips-2010-Tight Sample Complexity of Large-Margin Learning</a></p>
<p>271 <a title="nips-2010-271" href="../nips2010/nips-2010-Tiled_convolutional_neural_networks.html">nips-2010-Tiled convolutional neural networks</a></p>
<p>272 <a title="nips-2010-272" href="../nips2010/nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>273 <a title="nips-2010-273" href="../nips2010/nips-2010-Towards_Property-Based_Classification_of_Clustering_Paradigms.html">nips-2010-Towards Property-Based Classification of Clustering Paradigms</a></p>
<p>274 <a title="nips-2010-274" href="../nips2010/nips-2010-Trading_off_Mistakes_and_Don%27t-Know_Predictions.html">nips-2010-Trading off Mistakes and Don't-Know Predictions</a></p>
<p>275 <a title="nips-2010-275" href="../nips2010/nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>276 <a title="nips-2010-276" href="../nips2010/nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>277 <a title="nips-2010-277" href="../nips2010/nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>278 <a title="nips-2010-278" href="../nips2010/nips-2010-Universal_Consistency_of_Multi-Class_Support_Vector_Classification.html">nips-2010-Universal Consistency of Multi-Class Support Vector Classification</a></p>
<p>279 <a title="nips-2010-279" href="../nips2010/nips-2010-Universal_Kernels_on_Non-Standard_Input_Spaces.html">nips-2010-Universal Kernels on Non-Standard Input Spaces</a></p>
<p>280 <a title="nips-2010-280" href="../nips2010/nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>281 <a title="nips-2010-281" href="../nips2010/nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<p>282 <a title="nips-2010-282" href="../nips2010/nips-2010-Variable_margin_losses_for_classifier_design.html">nips-2010-Variable margin losses for classifier design</a></p>
<p>283 <a title="nips-2010-283" href="../nips2010/nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>284 <a title="nips-2010-284" href="../nips2010/nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>285 <a title="nips-2010-285" href="../nips2010/nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>286 <a title="nips-2010-286" href="../nips2010/nips-2010-Word_Features_for_Latent_Dirichlet_Allocation.html">nips-2010-Word Features for Latent Dirichlet Allocation</a></p>
<p>287 <a title="nips-2010-287" href="../nips2010/nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>288 <a title="nips-2010-288" href="../nips2010/nips-2010-Worst-case_bounds_on_the_quality_of_max-product_fixed-points.html">nips-2010-Worst-case bounds on the quality of max-product fixed-points</a></p>
<p>289 <a title="nips-2010-289" href="../nips2010/nips-2010-b-Bit_Minwise_Hashing_for_Estimating_Three-Way_Similarities.html">nips-2010-b-Bit Minwise Hashing for Estimating Three-Way Similarities</a></p>
<p>290 <a title="nips-2010-290" href="../nips2010/nips-2010-t-logistic_regression.html">nips-2010-t-logistic regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
