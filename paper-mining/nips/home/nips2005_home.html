<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>nips 2005 knowledge graph</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="#">nips2005</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>nips 2005 knowledge graph</h1>
<br/><h3>similar papers computed by tfidf model</h3><br/><h3>similar papers computed by <a title="lsi-model" href="./nips2005_lsi.html">lsi model</a></h3><br/><h3>similar papers computed by <a title="lda-model" href="./nips2005_lda.html">lda model</a></h3><br/><h2>papers list:</h2><p>1 <a title="nips-2005-1" href="../nips2005/nips-2005-AER_Building_Blocks_for_Multi-Layer_Multi-Chip_Neuromorphic_Vision_Systems.html">nips-2005-AER Building Blocks for Multi-Layer Multi-Chip Neuromorphic Vision Systems</a></p>
<p>Author: R. Serrano-Gotarredona, M. Oster, P. Lichtsteiner, A. Linares-Barranco, R. Paz-Vicente, F. Gomez-Rodriguez, H. Kolle Riis, T. Delbruck, S. C. Liu, S. Zahnd, A. M. Whatley, R. Douglas, P. Hafliger, G. Jimenez-Moreno, A. Civit, T. Serrano-Gotarredona, A. Acosta-Jimenez, B. Linares-Barranco</p><p>Abstract: A 5-layer neuromorphic vision processor whose components communicate spike events asychronously using the address-eventrepresentation (AER) is demonstrated. The system includes a retina chip, two convolution chips, a 2D winner-take-all chip, a delay line chip, a learning classiﬁer chip, and a set of PCBs for computer interfacing and address space remappings. The components use a mixture of analog and digital computation and will learn to classify trajectories of a moving object. A complete experimental setup and measurements results are shown.</p><p>2 <a title="nips-2005-2" href="../nips2005/nips-2005-A_Bayes_Rule_for_Density_Matrices.html">nips-2005-A Bayes Rule for Density Matrices</a></p>
<p>Author: Manfred K. Warmuth</p><p>Abstract: The classical Bayes rule computes the posterior model probability from the prior probability and the data likelihood. We generalize this rule to the case when the prior is a density matrix (symmetric positive deﬁnite and trace one) and the data likelihood a covariance matrix. The classical Bayes rule is retained as the special case when the matrices are diagonal. In the classical setting, the calculation of the probability of the data is an expected likelihood, where the expectation is over the prior distribution. In the generalized setting, this is replaced by an expected variance calculation where the variance is computed along the eigenvectors of the prior density matrix and the expectation is over the eigenvalues of the density matrix (which form a probability vector). The variances along any direction is determined by the covariance matrix. Curiously enough this expected variance calculation is a quantum measurement where the co-variance matrix speciﬁes the instrument and the prior density matrix the mixture state of the particle. We motivate both the classical and the generalized Bayes rule with a minimum relative entropy principle, where the Kullbach-Leibler version gives the classical Bayes rule and Umegaki’s quantum relative entropy the new Bayes rule for density matrices. 1</p><p>3 <a title="nips-2005-3" href="../nips2005/nips-2005-A_Bayesian_Framework_for_Tilt_Perception_and_Confidence.html">nips-2005-A Bayesian Framework for Tilt Perception and Confidence</a></p>
<p>Author: Odelia Schwartz, Peter Dayan, Terrence J. Sejnowski</p><p>Abstract: The misjudgement of tilt in images lies at the heart of entertaining visual illusions and rigorous perceptual psychophysics. A wealth of ﬁndings has attracted many mechanistic models, but few clear computational principles. We adopt a Bayesian approach to perceptual tilt estimation, showing how a smoothness prior offers a powerful way of addressing much confusing data. In particular, we faithfully model recent results showing that conﬁdence in estimation can be systematically affected by the same aspects of images that affect bias. Conﬁdence is central to Bayesian modeling approaches, and is applicable in many other perceptual domains. Perceptual anomalies and illusions, such as the misjudgements of motion and tilt evident in so many psychophysical experiments, have intrigued researchers for decades.1–3 A Bayesian view4–8 has been particularly inﬂuential in models of motion processing, treating such anomalies as the normative product of prior information (often statistically codifying Gestalt laws) with likelihood information from the actual scenes presented. Here, we expand the range of statistically normative accounts to tilt estimation, for which there are classes of results (on estimation conﬁdence) that are so far not available for motion. The tilt illusion arises when the perceived tilt of a center target is misjudged (ie bias) in the presence of ﬂankers. Another phenomenon, called Crowding, refers to a loss in the conﬁdence (ie sensitivity) of perceived target tilt in the presence of ﬂankers. Attempts have been made to formalize these phenomena quantitatively. Crowding has been modeled as compulsory feature pooling (ie averaging of orientations), ignoring spatial positions.9, 10 The tilt illusion has been explained by lateral interactions11, 12 in populations of orientationtuned units; and by calibration.13 However, most models of this form cannot explain a number of crucial aspects of the data. First, the geometry of the positional arrangement of the stimuli affects attraction versus repulsion in bias, as emphasized by Kapadia et al14 (ﬁgure 1A), and others.15, 16 Second, Solomon et al. recently measured bias and sensitivity simultaneously.11 The rich and surprising range of sensitivities, far from ﬂat as a function of ﬂanker angles (ﬁgure 1B), are outside the reach of standard models. Moreover, current explanations do not offer a computational account of tilt perception as the outcome of a normative inference process. Here, we demonstrate that a Bayesian framework for orientation estimation, with a prior favoring smoothness, can naturally explain a range of seemingly puzzling tilt data. We explicitly consider both the geometry of the stimuli, and the issue of conﬁdence in the esti- 6 5 4 3 2 1 0 -1 -2 (B) Attraction Repulsion Sensititvity (1/deg) Bias (deg) (A) 0.6 0.5 0.4 0.3 0.2 0.1 -80 -60 -40 -20 0 20 40 60 80 Flanker tilt (deg) Figure 1: Tilt biases and sensitivities in visual perception. (A) Kapadia et al demonstrated the importance of geometry on tilt bias, with bar stimuli in the fovea (and similar results in the periphery). When 5 degrees clockwise ﬂankers are arranged colinearly, the center target appears attracted in the direction of the ﬂankers; when ﬂankers are lateral, the target appears repulsed. Data are an average of 5 subjects.14 (B) Solomon et al measured both biases and sensitivities for gratings in the visual periphery.11 On the top are example stimuli, with ﬂankers tilted 22.5 degrees clockwise. This constitutes the classic tilt illusion, with a repulsive bias percept. In addition, sensitivities vary as a function of ﬂanker angles, in a systematic way (even in cases when there are no biases at all). Sensitivities are given in units of the inverse of standard deviation of the tilt estimate. More detailed data for both experiments are shown in the results section. mation. Bayesian analyses have most frequently been applied to bias. Much less attention has been paid to the equally important phenomenon of sensitivity. This aspect of our model should be applicable to other perceptual domains. In section 1 we formulate the Bayesian model. The prior is determined by the principle of creating a smooth contour between the target and ﬂankers. We describe how to extract the bias and sensitivity. In section 2 we show experimental data of Kapadia et al and Solomon et al, alongside the model simulations, and demonstrate that the model can account for both geometry, and bias and sensitivity measurements in the data. Our results suggest a more uniﬁed, rational, approach to understanding tilt perception. 1 Bayesian model Under our Bayesian model, inference is controlled by the posterior distribution over the tilt of the target element. This comes from the combination of a prior favoring smooth conﬁgurations of the ﬂankers and target, and the likelihood associated with the actual scene. A complete distribution would consider all possible angles and relative spatial positions of the bars, and marginalize the posterior over all but the tilt of the central element. For simplicity, we make two benign approximations: conditionalizing over (ie clamping) the angles of the ﬂankers, and exploring only a small neighborhood of their positions. We now describe the steps of inference. Smoothness prior: Under these approximations, we consider a given actual conﬁguration (see ﬁg 2A) of ﬂankers f1 = (φ1 , x1 ), f2 = (φ2 , x2 ) and center target c = (φc , xc ), arranged from top to bottom. We have to generate a prior over φc and δ1 = x1 − xc and δ2 = x2 − xc based on the principle of smoothness. As a less benign approximation, we do this in two stages: articulating a principle that determines a single optimal conﬁguration; and generating a prior as a mixture of a Gaussian about this optimum and a uniform distribution, with the mixing proportion of the latter being determined by the smoothness of the optimum. Smoothness has been extensively studied in the computer vision literature.17–20 One widely (B) (C) f1 f1 β1 R Probability max smooth Max smooth target (deg) (A) 40 20 0 -20 c δ1 c -40 Φc f2 f2 1 0.8 0.6 0.4 0.2 0 -80 -60 -40 -20 0 20 40 Flanker tilt (deg) 60 80 -80 -60 -40 20 0 20 40 Flanker tilt (deg) 60 80 Figure 2: Geometry and smoothness for ﬂankers, f1 and f2 , and center target, c. (A) Example actual conﬁguration of ﬂankers and target, aligned along the y axis from top to bottom. (B) The elastica procedure can rotate the target angle (to Φc ) and shift the relative ﬂanker and target positions on the x axis (to δ1 and δ2 ) in its search for the maximally smooth solution. Small spatial shifts (up to 1/15 the size of R) of positions are allowed, but positional shift is overemphasized in the ﬁgure for visibility. (C) Top: center tilt that results in maximal smoothness, as a function of ﬂanker tilt. Boxed cartoons show examples for given ﬂanker tilts, of the optimally smooth conﬁguration. Note attraction of target towards ﬂankers for small ﬂanker angles; here ﬂankers and target are positioned in a nearly colinear arrangement. Note also repulsion of target away from ﬂankers for intermediate ﬂanker angles. Bottom: P [c, f1 , f2 ] for center tilt that yields maximal smoothness. The y axis is normalized between 0 and 1. used principle, elastica, known even to Euler, has been applied to contour completion21 and other computer vision applications.17 The basic idea is to ﬁnd the curve with minimum energy (ie, square of curvature). Sharon et al19 showed that the elastica function can be well approximated by a number of simpler forms. We adopt a version that Leung and Malik18 adopted from Sharon et al.19 We assume that the probability for completing a smooth curve, can be factorized into two terms: P [c, f1 , f2 ] = G(c, f1 )G(c, f2 ) (1) with the term G(c, f1 ) (and similarly, G(c, f2 )) written as: R Dβ 2 2 Dβ = β1 + βc − β1 βc (2) − ) where σR σβ and β1 (and similarly, βc ) is the angle between the orientation at f1 , and the line joining f1 and c. The distance between the centers of f1 and c is given by R. The two constants, σβ and σR , control the relative contribution to smoothness of the angle versus the spatial distance. Here, we set σβ = 1, and σR = 1.5. Figure 2B illustrates an example geometry, in which φc , δ1 , and δ2 , have been shifted from the actual scene (of ﬁgure 2A). G(c, f1 ) = exp(− We now estimate the smoothest solution for given conﬁgurations. Figure 2C shows for given ﬂanker tilts, the center tilt that yields maximal smoothness, and the corresponding probability of smoothness. For near vertical ﬂankers, the spatial lability leads to very weak attraction and high probability of smoothness. As the ﬂanker angle deviates farther from vertical, there is a large repulsion, but also lower probability of smoothness. These observations are key to our model: the maximally smooth center tilt will inﬂuence attractive and repulsive interactions of tilt estimation; the probability of smoothness will inﬂuence the relative weighting of the prior versus the likelihood. From the smoothness principle, we construct a two dimensional prior (ﬁgure 3A). One dimension represents tilt, the other dimension, the overall positional shift between target (B) Likelihood (D) Marginalized Posterior (C) Posterior 20 0.03 10 -10 -20 0 Probability 0 10 Angle Angle Angle 10 0 -10 -20 0.01 -10 -20 0.02 0 -0. 2 0 Position 0.2 (E) Psychometric function 20 -0. 2 0 0.2 -0. 2 0 0.2 Position Position -10 -5 0 Angle 5 10 Probability clockwise (A) Prior 20 1 0.8 0.6 0.4 0.2 0 -20 -10 0 10 20 Target angle (deg) Counter-clockwise Clockwise Figure 3: Bayes model for example ﬂankers and target. (A) Prior 2D distribution for ﬂankers set at 22.5 degrees (note repulsive preference for -5.5 degrees). (B) Likelihood 2D distribution for a target tilt of 3 degrees; (C) Posterior 2D distribution. All 2D distributions are drawn on the same grayscale range, and the presence of a larger baseline in the prior causes it to appear more dimmed. (D) Marginalized posterior, resulting in 1D distribution over tilt. Dashed line represents the mean, with slight preference for negative angle. (E) For this target tilt, we calculate probability clockwise, and obtain one point on psychometric curve. and ﬂankers (called ’position’). The prior is a 2D Gaussian distribution, sat upon a constant baseline.22 The Gaussian is centered at the estimated smoothest target angle and relative position, and the baseline is determined by the probability of smoothness. The baseline, and its dependence on the ﬂanker orientation, is a key difference from Weiss et al’s Gaussian prior for smooth, slow motion. It can be seen as a mechanism to allow segmentation (see Posterior description below). The standard deviation of the Gaussian is a free parameter. Likelihood: The likelihood over tilt and position (ﬁgure 3B) is determined by a 2D Gaussian distribution with an added baseline.22 The Gaussian is centered at the actual target tilt; and at a position taken as zero, since this is the actual position, to which the prior is compared. The standard deviation and baseline constant are free parameters. Posterior and marginalization: The posterior comes from multiplying likelihood and prior (ﬁgure 3C) and then marginalizing over position to obtain a 1D distribution over tilt. Figure 3D shows an example in which this distribution is bimodal. Other likelihoods, with closer agreement between target and smooth prior, give unimodal distributions. Note that the bimodality is a direct consequence of having an added baseline to the prior and likelihood (if these were Gaussian without a baseline, the posterior would always be Gaussian). The viewer is effectively assessing whether the target is associated with the same object as the ﬂankers, and this is reﬂected in the baseline, and consequently, in the bimodality, and conﬁdence estimate. We deﬁne α as the mean angle of the 1D posterior distribution (eg, value of dashed line on the x axis), and β as the height of the probability distribution at that mean angle (eg, height of dashed line). The term β is an indication of conﬁdence in the angle estimate, where for larger values we are more certain of the estimate. Decision of probability clockwise: The probability of a clockwise tilt is estimated from the marginalized posterior: 1 P = 1 + exp (3) −α.∗k − log(β+η) where α and β are deﬁned as above, k is a free parameter and η a small constant. Free parameters are set to a single constant value for all ﬂanker and center conﬁgurations. Weiss et al use a similar compressive nonlinearity, but without the term β. We also tried a decision function that integrates the posterior, but the resulting curves were far from the sigmoidal nature of the data. Bias and sensitivity: For one target tilt, we generate a single probability and therefore a single point on the psychometric function relating tilt to the probability of choosing clockwise. We generate the full psychometric curve from all target tilts and ﬁt to it a cumulative 60 40 20 -5 0 5 Target tilt (deg) 10 80 60 40 20 0 -10 (C) Data -5 0 5 Target tilt (deg) 10 80 60 40 20 0 -10 (D) Model 100 100 100 80 0 -10 Model Frequency responding clockwise (B) Data Frequency responding clockwise Frequency responding clockwise Frequency responding clockwise (A) 100 -5 0 5 Target tilt (deg) 10 80 60 40 20 0 -10 -5 0 5 10 Target tilt (deg) Figure 4: Kapadia et al data,14 versus Bayesian model. Solid lines are ﬁts to a cumulative Gaussian distribution. (A) Flankers are tilted 5 degrees clockwise (black curve) or anti-clockwise (gray) of vertical, and positioned spatially in a colinear arrangement. The center bar appears tilted in the direction of the ﬂankers (attraction), as can be seen by the attractive shift of the psychometric curve. The boxed stimuli cartoon illustrates a vertical target amidst the ﬂankers. (B) Model for colinear bars also produces attraction. (C) Data and (D) model for lateral ﬂankers results in repulsion. All data are collected in the fovea for bars. Gaussian distribution N (µ, σ) (ﬁgure 3E). The mean µ of the ﬁt corresponds to the bias, 1 and σ to the sensitivity, or conﬁdence in the bias. The ﬁt to a cumulative Gaussian and extraction of these parameters exactly mimic psychophysical procedures.11 2 Results: data versus model We ﬁrst consider the geometry of the center and ﬂanker conﬁgurations, modeling the full psychometric curve for colinear and parallel ﬂanks (recall that ﬁgure 1A showed summary biases). Figure 4A;B demonstrates attraction in the data and model; that is, the psychometric curve is shifted towards the ﬂanker, because of the nature of smooth completions for colinear ﬂankers. Figure 4C;D shows repulsion in the data and model. In this case, the ﬂankers are arranged laterally instead of colinearly. The smoothest solution in the model arises by shifting the target estimate away from the ﬂankers. This shift is rather minor, because the conﬁguration has a low probability of smoothness (similar to ﬁgure 2C), and thus the prior exerts only a weak effect. The above results show examples of changes in the psychometric curve, but do not address both bias and, particularly, sensitivity, across a whole range of ﬂanker conﬁgurations. Figure 5 depicts biases and sensitivity from Solomon et al, versus the Bayes model. The data are shown for a representative subject, but the qualitative behavior is consistent across all subjects tested. In ﬁgure 5A, bias is shown, for the condition that both ﬂankers are tilted at the same angle. The data exhibit small attraction at near vertical ﬂanker angles (this arrangement is close to colinear); large repulsion at intermediate ﬂanker angles of 22.5 and 45 degrees from vertical; and minimal repulsion at large angles from vertical. This behavior is also exhibited in the Bayes model (Figure 5B). For intermediate ﬂanker angles, the smoothest solution in the model is repulsive, and the effect of the prior is strong enough to induce a signiﬁcant repulsion. For large angles, the prior exerts almost no effect. Interestingly, sensitivity is far from ﬂat in both data and model. In the data (Figure 5C), there is most loss in sensitivity at intermediate ﬂanker angles of 22.5 and 45 degrees (ie, the subject is less certain); and sensitivity is higher for near vertical or near horizontal ﬂankers. The model shows the same qualitative behavior (Figure 5D). In the model, there are two factors driving sensitivity: one is the probability of completing a smooth curvature for a given ﬂanker conﬁguration, as in Figure 2B; this determines the strength of the prior. The other factor is certainty in a particular center estimation; this is determined by β, derived from the posterior distribution, and incorporated into the decision stage of the model Data 5 0 -60 -40 -80 -60 -40 -20 0 20 40 Flanker tilt (deg) -20 0 20 40 Flanker tilt (deg) 60 60 80 -60 -40 0.6 0.5 0.4 0.3 0.2 0.1 -20 0 20 40 Flanker tilt (deg) 60 80 60 80 -80 -60 -40 -20 0 20 40 Flanker tilt (deg) -80 -60 -40 -20 0 20 40 Flanker tilt (deg) 60 80 -20 0 20 40 Flanker tilt (deg) 60 80 (F) Bias (deg) 10 5 0 -5 0.6 0.5 0.4 0.3 0.2 0.1 -80 (D) 10 -10 -10 80 Sensitivity (1/deg) -80 5 0 -5 -80 -80 -60 -60 -40 -40 -20 0 20 40 Flanker tilt (deg) -20 0 20 40 Flanker tilt (deg) 60 -10 80 (H) 60 80 Sensitivity (1/deg) Sensititvity (1/deg) Bias (deg) 0.6 0.5 0.4 0.3 0.2 0.1 (G) Sensititvity (1/deg) 0 -5 (C) (E) 5 -5 -10 Model (B) 10 Bias (deg) Bias (deg) (A) 10 0.6 0.5 0.4 0.3 0.2 0.1 -80 -60 -40 Figure 5: Solomon et al data11 (subject FF), versus Bayesian model. (A) Data and (B) model biases with same-tilted ﬂankers; (C) Data and (D) model sensitivities with same-tilted ﬂankers; (E;G) data and (F;H) model as above, but for opposite-tilted ﬂankers (note that opposite-tilted data was collected for less ﬂanker angles). Each point in the ﬁgure is derived by ﬁtting a cummulative Gaussian distribution N (µ, σ) to corresponding psychometric curve, and setting bias 1 equal to µ and sensitivity to σ . In all experiments, ﬂanker and target gratings are presented in the visual periphery. Both data and model stimuli are averages of two conﬁgurations, on the left hand side (9 O’clock position) and right hand side (3 O’clock position). The conﬁgurations are similar to Figure 1 (B), but slightly shifted according to an iso-eccentric circle, so that all stimuli are similarly visible in the periphery. (equation 3). For ﬂankers that are far from vertical, the prior has minimal effect because one cannot ﬁnd a smooth solution (eg, the likelihood dominates), and thus sensitivity is higher. The low sensitivity at intermediate angles arises because the prior has considerable effect; and there is conﬂict between the prior (tilt, position), and likelihood (tilt, position). This leads to uncertainty in the target angle estimation . For ﬂankers near vertical, the prior exerts a strong effect; but there is less conﬂict between the likelihood and prior estimates (tilt, position) for a vertical target. This leads to more conﬁdence in the posterior estimate, and therefore, higher sensitivity. The only aspect that our model does not reproduce is the (more subtle) sensitivity difference between 0 and +/- 5 degree ﬂankers. Figure 5E-H depict data and model for opposite tilted ﬂankers. The bias is now close to zero in the data (Figure 5E) and model (Figure 5F), as would be expected (since the maximally smooth angle is now always roughly vertical). Perhaps more surprisingly, the sensitivities continue to to be non-ﬂat in the data (Figure 5G) and model (Figure 5H). This behavior arises in the model due to the strength of prior, and positional uncertainty. As before, there is most loss in sensitivity at intermediate angles. Note that to ﬁt Kapadia et al, simulations used a constant parameter of k = 9 in equation 3, whereas for the Solomon et al. simulations, k = 2.5. This indicates that, in our model, there was higher conﬁdence in the foveal experiments than in the peripheral ones. 3 Discussion We applied a Bayesian framework to the widely studied tilt illusion, and demonstrated the model on examples from two different data sets involving foveal and peripheral estimation. Our results support the appealing hypothesis that perceptual misjudgements are not a consequence of poor system design, but rather can be described as optimal inference.4–8 Our model accounts correctly for both attraction and repulsion, determined by the smoothness prior and the geometry of the scene. We emphasized the issue of estimation conﬁdence. The dataset showing how conﬁdence is affected by the same issues that affect bias,11 was exactly appropriate for a Bayesian formulation; other models in the literature typically do not incorporate conﬁdence in a thoroughly probabilistic manner. In fact, our model ﬁts the conﬁdence (and bias) data more proﬁciently than an account based on lateral interactions among a population of orientationtuned cells.11 Other Bayesian work, by Stocker et al,6 utilized the full slope of the psychometric curve in ﬁtting a prior and likelihood to motion data, but did not examine the issue of conﬁdence. Estimation conﬁdence plays a central role in Bayesian formulations as a whole. Understanding how priors affect conﬁdence should have direct bearing on many other Bayesian calculations such as multimodal integration.23 Our model is obviously over-simpliﬁed in a number of ways. First, we described it in terms of tilts and spatial positions; a more complete version should work in the pixel/ﬁltering domain.18, 19 We have also only considered two ﬂanking elements; the model is extendible to a full-ﬁeld surround, whereby smoothness operates along a range of geometric directions, and some directions are more (smoothly) dominant than others. Second, the prior is constructed by summarizing the maximal smoothness information; a more probabilistically correct version should capture the full probability of smoothness in its prior. Third, our model does not incorporate a formal noise representation; however, sensitivities could be inﬂuenced both by stimulus-driven noise and conﬁdence. Fourth, our model does not address attraction in the so-called indirect tilt illusion, thought to be mediated by a different mechanism. Finally, we have yet to account for neurophysiological data within this framework, and incorporate constraints at the neural implementation level. However, versions of our computations are oft suggested for intra-areal and feedback cortical circuits; and smoothness principles form a key part of the association ﬁeld connection scheme in Li’s24 dynamical model of contour integration in V1. Our model is connected to a wealth of literature in computer vision and perception. Notably, occlusion and contour completion might be seen as the extreme example in which there is no likelihood information at all for the center target; a host of papers have shown that under these circumstances, smoothness principles such as elastica and variants explain many aspects of perception. The model is also associated with many studies on contour integration motivated by Gestalt principles;25, 26 and exploration of natural scene statistics and Gestalt,27, 28 including the relation to contour grouping within a Bayesian framework.29, 30 Indeed, our model could be modiﬁed to include a prior from natural scenes. There are various directions for the experimental test and reﬁnement of our model. Most pressing is to determine bias and sensitivity for different center and ﬂanker contrasts. As in the case of motion, our model predicts that when there is more uncertainty in the center element, prior information is more dominant. Another interesting test would be to design a task such that the center element is actually part of a different ﬁgure and unrelated to the ﬂankers; our framework predicts that there would be minimal bias, because of segmentation. Our model should also be applied to other tilt-based illusions such as the Fraser spiral and Z¨ llner. Finally, our model can be applied to other perceptual domains;31 and given o the apparent similarities between the tilt illusion and the tilt after-effect, we plan to extend the model to adaptation, by considering smoothness in time as well as space. Acknowledgements This work was funded by the HHMI (OS, TJS) and the Gatsby Charitable Foundation (PD). We are very grateful to Serge Belongie, Leanne Chukoskie, Philip Meier and Joshua Solomon for helpful discussions. References [1] J J Gibson. Adaptation, after-effect, and contrast in the perception of tilted lines. Journal of Experimental Psychology, 20:553–569, 1937. [2] C Blakemore, R H S Carpentar, and M A Georgeson. Lateral inhibition between orientation detectors in the human visual system. Nature, 228:37–39, 1970. [3] J A Stuart and H M Burian. A study of separation difﬁculty: Its relationship to visual acuity in normal and amblyopic eyes. American Journal of Ophthalmology, 53:471–477, 1962. [4] A Yuille and H H Bulthoff. Perception as bayesian inference. In Knill and Whitman, editors, Bayesian decision theory and psychophysics, pages 123–161. Cambridge University Press, 1996. [5] Y Weiss, E P Simoncelli, and E H Adelson. Motion illusions as optimal percepts. Nature Neuroscience, 5:598–604, 2002. [6] A Stocker and E P Simoncelli. Constraining a bayesian model of human visual speed perception. Adv in Neural Info Processing Systems, 17, 2004. [7] D Kersten, P Mamassian, and A Yuille. Object perception as bayesian inference. Annual Review of Psychology, 55:271–304, 2004. [8] K Kording and D Wolpert. Bayesian integration in sensorimotor learning. Nature, 427:244–247, 2004. [9] L Parkes, J Lund, A Angelucci, J Solomon, and M Morgan. Compulsory averaging of crowded orientation signals in human vision. Nature Neuroscience, 4:739–744, 2001. [10] D G Pelli, M Palomares, and N J Majaj. Crowding is unlike ordinary masking: Distinguishing feature integration from detection. Journal of Vision, 4:1136–1169, 2002. [11] J Solomon, F M Felisberti, and M Morgan. Crowding and the tilt illusion: Toward a uniﬁed account. Journal of Vision, 4:500–508, 2004. [12] J A Bednar and R Miikkulainen. Tilt aftereffects in a self-organizing model of the primary visual cortex. Neural Computation, 12:1721–1740, 2000. [13] C W Clifford, P Wenderoth, and B Spehar. A functional angle on some after-effects in cortical vision. Proc Biol Sci, 1454:1705–1710, 2000. [14] M K Kapadia, G Westheimer, and C D Gilbert. Spatial distribution of contextual interactions in primary visual cortex and in visual perception. J Neurophysiology, 4:2048–262, 2000. [15] C C Chen and C W Tyler. Lateral modulation of contrast discrimination: Flanker orientation effects. Journal of Vision, 2:520–530, 2002. [16] I Mareschal, M P Sceniak, and R M Shapley. Contextual inﬂuences on orientation discrimination: binding local and global cues. Vision Research, 41:1915–1930, 2001. [17] D Mumford. Elastica and computer vision. In Chandrajit Bajaj, editor, Algebraic geometry and its applications. Springer Verlag, 1994. [18] T K Leung and J Malik. Contour continuity in region based image segmentation. In Proc. ECCV, pages 544–559, 1998. [19] E Sharon, A Brandt, and R Basri. Completion energies and scale. IEEE Pat. Anal. Mach. Intell., 22(10), 1997. [20] S W Zucker, C David, A Dobbins, and L Iverson. The organization of curve detection: coarse tangent ﬁelds. Computer Graphics and Image Processing, 9(3):213–234, 1988. [21] S Ullman. Filling in the gaps: the shape of subjective contours and a model for their generation. Biological Cybernetics, 25:1–6, 1976. [22] G E Hinton and A D Brown. Spiking boltzmann machines. Adv in Neural Info Processing Systems, 12, 1998. [23] R A Jacobs. What determines visual cue reliability? Trends in Cognitive Sciences, 6:345–350, 2002. [24] Z Li. A saliency map in primary visual cortex. Trends in Cognitive Science, 6:9–16, 2002. [25] D J Field, A Hayes, and R F Hess. Contour integration by the human visual system: evidence for a local “association ﬁeld”. Vision Research, 33:173–193, 1993. [26] J Beck, A Rosenfeld, and R Ivry. Line segregation. Spatial Vision, 4:75–101, 1989. [27] M Sigman, G A Cecchi, C D Gilbert, and M O Magnasco. On a common circle: Natural scenes and gestalt rules. PNAS, 98(4):1935–1940, 2001. [28] S Mahumad, L R Williams, K K Thornber, and K Xu. Segmentation of multiple salient closed contours from real images. IEEE Pat. Anal. Mach. Intell., 25(4):433–444, 1997. [29] W S Geisler, J S Perry, B J Super, and D P Gallogly. Edge co-occurence in natural images predicts contour grouping performance. Vision Research, 6:711–724, 2001. [30] J H Elder and R M Goldberg. Ecological statistics of gestalt laws for the perceptual organization of contours. Journal of Vision, 4:324–353, 2002. [31] S R Lehky and T J Sejnowski. Neural model of stereoacuity and depth interpolation based on a distributed representation of stereo disparity. Journal of Neuroscience, 10:2281–2299, 1990.</p><p>4 <a title="nips-2005-4" href="../nips2005/nips-2005-A_Bayesian_Spatial_Scan_Statistic.html">nips-2005-A Bayesian Spatial Scan Statistic</a></p>
<p>Author: Daniel B. Neill, Andrew W. Moore, Gregory F. Cooper</p><p>Abstract: We propose a new Bayesian method for spatial cluster detection, the “Bayesian spatial scan statistic,” and compare this method to the standard (frequentist) scan statistic approach. We demonstrate that the Bayesian statistic has several advantages over the frequentist approach, including increased power to detect clusters and (since randomization testing is unnecessary) much faster runtime. We evaluate the Bayesian and frequentist methods on the task of prospective disease surveillance: detecting spatial clusters of disease cases resulting from emerging disease outbreaks. We demonstrate that our Bayesian methods are successful in rapidly detecting outbreaks while keeping number of false positives low. 1</p><p>5 <a title="nips-2005-5" href="../nips2005/nips-2005-A_Computational_Model_of_Eye_Movements_during_Object_Class_Detection.html">nips-2005-A Computational Model of Eye Movements during Object Class Detection</a></p>
<p>Author: Wei Zhang, Hyejin Yang, Dimitris Samaras, Gregory J. Zelinsky</p><p>Abstract: We present a computational model of human eye movements in an object class detection task. The model combines state-of-the-art computer vision object class detection methods (SIFT features trained using AdaBoost) with a biologically plausible model of human eye movement to produce a sequence of simulated ﬁxations, culminating with the acquisition of a target. We validated the model by comparing its behavior to the behavior of human observers performing the identical object class detection task (looking for a teddy bear among visually complex nontarget objects). We found considerable agreement between the model and human data in multiple eye movement measures, including number of ﬁxations, cumulative probability of ﬁxating the target, and scanpath distance.</p><p>6 <a title="nips-2005-6" href="../nips2005/nips-2005-A_Connectionist_Model_for_Constructive_Modal_Reasoning.html">nips-2005-A Connectionist Model for Constructive Modal Reasoning</a></p>
<p>Author: Artur Garcez, Luis C. Lamb, Dov M. Gabbay</p><p>Abstract: We present a new connectionist model for constructive, intuitionistic modal reasoning. We use ensembles of neural networks to represent intuitionistic modal theories, and show that for each intuitionistic modal program there exists a corresponding neural network ensemble that computes the program. This provides a massively parallel model for intuitionistic modal reasoning, and sets the scene for integrated reasoning, knowledge representation, and learning of intuitionistic theories in neural networks, since the networks in the ensemble can be trained by examples using standard neural learning algorithms. 1</p><p>7 <a title="nips-2005-7" href="../nips2005/nips-2005-A_Cortically-Plausible_Inverse_Problem_Solving_Method_Applied_to_Recognizing_Static_and_Kinematic_3D_Objects.html">nips-2005-A Cortically-Plausible Inverse Problem Solving Method Applied to Recognizing Static and Kinematic 3D Objects</a></p>
<p>Author: David Arathorn</p><p>Abstract: Recent neurophysiological evidence suggests the ability to interpret biological motion is facilitated by a neuronal</p><p>8 <a title="nips-2005-8" href="../nips2005/nips-2005-A_Criterion_for_the_Convergence_of_Learning_with_Spike_Timing_Dependent_Plasticity.html">nips-2005-A Criterion for the Convergence of Learning with Spike Timing Dependent Plasticity</a></p>
<p>Author: Robert A. Legenstein, Wolfgang Maass</p><p>Abstract: We investigate under what conditions a neuron can learn by experimentally supported rules for spike timing dependent plasticity (STDP) to predict the arrival times of strong “teacher inputs” to the same neuron. It turns out that in contrast to the famous Perceptron Convergence Theorem, which predicts convergence of the perceptron learning rule for a simpliﬁed neuron model whenever a stable solution exists, no equally strong convergence guarantee can be given for spiking neurons with STDP. But we derive a criterion on the statistical dependency structure of input spike trains which characterizes exactly when learning with STDP will converge on average for a simple model of a spiking neuron. This criterion is reminiscent of the linear separability criterion of the Perceptron Convergence Theorem, but it applies here to the rows of a correlation matrix related to the spike inputs. In addition we show through computer simulations for more realistic neuron models that the resulting analytically predicted positive learning results not only hold for the common interpretation of STDP where STDP changes the weights of synapses, but also for a more realistic interpretation suggested by experimental data where STDP modulates the initial release probability of dynamic synapses. 1</p><p>9 <a title="nips-2005-9" href="../nips2005/nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>Author: Zhenyue Zhang, Hongyuan Zha</p><p>Abstract: We propose a fast manifold learning algorithm based on the methodology of domain decomposition. Starting with the set of sample points partitioned into two subdomains, we develop the solution of the interface problem that can glue the embeddings on the two subdomains into an embedding on the whole domain. We provide a detailed analysis to assess the errors produced by the gluing process using matrix perturbation theory. Numerical examples are given to illustrate the efﬁciency and effectiveness of the proposed methods. 1</p><p>10 <a title="nips-2005-10" href="../nips2005/nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>Author: Sören Sonnenburg, Gunnar Rätsch, Christin Schäfer</p><p>Abstract: While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lankriet et al. (2004) considered conic combinations of kernel matrices for classiﬁcation, leading to a convex quadratically constraint quadratic program. We show that it can be rewritten as a semi-inﬁnite linear program that can be efﬁciently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classiﬁcation. Experimental results show that the proposed algorithm helps for automatic model selection, improving the interpretability of the learning result and works for hundred thousands of examples or hundreds of kernels to be combined. 1</p><p>11 <a title="nips-2005-11" href="../nips2005/nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>Author: Long Zhu, Alan L. Yuille</p><p>Abstract: We describe a hierarchical compositional system for detecting deformable objects in images. Objects are represented by graphical models. The algorithm uses a hierarchical tree where the root of the tree corresponds to the full object and lower-level elements of the tree correspond to simpler features. The algorithm proceeds by passing simple messages up and down the tree. The method works rapidly, in under a second, on 320 × 240 images. We demonstrate the approach on detecting cats, horses, and hands. The method works in the presence of background clutter and occlusions. Our approach is contrasted with more traditional methods such as dynamic programming and belief propagation. 1</p><p>12 <a title="nips-2005-12" href="../nips2005/nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>Author: François Laviolette, Mario Marchand, Mohak Shah</p><p>Abstract: We design a new learning algorithm for the Set Covering Machine from a PAC-Bayes perspective and propose a PAC-Bayes risk bound which is minimized for classiﬁers achieving a non trivial margin-sparsity trade-oﬀ. 1</p><p>13 <a title="nips-2005-13" href="../nips2005/nips-2005-A_Probabilistic_Approach_for_Optimizing_Spectral_Clustering.html">nips-2005-A Probabilistic Approach for Optimizing Spectral Clustering</a></p>
<p>Author: Rong Jin, Feng Kang, Chris H. Ding</p><p>Abstract: Spectral clustering enjoys its success in both data clustering and semisupervised learning. But, most spectral clustering algorithms cannot handle multi-class clustering problems directly. Additional strategies are needed to extend spectral clustering algorithms to multi-class clustering problems. Furthermore, most spectral clustering algorithms employ hard cluster membership, which is likely to be trapped by the local optimum. In this paper, we present a new spectral clustering algorithm, named “Soft Cut”. It improves the normalized cut algorithm by introducing soft membership, and can be efﬁciently computed using a bound optimization algorithm. Our experiments with a variety of datasets have shown the promising performance of the proposed clustering algorithm. 1</p><p>14 <a title="nips-2005-14" href="../nips2005/nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>Author: Yves Grandvalet, Johnny Mariethoz, Samy Bengio</p><p>Abstract: In this paper, we show that the hinge loss can be interpreted as the neg-log-likelihood of a semi-parametric model of posterior probabilities. From this point of view, SVMs represent the parametric component of a semi-parametric model ﬁtted by a maximum a posteriori estimation procedure. This connection enables to derive a mapping from SVM scores to estimated posterior probabilities. Unlike previous proposals, the suggested mapping is interval-valued, providing a set of posterior probabilities compatible with each SVM score. This framework offers a new way to adapt the SVM optimization problem to unbalanced classiﬁcation, when decisions result in unequal (asymmetric) losses. Experiments show improvements over state-of-the-art procedures. 1</p><p>15 <a title="nips-2005-15" href="../nips2005/nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>Author: Eizaburo Doi, Doru C. Balcan, Michael S. Lewicki</p><p>Abstract: Biological sensory systems are faced with the problem of encoding a high-ﬁdelity sensory signal with a population of noisy, low-ﬁdelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and twodimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of highdimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets. 1</p><p>16 <a title="nips-2005-16" href="../nips2005/nips-2005-A_matching_pursuit_approach_to_sparse_Gaussian_process_regression.html">nips-2005-A matching pursuit approach to sparse Gaussian process regression</a></p>
<p>Author: Sathiya Keerthi, Wei Chu</p><p>Abstract: In this paper we propose a new basis selection criterion for building sparse GP regression models that provides promising gains in accuracy as well as efﬁciency over previous methods. Our algorithm is much faster than that of Smola and Bartlett, while, in generalization it greatly outperforms the information gain approach proposed by Seeger et al, especially on the quality of predictive distributions. 1</p><p>17 <a title="nips-2005-17" href="../nips2005/nips-2005-Active_Bidirectional_Coupling_in_a_Cochlear_Chip.html">nips-2005-Active Bidirectional Coupling in a Cochlear Chip</a></p>
<p>Author: Bo Wen, Kwabena A. Boahen</p><p>Abstract: We present a novel cochlear model implemented in analog very large scale integration (VLSI) technology that emulates nonlinear active cochlear behavior. This silicon cochlea includes outer hair cell (OHC) electromotility through active bidirectional coupling (ABC), a mechanism we proposed in which OHC motile forces, through the microanatomical organization of the organ of Corti, realize the cochlear ampliﬁer. Our chip measurements demonstrate that frequency responses become larger and more sharply tuned when ABC is turned on; the degree of the enhancement decreases with input intensity as ABC includes saturation of OHC forces. 1 Silicon Cochleae Cochlear models, mathematical and physical, with the shared goal of emulating nonlinear active cochlear behavior, shed light on how the cochlea works if based on cochlear micromechanics. Among the modeling efforts, silicon cochleae have promise in meeting the need for real-time performance and low power consumption. Lyon and Mead developed the ﬁrst analog electronic cochlea [1], which employed a cascade of second-order ﬁlters with exponentially decreasing resonant frequencies. However, the cascade structure suffers from delay and noise accumulation and lacks fault-tolerance. Modeling the cochlea more faithfully, Watts built a two-dimensional (2D) passive cochlea that addressed these shortcomings by incorporating the cochlear ﬂuid using a resistive network [2]. This parallel structure, however, has its own problem: response gain is diminished by interference among the second-order sections’ outputs due to the large phase change at resonance [3]. Listening more to biology, our silicon cochlea aims to overcome the shortcomings of existing architectures by mimicking the cochlear micromechanics while including outer hair cell (OHC) electromotility. Although how exactly OHC motile forces boost the basilar membrane’s (BM) vibration remains a mystery, cochlear microanatomy provides clues. Based on these clues, we previously proposed a novel mechanism, active bidirectional coupling (ABC), for the cochlear ampliﬁer [4]. Here, we report an analog VLSI chip that implements this mechanism. In essence, our implementation is the ﬁrst silicon cochlea that employs stimulus enhancement (i.e., active behavior) instead of undamping (i.e., high ﬁlter Q [5]). The paper is organized as follows. In Section 2, we present the hypothesized mechanism (ABC), ﬁrst described in [4]. In Section 3, we provide a mathematical formulation of the Oval window organ of Corti BM Round window IHC RL A OHC PhP DC BM Basal @ Stereocilia i -1 i i+1 Apical B Figure 1: The inner ear. A Cutaway showing cochlear ducts (adapted from [6]). B Longitudinal view of cochlear partition (CP) (modiﬁed from [7]-[8]). Each outer hair cell (OHC) tilts toward the base while the Deiter’s cell (DC) on which it sits extends a phalangeal process (PhP) toward the apex. The OHCs’ stereocilia and the PhPs’ apical ends form the reticular lamina (RL). d is the tilt distance, and the segment size. IHC: inner hair cell. model as the basis of cochlear circuit design. Then we proceed in Section 4 to synthesize the circuit for the cochlear chip. Last, we present chip measurements in Section 5 that demonstrate nonlinear active cochlear behavior. 2 Active Bidirectional Coupling The cochlea actively ampliﬁes acoustic signals as it performs spectral analysis. The movement of the stapes sets the cochlear ﬂuid into motion, which passes the stimulus energy onto a certain region of the BM, the main vibrating organ in the cochlea (Figure 1A). From the base to the apex, BM ﬁbers increase in width and decrease in thickness, resulting in an exponential decrease in stiffness which, in turn, gives rise to the passive frequency tuning of the cochlea. The OHCs’ electromotility is widely thought to account for the cochlea’s exquisite sensitivity and discriminability. The exact way that OHC motile forces enhance the BM’s motion, however, remains unresolved. We propose that the triangular mechanical unit formed by an OHC, a phalangeal process (PhP) extended from the Deiter’s cell (DC) on which the OHC sits, and a portion of the reticular lamina (RL), between the OHC’s stereocilia end and the PhP’s apical tip, plays an active role in enhancing the BM’s responses (Figure 1B). The cochlear partition (CP) is divided into a number of segments longitudinally. Each segment includes one DC, one PhP’s apical tip and one OHC’s stereocilia end, both attached to the RL. Approximating the anatomy, we assume that when an OHC’s stereocilia end lies in segment i − 1, its basolateral end lies in the immediately apical segment i. Furthermore, the DC in segment i extends a PhP that angles toward the apex of the cochlea, with its apical end inserted just behind the stereocilia end of the OHC in segment i + 1. Our hypothesis (ABC) includes both feedforward and feedbackward interactions. On one hand, the feedforward mechanism, proposed in [9], hypothesized that the force resulting from OHC contraction or elongation is exerted onto an adjacent downstream BM segment due to the OHC’s basal tilt. On the other hand, the novel insight of the feedbackward mechanism is that the OHC force is delivered onto an adjacent upstream BM segment due to the apical tilt of the PhP extending from the DC’s main trunk. In a nutshell, the OHC motile forces, through the microanatomy of the CP, feed forward and backward, in harmony with each other, resulting in bidirectional coupling between BM segments in the longitudinal direction. Speciﬁcally, due to the opposite action of OHC S x M x Re Zm 1 0.5 0 0.2 0 A 5 10 15 20 Distance from stapes mm 25 B Figure 2: Wave propagation (WP) and basilar membrane (BM) impedance in the active cochlear model with a 2kHz pure tone (α = 0.15, γ = 0.3). A WP in ﬂuid and BM. B BM impedance Zm (i.e., pressure divided by velocity), normalized by S(x)M (x). Only the resistive component is shown; dot marks peak location. forces on the BM and the RL, the motion of BM segment i − 1 reinforces that of segment i while the motion of segment i + 1 opposes that of segment i, as described in detail in [4]. 3 The 2D Nonlinear Active Model To provide a blueprint for the cochlear circuit design, we formulate a 2D model of the cochlea that includes ABC. Both the cochlea’s length (BM) and height (cochlear ducts) are discretized into a number of segments, with the original aspect ratio of the cochlea maintained. In the following expressions, x represents the distance from the stapes along the CP, with x = 0 at the base (or the stapes) and x = L (uncoiled cochlear duct length) at the apex; y represents the vertical distance from the BM, with y = 0 at the BM and y = ±h (cochlear duct radius) at the bottom/top wall. Providing that the assumption of ﬂuid incompressibility holds, the velocity potential φ of the ﬂuids is required to satisfy 2 φ(x, y, t) = 0, where 2 denotes the Laplacian operator. By deﬁnition, this potential is related to ﬂuid velocities in the x and y directions: Vx = −∂φ/∂x and Vy = −∂φ/∂y. The BM is driven by the ﬂuid pressure difference across it. Hence, the BM’s vertical motion (with downward displacement being positive) can be described as follows. ˙ ¨ Pd (x) + FOHC (x) = S(x)δ(x) + β(x)δ(x) + M (x)δ(x), (1) where S(x) is the stiffness, β(x) is the damping, and M (x) is the mass, per unit area, of the BM; δ is the BM’s downward displacement. Pd = ρ ∂(φSV (x, y, t) − φST (x, y, t))/∂t is the pressure difference between the two ﬂuid ducts (the scala vestibuli (SV) and the scala tympani (ST)), evaluated at the BM (y = 0); ρ is the ﬂuid density. The FOHC(x) term combines feedforward and feedbackward OHC forces, described by FOHC (x) = s0 tanh(αγS(x)δ(x − d)/s0 ) − tanh(αS(x)δ(x + d)/s0 ) , (2) where α denotes the OHC motility, expressed as a fraction of the BM stiffness, and γ is the ratio of feedforward to feedbackward coupling, representing relative strengths of the OHC forces exerted on the BM segment through the DC, directly and via the tilted PhP. d denotes the tilt distance, which is the horizontal displacement between the source and the recipient of the OHC force, assumed to be equal for the forward and backward cases. We use the hyperbolic tangent function to model saturation of the OHC forces, the nonlinearity that is evident in physiological measurements [8]; s0 determines the saturation level. We observed wave propagation in the model and computed the BM’s impedance (i.e., the ratio of driving pressure to velocity). Following the semi-analytical approach in [2], we simulated a linear version of the model (without saturation). The traveling wave transitions from long-wave to short-wave before the BM vibration peaks; the wavelength around the characteristic place is comparable to the tilt distance (Figure 2A). The BM impedance’s real part (i.e., the resistive component) becomes negative before the peak (Figure 2B). On the whole, inclusion of OHC motility through ABC boosts the traveling wave by pumping energy onto the BM when the wavelength matches the tilt of the OHC and PhP. 4 Analog VLSI Design and Implementation Based on our mathematical model, which produces realistic responses, we implemented a 2D nonlinear active cochlear circuit in analog VLSI, taking advantage of the 2D nature of silicon chips. We ﬁrst synthesize a circuit analog of the mathematical model, and then we implement the circuit in the log-domain. We start by synthesizing a passive model, and then extend it to a nonlinear active one by including ABC with saturation. 4.1 Synthesizing the BM Circuit The model consists of two fundamental parts: the cochlear ﬂuid and the BM. First, we design the ﬂuid element and thus the ﬂuid network. In discrete form, the ﬂuids can be viewed as a grid of elements with a speciﬁc resistance that corresponds to the ﬂuid density or mass. Since charge is conserved for a small sheet of resistance and so are particles for a small volume of ﬂuid, we use current to simulate ﬂuid velocity. At the transistor level, the current ﬂowing through the channel of a MOS transistor, operating subthreshold as a diffusive element, can be used for this purpose. Therefore, following the approach in [10], we implement the cochlear ﬂuid network using a diffusor network formed by a 2D grid of nMOS transistors. Second, we design the BM element and thus the BM. As current represents velocity, we rewrite the BM boundary condition (Equation 1, without the FOHC term): ˙ Iin = S(x) Imem dt + β(x)Imem + M (x)I˙mem , (3) where Iin , obtained by applying the voltage from the diffusor network to the gate of a pMOS transistor, represents the velocity potential scaled by the ﬂuid density. In turn, Imem ˙ drives the diffusor network to match the ﬂuid velocity with the BM velocity, δ. The FOHC term is dealt with in Section 4.2. Implementing this second-order system requires two state-space variables, which we name Is and Io . And with s = jω, our synthesized BM design (passive) is τ1 Is s + Is τ2 Io s + Io Imem = −Iin + Io , = Iin − bIs , = Iin + Is − Io , (4) (5) (6) where the two ﬁrst-order systems are both low-pass ﬁlters (LPFs), with time constants τ1 and τ2 , respectively; b is a gain factor. Thus, Iin can be expressed in terms of Imem as: Iin s2 = (b + 1)/τ1 τ2 + ((τ1 + τ2 )/τ1 τ2)s + s2 Imem . Comparing this expression with the design target (Equation 3) yields the circuit analogs: S(x) = (b + 1)/τ1τ2 , β(x) = (τ1 + τ2 )/τ1 τ2 , and M (x) = 1. Note that the mass M (x) is a constant (i.e., 1), which was also the case in our mathematical model simulation. These analogies require that τ1 and τ2 increase exponentially to Half LPF ( ) + Iout- Iin+ Iout+ Iout Vq Iin+ Iin- C+ B Iin- A Iin+ + - - Iin- + + + C To neighbors Is- Is+ > + > + IT+ IT- + - + From neighbors Io- Io+ + + + + - - + + LPF Iout+ Iout- BM Imem+ Imem- Figure 3: Low-pass ﬁlter (LPF) and second-order section circuit design. A Half-LPF circuit. B Complete LPF circuit formed by two half-LPF circuits. C Basilar membrane (BM) circuit. It consists of two LPFs and connects to its neighbors through Is and IT . simulate the exponentially decreasing BM stiffness (and damping); b allows us to achieve a reasonable stiffness for a practical choice of τ1 and τ2 (capacitor size is limited by silicon area). 4.2 Adding Active Bidirectional Coupling To include ABC in the BM boundary condition, we replace δ in Equation 2 with Imem dt to obtain FOHC = rﬀ S(x)T Imem (x − d)dt − rfb S(x)T Imem (x + d)dt , where rﬀ = αγ and rfb = α denote the feedforward and feedbackward OHC motility factors, and T denotes saturation. The saturation is applied to the displacement, instead of the force, as this simpliﬁes the implementation. We obtain the integrals by observing that, in the passive design, the state variable Is = −Imem /sτ1 . Thus, Imem (x − d)dt = −τ1f Isf and Imem (x + d)dt = −τ1b Isb . Here, Isf and Isb represent the outputs of the ﬁrst LPF in the upstream and downstream BM segments, respectively; τ1f and τ1b represent their respective time constants. To reduce complexity in implementation, we use τ1 to approximate both τ1f and τ1b as the longitudinal span is small. We obtain the active BM design by replacing Equation 5 with the synthesis result: τ2 Ios + Io = Iin − bIs + rfb (b + 1)T (−Isb ) − rﬀ (b + 1)T (−Isf ). Note that, to implement ABC, we only need to add two currents to the second LPF in the passive system. These currents, Isf and Isb , come from the upstream and downstream neighbors of each segment. ISV Fluid Base BM IST Apex Fluid A IT + IT Is+ Is- + Vsat Imem Iin+ Imem- Iin- Is+ Is+ Is- IsBM IT + IT + - I IT T Vsat IT + IT Is+ Is- B Figure 4: Cochlear chip. A Architecture: Two diffusive grids with embedded BM circuits model the cochlea. B Detail. BM circuits exchange currents with their neighbors. 4.3 Class AB Log-domain Implementation We employ the log-domain ﬁltering technique [11] to realize current-mode operation. In addition, following the approach proposed in [12], we implement the circuit in Class AB to increase dynamic range, reduce the effect of mismatch and lower power consumption. This differential signaling is inspired by the way the biological cochlea works—the vibration of BM is driven by the pressure difference across it. Taking a bottom-up strategy, we start by designing a Class AB LPF, a building block for the BM circuit. It is described by + − + − + − + − + − 2 τ (Iout − Iout )s + (Iout − Iout ) = Iin − Iin and τ Iout Iout s + Iout Iout = Iq , where Iq sets the geometric mean of the positive and negative components of the output current, and τ sets the time constant. Combining the common-mode constraint with the differential design equation yields the nodal equation for the positive path (the negative path has superscripts + and − swapped): + − + + + − 2 ˙+ C Vout = Iτ (Iin − Iin ) + (Iq /Iout − Iout ) /(Iout + Iout ). + This nodal equation suggests the half-LPF circuit shown in Figure 3A. Vout , the voltage on + the positive capacitor (C ), gates a pMOS transistor to produce the corresponding current + − − signal, Iout (Vout and Iout are similarly related). The bias Vq sets the quiescent current Iq while Vτ determines the current Iτ , which is related to the time constant by τ = CuT/κIτ (κ is the subthreshold slope coefﬁcient and uT is the thermal voltage). Two of these subcircuits, connected in push–pull, form a complete LPF (Figure 3B). The BM circuit is implemented using two LPFs interacting in accordance with the synthesized design equations (Figure 3C). Imem is the combination of three currents, Iin , Is , and Io . Each BM sends out Is and receives IT , a saturated version of its neighbor’s Is . The saturation is accomplished by a current-limiting transistor (see Figure 4B), which yields IT = T (Is ) = Is Isat /(Is + Isat ), where Isat is set by a bias voltage Vsat. 4.4 Chip Architecture We fabricated a version of our cochlear chip architecture (Figure 4) with 360 BM circuits and two 4680-element ﬂuid grids (360 ×13). This chip occupies 10.9mm2 of silicon area in 0.25µm CMOS technology. Differential input signals are applied at the base while the two ﬂuid grids are connected at the apex through a ﬂuid element that represents the helicotrema. 5 Chip Measurements We carried out two measurements that demonstrate the desired ampliﬁcation by ABC, and the compressive growth of BM responses due to saturation. To obtain sinusoidal current as the input to the BM subcircuits, we set the voltages applied at the base to be the logarithm of a half-wave rectiﬁed sinusoid. We ﬁrst investigated BM-velocity frequency responses at six linearly spaced cochlear positions (Figure 5). The frequency that maximally excites the ﬁrst position (Stage 30), deﬁned as its characteristic frequency (CF), is 12.1kHz. The remaining ﬁve CFs, from early to later stages, are 8.2k, 1.7k, 905, 366, and 218Hz, respectively. Phase accumulation at the CFs ranges from 0.56 to 2.67π radians, comparable to 1.67π radians in the mammalian cochlea [13]. Q10 factor (the ratio of the CF to the bandwidth 10dB below the peak) ranges from 1.25 to 2.73, comparable to 2.55 at mid-sound intensity in biology (computed from [13]). The cutoff slope ranges from -20 to -54dB/octave, as compared to -85dB/octave in biology (computed from [13]). BM Velocity Amplitude dB 40 Stage 0 230 190 150 110 70 30 30 20 10 0 BM Velocity Phase Π radians 50 2 4 10 0.1 0.2 0.5 1 2 5 Frequency kHz A 10 20 0.1 0.2 0.5 1 2 5 Frequency kHz 10 20 B Figure 5: Measured BM-velocity frequency responses at six locations. A Amplitude. B Phase. Dashed lines: Biological data (adapted from [13]). Dots mark peaks. We then explored the longitudinal pattern of BM-velocity responses and the effect of ABC. Stimulating the chip using four different pure tones, we obtained responses in which a 4kHz input elicits a peak around Stage 85 while 500Hz sound travels all the way to Stage 178 and peaks there (Figure 6A). We varied the input voltage level and obtained frequency responses at Stage 100 (Figure 6B). Input voltage level increases linearly such that the current increases exponentially; the input current level (in dB) was estimated based on the measured κ for this chip. As expected, we observed linearly increasing responses at low frequencies in the logarithmic plot. In contrast, the responses around the CF increase less and become broader with increasing input level as saturation takes effect in that region (resembling a passive cochlea). We observed 24dB compression as compared to 27 to 47dB in biology [13]. At the highest intensities, compression also occurs at low frequencies. These chip measurements demonstrate that inclusion of ABC, simply through coupling neighboring BM elements, transforms a passive cochlea into an active one. This active cochlear model’s nonlinear responses are qualitatively comparable to physiological data. 6 Conclusions We presented an analog VLSI implementation of a 2D nonlinear cochlear model that utilizes a novel active mechanism, ABC, which we proposed to account for the cochlear ampliﬁer. ABC was shown to pump energy into the traveling wave. Rather than detecting the wave’s amplitude and implementing an automatic-gain-control loop, our biomorphic model accomplishes this simply by nonlinear interactions between adjacent neighbors. Im- 60 Frequency 4k 2k 1k 500 Hz BM Velocity Amplitude dB BM Velocity Amplitude dB 20 10 0 Input Level 40 48 dB 20 Stage 100 32 dB 16 dB 0 0 dB 10 0 50 100 150 Stage Number A 200 0.2 0.5 1 2 5 Frequency kHz 10 20 B Figure 6: Measured BM-velocity responses (cont’d). A Longitudinal responses (20-stage moving average). Peak shifts to earlier (basal) stages as input frequency increases from 500 to 4kHz. B Effects of increasing input intensity. Responses become broader and show compressive growth. plemented in the log-domain, with Class AB operation, our silicon cochlea shows enhanced frequency responses, with compressive behavior around the CF, when ABC is turned on. These features are desirable in prosthetic applications and automatic speech recognition systems as they capture the properties of the biological cochlea. References [1] Lyon, R.F. & Mead, C.A. (1988) An analog electronic cochlea. IEEE Trans. Acoust. Speech and Signal Proc., 36: 1119-1134. [2] Watts, L. (1993) Cochlear Mechanics: Analysis and Analog VLSI . Ph.D. thesis, Pasadena, CA: California Institute of Technology. [3] Fragni`re, E. (2005) A 100-Channel analog CMOS auditory ﬁlter bank for speech recognition. e IEEE International Solid-State Circuits Conference (ISSCC 2005) , pp. 140-141. [4] Wen, B. & Boahen, K. (2003) A linear cochlear model with active bi-directional coupling. The 25th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC 2003), pp. 2013-2016. [5] Sarpeshkar, R., Lyon, R.F., & Mead, C.A. (1996) An analog VLSI cochlear model with new transconductance ampliﬁer and nonlinear gain control. Proceedings of the IEEE Symposium on Circuits and Systems (ISCAS 1996) , 3: 292-295. [6] Mead, C.A. (1989) Analog VLSI and Neural Systems . Reading, MA: Addison-Wesley. [7] Russell, I.J. & Nilsen, K.E. (1997) The location of the cochlear ampliﬁer: Spatial representation of a single tone on the guinea pig basilar membrane. Proc. Natl. Acad. Sci. USA, 94: 2660-2664. [8] Geisler, C.D. (1998) From sound to synapse: physiology of the mammalian ear . Oxford University Press. [9] Geisler, C.D. & Sang, C. (1995) A cochlear model using feed-forward outer-hair-cell forces. Hearing Research , 86: 132-146. [10] Boahen, K.A. & Andreou, A.G. (1992) A contrast sensitive silicon retina with reciprocal synapses. In Moody, J.E. and Lippmann, R.P. (eds.), Advances in Neural Information Processing Systems 4 (NIPS 1992) , pp. 764-772, Morgan Kaufmann, San Mateo, CA. [11] Frey, D.R. (1993) Log-domain ﬁltering: an approach to current-mode ﬁltering. IEE Proc. G, Circuits Devices Syst., 140 (6): 406-416. [12] Zaghloul, K. & Boahen, K.A. (2005) An On-Off log-domain circuit that recreates adaptive ﬁltering in the retina. IEEE Transactions on Circuits and Systems I: Regular Papers , 52 (1): 99-107. [13] Ruggero, M.A., Rich, N.C., Narayan, S.S., & Robles, L. (1997) Basilar membrane responses to tones at the base of the chinchilla cochlea. J. Acoust. Soc. Am., 101 (4): 2151-2163.</p><p>18 <a title="nips-2005-18" href="../nips2005/nips-2005-Active_Learning_For_Identifying_Function_Threshold_Boundaries.html">nips-2005-Active Learning For Identifying Function Threshold Boundaries</a></p>
<p>Author: Brent Bryan, Robert C. Nichol, Christopher R. Genovese, Jeff Schneider, Christopher J. Miller, Larry Wasserman</p><p>Abstract: We present an efﬁcient algorithm to actively select queries for learning the boundaries separating a function domain into regions where the function is above and below a given threshold. We develop experiment selection methods based on entropy, misclassiﬁcation rate, variance, and their combinations, and show how they perform on a number of data sets. We then show how these algorithms are used to determine simultaneously valid 1 − α conﬁdence intervals for seven cosmological parameters. Experimentation shows that the algorithm reduces the computation necessary for the parameter estimation problem by an order of magnitude.</p><p>19 <a title="nips-2005-19" href="../nips2005/nips-2005-Active_Learning_for_Misspecified_Models.html">nips-2005-Active Learning for Misspecified Models</a></p>
<p>Author: Masashi Sugiyama</p><p>Abstract: Active learning is the problem in supervised learning to design the locations of training input points so that the generalization error is minimized. Existing active learning methods often assume that the model used for learning is correctly speciﬁed, i.e., the learning target function can be expressed by the model at hand. In many practical situations, however, this assumption may not be fulﬁlled. In this paper, we ﬁrst show that the existing active learning method can be theoretically justiﬁed under slightly weaker condition: the model does not have to be correctly speciﬁed, but slightly misspeciﬁed models are also allowed. However, it turns out that the weakened condition is still restrictive in practice. To cope with this problem, we propose an alternative active learning method which can be theoretically justiﬁed for a wider class of misspeciﬁed models. Thus, the proposed method has a broader range of applications than the existing method. Numerical studies show that the proposed active learning method is robust against the misspeciﬁcation of models and is thus reliable. 1 Introduction and Problem Formulation Let us discuss the regression problem of learning a real-valued function Ê from training examples ´Ü Ý µ ´Ü µ · ¯ Ý Ò ´Üµ deﬁned on ½ where ¯ Ò ½ are i.i.d. noise with mean zero and unknown variance ¾. We use the following linear regression model for learning. ´Ü µ ´µ Ô ½ « ³ ´Ü µ where ³ Ü Ô ½ are ﬁxed linearly independent functions and are parameters to be learned. ´ µ « ´«½ «¾ « Ô µ We evaluate the goodness of the learned function Ü by the expected squared test error over test input points and noise (i.e., the generalization error). When the test input points are drawn independently from a distribution with density ÔØ Ü , the generalization error is expressed as ´ µ ¯ ´Üµ   ´Üµ ¾ Ô ´Üµ Ü Ø where ¯ denotes the expectation over the noise ¯ Ò Ô ´Üµ is known1. ½. In the following, we suppose that Ø In a standard setting of regression, the training input points are provided from the environment, i.e., Ü Ò ½ independently follow the distribution with density ÔØ Ü . On the other hand, in some cases, the training input points can be designed by users. In such cases, it is expected that the accuracy of the learning result can be improved if the training input points are chosen appropriately, e.g., by densely locating training input points in the regions of high uncertainty. ´ µ Active learning—also referred to as experimental design—is the problem of optimizing the location of training input points so that the generalization error is minimized. In active learning research, it is often assumed that the regression model is correctly speciﬁed [2, 1, 3], i.e., the learning target function Ü can be expressed by the model. In practice, however, this assumption is often violated. ´ µ In this paper, we ﬁrst show that the existing active learning method can still be theoretically justiﬁed when the model is approximately correct in a strong sense. Then we propose an alternative active learning method which can also be theoretically justiﬁed for approximately correct models, but the condition on the approximate correctness of the models is weaker than that for the existing method. Thus, the proposed method has a wider range of applications. In the following, we suppose that the training input points Ü Ò ½ are independently drawn from a user-deﬁned distribution with density ÔÜ Ü , and discuss the problem of ﬁnding the optimal density function. ´µ 2 Existing Active Learning Method The generalization error deﬁned by Eq.(1) can be decomposed as ·Î is the (squared) bias term and Î is the variance term given by where ¯ ´Üµ   ´Üµ ¾ Ô ´Üµ Ü Ø Î and ¯ ´Üµ   ¯ ´Üµ ¾ Ô ´Üµ Ü Ø A standard way to learn the parameters in the regression model (1) is the ordinary leastsquares learning, i.e., parameter vector « is determined as follows. « ÇÄË It is known that «ÇÄË is given by Ö« Ò Ñ « ÇÄË where Ä ÇÄË ´ µ ½ Ò ´Ü µ   Ý ½ Ä ÇÄË ³ ´Ü µ ¾ Ý and Ý ´Ý½ Ý¾ Ý Ò µ Let ÇÄË , ÇÄË and ÎÇÄË be , and Î for the learned function obtained by the ordinary least-squares learning, respectively. Then the following proposition holds. 1 In some application domains such as web page analysis or bioinformatics, a large number of unlabeled samples—input points without output values independently drawn from the distribution with density ÔØ ´Üµ—are easily gathered. In such cases, a reasonably good estimate of ÔØ ´Üµ may be obtained by some standard density estimation method. Therefore, the assumption that ÔØ ´Üµ is known may not be so restrictive. Proposition 1 ([2, 1, 3]) Suppose that the model is correctly speciﬁed, i.e., the learning target function Ü is expressed as ´µ Ô ´Ü µ Then ½ «£ ³ ´Üµ and ÎÇÄË are expressed as ÇÄË ¼ ÇÄË and Î ¾ ÇÄË Â ÇÄË where ØÖ´ÍÄ Â ÇÄË ÇÄË Ä ÇÄË µ ³ ´Üµ³ ´ÜµÔ ´Üµ Ü Í and Ø Therefore, for the correctly speciﬁed model (1), the generalization error as ÇÄË ¾ ÇÄË is expressed Â ÇÄË Based on this expression, the existing active learning method determines the location of training input points Ü Ò ½ (or the training input density ÔÜ Ü ) so that ÂÇÄË is minimized [2, 1, 3]. ´ µ 3 Analysis of Existing Method under Misspeciﬁcation of Models In this section, we investigate the validity of the existing active learning method for misspeciﬁed models. ´ µ Suppose the model does not exactly include the learning target function Ü , but it approximately includes it, i.e., for a scalar Æ such that Æ is small, Ü is expressed as ´ µ ´Ü µ ´Üµ · ÆÖ´Üµ where ´Üµ is the orthogonal projection of ´Üµ onto the span of residual Ö´Üµ is orthogonal to ³ ´Üµ ½ : Ô Ô ´Üµ ½ «£ ³ ´Üµ Ö´Üµ³ ´ÜµÔ ´Üµ Ü and In this case, the bias term Ø ¼ for ³ ´Üµ ½¾ Ô and the ½ Ô is expressed as ¾ ´ ´Üµ   ´Üµµ¾ Ô ´Üµ Ü is constant which does not depend on the training input density Ô ´Üµ, we subtract ¯ ´Üµ   ´Üµ Ô ´Üµ Ü · where Ø Ø Since in the following discussion. Ü Then we have the following lemma2 . Lemma 2 For the approximately correct model (3), we have ÇÄË   ÇÄË Î ÇÄË where 2 Þ Æ ¾ ÍÄ ¾Â Ö ÇÄË Þ Ä Þ Ç ´Ò ½ µ ´Ö´Ü½µ Ö´Ü¾µ Ö ÇÄË Ö Ô Ö ´Ü Proofs of lemmas are provided in an extended version [6]. Ò µµ Ç ´Æ ¾ µ Note that the asymptotic order in Eq.(1) is in probability since ÎÇÄË is a random variable that includes Ü Ò ½ . The above lemma implies that ½ Ó ´Ò  ¾ µ Therefore, the existing active learning method of minimizing Â is still justiﬁed if Æ ½   ¾ µ. However, when Æ Ó ´Ò  ½ µ, the existing method may not work well because ¾ Ó ´Ò the bias term   is not smaller than the variance term Î , so it can not be ÇÄË   ¾ · Ó ´Ò ½µ Â ÇÄË if Æ Ô Ô ÇÄË Ô Ô ÇÄË ÇÄË neglected. 4 New Active Learning Method In this section, we propose a new active learning method based on the weighted leastsquares learning. 4.1 Weighted Least-Squares Learning When the model is correctly speciﬁed, «ÇÄË is an unbiased estimator of «£ . However, for misspeciﬁed models, «ÇÄË is generally biased even asymptotically if Æ ÇÔ . ´½µ The bias of «ÇÄË is actually caused by the covariate shift [5]—the training input density ÔÜ Ü is different from the test input density ÔØ Ü . For correctly speciﬁed models, inﬂuence of the covariate shift can be ignored, as the existing active learning method does. However, for misspeciﬁed models, we should explicitly cope with the covariate shift. ´µ ´ µ Under the covariate shift, it is known that the following weighted least-squares learning is [5]. asymptotically unbiased even if Æ ÇÔ ´½µ Ô ´Ü µ Ô ´Ü µ ½ Ò Ö« Ò Ñ « Ï ÄË ¾ ´Ü µ   Ý Ø Ü Asymptotic unbiasedness of «Ï ÄË would be intuitively understood by the following identity, which is similar in spirit to importance sampling: ´Üµ   ´Üµ ¾ Ô ´Ü µ Ü ´Üµ   ´Üµ Ø ´µ ¾ Ô ´Üµ Ô ´Ü µ Ü Ô ´Üµ Ø Ü Ü In the following, we assume that ÔÜ Ü is strictly positive for all Ü. Let matrix with the -th diagonal element be the diagonal Ô ´Ü µ Ô ´Ü µ Ø Ü Then it can be conﬁrmed that «Ï ÄË is given by « Ä Ï ÄË Ï ÄË Ý where Ä ´ Ï ÄË µ ½ 4.2 Active Learning Based on Weighted Least-Squares Learning Let Ï ÄË , Ï ÄË and ÎÏ ÄË be , and Î for the learned function obtained by the above weighted least-squares learning, respectively. Then we have the following lemma. Lemma 3 For the approximately correct model (3), we have   Ï ÄË Î Æ ¾ ÍÄ ¾Â Ï ÄË where Ï ÄË Ï ÄË Â Ï ÄË Þ Ä Þ Ç ´Ò ½ µ Ö Ï ÄË Ö Ô Ô ØÖ´ÍÄ Ï ÄË Ä Ï ÄË Ç ´Æ ¾ Ò ½ µ µ This lemma implies that   ¾ Â · Ó ´Ò ½µ ´½µ if Æ ÓÔ Based on this expression, we propose determining the training input density ÔÜ ÂÏ ÄË is minimized. Ï ÄË Ï ÄË Ô ´Üµ so that ´½µ The use of the proposed criterion ÂÏ ÄË can be theoretically justiﬁed when Æ ÓÔ , ½ while the existing criterion ÂÇÄË requires Æ ÓÔ Ò  ¾ . Therefore, the proposed method has a wider range of applications. The effect of this extension is experimentally investigated in the next section. ´ 5 µ Numerical Examples We evaluate the usefulness of the proposed active learning method through experiments. Toy Data Set: setting. We ﬁrst illustrate how the proposed method works under a controlled ½ ´µ ´µ ½ · · ½¼¼ ´µ Let and the learning target function Ü be Ü   Ü Ü¾ ÆÜ¿. Let Ò ½¼¼ be i.i.d. Gaussian noise with mean zero and standard deviation and ¯ . Let ÔØ Ü ½ be the Gaussian density with mean and standard deviation , which is assumed to be known here. Let Ô and the basis functions be ³ Ü Ü  ½ for . Let us consider the following three cases. Æ , where each case corresponds to “correctly speciﬁed”, “approximately correct”, and “misspeciﬁed” (see Figure 1). We choose the training input density ÔÜ Ü from the Gaussian density with mean and standard , where deviation ¼¾ ¿ ´µ ¼  ¼ ¼¼ ¼ ¼ ¼ ½¼ ´µ ¼ ¼¿ ½¾¿ ¼¾ ¾ We compare the accuracy of the following three methods: (A) Proposed active learning criterion + WLS learning : The training input density is determined so that ÂÏ ÄË is minimized. Following the determined input density, training input points Ü ½¼¼ are created and corresponding output values Ý ½¼¼ ½ ½ are observed. Then WLS learning is used for estimating the parameters. (B) Existing active learning criterion + OLS learning [2, 1, 3]: The training input density is determined so that ÂÇÄË is minimized. OLS learning is used for estimating the parameters. (C) Passive learning + OLS learning: The test input density ÔØ Ü is used as the training input density. OLS learning is used for estimating the parameters. ´ µ First, we evaluate the accuracy of ÂÏ ÄË and ÂÇÄË as approximations of Ï ÄË and ÇÄË . The means and standard deviations of Ï ÄË , ÂÏ ÄË , ÇÄË , and ÂÇÄË over runs are (“correctly depicted as functions of  in Figure 2. These graphs show that when Æ speciﬁed”), both ÂÏ ÄË and ÂÇÄË give accurate estimates of Ï ÄË and ÇÄË . When Æ (“approximately correct”), ÂÏ ÄË again works well, while ÂÇÄË tends to be negatively biased for large . This result is surprising since as illustrated in Figure 1, the learning target functions with Æ and Æ are visually quite similar. Therefore, it intuitively seems that the result of Æ is not much different from that of Æ . However, the simulation result shows that this slight difference makes ÂÇÄË unreliable. (“misspeciﬁed”), ÂÏ ÄË is still reasonably accurate, while ÂÇÄË is heavily When Æ biased. ½¼¼ ¼ ¼¼ ¼ ¼ ¼¼ ¼¼ ¼ These results show that as an approximation of the generalization error, ÂÏ ÄË is more robust against the misspeciﬁcation of models than ÂÇÄË , which is in good agreement with the theoretical analyses given in Section 3 and Section 4. Learning target function f(x) 8 δ=0 δ=0.04 δ=0.5 6 Table 1: The means and standard deviations of the generalization error for Toy data set. The best method and comparable ones by the t-test at the are described with boldface. signiﬁcance level The value of method (B) for Æ is extremely large but it is not a typo. 4 ± 2 0 −1.5 −1 −0.5 0 0.5 1 1.5 2 Input density functions 1.5 ¼ pt(x) Æ ¼ ½ ¦¼ ¼ px(x) 1 0.5 0 −1.5 −1 −0.5 0 0.5 1 1.5 2 Figure 1: Learning target function and input density functions. ¼ Æ (A) (B) (C) ¼¼ Æ −3 −3 −3 G−WLS 12 4 3 G−WLS 5 4 ¼ x 10 6 5 ½¼¿. “misspeciﬁed” x 10 G−WLS ¼ ¦¼ ¼ ¿¼¿ ¦ ½ ¦½ ½ ¿ ¾ ¦ ½ ¾¿ ¾ ¾¦¼ ¿ “approximately correct” x 10 6 Æ All values in the table are multiplied by Æ “correctly speciﬁed” ¦¼ ¼ ¾ ¼¦¼ ½¿ ¼¼ Æ ¾ ¼¾ ¦ ¼ ¼ 3 10 8 6 0.8 1.2 1.6 2 0.07 2.4 J−WLS 0.06 0.8 1.2 1.6 2 0.07 2.4 0.8 1.2 1.6 2 0.07 J−WLS 0.06 0.05 0.05 0.05 0.04 0.04 0.04 0.03 0.03 2.4 J−WLS 0.06 0.8 −3 x 10 1.2 1.6 2 2.4 G−OLS 5 0.03 0.8 −3 x 10 1.2 1.6 2 3 1.2 1.6 2 1.6 2.4 2 G−OLS 0.4 4 3 0.8 0.5 G−OLS 5 4 2.4 0.3 0.2 0.1 2 2 0.8 1.2 1.6 2 0.06 2.4 J−OLS 0.8 1.2 1.6 2 0.06 2.4 0.8 1.2 0.06 J−OLS 0.05 0.05 0.05 0.04 0.04 0.04 0.03 0.03 0.02 0.02 2.4 J−OLS 0.8 1.2 1.6 c 2 2.4 0.03 0.02 0.8 Figure 2: The means and error bars of functions of . 1.2 1.6 c Ï ÄË , 2 Â Ï ÄË 2.4 , 0.8 ÇÄË 1.2 1.6 c , and ÂÇÄË over 2 2.4 ½¼¼ runs as In Table 1, the mean and standard deviation of the generalization error obtained by each method is described. When Æ , the existing method (B) works better than the proposed method (A). Actually, in this case, training input densities that approximately minimize Ï ÄË and ÇÄË were found by ÂÏ ÄË and ÂÇÄË . Therefore, the difference of the errors is caused by the difference of WLS and OLS: WLS generally has larger variance than OLS. Since bias is zero for both WLS and OLS if Æ , OLS would be more accurate than WLS. Although the proposed method (A) is outperformed by the existing method (B), it still works better than the passive learning scheme (C). When Æ and Æ the proposed method (A) gives signiﬁcantly smaller errors than other methods. ¼ ¼ ¼¼ ¼ Overall, we found that for all three cases, the proposed method (A) works reasonably well and outperforms the passive learning scheme (C). On the other hand, the existing method (B) works excellently in the correctly speciﬁed case, although it tends to perform poorly once the correctness of the model is violated. Therefore, the proposed method (A) is found to be robust against the misspeciﬁcation of models and thus it is reliable. Table 2: The means and standard deviations of the test error for DELVE data sets. All values in the table are multiplied by ¿. Bank-8fm Bank-8fh Bank-8nm Bank-8nh (A) ¼ ¿½ ¦ ¼ ¼ ¾ ½¼ ¦ ¼ ¼ ¾ ¦ ½ ¾¼ ¿ ¦ ½ ½½ (B) ¦ ¦ ¦ ¦ (C) ¦ ¦ ¦ ¦ ½¼ ¼ ¼¼ ¼¿ ¼¼ ¾ ¾½ ¼ ¼ ¾ ¾¼ ¼ ¼ Kin-8fm Kin-8fh ½ ¦¼ ¼ ½ ¦¼ ¼ ½ ¼¦¼ ¼ (A) (B) (C) ¾ ½ ¼ ¿ ½ ½¿ ¾ ¿ ½¿ ¿ ½¿ Kin-8nm ¼¦¼ ½ ¿ ¦ ¼ ½¿ ¾ ¦¼ ¾ Kin-8nh ¿ ¦¼ ¼ ¿ ¼¦ ¼ ¼ ¿ ¦¼ ½ ¼ ¾¦¼ ¼ ¼ ¦¼ ¼ ¼ ½¦¼ ¼ (A)/(C) (B)/(C) (C)/(C) 1.2 1.1 1 0.9 Bank−8fm Bank−8fh Bank−8nm Bank−8nh Kin−8fm Kin−8fh Kin−8nm Kin−8nh Figure 3: Mean relative performance of (A) and (B) compared with (C). For each run, the test errors of (A) and (B) are normalized by the test error of (C), and then the values are averaged over runs. Note that the error bars were reasonably small so they were omitted. ½¼¼ Realistic Data Set: Here we use eight practical data sets provided by DELVE [4]: Bank8fm, Bank-8fh, Bank-8nm, Bank-8nh, Kin-8fm, Kin-8fh, Kin-8nm, and Kin-8nh. Each data set includes samples, consisting of -dimensional input and -dimensional output values. For convenience, every attribute is normalized into . ½¾ ¼ ½℄ ½¾ ½ Suppose we are given all input points (i.e., unlabeled samples). Note that output values are unknown. From the pool of unlabeled samples, we choose Ò input points Ü ½¼¼¼ for training and observe the corresponding output values Ý ½¼¼¼. The ½ ½ task is to predict the output values of all unlabeled samples. ½¼¼¼ In this experiment, the test input density independent Gaussian density. Ô ´Üµ and ­ Ø ´¾ ­¾ ÅÄ Ô ´Üµ is unknown. Ø µ  ÜÔ    Ü   ¾ ÅÄ So we estimate it using the ¾ ´¾­¾ µ¡ ÅÄ where Å Ä are the maximum likelihood estimates of the mean and standard ÅÄ and the basis functions be deviation obtained from all unlabeled samples. Let Ô where Ø ³ ´Üµ ¼ ½   ÜÔ   Ü   Ø ¾ ¡ ¾ ¼ for ½¾ ¼ are template points randomly chosen from the pool of unlabeled samples. ´µ We select the training input density ÔÜ Ü from the independent Gaussian density with mean Å Ä and standard deviation ­Å Ä , where  ¼ ¼ ¼ ¾ In this simulation, we can not create the training input points in an arbitrary location because we only have samples. Therefore, we ﬁrst create temporary input points following the determined training input density, and then choose the input points from the pool of unlabeled samples that are closest to the temporary input points. For each data set, we repeat this simulation times, by changing the template points Ø ¼ ½ in each run. ½¾ ½¼¼ ½¼¼ The means and standard deviations of the test error over runs are described in Table 2. The proposed method (A) outperforms the existing method (B) for ﬁve data sets, while it is outperformed by (B) for the other three data sets. We conjecture that the model used for learning is almost correct in these three data sets. This result implies that the proposed method (A) is slightly better than the existing method (B). Figure 3 depicts the relative performance of the proposed method (A) and the existing method (B) compared with the passive learning scheme (C). This shows that (A) outperforms (C) for all eight data sets, while (B) is comparable or is outperformed by (C) for ﬁve data sets. Therefore, the proposed method (A) is overall shown to work better than other schemes. 6 Conclusions We argued that active learning is essentially the situation under the covariate shift—the training input density is different from the test input density. When the model used for learning is correctly speciﬁed, the covariate shift does not matter. However, for misspeciﬁed models, we have to explicitly cope with the covariate shift. In this paper, we proposed a new active learning method based on the weighted least-squares learning. The numerical study showed that the existing method works better than the proposed method if model is correctly speciﬁed. However, the existing method tends to perform poorly once the correctness of the model is violated. On the other hand, the proposed method overall worked reasonably well and it consistently outperformed the passive learning scheme. Therefore, the proposed method would be robust against the misspeciﬁcation of models and thus it is reliable. The proposed method can be theoretically justiﬁed if the model is approximately correct in a weak sense. However, it is no longer valid for totally misspeciﬁed models. A natural future direction would be therefore to devise an active learning method which has theoretical guarantee with totally misspeciﬁed models. It is also important to notice that when the model is totally misspeciﬁed, even learning with optimal training input points would not be successful anyway. In such cases, it is of course important to carry out model selection. In active learning research—including the present paper, however, the location of training input points are designed for a single model at hand. That is, the model should have been chosen before performing active learning. Devising a method for simultaneously optimizing models and the location of training input points would be a more important and promising future direction. Acknowledgments: The author would like to thank MEXT (Grant-in-Aid for Young Scientists 17700142) for partial ﬁnancial support. References [1] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active learning with statistical models. Journal of Artiﬁcial Intelligence Research, 4:129–145, 1996. [2] V. V. Fedorov. Theory of Optimal Experiments. Academic Press, New York, 1972. [3] K. Fukumizu. Statistical active learning in multilayer perceptrons. IEEE Transactions on Neural Networks, 11(1):17–26, 2000. [4] C. E. Rasmussen, R. M. Neal, G. E. Hinton, D. van Camp, M. Revow, Z. Ghahramani, R. Kustra, and R. Tibshirani. The DELVE manual, 1996. [5] H. Shimodaira. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of Statistical Planning and Inference, 90(2):227–244, 2000. [6] M. Sugiyama. Active learning for misspeciﬁed models. Technical report, Department of Computer Science, Tokyo Institute of Technology, 2005.</p><p>20 <a title="nips-2005-20" href="../nips2005/nips-2005-Affine_Structure_From_Sound.html">nips-2005-Affine Structure From Sound</a></p>
<p>Author: Sebastian Thrun</p><p>Abstract: We consider the problem of localizing a set of microphones together with a set of external acoustic events (e.g., hand claps), emitted at unknown times and unknown locations. We propose a solution that approximates this problem under a far ﬁeld approximation deﬁned in the calculus of afﬁne geometry, and that relies on singular value decomposition (SVD) to recover the afﬁne structure of the problem. We then deﬁne low-dimensional optimization techniques for embedding the solution into Euclidean geometry, and further techniques for recovering the locations and emission times of the acoustic events. The approach is useful for the calibration of ad-hoc microphone arrays and sensor networks. 1</p><p>21 <a title="nips-2005-21" href="../nips2005/nips-2005-An_Alternative_Infinite_Mixture_Of_Gaussian_Process_Experts.html">nips-2005-An Alternative Infinite Mixture Of Gaussian Process Experts</a></p>
<p>22 <a title="nips-2005-22" href="../nips2005/nips-2005-An_Analog_Visual_Pre-Processing_Processor_Employing_Cyclic_Line_Access_in_Only-Nearest-Neighbor-Interconnects_Architecture.html">nips-2005-An Analog Visual Pre-Processing Processor Employing Cyclic Line Access in Only-Nearest-Neighbor-Interconnects Architecture</a></p>
<p>23 <a title="nips-2005-23" href="../nips2005/nips-2005-An_Application_of_Markov_Random_Fields_to_Range_Sensing.html">nips-2005-An Application of Markov Random Fields to Range Sensing</a></p>
<p>24 <a title="nips-2005-24" href="../nips2005/nips-2005-An_Approximate_Inference_Approach_for_the_PCA_Reconstruction_Error.html">nips-2005-An Approximate Inference Approach for the PCA Reconstruction Error</a></p>
<p>25 <a title="nips-2005-25" href="../nips2005/nips-2005-An_aVLSI_Cricket_Ear_Model.html">nips-2005-An aVLSI Cricket Ear Model</a></p>
<p>26 <a title="nips-2005-26" href="../nips2005/nips-2005-An_exploration-exploitation_model_based_on_norepinepherine_and_dopamine_activity.html">nips-2005-An exploration-exploitation model based on norepinepherine and dopamine activity</a></p>
<p>27 <a title="nips-2005-27" href="../nips2005/nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>28 <a title="nips-2005-28" href="../nips2005/nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<p>29 <a title="nips-2005-29" href="../nips2005/nips-2005-Analyzing_Coupled_Brain_Sources%3A_Distinguishing_True_from_Spurious_Interaction.html">nips-2005-Analyzing Coupled Brain Sources: Distinguishing True from Spurious Interaction</a></p>
<p>30 <a title="nips-2005-30" href="../nips2005/nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>31 <a title="nips-2005-31" href="../nips2005/nips-2005-Asymptotics_of_Gaussian_Regularized_Least_Squares.html">nips-2005-Asymptotics of Gaussian Regularized Least Squares</a></p>
<p>32 <a title="nips-2005-32" href="../nips2005/nips-2005-Augmented_Rescorla-Wagner_and_Maximum_Likelihood_Estimation.html">nips-2005-Augmented Rescorla-Wagner and Maximum Likelihood Estimation</a></p>
<p>33 <a title="nips-2005-33" href="../nips2005/nips-2005-Bayesian_Sets.html">nips-2005-Bayesian Sets</a></p>
<p>34 <a title="nips-2005-34" href="../nips2005/nips-2005-Bayesian_Surprise_Attracts_Human_Attention.html">nips-2005-Bayesian Surprise Attracts Human Attention</a></p>
<p>35 <a title="nips-2005-35" href="../nips2005/nips-2005-Bayesian_model_learning_in_human_visual_perception.html">nips-2005-Bayesian model learning in human visual perception</a></p>
<p>36 <a title="nips-2005-36" href="../nips2005/nips-2005-Bayesian_models_of_human_action_understanding.html">nips-2005-Bayesian models of human action understanding</a></p>
<p>37 <a title="nips-2005-37" href="../nips2005/nips-2005-Benchmarking_Non-Parametric_Statistical_Tests.html">nips-2005-Benchmarking Non-Parametric Statistical Tests</a></p>
<p>38 <a title="nips-2005-38" href="../nips2005/nips-2005-Beyond_Gaussian_Processes%3A_On_the_Distributions_of_Infinite_Networks.html">nips-2005-Beyond Gaussian Processes: On the Distributions of Infinite Networks</a></p>
<p>39 <a title="nips-2005-39" href="../nips2005/nips-2005-Beyond_Pair-Based_STDP%3A_a_Phenomenological_Rule_for_Spike_Triplet_and_Frequency_Effects.html">nips-2005-Beyond Pair-Based STDP: a Phenomenological Rule for Spike Triplet and Frequency Effects</a></p>
<p>40 <a title="nips-2005-40" href="../nips2005/nips-2005-CMOL_CrossNets%3A_Possible_Neuromorphic_Nanoelectronic_Circuits.html">nips-2005-CMOL CrossNets: Possible Neuromorphic Nanoelectronic Circuits</a></p>
<p>41 <a title="nips-2005-41" href="../nips2005/nips-2005-Coarse_sample_complexity_bounds_for_active_learning.html">nips-2005-Coarse sample complexity bounds for active learning</a></p>
<p>42 <a title="nips-2005-42" href="../nips2005/nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>43 <a title="nips-2005-43" href="../nips2005/nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>44 <a title="nips-2005-44" href="../nips2005/nips-2005-Computing_the_Solution_Path_for_the_Regularized_Support_Vector_Regression.html">nips-2005-Computing the Solution Path for the Regularized Support Vector Regression</a></p>
<p>45 <a title="nips-2005-45" href="../nips2005/nips-2005-Conditional_Visual_Tracking_in_Kernel_Space.html">nips-2005-Conditional Visual Tracking in Kernel Space</a></p>
<p>46 <a title="nips-2005-46" href="../nips2005/nips-2005-Consensus_Propagation.html">nips-2005-Consensus Propagation</a></p>
<p>47 <a title="nips-2005-47" href="../nips2005/nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>48 <a title="nips-2005-48" href="../nips2005/nips-2005-Context_as_Filtering.html">nips-2005-Context as Filtering</a></p>
<p>49 <a title="nips-2005-49" href="../nips2005/nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>50 <a title="nips-2005-50" href="../nips2005/nips-2005-Convex_Neural_Networks.html">nips-2005-Convex Neural Networks</a></p>
<p>51 <a title="nips-2005-51" href="../nips2005/nips-2005-Correcting_sample_selection_bias_in_maximum_entropy_density_estimation.html">nips-2005-Correcting sample selection bias in maximum entropy density estimation</a></p>
<p>52 <a title="nips-2005-52" href="../nips2005/nips-2005-Correlated_Topic_Models.html">nips-2005-Correlated Topic Models</a></p>
<p>53 <a title="nips-2005-53" href="../nips2005/nips-2005-Cyclic_Equilibria_in_Markov_Games.html">nips-2005-Cyclic Equilibria in Markov Games</a></p>
<p>54 <a title="nips-2005-54" href="../nips2005/nips-2005-Data-Driven_Online_to_Batch_Conversions.html">nips-2005-Data-Driven Online to Batch Conversions</a></p>
<p>55 <a title="nips-2005-55" href="../nips2005/nips-2005-Describing_Visual_Scenes_using_Transformed_Dirichlet_Processes.html">nips-2005-Describing Visual Scenes using Transformed Dirichlet Processes</a></p>
<p>56 <a title="nips-2005-56" href="../nips2005/nips-2005-Diffusion_Maps%2C_Spectral_Clustering_and_Eigenfunctions_of_Fokker-Planck_Operators.html">nips-2005-Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators</a></p>
<p>57 <a title="nips-2005-57" href="../nips2005/nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>58 <a title="nips-2005-58" href="../nips2005/nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>59 <a title="nips-2005-59" href="../nips2005/nips-2005-Dual-Tree_Fast_Gauss_Transforms.html">nips-2005-Dual-Tree Fast Gauss Transforms</a></p>
<p>60 <a title="nips-2005-60" href="../nips2005/nips-2005-Dynamic_Social_Network_Analysis_using_Latent_Space_Models.html">nips-2005-Dynamic Social Network Analysis using Latent Space Models</a></p>
<p>61 <a title="nips-2005-61" href="../nips2005/nips-2005-Dynamical_Synapses_Give_Rise_to_a_Power-Law_Distribution_of_Neuronal_Avalanches.html">nips-2005-Dynamical Synapses Give Rise to a Power-Law Distribution of Neuronal Avalanches</a></p>
<p>62 <a title="nips-2005-62" href="../nips2005/nips-2005-Efficient_Estimation_of_OOMs.html">nips-2005-Efficient Estimation of OOMs</a></p>
<p>63 <a title="nips-2005-63" href="../nips2005/nips-2005-Efficient_Unsupervised_Learning_for_Localization_and_Detection_in_Object_Categories.html">nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</a></p>
<p>64 <a title="nips-2005-64" href="../nips2005/nips-2005-Efficient_estimation_of_hidden_state_dynamics_from_spike_trains.html">nips-2005-Efficient estimation of hidden state dynamics from spike trains</a></p>
<p>65 <a title="nips-2005-65" href="../nips2005/nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>66 <a title="nips-2005-66" href="../nips2005/nips-2005-Estimation_of_Intrinsic_Dimensionality_Using_High-Rate_Vector_Quantization.html">nips-2005-Estimation of Intrinsic Dimensionality Using High-Rate Vector Quantization</a></p>
<p>67 <a title="nips-2005-67" href="../nips2005/nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>68 <a title="nips-2005-68" href="../nips2005/nips-2005-Factorial_Switching_Kalman_Filters_for_Condition_Monitoring_in_Neonatal_Intensive_Care.html">nips-2005-Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care</a></p>
<p>69 <a title="nips-2005-69" href="../nips2005/nips-2005-Fast_Gaussian_Process_Regression_using_KD-Trees.html">nips-2005-Fast Gaussian Process Regression using KD-Trees</a></p>
<p>70 <a title="nips-2005-70" href="../nips2005/nips-2005-Fast_Information_Value_for_Graphical_Models.html">nips-2005-Fast Information Value for Graphical Models</a></p>
<p>71 <a title="nips-2005-71" href="../nips2005/nips-2005-Fast_Krylov_Methods_for_N-Body_Learning.html">nips-2005-Fast Krylov Methods for N-Body Learning</a></p>
<p>72 <a title="nips-2005-72" href="../nips2005/nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>73 <a title="nips-2005-73" href="../nips2005/nips-2005-Fast_biped_walking_with_a_reflexive_controller_and_real-time_policy_searching.html">nips-2005-Fast biped walking with a reflexive controller and real-time policy searching</a></p>
<p>74 <a title="nips-2005-74" href="../nips2005/nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>75 <a title="nips-2005-75" href="../nips2005/nips-2005-Fixing_two_weaknesses_of_the_Spectral_Method.html">nips-2005-Fixing two weaknesses of the Spectral Method</a></p>
<p>76 <a title="nips-2005-76" href="../nips2005/nips-2005-From_Batch_to_Transductive_Online_Learning.html">nips-2005-From Batch to Transductive Online Learning</a></p>
<p>77 <a title="nips-2005-77" href="../nips2005/nips-2005-From_Lasso_regression_to_Feature_vector_machine.html">nips-2005-From Lasso regression to Feature vector machine</a></p>
<p>78 <a title="nips-2005-78" href="../nips2005/nips-2005-From_Weighted_Classification_to_Policy_Search.html">nips-2005-From Weighted Classification to Policy Search</a></p>
<p>79 <a title="nips-2005-79" href="../nips2005/nips-2005-Fusion_of_Similarity_Data_in_Clustering.html">nips-2005-Fusion of Similarity Data in Clustering</a></p>
<p>80 <a title="nips-2005-80" href="../nips2005/nips-2005-Gaussian_Process_Dynamical_Models.html">nips-2005-Gaussian Process Dynamical Models</a></p>
<p>81 <a title="nips-2005-81" href="../nips2005/nips-2005-Gaussian_Processes_for_Multiuser_Detection_in_CDMA_receivers.html">nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</a></p>
<p>82 <a title="nips-2005-82" href="../nips2005/nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>83 <a title="nips-2005-83" href="../nips2005/nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>84 <a title="nips-2005-84" href="../nips2005/nips-2005-Generalization_in_Clustering_with_Unobserved_Features.html">nips-2005-Generalization in Clustering with Unobserved Features</a></p>
<p>85 <a title="nips-2005-85" href="../nips2005/nips-2005-Generalization_to_Unseen_Cases.html">nips-2005-Generalization to Unseen Cases</a></p>
<p>86 <a title="nips-2005-86" href="../nips2005/nips-2005-Generalized_Nonnegative_Matrix_Approximations_with_Bregman_Divergences.html">nips-2005-Generalized Nonnegative Matrix Approximations with Bregman Divergences</a></p>
<p>87 <a title="nips-2005-87" href="../nips2005/nips-2005-Goal-Based_Imitation_as_Probabilistic_Inference_over_Graphical_Models.html">nips-2005-Goal-Based Imitation as Probabilistic Inference over Graphical Models</a></p>
<p>88 <a title="nips-2005-88" href="../nips2005/nips-2005-Gradient_Flow_Independent_Component_Analysis_in_Micropower_VLSI.html">nips-2005-Gradient Flow Independent Component Analysis in Micropower VLSI</a></p>
<p>89 <a title="nips-2005-89" href="../nips2005/nips-2005-Group_and_Topic_Discovery_from_Relations_and_Their_Attributes.html">nips-2005-Group and Topic Discovery from Relations and Their Attributes</a></p>
<p>90 <a title="nips-2005-90" href="../nips2005/nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs.html">nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</a></p>
<p>91 <a title="nips-2005-91" href="../nips2005/nips-2005-How_fast_to_work%3A_Response_vigor%2C_motivation_and_tonic_dopamine.html">nips-2005-How fast to work: Response vigor, motivation and tonic dopamine</a></p>
<p>92 <a title="nips-2005-92" href="../nips2005/nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>93 <a title="nips-2005-93" href="../nips2005/nips-2005-Ideal_Observers_for_Detecting_Motion%3A_Correspondence_Noise.html">nips-2005-Ideal Observers for Detecting Motion: Correspondence Noise</a></p>
<p>94 <a title="nips-2005-94" href="../nips2005/nips-2005-Identifying_Distributed_Object_Representations_in_Human_Extrastriate_Visual_Cortex.html">nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</a></p>
<p>95 <a title="nips-2005-95" href="../nips2005/nips-2005-Improved_risk_tail_bounds_for_on-line_algorithms.html">nips-2005-Improved risk tail bounds for on-line algorithms</a></p>
<p>96 <a title="nips-2005-96" href="../nips2005/nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>97 <a title="nips-2005-97" href="../nips2005/nips-2005-Inferring_Motor_Programs_from_Images_of_Handwritten_Digits.html">nips-2005-Inferring Motor Programs from Images of Handwritten Digits</a></p>
<p>98 <a title="nips-2005-98" href="../nips2005/nips-2005-Infinite_latent_feature_models_and_the_Indian_buffet_process.html">nips-2005-Infinite latent feature models and the Indian buffet process</a></p>
<p>99 <a title="nips-2005-99" href="../nips2005/nips-2005-Integrate-and-Fire_models_with_adaptation_are_good_enough.html">nips-2005-Integrate-and-Fire models with adaptation are good enough</a></p>
<p>100 <a title="nips-2005-100" href="../nips2005/nips-2005-Interpolating_between_types_and_tokens_by_estimating_power-law_generators.html">nips-2005-Interpolating between types and tokens by estimating power-law generators</a></p>
<p>101 <a title="nips-2005-101" href="../nips2005/nips-2005-Is_Early_Vision_Optimized_for_Extracting_Higher-order_Dependencies%3F.html">nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</a></p>
<p>102 <a title="nips-2005-102" href="../nips2005/nips-2005-Kernelized_Infomax_Clustering.html">nips-2005-Kernelized Infomax Clustering</a></p>
<p>103 <a title="nips-2005-103" href="../nips2005/nips-2005-Kernels_for_gene_regulatory_regions.html">nips-2005-Kernels for gene regulatory regions</a></p>
<p>104 <a title="nips-2005-104" href="../nips2005/nips-2005-Laplacian_Score_for_Feature_Selection.html">nips-2005-Laplacian Score for Feature Selection</a></p>
<p>105 <a title="nips-2005-105" href="../nips2005/nips-2005-Large-Scale_Multiclass_Transduction.html">nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>106 <a title="nips-2005-106" href="../nips2005/nips-2005-Large-scale_biophysical_parameter_estimation_in_single_neurons_via_constrained_linear_regression.html">nips-2005-Large-scale biophysical parameter estimation in single neurons via constrained linear regression</a></p>
<p>107 <a title="nips-2005-107" href="../nips2005/nips-2005-Large_scale_networks_fingerprinting_and_visualization_using_the_k-core_decomposition.html">nips-2005-Large scale networks fingerprinting and visualization using the k-core decomposition</a></p>
<p>108 <a title="nips-2005-108" href="../nips2005/nips-2005-Layered_Dynamic_Textures.html">nips-2005-Layered Dynamic Textures</a></p>
<p>109 <a title="nips-2005-109" href="../nips2005/nips-2005-Learning_Cue-Invariant_Visual_Responses.html">nips-2005-Learning Cue-Invariant Visual Responses</a></p>
<p>110 <a title="nips-2005-110" href="../nips2005/nips-2005-Learning_Depth_from_Single_Monocular_Images.html">nips-2005-Learning Depth from Single Monocular Images</a></p>
<p>111 <a title="nips-2005-111" href="../nips2005/nips-2005-Learning_Influence_among_Interacting_Markov_Chains.html">nips-2005-Learning Influence among Interacting Markov Chains</a></p>
<p>112 <a title="nips-2005-112" href="../nips2005/nips-2005-Learning_Minimum_Volume_Sets.html">nips-2005-Learning Minimum Volume Sets</a></p>
<p>113 <a title="nips-2005-113" href="../nips2005/nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<p>114 <a title="nips-2005-114" href="../nips2005/nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>115 <a title="nips-2005-115" href="../nips2005/nips-2005-Learning_Shared_Latent_Structure_for_Image_Synthesis_and_Robotic_Imitation.html">nips-2005-Learning Shared Latent Structure for Image Synthesis and Robotic Imitation</a></p>
<p>116 <a title="nips-2005-116" href="../nips2005/nips-2005-Learning_Topology_with_the_Generative_Gaussian_Graph_and_the_EM_Algorithm.html">nips-2005-Learning Topology with the Generative Gaussian Graph and the EM Algorithm</a></p>
<p>117 <a title="nips-2005-117" href="../nips2005/nips-2005-Learning_from_Data_of_Variable_Quality.html">nips-2005-Learning from Data of Variable Quality</a></p>
<p>118 <a title="nips-2005-118" href="../nips2005/nips-2005-Learning_in_Silicon%3A_Timing_is_Everything.html">nips-2005-Learning in Silicon: Timing is Everything</a></p>
<p>119 <a title="nips-2005-119" href="../nips2005/nips-2005-Learning_to_Control_an_Octopus_Arm_with_Gaussian_Process_Temporal_Difference_Methods.html">nips-2005-Learning to Control an Octopus Arm with Gaussian Process Temporal Difference Methods</a></p>
<p>120 <a title="nips-2005-120" href="../nips2005/nips-2005-Learning_vehicular_dynamics%2C_with_application_to_modeling_helicopters.html">nips-2005-Learning vehicular dynamics, with application to modeling helicopters</a></p>
<p>121 <a title="nips-2005-121" href="../nips2005/nips-2005-Location-based_activity_recognition.html">nips-2005-Location-based activity recognition</a></p>
<p>122 <a title="nips-2005-122" href="../nips2005/nips-2005-Logic_and_MRF_Circuitry_for_Labeling_Occluding_and_Thinline_Visual_Contours.html">nips-2005-Logic and MRF Circuitry for Labeling Occluding and Thinline Visual Contours</a></p>
<p>123 <a title="nips-2005-123" href="../nips2005/nips-2005-Maximum_Margin_Semi-Supervised_Learning_for_Structured_Variables.html">nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</a></p>
<p>124 <a title="nips-2005-124" href="../nips2005/nips-2005-Measuring_Shared_Information_and_Coordinated_Activity_in_Neuronal_Networks.html">nips-2005-Measuring Shared Information and Coordinated Activity in Neuronal Networks</a></p>
<p>125 <a title="nips-2005-125" href="../nips2005/nips-2005-Message_passing_for_task_redistribution_on_sparse_graphs.html">nips-2005-Message passing for task redistribution on sparse graphs</a></p>
<p>126 <a title="nips-2005-126" href="../nips2005/nips-2005-Metric_Learning_by_Collapsing_Classes.html">nips-2005-Metric Learning by Collapsing Classes</a></p>
<p>127 <a title="nips-2005-127" href="../nips2005/nips-2005-Mixture_Modeling_by_Affinity_Propagation.html">nips-2005-Mixture Modeling by Affinity Propagation</a></p>
<p>128 <a title="nips-2005-128" href="../nips2005/nips-2005-Modeling_Memory_Transfer_and_Saving_in_Cerebellar_Motor_Learning.html">nips-2005-Modeling Memory Transfer and Saving in Cerebellar Motor Learning</a></p>
<p>129 <a title="nips-2005-129" href="../nips2005/nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>130 <a title="nips-2005-130" href="../nips2005/nips-2005-Modeling_Neuronal_Interactivity_using_Dynamic_Bayesian_Networks.html">nips-2005-Modeling Neuronal Interactivity using Dynamic Bayesian Networks</a></p>
<p>131 <a title="nips-2005-131" href="../nips2005/nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>132 <a title="nips-2005-132" href="../nips2005/nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>133 <a title="nips-2005-133" href="../nips2005/nips-2005-Nested_sampling_for_Potts_models.html">nips-2005-Nested sampling for Potts models</a></p>
<p>134 <a title="nips-2005-134" href="../nips2005/nips-2005-Neural_mechanisms_of_contrast_dependent_receptive_field_size_in_V1.html">nips-2005-Neural mechanisms of contrast dependent receptive field size in V1</a></p>
<p>135 <a title="nips-2005-135" href="../nips2005/nips-2005-Neuronal_Fiber_Delineation_in_Area_of_Edema_from_Diffusion_Weighted_MRI.html">nips-2005-Neuronal Fiber Delineation in Area of Edema from Diffusion Weighted MRI</a></p>
<p>136 <a title="nips-2005-136" href="../nips2005/nips-2005-Noise_and_the_two-thirds_power_Law.html">nips-2005-Noise and the two-thirds power Law</a></p>
<p>137 <a title="nips-2005-137" href="../nips2005/nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<p>138 <a title="nips-2005-138" href="../nips2005/nips-2005-Non-Local_Manifold_Parzen_Windows.html">nips-2005-Non-Local Manifold Parzen Windows</a></p>
<p>139 <a title="nips-2005-139" href="../nips2005/nips-2005-Non-iterative_Estimation_with_Perturbed_Gaussian_Markov_Processes.html">nips-2005-Non-iterative Estimation with Perturbed Gaussian Markov Processes</a></p>
<p>140 <a title="nips-2005-140" href="../nips2005/nips-2005-Nonparametric_inference_of_prior_probabilities_from_Bayes-optimal_behavior.html">nips-2005-Nonparametric inference of prior probabilities from Bayes-optimal behavior</a></p>
<p>141 <a title="nips-2005-141" href="../nips2005/nips-2005-Norepinephrine_and_Neural_Interrupts.html">nips-2005-Norepinephrine and Neural Interrupts</a></p>
<p>142 <a title="nips-2005-142" href="../nips2005/nips-2005-Oblivious_Equilibrium%3A_A_Mean_Field_Approximation_for_Large-Scale_Dynamic_Games.html">nips-2005-Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games</a></p>
<p>143 <a title="nips-2005-143" href="../nips2005/nips-2005-Off-Road_Obstacle_Avoidance_through_End-to-End_Learning.html">nips-2005-Off-Road Obstacle Avoidance through End-to-End Learning</a></p>
<p>144 <a title="nips-2005-144" href="../nips2005/nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>145 <a title="nips-2005-145" href="../nips2005/nips-2005-On_Local_Rewards_and_Scaling_Distributed_Reinforcement_Learning.html">nips-2005-On Local Rewards and Scaling Distributed Reinforcement Learning</a></p>
<p>146 <a title="nips-2005-146" href="../nips2005/nips-2005-On_the_Accuracy_of_Bounded_Rationality%3A_How_Far_from_Optimal_Is_Fast_and_Frugal%3F.html">nips-2005-On the Accuracy of Bounded Rationality: How Far from Optimal Is Fast and Frugal?</a></p>
<p>147 <a title="nips-2005-147" href="../nips2005/nips-2005-On_the_Convergence_of_Eigenspaces_in_Kernel_Principal_Component_Analysis.html">nips-2005-On the Convergence of Eigenspaces in Kernel Principal Component Analysis</a></p>
<p>148 <a title="nips-2005-148" href="../nips2005/nips-2005-Online_Discovery_and_Learning_of_Predictive_State_Representations.html">nips-2005-Online Discovery and Learning of Predictive State Representations</a></p>
<p>149 <a title="nips-2005-149" href="../nips2005/nips-2005-Optimal_cue_selection_strategy.html">nips-2005-Optimal cue selection strategy</a></p>
<p>150 <a title="nips-2005-150" href="../nips2005/nips-2005-Optimizing_spatio-temporal_filters_for_improving_Brain-Computer_Interfacing.html">nips-2005-Optimizing spatio-temporal filters for improving Brain-Computer Interfacing</a></p>
<p>151 <a title="nips-2005-151" href="../nips2005/nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<p>152 <a title="nips-2005-152" href="../nips2005/nips-2005-Phase_Synchrony_Rate_for_the_Recognition_of_Motor_Imagery_in_Brain-Computer_Interface.html">nips-2005-Phase Synchrony Rate for the Recognition of Motor Imagery in Brain-Computer Interface</a></p>
<p>153 <a title="nips-2005-153" href="../nips2005/nips-2005-Policy-Gradient_Methods_for_Planning.html">nips-2005-Policy-Gradient Methods for Planning</a></p>
<p>154 <a title="nips-2005-154" href="../nips2005/nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>155 <a title="nips-2005-155" href="../nips2005/nips-2005-Predicting_EMG_Data_from_M1_Neurons_with_Variational_Bayesian_Least_Squares.html">nips-2005-Predicting EMG Data from M1 Neurons with Variational Bayesian Least Squares</a></p>
<p>156 <a title="nips-2005-156" href="../nips2005/nips-2005-Prediction_and_Change_Detection.html">nips-2005-Prediction and Change Detection</a></p>
<p>157 <a title="nips-2005-157" href="../nips2005/nips-2005-Principles_of_real-time_computing_with_feedback_applied_to_cortical_microcircuit_models.html">nips-2005-Principles of real-time computing with feedback applied to cortical microcircuit models</a></p>
<p>158 <a title="nips-2005-158" href="../nips2005/nips-2005-Products_of_%60%60Edge-perts.html">nips-2005-Products of ``Edge-perts</a></p>
<p>159 <a title="nips-2005-159" href="../nips2005/nips-2005-Q-Clustering.html">nips-2005-Q-Clustering</a></p>
<p>160 <a title="nips-2005-160" href="../nips2005/nips-2005-Query_by_Committee_Made_Real.html">nips-2005-Query by Committee Made Real</a></p>
<p>161 <a title="nips-2005-161" href="../nips2005/nips-2005-Radial_Basis_Function_Network_for_Multi-task_Learning.html">nips-2005-Radial Basis Function Network for Multi-task Learning</a></p>
<p>162 <a title="nips-2005-162" href="../nips2005/nips-2005-Rate_Distortion_Codes_in_Sensor_Networks%3A_A_System-level_Analysis.html">nips-2005-Rate Distortion Codes in Sensor Networks: A System-level Analysis</a></p>
<p>163 <a title="nips-2005-163" href="../nips2005/nips-2005-Recovery_of_Jointly_Sparse_Signals_from_Few_Random_Projections.html">nips-2005-Recovery of Jointly Sparse Signals from Few Random Projections</a></p>
<p>164 <a title="nips-2005-164" href="../nips2005/nips-2005-Representing_Part-Whole_Relationships_in_Recurrent_Neural_Networks.html">nips-2005-Representing Part-Whole Relationships in Recurrent Neural Networks</a></p>
<p>165 <a title="nips-2005-165" href="../nips2005/nips-2005-Response_Analysis_of_Neuronal_Population_with_Synaptic_Depression.html">nips-2005-Response Analysis of Neuronal Population with Synaptic Depression</a></p>
<p>166 <a title="nips-2005-166" href="../nips2005/nips-2005-Robust_Fisher_Discriminant_Analysis.html">nips-2005-Robust Fisher Discriminant Analysis</a></p>
<p>167 <a title="nips-2005-167" href="../nips2005/nips-2005-Robust_design_of_biological_experiments.html">nips-2005-Robust design of biological experiments</a></p>
<p>168 <a title="nips-2005-168" href="../nips2005/nips-2005-Rodeo%3A_Sparse_Nonparametric_Regression_in_High_Dimensions.html">nips-2005-Rodeo: Sparse Nonparametric Regression in High Dimensions</a></p>
<p>169 <a title="nips-2005-169" href="../nips2005/nips-2005-Saliency_Based_on_Information_Maximization.html">nips-2005-Saliency Based on Information Maximization</a></p>
<p>170 <a title="nips-2005-170" href="../nips2005/nips-2005-Scaling_Laws_in_Natural_Scenes_and_the_Inference_of_3D_Shape.html">nips-2005-Scaling Laws in Natural Scenes and the Inference of 3D Shape</a></p>
<p>171 <a title="nips-2005-171" href="../nips2005/nips-2005-Searching_for_Character_Models.html">nips-2005-Searching for Character Models</a></p>
<p>172 <a title="nips-2005-172" href="../nips2005/nips-2005-Selecting_Landmark_Points_for_Sparse_Manifold_Learning.html">nips-2005-Selecting Landmark Points for Sparse Manifold Learning</a></p>
<p>173 <a title="nips-2005-173" href="../nips2005/nips-2005-Sensory_Adaptation_within_a_Bayesian_Framework_for_Perception.html">nips-2005-Sensory Adaptation within a Bayesian Framework for Perception</a></p>
<p>174 <a title="nips-2005-174" href="../nips2005/nips-2005-Separation_of_Music_Signals_by_Harmonic_Structure_Modeling.html">nips-2005-Separation of Music Signals by Harmonic Structure Modeling</a></p>
<p>175 <a title="nips-2005-175" href="../nips2005/nips-2005-Sequence_and_Tree_Kernels_with_Statistical_Feature_Mining.html">nips-2005-Sequence and Tree Kernels with Statistical Feature Mining</a></p>
<p>176 <a title="nips-2005-176" href="../nips2005/nips-2005-Silicon_growth_cones_map_silicon_retina.html">nips-2005-Silicon growth cones map silicon retina</a></p>
<p>177 <a title="nips-2005-177" href="../nips2005/nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>178 <a title="nips-2005-178" href="../nips2005/nips-2005-Soft_Clustering_on_Graphs.html">nips-2005-Soft Clustering on Graphs</a></p>
<p>179 <a title="nips-2005-179" href="../nips2005/nips-2005-Sparse_Gaussian_Processes_using_Pseudo-inputs.html">nips-2005-Sparse Gaussian Processes using Pseudo-inputs</a></p>
<p>180 <a title="nips-2005-180" href="../nips2005/nips-2005-Spectral_Bounds_for_Sparse_PCA%3A_Exact_and_Greedy_Algorithms.html">nips-2005-Spectral Bounds for Sparse PCA: Exact and Greedy Algorithms</a></p>
<p>181 <a title="nips-2005-181" href="../nips2005/nips-2005-Spiking_Inputs_to_a_Winner-take-all_Network.html">nips-2005-Spiking Inputs to a Winner-take-all Network</a></p>
<p>182 <a title="nips-2005-182" href="../nips2005/nips-2005-Statistical_Convergence_of_Kernel_CCA.html">nips-2005-Statistical Convergence of Kernel CCA</a></p>
<p>183 <a title="nips-2005-183" href="../nips2005/nips-2005-Stimulus_Evoked_Independent_Factor_Analysis_of_MEG_Data_with_Large_Background_Activity.html">nips-2005-Stimulus Evoked Independent Factor Analysis of MEG Data with Large Background Activity</a></p>
<p>184 <a title="nips-2005-184" href="../nips2005/nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>185 <a title="nips-2005-185" href="../nips2005/nips-2005-Subsequence_Kernels_for_Relation_Extraction.html">nips-2005-Subsequence Kernels for Relation Extraction</a></p>
<p>186 <a title="nips-2005-186" href="../nips2005/nips-2005-TD%280%29_Leads_to_Better_Policies_than_Approximate_Value_Iteration.html">nips-2005-TD(0) Leads to Better Policies than Approximate Value Iteration</a></p>
<p>187 <a title="nips-2005-187" href="../nips2005/nips-2005-Temporal_Abstraction_in_Temporal-difference_Networks.html">nips-2005-Temporal Abstraction in Temporal-difference Networks</a></p>
<p>188 <a title="nips-2005-188" href="../nips2005/nips-2005-Temporally_changing_synaptic_plasticity.html">nips-2005-Temporally changing synaptic plasticity</a></p>
<p>189 <a title="nips-2005-189" href="../nips2005/nips-2005-Tensor_Subspace_Analysis.html">nips-2005-Tensor Subspace Analysis</a></p>
<p>190 <a title="nips-2005-190" href="../nips2005/nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>191 <a title="nips-2005-191" href="../nips2005/nips-2005-The_Forgetron%3A_A_Kernel-Based_Perceptron_on_a_Fixed_Budget.html">nips-2005-The Forgetron: A Kernel-Based Perceptron on a Fixed Budget</a></p>
<p>192 <a title="nips-2005-192" href="../nips2005/nips-2005-The_Information-Form_Data_Association_Filter.html">nips-2005-The Information-Form Data Association Filter</a></p>
<p>193 <a title="nips-2005-193" href="../nips2005/nips-2005-The_Role_of_Top-down_and_Bottom-up_Processes_in_Guiding_Eye_Movements_during_Visual_Search.html">nips-2005-The Role of Top-down and Bottom-up Processes in Guiding Eye Movements during Visual Search</a></p>
<p>194 <a title="nips-2005-194" href="../nips2005/nips-2005-Top-Down_Control_of_Visual_Attention%3A_A_Rational_Account.html">nips-2005-Top-Down Control of Visual Attention: A Rational Account</a></p>
<p>195 <a title="nips-2005-195" href="../nips2005/nips-2005-Transfer_learning_for_text_classification.html">nips-2005-Transfer learning for text classification</a></p>
<p>196 <a title="nips-2005-196" href="../nips2005/nips-2005-Two_view_learning%3A_SVM-2K%2C_Theory_and_Practice.html">nips-2005-Two view learning: SVM-2K, Theory and Practice</a></p>
<p>197 <a title="nips-2005-197" href="../nips2005/nips-2005-Unbiased_Estimator_of_Shape_Parameter_for_Spiking_Irregularities_under_Changing_Environments.html">nips-2005-Unbiased Estimator of Shape Parameter for Spiking Irregularities under Changing Environments</a></p>
<p>198 <a title="nips-2005-198" href="../nips2005/nips-2005-Using_%60%60epitomes%27%27_to_model_genetic_diversity%3A_Rational_design_of_HIV_vaccine_cocktails.html">nips-2005-Using ``epitomes'' to model genetic diversity: Rational design of HIV vaccine cocktails</a></p>
<p>199 <a title="nips-2005-199" href="../nips2005/nips-2005-Value_Function_Approximation_with_Diffusion_Wavelets_and_Laplacian_Eigenfunctions.html">nips-2005-Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions</a></p>
<p>200 <a title="nips-2005-200" href="../nips2005/nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>201 <a title="nips-2005-201" href="../nips2005/nips-2005-Variational_Bayesian_Stochastic_Complexity_of_Mixture_Models.html">nips-2005-Variational Bayesian Stochastic Complexity of Mixture Models</a></p>
<p>202 <a title="nips-2005-202" href="../nips2005/nips-2005-Variational_EM_Algorithms_for_Non-Gaussian_Latent_Variable_Models.html">nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</a></p>
<p>203 <a title="nips-2005-203" href="../nips2005/nips-2005-Visual_Encoding_with_Jittering_Eyes.html">nips-2005-Visual Encoding with Jittering Eyes</a></p>
<p>204 <a title="nips-2005-204" href="../nips2005/nips-2005-Walk-Sum_Interpretation_and_Analysis_of_Gaussian_Belief_Propagation.html">nips-2005-Walk-Sum Interpretation and Analysis of Gaussian Belief Propagation</a></p>
<p>205 <a title="nips-2005-205" href="../nips2005/nips-2005-Worst-Case_Bounds_for_Gaussian_Process_Models.html">nips-2005-Worst-Case Bounds for Gaussian Process Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
