<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>nips 2007 knowledge graph</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="#">nips2007</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>nips 2007 knowledge graph</h1>
<br/><h3>similar papers computed by tfidf model</h3><br/><h3>similar papers computed by <a title="lsi-model" href="./nips2007_lsi.html">lsi model</a></h3><br/><h3>similar papers computed by <a title="lda-model" href="./nips2007_lda.html">lda model</a></h3><br/><h2>papers list:</h2><p>1 <a title="nips-2007-1" href="../nips2007/nips-2007-A_Bayesian_Framework_for_Cross-Situational_Word-Learning.html">nips-2007-A Bayesian Framework for Cross-Situational Word-Learning</a></p>
<p>Author: Noah Goodman, Joshua B. Tenenbaum, Michael J. Black</p><p>Abstract: For infants, early word learning is a chicken-and-egg problem. One way to learn a word is to observe that it co-occurs with a particular referent across different situations. Another way is to use the social context of an utterance to infer the intended referent of a word. Here we present a Bayesian model of cross-situational word learning, and an extension of this model that also learns which social cues are relevant to determining reference. We test our model on a small corpus of mother-infant interaction and ﬁnd it performs better than competing models. Finally, we show that our model accounts for experimental phenomena including mutual exclusivity, fast-mapping, and generalization from social cues. To understand the difﬁculty of an infant word-learner, imagine walking down the street with a friend who suddenly says “dax blicket philbin na ﬁvy!” while at the same time wagging her elbow. If you knew any of these words you might infer from the syntax of her sentence that blicket is a novel noun, and hence the name of a novel object. At the same time, if you knew that this friend indicated her attention by wagging her elbow at objects, you might infer that she intends to refer to an object in a nearby show window. On the other hand if you already knew that “blicket” meant the object in the window, you might be able to infer these elements of syntax and social cues. Thus, the problem of early word-learning is a classic chicken-and-egg puzzle: in order to learn word meanings, learners must use their knowledge of the rest of language (including rules of syntax, parts of speech, and other word meanings) as well as their knowledge of social situations. But in order to learn about the facts of their language they must ﬁrst learn some words, and in order to determine which cues matter for establishing reference (for instance, pointing and looking at an object but normally not waggling your elbow) they must ﬁrst have a way to know the intended referent in some situations. For theories of language acquisition, there are two common ways out of this dilemma. The ﬁrst involves positing a wide range of innate structures which determine the syntax and categories of a language and which social cues are informative. (Though even when all of these elements are innately determined using them to learn a language from evidence may not be trivial [1].) The other alternative involves bootstrapping: learning some words, then using those words to learn how to learn more. This paper gives a proposal for the second alternative. We ﬁrst present a Bayesian model of how learners could use a statistical strategy—cross-situational word-learning—to learn how words map to objects, independent of syntactic and social cues. We then extend this model to a true bootstrapping situation: using social cues to learn words while using words to learn social cues. Finally, we examine several important phenomena in word learning: mutual exclusivity (the tendency to assign novel words to novel referents), fast-mapping (the ability to assign a novel word in a linguistic context to a novel referent after only a single use), and social generalization (the ability to use social context to learn the referent of a novel word). Without adding additional specialized machinery, we show how these can be explained within our model as the result of domain-general probabilistic inference mechanisms operating over the linguistic domain. 1 Os r, b Is Ws Figure 1: Graphical model describing the generation of words (Ws ) from an intention (Is ) and lexicon ( ), and intention from the objects present in a situation (Os ). The plate indicates multiple copies of the model for different situation/utterance pairs (s). Dotted portions indicate additions to include the generation of social cues Ss from intentions. Ss ∀s 1 The Model Behind each linguistic utterance is a meaning that the speaker intends to communicate. Our model operates by attempting to infer this intended meaning (which we call the intent) on the basis of the utterance itself and observations of the physical and social context. For the purpose of modeling early word learning—which consists primarily of learning words for simple object categories—in our model, we assume that intents are simply groups of objects. To state the model formally, we assume the non-linguistic situation consists of a set Os of objects and that utterances are unordered sets of words Ws 1 . The lexicon is a (many-to-many) map from words to objects, which captures the meaning of those words. (Syntax enters our model only obliquely by different treatment of words depending on whether they are in the lexicon or not—that is, whether they are common nouns or other types of words.) In this setting the speaker’s intention will be captured by a set of objects in the situation to which she intends to refer: Is ⊆ Os . This setup is indicated in the graphical model of Fig. 1. Different situation-utterance pairs Ws , Os are independent given the lexicon , giving: P (Ws |Is , ) · P (Is |Os ). P (W| , O) = s (1) Is We further simplify by assuming that P (Is |Os ) ∝ 1 (which could be reﬁned by adding a more detailed model of the communicative intentions a person is likely to form in different situations). We will assume that words in the utterance are generated independently given the intention and the lexicon and that the length of the utterance is observed. Each word is then generated from the intention set and lexicon by ﬁrst choosing whether the word is a referential word or a non-referential word (from a binomial distribution of weight γ), then, for referential words, choosing which object in the intent it refers to (uniformly). This process gives: P (Ws |Is , ) = (1 − γ)PNR (w| ) + γ w∈Ws x∈Is 1 PR (w|x, ) . |Is | The probability of word w referring to object x is PR (w|x, ) ∝ δx∈ w occurring as a non-referring word is PNR (w| ) ∝ 1 if (w) = ∅, κ otherwise. (w) , (2) and the probability of word (3) (this probability is a distribution over all words in the vocabulary, not just those in lexicon ). The constant κ is a penalty for using a word in the lexicon as a non-referring word—this penalty indirectly enforces a light-weight difference between two different groups of words (parts-of-speech): words that refer and words that do not refer. Because the generative structure of this model exposes the role of speaker’s intentions, it is straightforward to add non-linguistic social cues. We assume that social cues such as pointing are generated 1 Note that, since we ignore word order, the distribution of words in a sentence should be exchangeable given the lexicon and situation. This implies, by de Finetti’s theorem, that they are independent conditioned on a latent state—we assume that the latent state giving rise to words is the intention of the speaker. 2 from the speaker’s intent independently of the linguistic aspects (as shown in the dotted arrows of Fig. 1). With the addition of social cues Ss , Eq. 1 becomes: P (Ws |Is , ) · P (Ss |Is ) · P (Is |Os ). P (W| , O) = s (4) Is We assume that the social cues are a set Si (x) of independent binary (cue present or not) feature values for each object x ∈ Os , which are generated through a noisy-or process: P (Si (x)=1|Is , ri , bi ) = 1 − (1 − bi )(1 − ri )δx∈Is . (5) Here ri is the relevance of cue i, while bi is its base rate. For the model without social cues the posterior probability of a lexicon given a set of situated utterances is: P ( |W, O) ∝ P (W| , O)P ( ). (6) And for the model with social cues the joint posterior over lexicon and cue parameters is: P ( , r, b|W, O) ∝ P (W| , r, b, O)P ( )P (r, b). (7) We take the prior probability of a lexicon to be exponential in its size: P ( ) ∝ e−α| | , and the prior probability of social cue parameters to be uniform. Given the model above and the corpus described below, we found the best lexicon (or lexicon and cue parameters) according to Eq. 6 and 7 by MAP inference using stochastic search2 . 2 Previous work While cross-situational word-learning has been widely discussed in the empirical literature, e.g., [2], there have been relatively few attempts to model this process computationally. Siskind [3] created an ambitious model which used deductive rules to make hypotheses about propositional word meanings their use across situations. This model achieved surprising success in learning word meanings in artiﬁcial corpora, but was extremely complex and relied on the availability of fully coded representations of the meaning of each sentence, making it difﬁcult to extend to empirical corpus data. More recently, Yu and Ballard [4] have used a machine translation model (similar to IBM Translation Model I) to learn word-object association probabilities. In their study, they used a pre-existing corpus of mother-infant interactions and coded the objects present during each utterance (an example from this corpus—illustrated with our own coding scheme—is shown in Fig. 2). They applied their translation model to estimate the probability of an object given a word, creating a table of associations between words and objects. Using this table, they extracted a lexicon (a group of word-object mappings) which was relatively accurate in its guesses about the names of objects that were being talked about. They further extended their model to incorporate prosodic emphasis on words (a useful cue which we will not discuss here) and joint attention on objects. Joint attention was coded by hand, isolating a subset of objects which were attended to by both mother and infant. Their results reﬂected a sizable increase in recall with the use of social cues. 3 Materials and Assessment Methods To test the performance of our model on natural data, we used the Rollins section of the CHILDES corpus[5]. For comparison with the model by Yu and Ballard [4], we chose the ﬁles me03 and di06, each of which consisted of approximately ten minutes of interaction between a mother and a preverbal infant playing with objects found in a box of toys. Because we were not able to obtain the exact corpus Yu and Ballard used, we recoded the objects in the videos and added a coding of social cues co-occurring with each utterance. We annotated each utterance with the set of objects visible to the infant and with a social coding scheme (for an illustrated example, see Figure 2). Our social code included seven features: infants eyes, infants hands, infants mouth, infant touching, mothers hands, mothers eyes, mother touching. For each utterance, this coding created an object by social feature matrix. 2 In order to speed convergence we used a simulated tempering scheme with three temperature chains and a range of data-driven proposals. 3 Figure 2: A still frame from our corpus showing the coding of objects and social cues. We coded all mid-sized objects visible to the infant as well as social information including what both mother and infant were touching and looking at. We evaluated all models based on their coverage of a gold-standard lexicon, computing precision (how many of the word-object mappings in a lexicon were correct relative to the gold-standard), recall (how many of the total correct mappings were found), and their geometric mean, F-score. However, the gold-standard lexicon for word-learning is not obvious. For instance, should it include the mapping between the plural “pigs” or the sound “oink” and the object PIG? Should a goldstandard lexicon include word-object pairings that are correct but were not present in the learning situation? In the results we report, we included those pairings which would be useful for a child to learn (e.g., “oink” → PIG) but not including those pairings which were not observed to co-occur in the corpus (however, modifying these decisions did not affect the qualitative pattern of results). 4 Results For the purpose of comparison, we give scores for several other models on the same corpus. We implemented a range of simple associative models based on co-occurrence frequency, conditional probability (both word given object and object given word), and point-wise mutual information. In each of these models, we computed the relevant statistic across the entire corpus and then created a lexicon by including all word-object pairings for which the association statistic met a threshold value. We additionally implemented a translation model (based on Yu and Ballard [4]). Because Yu and Ballard did not include details on how they evaluated their model, we scored it in the same way as the other associative models, by creating an association matrix based on the scores P (O|W ) (as given in equation (3) in their paper) and then creating a lexicon based on a threshold value. In order to simulate this type of threshold value for our model, we searched for the MAP lexicon over a range of parameters α in our prior (the larger the prior value, the less probable a larger lexicon, thus this manipulation served to create more or less selective lexicons) . Base model. In Figure 3, we plot the precision and the recall for lexicons across a range of prior parameter values for our model and the full range of threshold values for the translation model and two of the simple association models (since results for the conditional probability models were very similar but slightly inferior to the performance of mutual information, we did not include them). For our model, we averaged performance at each threshold value across three runs of 5000 search iterations each. Our model performed better than any of the other models on a number of dimensions (best lexicon shown in Table 1), both achieving the highest F-score and showing a better tradeoff between precision and recall at sub-optimal threshold values. The translation model also performed well, increasing precision as the threshold of association was raised. Surprisingly, standard cooccurrence statistics proved to be relatively ineffective at extracting high-scoring lexicons: at any given threshold value, these models included a very large number of incorrect pairs. Table 1: The best lexicon found by the Bayesian model (α=11, γ=0.2, κ=0.01). baby → book hand → hand bigbird → bird hat → hat on → ring bird → rattle meow → kitty ring → ring 4 birdie → duck moocow → cow sheep → sheep book → book oink → pig 1 Co!occurrence frequency Mutual information Translation model Bayesian model 0.9 0.8 0.7 recall 0.6 0.5 0.4 0.3 F=0.54 F=0.44 F=0.21 F=0.12 0.2 0.1 0 0 0.2 0.4 0.6 precision 0.8 1 Figure 3: Comparison of models on corpus data: we plot model precision vs. recall across a range of threshold values for each model (see text). Unlike standard ROC curves for classiﬁcation tasks, the precision and recall of a lexicon depends on the entire lexicon, and irregularities in the curves reﬂect the small size of the lexicons). One additional virtue of our model over other associative models is its ability to determine which objects the speaker intended to refer to. In Table 2, we give some examples of situations in which the model correctly inferred the objects that the speaker was talking about. Social model. While the addition of social cues did not increase corpus performance above that found in the base model, the lexicons which were found by the social model did have several properties that were not present in the base model. First, the model effectively and quickly converged on the social cues that we found subjectively important in viewing the corpus videos. The two cues which were consistently found relevant across the model were (1) the target of the infant’s gaze and (2) the caregiver’s hand. These data are especially interesting in light of the speculation that infants initially believe their own point of gaze is a good cue to reference, and must learn over the second year that the true cue is the caregiver’s point of gaze, not their own [6]. Second, while the social model did not outperform the base model on the full corpus (where many words were paired with their referents several times), on a smaller corpus (taking every other utterance), the social cue model did slightly outperform a model without social cues (max F-score=0.43 vs. 0.37). Third, the addition of social cues allowed the model to infer the intent of a speaker even in the absence of a word being used. In the right-hand column of Table 2, we give an example of a situation in which the caregiver simply says ”see that?” but from the direction of the infant’s eyes and the location of her hand, the model correctly infers that she is talking about the COW, not either of the other possible referents. This kind of inference might lead the way in allowing infants to learn words like pronouns, which serve pick out an unambiguous focus of attention (one that is so obvious based on social and contextual cues that it does not need to be named). Finally, in the next section we show that the addition of social cues to the model allows correct performance in experimental tests of social generalization which only children older than 18 months can pass, suggesting perhaps that the social model is closer to the strategy used by more mature word learners. Table 2: Intentions inferred by the Bayesian model after having learned a lexicon from the corpus. (IE=Infant’s eyes, CH=Caregiver’s hands). Words Objects Social Cues Inferred intention “look at the moocow” COW GIRL BEAR “see the bear by the rattle?” BEAR RATTLE COW COW BEAR RATTLE 5 “see that?” BEAR RATTLE COW IE & CH→COW COW situation: !7.3, corpus: !631.1, total: !638.4</p><p>2 <a title="nips-2007-2" href="../nips2007/nips-2007-A_Bayesian_LDA-based_model_for_semi-supervised_part-of-speech_tagging.html">nips-2007-A Bayesian LDA-based model for semi-supervised part-of-speech tagging</a></p>
<p>Author: Kristina Toutanova, Mark Johnson</p><p>Abstract: We present a novel Bayesian model for semi-supervised part-of-speech tagging. Our model extends the Latent Dirichlet Allocation model and incorporates the intuition that words’ distributions over tags, p(t|w), are sparse. In addition we introduce a model for determining the set of possible tags of a word which captures important dependencies in the ambiguity classes of words. Our model outperforms the best previously proposed model for this task on a standard dataset. 1</p><p>3 <a title="nips-2007-3" href="../nips2007/nips-2007-A_Bayesian_Model_of_Conditioned_Perception.html">nips-2007-A Bayesian Model of Conditioned Perception</a></p>
<p>Author: Alan Stocker, Eero P. Simoncelli</p><p>Abstract: unkown-abstract</p><p>4 <a title="nips-2007-4" href="../nips2007/nips-2007-A_Constraint_Generation_Approach_to_Learning_Stable_Linear_Dynamical_Systems.html">nips-2007-A Constraint Generation Approach to Learning Stable Linear Dynamical Systems</a></p>
<p>Author: Byron Boots, Geoffrey J. Gordon, Sajid M. Siddiqi</p><p>Abstract: Stability is a desirable characteristic for linear dynamical systems, but it is often ignored by algorithms that learn these systems from data. We propose a novel method for learning stable linear dynamical systems: we formulate an approximation of the problem as a convex program, start with a solution to a relaxed version of the program, and incrementally add constraints to improve stability. Rather than continuing to generate constraints until we reach a feasible solution, we test stability at each step; because the convex program is only an approximation of the desired problem, this early stopping rule can yield a higher-quality solution. We apply our algorithm to the task of learning dynamic textures from image sequences as well as to modeling biosurveillance drug-sales data. The constraint generation approach leads to noticeable improvement in the quality of simulated sequences. We compare our method to those of Lacy and Bernstein [1, 2], with positive results in terms of accuracy, quality of simulated sequences, and efﬁciency. 1</p><p>5 <a title="nips-2007-5" href="../nips2007/nips-2007-A_Game-Theoretic_Approach_to_Apprenticeship_Learning.html">nips-2007-A Game-Theoretic Approach to Apprenticeship Learning</a></p>
<p>Author: Umar Syed, Robert E. Schapire</p><p>Abstract: We study the problem of an apprentice learning to behave in an environment with an unknown reward function by observing the behavior of an expert. We follow on the work of Abbeel and Ng [1] who considered a framework in which the true reward function is assumed to be a linear combination of a set of known and observable features. We give a new algorithm that, like theirs, is guaranteed to learn a policy that is nearly as good as the expert’s, given enough examples. However, unlike their algorithm, we show that ours may produce a policy that is substantially better than the expert’s. Moreover, our algorithm is computationally faster, is easier to implement, and can be applied even in the absence of an expert. The method is based on a game-theoretic view of the problem, which leads naturally to a direct application of the multiplicative-weights algorithm of Freund and Schapire [2] for playing repeated matrix games. In addition to our formal presentation and analysis of the new algorithm, we sketch how the method can be applied when the transition function itself is unknown, and we provide an experimental demonstration of the algorithm on a toy video-game environment. 1</p><p>6 <a title="nips-2007-6" href="../nips2007/nips-2007-A_General_Boosting_Method_and_its_Application_to_Learning_Ranking_Functions_for_Web_Search.html">nips-2007-A General Boosting Method and its Application to Learning Ranking Functions for Web Search</a></p>
<p>Author: Zhaohui Zheng, Hongyuan Zha, Tong Zhang, Olivier Chapelle, Keke Chen, Gordon Sun</p><p>Abstract: We present a general boosting method extending functional gradient boosting to optimize complex loss functions that are encountered in many machine learning problems. Our approach is based on optimization of quadratic upper bounds of the loss functions which allows us to present a rigorous convergence analysis of the algorithm. More importantly, this general framework enables us to use a standard regression base learner such as single regression tree for £tting any loss function. We illustrate an application of the proposed method in learning ranking functions for Web search by combining both preference data and labeled data for training. We present experimental results for Web search using data from a commercial search engine that show signi£cant improvements of our proposed methods over some existing methods. 1</p><p>7 <a title="nips-2007-7" href="../nips2007/nips-2007-A_Kernel_Statistical_Test_of_Independence.html">nips-2007-A Kernel Statistical Test of Independence</a></p>
<p>Author: Arthur Gretton, Kenji Fukumizu, Choon H. Teo, Le Song, Bernhard Schölkopf, Alex J. Smola</p><p>Abstract: Although kernel measures of independence have been widely applied in machine learning (notably in kernel ICA), there is as yet no method to determine whether they have detected statistically signiﬁcant dependence. We provide a novel test of the independence hypothesis for one particular kernel independence measure, the Hilbert-Schmidt independence criterion (HSIC). The resulting test costs O(m2 ), where m is the sample size. We demonstrate that this test outperforms established contingency table and functional correlation-based tests, and that this advantage is greater for multivariate data. Finally, we show the HSIC test also applies to text (and to structured data more generally), for which no other independence test presently exists.</p><p>8 <a title="nips-2007-8" href="../nips2007/nips-2007-A_New_View_of_Automatic_Relevance_Determination.html">nips-2007-A New View of Automatic Relevance Determination</a></p>
<p>Author: David P. Wipf, Srikantan S. Nagarajan</p><p>Abstract: Automatic relevance determination (ARD) and the closely-related sparse Bayesian learning (SBL) framework are effective tools for pruning large numbers of irrelevant features leading to a sparse explanatory subset. However, popular update rules used for ARD are either difﬁcult to extend to more general problems of interest or are characterized by non-ideal convergence properties. Moreover, it remains unclear exactly how ARD relates to more traditional MAP estimation-based methods for learning sparse representations (e.g., the Lasso). This paper furnishes an alternative means of expressing the ARD cost function using auxiliary functions that naturally addresses both of these issues. First, the proposed reformulation of ARD can naturally be optimized by solving a series of re-weighted 1 problems. The result is an efﬁcient, extensible algorithm that can be implemented using standard convex programming toolboxes and is guaranteed to converge to a local minimum (or saddle point). Secondly, the analysis reveals that ARD is exactly equivalent to performing standard MAP estimation in weight space using a particular feature- and noise-dependent, non-factorial weight prior. We then demonstrate that this implicit prior maintains several desirable advantages over conventional priors with respect to feature selection. Overall these results suggest alternative cost functions and update procedures for selecting features and promoting sparse solutions in a variety of general situations. In particular, the methodology readily extends to handle problems such as non-negative sparse coding and covariance component estimation. 1</p><p>9 <a title="nips-2007-9" href="../nips2007/nips-2007-A_Probabilistic_Approach_to_Language_Change.html">nips-2007-A Probabilistic Approach to Language Change</a></p>
<p>Author: Alexandre Bouchard-côté, Percy Liang, Dan Klein, Thomas L. Griffiths</p><p>Abstract: We present a probabilistic approach to language change in which word forms are represented by phoneme sequences that undergo stochastic edits along the branches of a phylogenetic tree. This framework combines the advantages of the classical comparative method with the robustness of corpus-based probabilistic models. We use this framework to explore the consequences of two different schemes for deﬁning probabilistic models of phonological change, evaluating these schemes by reconstructing ancient word forms of Romance languages. The result is an efﬁcient inference procedure for automatically inferring ancient word forms from modern languages, which can be generalized to support inferences about linguistic phylogenies. 1</p><p>10 <a title="nips-2007-10" href="../nips2007/nips-2007-A_Randomized_Algorithm_for_Large_Scale_Support_Vector_Learning.html">nips-2007-A Randomized Algorithm for Large Scale Support Vector Learning</a></p>
<p>Author: Krishnan Kumar, Chiru Bhattacharya, Ramesh Hariharan</p><p>Abstract: This paper investigates the application of randomized algorithms for large scale SVM learning. The key contribution of the paper is to show that, by using ideas random projections, the minimal number of support vectors required to solve almost separable classiﬁcation problems, such that the solution obtained is near optimal with a very high probability, is given by O(log n); if on removal of properly chosen O(log n) points the data becomes linearly separable then it is called almost separable. The second contribution is a sampling based algorithm, motivated from randomized algorithms, which solves a SVM problem by considering subsets of the dataset which are greater in size than the number of support vectors for the problem. These two ideas are combined to obtain an algorithm for SVM classiﬁcation problems which performs the learning by considering only O(log n) points at a time. Experiments done on synthetic and real life datasets show that the algorithm does scale up state of the art SVM solvers in terms of memory required and execution time without loss in accuracy. It is to be noted that the algorithm presented here nicely complements existing large scale SVM learning approaches as it can be used to scale up any SVM solver. 1</p><p>11 <a title="nips-2007-11" href="../nips2007/nips-2007-A_Risk_Minimization_Principle_for_a_Class_of_Parzen_Estimators.html">nips-2007-A Risk Minimization Principle for a Class of Parzen Estimators</a></p>
<p>Author: Kristiaan Pelckmans, Johan Suykens, Bart D. Moor</p><p>Abstract: This paper1 explores the use of a Maximal Average Margin (MAM) optimality principle for the design of learning algorithms. It is shown that the application of this risk minimization principle results in a class of (computationally) simple learning machines similar to the classical Parzen window classiﬁer. A direct relation with the Rademacher complexities is established, as such facilitating analysis and providing a notion of certainty of prediction. This analysis is related to Support Vector Machines by means of a margin transformation. The power of the MAM principle is illustrated further by application to ordinal regression tasks, resulting in an O(n) algorithm able to process large datasets in reasonable time. 1</p><p>12 <a title="nips-2007-12" href="../nips2007/nips-2007-A_Spectral_Regularization_Framework_for_Multi-Task_Structure_Learning.html">nips-2007-A Spectral Regularization Framework for Multi-Task Structure Learning</a></p>
<p>Author: Andreas Argyriou, Massimiliano Pontil, Yiming Ying, Charles A. Micchelli</p><p>Abstract: Learning the common structure shared by a set of supervised tasks is an important practical and theoretical problem. Knowledge of this structure may lead to better generalization performance on the tasks and may also facilitate learning new tasks. We propose a framework for solving this problem, which is based on regularization with spectral functions of matrices. This class of regularization problems exhibits appealing computational properties and can be optimized efﬁciently by an alternating minimization algorithm. In addition, we provide a necessary and sufﬁcient condition for convexity of the regularizer. We analyze concrete examples of the framework, which are equivalent to regularization with Lp matrix norms. Experiments on two real data sets indicate that the algorithm scales well with the number of tasks and improves on state of the art statistical performance. 1</p><p>13 <a title="nips-2007-13" href="../nips2007/nips-2007-A_Unified_Near-Optimal_Estimator_For_Dimension_Reduction_in_%24l_%5Calpha%24_%28%240%3C%5Calpha%5Cleq_2%24%29_Using_Stable_Random_Projections.html">nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</a></p>
<p>Author: Ping Li, Trevor J. Hastie</p><p>Abstract: Many tasks (e.g., clustering) in machine learning only require the lα distances instead of the original data. For dimension reductions in the lα norm (0 < α ≤ 2), the method of stable random projections can efﬁciently compute the lα distances in massive datasets (e.g., the Web or massive data streams) in one pass of the data. The estimation task for stable random projections has been an interesting topic. We propose a simple estimator based on the fractional power of the samples (projected data), which is surprisingly near-optimal in terms of the asymptotic variance. In fact, it achieves the Cram´ r-Rao bound when α = 2 and α = 0+. This e new result will be useful when applying stable random projections to distancebased clustering, classiﬁcations, kernels, massive data streams etc.</p><p>14 <a title="nips-2007-14" href="../nips2007/nips-2007-A_configurable_analog_VLSI_neural_network_with_spiking_neurons_and_self-regulating_plastic_synapses.html">nips-2007-A configurable analog VLSI neural network with spiking neurons and self-regulating plastic synapses</a></p>
<p>Author: Massimiliano Giulioni, Mario Pannunzi, Davide Badoni, Vittorio Dante, Paolo D. Giudice</p><p>Abstract: We summarize the implementation of an analog VLSI chip hosting a network of 32 integrate-and-ﬁre (IF) neurons with spike-frequency adaptation and 2,048 Hebbian plastic bistable spike-driven stochastic synapses endowed with a selfregulating mechanism which stops unnecessary synaptic changes. The synaptic matrix can be ﬂexibly conﬁgured and provides both recurrent and AER-based connectivity with external, AER compliant devices. We demonstrate the ability of the network to efﬁciently classify overlapping patterns, thanks to the self-regulating mechanism.</p><p>15 <a title="nips-2007-15" href="../nips2007/nips-2007-A_general_agnostic_active_learning_algorithm.html">nips-2007-A general agnostic active learning algorithm</a></p>
<p>Author: Sanjoy Dasgupta, Claire Monteleoni, Daniel J. Hsu</p><p>Abstract: We present an agnostic active learning algorithm for any hypothesis class of bounded VC dimension under arbitrary data distributions. Most previous work on active learning either makes strong distributional assumptions, or else is computationally prohibitive. Our algorithm extends the simple scheme of Cohn, Atlas, and Ladner [1] to the agnostic setting, using reductions to supervised learning that harness generalization bounds in a simple but subtle manner. We provide a fall-back guarantee that bounds the algorithm’s label complexity by the agnostic PAC sample complexity. Our analysis yields asymptotic label complexity improvements for certain hypothesis classes and distributions. We also demonstrate improvements experimentally. 1</p><p>16 <a title="nips-2007-16" href="../nips2007/nips-2007-A_learning_framework_for_nearest_neighbor_search.html">nips-2007-A learning framework for nearest neighbor search</a></p>
<p>Author: Lawrence Cayton, Sanjoy Dasgupta</p><p>Abstract: Can we leverage learning techniques to build a fast nearest-neighbor (ANN) retrieval data structure? We present a general learning framework for the NN problem in which sample queries are used to learn the parameters of a data structure that minimize the retrieval time and/or the miss rate. We explore the potential of this novel framework through two popular NN data structures: KD-trees and the rectilinear structures employed by locality sensitive hashing. We derive a generalization theory for these data structure classes and present simple learning algorithms for both. Experimental results reveal that learning often improves on the already strong performance of these data structures. 1</p><p>17 <a title="nips-2007-17" href="../nips2007/nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>Author: Omer Bobrowski, Ron Meir, Shy Shoham, Yonina Eldar</p><p>Abstract: It is becoming increasingly evident that organisms acting in uncertain dynamical environments often employ exact or approximate Bayesian statistical calculations in order to continuously estimate the environmental state, integrate information from multiple sensory modalities, form predictions and choose actions. What is less clear is how these putative computations are implemented by cortical neural networks. An additional level of complexity is introduced because these networks observe the world through spike trains received from primary sensory afferents, rather than directly. A recent line of research has described mechanisms by which such computations can be implemented using a network of neurons whose activity directly represents a probability distribution across the possible “world states”. Much of this work, however, uses various approximations, which severely restrict the domain of applicability of these implementations. Here we make use of rigorous mathematical results from the theory of continuous time point process ﬁltering, and show how optimal real-time state estimation and prediction may be implemented in a general setting using linear neural networks. We demonstrate the applicability of the approach with several examples, and relate the required network properties to the statistical nature of the environment, thereby quantifying the compatibility of a given network with its environment. 1</p><p>18 <a title="nips-2007-18" href="../nips2007/nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>Author: Gwenn Englebienne, Tim Cootes, Magnus Rattray</p><p>Abstract: The present work aims to model the correspondence between facial motion and speech. The face and sound are modelled separately, with phonemes being the link between both. We propose a sequential model and evaluate its suitability for the generation of the facial animation from a sequence of phonemes, which we obtain from speech. We evaluate the results both by computing the error between generated sequences and real video, as well as with a rigorous double-blind test with human subjects. Experiments show that our model compares favourably to other existing methods and that the sequences generated are comparable to real video sequences. 1</p><p>19 <a title="nips-2007-19" href="../nips2007/nips-2007-Active_Preference_Learning_with_Discrete_Choice_Data.html">nips-2007-Active Preference Learning with Discrete Choice Data</a></p>
<p>Author: Brochu Eric, Nando D. Freitas, Abhijeet Ghosh</p><p>Abstract: We propose an active learning algorithm that learns a continuous valuation model from discrete preferences. The algorithm automatically decides what items are best presented to an individual in order to ﬁnd the item that they value highly in as few trials as possible, and exploits quirks of human psychology to minimize time and cognitive burden. To do this, our algorithm maximizes the expected improvement at each query without accurately modelling the entire valuation surface, which would be needlessly expensive. The problem is particularly difﬁcult because the space of choices is inﬁnite. We demonstrate the effectiveness of the new algorithm compared to related active learning methods. We also embed the algorithm within a decision making tool for assisting digital artists in rendering materials. The tool ﬁnds the best parameters while minimizing the number of queries. 1</p><p>20 <a title="nips-2007-20" href="../nips2007/nips-2007-Adaptive_Embedded_Subgraph_Algorithms_using_Walk-Sum_Analysis.html">nips-2007-Adaptive Embedded Subgraph Algorithms using Walk-Sum Analysis</a></p>
<p>Author: Venkat Chandrasekaran, Alan S. Willsky, Jason K. Johnson</p><p>Abstract: We consider the estimation problem in Gaussian graphical models with arbitrary structure. We analyze the Embedded Trees algorithm, which solves a sequence of problems on tractable subgraphs thereby leading to the solution of the estimation problem on an intractable graph. Our analysis is based on the recently developed walk-sum interpretation of Gaussian estimation. We show that non-stationary iterations of the Embedded Trees algorithm using any sequence of subgraphs converge in walk-summable models. Based on walk-sum calculations, we develop adaptive methods that optimize the choice of subgraphs used at each iteration with a view to achieving maximum reduction in error. These adaptive procedures provide a signiﬁcant speedup in convergence over stationary iterative methods, and also appear to converge in a larger class of models. 1</p><p>21 <a title="nips-2007-21" href="../nips2007/nips-2007-Adaptive_Online_Gradient_Descent.html">nips-2007-Adaptive Online Gradient Descent</a></p>
<p>22 <a title="nips-2007-22" href="../nips2007/nips-2007-Agreement-Based_Learning.html">nips-2007-Agreement-Based Learning</a></p>
<p>23 <a title="nips-2007-23" href="../nips2007/nips-2007-An_Analysis_of_Convex_Relaxations_for_MAP_Estimation.html">nips-2007-An Analysis of Convex Relaxations for MAP Estimation</a></p>
<p>24 <a title="nips-2007-24" href="../nips2007/nips-2007-An_Analysis_of_Inference_with_the_Universum.html">nips-2007-An Analysis of Inference with the Universum</a></p>
<p>25 <a title="nips-2007-25" href="../nips2007/nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<p>26 <a title="nips-2007-26" href="../nips2007/nips-2007-An_online_Hebbian_learning_rule_that_performs_Independent_Component_Analysis.html">nips-2007-An online Hebbian learning rule that performs Independent Component Analysis</a></p>
<p>27 <a title="nips-2007-27" href="../nips2007/nips-2007-Anytime_Induction_of_Cost-sensitive_Trees.html">nips-2007-Anytime Induction of Cost-sensitive Trees</a></p>
<p>28 <a title="nips-2007-28" href="../nips2007/nips-2007-Augmented_Functional_Time_Series_Representation_and_Forecasting_with_Gaussian_Processes.html">nips-2007-Augmented Functional Time Series Representation and Forecasting with Gaussian Processes</a></p>
<p>29 <a title="nips-2007-29" href="../nips2007/nips-2007-Automatic_Generation_of_Social_Tags_for_Music_Recommendation.html">nips-2007-Automatic Generation of Social Tags for Music Recommendation</a></p>
<p>30 <a title="nips-2007-30" href="../nips2007/nips-2007-Bayes-Adaptive_POMDPs.html">nips-2007-Bayes-Adaptive POMDPs</a></p>
<p>31 <a title="nips-2007-31" href="../nips2007/nips-2007-Bayesian_Agglomerative_Clustering_with_Coalescents.html">nips-2007-Bayesian Agglomerative Clustering with Coalescents</a></p>
<p>32 <a title="nips-2007-32" href="../nips2007/nips-2007-Bayesian_Co-Training.html">nips-2007-Bayesian Co-Training</a></p>
<p>33 <a title="nips-2007-33" href="../nips2007/nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>34 <a title="nips-2007-34" href="../nips2007/nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>35 <a title="nips-2007-35" href="../nips2007/nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>36 <a title="nips-2007-36" href="../nips2007/nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>37 <a title="nips-2007-37" href="../nips2007/nips-2007-Blind_channel_identification_for_speech_dereverberation_using_l1-norm_sparse_learning.html">nips-2007-Blind channel identification for speech dereverberation using l1-norm sparse learning</a></p>
<p>38 <a title="nips-2007-38" href="../nips2007/nips-2007-Boosting_Algorithms_for_Maximizing_the_Soft_Margin.html">nips-2007-Boosting Algorithms for Maximizing the Soft Margin</a></p>
<p>39 <a title="nips-2007-39" href="../nips2007/nips-2007-Boosting_the_Area_under_the_ROC_Curve.html">nips-2007-Boosting the Area under the ROC Curve</a></p>
<p>40 <a title="nips-2007-40" href="../nips2007/nips-2007-Bundle_Methods_for_Machine_Learning.html">nips-2007-Bundle Methods for Machine Learning</a></p>
<p>41 <a title="nips-2007-41" href="../nips2007/nips-2007-COFI_RANK_-_Maximum_Margin_Matrix_Factorization_for_Collaborative_Ranking.html">nips-2007-COFI RANK - Maximum Margin Matrix Factorization for Collaborative Ranking</a></p>
<p>42 <a title="nips-2007-42" href="../nips2007/nips-2007-CPR_for_CSPs%3A_A_Probabilistic_Relaxation_of_Constraint_Propagation.html">nips-2007-CPR for CSPs: A Probabilistic Relaxation of Constraint Propagation</a></p>
<p>43 <a title="nips-2007-43" href="../nips2007/nips-2007-Catching_Change-points_with_Lasso.html">nips-2007-Catching Change-points with Lasso</a></p>
<p>44 <a title="nips-2007-44" href="../nips2007/nips-2007-Catching_Up_Faster_in_Bayesian_Model_Selection_and_Model_Averaging.html">nips-2007-Catching Up Faster in Bayesian Model Selection and Model Averaging</a></p>
<p>45 <a title="nips-2007-45" href="../nips2007/nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>46 <a title="nips-2007-46" href="../nips2007/nips-2007-Cluster_Stability_for_Finite_Samples.html">nips-2007-Cluster Stability for Finite Samples</a></p>
<p>47 <a title="nips-2007-47" href="../nips2007/nips-2007-Collapsed_Variational_Inference_for_HDP.html">nips-2007-Collapsed Variational Inference for HDP</a></p>
<p>48 <a title="nips-2007-48" href="../nips2007/nips-2007-Collective_Inference_on_Markov_Models_for_Modeling_Bird_Migration.html">nips-2007-Collective Inference on Markov Models for Modeling Bird Migration</a></p>
<p>49 <a title="nips-2007-49" href="../nips2007/nips-2007-Colored_Maximum_Variance_Unfolding.html">nips-2007-Colored Maximum Variance Unfolding</a></p>
<p>50 <a title="nips-2007-50" href="../nips2007/nips-2007-Combined_discriminative_and_generative_articulated_pose_and_non-rigid_shape_estimation.html">nips-2007-Combined discriminative and generative articulated pose and non-rigid shape estimation</a></p>
<p>51 <a title="nips-2007-51" href="../nips2007/nips-2007-Comparing_Bayesian_models_for_multisensory_cue_combination_without_mandatory_integration.html">nips-2007-Comparing Bayesian models for multisensory cue combination without mandatory integration</a></p>
<p>52 <a title="nips-2007-52" href="../nips2007/nips-2007-Competition_Adds_Complexity.html">nips-2007-Competition Adds Complexity</a></p>
<p>53 <a title="nips-2007-53" href="../nips2007/nips-2007-Compressed_Regression.html">nips-2007-Compressed Regression</a></p>
<p>54 <a title="nips-2007-54" href="../nips2007/nips-2007-Computational_Equivalence_of_Fixed_Points_and_No_Regret_Algorithms%2C_and_Convergence_to_Equilibria.html">nips-2007-Computational Equivalence of Fixed Points and No Regret Algorithms, and Convergence to Equilibria</a></p>
<p>55 <a title="nips-2007-55" href="../nips2007/nips-2007-Computing_Robust_Counter-Strategies.html">nips-2007-Computing Robust Counter-Strategies</a></p>
<p>56 <a title="nips-2007-56" href="../nips2007/nips-2007-Configuration_Estimates_Improve_Pedestrian_Finding.html">nips-2007-Configuration Estimates Improve Pedestrian Finding</a></p>
<p>57 <a title="nips-2007-57" href="../nips2007/nips-2007-Congruence_between_model_and_human_attention_reveals_unique_signatures_of_critical_visual_events.html">nips-2007-Congruence between model and human attention reveals unique signatures of critical visual events</a></p>
<p>58 <a title="nips-2007-58" href="../nips2007/nips-2007-Consistent_Minimization_of_Clustering_Objective_Functions.html">nips-2007-Consistent Minimization of Clustering Objective Functions</a></p>
<p>59 <a title="nips-2007-59" href="../nips2007/nips-2007-Continuous_Time_Particle_Filtering_for_fMRI.html">nips-2007-Continuous Time Particle Filtering for fMRI</a></p>
<p>60 <a title="nips-2007-60" href="../nips2007/nips-2007-Contraction_Properties_of_VLSI_Cooperative_Competitive_Neural_Networks_of_Spiking_Neurons.html">nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</a></p>
<p>61 <a title="nips-2007-61" href="../nips2007/nips-2007-Convex_Clustering_with_Exemplar-Based_Models.html">nips-2007-Convex Clustering with Exemplar-Based Models</a></p>
<p>62 <a title="nips-2007-62" href="../nips2007/nips-2007-Convex_Learning_with_Invariances.html">nips-2007-Convex Learning with Invariances</a></p>
<p>63 <a title="nips-2007-63" href="../nips2007/nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>64 <a title="nips-2007-64" href="../nips2007/nips-2007-Cooled_and_Relaxed_Survey_Propagation_for_MRFs.html">nips-2007-Cooled and Relaxed Survey Propagation for MRFs</a></p>
<p>65 <a title="nips-2007-65" href="../nips2007/nips-2007-DIFFRAC%3A_a_discriminative_and_flexible_framework_for_clustering.html">nips-2007-DIFFRAC: a discriminative and flexible framework for clustering</a></p>
<p>66 <a title="nips-2007-66" href="../nips2007/nips-2007-Density_Estimation_under_Independent_Similarly_Distributed_Sampling_Assumptions.html">nips-2007-Density Estimation under Independent Similarly Distributed Sampling Assumptions</a></p>
<p>67 <a title="nips-2007-67" href="../nips2007/nips-2007-Direct_Importance_Estimation_with_Model_Selection_and_Its_Application_to_Covariate_Shift_Adaptation.html">nips-2007-Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation</a></p>
<p>68 <a title="nips-2007-68" href="../nips2007/nips-2007-Discovering_Weakly-Interacting_Factors_in_a_Complex_Stochastic_Process.html">nips-2007-Discovering Weakly-Interacting Factors in a Complex Stochastic Process</a></p>
<p>69 <a title="nips-2007-69" href="../nips2007/nips-2007-Discriminative_Batch_Mode_Active_Learning.html">nips-2007-Discriminative Batch Mode Active Learning</a></p>
<p>70 <a title="nips-2007-70" href="../nips2007/nips-2007-Discriminative_K-means_for_Clustering.html">nips-2007-Discriminative K-means for Clustering</a></p>
<p>71 <a title="nips-2007-71" href="../nips2007/nips-2007-Discriminative_Keyword_Selection_Using_Support_Vector_Machines.html">nips-2007-Discriminative Keyword Selection Using Support Vector Machines</a></p>
<p>72 <a title="nips-2007-72" href="../nips2007/nips-2007-Discriminative_Log-Linear_Grammars_with_Latent_Variables.html">nips-2007-Discriminative Log-Linear Grammars with Latent Variables</a></p>
<p>73 <a title="nips-2007-73" href="../nips2007/nips-2007-Distributed_Inference_for_Latent_Dirichlet_Allocation.html">nips-2007-Distributed Inference for Latent Dirichlet Allocation</a></p>
<p>74 <a title="nips-2007-74" href="../nips2007/nips-2007-EEG-Based_Brain-Computer_Interaction%3A_Improved_Accuracy_by_Automatic_Single-Trial_Error_Detection.html">nips-2007-EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection</a></p>
<p>75 <a title="nips-2007-75" href="../nips2007/nips-2007-Efficient_Bayesian_Inference_for_Dynamically_Changing_Graphs.html">nips-2007-Efficient Bayesian Inference for Dynamically Changing Graphs</a></p>
<p>76 <a title="nips-2007-76" href="../nips2007/nips-2007-Efficient_Convex_Relaxation_for_Transductive_Support_Vector_Machine.html">nips-2007-Efficient Convex Relaxation for Transductive Support Vector Machine</a></p>
<p>77 <a title="nips-2007-77" href="../nips2007/nips-2007-Efficient_Inference_for_Distributions_on_Permutations.html">nips-2007-Efficient Inference for Distributions on Permutations</a></p>
<p>78 <a title="nips-2007-78" href="../nips2007/nips-2007-Efficient_Principled_Learning_of_Thin_Junction_Trees.html">nips-2007-Efficient Principled Learning of Thin Junction Trees</a></p>
<p>79 <a title="nips-2007-79" href="../nips2007/nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<p>80 <a title="nips-2007-80" href="../nips2007/nips-2007-Ensemble_Clustering_using_Semidefinite_Programming.html">nips-2007-Ensemble Clustering using Semidefinite Programming</a></p>
<p>81 <a title="nips-2007-81" href="../nips2007/nips-2007-Estimating_disparity_with_confidence_from_energy_neurons.html">nips-2007-Estimating disparity with confidence from energy neurons</a></p>
<p>82 <a title="nips-2007-82" href="../nips2007/nips-2007-Estimating_divergence_functionals_and_the_likelihood_ratio_by_penalized_convex_risk_minimization.html">nips-2007-Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization</a></p>
<p>83 <a title="nips-2007-83" href="../nips2007/nips-2007-Evaluating_Search_Engines_by_Modeling_the_Relationship_Between_Relevance_and_Clicks.html">nips-2007-Evaluating Search Engines by Modeling the Relationship Between Relevance and Clicks</a></p>
<p>84 <a title="nips-2007-84" href="../nips2007/nips-2007-Expectation_Maximization_and_Posterior_Constraints.html">nips-2007-Expectation Maximization and Posterior Constraints</a></p>
<p>85 <a title="nips-2007-85" href="../nips2007/nips-2007-Experience-Guided_Search%3A_A_Theory_of_Attentional_Control.html">nips-2007-Experience-Guided Search: A Theory of Attentional Control</a></p>
<p>86 <a title="nips-2007-86" href="../nips2007/nips-2007-Exponential_Family_Predictive_Representations_of_State.html">nips-2007-Exponential Family Predictive Representations of State</a></p>
<p>87 <a title="nips-2007-87" href="../nips2007/nips-2007-Fast_Variational_Inference_for_Large-scale_Internet_Diagnosis.html">nips-2007-Fast Variational Inference for Large-scale Internet Diagnosis</a></p>
<p>88 <a title="nips-2007-88" href="../nips2007/nips-2007-Fast_and_Scalable_Training_of_Semi-Supervised_CRFs_with_Application_to_Activity_Recognition.html">nips-2007-Fast and Scalable Training of Semi-Supervised CRFs with Application to Activity Recognition</a></p>
<p>89 <a title="nips-2007-89" href="../nips2007/nips-2007-Feature_Selection_Methods_for_Improving_Protein_Structure_Prediction_with_Rosetta.html">nips-2007-Feature Selection Methods for Improving Protein Structure Prediction with Rosetta</a></p>
<p>90 <a title="nips-2007-90" href="../nips2007/nips-2007-FilterBoost%3A_Regression_and_Classification_on_Large_Datasets.html">nips-2007-FilterBoost: Regression and Classification on Large Datasets</a></p>
<p>91 <a title="nips-2007-91" href="../nips2007/nips-2007-Fitted_Q-iteration_in_continuous_action-space_MDPs.html">nips-2007-Fitted Q-iteration in continuous action-space MDPs</a></p>
<p>92 <a title="nips-2007-92" href="../nips2007/nips-2007-Fixing_Max-Product%3A_Convergent_Message_Passing_Algorithms_for_MAP_LP-Relaxations.html">nips-2007-Fixing Max-Product: Convergent Message Passing Algorithms for MAP LP-Relaxations</a></p>
<p>93 <a title="nips-2007-93" href="../nips2007/nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>94 <a title="nips-2007-94" href="../nips2007/nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>95 <a title="nips-2007-95" href="../nips2007/nips-2007-HM-BiTAM%3A_Bilingual_Topic_Exploration%2C_Word_Alignment%2C_and_Translation.html">nips-2007-HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation</a></p>
<p>96 <a title="nips-2007-96" href="../nips2007/nips-2007-Heterogeneous_Component_Analysis.html">nips-2007-Heterogeneous Component Analysis</a></p>
<p>97 <a title="nips-2007-97" href="../nips2007/nips-2007-Hidden_Common_Cause_Relations_in_Relational_Learning.html">nips-2007-Hidden Common Cause Relations in Relational Learning</a></p>
<p>98 <a title="nips-2007-98" href="../nips2007/nips-2007-Hierarchical_Apprenticeship_Learning_with_Application_to_Quadruped_Locomotion.html">nips-2007-Hierarchical Apprenticeship Learning with Application to Quadruped Locomotion</a></p>
<p>99 <a title="nips-2007-99" href="../nips2007/nips-2007-Hierarchical_Penalization.html">nips-2007-Hierarchical Penalization</a></p>
<p>100 <a title="nips-2007-100" href="../nips2007/nips-2007-Hippocampal_Contributions_to_Control%3A_The_Third_Way.html">nips-2007-Hippocampal Contributions to Control: The Third Way</a></p>
<p>101 <a title="nips-2007-101" href="../nips2007/nips-2007-How_SVMs_can_estimate_quantiles_and_the_median.html">nips-2007-How SVMs can estimate quantiles and the median</a></p>
<p>102 <a title="nips-2007-102" href="../nips2007/nips-2007-Incremental_Natural_Actor-Critic_Algorithms.html">nips-2007-Incremental Natural Actor-Critic Algorithms</a></p>
<p>103 <a title="nips-2007-103" href="../nips2007/nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>104 <a title="nips-2007-104" href="../nips2007/nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>105 <a title="nips-2007-105" href="../nips2007/nips-2007-Infinite_State_Bayes-Nets_for_Structured_Domains.html">nips-2007-Infinite State Bayes-Nets for Structured Domains</a></p>
<p>106 <a title="nips-2007-106" href="../nips2007/nips-2007-Invariant_Common_Spatial_Patterns%3A_Alleviating_Nonstationarities_in_Brain-Computer_Interfacing.html">nips-2007-Invariant Common Spatial Patterns: Alleviating Nonstationarities in Brain-Computer Interfacing</a></p>
<p>107 <a title="nips-2007-107" href="../nips2007/nips-2007-Iterative_Non-linear_Dimensionality_Reduction_with_Manifold_Sculpting.html">nips-2007-Iterative Non-linear Dimensionality Reduction with Manifold Sculpting</a></p>
<p>108 <a title="nips-2007-108" href="../nips2007/nips-2007-Kernel_Measures_of_Conditional_Dependence.html">nips-2007-Kernel Measures of Conditional Dependence</a></p>
<p>109 <a title="nips-2007-109" href="../nips2007/nips-2007-Kernels_on_Attributed_Pointsets_with_Applications.html">nips-2007-Kernels on Attributed Pointsets with Applications</a></p>
<p>110 <a title="nips-2007-110" href="../nips2007/nips-2007-Learning_Bounds_for_Domain_Adaptation.html">nips-2007-Learning Bounds for Domain Adaptation</a></p>
<p>111 <a title="nips-2007-111" href="../nips2007/nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>112 <a title="nips-2007-112" href="../nips2007/nips-2007-Learning_Monotonic_Transformations_for_Classification.html">nips-2007-Learning Monotonic Transformations for Classification</a></p>
<p>113 <a title="nips-2007-113" href="../nips2007/nips-2007-Learning_Visual_Attributes.html">nips-2007-Learning Visual Attributes</a></p>
<p>114 <a title="nips-2007-114" href="../nips2007/nips-2007-Learning_and_using_relational_theories.html">nips-2007-Learning and using relational theories</a></p>
<p>115 <a title="nips-2007-115" href="../nips2007/nips-2007-Learning_the_2-D_Topology_of_Images.html">nips-2007-Learning the 2-D Topology of Images</a></p>
<p>116 <a title="nips-2007-116" href="../nips2007/nips-2007-Learning_the_structure_of_manifolds_using_random_projections.html">nips-2007-Learning the structure of manifolds using random projections</a></p>
<p>117 <a title="nips-2007-117" href="../nips2007/nips-2007-Learning_to_classify_complex_patterns_using_a_VLSI_network_of_spiking_neurons.html">nips-2007-Learning to classify complex patterns using a VLSI network of spiking neurons</a></p>
<p>118 <a title="nips-2007-118" href="../nips2007/nips-2007-Learning_with_Transformation_Invariant_Kernels.html">nips-2007-Learning with Transformation Invariant Kernels</a></p>
<p>119 <a title="nips-2007-119" href="../nips2007/nips-2007-Learning_with_Tree-Averaged_Densities_and_Distributions.html">nips-2007-Learning with Tree-Averaged Densities and Distributions</a></p>
<p>120 <a title="nips-2007-120" href="../nips2007/nips-2007-Linear_programming_analysis_of_loopy_belief_propagation_for_weighted_matching.html">nips-2007-Linear programming analysis of loopy belief propagation for weighted matching</a></p>
<p>121 <a title="nips-2007-121" href="../nips2007/nips-2007-Local_Algorithms_for_Approximate_Inference_in_Minor-Excluded_Graphs.html">nips-2007-Local Algorithms for Approximate Inference in Minor-Excluded Graphs</a></p>
<p>122 <a title="nips-2007-122" href="../nips2007/nips-2007-Locality_and_low-dimensions_in_the_prediction_of_natural_experience_from_fMRI.html">nips-2007-Locality and low-dimensions in the prediction of natural experience from fMRI</a></p>
<p>123 <a title="nips-2007-123" href="../nips2007/nips-2007-Loop_Series_and_Bethe_Variational_Bounds_in_Attractive_Graphical_Models.html">nips-2007-Loop Series and Bethe Variational Bounds in Attractive Graphical Models</a></p>
<p>124 <a title="nips-2007-124" href="../nips2007/nips-2007-Managing_Power_Consumption_and_Performance_of_Computing_Systems_Using_Reinforcement_Learning.html">nips-2007-Managing Power Consumption and Performance of Computing Systems Using Reinforcement Learning</a></p>
<p>125 <a title="nips-2007-125" href="../nips2007/nips-2007-Markov_Chain_Monte_Carlo_with_People.html">nips-2007-Markov Chain Monte Carlo with People</a></p>
<p>126 <a title="nips-2007-126" href="../nips2007/nips-2007-McRank%3A_Learning_to_Rank_Using_Multiple_Classification_and_Gradient_Boosting.html">nips-2007-McRank: Learning to Rank Using Multiple Classification and Gradient Boosting</a></p>
<p>127 <a title="nips-2007-127" href="../nips2007/nips-2007-Measuring_Neural_Synchrony_by_Message_Passing.html">nips-2007-Measuring Neural Synchrony by Message Passing</a></p>
<p>128 <a title="nips-2007-128" href="../nips2007/nips-2007-Message_Passing_for_Max-weight_Independent_Set.html">nips-2007-Message Passing for Max-weight Independent Set</a></p>
<p>129 <a title="nips-2007-129" href="../nips2007/nips-2007-Mining_Internet-Scale_Software_Repositories.html">nips-2007-Mining Internet-Scale Software Repositories</a></p>
<p>130 <a title="nips-2007-130" href="../nips2007/nips-2007-Modeling_Natural_Sounds_with_Modulation_Cascade_Processes.html">nips-2007-Modeling Natural Sounds with Modulation Cascade Processes</a></p>
<p>131 <a title="nips-2007-131" href="../nips2007/nips-2007-Modeling_homophily_and_stochastic_equivalence_in_symmetric_relational_data.html">nips-2007-Modeling homophily and stochastic equivalence in symmetric relational data</a></p>
<p>132 <a title="nips-2007-132" href="../nips2007/nips-2007-Modeling_image_patches_with_a_directed_hierarchy_of_Markov_random_fields.html">nips-2007-Modeling image patches with a directed hierarchy of Markov random fields</a></p>
<p>133 <a title="nips-2007-133" href="../nips2007/nips-2007-Modelling_motion_primitives_and_their_timing_in_biologically_executed_movements.html">nips-2007-Modelling motion primitives and their timing in biologically executed movements</a></p>
<p>134 <a title="nips-2007-134" href="../nips2007/nips-2007-Multi-Task_Learning_via_Conic_Programming.html">nips-2007-Multi-Task Learning via Conic Programming</a></p>
<p>135 <a title="nips-2007-135" href="../nips2007/nips-2007-Multi-task_Gaussian_Process_Prediction.html">nips-2007-Multi-task Gaussian Process Prediction</a></p>
<p>136 <a title="nips-2007-136" href="../nips2007/nips-2007-Multiple-Instance_Active_Learning.html">nips-2007-Multiple-Instance Active Learning</a></p>
<p>137 <a title="nips-2007-137" href="../nips2007/nips-2007-Multiple-Instance_Pruning_For_Learning_Efficient_Cascade_Detectors.html">nips-2007-Multiple-Instance Pruning For Learning Efficient Cascade Detectors</a></p>
<p>138 <a title="nips-2007-138" href="../nips2007/nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>139 <a title="nips-2007-139" href="../nips2007/nips-2007-Nearest-Neighbor-Based_Active_Learning_for_Rare_Category_Detection.html">nips-2007-Nearest-Neighbor-Based Active Learning for Rare Category Detection</a></p>
<p>140 <a title="nips-2007-140" href="../nips2007/nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>141 <a title="nips-2007-141" href="../nips2007/nips-2007-New_Outer_Bounds_on_the_Marginal_Polytope.html">nips-2007-New Outer Bounds on the Marginal Polytope</a></p>
<p>142 <a title="nips-2007-142" href="../nips2007/nips-2007-Non-parametric_Modeling_of_Partially_Ranked_Data.html">nips-2007-Non-parametric Modeling of Partially Ranked Data</a></p>
<p>143 <a title="nips-2007-143" href="../nips2007/nips-2007-Object_Recognition_by_Scene_Alignment.html">nips-2007-Object Recognition by Scene Alignment</a></p>
<p>144 <a title="nips-2007-144" href="../nips2007/nips-2007-On_Ranking_in_Survival_Analysis%3A_Bounds_on_the_Concordance_Index.html">nips-2007-On Ranking in Survival Analysis: Bounds on the Concordance Index</a></p>
<p>145 <a title="nips-2007-145" href="../nips2007/nips-2007-On_Sparsity_and_Overcompleteness_in_Image_Models.html">nips-2007-On Sparsity and Overcompleteness in Image Models</a></p>
<p>146 <a title="nips-2007-146" href="../nips2007/nips-2007-On_higher-order_perceptron_algorithms.html">nips-2007-On higher-order perceptron algorithms</a></p>
<p>147 <a title="nips-2007-147" href="../nips2007/nips-2007-One-Pass_Boosting.html">nips-2007-One-Pass Boosting</a></p>
<p>148 <a title="nips-2007-148" href="../nips2007/nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</a></p>
<p>149 <a title="nips-2007-149" href="../nips2007/nips-2007-Optimal_ROC_Curve_for_a_Combination_of_Classifiers.html">nips-2007-Optimal ROC Curve for a Combination of Classifiers</a></p>
<p>150 <a title="nips-2007-150" href="../nips2007/nips-2007-Optimal_models_of_sound_localization_by_barn_owls.html">nips-2007-Optimal models of sound localization by barn owls</a></p>
<p>151 <a title="nips-2007-151" href="../nips2007/nips-2007-Optimistic_Linear_Programming_gives_Logarithmic_Regret_for_Irreducible_MDPs.html">nips-2007-Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs</a></p>
<p>152 <a title="nips-2007-152" href="../nips2007/nips-2007-Parallelizing_Support_Vector_Machines_on_Distributed_Computers.html">nips-2007-Parallelizing Support Vector Machines on Distributed Computers</a></p>
<p>153 <a title="nips-2007-153" href="../nips2007/nips-2007-People_Tracking_with_the_Laplacian_Eigenmaps_Latent_Variable_Model.html">nips-2007-People Tracking with the Laplacian Eigenmaps Latent Variable Model</a></p>
<p>154 <a title="nips-2007-154" href="../nips2007/nips-2007-Predicting_Brain_States_from_fMRI_Data%3A_Incremental_Functional_Principal_Component_Regression.html">nips-2007-Predicting Brain States from fMRI Data: Incremental Functional Principal Component Regression</a></p>
<p>155 <a title="nips-2007-155" href="../nips2007/nips-2007-Predicting_human_gaze_using_low-level_saliency_combined_with_face_detection.html">nips-2007-Predicting human gaze using low-level saliency combined with face detection</a></p>
<p>156 <a title="nips-2007-156" href="../nips2007/nips-2007-Predictive_Matrix-Variate_t_Models.html">nips-2007-Predictive Matrix-Variate t Models</a></p>
<p>157 <a title="nips-2007-157" href="../nips2007/nips-2007-Privacy-Preserving_Belief_Propagation_and_Sampling.html">nips-2007-Privacy-Preserving Belief Propagation and Sampling</a></p>
<p>158 <a title="nips-2007-158" href="../nips2007/nips-2007-Probabilistic_Matrix_Factorization.html">nips-2007-Probabilistic Matrix Factorization</a></p>
<p>159 <a title="nips-2007-159" href="../nips2007/nips-2007-Progressive_mixture_rules_are_deviation_suboptimal.html">nips-2007-Progressive mixture rules are deviation suboptimal</a></p>
<p>160 <a title="nips-2007-160" href="../nips2007/nips-2007-Random_Features_for_Large-Scale_Kernel_Machines.html">nips-2007-Random Features for Large-Scale Kernel Machines</a></p>
<p>161 <a title="nips-2007-161" href="../nips2007/nips-2007-Random_Projections_for_Manifold_Learning.html">nips-2007-Random Projections for Manifold Learning</a></p>
<p>162 <a title="nips-2007-162" href="../nips2007/nips-2007-Random_Sampling_of_States_in_Dynamic_Programming.html">nips-2007-Random Sampling of States in Dynamic Programming</a></p>
<p>163 <a title="nips-2007-163" href="../nips2007/nips-2007-Receding_Horizon_Differential_Dynamic_Programming.html">nips-2007-Receding Horizon Differential Dynamic Programming</a></p>
<p>164 <a title="nips-2007-164" href="../nips2007/nips-2007-Receptive_Fields_without_Spike-Triggering.html">nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>165 <a title="nips-2007-165" href="../nips2007/nips-2007-Regret_Minimization_in_Games_with_Incomplete_Information.html">nips-2007-Regret Minimization in Games with Incomplete Information</a></p>
<p>166 <a title="nips-2007-166" href="../nips2007/nips-2007-Regularized_Boost_for_Semi-Supervised_Learning.html">nips-2007-Regularized Boost for Semi-Supervised Learning</a></p>
<p>167 <a title="nips-2007-167" href="../nips2007/nips-2007-Regulator_Discovery_from_Gene_Expression_Time_Series_of_Malaria_Parasites%3A_a_Hierachical_Approach.html">nips-2007-Regulator Discovery from Gene Expression Time Series of Malaria Parasites: a Hierachical Approach</a></p>
<p>168 <a title="nips-2007-168" href="../nips2007/nips-2007-Reinforcement_Learning_in_Continuous_Action_Spaces_through_Sequential_Monte_Carlo_Methods.html">nips-2007-Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods</a></p>
<p>169 <a title="nips-2007-169" href="../nips2007/nips-2007-Retrieved_context_and_the_discovery_of_semantic_structure.html">nips-2007-Retrieved context and the discovery of semantic structure</a></p>
<p>170 <a title="nips-2007-170" href="../nips2007/nips-2007-Robust_Regression_with_Twinned_Gaussian_Processes.html">nips-2007-Robust Regression with Twinned Gaussian Processes</a></p>
<p>171 <a title="nips-2007-171" href="../nips2007/nips-2007-Scan_Strategies_for_Meteorological_Radars.html">nips-2007-Scan Strategies for Meteorological Radars</a></p>
<p>172 <a title="nips-2007-172" href="../nips2007/nips-2007-Scene_Segmentation_with_CRFs_Learned_from_Partially_Labeled_Images.html">nips-2007-Scene Segmentation with CRFs Learned from Partially Labeled Images</a></p>
<p>173 <a title="nips-2007-173" href="../nips2007/nips-2007-Second_Order_Bilinear_Discriminant_Analysis_for_single_trial_EEG_analysis.html">nips-2007-Second Order Bilinear Discriminant Analysis for single trial EEG analysis</a></p>
<p>174 <a title="nips-2007-174" href="../nips2007/nips-2007-Selecting_Observations_against_Adversarial_Objectives.html">nips-2007-Selecting Observations against Adversarial Objectives</a></p>
<p>175 <a title="nips-2007-175" href="../nips2007/nips-2007-Semi-Supervised_Multitask_Learning.html">nips-2007-Semi-Supervised Multitask Learning</a></p>
<p>176 <a title="nips-2007-176" href="../nips2007/nips-2007-Sequential_Hypothesis_Testing_under_Stochastic_Deadlines.html">nips-2007-Sequential Hypothesis Testing under Stochastic Deadlines</a></p>
<p>177 <a title="nips-2007-177" href="../nips2007/nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>178 <a title="nips-2007-178" href="../nips2007/nips-2007-Simulated_Annealing%3A_Rigorous_finite-time_guarantees_for_optimization_on_continuous_domains.html">nips-2007-Simulated Annealing: Rigorous finite-time guarantees for optimization on continuous domains</a></p>
<p>179 <a title="nips-2007-179" href="../nips2007/nips-2007-SpAM%3A_Sparse_Additive_Models.html">nips-2007-SpAM: Sparse Additive Models</a></p>
<p>180 <a title="nips-2007-180" href="../nips2007/nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>181 <a title="nips-2007-181" href="../nips2007/nips-2007-Sparse_Overcomplete_Latent_Variable_Decomposition_of_Counts_Data.html">nips-2007-Sparse Overcomplete Latent Variable Decomposition of Counts Data</a></p>
<p>182 <a title="nips-2007-182" href="../nips2007/nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>183 <a title="nips-2007-183" href="../nips2007/nips-2007-Spatial_Latent_Dirichlet_Allocation.html">nips-2007-Spatial Latent Dirichlet Allocation</a></p>
<p>184 <a title="nips-2007-184" href="../nips2007/nips-2007-Stability_Bounds_for_Non-i.i.d._Processes.html">nips-2007-Stability Bounds for Non-i.i.d. Processes</a></p>
<p>185 <a title="nips-2007-185" href="../nips2007/nips-2007-Stable_Dual_Dynamic_Programming.html">nips-2007-Stable Dual Dynamic Programming</a></p>
<p>186 <a title="nips-2007-186" href="../nips2007/nips-2007-Statistical_Analysis_of_Semi-Supervised_Regression.html">nips-2007-Statistical Analysis of Semi-Supervised Regression</a></p>
<p>187 <a title="nips-2007-187" href="../nips2007/nips-2007-Structured_Learning_with_Approximate_Inference.html">nips-2007-Structured Learning with Approximate Inference</a></p>
<p>188 <a title="nips-2007-188" href="../nips2007/nips-2007-Subspace-Based_Face_Recognition_in_Analog_VLSI.html">nips-2007-Subspace-Based Face Recognition in Analog VLSI</a></p>
<p>189 <a title="nips-2007-189" href="../nips2007/nips-2007-Supervised_Topic_Models.html">nips-2007-Supervised Topic Models</a></p>
<p>190 <a title="nips-2007-190" href="../nips2007/nips-2007-Support_Vector_Machine_Classification_with_Indefinite_Kernels.html">nips-2007-Support Vector Machine Classification with Indefinite Kernels</a></p>
<p>191 <a title="nips-2007-191" href="../nips2007/nips-2007-Temporal_Difference_Updating_without_a_Learning_Rate.html">nips-2007-Temporal Difference Updating without a Learning Rate</a></p>
<p>192 <a title="nips-2007-192" href="../nips2007/nips-2007-Testing_for_Homogeneity_with_Kernel_Fisher_Discriminant_Analysis.html">nips-2007-Testing for Homogeneity with Kernel Fisher Discriminant Analysis</a></p>
<p>193 <a title="nips-2007-193" href="../nips2007/nips-2007-The_Distribution_Family_of_Similarity_Distances.html">nips-2007-The Distribution Family of Similarity Distances</a></p>
<p>194 <a title="nips-2007-194" href="../nips2007/nips-2007-The_Epoch-Greedy_Algorithm_for_Multi-armed_Bandits_with_Side_Information.html">nips-2007-The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information</a></p>
<p>195 <a title="nips-2007-195" href="../nips2007/nips-2007-The_Generalized_FITC_Approximation.html">nips-2007-The Generalized FITC Approximation</a></p>
<p>196 <a title="nips-2007-196" href="../nips2007/nips-2007-The_Infinite_Gamma-Poisson_Feature_Model.html">nips-2007-The Infinite Gamma-Poisson Feature Model</a></p>
<p>197 <a title="nips-2007-197" href="../nips2007/nips-2007-The_Infinite_Markov_Model.html">nips-2007-The Infinite Markov Model</a></p>
<p>198 <a title="nips-2007-198" href="../nips2007/nips-2007-The_Noisy-Logical_Distribution_and_its_Application_to_Causal_Inference.html">nips-2007-The Noisy-Logical Distribution and its Application to Causal Inference</a></p>
<p>199 <a title="nips-2007-199" href="../nips2007/nips-2007-The_Price_of_Bandit_Information_for_Online_Optimization.html">nips-2007-The Price of Bandit Information for Online Optimization</a></p>
<p>200 <a title="nips-2007-200" href="../nips2007/nips-2007-The_Tradeoffs_of_Large_Scale_Learning.html">nips-2007-The Tradeoffs of Large Scale Learning</a></p>
<p>201 <a title="nips-2007-201" href="../nips2007/nips-2007-The_Value_of_Labeled_and_Unlabeled_Examples_when_the_Model_is_Imperfect.html">nips-2007-The Value of Labeled and Unlabeled Examples when the Model is Imperfect</a></p>
<p>202 <a title="nips-2007-202" href="../nips2007/nips-2007-The_discriminant_center-surround_hypothesis_for_bottom-up_saliency.html">nips-2007-The discriminant center-surround hypothesis for bottom-up saliency</a></p>
<p>203 <a title="nips-2007-203" href="../nips2007/nips-2007-The_rat_as_particle_filter.html">nips-2007-The rat as particle filter</a></p>
<p>204 <a title="nips-2007-204" href="../nips2007/nips-2007-Theoretical_Analysis_of_Heuristic_Search_Methods_for_Online_POMDPs.html">nips-2007-Theoretical Analysis of Heuristic Search Methods for Online POMDPs</a></p>
<p>205 <a title="nips-2007-205" href="../nips2007/nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>206 <a title="nips-2007-206" href="../nips2007/nips-2007-Topmoumoute_Online_Natural_Gradient_Algorithm.html">nips-2007-Topmoumoute Online Natural Gradient Algorithm</a></p>
<p>207 <a title="nips-2007-207" href="../nips2007/nips-2007-Transfer_Learning_using_Kolmogorov_Complexity%3A_Basic_Theory_and_Empirical_Evaluations.html">nips-2007-Transfer Learning using Kolmogorov Complexity: Basic Theory and Empirical Evaluations</a></p>
<p>208 <a title="nips-2007-208" href="../nips2007/nips-2007-TrueSkill_Through_Time%3A_Revisiting_the_History_of_Chess.html">nips-2007-TrueSkill Through Time: Revisiting the History of Chess</a></p>
<p>209 <a title="nips-2007-209" href="../nips2007/nips-2007-Ultrafast_Monte_Carlo_for_Statistical_Summations.html">nips-2007-Ultrafast Monte Carlo for Statistical Summations</a></p>
<p>210 <a title="nips-2007-210" href="../nips2007/nips-2007-Unconstrained_On-line_Handwriting_Recognition_with_Recurrent_Neural_Networks.html">nips-2007-Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks</a></p>
<p>211 <a title="nips-2007-211" href="../nips2007/nips-2007-Unsupervised_Feature_Selection_for_Accurate_Recommendation_of_High-Dimensional_Image_Data.html">nips-2007-Unsupervised Feature Selection for Accurate Recommendation of High-Dimensional Image Data</a></p>
<p>212 <a title="nips-2007-212" href="../nips2007/nips-2007-Using_Deep_Belief_Nets_to_Learn_Covariance_Kernels_for_Gaussian_Processes.html">nips-2007-Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes</a></p>
<p>213 <a title="nips-2007-213" href="../nips2007/nips-2007-Variational_Inference_for_Diffusion_Processes.html">nips-2007-Variational Inference for Diffusion Processes</a></p>
<p>214 <a title="nips-2007-214" href="../nips2007/nips-2007-Variational_inference_for_Markov_jump_processes.html">nips-2007-Variational inference for Markov jump processes</a></p>
<p>215 <a title="nips-2007-215" href="../nips2007/nips-2007-What_makes_some_POMDP_problems_easy_to_approximate%3F.html">nips-2007-What makes some POMDP problems easy to approximate?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
