<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>nips 2002 knowledge graph</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="#">nips2002</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>nips 2002 knowledge graph</h1>
<br/><h3>similar papers computed by tfidf model</h3><br/><h3>similar papers computed by <a title="lsi-model" href="./nips2002_lsi.html">lsi model</a></h3><br/><h3>similar papers computed by <a title="lda-model" href="./nips2002_lda.html">lda model</a></h3><br/><h2>papers list:</h2><p>1 <a title="nips-2002-1" href="../nips2002/nips-2002-%22Name_That_Song%21%22_A_Probabilistic_Approach_to_Querying_on_Music_and_Text.html">nips-2002-"Name That Song!" A Probabilistic Approach to Querying on Music and Text</a></p>
<p>Author: Brochu Eric, Nando de Freitas</p><p>Abstract: We present a novel, ﬂexible statistical approach for modelling music and text jointly. The approach is based on multi-modal mixture models and maximum a posteriori estimation using EM. The learned models can be used to browse databases with documents containing music and text, to search for music using queries consisting of music and text (lyrics and other contextual information), to annotate text documents with music, and to automatically recommend or identify similar songs.</p><p>2 <a title="nips-2002-2" href="../nips2002/nips-2002-A_Bilinear_Model_for_Sparse_Coding.html">nips-2002-A Bilinear Model for Sparse Coding</a></p>
<p>Author: David B. Grimes, Rajesh P. Rao</p><p>Abstract: Recent algorithms for sparse coding and independent component analysis (ICA) have demonstrated how localized features can be learned from natural images. However, these approaches do not take image transformations into account. As a result, they produce image codes that are redundant because the same feature is learned at multiple locations. We describe an algorithm for sparse coding based on a bilinear generative model of images. By explicitly modeling the interaction between image features and their transformations, the bilinear approach helps reduce redundancy in the image code and provides a basis for transformationinvariant vision. We present results demonstrating bilinear sparse coding of natural images. We also explore an extension of the model that can capture spatial relationships between the independent features of an object, thereby providing a new framework for parts-based object recognition.</p><p>3 <a title="nips-2002-3" href="../nips2002/nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">nips-2002-A Convergent Form of Approximate Policy Iteration</a></p>
<p>Author: Theodore J. Perkins, Doina Precup</p><p>Abstract: We study a new, model-free form of approximate policy iteration which uses Sarsa updates with linear state-action value function approximation for policy evaluation, and a “policy improvement operator” to generate a new policy based on the learned state-action values. We prove that if the policy improvement operator produces -soft policies and is Lipschitz continuous in the action values, with a constant that is not too large, then the approximate policy iteration algorithm converges to a unique solution from any initial policy. To our knowledge, this is the ﬁrst convergence result for any form of approximate policy iteration under similar computational-resource assumptions.</p><p>4 <a title="nips-2002-4" href="../nips2002/nips-2002-A_Differential_Semantics_for_Jointree_Algorithms.html">nips-2002-A Differential Semantics for Jointree Algorithms</a></p>
<p>Author: James D. Park, Adnan Darwiche</p><p>Abstract: A new approach to inference in belief networks has been recently proposed, which is based on an algebraic representation of belief networks using multi–linear functions. According to this approach, the key computational question is that of representing multi–linear functions compactly, since inference reduces to a simple process of ev aluating and diﬀerentiating such functions. W e show here that mainstream inference algorithms based on jointrees are a special case of this approach in a v ery precise sense. W e use this result to prov e new properties of jointree algorithms, and then discuss some of its practical and theoretical implications. 1</p><p>5 <a title="nips-2002-5" href="../nips2002/nips-2002-A_Digital_Antennal_Lobe_for_Pattern_Equalization%3A_Analysis_and_Design.html">nips-2002-A Digital Antennal Lobe for Pattern Equalization: Analysis and Design</a></p>
<p>Author: Alex Holub, Gilles Laurent, Pietro Perona</p><p>Abstract: Re-mapping patterns in order to equalize their distribution may greatly simplify both the structure and the training of classifiers. Here, the properties of one such map obtained by running a few steps of discrete-time dynamical system are explored. The system is called 'Digital Antennal Lobe' (DAL) because it is inspired by recent studies of the antennallobe, a structure in the olfactory system of the grasshopper. The pattern-spreading properties of the DAL as well as its average behavior as a function of its (few) design parameters are analyzed by extending previous results of Van Vreeswijk and Sompolinsky. Furthermore, a technique for adapting the parameters of the initial design in order to obtain opportune noise-rejection behavior is suggested. Our results are demonstrated with a number of simulations. 1</p><p>6 <a title="nips-2002-6" href="../nips2002/nips-2002-A_Formulation_for_Minimax_Probability_Machine_Regression.html">nips-2002-A Formulation for Minimax Probability Machine Regression</a></p>
<p>Author: Thomas Strohmann, Gregory Z. Grudic</p><p>Abstract: We formulate the regression problem as one of maximizing the minimum probability, symbolized by Ω, that future predicted outputs of the regression model will be within some ±ε bound of the true regression function. Our formulation is unique in that we obtain a direct estimate of this lower probability bound Ω. The proposed framework, minimax probability machine regression (MPMR), is based on the recently described minimax probability machine classiﬁcation algorithm [Lanckriet et al.] and uses Mercer Kernels to obtain nonlinear regression models. MPMR is tested on both toy and real world data, verifying the accuracy of the Ω bound, and the efﬁcacy of the regression models. 1</p><p>7 <a title="nips-2002-7" href="../nips2002/nips-2002-A_Hierarchical_Bayesian_Markovian_Model_for_Motifs_in_Biopolymer_Sequences.html">nips-2002-A Hierarchical Bayesian Markovian Model for Motifs in Biopolymer Sequences</a></p>
<p>Author: Eric P. Xing, Michael I. Jordan, Richard M. Karp, Stuart Russell</p><p>Abstract: We propose a dynamic Bayesian model for motifs in biopolymer sequences which captures rich biological prior knowledge and positional dependencies in motif structure in a principled way. Our model posits that the position-speciﬁc multinomial parameters for monomer distribution are distributed as a latent Dirichlet-mixture random variable, and the position-speciﬁc Dirichlet component is determined by a hidden Markov process. Model parameters can be ﬁt on training motifs using a variational EM algorithm within an empirical Bayesian framework. Variational inference is also used for detecting hidden motifs. Our model improves over previous models that ignore biological priors and positional dependence. It has much higher sensitivity to motifs during detection and a notable ability to distinguish genuine motifs from false recurring patterns.</p><p>8 <a title="nips-2002-8" href="../nips2002/nips-2002-A_Maximum_Entropy_Approach_to_Collaborative_Filtering_in_Dynamic%2C_Sparse%2C_High-Dimensional_Domains.html">nips-2002-A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains</a></p>
<p>Author: Dmitry Y. Pavlov, David M. Pennock</p><p>Abstract: We develop a maximum entropy (maxent) approach to generating recommendations in the context of a user’s current navigation stream, suitable for environments where data is sparse, high-dimensional, and dynamic— conditions typical of many recommendation applications. We address sparsity and dimensionality reduction by ﬁrst clustering items based on user access patterns so as to attempt to minimize the apriori probability that recommendations will cross cluster boundaries and then recommending only within clusters. We address the inherent dynamic nature of the problem by explicitly modeling the data as a time series; we show how this representational expressivity ﬁts naturally into a maxent framework. We conduct experiments on data from ResearchIndex, a popular online repository of over 470,000 computer science documents. We show that our maxent formulation outperforms several competing algorithms in ofﬂine tests simulating the recommendation of documents to ResearchIndex users.</p><p>9 <a title="nips-2002-9" href="../nips2002/nips-2002-A_Minimal_Intervention_Principle_for_Coordinated_Movement.html">nips-2002-A Minimal Intervention Principle for Coordinated Movement</a></p>
<p>Author: Emanuel Todorov, Michael I. Jordan</p><p>Abstract: Behavioral goals are achieved reliably and repeatedly with movements rarely reproducible in their detail. Here we offer an explanation: we show that not only are variability and goal achievement compatible, but indeed that allowing variability in redundant dimensions is the optimal control strategy in the face of uncertainty. The optimal feedback control laws for typical motor tasks obey a “minimal intervention” principle: deviations from the average trajectory are only corrected when they interfere with the task goals. The resulting behavior exhibits task-constrained variability, as well as synergetic coupling among actuators—which is another unexplained empirical phenomenon.</p><p>10 <a title="nips-2002-10" href="../nips2002/nips-2002-A_Model_for_Learning_Variance_Components_of_Natural_Images.html">nips-2002-A Model for Learning Variance Components of Natural Images</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: We present a hierarchical Bayesian model for learning efﬁcient codes of higher-order structure in natural images. The model, a non-linear generalization of independent component analysis, replaces the standard assumption of independence for the joint distribution of coefﬁcients with a distribution that is adapted to the variance structure of the coefﬁcients of an efﬁcient image basis. This offers a novel description of higherorder image structure and provides a way to learn coarse-coded, sparsedistributed representations of abstract image properties such as object location, scale, and texture.</p><p>11 <a title="nips-2002-11" href="../nips2002/nips-2002-A_Model_for_Real-Time_Computation_in_Generic_Neural_Microcircuits.html">nips-2002-A Model for Real-Time Computation in Generic Neural Microcircuits</a></p>
<p>Author: Wolfgang Maass, Thomas Natschläger, Henry Markram</p><p>Abstract: A key challenge for neural modeling is to explain how a continuous stream of multi-modal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-ﬁre neurons in real-time. We propose a new computational model that is based on principles of high dimensional dynamical systems in combination with statistical learning theory. It can be implemented on generic evolved or found recurrent circuitry.</p><p>12 <a title="nips-2002-12" href="../nips2002/nips-2002-A_Neural_Edge-Detection_Model_for_Enhanced_Auditory_Sensitivity_in_Modulated_Noise.html">nips-2002-A Neural Edge-Detection Model for Enhanced Auditory Sensitivity in Modulated Noise</a></p>
<p>Author: Alon Fishbach, Bradford J. May</p><p>Abstract: Psychophysical data suggest that temporal modulations of stimulus amplitude envelopes play a prominent role in the perceptual segregation of concurrent sounds. In particular, the detection of an unmodulated signal can be significantly improved by adding amplitude modulation to the spectral envelope of a competing masking noise. This perceptual phenomenon is known as “Comodulation Masking Release” (CMR). Despite the obvious influence of temporal structure on the perception of complex auditory scenes, the physiological mechanisms that contribute to CMR and auditory streaming are not well known. A recent physiological study by Nelken and colleagues has demonstrated an enhanced cortical representation of auditory signals in modulated noise. Our study evaluates these CMR-like response patterns from the perspective of a hypothetical auditory edge-detection neuron. It is shown that this simple neural model for the detection of amplitude transients can reproduce not only the physiological data of Nelken et al., but also, in light of previous results, a variety of physiological and psychoacoustical phenomena that are related to the perceptual segregation of concurrent sounds. 1 In t rod u ct i on The temporal structure of a complex sound exerts strong influences on auditory physiology (e.g. [10, 16]) and perception (e.g. [9, 19, 20]). In particular, studies of auditory scene analysis have demonstrated the importance of the temporal structure of amplitude envelopes in the perceptual segregation of concurrent sounds [2, 7]. Common amplitude transitions across frequency serve as salient cues for grouping sound energy into unified perceptual objects. Conversely, asynchronous amplitude transitions enhance the separation of competing acoustic events [3, 4]. These general principles are manifested in perceptual phenomena as diverse as comodulation masking release (CMR) [13], modulation detection interference [22] and synchronous onset grouping [8]. Despite the obvious importance of timing information in psychoacoustic studies of auditory masking, the way in which the CNS represents the temporal structure of an amplitude envelope is not well understood. Certainly many physiological studies have demonstrated neural sensitivities to envelope transitions, but this sensitivity is only beginning to be related to the variety of perceptual experiences that are evoked by signals in noise. Nelken et al. [15] have suggested a correspondence between neural responses to time-varying amplitude envelopes and psychoacoustic masking phenomena. In their study of neurons in primary auditory cortex (A1), adding temporal modulation to background noise lowered the detection thresholds of unmodulated tones. This enhanced signal detection is similar to the perceptual phenomenon that is known as comodulation masking release [13]. Fishbach et al. [11] have recently proposed a neural model for the detection of “auditory edges” (i.e., amplitude transients) that can account for numerous physiological [14, 17, 18] and psychoacoustical [3, 21] phenomena. The encompassing utility of this edge-detection model suggests a common mechanism that may link the auditory processing and perception of auditory signals in a complex auditory scene. Here, it is shown that the auditory edge detection model can accurately reproduce the cortical CMR-like responses previously described by Nelken and colleagues. 2 Th e M od el The model is described in detail elsewhere [11]. In short, the basic operation of the model is the calculation of the first-order time derivative of the log-compressed envelope of the stimulus. A computational model [23] is used to convert the acoustic waveform to a physiologically plausible auditory nerve representation (Fig 1a). The simulated neural response has a medium spontaneous rate and a characteristic frequency that is set to the frequency of the target tone. To allow computation of the time derivative of the stimulus envelope, we hypothesize the existence of a temporal delay dimension, along which the stimulus is progressively delayed. The intermediate delay layer (Fig 1b) is constructed from an array of neurons with ascending membrane time constants (τ); each neuron is modeled by a conventional integrate-and-fire model (I&F;, [12]). Higher membrane time constant induces greater delay in the neuron’s response [1]. The output of the delay layer converges to a single output neuron (Fig. 1c) via a set of connection with various efficacies that reflect a receptive field of a gaussian derivative. This combination of excitatory and inhibitory connections carries out the time-derivative computation. Implementation details and parameters are given in [11]. The model has 2 adjustable and 6 fixed parameters, the former were used to fit the responses of the model to single unit responses to variety of stimuli [11]. The results reported here are not sensitive to these parameters. (a) AN model (b) delay-layer (c) edge-detector neuron τ=6 ms I&F; Neuron τ=4 ms τ=3 ms bandpass log d dt RMS Figure 1: Schematic diagram of the model and a block diagram of the basic operation of each model component (shaded area). The stimulus is converted to a neural representation (a) that approximates the average firing rate of a medium spontaneous-rate AN fiber [23]. The operation of this stage can be roughly described as the log-compressed rms output of a bandpass filter. The neural representation is fed to a series of neurons with ascending membrane time constant (b). The kernel functions that are used to simulate these neurons are plotted for a few neurons along with the time constants used. The output of the delay-layer neurons converge to a single I&F; neuron (c) using a set of connections with weights that reflect a shape of a gaussian derivative. Solid arrows represent excitatory connections and white arrows represent inhibitory connections. The absolute efficacy is represented by the width of the arrows. 3 Resu lt s Nelken et al. [15] report that amplitude modulation can substantially modify the noise-driven discharge rates of A1 neurons in Halothane-anesthetized cats. Many cortical neurons show only a transient onset response to unmodulated noise but fire in synchrony (“lock”) to the envelope of modulated noise. A significant reduction in envelope-locked discharge rates is observed if an unmodulated tone is added to modulated noise. As summarized in Fig. 2, this suppression of envelope locking can reveal the presence of an auditory signal at sound pressure levels that are not detectable in unmodulated noise. It has been suggested that this pattern of neural responding may represent a physiological equivalent of CMR. Reproduction of CMR-like cortical activity can be illustrated by a simplified case in which the analytical amplitude envelope of the stimulus is used as the input to the edge-detector model. In keeping with the actual physiological approach of Nelken et al., the noise envelope is shaped by a trapezoid modulator for these simulations. Each cycle of modulation, E N(t), is given by: t 0≤t  < 3D E N (t ) = P P − D (t − 3 D ) 3 D ≤ t < 4 D 0 4 D ≤ t < 8D £ P D     ¢     ¡ where P is the peak pressure level and D is set to 12.5 ms. (b) Modulated noise 76 Spikes/sec Tone level (dB SPL) (a) Unmodulated noise 26 0 150 300 0 150 300 Time (ms) Figure 2: Responses of an A1 unit to a combination of noise and tone at many tone levels, replotted from Nelken et al. [15]. (a) Unmodulated noise and (b) modulated noise. The noise envelope is illustrated by the thick line above each figure. Each row shows the response of the neuron to the noise plus the tone at the level specified on the ordinate. The dashed line in (b) indicates the detection threshold level for the tone. The detection threshold (as defined and calculated by Nelken et al.) in the unmodulated noise was not reached. Since the basic operation of the model is the calculation of the rectified timederivative of the log-compressed envelope of the stimulus, the expected noisedriven rate of the model can be approximated by: ( ) ¢ E (t ) P0   d A ln 1 + dt ¡ M N ( t ) = max 0, ¥ ¤ £ where A=20/ln(10) and P0 =2e-5 Pa. The expected firing rate in response to the noise plus an unmodulated signal (tone) can be similarly approximated by: ) ¨ E ( t ) + PS P0 ¦ ( d A ln 1 + dt § M N + S ( t ) = max 0,   © where PS is the peak pressure level of the tone. Clearly, both MN (t) and MN+S (t) are identically zero outside the interval [0 D]. Within this interval it holds that: M N (t ) = AP D P0 + P D t 0≤t < D Clearly, M N + S < M N for the interval [0 D] of each modulation cycle. That is, the addition of a tone reduces the responses of the model to the rising part of the modulated envelope. Higher tone levels (Ps ) cause greater reduction in the model’s firing rate. (c) (b) Level derivative (dB SPL/ms) Level (dB SPL) (a) (d) Time (ms) Figure 3: An illustration of the basic operation of the model on various amplitude envelopes. The simplified operation of the model includes log compression of the amplitude envelope (a and c) and rectified time-derivative of the log-compressed envelope (b and d). (a) A 30 dB SPL tone is added to a modulated envelope (peak level of 70 dB SPL) 300 ms after the beginning of the stimulus (as indicated by the horizontal line). The addition of the tone causes a great reduction in the time derivative of the log-compressed envelope (b). When the envelope of the noise is unmodulated (c), the time-derivative of the log-compressed envelope (d) shows a tiny spike when the tone is added (marked by the arrow). Fig. 3 demonstrates the effect of a low-level tone on the time-derivative of the logcompressed envelope of a noise. When the envelope is modulated (Fig. 3a) the addition of the tone greatly reduces the derivative of the rising part of the modulation (Fig. 3b). In the absence of modulations (Fig. 3c), the tone presentation produces a negligible effect on the level derivative (Fig. 3d). Model simulations of neural responses to the stimuli used by Nelken et al. are plotted in Fig. 4. As illustrated schematically in Fig 3 (d), the presence of the tone does not cause any significant change in the responses of the model to the unmodulated noise (Fig. 4a). In the modulated noise, however, tones of relatively low levels reduce the responses of the model to the rising part of the envelope modulations. (b) Modulated noise 76 Spikes/sec Tone level (dB SPL) (a) Unmodulated noise 26 0 150 300 0 Time (ms) 150 300 Figure 4: Simulated responses of the model to a combination of a tone and Unmodulated noise (a) and modulated noise (b). All conventions are as in Fig. 2. 4 Di scu ssi on This report uses an auditory edge-detection model to simulate the actual physiological consequences of amplitude modulation on neural sensitivity in cortical area A1. The basic computational operation of the model is the calculation of the smoothed time-derivative of the log-compressed stimulus envelope. The ability of the model to reproduce cortical response patterns in detail across a variety of stimulus conditions suggests similar time-sensitive mechanisms may contribute to the physiological correlates of CMR. These findings augment our previous observations that the simple edge-detection model can successfully predict a wide range of physiological and perceptual phenomena [11]. Former applications of the model to perceptual phenomena have been mainly related to auditory scene analysis, or more specifically the ability of the auditory system to distinguish multiple sound sources. In these cases, a sharp amplitude transition at stimulus onset (“auditory edge”) was critical for sound segregation. Here, it is shown that the detection of acoustic signals also may be enhanced through the suppression of ongoing responses to the concurrent modulations of competing background sounds. Interestingly, these temporal fluctuations appear to be a common property of natural soundscapes [15]. The model provides testable predictions regarding how signal detection may be influenced by the temporal shape of amplitude modulation. Carlyon et al. [6] measured CMR in human listeners using three types of noise modulation: squarewave, sine wave and multiplied noise. From the perspective of the edge-detection model, these psychoacoustic results are intriguing because the different modulator types represent manipulations of the time derivative of masker envelopes. Squarewave modulation had the most sharply edged time derivative and produced the greatest masking release. Fig. 5 plots the responses of the model to a pure-tone signal in square-wave and sine-wave modulated noise. As in the psychoacoustical data of Carlyon et al., the simulated detection threshold was lower in the context of square-wave modulation. Our modeling results suggest that the sharply edged square wave evoked higher levels of noise-driven activity and therefore created a sensitive background for the suppressing effects of the unmodulated tone. (b) 60 Spikes/sec Tone level (dB SPL) (a) 10 0 200 400 600 0 Time (ms) 200 400 600 Figure 5: Simulated responses of the model to a combination of a tone at various levels and a sine-wave modulated noise (a) or a square-wave modulated noise (b). Each row shows the response of the model to the noise plus the tone at the level specified on the abscissa. The shape of the noise modulator is illustrated above each figure. The 100 ms tone starts 250 ms after the noise onset. Note that the tone detection threshold (marked by the dashed line) is 10 dB lower for the square-wave modulator than for the sine-wave modulator, in accordance with the psychoacoustical data of Carlyon et al. [6]. Although the physiological basis of our model was derived from studies of neural responses in the cat auditory system, the key psychoacoustical observations of Carlyon et al. have been replicated in recent behavioral studies of cats (Budelis et al. [5]). These data support the generalization of human perceptual processing to other species and enhance the possible correspondence between the neuronal CMR-like effect and the psychoacoustical masking phenomena. Clearly, the auditory system relies on information other than the time derivative of the stimulus envelope for the detection of auditory signals in background noise. Further physiological and psychoacoustic assessments of CMR-like masking effects are needed not only to refine the predictive abilities of the edge-detection model but also to reveal the additional sources of acoustic information that influence signal detection in constantly changing natural environments. Ackn ow led g men t s This work was supported in part by a NIDCD grant R01 DC004841. Refe ren ces [1] Agmon-Snir H., Segev I. (1993). “Signal delay and input synchronization in passive dendritic structure”, J. Neurophysiol. 70, 2066-2085. [2] Bregman A.S. (1990). “Auditory scene analysis: The perceptual organization of sound”, MIT Press, Cambridge, MA. [3] Bregman A.S., Ahad P.A., Kim J., Melnerich L. (1994) “Resetting the pitch-analysis system. 1. Effects of rise times of tones in noise backgrounds or of harmonics in a complex tone”, Percept. Psychophys. 56 (2), 155-162. [4] Bregman A.S., Ahad P.A., Kim J. (1994) “Resetting the pitch-analysis system. 2. Role of sudden onsets and offsets in the perception of individual components in a cluster of overlapping tones”, J. Acoust. Soc. Am. 96 (5), 2694-2703. [5] Budelis J., Fishbach A., May B.J. (2002) “Behavioral assessments of comodulation masking release in cats”, Abst. Assoc. for Res. in Otolaryngol. 25. [6] Carlyon R.P., Buus S., Florentine M. (1989) “Comodulation masking release for three types of modulator as a function of modulation rate”, Hear. Res. 42, 37-46. [7] Darwin C.J. (1997) “Auditory grouping”, Trends in Cog. Sci. 1(9), 327-333. [8] Darwin C.J., Ciocca V. (1992) “Grouping in pitch perception: Effects of onset asynchrony and ear of presentation of a mistuned component”, J. Acoust. Soc. Am. 91 , 33813390. [9] Drullman R., Festen H.M., Plomp R. (1994) “Effect of temporal envelope smearing on speech reception”, J. Acoust. Soc. Am. 95 (2), 1053-1064. [10] Eggermont J J. (1994). “Temporal modulation transfer functions for AM and FM stimuli in cat auditory cortex. Effects of carrier type, modulating waveform and intensity”, Hear. Res. 74, 51-66. [11] Fishbach A., Nelken I., Yeshurun Y. (2001) “Auditory edge detection: a neural model for physiological and psychoacoustical responses to amplitude transients”, J. Neurophysiol. 85, 2303–2323. [12] Gerstner W. (1999) “Spiking neurons”, in Pulsed Neural Networks , edited by W. Maass, C. M. Bishop, (MIT Press, Cambridge, MA). [13] Hall J.W., Haggard M.P., Fernandes M.A. (1984) “Detection in noise by spectrotemporal pattern analysis”, J. Acoust. Soc. Am. 76, 50-56. [14] Heil P. (1997) “Auditory onset responses revisited. II. Response strength”, J. Neurophysiol. 77, 2642-2660. [15] Nelken I., Rotman Y., Bar-Yosef O. (1999) “Responses of auditory cortex neurons to structural features of natural sounds”, Nature 397, 154-157. [16] Phillips D.P. (1988). “Effect of Tone-Pulse Rise Time on Rate-Level Functions of Cat Auditory Cortex Neurons: Excitatory and Inhibitory Processes Shaping Responses to Tone Onset”, J. Neurophysiol. 59, 1524-1539. [17] Phillips D.P., Burkard R. (1999). “Response magnitude and timing of auditory response initiation in the inferior colliculus of the awake chinchilla”, J. Acoust. Soc. Am. 105, 27312737. [18] Phillips D.P., Semple M.N., Kitzes L.M. (1995). “Factors shaping the tone level sensitivity of single neurons in posterior field of cat auditory cortex”, J. Neurophysiol. 73, 674-686. [19] Rosen S. (1992) “Temporal information in speech: acoustic, auditory and linguistic aspects”, Phil. Trans. R. Soc. Lond. B 336, 367-373. [20] Shannon R.V., Zeng F.G., Kamath V., Wygonski J, Ekelid M. (1995) “Speech recognition with primarily temporal cues”, Science 270, 303-304. [21] Turner C.W., Relkin E.M., Doucet J. (1994). “Psychophysical and physiological forward masking studies: probe duration and rise-time effects”, J. Acoust. Soc. Am. 96 (2), 795-800. [22] Yost W.A., Sheft S. (1994) “Modulation detection interference – across-frequency processing and auditory grouping”, Hear. Res. 79, 48-58. [23] Zhang X., Heinz M.G., Bruce I.C., Carney L.H. (2001). “A phenomenological model for the responses of auditory-nerve fibers: I. Nonlinear tuning with compression and suppression”, J. Acoust. Soc. Am. 109 (2), 648-670.</p><p>13 <a title="nips-2002-13" href="../nips2002/nips-2002-A_Note_on_the_Representational_Incompatibility_of_Function_Approximation_and_Factored_Dynamics.html">nips-2002-A Note on the Representational Incompatibility of Function Approximation and Factored Dynamics</a></p>
<p>Author: Eric Allender, Sanjeev Arora, Michael Kearns, Cristopher Moore, Alexander Russell</p><p>Abstract: We establish a new hardness result that shows that the difﬁculty of planning in factored Markov decision processes is representational rather than just computational. More precisely, we give a ﬁxed family of factored MDPs with linear rewards whose optimal policies and value functions simply cannot be represented succinctly in any standard parametric form. Previous hardness results indicated that computing good policies from the MDP parameters was difﬁcult, but left open the possibility of succinct function approximation for any ﬁxed factored MDP. Our result applies even to policies which yield a polynomially poor approximation to the optimal value, and highlights interesting connections with the complexity class of Arthur-Merlin games.</p><p>14 <a title="nips-2002-14" href="../nips2002/nips-2002-A_Probabilistic_Approach_to_Single_Channel_Blind_Signal_Separation.html">nips-2002-A Probabilistic Approach to Single Channel Blind Signal Separation</a></p>
<p>Author: Gil-jin Jang, Te-Won Lee</p><p>Abstract: We present a new technique for achieving source separation when given only a single channel recording. The main idea is based on exploiting the inherent time structure of sound sources by learning a priori sets of basis ﬁlters in time domain that encode the sources in a statistically efﬁcient manner. We derive a learning algorithm using a maximum likelihood approach given the observed single channel data and sets of basis ﬁlters. For each time point we infer the source signals and their contribution factors. This inference is possible due to the prior knowledge of the basis ﬁlters and the associated coefﬁcient densities. A ﬂexible model for density estimation allows accurate modeling of the observation and our experimental results exhibit a high level of separation performance for mixtures of two music signals as well as the separation of two voice signals.</p><p>15 <a title="nips-2002-15" href="../nips2002/nips-2002-A_Probabilistic_Model_for_Learning_Concatenative_Morphology.html">nips-2002-A Probabilistic Model for Learning Concatenative Morphology</a></p>
<p>Author: Matthew G. Snover, Michael R. Brent</p><p>Abstract: This paper describes a system for the unsupervised learning of morphological sufﬁxes and stems from word lists. The system is composed of a generative probability model and hill-climbing and directed search algorithms. By extracting and examining morphologically rich subsets of an input lexicon, the directed search identiﬁes highly productive paradigms. The hill-climbing algorithm then further maximizes the probability of the hypothesis. Quantitative results are shown by measuring the accuracy of the morphological relations identiﬁed. Experiments in English and Polish, as well as comparisons with another recent unsupervised morphology learning algorithm demonstrate the effectiveness of this technique.</p><p>16 <a title="nips-2002-16" href="../nips2002/nips-2002-A_Prototype_for_Automatic_Recognition_of_Spontaneous_Facial_Actions.html">nips-2002-A Prototype for Automatic Recognition of Spontaneous Facial Actions</a></p>
<p>Author: M.S. Bartlett, G.C. Littlewort, T.J. Sejnowski, J.R. Movellan</p><p>Abstract: We present ongoing work on a project for automatic recognition of spontaneous facial actions. Spontaneous facial expressions differ substantially from posed expressions, similar to how continuous, spontaneous speech differs from isolated words produced on command. Previous methods for automatic facial expression recognition assumed images were collected in controlled environments in which the subjects deliberately faced the camera. Since people often nod or turn their heads, automatic recognition of spontaneous facial behavior requires methods for handling out-of-image-plane head rotations. Here we explore an approach based on 3-D warping of images into canonical views. We evaluated the performance of the approach as a front-end for a spontaneous expression recognition system using support vector machines and hidden Markov models. This system employed general purpose learning mechanisms that can be applied to recognition of any facial movement. The system was tested for recognition of a set of facial actions deﬁned by the Facial Action Coding System (FACS). We showed that 3D tracking and warping followed by machine learning techniques directly applied to the warped images, is a viable and promising technology for automatic facial expression recognition. One exciting aspect of the approach presented here is that information about movement dynamics emerged out of ﬁlters which were derived from the statistics of images.</p><p>17 <a title="nips-2002-17" href="../nips2002/nips-2002-A_Statistical_Mechanics_Approach_to_Approximate_Analytical_Bootstrap_Averages.html">nips-2002-A Statistical Mechanics Approach to Approximate Analytical Bootstrap Averages</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We apply the replica method of Statistical Physics combined with a variational method to the approximate analytical computation of bootstrap averages for estimating the generalization error. We demonstrate our approach on regression with Gaussian processes and compare our results with averages obtained by Monte-Carlo sampling.</p><p>18 <a title="nips-2002-18" href="../nips2002/nips-2002-Adaptation_and_Unsupervised_Learning.html">nips-2002-Adaptation and Unsupervised Learning</a></p>
<p>Author: Peter Dayan, Maneesh Sahani, Gregoire Deback</p><p>Abstract: Adaptation is a ubiquitous neural and psychological phenomenon, with a wealth of instantiations and implications. Although a basic form of plasticity, it has, bar some notable exceptions, attracted computational theory of only one main variety. In this paper, we study adaptation from the perspective of factor analysis, a paradigmatic technique of unsupervised learning. We use factor analysis to re-interpret a standard view of adaptation, and apply our new model to some recent data on adaptation in the domain of face discrimination.</p><p>19 <a title="nips-2002-19" href="../nips2002/nips-2002-Adapting_Codes_and_Embeddings_for_Polychotomies.html">nips-2002-Adapting Codes and Embeddings for Polychotomies</a></p>
<p>Author: Gunnar Rätsch, Sebastian Mika, Alex J. Smola</p><p>Abstract: In this paper we consider formulations of multi-class problems based on a generalized notion of a margin and using output coding. This includes, but is not restricted to, standard multi-class SVM formulations. Differently from many previous approaches we learn the code as well as the embedding function. We illustrate how this can lead to a formulation that allows for solving a wider range of problems with for instance many classes or even “missing classes”. To keep our optimization problems tractable we propose an algorithm capable of solving them using twoclass classiﬁers, similar in spirit to Boosting.</p><p>20 <a title="nips-2002-20" href="../nips2002/nips-2002-Adaptive_Caching_by_Refetching.html">nips-2002-Adaptive Caching by Refetching</a></p>
<p>Author: Robert B. Gramacy, Manfred K. Warmuth, Scott A. Brandt, Ismail Ari</p><p>Abstract: We are constructing caching policies that have 13-20% lower miss rates than the best of twelve baseline policies over a large variety of request streams. This represents an improvement of 49–63% over Least Recently Used, the most commonly implemented policy. We achieve this not by designing a speciﬁc new policy but by using on-line Machine Learning algorithms to dynamically shift between the standard policies based on their observed miss rates. A thorough experimental evaluation of our techniques is given, as well as a discussion of what makes caching an interesting on-line learning problem.</p><p>21 <a title="nips-2002-21" href="../nips2002/nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">nips-2002-Adaptive Classification by Variational Kalman Filtering</a></p>
<p>22 <a title="nips-2002-22" href="../nips2002/nips-2002-Adaptive_Nonlinear_System_Identification_with_Echo_State_Networks.html">nips-2002-Adaptive Nonlinear System Identification with Echo State Networks</a></p>
<p>23 <a title="nips-2002-23" href="../nips2002/nips-2002-Adaptive_Quantization_and_Density_Estimation_in_Silicon.html">nips-2002-Adaptive Quantization and Density Estimation in Silicon</a></p>
<p>24 <a title="nips-2002-24" href="../nips2002/nips-2002-Adaptive_Scaling_for_Feature_Selection_in_SVMs.html">nips-2002-Adaptive Scaling for Feature Selection in SVMs</a></p>
<p>25 <a title="nips-2002-25" href="../nips2002/nips-2002-An_Asynchronous_Hidden_Markov_Model_for_Audio-Visual_Speech_Recognition.html">nips-2002-An Asynchronous Hidden Markov Model for Audio-Visual Speech Recognition</a></p>
<p>26 <a title="nips-2002-26" href="../nips2002/nips-2002-An_Estimation-Theoretic_Framework_for_the_Presentation_of_Multiple_Stimuli.html">nips-2002-An Estimation-Theoretic Framework for the Presentation of Multiple Stimuli</a></p>
<p>27 <a title="nips-2002-27" href="../nips2002/nips-2002-An_Impossibility_Theorem_for_Clustering.html">nips-2002-An Impossibility Theorem for Clustering</a></p>
<p>28 <a title="nips-2002-28" href="../nips2002/nips-2002-An_Information_Theoretic_Approach_to_the_Functional_Classification_of_Neurons.html">nips-2002-An Information Theoretic Approach to the Functional Classification of Neurons</a></p>
<p>29 <a title="nips-2002-29" href="../nips2002/nips-2002-Analysis_of_Information_in_Speech_Based_on_MANOVA.html">nips-2002-Analysis of Information in Speech Based on MANOVA</a></p>
<p>30 <a title="nips-2002-30" href="../nips2002/nips-2002-Annealing_and_the_Rate_Distortion_Problem.html">nips-2002-Annealing and the Rate Distortion Problem</a></p>
<p>31 <a title="nips-2002-31" href="../nips2002/nips-2002-Application_of_Variational_Bayesian_Approach_to_Speech_Recognition.html">nips-2002-Application of Variational Bayesian Approach to Speech Recognition</a></p>
<p>32 <a title="nips-2002-32" href="../nips2002/nips-2002-Approximate_Inference_and_Protein-Folding.html">nips-2002-Approximate Inference and Protein-Folding</a></p>
<p>33 <a title="nips-2002-33" href="../nips2002/nips-2002-Approximate_Linear_Programming_for_Average-Cost_Dynamic_Programming.html">nips-2002-Approximate Linear Programming for Average-Cost Dynamic Programming</a></p>
<p>34 <a title="nips-2002-34" href="../nips2002/nips-2002-Artefactual_Structure_from_Least-Squares_Multidimensional_Scaling.html">nips-2002-Artefactual Structure from Least-Squares Multidimensional Scaling</a></p>
<p>35 <a title="nips-2002-35" href="../nips2002/nips-2002-Automatic_Acquisition_and_Efficient_Representation_of_Syntactic_Structures.html">nips-2002-Automatic Acquisition and Efficient Representation of Syntactic Structures</a></p>
<p>36 <a title="nips-2002-36" href="../nips2002/nips-2002-Automatic_Alignment_of_Local_Representations.html">nips-2002-Automatic Alignment of Local Representations</a></p>
<p>37 <a title="nips-2002-37" href="../nips2002/nips-2002-Automatic_Derivation_of_Statistical_Algorithms%3A_The_EM_Family_and_Beyond.html">nips-2002-Automatic Derivation of Statistical Algorithms: The EM Family and Beyond</a></p>
<p>38 <a title="nips-2002-38" href="../nips2002/nips-2002-Bayesian_Estimation_of_Time-Frequency_Coefficients_for_Audio_Signal_Enhancement.html">nips-2002-Bayesian Estimation of Time-Frequency Coefficients for Audio Signal Enhancement</a></p>
<p>39 <a title="nips-2002-39" href="../nips2002/nips-2002-Bayesian_Image_Super-Resolution.html">nips-2002-Bayesian Image Super-Resolution</a></p>
<p>40 <a title="nips-2002-40" href="../nips2002/nips-2002-Bayesian_Models_of_Inductive_Generalization.html">nips-2002-Bayesian Models of Inductive Generalization</a></p>
<p>41 <a title="nips-2002-41" href="../nips2002/nips-2002-Bayesian_Monte_Carlo.html">nips-2002-Bayesian Monte Carlo</a></p>
<p>42 <a title="nips-2002-42" href="../nips2002/nips-2002-Bias-Optimal_Incremental_Problem_Solving.html">nips-2002-Bias-Optimal Incremental Problem Solving</a></p>
<p>43 <a title="nips-2002-43" href="../nips2002/nips-2002-Binary_Coding_in_Auditory_Cortex.html">nips-2002-Binary Coding in Auditory Cortex</a></p>
<p>44 <a title="nips-2002-44" href="../nips2002/nips-2002-Binary_Tuning_is_Optimal_for_Neural_Rate_Coding_with_High_Temporal_Resolution.html">nips-2002-Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution</a></p>
<p>45 <a title="nips-2002-45" href="../nips2002/nips-2002-Boosted_Dyadic_Kernel_Discriminants.html">nips-2002-Boosted Dyadic Kernel Discriminants</a></p>
<p>46 <a title="nips-2002-46" href="../nips2002/nips-2002-Boosting_Density_Estimation.html">nips-2002-Boosting Density Estimation</a></p>
<p>47 <a title="nips-2002-47" href="../nips2002/nips-2002-Branching_Law_for_Axons.html">nips-2002-Branching Law for Axons</a></p>
<p>48 <a title="nips-2002-48" href="../nips2002/nips-2002-Categorization_Under_Complexity%3A_A_Unified_MDL_Account_of_Human_Learning_of_Regular_and_Irregular_Categories.html">nips-2002-Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories</a></p>
<p>49 <a title="nips-2002-49" href="../nips2002/nips-2002-Charting_a_Manifold.html">nips-2002-Charting a Manifold</a></p>
<p>50 <a title="nips-2002-50" href="../nips2002/nips-2002-Circuit_Model_of_Short-Term_Synaptic_Dynamics.html">nips-2002-Circuit Model of Short-Term Synaptic Dynamics</a></p>
<p>51 <a title="nips-2002-51" href="../nips2002/nips-2002-Classifying_Patterns_of_Visual_Motion_-_a_Neuromorphic_Approach.html">nips-2002-Classifying Patterns of Visual Motion - a Neuromorphic Approach</a></p>
<p>52 <a title="nips-2002-52" href="../nips2002/nips-2002-Cluster_Kernels_for_Semi-Supervised_Learning.html">nips-2002-Cluster Kernels for Semi-Supervised Learning</a></p>
<p>53 <a title="nips-2002-53" href="../nips2002/nips-2002-Clustering_with_the_Fisher_Score.html">nips-2002-Clustering with the Fisher Score</a></p>
<p>54 <a title="nips-2002-54" href="../nips2002/nips-2002-Combining_Dimensions_and_Features_in_Similarity-Based_Representations.html">nips-2002-Combining Dimensions and Features in Similarity-Based Representations</a></p>
<p>55 <a title="nips-2002-55" href="../nips2002/nips-2002-Combining_Features_for_BCI.html">nips-2002-Combining Features for BCI</a></p>
<p>56 <a title="nips-2002-56" href="../nips2002/nips-2002-Concentration_Inequalities_for_the_Missing_Mass_and_for_Histogram_Rule_Error.html">nips-2002-Concentration Inequalities for the Missing Mass and for Histogram Rule Error</a></p>
<p>57 <a title="nips-2002-57" href="../nips2002/nips-2002-Concurrent_Object_Recognition_and_Segmentation_by_Graph_Partitioning.html">nips-2002-Concurrent Object Recognition and Segmentation by Graph Partitioning</a></p>
<p>58 <a title="nips-2002-58" href="../nips2002/nips-2002-Conditional_Models_on_the_Ranking_Poset.html">nips-2002-Conditional Models on the Ranking Poset</a></p>
<p>59 <a title="nips-2002-59" href="../nips2002/nips-2002-Constraint_Classification_for_Multiclass_Classification_and_Ranking.html">nips-2002-Constraint Classification for Multiclass Classification and Ranking</a></p>
<p>60 <a title="nips-2002-60" href="../nips2002/nips-2002-Convergence_Properties_of_Some_Spike-Triggered_Analysis_Techniques.html">nips-2002-Convergence Properties of Some Spike-Triggered Analysis Techniques</a></p>
<p>61 <a title="nips-2002-61" href="../nips2002/nips-2002-Convergent_Combinations_of_Reinforcement_Learning_with_Linear_Function_Approximation.html">nips-2002-Convergent Combinations of Reinforcement Learning with Linear Function Approximation</a></p>
<p>62 <a title="nips-2002-62" href="../nips2002/nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems.html">nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</a></p>
<p>63 <a title="nips-2002-63" href="../nips2002/nips-2002-Critical_Lines_in_Symmetry_of_Mixture_Models_and_its_Application_to_Component_Splitting.html">nips-2002-Critical Lines in Symmetry of Mixture Models and its Application to Component Splitting</a></p>
<p>64 <a title="nips-2002-64" href="../nips2002/nips-2002-Data-Dependent_Bounds_for_Bayesian_Mixture_Methods.html">nips-2002-Data-Dependent Bounds for Bayesian Mixture Methods</a></p>
<p>65 <a title="nips-2002-65" href="../nips2002/nips-2002-Derivative_Observations_in_Gaussian_Process_Models_of_Dynamic_Systems.html">nips-2002-Derivative Observations in Gaussian Process Models of Dynamic Systems</a></p>
<p>66 <a title="nips-2002-66" href="../nips2002/nips-2002-Developing_Topography_and_Ocular_Dominance_Using_Two_aVLSI_Vision_Sensors_and_a_Neurotrophic_Model_of_Plasticity.html">nips-2002-Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity</a></p>
<p>67 <a title="nips-2002-67" href="../nips2002/nips-2002-Discriminative_Binaural_Sound_Localization.html">nips-2002-Discriminative Binaural Sound Localization</a></p>
<p>68 <a title="nips-2002-68" href="../nips2002/nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">nips-2002-Discriminative Densities from Maximum Contrast Estimation</a></p>
<p>69 <a title="nips-2002-69" href="../nips2002/nips-2002-Discriminative_Learning_for_Label_Sequences_via_Boosting.html">nips-2002-Discriminative Learning for Label Sequences via Boosting</a></p>
<p>70 <a title="nips-2002-70" href="../nips2002/nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</a></p>
<p>71 <a title="nips-2002-71" href="../nips2002/nips-2002-Dopamine_Induced_Bistability_Enhances_Signal_Processing_in_Spiny_Neurons.html">nips-2002-Dopamine Induced Bistability Enhances Signal Processing in Spiny Neurons</a></p>
<p>72 <a title="nips-2002-72" href="../nips2002/nips-2002-Dyadic_Classification_Trees_via_Structural_Risk_Minimization.html">nips-2002-Dyadic Classification Trees via Structural Risk Minimization</a></p>
<p>73 <a title="nips-2002-73" href="../nips2002/nips-2002-Dynamic_Bayesian_Networks_with_Deterministic_Latent_Tables.html">nips-2002-Dynamic Bayesian Networks with Deterministic Latent Tables</a></p>
<p>74 <a title="nips-2002-74" href="../nips2002/nips-2002-Dynamic_Structure_Super-Resolution.html">nips-2002-Dynamic Structure Super-Resolution</a></p>
<p>75 <a title="nips-2002-75" href="../nips2002/nips-2002-Dynamical_Causal_Learning.html">nips-2002-Dynamical Causal Learning</a></p>
<p>76 <a title="nips-2002-76" href="../nips2002/nips-2002-Dynamical_Constraints_on_Computing_with_Spike_Timing_in_the_Cortex.html">nips-2002-Dynamical Constraints on Computing with Spike Timing in the Cortex</a></p>
<p>77 <a title="nips-2002-77" href="../nips2002/nips-2002-Effective_Dimension_and_Generalization_of_Kernel_Learning.html">nips-2002-Effective Dimension and Generalization of Kernel Learning</a></p>
<p>78 <a title="nips-2002-78" href="../nips2002/nips-2002-Efficient_Learning_Equilibrium.html">nips-2002-Efficient Learning Equilibrium</a></p>
<p>79 <a title="nips-2002-79" href="../nips2002/nips-2002-Evidence_Optimization_Techniques_for_Estimating_Stimulus-Response_Functions.html">nips-2002-Evidence Optimization Techniques for Estimating Stimulus-Response Functions</a></p>
<p>80 <a title="nips-2002-80" href="../nips2002/nips-2002-Exact_MAP_Estimates_by_%28Hyper%29tree_Agreement.html">nips-2002-Exact MAP Estimates by (Hyper)tree Agreement</a></p>
<p>81 <a title="nips-2002-81" href="../nips2002/nips-2002-Expected_and_Unexpected_Uncertainty%3A_ACh_and_NE_in_the_Neocortex.html">nips-2002-Expected and Unexpected Uncertainty: ACh and NE in the Neocortex</a></p>
<p>82 <a title="nips-2002-82" href="../nips2002/nips-2002-Exponential_Family_PCA_for_Belief_Compression_in_POMDPs.html">nips-2002-Exponential Family PCA for Belief Compression in POMDPs</a></p>
<p>83 <a title="nips-2002-83" href="../nips2002/nips-2002-Extracting_Relevant_Structures_with_Side_Information.html">nips-2002-Extracting Relevant Structures with Side Information</a></p>
<p>84 <a title="nips-2002-84" href="../nips2002/nips-2002-Fast_Exact_Inference_with_a_Factored_Model_for_Natural_Language_Parsing.html">nips-2002-Fast Exact Inference with a Factored Model for Natural Language Parsing</a></p>
<p>85 <a title="nips-2002-85" href="../nips2002/nips-2002-Fast_Kernels_for_String_and_Tree_Matching.html">nips-2002-Fast Kernels for String and Tree Matching</a></p>
<p>86 <a title="nips-2002-86" href="../nips2002/nips-2002-Fast_Sparse_Gaussian_Process_Methods%3A_The_Informative_Vector_Machine.html">nips-2002-Fast Sparse Gaussian Process Methods: The Informative Vector Machine</a></p>
<p>87 <a title="nips-2002-87" href="../nips2002/nips-2002-Fast_Transformation-Invariant_Factor_Analysis.html">nips-2002-Fast Transformation-Invariant Factor Analysis</a></p>
<p>88 <a title="nips-2002-88" href="../nips2002/nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</a></p>
<p>89 <a title="nips-2002-89" href="../nips2002/nips-2002-Feature_Selection_by_Maximum_Marginal_Diversity.html">nips-2002-Feature Selection by Maximum Marginal Diversity</a></p>
<p>90 <a title="nips-2002-90" href="../nips2002/nips-2002-Feature_Selection_in_Mixture-Based_Clustering.html">nips-2002-Feature Selection in Mixture-Based Clustering</a></p>
<p>91 <a title="nips-2002-91" href="../nips2002/nips-2002-Field-Programmable_Learning_Arrays.html">nips-2002-Field-Programmable Learning Arrays</a></p>
<p>92 <a title="nips-2002-92" href="../nips2002/nips-2002-FloatBoost_Learning_for_Classification.html">nips-2002-FloatBoost Learning for Classification</a></p>
<p>93 <a title="nips-2002-93" href="../nips2002/nips-2002-Forward-Decoding_Kernel-Based_Phone_Recognition.html">nips-2002-Forward-Decoding Kernel-Based Phone Recognition</a></p>
<p>94 <a title="nips-2002-94" href="../nips2002/nips-2002-Fractional_Belief_Propagation.html">nips-2002-Fractional Belief Propagation</a></p>
<p>95 <a title="nips-2002-95" href="../nips2002/nips-2002-Gaussian_Process_Priors_with_Uncertain_Inputs_Application_to_Multiple-Step_Ahead_Time_Series_Forecasting.html">nips-2002-Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting</a></p>
<p>96 <a title="nips-2002-96" href="../nips2002/nips-2002-Generalized%C3%82%CB%9B_Linear%C3%82%CB%9B_Models.html">nips-2002-GeneralizedÂ˛ LinearÂ˛ Models</a></p>
<p>97 <a title="nips-2002-97" href="../nips2002/nips-2002-Global_Versus_Local_Methods_in_Nonlinear_Dimensionality_Reduction.html">nips-2002-Global Versus Local Methods in Nonlinear Dimensionality Reduction</a></p>
<p>98 <a title="nips-2002-98" href="../nips2002/nips-2002-Going_Metric%3A_Denoising_Pairwise_Data.html">nips-2002-Going Metric: Denoising Pairwise Data</a></p>
<p>99 <a title="nips-2002-99" href="../nips2002/nips-2002-Graph-Driven_Feature_Extraction_From_Microarray_Data_Using_Diffusion_Kernels_and_Kernel_CCA.html">nips-2002-Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA</a></p>
<p>100 <a title="nips-2002-100" href="../nips2002/nips-2002-Half-Lives_of_EigenFlows_for_Spectral_Clustering.html">nips-2002-Half-Lives of EigenFlows for Spectral Clustering</a></p>
<p>101 <a title="nips-2002-101" href="../nips2002/nips-2002-Handling_Missing_Data_with_Variational_Bayesian_Learning_of_ICA.html">nips-2002-Handling Missing Data with Variational Bayesian Learning of ICA</a></p>
<p>102 <a title="nips-2002-102" href="../nips2002/nips-2002-Hidden_Markov_Model_of_Cortical_Synaptic_Plasticity%3A_Derivation_of_the_Learning_Rule.html">nips-2002-Hidden Markov Model of Cortical Synaptic Plasticity: Derivation of the Learning Rule</a></p>
<p>103 <a title="nips-2002-103" href="../nips2002/nips-2002-How_Linear_are_Auditory_Cortical_Responses%3F.html">nips-2002-How Linear are Auditory Cortical Responses?</a></p>
<p>104 <a title="nips-2002-104" href="../nips2002/nips-2002-How_the_Poverty_of_the_Stimulus_Solves_the_Poverty_of_the_Stimulus.html">nips-2002-How the Poverty of the Stimulus Solves the Poverty of the Stimulus</a></p>
<p>105 <a title="nips-2002-105" href="../nips2002/nips-2002-How_to_Combine_Color_and_Shape_Information_for_3D_Object_Recognition%3A_Kernels_do_the_Trick.html">nips-2002-How to Combine Color and Shape Information for 3D Object Recognition: Kernels do the Trick</a></p>
<p>106 <a title="nips-2002-106" href="../nips2002/nips-2002-Hyperkernels.html">nips-2002-Hyperkernels</a></p>
<p>107 <a title="nips-2002-107" href="../nips2002/nips-2002-Identity_Uncertainty_and_Citation_Matching.html">nips-2002-Identity Uncertainty and Citation Matching</a></p>
<p>108 <a title="nips-2002-108" href="../nips2002/nips-2002-Improving_Transfer_Rates_in_Brain_Computer_Interfacing%3A_A_Case_Study.html">nips-2002-Improving Transfer Rates in Brain Computer Interfacing: A Case Study</a></p>
<p>109 <a title="nips-2002-109" href="../nips2002/nips-2002-Improving_a_Page_Classifier_with_Anchor_Extraction_and_Link_Analysis.html">nips-2002-Improving a Page Classifier with Anchor Extraction and Link Analysis</a></p>
<p>110 <a title="nips-2002-110" href="../nips2002/nips-2002-Incremental_Gaussian_Processes.html">nips-2002-Incremental Gaussian Processes</a></p>
<p>111 <a title="nips-2002-111" href="../nips2002/nips-2002-Independent_Components_Analysis_through_Product_Density_Estimation.html">nips-2002-Independent Components Analysis through Product Density Estimation</a></p>
<p>112 <a title="nips-2002-112" href="../nips2002/nips-2002-Inferring_a_Semantic_Representation_of_Text_via_Cross-Language_Correlation_Analysis.html">nips-2002-Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis</a></p>
<p>113 <a title="nips-2002-113" href="../nips2002/nips-2002-Information_Diffusion_Kernels.html">nips-2002-Information Diffusion Kernels</a></p>
<p>114 <a title="nips-2002-114" href="../nips2002/nips-2002-Information_Regularization_with_Partially_Labeled_Data.html">nips-2002-Information Regularization with Partially Labeled Data</a></p>
<p>115 <a title="nips-2002-115" href="../nips2002/nips-2002-Informed_Projections.html">nips-2002-Informed Projections</a></p>
<p>116 <a title="nips-2002-116" href="../nips2002/nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</a></p>
<p>117 <a title="nips-2002-117" href="../nips2002/nips-2002-Intrinsic_Dimension_Estimation_Using_Packing_Numbers.html">nips-2002-Intrinsic Dimension Estimation Using Packing Numbers</a></p>
<p>118 <a title="nips-2002-118" href="../nips2002/nips-2002-Kernel-Based_Extraction_of_Slow_Features%3A_Complex_Cells_Learn_Disparity_and_Translation_Invariance_from_Natural_Images.html">nips-2002-Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images</a></p>
<p>119 <a title="nips-2002-119" href="../nips2002/nips-2002-Kernel_Dependency_Estimation.html">nips-2002-Kernel Dependency Estimation</a></p>
<p>120 <a title="nips-2002-120" href="../nips2002/nips-2002-Kernel_Design_Using_Boosting.html">nips-2002-Kernel Design Using Boosting</a></p>
<p>121 <a title="nips-2002-121" href="../nips2002/nips-2002-Knowledge-Based_Support_Vector_Machine_Classifiers.html">nips-2002-Knowledge-Based Support Vector Machine Classifiers</a></p>
<p>122 <a title="nips-2002-122" href="../nips2002/nips-2002-Learning_About_Multiple_Objects_in_Images%3A_Factorial_Learning_without_Factorial_Search.html">nips-2002-Learning About Multiple Objects in Images: Factorial Learning without Factorial Search</a></p>
<p>123 <a title="nips-2002-123" href="../nips2002/nips-2002-Learning_Attractor_Landscapes_for_Learning_Motor_Primitives.html">nips-2002-Learning Attractor Landscapes for Learning Motor Primitives</a></p>
<p>124 <a title="nips-2002-124" href="../nips2002/nips-2002-Learning_Graphical_Models_with_Mercer_Kernels.html">nips-2002-Learning Graphical Models with Mercer Kernels</a></p>
<p>125 <a title="nips-2002-125" href="../nips2002/nips-2002-Learning_Semantic_Similarity.html">nips-2002-Learning Semantic Similarity</a></p>
<p>126 <a title="nips-2002-126" href="../nips2002/nips-2002-Learning_Sparse_Multiscale_Image_Representations.html">nips-2002-Learning Sparse Multiscale Image Representations</a></p>
<p>127 <a title="nips-2002-127" href="../nips2002/nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</a></p>
<p>128 <a title="nips-2002-128" href="../nips2002/nips-2002-Learning_a_Forward_Model_of_a_Reflex.html">nips-2002-Learning a Forward Model of a Reflex</a></p>
<p>129 <a title="nips-2002-129" href="../nips2002/nips-2002-Learning_in_Spiking_Neural_Assemblies.html">nips-2002-Learning in Spiking Neural Assemblies</a></p>
<p>130 <a title="nips-2002-130" href="../nips2002/nips-2002-Learning_in_Zero-Sum_Team_Markov_Games_Using_Factored_Value_Functions.html">nips-2002-Learning in Zero-Sum Team Markov Games Using Factored Value Functions</a></p>
<p>131 <a title="nips-2002-131" href="../nips2002/nips-2002-Learning_to_Classify_Galaxy_Shapes_Using_the_EM_Algorithm.html">nips-2002-Learning to Classify Galaxy Shapes Using the EM Algorithm</a></p>
<p>132 <a title="nips-2002-132" href="../nips2002/nips-2002-Learning_to_Detect_Natural_Image_Boundaries_Using_Brightness_and_Texture.html">nips-2002-Learning to Detect Natural Image Boundaries Using Brightness and Texture</a></p>
<p>133 <a title="nips-2002-133" href="../nips2002/nips-2002-Learning_to_Perceive_Transparency_from_the_Statistics_of_Natural_Scenes.html">nips-2002-Learning to Perceive Transparency from the Statistics of Natural Scenes</a></p>
<p>134 <a title="nips-2002-134" href="../nips2002/nips-2002-Learning_to_Take_Concurrent_Actions.html">nips-2002-Learning to Take Concurrent Actions</a></p>
<p>135 <a title="nips-2002-135" href="../nips2002/nips-2002-Learning_with_Multiple_Labels.html">nips-2002-Learning with Multiple Labels</a></p>
<p>136 <a title="nips-2002-136" href="../nips2002/nips-2002-Linear_Combinations_of_Optic_Flow_Vectors_for_Estimating_Self-Motion_-_a_Real-World_Test_of_a_Neural_Model.html">nips-2002-Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model</a></p>
<p>137 <a title="nips-2002-137" href="../nips2002/nips-2002-Location_Estimation_with_a_Differential_Update_Network.html">nips-2002-Location Estimation with a Differential Update Network</a></p>
<p>138 <a title="nips-2002-138" href="../nips2002/nips-2002-Manifold_Parzen_Windows.html">nips-2002-Manifold Parzen Windows</a></p>
<p>139 <a title="nips-2002-139" href="../nips2002/nips-2002-Margin-Based_Algorithms_for_Information_Filtering.html">nips-2002-Margin-Based Algorithms for Information Filtering</a></p>
<p>140 <a title="nips-2002-140" href="../nips2002/nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">nips-2002-Margin Analysis of the LVQ Algorithm</a></p>
<p>141 <a title="nips-2002-141" href="../nips2002/nips-2002-Maximally_Informative_Dimensions%3A_Analyzing_Neural_Responses_to_Natural_Signals.html">nips-2002-Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals</a></p>
<p>142 <a title="nips-2002-142" href="../nips2002/nips-2002-Maximum_Likelihood_and_the_Information_Bottleneck.html">nips-2002-Maximum Likelihood and the Information Bottleneck</a></p>
<p>143 <a title="nips-2002-143" href="../nips2002/nips-2002-Mean_Field_Approach_to_a_Probabilistic_Model_in_Information_Retrieval.html">nips-2002-Mean Field Approach to a Probabilistic Model in Information Retrieval</a></p>
<p>144 <a title="nips-2002-144" href="../nips2002/nips-2002-Minimax_Differential_Dynamic_Programming%3A_An_Application_to_Robust_Biped_Walking.html">nips-2002-Minimax Differential Dynamic Programming: An Application to Robust Biped Walking</a></p>
<p>145 <a title="nips-2002-145" href="../nips2002/nips-2002-Mismatch_String_Kernels_for_SVM_Protein_Classification.html">nips-2002-Mismatch String Kernels for SVM Protein Classification</a></p>
<p>146 <a title="nips-2002-146" href="../nips2002/nips-2002-Modeling_Midazolam%27s_Effect_on_the_Hippocampus_and_Recognition_Memory.html">nips-2002-Modeling Midazolam's Effect on the Hippocampus and Recognition Memory</a></p>
<p>147 <a title="nips-2002-147" href="../nips2002/nips-2002-Monaural_Speech_Separation.html">nips-2002-Monaural Speech Separation</a></p>
<p>148 <a title="nips-2002-148" href="../nips2002/nips-2002-Morton-Style_Factorial_Coding_of_Color_in_Primary_Visual_Cortex.html">nips-2002-Morton-Style Factorial Coding of Color in Primary Visual Cortex</a></p>
<p>149 <a title="nips-2002-149" href="../nips2002/nips-2002-Multiclass_Learning_by_Probabilistic_Embeddings.html">nips-2002-Multiclass Learning by Probabilistic Embeddings</a></p>
<p>150 <a title="nips-2002-150" href="../nips2002/nips-2002-Multiple_Cause_Vector_Quantization.html">nips-2002-Multiple Cause Vector Quantization</a></p>
<p>151 <a title="nips-2002-151" href="../nips2002/nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines.html">nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</a></p>
<p>152 <a title="nips-2002-152" href="../nips2002/nips-2002-Nash_Propagation_for_Loopy_Graphical_Games.html">nips-2002-Nash Propagation for Loopy Graphical Games</a></p>
<p>153 <a title="nips-2002-153" href="../nips2002/nips-2002-Neural_Decoding_of_Cursor_Motion_Using_a_Kalman_Filter.html">nips-2002-Neural Decoding of Cursor Motion Using a Kalman Filter</a></p>
<p>154 <a title="nips-2002-154" href="../nips2002/nips-2002-Neuromorphic_Bisable_VLSI_Synapses_with_Spike-Timing-Dependent_Plasticity.html">nips-2002-Neuromorphic Bisable VLSI Synapses with Spike-Timing-Dependent Plasticity</a></p>
<p>155 <a title="nips-2002-155" href="../nips2002/nips-2002-Nonparametric_Representation_of_Policies_and_Value_Functions%3A_A_Trajectory-Based_Approach.html">nips-2002-Nonparametric Representation of Policies and Value Functions: A Trajectory-Based Approach</a></p>
<p>156 <a title="nips-2002-156" href="../nips2002/nips-2002-On_the_Complexity_of_Learning_the_Kernel_Matrix.html">nips-2002-On the Complexity of Learning the Kernel Matrix</a></p>
<p>157 <a title="nips-2002-157" href="../nips2002/nips-2002-On_the_Dirichlet_Prior_and_Bayesian_Regularization.html">nips-2002-On the Dirichlet Prior and Bayesian Regularization</a></p>
<p>158 <a title="nips-2002-158" href="../nips2002/nips-2002-One-Class_LP_Classifiers_for_Dissimilarity_Representations.html">nips-2002-One-Class LP Classifiers for Dissimilarity Representations</a></p>
<p>159 <a title="nips-2002-159" href="../nips2002/nips-2002-Optimality_of_Reinforcement_Learning_Algorithms_with_Linear_Function_Approximation.html">nips-2002-Optimality of Reinforcement Learning Algorithms with Linear Function Approximation</a></p>
<p>160 <a title="nips-2002-160" href="../nips2002/nips-2002-Optoelectronic_Implementation_of_a_FitzHugh-Nagumo_Neural_Model.html">nips-2002-Optoelectronic Implementation of a FitzHugh-Nagumo Neural Model</a></p>
<p>161 <a title="nips-2002-161" href="../nips2002/nips-2002-PAC-Bayes_%26_Margins.html">nips-2002-PAC-Bayes & Margins</a></p>
<p>162 <a title="nips-2002-162" href="../nips2002/nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">nips-2002-Parametric Mixture Models for Multi-Labeled Text</a></p>
<p>163 <a title="nips-2002-163" href="../nips2002/nips-2002-Prediction_and_Semantic_Association.html">nips-2002-Prediction and Semantic Association</a></p>
<p>164 <a title="nips-2002-164" href="../nips2002/nips-2002-Prediction_of_Protein_Topologies_Using_Generalized_IOHMMs_and_RNNs.html">nips-2002-Prediction of Protein Topologies Using Generalized IOHMMs and RNNs</a></p>
<p>165 <a title="nips-2002-165" href="../nips2002/nips-2002-Ranking_with_Large_Margin_Principle%3A_Two_Approaches.html">nips-2002-Ranking with Large Margin Principle: Two Approaches</a></p>
<p>166 <a title="nips-2002-166" href="../nips2002/nips-2002-Rate_Distortion_Function_in_the_Spin_Glass_State%3A_A_Toy_Model.html">nips-2002-Rate Distortion Function in the Spin Glass State: A Toy Model</a></p>
<p>167 <a title="nips-2002-167" href="../nips2002/nips-2002-Rational_Kernels.html">nips-2002-Rational Kernels</a></p>
<p>168 <a title="nips-2002-168" href="../nips2002/nips-2002-Real-Time_Monitoring_of_Complex_Industrial_Processes_with_Particle_Filters.html">nips-2002-Real-Time Monitoring of Complex Industrial Processes with Particle Filters</a></p>
<p>169 <a title="nips-2002-169" href="../nips2002/nips-2002-Real-Time_Particle_Filters.html">nips-2002-Real-Time Particle Filters</a></p>
<p>170 <a title="nips-2002-170" href="../nips2002/nips-2002-Real_Time_Voice_Processing_with_Audiovisual_Feedback%3A_Toward_Autonomous_Agents_with_Perfect_Pitch.html">nips-2002-Real Time Voice Processing with Audiovisual Feedback: Toward Autonomous Agents with Perfect Pitch</a></p>
<p>171 <a title="nips-2002-171" href="../nips2002/nips-2002-Reconstructing_Stimulus-Driven_Neural_Networks_from_Spike_Times.html">nips-2002-Reconstructing Stimulus-Driven Neural Networks from Spike Times</a></p>
<p>172 <a title="nips-2002-172" href="../nips2002/nips-2002-Recovering_Articulated_Model_Topology_from_Observed_Rigid_Motion.html">nips-2002-Recovering Articulated Model Topology from Observed Rigid Motion</a></p>
<p>173 <a title="nips-2002-173" href="../nips2002/nips-2002-Recovering_Intrinsic_Images_from_a_Single_Image.html">nips-2002-Recovering Intrinsic Images from a Single Image</a></p>
<p>174 <a title="nips-2002-174" href="../nips2002/nips-2002-Regularized_Greedy_Importance_Sampling.html">nips-2002-Regularized Greedy Importance Sampling</a></p>
<p>175 <a title="nips-2002-175" href="../nips2002/nips-2002-Reinforcement_Learning_to_Play_an_Optimal_Nash_Equilibrium_in_Team_Markov_Games.html">nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</a></p>
<p>176 <a title="nips-2002-176" href="../nips2002/nips-2002-Replay%2C_Repair_and_Consolidation.html">nips-2002-Replay, Repair and Consolidation</a></p>
<p>177 <a title="nips-2002-177" href="../nips2002/nips-2002-Retinal_Processing_Emulation_in_a_Programmable_2-Layer_Analog_Array_Processor_CMOS_Chip.html">nips-2002-Retinal Processing Emulation in a Programmable 2-Layer Analog Array Processor CMOS Chip</a></p>
<p>178 <a title="nips-2002-178" href="../nips2002/nips-2002-Robust_Novelty_Detection_with_Single-Class_MPM.html">nips-2002-Robust Novelty Detection with Single-Class MPM</a></p>
<p>179 <a title="nips-2002-179" href="../nips2002/nips-2002-Scaling_of_Probability-Based_Optimization_Algorithms.html">nips-2002-Scaling of Probability-Based Optimization Algorithms</a></p>
<p>180 <a title="nips-2002-180" href="../nips2002/nips-2002-Selectivity_and_Metaplasticity_in_a_Unified_Calcium-Dependent_Model.html">nips-2002-Selectivity and Metaplasticity in a Unified Calcium-Dependent Model</a></p>
<p>181 <a title="nips-2002-181" href="../nips2002/nips-2002-Self_Supervised_Boosting.html">nips-2002-Self Supervised Boosting</a></p>
<p>182 <a title="nips-2002-182" href="../nips2002/nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">nips-2002-Shape Recipes: Scene Representations that Refer to the Image</a></p>
<p>183 <a title="nips-2002-183" href="../nips2002/nips-2002-Source_Separation_with_a_Sensor_Array_using_Graphical_Models_and_Subband_Filtering.html">nips-2002-Source Separation with a Sensor Array using Graphical Models and Subband Filtering</a></p>
<p>184 <a title="nips-2002-184" href="../nips2002/nips-2002-Spectro-Temporal_Receptive_Fields_of_Subthreshold_Responses_in_Auditory_Cortex.html">nips-2002-Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex</a></p>
<p>185 <a title="nips-2002-185" href="../nips2002/nips-2002-Speeding_up_the_Parti-Game_Algorithm.html">nips-2002-Speeding up the Parti-Game Algorithm</a></p>
<p>186 <a title="nips-2002-186" href="../nips2002/nips-2002-Spike_Timing-Dependent_Plasticity_in_the_Address_Domain.html">nips-2002-Spike Timing-Dependent Plasticity in the Address Domain</a></p>
<p>187 <a title="nips-2002-187" href="../nips2002/nips-2002-Spikernels%3A_Embedding_Spiking_Neurons_in_Inner-Product_Spaces.html">nips-2002-Spikernels: Embedding Spiking Neurons in Inner-Product Spaces</a></p>
<p>188 <a title="nips-2002-188" href="../nips2002/nips-2002-Stability-Based_Model_Selection.html">nips-2002-Stability-Based Model Selection</a></p>
<p>189 <a title="nips-2002-189" href="../nips2002/nips-2002-Stable_Fixed_Points_of_Loopy_Belief_Propagation_Are_Local_Minima_of_the_Bethe_Free_Energy.html">nips-2002-Stable Fixed Points of Loopy Belief Propagation Are Local Minima of the Bethe Free Energy</a></p>
<p>190 <a title="nips-2002-190" href="../nips2002/nips-2002-Stochastic_Neighbor_Embedding.html">nips-2002-Stochastic Neighbor Embedding</a></p>
<p>191 <a title="nips-2002-191" href="../nips2002/nips-2002-String_Kernels%2C_Fisher_Kernels_and_Finite_State_Automata.html">nips-2002-String Kernels, Fisher Kernels and Finite State Automata</a></p>
<p>192 <a title="nips-2002-192" href="../nips2002/nips-2002-Support_Vector_Machines_for_Multiple-Instance_Learning.html">nips-2002-Support Vector Machines for Multiple-Instance Learning</a></p>
<p>193 <a title="nips-2002-193" href="../nips2002/nips-2002-Temporal_Coherence%2C_Natural_Image_Sequences%2C_and_the_Visual_Cortex.html">nips-2002-Temporal Coherence, Natural Image Sequences, and the Visual Cortex</a></p>
<p>194 <a title="nips-2002-194" href="../nips2002/nips-2002-The_Decision_List_Machine.html">nips-2002-The Decision List Machine</a></p>
<p>195 <a title="nips-2002-195" href="../nips2002/nips-2002-The_Effect_of_Singularities_in_a_Learning_Machine_when_the_True_Parameters_Do_Not_Lie_on_such_Singularities.html">nips-2002-The Effect of Singularities in a Learning Machine when the True Parameters Do Not Lie on such Singularities</a></p>
<p>196 <a title="nips-2002-196" href="../nips2002/nips-2002-The_RA_Scanner%3A_Prediction_of_Rheumatoid_Joint_Inflammation_Based_on_Laser_Imaging.html">nips-2002-The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging</a></p>
<p>197 <a title="nips-2002-197" href="../nips2002/nips-2002-The_Stability_of_Kernel_Principal_Components_Analysis_and_its_Relation_to_the_Process_Eigenspectrum.html">nips-2002-The Stability of Kernel Principal Components Analysis and its Relation to the Process Eigenspectrum</a></p>
<p>198 <a title="nips-2002-198" href="../nips2002/nips-2002-Theory-Based_Causal_Inference.html">nips-2002-Theory-Based Causal Inference</a></p>
<p>199 <a title="nips-2002-199" href="../nips2002/nips-2002-Timing_and_Partial_Observability_in_the_Dopamine_System.html">nips-2002-Timing and Partial Observability in the Dopamine System</a></p>
<p>200 <a title="nips-2002-200" href="../nips2002/nips-2002-Topographic_Map_Formation_by_Silicon_Growth_Cones.html">nips-2002-Topographic Map Formation by Silicon Growth Cones</a></p>
<p>201 <a title="nips-2002-201" href="../nips2002/nips-2002-Transductive_and_Inductive_Methods_for_Approximate_Gaussian_Process_Regression.html">nips-2002-Transductive and Inductive Methods for Approximate Gaussian Process Regression</a></p>
<p>202 <a title="nips-2002-202" href="../nips2002/nips-2002-Unsupervised_Color_Constancy.html">nips-2002-Unsupervised Color Constancy</a></p>
<p>203 <a title="nips-2002-203" href="../nips2002/nips-2002-Using_Tarjan%27s_Red_Rule_for_Fast_Dependency_Tree_Construction.html">nips-2002-Using Tarjan's Red Rule for Fast Dependency Tree Construction</a></p>
<p>204 <a title="nips-2002-204" href="../nips2002/nips-2002-VIBES%3A_A_Variational_Inference_Engine_for_Bayesian_Networks.html">nips-2002-VIBES: A Variational Inference Engine for Bayesian Networks</a></p>
<p>205 <a title="nips-2002-205" href="../nips2002/nips-2002-Value-Directed_Compression_of_POMDPs.html">nips-2002-Value-Directed Compression of POMDPs</a></p>
<p>206 <a title="nips-2002-206" href="../nips2002/nips-2002-Visual_Development_Aids_the_Acquisition_of_Motion_Velocity_Sensitivities.html">nips-2002-Visual Development Aids the Acquisition of Motion Velocity Sensitivities</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
