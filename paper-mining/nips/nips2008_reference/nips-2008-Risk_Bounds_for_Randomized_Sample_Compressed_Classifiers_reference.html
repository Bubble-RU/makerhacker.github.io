<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-199" href="../nips2008/nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">nips2008-199</a> <a title="nips-2008-199-reference" href="#">nips2008-199-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</h1>
<br/><p>Source: <a title="nips-2008-199-pdf" href="http://papers.nips.cc/paper/3388-risk-bounds-for-randomized-sample-compressed-classifiers.pdf">pdf</a></p><p>Author: Mohak Shah</p><p>Abstract: We derive risk bounds for the randomized classiﬁers in Sample Compression setting where the classiﬁer-speciﬁcation utilizes two sources of information viz. the compression set and the message string. By extending the recently proposed Occam’s Hammer principle to the data-dependent settings, we derive point-wise versions of the bounds on the stochastic sample compressed classiﬁers and also recover the corresponding classical PAC-Bayes bound. We further show how these compare favorably to the existing results.</p><br/>
<h2>reference text</h2><p>Gilles Blanchard and Francois Fleuret. Occam’s hammer. In Proceedings of the 20th Annual Con¸ ference on Learning Theory (COLT-2007), volume 4539 of Lecture Notes on Computer Science, pages 112–126, 2007. Sally Floyd and Manfred Warmuth. Sample compression, learnability, and the Vapnik-Chervonenkis dimension. Machine Learning, 21(3):269–304, 1995. John Langford. Tutorial on practical prediction theory for classiﬁcation. Journal of Machine Learning Research, 3:273–306, 2005. Francois Laviolette and Mario Marchand. PAC-Bayes risk bounds for stochastic averages and major¸ ity votes of sample-compressed classiﬁers. Journal of Machine Learning Research, 8:1461–1487, 2007. Francois Laviolette, Mario Marchand, and Mohak Shah. Margin-sparsity trade-off for the set covering machine. In Proceedings of the 16th European Conference on Machine Learning, ECML 2005, volume 3720 of Lecture Notes in Artiﬁcial Intelligence, pages 206–217. Springer, 2005. N. Littlestone and M. Warmuth. Relating data compression and learnability. Technical report, University of California Santa Cruz, Santa Cruz, CA, 1986. Mario Marchand and John Shawe-Taylor. The Set Covering Machine. Journal of Machine Learning Reasearch, 3:723–746, 2002. David McAllester. Some PAC-Bayesian theorems. Machine Learning, 37:355–363, 1999.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
