<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>126 nips-2008-Localized Sliced Inverse Regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-126" href="../nips2008/nips-2008-Localized_Sliced_Inverse_Regression.html">nips2008-126</a> <a title="nips-2008-126-reference" href="#">nips2008-126-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>126 nips-2008-Localized Sliced Inverse Regression</h1>
<br/><p>Source: <a title="nips-2008-126-pdf" href="http://papers.nips.cc/paper/3595-localized-sliced-inverse-regression.pdf">pdf</a></p><p>Author: Qiang Wu, Sayan Mukherjee, Feng Liang</p><p>Abstract: We developed localized sliced inverse regression for supervised dimension reduction. It has the advantages of preventing degeneracy, increasing estimation accuracy, and automatic subclass discovery in classiﬁcation problems. A semisupervised version is proposed for the use of unlabeled data. The utility is illustrated on simulated as well as real data sets.</p><br/>
<h2>reference text</h2><p>[1] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15(6):1373–1396, 2003.</p>
<p>[2] R. Cook and L. Ni. Using intra-slice covariances for improved estimation of the central subspace in regression. Biometrika, 93(1):65–74, 2006.</p>
<p>[3] R. Cook and S. Weisberg. Disussion of li (1991). J. Amer. Statist. Assoc., 86:328–332, 1991.</p>
<p>[4] R. Cook and X. Yin. Dimension reduction and visualization in discriminant analysis (with discussion). Aust. N. Z. J. Stat., 43(2):147–199, 2001.</p>
<p>[5] D. Donoho and C. Grimes. Hessian eigenmaps: new locally linear embedding techniques for highdimensional data. PNAS, 100:5591–5596, 2003.</p>
<p>[6] K. Fukumizu, F. R. Bach, and M. I. Jordan. Kernel dimension reduction in regression. Annals of Statistics, to appear, 2008.</p>
<p>[7] A. Globerson and S. Roweis. Metric learning by collapsing classes. In Y. Weiss, B. Sch¨ lkopf, o and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 451–458. MIT Press, Cambridge, MA, 2006.</p>
<p>[8] T. Golub, D. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J. Mesirov, H. Coller, M. Loh, J. Downing, M. Caligiuri, C. Bloomﬁeld, and E. Lander. Molecular classiﬁcation of cancer: class discovery and class prediction by gene expression monitoring. Science, 286:531–537, 1999.</p>
<p>[9] T. Hastie and R. Tibshirani. Discrminant adaptive nearest neighbor classiﬁcation. IEEE Transacations on Pattern Analysis and Machine Intelligence, 18(6):607–616, 1996.</p>
<p>[10] K. Li. Sliced inverse regression for dimension reduction (with discussion). J. Amer. Statist. Assoc., 86:316–342, 1991.</p>
<p>[11] K. C. Li. On principal hessian directions for data visulization and dimension reduction: another application of stein’s lemma. J. Amer. Statist. Assoc., 87:1025–1039, 1992.</p>
<p>[12] K. C. Li. High dimensional data analysis via the sir/phd approach, 2000.</p>
<p>[13] J. Nilsson, F. Sha, and M. I. Jordan. Regression on manifold using kernel dimension reduction. In Proc. of ICML 2007, 2007.</p>
<p>[14] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290:2323–2326, 2000.</p>
<p>[15] M. Sugiyam. Dimension reduction of multimodal labeled data by local ﬁsher discriminatn analysis. Journal of Machine Learning Research, 8:1027–1061, 2007.</p>
<p>[16] J. Tenenbaum, V. de Silva, and J. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290:2319–2323, 2000.</p>
<p>[17] Q. Wu, F. Liang, and S. Mukherjee. Regularized sliced inverse regression for kernel models. Technical report, ISDS Discussion Paper, Duke University, 2007.</p>
<p>[18] Y. Xia, H. Tong, W. Li, and L.-X. Zhu. An adaptive estimation of dimension reduction space. J. R. Statist. Soc. B, 64(3):363–410, 2002.</p>
<p>[19] G. Young. Maximum likelihood estimation and factor analysis. Psychometrika, 6:49–53, 1941.</p>
<p>[20] W. Zhong, P. Zeng, P. Ma, J. S. Liu, and Y. Zhu. RSIR: regularized sliced inverse regression for motif discovery. Bioinformatics, 21(22):4169–4175, 2005.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
