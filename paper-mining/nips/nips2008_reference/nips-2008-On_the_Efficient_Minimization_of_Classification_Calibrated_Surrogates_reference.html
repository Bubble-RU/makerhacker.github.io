<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-163" href="../nips2008/nips-2008-On_the_Efficient_Minimization_of_Classification_Calibrated_Surrogates.html">nips2008-163</a> <a title="nips-2008-163-reference" href="#">nips2008-163-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</h1>
<br/><p>Source: <a title="nips-2008-163-pdf" href="http://papers.nips.cc/paper/3485-on-the-efficient-minimization-of-classification-calibrated-surrogates.pdf">pdf</a></p><p>Author: Richard Nock, Frank Nielsen</p><p>Abstract: Bartlett et al (2006) recently proved that a ground condition for convex surrogates, classiﬁcation calibration, ties up the minimization of the surrogates and classiﬁcation risks, and left as an important problem the algorithmic questions about the minimization of these surrogates. In this paper, we propose an algorithm which provably minimizes any classiﬁcation calibrated surrogate strictly convex and differentiable — a set whose losses span the exponential, logistic and squared losses —, with boosting-type guaranteed convergence rates under a weak learning assumption. A particular subclass of these surrogates, that we call balanced convex surrogates, has a key rationale that ties it to maximum likelihood estimation, zerosum games and the set of losses that satisfy some of the most common requirements for losses in supervised learning. We report experiments on more than 50 readily available domains of 11 ﬂavors of the algorithm, that shed light on new surrogates, and the potential of data dependent strategies to tune surrogates. 1</p><br/>
<h2>reference text</h2><p>[1] A. Banerjee, X. Guo, and H. Wang. On the optimality of conditional expectation as a bregman predictor. IEEE Trans. on Information Theory, 51:2664–2669, 2005.</p>
<p>[2] A. Banerjee, S. Merugu, I. Dhillon, and J. Ghosh. Clustering with Bregman divergences. Journal of Machine Learning Research, 6:1705–1749, 2005.</p>
<p>[3] P. Bartlett, M. Jordan, and J. D. McAuliffe. Convexity, classiﬁcation, and risk bounds. Journal of the Am. Stat. Assoc., 101:138–156, 2006.</p>
<p>[4] P. Bartlett and M. Traskin. Adaboost is consistent. In NIPS*19, 2006.</p>
<p>[5] L. M. Bregman. The relaxation method of ﬁnding the common point of convex sets and its application to the solution of problems in convex programming. USSR Comp. Math. and Math. Phys., 7:200–217, 1967.</p>
<p>[6] M. Collins, R. Schapire, and Y. Singer. Logistic regression, adaboost and Bregman distances. In COLT’00, pages 158–169, 2000.</p>
<p>[7] J. Friedman, T. Hastie, and R. Tibshirani. Additive Logistic Regression : a Statistical View of Boosting. Ann. of Stat., 28:337–374, 2000.</p>
<p>[8] C. Gentile and M. Warmuth. Linear hinge loss and average margin. In NIPS*11, pages 225–231, 1998.</p>
<p>[9] P. Gr¨ nwald and P. Dawid. Game theory, maximum entropy, minimum discrepancy and robust Bayesian u decision theory. Ann. of Statistics, 32:1367–1433, 2004.</p>
<p>[10] M.J. Kearns and Y. Mansour. On the boosting ability of top-down decision tree learning algorithms. Journal of Comp. Syst. Sci., 58:109–128, 1999.</p>
<p>[11] K. Matsushita. Decision rule, based on distance, for the classiﬁcation problem. Ann. of the Inst. for Stat. Math., 8:67–77, 1956.</p>
<p>[12] R. Nock and F. Nielsen. A Real Generalization of discrete AdaBoost. Artif. Intell., 171:25–41, 2007.</p>
<p>[13] R. E. Schapire and Y. Singer. Improved boosting algorithms using conﬁdence-rated predictions. In COLT’98, pages 80–91, 1998.</p>
<p>[14] M. Warmuth, J. Liao, and G. R¨ tsch. Totally corrective boosting algorithms that maximize the margin. In a ICML’06, pages 1001–1008, 2006.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
