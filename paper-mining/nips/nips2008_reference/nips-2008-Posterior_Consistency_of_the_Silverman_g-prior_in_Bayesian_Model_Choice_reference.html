<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>182 nips-2008-Posterior Consistency of the Silverman g-prior in Bayesian Model Choice</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-182" href="../nips2008/nips-2008-Posterior_Consistency_of_the_Silverman_g-prior_in_Bayesian_Model_Choice.html">nips2008-182</a> <a title="nips-2008-182-reference" href="#">nips2008-182-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>182 nips-2008-Posterior Consistency of the Silverman g-prior in Bayesian Model Choice</h1>
<br/><p>Source: <a title="nips-2008-182-pdf" href="http://papers.nips.cc/paper/3462-posterior-consistency-of-the-silverman-g-prior-in-bayesian-model-choice.pdf">pdf</a></p><p>Author: Zhihua Zhang, Michael I. Jordan, Dit-Yan Yeung</p><p>Abstract: Kernel supervised learning methods can be uniﬁed by utilizing the tools from regularization theory. The duality between regularization and prior leads to interpreting regularization methods in terms of maximum a posteriori estimation and has motivated Bayesian interpretations of kernel methods. In this paper we pursue a Bayesian interpretation of sparsity in the kernel setting by making use of a mixture of a point-mass distribution and prior that we refer to as “Silverman’s g-prior.” We provide a theoretical analysis of the posterior consistency of a Bayesian model choice procedure based on this prior. We also establish the asymptotic relationship between this procedure and the Bayesian information criterion. 1</p><br/>
<h2>reference text</h2><p>[1] C. Fern´ ndez, E. Ley, and M. F. J. Steel. Benchmark priors for Bayesian model averaging. a Journal of Econometrics, 100:381–427, 2001.</p>
<p>[2] E. I. George and R. E. McCulloch. Approaches for Bayesian variable selection. Statistica Sinica, 7:339–374, 1997.</p>
<p>[3] R. E. Kass and A. E. Raftery. Bayes factors. Journal of the American Statistical Association, 90:773–795, 1995.</p>
<p>[4] F. Liang, R. Paulo, G. Molina, M. A. Clyde, and J. O. Berger. Mixtures of g-priors for Bayesian variable selection. Journal of the American Statistical Association, 103(481):410–423, 2008.</p>
<p>[5] B. W. Silverman. Some aspects of the spline smoothing approach to non-parametric regression curve ﬁtting (with discussion). Journal of the Royal Statistical Society, B, 47(1):1–52, 1985.</p>
<p>[6] M. Smith and R. Kohn. Nonparametric regression using Bayesian variable selection. Journal of Econometrics, 75:317–344, 1996.</p>
<p>[7] G. Wahba. Spline Models for Observational Data. SIAM, Philadelphia, 1990.</p>
<p>[8] M. West. Bayesian factor regression models in the “large p, small n” paradigm. In J. M. Bernardo, M. J. Bayarri, J. .O Berger, A. P. Dawid, D. Heckerman, A. F. M. Smith, and M. West, editors, Bayesian Statistics 7, pages 723–732. Oxford University Press, 2003.</p>
<p>[9] A. Zellner. On assessing prior distributions and Bayesian regression analysis with g−prior distributions. In P. K. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques: Essays in Honor of Bruno de Finetti, pages 233–243. North-Holland, Amsterdam, 1986.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
