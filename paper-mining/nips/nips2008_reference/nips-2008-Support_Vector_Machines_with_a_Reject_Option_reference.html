<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>228 nips-2008-Support Vector Machines with a Reject Option</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-228" href="../nips2008/nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">nips2008-228</a> <a title="nips-2008-228-reference" href="#">nips2008-228-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>228 nips-2008-Support Vector Machines with a Reject Option</h1>
<br/><p>Source: <a title="nips-2008-228-pdf" href="http://papers.nips.cc/paper/3594-support-vector-machines-with-a-reject-option.pdf">pdf</a></p><p>Author: Yves Grandvalet, Alain Rakotomamonjy, Joseph Keshet, Stéphane Canu</p><p>Abstract: We consider the problem of binary classiﬁcation where the classiﬁer may abstain instead of classifying each observation. The Bayes decision rule for this setup, known as Chow’s rule, is deﬁned by two thresholds on posterior probabilities. From simple desiderata, namely the consistency and the sparsity of the classiﬁer, we derive the double hinge loss function that focuses on estimating conditional probabilities only in the vicinity of the threshold points of the optimal decision rule. We show that, for suitable kernel machines, our approach is universally consistent. We cast the problem of minimizing the double hinge loss as a quadratic program akin to the standard SVM optimization problem and propose an active set method to solve it efﬁciently. We ﬁnally provide preliminary experimental results illustrating the interest of our constructive approach to devising loss functions. 1</p><br/>
<h2>reference text</h2><p>Bartlett, P. L., & Tewari, A. (2007). Sparseness vs estimating conditional probabilities: Some asymptotic results. Journal of Machine Learning Research, 8, 775–790. Bartlett, P. L., & Wegkamp, M. H. (2008). Classiﬁcation with a reject option using a hinge loss. Journal of Machine Learning Research, 9, 1823–1840. Chow, C. K. (1970). On optimum recognition error and reject tradeoff. IEEE Trans. on Info. Theory, 16, 41–46. Fumera, G., & Roli, F. (2002). Support vector machines with embedded reject option. Pattern Recognition with Support Vector Machines: First International Workshop (pp. 68–82). Springer. Grandvalet, Y., Mari´ thoz, J., & Bengio, S. (2006). A probabilistic interpretation of SVMs with an application e to unbalanced classiﬁcation. NIPS 18 (pp. 467–474). MIT Press. Herbei, R., & Wegkamp, M. H. (2006). Classiﬁcation with reject option. The Canadian Journal of Statistics, 34, 709–721. Kwok, J. T. (1999). Moderating the outputs of support vector machine classiﬁers. IEEE Trans. on Neural Networks, 10, 1018–1031. Steinwart, I. (2005). Consistency of support vector machine and other regularized kernel classiﬁers. IEEE Trans. on Info. Theory, 51, 128–142. Vapnik, V. N. (1995). The nature of statistical learning theory. Springer Series in Statistics. Springer. Vishwanathan, S. V. N., Smola, A., & Murty, N. (2003). SimpleSVM. Proceedings of the Twentieth International Conference on Machine Learning (pp. 68–82). AAAI.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
