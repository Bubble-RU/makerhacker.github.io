<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>84 nips-2008-Fast Prediction on a Tree</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-84" href="../nips2008/nips-2008-Fast_Prediction_on_a_Tree.html">nips2008-84</a> <a title="nips-2008-84-reference" href="#">nips2008-84-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>84 nips-2008-Fast Prediction on a Tree</h1>
<br/><p>Source: <a title="nips-2008-84-pdf" href="http://papers.nips.cc/paper/3549-fast-prediction-on-a-tree.pdf">pdf</a></p><p>Author: Mark Herbster, Massimiliano Pontil, Sergio R. Galeano</p><p>Abstract: Given an n-vertex weighted tree with structural diameter S and a subset of m vertices, we present a technique to compute a corresponding m × m Gram matrix of the pseudoinverse of the graph Laplacian in O(n + m2 + mS) time. We discuss the application of this technique to fast label prediction on a generic graph. We approximate the graph with a spanning tree and then we predict with the kernel perceptron. We address the approximation of the graph with either a minimum spanning tree or a shortest path tree. The fast computation of the pseudoinverse enables us to address prediction problems on large graphs. We present experiments on two web-spam classiﬁcation tasks, one of which includes a graph with 400,000 vertices and more than 10,000,000 edges. The results indicate that the accuracy of our technique is competitive with previous methods using the full graph information. 1</p><br/>
<h2>reference text</h2><p>[1] J. Abernethy, O. Chapelle and C. Castillo. Webspam Identiﬁcation Through Content and Hyperlinks. Proc. Adversarial Information Retrieval on Web, 2008.</p>
<p>[2] A. Argyriou, M. Herbster, and M. Pontil. Combining graph Laplacians for semi-supervised learning. Advances in Neural Information Processing Systems 17. MIT Press, Cambridge, MA, 2005.</p>
<p>[3] M. Belkin, I. Matveeva, P. Niyogi. Regularization and Semi-supervised Learning on Large Graphs. Proceedings of the 17-th Conference on Learning Theory (COLT’ 04), pages 624–638, 2004.</p>
<p>[4] C. Biemann. Chinese Whispers – an Efﬁcient Graph Clustering Algorithm and its Application to Natural Language Processing Problems. Proc. HLT-NAACL-06 Workshop on Textgraphs-06, 2006.</p>
<p>[5] A. Blum, J. Lafferty, M. R. Rwebangira, and R. Reddy. Semi-supervised learning using randomized mincuts. Proc. 21-st International Conference on Machine Learning, page 13, 2004.</p>
<p>[6] U. Brandes and D. Fleischer. Centrality measures based on current ﬂow. Proc. 22-nd Annual Symposium on Theoretical Aspects of Computer Science, pages 533–544, 2005.</p>
<p>[7] C. Castillo, B. D. Davison, L. Denoyer and P. Gallinari. Proc. of the Graph Labelling Workshop and Web-spam Challenge (ECML Workshop), 2007.</p>
<p>[8] T. H. Cormen, C. E. Leiserson, and R. L. Rivest. Introduction to Algorithms. MIT Press, 1990.</p>
<p>[9] P. Drineas and M. W. Mahoney, On the Nystr¨ m Method for Approximating a Gram Matrix for Improved o Kernel-Based Learning. J. Mach. Learn. Res., 6:2153–2175, 2005.</p>
<p>[10] A. Ghosh, S. Boyd and A. Saberi. Minimizing Effective Resistance of a Graph. SIAM Review, problems and techniques section, 50(1):37-66, 2008.</p>
<p>[11] T. Jebara. Bayesian Out-Trees. Proc. Uncertainty in Artiﬁcal Intelligence, 2008.</p>
<p>[12] R. E. Haymond, J. Jarvis and D. R. Shier. Algorithm 613: Minimum Spanning Tree for Moderate Integer Weights. ACM Trans. Math. Softw., 10(1):108–111, 1984.</p>
<p>[13] M. Herbster and M. Pontil. Prediction on a graph with a perceptron. Advances in Neural Information Processing Systems 19, pages 577–584. MIT Press, 2007.</p>
<p>[14] M. Herbster, M. Pontil, and L. Wainer. Online learning over graphs. In ICML ’05: Proceedings of the 22nd international conference on Machine learning, pages 305–312, 2005.</p>
<p>[15] N.-D. Ho and P. V. Dooren. On the pseudo-inverse of the Laplacian of a bipartite graph. Appl. Math. Lett., 18(8):917–922, 2005.</p>
<p>[16] D. Klein and M. Randi´ . Resistance distance. J. of Mathematical Chemistry, 12(1):81–95, 1993. c</p>
<p>[17] M. E. J. Newman. A measure of betweenness centrality based on random walks. Soc. Networks, 27:39– 54, 2005.</p>
<p>[18] D. A. Spielman and S.-H. Teng. Nearly-linear time algorithms for graph partitioning, graph sparsiﬁcation, and solving linear systems. Proc. 36-th Annual ACM Symposium Theory of Computing, 2004.</p>
<p>[19] C.K.I. Williams and M. Seeger. Using the Nystr¨ m Method to Speed Up Kernel Machines. Neural o Information Processing Systems 13, pages 682–688, MIT Press, 2001</p>
<p>[20] X. Zhu, J. Lafferty, and Z. Ghahramani. Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions. Proc of the the 20-th International Conference on Machine Learning, pages 912–919, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
