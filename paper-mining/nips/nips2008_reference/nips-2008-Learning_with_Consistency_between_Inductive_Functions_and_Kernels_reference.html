<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>122 nips-2008-Learning with Consistency between Inductive Functions and Kernels</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-122" href="../nips2008/nips-2008-Learning_with_Consistency_between_Inductive_Functions_and_Kernels.html">nips2008-122</a> <a title="nips-2008-122-reference" href="#">nips2008-122-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>122 nips-2008-Learning with Consistency between Inductive Functions and Kernels</h1>
<br/><p>Source: <a title="nips-2008-122-pdf" href="http://papers.nips.cc/paper/3460-learning-with-consistency-between-inductive-functions-and-kernels.pdf">pdf</a></p><p>Author: Haixuan Yang, Irwin King, Michael Lyu</p><p>Abstract: Regularized Least Squares (RLS) algorithms have the ability to avoid over-ﬁtting problems and to express solutions as kernel expansions. However, we observe that the current RLS algorithms cannot provide a satisfactory interpretation even on the penalty of a constant function. Based on the intuition that a good kernelbased inductive function should be consistent with both the data and the kernel, a novel learning scheme is proposed. The advantages of this scheme lie in its corresponding Representer Theorem, its strong interpretation ability about what kind of functions should not be penalized, and its promising accuracy improvements shown in a number of experiments. Furthermore, we provide a detailed technical description about heat kernels, which serves as an example for the readers to apply similar techniques for other kernels. Our work provides a preliminary step in a new direction to explore the varying consistency between inductive functions and kernels under various distributions. 1</p><br/>
<h2>reference text</h2><p>[1] GMikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of Machine Learning Research, 7:2399–2434, 2006.</p>
<p>[2] F. Cucker and S. Smale. On the mathematical foundations of learning. Bulletin (New Series) of the American Mathematical Society, 39(1):1–49, 2002.</p>
<p>[3] Lokenath Debnath and Piotr Mikusinski. Introduction to Hilbert Spaces with Applications. Academic Press, San Diego, second edition, 1999.</p>
<p>[4] T. Evgeniou, M. Pontil, and T. Poggio. Regularization networks and support vector machines. Advances in Computational Mathematics, 13:1–50, 2000.</p>
<p>[5] T. Hastie and C. Loader. Local regression: Automatic kernel carpentry. Statistical Science, 8(1):120–129, 1993.</p>
<p>[6] John Lafferty and Guy Lebanon. Diffusion kernels on statistical manifolds. Journal of Machine Learning Research, 6:129–163, 2005.</p>
<p>[7] Wenye Li, Kin-Hong Lee, and Kwong-Sak Leung. Generalized regularized least-squares learning with predeﬁned features in a Hilbert space. In NIPS, 2006.</p>
<p>[8] E. A. Nadaraya. On estimating regression. Theory of Probability and Its Applications, 9(1):141–142, 1964.</p>
<p>[9] R.M. Rifkin and R.A. Lippert. Notes on regularized least-squares. Technical Report 2007-019, Massachusetts Institute of Technology, 2007.</p>
<p>[10] S. Rosenberg. The Laplacian on a Riemmannian Manifold. Cambridge University Press, 1997.</p>
<p>[11] Bernhard Sch¨ lkopf, Ralf Herbrich, and Alex J. Smola. A generalized representer theorem. In o COLT, 2001.</p>
<p>[12] I. Sch¨ nberg. Spline functions and the problem of graduation. Proc. Nat. Acad. Sci. USA, o 52:947–950, 1964.</p>
<p>[13] A. N. Tikhonov and V. Y. Arsenin. Solutions of Ill-posed Problems. W. H. Winston, 1977.</p>
<p>[14] G. S. Watson. Smooth regression analysis. Sankhy´ , Series A, 26:359–372, 1964. a</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
