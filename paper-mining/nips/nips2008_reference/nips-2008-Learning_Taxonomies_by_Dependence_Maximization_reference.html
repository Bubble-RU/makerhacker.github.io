<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>117 nips-2008-Learning Taxonomies by Dependence Maximization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-117" href="../nips2008/nips-2008-Learning_Taxonomies_by_Dependence_Maximization.html">nips2008-117</a> <a title="nips-2008-117-reference" href="#">nips2008-117-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>117 nips-2008-Learning Taxonomies by Dependence Maximization</h1>
<br/><p>Source: <a title="nips-2008-117-pdf" href="http://papers.nips.cc/paper/3592-learning-taxonomies-by-dependence-maximization.pdf">pdf</a></p><p>Author: Matthew Blaschko, Arthur Gretton</p><p>Abstract: We introduce a family of unsupervised algorithms, numerical taxonomy clustering, to simultaneously cluster data, and to learn a taxonomy that encodes the relationship between the clusters. The algorithms work by maximizing the dependence between the taxonomy and the original data. The resulting taxonomy is a more informative visualization of complex data than simple clustering; in addition, taking into account the relations between different clusters is shown to substantially improve the quality of the clustering, when compared with state-ofthe-art algorithms in the literature (both spectral clustering and a previous dependence maximization approach). We demonstrate our algorithm on image and text data. 1</p><br/>
<h2>reference text</h2><p>[1] R. Agarwala, V. Bafna, M. Farach, B. Narayanan, M. Paterson, and M. Thorup. On the approximability of numerical taxonomy (ﬁtting distances by tree metrics). In SODA, pages 365–372, 1996.</p>
<p>[2] N. Ailon and M. Charikar. Fitting tree metrics: Hierarchical clustering and phylogeny. In Foundations of Computer Science, pages 73–82, 2005.</p>
<p>[3] F. R. Bach and M. I. Jordan. Learning spectral clustering, with application to speech separation. JMLR, 7:1963–2001, 2006.</p>
<p>[4] R. Baire. Lecons sur les Fonctions Discontinues. Gauthier Villars, 1905. ¸</p>
<p>[5] C. Baker. Joint measures and cross-covariance operators. Transactions of the American Mathematical Society, 186:273–289, 1973.</p>
<p>[6] M. B. Blaschko and A. Gretton. Taxonomy inference using kernel dependence measures. Technical report, Max Planck Institute for Biological Cybernetics, 2008.</p>
<p>[7] D. Blei, T. Grifﬁths, M. Jordan, and J. Tenenbaum. Hierarchical topic models and the nested chinese restaurant process. In NIPS 16, 2004.</p>
<p>[8] P. Buneman. The Recovery of Trees from Measures of Dissimilarity. In D. Kendall and P. Tautu, editors, Mathematics the the Archeological and Historical Sciences, pages 387–395. Edinburgh U.P., 1971.</p>
<p>[9] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley, 1991.</p>
<p>[10] N. Cristianini, J. Shawe-Taylor, A. Elisseeff, and J. Kandola. On kernel-target alignment. In NIPS 14, 2002.</p>
<p>[11] M. Farach, S. Kannan, and T. Warnow. A robust model for ﬁnding optimal evolutionary trees. In STOC, pages 137–145, 1993.</p>
<p>[12] K. Fukumizu, F. R. Bach, and M. I. Jordan. Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces. JMLR, 5:73–99, 2004.</p>
<p>[13] K. Fukumizu, A. Gretton, X. Sun, and B. Sch¨ lkopf. Kernel measures of conditional dependence. In o NIPS 20, 2008.</p>
<p>[14] A. Gretton, O. Bousquet, A. Smola, and B. Sch¨ lkopf. Measuring statistical dependence with Hilberto Schmidt norms. In Algorithmic Learning Theory, pages 63–78, 2005.</p>
<p>[15] B. Harb, S. Kannan, and A. McGregor. Approximating the best-ﬁt tree under lp norms. In APPROXRANDOM, pages 123–133, 2005.</p>
<p>[16] D. Haussler. Convolution kernels on discrete structures. Technical Report UCSC-CRL-99-10, University of California at Santa Cruz, 1999.</p>
<p>[17] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, 1985.</p>
<p>[18] A. K. Jain and R. C. Dubes. Algorithms for Clustering Data. Prentice Hall, 1988.</p>
<p>[19] P. Macnaughton Smith, W. Williams, M. Dale, and L. Mockett. Dissimilarity analysis: a new technique of hierarchical subdivision. Nature, 202:1034–1035, 1965.</p>
<p>[20] C. D. Meyer, Jr. Generalized inversion of modiﬁed matrices. SIAM Journal on Applied Mathematics, 24(3):315–323, 1973.</p>
<p>[21] A. Y. Ng, M. I. Jordan, and Y. Weiss. On Spectral Clustering: Analysis and an Algorithm. In NIPS, pages 849–856, 2001.</p>
<p>[22] L. Song, A. Smola, A. Gretton, and K. M. Borgwardt. A Dependence Maximization View of Clustering. In ICML, pages 815–822, 2007.</p>
<p>[23] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical dirichlet processes. JASA, 101(476):1566–1581, 2006.</p>
<p>[24] U. von Luxburg. A Tutorial on Spectral Clustering. Statistics and Computing, 17(4):395–416, 2007.</p>
<p>[25] M. S. Waterman, T. F. Smith, M. Singh, and W. A. Beyer. Additive Evolutionary Trees. Journal of Theoretical Biology, 64:199–213, 1977.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
