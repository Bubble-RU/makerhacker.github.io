<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>153 nips-2008-Nonlinear causal discovery with additive noise models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-153" href="../nips2008/nips-2008-Nonlinear_causal_discovery_with_additive_noise_models.html">nips2008-153</a> <a title="nips-2008-153-reference" href="#">nips2008-153-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>153 nips-2008-Nonlinear causal discovery with additive noise models</h1>
<br/><p>Source: <a title="nips-2008-153-pdf" href="http://papers.nips.cc/paper/3548-nonlinear-causal-discovery-with-additive-noise-models.pdf">pdf</a></p><p>Author: Patrik O. Hoyer, Dominik Janzing, Joris M. Mooij, Jan R. Peters, Bernhard Schölkopf</p><p>Abstract: The discovery of causal relationships between a set of observed variables is a fundamental problem in science. For continuous-valued data linear acyclic causal models with additive noise are often used because these models are well understood and there are well-known methods to ﬁt them to data. In reality, of course, many causal relationships are more or less nonlinear, raising some doubts as to the applicability and usefulness of purely linear methods. In this contribution we show that the basic linear framework can be generalized to nonlinear models. In this extended framework, nonlinearities in the data-generating process are in fact a blessing rather than a curse, as they typically provide information on the underlying causal system and allow more aspects of the true data-generating mechanisms to be identiﬁed. In addition to theoretical results we show simulations and some simple real data experiments illustrating the identiﬁcation power provided by nonlinearities. 1</p><br/>
<h2>reference text</h2><p>[1] J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, 2000.</p>
<p>[2] P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. Springer-Verlag, 1993. (2nd ed. MIT Press 2000).</p>
<p>[3] D. Geiger and D. Heckerman. Learning Gaussian networks. In Proc. of the 10th Annual Conference on Uncertainty in Artiﬁcial Intelligence, pages 235–243, 1994.</p>
<p>[4] D. Heckerman, C. Meek, and G. Cooper. A Bayesian approach to causal discovery. In C. Glymour and G. F. Cooper, editors, Computation, Causation, and Discovery, pages 141–166. MIT Press, 1999.</p>
<p>[5] T. Richardson and P. Spirtes. Automated discovery of linear feedback models. In C. Glymour and G. F. Cooper, editors, Computation, Causation, and Discovery, pages 253–304. MIT Press, 1999.</p>
<p>[6] R. Silva, R. Scheines, C. Glymour, and P. Spirtes. Learning the structure of linear latent variable models. Journal of Machine Learning Research, 7:191–246, 2006.</p>
<p>[7] S. Shimizu, P. O. Hoyer, A. Hyv¨ rinen, and A. J. Kerminen. A linear non-Gaussian acyclic model for a causal discovery. Journal of Machine Learning Research, 7:2003–2030, 2006.</p>
<p>[8] X. Sun, D. Janzing, and B. Sch¨ lkopf. Distinguishing between cause and effect via kernel-based como plexity measures for conditional probability densities. Neurocomputing, pages 1248–1256, 2008.</p>
<p>[9] K. A. Bollen. Structural Equations with Latent Variables. John Wiley & Sons, 1989.</p>
<p>[10] N. Friedman and I. Nachman. Gaussian process networks. In Proc. of the 16th Annual Conference on Uncertainty in Artiﬁcial Intelligence, pages 211–219, 2000.</p>
<p>[11] X. Sun, D. Janzing, and B. Sch¨ lkopf. Causal inference by choosing graphs with most plausible Markov o kernels. In Proceeding of the 9th Int. Symp. Art. Int. and Math., Fort Lauderdale, Florida, 2006.</p>
<p>[12] C. E. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.</p>
<p>[13] A. Gretton, R. Herbrich, A. Smola, O. Bousquet, and B. Sch¨ lkopf. Kernel methods for measuring o independence. Journal of Machine Learning Research, 6:2075–2129, 2005.</p>
<p>[14] GPML code. http://www.gaussianprocess.org/gpml/code.</p>
<p>[15] B. Sch¨ lkopf, A. J. Smola, and R. Williamson. Shrinking the tube: A new support vector regression o algorithm. In Advances in Neural Information Processing 11 (Proc. NIPS*1998). MIT Press, 1999.</p>
<p>[16] G. Wahba. Spline Models for Observational Data. Series in Applied Math., Vol. 59, SIAM, Philadelphia, 1990.</p>
<p>[17] A. Azzalini and A. W. Bowman. A look at some data on the Old Faithful Geyser. Applied Statistics, 39(3):357–365, 1990.</p>
<p>[18] A. Asuncion and D.J. Newman. UCI machine learning repository, 2007.</p>
<p>[19] Climate data collected by the Deutscher Wetter Dienst. http://www.dwd.de/.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
