<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>103 nips-2008-Implicit Mixtures of Restricted Boltzmann Machines</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-103" href="../nips2008/nips-2008-Implicit_Mixtures_of_Restricted_Boltzmann_Machines.html">nips2008-103</a> <a title="nips-2008-103-reference" href="#">nips2008-103-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>103 nips-2008-Implicit Mixtures of Restricted Boltzmann Machines</h1>
<br/><p>Source: <a title="nips-2008-103-pdf" href="http://papers.nips.cc/paper/3536-implicit-mixtures-of-restricted-boltzmann-machines.pdf">pdf</a></p><p>Author: Vinod Nair, Geoffrey E. Hinton</p><p>Abstract: We present a mixture model whose components are Restricted Boltzmann Machines (RBMs). This possibility has not been considered before because computing the partition function of an RBM is intractable, which appears to make learning a mixture of RBMs intractable as well. Surprisingly, when formulated as a third-order Boltzmann machine, such a mixture model can be learned tractably using contrastive divergence. The energy function of the model captures threeway interactions among visible units, hidden units, and a single hidden discrete variable that represents the cluster label. The distinguishing feature of this model is that, unlike other mixture models, the mixing proportions are not explicitly parameterized. Instead, they are deﬁned implicitly via the energy function and depend on all the parameters in the model. We present results for the MNIST and NORB datasets showing that the implicit mixture of RBMs learns clusters that reﬂect the class structure in the data. 1</p><br/>
<h2>reference text</h2><p>[1] Mnist database, http://yann.lecun.com/exdb/mnist/.</p>
<p>[2] C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.</p>
<p>[3] Z. Ghahramani and G. E. Hinton. The em algorithm for mixtures of factor analyzers. Technical Report CRG-TR-96-1, Dept. of Computer Science, University of Toronto, 1996.</p>
<p>[4] X. He, R. S. Zemel, and M. A. Carreira-Perpinan. Multiscale conditional random ﬁelds for image labeling. In CVPR, pages 695–702, 2004.</p>
<p>[5] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1711–1800, 2002.</p>
<p>[6] G. E. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313:504–507, 2006.</p>
<p>[7] Y. LeCun, F. J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In CVPR, Washington, D.C., 2004.</p>
<p>[8] S. Roth and M. J. Black. Fields of experts: A framework for learning image priors. In CVPR, pages 860–867, 2005.</p>
<p>[9] S. Roth and M. J. Black. Steerable random ﬁelds. In ICCV, 2007.</p>
<p>[10] N. Le Roux and Y. Bengio. Representational power of restricted boltzmann machines and deep belief networks. Neural Computation, To appear.</p>
<p>[11] R. Salakhutdinov and I. Murray. On the quantitative analysis of deep belief networks. In ICML, Helsinki, 2008.</p>
<p>[12] I. Sutskever and G. E. Hinton. Deep narrow sigmoid belief networks are universal approximators. Neural Computation, To appear.</p>
<p>[13] M. Welling, M. Rosen-Zvi, and G. E. Hinton. Exponential family harmoniums with an application to information retrieval. In NIPS 17, 2005.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
