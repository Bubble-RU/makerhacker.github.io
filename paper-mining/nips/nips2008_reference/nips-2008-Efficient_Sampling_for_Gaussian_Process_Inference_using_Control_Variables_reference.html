<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-71" href="../nips2008/nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">nips2008-71</a> <a title="nips-2008-71-reference" href="#">nips2008-71-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</h1>
<br/><p>Source: <a title="nips-2008-71-pdf" href="http://papers.nips.cc/paper/3414-efficient-sampling-for-gaussian-process-inference-using-control-variables.pdf">pdf</a></p><p>Author: Neil D. Lawrence, Magnus Rattray, Michalis K. Titsias</p><p>Abstract: Sampling functions in Gaussian process (GP) models is challenging because of the highly correlated posterior distribution. We describe an efﬁcient Markov chain Monte Carlo algorithm for sampling from the posterior process of the GP model. This algorithm uses control variables which are auxiliary function values that provide a low dimensional representation of the function. At each iteration, the algorithm proposes new values for the control variables and generates the function from the conditional GP prior. The control variable input locations are found by minimizing an objective function. We demonstrate the algorithm on regression and classiﬁcation problems and we use it to estimate the parameters of a differential equation model of gene regulation. 1</p><br/>
<h2>reference text</h2><p>[1] U. Alon. An Introduction to Systems Biology: Design Principles of Biological Circuits. Chapman and Hall/CRC, 2006.</p>
<p>[2] M. Barenco, D. Tomescu, D. Brewer, J. Callard, R. Stark, and M. Hubank. Ranked prediction of p53 targets using hidden variable dynamic modeling. Genome Biology, 7(3), 2006.</p>
<p>[3] B. Calderhead, M. Girolami, and N.D. Lawrence. Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes. In Neural Information Processing Systems, 22, 2008.</p>
<p>[4] L. Csato and M. Opper. Sparse online Gaussian processes. Neural Computation, 14:641–668, 2002.</p>
<p>[5] M. N. Gibbs and D. J. C. MacKay. Variational Gaussian process classiﬁers. IEEE Transactions on Neural Networks, 11(6):1458–1464, 2000.</p>
<p>[6] M. Kuss and C. E. Rasmussen. Assessing Approximate Inference for Binary Gaussian Process Classiﬁcation. Journal of Machine Learning Research, 6:1679–1704, 2005.</p>
<p>[7] N. D. Lawerence, M. Seeger, and R. Herbrich. Fast sparse Gaussian process methods: the informative vector machine. In Advances in Neural Information Processing Systems, 13. MIT Press, 2002.</p>
<p>[8] N. D. Lawrence, G. Sanguinetti, and M. Rattray. Modelling transcriptional regulation using Gaussian processes. In Advances in Neural Information Processing Systems, 19. MIT Press, 2007.</p>
<p>[9] T. Minka. Expectation propagation for approximate Bayesian inference. In UAI, pages 362–369, 2001.</p>
<p>[10] R. M. Neal. Monte Carlo implementation of Gaussian process models for Bayesian regression and classiﬁcation. Technical report, Dept. of Statistics, University of Toronto, 1997.</p>
<p>[11] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.</p>
<p>[12] C. P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer-Verlag, 2nd edition, 2004.</p>
<p>[13] S. Rogers, R. Khanin, and M. Girolami. Bayesian model-based inference of transcription factor activity. BMC Bioinformatics, 8(2), 2006.</p>
<p>[14] H. Rue, S. Martino, and N. Chopin. Approximate Bayesian inference for latent Gaussian models using integrated nested Laplace approximations. NTNU Statistics Preprint, 2007.</p>
<p>[15] E. Snelson and Z. Ghahramani. Sparse Gaussian process using pseudo inputs. In Advances in Neural Information Processing Systems, 13. MIT Press, 2006.</p>
<p>[16] C. K. I. Williams and D. Barber. Bayesian classiﬁcation with Gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1342–1351, 1998.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
