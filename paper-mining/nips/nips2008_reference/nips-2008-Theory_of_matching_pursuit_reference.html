<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>238 nips-2008-Theory of matching pursuit</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-238" href="../nips2008/nips-2008-Theory_of_matching_pursuit.html">nips2008-238</a> <a title="nips-2008-238-reference" href="#">nips2008-238-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>238 nips-2008-Theory of matching pursuit</h1>
<br/><p>Source: <a title="nips-2008-238-pdf" href="http://papers.nips.cc/paper/3612-theory-of-matching-pursuit.pdf">pdf</a></p><p>Author: Zakria Hussain, John S. Shawe-taylor</p><p>Abstract: We analyse matching pursuit for kernel principal components analysis (KPCA) by proving that the sparse subspace it produces is a sample compression scheme. We show that this bound is tighter than the KPCA bound of Shawe-Taylor et al [7] and highly predictive of the size of the subspace needed to capture most of the variance in the data. We analyse a second matching pursuit algorithm called kernel matching pursuit (KMP) which does not correspond to a sample compression scheme. However, we give a novel bound that views the choice of subspace of the KMP algorithm as a compression scheme and hence provide a VC bound to upper bound its future loss. Finally we describe how the same bound can be applied to other matching pursuit related algorithms. 1</p><br/>
<h2>reference text</h2><p>[1] M. Anthony. Partitioning points by parallel planes. Discrete Mathematics, 282:17–21, 2004.</p>
<p>[2] S. Floyd and M. Warmuth. Sample compression, learnability, and the Vapnik-Chervonenkis dimension. Machine Learning, 21(3):269–304, 1995.</p>
<p>[3] N. Littlestone and M. K. Warmuth. Relating data compression and learnability. Technical report, University of California Santa Cruz, Santa Cruz, CA, 1986.</p>
<p>[4] S. Mallat and Z. Zhang. Matching pursuit with time-frequency dictionaries. IEEE Transactions on Signal Processing, 41(12):3397–3415, 1993.</p>
<p>[5] B. Sch¨ lkopf and A. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002. o</p>
<p>[6] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, Cambridge, U.K., 2004.</p>
<p>[7] J. Shawe-Taylor, C. K. I. Williams, N. Cristianini, and J. Kandola. On the eigenspectrum of the Gram matrix and the generalization error of kernel-PCA. IEEE Transactions on Information Theory, 51(7):2510–2522, 2005.</p>
<p>[8] A. J. Smola and B. Sch¨ lkopf. Sparse greedy matrix approximation for machine learning. In o Proceedings of 17th International Conference on Machine Learning, pages 911–918. Morgan Kaufmann, San Francisco, CA, 2000.</p>
<p>[9] V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications, 16(2):264–280, 1971.</p>
<p>[10] P. Vincent and Y. Bengio. Kernel matching pursuit. Machine Learning, 48:165–187, 2002.</p>
<p>[11] C. K. I. Williams and M. Seeger. Using the Nystr¨ m method to speed up kernel machines. In o Advances in Neural Information Processing Systems, volume 13, pages 682–688. MIT Press, 2001.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
