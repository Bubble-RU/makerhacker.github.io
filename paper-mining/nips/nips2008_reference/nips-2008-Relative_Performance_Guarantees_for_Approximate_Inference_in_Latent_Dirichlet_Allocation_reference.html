<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-197" href="../nips2008/nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">nips2008-197</a> <a title="nips-2008-197-reference" href="#">nips2008-197-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</h1>
<br/><p>Source: <a title="nips-2008-197-pdf" href="http://papers.nips.cc/paper/3455-relative-performance-guarantees-for-approximate-inference-in-latent-dirichlet-allocation.pdf">pdf</a></p><p>Author: Indraneel Mukherjee, David M. Blei</p><p>Abstract: Hierarchical probabilistic modeling of discrete data has emerged as a powerful tool for text analysis. Posterior inference in such models is intractable, and practitioners rely on approximate posterior inference methods such as variational inference or Gibbs sampling. There has been much research in designing better approximations, but there is yet little theoretical understanding of which of the available techniques are appropriate, and in which data analysis settings. In this paper we provide the beginnings of such understanding. We analyze the improvement that the recently proposed collapsed variational inference (CVB) provides over mean ﬁeld variational inference (VB) in latent Dirichlet allocation. We prove that the difference in the tightness of the bound on the likelihood of a document decreases as O(k − 1) + log m/m, where k is the number of topics in the model and m is the number of words in a document. As a consequence, the advantage of CVB over VB is lost for long documents but increases with the number of topics. We demonstrate empirically that the theory holds, using simulated text data and two text corpora. We provide practical guidelines for choosing an approximation. 1</p><br/>
<h2>reference text</h2><p>[1] S. Deerwester, S. Dumais, T. Landauer, G. Furnas, and R. Harshman. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391–407, 1990. 7</p>
<p>[2] T. Hofmann. Probabilistic latent semantic analysis. In UAI, 1999.</p>
<p>[3] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022, 2003.</p>
<p>[4] W. Buntine and A. Jakulin. Discrete component analysis. In Subspace, Latent Structure and Feature Selection. Springer, 2006.</p>
<p>[5] M. Girolami and A. Kaban. Simplicial mixtures of Markov chains: Distributed modelling of dynamic user proﬁles. In NIPS 16, pages 9–16. MIT Press, 2004.</p>
<p>[6] H. Wallach. Topic modeling: Beyond bag of words. In Proceedings of the 23rd International Conference on Machine Learning, 2006.</p>
<p>[7] M. Rosen-Zvi, T. Grifﬁths, M. Steyvers, and P. Smith. The author-topic model for authors and documents. In Proceedings of the 20th Conference on Uncertainty in Artiﬁcial Intelligence, pages 487–494. AUAI Press, 2004.</p>
<p>[8] A. McCallum, A. Corrada-Emmanuel, and X. Wang. The author-recipient-topic model for topic and role discovery in social networks: Experiments with Enron and academic email. Technical report, University of Massachusetts, Amherst, 2004.</p>
<p>[9] E. Airoldi, D. Blei, S. Fienberg, and E. Xing. Mixed membership stochastic blockmodels. arXiv, May 2007.</p>
<p>[10] D. Zhou, E. Manavoglu, J. Li, C. Giles, and H. Zha. Probabilistic models for discovering e-communities. In WWW Conference, pages 173–182, 2006.</p>
<p>[11] L. Fei-Fei and P. Perona. A Bayesian hierarchical model for learning natural scene categories. IEEE Computer Vision and Pattern Recognition, pages 524–531, 2005.</p>
<p>[12] B. Russell, A. Efros, J. Sivic, W. Freeman, and A. Zisserman. Using multiple segmentations to discover objects and their extent in image collections. In IEEE Conference on Computer Vision and Pattern Recognition, pages 1605–1614, 2006.</p>
<p>[13] S. Rogers, M. Girolami, C. Campbell, and R. Breitling. The latent process decomposition of cDNA microarray data sets. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2(2):143–156, 2005.</p>
<p>[14] X. Wei and B. Croft. LDA-based document models for ad-hoc retrieval. In SIGIR, 2006.</p>
<p>[15] D. Mimno and A. McCallum. Organizing the OCA: Learning faceted subjects from a library of digital books. In Joint Conference on Digital Libraries, 2007.</p>
<p>[16] B. Marlin. Collaborative ﬁltering: A machine learning perspective. Master’s thesis, University of Toronto, 2004.</p>
<p>[17] C. Chemudugunta, P. Smyth, and M. Steyvers. Modeling general and speciﬁc aspects of documents with a probabilistic topic model. In NIPS 19, 2006.</p>
<p>[18] D. Andrzejewski, A. Mulhern, B. Liblit, and X. Zhu. Statistical debugging using latent topic models. In European Conference on Machine Learning, 2007.</p>
<p>[19] T. Grifﬁths and M. Steyvers. Probabilistic topic models. In T. Landauer, D. McNamara, S. Dennis, and W. Kintsch, editors, Latent Semantic Analysis: A Road to Meaning. Laurence Erlbaum, 2006.</p>
<p>[20] T. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In Uncertainty in Artiﬁcial Intelligence (UAI), 2002.</p>
<p>[21] Y. Teh, D. Newman, and M. Welling. A collapsed variational bayesian inference algorithm for latent dirichlet allocation. In NIPS, pages 1353–1360, 2006.</p>
<p>[22] K. Kurihara, M. Welling, and Y. Teh. Collapsed variational Dirichlet process mixture models. 2007.</p>
<p>[23] M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. Machine Learning, 37:183–233, 1999.</p>
<p>[24] K. Watanabe and S. Watanabe. Stochastic complexities of gaussian mixtures in variational bayesian approximation. Journal of Machine Learning Research, 7:625–644, 2006.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
