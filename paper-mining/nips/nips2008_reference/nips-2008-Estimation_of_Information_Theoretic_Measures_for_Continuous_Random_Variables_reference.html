<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>76 nips-2008-Estimation of Information Theoretic Measures for Continuous Random Variables</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-76" href="../nips2008/nips-2008-Estimation_of_Information_Theoretic_Measures_for_Continuous_Random_Variables.html">nips2008-76</a> <a title="nips-2008-76-reference" href="#">nips2008-76-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>76 nips-2008-Estimation of Information Theoretic Measures for Continuous Random Variables</h1>
<br/><p>Source: <a title="nips-2008-76-pdf" href="http://papers.nips.cc/paper/3417-estimation-of-information-theoretic-measures-for-continuous-random-variables.pdf">pdf</a></p><p>Author: Fernando Pérez-Cruz</p><p>Abstract: We analyze the estimation of information theoretic measures of continuous random variables such as: differential entropy, mutual information or KullbackLeibler divergence. The objective of this paper is two-fold. First, we prove that the information theoretic measure estimates using the k-nearest-neighbor density estimation with ﬁxed k converge almost surely, even though the k-nearest-neighbor density estimation with ﬁxed k does not converge to its true measure. Second, we show that the information theoretic measure estimates do not converge for k growing linearly with the number of samples. Nevertheless, these nonconvergent estimates can be used for solving the two-sample problem and assessing if two random variables are independent. We show that the two-sample and independence tests based on these nonconvergent estimates compare favorably with the maximum mean discrepancy test and the Hilbert Schmidt independence criterion. 1</p><br/>
<h2>reference text</h2><p>[1] N. Anderson, P. Hall, and D. Titterington. Two-sample test statistics for measuring discrepancies between two multivariate probability density functions using kernel-based density estimates. Journal of Multivariate Analysis, 50(1):41–54, 7 1994.</p>
<p>[2] F. R. Bach and M. I. Jordan. Kernel independent component analysis. JMLR, 3:1–48, 2004.</p>
<p>[3] K. Balakrishnan and A. P. Basu. The Exponential Distribution: Theory, Methods and Applications. Gordon and Breach Publishers, Amsterdam, Netherlands, 1996.</p>
<p>[4] J. Beirlant, E. Dudewicz, L. Gyorﬁ, and E. van der Meulen. Nonparametric entropy estimation: An overview. nternational Journal of the Mathematical Statistics Sciences, pages 17–39, 1997.</p>
<p>[5] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley, New York, USA, 1991.</p>
<p>[6] G. A. Darbellay and I. Vajda. Estimation of the information by an adaptive partitioning of the observation space. IEEE Trans. Information Theory, 45(4):1315–1321, 5 1999.</p>
<p>[7] R. L. Dobrushin. A simpliﬁed method for experimental estimate of the entropy of a stationary sequence. Theory of Probability and its Applications, (4):428–430, 1958.</p>
<p>[8] F. Fleuret. Fast binary feature selection with conditional mutual information. JMLR, 5:1531–1555, 2004.</p>
<p>[9] A. Gretton, K. M. Borgwardt, M. Rasch, B. Sch¨ lkopf, and A. Smola. A kernel method for the twoo sample-problem. In B. Sch¨ lkopf, J. Platt, and T. Hofmann, editors, Advances in Neural Information o Processing Systems 19, Cambridge, MA, 2007. MIT Press.</p>
<p>[10] A. Gretton, K. Fukumizu, C. H. Teo, L. Song, B. Sch¨ lkopf, and A. Smola. A kernel statistical test of o independence. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, Cambridge, MA, 2008. MIT Press.</p>
<p>[11] G.R. Grimmett and D.R. Stirzaker. Probability and Random Processes. Oxford University Press, Oxford, UK, 3 edition, 2001.</p>
<p>[12] Julian Havil. Gamma: Exploring Euler’s Constant. Princeton University Press, New York, USA, 2003.</p>
<p>[13] S. Mallela I. S. Dhillon and R. Kumar. A divisive information-theoretic feature clustering algorithm for text classiﬁcation. JMLR, 3:1265–1287, 3 2003.</p>
<p>[14] Leonard Kleinrock. Queueing Systems. Volume 1: Theory. Wiley, New York, USA, 1975.</p>
<p>[15] L. F. Kozachenko and N. N. Leonenko. Sample estimate of the entropy of a random vector. Problems Inform. Transmission, 23(2):95–101, 4 1987.</p>
<p>[16] A. Kraskov, H. St¨ gbauer, and P. Grassberger. Estimating mutual information. Physical Review E, o 69(6):1–16, 6 2004.</p>
<p>[17] S. Kullback and R. A. Leibler. On information and sufﬁciency. Ann. Math. Stats., 22(1):79–86, 3 1951.</p>
<p>[18] N. N. Leonenko, L. Pronzato, and V. Savani. A class of renyi information estimators for multidimensional densities. Annals of Statistics, 2007. Submitted.</p>
<p>[19] P. J. Moreno, P. P. Ho, and N. Vasconcelos. A kullback-leibler divergence based kernel for svm classiﬁcation in multimedia applications. Technical Report HPL-2004-4, HP Laboratories, 2004.</p>
<p>[20] X. Nguyen, M. J. Wainwright, and M. I. Jordan. Nonparametric estimation of the likelihood ratio and divergence functionals. In IEEE Int. Symp. Information Theory, Nice, France, 6 2007.</p>
<p>[21] X. Nguyen, M. J. Wainwright, and M. I. Jordan. Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, Cambridge, MA, 2008. MIT Press.</p>
<p>[22] L. Paninski. Estimation of entropy and mutual information. Neural Compt, 15(6):1191–1253, 6 2003.</p>
<p>[23] C. E. Shannon. A mathematical theory of communication. Bell System Tech. J., pages 379–423, 1948.</p>
<p>[24] K. Torkkola. Feature extraction by non parametric mutual information maximization. JMLR, 3:1415– 1438, 2003.</p>
<p>[25] Q. Wang, S. Kulkarni, and S. Verd´ . Divergence estimation of continuous distributions based on datau dependent partitions. IEEE Trans. Information Theory, 51(9):3064–3074, 9 2005.</p>
<p>[26] Q. Wang, S. Kulkarni, and S. Verd´ . A nearest-neighbor approach to estimating divergence between u continuous random vectors. In IEEE Int. Symp. Information Theory, Seattle, USA, 7 2006.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
