<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-2" href="../nips2008/nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions.html">nips2008-2</a> <a title="nips-2008-2-reference" href="#">nips2008-2-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</h1>
<br/><p>Source: <a title="nips-2008-2-pdf" href="http://papers.nips.cc/paper/3422-a-convex-upper-bound-on-the-log-partition-function-for-binary-distributions.pdf">pdf</a></p><p>Author: Laurent E. Ghaoui, Assane Gueye</p><p>Abstract: We consider the problem of bounding from above the log-partition function corresponding to second-order Ising models for binary distributions. We introduce a new bound, the cardinality bound, which can be computed via convex optimization. The corresponding error on the logpartition function is bounded above by twice the distance, in model parameter space, to a class of “standard” Ising models, for which variable inter-dependence is described via a simple mean ﬁeld term. In the context of maximum-likelihood, using the new bound instead of the exact log-partition function, while constraining the distance to the class of standard Ising models, leads not only to a good approximation to the log-partition function, but also to a model that is parsimonious, and easily interpretable. We compare our bound with the log-determinant bound introduced by Wainwright and Jordan (2006), and show that when the l1 -norm of the model parameter vector is small enough, the latter is outperformed by the new bound. 1 1.1</p><br/>
<h2>reference text</h2><p>[1] P. Ravikumar and J. Lafferty. Variational Chernoff bounds for graphical models. In Proc. Advances in Neural Information Processing Systems (NIPS), December 2007.</p>
<p>[2] Martin J. Wainwright and Michael I. Jordan. Log-determinant relaxation for approximate inference in discrete Markov random ﬁelds. IEEE Trans. Signal Processing, 2006.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
