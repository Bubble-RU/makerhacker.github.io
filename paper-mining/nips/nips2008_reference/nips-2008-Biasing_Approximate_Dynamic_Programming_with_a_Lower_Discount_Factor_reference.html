<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-37" href="../nips2008/nips-2008-Biasing_Approximate_Dynamic_Programming_with_a_Lower_Discount_Factor.html">nips2008-37</a> <a title="nips-2008-37-reference" href="#">nips2008-37-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</h1>
<br/><p>Source: <a title="nips-2008-37-pdf" href="http://papers.nips.cc/paper/3523-biasing-approximate-dynamic-programming-with-a-lower-discount-factor.pdf">pdf</a></p><p>Author: Marek Petrik, Bruno Scherrer</p><p>Abstract: Most algorithms for solving Markov decision processes rely on a discount factor, which ensures their convergence. It is generally assumed that using an artiﬁcially low discount factor will improve the convergence rate, while sacriﬁcing the solution quality. We however demonstrate that using an artiﬁcially low discount factor may signiﬁcantly improve the solution quality, when used in approximate dynamic programming. We propose two explanations of this phenomenon. The ﬁrst justiﬁcation follows directly from the standard approximation error bounds: using a lower discount factor may decrease the approximation error bounds. However, we also show that these bounds are loose, thus their decrease does not entirely justify the improved solution quality. We thus propose another justiﬁcation: when the rewards are received only sporadically (as in the case of Tetris), we can derive tighter bounds, which support a signiﬁcant improvement in the solution quality with a decreased discount factor. 1</p><br/>
<h2>reference text</h2><p>[1] Dimitri P. Bertsekas and Sergey Ioffe. Temporal differences-based policy iteration and applications in neuro-dynamic programming. Technical Report LIDS-P-2349, LIDS, 1997.</p>
<p>[2] Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-dynamic programming. Athena Scientiﬁc, 1996.</p>
<p>[3] V.F. Farias and B. Van Roy. Probabilistic and Randomized Methods for Design Under Uncertainty, chapter 6: Tetris: A Study of Randomized Constraint Sampling. Springer-Verlag, 2006.</p>
<p>[4] Sham Machandranath Kakade. A Natural Policy Gradient. In Advances in neural information processing systems, pages 1531–1538. MIT Press, 2001.</p>
<p>[5] Ronald Parr, Lihong Li, Gavin Taylor, Christopher Painter-Wakeﬁeld, and Michael L. Littman. An analysis of linear models, linear value function approximation, and feature selection for reinforcement learning. In International Conference on Machine Learning, 2008.</p>
<p>[6] Warren B. Powell. Approximate Dynamic Programming. Wiley-Interscience, 2007.</p>
<p>[7] Martin L. Puterman. Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons, Inc., 2005.</p>
<p>[8] Richard S. Sutton and Andrew Barto. Reinforcement learning. MIT Press, 1998.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
