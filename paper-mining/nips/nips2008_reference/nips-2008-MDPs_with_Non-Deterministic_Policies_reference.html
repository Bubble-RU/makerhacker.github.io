<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>131 nips-2008-MDPs with Non-Deterministic Policies</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-131" href="../nips2008/nips-2008-MDPs_with_Non-Deterministic_Policies.html">nips2008-131</a> <a title="nips-2008-131-reference" href="#">nips2008-131-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>131 nips-2008-MDPs with Non-Deterministic Policies</h1>
<br/><p>Source: <a title="nips-2008-131-pdf" href="http://papers.nips.cc/paper/3504-mdps-with-non-deterministic-policies.pdf">pdf</a></p><p>Author: Mahdi M. Fard, Joelle Pineau</p><p>Abstract: Markov Decision Processes (MDPs) have been extensively studied and used in the context of planning and decision-making, and many methods exist to ﬁnd the optimal policy for problems modelled as MDPs. Although ﬁnding the optimal policy is sufﬁcient in many domains, in certain applications such as decision support systems where the policy is executed by a human (rather than a machine), ﬁnding all possible near-optimal policies might be useful as it provides more ﬂexibility to the person executing the policy. In this paper we introduce the new concept of non-deterministic MDP policies, and address the question of ﬁnding near-optimal non-deterministic policies. We propose two solutions to this problem, one based on a Mixed Integer Program and the other one based on a search algorithm. We include experimental results obtained from applying this framework to optimize treatment choices in the context of a medical decision support system. 1</p><br/>
<h2>reference text</h2><p>[1] A. Schaefer, M. Bailey, S. Shechter, and M. Roberts. Handbook of Operations Research / Management Science Applications in Health Care, chapter Medical decisions using Markov decision processes. Kluwer Academic Publishers, 2004.</p>
<p>[2] M. Hauskrecht and H. Fraser. Planning treatment of ischemic heart disease with partially observable Markov decision processes. Artiﬁcial Intelligence in Medicine, 18(3):221–244, 2000.</p>
<p>[3] P. Magni, S. Quaglini, M. Marchetti, and G. Barosi. Deciding when to intervene: a Markov decision process approach. International Journal of Medical Informatics, 60(3):237–253, 2000.</p>
<p>[4] D. Ernst, G. B. Stan, J. Concalves, and L. Wehenkel. Clinical data based optimal sti strategies for hiv: a reinforcement learning approach. In Proceedings of Benelearn, 2006.</p>
<p>[5] D.P. Bertsekas. Dynamic Programming and Optimal Control, Vol 2. Athena Scientiﬁc, 1995.</p>
<p>[6] R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998.</p>
<p>[7] M. Kearns and S. Singh. Near-optimal reinforcement learning in poly. time. Machine Learning, 49, 2002.</p>
<p>[8] S. Mannor, D. Simester, P. Sun, and J.N. Tsitsiklis. Bias and variance in value function estimation. In Proceedings of ICML, 2004.</p>
<p>[9] M. Fava, A.J. Rush, and M.H. Trivedi et al. Background and rationale for the sequenced treatment alternatives to relieve depression (STAR*D) study. Psychiatr Clin North Am, 26(2):457–94, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
