<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-250" href="../nips2008/nips-2008-Weighted_Sums_of_Random_Kitchen_Sinks%3A_Replacing_minimization_with_randomization_in_learning.html">nips2008-250</a> <a title="nips-2008-250-reference" href="#">nips2008-250-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</h1>
<br/><p>Source: <a title="nips-2008-250-pdf" href="http://papers.nips.cc/paper/3495-weighted-sums-of-random-kitchen-sinks-replacing-minimization-with-randomization-in-learning.pdf">pdf</a></p><p>Author: Ali Rahimi, Benjamin Recht</p><p>Abstract: Randomized neural networks are immortalized in this AI Koan: In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. “What are you doing?” asked Minsky. “I am training a randomly wired neural net to play tic-tac-toe,” Sussman replied. “Why is the net wired randomly?” asked Minsky. Sussman replied, “I do not want it to have any preconceptions of how to play.” Minsky then shut his eyes. “Why do you close your eyes?” Sussman asked his teacher. “So that the room will be empty,” replied Minsky. At that moment, Sussman was enlightened. We analyze shallow random networks with the help of concentration of measure inequalities. Speciﬁcally, we consider architectures that compute a weighted sum of their inputs after passing them through a bank of arbitrary randomized nonlinearities. We identify conditions under which these networks exhibit good classiﬁcation performance, and bound their test error in terms of the size of the dataset and the number of random nonlinearities. 1</p><br/>
<h2>reference text</h2><p>[1] H. D. Block. The perceptron: a model for brain functioning. Review of modern physics, 34:123–135, January 1962.</p>
<p>[2] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation, 9(7):1545–1588, 1997.</p>
<p>[3] F. Moosmann, B. Triggs, and F. Jurie. Randomized clustering forests for building fast and discriminative visual vocabularies. In Advances in Neural Information Processing Systems (NIPS), 2006.</p>
<p>[4] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems (NIPS), 2007.</p>
<p>[5] W. Maass and H. Markram. On the computational power of circuits of spiking neurons. Journal of Computer and System Sciences, 69:593–616, December 2004.</p>
<p>[6] E. Osuna, R. Freund, and F. Girosi. Training support vector machines: an application to face detection. In Computer Vision and Pattern Recognition (CVPR), 1997.</p>
<p>[7] R. E. Schapire. The boosting approach to machine learning: An overview. In D. D. Denison, M. H. Hansen, C. Holmes, B. Mallick, and B. Yu, editors, Nonlinear Estimation and Classiﬁcation. Springer, 2003.</p>
<p>[8] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. Technical report, Dept. of Statistics, Stanford University, 1998.</p>
<p>[9] L. K. Jones. A simple lemma on greedy approximation in Hilbert space and convergence rates for projection pursuit regression and neural network training. The Annals of Statistics, 20(1):608–613, March 1992.</p>
<p>[10] R. Rifkin, G. Yeo, and T. Poggio. Regularized least squares classiﬁcation. Advances in Learning Theory: Methods, Model and Applications, NATO Science Series III: Computer and Systems Sciences, 190, 2003.</p>
<p>[11] A.R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information Theory, 39:930–945, May 1993.</p>
<p>[12] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research (JMLR), 3:463–482, 2002.</p>
<p>[13] F. Girosi. Approximation error bounds that use VC-bounds. In International Conference on Neural Networks, pages 295–302, 1995.</p>
<p>[14] G. Gnecco and M. Sanguineti. Approximation error bounds via Rademacher’s complexity. Applied Mathematical Sciences, 2(4):153–176, 2008.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
