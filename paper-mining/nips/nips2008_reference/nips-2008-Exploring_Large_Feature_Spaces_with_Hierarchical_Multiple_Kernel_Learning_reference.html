<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-79" href="../nips2008/nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">nips2008-79</a> <a title="nips-2008-79-reference" href="#">nips2008-79-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</h1>
<br/><p>Source: <a title="nips-2008-79-pdf" href="http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning.pdf">pdf</a></p><p>Author: Francis R. Bach</p><p>Abstract: For supervised and unsupervised learning, positive deﬁnite kernels allow to use large and potentially inﬁnite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the ℓ1 -norm or the block ℓ1 -norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efﬁciently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.</p><br/>
<h2>reference text</h2><p>[1] B. Sch¨ lkopf and A. J. Smola. Learning with Kernels. MIT Press, 2002. o</p>
<p>[2] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Camb. U. P., 2004.</p>
<p>[3] P. Zhao and B. Yu. On model selection consistency of Lasso. JMLR, 7:2541–2563, 2006.</p>
<p>[4] F. Bach. Consistency of the group Lasso and multiple kernel learning. JMLR, 9:1179–1225, 2008.</p>
<p>[5] P. Zhao, G. Rocha, and B. Yu. Grouped and hierarchical model selection through composite absolute penalties. Ann. Stat., To appear, 2008.</p>
<p>[6] C. K. I. Williams and M. Seeger. The effect of the input density distribution on kernel-based classiﬁers. In Proc. ICML, 2000.</p>
<p>[7] M. Szafranski, Y. Grandvalet, and A. Rakotomamonjy. Composite kernel learning. In Proc. ICML, 2008.</p>
<p>[8] A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet. Simplemkl. JMLR, 9:2491–2521, 2008.</p>
<p>[9] M. Pontil and C.A. Micchelli. Learning the kernel function via regularization. JMLR, 6:1099–1125, 2005.</p>
<p>[10] F. Bach. Exploring large feature spaces with hierarchical MKL. Technical Report 00319660, HAL, 2008.</p>
<p>[11] H. Lee, A. Battle, R. Raina, and A. Ng. Efﬁcient sparse coding algorithms. In NIPS, 2007.</p>
<p>[12] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge Univ. Press, 2003.</p>
<p>[13] K. Bennett, M. Momma, and J. Embrechts. Mark: A boosting algorithm for heterogeneous kernel models. In Proc. SIGKDD, 2002.</p>
<p>[14] V. Roth. The generalized Lasso. IEEE Trans. on Neural Networks, 15(1), 2004.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
