<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-145" href="../nips2008/nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">nips2008-145</a> <a title="nips-2008-145-reference" href="#">nips2008-145-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</h1>
<br/><p>Source: <a title="nips-2008-145-pdf" href="http://papers.nips.cc/paper/3526-multi-stage-convex-relaxation-for-learning-with-sparse-regularization.pdf">pdf</a></p><p>Author: Tong Zhang</p><p>Abstract: We study learning formulations with non-convex regularizaton that are natural for sparse linear models. There are two approaches to this problem: • Heuristic methods such as gradient descent that only ﬁnd a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L1 -regularization that solves the problem under some conditions. However it often leads to sub-optimal sparsity in reality. This paper tries to remedy the above gap between theory and practice. In particular, we investigate a multi-stage convex relaxation scheme for solving problems with non-convex regularization. Theoretically, we analyze the behavior of a resulting two-stage relaxation scheme for the capped-L1 regularization. Our performance bound shows that the procedure is superior to the standard L1 convex relaxation for learning sparse targets. Experiments conﬁrm the effectiveness of this method on some simulation and real data. 1</p><br/>
<h2>reference text</h2><p>[1] Florentina Bunea, Alexandre Tsybakov, and Marten H. Wegkamp. Sparsity oracle inequalities for the Lasso. Electronic Journal of Statistics, 1:169–194, 2007.</p>
<p>[2] Emmanuel Candes and Terence Tao. The Dantzig selector: statistical estimation when p is much larger than n. Annals of Statistics, 2007.</p>
<p>[3] David L. Donoho, Michael Elad, and Vladimir N. Temlyakov. Stable recovery of sparse overcomplete representations in the presence of noise. IEEE Trans. Info. Theory, 52(1):6–18, 2006. 7  80  q  q=0 q=1 q=2 Lp (p=0.5)  q q  q q  q  q  q  q=0 q=1 q=2 Lp (p=0.5) q  70  q  40  50 60  q  q q  q q  30  q  q  q  50  q  q q  10  q  q  q  q q  q  q  q  q q  q q  q  0.1  q  q  q  q  q  q  q q  q  q  60  q  q  test error  q  20  training error  q  0.2  0.5  1.0  2.0  0.1  0.2  0.5  lambda  1.0  2.0  lambda  q q  q  q  q  q=0 q=1 q=2 Lp (p=0.5)  q  q q  test error  10.0  q q  q q  q  q q q q  5.0  training error  200  q q q q  q q  q  q  q q  q q  q  q q  q q  q  q q  q  q q  q q  q q  q  1.0  2.0  250  q  q=0 q=1 q=2 Lp (p=0.5)  150  q  100  q  50.0  200.0  Figure 4: Performance of multi-stage convex relaxation on the original Boston Housing data. Left: average training squared error versus λ; Right: test squared error versus λ.  q  q q  q  0.5  q  0.1  0.2  0.5  1.0  2.0  5.0  0.1  lambda  q  0.2  q q  q  q  q  q  0.5  1.0  2.0  5.0  lambda  Figure 5: Performance of multi-stage convex relaxation on the modiﬁed Boston Housing data. Left: average training squared error versus λ; Right: test squared error versus λ.</p>
<p>[4] Vladimir Koltchinskii. Sparsity in penalized empirical risk minimization. Annales de l’Institut Henri Poincaré, 2008.</p>
<p>[5] Nicolai Meinshausen. Lasso with relaxation. ETH Research Report, 2005.</p>
<p>[6] R. Tyrrell Rockafellar. Convex analysis. Princeton University Press, Princeton, NJ, 1970.</p>
<p>[7] Alan L. Yuille and Anand Rangarajan. The concave-convex procedure. Neural Computation, 15:915–936, 2003.</p>
<p>[8] Tong Zhang. Some sharp performance bounds for least squares regression with L1 regularization. The Annals of Statistics, 2009. to appear.</p>
<p>[9] Peng Zhao and Bin Yu. On model selection consistency of Lasso. Journal of Machine Learning Research, 7:2541–2567, 2006.</p>
<p>[10] Hui Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical Association, 101:1418–1429, 2006.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
