<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>20 nips-2008-An Extended Level Method for Efficient Multiple Kernel Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-20" href="../nips2008/nips-2008-An_Extended_Level_Method_for_Efficient_Multiple_Kernel_Learning.html">nips2008-20</a> <a title="nips-2008-20-reference" href="#">nips2008-20-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>20 nips-2008-An Extended Level Method for Efficient Multiple Kernel Learning</h1>
<br/><p>Source: <a title="nips-2008-20-pdf" href="http://papers.nips.cc/paper/3520-an-extended-level-method-for-efficient-multiple-kernel-learning.pdf">pdf</a></p><p>Author: Zenglin Xu, Rong Jin, Irwin King, Michael Lyu</p><p>Abstract: We consider the problem of multiple kernel learning (MKL), which can be formulated as a convex-concave problem. In the past, two efﬁcient methods, i.e., Semi-Inﬁnite Linear Programming (SILP) and Subgradient Descent (SD), have been proposed for large-scale multiple kernel learning. Despite their success, both methods have their own shortcomings: (a) the SD method utilizes the gradient of only the current solution, and (b) the SILP method does not regularize the approximate solution obtained from the cutting plane model. In this work, we extend the level method, which was originally designed for optimizing non-smooth objective functions, to convex-concave optimization, and apply it to multiple kernel learning. The extended level method overcomes the drawbacks of SILP and SD by exploiting all the gradients computed in past iterations and by regularizing the solution via a projection to a level set. Empirical study with eight UCI datasets shows that the extended level method can signiﬁcantly improve efﬁciency by saving on average 91.9% of computational time over the SILP method and 70.3% over the SD method. 1</p><br/>
<h2>reference text</h2><p>[1] F. R. Bach. Consistency of the group Lasso and multiple kernel learning. Journal of Machine Learning Research, 9:1179–1225, 2008.  Evolution of the kernel weight values in SD  Evolution of the kernel weight values in SILP  Evolution of the kernel weight values in Level method  0.8  0.8  0.7  0.7  0.7  0.6 0.5 0.4  p values  1 0.9  p values  1 0.9  0.8  p values  1 0.9  0.6 0.5 0.4  0.6 0.5 0.4  0.3  0.3  0.3  0.2  0.2  0.2  0.1  0.1  0  0  20  40  60  80  0  100  0.1 0  100  200  iteration  300  400  0  500  (a) Iono/SD  10  (b) Iono/SILP  Evolution of the kernel weight values in SD  15  20  25  30  35  (c) Iono/Level  Evolution of the kernel weight values in SILP  Evolution of the kernel weight values in Level method  0.9  0.9  0.8  0.8  0.8  0.7  0.7  0.7  0.6 0.5 0.4  p values  1  p values  1  0.9  p values  5  iteration  1  0.6 0.5 0.4  0.6 0.5 0.4  0.3  0.3  0.3  0.2  0.2  0.2  0.1  0.1  0  0  0  5  10  15  20  25  0.1 0  20  40  60  iteration  80  100  120  0  140  0  5  10  iteration  (d) Breast/SD  (e) Breast/SILP  Evolution of the kernel weight values in SD  15  20  iteration  (f) Breast/Level  Evolution of the kernel weight values in SILP  Evolution of the kernel weight values in Level method 1  0.9  0.9  0.9  0.8  0.8  0.8  0.7  0.7  0.7  0.6 0.5 0.4  p values  1  p values  1  p values  0  iteration  0.6 0.5 0.4  0.6 0.5 0.4  0.3  0.3  0.3  0.2  0.2  0.2  0.1  0.1  0  0  0  5  10  15  20  iteration  (g) Pima/SD  25  30  0.1 0  20  40  60  80  100  0  0  iteration  (h) Pima/SILP  5  10  15  20  25  30  iteration  (i) Pima/Level  Figure 2: The evolution curves of the ﬁve largest kernel weights for datasets “Iono”, “Breast” and “Pima” computed by the three MKL algorithms</p>
<p>[2] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the SMO algorithm. In ICML, 2004.</p>
<p>[3] J. Bonnans, J. Gilbert, C. Lemar´ chal, and C. Sagastiz´ bal. Numerical Optimization, Theoretical and e a Practical Aspects. Springer-Verlag, Berlin, 2nd ed., 2006.</p>
<p>[4] N. Cristianini, J. Shawe-Taylor, A. Elisseeff, and J. S. Kandola. On kernel-target alignment. In NIPS 13, pages 367–373, 2001.</p>
<p>[5] G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, and M. I. Jordan. Learning the kernel matrix with semideﬁnite programming. Journal of Machine Learning Research, 5, 2004.</p>
<p>[6] C. Lemar´ chal, A. Nemirovski, and Y. Nesterov. New variants of bundle methods. Mathematical Proe gramming, 69(1), 1995.</p>
<p>[7] C. A. Micchelli and M. Pontil. Learning the kernel function via regularization. Journal of Machine Learning Research, 6, 2005.</p>
<p>[8] A. Nemirovski and D. Yudin. Problem Complexity and Method Efﬁciency in Optimization. John Wiley and Sons Ltd, 1983.</p>
<p>[9] C. S. Ong, A. J. Smola, and R. C. Williamson. Learning the kernel with hyperkernels. Journal of Machine Learning Research, 6, 2005.</p>
<p>[10] A. Rakotomamonjy, F. R. Bach, S. Canu, and Y. Grandvalet. SimpleMKL. Technical Report HAL00218338, INRIA, 2008.</p>
<p>[11] A. Smola, S. V. N. Vishwanathan, and Q. Le. Bundle methods for machine learning. In NIPS 20, pages 1377–1384, 2007.</p>
<p>[12] S. Sonnenburg, G. R¨ tsch, C. Sch¨ fer, and B. Sch¨ lkopf. Large scale multiple kernel learning. Journal of a a o Machine Learning Research, 7, 2006.</p>
<p>[13] J. Ye, J. Chen, and S. Ji. Discriminant kernel and regularization parameter learning via semideﬁnite programming. In ICML, 2007.</p>
<p>[14] A. Zien and C. S. Ong. Multiclass multiple kernel learning. In ICML, 2007.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
