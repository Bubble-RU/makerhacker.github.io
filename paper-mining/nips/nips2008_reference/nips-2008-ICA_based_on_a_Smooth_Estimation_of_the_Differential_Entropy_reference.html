<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-102" href="../nips2008/nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy.html">nips2008-102</a> <a title="nips-2008-102-reference" href="#">nips2008-102-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</h1>
<br/><p>Source: <a title="nips-2008-102-pdf" href="http://papers.nips.cc/paper/3500-ica-based-on-a-smooth-estimation-of-the-differential-entropy.pdf">pdf</a></p><p>Author: Lev Faivishevsky, Jacob Goldberger</p><p>Abstract: In this paper we introduce the MeanNN approach for estimation of main information theoretic measures such as differential entropy, mutual information and divergence. As opposed to other nonparametric approaches the MeanNN results in smooth differentiable functions of the data samples with clear geometrical interpretation. Then we apply the proposed estimators to the ICA problem and obtain a smooth expression for the mutual information that can be analytically optimized by gradient descent methods. The improved performance of the proposed ICA algorithm is demonstrated on several test examples in comparison with state-ofthe-art techniques. 1</p><br/>
<h2>reference text</h2><p>[1] S. Amari, A. Cichoki, and H.H.Yang. A new learning algorithm for blind signal separation. Advances in Neural Information Processing Systems, 8, 1996.</p>
<p>[2] F. Bach and M. Jordan. Kernel independent component analysis. Journal of Machine Learning Research, 3, 2002.</p>
<p>[3] A. J. Bell and T. J. Sejnowski. An information-maximization approach to blind separation and blind deconvolution. Neural Computatiuon, 7, 1995.</p>
<p>[4] J.-F. Cardoso. Multidimensional independent component analysis. Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP’98), 1998.</p>
<p>[5] C.Jutten and J.Herault. Blind separation of sources, part 1: An adaptive algorithm based on neuromimetic architecture. Signal Processing, 1991.</p>
<p>[6] P. Comon. Independent component analysis, a new concept? Signal Processing, 36(3), 1994.</p>
<p>[7] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. Wiley-Interscience, August 1991.</p>
<p>[8] D.T.Pham and P.Garat. Blind separation of mixtures of independent signals through a quasi-maximum likelihood approach. IEEE transactions on Signal Processing 45(7), 1997.</p>
<p>[9] A. Hyvarinen and E.Oja. A fast ﬁxed point algorithm for independent component analysis. Neural computation, 9(7), 1997.</p>
<p>[10] A. Hyvarinen, J. Karhunen, and E. Oja. Independent component analysis. 2001.</p>
<p>[11] L. Kozachenko and N. Leonenko. On statistical estimation of entropy of random vector. Problems Infor. Transmiss., 23 (2), 1987.</p>
<p>[12] A. Kraskov, H. St¨ gbauer, and P. Grassberger. Estimating mutual information. Physical Review E, o 69:066138, 2004.</p>
<p>[13] E. Miller and J. Fisher. Ica using spacing estimates of entropy. Proc. Fourth International Symposium on Independent Component Analysis and Blind Signal Separation, Nara, Japan, Apr. 2003, pp. 1047–1052., 2003.</p>
<p>[14] J. Peltonen and S. Kaski. Discriminative components of data. IEEE Transactions on Neural Networks, 16(1), 2005.</p>
<p>[15] H. Singh, N. Misra, V. Hnizdo, A. Fedorowicz, and Eugene Demchuk. Nearest neighbor estimates of entropy. American Journal of Mathematical and Management Sciences, 2003.</p>
<p>[16] H. St¨ gbauer, A. Kraskov, S. Astakhov, and P. Grassberger. Least-dependent-component analysis based o on mutual information. Phys. Rev. E, 70(6):066123, Dec 2004.</p>
<p>[17] O. Vasicek. A test for normality based on sample entropy. J. Royal Stat. Soc. B, 38 (1):54–59, 1976.</p>
<p>[18] J. D. Victor. Binless strategies for estimation of information from neural data. Physical Review, 2002.</p>
<p>[19] Q. Wang, S. R. Kulkarni, and S. Verdu. A nearest-neighbor approach to estimating divergence between continuous random vectors. IEEE Int. Symp. Information Theory, Seattle, WA, 2006.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
