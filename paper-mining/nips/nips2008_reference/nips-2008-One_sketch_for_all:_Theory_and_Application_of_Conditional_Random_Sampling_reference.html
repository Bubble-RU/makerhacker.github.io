<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-167" href="../nips2008/nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling.html">nips2008-167</a> <a title="nips-2008-167-reference" href="#">nips2008-167-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</h1>
<br/><p>Source: <a title="nips-2008-167-pdf" href="http://papers.nips.cc/paper/3572-one-sketch-for-all-theory-and-application-of-conditional-random-sampling.pdf">pdf</a></p><p>Author: Ping Li, Kenneth W. Church, Trevor J. Hastie</p><p>Abstract: Conditional Random Sampling (CRS) was originally proposed for efﬁciently computing pairwise (l2 , l1 ) distances, in static, large-scale, and sparse data. This study modiﬁes the original CRS and extends CRS to handle dynamic or streaming data, which much better reﬂect the real-world situation than assuming static data. Compared with many other sketching algorithms for dimension reductions such as stable random projections, CRS exhibits a signiﬁcant advantage in that it is “one-sketch-for-all.” In particular, we demonstrate the effectiveness of CRS in efﬁciently computing the Hamming norm, the Hamming distance, the lp distance, and the χ2 distance. A generic estimator and an approximate variance formula are also provided, for approximating any type of distances. We recommend CRS as a promising tool for building highly scalable systems, in machine learning, data mining, recommender systems, and information retrieval. 1</p><br/>
<h2>reference text</h2><p>[1] Charu C. Aggarwal, Jiawei Han, Jianyong Wang, and Philip S. Yu. On demand classiﬁcation of data streams. In KDD, 503–508, 2004.</p>
<p>[2] L´ on Bottou, Olivier Chapelle, Dennis DeCoste, and Jason Weston, editors. Large-Scale Kernel Machines. The MIT Press, 2007. e</p>
<p>[3] Olivier Chapelle, Patrick Haffner, and Vladimir N. Vapnik. Support vector machines for histogram-based image classiﬁcation. IEEE Trans. Neural Networks, 10(5):1055–1064, 1999.</p>
<p>[4] Graham Cormode, Mayur Datar, Piotr Indyk, and S. Muthukrishnan. Comparing data streams using hamming norms (how to zero in). IEEE Transactions on Knowledge and Data Engineering, 15(3):529–540, 2003.</p>
<p>[5] Carlotta Domeniconi and Dimitrios Gunopulos. Incremental support vector machine construction. In ICDM, pages 589–592, 2001.</p>
<p>[6] Sudipto Guha, Piotr Indyk, and Andrew McGregor. Sketching infomration divergence. In COLT, pages 424–438, 2007.</p>
<p>[7] M. Hein and O. Bousquet. Hilbertian metrics and positive deﬁnite kernels on probability measures. In AISTATS, pages 136–143, 2005.</p>
<p>[8] Piotr Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream computation. J. of ACM, 53(3):307–323, 2006.</p>
<p>[9] Yugang Jiang, Chongwah Ngo, and Jun Yang. Towards optimal bag-of-features for object categorization and semantic video retrieval. In CIVR, pages 494–501, 2007.</p>
<p>[10] Ping Li. Estimators and tail bounds for dimension reduction in lα (0 < α ≤ 2) using stable random projections. In SODA, 2008.</p>
<p>[11] Ping Li. Compressed Counting. In SODA, 2009.</p>
<p>[12] Ping Li and Kenneth W. Church. A sketch algorithm for estimating two-way and multi-way Associations. Computational Linguistics, 33(3):305-354, 2007. Preliminary results appeared in HLT/EMNLP, 2005.</p>
<p>[13] Ping Li, Kenneth W. Church, and Trevor J. Hastie. Conditional random sampling: A sketch-based sampling technique for sparse data. In NIPS, pages 873–880, 2007.</p>
<p>[14] Ping Li. Computationally efﬁcient estimators for dimension reductions using stable random projections. In ICDM, 2008.</p>
<p>[15] S. Muthukrishnan. Data streams: Algorithms and applications. Found. and Trends in Theoretical Computer Science, 1:117–236, 2 2005.</p>
<p>[16] John C. Platt. Using analytic QP and sparseness to speed training of support vector machines. In NIPS, pages 557–563, 1998.</p>
<p>[17] Bernhard Sch¨ lkopf and Alexander J. Smola. Learning with Kernels. The MIT Press, 2002. o</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
