<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-21" href="../nips2008/nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">nips2008-21</a> <a title="nips-2008-21-reference" href="#">nips2008-21-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</h1>
<br/><p>Source: <a title="nips-2008-21-pdf" href="http://papers.nips.cc/paper/3431-an-homotopy-algorithm-for-the-lasso-with-online-observations.pdf">pdf</a></p><p>Author: Pierre Garrigues, Laurent E. Ghaoui</p><p>Abstract: It has been shown that the problem of 1 -penalized least-square regression commonly referred to as the Lasso or Basis Pursuit DeNoising leads to solutions that are sparse and therefore achieves model selection. We propose in this paper RecLasso, an algorithm to solve the Lasso with online (sequential) observations. We introduce an optimization problem that allows us to compute an homotopy from the current solution to the solution after observing a new data point. We compare our method to Lars and Coordinate Descent, and present an application to compressive sensing with sequential observations. Our approach can easily be extended to compute an homotopy from the current solution to the solution that corresponds to removing a data point, which leads to an efﬁcient algorithm for leave-one-out cross-validation. We also propose an algorithm to automatically update the regularization parameter after observing a new data point. 1</p><br/>
<h2>reference text</h2><p>[1] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B, 58(1):267–288, 1996.</p>
<p>[2] S. Chen, D. Donoho, and M. Saunders. Atomic decomposition by basis pursuit. SIAM Review, 43(1):129– 159, 2001.</p>
<p>[3] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge Univ. Press, 2004.</p>
<p>[4] S-J. Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinevsky. An interior-point method for large-scale l1-regularized least squares. IEEE Journal of Selected Topics in Signal Processing, 1(4):606–617, 2007.</p>
<p>[5] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32(2):407–499, 2004.</p>
<p>[6] M.R. Osborne, B. Presnell, and B.A. Turlach. A new approach to variable selection in least squares problems. IMA Journal of Numerical Analysis, 20:389–404, 2000.</p>
<p>[7] D.M. Malioutov, M. Cetin, and A.S. Willsky. Homotopy continuation for sparse signal representation. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Philadelphia, PA, March 2005.</p>
<p>[8] I. Drori and D.L. Donoho. Solution of 1 minimization problems by lars/homotopy methods. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Toulouse, France, May 2006.</p>
<p>[9] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Communications on Pure and Applied Mathematics, 57:1413–1541, 2004.</p>
<p>[10] C.J. Rozell, D.H. Johnson, R.G. Baraniuk, and B.A. Olshausen. Locally competitive algorithms for sparse approximation. In Proceedings of the International Conference on Image Processing (ICIP), San Antonio, TX, September 2007.</p>
<p>[11] J. Friedman, T. Hastie, H. Hoeﬂing, and R. Tibshirani. Pathwise coordinate optimization. The Annals of Applied Statistics, 1(2):302–332, 2007.</p>
<p>[12] H. Lee, A. Battle, R. Raina, and A.Y. Ng. Efﬁcient sparse coding algorithms. In Proceedings of the Neural Information Processing Systems (NIPS), 2007.</p>
<p>[13] M. Figueiredo and R. Nowak. A bound optimization approach to wavelet-based image deconvolution. In Proceedings of the International Conference on Image Processing (ICIP), Genova, Italy, September 2005.</p>
<p>[14] M. Figueiredo, R. Nowak, and S. Wright. Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems. IEEE Journal of Selected Topics in Signal Processing, 1(4):586–597, 2007.</p>
<p>[15] M Osborne. An effective method for computing regression quantiles. IMA Journal of Numerical Analysis, Jan 1992.</p>
<p>[16] E. Cand` s. Compressive sampling. Proceedings of the International Congress of Mathematicians, 2006. e</p>
<p>[17] D.L. Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289–1306, 2006.</p>
<p>[18] S. Sra and J.A. Tropp. Row-action methods for compressed sensing. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Toulouse, France, May 2006.</p>
<p>[19] D. Malioutov, S. Sanghavi, and A. Willsky. Compressed sensing with sequential observations. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Las Vegas, NV, March 2008.</p>
<p>[20] Y. Tsaig and D.L. Donoho. Extensions of compressed sensing. Signal Processing, 86(3):549–571, 2006.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
