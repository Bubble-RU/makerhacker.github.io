<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>121 nips-2008-Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-121" href="../nips2008/nips-2008-Learning_to_Use_Working_Memory_in_Partially_Observable_Environments_through_Dopaminergic_Reinforcement.html">nips2008-121</a> <a title="nips-2008-121-reference" href="#">nips2008-121-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>121 nips-2008-Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement</h1>
<br/><p>Source: <a title="nips-2008-121-pdf" href="http://papers.nips.cc/paper/3508-learning-to-use-working-memory-in-partially-observable-environments-through-dopaminergic-reinforcement.pdf">pdf</a></p><p>Author: Michael T. Todd, Yael Niv, Jonathan D. Cohen</p><p>Abstract: Working memory is a central topic of cognitive neuroscience because it is critical for solving real-world problems in which information from multiple temporally distant sources must be combined to generate appropriate behavior. However, an often neglected fact is that learning to use working memory effectively is itself a difficult problem. The Gating framework [14] is a collection of psychological models that show how dopamine can train the basal ganglia and prefrontal cortex to form useful working memory representations in certain types of problems. We unite Gating with machine learning theory concerning the general problem of memory-based optimal control [5-6]. We present a normative model that learns, by online temporal difference methods, to use working memory to maximize discounted future reward in partially observable settings. The model successfully solves a benchmark working memory problem, and exhibits limitations similar to those observed in humans. Our purpose is to introduce a concise, normative definition of high level cognitive concepts such as working memory and cognitive control in terms of maximizing discounted future rewards. 1 I n t ro d u c t i o n Working memory is loosely defined in cognitive neuroscience as information that is (1) internally maintained on a temporary or short term basis, and (2) required for tasks in which immediate observations cannot be mapped to correct actions. It is widely assumed that prefrontal cortex (PFC) plays a role in maintaining and updating working memory. However, relatively little is known about how PFC develops useful working memory representations for a new task. Furthermore, current work focuses on describing the structure and limitations of working memory, but does not ask why, or in what general class of tasks, is it necessary. Borrowing from the theory of optimal control in partially observable Markov decision problems (POMDPs), we frame the psychological concept of working memory as an internal state representation, developed and employed to maximize future reward in partially observable environments. We combine computational insights from POMDPs and neurobiologically plausible models from cognitive neuroscience to suggest a simple reinforcement learning (RL) model of working memory function that can be implemented through dopaminergic training of the basal ganglia and PFC. The Gating framework is a series of cognitive neuroscience models developed to explain how dopaminergic RL signals can shape useful working memory representations [1-4]. Computationally this framework models working memory as a collection of past observations, each of which can occasionally be replaced with the current observation, and addresses the problem of learning when to update each memory element versus maintaining it. In the original Gating model [1-2] the PFC contained a unitary working memory representation that was updated whenever a phasic dopamine (DA) burst occurred (e.g., due to unexpected reward or novelty). That model was the first to connect working memory and RL via the temporal difference (TD) model of DA firing [7-8], and thus to suggest how working memory might serve a normative purpose. However, that model had limited computational flexibility due to the unitary nature of the working memory (i.e., a singleobservation memory controlled by a scalar DA signal). More recent work [3-4] has partially repositioned the Gating framework within the Actor/Critic model of mesostriatal RL [9-10], positing memory updating as but another cortical action controlled by the dorsal striatal</p><br/>
<h2>reference text</h2><p>[1] Braver, T. S., & Cohen, J. D. (1999). Dopamine, cognitive control, and schizophrenia: The gating model. In J. A. Reggia, E. Ruppin, & D. Glanzman (Eds.), Progress in Brain Research (pp. 327-349). Amsterdam, North-Holland: Elsevier Science.</p>
<p>[2] Braver, T. S., & Cohen, J. D. (2000). On the Control of Control: The Role of Dopamine in Regulating Prefrontal Function and Working Memory. In S. Monsell, & J. S. Driver (Eds.), Control of Cognitive Processes: Attention and Performance XVIII (pp. 713-737). Cambridge, MA: MIT Press.</p>
<p>[3] Rougier, A., Noelle, D., Braver, T., Cohen, J., & O'Reilly, R. (2005). Prefrontal Cortex and Flexible Cognitive Control: Rules Without Symbols. Proceedings of the National Academy of Sciences , 102 (20), 7338-7343.</p>
<p>[4] O'Reilly, R. C., & Frank, M. J. (2006). Making Working Memory Work: A Computational Model of Learning in the Prefrontal Cortex and Basal Ganglia. Neural Computation , 18, 283-328.</p>
<p>[5] McCallum, A. (1995). Instance-Based Utile Distinctions for Reinforcement Learning with Hidden State. International Conference on Machine Learning, (pp. 387-395).</p>
<p>[6] Peshkin, L., Meuleau, N., & Kaelbling, L. (1999). Learning Policies with External Memory. Sixteenth International Conference on Machine Learning, (pp. 307-314).</p>
<p>[7] Montague, P. R., Dayan, P., & Sejnowski, T. J. (1996). A Framework for Mesencephalic Dopamine Systems Based on Predictive Hebbian Learning. The Journal of Neuroscience , 16 (5), 1936-1947.</p>
<p>[8] Schultz, W., Dayan, P., & Montague, P. R. (1997). A Neural Substrate of Prediction and Reward. Science , 275, 1593-1599.</p>
<p>[9] Houk, J., Adams, J., & Barto, A. (1995). A Model of how the Basal Ganglia Generate and use Neural Signals that Predict Reinforcement. In J. Houk, J. Davis, & D. Beiser, Models of Information Processing in the Basal Ganglia. MIT Press.</p>
<p>[10] Joel, D., Niv, Y., & Ruppin, E. (2002). Actor-critic Models of the Basal Ganglia: New Anatomical and Computational Perspectives. Neural Networks , 15, 535-547.</p>
<p>[11] Cleeremans, A., & McClelland, J. (1991). Learning the Structure of Event Sequences. Journal of Experimental Psychology: General , 120 (3), 235-253.</p>
<p>[12] Cowan, N. (2000). The Magical Number 4 in Short-term Memory: A Reconsideration of Mental Storage Capacity. Behavioral and Brain Sciences , 24, 87-114.</p>
<p>[13] Dayan, P., & Abbott, L. (2001). Theoretical Neuroscience. Cambridge, MA: MIT Press.</p>
<p>[14] Williams, R. (1992). Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning. Machine Learning , 8, 229-256.</p>
<p>[15] Singh, S., Jaakkola, T., & Jordan, M. I. (1994). Learning Without State-Estimation in Partially Observable Markovian Decision Processes. Eleventh International Conference on Machine Learning, (pp. 284-292).</p>
<p>[16] Loch, J., & Singh, S. (1998). Using Eligibility Traces to Find the Best Memoryless Policy in Partially Observable Markov Decision Processes. Fifteenth International Conference on Machine Learning, (pp. 323331).</p>
<p>[17] Sutton, R., & Barto, A. (1998). Reinforcement Learning: An Introduction. Cambridge, MA: The MIT Press.</p>
<p>[18] Daw, N., Courville, A., & Touretzky, D. (2006). Representation and Timing in Theories of the Dopamine System. Neural Computation , 18, 1637-1677.</p>
<p>[19] Samejima, K., & Doya, K. (2007). Multiple Representations of Belief States and Action Values in Corticobasal Ganglia Loops. Annals of the New York Academy of Sciences , 213-228.</p>
<p>[20] Yoshida, W., & Ishii, S. (2006). Resolution of Uncertainty in Prefrontal Cortex. Neuron , 50, 781-789.</p>
<p>[21] Dayan, P. (2007). Bilinearity, Rules, and Prefrontal Cortex. Frontiers in Computational Neuroscience , 1, 1-14.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
