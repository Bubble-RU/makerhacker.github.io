<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>245 nips-2008-Unlabeled data: Now it helps, now it doesn't</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-245" href="../nips2008/nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">nips2008-245</a> <a title="nips-2008-245-reference" href="#">nips2008-245-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>245 nips-2008-Unlabeled data: Now it helps, now it doesn't</h1>
<br/><p>Source: <a title="nips-2008-245-pdf" href="http://papers.nips.cc/paper/3551-unlabeled-data-now-it-helps-now-it-doesnt.pdf">pdf</a></p><p>Author: Aarti Singh, Robert Nowak, Xiaojin Zhu</p><p>Abstract: Empirical evidence shows that in favorable situations semi-supervised learning (SSL) algorithms can capitalize on the abundance of unlabeled training data to improve the performance of a learning task, in the sense that fewer labeled training data are needed to achieve a target error bound. However, in other situations unlabeled data do not seem to help. Recent attempts at theoretically characterizing SSL gains only provide a partial and sometimes apparently conﬂicting explanations of whether, and to what extent, unlabeled data can help. In this paper, we attempt to bridge the gap between the practice and theory of semi-supervised learning. We develop a ﬁnite sample analysis that characterizes the value of unlabeled data and quantiﬁes the performance improvement of SSL compared to supervised learning. We show that there are large classes of problems for which SSL can signiﬁcantly outperform supervised learning, in ﬁnite sample regimes and sometimes also in terms of error convergence rates.</p><br/>
<h2>reference text</h2><p>[1] Balcan, M.F., Blum, A.: A PAC-style model for learning from labeled and unlabeled data. In: 18th Annual Conference on Learning Theory, COLT. (2005)</p>
<p>[2] K¨ ari¨ inen, M.: Generalization error bounds using unlabeled data. In: 18th Annual Conference on a¨ a Learning Theory, COLT. (2005)</p>
<p>[3] Rigollet, P.: Generalization error bounds in semi-supervised classiﬁcation under the cluster assumption. Journal of Machine Learning Research 8 (2007) 1369–1392</p>
<p>[4] Lafferty, J., Wasserman, L.: Statistical analysis of semi-supervised regression. In: Advances in Neural Information Processing Systems 21, NIPS. (2007) 801–808</p>
<p>[5] Ben-David, S., Lu, T., Pal, D.: Does unlabeled data provably help? worst-case analysis of the sample complexity of semi-supervised learning. In: 21st Annual Conference on Learning Theory, COLT. (2008)</p>
<p>[6] Niyogi, P.: Manifold regularization and semi-supervised learning: Some theoretical analyses. Technical Report TR-2008-01, Computer Science Department, University of Chicago. URL http://people.cs.uchicago.edu/∼niyogi/papersps/ssminimax2.pdf (2008)</p>
<p>[7] Seeger, M.: Learning with labeled and unlabeled data. Technical report, Institute for ANC, Edinburgh, UK. URL http://www.dai.ed.ac.uk/∼seeger/papers.html (2000)</p>
<p>[8] Castelli, V., Cover, T.M.: On the exponential value of labeled samples. Pattern Recognition Letters 16(1) (1995) 105–111</p>
<p>[9] Castelli, V., Cover, T.M.: The relative value of labeled and unlabeled samples in pattern recognition. IEEE Transactions on Information Theory 42(6) (1996) 2102–2117</p>
<p>[10] Bickel, P.J., Li, B.: Local polynomial regression on unknown manifolds. In: IMS Lecture NotesMonograph Series, Complex Datasets and Inverse Problems: Tomography, Networks and Beyond. Volume 54. (2007) 177–186</p>
<p>[11] Korostelev, A.P., Tsybakov, A.B.: Minimax Theory of Image Reconstruction. Springer, NY (1993)</p>
<p>[12] Castro, R., Willett, R., Nowak, R.: Faster rates in regression via active learning. Technical Report ECE-05-03, ECE Department, University of Wisconsin - Madison. URL http://www.ece.wisc.edu/∼nowak/ECE-05-03.pdf (2005)</p>
<p>[13] Singh, A., Nowak, R., Zhu, X.: Finite sample analysis of semi-supervised learning. Technical Report ECE-08-03, ECE Department, University of Wisconsin - Madison. URL http://www.ece.wisc.edu/∼nowak/SSL TR.pdf (2008)</p>
<p>[14] Korostelev, A., Nussbaum, M.: The asymptotic minimax constant for sup-norm loss in nonparametric density estimation. Bernoulli 5(6) (1999) 1099–1118</p>
<p>[15] Chapelle, O., Zien, A.: Semi-supervised classiﬁcation by low density separation. In: Tenth International Workshop on Artiﬁcial Intelligence and Statistics. (2005) 57–64</p>
<p>[16] Tsybakov, A.B.: Introduction a l’estimation non-parametrique. Springer, Berlin Heidelberg (2004)</p>
<p>[17] Stone, C.J.: Optimal rates of convergence for nonparametric estimators. The Annals of Statistics 8(6) (1980) 1348–1360  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
