<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-164" href="../nips2008/nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">nips2008-164</a> <a title="nips-2008-164-reference" href="#">nips2008-164-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</h1>
<br/><p>Source: <a title="nips-2008-164-pdf" href="http://papers.nips.cc/paper/3457-on-the-generalization-ability-of-online-strongly-convex-programming-algorithms.pdf">pdf</a></p><p>Author: Sham M. Kakade, Ambuj Tewari</p><p>Abstract: This paper examines the generalization properties of online convex programming algorithms when the loss function is Lipschitz and strongly convex. Our main result is a sharp bound, that holds with high probability, on the excess risk of the output of an online algorithm in terms of the average regret. This allows one to use recent algorithms with logarithmic cumulative regret guarantees to achieve fast convergence rates for the excess risk with high probability. As a corollary, we characterize the convergence rate of P EGASOS (with high probability), a recently proposed method for solving the SVM optimization problem. 1</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
