<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-90" href="../nips2008/nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">nips2008-90</a> <a title="nips-2008-90-reference" href="#">nips2008-90-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</h1>
<br/><p>Source: <a title="nips-2008-90-pdf" href="http://papers.nips.cc/paper/3494-gaussian-process-factor-analysis-for-low-dimensional-single-trial-analysis-of-neural-population-activity.pdf">pdf</a></p><p>Author: Byron M. Yu, John P. Cunningham, Gopal Santhanam, Stephen I. Ryu, Krishna V. Shenoy, Maneesh Sahani</p><p>Abstract: We consider the problem of extracting smooth, low-dimensional neural trajectories that summarize the activity recorded simultaneously from tens to hundreds of neurons on individual experimental trials. Current methods for extracting neural trajectories involve a two-stage process: the data are ﬁrst “denoised” by smoothing over time, then a static dimensionality reduction technique is applied. We ﬁrst describe extensions of the two-stage methods that allow the degree of smoothing to be chosen in a principled way, and account for spiking variability that may vary both across neurons and across time. We then present a novel method for extracting neural trajectories, Gaussian-process factor analysis (GPFA), which uniﬁes the smoothing and dimensionality reduction operations in a common probabilistic framework. We applied these methods to the activity of 61 neurons recorded simultaneously in macaque premotor and motor cortices during reach planning and execution. By adopting a goodness-of-ﬁt metric that measures how well the activity of each neuron can be predicted by all other recorded neurons, we found that GPFA provided a better characterization of the population activity than the two-stage methods. 1</p><br/>
<h2>reference text</h2><p>[1] K. L. Briggman, H. D. I. Abarbanel, and W. B. Kristan Jr. Science, 307(5711):896–901, Feb. 2005.</p>
<p>[2] K. L. Briggman, H. D. I. Abarbanel, and W. B. Kristan Jr. Curr Opin Neurobiol, 16(2):135–144, 2006.</p>
<p>[3] B. M. Yu, A. Afshar, G. Santhanam, S. I. Ryu, K. V. Shenoy, and M. Sahani. In Y. Weiss, B. Scholkopf, and J. Platt, eds., Adv Neural Info Processing Sys 18, pp. 1545–1552. MIT Press, 2006.</p>
<p>[4] M. M. Churchland, B. M. Yu, M. Sahani, and K. V. Shenoy. Curr Opin Neurobiol, 17(5):609–618, 2007.</p>
<p>[5] A. C. Smith and E. N. Brown. Neural Comput, 15(5):965–991, 2003.</p>
<p>[6] M. Stopfer, V. Jayaraman, and G. Laurent. Neuron, 39:991–1004, Sept. 2003.</p>
<p>[7] S. L. Brown, J. Joseph, and M. Stopfer. Nat Neurosci, 8(11):1568–1576, Nov. 2005.</p>
<p>[8] R. Levi, R. Varona, Y. I. Arshavsky, M. I. Rabinovich, and A. I. Selverston. J Neurosci, 25(42):9807– 9815, Oct. 2005.</p>
<p>[9] O. Mazor and G. Laurent. Neuron, 48:661–673, Nov. 2005.</p>
<p>[10] B. M. Broome, V. Jayaraman, and G. Laurent. Neuron, 51:467–482, Aug. 2006.</p>
<p>[11] M. A. L. Nicolelis, L. A. Baccala, R. C. S. Lin, and J. K. Chapin. Science, 268(5215):1353–1358, 1995.</p>
<p>[12] I. DiMatteo, C. R. Genovese, and R. E. Kass. Biometrika, 88(4):1055–1071, 2001.</p>
<p>[13] J. P. Cunningham, B. M. Yu, K. V. Shenoy, and M. Sahani. In J. Platt, D. Koller, Y. Singer, and S. Roweis, eds., Adv Neural Info Processing Sys 20. MIT Press, 2008.</p>
<p>[14] S. T. Roweis and L. K. Saul. Science, 290(5500):2323–2326, Dec. 2000.</p>
<p>[15] S. Roweis and Z. Ghahramani. Neural Comput, 11(2):305–345, 1999.</p>
<p>[16] N. A. Thacker and P. A. Bromiley. The effects of a square root transform on a Poisson distributed quantity. Technical Report 2001-010, University of Manchester, 2001.</p>
<p>[17] D. J. Tolhurst, J. A. Movshon, and A. F. Dean. Vision Res, 23(8):775–785, 1983.</p>
<p>[18] C. E. Rasmussen and C. K. I. Williams. Gaussian processes for machine learning. MIT Press, 2006.</p>
<p>[19] Y. W. Teh, M. Seeger, and M. I. Jordan. In R. G. Cowell and Z. Ghahramani, eds., Proceedings of the Tenth International Workshop on Artiﬁcial Intelligence and Statistics (AISTATS). Society for Artiﬁcial Intelligence and Statistics, 2005.</p>
<p>[20] N. D. Lawrence and A. J. Moore. In Z. Ghahramani, ed., Proceedings of the 24th Annual International Conference on Machine Learning (ICML 2007), pp. 481–488. Omnipress, 2007.</p>
<p>[21] R. E. Turner and M. Sahani. Neural Comput, 19(4):1022–1038, 2007.</p>
<p>[22] M. M. Churchland, B. M. Yu, S. I. Ryu, G. Santhanam, and K. V. Shenoy. J Neurosci, 26(14):3697–3712, Apr. 2006.</p>
<p>[23] P. Sollich and C. K. I. Williams. In L. K. Saul, Y. Weiss, and L. Bottou, eds., Advances in Neural Information Processing Systems 17, pp. 1313–1320. MIT Press, 2005.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
