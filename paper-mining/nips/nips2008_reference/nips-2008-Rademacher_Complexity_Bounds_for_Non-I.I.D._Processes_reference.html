<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>189 nips-2008-Rademacher Complexity Bounds for Non-I.I.D. Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-189" href="../nips2008/nips-2008-Rademacher_Complexity_Bounds_for_Non-I.I.D._Processes.html">nips2008-189</a> <a title="nips-2008-189-reference" href="#">nips2008-189-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>189 nips-2008-Rademacher Complexity Bounds for Non-I.I.D. Processes</h1>
<br/><p>Source: <a title="nips-2008-189-pdf" href="http://papers.nips.cc/paper/3489-rademacher-complexity-bounds-for-non-iid-processes.pdf">pdf</a></p><p>Author: Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents the ﬁrst Rademacher complexity-based error bounds for noni.i.d. settings, a generalization of similar existing bounds derived for the i.i.d. case. Our bounds hold in the scenario of dependent samples generated by a stationary β-mixing process, which is commonly adopted in many previous studies of noni.i.d. settings. They beneﬁt from the crucial advantages of Rademacher complexity over other measures of the complexity of hypothesis classes. In particular, they are data-dependent and measure the complexity of a class of hypotheses based on the training sample. The empirical Rademacher complexity can be estimated from such ﬁnite samples and lead to tighter generalization bounds. We also present the ﬁrst margin bounds for kernel-based classiﬁcation in this non-i.i.d. setting and brieﬂy study their convergence.</p><br/>
<h2>reference text</h2><p>[1] M. Anthony and P. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, Cambridge, UK, 1999.</p>
<p>[2] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:2002, 2002.</p>
<p>[3] A. Irle. On the consistency in nonparametric estimation under mixing assumptions. Journal of Multivariate Analysis, 60:123–147, 1997.</p>
<p>[4] V. Koltchinskii and D. Panchenko. Rademacher processes and bounding the risk of function learning. In High Dimensional Probability II, pages 443–459. preprint, 2000.</p>
<p>[5] V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization error of combined classiﬁers. Annals of Statistics, 30, 2002.</p>
<p>[6] M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. Springer, 1991.</p>
<p>[7] A. Lozano, S. Kulkarni, and R. Schapire. Convergence and consistency of regularized boosting algorithms with stationary β-mixing observations. Advances in Neural Information Processing Systems, 18, 2006.</p>
<p>[8] C. McDiarmid. On the method of bounded differences. In Surveys in Combinatorics, pages 148–188. Cambridge University Press, 1989.</p>
<p>[9] R. Meir. Nonparametric time series prediction through adaptive model selection. Machine Learning, 39(1):5–34, 2000.</p>
<p>[10] M. Mohri and A. Rostamizadeh. Stability bounds for non-iid processes. Advances in Neural Information Processing Systems, 2007.</p>
<p>[11] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004.</p>
<p>[12] I. Steinwart, D. Hush, and C. Scovel. Learning from dependent observations. Technical Report LA-UR06-3507, Los Alamos National Laboratory, 2007.</p>
<p>[13] L. G. Valiant. A theory of the learnable. ACM Press New York, NY, USA, 1984.</p>
<p>[14] M. Vidyasagar. Learning and Generalization: with Applications to Neural Networks. Springer, 2003.</p>
<p>[15] B. Yu. Rates of convergence for empirical processes of stationary mixing sequences. Annals Probability, 22(1):94–116, 1994.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
