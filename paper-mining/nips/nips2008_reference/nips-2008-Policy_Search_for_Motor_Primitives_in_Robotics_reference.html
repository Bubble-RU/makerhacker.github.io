<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>181 nips-2008-Policy Search for Motor Primitives in Robotics</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-181" href="../nips2008/nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">nips2008-181</a> <a title="nips-2008-181-reference" href="#">nips2008-181-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>181 nips-2008-Policy Search for Motor Primitives in Robotics</h1>
<br/><p>Source: <a title="nips-2008-181-pdf" href="http://papers.nips.cc/paper/3545-policy-search-for-motor-primitives-in-robotics.pdf">pdf</a></p><p>Author: Jens Kober, Jan R. Peters</p><p>Abstract: Many motor skills in humanoid robotics can be learned using parametrized motor primitives as done in imitation learning. However, most interesting motor learning problems are high-dimensional reinforcement learning problems often beyond the reach of current methods. In this paper, we extend previous work on policy learning from the immediate reward case to episodic reinforcement learning. We show that this results in a general, common framework also connected to policy gradient methods and yielding a novel algorithm for policy learning that is particularly well-suited for dynamic motor primitives. The resulting algorithm is an EM-inspired algorithm applicable to complex motor learning tasks. We compare this algorithm to several well-known parametrized policy search methods and show that it outperforms them. We apply it in the context of motor learning and show that it can learn a complex Ball-in-a-Cup task using a real Barrett WAMTM robot arm. 1</p><br/>
<h2>reference text</h2><p>[1] R. Sutton and A. Barto. Reinforcement Learning. MIT Press, 1998.</p>
<p>[2] J. Bagnell, S. Kadade, A. Ng, and J. Schneider. Policy search by dynamic programming. In Advances in Neural Information Processing Systems (NIPS), 2003.</p>
<p>[3] A. Ng and M. Jordan. PEGASUS: A policy search method for large MDPs and POMDPs. In International Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2000.</p>
<p>[4] F. Guenter, M. Hersch, S. Calinon, and A. Billard. Reinforcement learning for imitating constrained reaching movements. RSJ Advanced Robotics, 21, 1521-1544, 2007.</p>
<p>[5] M. Toussaint and C. Goerick. Probabilistic inference for structured planning in robotics. In International Conference on Intelligent Robots and Systems (IROS), 2007.</p>
<p>[6] M. Hoffman, A. Doucet, N. de Freitas, and A. Jasra. Bayesian policy learning with transdimensional MCMC. In Advances in Neural Information Processing Systems (NIPS), 2007.</p>
<p>[7] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229–256, 1992.</p>
<p>[8] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems (NIPS), 2000.</p>
<p>[9] J. Bagnell and J. Schneider. Covariant policy search. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2003.</p>
<p>[10] J. Peters and S. Schaal. Policy gradient methods for robotics. In International Conference on Intelligent Robots and Systems (IROS), 2006.</p>
<p>[11] G. Lawrence, N. Cowan, and S. Russell. Efﬁcient gradient estimation for motor control learning. In International Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2003.</p>
<p>[12] H. Attias. Planning by probabilistic inference. In Ninth International Workshop on Artiﬁcial Intelligence and Statistics (AISTATS), 2003.</p>
<p>[13] J. Binder, D. Koller, S. Russell, and K. Kanazawa. Adaptive probabilistic networks with hidden variables. Machine Learning, 29:213–244, 1997.</p>
<p>[14] G. Wulf. Attention and motor skill learning. Human Kinetics, Champaign, IL, 2007.</p>
<p>[15] D. E. Kirk. Optimal control theory. Prentice-Hall, Englewood Cliffs, New Jersey, 1970.</p>
<p>[16] G. J. McLachan and T. Krishnan. The EM Algorithm and Extensions. Wiley Series in Probability and Statistics. John Wiley & Sons, 1997.</p>
<p>[17] P. Dayan and G. E. Hinton. Using expectation-maximization for reinforcement learning. Neural Computation, 9(2):271–278, 1997.</p>
<p>[18] J. Peters and S. Schaal. Reinforcement learning by reward-weighted regression for operational space control. In International Conference on Machine Learning (ICML), 2007.</p>
<p>[19] T. Rückstieß, M. Felder, and J. Schmidhuber. State-dependent exploration for policy gradient methods. In European Conference on Machine Learning (ECML), 2008.</p>
<p>[20] M. Kawato, F. Gandolfo, H. Gomi, and Y. Wada. Teaching by showing in kendama based on optimization principle. In International Conference on Artiﬁcial Neural Networks, 1994.</p>
<p>[21] C. G. Atkeson. Using local trajectory optimizers to speed up global optimization in dynamic programming. In Advances in Neural Information Processing Systems (NIPS), 1994.</p>
<p>[22] A. Ijspeert, J. Nakanishi, and S. Schaal. Learning attractor landscapes for learning motor primitives. In Advances in Neural Information Processing Systems (NIPS), 2003.</p>
<p>[23] S. Schaal, P. Mohajerian, and A. Ijspeert. Dynamics systems vs. optimal control — a unifying view. Progress in Brain Research, 165(1):425–445, 2007.</p>
<p>[24] Wikipedia, May 31, 2008. http://en.wikipedia.org/wiki/Ball_in_a_cup</p>
<p>[25] J. Kober, B. Mohler, and J. Peters. Learning perceptual coupling for motor primitives. In International Conference on Intelligent RObots and Systems (IROS), 2008.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
