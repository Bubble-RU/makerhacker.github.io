<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-25" href="../nips2008/nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">nips2008-25</a> <a title="nips-2008-25-reference" href="#">nips2008-25-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</h1>
<br/><p>Source: <a title="nips-2008-25-pdf" href="http://papers.nips.cc/paper/3614-an-interior-point-stochastic-approximation-method-and-an-l1-regularized-delta-rule.pdf">pdf</a></p><p>Author: Peter Carbonetto, Mark Schmidt, Nando D. Freitas</p><p>Abstract: The stochastic approximation method is behind the solution to many important, actively-studied problems in machine learning. Despite its farreaching application, there is almost no work on applying stochastic approximation to learning problems with general constraints. The reason for this, we hypothesize, is that no robust, widely-applicable stochastic approximation method exists for handling such problems. We propose that interior-point methods are a natural solution. We establish the stability of a stochastic interior-point approximation method both analytically and empirically, and demonstrate its utility by deriving an on-line learning algorithm that also performs feature selection via L1 regularization. 1</p><br/>
<h2>reference text</h2><p>[1] L. Bottou and O. Bousquet, The tradeoﬀs of large scale learning, in Advances in Neural Information Processing Systems, vol. 20, 1998.</p>
<p>[2] S. Boyd and L. Vandenberghe, Convex optimization, Cambridge University Press, 2004.</p>
<p>[3] S. Chen, D. Donoho, and M. Saunders, Atomic decomposition by basis pursuit, SIAM Journal on Scientiﬁc Computing, 20 (1999), pp. 33–61.</p>
<p>[4] G. V. Cormack and T. R. Lynam, Spam corpus creation for TREC, in Proc. 2nd CEAS, 2005.</p>
<p>[5] , Online supervised spam ﬁlter evaluation, ACM Trans. Information Systems, 25 (2007).</p>
<p>[6] A. V. Fiacco and G. P. McCormick, Nonlinear programming: sequential unconstrained minimization techniques, John Wiley and Sons, 1968.</p>
<p>[7] A. Forsgren, P. E. Gill, and M. H. Wright, Interior methods for nonlinear optimization, SIAM Review, 44 (2002), pp. 525–597.</p>
<p>[8] A. Genkin, D. D. Lewis, and D. Madigan, Large-scale Bayesian logistic regression for text categorization, Technometrics, 49 (2007), pp. 291–304.</p>
<p>[9] T. Hastie, R. Tibshirani, and J. Friedman, The elements of statistical learning, Springer, 2001.</p>
<p>[10] S.-J. Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinevsky, An interior-point method for large-scale L1-regularized least squares, IEEE J. Selected Topics in Signal Processing, 1 (2007).</p>
<p>[11] H. J. Kushner and D. S. Clark, Stochastic approximation methods for constrained and unconstrained systems, Springer-Verlag, 1978.</p>
<p>[12] T. M. Mitchell, Machine Learning, McGraw-Hill, 1997.</p>
<p>[13] J. Nocedal and S. J. Wright, Numerical Optimization, Springer, 2nd ed., 2006.</p>
<p>[14] B. T. Poljak, Nonlinear programming methods in the presence of noise, Mathematical Programming, 14 (1978), pp. 87–97.</p>
<p>[15] H. Robbins and S. Monro, A stochastic approximation method, Annals Math. Stats., 22 (1951).</p>
<p>[16] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman, LabelMe: a database and web-based tool for image annotation, Intl. Journal of Computer Vision, 77 (2008), pp. 157–173.</p>
<p>[17] D. Saad, ed., On-line learning in neural networks, Cambridge University Press, 1998.</p>
<p>[18] M. Schmidt, G. Fung, and R. Rosales, Fast optimization methods for L1 regularization, in Proceedings of the 18th European Conference on Machine Learning, 2007, pp. 286–297.</p>
<p>[19] S. Shalev-Shwartz, Y. Singer, and N. Srebro, Pegasos: primal estimated sub-gradient solver for SVM, in Proceedings of the 24th Intl. Conference on Machine learning, 2007, pp. 807–814.</p>
<p>[20] J. C. Spall, Adaptive stochastic approximation by the simultaneous perturbation method, IEEE Transactions on Automatic Control, 45 (2000), pp. 1839–1853.</p>
<p>[21] J. C. Spall and J. A. Cristion, Model-free control of nonlinear stochastic systems with discretetime measurements, IEEE Transactions on Automatic Control, 43 (1998), pp. 1148–1210.</p>
<p>[22] R. Tibshirani, Regression shrinkage and selection via the Lasso, Journal of the Royal Statistical Society, 58 (1996), pp. 267–288.</p>
<p>[23] L. N. Trefethen and D. Bau, Numerical linear algebra, SIAM, 1997.</p>
<p>[24] I. Wang and J. C. Spall, Stochastic optimization with inequality constraints using simultaneous perturbations and penalty functions, in Proc. 42nd IEEE Conf. Decision and Control, 2003.</p>
<p>[25] M. H. Wright, Some properties of the Hessian of the logarithmic barrier function, Mathematical Programming, 67 (1994), pp. 265–295. , Ill-conditioning and computational error in interior methods for nonlinear programming,</p>
<p>[26] SIAM Journal on Optimization, 9 (1998), pp. 84–111.</p>
<p>[27] A. Zheng, Statistical software debugging, PhD thesis, University of California, Berkeley, 2005.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
