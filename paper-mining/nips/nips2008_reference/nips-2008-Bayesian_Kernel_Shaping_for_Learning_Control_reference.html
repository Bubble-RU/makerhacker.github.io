<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>32 nips-2008-Bayesian Kernel Shaping for Learning Control</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-32" href="../nips2008/nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">nips2008-32</a> <a title="nips-2008-32-reference" href="#">nips2008-32-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>32 nips-2008-Bayesian Kernel Shaping for Learning Control</h1>
<br/><p>Source: <a title="nips-2008-32-pdf" href="http://papers.nips.cc/paper/3393-bayesian-kernel-shaping-for-learning-control.pdf">pdf</a></p><p>Author: Jo-anne Ting, Mrinal Kalakrishnan, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In kernel-based regression learning, optimizing each kernel individually is useful when the data density, curvature of regression surfaces (or decision boundaries) or magnitude of output noise varies spatially. Previous work has suggested gradient descent techniques or complex statistical hypothesis methods for local kernel shaping, typically requiring some amount of manual tuning of meta parameters. We introduce a Bayesian formulation of nonparametric regression that, with the help of variational approximations, results in an EM-like algorithm for simultaneous estimation of regression and kernel parameters. The algorithm is computationally efﬁcient, requires no sampling, automatically rejects outliers and has only one prior to be speciﬁed. It can be used for nonparametric regression with local polynomials or as a novel method to achieve nonstationary regression with Gaussian processes. Our methods are particularly useful for learning control, where reliable estimation of local tangent planes is essential for adaptive controllers and reinforcement learning. We evaluate our methods on several synthetic data sets and on an actual robot which learns a task-level control law. 1</p><br/>
<h2>reference text</h2><p>[1] C. K. I. Williams and C. E. Rasmussen. Gaussian processes for regression. In David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, editors, In Advances in Neural Information Processing Systems 8, volume 8. MIT Press, 1995.</p>
<p>[2] J. H. Friedman. A variable span smoother. Technical report, Stanford University, 1984.</p>
<p>[3] T. Poggio and F. Girosi. Regularization algorithms for learning that are equivalent to multilayer networks. Science, 247:213–225, 1990.</p>
<p>[4] J. Fan and I. Gijbels. Local polynomial modeling and its applications. Chapman and Hall, 1996.</p>
<p>[5] C. J. Paciorek and M. J. Schervish. Nonstationary covariance functions for Gaussian process regression. In Advances in Neural Information Processing Systems 16. MIT Press, 2004.</p>
<p>[6] J. Fan and I. Gijbels. Data-driven bandwidth selection in local polynomial ﬁtting: Variable bandwidth and spatial adaptation. Journal of the Royal Statistical Society B, 57:371–395, 1995.</p>
<p>[7] S. Schaal and C.G. Atkeson. Assessing the quality of learned local models. In G. Tesauro J. Cowan and J. Alspector, editors, Advances in Neural Information Processing Systems, pages 160–167. Morgan Kaufmann, 1994.</p>
<p>[8] C. E. Rasmussen and Z. Ghahramani. Inﬁnite mixtures of Gaussian processes. In Advances in Neural Information Processing Systems 14. MIT Press, 2002.</p>
<p>[9] E. Meeds and S. Osindero. An alternative inﬁnite mixture of Gaussian process experts. In Advances in Neural Information Processing Systems 17. MIT Press, 2005.</p>
<p>[10] C. Atkeson and S. Schaal. Robot learning from demonstration. In Proceedings of the 14th international conference on Machine learning, pages 12–20. Morgan Kaufmann, 1997.</p>
<p>[11] C. Atkeson, A. Moore, and S. Schaal. Locally weighted learning. AI Review, 11:11–73, April 1997.</p>
<p>[12] A. D’Souza, S. Vijayakumar, and S. Schaal. The Bayesian backﬁtting relevance vector machine. In Proceedings of the 21st International Conference on Machine Learning. ACM Press, 2004.</p>
<p>[13] A. Gelman, J. Carlin, H.S. Stern, and D.B. Rubin. Bayesian Data Analysis. Chapman and Hall, 2000.</p>
<p>[14] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of Royal Statistical Society. Series B, 39(1):1–38, 1977.</p>
<p>[15] Z. Ghahramani and M.J. Beal. Graphical models and variational methods. In D. Saad and M. Opper, editors, Advanced Mean Field Methods - Theory and Practice. MIT Press, 2000.</p>
<p>[16] T. S. Jaakkola and M. I. Jordan. Bayesian parameter estimation via variational methods. Statistics and Computing, 10:25–37, 2000.</p>
<p>[17] E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances in Neural Information Processing Systems 18. MIT Press, 2006.</p>
<p>[18] V. Tresp. Mixtures of Gaussian processes. In Advances in Neural Information Processing Systems 13. MIT Press, 2000.</p>
<p>[19] A. M. Schmidt and A. O’Hagan. Bayesian inference for nonstationary spatial covariance structure via spatial deformations. Journal of Royal Statistical Society. Series B, 65:745–758, 2003.</p>
<p>[20] B. W. Silverman. Some aspects of the spline smoothing approach to non-parametric regression curve ﬁtting. Journal of Royal Statistical Society. Series B, 47:1–52, 1985.</p>
<p>[21] J. Peters and S. Schaal. Learning to control in operational space. International Journal of Robotics Research, 27:197–212, 2008.</p>
<p>[22] M. I. Jordan and D. E. Rumelhart. Internal world models and supervised learning. In Machine Learning: Proceedings of Eighth Internatinoal Workshop, pages 70–85. Morgan Kaufmann, 1991.</p>
<p>[23] Z. Ghahramani. Solving inverse problems using an EM approach to density estimation. In Proceedings of the 1993 Connectionist Models summer school, pages 316–323. Erlbaum Associates, 1994.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
