<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-162" href="../nips2008/nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">nips2008-162</a> <a title="nips-2008-162-reference" href="#">nips2008-162-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</h1>
<br/><p>Source: <a title="nips-2008-162-pdf" href="http://papers.nips.cc/paper/3591-on-the-design-of-loss-functions-for-classification-theory-robustness-to-outliers-and-savageboost.pdf">pdf</a></p><p>Author: Hamed Masnadi-shirazi, Nuno Vasconcelos</p><p>Abstract: The machine learning problem of classiﬁer design is studied from the perspective of probability elicitation, in statistics. This shows that the standard approach of proceeding from the speciﬁcation of a loss, to the minimization of conditional risk is overly restrictive. It is shown that a better alternative is to start from the speciﬁcation of a functional form for the minimum conditional risk, and derive the loss function. This has various consequences of practical interest, such as showing that 1) the widely adopted practice of relying on convex loss functions is unnecessary, and 2) many new losses can be derived for classiﬁcation problems. These points are illustrated by the derivation of a new loss which is not convex, but does not compromise the computational tractability of classiﬁer design, and is robust to the contamination of data with outliers. A new boosting algorithm, SavageBoost, is derived for the minimization of this loss. Experimental results show that it is indeed less sensitive to outliers than conventional methods, such as Ada, Real, or LogitBoost, and converges in fewer iterations. 1</p><br/>
<h2>reference text</h2><p>[1] T. G. Dietterich, “An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization,” Machine Learning, 2000.</p>
<p>[2] Y. Wu and Y. Liu, “Robust truncated-hinge-loss support vector machines,” JASA, 2007.</p>
<p>[3] J. Friedman, T. Hastie, and R. Tibshirani, “Additive logistic regression: A statistical view of boosting,” Annals of Statistics, 2000.</p>
<p>[4] T. Zhang, “Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization,” Annals of Statistics, 2004.</p>
<p>[5] P. Bartlett, M. Jordan, and J. D. McAuliffe, “Convexity, classiﬁcation, and risk bounds,” JASA, 2006.</p>
<p>[6] L. J. Savage, “The elicitation of personal probabilities and expectations,” JASA, vol. 66, pp. 783–801, 1971.</p>
<p>[7] S. Boyd and L. Vandenberghe, Convex Optimization.  Cambridge: Cambridge University Press, 2004.</p>
<p>[8] R. McDonald, D. Hand, and I. Eckley, “An empirical comparison of three boosting algorithms on real data sets with artiﬁcial class noise,” in International Workshop on Multiple Classiﬁer Systems, 2003.</p>
<p>[9] Y. Freund and R. Schapire, “A decision-theoretic generalization of on-line learning and an application to boosting,” Journal of Computer and System Sciences, 1997.</p>
<p>[10] T. Zhang and B. Yu, “Boosting with early stopping: Convergence and consistency,” Annals of Statistics, 2005.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
