<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>229 nips-2008-Syntactic Topic Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-229" href="../nips2008/nips-2008-Syntactic_Topic_Models.html">nips2008-229</a> <a title="nips-2008-229-reference" href="#">nips2008-229-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>229 nips-2008-Syntactic Topic Models</h1>
<br/><p>Source: <a title="nips-2008-229-pdf" href="http://papers.nips.cc/paper/3398-syntactic-topic-models.pdf">pdf</a></p><p>Author: Jordan L. Boyd-graber, David M. Blei</p><p>Abstract: We develop the syntactic topic model (STM), a nonparametric Bayesian model of parsed documents. The STM generates words that are both thematically and syntactically constrained, which combines the semantic insights of topic models with the syntactic information available from parse trees. Each word of a sentence is generated by a distribution that combines document-speciﬁc topic weights and parse-tree-speciﬁc syntactic transitions. Words are assumed to be generated in an order that respects the parse tree. We derive an approximate posterior inference method based on variational methods for hierarchical Dirichlet processes, and we report qualitative and quantitative results on both synthetic data and hand-parsed documents. 1</p><br/>
<h2>reference text</h2><p>[1] Wei, X., B. Croft. LDA- based document models for ad- hoc retrieval. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval. 2006.</p>
<p>[2] Cai, J. F., W. S. Lee, Y. W. Teh. NUS- ML:Improving word sense disambiguation using topic features. In Proceedings of SemEval- 2007. Association for Computational Linguistics, 2007.  7</p>
<p>[3] Fei-Fei Li, P. Perona. A Bayesian hierarchical model for learning natural scene categories. In CVPR ’05 - Volume 2, pages 524–531. IEEE Computer Society, Washington, DC, USA, 2005.</p>
<p>[4] Marlin, B. Modeling user rating proﬁles for collaborative ﬁltering. In S. Thrun, L. Saul, B. Sch¨ lkopf, o eds., Advances in Neural Information Processing Systems. MIT Press, Cambridge, MA, 2004.</p>
<p>[5] Grifﬁths, T., M. Steyvers. Probabilistic topic models. In T. Landauer, D. McNamara, S. Dennis, W. Kintsch, eds., Latent Semantic Analysis: A Road to Meaning. Laurence Erlbaum, 2006.</p>
<p>[6] Pad´ , S., M. Lapata. Dependency-based construction of semantic space models. Computational Linguiso tics, 33(2):161–199, 2007.</p>
<p>[7] Lin, D. An information-theoretic deﬁnition of similarity. In Proceedings of International Conference of Machine Learning, pages 296–304. 1998.</p>
<p>[8] Grifﬁths, T. L., M. Steyvers, D. M. Blei, et al. Integrating topics and syntax. In L. K. Saul, Y. Weiss, L. Bottou, eds., Advances in Neural Information Processing Systems, pages 537–544. MIT Press, Cambridge, MA, 2005.</p>
<p>[9] Gruber, A., M. Rosen-Zvi, Y. Weiss. Hidden topic Markov models. In Proceedings of Artiﬁcial Intelligence and Statistics. San Juan, Puerto Rico, 2007.</p>
<p>[10] Marcus, M. P., B. Santorini, M. A. Marcinkiewicz. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19(2):313–330, 1994.</p>
<p>[11] Hinton, G. Products of experts. In Proceedings of the Ninth International Conference on Artiﬁcial Neural Networks, pages 1–6. IEEE, Edinburgh, Scotland, 1999.</p>
<p>[12] Tee, Y. W., M. I. Jordan, M. J. Beal, et al. Hierarchical dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581, 2006.</p>
<p>[13] Blei, D., A. Ng, M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993– 1022, 2003.</p>
<p>[14] Finkel, J. R., T. Grenager, C. D. Manning. The inﬁnite tree. In Proceedings of Association for Computational Linguistics, pages 272–279. Association for Computational Linguistics, Prague, Czech Republic, 2007.</p>
<p>[15] Jordan, M., Z. Ghahramani, T. S. Jaakkola, et al. An introduction to variational methods for graphical models. Machine Learning, 37(2):183–233, 1999.</p>
<p>[16] Liang, P., S. Petrov, M. Jordan, et al. The inﬁnite PCFG using hierarchical Dirichlet processes. In Proceedings of Emperical Methods in Natural Language Processing, pages 688–697. 2007.</p>
<p>[17] Boyd, S., L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.</p>
<p>[18] Johansson, R., P. Nugues. Extended constituent-to-dependency conversion for English. In (NODALIDA). 2007.</p>
<p>[19] Koeling, R., D. McCarthy. Sussx: WSD using automatically acquired predominant senses. In Proceedings of SemEval-2007. Association for Computational Linguistics, 2007.</p>
<p>[20] Boyd-Graber, J., D. Blei. PUTOP: Turning predominant senses into a topic model for WSD. In Proceedings of SemEval-2007. Association for Computational Linguistics, 2007.</p>
<p>[21] McCarthy, D., R. Koeling, J. Weeds, et al. Finding predominant word senses in untagged text. In Proceedings of Association for Computational Linguistics, pages 280–287. Association for Computational Linguistics, 2004.</p>
<p>[22] Collins, M. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637, 2003.</p>
<p>[23] Klein, D., C. Manning. Accurate unlexicalized parsing. In Proceedings of Association for Computational Linguistics, pages 423–430. Association for Computational Linguistics, 2003.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
