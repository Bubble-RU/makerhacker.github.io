<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>155 nips-2008-Nonparametric regression and classification with joint sparsity constraints</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-155" href="../nips2008/nips-2008-Nonparametric_regression_and_classification_with_joint_sparsity_constraints.html">nips2008-155</a> <a title="nips-2008-155-reference" href="#">nips2008-155-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>155 nips-2008-Nonparametric regression and classification with joint sparsity constraints</h1>
<br/><p>Source: <a title="nips-2008-155-pdf" href="http://papers.nips.cc/paper/3616-nonparametric-regression-and-classification-with-joint-sparsity-constraints.pdf">pdf</a></p><p>Author: Han Liu, Larry Wasserman, John D. Lafferty</p><p>Abstract: We propose new families of models and algorithms for high-dimensional nonparametric learning with joint sparsity constraints. Our approach is based on a regularization method that enforces common sparsity patterns across different function components in a nonparametric additive model. The algorithms employ a coordinate descent approach that is based on a functional soft-thresholding operator. The framework yields several new models, including multi-task sparse additive models, multi-response sparse additive models, and sparse additive multi-category logistic regression. The methods are illustrated with experiments on synthetic data and gene microarray data. 1</p><br/>
<h2>reference text</h2><p>F ORNASIER , M. and R AUHUT, H. (2008). Recovery algorithms for vector valued data with joint sparsity constraints. SIAM Journal of Numerical Analysis 46 577–613. F RIEDMAN , J. H. (1991). Multivariate adaptive regression splines. The Annals of Statistics 19 1–67. K HAN , J., W EI , J. S., R INGNER , M., S AA , L. H., L ADANYI , M., W ESTERMANN , F., B ERTHOLD , F., S CHWAB , M., A NTONESCU , C. R., P ETERSON , C. and M ELTZER , P. S. (2001). Classiﬁcation and diagnostic prediction of cancers using gene expression proﬁling and artiﬁcial neural networks. Nature Medicine 7 673 –679. K RISHNAPURAM , B., C ARIN , L., F IGUEIREDO , M. and H ARTEMINK , A. (2005). Sparse multinomial logistic regression: Fast algorithms and generalization bounds. IEEE Transactions on Pattern Analysis and Machine Intelligence 27 957– 968. R AVIKUMAR , P., L IU , H., L AFFERTY, J. and WASSERMAN , L. (2007). SpAM: Sparse additive models. In Advances in Neural Information Processing Systems 20. MIT Press. ROCKAFELLAR , R. T. and W ETS , R. J.-B. (1998). Variational Analysis. Springer-Verlag Inc. T IBSHIRANI , R., H ASTIE , T., NARASIMHAN , B., and C HU , G. (2002). Diagnosis of multiple cancer types by shrunken centroids of gene expression. Proc Natl Acad Sci U.S.A. 99 6567–6572. T ROPP, J., G ILBERT, A. C. and S TRAUSS , M. J. (2006). Algorithms for simultaneous sparse approximation. Part II: Convex relaxation. Signal Processing 86 572–588. T URLACH , B., V ENABLES , W. N. and W RIGHT, S. J. (2005). Simultaneous variable selection. Technometrics 27 349–363. Z HANG , H. H., L IU , Y., W U , Y. and Z HU , J. (2008). Variable selection for the multicategory SVM via adaptive sup-norm regularization. Electronic Journal of Statistics 2 149–1167. Z HANG , J. (2006). A probabilistic framework for multitask learning. Tech. Rep. CMU-LTI-06-006, Ph.D. thesis, Carnegie Mellon University.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
