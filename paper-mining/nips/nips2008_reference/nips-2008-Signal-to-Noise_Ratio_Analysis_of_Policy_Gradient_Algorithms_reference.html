<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>210 nips-2008-Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-210" href="../nips2008/nips-2008-Signal-to-Noise_Ratio_Analysis_of_Policy_Gradient_Algorithms.html">nips2008-210</a> <a title="nips-2008-210-reference" href="#">nips2008-210-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>210 nips-2008-Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms</h1>
<br/><p>Source: <a title="nips-2008-210-pdf" href="http://papers.nips.cc/paper/3511-signal-to-noise-ratio-analysis-of-policy-gradient-algorithms.pdf">pdf</a></p><p>Author: John W. Roberts, Russ Tedrake</p><p>Abstract: Policy gradient (PG) reinforcement learning algorithms have strong (local) convergence guarantees, but their learning performance is typically limited by a large variance in the estimate of the gradient. In this paper, we formulate the variance reduction problem by describing a signal-to-noise ratio (SNR) for policy gradient algorithms, and evaluate this SNR carefully for the popular Weight Perturbation (WP) algorithm. We conﬁrm that SNR is a good predictor of long-term learning performance, and that in our episodic formulation, the cost-to-go function is indeed the optimal baseline. We then propose two modiﬁcations to traditional model-free policy gradient algorithms in order to optimize the SNR. First, we examine WP using anisotropic sampling distributions, which introduces a bias into the update but increases the SNR; this bias can be interpreted as following the natural gradient of the cost function. Second, we show that non-Gaussian distributions can also increase the SNR, and argue that the optimal isotropic distribution is a ‘shell’ distribution with a constant magnitude and uniform distribution in direction. We demonstrate that both modiﬁcations produce substantial improvements in learning performance in challenging policy gradient experiments. 1</p><br/>
<h2>reference text</h2><p>Amari, S. (1998). Natural gradient works efﬁciently in learning. Neural Computation, 10, 251–276. Baxter, J., & Bartlett, P. (2001). Inﬁnite-horizon policy-gradient estimation. Journal of Artiﬁcial Intelligence Research, 15, 319–350. Greensmith, E., Bartlett, P. L., & Baxter, J. (2004). Variance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learning Research, 5, 1471–1530. Jabri, M., & Flower, B. (1992). Weight perturbation: An optimal architecture and learning technique for analog VLSI feedforward and recurrent multilayer networks. IEEE Trans. Neural Netw., 3, 154–157. Kohl, N., & Stone, P. (2004). Policy gradient reinforcement learning for fast quadrupedal locomotion. Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). Meuleau, N., Peshkin, L., Kaelbling, L. P., & Kim, K.-E. (2000). Off-policy policy search. NIPS. Peters, J., Vijayakumar, S., & Schaal, S. (2003a). Policy gradient methods for robot control (Technical Report CS-03-787). University of Southern California. Peters, J., Vijayakumar, S., & Schaal, S. (2003b). Reinforcement learning for humanoid robotics. Proceedings of the Third IEEE-RAS International Conference on Humanoid Robots. Riedmiller, M., Peters, J., & Schaal, S. (2007). Evaluation of policy gradient methods and variants on the cart-pole benchmark. Symposium on Approximate Dynamic Programming and Reinforcement Learning (pp. 254–261). Shelley, M., Vandenberghe, N., & Zhang, J. (2005). Heavy ﬂags undergo spontaneous oscillations in ﬂowing water. Physical Review Letters, 94. Tedrake, R., Zhang, T. W., & Seung, H. S. (2004). Stochastic policy gradient reinforcement learning on a simple 3D biped. Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS) (pp. 2849–2854). Sendai, Japan. Vandenberghe, N., Zhang, J., & Childress, S. (2004). Symmetry breaking leads to forward ﬂapping ﬂight. Journal of Fluid Mechanics, 506, 147–155. Williams, J. L., III, J. W. F., & Willsky, A. S. (2006). Importance sampling actor-critic algorithms. Proceedings of the 2006 American Control Conference. Williams, R. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8, 229–256.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
