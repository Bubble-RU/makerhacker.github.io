<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>93 nips-2008-Global Ranking Using Continuous Conditional Random Fields</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-93" href="../nips2008/nips-2008-Global_Ranking_Using_Continuous_Conditional_Random_Fields.html">nips2008-93</a> <a title="nips-2008-93-reference" href="#">nips2008-93-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>93 nips-2008-Global Ranking Using Continuous Conditional Random Fields</h1>
<br/><p>Source: <a title="nips-2008-93-pdf" href="http://papers.nips.cc/paper/3402-global-ranking-using-continuous-conditional-random-fields.pdf">pdf</a></p><p>Author: Tao Qin, Tie-yan Liu, Xu-dong Zhang, De-sheng Wang, Hang Li</p><p>Abstract: This paper studies global ranking problem by learning to rank methods. Conventional learning to rank methods are usually designed for ‘local ranking’, in the sense that the ranking model is deﬁned on a single object, for example, a document in information retrieval. For many applications, this is a very loose approximation. Relations always exist between objects and it is better to deﬁne the ranking model as a function on all the objects to be ranked (i.e., the relations are also included). This paper refers to the problem as global ranking and proposes employing a Continuous Conditional Random Fields (CRF) for conducting the learning task. The Continuous CRF model is deﬁned as a conditional probability distribution over ranking scores of objects conditioned on the objects. It can naturally represent the content information of objects as well as the relation information between objects, necessary for global ranking. Taking two speciﬁc information retrieval tasks as examples, the paper shows how the Continuous CRF method can perform global ranking better than baselines.</p><br/>
<h2>reference text</h2><p>[1] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. J. Mach. Learn. Res., 7:2399–2434, 2006.</p>
<p>[2] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. Learning to rank: from pairwise approach to listwise approach. In ICML ’07, pages 129–136, 2007.</p>
<p>[3] W. Chu and Z. Ghahramani. Gaussian processes for ordinal regression. Journal of Machine Learning Research, 6:1019–1041, 2005.</p>
<p>[4] I. S. Dhillon. Co-clustering documents and words using bipartite spectral graph partitioning. In KDD ’01.</p>
<p>[5] G. H. Golub and C. F. V. Loan. Matrix computations (3rd ed.). Johns Hopkins University Press, 1996.</p>
<p>[6] K. J¨ rvelin and J. Kek¨ l¨ inen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., a aa 20(4):422–446, 2002.</p>
<p>[7] T. Joachims. Optimizing search engines using clickthrough data. In KDD ’02, pages 133–142, 2002.</p>
<p>[8] K. L. Kwok. A document-document similarity measure based on cited titles and probability theory, and its application to relevance feedback retrieval. In SIGIR ’84, pages 221–231, 1984.</p>
<p>[9] J. G. Lewis. Algorithm 582: The gibbs-poole-stockmeyer and gibbs-king algorithms for reordering sparse matrices. ACM Trans. Math. Softw., 8(2):190–194, 1982.</p>
<p>[10] T.-Y. Liu, J. Xu, T. Qin, W.-Y. Xiong, and H. Li. Letor: Benchmark dataset for research on learning to rank for information retrieval. In SIGIR ’07 Workshop, 2007.</p>
<p>[11] T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, and W.-Y. Ma. A study of relevance propagation for web search. In SIGIR ’05, pages 408–415, 2005.</p>
<p>[12] T. Qin, T.-Y. Liu, X.-D. Zhang, G. Feng, D.-S. Wang, and W.-Y. Ma. Topic distillation via sub-site retrieval. Information Processing & Management, 43(2):445–460, 2007.</p>
<p>[13] T. Qin, T.-Y. Liu, X.-D. Zhang, D.-S. Wang, and H. Li. Global ranking of documents using continuous conditional random ﬁelds. Technical Report MSR-TR-2008-156, Microsoft Corporation, 2008.</p>
<p>[14] T. Qin, T.-Y. Liu, X.-D. Zhang, D.-S. Wang, W.-Y. Xiong, and H. Li. Learning to rank relational objects and its application to web search. In WWW ’08, 2008.</p>
<p>[15] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888–905, 2000.</p>
<p>[16] C. Sutton and A. McCallum. An introduction to conditional random ﬁelds for relational learning. In L. Getoor and B. Taskar, editors, Introduction to Statistical Relational Learning. MIT Press, 2006.</p>
<p>[17] T. Tao and C. Zhai. Regularized estimation of mixture models for robust pseudo-relevance feedback. In SIGIR ’06, pages 162–169, 2006.</p>
<p>[18] C. X. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In SIGIR ’03, pages 10–17, 2003.</p>
<p>[19] D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Sch¨ lkopf. Learning with local and global consistency, o 2003. In 18th Annual Conf. on Neural Information Processing Systems.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
