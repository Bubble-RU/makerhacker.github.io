<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-175" href="../nips2008/nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">nips2008-175</a> <a title="nips-2008-175-reference" href="#">nips2008-175-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</h1>
<br/><p>Source: <a title="nips-2008-175-pdf" href="http://papers.nips.cc/paper/3611-psdboost-matrix-generation-linear-programming-for-positive-semidefinite-matrices-learning.pdf">pdf</a></p><p>Author: Chunhua Shen, Alan Welsh, Lei Wang</p><p>Abstract: In this work, we consider the problem of learning a positive semideﬁnite matrix. The critical issue is how to preserve positive semideﬁniteness during the course of learning. Our algorithm is mainly inspired by LPBoost [1] and the general greedy convex optimization framework of Zhang [2]. We demonstrate the essence of the algorithm, termed PSDBoost (positive semideﬁnite Boosting), by focusing on a few different applications in machine learning. The proposed PSDBoost algorithm extends traditional Boosting algorithms in that its parameter is a positive semideﬁnite matrix with trace being one instead of a classiﬁer. PSDBoost is based on the observation that any trace-one positive semideﬁnite matrix can be decomposed into linear convex combinations of trace-one rank-one matrices, which serve as base learners of PSDBoost. Numerical experiments are presented. 1</p><br/>
<h2>reference text</h2><p>[1] A. Demiriz, K.P. Bennett, and J. Shawe-Taylor. Linear programming boosting via column generation. Mach. Learn., 46(1-3):225–254, 2002.</p>
<p>[2] T. Zhang. Sequential greedy approximation for certain convex optimization problems. IEEE Trans. Inf. Theory, 49(3):682–691, 2003.</p>
<p>[3] M. E. L¨ bbecke and J. Desrosiers. Selected topics in column generation. Operation Res., 53(6):1007–1023, 2005. u</p>
<p>[4] ILOG, Inc. CPLEX 11.1, 2008. http://www.ilog.com/products/cplex/.</p>
<p>[5] G. B. Dantzig and P. Wolfe. Decomposition principle for linear programs. Operation Res., 8(1):101–111, 1960.</p>
<p>[6] E. Xing, A. Ng, M. Jordan, and S. Russell. Distance metric learning, with application to clustering with side-information. In Proc. Adv. Neural Inf. Process. Syst. MIT Press, 2002.</p>
<p>[7] K. Q. Weinberger, J. Blitzer, and L. K. Saul. Distance metric learning for large margin nearest neighbor classiﬁcation. In Proc. Adv. Neural Inf. Process. Syst., pages 1473–1480, 2005.</p>
<p>[8] R. Rosales and G. Fung. Learning sparse metrics via linear programming. In Proc. ACM Int. Conf. Knowledge Discovery & Data Mining, pages 367–373, Philadelphia, PA, USA, 2006.</p>
<p>[9] M. Krein and D. Milman. On extreme points of regular convex sets. Studia Mathematica, 9:133–138, 1940.</p>
<p>[10] M. L. Overton and R. S. Womersley. On the sum of the largest eigenvalues of a symmetric matrix. SIAM J. Matrix Anal. Appl., 13(1):41–45, 1992.</p>
<p>[11] P. A. Fillmore and J. P. Williams. Some convexity theorems for matrices. Glasgow Math. Journal, 12:110–117, 1971.</p>
<p>[12] R. E. Schapire. Theoretical views of boosting and applications. In Proc. Int. Conf. Algorithmic Learn. Theory, pages 13–25, London, UK, 1999. Springer-Verlag.</p>
<p>[13] B. Kulis, M. Sustik, and I. Dhillon. Learning low-rank kernel matrices. In Proc. Int. Conf. Mach. Learn., pages 505–512, Pittsburgh, Pennsylvania, 2006.</p>
<p>[14] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.</p>
<p>[15] B. Borchers. CSDP, a C library for semideﬁnite programming. Optim. Methods and Softw., 11(1):613–623, 1999.</p>
<p>[16] D. Calvetti, L. Reichel, and D. C. Sorensen. An implicitly restarted Lanczos method for large symmetric eigenvalue problems. Elec. Trans. Numer. Anal, 2:1–21, Mar 1994. http://etna.mcs.kent.edu.</p>
<p>[17] J. F. Bonnans, J. C. Gilbert, C. Lemar´ chal, and C. A. Sagastiz´ bal. Numerical Optimization: Theoretical and Practical Aspects (1st e a edition). Springer-Verlag, Berlin, 2003.  200  −5  100  1  Opt(D1)  300  0  Opt(D )  5  −10  0  −15  −100  −20  −200  −25 0  50  100 iterations  150  200  −300 0  50  100  150  iterations  Figure 1: The objective value of the dual problem (D1 ) on the ﬁrst (left) and second (right) experiment. The dashed line shows the ground truth obtained by directly solving the original primal SDP (3) using interior-point methods.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
