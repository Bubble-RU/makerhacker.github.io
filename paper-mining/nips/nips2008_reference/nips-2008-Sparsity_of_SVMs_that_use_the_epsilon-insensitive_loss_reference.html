<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>217 nips-2008-Sparsity of SVMs that use the epsilon-insensitive loss</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-217" href="../nips2008/nips-2008-Sparsity_of_SVMs_that_use_the_epsilon-insensitive_loss.html">nips2008-217</a> <a title="nips-2008-217-reference" href="#">nips2008-217-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>217 nips-2008-Sparsity of SVMs that use the epsilon-insensitive loss</h1>
<br/><p>Source: <a title="nips-2008-217-pdf" href="http://papers.nips.cc/paper/3466-sparsity-of-svms-that-use-the-epsilon-insensitive-loss.pdf">pdf</a></p><p>Author: Ingo Steinwart, Andreas Christmann</p><p>Abstract: In this paper lower and upper bounds for the number of support vectors are derived for support vector machines (SVMs) based on the -insensitive loss function. It turns out that these bounds are asymptotically tight under mild assumptions on the data generating distribution. Finally, we brieﬂy discuss a trade-off in between sparsity and accuracy if the SVM is used to estimate the conditional median. 1</p><br/>
<h2>reference text</h2><p>[1] A. Christmann and I. Steinwart. Consistency and robustness of kernel based regression. Bernoulli, 13:799–819, 2007.</p>
<p>[2] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines. Cambridge University Press, Cambridge, 2000.</p>
<p>[3] E. De Vito, L. Rosasco, A. Caponnetto, M. Piana, and A. Verri. Some properties of regularized kernel methods. J. Mach. Learn. Res., 5:1363–1390, 2004.</p>
<p>[4] L. Devroye, L. Gy¨ rﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, New o York, 1996.</p>
<p>[5] I. Steinwart. Sparseness of support vector machines. J. Mach. Learn. Res., 4:1071–1105, 2003.</p>
<p>[6] I. Steinwart. How to compare different loss functions. Constr. Approx., 26:225–287, 2007.</p>
<p>[7] I. Steinwart and A. Christmann. Support Vector Machines. Springer, New York, 2008.</p>
<p>[8] I. Steinwart and A. Christmann. How SVMs can estimate quantiles and the median. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 305–312. MIT Press, Cambridge, MA, 2008.</p>
<p>[9] I. Steinwart, D. Hush, and C. Scovel. Function classes that approximate the Bayes risk. In G. Lugosi and H. U. Simon, editors, Proceedings of the 19th Annual Conference on Learning Theory, pages 79–93. Springer, New York, 2006.</p>
<p>[10] V. Vapnik, S. Golowich, and A. Smola. Support vector method for function approximation, regression estimation, and signal processing. In M. Mozer, M. Jordan, and T. Petsche, editors, Advances in Neural Information Processing Systems 9, pages 81–287. MIT Press, Cambridge, MA, 1997.</p>
<p>[11] V. N. Vapnik. Statistical Learning Theory. John Wiley & Sons, New York, 1998.</p>
<p>[12] V. Yurinsky. Sums and Gaussian Vectors. Lecture Notes in Math. 1617. Springer, Berlin, 1995.</p>
<p>[13] T. Zhang. Convergence of large margin separable linear classiﬁcation. In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 357–363. MIT Press, Cambridge, MA, 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
