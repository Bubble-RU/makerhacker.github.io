<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-87" href="../nips2008/nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">nips2008-87</a> <a title="nips-2008-87-reference" href="#">nips2008-87-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</h1>
<br/><p>Source: <a title="nips-2008-87-pdf" href="http://papers.nips.cc/paper/3501-fitted-q-iteration-by-advantage-weighted-regression.pdf">pdf</a></p><p>Author: Gerhard Neumann, Jan R. Peters</p><p>Abstract: Recently, ﬁtted Q-iteration (FQI) based methods have become more popular due to their increased sample efﬁciency, a more stable learning process and the higher quality of the resulting policy. However, these methods remain hard to use for continuous action spaces which frequently occur in real-world tasks, e.g., in robotics and other technical applications. The greedy action selection commonly used for the policy improvement step is particularly problematic as it is expensive for continuous actions, can cause an unstable learning process, introduces an optimization bias and results in highly non-smooth policies unsuitable for real-world systems. In this paper, we show that by using a soft-greedy action selection the policy improvement step used in FQI can be simpliﬁed to an inexpensive advantageweighted regression. With this result, we are able to derive a new, computationally efﬁcient FQI algorithm which can even deal with high dimensional action spaces. 1</p><br/>
<h2>reference text</h2><p>[1] R. Sutton and A. Barto, Reinforcement Learning. Boston, MA: MIT Press, 1998.</p>
<p>[2] J. A. Boyan and A. W. Moore, “Generalization in reinforcement learning: Safely approximating the value function,” in Advances in Neural Information Processing Systems 7, pp. 369–376, MIT Press, 1995.</p>
<p>[3] P. Viviani and T. Flash, “Minimum-jerk, two-thirds power law, and isochrony: Converging approaches to movement planning,” Journal of Experimental Psychology: Human Perception and Performance, vol. 21, no. 1, pp. 32–53, 1995.</p>
<p>[4] R. M. Alexander, “A minimum energy cost hypothesis for human arm trajectories,” Biological Cybernetics, vol. 76, pp. 97–105, 1997.</p>
<p>[5] C. M. Harris and D. M. Wolpert, “Signal-dependent noise determines motor planning.,” Nature, vol. 394, pp. 780–784, August 1998.</p>
<p>[6] M. Riedmiller, “Neural ﬁtted Q-iteration - ﬁrst experiences with a data efﬁcient neural reinforcement learning method,” in Proceedings of the European Conference on Machine Learning (ECML), 2005.</p>
<p>[7] R. Sutton, “Generalization in reinforcement learning: Successful examples using sparse coarse coding,” in Advances in Neural Information Processing Systems 8, pp. 1038–1044, MIT Press, 1996.</p>
<p>[8] D. Ernst, P. Geurts, and L. Wehenkel, “Tree-based batch mode reinforcement learning,” J. Mach. Learn. Res., vol. 6, pp. 503–556, 2005.</p>
<p>[9] A. Antos, R. Munos, and C. Szepesvari, “Fitted Q-iteration in continuous action-space MDPs,” in Advances in Neural Information Processing Systems 20, pp. 9–16, Cambridge, MA: MIT Press, 2008.</p>
<p>[10] S. Timmer and M. Riedmiller, “Fitted Q-iteration with CMACs,” pp. 1–8, 2007.</p>
<p>[11] J. Peters and S. Schaal, “Policy learning for motor skills,” in Proceedings of 14th International Conference on Neural Information Processing (ICONIP), 2007.</p>
<p>[12] P.-T. de Boer, D. Kroese, S. Mannor, and R. Rubinstein, “A tutorial on the cross-entropy method,” Annals of Operations Research, vol. 134, pp. 19–67, January 2005.</p>
<p>[13] J. Peters and S. Schaal, “Reinforcement learning by reward-weighted regression for operational space control,” in Proceedings of the International Conference on Machine Learning (ICML), 2007.</p>
<p>[14] C. G. Atkeson, A. W. Moore, and S. Schaal, “Locally weighted learning,” Artiﬁcial Intelligence Review, vol. 11, no. 1-5, pp. 11–73, 1997.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
