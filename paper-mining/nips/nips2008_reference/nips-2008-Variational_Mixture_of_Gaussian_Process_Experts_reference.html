<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>249 nips-2008-Variational Mixture of Gaussian Process Experts</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-249" href="../nips2008/nips-2008-Variational_Mixture_of_Gaussian_Process_Experts.html">nips2008-249</a> <a title="nips-2008-249-reference" href="#">nips2008-249-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>249 nips-2008-Variational Mixture of Gaussian Process Experts</h1>
<br/><p>Source: <a title="nips-2008-249-pdf" href="http://papers.nips.cc/paper/3395-variational-mixture-of-gaussian-process-experts.pdf">pdf</a></p><p>Author: Chao Yuan, Claus Neubauer</p><p>Abstract: Mixture of Gaussian processes models extended a single Gaussian process with ability of modeling multi-modal data and reduction of training complexity. Previous inference algorithms for these models are mostly based on Gibbs sampling, which can be very slow, particularly for large-scale data sets. We present a new generative mixture of experts model. Each expert is still a Gaussian process but is reformulated by a linear model. This breaks the dependency among training outputs and enables us to use a much faster variational Bayesian algorithm for training. Our gating network is more ﬂexible than previous generative approaches as inputs for each expert are modeled by a Gaussian mixture model. The number of experts and number of Gaussian components for an expert are inferred automatically. A variety of tests show the advantages of our method. 1</p><br/>
<h2>reference text</h2><p>[1] C. E. Rasmussen and Z. Ghahramani. Inﬁnite mixtures of Gaussian process experts. In Advances in Neural Information Processing Systems 14. MIT Press, 2002.</p>
<p>[2] E. Meeds and S. Osindero. An alternative inﬁnite mixture of Gaussian process experts. In Advances in Neural Information Processing Systems 18. MIT Press, 2006.</p>
<p>[3] L. Xu, M. I. Jordan, and G. E. Hinton. An alternative model for mixtures of experts. In Advances in Neural Information Processing Systems 7. MIT Press, 1995.</p>
<p>[4] N. Ueda and Z. Ghahramani. Bayesian model search for mixture models based on optimizing variational bounds. Neural Networks, 15(10):1223–1241, 2002.</p>
<p>[5] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.</p>
<p>[6] A. J. Smola and P. Bartlett. Sparse greedy Gaussian process regression. In Advances in Neural Information Processing Systems 13. MIT Press, 2001.</p>
<p>[7] M. Seeger, C. K. I. Williams, and N. D. Lawrence. Fast forward selection to speed up sparse Gaussian process regression. In Workshop on Artiﬁcial Intelligence and Statistics 9, 2003.</p>
<p>[8] S. S. Keerthi and W. Chu. A matching pursuit approach to sparse Gaussian process regression. In Advances in Neural Information Processing Systems 18. MIT Press, 2006.</p>
<p>[9] E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances in Neural Information Processing Systems 18. MIT Press, 2006.</p>
<p>[10] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixture of local experts. Neural computation, 3:79–87, 1991.</p>
<p>[11] S. Waterhouse. Classiﬁcation and regression using mixtures of experts. PhD Theis, Department of Engineering, Cambridge University, 1997.</p>
<p>[12] C. M. Bishop and M. Svens´ n. Bayesian hierarchical mixtures of experts. In Proc. Uncertainty in Artiﬁcial e Intelligence, 2003.</p>
<p>[13] V. Tresp. Mixtures of Gaussian processes. In Advances in Neural Information Processing Systems 13. MIT Press, 2001.</p>
<p>[14] B. W. Silverman. Some aspects of the spline smoothing approach to non-parametric regression curve ﬁtting. J. Royal. Stat. Society. B, 47(1):1–52, 1985.</p>
<p>[15] C. E. Rasmussen. The inﬁnite Gaussian mixture model. In Advances in Neural Information Processing Systems 12. MIT Press, 2000.</p>
<p>[16] L. Csat´ and M. Opper. Sparse on-line Gaussian processes. Neural Computation, 14(3):641–668, 2002. o  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
