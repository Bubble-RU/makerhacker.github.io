<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-177" href="../nips2008/nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">nips2008-177</a> <a title="nips-2008-177-reference" href="#">nips2008-177-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</h1>
<br/><p>Source: <a title="nips-2008-177-pdf" href="http://papers.nips.cc/paper/3397-particle-filter-based-policy-gradient-in-pomdps.pdf">pdf</a></p><p>Author: Pierre-arnaud Coquelin, Romain Deguest, Rémi Munos</p><p>Abstract: Our setting is a Partially Observable Markov Decision Process with continuous state, observation and action spaces. Decisions are based on a Particle Filter for estimating the belief state given past observations. We consider a policy gradient approach for parameterized policy optimization. For that purpose, we investigate sensitivity analysis of the performance measure with respect to the parameters of the policy, focusing on Finite Difference (FD) techniques. We show that the naive FD is subject to variance explosion because of the non-smoothness of the resampling procedure. We propose a more sophisticated FD method which overcomes this problem and establish its consistency. 1</p><br/>
<h2>reference text</h2><p>Andrieu, C., Doucet, A., Singh, S., & Tadic, V. (2004). Particle methods for change detection, identiﬁcation and control. Proceedings of the IEEE, 92, 423–438. Baxter, J., & Bartlett, P. (1999). Direct gradient-based reinforcement learning. Journal of Artiﬁcial Inteligence Reseach. Capp´ , O., Douc, R., & Moulines, E. (2005). Comparaison of resampling schemes for particle ﬁltering. 4th e International Symposium on Image and Signal Processing and Analysis. C´ rou, F., LeGland, F., & Newton, N. (2001). Stochastic particle methods for linear tangent ﬁltering equations, e 231–240. IOS Press, Amsterdam. Coquelin, P., Deguest, R., & Munos, R. (2008). Sensitivity analysis in particle ﬁlters. Application to policy optimization in POMDPs (Technical Report). INRIA, RR-6710. Del Moral, P. (2004). Feynman-kac formulae, genealogical and interacting particle systems with applications. Springer. Del Moral, P., & Miclo, L. (2000). Branching and interacting particle systems. approximations of feynman-kac formulae with applications to non-linear ﬁltering. S´ minaire de probabilit´ s de Strasbourg, 34, 1–145. e e Douc, R., & Moulines, E. (2008). Limit theorems for weighted samples with applications to sequential monte carlo methods. To appear in Annals of Statistics. Doucet, A., Freitas, N. D., & Gordon, N. (2001). Sequential monte carlo methods in practice. Springer. Doucet, A., & Tadic, V. (2003). Parameter estimation in general state-space models using particle methods. Ann. Inst. Stat. Math. Fichoud, J., LeGland, F., & Mevel, L. (2003). Particle-based methods for parameter estimation and tracking : numerical experiments (Technical Report 1604). IRISA. Fox, D., Thrun, S., Burgard, W., & Dellaert, F. (2001). Particle ﬁlters for mobile robot localization. Sequential Monte Carlo Methods in Practice. New York: Springer. Glasserman, P. (1991). Gradient estimation via perturbation analysis. Kluwer. Glasserman, P. (2003). Monte carlo methods in ﬁnancial engineering. Springer. Gordon, N., Salmond, D., & Smith, A. F. M. (1993). Novel approach to nonlinear and non-gaussian bayesian state estimation. Proceedings IEE-F (pp. 107–113). Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting in partially observable stochastic domains. Artiﬁcial Intelligence, 101, 99–134. Kitagawa, G. (1996). Monte-Carlo ﬁlter and smoother for non-Gaussian nonlinear state space models. J. Comput. Graph. Stat., 5, 1–25. Lovejoy, W. S. (1991). A survey of algorithmic methods for partially observable Markov decision processes. Annals of Operations Research, 28, 47–66. Poyadjis, G., Doucet, A., & Singh, S. (2005). Particle methods for optimal ﬁlter derivative: Application to parameter estimation. IEEE ICASSP. Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77, 257–286. Spall, J. C. (2000). Adaptive stochastic approximation by the simultaneous perturbation method. IEEE transaction on automatic control, 45, 1839–1853.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
