<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-127" href="../nips2008/nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">nips2008-127</a> <a title="nips-2008-127-reference" href="#">nips2008-127-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</h1>
<br/><p>Source: <a title="nips-2008-127-pdf" href="http://papers.nips.cc/paper/3559-logistic-normal-priors-for-unsupervised-probabilistic-grammar-induction.pdf">pdf</a></p><p>Author: Shay B. Cohen, Kevin Gimpel, Noah A. Smith</p><p>Abstract: We explore a new Bayesian model for probabilistic grammars, a family of distributions over discrete structures that includes hidden Markov models and probabilistic context-free grammars. Our model extends the correlated topic model framework to probabilistic grammars, exploiting the logistic normal distribution as a prior over the grammar parameters. We derive a variational EM algorithm for that model, and then experiment with the task of unsupervised grammar induction for natural language dependency parsing. We show that our model achieves superior results over previous models that use diﬀerent priors. 1</p><br/>
<h2>reference text</h2><p>[1] A. Ahmed and E. Xing. On tight approximate inference of the logistic normal topic admixture model. In Proc. of AISTATS, 2007.</p>
<p>[2] J. Aitchison and S. M. Shen. Logistic-normal distributions: some properties and uses. Biometrika, 67:261–272, 1980.</p>
<p>[3] H. Alshawi and A. L. Buchsbaum. Head automata and bilingual tiling: Translation with minimal representations. In Proc. of ACL, 1996.</p>
<p>[4] D. Blei and J. D. Laﬀerty. Correlated topic models. In Proc. of NIPS, 2006.</p>
<p>[5] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022, 2003.</p>
<p>[6] J. Eisner. Bilexical grammars and a cubic-time probabilistic parser. In Proc. of IWPT, 1997.</p>
<p>[7] J. Eisner. Transformational priors over grammars. In Proc. of EMNLP, 2002.</p>
<p>[8] J. R. Finkel, C. D. Manning, and A. Y. Ng. Solving the problem of cascading errors: Approximate Bayesian inference for linguistic annotation pipelines. In Proc. of EMNLP, 2006.</p>
<p>[9] H. Gaifman. Dependency systems and phrase-structure systems. Information and Control, 8, 1965.</p>
<p>[10] S. Goldwater and T. L. Griﬃths. A fully Bayesian approach to unsupervised part-ofspeech tagging. In Proc. of ACL, 2007.</p>
<p>[11] M. Johnson, T. L. Griﬃths, and S. Goldwater. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proc. of NAACL, 2007.</p>
<p>[12] M. I. Jordan, Z. Ghahramani, T. S. Jaakola, and L. K. Saul. An introduction to variational methods for graphical models. Machine Learning, 37(2):183–233, 1999.</p>
<p>[13] D. Klein and C. D. Manning. A generative constituent-context model for improved grammar induction. In Proc. of ACL, 2002.</p>
<p>[14] D. Klein and C. D. Manning. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proc. of ACL, 2004.</p>
<p>[15] K. Kurihara and T. Sato. Variational Bayesian grammar induction for natural language. In Proc. of ICGI, 2006.</p>
<p>[16] P. Liang, S. Petrov, M. Jordan, and D. Klein. The inﬁnite PCFG using hierarchical Dirichlet processes. In Proc. of EMNLP, 2007.</p>
<p>[17] M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19:313–330, 1993.</p>
<p>[18] N. A. Smith and J. Eisner. Annealing structural bias in multilingual weighted grammar induction. In Proc. of COLING-ACL, 2006. ´e</p>
<p>[19] L. Tesni`re. El´ment de Syntaxe Structurale. Klincksieck, 1959. e</p>
<p>[20] K. Toutanova and M. Johnson. A Bayesian LDA-based model for semi-supervised part-of-speech tagging. In Proc. of NIPS, 2007.  A  VB-EM for Logistic-Normal Probabilistic Grammars  The algorithm for variational inference with probabilistic grammars using logistic normal ˜l,(t) prior follows.7 Since the updates for ζk are fast, we perform them after each optimization routine in the E-step (suppressed for clarity). There are variational parameters for each training example, indexed by . We denote by B the variational bound in Eq. 8. Our stopping criterion relies on the likelihood of a held-out set (§5) using a point estimate of the model. Input: initial parameters µ(0) , Σ(0) , training data x, and development data x Output: learned parameters µ, Σ t←1; repeat E-step (for each training example = 1, ..., M ): repeat ,(t) optimize for µk , k = 1, ..., K: use conjugate gradient descent ” with “ ˜ ” ´ ` PNk “ ˜ (t−1) −1 (t−1) ∂L ˜ ˜ ˜ ˜2 = − (Σk ) )(µk − µk ) − fk,i + i =1 fk,i /ζk exp µk,i + σk,i /2 ; ˜ ∂µ ˜ i  k,i  ,(t)  optimize σk ˜  , k = 1,“ K: use Newton’s method for each coordinate (with σk,i > 0) with ..., ˜ PNk ˜ ” (t−1) ∂L ˜ µ ˜2 σ2 fk,i exp(˜k,i + σk,i /2)/2ζk + 1/2˜k,i ; = −Σk,ii /2 − i =1 ∂ σk,i ˜2 “ ” ˜ ,(t) , ∀k: ζ ,(t) ← PNk exp µ ,(t) + (˜ ,(t) )2 /2 ; ˜ ˜k,i σk,i update ζk k i=1 “ ” PNk ,(t) ,(t) ,(t) 1 ˜ ,(t) ˜ ,(t) ˜ ,(t) update ψ k , ∀k: ψk,i ← µk,i − log ζk + 1 − ˜ ,(t) i =1 exp µk,i + (˜k,i )2 /2 ; ˜ ˜ σ ζk  ,(t) compute expected counts ˜k , k = 1, ..., K: use an inside-outside algorithm to re-estimate f ˜ ,(t) ˜ expected counts fk,i in weighted grammar q(y) with weights eψ ; until B does not change ; M-step: Estimate µ(t) and Σ(t) using the following maximum likelihood closed form solution: P (t) ,(t) 1 µk,i ← M M µk,i =1 ˜ h i “P ” (t) ,(t) ,(t) (t) (t) (t) P ,(t) (t) P ,(t) M 1 Σk ← M ˜ ˜ σ ,(t) )2 δi,j + M µk,i µk,j − µk,j M µk,i − µk,i M µk,j k,i =1 µk,i µk,j + (˜ =1 ˜ =1 ˜ i,j  where δi,j = 1 if i = j and 0 otherwise. until likelihood of held-out data, p(x | E[µ(t) ]), decreases ; t ← t + 1; return µ(t) , Σ(t)  7  An implementation of the algorithm is available at http://www.ark.cs.cmu.edu/DAGEEM.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
