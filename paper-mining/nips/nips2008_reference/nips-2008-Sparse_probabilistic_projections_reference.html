<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>216 nips-2008-Sparse probabilistic projections</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-216" href="../nips2008/nips-2008-Sparse_probabilistic_projections.html">nips2008-216</a> <a title="nips-2008-216-reference" href="#">nips2008-216-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>216 nips-2008-Sparse probabilistic projections</h1>
<br/><p>Source: <a title="nips-2008-216-pdf" href="http://papers.nips.cc/paper/3380-sparse-probabilistic-projections.pdf">pdf</a></p><p>Author: Cédric Archambeau, Francis R. Bach</p><p>Abstract: We present a generative model for performing sparse probabilistic projections, which includes sparse principal component analysis and sparse canonical correlation analysis as special cases. Sparsity is enforced by means of automatic relevance determination or by imposing appropriate prior distributions, such as generalised hyperbolic distributions. We derive a variational Expectation-Maximisation algorithm for the estimation of the hyperparameters and show that our novel probabilistic approach compares favourably to existing techniques. We illustrate how the proposed method can be applied in the context of cryptoanalysis as a preprocessing tool for the construction of template attacks. 1</p><br/>
<h2>reference text</h2><p>[1] C. Archambeau, E. Peeters, F.-X. Standaert, and J.-J. Quisquater. Template attacks in principal subspaces. In L. Goubin and M. Matsui, editors, 8th International Workshop on Cryptographic Hardware and Embedded Systems(CHES), volume 4249 of Lecture Notes in Computer Science, pages 1–14. Springer, 2006.</p>
<p>[2] F. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical Report 688, Department of Statistics, University of California, Berkeley, 2005.</p>
<p>[3] O. Barndorff-Nielsen and R. Stelzer. Absolute moments of generalized hyperbolic distributions and approximate scaling of normal inverse Gaussian L´ vy processes. Scandinavian Journal of Statistics, e 32(4):617–637, 2005.</p>
<p>[4] P. J. Brown and J. E. Grifﬁn. Bayesian adaptive lassos with non-convex penalization. Technical Report CRiSM 07-02, Department of Statistics, University of Warwick, 2007.</p>
<p>[5] F. Caron and A. Doucet. Sparse bayesian nonparametric regression. In 25th International Conference on Machine Learning (ICML). ACM, 2008.</p>
<p>[6] A. d’Aspremont, E. L. Ghaoui, M. I. Jordan, and G. R. G. Lanckriet. A direct formulation for sparse PCA using semideﬁnite programming. SIAM Review, 49(3):434–48, 2007.</p>
<p>[7] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96:1348–1360, 2001.</p>
<p>[8] A. C. Faul and M. E. Tipping. Analysis of sparse Bayesian learning. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14 (NIPS), pages 383–389. The MIT Press, 2002.</p>
<p>[9] Z. Ghahramani and G. E. Hinton. The EM algorithm for mixtures of factor analyzers. Technical Report CRG-TR-96-1, Department of Computer Science, University of Toronto, 1996.</p>
<p>[10] D. Hardoon and J. Shawe-Taylor. Sparse canonical correlation analysis. Technical report, PASCAL EPrints, 2007.</p>
<p>[11] Wenbo Hu. Calibration of multivariate generalized hyperbolic distributions using the EM algorithm, with applications in risk management, portfolio optimization and portfolio credit risk. PhD thesis, Florida State University, United States of America, 2005.</p>
<p>[12] B. Jørgensen. Statistical Properties of the Generalized Inverse Gaussian Distribution. Springer-Verlag, 1982.</p>
<p>[13] A. Klami and S. Kaski. Local dependent components. In Z. Ghahramani, editor, 24th International Conference on Machine Learning (ICML), pages 425–432. Omnipress, 2007.</p>
<p>[14] D. J. C. MacKay. Bayesian methods for backprop networks. In E. Domany, J.L. van Hemmen, and K. Schulten, editors, Models of Neural Networks, III, pages 211–254. 1994.</p>
<p>[15] R. M. Neal and G. E. Hinton. A view of the EM algorithm that justiﬁes incremental, sparse, and other variants. In M. I. Jordan, editor, Learning in Graphical Models, pages 355–368. The MIT press, 1998.</p>
<p>[16] C. D. Sigg and J. M. Buhmann. Expectation-maximization for sparse and non-negative PCA. In 25th International Conference on Machine Learning (ICML). ACM, 2008.</p>
<p>[17] R. Tibshirani. Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society B, 58:267–288, 1996.</p>
<p>[18] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society B, 61:611–622, 1999.</p>
<p>[19] D. Torres, D. Turnbull, B. K. Sriperumbudur, L. Barrington, and G.Lanckriet. Finding musically meaningful words using sparse CCA. In NIPS workshop on Music, Brain and Cognition, 2007.</p>
<p>[20] H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis. Journal of Computational and Graphical Statistics, 15(2):265–286, 2006.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
