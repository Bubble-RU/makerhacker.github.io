<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>139 nips-2008-Modeling the effects of memory on human online sentence processing with particle filters</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-139" href="../nips2008/nips-2008-Modeling_the_effects_of_memory_on_human_online_sentence_processing_with_particle_filters.html">nips2008-139</a> <a title="nips-2008-139-reference" href="#">nips2008-139-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>139 nips-2008-Modeling the effects of memory on human online sentence processing with particle filters</h1>
<br/><p>Source: <a title="nips-2008-139-pdf" href="http://papers.nips.cc/paper/3573-modeling-the-effects-of-memory-on-human-online-sentence-processing-with-particle-filters.pdf">pdf</a></p><p>Author: Roger P. Levy, Florencia Reali, Thomas L. Griffiths</p><p>Abstract: Language comprehension in humans is signiﬁcantly constrained by memory, yet rapid, highly incremental, and capable of utilizing a wide range of contextual information to resolve ambiguity and form expectations about future input. In contrast, most of the leading psycholinguistic models and ﬁelded algorithms for natural language parsing are non-incremental, have run time superlinear in input length, and/or enforce structural locality constraints on probabilistic dependencies between events. We present a new limited-memory model of sentence comprehension which involves an adaptation of the particle ﬁlter, a sequential Monte Carlo method, to the problem of incremental parsing. We show that this model can reproduce classic results in online sentence comprehension, and that it naturally provides the ﬁrst rational account of an outstanding problem in psycholinguistics, in which the preferred alternative in a syntactic ambiguity seems to grow more attractive over time even in the absence of strong disambiguating information. 1</p><br/>
<h2>reference text</h2><p>[1] C. D. Manning and H. Sch¨ tze. Foundations of Statistical Natural Language Processing. MIT Press, 1999. u</p>
<p>[2] D. Jurafsky and J. H. Martin. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice-Hall, second edition, 2008.</p>
<p>[3] D. Jurafsky. Probabilistic modeling in psycholinguistics: Linguistic comprehension and production. In Rens Bod, Jennifer Hay, and Stefanie Jannedy, editors, Probabilistic Linguistics, pages 39–95. MIT Press, 2003.</p>
<p>[4] M. K. Tanenhaus, M. J. Spivey-Knowlton, K. Eberhard, and J. C. Sedivy. Integration of visual and linguistic information in spoken language comprehension. Science, 268:1632–1634, 1995.</p>
<p>[5] G. T. Altmann and Y. Kamide. Incremental interpretation at verbs: restricting the domain of subsequent reference. Cognition, 73(3):247–264, 1999.</p>
<p>[6] D. Jurafsky. A probabilistic model of lexical and syntactic access and disambiguation. Cognitive Science, 20(2):137–194, 1996.</p>
<p>[7] N. Chater, M. Crocker, and M. Pickering. The rational analysis of inquiry: The case for parsing. In M. Oaksford and N. Chater, editors, Rational models of cognition. Oxford, 1998.</p>
<p>[8] J. Hale. A probabilistic Earley parser as a psycholinguistic model. In Proceedings of NAACL, volume 2, pages 159–166, 2001.</p>
<p>[9] R. Levy. Expectation-based syntactic comprehension. Cognition, 106:1126–1177, 2008.</p>
<p>[10] J. Earley. An efﬁcient context-free parsing algorithm. Communications of the ACM, 13(2):94–102, 1970.</p>
<p>[11] A. Stolcke. An efﬁcient probabilistic context-free parsing algorithm that computes preﬁx probabilities. Computational Linguistics, 21(2):165–201, 1995.</p>
<p>[12] M.-J. Nederhof. The computational complexity of the correct-preﬁx property for TAGs. Computational Linguistics, 25(3):345–360, 1999.</p>
<p>[13] L. Huang and D. Chiang. Better k-best parsing. In Proceedings of the International Workshop on Parsing Technologies, 2005.</p>
<p>[14] M. Johnson, T. L. Grifﬁths, and S. Goldwater. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, 2007.</p>
<p>[15] W. Tabor and S. Hutchins. Evidence for self-organized sentence processing: Digging in effects. Journal of Experimental Psychology: Learning, Memory, and Cognition,, 30(2):431–450, 2004.</p>
<p>[16] B. Roark. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276, 2001.</p>
<p>[17] M. Collins and B. Roark. Incremental parsing with the perceptron algorithm. In Proceedings of the ACL, 2004.</p>
<p>[18] J. Henderson. Lookahead in deterministic left-corner parsing. In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, 2004.</p>
<p>[19] A. Doucet, N. de Freitas, and N. Gordon, editors. Sequential Monte Carlo Methods in Practice. Springer, 2001.</p>
<p>[20] A. N. Sanborn, T. L. Grifﬁths, and D. J. Navarro. A more rational model of categorization. In Proceedings of the 28th Annual Conference of the Cognitive Science Society, Mahwah, NJ, 2006. Erlbaum.</p>
<p>[21] N. Daw and A. Courville. The pigeon as particle ﬁlter. In Advances in Neural Information Processing Systems 20, Cambridge, MA, 2008. MIT Press.</p>
<p>[22] A. Doucet, N. de Freitas, K. Murphy, and S. Russell. Rao-Blackwellised particle ﬁltering for dynamic Bayesian networks. In Advances in Neural Information Processing Systems, 2000.</p>
<p>[23] N. Smith and R. Levy. Optimal processing times in reading: a formal model and empirical investigation. In Proceedings of the 30th Annual Meeting of the Cognitive Science Society, 2008.</p>
<p>[24] M. C. MacDonald. Probabilistic constraints and syntactic ambiguity resolution. Language and Cognitive Processes, 9(2):157–201, 1994.</p>
<p>[25] M. J. Spivey and M. K. Tanenhaus. Syntactic ambiguity resolution in discourse: Modeling the effects of referential content and lexical frequency. Journal of Experimental Psychology: Learning, Memory, and Cognition, 24(6):1521–1543, 1998.</p>
<p>[26] L. Frazier and K. Rayner. Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences. Cognitive Psychology, 14:178–210, 1982.</p>
<p>[27] F. Ferreira and J. M. Henderson. Recovery from misanalyses of garden-path sentences. Journal of Memory and Language, 31:725–745, 1991.</p>
<p>[28] N. Chopin. A sequential particle ﬁlter method for static models. Biometrika, 89:539–552, 2002.</p>
<p>[29] P. Sturt, M. J. Pickering, and M. W. Crocker. Structural change and reanalysis difﬁculty in language comprehension. Journal of Memory and Language, 40:143–150, 1999.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
