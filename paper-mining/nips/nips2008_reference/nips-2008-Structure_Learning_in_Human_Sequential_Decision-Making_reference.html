<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>223 nips-2008-Structure Learning in Human Sequential Decision-Making</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-223" href="../nips2008/nips-2008-Structure_Learning_in_Human_Sequential_Decision-Making.html">nips2008-223</a> <a title="nips-2008-223-reference" href="#">nips2008-223-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>223 nips-2008-Structure Learning in Human Sequential Decision-Making</h1>
<br/><p>Source: <a title="nips-2008-223-pdf" href="http://papers.nips.cc/paper/3615-structure-learning-in-human-sequential-decision-making.pdf">pdf</a></p><p>Author: Daniel Acuna, Paul R. Schrater</p><p>Abstract: We use graphical models and structure learning to explore how people learn policies in sequential decision making tasks. Studies of sequential decision-making in humans frequently ﬁnd suboptimal performance relative to an ideal actor that knows the graph model that generates reward in the environment. We argue that the learning problem humans face also involves learning the graph structure for reward generation in the environment. We formulate the structure learning problem using mixtures of reward models, and solve the optimal action selection problem using Bayesian Reinforcement Learning. We show that structure learning in one and two armed bandit problems produces many of the qualitative behaviors deemed suboptimal in previous studies. Our argument is supported by the results of experiments that demonstrate humans rapidly learn and exploit new reward structure. 1</p><br/>
<h2>reference text</h2><p>[1] Pascal Poupart, Nikos Vlassis, Jesse Hoey, and Kevin Regan. An analytic solution to discrete bayesian reinforcement learning. In 23rd International Conference on Machine Learning, Pittsburgh, Penn, 2006.</p>
<p>[2] Richard Ernest Bellman. Dynamic programming. Princeton University Press, Princeton, 1957.</p>
<p>[3] Noah Gans, George Knox, and Rachel Croson. Simple models of discrete choice and their performance in bandit experiments. Manufacturing and Service Operations Management, 9(4):383–408, 2007.</p>
<p>[4] C.M. Anderson. Behavioral Models of Strategies in Multi-Armed Bandit Problems. PhD thesis, Pasadena, CA., 2001.</p>
<p>[5] Jeffrey Banks, David Porter, and Mark Olson. An experimental analysis of the bandit problem. Economic Theory, 10(1):55–77, 1997.</p>
<p>[6] R. J. Meyer and Y. Shi. Sequential choice under ambiguity: Intuitive solutions to the armedbandit problem. Management Science, 41:817–83, 1995.</p>
<p>[7] N Vulkan. An economist’s perspective on probability matching. Journal of Economic Surveys, 14:101–118, 2000.</p>
<p>[8] Yvonne Brackbill and Anthony Bravos. Supplementary report: The utility of correctly predicting infrequent events. Journal of Experimental Psychology, 64(6):648–649, 1962.</p>
<p>[9] W Edwards. Probability learning in 1000 trials. Journal of Experimental Psychology, 62:385– 394, 1961.</p>
<p>[10] W Edwards. Reward probability, amount, and information as determiners of sequential twoalternative decisions. J Exp Psychol, 52(3):177–88, 1956.</p>
<p>[11] E. Fantino and A Esfandiari. Probability matching: Encouraging optimal responding in humans. Canadian Journal of Experimental Psychology, 56:58 – 63, 2002.</p>
<p>[12] Timothy E J Behrens, Mark W Woolrich, Mark E Walton, and Matthew F S Rushworth. Learning the value of information in an uncertain world. Nat Neurosci, 10(9):1214–1221, 2007.</p>
<p>[13] N. D. Daw, J. P. O’Doherty, P. Dayan, B. Seymour, and R. J. Dolan. Cortical substrates for exploratory decisions in humans. Nature, 441(7095):876–879, 2006.</p>
<p>[14] JS Banks and RK Sundaram. A class of bandit problems yielding myopic optimal strategies. Journal of Applied Probability, 29(3):625–632, 1992.</p>
<p>[15] John Gittins and You-Gan Wang. The learning component of dynamic allocation indices. The Annals of Statistics, 20(2):1626–1636, 1992.</p>
<p>[16] J. C. Gittins and D. M. Jones. A dynamic allocation index for the sequential design of experiments. Progress in Statistics, pages 241–266, 1974.</p>
<p>[17] Joshua B. Tenenbaum, Thomas L. Grifﬁths, and Charles Kemp. Theory-based bayesian models of inductive learning and reasoning. Trends in Cognitive Sciences, 10(7):309–318, 2006.</p>
<p>[18] Joshua B. Tenenbaum and Thomas L. Grifﬁths. Structure learning in human causal induction. NIPS 13, pages 59–65, 2000.</p>
<p>[19] A. C. Courville, N. D. Daw, G. J. Gordon, and D. S. Touretzky. Model uncertainty in classical conditioning. Advances in Neural Information Processing Systems, (16):977–986, 2004.</p>
<p>[20] Daniel Acuna and Paul Schrater. Bayesian modeling of human sequential decision-making on the multi-armed bandit problem. In CogSci, 2008.</p>
<p>[21] Michael D. Lee. A hierarchical bayesian model of human decision-making on an optimal stopping problem. Cognitive Science: A Multidisciplinary Journal, 30:1 – 26, 2006.</p>
<p>[22] Ido Erev and Alvin E. Roth. Predicting how people play games: Reinforcement learning in experimental games with unique, mixed strategy equilibria. The American Economic Review, 88(4):848–881, 1998.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
