<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>193 nips-2008-Regularized Co-Clustering with Dual Supervision</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-193" href="../nips2008/nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">nips2008-193</a> <a title="nips-2008-193-reference" href="#">nips2008-193-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>193 nips-2008-Regularized Co-Clustering with Dual Supervision</h1>
<br/><p>Source: <a title="nips-2008-193-pdf" href="http://papers.nips.cc/paper/3562-regularized-co-clustering-with-dual-supervision.pdf">pdf</a></p><p>Author: Vikas Sindhwani, Jianying Hu, Aleksandra Mojsilovic</p><p>Abstract: By attempting to simultaneously partition both the rows (examples) and columns (features) of a data matrix, Co-clustering algorithms often demonstrate surprisingly impressive performance improvements over traditional one-sided row clustering techniques. A good clustering of features may be seen as a combinatorial transformation of the data matrix, effectively enforcing a form of regularization that may lead to a better clustering of examples (and vice-versa). In many applications, partial supervision in the form of a few row labels as well as column labels may be available to potentially assist co-clustering. In this paper, we develop two novel semi-supervised multi-class classiﬁcation algorithms motivated respectively by spectral bipartite graph partitioning and matrix approximation formulations for co-clustering. These algorithms (i) support dual supervision in the form of labels for both examples and/or features, (ii) provide principled predictive capability on out-of-sample test data, and (iii) arise naturally from the classical Representer theorem applied to regularization problems posed on a collection of Reproducing Kernel Hilbert Spaces. Empirical results demonstrate the effectiveness and utility of our algorithms. 1</p><br/>
<h2>reference text</h2><p>[1] A. Banerjee, I. Dhillon, J. Ghosh, S.Merugu, and D.S. Modha. A generalized maximum entropy approach to bregman co-clustering and matrix approximation. JMLR, 8:1919–1986, 2007.</p>
<p>[2] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. JMLR, 7:2399–2434, 2006.</p>
<p>[3] O. Chapelle, B. Sch¨ lkopf, and A. Zien, editors. Semi-Supervised Learning. MIT Press, 2006. o</p>
<p>[4] F. Chung, editor. Spectral Graph Theory. AMS, 1997.</p>
<p>[5] I. Dhillon. Co-clustering documents and words using bipartite spectral graph partitioning. In KDD, 2001.</p>
<p>[6] C. Ding, X. He, and H.D. Simon. On the equivalence of nonnegative matrix factorization and spectral clustering. In SDM, 2005.</p>
<p>[7] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal nonnegative matrix tri-factorizations for clustering. In KDD, 2006.</p>
<p>[8] G. Druck, G. Mann, and A. McCallum. Learning from labeled features using generalized expectation criteria. In SIGIR, 2008.</p>
<p>[9] J. Gardiner, Laub A.J, Amato J.J, and Moler C.B. Solution of the Sylvester matrix equation AXBT + CXDT = E. ACM Transactions on Mathematical Software, 18(2):223–231, 1992.</p>
<p>[10] D. Harville. Matrix Algebra From a Statistician’s Perspective. Springer, New York, 1997.</p>
<p>[11] T.M. Huang and V. Kecman. Semi-supervised learning from unbalanced labeled data an improvement. Lecture Notes in Computer Science, 3215:765–771, 2004.</p>
<p>[12] A. Langville, C. Meyer, and R. Albright. Initializations for the non-negative matrix factorization. In KDD, 2006.</p>
<p>[13] T. Li and C. Ding. The relationships among various nonnegative matrix factorization methods for clustering. In ICDM, 2006.</p>
<p>[14] V. Sindhwani and P. Melville. Document-word co-regularization for semi-supervised sentiment analysis. In ICDM, 2008.</p>
<p>[15] N. Slonim and N. Tishby. Document clustering using word clusters via the information bottleneck method. In SIGIR, 2000.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
