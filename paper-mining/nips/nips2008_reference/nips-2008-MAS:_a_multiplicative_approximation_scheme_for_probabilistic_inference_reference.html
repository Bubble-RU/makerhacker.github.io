<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-129" href="../nips2008/nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">nips2008-129</a> <a title="nips-2008-129-reference" href="#">nips2008-129-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</h1>
<br/><p>Source: <a title="nips-2008-129-pdf" href="http://papers.nips.cc/paper/3479-mas-a-multiplicative-approximation-scheme-for-probabilistic-inference.pdf">pdf</a></p><p>Author: Ydo Wexler, Christopher Meek</p><p>Abstract: We propose a multiplicative approximation scheme (MAS) for inference problems in graphical models, which can be applied to various inference algorithms. The method uses -decompositions which decompose functions used throughout the inference procedure into functions over smaller sets of variables with a known error . MAS translates these local approximations into bounds on the accuracy of the results. We show how to optimize -decompositions and provide a fast closed-form solution for an L2 approximation. Applying MAS to the Variable Elimination inference algorithm, we introduce an algorithm we call DynaDecomp which is extremely fast in practice and provides guaranteed error bounds on the result. The superior accuracy and efﬁciency of DynaDecomp is demonstrated. 1</p><br/>
<h2>reference text</h2><p>[1]</p>
<p>[2]</p>
<p>[3]</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]</p>
<p>[19]</p>
<p>[20]</p>
<p>[21]</p>
<p>[22]</p>
<p>[23]</p>
<p>[24]</p>
<p>[25]</p>
<p>[26]</p>
<p>[27]</p>
<p>[28]  Evaluation of probabilistic inference systems: http://tinyurl.com/3k9l4b, 2006. Bidyuk and Dechter. An anytime scheme for bounding posterior beliefs. AAAI 2006. Bidyuk and Dechter. Improving bound propagation. In ECAI 342–346, 2006. Cheng and Druzdzel. AIS-BN: An adaptive importance sampling algorithm for evidential reasoning in large Bayesian networks. JAIR 13:155–188, 2000. Choi and Darwiche. A variational approach for approximating Bayesian networks by edge deletion. UAI 2006. Cooper. The computational complexity of probabilistic inference using Bayesian belief networks. AI 42(2-3):393–405, 1990. Dagum and Luby. Approximating probabilistic inference in Bayesian belief networks is NP-hard. AI, 60(1):141–153, 1993. Darwiche, Chan, and Choi. On Bayesian network approximation by edge deletion. UAI 2005. Dechter. Bucket elimination: A unifying framework for reasoning. AI 113(1-2):41–85, 1999. Dechter and Rish. Mini-buckets:A general scheme for bounded inference. J.ACM 50:107–153, 2003. W. Freeman, W. Pasztor, and O. Carmichael. Learning low-level vision. IJCV 40:25–47, 2000. Geiger, Meek, and Wexler. A variational inference procedure allowing internal structure for overlapping clusters and deterministic constraints. JAIR 27:1–23, 2006. Henrion. Propagating uncertainty in bayesian networks by probabilistic logic sampling. UAI 1988. Jensen, Lauritzen, and Olesen. Bayesian updating in causal probabilistic networks by local computations. Comp. Stat. Quaterly 4:269–282, 1990. Jojic, Jojic, Meek, Geiger, Siepel, Haussler, and Heckerman. Efﬁcient approximations for learning phylogenetic hmm models from data. ISMB 2004. Jordan, Ghahramani, Jaakkola, and Saul. An introduction to variational methods for graphical models. Machine Learning 37(2):183–233, 1999. Mateescu, Dechter, and Kask. Partition-based anytime approximation for belief updating. 2001. Boyd and Vandenberghe. Convex Optimization. Cambridge University Press, 2004. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, 1988. Shachter, D’Ambrosio, and Del Favero. Symbolic probabilistic inference in belief networks. AAAI 1990. Shachter and Peot. Simulation approaches to general probabilistic inference on belief networks.UAI 1989. Siepel and Haussler. Combining phylogenetic and HMMs in biosequence analysis. RECOMB 2003. Wainwright, Jaakkola, and Willsky. A new class of upper bounds on the log partition function. IEEE Trans. Info. Theory 51(7):2313–2335, 2005. Weiss. Belief propagation and revision in networks with loops. Technical Report AIM-1616, 1997. Wexler and Geiger. Importance sampling via variational optimization. UAI 2007. Wexler and Geiger. Variational upper bounds for probabilistic phylogenetic models. RECOMB 2007. Wexler and Meek. Inference for multiplicative models. UAI 2008. Xing, Jordan, and Russell. Graph partition strategies for generalized mean ﬁeld inference. UAI 2004.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
