<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>109 nips-2008-Interpreting the neural code with Formal Concept Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-109" href="../nips2008/nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">nips2008-109</a> <a title="nips-2008-109-reference" href="#">nips2008-109-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>109 nips-2008-Interpreting the neural code with Formal Concept Analysis</h1>
<br/><p>Source: <a title="nips-2008-109-pdf" href="http://papers.nips.cc/paper/3421-interpreting-the-neural-code-with-formal-concept-analysis.pdf">pdf</a></p><p>Author: Dominik Endres, Peter Foldiak</p><p>Abstract: We propose a novel application of Formal Concept Analysis (FCA) to neural decoding: instead of just trying to ﬁgure out which stimulus was presented, we demonstrate how to explore the semantic relationships in the neural representation of large sets of stimuli. FCA provides a way of displaying and interpreting such relationships via concept lattices. We explore the effects of neural code sparsity on the lattice. We then analyze neurophysiological data from high-level visual cortical area STSa, using an exact Bayesian approach to construct the formal context needed by FCA. Prominent features of the resulting concept lattices are discussed, including hierarchical face representation and indications for a product-of-experts code in real neurons. 1</p><br/>
<h2>reference text</h2><p>[1] A. P. Georgopoulos, A. B. Schwartz, and R. E. Kettner. Neuronal population coding of movement direction. Science, 233(4771):1416–1419, 1986.</p>
<p>[2] P F¨ ldi´ k. The ’Ideal Homunculus’: Decoding neural population responses by Bayesian infero a ence. Perception, 22 suppl:43, 1993.</p>
<p>[3] MW Oram, P F¨ ldi´ k, DI Perrett, and F Sengpiel. The ’Ideal Homunculus’: decoding neural o a population signals. Trends In Neurosciences, 21:259–265, June 1998.</p>
<p>[4] R. Q. Quiroga, L. Reddy, C. Koch, and I. Fried. Decoding Visual Inputs From Multiple Neurons in the Human Temporal Lobe. J Neurophysiol, 98(4):1997–2007, 2007.</p>
<p>[5] OR Duda, PE Hart, and DG Stork. Pattern classiﬁcation. John Wiley & Sons, New York, Chichester, 2001.</p>
<p>[6] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons, New York, 1991.</p>
<p>[7] P F¨ ldi´ k. Sparse neural representation for semantic indexing. In XIII Conference o a of the European Society of Cognitive Psychology (ESCOP-2003), 2003. http://www.standrews.ac.uk/∼pf2/escopill2.pdf.</p>
<p>[8] R. Wille. Restructuring lattice theory: an approach based on hierarchies of concepts. In I. Rival, editor, Ordered sets, pages 445–470. Reidel, Dordrecht-Boston, 1982.</p>
<p>[9] Bernhard Ganter and Rudolf Wille. Formal Concept Analysis: Mathematical foundations. Springer, 1999.</p>
<p>[10] B. Ganter, G. Stumme, and R. Wille, editors. Formal Concept Analysis, Foundations and Applications, volume 3626 of Lecture Notes in Computer Science. Springer, 2005.</p>
<p>[11] U. Priss. Formal concept analysis in information science. Annual Review of Information Science and Technology, 40:521–543, 2006.</p>
<p>[12] P F¨ ldi´ k. Sparse coding in the primate cortex. In Michael A Arbib, editor, The Handbook of o a Brain Theory and Neural Networks, pages 1064–1068. MIT Press, second edition, 2002.</p>
<p>[13] P F¨ ldi´ k and D Endres. o a Sparse coding. Scholarpedia, 3(1):2984, 2008. http://www.scholarpedia.org/article/Sparse coding.</p>
<p>[14] P F¨ ldi´ k. Forming sparse representations by local anti-Hebbian learning. Biological Cybero a netics, 64:165–170, 1990.</p>
<p>[15] B. A Olshausen, D. J Field, and A Pelah. Sparse coding with an overcomplete basis set: a strategy employed by V1. Vision Res., 37(23):3311–3325, 1997.</p>
<p>[16] Eero P Simoncelli and Bruno A Olshausen. Natural image statistics and neural representation. Annual Review of Neuroscience, 24:1193–1216, 2001.</p>
<p>[17] ET Rolls and A Treves. The relative advantages of sparse versus distributed encoding for neuronal networks in the brain. Network, 1:407–421, 1990.</p>
<p>[18] P Dayan and LF Abbott. Theoretical Neuroscience. MIT Press, London, Cambridge, 2001.</p>
<p>[19] J.P. Jones and L. A. Palmer. An evaluation of the two-dimensional Gabor ﬁlter model of simple receptive ﬁelds in cat striate cortex. Journal of Neurophysiology, 58(6):1233–1258, 1987.</p>
<p>[20] D. L. Ringach. Spatial structure and symmetry of simple-cell receptive ﬁelds in macaque primary visual cortex. Journal of Neurophysiology, 88:455–463, 2002.</p>
<p>[21] P F¨ ldi´ k, D Xiao, C Keysers, R Edwards, and DI Perrett. Rapid serial visual presentation o a for the determination of neural selectivity in area STSa. Progress in Brain Research, pages 107–116, 2004.</p>
<p>[22] M. W. Oram and D. I. Perrett. Time course of neural responses discriminating different views of the face and head. Journal of Neurophysiology, 68(1):70–84, 1992.</p>
<p>[23] R. Wille and F. Lehmann. A triadic approach to formal concept analysis. In G. Ellis, R. Levinson, W. Rich, and J. F. Sowa, editors, Conceptual structures: applications, implementation and theory, pages 32–43. Springer, Berlin-Heidelberg-New York, 1995.</p>
<p>[24] D. Endres. Bayesian and Information-Theoretic Tools for Neuroscience. PhD thesis, School of Psychology, University of St. Andrews, U.K., 2006. http://hdl.handle.net/10023/162.</p>
<p>[25] GE Hinton. Products of experts. In Ninth International Conference on Artiﬁcial Neural Networks ICANN 99, number 470 in ICANN, 1999.</p>
<p>[26] R Kiani, H Esteky, K Mirpour, and K Tanaka. Object category structure in response patterns of neuronal population in monkey inferior temporal cortex. Journal of Neurophysiology, 97(6):4296–4309, April 2007.</p>
<p>[27] K. N. Kay, T. Naselaris, R. J. Prenger, and J. L. Gallant. Identifying natural images from human brain activity. Nature, 452:352–255, 2008. http://dx.doi.org/10.1038/nature06713.</p>
<p>[28] D. Endres and P. F¨ ldi´ k. Exact Bayesian bin classiﬁcation: a fast alternative to bayesian o a classiﬁcation and its application to neural response analysis. Journal of Computational Neuroscience, 24(1):24–35, 2008. DOI: 10.1007/s10827-007-0039-5.  A  Method of Bayesian thresholding  A standard way of obtaining binary responses from neurons is thresholding the spike count within a certain time window. This is a relatively straightforward task, if the stimuli are presented well separated in time and a lot of trials per stimulus are available. Then latencies and response offsets are often clearly discernible and thus choosing the time window is not too difﬁcult. However, under RSVP conditions with few trials per stimulus, response separation becomes more tricky, as the responses to subsequent stimuli will tend to follow each other without an intermediate return to baseline activity. Moreover, neural resposes tend to be rather noisy. We will therefore employ a simpliﬁed version of the generative Bayesian Bin classiﬁcation algorithm (BBCa) [28], which was shown to perform well on RSVP data [24]. BBCa was designed for the purpose of inferring stimulus labels g from a continuous-valued, scalar measure z of a neural response. The range of z is divided into a number of contiguous bins. Within each bin, the observation model for the g is a Bernoulli scheme with a Dirichlet prior over its parameters. It is shown in [28] that one can iterate/integrate over all possible bin boundary conﬁgurations efﬁciently, thus making exact Bayesian inference feasible. We make two simpliﬁcations to BBCa: 1) z is discrete, because we are counting spikes and 2) we use models with only 1 bin boundary in the range of z. The bin membership of a given neural response can then serve as the binary attribute required for FCA, since BBCa weighs bin conﬁgurations by their classiﬁcation (i.e. stimulus label decoding) performance. We proceed in a straight Bayesian fashion: since the bin membership is the only variable we are interested in, all other parameters (counting window size and position, class membership probabilities, bin boundaries) are marginalized. This minimizes the risk of spurious results due to ”contrived” information (i.e. choices of parameters) made at some stage of the inference process. Afterwards, the probability that the response belongs to the upper bin is thresholded at a probability of 0.5. BBCa can also be used for model comparison. Running the algorithm with no bin boundaries in the range of z effectively yields the probability of the data given the ”null hypothesis” H0 : z does not contain any information about g. We can then compare it against the alternative hypothesis described above (i.e. the information which bin z is in tells us something about g) to determine whether the cell has responded at all.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
