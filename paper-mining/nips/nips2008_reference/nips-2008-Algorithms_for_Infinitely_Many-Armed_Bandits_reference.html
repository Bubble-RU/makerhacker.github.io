<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 nips-2008-Algorithms for Infinitely Many-Armed Bandits</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-17" href="../nips2008/nips-2008-Algorithms_for_Infinitely_Many-Armed_Bandits.html">nips2008-17</a> <a title="nips-2008-17-reference" href="#">nips2008-17-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>17 nips-2008-Algorithms for Infinitely Many-Armed Bandits</h1>
<br/><p>Source: <a title="nips-2008-17-pdf" href="http://papers.nips.cc/paper/3452-algorithms-for-infinitely-many-armed-bandits.pdf">pdf</a></p><p>Author: Yizao Wang, Jean-yves Audibert, Rémi Munos</p><p>Abstract: We consider multi-armed bandit problems where the number of arms is larger than the possible number of experiments. We make a stochastic assumption on the mean-reward of a new selected arm which characterizes its probability of being a near-optimal arm. Our assumption is weaker than in previous works. We describe algorithms based on upper-conﬁdence-bounds applied to a restricted set of randomly selected arms and provide upper-bounds on the resulting expected regret. We also derive a lower-bound which matches (up to a logarithmic factor) the upper-bound in some cases. 1</p><br/>
<h2>reference text</h2><p>[1] R. Agrawal. The continuum-armed bandit problem. SIAM J. Control and Optimization, 33:1926–1951, 1995.</p>
<p>[2] J.-Y. Audibert, R. Munos, and C. Szepesvári. Tuning bandit algorithms in stochastic environments. In M. Hutter, R. A. Servedio, and E. Takimoto, editors, ALT, volume 4754 of Lecture Notes in Computer Science, pages 150–165. Springer, 2007.</p>
<p>[3] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2/3):235–256, 2002.</p>
<p>[4] P. Auer, R. Ortner, and C. Szepesvári. Improved rates for the stochastic continuum-armed bandit problem. 20th COLT, San Diego, CA, USA, 2007.</p>
<p>[5] D. A. Berry, R. W. Chen, A. Zame, D. C. Heath, and L. A. Shepp. Bandit problems with inﬁnitely many arms. The Annals of Statistics, 25(5):2103–2116, 1997.</p>
<p>[6] R. Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In NIPS-2004, 2004.</p>
<p>[7] R. Kleinberg, A. Slivkins, and E. Upfal. Multi-armed bandit problems in metric spaces. In Proceedings of the 40th ACM Symposium on Theory of Computing, 2008.</p>
<p>[8] T. L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in Applied Mathematics, 6:4–22, 1985.</p>
<p>[9] O. Teytaud, S. Gelly, and M. Sebag. Anytime many-armed bandit. Conférence francophone sur l’Apprentissage automatique (CAp) Grenoble, France, 2007.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
