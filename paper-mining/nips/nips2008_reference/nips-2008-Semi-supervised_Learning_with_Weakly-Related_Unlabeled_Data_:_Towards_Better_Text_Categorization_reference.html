<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-205" href="../nips2008/nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">nips2008-205</a> <a title="nips-2008-205-reference" href="#">nips2008-205-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</h1>
<br/><p>Source: <a title="nips-2008-205-pdf" href="http://papers.nips.cc/paper/3488-semi-supervised-learning-with-weakly-related-unlabeled-data-towards-better-text-categorization.pdf">pdf</a></p><p>Author: Liu Yang, Rong Jin, Rahul Sukthankar</p><p>Abstract: The cluster assumption is exploited by most semi-supervised learning (SSL) methods. However, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classiﬁcation. In such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classiﬁcation accuracy becomes a challenge. We introduce “Semi-supervised Learning with Weakly-Related Unlabeled Data” (SSLW), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information. Although the SSLW could improve a wide range of classiﬁcation tasks, in this paper, we focus on text categorization with a small training pool. The key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent. To this end, SSLW estimates the optimal wordcorrelation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents. For empirical evaluation, we present a direct comparison with a number of stateof-the-art methods for inductive semi-supervised learning and text categorization. We show that SSLW results in a signiﬁcant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test domain.</p><br/>
<h2>reference text</h2><p>[1] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Technical report, Univ. of Chicago, Dept. of Comp. Sci., 2004.</p>
<p>[2] K. Bennett and A. Demiriz. Semi-supervised support vector machines. In Proc. NIPS, 1998.</p>
<p>[3] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge University Press, 2004.</p>
<p>[4] C. Burges. A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery, 2(2), 1998.</p>
<p>[5] M. Fazel, H. Hindi, and S. Boyd. A rank minimization heuristic with application to minimum order system approximation. In Proc. American Control Conf., 2001.</p>
<p>[6] P. O. Hoyer. Non-negative matrix factorization with sparseness constraints. J. Mach. Learn. Res., 5, 2004.</p>
<p>[7] T. Joachims. Text categorization with support vector machines: learning with many relevant features. In Proc. ECML, 1998.</p>
<p>[8] T. Joachims. Transductive inference for text classiﬁcation using support vector machines. In Proc. ICML, 1999.</p>
<p>[9] M. Lan, C. L. Tan, H.-B. Low, and S. Y. Sung. A comprehensive comparative study on term weighting schemes for text categorization with support vector machines. In Proc. WWW, 2005.</p>
<p>[10] H. Lee, A. Battle, R. Rajat, and A. Ng. Efﬁcient sparse coding algorithms. In Proc. NIPS, 2007.</p>
<p>[11] A. Z. Olivier Chapelle. Semi-supervised classiﬁcation by low density separation. In Proc. Inter. Workshop on Artiﬁcial Intelligence and Statistics, 2005.</p>
<p>[12] F. Provost, T. Fawcett, and R. Kohavi. The case against accuracy estimation for comparing induction algorithms. In Proc. ICML, 1998.</p>
<p>[13] R. Raina, A. Battle, H. Lee, B. Packer, and A. Y. Ng. Self-taught learning: transfer learning from unlabeled data. In Proc. ICML, 2007.</p>
<p>[14] M. Seeger. Learning with labeled and unlabeled data. Technical report, Univ. of Edinburgh, 2001.</p>
<p>[15] V. Sindhwani and S. S. Keerthi. Large scale semi-supervised linear support vector machines. In Proc. ACM SIGIR, 2006.</p>
<p>[16] J. F. Sturm. Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones. Optimization Methods Software, 11/12(1–4), 1999.</p>
<p>[17] M. Szummer and T. Jaakkola. Information regularization with partially labeled data. In Proc. NIPS, 2002.</p>
<p>[18] L. Yang, R. Jin, C. Pantofaru, and R. Sukthankar. Discriminative cluster reﬁnement: Improving object category recognition given limited training data. In Proc. CVPR, 2007.</p>
<p>[19] Y. Yang. An evaluation of statistical approaches to text categorization. Journal of Info. Retrieval, 1999.</p>
<p>[20] Y. Yang and J. O. Pedersen. A comparative study on feature selection in text categorization. In Proc. ICML, 1997.</p>
<p>[21] X. Zhu. Semi-supervised learning literature survey. Technical report, UW-Madison, Comp. Sci., 2006.</p>
<p>[22] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic functions. In Proc. ICML, 2003.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
