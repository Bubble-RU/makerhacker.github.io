<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-52" href="../nips2008/nips-2008-Correlated_Bigram_LSA_for_Unsupervised_Language_Model_Adaptation.html">nips2008-52</a> <a title="nips-2008-52-reference" href="#">nips2008-52-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</h1>
<br/><p>Source: <a title="nips-2008-52-pdf" href="http://papers.nips.cc/paper/3564-correlated-bigram-lsa-for-unsupervised-language-model-adaptation.pdf">pdf</a></p><p>Author: Yik-cheung Tam, Tanja Schultz</p><p>Abstract: We present a correlated bigram LSA approach for unsupervised LM adaptation for automatic speech recognition. The model is trained using efﬁcient variational EM and smoothed using the proposed fractional Kneser-Ney smoothing which handles fractional counts. We address the scalability issue to large training corpora via bootstrapping of bigram LSA from unigram LSA. For LM adaptation, unigram and bigram LSA are integrated into the background N-gram LM via marginal adaptation and linear interpolation respectively. Experimental results on the Mandarin RT04 test set show that applying unigram and bigram LSA together yields 6%–8% relative perplexity reduction and 2.5% relative character error rate reduction which is statistically signiﬁcant compared to applying only unigram LSA. On the large-scale evaluation on Arabic, 3% relative word error rate reduction is achieved which is also statistically signiﬁcant. 1</p><br/>
<h2>reference text</h2><p>[1] J. R. Bellegarda, “Large Vocabulary Speech Recognition with Multispan Statistical Language Models,” IEEE Transactions on Speech and Audio Processing, vol. 8, no. 1, pp. 76–84, Jan 2000.</p>
<p>[2] D. Blei, A. Ng, and M. Jordan, “Latent Dirichlet Allocation,” in Journal of Machine Learning Research, 2003, pp. 1107–1135.</p>
<p>[3] Y. C. Tam and T. Schultz, “Language model adaptation using variational Bayes inference,” in Proceedings of Interspeech, 2005.</p>
<p>[4] D. Mrva and P. C. Woodland, “Unsupervised language model adaptation for mandarin broadcast conversation transcription,” in Proceedings of Interspeech, 2006.</p>
<p>[5] T. Grifﬁths, M. Steyvers, D. Blei, and J. Tenenbaum, “Integrating topics and syntax,” in Advances in Neural Information Processing Systems, 2004.</p>
<p>[6] B. J. Hsu and J. Glass, “Style and topic language model adaptation using HMM-LDA,” in Proceedings of Empirical Methods on Natural Language Processing (EMNLP), 2006.</p>
<p>[7] Hanna M. Wallach, “Topic Modeling: Beyond Bag-of-Words,” in International Conference on Machine Learning, 2006.</p>
<p>[8] P. Xu, A. Emami, and F. Jelinek, “Training connectionist models for the structured language model,” in Proceedings of Empirical Methods on Natural Language Processing (EMNLP), 2003.</p>
<p>[9] R. Kneser and H. Ney, “Improved backing-off for M-gram language modeling,” in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 1995, vol. 1, pp. 181–184.</p>
<p>[10] R. Kneser, J. Peters, and D. Klakow, “Language model adaptation using dynamic marginals,” in Proceedings of European Conference on Speech Communication and Technology (EUROSPEECH), 1997, pp. 1971–1974.</p>
<p>[11] R. Iyer and M. Ostendorf, “Modeling long distance dependence in language: Topic mixtures versus dynamic cache models,” IEEE Transactions on Speech and Audio Processing, vol. 7, no. 1, pp. 30–39, Jan 1999.</p>
<p>[12] X. Wang, A. McCallum, and X. Wei, “Topical N-grams: Phrase and topic discovery, with an application to information retrieval,” in IEEE International Conference on Data Mining, 2007.</p>
<p>[13] T. Minka, “The dirichlet-tree distribution,” 1999.</p>
<p>[14] Y. C. Tam and T. Schultz, “Correlated latent semantic model for unsupervised language model adaptation,” in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2007.</p>
<p>[15] A. Stolcke, “SRILM - an extensible language modeling toolkit,” in Proceedings of International Conference on Spoken Language Processing (ICSLP), 2002.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
