<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>99 nips-2008-High-dimensional support union recovery in multivariate regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-99" href="../nips2008/nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">nips2008-99</a> <a title="nips-2008-99-reference" href="#">nips2008-99-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>99 nips-2008-High-dimensional support union recovery in multivariate regression</h1>
<br/><p>Source: <a title="nips-2008-99-pdf" href="http://papers.nips.cc/paper/3432-high-dimensional-support-union-recovery-in-multivariate-regression.pdf">pdf</a></p><p>Author: Guillaume R. Obozinski, Martin J. Wainwright, Michael I. Jordan</p><p>Abstract: We study the behavior of block 1 / 2 regularization for multivariate regression, where a K-dimensional response vector is regressed upon a ﬁxed set of p covariates. The problem of support union recovery is to recover the subset of covariates that are active in at least one of the regression problems. Studying this problem under high-dimensional scaling (where the problem parameters as well as sample size n tend to inﬁnity simultaneously), our main result is to show that exact recovery is possible once the order parameter given by θ 1 / 2 (n, p, s) : = n/[2ψ(B ∗ ) log(p − s)] exceeds a critical threshold. Here n is the sample size, p is the ambient dimension of the regression model, s is the size of the union of supports, and ψ(B ∗ ) is a sparsity-overlap function that measures a combination of the sparsities and overlaps of the K-regression coefﬁcient vectors that constitute the model. This sparsity-overlap function reveals that block 1 / 2 regularization for multivariate regression never harms performance relative to a naive 1 -approach, and can yield substantial improvements in sample complexity (up to a factor of K) when the regression vectors are suitably orthogonal relative to the design. We complement our theoretical results with simulations that demonstrate the sharpness of the result, even for relatively small problems. 1</p><br/>
<h2>reference text</h2><p>[1] F. Bach. Consistency of the group Lasso and multiple kernel learning. Technical report, INRIA D´ partement d’Informatique, Ecole Normale Sup´ rieure, 2008. e e</p>
<p>[2] F. Bach, G. Lanckriet, and M. Jordan. Multiple kernel learning, conic duality, and the SMO algorithm. In Proc. Int. Conf. Machine Learning (ICML). Morgan Kaufmann, 2004.</p>
<p>[3] D. Donoho, M. Elad, and V. M. Temlyakov. Stable recovery of sparse overcomplete representations in the presence of noise. IEEE Trans. Info Theory, 52(1):6–18, January 2006.</p>
<p>[4] H. Liu and J. Zhang. On the Mellon University, 2008.  1− q  regularized regression. Technical Report arXiv:0802.1517v1, Carnegie</p>
<p>[5] L. Meier, S. van de Geer, and P. B¨ hlmann. The group lasso for logistic regression. Technical report, u Mathematics Department, Swiss Federal Institute of Technology Z¨ rich, 2007. u</p>
<p>[6] Y. Nardi and A. Rinaldo. On the asymptotic properties of the group lasso estimator for linear models. Electronic Journal of Statistics, 2:605–633, 2008.</p>
<p>[7] G. Obozinski, B. Taskar, and M. Jordan. Joint covariate selection and joint subspace selection for multiple classiﬁcation problems. Statistics and Computing, 2009. To appear.</p>
<p>[8] M. Pontil and C.A. Michelli. Learning the kernel function via regularization. Journal of Machine Learning Research, 6:1099–1125, 2005.</p>
<p>[9] P. Ravikumar, J. Lafferty, H. Liu, and L. Wasserman. SpAM: sparse additive models. In Neural Info. Proc. Systems (NIPS) 21, Vancouver, Canada, December 2007.</p>
<p>[10] J. A. Tropp. Just relax: Convex programming methods for identifying sparse signals in noise. IEEE Trans. Info Theory, 52(3):1030–1051, March 2006.</p>
<p>[11] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy recovery of sparsity using using 1 -constrained quadratic programs. Technical Report 709, Department of Statistics, UC Berkeley, 2006.</p>
<p>[12] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society B, 1(68):4967, 2006.</p>
<p>[13] P. Zhao, G. Rocha, and B. Yu. Grouped and hierarchical model selection through composite absolute penalties. Technical report, Statistics Department, UC Berkeley, 2007.</p>
<p>[14] P. Zhao and B. Yu. Model selection with the lasso. J. of Machine Learning Research, pages 2541–2567, 2007.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
