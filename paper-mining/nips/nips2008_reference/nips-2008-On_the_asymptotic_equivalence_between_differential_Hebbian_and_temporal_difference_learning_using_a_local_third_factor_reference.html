<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>166 nips-2008-On the asymptotic equivalence between differential Hebbian and temporal difference learning using a local third factor</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-166" href="../nips2008/nips-2008-On_the_asymptotic_equivalence_between_differential_Hebbian_and_temporal_difference_learning_using_a_local_third_factor.html">nips2008-166</a> <a title="nips-2008-166-reference" href="#">nips2008-166-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>166 nips-2008-On the asymptotic equivalence between differential Hebbian and temporal difference learning using a local third factor</h1>
<br/><p>Source: <a title="nips-2008-166-pdf" href="http://papers.nips.cc/paper/3419-on-the-asymptotic-equivalence-between-differential-hebbian-and-temporal-difference-learning-using-a-local-third-factor.pdf">pdf</a></p><p>Author: Christoph Kolodziejski, Bernd Porr, Minija Tamosiunaite, Florentin Wörgötter</p><p>Abstract: In this theoretical contribution we provide mathematical proof that two of the most important classes of network learning - correlation-based differential Hebbian learning and reward-based temporal difference learning - are asymptotically equivalent when timing the learning with a local modulatory signal. This opens the opportunity to consistently reformulate most of the abstract reinforcement learning framework from a correlation based perspective that is more closely related to the biophysics of neurons. 1</p><br/>
<h2>reference text</h2><p>[1] R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998.</p>
<p>[2] R. S. Sutton. Learning to predict by the method of temporal differences. Mach. Learn., 3:9–44, 1988.</p>
<p>[3] E. Izhikevich. Solving the distal reward problem through linkage of stdp and dopamine signaling. Cereb. Cortex., 17:2443–2452, 2007.</p>
<p>[4] PD. Roberts, RA. Santiago, and G. Lafferriere. An implementation of reinforcement learning based on spike-timing dependent plasticity. Biol. Cybern., in press.</p>
<p>[5] R. V. Florian. Reinforcement learning through modulation of spike-timing-dependent synaptic plasticity. Neural Comput., 19:1468–1502, 2007.</p>
<p>[6] W. Potjans, A. Morrison, and M. Diesmann. A spiking neural network model of an actor-critic learning agent. Neural Comput., 21:301–339, 2009.</p>
<p>[7] A. H. Klopf. A neuronal model of classical conditioning. Psychobiol., 16(2):85–123, 1988.</p>
<p>[8] R. Sutton and A. Barto. Towards a modern theory of adaptive networks: Expectation and prediction. Psychol. Review, 88:135–170, 1981.</p>
<p>[9] E. Oja. A simpliﬁed neuron model as a principal component analyzer. J. Math. Biol., 15(3):267–273, 1982.</p>
<p>[10] M. Tamosiunaite, J. Ainge, T. Kulvicius, B. Porr, P. Dudchenko, and F. Wörgötter. Pathﬁnding in real and simulated rats: On the usefulness of forgetting and frustration for navigation learning. J. Comp. Neurosci., 25(3):562–582, 2008.</p>
<p>[11] M. Wiering. Convergence and divergence in standard averaging reinforcement learning. In J Boulicaut, F Esposito, F Giannotti, and D Pedreschi, editors, Proceedings of the 15th European Conference on Machine learning ECML’04, pages 477–488, 2004.</p>
<p>[12] P. Dayan and T. Sejnowski. Td(λ) converges with probability 1. Mach. Learn., 14(3):295–301, 1994.</p>
<p>[13] H. Markram, J. Lübke, M. Frotscher, and B. Sakmann. Regulation of synaptic efﬁcacy by coincidence of postsynaptic APs and EPSPs. Science, 275:213–215, 1997.</p>
<p>[14] B. Porr and F. Wörgötter. Learning with “relevance”: Using a third factor to stabilise hebbian learning. Neural Comput., 19:2694–2719, 2007.</p>
<p>[15] C. Watkins and P. Dayan. Technical note:Q-Learning. Mach. Learn., 8:279–292, 1992.</p>
<p>[16] S. P. Singh, T. Jaakkola, M. L. Littman, and C. Szepesvári. Convergence results for single-step on-policy reinforcement-learning algorithms. Mach. Learn., 38(3):287–308, 2000.</p>
<p>[17] W. Gerstner, R. Kempter, L. van Hemmen, and H. Wagner. A neuronal learning rule for submillisecond temporal coding. Nature, 383:76– 78, 1996.</p>
<p>[18] R. Rao and T. Sejnowski. Spike-timing-dependent hebbian plasticity as temporal difference learning. Neural Comput., 13:2221–2237, 2001.</p>
<p>[19] J. Baxter, P. L. Bartlett, and L. Weaver. Experiments with inﬁnite-horizon,policy-gradient estimation. J. Artif. Intell. Res., 15:351–381, 2001.</p>
<p>[20] W. Schultz, P. Apicella, E. Scarnati, and T. Ljungberg. Neuronal activity in monkey ventral striatum related to the expectation of reward. J. Neurosci., 12(12):4595–610, 1992.</p>
<p>[21] P. R. Montague, P. Dayan, and T. J. Sejnowski. A framework for mesencephalic dopamine systems based on predictive hebbian learning. J. Neurosci., 76(5):1936–1947, 1996.</p>
<p>[22] G. Morris, A. Nevet, D. Arkadir, E. Vaadia, and H. Bergman. Midbrain dopamine neurons encode decisions for future action. Nat. Neurosci., 9 (8):1057–1063, 2006.</p>
<p>[23] P. Dayan. Matters temporal. Trends. Cogn. Sci., 6(3):105–106, 2002.</p>
<p>[24] C. Kolodziejski, B. Porr, and F. Wörgötter. Mathematical properties of neuronal TD-rules and differential hebbian learning: A comparison. Biol. Cybern., 98(3):259–272, 2008.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
