<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>57 nips-2006-Conditional mean field</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-57" href="../nips2006/nips-2006-Conditional_mean_field.html">nips2006-57</a> <a title="nips-2006-57-reference" href="#">nips2006-57-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>57 nips-2006-Conditional mean field</h1>
<br/><p>Source: <a title="nips-2006-57-pdf" href="http://papers.nips.cc/paper/3118-conditional-mean-field.pdf">pdf</a></p><p>Author: Peter Carbonetto, Nando D. Freitas</p><p>Abstract: Despite all the attention paid to variational methods based on sum-product message passing (loopy belief propagation, tree-reweighted sum-product), these methods are still bound to inference on a small set of probabilistic models. Mean ﬁeld approximations have been applied to a broader set of problems, but the solutions are often poor. We propose a new class of conditionally-speciﬁed variational approximations based on mean ﬁeld theory. While not usable on their own, combined with sequential Monte Carlo they produce guaranteed improvements over conventional mean ﬁeld. Moreover, experiments on a well-studied problem— inferring the stable conﬁgurations of the Ising spin glass—show that the solutions can be signiﬁcantly better than those obtained using sum-product-based methods. 1</p><br/>
<h2>reference text</h2><p>[1] S. M. Aji and R. J. McEliece. The Generalized distributive law and free energy minimization. In Proceedings of the 39th Allerton Conference, pages 672–681, 2001.</p>
<p>[2] B. Arnold, E. Castillo, and J.-M. Sarabia. Conditional Speciﬁcation of Statistical Models. Springer, 1999.</p>
<p>[3] J. Besag. Spatial interaction and the statistical analysis of lattice systems. J. Roy. Statist. Soc., Ser. B, 36:192–236, 1974.</p>
<p>[4] J. Besag. Comment to “Conditionally speciﬁed distributions”. Statist. Sci., 16:265–267, 2001.</p>
<p>[5] W. Buntine and A. Jakulin. Applying discrete PCA in data analysis. In Uncertainty in Artiﬁcial Intelligence, volume 20, pages 59–66, 2004.</p>
<p>[6] N. de Freitas, P. Højen-Sørensen, M. I. Jordan, and S. Russell. Variational MCMC. In Uncertainty in Artiﬁcial Intelligence, volume 17, pages 120–127, 2001.</p>
<p>[7] P. del Moral, A. Doucet, and A. Jasra. Sequential Monte Carlo samplers. J. Roy. Statist. Soc., Ser. B, 68:411–436, 2006.</p>
<p>[8] Z. Ghahramani and M. J. Beal. Variational inference for Bayesian mixtures of factor analysers. In Advances in Neural Information Processing Systems, volume 12, pages 449–455, 1999.</p>
<p>[9] F. Hamze and N. de Freitas. Hot Coupling: a particle approach to inference and normalization on pairwise undirected graphs. Advances in Neural Information Processing Systems, 18:491–498, 2005.</p>
<p>[10] T. Heskes, K. Albers, and B. Kappen. Approximate inference and constrained optimization. In Uncertainty in Artiﬁcial Intelligence, volume 19, pages 313–320, 2003.</p>
<p>[11] C. Jarzynski. Nonequilibrium equality for free energy differences. Phys. Rev. Lett., 78:2690–2693, 1997.</p>
<p>[12] M. Jerrum and A. Sinclair. The Markov chain Monte Carlo method: an approach to approximate counting and integration. In Approximation Algorithms for NP-hard Problems, pages 482–520. PWS Pubs., 1996.</p>
<p>[13] G. Kitagawa. Monte Carlo ﬁlter and smoother for non-Gaussian nonlinear state space models. J. Comput. Graph. Statist., 5:1–25, 1996.</p>
<p>[14] P. Muyan and N. de Freitas. A blessing of dimensionality: measure concentration and probabilistic inference. In Proceedings of the 19th Workshop on Artiﬁcial Intelligence and Statistics, 2003.</p>
<p>[15] R. M. Neal. Annealed importance sampling. Statist. and Comput., 11:125–139, 2001.</p>
<p>[16] M. Newman and G. Barkema. Monte Carlo Methods in Statistical Physics. Oxford Univ. Press, 1999.</p>
<p>[17] M. Opper and D. Saad, editors. Advanced Mean Field Methods, Theory and Practice. MIT Press, 2001.</p>
<p>[18] C. P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer, 2nd edition, 2004.</p>
<p>[19] M. N. Rosenbluth and A. W. Rosenbluth. Monte Carlo calculation of the average extension of molecular chains. J. Chem. Phys., 23:356–359, 1955.</p>
<p>[20] J. S. Sadowsky and J. A. Bucklew. On large deviations theory and asymptotically efﬁcient Monte Carlo estimation. IEEE Trans. Inform. Theory, 36:579–588, 1990.</p>
<p>[21] L. K. Saul, T. Jaakola, and M. I. Jordan. Mean ﬁeld theory for sigmoid belief networks. J. Artiﬁcial Intelligence Res., 4:61–76, 1996.</p>
<p>[22] L. K. Saul and M. I. Jordan. Exploiting tractable structures in intractable networks. In Advances in Neural Information Processing Systems, volume 8, pages 486–492, 1995.</p>
<p>[23] E. B. Sudderth, A. T. Ihler, W. T. Freeman, and A. S. Willsky. Nonparametric belief propagation. In Computer Vision and Pattern Recognition,, volume I, pages 605–612, 2003.</p>
<p>[24] M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky. A new class of upper bounds on the log partition function. IEEE Trans. Inform. Theory, 51:2313–2335, 2005.</p>
<p>[25] M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Technical report, EECS Dept., University of California, Berkeley, 2003.</p>
<p>[26] W. Wiegerinck. Variational approximations between mean ﬁeld theory and the junction tree algorithm. In Uncertainty in Artiﬁcial Intelligence, volume 16, pages 626–633, 2000.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
