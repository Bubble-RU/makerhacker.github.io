<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>20 nips-2006-Active learning for misspecified generalized linear models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-20" href="../nips2006/nips-2006-Active_learning_for_misspecified_generalized_linear_models.html">nips2006-20</a> <a title="nips-2006-20-reference" href="#">nips2006-20-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>20 nips-2006-Active learning for misspecified generalized linear models</h1>
<br/><p>Source: <a title="nips-2006-20-pdf" href="http://papers.nips.cc/paper/2961-active-learning-for-misspecified-generalized-linear-models.pdf">pdf</a></p><p>Author: Francis R. Bach</p><p>Abstract: Active learning refers to algorithmic frameworks aimed at selecting training data points in order to reduce the number of required training data points and/or improve the generalization performance of a learning method. In this paper, we present an asymptotic analysis of active learning for generalized linear models. Our analysis holds under the common practical situation of model misspeciﬁcation, and is based on realistic assumptions regarding the nature of the sampling distributions, which are usually neither independent nor identical. We derive unbiased estimators of generalization performance, as well as estimators of expected reduction in generalization error after adding a new training data point, that allow us to optimize its sampling distribution through a convex optimization problem. Our analysis naturally leads to an algorithm for sequential active learning which is applicable for all tasks supported by generalized linear models (e.g., binary classiﬁcation, multi-class classiﬁcation, regression) and can be applied in non-linear settings through the use of Mercer kernels. 1</p><br/>
<h2>reference text</h2><p>[1] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active learning with statistical models. J. Art. Intel. Res., 4:129–145, 1996.</p>
<p>[2] V. V. Fedorov. Theory of optimal experiments. Academic Press, 1972.</p>
<p>[3] P. Chaudhuri and P. A. Mykland. On efﬁcient designing of nonlinear experiments. Stat. Sin., 5:421–440, 1995.</p>
<p>[4] S. Dasgupta. Coarse sample complexity bounds for active learning. In Adv. NIPS 18, 2006.</p>
<p>[5] N. Roy and A. McCallum. Toward optimal active learning through sampling estimation of error reduction. In Proc. ICML, 2001.</p>
<p>[6] S. Tong and E. Chang. Support vector machine active learning for image retrieval. In Proc. ACM Multimedia, 2001.</p>
<p>[7] M. Warmuth, G. R¨ tsch, M. Mathieson, J. Liao, and C. Lemmen. Active learning in the drug discovery a process. In Adv. NIPS 14, 2002.</p>
<p>[8] X. Zhu, J. Lafferty, and Z. Ghahramani. Combining active learning and semi-supervised learning using Gaussian ﬁelds and harmonic functions. In Proc. ICML, 2003.</p>
<p>[9] A I. Schein. Active Learning for Logistic Regression. Ph.D. diss., U. Penn., 2005. CIS Dpt.</p>
<p>[10] P. McCullagh and J. A. Nelder. Generalized Linear Models. Chapman and Hall, 1989.</p>
<p>[11] T. Zhang and F. J. Oles. A probability analysis on the value of unlabeled data for classiﬁcation problems. In Proc. ICML, 2000.</p>
<p>[12] O. Chapelle. Active learning for parzen window classiﬁer. In Proc. AISTATS, 2005.</p>
<p>[13] H. Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. J. Stat. Plan. Inf., 90:227–244, 2000.</p>
<p>[14] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge Univ. Press, 2003.</p>
<p>[15] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univ. Press, 2004.</p>
<p>[16] S. Fine and K. Scheinberg. Efﬁcient SVM training using low-rank kernel representations. J. Mach. Learn. Res., 2:243–264, 2001.</p>
<p>[17] S. Tong and D. Koller. Support vector machine active learning with applications to text classiﬁcation. In Proc. ICML, 2000.</p>
<p>[18] K. Fukumizu. Active learning in multilayer perceptrons. In Adv. NIPS 8, 1996.</p>
<p>[19] T. Kanamori and H. Shimodaira. Active learning algorithm using the maximum weighted log-likelihood estimator. J. Stat. Plan. Inf., 116:149–162, 2003.</p>
<p>[20] T. Kanamori. Statistical asymptotic theory of active learning. Ann. Inst. Stat. Math., 54(3):459–475, 2002.</p>
<p>[21] H. White. Maximum likelihood estimation of misspeciﬁed models. Econometrica, 50(1):1–26, 1982.</p>
<p>[22] H. Akaike. A new look at statistical model identiﬁcation. IEEE Trans. Aut. Cont., 19:716–722, 1974.</p>
<p>[23] F. R. Bach. Active learning for misspeciﬁed generalized linear models. Technical Report N15/06/MM, Ecole des Mines de Paris, 2006.</p>
<p>[24] A. W. Van der Vaart. Asymptotic Statistics. Cambridge Univ. Press, 1998.</p>
<p>[25] D. MacKay. Information-based objective functions for active data selection. Neural Computation, 4(4):590–604, 1992.</p>
<p>[26] Y. Bengio and Y Grandvalet. Semi-supervised learning by entropy minimization. In Adv. NIPS 17, 2005.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
