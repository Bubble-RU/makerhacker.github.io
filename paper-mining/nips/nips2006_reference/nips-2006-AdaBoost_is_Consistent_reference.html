<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>21 nips-2006-AdaBoost is Consistent</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-21" href="../nips2006/nips-2006-AdaBoost_is_Consistent.html">nips2006-21</a> <a title="nips-2006-21-reference" href="#">nips2006-21-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>21 nips-2006-AdaBoost is Consistent</h1>
<br/><p>Source: <a title="nips-2006-21-pdf" href="http://papers.nips.cc/paper/2963-adaboost-is-consistent.pdf">pdf</a></p><p>Author: Peter L. Bartlett, Mikhail Traskin</p><p>Abstract: The risk, or probability of error, of the classiﬁer produced by the AdaBoost algorithm is investigated. In particular, we consider the stopping strategy to be used in AdaBoost to achieve universal consistency. We show that provided AdaBoost is stopped after nν iterations—for sample size n and ν < 1—the sequence of risks of the classiﬁers it produces approaches the Bayes risk if Bayes risk L∗ > 0. 1</p><br/>
<h2>reference text</h2><p>[1] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119–139, 1997.</p>
<p>[2] Leo Breiman. Bagging predictors. Machine Learning, 24(2):123–140, 1996.</p>
<p>[3] Leo Breiman. Arcing classiﬁers (with discussion). The Annals of Statistics, 26(3):801–849, 1998. (Was Department of Statistics, U.C. Berkeley Technical Report 460, 1996).</p>
<p>[4] Leo Breiman. Some inﬁnite theory for predictor ensembles. Technical Report 579, Department of Statistics, University of California, Berkeley, 2000.</p>
<p>[5] Wenxin Jiang. On weak base hypotheses and their implications for boosting regression and classiﬁcation. The Annals of Statistics, 30:51–73, 2002.</p>
<p>[6] G´ bor Lugosi and Nicolas Vayatis. On the Bayes-risk consistency of regularized boosting a methods. The Annals of Statistics, 32(1):30–55, 2004.</p>
<p>[7] Tong Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization. The Annals of Statistics, 32(1):56–85, 2004.</p>
<p>[8] Tong Zhang and Bin Yu. Boosting with early stopping: convergence and consistency. The Annals of Statistics, 33:1538–1579, 2005.</p>
<p>[9] Wenxin Jiang. Process consistency for AdaBoost. The Annals of Statistics, 32(1):13–29, 2004.</p>
<p>[10] P. J. Bickel, Y. Ritov, and A. Zakai. Some theory for generalized boosting algorithms. Journal of Machine Learning Research, 7:705–732, May 2006.</p>
<p>[11] Martin Anthony and Peter Bartlett. Neural network learning: theoretical foundations. Cambridge University Press, 1999.</p>
<p>[12] V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization error of combined classiﬁers. The Annals of Statistics, 30:1–50, 2002.</p>
<p>[13] Michel Ledoux and Michel Talagrand. Probability in Banach Spaces. Springer-Verlag, New York, 1991.</p>
<p>[14] Richard M. Dudley. Uniform central limit theorems. Cambridge University Press, Cambridge, MA, 1999.</p>
<p>[15] David Pollard. Empirical Processes: Theory and Applications. IMS, 1990.</p>
<p>[16] Luc Devroye, L´ szl´ Gy¨ rﬁ, and G´ bor Lugosi. A Probabilistic Theory of Pattern Recognition. a o o a Springer, New York, 1996.</p>
<p>[17] Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classiﬁcation, and risk bounds. Journal of the American Statistical Association, 101(473):138–156, 2006.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
