<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 nips-2006-Conditional Random Sampling: A Sketch-based Sampling Technique for Sparse Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-56" href="../nips2006/nips-2006-Conditional_Random_Sampling%3A_A_Sketch-based_Sampling_Technique_for_Sparse_Data.html">nips2006-56</a> <a title="nips-2006-56-reference" href="#">nips2006-56-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>56 nips-2006-Conditional Random Sampling: A Sketch-based Sampling Technique for Sparse Data</h1>
<br/><p>Source: <a title="nips-2006-56-pdf" href="http://papers.nips.cc/paper/2980-conditional-random-sampling-a-sketch-based-sampling-technique-for-sparse-data.pdf">pdf</a></p><p>Author: Ping Li, Kenneth W. Church, Trevor J. Hastie</p><p>Abstract: We1 develop Conditional Random Sampling (CRS), a technique particularly suitable for sparse data. In large-scale applications, the data are often highly sparse. CRS combines sketching and sampling in that it converts sketches of the data into conditional random samples online in the estimation stage, with the sample size determined retrospectively. This paper focuses on approximating pairwise l2 and l1 distances and comparing CRS with random projections. For boolean (0/1) data, CRS is provably better than random projections. We show using real-world data that CRS often outperforms random projections. This technique can be applied in learning, data mining, information retrieval, and database query optimizations.</p><br/>
<h2>reference text</h2><p>[1] D. Achlioptas. Database-friendly random projections: Johnson-Lindenstrauss with binary coins. Journal of Computer and System Sciences, 66(4):671–687, 2003.</p>
<p>[2] D. Achlioptas, F. McSherry, and B. Sch¨ lkopf. Sampling techniques for kernel methods. In NIPS, pages 335–342, 2001. o</p>
<p>[3] R. Arriaga and S. Vempala. An algorithmic theory of learning: Robust concepts and random projection. Machine Learning, 63(2):161– 182, 2006.</p>
<p>[4] E. Bingham and H. Mannila. Random projection in dimensionality reduction: Applications to image and text data. In KDD, pages 245–250, 2001.</p>
<p>[5] B. Brinkman and M. Charikar. On the impossibility of dimension reduction in l1 . Journal of ACM, 52(2):766–788, 2005.</p>
<p>[6] A. Broder. On the resemblance and containment of documents. In the Compression and Complexity of Sequences, pages 21–29, 1997.</p>
<p>[7] I. Dhillon and D. Modha. Concept decompositions for large sparse text data using clustering. Machine Learning, 42(1-2):143–175, 2001.</p>
<p>[8] P. Drineas and M. Mahoney. On the nystrom method for approximating a gram matrix for improved kernel-based learning. Journal of Machine Learning Research, 6(Dec):2153–2175, 2005.</p>
<p>[9] P. Indyk. Stable distributions, pseudorandom generators, embeddings and data stream computation. In FOCS, pages 189–197, 2000.</p>
<p>[10] P. Li. Very sparse stable random projections, estimators and tail bounds for stable random projections. Technical report, http: //arxiv.org/PS cache/cs/pdf/0611/0611114.pdf, 2006.</p>
<p>[11] P. Li and K. Church. Using sketches to estimate associations. In HLT/EMNLP, pages 708–715, 2005.</p>
<p>[12] P. Li and K. Church. A sketch algorithm for estimating two-way and multi-way associations. Computational Linguistics, To Appear.</p>
<p>[13] P. Li, K. Church, and T. Hastie. Conditional random sampling: A sketched-based sampling technique for sparse data. Technical Report 2006-08, Department of Statistics, Stanford University), 2006.</p>
<p>[14] P. Li, K. Church, and T. Hastie. Nonlinear estimators and tail bounds for dimensional reduction in l1 using Cauchy random projections. (http://arxiv.org/PS cache/cs/pdf/0610/0610155.pdf), 2006.</p>
<p>[15] P. Li, T. Hastie, and K. Church. Improving random projections using marginal information. In COLT, pages 635–649, 2006.</p>
<p>[16] P. Li, T. Hastie, and K. Church. Very sparse random projections. In KDD, pages 287–296, 2006.</p>
<p>[17] S. Vempala. The Random Projection Method. American Mathematical Society, Providence, RI, 2004.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
