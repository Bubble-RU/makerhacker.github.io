<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>156 nips-2006-Ordinal Regression by Extended Binary Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-156" href="../nips2006/nips-2006-Ordinal_Regression_by_Extended_Binary_Classification.html">nips2006-156</a> <a title="nips-2006-156-reference" href="#">nips2006-156-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>156 nips-2006-Ordinal Regression by Extended Binary Classification</h1>
<br/><p>Source: <a title="nips-2006-156-pdf" href="http://papers.nips.cc/paper/3125-ordinal-regression-by-extended-binary-classification.pdf">pdf</a></p><p>Author: Ling Li, Hsuan-tien Lin</p><p>Abstract: We present a reduction framework from ordinal regression to binary classiﬁcation based on extended examples. The framework consists of three steps: extracting extended examples from the original examples, learning a binary classiﬁer on the extended examples with any binary classiﬁcation algorithm, and constructing a ranking rule from the binary classiﬁer. A weighted 0/1 loss of the binary classiﬁer would then bound the mislabeling cost of the ranking rule. Our framework allows not only to design good ordinal regression algorithms based on well-tuned binary classiﬁcation approaches, but also to derive new generalization bounds for ordinal regression from known bounds for binary classiﬁcation. In addition, our framework uniﬁes many existing ordinal regression algorithms, such as perceptron ranking and support vector ordinal regression. When compared empirically on benchmark data sets, some of our newly designed algorithms enjoy advantages in terms of both training speed and generalization performance over existing algorithms, which demonstrates the usefulness of our framework. 1</p><br/>
<h2>reference text</h2><p>[1] K. Crammer and Y. Singer. Pranking with ranking. In T. G. Dietterich, S. Becker, and Z. Ghahramani, eds., Advances in Neural Information Processing Systems 14, vol. 1, pp. 641–647. MIT Press, 2002.</p>
<p>[2] A. Shashua and A. Levin. Ranking with large margin principle: Two approaches. In S. Becker, S. Thrun, and K. Obermayer, eds., Advances in Neural Information Processing Systems 15, pp. 961–968. MIT Press, 2003.</p>
<p>[3] S. Rajaram, A. Garg, X. S. Zhou, and T. S. Huang. Classiﬁcation approach towards ranking and sorting problems. In N. Lavraˇ , D. Gamberger, H. Blockeel, and L. Todorovski, eds., Machine Learning: c ECML 2003, vol. 2837 of Lecture Notes in Artiﬁcial Intelligence, pp. 301–312. Springer-Verlag, 2003.</p>
<p>[4] W. Chu and S. S. Keerthi. New approaches to support vector ordinal regression. In L. D. Raedt and S. Wrobel, eds., ICML 2005: Proceedings of the 22nd International Conference on Machine Learning, pp. 145–152. Omnipress, 2005.</p>
<p>[5] R. Herbrich, T. Graepel, and K. Obermayer. Large margin rank boundaries for ordinal regression. In A. J. Smola, P. L. Bartlett, B. Sch¨ lkopf, and D. Schuurmans, eds., Advances in Large Margin Classiﬁers, o chapter 7, pp. 115–132. MIT Press, 2000.</p>
<p>[6] E. Frank and M. Hall. A simple approach to ordinal classiﬁcation. In L. D. Raedt and P. Flach, eds., Machine Learning: ECML 2001, vol. 2167 of Lecture Notes in Artiﬁcial Intelligence, pp. 145–156. SpringerVerlag, 2001.</p>
<p>[7] H.-T. Lin and L. Li. Large-margin thresholded ensembles for ordinal regression: Theory and practice. In J. L. Balc´ zar, P. M. Long, and F. Stephan, eds., Algorithmic Learning Theory: ALT 2006, vol. 4264 of a Lecture Notes in Artiﬁcial Intelligence, pp. 319–333. Springer-Verlag, 2006.</p>
<p>[8] Y. Freund and R. E. Schapire. Large margin classiﬁcation using the perceptron algorithm. Machine Learning, 37(3):277–296, 1999.</p>
<p>[9] V. N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, 2nd edition, 1999.</p>
<p>[10] P. Bartlett and J. Shawe-Taylor. Generalization performance of support vector machines and other pattern classiﬁers. In B. Sch¨ lkopf, C. J. C. Burges, and A. J. Smola, eds., Advances in Kernel Methods: Support o Vector Learning, chapter 4, pp. 43–54. MIT Press, 1998.</p>
<p>[11] J. R. Quinlan. Induction of decision trees. Machine Learning, 1(1):81–106, 1986.</p>
<p>[12] H.-T. Lin and L. Li. Novel distance-based SVM kernels for inﬁnite ensemble learning. In Proceedings of the 12th International Conference on Neural Information Processing, pp. 761–766, 2005.</p>
<p>[13] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines, 2001. Software available at http://www.csie.ntu.edu.tw/˜cjlin/libsvm.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
