<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>200 nips-2006-Unsupervised Regression with Applications to Nonlinear System Identification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-200" href="../nips2006/nips-2006-Unsupervised_Regression_with_Applications_to_Nonlinear_System_Identification.html">nips2006-200</a> <a title="nips-2006-200-reference" href="#">nips2006-200-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>200 nips-2006-Unsupervised Regression with Applications to Nonlinear System Identification</h1>
<br/><p>Source: <a title="nips-2006-200-pdf" href="http://papers.nips.cc/paper/3039-unsupervised-regression-with-applications-to-nonlinear-system-identification.pdf">pdf</a></p><p>Author: Ali Rahimi, Ben Recht</p><p>Abstract: We derive a cost functional for estimating the relationship between highdimensional observations and the low-dimensional process that generated them with no input-output examples. Limiting our search to invertible observation functions confers numerous beneﬁts, including a compact representation and no suboptimal local minima. Our approximation algorithms for optimizing this cost functional are fast and give diagnostic bounds on the quality of their solution. Our method can be viewed as a manifold learning algorithm that utilizes a prior on the low-dimensional manifold coordinates. The beneﬁts of taking advantage of such priors in manifold learning and searching for the inverse observation functions in system identiﬁcation are demonstrated empirically by learning to track moving targets from raw measurements in a sensor network setting and in an RFID tracking experiment. 1</p><br/>
<h2>reference text</h2><p>[1] P.H. Algoet and T.M. Cover. A sandwich proof of the Shannon-McMillan-Breiman theorem. The Annals of Probability, 16:899–909, 1988.</p>
<p>[2] Aharon Ben-Tal and Arkadi Nemirovski. Non-euclidean restricted memory level method for large-scale convex optimization. Mathematical Programming, 102:407–456, 2005.</p>
<p>[3] Z. Ghahramani and S. Roweis. Learning nonlinear dynamical systems using an EM algorithm. In Advances in Neural Information Processing Systems (NIPS), 1998.</p>
<p>[4] G. Golub and C.F. Van Loan. Matrix Computations. The Johns Hopkins University Press, 1989.</p>
<p>[5] V. Guilleman and A. Pollack. Differential Topology. Prentice Hall, Englewood Cliffs, New Jersey, 1974.</p>
<p>[6] O. Jenkins and M. Mataric. A spatio-temporal extension to isomap nonlinear dimension reduction. In International Conference on Machine Learning (ICML), 2004.</p>
<p>[7] F. Jones. Advanced Calculus. http://www.owlnet.rice.edu/˜fjones, unpublished.</p>
<p>[8] A. Juditsky, H. Hjalmarsson, A. Benveniste, B. Delyon, L. Ljung, J. Sj¨ berg, and Q. Zhang. Nonlinear o black-box models in system identiﬁcation: Mathematical foundations. Automatica, 31(12):1725–1750, 1995.</p>
<p>[9] N. D. Lawrence. Gaussian process latent variable models for visualisation of high dimensional data. In Advances in Neural Information Processing Systems (NIPS), 2004.</p>
<p>[10] J. Patten, H. Ishii, J. Hines, and G. Pangaro. Sensetable: A wireless object tracking platform for tangible user interfaces. In CHI, 2001.</p>
<p>[11] N. Patwari and A. O. Hero. Manifold learning algorithms for localization in wireless sensor networks. In International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2004.</p>
<p>[12] A. Rahimi. Learning to Transform Time Series with a Few Examples. PhD thesis, Massachusetts Institute of Technology, Computer Science and AI Lab, Cambridge, Massachusetts, USA, 2005.</p>
<p>[13] A. Rahimi, B. Recht, and T. Darrell. Learning appearance manifolds from video. In Computer Vision and Pattern Recognition (CVPR), 2005.</p>
<p>[14] I. K. Rana. An Introduction to Measure Theory and Integration. AMA, second edition, 2002.</p>
<p>[15] B. Sch¨ lkopf, A. Smola, and K-R. M¨ ller. Nonlinear component analysis as a kernel eigenvalue problem. o u Neural Computation, 10:1299–1319, 1998.</p>
<p>[16] A. Smola, S. Mika, B. Schoelkopf, and R. C. Williamson. Regularized principal manifolds. Journal of Machine Learning, 1:179–209, 2001.</p>
<p>[17] M. Pontil T. Evgeniou and T. Poggio. Regularization networks and support vector machines. Advances in Computational Mathematics, 2000.</p>
<p>[18] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000.</p>
<p>[19] H. Valpola and J. Karhunen. An unsupervised ensemble learning method for nonlinear dynamic statespace models. Neural Computation, 14(11):2647–2692, 2002.</p>
<p>[20] Lieven Vandenberghe, Stephen Boyd, and Shao-Po Wu. Determinant maximization with linear matrix inequality constraints. SIAM Journal on Matrix Analysis and Applications, 19(2):499–533, 1998.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
