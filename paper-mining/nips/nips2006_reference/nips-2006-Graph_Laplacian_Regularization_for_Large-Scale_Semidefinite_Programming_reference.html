<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>87 nips-2006-Graph Laplacian Regularization for Large-Scale Semidefinite Programming</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-87" href="../nips2006/nips-2006-Graph_Laplacian_Regularization_for_Large-Scale_Semidefinite_Programming.html">nips2006-87</a> <a title="nips-2006-87-reference" href="#">nips2006-87-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>87 nips-2006-Graph Laplacian Regularization for Large-Scale Semidefinite Programming</h1>
<br/><p>Source: <a title="nips-2006-87-pdf" href="http://papers.nips.cc/paper/3060-graph-laplacian-regularization-for-large-scale-semidefinite-programming.pdf">pdf</a></p><p>Author: Kilian Q. Weinberger, Fei Sha, Qihui Zhu, Lawrence K. Saul</p><p>Abstract: In many areas of science and engineering, the problem arises how to discover low dimensional representations of high dimensional data. Recently, a number of researchers have converged on common solutions to this problem using methods from convex optimization. In particular, many results have been obtained by constructing semideﬁnite programs (SDPs) with low rank solutions. While the rank of matrix variables in SDPs cannot be directly constrained, it has been observed that low rank solutions emerge naturally by computing high variance or maximal trace solutions that respect local distance constraints. In this paper, we show how to solve very large problems of this type by a matrix factorization that leads to much smaller SDPs than those previously studied. The matrix factorization is derived by expanding the solution of the original problem in terms of the bottom eigenvectors of a graph Laplacian. The smaller SDPs obtained from this matrix factorization yield very good approximations to solutions of the original problem. Moreover, these approximations can be further reﬁned by conjugate gradient descent. We illustrate the approach on localization in large scale sensor networks, where optimizations involving tens of thousands of nodes can be solved in just a few minutes. 1</p><br/>
<h2>reference text</h2><p>[1] P. Biswas, T.-C. Liang, K.-C. Toh, T.-C. Wang, and Y. Ye. Semideﬁnite programming approaches for sensor network localization with noisy distance measurements. IEEE Transactions on Automation Science and Engineering, 3(4):360–371, 2006.</p>
<p>[2] B. Borchers. CSDP, a C library for semideﬁnite programming. Optimization Methods and Software 11(1):613-623, 1999.</p>
<p>[3] M. Bowling, A. Ghodsi, and D. Wilkinson. Action respecting embedding. In Proceedings of the Twenty Second International Conference on Machine Learning (ICML-05), pages 65–72, Bonn, Germany, 2005.</p>
<p>[4] O. Chapelle, B. Sch¨ lkopf, and A. Zien, editors. Semi-Supervised Learning. MIT Press, Cambridge, MA, o 2006.</p>
<p>[5] F. R. K. Chung. Spectral Graph Theory. American Mathematical Society, 1997.</p>
<p>[6] F. Lu, S. Keles, S. Wright, and G. Wahba. Framework for kernel regularization with application to protein clustering. Proceedings of the National Academy of Sciences, 102:12332–12337, 2005.</p>
<p>[7] F. Lu, Y. Lin, and G. Wahba. Robust manifold unfolding with kernel regularization. Technical Report 1108, Department of Statistics, University of Wisconsin-Madison, 2005.</p>
<p>[8] F. Sha and L. K. Saul. Analysis and extension of spectral methods for nonlinear dimensionality reduction. In Proceedings of the Twenty Second International Conference on Machine Learning (ICML-05), pages 785–792, Bonn, Germany, 2005.</p>
<p>[9] J. Sun, S. Boyd, L. Xiao, and P. Diaconis. The fastest mixing Markov process on a graph and a connection to a maximum variance unfolding problem. SIAM Review, 48(4):681–699, 2006.</p>
<p>[10] L. Vandenberghe and S. P. Boyd. Semideﬁnite programming. SIAM Review, 38(1):49–95, March 1996.</p>
<p>[11] K. Q. Weinberger, B. D. Packer, and L. K. Saul. Nonlinear dimensionality reduction by semideﬁnite programming and kernel matrix factorization. In Z. Ghahramani and R. Cowell, editors, Proceedings of the Tenth International Workshop on Artiﬁcial Intelligence and Statistics (AISTATS-05), pages 381–388, Barbados, West Indies, 2005.</p>
<p>[12] K. Q. Weinberger and L. K. Saul. Unsupervised learning of image manifolds by semideﬁnite programming. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR-04), volume 2, pages 988–995, Washington D.C., 2004. Extended version in International Journal of Computer Vision, 70(1): 77-90, 2006.</p>
<p>[13] K. Q. Weinberger and L. K. Saul. An introduction to nonlinear dimensionality reduction by maximum variance unfolding. In Proceedings of the Twenty First National Conference on Artiﬁcial Intelligence (AAAI-06), Cambridge, MA, 2006.</p>
<p>[14] K. Q. Weinberger, F. Sha, and L. K. Saul. Learning a kernel matrix for nonlinear dimensionality reduction. In Proceedings of the Twenty First International Conference on Machine Learning (ICML-04), pages 839–846, Banff, Canada, 2004.</p>
<p>[15] L. Xiao, J. Sun, and S. Boyd. A duality view of spectral methods for dimensionality reduction. In Proceedings of the Twenty Third International Conference on Machine Learning (ICML-06), pages 1041– 1048, Pittsburgh, PA, 2006.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
