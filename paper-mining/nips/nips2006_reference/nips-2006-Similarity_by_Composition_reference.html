<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>174 nips-2006-Similarity by Composition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-174" href="../nips2006/nips-2006-Similarity_by_Composition.html">nips2006-174</a> <a title="nips-2006-174-reference" href="#">nips2006-174-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>174 nips-2006-Similarity by Composition</h1>
<br/><p>Source: <a title="nips-2006-174-pdf" href="http://papers.nips.cc/paper/3127-similarity-by-composition.pdf">pdf</a></p><p>Author: Oren Boiman, Michal Irani</p><p>Abstract: We propose a new approach for measuring similarity between two signals, which is applicable to many machine learning tasks, and to many signal types. We say that a signal S1 is “similar” to a signal S2 if it is “easy” to compose S1 from few large contiguous chunks of S2 . Obviously, if we use small enough pieces, then any signal can be composed of any other. Therefore, the larger those pieces are, the more similar S1 is to S2 . This induces a local similarity score at every point in the signal, based on the size of its supported surrounding region. These local scores can in turn be accumulated in a principled information-theoretic way into a global similarity score of the entire S1 to S2 . “Similarity by Composition” can be applied between pairs of signals, between groups of signals, and also between different portions of the same signal. It can therefore be employed in a wide variety of machine learning problems (clustering, classiﬁcation, retrieval, segmentation, attention, saliency, labelling, etc.), and can be applied to a wide range of signal types (images, video, audio, biological data, etc.) We show a few such examples. 1</p><br/>
<h2>reference text</h2><p>[1] S. Altschul, W. Gish, W. Miller, E. Myers, and D. Lipman. Basic local alignment search tool. JMolBiol, 215:403–410, 1990.</p>
<p>[2] E. Bart and S. Ullman. Class-based matching of object parts. In VideoRegister04, page 173, 2004.</p>
<p>[3] A. Birnbaum. On the foundations of statistical inference. J. Amer. Statist. Assoc, 1962.</p>
<p>[4] M. Blank, L. Gorelick, E. Shechtman, M. Irani, and R. Basri. Actions as space-time shapes. In ICCV05.</p>
<p>[5] O. Boiman and M. Irani. Detecting irregularities in images and in video. In ICCV05, pages I: 462–469.</p>
<p>[6] P. Felzenszwalb and D. Huttenlocher. Pictorial structures for object recognition. IJCV, 61, 2005.</p>
<p>[7] R. Fergus, P. Perona, and A. Zisserman. Object class recognition by unsupervised scale-invariant learning. In CVPR03.</p>
<p>[8] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In CVPR06.</p>
<p>[9] D. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60(2):91–110, 2004.</p>
<p>[10] J. Shi and J. Malik. Normalized cuts and image segmentation. PAMI, 22(8):888–905, August 2000.</p>
<p>[11] J. Sivic, B. Russell, A. Efros, A. Zisserman, and W. Freeman. Discovering objects and their localization in images. In ICCV05, pages I: 370–377.</p>
<p>[12] P. Viola and W. Wells, III. Alignment by maximization of mutual information. In ICCV95, pages 16–23.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
