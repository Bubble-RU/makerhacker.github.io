<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>161 nips-2006-Particle Filtering for Nonparametric Bayesian Matrix Factorization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-161" href="../nips2006/nips-2006-Particle_Filtering_for_Nonparametric_Bayesian_Matrix_Factorization.html">nips2006-161</a> <a title="nips-2006-161-reference" href="#">nips2006-161-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>161 nips-2006-Particle Filtering for Nonparametric Bayesian Matrix Factorization</h1>
<br/><p>Source: <a title="nips-2006-161-pdf" href="http://papers.nips.cc/paper/3147-particle-filtering-for-nonparametric-bayesian-matrix-factorization.pdf">pdf</a></p><p>Author: Frank Wood, Thomas L. Griffiths</p><p>Abstract: Many unsupervised learning problems can be expressed as a form of matrix factorization, reconstructing an observed data matrix as the product of two matrices of latent variables. A standard challenge in solving these problems is determining the dimensionality of the latent matrices. Nonparametric Bayesian matrix factorization is one way of dealing with this challenge, yielding a posterior distribution over possible factorizations of unbounded dimensionality. A drawback to this approach is that posterior estimation is typically done using Gibbs sampling, which can be slow for large problems and when conjugate priors cannot be used. As an alternative, we present a particle ﬁlter for posterior estimation in nonparametric Bayesian matrix factorization models. We illustrate this approach with two matrix factorization models and show favorable performance relative to Gibbs sampling.</p><br/>
<h2>reference text</h2><p>[1] T. L. Grifﬁths and Z. Ghahramani, “Inﬁnite latent feature models and the Indian buffet process,” Gatsby Computational Neuroscience Unit, Tech. Rep. 2005-001, 2005.</p>
<p>[2] F. Wood, T. L. Grifﬁths, and Z. Ghahramani, “A non-parametric Bayesian method for inferring hidden causes,” in Proceeding of the 22nd Conference on Uncertainty in Artiﬁcial Intelligence. in press, 2006.</p>
<p>[3] T. Ferguson, “A Bayesian analysis of some nonparametric problems,” The Annals of Statistics, vol. 1, pp. 209–230, 1973.</p>
<p>[4] R. M. Neal, “Markov chain sampling methods for Dirichlet process mixture models,” Department of Statistics, University of Toronto, Tech. Rep. 9815, 1998.</p>
<p>[5] P. Fearnhead, “Particle ﬁlters for mixture models with an unknown number of components,” Journal of Statistics and Computing, vol. 14, pp. 11–21, 2004.</p>
<p>[6] S. N. MacEachern, M. Clyde, and J. Liu, “Sequential importance sampling for nonparametric Bayes models: the next generation,” The Canadian Journal of Statistics, vol. 27, pp. 251–267, 1999.</p>
<p>[7] T. Grifﬁths and Z. Ghahramani, “Inﬁnite latent feature models and the Indian buffet process,” in Advances in Neural Information Processing Systems 18, Y. Weiss, B. Sch¨ lkopf, and J. Platt, Eds. Cambridge, o MA: MIT Press, 2006.</p>
<p>[8] A. Doucet, N. de Freitas, and N. Gordon, Sequential Monte Carlo Methods in Practice. Springer, 2001.</p>
<p>[9] D. G¨ r¨ r, F. J¨ kel, and C. R. Rasmussen, “A choice model with inﬁnitely many latent features,” in Proou a ceeding of the 23rd International Conference on Machine Learning, 2006.</p>
<p>[10] S. Barnett, Matrix Methods for Engineers and Scientists.</p>
<p>[11] J. Pearl, Probabilistic reasoning in intelligent systems.  McGraw-Hill, 1979. San Francisco, CA: Morgan Kaufmann, 1988.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
