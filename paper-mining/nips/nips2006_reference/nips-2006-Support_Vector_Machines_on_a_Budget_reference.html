<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>186 nips-2006-Support Vector Machines on a Budget</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-186" href="../nips2006/nips-2006-Support_Vector_Machines_on_a_Budget.html">nips2006-186</a> <a title="nips-2006-186-reference" href="#">nips2006-186-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>186 nips-2006-Support Vector Machines on a Budget</h1>
<br/><p>Source: <a title="nips-2006-186-pdf" href="http://papers.nips.cc/paper/3086-support-vector-machines-on-a-budget.pdf">pdf</a></p><p>Author: Ofer Dekel, Yoram Singer</p><p>Abstract: The standard Support Vector Machine formulation does not provide its user with the ability to explicitly control the number of support vectors used to deﬁne the generated classiﬁer. We present a modiﬁed version of SVM that allows the user to set a budget parameter B and focuses on minimizing the loss attained by the B worst-classiﬁed examples while ignoring the remaining examples. This idea can be used to derive sparse versions of both L1-SVM and L2-SVM. Technically, we obtain these new SVM variants by replacing the 1-norm in the standard SVM formulation with various interpolation-norms. We also adapt the SMO optimization algorithm to our setting and report on some preliminary experimental results. 1</p><br/>
<h2>reference text</h2><p>[1] B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for optimal margin classiﬁers. In Proc. of the Fifth Annual ACM Workshop on Computational Learning Theory, pages 144–152, 1992.</p>
<p>[2] V. N. Vapnik. Statistical Learning Theory. Wiley, 1998.</p>
<p>[3] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines. Cambridge University Press, 2000.</p>
<p>[4] P. Bartlett and A. Tewari. Sparseness vs estimating conditional probabilities: Some asymptotic results. In Proc. of the Seventeenth Annual Conference on Computational Learning Theory, pages 564–578, 2004.</p>
<p>[5] J. C. Platt. Fast training of Support Vector Machines using sequential minimal optimization. In B. Sch¨ lkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learno ing. MIT Press, 1998.</p>
<p>[6] C.J.C. Burges. Simpliﬁed support vector decision rules. In Proc. of the Thirteenth International Conference on Machine Learning, pages 71–77, 1996.</p>
<p>[7] E. Osuna and F. Girosi. Reducing the run-time complexity of support vector machines. In B. Sch¨ lkopf, o C. Burges, and A. Smola, editors, Advances in Kernel Methods: Support Vector Learning, pages 271–284. MIT Press, 1999.</p>
<p>[8] B. Sch¨ lkopf, S. Mika, C.J.C. Burges, P. Knirsch, K-R M¨ ller, G. R¨ tsch, and A.J. Smola. Input space o u a versus feature space in kernel-based methods. IEEE Transactions on Neural Networks, 10(5):1000–1017, September 1999.</p>
<p>[9] J-H. Chen and C-S. Chen. Reducing SVM classiﬁcation time using multiple mirror classiﬁers. IEEE transactions on systems, man and cybernetics – part B: Cybernetics, 34(2):1173–1183, April 2004.</p>
<p>[10] M. Wu, B. Sch¨ lkopf, and G. Bakir. A direct method for building sparse kernel learning algorithms. o Journal of Machine Learning Research, 7:603–624, 2006.</p>
<p>[11] K.P. Bennett. Combining support vector and mathematical programming methods for classiﬁcation. In Advances in kernel methods: support vector learning, pages 307–326. MIT Press, 1999.</p>
<p>[12] Y. Lee and O.L. Mangasarian. RSVM: Reduced support vector machines. In Proc. of the First SIAM International Conference on Data Mining, 2001.</p>
<p>[13] K. Crammer, J. Kandola, and Y. Singer. Online classiﬁcation on a budget. In Advances in Neural Information Processing Systems 16, 2003.</p>
<p>[14] O. Dekel, S. Shalev-Shwartz, and Y. Singer. The Forgetron: A kernel-based perceptron on a ﬁxed budget. In Advances in Neural Information Processing Systems 18, 2005.</p>
<p>[15] N. Cesa-Bianchi and C. Gentile. Tracking the best hyperplane with a simple budget perceptron. In Proc. of the Nineteenth Annual Conference on Computational Learning Theory, 2006.</p>
<p>[16] N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical Society, 68(3):337–404, May 1950.</p>
<p>[17] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, 1985.</p>
<p>[18] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.</p>
<p>[19] C. Bennett and R. Sharpley. Interpolation of Operators. Academic Press, 1998.</p>
<p>[20] T. Holmstedt. Interpolation of quasi-normed spaces. Mathematica Scandinavica, 26:177–190, 1970.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
