<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>163 nips-2006-Prediction on a Graph with a Perceptron</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-163" href="../nips2006/nips-2006-Prediction_on_a_Graph_with_a_Perceptron.html">nips2006-163</a> <a title="nips-2006-163-reference" href="#">nips2006-163-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>163 nips-2006-Prediction on a Graph with a Perceptron</h1>
<br/><p>Source: <a title="nips-2006-163-pdf" href="http://papers.nips.cc/paper/3100-prediction-on-a-graph-with-a-perceptron.pdf">pdf</a></p><p>Author: Mark Herbster, Massimiliano Pontil</p><p>Abstract: We study the problem of online prediction of a noisy labeling of a graph with the perceptron. We address both label noise and concept noise. Graph learning is framed as an instance of prediction on a ﬁnite set. To treat label noise we show that the hinge loss bounds derived by Gentile [1] for online perceptron learning can be transformed to relative mistake bounds with an optimal leading constant when applied to prediction on a ﬁnite set. These bounds depend crucially on the norm of the learned concept. Often the norm of a concept can vary dramatically with only small perturbations in a labeling. We analyze a simple transformation that stabilizes the norm under perturbations. We derive an upper bound that depends only on natural properties of the graph – the graph diameter and the cut size of a partitioning of the graph – which are only indirectly dependent on the size of the graph. The impossibility of such bounds for the graph geodesic nearest neighbors algorithm will be demonstrated. 1</p><br/>
<h2>reference text</h2><p>[1] C. Gentile. The robustness of the p-norm algorithms. Machine Learning, 53(3):265–299, 2003.</p>
<p>[2] A. Blum and S. Chawla. Learning from labeled and unlabeled data using graph mincuts. In ICML 2002, pages 19–26. Morgan Kaufmann, San Francisco, CA, 2002.</p>
<p>[3] R. I. Kondor and J. Lafferty. Diffusion kernels on graphs and other discrete input spaces. In ICML 2002, pages 315–322. Morgan Kaufmann, San Francisco, CA, 2002.</p>
<p>[4] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic functions. In ICML 2003, pages 912–919, 2003.</p>
<p>[5] A. Smola and R.I. Kondor. Kernels and regularization on graphs. In COLT 2003, pages 144–158, 2003.</p>
<p>[6] M. Belkin, I. Matveeva, and P. Niyogi. Regularization and semi-supervised learning on large graphs. In COLT 2004, pages 624 – 638, Banff, Alberta, 2004. Springer.</p>
<p>[7] T. Zhang and R. Ando. Analysis of spectral kernel design based semi-supervised learning. In Y. Weiss, B. Sch¨ lkopf, and J. Platt, editors, NIPS 18, pages 1601–1608. MIT Press, Cambridge, MA, 2006. o</p>
<p>[8] M. Herbster, M. Pontil, and L. Wainer. Online learning over graphs. In ICML 2005, pages 305–312, New York, NY, USA, 2005. ACM Press.</p>
<p>[9] Y. Freund and R. E. Schapire. Large margin classiﬁcation using the perceptron algorithm. Machine Learning, 37(3):277–296, 1999.</p>
<p>[10] D. Klein and M. Randi´ . Resistance distance. Journal of Mathematical Chemistry, 12(1):81–95, 1993. c</p>
<p>[11] J. M. Barzdin and R. V. Frievald. On the prediction of general recursive functions. Soviet Math. Doklady, 13:1224–1228, 1972.</p>
<p>[12] N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc., 68:337–404, 1950.</p>
<p>[13] D. Karger and C. Stein. A new approach to the minimum cut problem. JACM, 43(4):601–640, 1996.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
