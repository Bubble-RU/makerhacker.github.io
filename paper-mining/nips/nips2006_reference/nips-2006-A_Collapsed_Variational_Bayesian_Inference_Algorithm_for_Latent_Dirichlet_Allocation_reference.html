<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2 nips-2006-A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-2" href="../nips2006/nips-2006-A_Collapsed_Variational_Bayesian_Inference_Algorithm_for_Latent_Dirichlet_Allocation.html">nips2006-2</a> <a title="nips-2006-2-reference" href="#">nips2006-2-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>2 nips-2006-A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation</h1>
<br/><p>Source: <a title="nips-2006-2-pdf" href="http://papers.nips.cc/paper/3113-a-collapsed-variational-bayesian-inference-algorithm-for-latent-dirichlet-allocation.pdf">pdf</a></p><p>Author: Yee W. Teh, David Newman, Max Welling</p><p>Abstract: Latent Dirichlet allocation (LDA) is a Bayesian network that has recently gained much popularity in applications ranging from document modeling to computer vision. Due to the large scale nature of these applications, current inference procedures like variational Bayes and Gibbs sampling have been found lacking. In this paper we propose the collapsed variational Bayesian inference algorithm for LDA, and show that it is computationally efﬁcient, easy to implement and signiﬁcantly more accurate than standard variational Bayesian inference for LDA.</p><br/>
<h2>reference text</h2><p>[1] D. Heckerman. A tutorial on learning with Bayesian networks. In M. I. Jordan, editor, Learning in Graphical Models. Kluwer Academic Publishers, 1999.</p>
<p>[2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. JMLR, 3, 2003.</p>
<p>[3] W. Buntine. Variational extensions to EM and multinomial PCA. In ECML, 2002.</p>
<p>[4] M. Rosen-Zvi, T. Grifﬁths, M. Steyvers, and P. Smyth. The author-topic model for authors and documents. In UAI, 2004.</p>
<p>[5] T. L. Grifﬁths and M. Steyvers. Finding scientiﬁc topics. In PNAS, 2004.</p>
<p>[6] L. Fei-Fei and P. Perona. A Bayesian hierarchical model for learning natural scene categories. In CVPR, 2005.</p>
<p>[7] T. P. Minka and J. Lafferty. Expectation propagation for the generative aspect model. In UAI, 2002.</p>
<p>[8] W. Buntine and A. Jakulin. Applying discrete PCA in data analysis. In UAI, 2004.</p>
<p>[9] M. Opper and O. Winther. From naive mean ﬁeld theory to the TAP equations. In D. Saad and M. Opper, editors, Advanced Mean Field Methods : Theory and Practice. The MIT Press, 2001.</p>
<p>[10] G. Casella and C. P. Robert. Rao-Blackwellisation of sampling schemes. Biometrika, 83(1):81–94, 1996.</p>
<p>[11] M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, Gatsby Computational Neuroscience Unit, University College London, 2003.</p>
<p>[12] J. Sung, Z. Ghahramani, and S. Choi. Variational Bayesian EM: A second-order approach. Unpublished manuscript, 2005.</p>
<p>[13] W. Li and A. McCallum. Pachinko allocation: DAG-structured mixture models of topic correlations. In ICML, 2006.</p>
<p>[14] E. M. Airoldi, D. M. Blei, E. P. Xing, and S. E. Fienberg. Mixed membership stochastic block models for relational data with application to protein-protein interactions. In Proceedings of the International Biometrics Society Annual Meeting, 2006.</p>
<p>[15] M. Welling and K. Kurihara. Bayesian K-means as a “maximization-expectation” algorithm. In SIAM Conference on Data Mining, 2006.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
