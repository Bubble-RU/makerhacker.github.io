<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>118 nips-2006-Learning to Model Spatial Dependency: Semi-Supervised Discriminative Random Fields</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-118" href="../nips2006/nips-2006-Learning_to_Model_Spatial_Dependency%3A_Semi-Supervised_Discriminative_Random_Fields.html">nips2006-118</a> <a title="nips-2006-118-reference" href="#">nips2006-118-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>118 nips-2006-Learning to Model Spatial Dependency: Semi-Supervised Discriminative Random Fields</h1>
<br/><p>Source: <a title="nips-2006-118-pdf" href="http://papers.nips.cc/paper/3029-learning-to-model-spatial-dependency-semi-supervised-discriminative-random-fields.pdf">pdf</a></p><p>Author: Chi-hoon Lee, Shaojun Wang, Feng Jiao, Dale Schuurmans, Russell Greiner</p><p>Abstract: We present a novel, semi-supervised approach to training discriminative random ﬁelds (DRFs) that efﬁciently exploits labeled and unlabeled training data to achieve improved accuracy in a variety of image processing tasks. We formulate DRF training as a form of MAP estimation that combines conditional loglikelihood on labeled data, given a data-dependent prior, with a conditional entropy regularizer deﬁned on unlabeled data. Although the training objective is no longer concave, we develop an efﬁcient local optimization procedure that produces classiﬁers that are more accurate than ones based on standard supervised DRF training. We then apply our semi-supervised approach to train DRFs to segment both synthetic and real data sets, and demonstrate signiﬁcant improvements over supervised DRFs in each case.</p><br/>
<h2>reference text</h2><p>[1] Y. Altun, D. McAllester, and M. Belkin. Maximum margin semi-supervised learning for structured variables. In NIPS 18. 2006.</p>
<p>[2] J. Besag. On the statistical analysis of dirty pictures. Journal of Royal Statistical Society. Series B, 48:3:259–302, 1986.</p>
<p>[3] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In COLT, 1998.</p>
<p>[4] Yuri Boykov, Olga Veksler, and Ramin Zabih. Fast approximate energy minimization via graph cuts. In ICCV (1), pages 377–384, 1999.</p>
<p>[5] G. Celeux and G. Govaert. A classiﬁcation EM algorithm for clustering and two stochastic versions. Comput. Stat. Data Anal., 14(3):315–332, 1992.</p>
<p>[6] A. Corduneanu and T. Jaakkola. Data dependent regularization. In O. Chapelle, B. Schoelkopf, and A. Zien, editors, Semi-Supervised Learning. MIT Press, 2006.</p>
<p>[7] C. Garcia and J.A. Moreno. Kernel based method for segmentation and modeling of magnetic resonance images. LNCS, 3315:636–645, Oct 2004.</p>
<p>[8] Y. Grandvalet and Y. Bengio. Semi-supervised learning by entropy minimization. In NIPS 17, 2004.</p>
<p>[9] F. Jiao, S. Wang, C. Lee, R. Greiner, and D Schuurmans. Semi-supervised conditional random ﬁelds for improved sequence segmentation and labeling. In COLING/ACL, 2006.</p>
<p>[10] S. Kumar and M. Hebert. Discriminative ﬁelds for modeling spatial dependencies in natural images. In NIPS 16, 2003.</p>
<p>[11] S. Kumar and M. Hebert. Discriminative random ﬁelds: A discriminative framework for contextual interaction in classiﬁcation. In CVPR, 2003.</p>
<p>[12] J. Lafferty, F. Pereira, and A. McCallum. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In ICML, 2001.</p>
<p>[13] C. Lee, R. Greiner, and O. Za¨ane. Efﬁcient spatial classiﬁcation using decoupled conditional ı random ﬁelds. In 10th European Conference on Principles and Practice of Knowledge Discovery in Databases, pages 272–283, 2006.</p>
<p>[14] K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classiﬁcation from labeled and unlabeled documents using EM. Machine Learning, 39(2/3):103–134, 2000.</p>
<p>[15] A. Quattoni, M. Collins, and T. Darrell. Conditional random ﬁelds for object recognition. In NIPS 17, 2004.</p>
<p>[16] S. Roberts, R. Everson, and I. Rezek. Maximum certainty data partitioning, 2000.</p>
<p>[17] A. Torralba, K. Murphy, and W. Freeman. Contextual models for object detection using boosted random ﬁelds. In NIPS 17, 2004.</p>
<p>[18] V. Vapnik. Statistical Learning Theory. John-Wiley, 1998.</p>
<p>[19] S.V.N. Vishwanathan, N. Schraudolph, M. Schmidt, and K. Murphy. Accelerated training of conditional random ﬁelds with stochastic gradient methods. In ICML, 2006.</p>
<p>[20] J. Yedidia, W. Freeman, and Y. Weiss. Generalized belief propagation. In NIPS 13, pages 689–695, 2000.</p>
<p>[21] J. Zhang, K. Ma, M.H. Er, and V. Chong. Tumor segmentation from magnetic resonance imaging by learning via one-class support vector machine. Intl. Workshop on Advanced Image Technology, 2004.</p>
<p>[22] D. Zhou, O. Bousquet, T. Navin Lal, J. Weston, and B. Sch¨ lkopf. Learning with local and o global consistency. In NIPS 16, 2004.</p>
<p>[23] D. Zhou, J. Huang, and B. Sch¨ lkopf. Learning from labeled and unlabeled data on a directed o graph. In ICML, 2005.</p>
<p>[24] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic functions. In ICML, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
