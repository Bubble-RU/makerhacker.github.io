<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>151 nips-2006-On the Relation Between Low Density Separation, Spectral Clustering and Graph Cuts</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-151" href="../nips2006/nips-2006-On_the_Relation_Between_Low_Density_Separation%2C_Spectral_Clustering_and_Graph_Cuts.html">nips2006-151</a> <a title="nips-2006-151-reference" href="#">nips2006-151-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>151 nips-2006-On the Relation Between Low Density Separation, Spectral Clustering and Graph Cuts</h1>
<br/><p>Source: <a title="nips-2006-151-pdf" href="http://papers.nips.cc/paper/3000-on-the-relation-between-low-density-separation-spectral-clustering-and-graph-cuts.pdf">pdf</a></p><p>Author: Hariharan Narayanan, Mikhail Belkin, Partha Niyogi</p><p>Abstract: One of the intuitions underlying many graph-based methods for clustering and semi-supervised learning, is that class or cluster boundaries pass through areas of low probability density. In this paper we provide some formal analysis of that notion for a probability distribution. We introduce a notion of weighted boundary volume, which measures the length of the class/cluster boundary weighted by the density of the underlying probability distribution. We show that sizes of the cuts of certain commonly used data adjacency graphs converge to this continuous weighted volume of the boundary. keywords: Clustering, Semi-Supervised Learning 1</p><br/>
<h2>reference text</h2><p>[1] M. Belkin and P. Niyogi (2004).“Semi-supervised Learning on Riemannian Manifolds.” In Machine Learning 56, Special Issue on Clustering, 209-239.</p>
<p>[2]</p>
<p>[3]</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]  M. Belkin and P. Niyogi. “Toward a theoretical foundation for Laplacian-based manifold methods.” COLT 2005. A. Blum and S. Chawla, “Learning from labeled and unlabeled data using graph mincuts“, ICML 2001. O.Chapelle, J. Weston, B. Scholkopf, “Cluster kernels for semi-supervised learning”, NIPS 2002. O. Chapelle and A. Zien, “Semi-supervised Classiﬁcation by Low Density Separation”, AISTATS 2005. Chris Ding, Spectral Clustering, ICML 2004 Tutorial. Samuel Kutin, Partha Niyogi, “Almost-everywhere Algorithmic Stability and Generalization Error.”, UAI 2002, 275-282 S. Kutin, TR-2002-04, “Extensions to McDiarmid’s inequality when differences are bounded with high probability.” Technical report TR-2002-04 at the Department of Computer Science, University of Chicago. S. Lafon, Diffusion Maps and Geodesic Harmonics, Ph. D. Thesis, Yale University, 2004. M. Hein, J.-Y. Audibert, U. von Luxburg, From Graphs to Manifolds – Weak and Strong Pointwise Consistency of Graph Laplacians, COLT 2005. U. von Luxburg, M. Belkin, O. Bousquet, Consistency of Spectral Clustering, Max Planck Institute for Biological Cybernetics Technical Report TR 134, 2004. M. Meila and J. Shi. A Random Walks View of Spectral Segmentation, NIPS 2001.</p>
<p>[13] B. Nadler, S. Lafon, R. R. Coifman,and I. G. Kevrekidis. Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators, NIPS 2006.</p>
<p>[14] J. Shi and J. Malik. “Normalized cuts and image segmentation.”</p>
<p>[15] X. Zhu, J. Lafferty and Z. Ghahramani, Semi-supervised learning using Gaussian ﬁelds and harmonic functions, ICML 2003.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
