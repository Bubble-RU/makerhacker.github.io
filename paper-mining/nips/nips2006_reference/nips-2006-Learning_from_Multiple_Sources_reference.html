<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>116 nips-2006-Learning from Multiple Sources</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-116" href="../nips2006/nips-2006-Learning_from_Multiple_Sources.html">nips2006-116</a> <a title="nips-2006-116-reference" href="#">nips2006-116-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>116 nips-2006-Learning from Multiple Sources</h1>
<br/><p>Source: <a title="nips-2006-116-pdf" href="http://papers.nips.cc/paper/2972-learning-from-multiple-sources.pdf">pdf</a></p><p>Author: Koby Crammer, Michael Kearns, Jennifer Wortman</p><p>Abstract: We consider the problem of learning accurate models from multiple sources of “nearby” data. Given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. This theory is applicable in a broad decision-theoretic learning framework, and yields results for classiﬁcation and regression generally, and for density estimation within the exponential family. A key component of our approach is the development of approximate triangle inequalities for expected loss, which may be of independent interest. 1</p><br/>
<h2>reference text</h2><p>[1]</p>
<p>[2]</p>
<p>[3]</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]  K. Crammer, M. Kearns, and J. Wortman. Learning from data of variable quality. In NIPS 18, 2006. P. Wu and T. Dietterich. Improving SVM accuracy by training on auxiliary data sources. In ICML, 2004. J. Baxter. Learning internal representations. In COLT, 1995. S. Ben-David. Exploiting task relatedness for multiple task learning. In COLT, 2003. D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning applications. Information and Computation, 1992. V. N. Vapnik. Statistical Learning Theory. Wiley, 1998. M. Kearns and R. Schapire. Efﬁcient distribution-free learning of probabilistic concepts. JCSS, 1994. Y. Censor and S.A. Zenios. Parallel Optimization: Theory, Algorithms, and Applications. Oxford University Press, New York, NY, USA, 1997. M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Technical Report 649, Department of Statistics, University of California, Berkeley, 2003. P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 2002. V. Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Trans. Info. Theory, 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
