<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>74 nips-2006-Efficient Structure Learning of Markov Networks using $L 1$-Regularization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-74" href="../nips2006/nips-2006-Efficient_Structure_Learning_of_Markov_Networks_using_%24L_1%24-Regularization.html">nips2006-74</a> <a title="nips-2006-74-reference" href="#">nips2006-74-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>74 nips-2006-Efficient Structure Learning of Markov Networks using $L 1$-Regularization</h1>
<br/><p>Source: <a title="nips-2006-74-pdf" href="http://papers.nips.cc/paper/3003-efficient-structure-learning-of-markov-networks-using-l_1-regularization.pdf">pdf</a></p><p>Author: Su-in Lee, Varun Ganapathi, Daphne Koller</p><p>Abstract: Markov networks are commonly used in a wide variety of applications, ranging from computer vision, to natural language, to computational biology. In most current applications, even those that rely heavily on learned models, the structure of the Markov network is constructed by hand, due to the lack of effective algorithms for learning Markov network structure from data. In this paper, we provide a computationally efﬁcient method for learning Markov network structure from data. Our method is based on the use of L1 regularization on the weights of the log-linear model, which has the effect of biasing the model towards solutions where many of the parameters are zero. This formulation converts the Markov network learning problem into a convex optimization problem in a continuous space, which can be solved using efﬁcient gradient methods. A key issue in this setting is the (unavoidable) use of approximate inference, which can lead to errors in the gradient computation when the network structure is dense. Thus, we explore the use of different feature introduction schemes and compare their performance. We provide results for our method on synthetic data, and on two real world data sets: pixel values in the MNIST data, and genetic sequence variations in the human HapMap data. We show that our L1 -based method achieves considerably higher generalization performance than the more standard L2 -based method (a Gaussian parameter prior) or pure maximum-likelihood learning. We also show that we can learn MRF network structure at a computational cost that is not much greater than learning parameters alone, demonstrating the existence of a feasible method for this important problem.</p><br/>
<h2>reference text</h2><p>[1] F. Bach and M. Jordan. Thin junction trees. In NIPS 14, 2002.</p>
<p>[2] F.R. Bach, G.R.G. Lanckriet, and M.I. Jordan. Multiple kernel learning, conic duality, and the smo algorithm, 2004.</p>
<p>[3] The International HapMap Consortium. The international hapmap project. Nature, 426:789–796, 2003.</p>
<p>[4] Robert G. Cowell and David J. Spiegelhalter. Probabilistic Networks and Expert Systems. Springer-Verlag New York, Inc., Secaucus, NJ, USA, 1999.</p>
<p>[5] H. Daum´ III. Notes on CG and LM-BFGS optimization of logistic regression. August 2004. e</p>
<p>[6] S. Della Pietra, V.J. Della Pietra, and J.D. Lafferty. Inducing features of random ﬁelds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380–393, 1997.</p>
<p>[7] A. Deshpande, M.N. Garofalakis, and M.I. Jordan. Efﬁcient stepwise selection in decomposable models. In Proc. UAI, pages 128–135, 2001.</p>
<p>[8] D. Donoho and X. Huo. Uncertainty principles and ideal atomic decomposition, 1999.</p>
<p>[9] A. Genkin, D. D. Lewis, and D. Madigan. Large-scale bayesian logistic regression for text categorization. 2004.</p>
<p>[10] J. Goodman. Exponential priors for maximum entropy models. In North American ACL, 2005.</p>
<p>[11] Alexander T. Ihler, John W. Fischer III, and Alan S. Willsky. Loopy belief propagation: Convergence and effects of message errors. J. Mach. Learn. Res., 6:905–936, 2005.</p>
<p>[12] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, November 1998.</p>
<p>[13] Martijn A. R. Leisink and Hilbert J. Kappen. General lower bounds based on computer generated higher order expansions. In UAI, pages 293–300, 2002.</p>
<p>[14] A. McCallum. Efﬁciently inducing features of conditional random ﬁelds. In Proc. UAI, 2003.</p>
<p>[15] Thomas P. Minka. Algorithms for maximum-likelihood logistic regression. 2001.</p>
<p>[16] K. P. Murphy, Y. Weiss, and M. I. Jordan. Loopy belief propagation for approximate inference: an empirical study. pages 467–475, 1999.</p>
<p>[17] J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, 1988.</p>
<p>[18] S. Perkins, K. Lacker, and J. Theiler. Grafting: Fast, incremental feature selection by gradient descent in function space. 3(2003):1333–1356, 2003.</p>
<p>[19] Stefan Riezler and Alexander Vasserman. Incremental feature selection and l1 regularization for relaxed maximum-entropy modeling. In Proceedings of EMNLP 2004.</p>
<p>[20] Sekhar Tatikonda and Michael I. Jordan. Loopy belief propogation and gibbs measures. In UAI, pages 493–500, 2002.</p>
<p>[21] R Tibshirani. Regression shrinkage and selection via the lasso. J. Royal. Statist. Soc B, 1996.</p>
<p>[22] Martin J. Wainwright, Pradeep Ravikumar, and Lafferty. Inferring graphical model structure using 1 regularized pseudo-likelihood. In Advances in Neural Information Processing Systems 19, 2007.</p>
<p>[23] Martin J. Wainwright, Erik B. Sudderth, and Alan S. Willsky. Tree-based modeling and estimation of gaussian processes on graphs with cycles. In Todd K. Leen, Thomas G. Dietterich, and Volker Tresp, editors, Advances in Neural Information Processing Systems 13, pages 661–667. MIT Press, 2001.</p>
<p>[24] Jonathan S. Yedidia, William T. Freeman, and Yair Weiss. Generalized belief propagation. In Advances in Neural Information Processing Systems 13. MIT Press, 2001.</p>
<p>[25] J. Zhu, S. Rosset, T. Hastie, and R. Tibshirani. 1-norm support vector machines. In Proc. NIPS, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
