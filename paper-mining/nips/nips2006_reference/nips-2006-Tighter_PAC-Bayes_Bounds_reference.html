<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>193 nips-2006-Tighter PAC-Bayes Bounds</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-193" href="../nips2006/nips-2006-Tighter_PAC-Bayes_Bounds.html">nips2006-193</a> <a title="nips-2006-193-reference" href="#">nips2006-193-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>193 nips-2006-Tighter PAC-Bayes Bounds</h1>
<br/><p>Source: <a title="nips-2006-193-pdf" href="http://papers.nips.cc/paper/3058-tighter-pac-bayes-bounds.pdf">pdf</a></p><p>Author: Amiran Ambroladze, Emilio Parrado-hernández, John S. Shawe-taylor</p><p>Abstract: This paper proposes a PAC-Bayes bound to measure the performance of Support Vector Machine (SVM) classiﬁers. The bound is based on learning a prior over the distribution of classiﬁers with a part of the training samples. Experimental work shows that this bound is tighter than the original PAC-Bayes, resulting in an enhancement of the predictive capabilities of the PAC-Bayes bound. In addition, it is shown that the use of this bound as a means to estimate the hyperparameters of the classiﬁer compares favourably with cross validation in terms of accuracy of the model, while saving a lot of computational burden. 1</p><br/>
<h2>reference text</h2><p>[1] C L Blake and C J Merz. UCI Repository of machine learning databases. University of California, Irvine, Dept. of Information and Computer Sciences, [http://www.ics.uci.edu/∼mlearn/MLRepository.html], 1998.</p>
<p>[2] Bernhard E. Boser, Isabelle Guyon, and Vladimir Vapnik. A training algorithm for optimal margin classiﬁers. In Computational Learing Theory, pages 144–152, 1992.</p>
<p>[3] J Langford. Tutorial on practical prediction theory for classiﬁcation. Journal of Machine Learning Research, 6(Mar):273–306, 2005.</p>
<p>[4] J Langford and J Shawe-Taylor. PAC-Bayes & Margins. In Advances in Neural Information Processing Systems, volume 14, Cambridge MA, 2002. MIT Press.</p>
<p>[5] D McAllester. Pac-bayesian stochastic model selection. Machine Learning, 51(1):5–21, 2003.</p>
<p>[6] M Seeger. PAC-Bayesian Generalization Error Bounds for Gaussian Process Classiﬁcation. Journal of Machine Learning Research, 3:233–269, 2002.</p>
<p>[7] J Shawe-Taylor, P L Bartlett, R C Williamson, and M Anthony. Structural risk minimization over data-dependent hierarchies. IEEE Trans. Information Theory, 44(5):1926 – 1940, 1998.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
