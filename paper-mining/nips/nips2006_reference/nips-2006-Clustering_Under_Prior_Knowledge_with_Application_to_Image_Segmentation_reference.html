<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-51" href="../nips2006/nips-2006-Clustering_Under_Prior_Knowledge_with_Application_to_Image_Segmentation.html">nips2006-51</a> <a title="nips-2006-51-reference" href="#">nips2006-51-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>51 nips-2006-Clustering Under Prior Knowledge with Application to Image Segmentation</h1>
<br/><p>Source: <a title="nips-2006-51-pdf" href="http://papers.nips.cc/paper/3008-clustering-under-prior-knowledge-with-application-to-image-segmentation.pdf">pdf</a></p><p>Author: Dong S. Cheng, Vittorio Murino, Mário Figueiredo</p><p>Abstract: This paper proposes a new approach to model-based clustering under prior knowledge. The proposed formulation can be interpreted from two different angles: as penalized logistic regression, where the class labels are only indirectly observed (via the probability density of each class); as ﬁnite mixture learning under a grouping prior. To estimate the parameters of the proposed model, we derive a (generalized) EM algorithm with a closed-form E-step, in contrast with other recent approaches to semi-supervised probabilistic clustering which require Gibbs sampling or suboptimal shortcuts. We show that our approach is ideally suited for image segmentation: it avoids the combinatorial nature Markov random ﬁeld priors, and opens the door to more sophisticated spatial priors (e.g., wavelet-based) in a simple and computationally efﬁcient way. Finally, we extend our formulation to work in unsupervised, semi-supervised, or discriminative modes. 1</p><br/>
<h2>reference text</h2><p>[1] M. Figueiredo. “Bayesian image segmentation using wavelet-based priors”, Proc. IEEE Conf. Computer Vision and Pattern Recognition - CVPR’2005, San Diego, CA, 2005.</p>
<p>[2] N. Balram, J. Moura. “Noncausal Gauss-Markov random ﬁelds: parameter structure and estimation”, IEEE Trans. Information Theory, vol. 39, pp. 1333–1355, 1993.</p>
<p>[3] A. Banerjee, S. Merugu, I. Dhillon, J. Ghosh. “Clustering with Bregman divergences.” Proc. SIAM Intern. Conf. Data Mining – SDM’2004, Lake Buena Vista, FL, 2004.</p>
<p>[4] S. Basu, M. Bilenko, R. Mooney. “A probabilistic framework for semi-supervised clustering.” Proc. of the KDD-2004, Seattle, WA, 2004.</p>
<p>[5] D. B¨ hning. “Multinomial logistic regression”, Annals Inst. Stat. Math., vol. 44, pp. 197-200, 1992. o</p>
<p>[6] G. Chang, B. Yu, M. Vetterli. “Adaptive wavelet thresholding for image denoising and compression.” IEEE Trans. Image Proc., vol. 9, pp. 1532–1546, 2000.</p>
<p>[7] T. Hastie, R. Tibshirani, J. Friedman. The Elements of Statistical Learning, Springer, 2001.</p>
<p>[8] K. I. Kim, K. Jung, S. H. Park, H. J. Kim. “Support vector machines for texture classiﬁcation.” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 24, pp. 1542–1550, 2002.</p>
<p>[9] D. Hunter, K. Lange. “A tutorial on MM algorithms”, The American Statistician, vol. 58, pp. 30–37, 2004.</p>
<p>[10] M. Law, A. Topchy, A. K. Jain. “Model-based clustering with probabilistic constraints.” In Proc. of the SIAM Conf. on Data Mining, pp. 641-645, Newport Beach, CA, 2005.</p>
<p>[11] Z. Lu, T. Leen. “Probabilistic penalized clustering.” In NIPS 17, MIT Press, 2005.</p>
<p>[12] S. Mallat. A Wavelet Tour of Signal Proc.. Academic Press, San Diego, CA, 1998.</p>
<p>[13] G. McLachlan, T. Krishnan. The EM Algorithm and Extensions, Wiley, N. York, 1997.</p>
<p>[14] P. Moulin, J. Liu. “Analysis of multiresolution image denoising schemes using generalized - Gaussian and complexity priors,” IEEE Trans. Inform. Theory, vol. 45, pp. 909–919, 1999.</p>
<p>[15] N. Shental, A. Bar-Hillel, T. Hertz, D. Weinshall. “Computing Gaussian mixture models with EM using equivalence constraints.” In NIPS 15, MIT Press, Cambridge, MA, 2003.</p>
<p>[16] J. Shi, J. Malik, “Normalized cuts and image segmentation.” IEEE-TPAMI, vol. 22, pp. 888–905, 2000.</p>
<p>[17] K. Wagstaff, C. Cardie, S. Rogers, S. Schr¨ dl. “Constrained K-means clustering with background knowlo edge.” In Proc. of ICML’2001, Williamstown, MA, 2001.</p>
<p>[18] C. Wu. “On the convergence properties of the EM algorithm,” Ann. Statistics, vol. 11, pp. 95-103, 1983.</p>
<p>[19] R. Zabih, V. Kolmogorov, “Spatially coherent clustering with graph cuts.” Proc. IEEE-CVPR, vol. II, pp. 437–444, 2004.</p>
<p>[20] X. Zhu. “Semi-Supervised Learning Literature Survey”, TR-1530, Comp. Sci. Dept., Univ. of Wisconsin, Madison, 2006. Available at www.cs.wisc.edu/˜jerryzhu/pub/ssl_survey.pdf</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
