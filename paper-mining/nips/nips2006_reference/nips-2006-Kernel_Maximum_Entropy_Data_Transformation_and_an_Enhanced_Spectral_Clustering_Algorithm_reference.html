<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>102 nips-2006-Kernel Maximum Entropy Data Transformation and an Enhanced Spectral Clustering Algorithm</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-102" href="../nips2006/nips-2006-Kernel_Maximum_Entropy_Data_Transformation_and_an_Enhanced_Spectral_Clustering_Algorithm.html">nips2006-102</a> <a title="nips-2006-102-reference" href="#">nips2006-102-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>102 nips-2006-Kernel Maximum Entropy Data Transformation and an Enhanced Spectral Clustering Algorithm</h1>
<br/><p>Source: <a title="nips-2006-102-pdf" href="http://papers.nips.cc/paper/3121-kernel-maximum-entropy-data-transformation-and-an-enhanced-spectral-clustering-algorithm.pdf">pdf</a></p><p>Author: Robert Jenssen, Torbjørn Eltoft, Mark Girolami, Deniz Erdogmus</p><p>Abstract: We propose a new kernel-based data transformation technique. It is founded on the principle of maximum entropy (MaxEnt) preservation, hence named kernel MaxEnt. The key measure is Renyi’s entropy estimated via Parzen windowing. We show that kernel MaxEnt is based on eigenvectors, and is in that sense similar to kernel PCA, but may produce strikingly different transformed data sets. An enhanced spectral clustering algorithm is proposed, by replacing kernel PCA by kernel MaxEnt as an intermediate step. This has a major impact on performance.</p><br/>
<h2>reference text</h2><p>[1] S. Roweis and L. Saul, “Nonlinear Dimensionality Reduction by Locally Linear Embedding,” Science, vol. 290, pp. 2323–2326, 2000.  100  80  80  Performance %  Performance %  100  60 40 KPCA ME NG  20 0  0.5  1 σ  1.5  60 40 20  KPCA ME NG  0 1  2  2  3 σ  (a)  4  (b)  100  Performance %  80 60 40 KPCA ME NG  20 0  0.5  1  1.5  σ  2  2.5  3  (c)  Figure 3: Clustering results.</p>
<p>[2] J. Tenenbaum, V. de Silva, and J. C. Langford, “A Global Geometric Framework for Nonlinear Dimensionality Reduction,” Science, vol. 290, pp. 2319–2323, 2000.</p>
<p>[3] B. Sch¨ lkopf, A. J. Smola, and K. R. M¨ ller, “Nonlinear Component Analysis as a Kernel Eigenvalue o u Problem,” Neural Computation, vol. 10, pp. 1299–1319, 1998.</p>
<p>[4] J. Shawe-Taylor and N. Cristianini, Kernel Methods for Pattern Analysis, Cambridge University Press, 2004.</p>
<p>[5] R. Jenssen, D. Erdogmus, J. C. Principe, and T. Eltoft, “The Laplacian PDF Distance: A Cost Function for Clustering in a Kernel Feature Space,” in Advances in Neural Information Processing Systems 17, MIT Press, Cambridge, 2005, pp. 625–632.</p>
<p>[6] R. Jenssen, D. Erdogmus, J. C. Principe, and T. Eltoft, “Some Equivalences between Kernel Methods and Information Theoretic Methods,” Journal of VLSI Signal Processing, to appear, 2006.</p>
<p>[7] R. Jenssen, D. Erdogmus, J. C. Principe, and T. Eltoft, “Information Theoretic Angle-Based Spectral Clustering: A Theoretical Analysis and an Algorithm,” in Proceedings of International Joint Conference on Neural Networks, Vancouver, Canada, July 16-21, 2006, pp. 4904–4911.</p>
<p>[8] M. Girolami, “Orthogonal Series Density Estimation and the Kernel Eigenvalue Problem,” Neural Computation, vol. 14, no. 3, pp. 669–688, 2002.</p>
<p>[9] A. Renyi, “On Measures of Entropy and Information,” Selected Papers of Alfred Renyi, Akademiai Kiado, Budapest, vol. 2, pp. 565–580, 1976.</p>
<p>[10] E. Parzen, “On the Estimation of a Probability Density Function and the Mode,” The Annals of Mathematical Statistics, vol. 32, pp. 1065–1076, 1962.</p>
<p>[11] A. Y. Ng, M. Jordan, and Y. Weiss, “On Spectral Clustering: Analysis and an Algorithm,” in Advances in Neural Information Processing Systems, 14, MIT Press, Cambridge, 2002, pp. 849–856.</p>
<p>[12] B. W. Silverman, Density Estimation for Statistics and Data Analysis, Chapman and Hall, London, 1986.</p>
<p>[13] G. R¨ etsch, T. Onoda, and K. R. M¨ ller, “Soft Margins for Adaboost,” Machine Learning, vol. 42, pp. a u 287–320, 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
