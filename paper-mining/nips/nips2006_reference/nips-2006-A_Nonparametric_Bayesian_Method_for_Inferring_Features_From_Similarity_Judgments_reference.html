<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 nips-2006-A Nonparametric Bayesian Method for Inferring Features From Similarity Judgments</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-9" href="../nips2006/nips-2006-A_Nonparametric_Bayesian_Method_for_Inferring_Features_From_Similarity_Judgments.html">nips2006-9</a> <a title="nips-2006-9-reference" href="#">nips2006-9-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>9 nips-2006-A Nonparametric Bayesian Method for Inferring Features From Similarity Judgments</h1>
<br/><p>Source: <a title="nips-2006-9-pdf" href="http://papers.nips.cc/paper/3136-a-nonparametric-bayesian-method-for-inferring-features-from-similarity-judgments.pdf">pdf</a></p><p>Author: Daniel J. Navarro, Thomas L. Griffiths</p><p>Abstract: The additive clustering model is widely used to infer the features of a set of stimuli from their similarities, on the assumption that similarity is a weighted linear function of common features. This paper develops a fully Bayesian formulation of the additive clustering model, using methods from nonparametric Bayesian statistics to allow the number of features to vary. We use this to explore several approaches to parameter estimation, showing that the nonparametric Bayesian approach provides a straightforward way to obtain estimates of both the number of features used in producing similarity judgments and their importance. 1</p><br/>
<h2>reference text</h2><p>[1] W. S. Torgerson. Theory and Methods of Scaling. Wiley, New York, 1958.</p>
<p>[2] R. N. Shepard and P. Arabie. Additive clustering: Representation of similarities as combinations of discrete overlapping properties. Psychological Review, 86:87–123, 1979.</p>
<p>[3] J. B. Tenenbaum. Learning the structure of similarity. In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo, editors, Advances in Neural Information Processing Systems , volume 8, pages 3–9. MIT Press, Cambridge, MA, 1996.</p>
<p>[4] L. L. Thurstone. Multiple-Factor Analysis. University of Chicago Press, Chicago, 1947.</p>
<p>[5] M. D. Lee. Generating additive clustering models with limited stochastic complexity. Journal of Classiﬁcation, 19:69–85, 2002.</p>
<p>[6] T. L. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process. Technical Report 2005-001, Gatsby Computational Neuroscience Unit, 2005.</p>
<p>[7] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721–741, 1984.</p>
<p>[8] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. Equations of state calculations by fast computing machines. Journal of Chemical Physics , 21:1087–1092, 1953.</p>
<p>[9] Q.-M. Shao M.-H. Chen and J. G. Ibrahim. Monte Carlo Methods in Bayesian Computation . Springer, New York, 2000.</p>
<p>[10] M. K. Cowles and B. P. Carlin. Markov chain Monte Carlo convergence diagnostics: A comparative review. Journal of the American Statistical Association , 91:833–904, 1996.</p>
<p>[11] P. Arabie and J. Douglas Carroll. MAPCLUS: A mathematical programming approach to ﬁtting the ADCLUS model. Psychometrika, 45:211–235, 1980.</p>
<p>[12] W. Ruml. Constructing distributed representations using additive clustering. In Advances in Neural Information Processing Systems 14, Cambridge, MA, 2001. MIT Press.</p>
<p>[13] M. D. Lee and D. J. Navarro. Extending the ALCOVE model of category learning to featural stimulus domains. Psychonomic Bulletin and Review, 9:43–58, 2002.</p>
<p>[14] D. J. Navarro. Representing Stimulus Similarity. Ph.D. Thesis, University of Adelaide, 2003.</p>
<p>[15] L. E. Frank and W. J. Heiser. Feature selection in Feature Network Models: Finding predictive subsets of features with the Positive Lasso. British Journal of Mathematical and Statistical Psychology , in press.</p>
<p>[16] D. L. Medin and A. Ortony. Psychological essentialism. In Similarity and Analogical Reasoning . Cambridge University Press, New York, 1989.</p>
<p>[17] R. N. Shepard, D. W. Kilpatric, and J. P. Cunningham. The internal representation of numbers. Cognitive Psychology, 7:82–138, 1975.</p>
<p>[18] D. J. Navarro and M. D. Lee. Commonalities and distinctions in featural stimulus representations. In Proceedings of the 24th Annual Conference of the Cognitive Science Society , pages 685–690, Mahwah, NJ, 2002. Lawrence Erlbaum.</p>
<p>[19] E. Z. Rothkopf. A measure of stimulus similarity and errors in some paired-associate learning tasks. Journal of Experimental Psychology, 53:94–101, 1957.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
