<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>25 nips-2006-An Application of Reinforcement Learning to Aerobatic Helicopter Flight</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-25" href="../nips2006/nips-2006-An_Application_of_Reinforcement_Learning_to_Aerobatic_Helicopter_Flight.html">nips2006-25</a> <a title="nips-2006-25-reference" href="#">nips2006-25-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>25 nips-2006-An Application of Reinforcement Learning to Aerobatic Helicopter Flight</h1>
<br/><p>Source: <a title="nips-2006-25-pdf" href="http://papers.nips.cc/paper/3151-an-application-of-reinforcement-learning-to-aerobatic-helicopter-flight.pdf">pdf</a></p><p>Author: Pieter Abbeel, Adam Coates, Morgan Quigley, Andrew Y. Ng</p><p>Abstract: Autonomous helicopter ﬂight is widely regarded to be a highly challenging control problem. This paper presents the ﬁrst successful autonomous completion on a real RC helicopter of the following four aerobatic maneuvers: forward ﬂip and sideways roll at low speed, tail-in funnel, and nose-in funnel. Our experimental results signiﬁcantly extend the state of the art in autonomous helicopter ﬂight. We used the following approach: First we had a pilot ﬂy the helicopter to help us ﬁnd a helicopter dynamics model and a reward (cost) function. Then we used a reinforcement learning (optimal control) algorithm to ﬁnd a controller that is optimized for the resulting model and reward function. More speciﬁcally, we used differential dynamic programming (DDP), an extension of the linear quadratic regulator (LQR). 1</p><br/>
<h2>reference text</h2><p>[1] P. Abbeel, Varun Ganapathi, and Andrew Y. Ng. Learning vehicular dynamics with application to modeling helicopters. In NIPS 18, 2006.</p>
<p>[2] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proc. ICML, 2004.</p>
<p>[3] P. Abbeel and A. Y. Ng. Exploration and apprenticeship learning in reinforcement learning. In Proc. ICML, 2005.</p>
<p>[4] P. Abbeel and A. Y. Ng. Learning ﬁrst order Markov models for control. In NIPS 18, 2005.</p>
<p>[5] B. Anderson and J. Moore. Optimal Control: Linear Quadratic Methods. Prentice-Hall, 1989.</p>
<p>[6] J. Bagnell and J. Schneider. Autonomous helicopter control using reinforcement learning policy search methods. In International Conference on Robotics and Automation. IEEE, 2001.</p>
<p>[7] Ronen I. Brafman and Moshe Tennenholtz. R-max, a general polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning Research, 2002.</p>
<p>[8] V. Gavrilets, I. Martinos, B. Mettler, and E. Feron. Flight test and simulation results for an autonomous aerobatic helicopter. In AIAA/IEEE Digital Avionics Systems Conference, 2002.</p>
<p>[9] V. Gavrilets, B. Mettler, and E. Feron. Human-inspired control logic for automated maneuvering of miniature helicopter. Journal of Guidance, Control, and Dynamics, 27(5):752–759, 2004.</p>
<p>[10] S. Kakade, M. Kearns, and J. Langford. Exploration in metric state spaces. In Proc. ICML, 2003.</p>
<p>[11] M. Kearns and D. Koller. Efﬁcient reinforcement learning in factored MDPs. In Proc. IJCAI, 1999.</p>
<p>[12] M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. Machine Learning Journal, 2002.</p>
<p>[13] M. La Civita, G. Papageorgiou, W. C. Messner, and T. Kanade. Design and ﬂight testing of a highbandwidth H∞ loop shaping controller for a robotic helicopter. Journal of Guidance, Control, and Dynamics, 29(2):485–494, March-April 2006.</p>
<p>[14] J. Leishman. Principles of Helicopter Aerodynamics. Cambridge University Press, 2000.</p>
<p>[15] B. Mettler, M. Tischler, and T. Kanade. System identiﬁcation of small-size unmanned helicopter dynamics. In American Helicopter Society, 55th Forum, 1999.</p>
<p>[16] A. Y. Ng, A. Coates, M. Diel, V. Ganapathi, J. Schulte, B. Tse, E. Berger, and E. Liang. Autonomous inverted helicopter ﬂight via reinforcement learning. In Int’l Symposium on Experimental Robotics, 2004.</p>
<p>[17] Andrew Y. Ng, H. Jin Kim, Michael Jordan, and Shankar Sastry. Autonomous helicopter ﬂight via reinforcement learning. In NIPS 16, 2004.</p>
<p>[18] Jonathan M. Roberts, Peter I. Corke, and Gregg Buskey. Low-cost ﬂight control system for a small autonomous helicopter. In IEEE Int’l Conf. on Robotics and Automation, 2003.</p>
<p>[19] S. Saripalli, J. F. Montgomery, and G. S. Sukhatme. Visually-guided landing of an unmanned aerial vehicle. IEEE Transactions on Robotics and Autonomous Systems, 2003.</p>
<p>[20] J. Seddon. Basic Helicopter Aerodynamics. AIAA Education Series. America Institute of Aeronautics and Astronautics, 1990.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
