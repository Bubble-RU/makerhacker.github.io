<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>157 nips-2006-PAC-Bayes Bounds for the Risk of the Majority Vote and the Variance of the Gibbs Classifier</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-157" href="../nips2006/nips-2006-PAC-Bayes_Bounds_for_the_Risk_of_the_Majority_Vote_and_the_Variance_of_the_Gibbs_Classifier.html">nips2006-157</a> <a title="nips-2006-157-reference" href="#">nips2006-157-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>157 nips-2006-PAC-Bayes Bounds for the Risk of the Majority Vote and the Variance of the Gibbs Classifier</h1>
<br/><p>Source: <a title="nips-2006-157-pdf" href="http://papers.nips.cc/paper/2959-pac-bayes-bounds-for-the-risk-of-the-majority-vote-and-the-variance-of-the-gibbs-classifier.pdf">pdf</a></p><p>Author: Alexandre Lacasse, François Laviolette, Mario Marchand, Pascal Germain, Nicolas Usunier</p><p>Abstract: We propose new PAC-Bayes bounds for the risk of the weighted majority vote that depend on the mean and variance of the error of its associated Gibbs classiﬁer. We show that these bounds can be smaller than the risk of the Gibbs classiﬁer and can be arbitrarily close to zero even if the risk of the Gibbs classiﬁer is close to 1/2. Moreover, we show that these bounds can be uniformly estimated on the training data for all possible posteriors Q. Moreover, they can be improved by using a large sample of unlabelled data. 1</p><br/>
<h2>reference text</h2><p>[1] David McAllester. Some PAC-Bayesian theorems. Machine Learning, 37:355–363, 1999.</p>
<p>[2] David McAllester. PAC-Bayesian stochastic model selection. Machine Learning, 51:5–21, 2003.</p>
<p>[3] David McAllester. Simpliﬁed PAC-Bayesian margin bounds. Proceedings of the 16th Annual Conference on Learning Theory, Lecture Notes in Artiﬁcial Intelligence, 2777:203–215, 2003.</p>
<p>[4] Francois Laviolette and Mario Marchand. PAC-Bayes risk bounds for sample-compressed Gibbs classiﬁers. ¸ Proc. of the 22nth International Conference on Machine Learning (ICML 2005), pages 481–488, 2005.</p>
<p>[5] John Langford and John Shawe-Taylor. PAC-Bayes & margins. In S. Thrun S. Becker and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 423–430. MIT Press, Cambridge, MA, 2003.</p>
<p>[6] Luc Devroye, L´ szl´ Gy¨ rﬁ, and G´ bor Lugosi. A Probabilistic Theory of Pattern Recognition. Springer a o o a Verlag, New York, NY, 1996.</p>
<p>[7] Leo Breiman. Random forests. Machine Learning, 45(1):5–32, 2001.</p>
<p>[8] Dimitris Bertsimas and Ioana Popescu. Optimal inequalities in probability theory: A convex optimization approach. SIAM J. on Optimization, 15(3):780–804, 2005.</p>
<p>[9] Robert E. Schapire and Yoram Singer. Improved boosting using conﬁdence-rated predictions. Machine Learning, 37(3):297–336, 1999.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
