<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>129 nips-2006-Map-Reduce for Machine Learning on Multicore</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-129" href="../nips2006/nips-2006-Map-Reduce_for_Machine_Learning_on_Multicore.html">nips2006-129</a> <a title="nips-2006-129-reference" href="#">nips2006-129-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>129 nips-2006-Map-Reduce for Machine Learning on Multicore</h1>
<br/><p>Source: <a title="nips-2006-129-pdf" href="http://papers.nips.cc/paper/3150-map-reduce-for-machine-learning-on-multicore.pdf">pdf</a></p><p>Author: Cheng-tao Chu, Sang K. Kim, Yi-an Lin, Yuanyuan Yu, Gary Bradski, Kunle Olukotun, Andrew Y. Ng</p><p>Abstract: We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and uniﬁed way for machine learning to take advantage of the potential speed up. In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. Speciﬁcally, we show that algorithms that ﬁt the Statistical Query model [15] can be written in a certain “summation form,” which allows them to be easily parallelized on multicore computers. We adapt Google’s map-reduce [7] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM, ICA, PCA, gaussian discriminant analysis (GDA), EM, and backpropagation (NN). Our experimental results show basically linear speedup with an increasing number of processors. 1</p><br/>
<h2>reference text</h2><p>[1] Sejnowski TJ. Bell AJ. An information-maximization approach to blind separation and blind deconvolution. In Neural Computation, 1995.</p>
<p>[2] O. Chapelle. Training a support vector machine in the primal. Journal of Machine Learning Research (submitted), 2006.</p>
<p>[3] W. S. Cleveland and S. J. Devlin. Locally weighted regression: An approach to regression analysis by local ﬁtting. In J. Amer. Statist. Assoc. 83, pages 596–610, 1988.</p>
<p>[4] L. Csanky. Fast parallel matrix inversion algorithms. SIAM J. Comput., 5(4):618–623, 1976.</p>
<p>[5] A. Silvescu D. Caragea and V. Honavar. A framework for learning from distributed data using sufﬁcient statistics and its application to learning decision trees. International Journal of Hybrid Intelligent Systems, 2003.</p>
<p>[6] R. J. Williams D. E. Rumelhart, G. E. Hinton. Learning representation by back-propagating errors. In Nature, volume 323, pages 533–536, 1986.</p>
<p>[7] J. Dean and S. Ghemawat. Mapreduce: Simpliﬁed data processing on large clusters. Operating Systems Design and Implementation, pages 137–149, 2004.</p>
<p>[8] N.M. Dempster A.P., Laird and Rubin D.B.</p>
<p>[9] D.J. Frank. Power-constrained cmos scaling limits. IBM Journal of Research and Development, 46, 2002.</p>
<p>[10] P. Gelsinger. Microprocessors for the new millennium: Challenges, opportunities and new frontiers. In ISSCC Tech. Digest, pages 22–25, 2001.</p>
<p>[11] Leon Bottou Igor Durdanovic Hans Peter Graf, Eric Cosatto and Vladimire Vapnik. Parallel support vector machines: The cascade svm. In NIPS, 2004.</p>
<p>[12] J. Hartigan. Clustering Algorithms. Wiley, 1975.</p>
<p>[13] T. Hastie and R. Tibshirani. Discriminant analysis by gaussian mixtures. Journal of the Royal Statistical Society B, pages 155–176, 1996.</p>
<p>[14] R. Jin and G. Agrawal. Shared memory parallelization of data mining algorithms: Techniques, programming interface, and performance. In Second SIAM International Conference on Data Mining,, 2002.</p>
<p>[15] M. Kearns. Efﬁcient noise-tolerant learning from statistical queries. pages 392–401, 1999.</p>
<p>[16] Michael Kearns and Umesh V. Vazirani. An Introduction to Computational Learning Theory. MIT Press, 1994.</p>
<p>[17] David Lewis. Naive (bayes) at forty: The independence asssumption in information retrieval. In ECML98: Tenth European Conference On Machine Learning, 1998.</p>
<p>[18] Kun Liu and Hillow Kargupta. Distributed data mining bibliography. http://www.cs.umbc.edu/ hillol/DDMBIB/, 2006.</p>
<p>[19] T. K. MOON. The expectation-maximization algorithm. In IEEE Trans. Signal Process, pages 47–59, 1996.</p>
<p>[20] G. Moore. Progress in digital integrated electronics. In IEDM Tech. Digest, pages 11–13, 1975.</p>
<p>[21] Wayne Iba Pat Langley and Kevin Thompson. An analysis of bayesian classiﬁers. In AAAI, 1992.</p>
<p>[22] John C. Platt. Fast training of support vector machines using sequential minimal optimization. pages 185–208, 1999.</p>
<p>[23] Daryl Pregibon. Logistic regression diagnostics. In The Annals of Statistics, volume 9, pages 705–724, 1981.</p>
<p>[24] T. Studt. There’s a multicore in your future, http://tinyurl.com/ohd2m, 2006.</p>
<p>[25] Herb Sutter and James Larus. Software and the concurrency revolution. Queue, 3(7):54–62, 2005.</p>
<p>[26] L.G. Valiant. A theory of the learnable. Communications of the ACM, 3(11):1134–1142, 1984.</p>
<p>[27] V. Vapnik. Estimation of Dependencies Based on Empirical Data. Springer Verlag, 1982.</p>
<p>[28] R. E. Welsch and E. KUH. Linear regression diagnostics. In Working Paper 173, Nat. Bur. Econ. Res.Inc, 1977.</p>
<p>[29] K. Esbensen Wold, S. and P. Geladi. Principal component analysis. In Chemometrics and Intelligent Laboratory Systems, 1987.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
