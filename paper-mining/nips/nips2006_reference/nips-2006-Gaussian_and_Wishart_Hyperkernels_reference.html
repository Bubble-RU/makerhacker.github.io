<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>82 nips-2006-Gaussian and Wishart Hyperkernels</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-82" href="../nips2006/nips-2006-Gaussian_and_Wishart_Hyperkernels.html">nips2006-82</a> <a title="nips-2006-82-reference" href="#">nips2006-82-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>82 nips-2006-Gaussian and Wishart Hyperkernels</h1>
<br/><p>Source: <a title="nips-2006-82-pdf" href="http://papers.nips.cc/paper/3099-gaussian-and-wishart-hyperkernels.pdf">pdf</a></p><p>Author: Risi Kondor, Tony Jebara</p><p>Abstract: We propose a new method for constructing hyperkenels and deﬁne two promising special cases that can be computed in closed form. These we call the Gaussian and Wishart hyperkernels. The former is especially attractive in that it has an interpretable regularization scheme reminiscent of that of the Gaussian RBF kernel. We discuss how kernel learning can be used not just for improving the performance of classiﬁcation and regression methods, but also as a stand-alone algorithm for dimensionality reduction and relational or metric learning. 1</p><br/>
<h2>reference text</h2><p>[1] N. Cristianini, J. Shawe-Taylor, A. Elisseeﬀ, and J. Kandola. On kernel-target alignment. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 367 – 373, Cambridge, MA, 2002. MIT Press.</p>
<p>[2] G. S. Kimeldorf and G. Wahba. Some results on Tchebycheﬃan spline functions. J. Math. Anal. Applic., 33:82–95, 1971.</p>
<p>[3] R. Kondor and T. Jebara. A kernel between sets of vectors. In Machine Learning: Tenth International Conference, ICML 2003, 2003.</p>
<p>[4] R. Kondor and J. Laﬀerty. Diﬀusion kernels on graphs and other discrete input spaces. In Machine Learning: Proceedings of the Nineteenth International Conference (ICML ’02), 2002.</p>
<p>[5] G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. Learning the kernel matrix with semi-deﬁnite programming. Journal of Machine Learning Research, 5:27 – 72, 2004.</p>
<p>[6] T. P. Minka. Inferring a Gaussian distribution, 2001. http://www.stat.cmu.edu/ minka/papers/learning.html.  Tutorial paper available at</p>
<p>[7] C. S. Ong and A. J. Smola. Machine learning using hyperkernels. In Proceedings of the International Conference on Machine Learning, 2003.</p>
<p>[8] Cheng Soon Ong, Alexander J. Smola, and Robert C. Williamson. Hyperkernels. In S. Thrun S. Becker and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 478–485. MIT Press, Cambridge, MA, 2003.</p>
<p>[9] Cheng Soon Ong, Alexander J. Smola, and Robert C. Williamson. Learning the kernel with hyperkernels. Sumbitted to the Journal of Machine Learning Research, 2003.</p>
<p>[10] B. Sch¨lkopf and A. J. Smola. Learning with Kernels. MIT Press, 2002. o</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
