<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 nips-2006-Correcting Sample Selection Bias by Unlabeled Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-62" href="../nips2006/nips-2006-Correcting_Sample_Selection_Bias_by_Unlabeled_Data.html">nips2006-62</a> <a title="nips-2006-62-reference" href="#">nips2006-62-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>62 nips-2006-Correcting Sample Selection Bias by Unlabeled Data</h1>
<br/><p>Source: <a title="nips-2006-62-pdf" href="http://papers.nips.cc/paper/3075-correcting-sample-selection-bias-by-unlabeled-data.pdf">pdf</a></p><p>Author: Jiayuan Huang, Arthur Gretton, Karsten M. Borgwardt, Bernhard Schölkopf, Alex J. Smola</p><p>Abstract: We consider the scenario where training and test data are drawn from different distributions, commonly referred to as sample selection bias. Most algorithms for this setting try to ﬁrst recover sampling distributions and then make appropriate corrections based on the distribution estimate. We present a nonparametric method which directly produces resampling weights without distribution estimation. Our method works by matching distributions between training and testing sets in feature space. Experimental results demonstrate that our method works well in practice.</p><br/>
<h2>reference text</h2><p>[1] G. Casella and R. Berger. Statistical Inference. Duxbury, Paciﬁc Grove, CA, 2nd edition, 2002.</p>
<p>[2] M. Dudik, R.E. Schapire, and S.J. Phillips. Correcting sample selection bias in maximum entropy density estimation. In Advances in Neural Information Processing Systems 17, 2005.</p>
<p>[3] A. Gretton, K. Borgwardt, M. Rasch, B. Sch¨ lkopf, and A. Smola. A kernel method for the two-sampleo problem. In NIPS. MIT Press, 2006.</p>
<p>[4] S. Gruvberger, M. Ringner, Y.Chen, S.Panavally, L.H. Saal, C. Peterson A.Borg, M. Ferno, and P.S.Meltzer. Estrogen receptor status in breast cancer is associated with remarkably distinct gene expression patterns. Cancer Research, 61, 2001.</p>
<p>[5] J. Heckman. Sample selection bias as a speciﬁcation error. Econometrica, 47(1):153–161, 1979.</p>
<p>[6] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58:13–30, 1963.</p>
<p>[7] J. Huang, A. Smola, A. Gretton, K. Borgwardt, and B. Sch¨ lkopf. Correcting sample selection bias by o unlabeled data. Technical report, CS-2006-44, University of Waterloo, 2006.</p>
<p>[8] Y. Lin, Y. Lee, and G. Wahba. Support vector machines for classiﬁcation in nonstandard situations. Machine Learning, 46:191–202, 2002.</p>
<p>[9] S. Rosset, J. Zhu, H. Zou, and T. Hastie. A method for inferring label sampling mechanisms in semisupervised learning. In Advances in Neural Information Processing Systems 17, 2004.</p>
<p>[10] M. Schmidt and H. Gish. Speaker identiﬁcation via support vector classiﬁers. In Proc. ICASSP ’96, pages 105–108, Atlanta, GA, May 1996.</p>
<p>[11] B. Sch¨ lkopf, J. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. Estimating the support of a o high-dimensional distribution. Neural Computation, 13(7):1443–1471, 2001.</p>
<p>[12] H. Shimodaira. Improving predictive inference under convariance shift by weighting the log-likelihood function. Journal of Statistical Planning and Inference, 90, 2000.</p>
<p>[13] D. Singh, P. Febbo, K. Ross, D. Jackson, J. Manola, C. Ladd, P. Tamayo, A. Renshaw, A. DAmico, and J. Richie. Gene expression correlates of clinical prostate cancer behavior. Cancer Cell, 1(2), 2002.</p>
<p>[14] I. Steinwart. On the inﬂuence of the kernel on the consistency of support vector machines. Journal of Machine Learning Research, 2:67–93, 2002.</p>
<p>[15] I. Steinwart. Support vector machines are universally consistent. J. Compl., 18:768–791, 2002.</p>
<p>[16] M. Sugiyama and K.-R. M¨ ller. Input-dependent estimation of generalization error under covariate shift. u Statistics and Decisions, 23:249–279, 2005.</p>
<p>[17] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 2005.</p>
<p>[18] P. Warnat, R. Eils, and B. Brors. Cross-platform analysis of cancer microarray data improves gene expression based classiﬁcation of phenotypes. BMC Bioinformatics, 6:265, Nov 2005.</p>
<p>[19] M. West, C. Blanchette, H. Dressman, E. Huang, S. Ishida, R. Spang, H Zuzan, J.A. Olson Jr, J.R.Marks, and J.R.Nevins. Predicting the clinical status of human breast cancer by using gene expression proﬁles. PNAS, 98(20), 2001.</p>
<p>[20] B. Zadrozny. Learning and evaluating classiﬁers under sample selection bias. In International Conference on Machine Learning ICML’04, 2004.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
