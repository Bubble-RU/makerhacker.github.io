<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>37 nips-2006-Attribute-efficient learning of decision lists and linear threshold functions under unconcentrated distributions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-37" href="../nips2006/nips-2006-Attribute-efficient_learning_of_decision_lists_and_linear_threshold_functions_under_unconcentrated_distributions.html">nips2006-37</a> <a title="nips-2006-37-reference" href="#">nips2006-37-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>37 nips-2006-Attribute-efficient learning of decision lists and linear threshold functions under unconcentrated distributions</h1>
<br/><p>Source: <a title="nips-2006-37-pdf" href="http://papers.nips.cc/paper/3007-attribute-efficient-learning-of-decision-lists-and-linear-threshold-functions-under-unconcentrated-distributions.pdf">pdf</a></p><p>Author: Philip M. Long, Rocco Servedio</p><p>Abstract: We consider the well-studied problem of learning decision lists using few examples when many irrelevant features are present. We show that smooth boosting algorithms such as MadaBoost can efﬁciently learn decision lists of length k over n boolean variables using poly(k, log n) many examples provided that the marginal distribution over the relevant variables is “not too concentrated” in an L 2 -norm sense. Using a recent result of H˚ stad, we extend the analysis to obtain a similar a (though quantitatively weaker) result for learning arbitrary linear threshold functions with k nonzero coefﬁcients. Experimental results indicate that the use of a smooth boosting algorithm, which plays a crucial role in our analysis, has an impact on the actual performance of the algorithm.</p><br/>
<h2>reference text</h2><p>[1] D. Angluin. Queries and concept learning. Machine Learning, 2:319–342, 1988.</p>
<p>[2] J. Barzdin and R. Freivald. On the prediction of general recursive functions. Soviet Mathematics Doklady, 13:1224–1228, 1972.</p>
<p>[3] A. Blum. Learning Boolean functions in an inﬁnite attribute space. In Proceedings of the Twenty-Second Annual Symposium on Theory of Computing, pages 64–72, 1990.</p>
<p>[4] A. Blum. On-line algorithms in machine learning. available at http://www.cs.cmu.edu/˜avrim/Papers/pubs.html, 1996.</p>
<p>[5] A. Blum, L. Hellerstein, and N. Littlestone. Learning in the presence of ﬁnitely or inﬁnitely many irrelevant attributes. Journal of Computer and System Sciences, 50:32–40, 1995.</p>
<p>[6] A. Blum and P. Langley. Selection of relevant features and examples in machine learning. Artiﬁcial Intelligence, 97(1-2):245–271, 1997.</p>
<p>[7] N. Bshouty and D. Gavinsky. On boosting with optimal poly-bounded distributions. Journal of Machine Learning Research, 3:483–506, 2002.</p>
<p>[8] C. Domingo and O. Watanabe. Madaboost: a modiﬁed version of adaboost. In Proceedings of the Thirteenth Annual Conference on Computational Learning Theory, pages 180–189, 2000.</p>
<p>[9] Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119–139, 1997.</p>
<p>[10] Dmitry Gavinsky. Optimally-smooth adaptive boosting and application to agnostic learning. Journal of Machine Learning Research, 4:101–117, 2003.</p>
<p>[11] A. Hajnal, W. Maass, P. Pudlak, M. Szegedy, and G. Turan. Threshold circuits of bounded depth. Journal of Computer and System Sciences, 46:129–154, 1993.</p>
<p>[12] S. Hampson and D. Volper. Linear function neurons: structure and training. Biological Cybernetics, 53:203–217, 1986.</p>
<p>[13] R. Impagliazzo. Hard-core distributions for somewhat hard problems. In Proceedings of the Thirty-Sixth Annual Symposium on Foundations of Computer Science, pages 538–545, 1995.</p>
<p>[14] J. Jackson and M. Craven. Learning sparse perceptrons. In NIPS 8, pages 654–660, 1996.</p>
<p>[15] A. Klivans and R. Servedio. Boosting and hard-core sets. Machine Learning, 53(3):217–238, 2003. Preliminary version in Proc. FOCS’99.</p>
<p>[16] A. Klivans and R. Servedio. Toward attribute efﬁcient learning of decision lists and parities. In Proceedings of the 17th Annual Conference on Learning Theory,, pages 224–238, 2004.</p>
<p>[17] N. Littlestone. Learning quickly when irrelevant attributes abound: a new linear-threshold algorithm. Machine Learning, 2:285–318, 1988.</p>
<p>[18] M. Minsky and S. Papert. Perceptrons: an introduction to computational geometry. MIT Press, Cambridge, MA, 1968.</p>
<p>[19] T. Mitchell. Generalization as search. Artiﬁcial Intelligence, 18:203–226, 1982.</p>
<p>[20] S. Muroga, I. Toda, and S. Takasu. Theory of majority switching elements. J. Franklin Institute, 271:376– 418, 1961.</p>
<p>[21] Z. Nevo and R. El-Yaniv. On online learning of decision lists. Journal of Machine Learning Research, 3:271–301, 2002.</p>
<p>[22] V. V. Petrov. Limit theorems of probability theory. Oxford Science Publications, Oxford, England, 1995.</p>
<p>[23] G. Pisier. Remarques sur un resultat non publi’e de B. Maurey. Sem. d’Analyse Fonctionelle, 1(12):1980– 81, 1981.</p>
<p>[24] R. Rivest. Learning decision lists. Machine Learning, 2(3):229–246, 1987.</p>
<p>[25] R. Schapire. Theoretical views of boosting. In Proc. 10th ALT, pages 12–24, 1999.</p>
<p>[26] R. Servedio. On PAC learning using Winnow, Perceptron, and a Perceptron-like algorithm. In Proceedings of the Twelfth Annual Conference on Computational Learning Theory, pages 296–307, 1999.</p>
<p>[27] R. Servedio. Smooth boosting and learning with malicious noise. Journal of Machine Learning Research, 4:633–648, 2003. Preliminary version in Proc. COLT’01.</p>
<p>[28] R. Servedio. Every linear threshold function has a low-weight approximator. In Proceedings of the 21st Conference on Computational Complexity (CCC), pages 18–30, 2006.</p>
<p>[29] L. Valiant. Projection learning. Machine Learning, 37(2):115–130, 1999.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
