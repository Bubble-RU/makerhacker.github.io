<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>173 nips-2006-Shifting, One-Inclusion Mistake Bounds and Tight Multiclass Expected Risk Bounds</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-173" href="../nips2006/nips-2006-Shifting%2C_One-Inclusion_Mistake_Bounds_and_Tight_Multiclass_Expected_Risk_Bounds.html">nips2006-173</a> <a title="nips-2006-173-reference" href="#">nips2006-173-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>173 nips-2006-Shifting, One-Inclusion Mistake Bounds and Tight Multiclass Expected Risk Bounds</h1>
<br/><p>Source: <a title="nips-2006-173-pdf" href="http://papers.nips.cc/paper/2982-shifting-one-inclusion-mistake-bounds-and-tight-multiclass-expected-risk-bounds.pdf">pdf</a></p><p>Author: Benjamin I. Rubinstein, Peter L. Bartlett, J. H. Rubinstein</p><p>Abstract: Under the prediction model of learning, a prediction strategy is presented with an i.i.d. sample of n − 1 points in X and corresponding labels from a concept f ∈ F, and aims to minimize the worst-case probability of erring on an nth point. By exploiting the structure of F, Haussler et al. achieved a VC(F)/n bound for the natural one-inclusion prediction strategy, improving on bounds implied by PAC-type results by a O(log n) factor. The key data structure in their result is the natural subgraph of the hypercube—the one-inclusion graph; the key step is a d = VC(F) bound on one-inclusion graph density. The ﬁrst main result of this n n−1 paper is a density bound of n ≤d−1 / ( ≤d ) < d, which positively resolves a conjecture of Kuzmin & Warmuth relating to their unlabeled Peeling compression scheme and also leads to an improved mistake bound for the randomized (deterministic) one-inclusion strategy for all d (for d ≈ Θ(n)). The proof uses a new form of VC-invariant shifting and a group-theoretic symmetrization. Our second main result is a k-class analogue of the d/n mistake bound, replacing the VC-dimension by the Pollard pseudo-dimension and the one-inclusion strategy by its natural hypergraph generalization. This bound on expected risk improves on known PAC-based results by a factor of O(log n) and is shown to be optimal up to a O(log k) factor. The combinatorial technique of shifting takes a central role in understanding the one-inclusion (hyper)graph and is a running theme throughout. 1</p><br/>
<h2>reference text</h2><p>[1] Ben-David, S., Cesa-Bianchi, N., Haussler, D., Long, P. M.: Characterizations of learnability for classes of {0, . . . , n}-valued functions. Journal of Computer and System Sciences, 50(1) (1995) 74–86</p>
<p>[2] Ehrenfeucht, A., Haussler, D., Kearns, M., Valiant, L.: A general lower bound on the number of examples needed for learning. Information and Computation, 82(3) (1989) 247–261</p>
<p>[3] Haussler, D.: Sphere packing numbers for subsets of the boolean n-cube with bounded VapnikChervonenkis dimension. Journal of Combinatorial Theory (A) 69(2) (1995) 217–232</p>
<p>[4] Haussler, D., Littlestone, N., Warmuth, M. K.: Predicting {0, 1} functions on randomly drawn points. Information and Computation, 115(2) (1994) 284–293</p>
<p>[5] Kuzmin, D., Warmuth, M. K.: Unlabeled compression schemes for maximum classes. Journal of Machine Learning Research (2006) to appear</p>
<p>[6] Li, Y., Long, P. M., Srinivasan, A.: The one-inclusion graph algorithm is near optimal for the prediction model of learning. IEEE Transactions on Information Theory, 47(3) (2002) 1257–1261</p>
<p>[7] Littlestone, N., Warmuth, M. K.: Relating data compression and learnability. Unpublished manuscript, http://www.cse.ucsc.edu/˜manfred/pubs/lrnk-olivier.pdf (1986)</p>
<p>[8] Rubinstein, B. I. P., Bartlett, P. L., Rubinstein, J. H.: Shifting: One-Inclusion Mistake Bounds and Sample Compression. Technical report, EECS Department, UC Berkeley (2007) to appear</p>
<p>[9] Sauer, N.: On the density of families of sets. Journal of Combinatorial Theory (A), 13 (1972) 145–147</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
