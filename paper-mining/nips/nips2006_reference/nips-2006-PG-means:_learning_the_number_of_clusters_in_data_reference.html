<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>158 nips-2006-PG-means: learning the number of clusters in data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-158" href="../nips2006/nips-2006-PG-means%3A_learning_the_number_of_clusters_in_data.html">nips2006-158</a> <a title="nips-2006-158-reference" href="#">nips2006-158-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>158 nips-2006-PG-means: learning the number of clusters in data</h1>
<br/><p>Source: <a title="nips-2006-158-pdf" href="http://papers.nips.cc/paper/3080-pg-means-learning-the-number-of-clusters-in-data.pdf">pdf</a></p><p>Author: Yu Feng, Greg Hamerly</p><p>Abstract: We present a novel algorithm called PG-means which is able to learn the number of clusters in a classical Gaussian mixture model. Our method is robust and efﬁcient; it uses statistical hypothesis tests on one-dimensional projections of the data and model to determine if the examples are well represented by the model. In so doing, we are applying a statistical test for the entire model at once, not just on a per-cluster basis. We show that our method works well in difﬁcult cases such as non-Gaussian data, overlapping clusters, eccentric clusters, high dimension, and many true clusters. Further, our new method provides a much more stable estimate of the number of clusters than existing methods. 1</p><br/>
<h2>reference text</h2><p>[1] Hirotugu Akaike. A new look at the statistical model identiﬁcation. IEEE Transactions on Automatic Control, 19:716–723, 1974.</p>
<p>[2] Gerard E. Dallal and Leland Wilkinson. An analytic approximation to the distribution of Lilliefors’ test for normality. The American Statistician, 40:294–296, 1986.</p>
<p>[3] Sanjoy Dasgupta. Experiments with random projection. In Proceedings of the Sixteenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI-2000), pages 143–151. Morgan Kaufmann Publishers, 2000.</p>
<p>[4] Greg Hamerly and Charles Elkan. Learning the k in k-means. In Proceedings of the seventeenth annual conference on neural information processing systems (NIPS), pages 281–288, 2003.</p>
<p>[5] Robert E. Kass and Larry Wasserman. A reference Bayesian test for nested hypotheses and its relationship to the schwarz criterion. Journal of the American Statistical Association, 90(431):928–934, 1995.</p>
<p>[6] Hubert W. Lilliefors. On the Kolmogorov-Smirnov test of normality with mean and variance unknown. Journal of the American Statistical Association, 62(318):399–402, 1967.</p>
<p>[7] Frank J. Massey, Jr. The Kolmogorov-Smirnov test for goodness of ﬁt. Journal of the American Statistical Association, 46(253):68–78, 1951.</p>
<p>[8] Marina Meila. Comparing clusterings by the variation of information. In COLT, pages 173–187, 2003.</p>
<p>[9] Dan Pelleg and Andrew Moore. X-means: Extending k-means with efﬁcient estimation of the number of clusters. In Proceedings of the 17th International Conf. on Machine Learning, pages 727–734. Morgan Kaufmann, 2000.</p>
<p>[10] Jorma Rissanen. Modeling by shortest data description. Automatica, 14:465–471, 1978.</p>
<p>[11] Peter Sand and Andrew W. Moore. Repairing faulty mixture models using density estimation. In Proceedings of the 18th International Conf. on Machine Learning, pages 457–464, 2001.</p>
<p>[12] Gideon Schwarz. Estimating the dimension of a model. The Annnals of Statistics, 6(2):461–464, 1978.</p>
<p>[13] Robert Tibshirani, Guenther Walther, and Trevor Hastie. Estimating the number of clusters in a dataset via the Gap statistic. Journal of the Royal Statistical Society B, 63:411–423, 2001.</p>
<p>[14] Naonori Ueda and Ryohei Nakano. Deterministic annealing em algorithm. Neural Networks, 11(2):271– 282, 1998.</p>
<p>[15] Max Welling and Kenichi Kurihara. Bayesian k-means as a ’maximization-expectation’ algorithm. In SIAM conference on Data Mining SDM06, 2006.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
