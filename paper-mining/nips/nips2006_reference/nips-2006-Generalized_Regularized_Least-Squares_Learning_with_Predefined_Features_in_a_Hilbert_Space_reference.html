<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-84" href="../nips2006/nips-2006-Generalized_Regularized_Least-Squares_Learning_with_Predefined_Features_in_a_Hilbert_Space.html">nips2006-84</a> <a title="nips-2006-84-reference" href="#">nips2006-84-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>84 nips-2006-Generalized Regularized Least-Squares Learning with Predefined Features in a Hilbert Space</h1>
<br/><p>Source: <a title="nips-2006-84-pdf" href="http://papers.nips.cc/paper/3014-generalized-regularized-least-squares-learning-with-predefined-features-in-a-hilbert-space.pdf">pdf</a></p><p>Author: Wenye Li, Kin-hong Lee, Kwong-sak Leung</p><p>Abstract: Kernel-based regularized learning seeks a model in a hypothesis space by minimizing the empirical error and the model’s complexity. Based on the representer theorem, the solution consists of a linear combination of translates of a kernel. This paper investigates a generalized form of representer theorem for kernel-based learning. After mapping predeﬁned features and translates of a kernel simultaneously onto a hypothesis space by a speciﬁc way of constructing kernels, we proposed a new algorithm by utilizing a generalized regularizer which leaves part of the space unregularized. Using a squared-loss function in calculating the empirical error, a simple convex solution is obtained which combines predeﬁned features with translates of the kernel. Empirical evaluations have conﬁrmed the effectiveness of the algorithm for supervised learning tasks.</p><br/>
<h2>reference text</h2><p>[1] V.N. Vapnik. Statistical Learning Theory. John Wiley and Sons, 1998.</p>
<p>[2] B. Sch¨ lkopf and A.J. Smola. Learning with Kernels. The MIT Press, 2002. o</p>
<p>[3] J.S. Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004.</p>
<p>[4] T. Evgeniou, M. Pontil, and T. Poggio. Regularization networks and support vector machines. Adv. Comput. Math., 13:1–50, 2000.</p>
<p>[5] T. Poggio and F. Girosi. Regularization algorithms for learning that are equivalent to multilayer networks. Science, 247:978–982, 1990.</p>
<p>[6] T. Poggio and S. Smale. The mathematics of learning: Dealing with data. Not. Am. Math. Soc, 50:537– 544, 2003.</p>
<p>[7] A.N. Tikhonov and V.Y. Arsenin. Solutions of Ill-Posed Problems. Winston and Sons, 1977.</p>
<p>[8] V.A. Morozov. Methods for Solving Incorrectly Posed Problems. Springer-Verlag, 1984.</p>
<p>[9] G. Wahba. Spline Models for Observational Data. SIAM, 1990.</p>
<p>[10] G. Kimeldorf and G. Wahba. Some results on Tchebychefﬁan spline functions. J. Math. Anal. Appl., 33:82–95, 1971.</p>
<p>[11] F. Girosi, M.J. Jones, and T. Poggio. Regularization theory and neural networks architectures. Neural Comput., 7:219–269, 1995.</p>
<p>[12] B. Sch¨ lkopf, R. Herbrich, and A.J. Smola. A generalized representer theorem. In COLT’2001 and o EuroCOLT’2001, 2001.</p>
<p>[13] R.M. Rifkin. Everything Old is New Again: A Fresh Look at Historical Approaches in Machine Learning. PhD thesis, Massachusetts Institute of Technology, 2002.</p>
<p>[14] G. Fung and O.L. Mangasarian. Proximal support vector machine classiﬁers. In KDD’01, 2001.</p>
<p>[15] J.A.K. Suykens and J. Vandewalle. Least squares support vector machine classiﬁers. Neural Process. Lett., 9:293–300, 1999.</p>
<p>[16] W. Light and H. Wayne. Spaces of distributions, interpolation by translates of a basis function and error estimates. J. Numer. Math., 81:415–450, 1999.</p>
<p>[17] R.K. Beatson, W.A. Light, and S. Billings. Fast solution of the radial basis function interpolation equations: Domain decomposition methods. SIAM J. Sci. Comput., 22:1717–1740, 2000.</p>
<p>[18] A.K. McCallum. Bow: A toolkit for statistical language modeling, text retrieval, classiﬁcation and clustering. http://www.cs.cmu.edu/∼mccallum/bow, 1996.</p>
<p>[19] T. Hofmann. Probabilistic latent semantic analysis. In UAI’99, 1999.</p>
<p>[20] C.A. Micchelli. Interpolation of scattered data: Distances, matrices, and conditionally positive deﬁnite functions. Constr. Approx., 2:11–22, 1986.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
