<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>125 nips-2006-Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-125" href="../nips2006/nips-2006-Logarithmic_Online_Regret_Bounds_for_Undiscounted_Reinforcement_Learning.html">nips2006-125</a> <a title="nips-2006-125-reference" href="#">nips2006-125-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>125 nips-2006-Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2006-125-pdf" href="http://papers.nips.cc/paper/3052-logarithmic-online-regret-bounds-for-undiscounted-reinforcement-learning.pdf">pdf</a></p><p>Author: Peter Auer, Ronald Ortner</p><p>Abstract: We present a learning algorithm for undiscounted reinforcement learning. Our interest lies in bounds for the algorithm’s online performance after some ﬁnite number of steps. In the spirit of similar methods already successfully applied for the exploration-exploitation tradeoff in multi-armed bandit problems, we use upper conﬁdence bounds to show that our UCRL algorithm achieves logarithmic online regret in the number of steps taken with respect to an optimal policy. 1 1.1</p><br/>
<h2>reference text</h2><p>[1] Michael J. Kearns and Satinder P. Singh. Near-optimal reinforcement learning in polynomial time. Mach. Learn., 49:209–232, 2002.</p>
<p>[2] Ronen I. Brafman and Moshe Tennenholtz. R-max – a general polynomial time algorithm for near-optimal reinforcement learning. J. Mach. Learn. Res., 3:213–231, 2002.</p>
<p>[3] Sham M. Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, University College London, 2003.</p>
<p>[4] Alexander L. Strehl and Michael L. Littman. A theoretical analysis of model-based interval estimation. In Proc. 22nd ICML 2005, pages 857–864, 2005.</p>
<p>[5] Alexander L. Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L. Littman. Pac model-free reinforcement learning. In Proc. 23nd ICML 2006, pages 881–888, 2006.</p>
<p>[6] Claude-Nicolas Fiechter. Efﬁcient reinforcement learning. In Proc. 7th COLT, pages 88–97. ACM, 1994.</p>
<p>[7] Peter Auer and Ronald Ortner. Online regret bounds for a new reinforcement learning algorithm. In Proc. ¨ 1st ACVW, pages 35–42. OCG, 2005.</p>
<p>[8] Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. J. Mach. Learn. Res., 3:397– 422, 2002.</p>
<p>[9] Peter Auer, Nicol` Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multi-armed bandit probo lem. Mach. Learn., 47:235–256, 2002.</p>
<p>[10] Alexander L. Strehl and Michael L. Littman. An empirical evaluation of interval estimation for Markov decision processes. In Proc. 16th ICTAI, pages 128–135. IEEE Computer Society, 2004.</p>
<p>[11] Leslie P. Kaelbling. Learning in Embedded Systems. MIT Press, 1993.</p>
<p>[12] Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for reinforcement learning. In Proc. 20th ICML, pages 162–169. AAAI Press, 2003.</p>
<p>[13] Apostolos N. Burnetas and Michael N. Katehakis. Optimal adaptive policies for Markov decision processes. Math. Oper. Res., 22(1):222–255, 1997.</p>
<p>[14] Eyal Even-Dar, Sham M. Kakade, and Yishay Mansour. Experts in a Markov decision process. In Proc. 17th NIPS, pages 401–408. MIT Press, 2004.</p>
<p>[15] Martin L. Puterman. Markov Decision Processes. Discrete Stochastic Programming. Wiley, 1994.</p>
<p>[16] Grace E. Cho and Carl D. Meyer. Markov chain sensitivity measured by mean ﬁrst passage times. Linear Algebra Appl., 316:21–28, 2000.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
