<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>68 nips-2006-Dirichlet-Enhanced Spam Filtering based on Biased Samples</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-68" href="../nips2006/nips-2006-Dirichlet-Enhanced_Spam_Filtering_based_on_Biased_Samples.html">nips2006-68</a> <a title="nips-2006-68-reference" href="#">nips2006-68-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>68 nips-2006-Dirichlet-Enhanced Spam Filtering based on Biased Samples</h1>
<br/><p>Source: <a title="nips-2006-68-pdf" href="http://papers.nips.cc/paper/3158-dirichlet-enhanced-spam-filtering-based-on-biased-samples.pdf">pdf</a></p><p>Author: Steffen Bickel, Tobias Scheffer</p><p>Abstract: We study a setting that is motivated by the problem of ﬁltering spam messages for many users. Each user receives messages according to an individual, unknown distribution, reﬂected only in the unlabeled inbox. The spam ﬁlter for a user is required to perform well with respect to this distribution. Labeled messages from publicly available sources can be utilized, but they are governed by a distinct distribution, not adequately representing most inboxes. We devise a method that minimizes a loss function with respect to a user’s personal distribution based on the available biased sample. A nonparametric hierarchical Bayesian model furthermore generalizes across users by learning a common prior which is imposed on new email accounts. Empirically, we observe that bias-corrected learning outperforms naive reliance on the assumption of independent and identically distributed data; Dirichlet-enhanced generalization across users outperforms a single (“one size ﬁts all”) ﬁlter as well as independent ﬁlters for all users. 1</p><br/>
<h2>reference text</h2><p>[1] D. Blei and M. Jordan. Variational methods for the Dirichlet process. In Proceedings of the International Conference on Machine Learning, 2004.</p>
<p>[2] H. Drucker, D. Wu, and V. Vapnik. Support vector machines for spam categorization. IEEE Transactions on Neural Networks, 10(5):1048–1055, 1999.</p>
<p>[3] M. Dudik, R. Schapire, and S. Phillips. Correcting sample selection bias in maximum entropy density estimation. In Advances in Neural Information Processing Systems, 2005.</p>
<p>[4] C. Elkan. The foundations of cost-sensitive learning. In Proceedings of the International Joint Conference on Artiﬁcial Intellligence, 2001.</p>
<p>[5] T. Ferguson. A Bayesian analysis of some nonparametric problems. Annals of Statistics, 1:209–230, 1973.</p>
<p>[6] J. Heckman. Sample selection bias as a speciﬁcation error. Econometrica, 47:153–161, 1979.</p>
<p>[7] N. Japkowicz and S. Stephen. The class imbalance problem: A systematic study. Intelligent Data Analysis, 6:429–449, 2002.</p>
<p>[8] Bryan Klimt and Yiming Yang. The enron corpus: A new dataset for email classiﬁcation research. In Proceedings of the European Conference on Machine Learning, 2004.</p>
<p>[9] P. Komarek. Logistic Regression for Data Mining and High-Dimensional Classiﬁcation. Doctoral dissertation, Carnegie Mellon University, 2004.</p>
<p>[10] R. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics, 9:249–265, 2000.</p>
<p>[11] Matthew Prince, Benjamin Dahl, Lee Holloway, Arthur Kellera, and Eric Langheinrich. Understanding how spammers steal your e-mail address: An analysis of the ﬁrst six months of data from project honey pot. In Proceedings of the Conference on Email and Anti-Spam, 2005.</p>
<p>[12] M. Sugiyama and K.-R. M¨ ller. Model selection under covariate shift. In Proceedings of the International u Conference on Artiﬁcial Neural Networks, 2005.</p>
<p>[13] Volker Tresp and Kai Yu. An introduction to nonparametric hierarchical Bayesian modelling with a focus on multi-agent learning. In Switching and Learning in Feedback Systems, volume 3355 of Lecture Notes in Computer Science, pages 290–312. Springer, 2004.</p>
<p>[14] Bianca Zadrozny. Learning and evaluating classiﬁers under sample selection bias. In Proceedings of the International Conference on Machine Learning, 2004.</p>
<p>[15] T. Zhang and F. Oles. Text categorization based on regularized linear classiﬁers. Information Retrieval, 4(1):5–31, 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
