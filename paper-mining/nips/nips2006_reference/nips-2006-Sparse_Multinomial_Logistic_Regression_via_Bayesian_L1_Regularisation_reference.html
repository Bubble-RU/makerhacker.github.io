<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>178 nips-2006-Sparse Multinomial Logistic Regression via Bayesian L1 Regularisation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-178" href="../nips2006/nips-2006-Sparse_Multinomial_Logistic_Regression_via_Bayesian_L1_Regularisation.html">nips2006-178</a> <a title="nips-2006-178-reference" href="#">nips2006-178-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>178 nips-2006-Sparse Multinomial Logistic Regression via Bayesian L1 Regularisation</h1>
<br/><p>Source: <a title="nips-2006-178-pdf" href="http://papers.nips.cc/paper/3155-sparse-multinomial-logistic-regression-via-bayesian-l1-regularisation.pdf">pdf</a></p><p>Author: Gavin C. Cawley, Nicola L. Talbot, Mark Girolami</p><p>Abstract: Multinomial logistic regression provides the standard penalised maximumlikelihood solution to multi-class pattern recognition problems. More recently, the development of sparse multinomial logistic regression models has found application in text processing and microarray classiﬁcation, where explicit identiﬁcation of the most informative features is of value. In this paper, we propose a sparse multinomial logistic regression method, in which the sparsity arises from the use of a Laplace prior, but where the usual regularisation parameter is integrated out analytically. Evaluation over a range of benchmark datasets reveals this approach results in similar generalisation performance to that obtained using cross-validation, but at greatly reduced computational expense. 1</p><br/>
<h2>reference text</h2><p>[1] P. McCullagh and J. A. Nelder. Generalized linear models, volume 37 of Monographs on Statistics and Applied Probability. Chapman & Hall/CRC, second edition, 1989.</p>
<p>[2] D. W. Hosmer and S. Lemeshow. Applied logistic regression. Wiley, second edition, 2000.</p>
<p>[3] C. K. Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on Information Theory, 16(1):41–46, January 1970.</p>
<p>[4] M. Saerens, P. Latinne, and C. Decaestecker. Adjusting the outputs of a classiﬁer to new a priori probabilities: A simple procedure. Neural Computation, 14(1):21–41, 2001.</p>
<p>[5] J. O. Berger. Statistical decision theory and Bayesian analysis. Springer Series in Statistics. Springer, second edition, 1985.</p>
<p>[6] J. Zhu and T. Hastie. Classiﬁcation of gene microarrays by penalized logistic regression. Biostatistics, 5(3):427–443, 2004.</p>
<p>[7] X. Zhou, X. Wang, and E. R. Dougherty. Multi-class cancer classiﬁcation using multinomial probit regression with Bayesian gene selection. IEE Proceedings - Systems Biology, 153(2):70–76, March 2006.</p>
<p>[8] T. Zhang and F. J. Oles. Text categorization based on regularised linear classiﬁcation methods. Information Retrieval, 4(1):5–31, April 2001.</p>
<p>[9] L. Narlikar and A. J. Hartemink. Sequence features of DNA binding sites reveal structural class of associated transcription factor. Bioinformatics, 22(2):157–163, 2006.</p>
<p>[10] M. J. L. Orr. Regularisation in the selection of radial basis function centres. Neural Computation, 7(3):606–623, 1995.</p>
<p>[11] Y. Grandvalet. Least absolute shrinkage is equivalent to quadratic penalisation. In L. Niklasson, M. Bod´ n, and T. Ziemske, editors, Proceedings of the International Conference on Artiﬁcial Neural e Networks, Perspectives in Neural Computing, pages 201–206, Sk¨ vde, Sweeden, September 2–4 1998. o Springer.</p>
<p>[12] Y. Grandvalet and S. Canu. Outcomes of the quivalence of adaptive ridge with least absolute shrinkage. In Advances in Neural Information Processing Systems, volume 11. MIT Press, 1999.</p>
<p>[13] M. E. Tipping. Sparse Bayesian learning and the Relevance Vector Machine. Journal of Machine Learning Research, 1:211–244, 2001.</p>
<p>[14] A. C. Faul and M. E. Tipping. Fast marginal likelihood maximisation for sparse Bayesian models. In C. M. Bishop and B. J. Frey, editors, Proceedings of the Ninth International Workshop on Artiﬁcial Intelligence and Statistics, Key West, FL, USA, 3–6 January 2003.</p>
<p>[15] R. Tibshirani. Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society - Series B, 58:267–288, 1996.</p>
<p>[16] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32(2):407–499, 2004.</p>
<p>[17] P. M. Williams. Bayesian regularization and pruning using a Laplace prior. Neural Computation, 7(1):117–143, 1995.</p>
<p>[18] C. M. Bishop. Neural networks for pattern recognition. Oxford University Press, 1995.</p>
<p>[19] J. S. Bridle. Probabilistic interpretation of feedforward classiﬁcation network outputs, with relationships to statistical pattern recognition. In F. Fogelman Souli´ and J. H´ rault, editors, Neurocomputing: Algoe e rithms, architectures and applications, pages 227–236. Springer-Verlag, New York, 1990.</p>
<p>[20] A. N. Tikhonov and V. Y. Arsenin. Solutions of ill-posed problems. John Wiley, New York, 1977.</p>
<p>[21] S. Geman, E. Bienenstock, and R. Doursat. Neural networks and the bias/variance dilema. Neural Computation, 4(1):1–58, 1992.</p>
<p>[22] D. J. C. MacKay. The evidence framework applied to classiﬁcation networks. Neural Computation, 4(5):720–736, 1992.</p>
<p>[23] W. L. Buntine and A. S. Weigend. Bayesian back-propagation. Complex Systems, 5:603–643, 1991.</p>
<p>[24] I. S. Gradshteyn and I. M. Ryzhic. Table of Integrals, Series and Products. Academic Press, ﬁfth edition, 1994.</p>
<p>[25] S. K. Shevade and S. S. Keerthi. A simple and efﬁcient algorithm for gene selection using sparse logistic regression. Bioinformatics, 19(17):2246–2253, 2003.</p>
<p>[26] D. Madigan, A. Genkin, D. D. Lewis, and D. Fradkin. Bayesian multinomial logistic regression for author identiﬁcation. In AIP Conference Proceedings, volume 803, pages 509–516, 2005.</p>
<p>[27] P. M. Williams. A Marquardt algorithm for choosing the step size in backpropagation learning with conjugate gradients. Technical Report CSRP-229, University of Sussex, February 1991.</p>
<p>[28] G. J. McLachlan. Discriminant analysis and statistical pattern recognition. Wiley, 1992.</p>
<p>[29] M. Figueiredo. Adaptive sparseness for supervised learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(9):1150–1159, September 2003.</p>
<p>[30] B. Krishnapuram, L. Carin, M. A. T. Figueiredo, and A. J. Hartemink. Sprse multinomial logistic regression: Fast algorithms and generalisation bounds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(6):957–968, June 2005.</p>
<p>[31] J. M. Bioucas-Dias, M. A. T. Figueiredo, and J. P. Oliveira. Adaptive total variation image deconvolution: A majorization-minimization approach. In Proceedings of the European Signal Processing Conference (EUSIPCO’2006), Florence, Italy, September 2006.</p>
<p>[32] G. C. Cawley and N. L. C. Talbot. Gene selection in cancer classiﬁcation using sparse logistic regression with Bayesian regularisation. Bioinformatics, 22(19):2348–2355, October 2006.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
