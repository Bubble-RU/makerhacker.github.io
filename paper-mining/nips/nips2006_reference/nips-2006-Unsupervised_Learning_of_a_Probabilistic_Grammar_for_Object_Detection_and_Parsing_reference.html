<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>199 nips-2006-Unsupervised Learning of a Probabilistic Grammar for Object Detection and Parsing</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-199" href="../nips2006/nips-2006-Unsupervised_Learning_of_a_Probabilistic_Grammar_for_Object_Detection_and_Parsing.html">nips2006-199</a> <a title="nips-2006-199-reference" href="#">nips2006-199-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>199 nips-2006-Unsupervised Learning of a Probabilistic Grammar for Object Detection and Parsing</h1>
<br/><p>Source: <a title="nips-2006-199-pdf" href="http://papers.nips.cc/paper/3021-unsupervised-learning-of-a-probabilistic-grammar-for-object-detection-and-parsing.pdf">pdf</a></p><p>Author: Yuanhao Chen, Long Zhu, Alan L. Yuille</p><p>Abstract: We describe an unsupervised method for learning a probabilistic grammar of an object from a set of training examples. Our approach is invariant to the scale and rotation of the objects. We illustrate our approach using thirteen objects from the Caltech 101 database. In addition, we learn the model of a hybrid object class where we do not know the speciﬁc object or its position, scale or pose. This is illustrated by learning a hybrid class consisting of faces, motorbikes, and airplanes. The individual objects can be recovered as different aspects of the grammar for the object class. In all cases, we validate our results by learning the probability grammars from training datasets and evaluating them on the test datasets. We compare our method to alternative approaches. The advantages of our approach is the speed of inference (under one second), the parsing of the object, and increased accuracy of performance. Moreover, our approach is very general and can be applied to a large range of objects and structures. 1</p><br/>
<h2>reference text</h2><p>[1] D. McAllester, M. Collins, and F. Pereira. Case-Factor Diagrams for Structured Probabilistic Modeling in UAI, 2004.</p>
<p>[2] B.D. Ripley. “Pattern Recognition and Neural Networks”. Cambridge University Press. 1996.</p>
<p>[3] J. Lafferty, A. McCallum and F. Pereira. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. ICML-2001</p>
<p>[4] D. Klein and C. Manning, ”Natural Language Grammar Induction Using a Constituent-Context Model,” Advances in Neural Information Processing Systems 14 (NIPS-2001), 2001.</p>
<p>[5] H. Dechter and Robert Mateescu. AND/OR Search Spaces for Graphical Models. In Artiﬁcial Intelligence, 2006.</p>
<p>[6] H. Chen, Z.J. Xu, Z.Q. Liu, and S.C. Zhu. Composite Templates for Cloth Modeling and Sketching. Proc. IEEE Conf. on Pattern Recognition and Computer Vision, June, New York, 2006.</p>
<p>[7] M. Meila and M. I. Jordan. Mixture of Trees: Learning with mixtures of trees. Journal of Machine Learning Research, 1, 1-48, 2000.</p>
<p>[8] S. Della Pietra, V. Della Pietra, and J. Lafferty. Feature Induction for MRF: Inducing features of random ﬁelds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4), April 1997, pp. 380-393</p>
<p>[9] S. C. Zhu, Y. N. Wu, and D. Mumford, ”Minimax entropy principle and its application to texture modeling,” Neural Comp., vol. 9, no. 8, pp. 1627–1660, 1997.</p>
<p>[10] A. McCallum. Feature Induction for CRF: Efﬁciently Inducing Features of Conditional Random Fields. Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2003.</p>
<p>[11] L. S. Zettlemoyer and M. Collins. Learning to Map Sentences to Logical Form: Structured Classiﬁcation with Probabilistic Categorial Grammars. Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2005.</p>
<p>[12] N. Friedman. The Bayesian structural EM algorithm. Fourteenth Conf. on Uncertainty in Artiﬁcial Intelligence (UAI), 1998.</p>
<p>[13] T. Kadir and M. Brady. Scale, Saliency and Image Description. International Journal of Computer Vision. 45 (2):83-105, November 2001.</p>
<p>[14] D. G. Lowe, ”Distinctive image features from scale-invariant keypoints,” International Journal of Computer Vision, 60, 2. pp. 91-110. 2004.</p>
<p>[15] R.J.A. Little and D.B. Rubin. Statistical Analysis with Missing Data, Wiley, Hoboken, New Jersey. 2002.</p>
<p>[16] Fergus, R. , Perona, P. and Zisserman, A. Object Class Recognition by Unsupervised Scale-Invariant Learning Proc. of the IEEE Conf on Computer Vision and Pattern Recognition. 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
