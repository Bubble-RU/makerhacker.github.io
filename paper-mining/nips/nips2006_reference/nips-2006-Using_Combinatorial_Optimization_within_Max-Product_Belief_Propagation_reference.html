<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>201 nips-2006-Using Combinatorial Optimization within Max-Product Belief Propagation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-201" href="../nips2006/nips-2006-Using_Combinatorial_Optimization_within_Max-Product_Belief_Propagation.html">nips2006-201</a> <a title="nips-2006-201-reference" href="#">nips2006-201-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>201 nips-2006-Using Combinatorial Optimization within Max-Product Belief Propagation</h1>
<br/><p>Source: <a title="nips-2006-201-pdf" href="http://papers.nips.cc/paper/3117-using-combinatorial-optimization-within-max-product-belief-propagation.pdf">pdf</a></p><p>Author: Daniel Tarlow, Gal Elidan, Daphne Koller, John C. Duchi</p><p>Abstract: In general, the problem of computing a maximum a posteriori (MAP) assignment in a Markov random ﬁeld (MRF) is computationally intractable. However, in certain subclasses of MRF, an optimal or close-to-optimal assignment can be found very efﬁciently using combinatorial optimization algorithms: certain MRFs with mutual exclusion constraints can be solved using bipartite matching, and MRFs with regular potentials can be solved using minimum cut methods. However, these solutions do not apply to the many MRFs that contain such tractable components as sub-networks, but also other non-complying potentials. In this paper, we present a new method, called C OMPOSE, for exploiting combinatorial optimization for sub-networks within the context of a max-product belief propagation algorithm. C OMPOSE uses combinatorial optimization for computing exact maxmarginals for an entire sub-network; these can then be used for inference in the context of the network as a whole. We describe highly efﬁcient methods for computing max-marginals for subnetworks corresponding both to bipartite matchings and to regular networks. We present results on both synthetic and real networks encoding correspondence problems between images, which involve both matching constraints and pairwise geometric constraints. We compare to a range of current methods, showing that the ability of C OMPOSE to transmit information globally across the network leads to improved convergence, decreased running time, and higher-scoring assignments.</p><br/>
<h2>reference text</h2><p>[1] D. Anguelov, D. Koller, P. Srinivasan, S. Thrun, H. Pang, and J. Davis. The correlated correspondence algorithm for unsupervised registration of nonrigid surfaces. In NIPS, 2004.</p>
<p>[2] Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy minimization via graph cuts. In ICCV, 1999.</p>
<p>[3] G. Elidan, I. McGraw, and D. Koller. Residual belief propagation. In UAI, 2006.</p>
<p>[4] J. Hooker, G. Ottosson, E.S. Thorsteinsson, and H.J. Kim. A scheme for unifying optimization and constraint satisfaction methods. In Knowledge Engineering Review, 2000.</p>
<p>[5] H. Ishikawa. Exact optimization for Markov random ﬁelds with convex priors. PAMI, 2003.</p>
<p>[6] J. Kleinberg and E. Tardos. Algorithm Design. Addison-Wesley, 2005.</p>
<p>[7] P. Kohli and P. Torr. Measuring uncertainty in graph cut solutions - efﬁciently computing min-marginal energies using dynamic graph cuts. In ECCV, 2006.</p>
<p>[8] V. Kolmogorov and M. Wainwright. On the optimality of tree-reweighted max-product message-passing. In UAI ’05.</p>
<p>[9] V. Kolmogorov and R. Zabih. What energy functions can be minimized via graph cuts? In ECCV, 2002.</p>
<p>[10] E. Lawler. The quadratic assignment problem. In Management Science, 1963.</p>
<p>[11] K. Murphy and Y. Weiss. Loopy belief propagation for approximate inference: An empirical study. In UAI ’99.</p>
<p>[12] J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, 1988.</p>
<p>[13] A. Raj, G. Singh, and R. Zabih. MRF’s for MRI’s: Bayesian reconstruction of MR images via graph cuts. In CVPR, 2006. To appear.</p>
<p>[14] C. Rother, S. Kumar, V. Kolmogorov, and A. Blake. Digital tapestry. In CVPR, 2005.</p>
<p>[15] J. Shi and J. Malik. Normalized cuts and image segmentation. PAMI, 2000.</p>
<p>[16] T. St¨ tzle and M. Dorigo. ACO algorithms for the quadratic assignment problem. In New u Ideas in Optimization. 1999.</p>
<p>[17] R. Szeliski, R. Zabih, D. Scharstein, O. Veksler, V. Kolmogorov, A. Agarwala, M. Tappen, and C. Rother. A comparative study of energy minimization methods for Markov random ﬁelds. In ECCV, 2006.</p>
<p>[18] B. Taskar, V. Chatalbashev, and D. Koller. Learning associative markov networks. In ICML ’04.</p>
<p>[19] B. Taskar, V. Chatalbashev, D. Koller, and C. Guestrin. Learning structured prediction models: a large margin approach. In ICML ’05.</p>
<p>[20] Y. Weiss and W. Freeman. On the optimality of solutions of the max-product belief-propagation algorithm in arbitrary graphs. IEEE Transactions on Information Theory, 47, 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
