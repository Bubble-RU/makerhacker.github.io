<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>38 nips-2006-Automated Hierarchy Discovery for Planning in Partially Observable Environments</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-38" href="../nips2006/nips-2006-Automated_Hierarchy_Discovery_for_Planning_in_Partially_Observable_Environments.html">nips2006-38</a> <a title="nips-2006-38-reference" href="#">nips2006-38-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>38 nips-2006-Automated Hierarchy Discovery for Planning in Partially Observable Environments</h1>
<br/><p>Source: <a title="nips-2006-38-pdf" href="http://papers.nips.cc/paper/2968-automated-hierarchy-discovery-for-planning-in-partially-observable-environments.pdf">pdf</a></p><p>Author: Laurent Charlin, Pascal Poupart, Romy Shioda</p><p>Abstract: Planning in partially observable domains is a notoriously difﬁcult problem. However, in many real-world scenarios, planning can be simpliﬁed by decomposing the task into a hierarchy of smaller planning problems. Several approaches have been proposed to optimize a policy that decomposes according to a hierarchy speciﬁed a priori. In this paper, we investigate the problem of automatically discovering the hierarchy. More precisely, we frame the optimization of a hierarchical policy as a non-convex optimization problem that can be solved with general non-linear solvers, a mixed-integer non-linear approximation or a form of bounded hierarchical policy iteration. By encoding the hierarchical structure as variables of the optimization problem, we can automatically discover a hierarchy. Our method is ﬂexible enough to allow any parts of the hierarchy to be speciﬁed based on prior knowledge while letting the optimization discover the unknown parts. It can also discover hierarchical policies, including recursive policies, that are more compact (potentially inﬁnitely fewer parameters) and often easier to understand given the decomposition induced by the hierarchy. 1</p><br/>
<h2>reference text</h2><p>[1] C. Amato, D. Bernstein, and S. Zilberstein. Solving POMDPs using quadratically constrained linear programs. In To appear In International Joint Conferences on Artiﬁcial Intelligence (IJCAI), 2007.</p>
<p>[2] E. Balas. Disjunctive programming. Annals of Discrete Mathematics, 5:3–51, 1979.</p>
<p>[3] D. Braziunas and C. Boutilier. Stochastic local search for POMDP controllers. In AAAI, pages 690–696, 2004.</p>
<p>[4] A. Cassandra. Exact and approximate algorithms for partially observable Markov decision processes. PhD thesis, Brown University, Dept. of Computer Science, 1998.</p>
<p>[5] L. Charlin. Automated hierarchy discovery for planning in partially observable domains. Master’s thesis, University of Waterloo, 2006.</p>
<p>[6] T. Dietterich. Hierarchical reinforcement learning with the MAXQ value function decomposition. JAIR, 13:227–303, 2000.</p>
<p>[7] M. Ghavamzadeh and S. Mahadevan. Hierarchical policy gradient algorithms. In T. Fawcett and N. Mishra, editors, ICML, pages 226–233. AAAI Press, 2003.</p>
<p>[8] P. Gill, W. Murray, and M. Saunders. SNOPT: An SQP algorithm for large-scale constrained optimization. SIAM Review, 47(1):99–131, 2005.</p>
<p>[9] E. Hansen. An improved policy iteration algorithm for partially observable MDPs. In NIPS, 1998.</p>
<p>[10] E. Hansen and R. Zhou. Synthesis of hierarchical ﬁnite-state controllers for POMDPs. In E. Giunchiglia, N. Muscettola, and D. Nau, editors, ICAPS, pages 113–122. AAAI, 2003.</p>
<p>[11] B. Hengst. Discovering hierarchy in reinforcement learning with HEXQ. In ICML, pages 243–250, 2002.</p>
<p>[12] L. Kaelbling, M. Littman, and A. Cassandra. Planning and acting in partially observable stochastic domains. Artiﬁcial Intelligence, 101(1-2):99–134, 1998.</p>
<p>[13] A. McGovern and A. Barto. Automatic discovery of subgoals in reinforcement learning using diverse density. In ICML, pages 361–368, 2001.</p>
<p>[14] N. Meuleau, L. Peshkin, K.-E. Kim, and L. Kaelbling. Learning ﬁnite-state controllers for partially observable environments. In UAI, pages 427–436, 1999.</p>
<p>[15] R. Parr. Hierarchical Control and learning for Markov decision processes. PhD thesis, University of California at Berkeley, 1998.</p>
<p>[16] J. Pineau. Tractable Planning Under Uncertainty: Exploiting Structure. PhD thesis, Robotics Institute, Carnegie Mellon University, 2004.</p>
<p>[17] J. Pineau, G. Gordon, and S. Thrun. Policy-contingent abstraction for robust robot control. In UAI, pages 477–484, 2003.</p>
<p>[18] P. Poupart and C. Boutilier. Bounded ﬁnite state controllers. In NIPS, 2003.</p>
<p>[19] Pascal Poupart. Exploiting Structure to efﬁciently solve large scale partially observable Markov decision processes. PhD thesis, University of Toronto, 2005.</p>
<p>[20] M. Ryan. Using abstract models of behaviours to automatically generate reinforcement learning hierarchies. In ICML, pages 522–529, 2002.</p>
<p>[21] R. Sutton, D. Precup, and S. Singh. Between MDPs and Semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artiﬁcial Intelligence, 112(1-2):181–211, 1999.</p>
<p>[22] G. Theocharous, S. Mahadevan, and L. Kaelbling. Spatial and temporal abstractions in POMDPs applied to robot navigation. Technical Report MIT-CSAIL-TR-2005-058, Computer Science and Artiﬁcial Intelligence Laboratory, MIT, 2005.</p>
<p>[23] S. Thrun and A. Schwartz. Finding structure in reinforcement learning. In NIPS, pages 385–392, 1994.</p>
<p>[24] J. Williams and S. Youngs. Scaling POMDPs for dialogue management with composite summary pointbased value iteration (CSPBVI). In AAAI workshop on Statistical and Empirical Methods in Spoken Dialogue Systems, 2006.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
