<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>44 nips-2006-Bayesian Policy Gradient Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-44" href="../nips2006/nips-2006-Bayesian_Policy_Gradient_Algorithms.html">nips2006-44</a> <a title="nips-2006-44-reference" href="#">nips2006-44-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>44 nips-2006-Bayesian Policy Gradient Algorithms</h1>
<br/><p>Source: <a title="nips-2006-44-pdf" href="http://papers.nips.cc/paper/2993-bayesian-policy-gradient-algorithms.pdf">pdf</a></p><p>Author: Mohammad Ghavamzadeh, Yaakov Engel</p><p>Abstract: Policy gradient methods are reinforcement learning algorithms that adapt a parameterized policy by following a performance gradient estimate. Conventional policy gradient methods use Monte-Carlo techniques to estimate this gradient. Since Monte Carlo methods tend to have high variance, a large number of samples is required, resulting in slow convergence. In this paper, we propose a Bayesian framework that models the policy gradient as a Gaussian process. This reduces the number of samples needed to obtain accurate gradient estimates. Moreover, estimates of the natural gradient as well as a measure of the uncertainty in the gradient estimates are provided at little extra cost. 1</p><br/>
<h2>reference text</h2><p>[1] R. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229–256, 1992.</p>
<p>[2] P. Marbach. Simulated-Based Methods for Markov Decision Processes. PhD thesis, MIT, 1998.</p>
<p>[3] J. Baxter and P. Bartlett. Inﬁnite-horizon policy-gradient estimation. JAIR, 15:319–350, 2001.</p>
<p>[4] R. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Proceedings of NIPS 12, pages 1057–1063, 2000.</p>
<p>[5] S. Kakade. A natural policy gradient. In Proceedings of NIPS 14, 2002.</p>
<p>[6] J. Bagnell and J. Schneider. Covariant policy search. In Proceedings of the 18th IJCAI, 2003.</p>
<p>[7] J. Peters, S. Vijayakumar, and S. Schaal. Reinforcement learning for humanoid robotics. In Proceedings of the Third IEEE-RAS International Conference on Humanoid Robots, 2003.</p>
<p>[8] J. Berger and R. Wolpert. The Likelihood Principle. Inst. of Mathematical Statistics, Hayward, CA, 1984.</p>
<p>[9] A. O’Hagan. Monte Carlo is fundamentally unsound. The Statistician, 36:247–249, 1987.</p>
<p>[10] A. O’Hagan. Bayes-Hermite quadrature. Journal of Statistical Planning and Inference, 29, 1991.</p>
<p>[11] D. Bertsekas and J. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc, 1996.</p>
<p>[12] R. Sutton and A. Barto. An Introduction to Reinforcement Learning. MIT Press, 1998.</p>
<p>[13] T. Jaakkola and D. Haussler. Exploiting generative models in discriminative classiﬁers. In Proceedings of NIPS 11. MIT Press, 1998.</p>
<p>[14] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univ. Press, 2004.</p>
<p>[15] Y. Engel. Algorithms and Representations for Reinforcement Learning. PhD thesis, The Hebrew University of Jerusalem, Israel, 2005.</p>
<p>[16] C. Rasmussen and Z. Ghahramani. Bayesian Monte Carlo. In Proceedings of NIPS 15. MIT Press, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
