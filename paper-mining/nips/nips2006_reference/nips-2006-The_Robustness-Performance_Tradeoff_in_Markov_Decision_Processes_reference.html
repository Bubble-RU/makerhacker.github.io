<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>191 nips-2006-The Robustness-Performance Tradeoff in Markov Decision Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2006" href="../home/nips2006_home.html">nips2006</a> <a title="nips-2006-191" href="../nips2006/nips-2006-The_Robustness-Performance_Tradeoff_in_Markov_Decision_Processes.html">nips2006-191</a> <a title="nips-2006-191-reference" href="#">nips2006-191-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>191 nips-2006-The Robustness-Performance Tradeoff in Markov Decision Processes</h1>
<br/><p>Source: <a title="nips-2006-191-pdf" href="http://papers.nips.cc/paper/3053-the-robustness-performance-tradeoff-in-markov-decision-processes.pdf">pdf</a></p><p>Author: Huan Xu, Shie Mannor</p><p>Abstract: Computation of a satisfactory control policy for a Markov decision process when the parameters of the model are not exactly known is a problem encountered in many practical applications. The traditional robust approach is based on a worstcase analysis and may lead to an overly conservative policy. In this paper we consider the tradeoff between nominal performance and the worst case performance over all possible models. Based on parametric linear programming, we propose a method that computes the whole set of Pareto efﬁcient policies in the performancerobustness plane when only the reward parameters are subject to uncertainty. In the more general case when the transition probabilities are also subject to error, we show that the strategy with the “optimal” tradeoff might be non-Markovian and hence is in general not tractable. 1</p><br/>
<h2>reference text</h2><p>[1] A. L. Soyster. Convex programming with set-inclusive constraints and applications to inexact linear programming. Oper. Res., 1973.</p>
<p>[2] A. Bagnell, A. Ng, and J. Schneider. Solving uncertain markov decision processes. Technical Report CMU-RI-TR-01-25, Carnegie Mellon University, August 2001.</p>
<p>[3] A. Ben-Tal and A. Nemirovski. Robust solutions of uncertain linear programs. Oper. Res. Lett., 25(1):1– 13, August 1999.</p>
<p>[4] C. C. White III and H. K. El-Deib. Markov decision process with imprecise transition probabilities. Oper. Res., 42(4):739–748, July 1992.</p>
<p>[5] A. Nilim and L. El Ghaoui. Robust control of markov decision processes with uncertain transition matrices. Oper. Res., 53(5):780–798, September 2005.</p>
<p>[6] D. Bertsimas and M. Sim. The price of robustness. Oper. Res., 52(1):35–53, January 2004.</p>
<p>[7] M. Heger. Consideration of risk in reinforcement learning. In Proc. 11th International Conference on Machine Learning, pages 105–111. Morgan Kaufmann, 1994.</p>
<p>[8] R. Neuneier and O. Mihatsch. Risk sensitive reinforcement learning. In Advances in Neural Information Processing Systems 11, pages 1031–1037, Cambridge, MA, USA, 1999. MIT Press.</p>
<p>[9] P. Geibel. Reinforcement learning with bounded risk. In Proc. 18th International Conf. on Machine Learning, pages 162–169. Morgan Kaufmann, San Francisco, CA, 2001.</p>
<p>[10] D. Bertsimas and J. N. Tsitsiklis. Introduction to Linear Optimization. Athena Scientiﬁc, 1997.</p>
<p>[11] M. Ehrgott. Multicriteria Optimization. Springer-Verlag Berlin Heidelberg, 2000.</p>
<p>[12] K. G. Murty. Linear Programming. John Wiley & Sons, 1983.</p>
<p>[13] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc, 1996.</p>
<p>[14] M. L. Puterman. Markov Decision Processes. John Wiley & Sons, INC, 1994.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
