<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>60 nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-60" href="#">nips2007-60</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>60 nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</h1>
<br/><p>Source: <a title="nips-2007-60-pdf" href="http://papers.nips.cc/paper/3364-contraction-properties-of-vlsi-cooperative-competitive-neural-networks-of-spiking-neurons.pdf">pdf</a></p><p>Author: Emre Neftci, Elisabetta Chicca, Giacomo Indiveri, Jean-jeacques Slotine, Rodney J. Douglas</p><p>Abstract: A non–linear dynamic system is called contracting if initial conditions are forgotten exponentially fast, so that all trajectories converge to a single trajectory. We use contraction theory to derive an upper bound for the strength of recurrent connections that guarantees contraction for complex neural networks. Speciﬁcally, we apply this theory to a special class of recurrent networks, often called Cooperative Competitive Networks (CCNs), which are an abstract representation of the cooperative-competitive connectivity observed in cortex. This speciﬁc type of network is believed to play a major role in shaping cortical responses and selecting the relevant signal among distractors and noise. In this paper, we analyze contraction of combined CCNs of linear threshold units and verify the results of our analysis in a hybrid analog/digital VLSI CCN comprising spiking neurons and dynamic synapses. 1</p><p>Reference: <a title="nips-2007-60-reference" href="../nips2007_reference/nips-2007-Contraction_Properties_of_VLSI_Cooperative_Competitive_Neural_Networks_of_Spiking_Neurons_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ch  Abstract A non–linear dynamic system is called contracting if initial conditions are forgotten exponentially fast, so that all trajectories converge to a single trajectory. [sent-4, score-0.435]
</p><p>2 We use contraction theory to derive an upper bound for the strength of recurrent connections that guarantees contraction for complex neural networks. [sent-5, score-1.007]
</p><p>3 Speciﬁcally, we apply this theory to a special class of recurrent networks, often called Cooperative Competitive Networks (CCNs), which are an abstract representation of the cooperative-competitive connectivity observed in cortex. [sent-6, score-0.153]
</p><p>4 In this paper, we analyze contraction of combined CCNs of linear threshold units and verify the results of our analysis in a hybrid analog/digital VLSI CCN comprising spiking neurons and dynamic synapses. [sent-8, score-0.906]
</p><p>5 1  Introduction  Cortical neural networks are characterized by a large degree of recurrent excitatory connectivity, and local inhibitory connections. [sent-9, score-0.358]
</p><p>6 This type of connectivity among neurons is remarkably similar, across all areas in the cortex [1]. [sent-10, score-0.295]
</p><p>7 A CCN is a set of interacting neurons, in which cooperation is achieved by local recurrent excitatory connections and competition is achieved via a group of inhibitory neurons, driven by the excitatory neurons and inhibiting them (see Figure 1). [sent-12, score-0.663]
</p><p>8 The linear operations include analog gain (linear ampliﬁcation of the feed–forward input, mediated by the recurrent excitation and/or common mode input), and locus invariance [3]. [sent-14, score-0.198]
</p><p>9 CCN networks can be modeled using linear threshold units, as well as recurrent networks of spiking neurons. [sent-16, score-0.287]
</p><p>10 The latter can be efﬁciently implemented in silicon using Integrate–and–Fire (I&F;) neurons and dynamic synapses [7]. [sent-17, score-0.304]
</p><p>11 In this work we use a prototype VLSI CCN device, comprising 128 low power I&F; neurons [8] and 4096 dynamic synapses [9] that operate in real-time, in a massively parallel fashion. [sent-18, score-0.338]
</p><p>12 The theoretical foundations used to address these problems are based on contraction theory [10]. [sent-20, score-0.443]
</p><p>13 By applying this theory to CCN models of linear threshold units and to combinations of them we ﬁnd upper bounds to contraction conditions. [sent-21, score-0.516]
</p><p>14 We then test the theoretical results on the VLSI CCN of spiking neurons, and on a combination of two mutually coupled CCNs. [sent-22, score-0.127]
</p><p>15 (a) A CCN consisting of a population of nearest neighbor connected excitatory neurons (blue) receiving external input and an inhibitory neuron which receives input from all the excitatory neurons and inhibits them back (red). [sent-25, score-1.053]
</p><p>16 (c) Three coupled CCNs, showing examples of connectivity patterns between them. [sent-27, score-0.113]
</p><p>17 2  CCN of linear threshold units  Neural network models of linear threshold units (LTUs) ignore many of the non–linear processes that occur at the synaptic level and contain, by deﬁnition, no information about spike timing. [sent-28, score-0.245]
</p><p>18 However networks of LTUs can functionally behave as networks of I&F; neurons in a wide variety of cases [11]. [sent-29, score-0.332]
</p><p>19 Similarly boundary conditions found for LTU networks can be often applied also to their I&F; neuron network counterparts. [sent-30, score-0.277]
</p><p>20 , N dt  (1)  Where N is the total number of neurons in the system, the function g(x) = max(x, 0) is a half–wave rectiﬁcation non–linearity to ensure that x ≡ (x1 , . [sent-35, score-0.272]
</p><p>21 , xN )⊤ remains positive, bi are the external inputs applied to the neurons and τi are the time constants of the neurons. [sent-38, score-0.295]
</p><p>22 excitatory or inhibitory) have identical dynamics: we denote the time constant of excitatory neurons with τex and the one of inhibitory neurons with τih . [sent-41, score-0.782]
</p><p>23 Throughout the paper, we will use the following notation for the weights: ws for self excitation, we1 , we2 for 1st and 2nd nearest neighbor excitation respectively, and wie , wei for inhibitory to excitatory neuron and vice versa. [sent-42, score-0.633]
</p><p>24 W= (2)  w w2 0 w2 w1 wsel f −wei  1 wie wie wie wie wie wie 0 A CCN can be used to implement a WTA computation. [sent-49, score-0.916]
</p><p>25 A HWTA implements a max operation or selection mechanism: only the neuron receiving the strongest input can be active and all other neurons are suppressed by global inhibition. [sent-51, score-0.448]
</p><p>26 The activity of the ‘winning’ group of neurons can be ampliﬁed while other groups are suppressed. [sent-53, score-0.278]
</p><p>27 Depending on the strength of inhibitory and excitatory couplings different regimes are observed. [sent-54, score-0.245]
</p><p>28 1 Contraction of a single network A formal analysis of contraction theory applied to non–linear systems has been described in [10,12]. [sent-58, score-0.475]
</p><p>29 In a contracting system, all the trajectories converge to a single trajectory exponentially fast independent of the initial conditions. [sent-61, score-0.329]
</p><p>30 In particular, if the system has a steady state solution then, by deﬁnition, the state will contract and converge to that solution exponentially fast. [sent-62, score-0.103]
</p><p>31 Formally, the sysd tem is contracting if dt δ x is uniformally negative (i. [sent-63, score-0.318]
</p><p>32 This leads to the following theorem: d Consider a system whose dynamics is given by the differential equations dt x = f(x,t). [sent-67, score-0.128]
</p><p>33 The system is said to be contracting if all its trajectories converge exponentially to a single trajectory. [sent-68, score-0.364]
</p><p>34 If the symmetric part of the generalized Jacobian, Fs , is negative deﬁnite then the system is contracting. [sent-74, score-0.122]
</p><p>35 Let us now see under which conditions the system deﬁned by Eq. [sent-79, score-0.106]
</p><p>36 We assume that the wei and wie weights are not zero so we can use the constant metric:   τex 0 0  . [sent-84, score-0.205]
</p><p>37 With this metric, MJ can be written MJ = −I + D K, where Di j = g′ (yi )δi j , and K is similar to W but with wei in place of wie . [sent-97, score-0.205]
</p><p>38 This leads to a condition of the form λmax < 0, where λmax = 2we1 + 2we2 + ws − 1  (5)  A graphical representation of the boundaries deﬁned by this contraction condition is provided in Figure 2. [sent-99, score-0.513]
</p><p>39 The term |λmax | is called the contraction rate with respect to metric M. [sent-100, score-0.497]
</p><p>40 In addition it is possible to compute a lower bound for the full system’s contraction rate. [sent-112, score-0.443]
</p><p>41 Let Fs be the symmetric part of the Jacobian of two bi–directionally coupled subsystems, with symmetric feed–back couplings. [sent-113, score-0.112]
</p><p>42 By the eigenvalue interlacing theorem [13] we have that the contraction rate of the combined system is given by λmax (Fs ) ≤ mini λ (Fis ) i = 1, 2. [sent-116, score-0.589]
</p><p>43 For the speciﬁc example of a combined system comprising two identical subsystems coupled by a uniform coupling matrix G =w f b · I we have σ 2 (G) = w2 b . [sent-117, score-0.351]
</p><p>44 The combined system is contracting if: f |w f b | < λmax  (8)  The results obtained with this analysis can be generalized to more than two combined subsystems, and with different types of coupling matrices [17]. [sent-118, score-0.476]
</p><p>45 at least one of the ‘G–blocks’ in the non–symmetric form is negative semideﬁnite), the system is automatically contracting provided that both subsystems are contracting. [sent-121, score-0.454]
</p><p>46 Given this, the condition for contraction of the combined system described by Eq. [sent-122, score-0.598]
</p><p>47 Note that the contraction rate is an observable quantity, therefore one can build a contracting system consisting of an arbitrary number of CCNs as follows: 1. [sent-124, score-0.807]
</p><p>48 Determine the contraction rate of two CCNs by using Eq. [sent-125, score-0.469]
</p><p>49 Compute the upper bound to the contraction rate of the combined system as explained above. [sent-130, score-0.589]
</p><p>50 4  Contraction in a VLSI CCN of spiking neurons  The VLSI device used in this work implements a CCN of spiking neurons using an array of low– power I&F; neurons with dynamic synapses [8, 18]. [sent-133, score-0.948]
</p><p>51 It contains 124 excitatory neurons with self, 1st , 2nd , 3rd nearest–neighbor recurrent excitatory connections and 4 inhibitory neurons (all–to–all bi–directionally connected to the excitatory neurons). [sent-136, score-0.988]
</p><p>52 Each neuron receives input currents from a row of 32 afferent plastic synapses that use the Address Event Representation (AER) to receive spikes. [sent-137, score-0.268]
</p><p>53 The spiking activity of the neurons is also encoded using the AER. [sent-138, score-0.345]
</p><p>54 (a) A raster plot of the input stimulus(left) and the mean ﬁring rates(right): the membrane potential of the I&F; neurons are set to a random initial state by stimulating them with uncorrelated Poisson spike trains of constant mean frequency (up to the dashed line). [sent-141, score-0.476]
</p><p>55 Then the network is stimulated with 2 Gaussian bumps of different amplitude centered at Neuron 30 and Neuron 80, while, all the neurons received a constant level of uncorrelated input during the whole trial. [sent-142, score-0.409]
</p><p>56 (b) The response of the CCN to the stimulus presented in (a). [sent-143, score-0.124]
</p><p>57 The shaded area represents the mean input stimulus presented throughout the experiment. [sent-145, score-0.099]
</p><p>58 The system selects the largest input and suppresses the noise and the smaller bump, irrespective of initial conditions and noise. [sent-146, score-0.183]
</p><p>59 Neurons 124 to 128 are inhibitory neurons and do not receive external input. [sent-147, score-0.354]
</p><p>60 This board allows us to stimulate the synapses on the chip (e. [sent-149, score-0.163]
</p><p>61 with synthetic trains of spikes), monitor the activity of the I&F; neurons, and map events from one neuron to a synapse belonging to a neuron on the same chip and/or on a different chip. [sent-151, score-0.469]
</p><p>62 An analysis of the dynamics of our VLSI I&F; neurons can be found in [20] and although the leakage term in our implemented neurons is constant, it has been shown that such neurons exhibit responses qualitatively similar to standard linear I&F; neurons [20]. [sent-152, score-1.017]
</p><p>63 A steady state solution is easily computable for a network of linear threshold units [5, 21]: it is a ﬁxed point in state space, i. [sent-153, score-0.135]
</p><p>64 In a VLSI network of I&F; neurons the steady state will be modiﬁed by mismatch and the activities will ﬂuctuate due to external and microscopic perturbations (but remain in its vicinity if the system is contracting). [sent-156, score-0.403]
</p><p>65 To prove contraction experimentally in these types of networks, one would have to apply an input and test with all possible initial conditions. [sent-157, score-0.52]
</p><p>66 This is clearly not possible, but we can verify under which conditions the system is compatible with contraction by repeating the same experiment with different initial conditions (see Sec. [sent-158, score-0.62]
</p><p>67 1) and under which conditions the system is not compatible with contraction by observing if system settles to different solutions when stimulated with different initial conditions (see Sec. [sent-160, score-0.73]
</p><p>68 1  Convergence to a steady state with a static stimulus  The VLSI CCN is stimulated by uncorrelated Poisson spike trains whose mean rates form two Gaussian–shaped bumps along the array of neurons, one with a smaller amplitude than the other superimposed to background noise (see Figure 3a). [sent-164, score-0.309]
</p><p>69 We set the neurons into random initial conditions by stimulating them with uncorrelated Poisson spike trains with a spatially uniform and constant mean rate, before applying the real input stimulus (before the dashed line in Figure 3a ). [sent-166, score-0.536]
</p><p>70 Figure 3b shows the response of the CCN to this spike train, and Figure 3c is the response averaged over 100 trials. [sent-167, score-0.172]
</p><p>71 This experiment shows that regardless of the initial conditions, the ﬁnal response of the CCN in an SWTA conﬁguration is always the same (see the small error bars on Figure 3c), as we would expect from a contracting system. [sent-168, score-0.367]
</p><p>72 2  Convergence with non–static stimulus and contraction rate  As the condition for contraction does not depend on the external input, it will also hold for time– varying inputs. [sent-170, score-1.033]
</p><p>73 For example an interesting input stimulus is a bump of activity moving along the array of neurons at a constant speed. [sent-171, score-0.445]
</p><p>74 In this case, the ﬁring rates produced by the chip carry informa5  100  60  80 40  60 40  20  20  60  80 40  60 40  20  20 2  4 Time [s]  6  0  (a) Single trial weak CCN  1  80  120  Neuron  Neuron  100  2  4 Time [s]  6  0  (b) Single trial strong CCN  Rate (Normlized to max. [sent-172, score-0.216]
</p><p>75 4  (c) Activity of neuron #25  Figure 4: Contraction rate in VLSI CCNs using non–static stimulation. [sent-182, score-0.193]
</p><p>76 The input changed from an initial stage, where all the neurons were randomly stimulated with constant mean frequencies (up to 3 s), to a second stage in which the moving stimulus (freshly generated from trial to trial) is applied. [sent-183, score-0.455]
</p><p>77 This stimulus consists of a bump of activity that is shifted from one neuron to the next. [sent-184, score-0.305]
</p><p>78 The panel (c) compares the mean rates of neuron #25 in the weakly coupled CCN (green), the strong CCN (blue) and the input (red), all normalized to their peak of activity and calculated over 50 trials. [sent-186, score-0.339]
</p><p>79 We see how the blue line is delayed compared to the red and green line: the stronger recurrent couplings reduces the contraction rate. [sent-187, score-0.596]
</p><p>80 We measured the response of the chip to such a stimulus, for both strong and weak recurrent couplings (see Figure 4). [sent-189, score-0.355]
</p><p>81 The strong coupling case produces slower responses to the input than the weak coupling case, as expected from a system having a lower contraction rate (see Figure 4c). [sent-190, score-0.76]
</p><p>82 The system’s condition for contraction does not depend on the individual neuron’s time constants, although the contraction rate in the original metric does. [sent-191, score-0.975]
</p><p>83 This also applies to the non–static input case, where the system will converge to the expected solution, independently of the neurons time constants. [sent-192, score-0.354]
</p><p>84 Local mismatch effects in the VLSI chip lead to an effective weight matrix whose elements wsel f , w1 , w2 , wie are not identical throughout the array. [sent-193, score-0.257]
</p><p>85 This combined with the high gain of the strong coupling, and the variance produced by the input Poisson spike trains during the initial phase, explains the emergence of “pseudo-random” winners around neuron 30,60 and 80 in Figure 4b. [sent-194, score-0.4]
</p><p>86 3  A non–contracting example  We expect a CCN to be non–contracting when the coupling is strong: in this condition the CCN exhibits a hysteretic behavior [22], so the position of the winner strongly depends on the network’s initial conditions. [sent-196, score-0.144]
</p><p>87 Figure 5 illustrates this behavior with a CCN with very strong recurrent weights. [sent-197, score-0.137]
</p><p>88 4  Contraction of combined systems  By using a multi-chip AER communication infrastructure [19] we can connect multiple chips together with arbitrary connectivity matrices (e. [sent-199, score-0.1]
</p><p>89 Figure 6 shows the response of two CCNs, combined via a connectivity matrix as shown in Figure 6b, to three input bumps of activity in a contracting conﬁguration. [sent-206, score-0.534]
</p><p>90 5  Conclusion  We applied contraction theory to combined Cooperative Competitive Networks (CCN) of Linear Threshold Units (LTU) and determined sufﬁcient conditions for contraction. [sent-207, score-0.523]
</p><p>91 We then tested the theoretical predictions on neuromorphic VLSI implementations of CCNs, by measuring their response to different types of stimuli with different random initial conditions. [sent-208, score-0.141]
</p><p>92 We used these results to determine parameter settings of single and combined networks of spiking neurons which make the system behave as a contracting one. [sent-209, score-0.739]
</p><p>93 Similarly, we veriﬁed experimentally that CCNs with strong recurrent couplings are not contracting as predicted by the theory. [sent-210, score-0.455]
</p><p>94 We compare the CCN with very strong lateral recurrent excitation and low inhibition to a weakly coupled CCN. [sent-214, score-0.289]
</p><p>95 The ﬁgures present the raster plot and mean rates of the CCNs response (calculated after the dashed line) to the same stimuli starting from two different initial conditions. [sent-215, score-0.102]
</p><p>96 Panels (b) and (e) show the response of a contracting CCN, whereas panels (c) and (f) show that the system response depends on the initial conditions of (a) and (d). [sent-216, score-0.537]
</p><p>97 (a) and (d) Single trial responses of CCN1 and CCN2 to the input stimulus shown in (c); (b) Connectivity matrix that couples the two CCNs (inverted identity matrix); (e) Mean response of CCNs, averaged over 20 trials (data points) superimposed to average input frequencies (shaded area). [sent-219, score-0.265]
</p><p>98 The response of the coupled CCNs converged to the same mean solution, consistent with the hypothesis that the combined system is contracting. [sent-220, score-0.244]
</p><p>99 A VLSI array of low-power spiking neurons and bistable synapses with spike–timing dependent plasticity. [sent-284, score-0.397]
</p><p>100 Context dependent ampliﬁcation of both rate and eventcorrelation in a VLSI network of spiking neurons. [sent-343, score-0.125]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ccn', 0.544), ('contraction', 0.443), ('ccns', 0.265), ('contracting', 0.265), ('neurons', 0.242), ('vlsi', 0.186), ('neuron', 0.167), ('wie', 0.146), ('excitatory', 0.106), ('non', 0.102), ('recurrent', 0.1), ('subsystems', 0.093), ('inhibitory', 0.086), ('system', 0.073), ('chip', 0.071), ('excitation', 0.069), ('feed', 0.069), ('jacobian', 0.069), ('spiking', 0.067), ('fs', 0.065), ('hz', 0.064), ('response', 0.064), ('synapses', 0.062), ('stimulus', 0.06), ('coupled', 0.06), ('wei', 0.059), ('ltus', 0.053), ('swta', 0.053), ('couplings', 0.053), ('connectivity', 0.053), ('combined', 0.047), ('cooperative', 0.046), ('networks', 0.045), ('spike', 0.044), ('coupling', 0.044), ('units', 0.043), ('bump', 0.042), ('wsel', 0.04), ('neuromorphic', 0.039), ('trial', 0.039), ('input', 0.039), ('initial', 0.038), ('stimulated', 0.037), ('strong', 0.037), ('activity', 0.036), ('ampli', 0.035), ('condition', 0.035), ('ih', 0.035), ('wta', 0.035), ('comprising', 0.034), ('conditions', 0.033), ('frequency', 0.033), ('network', 0.032), ('js', 0.032), ('douglas', 0.032), ('recti', 0.032), ('mj', 0.031), ('threshold', 0.03), ('weak', 0.03), ('steady', 0.03), ('dt', 0.03), ('bumps', 0.03), ('board', 0.03), ('analog', 0.029), ('uncorrelated', 0.029), ('trains', 0.028), ('aer', 0.028), ('metric', 0.028), ('bi', 0.027), ('directionally', 0.027), ('hwta', 0.027), ('hysteretic', 0.027), ('lohmiller', 0.027), ('winfried', 0.027), ('rate', 0.026), ('symmetric', 0.026), ('array', 0.026), ('trajectories', 0.026), ('external', 0.026), ('poisson', 0.025), ('static', 0.025), ('guration', 0.025), ('competitive', 0.025), ('dynamics', 0.025), ('responses', 0.024), ('cortical', 0.024), ('ltu', 0.023), ('hahnloser', 0.023), ('inhibition', 0.023), ('cooperation', 0.023), ('bartolozzi', 0.023), ('indiveri', 0.023), ('stimulating', 0.023), ('wx', 0.023), ('negative', 0.023), ('circuits', 0.023), ('synaptic', 0.023), ('ex', 0.022), ('neural', 0.021), ('chicca', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="60-tfidf-1" href="./nips-2007-Contraction_Properties_of_VLSI_Cooperative_Competitive_Neural_Networks_of_Spiking_Neurons.html">60 nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</a></p>
<p>Author: Emre Neftci, Elisabetta Chicca, Giacomo Indiveri, Jean-jeacques Slotine, Rodney J. Douglas</p><p>Abstract: A non–linear dynamic system is called contracting if initial conditions are forgotten exponentially fast, so that all trajectories converge to a single trajectory. We use contraction theory to derive an upper bound for the strength of recurrent connections that guarantees contraction for complex neural networks. Speciﬁcally, we apply this theory to a special class of recurrent networks, often called Cooperative Competitive Networks (CCNs), which are an abstract representation of the cooperative-competitive connectivity observed in cortex. This speciﬁc type of network is believed to play a major role in shaping cortical responses and selecting the relevant signal among distractors and noise. In this paper, we analyze contraction of combined CCNs of linear threshold units and verify the results of our analysis in a hybrid analog/digital VLSI CCN comprising spiking neurons and dynamic synapses. 1</p><p>2 0.21761651 <a title="60-tfidf-2" href="./nips-2007-Learning_to_classify_complex_patterns_using_a_VLSI_network_of_spiking_neurons.html">117 nips-2007-Learning to classify complex patterns using a VLSI network of spiking neurons</a></p>
<p>Author: Srinjoy Mitra, Giacomo Indiveri, Stefano Fusi</p><p>Abstract: We propose a compact, low power VLSI network of spiking neurons which can learn to classify complex patterns of mean ﬁring rates on–line and in real–time. The network of integrate-and-ﬁre neurons is connected by bistable synapses that can change their weight using a local spike–based plasticity mechanism. Learning is supervised by a teacher which provides an extra input to the output neurons during training. The synaptic weights are updated only if the current generated by the plastic synapses does not match the output desired by the teacher (as in the perceptron learning rule). We present experimental results that demonstrate how this VLSI network is able to robustly classify uncorrelated linearly separable spatial patterns of mean ﬁring rates.</p><p>3 0.1705292 <a title="60-tfidf-3" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>Author: Jonathan W. Pillow, Peter E. Latham</p><p>Abstract: Point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, owing in part to the fact that they do not take into account multiple stages of processing. Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more ﬂexible for ﬁtting neural data. More importantly, it allows us to estimate connectivity patterns among neurons (both observed and unobserved), and may provide insight into how networks process sensory input. We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model’s performance using a simulated example network consisting of two coupled neurons.</p><p>4 0.1704758 <a title="60-tfidf-4" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><p>5 0.16929856 <a title="60-tfidf-5" href="./nips-2007-A_configurable_analog_VLSI_neural_network_with_spiking_neurons_and_self-regulating_plastic_synapses.html">14 nips-2007-A configurable analog VLSI neural network with spiking neurons and self-regulating plastic synapses</a></p>
<p>Author: Massimiliano Giulioni, Mario Pannunzi, Davide Badoni, Vittorio Dante, Paolo D. Giudice</p><p>Abstract: We summarize the implementation of an analog VLSI chip hosting a network of 32 integrate-and-ﬁre (IF) neurons with spike-frequency adaptation and 2,048 Hebbian plastic bistable spike-driven stochastic synapses endowed with a selfregulating mechanism which stops unnecessary synaptic changes. The synaptic matrix can be ﬂexibly conﬁgured and provides both recurrent and AER-based connectivity with external, AER compliant devices. We demonstrate the ability of the network to efﬁciently classify overlapping patterns, thanks to the self-regulating mechanism.</p><p>6 0.14041938 <a title="60-tfidf-6" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>7 0.13696676 <a title="60-tfidf-7" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>8 0.11457665 <a title="60-tfidf-8" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<p>9 0.10937389 <a title="60-tfidf-9" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>10 0.092547163 <a title="60-tfidf-10" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>11 0.0911185 <a title="60-tfidf-11" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>12 0.088108331 <a title="60-tfidf-12" href="./nips-2007-Stable_Dual_Dynamic_Programming.html">185 nips-2007-Stable Dual Dynamic Programming</a></p>
<p>13 0.063690908 <a title="60-tfidf-13" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>14 0.060402766 <a title="60-tfidf-14" href="./nips-2007-Subspace-Based_Face_Recognition_in_Analog_VLSI.html">188 nips-2007-Subspace-Based Face Recognition in Analog VLSI</a></p>
<p>15 0.058471896 <a title="60-tfidf-15" href="./nips-2007-Estimating_disparity_with_confidence_from_energy_neurons.html">81 nips-2007-Estimating disparity with confidence from energy neurons</a></p>
<p>16 0.057133127 <a title="60-tfidf-16" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>17 0.051242087 <a title="60-tfidf-17" href="./nips-2007-An_online_Hebbian_learning_rule_that_performs_Independent_Component_Analysis.html">26 nips-2007-An online Hebbian learning rule that performs Independent Component Analysis</a></p>
<p>18 0.047000147 <a title="60-tfidf-18" href="./nips-2007-Efficient_Bayesian_Inference_for_Dynamically_Changing_Graphs.html">75 nips-2007-Efficient Bayesian Inference for Dynamically Changing Graphs</a></p>
<p>19 0.046106786 <a title="60-tfidf-19" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>20 0.040050574 <a title="60-tfidf-20" href="./nips-2007-Second_Order_Bilinear_Discriminant_Analysis_for_single_trial_EEG_analysis.html">173 nips-2007-Second Order Bilinear Discriminant Analysis for single trial EEG analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.145), (1, 0.111), (2, 0.295), (3, 0.054), (4, -0.003), (5, -0.07), (6, 0.135), (7, 0.029), (8, 0.087), (9, 0.085), (10, -0.002), (11, -0.021), (12, -0.001), (13, -0.075), (14, -0.002), (15, 0.013), (16, -0.013), (17, -0.034), (18, -0.075), (19, -0.064), (20, 0.03), (21, 0.041), (22, -0.022), (23, -0.068), (24, 0.025), (25, 0.035), (26, -0.025), (27, 0.021), (28, 0.026), (29, 0.04), (30, -0.082), (31, 0.002), (32, 0.026), (33, -0.056), (34, -0.004), (35, -0.071), (36, 0.067), (37, -0.014), (38, 0.107), (39, -0.016), (40, -0.05), (41, -0.001), (42, 0.046), (43, -0.069), (44, -0.024), (45, -0.021), (46, 0.089), (47, -0.003), (48, -0.027), (49, -0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95830864 <a title="60-lsi-1" href="./nips-2007-Contraction_Properties_of_VLSI_Cooperative_Competitive_Neural_Networks_of_Spiking_Neurons.html">60 nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</a></p>
<p>Author: Emre Neftci, Elisabetta Chicca, Giacomo Indiveri, Jean-jeacques Slotine, Rodney J. Douglas</p><p>Abstract: A non–linear dynamic system is called contracting if initial conditions are forgotten exponentially fast, so that all trajectories converge to a single trajectory. We use contraction theory to derive an upper bound for the strength of recurrent connections that guarantees contraction for complex neural networks. Speciﬁcally, we apply this theory to a special class of recurrent networks, often called Cooperative Competitive Networks (CCNs), which are an abstract representation of the cooperative-competitive connectivity observed in cortex. This speciﬁc type of network is believed to play a major role in shaping cortical responses and selecting the relevant signal among distractors and noise. In this paper, we analyze contraction of combined CCNs of linear threshold units and verify the results of our analysis in a hybrid analog/digital VLSI CCN comprising spiking neurons and dynamic synapses. 1</p><p>2 0.84977549 <a title="60-lsi-2" href="./nips-2007-Learning_to_classify_complex_patterns_using_a_VLSI_network_of_spiking_neurons.html">117 nips-2007-Learning to classify complex patterns using a VLSI network of spiking neurons</a></p>
<p>Author: Srinjoy Mitra, Giacomo Indiveri, Stefano Fusi</p><p>Abstract: We propose a compact, low power VLSI network of spiking neurons which can learn to classify complex patterns of mean ﬁring rates on–line and in real–time. The network of integrate-and-ﬁre neurons is connected by bistable synapses that can change their weight using a local spike–based plasticity mechanism. Learning is supervised by a teacher which provides an extra input to the output neurons during training. The synaptic weights are updated only if the current generated by the plastic synapses does not match the output desired by the teacher (as in the perceptron learning rule). We present experimental results that demonstrate how this VLSI network is able to robustly classify uncorrelated linearly separable spatial patterns of mean ﬁring rates.</p><p>3 0.81267321 <a title="60-lsi-3" href="./nips-2007-A_configurable_analog_VLSI_neural_network_with_spiking_neurons_and_self-regulating_plastic_synapses.html">14 nips-2007-A configurable analog VLSI neural network with spiking neurons and self-regulating plastic synapses</a></p>
<p>Author: Massimiliano Giulioni, Mario Pannunzi, Davide Badoni, Vittorio Dante, Paolo D. Giudice</p><p>Abstract: We summarize the implementation of an analog VLSI chip hosting a network of 32 integrate-and-ﬁre (IF) neurons with spike-frequency adaptation and 2,048 Hebbian plastic bistable spike-driven stochastic synapses endowed with a selfregulating mechanism which stops unnecessary synaptic changes. The synaptic matrix can be ﬂexibly conﬁgured and provides both recurrent and AER-based connectivity with external, AER compliant devices. We demonstrate the ability of the network to efﬁciently classify overlapping patterns, thanks to the self-regulating mechanism.</p><p>4 0.74193329 <a title="60-lsi-4" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>Author: Dejan Pecevski, Wolfgang Maass, Robert A. Legenstein</p><p>Abstract: Reward-modulated spike-timing-dependent plasticity (STDP) has recently emerged as a candidate for a learning rule that could explain how local learning rules at single synapses support behaviorally relevant adaptive changes in complex networks of spiking neurons. However the potential and limitations of this learning rule could so far only be tested through computer simulations. This article provides tools for an analytic treatment of reward-modulated STDP, which allow us to predict under which conditions reward-modulated STDP will be able to achieve a desired learning effect. In particular, we can produce in this way a theoretical explanation and a computer model for a fundamental experimental ﬁnding on biofeedback in monkeys (reported in [1]).</p><p>5 0.71899176 <a title="60-lsi-5" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<p>Author: Devarajan Sridharan, Brian Percival, John Arthur, Kwabena A. Boahen</p><p>Abstract: We describe a neurobiologically plausible model to implement dynamic routing using the concept of neuronal communication through neuronal coherence. The model has a three-tier architecture: a raw input tier, a routing control tier, and an invariant output tier. The correct mapping between input and output tiers is realized by an appropriate alignment of the phases of their respective background oscillations by the routing control units. We present an example architecture, implemented on a neuromorphic chip, that is able to achieve circular-shift invariance. A simple extension to our model can accomplish circular-shift dynamic routing with only O(N ) connections, compared to O(N 2 ) connections required by traditional models. 1 Dynamic Routing Circuit Models for Circular-Shift Invariance Dynamic routing circuit models are among the most prominent neural models for invariant recognition [1] (also see [2] for review). These models implement shift invariance by dynamically changing spatial connectivity to transform an object to a standard position or orientation. The connectivity between the raw input and invariant output layers is controlled by routing units, which turn certain subsets of connections on or off (Figure 1A). An important feature of this model is the explicit representation of what and where information in the main network and the routing units, respectively; the routing units use the where information to create invariant representations. Traditional solutions for shift invariance are neurobiologically implausible for at least two reasons. First, there are too many synaptic connections: for N input neurons, N output neurons and N possible input-output mappings, the network requires O(N 2 ) connections in the routing layer— between each of the N routing units and each set of N connections that that routing unit gates (Figure 1A). Second, these connections must be extremely precise: each routing unit must activate an inputoutput mapping (N individual connections) corresponding to the desired shift (as highlighted in Figure 1A). Other approaches that have been proposed, including invariant feature networks [3,4], also suffer from signiﬁcant drawbacks, such as the inability to explicitly represent where information [2]. It remains an open question how biology could achieve shift invariance without proﬂigate and precise connections. In this article, we propose a simple solution for shift invariance for quantities that are circular or periodic in nature—circular-shift invariance (CSI)—orientation invariance in vision and key invariance in music. The visual system may create orientation-invariant representations to aid recognition under conditions of object rotation or head-tilt [5,6]; a similar mechanism could be employed by the auditory system to create key-invariant representations under conditions where the same melody 1 Figure 1: Dynamic routing. A In traditional dynamic routing, connections from the (raw) input layer to the (invariant) output layer are gated by routing units. For instance, the mapping from A to 5, B to 6, . . . , F to 4 is achieved by turning on the highlighted routing unit. B In time-division multiplexing (TDM), the encoder samples input channels periodically (using a rotating switch) while the decoder sends each sample to the appropriate output channel (based on its time bin). TDM can be extended to achieve a circular-shift transformation by altering the angle between encoder and decoder switches (θ), thereby creating a rotated mapping between input and output channels (adapted from [7]). is played in different keys. Similar to orientation, which is a periodic quantity, musical notes one octave apart sound alike, a phenomenon known as octave equivalence [8]. Thus, the problems of key invariance and orientation invariance admit similar solutions. Deriving inspiration from time-division multiplexing (TDM), we propose a neural network for CSI that uses phase to encode and decode information. We modulate the temporal window of communication between (raw) input and (invariant) output neurons to achieve the appropriate input–output mapping. Extending TDM, any particular circular-shift transformation can be accomplished by changing the relative angle, θ, between the rotating switches of the encoder (that encodes the raw input in time) and decoder (that decodes the invariant output in time) (Figure 1B). This obviates the need to hardwire routing control units that speciﬁcally modulate the strength of each possible inputoutput connection, thereby signiﬁcantly reducing the complexity inherent in the traditional dynamic routing solution. Similarly, a remapping between the input and output neurons can be achieved by introducing a relative phase-shift in their background oscillations. 2 Dynamic Routing through Neuronal Coherence To modulate the temporal window of communication, the model uses a ring of neurons (the oscillation ring) to select the pool of neurons (in the projection ring) that encode or decode information at a particular time (Figure 2A). Each projection pool encodes a speciﬁc value of the feature (for example, one of twelve musical notes). Upon activation by external input, each pool is active only when background inhibition generated by the oscillation ring (outer ring of neurons) is at a minimum. In addition to exciting 12 inhibitory interneurons in the projection ring, each oscillation ring neuron excites its nearest 18 neighbors in the clockwise direction around the oscillation ring. As a result, a wave of inhibition travels around the projection ring that allows only one pool to be excitable at any point in time. These neurons become excitable at roughly the same time (numbered sectors, inner ring) by virtue of recurrent excitatory intra-pool connections. Decoding is accomplished by a second tier of rings (Figure 2B). The projection ring of the ﬁrst (input) tier connects all-to-all to the projection ring of the second (output) tier. The two oscillation rings create a window of excitability for the pools of neurons in their respective projection rings. Hence, the most effective communication occurs between input and output pools that become excitable at the same time (i.e. are oscillating in phase with one another [9]). The CSI problem is solved by introducing a phase-shift between the input and output tiers. If they are exactly in phase, then an input pool is simply mapped to the output pool directly above it. If their 2 Figure 2: Double-Ring Network for Encoding and Decoding. A The projection (inner) ring is divided into (numbered) pools. The oscillation (outer) ring modulates sub-threshold activity (waveforms) of the projection ring by exciting (black distribution) inhibitory neurons that inhibit neighboring projection neurons. A wave of activity travels around the oscillation ring due to asymmetric excitatory connections, creating a corresponding wave of inhibitory activity in the projection ring, such that only one pool of projection neurons is excitable (spikes) at a given time. B Two instances of the double-ring structure from A. The input projection ring connects all-to-all to the output projection ring (dashed lines). Because each input pool will spike only during a distinct time bin, and each output pool is excitable only in a certain time bin, communication occurs between input and output pools that are oscillating in phase with each other. Appropriate phase offset between input and output oscillation rings realizes the desired circular shift (input pool H to output pool 1, solid arrow). C Interactions among pools highlighted in B. phases are different, the input is dynamically routed to an appropriate circularly shifted position in the output tier. Such changes in phase are analogous to adjusting the angle of the rotating switch at either the encoder or the decoder in TDM (see Figure 1B). There is some evidence that neural systems could employ phase relationships of subthreshold oscillations to selectively target neural populations [9-11]. 3 Implementation in Silicon We implemented this solution to CSI on a neuromorphic silicon chip [12]. The neuromorphic chip has neurons whose properties resemble that of biological neurons; these neurons even have intrinsic differences, thereby mimicking heterogeneity in real neurobiological systems. The chip uses a conductance-based spiking model for both inhibitory and excitatory neurons. Inhibitory neurons project to nearby excitatory and inhibitory neurons via a diffusor network that determines the spread of inhibition. A lookup table of excitatory synaptic connectivity is stored in a separate randomaccess memory (RAM) chip. Spikes occurring on-chip are converted to a neuron address, mapped to synapses (if any) via the lookup table, and routed to the targeted on-chip synapse. A universal serial bus (USB) interface chip communicates spikes to and from a computer, for external input and 3 Figure 3: Traveling-wave activity in the oscillation ring. A Population activity (5ms bins) of a pool of eighteen (adjacent) oscillation neurons. B Increasing the strength of feedforward excitation led to increasing frequencies of periodic ﬁring in the θ and α range (1-10 Hz). Strength of excitation is the amplitude change in post-synaptic conductance due to a single pre-synaptic spike (measured relative to minimum amplitude used). data analysis, respectively. Simulations on the chip occur in real-time, making it an attractive option for implementing the model. We conﬁgured the following parameters: • Magnitude of a potassium M-current: increasing this current’s magnitude increased the post-spike repolarization time of the membrane potential, thereby constraining spiking to a single time bin per cycle. • The strength of excitatory and inhibitory synapses: a correct balance had to be established between excitation and inhibition to make only a small subset of neurons in the projection rings ﬁre at a time—too much excitation led to widespread ﬁring and too much inhibition led to neurons that were entirely silent or ﬁred sporadically. • The space constant of inhibitory spread: increasing the spread was effective in preventing runaway excitation, which could occur due to the recurrent excitatory connections. We were able to create a stable traveling wave of background activity within the oscillation ring. We transiently stimulated a small subset of the neurons, which initiated a wave of activity that propagated in a stable manner around the ring after the transient external stimulation had ceased (Figure 3A). The network frequency determined from a Fourier transform of the network activity smoothed with a non-causal Gaussian kernel (FDHM = 80ms) was 7.4Hz. The frequency varied with the strength of the neurons’ excitatory connections (Figure 3B), measured as the amplitude of the step increase in membrane conductivity due to the arrival of a pre-synaptic spike. Over much of the range of the synaptic strengths tested, we observed stable oscillations in the θ and α bands (1-10Hz); the frequency appeared to increase logarithmically with synaptic strength. 4 Phase-based Encoding and Decoding In order to assess the best-case performance of the model, the background activity in the input and output projection rings was derived from the input oscillation ring. Their spikes were delivered to the appropriately circularly-shifted output oscillation neurons. The asymmetric feedforward connections were disabled in the output oscillation ring. For instance, in order to achieve a circular shift by k pools (i.e. mapping input projection pool 1 to output projection pool k + 1, input pool 2 to output pool k + 2, and so on), activity from the input oscillation neurons closest to input pool 1 was fed into the output oscillation neurons closest to output pool k. By providing the appropriate phase difference between input and output oscillation, we were able to assess the performance of the model under ideal conditions. In the Discussion section, we discuss a biologically plausible mechanism to control the relative phases. 4 Figure 4: Phase-based encoding. Rasters indicating activity of projection pools in 1ms bins, and mean phase of ﬁring (×’s) for each pool (relative to arbitrary zero time). The abscissa shows ﬁring time normalized by the period of oscillation (which may be converted to ﬁring phase by multiplication by 2π). Under constant input to the input projection ring, the input pools ﬁre approximately in sequence. Two cycles of pool activity normalized by maximum ﬁring rate for each pool are shown in left inset (for clarity, pools 1-6 are shown in the top panel and pools 7-12 are shown separately in the bottom panel); phase of background inhibition of pool 4 is shown (below) for reference. Phase-aligned average1 of activity (right inset) showed that the ﬁring times were relatively tight and uniform across pools: a standard deviation of 0.0945 periods, or equivalently, a spread of 1.135 pools at any instant of time. We veriﬁed that the input projection pools ﬁred in a phase-shifted fashion relative to one another, a property critical for accurate encoding (see Figure 2). We stimulated all pools in the input projection ring simultaneously while the input oscillation ring provided a periodic wave of background inhibition. The mean phase of ﬁring for each pool (relative to arbitrary zero time) increased nearly linearly with pool number, thereby providing evidence for accurate, phase-based encoding (Figure 4). The ﬁring times of all pools are shown for two cycles of background oscillatory activity (Figure 4 left inset). A phase-aligned average1 showed that the timing was relatively tight (standard deviation 1.135 pools) and uniform across pools of neurons (Figure 4 right inset). We then characterized the system’s ability to correctly decode this encoding under a given circular shift. The shift was set to seven pools, mapping input pool 1 to output pool 8, and so on. Each input pool was stimulated in turn. We expected to see only the appropriately shifted output pool become highly active. In fact, not only was this pool active, but other pools around it were also active, though to a lesser extent (Figure 5A). Thus, the phase-encoded input was decoded successfully, and circularly shifted, except that the output units were broadly tuned. To quantify the overall precision of encoding and decoding, we constructed an input-locked average of the tuning curves (Figure 5B): the curves were circularly shifted to the left by an amount corresponding to the stimulated input pool number, and the raw pool ﬁring rates were averaged. If the phase-based encoding and decoding were perfect, the peak should occur at a shift of 7 pools. 1 The phase-aligned average was constructed by shifting the pool-activity curves by the (# of the pool) × 1 ( 12 of the period) to align activity across pools, which was then averaged. 5 Figure 5: Decoding phase-encoded input. A In order to assess decoding performance under a given circular shift (here 7 pools) each input pool was stimulated in turn and activity in each output pool was recorded and averaged over 500ms. The pool’s response, normalized by its maximum ﬁring rate, is plotted for each stimulated input pool (arrows pointing to curves, color code as in Figure 4). Each input pool stimulation trial consistently resulted in peak activity in the appropriate output pool; however, adjacent pools were also active, but to a lesser extent, resulting in a broad tuning curve. B The best-ﬁt Gaussian (dot-dashed grey curve, σ = 2.30 pools) to the input-locked average of the raw pool ﬁring rates (see text for details) revealed a maximum between a shift of 7 and 8 pools (inverted grey triangle; expected peak at a shift of 7 pools). Indeed, the highest (average) ﬁring rate corresponded to a shift of 7 pools. However, the activity corresponding to a shift of 8 pools was nearly equal to that of 7 pools, and the best ﬁtting Gaussian curve to the activity histogram (grey dot-dashed line) peaked at a point between pools 7 and 8 (inverted grey triangle). The standard deviation (σ) was 2.30 pools, versus the expected ideal σ of 1.60, which corresponds to the encoding distribution (σ = 1.135 pools) convolved with itself. 5 Discussion We have demonstrated a biologically plausible mechanism for the dynamic routing of information in time that obviates the need for precise gating of connections. This mechanism requires that a wave of activity propagate around pools of neurons arranged in a ring. While previous work has described traveling waves in a ring of neurons [13], and a double ring architecture (for determining head-direction) [14], our work combines these two features (twin rings with phase-shifted traveling waves) to achieve dynamic routing. These features of the model are found in the cortex: Bonhoeffer and Grinwald [15] describe iso-orientation columns in the cat visual cortex that are arranged in ring-like pinwheel patterns, with orientation tuning changing gradually around the pinwheel center. Moreover, Rubino et al. [16] have shown that coherent oscillations can propagate as waves across the cortical surface in the motor cortex of awake, behaving monkeys performing a delayed reaching task. Our solution for CSI is also applicable to music perception. In the Western twelve-tone, equaltemperament tuning system (12-tone scale), each octave is divided into twelve logarithmicallyspaced notes. Human observers are known to construct mental representations for raw notes that are invariant of the (perceived) key of the music: a note of C heard in the key of C-Major is perceptually equivalent to the note C# heard in the key of C#-Major [8,17]. In previous dynamic routing models of key invariance, the tonic—the ﬁrst note of the key (e.g., C is the tonic of C-Major)— supplies the equivalent where information used by routing units that gate precise connections to map the raw note into a key-invariant output representation [17]. To achieve key invariance in our model, the bottom tier encodes raw note information while the top tier decodes key-invariant notes (Figure 6). The middle tier receives the tonic information and aligns the phase of the ﬁrst output pool (whose invariant representation corresponds to the tonic) with the appropriate input pool (whose raw note representation corresponds to the tonic of the perceived key). 6 Figure 6: Phase-based dynamic routing to achieve key-invariance. The input (bottom) tier encodes raw note information, and the output (top) tier decodes key-invariant information. The routing (middle) tier sets the phase of the background wave activity in the input and output oscillation rings (dashed arrows) such that the ﬁrst output pool is in phase with the input pool representing the note corresponding to the tonic. On the left, where G is the tonic, input pool G, output pool 1, and the routing tier are in phase with one another (black clocks), while input pool C and output pool 6 are in phase with one another (grey clocks). Thus, the raw note input, G, activates the invariant output 1, which corresponds to the perceived tonic invariant representation (heavy solid arrows). On the right, the same raw input note, G, is active, but the key is different and A is now the active tonic; thus the raw input, G, is now mapped to output pool 11. The tonic information is supplied to a speciﬁc pool in the routing ring according to the perceived key. This pool projects directly down to the input pool corresponding to the tonic. This ensures that the current tonic’s input pool is excitable in the same time bin as the ﬁrst output pool. Each of the remaining raw input notes of the octave is mapped by time binning to the corresponding key-invariant representation in the output tier, as the phases of input pools are all shifted by the same amount. Supporting evidence for phase-based encoding of note information comes from MEG recordings in humans: the phase of the MEG signal (predominantly over right hemispheric sensor locations) tracks the note of the heard note sequence with surprising accuracy [18]. The input and output tiers’ periods must be kept in lock-step, which can be accomplished through more plausible means than employed in the current implementation of this model. Here, we maintained a ﬁxed phase shift between the input and output oscillation rings by feeding activity from the input oscillation ring to the appropriately shifted pool in the output oscillation ring. This approach allowed us to avoid difﬁculties achieving coherent oscillations at identical frequencies in the input and output oscillation rings. Alternatively, entrainment could be achieved even when the frequencies are not identical—a more biologically plausible scenario—if the routing ring resets the phase of the input and output rings on a cycle-by-cycle basis. Lakatos et al. [19] have shown that somatosensory inputs can reset the phase of ongoing neuronal oscillations in the primary auditory cortex (A1), which helps in the generation of a uniﬁed auditory-tactile percept (the so-called “Hearing-Hands Effect”). A simple extension to our model can reduce the number of connections below the requirements of traditional dynamic routing models. Instead of having all-to-all connections between the input and output layers, a relay layer of very few (M N ) neurons could be used to transmit the spikes form the input neurons to the output neurons (analogous to the single wire connecting encoder and decoder in Figure 1B). A small number of (or ideally even one) relay neurons sufﬁces because encoding and decoding occur in time. Hence, the connections between each input pool and the relay neurons require O(M N ) ≈ O(N ) connections (as long as M does not scale with N ) and those between the relay neurons and each output pool require O(M N) ≈ O(N ) connections as well. Thus, by removing all-to-all connectivity between the input and output units (a standard feature in traditional dynamic routing models), the number of required connections is reduced from O(N 2 ) 7 to O(N ). Further, by replacing the strict pool boundaries with nearest neighbor connectivity in the projection rings, the proposed model can accommodate a continuum of rotation angles. In summary, we propose that the mechanism of dynamic routing through neuronal coherence could be a general mechanism that could be used by multiple sensory and motor modalities in the neocortex: it is particularly suitable for placing raw information in an appropriate context (deﬁned by the routing tier). Acknowledgments DS was supported by a Stanford Graduate Fellowship and BP was supported under a National Science Foundation Graduate Research Fellowship. References [1] Olshausen B.A., Anderson C.H. & Van Essen D.C. (1993). A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information. Journal of Neuroscience 13(11):47004719. [2] Wiskott L. (2004). How does our visual system achieve shift and size invariance? In J.L. van Hemmen & T.J. Sejnowski (Eds.), 23 Problems in Systems Neuroscience, Oxford University Press. [3] Fukushima K., Miyake S. & Ito T. (1983). A neural network model for a mechanism of visual pattern recognition. IEEE Transactions on Systems, Man and Cybernetics 13:826-834. [4] Mel B.W., Ruderman D.L & Archie K.A. (1998). Translation invariant orientation tuning in visual “complex” cells could derive from intradendritic computations. Journal of Neuroscience 18(11):4325-4334. [5] McKone, E. & Grenfell, T. (1999). Orientation invariance in naming rotated objects: Individual differences and repetition priming. Perception and Psychophysics, 61:1590-1603. [6] Harris IM & Dux PE. (2005). Orientation-invariant object recognition: evidence from repetition blindness. Cognition, 95(1):73-93. [7] Naval Electrical Engineering Training Series (NEETS). Module 17, Radio-Frequency Communication Principles, Chapter 3, pp.32. Published online at http://www.tpub.com/content/neets/14189 (Integrated Publishing). [8] Krumhansl C.L. (1990). Cognitive foundations of musical pitch. Oxford University Press, 1990. [9] Fries P. (2005). A mechanism for cognitive dynamics: neuronal communication through neuronal coherence. Trends in Cognitive Sciences 9(10):474-480. [10] Buzsaki G. & Draguhn A. (2004). Neuronal Oscillations in Cortical Networks. Science 304(5679):19261929. [11] Sejnowski T.J. & Paulsen O. (2006). Network oscillations: Emerging computational principles. Journal of Neuroscience 26(6):1673-1676. [12] Arthur J.A. & Boahen K. (2005). Learning in Silicon: Timing is Everything. Advances in Neural Information Processing Systems 17, B Sholkopf and Y Weiss, Eds, MIT Press, 2006. [13] Hahnloser R.H.R., Sarpeshkar R., Mahowald M.A., Douglas R.J., & Seung H.S. (2000). Digital selection and analogue ampliﬁcation coexist in a cortex-inspired silicon circuit. Nature 405:947-951. [14] Xie X., Hahnloser R.H.R., & Seung H.S (2002). Double-ring network modeling of the head-direction system. Phys. Rev. E66 041902:1-9. [15] Bonhoeffer K. & Grinwald A. (1991). Iso-orientation domains in cat visual cortex are arranged in pinwheel-like patterns. Nature 353:426-437. [16] Rubino D., Robbins K.A. & Hastopoulos N.G. (2006). Propagating waves mediate information transfer in the motor cortex. Nature Neuroscience 9:1549-1557. [17] Bharucha J.J. (1999). Neural nets, temporal composites and tonality. In D. Deutsch (Ed.), The Psychology of Music (2d Ed.) Academic Press, New York. [18] Patel A.D. & Balaban E. (2000). Temporal patterns of human cortical activity reﬂect tone sequence structure. Nature 404:80-84. [19] Lakatos P., Chen C., O’Connell M., Mills A. & Schroeder C. (2007). Neuronal oscillations and multisensory interaction in primary auditory cortex. Neuron 53(2):279-292. 8</p><p>6 0.56325817 <a title="60-lsi-6" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>7 0.54635918 <a title="60-lsi-7" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>8 0.51234335 <a title="60-lsi-8" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>9 0.51171768 <a title="60-lsi-9" href="./nips-2007-Estimating_disparity_with_confidence_from_energy_neurons.html">81 nips-2007-Estimating disparity with confidence from energy neurons</a></p>
<p>10 0.50766832 <a title="60-lsi-10" href="./nips-2007-An_online_Hebbian_learning_rule_that_performs_Independent_Component_Analysis.html">26 nips-2007-An online Hebbian learning rule that performs Independent Component Analysis</a></p>
<p>11 0.50648195 <a title="60-lsi-11" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>12 0.36318865 <a title="60-lsi-12" href="./nips-2007-Subspace-Based_Face_Recognition_in_Analog_VLSI.html">188 nips-2007-Subspace-Based Face Recognition in Analog VLSI</a></p>
<p>13 0.36239496 <a title="60-lsi-13" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>14 0.34329915 <a title="60-lsi-14" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>15 0.30709633 <a title="60-lsi-15" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>16 0.2701796 <a title="60-lsi-16" href="./nips-2007-Selecting_Observations_against_Adversarial_Objectives.html">174 nips-2007-Selecting Observations against Adversarial Objectives</a></p>
<p>17 0.25985074 <a title="60-lsi-17" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>18 0.24053639 <a title="60-lsi-18" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>19 0.2378723 <a title="60-lsi-19" href="./nips-2007-Unconstrained_On-line_Handwriting_Recognition_with_Recurrent_Neural_Networks.html">210 nips-2007-Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks</a></p>
<p>20 0.23675391 <a title="60-lsi-20" href="./nips-2007-Augmented_Functional_Time_Series_Representation_and_Forecasting_with_Gaussian_Processes.html">28 nips-2007-Augmented Functional Time Series Representation and Forecasting with Gaussian Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.031), (13, 0.045), (16, 0.466), (19, 0.023), (21, 0.031), (31, 0.018), (34, 0.019), (35, 0.017), (47, 0.089), (83, 0.074), (85, 0.012), (86, 0.035), (90, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90686482 <a title="60-lda-1" href="./nips-2007-Contraction_Properties_of_VLSI_Cooperative_Competitive_Neural_Networks_of_Spiking_Neurons.html">60 nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</a></p>
<p>Author: Emre Neftci, Elisabetta Chicca, Giacomo Indiveri, Jean-jeacques Slotine, Rodney J. Douglas</p><p>Abstract: A non–linear dynamic system is called contracting if initial conditions are forgotten exponentially fast, so that all trajectories converge to a single trajectory. We use contraction theory to derive an upper bound for the strength of recurrent connections that guarantees contraction for complex neural networks. Speciﬁcally, we apply this theory to a special class of recurrent networks, often called Cooperative Competitive Networks (CCNs), which are an abstract representation of the cooperative-competitive connectivity observed in cortex. This speciﬁc type of network is believed to play a major role in shaping cortical responses and selecting the relevant signal among distractors and noise. In this paper, we analyze contraction of combined CCNs of linear threshold units and verify the results of our analysis in a hybrid analog/digital VLSI CCN comprising spiking neurons and dynamic synapses. 1</p><p>2 0.88376027 <a title="60-lda-2" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><p>3 0.84658867 <a title="60-lda-3" href="./nips-2007-Topmoumoute_Online_Natural_Gradient_Algorithm.html">206 nips-2007-Topmoumoute Online Natural Gradient Algorithm</a></p>
<p>Author: Nicolas L. Roux, Pierre-antoine Manzagol, Yoshua Bengio</p><p>Abstract: Guided by the goal of obtaining an optimization algorithm that is both fast and yields good generalization, we study the descent direction maximizing the decrease in generalization error or the probability of not increasing generalization error. The surprising result is that from both the Bayesian and frequentist perspectives this can yield the natural gradient direction. Although that direction can be very expensive to compute we develop an efﬁcient, general, online approximation to the natural gradient descent which is suited to large scale problems. We report experimental results showing much faster convergence in computation time and in number of iterations with TONGA (Topmoumoute Online natural Gradient Algorithm) than with stochastic gradient descent, even on very large datasets.</p><p>4 0.83865196 <a title="60-lda-4" href="./nips-2007-Robust_Regression_with_Twinned_Gaussian_Processes.html">170 nips-2007-Robust Regression with Twinned Gaussian Processes</a></p>
<p>Author: Andrew Naish-guzman, Sean Holden</p><p>Abstract: We propose a Gaussian process (GP) framework for robust inference in which a GP prior on the mixing weights of a two-component noise model augments the standard process over latent function values. This approach is a generalization of the mixture likelihood used in traditional robust GP regression, and a specialization of the GP mixture models suggested by Tresp [1] and Rasmussen and Ghahramani [2]. The value of this restriction is in its tractable expectation propagation updates, which allow for faster inference and model selection, and better convergence than the standard mixture. An additional beneﬁt over the latter method lies in our ability to incorporate knowledge of the noise domain to inﬂuence predictions, and to recover with the predictive distribution information about the outlier distribution via the gating process. The model has asymptotic complexity equal to that of conventional robust methods, but yields more conﬁdent predictions on benchmark problems than classical heavy-tailed models and exhibits improved stability for data with clustered corruptions, for which they fail altogether. We show further how our approach can be used without adjustment for more smoothly heteroscedastic data, and suggest how it could be extended to more general noise models. We also address similarities with the work of Goldberg et al. [3].</p><p>5 0.67225033 <a title="60-lda-5" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>Author: Jonathan W. Pillow, Peter E. Latham</p><p>Abstract: Point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, owing in part to the fact that they do not take into account multiple stages of processing. Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more ﬂexible for ﬁtting neural data. More importantly, it allows us to estimate connectivity patterns among neurons (both observed and unobserved), and may provide insight into how networks process sensory input. We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model’s performance using a simulated example network consisting of two coupled neurons.</p><p>6 0.65593541 <a title="60-lda-6" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>7 0.62723196 <a title="60-lda-7" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>8 0.59484118 <a title="60-lda-8" href="./nips-2007-The_Generalized_FITC_Approximation.html">195 nips-2007-The Generalized FITC Approximation</a></p>
<p>9 0.56773466 <a title="60-lda-9" href="./nips-2007-Learning_to_classify_complex_patterns_using_a_VLSI_network_of_spiking_neurons.html">117 nips-2007-Learning to classify complex patterns using a VLSI network of spiking neurons</a></p>
<p>10 0.56142408 <a title="60-lda-10" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>11 0.55902767 <a title="60-lda-11" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>12 0.49220407 <a title="60-lda-12" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<p>13 0.48702753 <a title="60-lda-13" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>14 0.46231559 <a title="60-lda-14" href="./nips-2007-Fast_Variational_Inference_for_Large-scale_Internet_Diagnosis.html">87 nips-2007-Fast Variational Inference for Large-scale Internet Diagnosis</a></p>
<p>15 0.46081284 <a title="60-lda-15" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>16 0.45249441 <a title="60-lda-16" href="./nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">79 nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<p>17 0.43882993 <a title="60-lda-17" href="./nips-2007-Variational_Inference_for_Diffusion_Processes.html">213 nips-2007-Variational Inference for Diffusion Processes</a></p>
<p>18 0.42648599 <a title="60-lda-18" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>19 0.41884634 <a title="60-lda-19" href="./nips-2007-Adaptive_Online_Gradient_Descent.html">21 nips-2007-Adaptive Online Gradient Descent</a></p>
<p>20 0.41849038 <a title="60-lda-20" href="./nips-2007-Selecting_Observations_against_Adversarial_Objectives.html">174 nips-2007-Selecting Observations against Adversarial Objectives</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
