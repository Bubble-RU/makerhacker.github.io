<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2 nips-2007-A Bayesian LDA-based model for semi-supervised part-of-speech tagging</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-2" href="#">nips2007-2</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2 nips-2007-A Bayesian LDA-based model for semi-supervised part-of-speech tagging</h1>
<br/><p>Source: <a title="nips-2007-2-pdf" href="http://papers.nips.cc/paper/3317-a-bayesian-lda-based-model-for-semi-supervised-part-of-speech-tagging.pdf">pdf</a></p><p>Author: Kristina Toutanova, Mark Johnson</p><p>Abstract: We present a novel Bayesian model for semi-supervised part-of-speech tagging. Our model extends the Latent Dirichlet Allocation model and incorporates the intuition that words’ distributions over tags, p(t|w), are sparse. In addition we introduce a model for determining the set of possible tags of a word which captures important dependencies in the ambiguity classes of words. Our model outperforms the best previously proposed model for this task on a standard dataset. 1</p><p>Reference: <a title="nips-2007-2-reference" href="../nips2007_reference/nips-2007-A_Bayesian_LDA-based_model_for_semi-supervised_part-of-speech_tagging_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In addition we introduce a model for determining the set of possible tags of a word which captures important dependencies in the ambiguity classes of words. [sent-5, score-1.398]
</p><p>2 The second innovation of our approach is that we explicitly model ambiguity class (the set of part-ofspeech tags a word type can appear with). [sent-20, score-1.428]
</p><p>3 The set of part-of-speech tags for English we experiment with has the 17 tags deﬁned by Smith & Eisner [7], and is a coarse-grained version of the 45-tag set in the English Penn Treebank. [sent-28, score-0.588]
</p><p>4 We are also given a dictionary which speciﬁes the ambiguity classes s ⊆ T for a subset of the word types w. [sent-29, score-1.393]
</p><p>5 The ambiguity class of a word type is the set of all of its 1  ξ  α  s  β  u  θ  γ  m1 . [sent-30, score-1.088]
</p><p>6 ψ 4 T  T ω M4  si ui mj,i wi βi θi ti,j ϕk,ℓ ck,i,j  | | | | | | | | |  ξ si ui , ψj mi , ω α, si βi θi γ ti,j , ϕk  ∼ ∼ ∼ ∼ = ∼ ∼ ∼ ∼  w  c1 . [sent-39, score-0.827]
</p><p>7 In this model, each word type w is associated with a set s of possible parts-of-speech (ambiguity class), and each of its tokens is associated with a part-of-speech tag t, which generates the context words c surrounding that token. [sent-85, score-0.998]
</p><p>8 The ambiguity class s also generates the morphological features m of the word type w via a hidden tag u ∈ s. [sent-86, score-1.521]
</p><p>9 The dotted line divides the model into the ambiguity class model (on the left) and the word context model (on the right). [sent-87, score-1.242]
</p><p>10 For example, the dictionary might specify that walks has the ambiguity class {N, V } which means that walks can never have a tag which is not an N or a V. [sent-89, score-1.211]
</p><p>11 The task is to label each word token with its correct part-of-speech tag in the corresponding context. [sent-91, score-0.739]
</p><p>12 In the ﬁgure, T is the set of part-of-speech tags, L is the set of word types (i. [sent-96, score-0.497]
</p><p>13 , occurrences) of the word type w, and M 4 is the set of four-element morphological feature vectors described below. [sent-100, score-0.602]
</p><p>14 This is a generative model for a sequence of word tokens in a text corpus along with part-of-speech tags for all tokens, ambiguity classes for word types and other hidden variables. [sent-101, score-2.102]
</p><p>15 To generate the text corpus, the model generates the instances of every word type together with their contexts in 1 For some words, the dictionary speciﬁes only one possible tag, e. [sent-102, score-0.922]
</p><p>16 The generation of a word type and all of its occurrences can be decomposed into two steps, corresponding to the left and right parts of the model: the ambiguity class model, and the word context model (separated by a dotted line in the ﬁgure). [sent-107, score-1.655]
</p><p>17 For every word type wi ∈ L (plate L in the ﬁgure), in the ﬁrst step the model generates an ambiguity class si ⊆ T of possible parts of speech. [sent-108, score-1.518]
</p><p>18 The ambiguity class si is the set of parts-of-speech that tokens of wi can be labeled with. [sent-109, score-1.069]
</p><p>19 Our dictionary speciﬁes si for some but not all word types wi . [sent-110, score-1.11]
</p><p>20 The ambiguity class si is generated by a multinomial over 2T with parameters ξ, with support on the different values for s observed in the dictionary. [sent-111, score-0.877]
</p><p>21 The ambiguity class si for wi generates four different morphological features m1,i , . [sent-112, score-1.082]
</p><p>22 For completeness we generate the full surface form of the word type wi from a multinomial distribution selected by its morphology features m1,i , . [sent-121, score-0.691]
</p><p>23 We discuss the ambiguity class model in detail in Section 3. [sent-126, score-0.647]
</p><p>24 In the second step the word context model generates all instances wi,j of wi together with their part-of-speech tags ti,j and context words (plate W in the ﬁgure). [sent-128, score-1.168]
</p><p>25 This is done by ﬁrst choosing a multinomial distribution θi over the tags in the set si , which is drawn from a Dirichlet with parameters βi and support si , where βi,t = αt for t ∈ s. [sent-129, score-0.772]
</p><p>26 Each context word ck,i,j is generated by a multinomial with parameters ϕk,ti,j , where each ϕk,t is in turn generated by a Dirichlet with parameters γ. [sent-138, score-0.608]
</p><p>27 A classiﬁer that always chooses the most frequent tag for every word type, without looking at context, is 90. [sent-142, score-0.718]
</p><p>28 In this simpliﬁed model, we could say that for every word type wi we have a document consisting of all word tokens that occur in position −1 of the word type wi in the corpus. [sent-146, score-1.783]
</p><p>29 Each context word ci,j in wi ’s document is generated by ﬁrst choosing a tag (topic) from a word (document) speciﬁc distribution θi and then generating the word ci,j from a tag (topic) speciﬁc multinomial. [sent-147, score-2.006]
</p><p>30 The additional power of our model stems from the model of ambiguity classes si which can take advantage of the information provided by the dictionary, and from the incorporation of multiple context features. [sent-149, score-0.979]
</p><p>31 Finally, we note that our model is deﬁcient, because the same word token in the corpus is independently generated multiple times (e. [sent-150, score-0.582]
</p><p>32 3  3  Parameter estimation and tag prediction  Here we discuss our method of estimating the parameters of our model and making predictions, given an (incomplete) tagging dictionary and a set of natural language sentences. [sent-159, score-0.825]
</p><p>33 We train the parameters of the ambiguity class model, ξ, ψ, and ω, separately from the parameters of the word context model: α,θ,γ, and ϕ. [sent-160, score-1.185]
</p><p>34 This is because the two parts of the model are connected only via the variables si (the ambiguity classes of words), and when these ambiguity classes are given the two sets of parameters are completely decoupled. [sent-161, score-1.553]
</p><p>35 The dictionary gives us labeled training examples for the ambiguity class model, and we train the parameters of the ambiguity class model only from this data (i. [sent-162, score-1.604]
</p><p>36 After training the ambiguity class model from the dictionary we ﬁx its parameters and estimate the word context model given these parameters. [sent-165, score-1.503]
</p><p>37 1  Ambiguity class model: details and parameter estimation  Our ambiguity class model captures the strong regularities governing the possible tags of a word type. [sent-167, score-1.44]
</p><p>38 Empirically we observe that the number of occurring ambiguity classes is very small relative to the number of possible ambiguity classes. [sent-168, score-1.16]
</p><p>39 For example, in the WSJ Penn Treebank data, the 49, 206 word types belong to 118 ambiguity classes. [sent-169, score-1.037]
</p><p>40 Modeling these (rather than POS tags directly) constrains the model to avoid assignments of tags to word tokens which would result in improbable ambiguity classes for word types. [sent-170, score-2.231]
</p><p>41 The ambiguity class model contributes to biasing p(t|w) toward sparse distributions as well, because most ambiguity classes have very few elements. [sent-174, score-1.298]
</p><p>42 For example, the top ten most frequent ambiguity classes in the complete dictionary consist of one or two elements. [sent-175, score-0.955]
</p><p>43 The ambiguity class of a word type can be predicted from its surface morphological features. [sent-176, score-1.203]
</p><p>44 For example the sufﬁx -s of walks indicates that an ambiguity class of {N, V } is likely for this word. [sent-177, score-0.645]
</p><p>45 We deﬁne the sufﬁx of a word to be the longest character sufﬁx (up to three letters) which occurs as a sufﬁx of sufﬁciently many word types. [sent-179, score-0.876]
</p><p>46 2 We train the ambiguity class model on the set of word types present in the dictionary. [sent-180, score-1.163]
</p><p>47 We set the multinomial parameters ψk,l and ξ to maximize the joint likelihood of these word types and their morphological features. [sent-181, score-0.725]
</p><p>48 Maximum likelihood estimation for ψ is complicated by the hidden variable ui which selects a tag form the ambiguity class with uniform distribution. [sent-182, score-0.923]
</p><p>49 2  Parameter estimation for the word context model and prediction given complete dictionary  We restrict our attention at ﬁrst to the setting where a complete tagging dictionary is given. [sent-187, score-1.37]
</p><p>50 When every word is in the dictionary, the ambiguity class si for each word type wi is speciﬁed by the tagging dictionary, and the ambiguity class model becomes irrelevant. [sent-190, score-2.712]
</p><p>51 The contexts of word instances ck,i,j and the ambiguity classes si are observed. [sent-192, score-1.324]
</p><p>52 We set γ = 1 and we use Empirical Bayes to estimate α by maximizing the likelihood of the observed data given α and the ambiguity classes si . [sent-194, score-0.838]
</p><p>53 Note that if the ambiguity classes si and α are given, βi is ﬁxed. [sent-195, score-0.822]
</p><p>54 Below we use c to denote the vector of all contexts of all word instances, and s the vector of ambiguity classes for all word types. [sent-196, score-1.534]
</p><p>55 We use ϕ to denote the vector of all multinomials ϕk,l , θ to 2  A sufﬁx occurs with sufﬁciently many word types if its type-frequency rank is below 100. [sent-197, score-0.526]
</p><p>56 4  denote the vector of all θi and t to denote the vector of all tag sequences ti for word types wi . [sent-198, score-0.917]
</p><p>57 Each ϕk,l is distributed according to a Dirichlet distribution with variational parameters λk,l , each θi is also Dirichlet with parameters ηi and each tag ti,j is distributed according to a multinomial υi,j . [sent-205, score-0.467]
</p><p>58 Given ﬁxed variational parameters λk,l we maximize with respect to the variational parameters ηi and υi,j corresponding to word types and their instances. [sent-208, score-0.814]
</p><p>59 For predicting the tags ti,j of word tokens we use the same approximate posterior distribution Q. [sent-214, score-0.833]
</p><p>60 3  Parameter estimation for the word context model and prediction with incomplete dictionary  So far we have described the training of the parameters of the word context model in the setting where for all words, the ambiguity classes si are known and these variables are observed. [sent-217, score-2.294]
</p><p>61 When the ambiguity classes si are unknown for some words in the dataset, they become additional hidden variables, and the hidden variables in the word context model become dependent on the morphological features mi and the parameters of the ambiguity class model. [sent-218, score-2.369]
</p><p>62 Denote the vector of ambiguity classes for the known (in the dictionary) word types by sd and the ambiguity classes for the unknown word types by su . [sent-219, score-2.277]
</p><p>63 The posterior distribution over the hidden variables of interest given the observations becomes: P (ϕ, θ, t, su |sd , mu , c, α, γ), where mu are the morphological features of the unknown word types. [sent-220, score-0.675]
</p><p>64 Before we had, for every word type, a variational distribution over the hidden variables corresponding to that word type: Q(θi , ti |ηi , υi,j ) = D IR(θi |ηi )  Wi j=1  P (ti,j |υi,j )  We now introduce a variational distribution including new hidden variables si for unknown words. [sent-222, score-1.494]
</p><p>65 Q(θi , ti , si |mi , ηi,s , υi,j,s ) = P (si |mi )D IR(θi |ηi,si ) 5  Wi j=1  P (ti,j |υi,j,si )  That is, for each possible ambiguity class si of an unknown word wi we introduce variational parameters speciﬁc to that ambiguity class. [sent-223, score-2.331]
</p><p>66 Instead of single variational parameters θi and υi,j for a word with known si , we now have variational parameters {θi,s } and {υi,j,s } for all possible values s of si . [sent-224, score-1.136]
</p><p>67 For simplicity, we use the probability P (si |mi ) = P (si |mi , ξ, ψ) from the morphology-based ambiguity class model in the approximating distribution rather than introducing new variational parameters and learning this distribution. [sent-225, score-0.794]
</p><p>68 2, for word types whose ambiguity classes si are known. [sent-229, score-1.319]
</p><p>69 For words with unknown ambiguity classes, we need to maximize over ambiguity classes as well as tag assignments. [sent-230, score-1.526]
</p><p>70 For each possible tag set si , we ﬁnd the most likely assignment of tags given that ambiguity class t∗ (si ), using the variational distribution as in the case of known ambiguity classes. [sent-232, score-2.017]
</p><p>71 We then choose an ambiguity class and an assignment of tags according to: s∗ = arg maxsi P (si |mi , ψ, ξ)P (t∗ (si ), ci |si , D, α, γ) and t = t∗ (s∗ ). [sent-233, score-0.913]
</p><p>72 We compute P (t∗ (si ), ci |si , D, α, γ) by integrating with respect to the word context distributions ϕ whose approximate posterior given the data is Dirichlet with parameters λk,l , and by integrating with respect to θi which are Dirichlet with parameters α and dimensionality given by si . [sent-234, score-0.767]
</p><p>73 In the ﬁrst setting, a complete tagging dictionary is available, and in the other two settings the coverage of the dictionary is greatly reduced. [sent-237, score-0.812]
</p><p>74 The tagging dictionary was constructed by collecting for each word type, the set of parts-of-speech with which it occurs in the annotated WSJ Penn Treebank, including the test set. [sent-238, score-0.916]
</p><p>75 This method of constructing a tag dictionary is arguably unrealistic but has been used in previous research [7, 9, 6] and provides a reproducible framework for comparing different models. [sent-239, score-0.522]
</p><p>76 In the complete dictionary setting, we use the ambiguity class information for all words, and in the second and third setting we remove from the dictionary all word types that have occurred with frequency less than 2 and less than 3, respectively, in the test set of 1,005 sentences. [sent-240, score-1.71]
</p><p>77 The complete tagging dictionary contains entries for 49, 206 words. [sent-241, score-0.503]
</p><p>78 To see how much removing information from the dictionary impacts the hardness of the problem we can look at the accuracy of a classiﬁer choosing a tag at random from the possible tags of words, shown in the column Random of Table 1. [sent-245, score-0.816]
</p><p>79 If labeled corpus data were available, a model which assigns the most frequent tag to each word by using p(t|w) would do much better. [sent-249, score-0.837]
</p><p>80 ˆ The models in the table are: LDA is the model proposed in this paper, excluding the ambiguity class model. [sent-250, score-0.647]
</p><p>81 The ambiguity class model is irrelevant when a compete dictionary is available because all si are observed. [sent-251, score-1.125]
</p><p>82 In the other two settings for the LDA model we assume that si is the complete ambiguity class (all 17 tags) 3 We also limit the number of possible ambiguity classes per word to the three most likely ones and renormalize the probability mass among them. [sent-252, score-1.95]
</p><p>83 4 Frequency of tags is unigram frequency of tags p(t) by token in the unlabeled data. [sent-253, score-0.705]
</p><p>84 Since the tokens in the ˆ corpus are not actually labeled we compute the frequency by giving fractional counts to each possible tag of words in the dictionary. [sent-254, score-0.524]
</p><p>85 for words which are not in the dictionary and do not attempt to predict a more speciﬁc ambiguity class. [sent-287, score-0.886]
</p><p>86 For this model we estimate the variational parameters λk,l and the Dirichlet parameter α to maximize the variational bound on the log-likelihood of the word types which are in the dictionary only. [sent-291, score-1.105]
</p><p>87 We found that including unknown word types was detrimental to performance. [sent-292, score-0.524]
</p><p>88 LDA+AC is our full model including the model of ambiguity classes of words given their morphological features. [sent-293, score-0.897]
</p><p>89 We trained this model on all word types as discussed in Section 3. [sent-295, score-0.543]
</p><p>90 PLSA is the model analogous to LDA, which has the same structure as our word context model, but excludes the Bayesian components. [sent-301, score-0.549]
</p><p>91 The PLSA model does not have a prior on the word-speciﬁc distributions over tags θi = p(t|wi ) and it does not have a prior distribution on the topic-speciﬁc multinomials for context words ϕk,l . [sent-304, score-0.504]
</p><p>92 PLSA does not include the ambiguity class model for si and as in the LDA model, word types not in the dictionary were assumed to have ambiguity classes containing all 17 tags. [sent-307, score-2.242]
</p><p>93 PLSA+AC extends the PLSA model by the inclusion of the ambiguity class model. [sent-308, score-0.647]
</p><p>94 Bayesian HMM (G&G;) is a fully Bayesian HMM model for semi-supervised part-of-speech tagging proposed in [9], which incorporates sparse Dirichlet priors on p(w|t) of word tokens given part of speech tags and p(ti |ti−1 , ti−2 ) of transition probabilities in the HMM. [sent-315, score-1.17]
</p><p>95 This is consistent with the success of other models that used word context for part-of-speech prediction in different ways [4, 8]. [sent-326, score-0.503]
</p><p>96 Third, our ambiguity class model results in a signiﬁcant improvement as well; LDA+AC reduces the error of LDA by up to 31%. [sent-329, score-0.647]
</p><p>97 In their setting a small set of example word types (which they call prototypes) are provided for each possible tag (only three prototypes per tag were speciﬁed). [sent-335, score-1.032]
</p><p>98 We can not directly compare the performance of our model to theirs, because our model would need prototypes for every ambiguity class rather than for every tag. [sent-338, score-0.719]
</p><p>99 In future work we will explore whether a very small set of prototypical ambiguity classes and corresponding word types can achieve the performance we obtained with an incomplete tagging dictionary. [sent-339, score-1.353]
</p><p>100 Another interesting direction for future work is applying our model to other NLP disambiguation tasks, such as named entity recognition and induction of deeper syntactic or semantic structure, which could beneﬁt from both our ambiguity class model and our word context model. [sent-340, score-1.213]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ambiguity', 0.54), ('word', 0.438), ('tags', 0.294), ('dictionary', 0.276), ('tag', 0.246), ('tagging', 0.202), ('si', 0.202), ('wi', 0.135), ('variational', 0.116), ('morphological', 0.115), ('lda', 0.114), ('plsa', 0.105), ('tokens', 0.101), ('classes', 0.08), ('dirichlet', 0.08), ('hmm', 0.072), ('words', 0.07), ('context', 0.065), ('class', 0.061), ('types', 0.059), ('penn', 0.057), ('supervision', 0.057), ('ulti', 0.057), ('token', 0.055), ('ir', 0.055), ('mi', 0.052), ('ac', 0.05), ('type', 0.049), ('wsj', 0.046), ('model', 0.046), ('unlabeled', 0.044), ('walks', 0.044), ('hidden', 0.043), ('corpus', 0.043), ('multinomial', 0.043), ('ti', 0.039), ('contexts', 0.038), ('orthographic', 0.034), ('frequent', 0.034), ('acl', 0.034), ('incomplete', 0.034), ('bayesian', 0.032), ('emnlp', 0.031), ('spelling', 0.031), ('treebank', 0.031), ('parameters', 0.031), ('sparse', 0.031), ('labeled', 0.03), ('multinomials', 0.029), ('generates', 0.029), ('unknown', 0.027), ('kristina', 0.026), ('morphology', 0.026), ('toutanova', 0.026), ('prototypes', 0.026), ('instances', 0.026), ('oracle', 0.025), ('complete', 0.025), ('ce', 0.024), ('language', 0.024), ('freq', 0.023), ('maximize', 0.023), ('dan', 0.023), ('outperforms', 0.022), ('unsupervised', 0.022), ('incorporates', 0.022), ('christopher', 0.021), ('english', 0.021), ('smoothing', 0.021), ('priors', 0.021), ('amount', 0.02), ('text', 0.02), ('indicating', 0.02), ('plate', 0.02), ('train', 0.019), ('mu', 0.018), ('frequency', 0.018), ('parts', 0.018), ('assignment', 0.018), ('mark', 0.018), ('settings', 0.018), ('contrastive', 0.018), ('cutoff', 0.018), ('klein', 0.018), ('induction', 0.017), ('ui', 0.017), ('ml', 0.017), ('setting', 0.017), ('substantial', 0.016), ('supervised', 0.016), ('sd', 0.016), ('variables', 0.016), ('likelihood', 0.016), ('fractional', 0.016), ('johnson', 0.016), ('preceding', 0.016), ('smith', 0.016), ('speech', 0.015), ('coverage', 0.015), ('languages', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="2-tfidf-1" href="./nips-2007-A_Bayesian_LDA-based_model_for_semi-supervised_part-of-speech_tagging.html">2 nips-2007-A Bayesian LDA-based model for semi-supervised part-of-speech tagging</a></p>
<p>Author: Kristina Toutanova, Mark Johnson</p><p>Abstract: We present a novel Bayesian model for semi-supervised part-of-speech tagging. Our model extends the Latent Dirichlet Allocation model and incorporates the intuition that words’ distributions over tags, p(t|w), are sparse. In addition we introduce a model for determining the set of possible tags of a word which captures important dependencies in the ambiguity classes of words. Our model outperforms the best previously proposed model for this task on a standard dataset. 1</p><p>2 0.26893532 <a title="2-tfidf-2" href="./nips-2007-Automatic_Generation_of_Social_Tags_for_Music_Recommendation.html">29 nips-2007-Automatic Generation of Social Tags for Music Recommendation</a></p>
<p>Author: Douglas Eck, Paul Lamere, Thierry Bertin-mahieux, Stephen Green</p><p>Abstract: Social tags are user-generated keywords associated with some resource on the Web. In the case of music, social tags have become an important component of “Web2.0” recommender systems, allowing users to generate playlists based on use-dependent terms such as chill or jogging that have been applied to particular songs. In this paper, we propose a method for predicting these social tags directly from MP3 ﬁles. Using a set of boosted classiﬁers, we map audio features onto social tags collected from the Web. The resulting automatic tags (or autotags) furnish information about music that is otherwise untagged or poorly tagged, allowing for insertion of previously unheard music into a social recommender. This avoids the ”cold-start problem” common in such systems. Autotags can also be used to smooth the tag space from which similarities and recommendations are made by providing a set of comparable baseline tags for all tracks in a recommender system. 1</p><p>3 0.16038993 <a title="2-tfidf-3" href="./nips-2007-Spatial_Latent_Dirichlet_Allocation.html">183 nips-2007-Spatial Latent Dirichlet Allocation</a></p>
<p>Author: Xiaogang Wang, Eric Grimson</p><p>Abstract: In recent years, the language model Latent Dirichlet Allocation (LDA), which clusters co-occurring words into topics, has been widely applied in the computer vision ﬁeld. However, many of these applications have difﬁculty with modeling the spatial and temporal structure among visual words, since LDA assumes that a document is a “bag-of-words”. It is also critical to properly design “words” and “documents” when using a language model to solve vision problems. In this paper, we propose a topic model Spatial Latent Dirichlet Allocation (SLDA), which better encodes spatial structures among visual words that are essential for solving many vision problems. The spatial information is not encoded in the values of visual words but in the design of documents. Instead of knowing the partition of words into documents a priori, the word-document assignment becomes a random hidden variable in SLDA. There is a generative procedure, where knowledge of spatial structure can be ﬂexibly added as a prior, grouping visual words which are close in space into the same document. We use SLDA to discover objects from a collection of images, and show it achieves better performance than LDA. 1</p><p>4 0.14542753 <a title="2-tfidf-4" href="./nips-2007-A_Probabilistic_Approach_to_Language_Change.html">9 nips-2007-A Probabilistic Approach to Language Change</a></p>
<p>Author: Alexandre Bouchard-côté, Percy Liang, Dan Klein, Thomas L. Griffiths</p><p>Abstract: We present a probabilistic approach to language change in which word forms are represented by phoneme sequences that undergo stochastic edits along the branches of a phylogenetic tree. This framework combines the advantages of the classical comparative method with the robustness of corpus-based probabilistic models. We use this framework to explore the consequences of two different schemes for deﬁning probabilistic models of phonological change, evaluating these schemes by reconstructing ancient word forms of Romance languages. The result is an efﬁcient inference procedure for automatically inferring ancient word forms from modern languages, which can be generalized to support inferences about linguistic phylogenies. 1</p><p>5 0.14470981 <a title="2-tfidf-5" href="./nips-2007-HM-BiTAM%3A_Bilingual_Topic_Exploration%2C_Word_Alignment%2C_and_Translation.html">95 nips-2007-HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation</a></p>
<p>Author: Bing Zhao, Eric P. Xing</p><p>Abstract: We present a novel paradigm for statistical machine translation (SMT), based on a joint modeling of word alignment and the topical aspects underlying bilingual document-pairs, via a hidden Markov Bilingual Topic AdMixture (HM-BiTAM). In this paradigm, parallel sentence-pairs from a parallel document-pair are coupled via a certain semantic-ﬂow, to ensure coherence of topical context in the alignment of mapping words between languages, likelihood-based training of topic-dependent translational lexicons, as well as in the inference of topic representations in each language. The learned HM-BiTAM can not only display topic patterns like methods such as LDA [1], but now for bilingual corpora; it also offers a principled way of inferring optimal translation using document context. Our method integrates the conventional model of HMM — a key component for most of the state-of-the-art SMT systems, with the recently proposed BiTAM model [10]; we report an extensive empirical analysis (in many ways complementary to the description-oriented [10]) of our method in three aspects: bilingual topic representation, word alignment, and translation.</p><p>6 0.14027148 <a title="2-tfidf-6" href="./nips-2007-Expectation_Maximization_and_Posterior_Constraints.html">84 nips-2007-Expectation Maximization and Posterior Constraints</a></p>
<p>7 0.13319805 <a title="2-tfidf-7" href="./nips-2007-A_Bayesian_Framework_for_Cross-Situational_Word-Learning.html">1 nips-2007-A Bayesian Framework for Cross-Situational Word-Learning</a></p>
<p>8 0.12116959 <a title="2-tfidf-8" href="./nips-2007-Unconstrained_On-line_Handwriting_Recognition_with_Recurrent_Neural_Networks.html">210 nips-2007-Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks</a></p>
<p>9 0.12007435 <a title="2-tfidf-9" href="./nips-2007-Supervised_Topic_Models.html">189 nips-2007-Supervised Topic Models</a></p>
<p>10 0.10949049 <a title="2-tfidf-10" href="./nips-2007-Distributed_Inference_for_Latent_Dirichlet_Allocation.html">73 nips-2007-Distributed Inference for Latent Dirichlet Allocation</a></p>
<p>11 0.097712092 <a title="2-tfidf-11" href="./nips-2007-Collapsed_Variational_Inference_for_HDP.html">47 nips-2007-Collapsed Variational Inference for HDP</a></p>
<p>12 0.097279325 <a title="2-tfidf-12" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>13 0.09510348 <a title="2-tfidf-13" href="./nips-2007-Agreement-Based_Learning.html">22 nips-2007-Agreement-Based Learning</a></p>
<p>14 0.092309959 <a title="2-tfidf-14" href="./nips-2007-The_Infinite_Markov_Model.html">197 nips-2007-The Infinite Markov Model</a></p>
<p>15 0.073402472 <a title="2-tfidf-15" href="./nips-2007-Infinite_State_Bayes-Nets_for_Structured_Domains.html">105 nips-2007-Infinite State Bayes-Nets for Structured Domains</a></p>
<p>16 0.06207737 <a title="2-tfidf-16" href="./nips-2007-Discriminative_Keyword_Selection_Using_Support_Vector_Machines.html">71 nips-2007-Discriminative Keyword Selection Using Support Vector Machines</a></p>
<p>17 0.056372128 <a title="2-tfidf-17" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>18 0.054672055 <a title="2-tfidf-18" href="./nips-2007-Sparse_Overcomplete_Latent_Variable_Decomposition_of_Counts_Data.html">181 nips-2007-Sparse Overcomplete Latent Variable Decomposition of Counts Data</a></p>
<p>19 0.048935521 <a title="2-tfidf-19" href="./nips-2007-Multiple-Instance_Active_Learning.html">136 nips-2007-Multiple-Instance Active Learning</a></p>
<p>20 0.047353715 <a title="2-tfidf-20" href="./nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">79 nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.161), (1, 0.08), (2, -0.063), (3, -0.304), (4, 0.1), (5, -0.052), (6, 0.086), (7, -0.146), (8, 0.018), (9, 0.023), (10, 0.053), (11, 0.012), (12, -0.099), (13, -0.122), (14, -0.01), (15, -0.157), (16, -0.114), (17, 0.074), (18, 0.045), (19, 0.023), (20, -0.023), (21, -0.032), (22, 0.069), (23, -0.017), (24, -0.098), (25, -0.069), (26, 0.067), (27, -0.018), (28, -0.276), (29, 0.009), (30, 0.049), (31, 0.165), (32, -0.093), (33, -0.023), (34, 0.051), (35, -0.138), (36, 0.195), (37, -0.073), (38, -0.056), (39, -0.21), (40, -0.086), (41, 0.056), (42, -0.004), (43, -0.029), (44, 0.151), (45, -0.034), (46, 0.09), (47, -0.07), (48, -0.007), (49, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96679056 <a title="2-lsi-1" href="./nips-2007-A_Bayesian_LDA-based_model_for_semi-supervised_part-of-speech_tagging.html">2 nips-2007-A Bayesian LDA-based model for semi-supervised part-of-speech tagging</a></p>
<p>Author: Kristina Toutanova, Mark Johnson</p><p>Abstract: We present a novel Bayesian model for semi-supervised part-of-speech tagging. Our model extends the Latent Dirichlet Allocation model and incorporates the intuition that words’ distributions over tags, p(t|w), are sparse. In addition we introduce a model for determining the set of possible tags of a word which captures important dependencies in the ambiguity classes of words. Our model outperforms the best previously proposed model for this task on a standard dataset. 1</p><p>2 0.83161426 <a title="2-lsi-2" href="./nips-2007-Automatic_Generation_of_Social_Tags_for_Music_Recommendation.html">29 nips-2007-Automatic Generation of Social Tags for Music Recommendation</a></p>
<p>Author: Douglas Eck, Paul Lamere, Thierry Bertin-mahieux, Stephen Green</p><p>Abstract: Social tags are user-generated keywords associated with some resource on the Web. In the case of music, social tags have become an important component of “Web2.0” recommender systems, allowing users to generate playlists based on use-dependent terms such as chill or jogging that have been applied to particular songs. In this paper, we propose a method for predicting these social tags directly from MP3 ﬁles. Using a set of boosted classiﬁers, we map audio features onto social tags collected from the Web. The resulting automatic tags (or autotags) furnish information about music that is otherwise untagged or poorly tagged, allowing for insertion of previously unheard music into a social recommender. This avoids the ”cold-start problem” common in such systems. Autotags can also be used to smooth the tag space from which similarities and recommendations are made by providing a set of comparable baseline tags for all tracks in a recommender system. 1</p><p>3 0.77277756 <a title="2-lsi-3" href="./nips-2007-A_Bayesian_Framework_for_Cross-Situational_Word-Learning.html">1 nips-2007-A Bayesian Framework for Cross-Situational Word-Learning</a></p>
<p>Author: Noah Goodman, Joshua B. Tenenbaum, Michael J. Black</p><p>Abstract: For infants, early word learning is a chicken-and-egg problem. One way to learn a word is to observe that it co-occurs with a particular referent across different situations. Another way is to use the social context of an utterance to infer the intended referent of a word. Here we present a Bayesian model of cross-situational word learning, and an extension of this model that also learns which social cues are relevant to determining reference. We test our model on a small corpus of mother-infant interaction and ﬁnd it performs better than competing models. Finally, we show that our model accounts for experimental phenomena including mutual exclusivity, fast-mapping, and generalization from social cues. To understand the difﬁculty of an infant word-learner, imagine walking down the street with a friend who suddenly says “dax blicket philbin na ﬁvy!” while at the same time wagging her elbow. If you knew any of these words you might infer from the syntax of her sentence that blicket is a novel noun, and hence the name of a novel object. At the same time, if you knew that this friend indicated her attention by wagging her elbow at objects, you might infer that she intends to refer to an object in a nearby show window. On the other hand if you already knew that “blicket” meant the object in the window, you might be able to infer these elements of syntax and social cues. Thus, the problem of early word-learning is a classic chicken-and-egg puzzle: in order to learn word meanings, learners must use their knowledge of the rest of language (including rules of syntax, parts of speech, and other word meanings) as well as their knowledge of social situations. But in order to learn about the facts of their language they must ﬁrst learn some words, and in order to determine which cues matter for establishing reference (for instance, pointing and looking at an object but normally not waggling your elbow) they must ﬁrst have a way to know the intended referent in some situations. For theories of language acquisition, there are two common ways out of this dilemma. The ﬁrst involves positing a wide range of innate structures which determine the syntax and categories of a language and which social cues are informative. (Though even when all of these elements are innately determined using them to learn a language from evidence may not be trivial [1].) The other alternative involves bootstrapping: learning some words, then using those words to learn how to learn more. This paper gives a proposal for the second alternative. We ﬁrst present a Bayesian model of how learners could use a statistical strategy—cross-situational word-learning—to learn how words map to objects, independent of syntactic and social cues. We then extend this model to a true bootstrapping situation: using social cues to learn words while using words to learn social cues. Finally, we examine several important phenomena in word learning: mutual exclusivity (the tendency to assign novel words to novel referents), fast-mapping (the ability to assign a novel word in a linguistic context to a novel referent after only a single use), and social generalization (the ability to use social context to learn the referent of a novel word). Without adding additional specialized machinery, we show how these can be explained within our model as the result of domain-general probabilistic inference mechanisms operating over the linguistic domain. 1 Os r, b Is Ws Figure 1: Graphical model describing the generation of words (Ws ) from an intention (Is ) and lexicon ( ), and intention from the objects present in a situation (Os ). The plate indicates multiple copies of the model for different situation/utterance pairs (s). Dotted portions indicate additions to include the generation of social cues Ss from intentions. Ss ∀s 1 The Model Behind each linguistic utterance is a meaning that the speaker intends to communicate. Our model operates by attempting to infer this intended meaning (which we call the intent) on the basis of the utterance itself and observations of the physical and social context. For the purpose of modeling early word learning—which consists primarily of learning words for simple object categories—in our model, we assume that intents are simply groups of objects. To state the model formally, we assume the non-linguistic situation consists of a set Os of objects and that utterances are unordered sets of words Ws 1 . The lexicon is a (many-to-many) map from words to objects, which captures the meaning of those words. (Syntax enters our model only obliquely by different treatment of words depending on whether they are in the lexicon or not—that is, whether they are common nouns or other types of words.) In this setting the speaker’s intention will be captured by a set of objects in the situation to which she intends to refer: Is ⊆ Os . This setup is indicated in the graphical model of Fig. 1. Different situation-utterance pairs Ws , Os are independent given the lexicon , giving: P (Ws |Is , ) · P (Is |Os ). P (W| , O) = s (1) Is We further simplify by assuming that P (Is |Os ) ∝ 1 (which could be reﬁned by adding a more detailed model of the communicative intentions a person is likely to form in different situations). We will assume that words in the utterance are generated independently given the intention and the lexicon and that the length of the utterance is observed. Each word is then generated from the intention set and lexicon by ﬁrst choosing whether the word is a referential word or a non-referential word (from a binomial distribution of weight γ), then, for referential words, choosing which object in the intent it refers to (uniformly). This process gives: P (Ws |Is , ) = (1 − γ)PNR (w| ) + γ w∈Ws x∈Is 1 PR (w|x, ) . |Is | The probability of word w referring to object x is PR (w|x, ) ∝ δx∈ w occurring as a non-referring word is PNR (w| ) ∝ 1 if (w) = ∅, κ otherwise. (w) , (2) and the probability of word (3) (this probability is a distribution over all words in the vocabulary, not just those in lexicon ). The constant κ is a penalty for using a word in the lexicon as a non-referring word—this penalty indirectly enforces a light-weight difference between two different groups of words (parts-of-speech): words that refer and words that do not refer. Because the generative structure of this model exposes the role of speaker’s intentions, it is straightforward to add non-linguistic social cues. We assume that social cues such as pointing are generated 1 Note that, since we ignore word order, the distribution of words in a sentence should be exchangeable given the lexicon and situation. This implies, by de Finetti’s theorem, that they are independent conditioned on a latent state—we assume that the latent state giving rise to words is the intention of the speaker. 2 from the speaker’s intent independently of the linguistic aspects (as shown in the dotted arrows of Fig. 1). With the addition of social cues Ss , Eq. 1 becomes: P (Ws |Is , ) · P (Ss |Is ) · P (Is |Os ). P (W| , O) = s (4) Is We assume that the social cues are a set Si (x) of independent binary (cue present or not) feature values for each object x ∈ Os , which are generated through a noisy-or process: P (Si (x)=1|Is , ri , bi ) = 1 − (1 − bi )(1 − ri )δx∈Is . (5) Here ri is the relevance of cue i, while bi is its base rate. For the model without social cues the posterior probability of a lexicon given a set of situated utterances is: P ( |W, O) ∝ P (W| , O)P ( ). (6) And for the model with social cues the joint posterior over lexicon and cue parameters is: P ( , r, b|W, O) ∝ P (W| , r, b, O)P ( )P (r, b). (7) We take the prior probability of a lexicon to be exponential in its size: P ( ) ∝ e−α| | , and the prior probability of social cue parameters to be uniform. Given the model above and the corpus described below, we found the best lexicon (or lexicon and cue parameters) according to Eq. 6 and 7 by MAP inference using stochastic search2 . 2 Previous work While cross-situational word-learning has been widely discussed in the empirical literature, e.g., [2], there have been relatively few attempts to model this process computationally. Siskind [3] created an ambitious model which used deductive rules to make hypotheses about propositional word meanings their use across situations. This model achieved surprising success in learning word meanings in artiﬁcial corpora, but was extremely complex and relied on the availability of fully coded representations of the meaning of each sentence, making it difﬁcult to extend to empirical corpus data. More recently, Yu and Ballard [4] have used a machine translation model (similar to IBM Translation Model I) to learn word-object association probabilities. In their study, they used a pre-existing corpus of mother-infant interactions and coded the objects present during each utterance (an example from this corpus—illustrated with our own coding scheme—is shown in Fig. 2). They applied their translation model to estimate the probability of an object given a word, creating a table of associations between words and objects. Using this table, they extracted a lexicon (a group of word-object mappings) which was relatively accurate in its guesses about the names of objects that were being talked about. They further extended their model to incorporate prosodic emphasis on words (a useful cue which we will not discuss here) and joint attention on objects. Joint attention was coded by hand, isolating a subset of objects which were attended to by both mother and infant. Their results reﬂected a sizable increase in recall with the use of social cues. 3 Materials and Assessment Methods To test the performance of our model on natural data, we used the Rollins section of the CHILDES corpus[5]. For comparison with the model by Yu and Ballard [4], we chose the ﬁles me03 and di06, each of which consisted of approximately ten minutes of interaction between a mother and a preverbal infant playing with objects found in a box of toys. Because we were not able to obtain the exact corpus Yu and Ballard used, we recoded the objects in the videos and added a coding of social cues co-occurring with each utterance. We annotated each utterance with the set of objects visible to the infant and with a social coding scheme (for an illustrated example, see Figure 2). Our social code included seven features: infants eyes, infants hands, infants mouth, infant touching, mothers hands, mothers eyes, mother touching. For each utterance, this coding created an object by social feature matrix. 2 In order to speed convergence we used a simulated tempering scheme with three temperature chains and a range of data-driven proposals. 3 Figure 2: A still frame from our corpus showing the coding of objects and social cues. We coded all mid-sized objects visible to the infant as well as social information including what both mother and infant were touching and looking at. We evaluated all models based on their coverage of a gold-standard lexicon, computing precision (how many of the word-object mappings in a lexicon were correct relative to the gold-standard), recall (how many of the total correct mappings were found), and their geometric mean, F-score. However, the gold-standard lexicon for word-learning is not obvious. For instance, should it include the mapping between the plural “pigs” or the sound “oink” and the object PIG? Should a goldstandard lexicon include word-object pairings that are correct but were not present in the learning situation? In the results we report, we included those pairings which would be useful for a child to learn (e.g., “oink” → PIG) but not including those pairings which were not observed to co-occur in the corpus (however, modifying these decisions did not affect the qualitative pattern of results). 4 Results For the purpose of comparison, we give scores for several other models on the same corpus. We implemented a range of simple associative models based on co-occurrence frequency, conditional probability (both word given object and object given word), and point-wise mutual information. In each of these models, we computed the relevant statistic across the entire corpus and then created a lexicon by including all word-object pairings for which the association statistic met a threshold value. We additionally implemented a translation model (based on Yu and Ballard [4]). Because Yu and Ballard did not include details on how they evaluated their model, we scored it in the same way as the other associative models, by creating an association matrix based on the scores P (O|W ) (as given in equation (3) in their paper) and then creating a lexicon based on a threshold value. In order to simulate this type of threshold value for our model, we searched for the MAP lexicon over a range of parameters α in our prior (the larger the prior value, the less probable a larger lexicon, thus this manipulation served to create more or less selective lexicons) . Base model. In Figure 3, we plot the precision and the recall for lexicons across a range of prior parameter values for our model and the full range of threshold values for the translation model and two of the simple association models (since results for the conditional probability models were very similar but slightly inferior to the performance of mutual information, we did not include them). For our model, we averaged performance at each threshold value across three runs of 5000 search iterations each. Our model performed better than any of the other models on a number of dimensions (best lexicon shown in Table 1), both achieving the highest F-score and showing a better tradeoff between precision and recall at sub-optimal threshold values. The translation model also performed well, increasing precision as the threshold of association was raised. Surprisingly, standard cooccurrence statistics proved to be relatively ineffective at extracting high-scoring lexicons: at any given threshold value, these models included a very large number of incorrect pairs. Table 1: The best lexicon found by the Bayesian model (α=11, γ=0.2, κ=0.01). baby → book hand → hand bigbird → bird hat → hat on → ring bird → rattle meow → kitty ring → ring 4 birdie → duck moocow → cow sheep → sheep book → book oink → pig 1 Co!occurrence frequency Mutual information Translation model Bayesian model 0.9 0.8 0.7 recall 0.6 0.5 0.4 0.3 F=0.54 F=0.44 F=0.21 F=0.12 0.2 0.1 0 0 0.2 0.4 0.6 precision 0.8 1 Figure 3: Comparison of models on corpus data: we plot model precision vs. recall across a range of threshold values for each model (see text). Unlike standard ROC curves for classiﬁcation tasks, the precision and recall of a lexicon depends on the entire lexicon, and irregularities in the curves reﬂect the small size of the lexicons). One additional virtue of our model over other associative models is its ability to determine which objects the speaker intended to refer to. In Table 2, we give some examples of situations in which the model correctly inferred the objects that the speaker was talking about. Social model. While the addition of social cues did not increase corpus performance above that found in the base model, the lexicons which were found by the social model did have several properties that were not present in the base model. First, the model effectively and quickly converged on the social cues that we found subjectively important in viewing the corpus videos. The two cues which were consistently found relevant across the model were (1) the target of the infant’s gaze and (2) the caregiver’s hand. These data are especially interesting in light of the speculation that infants initially believe their own point of gaze is a good cue to reference, and must learn over the second year that the true cue is the caregiver’s point of gaze, not their own [6]. Second, while the social model did not outperform the base model on the full corpus (where many words were paired with their referents several times), on a smaller corpus (taking every other utterance), the social cue model did slightly outperform a model without social cues (max F-score=0.43 vs. 0.37). Third, the addition of social cues allowed the model to infer the intent of a speaker even in the absence of a word being used. In the right-hand column of Table 2, we give an example of a situation in which the caregiver simply says ”see that?” but from the direction of the infant’s eyes and the location of her hand, the model correctly infers that she is talking about the COW, not either of the other possible referents. This kind of inference might lead the way in allowing infants to learn words like pronouns, which serve pick out an unambiguous focus of attention (one that is so obvious based on social and contextual cues that it does not need to be named). Finally, in the next section we show that the addition of social cues to the model allows correct performance in experimental tests of social generalization which only children older than 18 months can pass, suggesting perhaps that the social model is closer to the strategy used by more mature word learners. Table 2: Intentions inferred by the Bayesian model after having learned a lexicon from the corpus. (IE=Infant’s eyes, CH=Caregiver’s hands). Words Objects Social Cues Inferred intention “look at the moocow” COW GIRL BEAR “see the bear by the rattle?” BEAR RATTLE COW COW BEAR RATTLE 5 “see that?” BEAR RATTLE COW IE & CH→COW COW situation: !7.3, corpus: !631.1, total: !638.4</p><p>4 0.61876285 <a title="2-lsi-4" href="./nips-2007-A_Probabilistic_Approach_to_Language_Change.html">9 nips-2007-A Probabilistic Approach to Language Change</a></p>
<p>Author: Alexandre Bouchard-côté, Percy Liang, Dan Klein, Thomas L. Griffiths</p><p>Abstract: We present a probabilistic approach to language change in which word forms are represented by phoneme sequences that undergo stochastic edits along the branches of a phylogenetic tree. This framework combines the advantages of the classical comparative method with the robustness of corpus-based probabilistic models. We use this framework to explore the consequences of two different schemes for deﬁning probabilistic models of phonological change, evaluating these schemes by reconstructing ancient word forms of Romance languages. The result is an efﬁcient inference procedure for automatically inferring ancient word forms from modern languages, which can be generalized to support inferences about linguistic phylogenies. 1</p><p>5 0.46460411 <a title="2-lsi-5" href="./nips-2007-HM-BiTAM%3A_Bilingual_Topic_Exploration%2C_Word_Alignment%2C_and_Translation.html">95 nips-2007-HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation</a></p>
<p>Author: Bing Zhao, Eric P. Xing</p><p>Abstract: We present a novel paradigm for statistical machine translation (SMT), based on a joint modeling of word alignment and the topical aspects underlying bilingual document-pairs, via a hidden Markov Bilingual Topic AdMixture (HM-BiTAM). In this paradigm, parallel sentence-pairs from a parallel document-pair are coupled via a certain semantic-ﬂow, to ensure coherence of topical context in the alignment of mapping words between languages, likelihood-based training of topic-dependent translational lexicons, as well as in the inference of topic representations in each language. The learned HM-BiTAM can not only display topic patterns like methods such as LDA [1], but now for bilingual corpora; it also offers a principled way of inferring optimal translation using document context. Our method integrates the conventional model of HMM — a key component for most of the state-of-the-art SMT systems, with the recently proposed BiTAM model [10]; we report an extensive empirical analysis (in many ways complementary to the description-oriented [10]) of our method in three aspects: bilingual topic representation, word alignment, and translation.</p><p>6 0.4629665 <a title="2-lsi-6" href="./nips-2007-Unconstrained_On-line_Handwriting_Recognition_with_Recurrent_Neural_Networks.html">210 nips-2007-Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks</a></p>
<p>7 0.39739782 <a title="2-lsi-7" href="./nips-2007-Expectation_Maximization_and_Posterior_Constraints.html">84 nips-2007-Expectation Maximization and Posterior Constraints</a></p>
<p>8 0.36539659 <a title="2-lsi-8" href="./nips-2007-Spatial_Latent_Dirichlet_Allocation.html">183 nips-2007-Spatial Latent Dirichlet Allocation</a></p>
<p>9 0.33054775 <a title="2-lsi-9" href="./nips-2007-Supervised_Topic_Models.html">189 nips-2007-Supervised Topic Models</a></p>
<p>10 0.31801763 <a title="2-lsi-10" href="./nips-2007-Distributed_Inference_for_Latent_Dirichlet_Allocation.html">73 nips-2007-Distributed Inference for Latent Dirichlet Allocation</a></p>
<p>11 0.31023365 <a title="2-lsi-11" href="./nips-2007-Modeling_homophily_and_stochastic_equivalence_in_symmetric_relational_data.html">131 nips-2007-Modeling homophily and stochastic equivalence in symmetric relational data</a></p>
<p>12 0.2970373 <a title="2-lsi-12" href="./nips-2007-Collapsed_Variational_Inference_for_HDP.html">47 nips-2007-Collapsed Variational Inference for HDP</a></p>
<p>13 0.29274699 <a title="2-lsi-13" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>14 0.2904731 <a title="2-lsi-14" href="./nips-2007-Optimal_models_of_sound_localization_by_barn_owls.html">150 nips-2007-Optimal models of sound localization by barn owls</a></p>
<p>15 0.27803496 <a title="2-lsi-15" href="./nips-2007-The_Infinite_Markov_Model.html">197 nips-2007-The Infinite Markov Model</a></p>
<p>16 0.27471122 <a title="2-lsi-16" href="./nips-2007-Nearest-Neighbor-Based_Active_Learning_for_Rare_Category_Detection.html">139 nips-2007-Nearest-Neighbor-Based Active Learning for Rare Category Detection</a></p>
<p>17 0.2735222 <a title="2-lsi-17" href="./nips-2007-Discriminative_Keyword_Selection_Using_Support_Vector_Machines.html">71 nips-2007-Discriminative Keyword Selection Using Support Vector Machines</a></p>
<p>18 0.25470179 <a title="2-lsi-18" href="./nips-2007-Agreement-Based_Learning.html">22 nips-2007-Agreement-Based Learning</a></p>
<p>19 0.24688104 <a title="2-lsi-19" href="./nips-2007-Infinite_State_Bayes-Nets_for_Structured_Domains.html">105 nips-2007-Infinite State Bayes-Nets for Structured Domains</a></p>
<p>20 0.24659024 <a title="2-lsi-20" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.055), (13, 0.083), (16, 0.031), (18, 0.035), (21, 0.05), (31, 0.018), (34, 0.01), (35, 0.029), (47, 0.06), (49, 0.014), (54, 0.219), (83, 0.123), (85, 0.04), (87, 0.093), (90, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79201448 <a title="2-lda-1" href="./nips-2007-A_Bayesian_LDA-based_model_for_semi-supervised_part-of-speech_tagging.html">2 nips-2007-A Bayesian LDA-based model for semi-supervised part-of-speech tagging</a></p>
<p>Author: Kristina Toutanova, Mark Johnson</p><p>Abstract: We present a novel Bayesian model for semi-supervised part-of-speech tagging. Our model extends the Latent Dirichlet Allocation model and incorporates the intuition that words’ distributions over tags, p(t|w), are sparse. In addition we introduce a model for determining the set of possible tags of a word which captures important dependencies in the ambiguity classes of words. Our model outperforms the best previously proposed model for this task on a standard dataset. 1</p><p>2 0.72920364 <a title="2-lda-2" href="./nips-2007-New_Outer_Bounds_on_the_Marginal_Polytope.html">141 nips-2007-New Outer Bounds on the Marginal Polytope</a></p>
<p>Author: David Sontag, Tommi S. Jaakkola</p><p>Abstract: We give a new class of outer bounds on the marginal polytope, and propose a cutting-plane algorithm for efﬁciently optimizing over these constraints. When combined with a concave upper bound on the entropy, this gives a new variational inference algorithm for probabilistic inference in discrete Markov Random Fields (MRFs). Valid constraints on the marginal polytope are derived through a series of projections onto the cut polytope. As a result, we obtain tighter upper bounds on the log-partition function. We also show empirically that the approximations of the marginals are signiﬁcantly more accurate when using the tighter outer bounds. Finally, we demonstrate the advantage of the new constraints for ﬁnding the MAP assignment in protein structure prediction. 1</p><p>3 0.67526674 <a title="2-lda-3" href="./nips-2007-Bundle_Methods_for_Machine_Learning.html">40 nips-2007-Bundle Methods for Machine Learning</a></p>
<p>Author: Quoc V. Le, Alex J. Smola, S.v.n. Vishwanathan</p><p>Abstract: We present a globally convergent method for regularized risk minimization problems. Our method applies to Support Vector estimation, regression, Gaussian Processes, and any other regularized risk minimization setting which leads to a convex optimization problem. SVMPerf can be shown to be a special case of our approach. In addition to the uniﬁed framework we present tight convergence bounds, which show that our algorithm converges in O(1/ ) steps to precision for general convex problems and in O(log(1/ )) steps for continuously differentiable problems. We demonstrate in experiments the performance of our approach. 1</p><p>4 0.64221346 <a title="2-lda-4" href="./nips-2007-HM-BiTAM%3A_Bilingual_Topic_Exploration%2C_Word_Alignment%2C_and_Translation.html">95 nips-2007-HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation</a></p>
<p>Author: Bing Zhao, Eric P. Xing</p><p>Abstract: We present a novel paradigm for statistical machine translation (SMT), based on a joint modeling of word alignment and the topical aspects underlying bilingual document-pairs, via a hidden Markov Bilingual Topic AdMixture (HM-BiTAM). In this paradigm, parallel sentence-pairs from a parallel document-pair are coupled via a certain semantic-ﬂow, to ensure coherence of topical context in the alignment of mapping words between languages, likelihood-based training of topic-dependent translational lexicons, as well as in the inference of topic representations in each language. The learned HM-BiTAM can not only display topic patterns like methods such as LDA [1], but now for bilingual corpora; it also offers a principled way of inferring optimal translation using document context. Our method integrates the conventional model of HMM — a key component for most of the state-of-the-art SMT systems, with the recently proposed BiTAM model [10]; we report an extensive empirical analysis (in many ways complementary to the description-oriented [10]) of our method in three aspects: bilingual topic representation, word alignment, and translation.</p><p>5 0.63140702 <a title="2-lda-5" href="./nips-2007-Distributed_Inference_for_Latent_Dirichlet_Allocation.html">73 nips-2007-Distributed Inference for Latent Dirichlet Allocation</a></p>
<p>Author: David Newman, Padhraic Smyth, Max Welling, Arthur U. Asuncion</p><p>Abstract: We investigate the problem of learning a widely-used latent-variable model – the Latent Dirichlet Allocation (LDA) or “topic” model – using distributed computation, where each of processors only sees of the total data set. We propose two distributed inference schemes that are motivated from different perspectives. The ﬁrst scheme uses local Gibbs sampling on each processor with periodic updates—it is simple to implement and can be viewed as an approximation to a single processor implementation of Gibbs sampling. The second scheme relies on a hierarchical Bayesian extension of the standard LDA model to directly account for the fact that data are distributed across processors—it has a theoretical guarantee of convergence but is more complex to implement than the approximate method. Using ﬁve real-world text corpora we show that distributed learning works very well for LDA models, i.e., perplexity and precision-recall scores for distributed learning are indistinguishable from those obtained with single-processor learning. Our extensive experimental results include large-scale distributed computation on 1000 virtual processors; and speedup experiments of learning topics in a 100-million word corpus using 16 processors. ¢ ¤ ¦¥£ ¢ ¢</p><p>6 0.62944168 <a title="2-lda-6" href="./nips-2007-Continuous_Time_Particle_Filtering_for_fMRI.html">59 nips-2007-Continuous Time Particle Filtering for fMRI</a></p>
<p>7 0.62735999 <a title="2-lda-7" href="./nips-2007-Supervised_Topic_Models.html">189 nips-2007-Supervised Topic Models</a></p>
<p>8 0.62684393 <a title="2-lda-8" href="./nips-2007-Combined_discriminative_and_generative_articulated_pose_and_non-rigid_shape_estimation.html">50 nips-2007-Combined discriminative and generative articulated pose and non-rigid shape estimation</a></p>
<p>9 0.60734135 <a title="2-lda-9" href="./nips-2007-Expectation_Maximization_and_Posterior_Constraints.html">84 nips-2007-Expectation Maximization and Posterior Constraints</a></p>
<p>10 0.60704106 <a title="2-lda-10" href="./nips-2007-Infinite_State_Bayes-Nets_for_Structured_Domains.html">105 nips-2007-Infinite State Bayes-Nets for Structured Domains</a></p>
<p>11 0.60595334 <a title="2-lda-11" href="./nips-2007-Collapsed_Variational_Inference_for_HDP.html">47 nips-2007-Collapsed Variational Inference for HDP</a></p>
<p>12 0.60429597 <a title="2-lda-12" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>13 0.60291493 <a title="2-lda-13" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>14 0.60129607 <a title="2-lda-14" href="./nips-2007-People_Tracking_with_the_Laplacian_Eigenmaps_Latent_Variable_Model.html">153 nips-2007-People Tracking with the Laplacian Eigenmaps Latent Variable Model</a></p>
<p>15 0.59927785 <a title="2-lda-15" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>16 0.59749222 <a title="2-lda-16" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>17 0.59463942 <a title="2-lda-17" href="./nips-2007-A_Probabilistic_Approach_to_Language_Change.html">9 nips-2007-A Probabilistic Approach to Language Change</a></p>
<p>18 0.59398097 <a title="2-lda-18" href="./nips-2007-Scene_Segmentation_with_CRFs_Learned_from_Partially_Labeled_Images.html">172 nips-2007-Scene Segmentation with CRFs Learned from Partially Labeled Images</a></p>
<p>19 0.59369421 <a title="2-lda-19" href="./nips-2007-Colored_Maximum_Variance_Unfolding.html">49 nips-2007-Colored Maximum Variance Unfolding</a></p>
<p>20 0.59288466 <a title="2-lda-20" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
