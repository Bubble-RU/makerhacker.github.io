<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>169 nips-2007-Retrieved context and the discovery of semantic structure</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-169" href="#">nips2007-169</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>169 nips-2007-Retrieved context and the discovery of semantic structure</h1>
<br/><p>Source: <a title="nips-2007-169-pdf" href="http://papers.nips.cc/paper/3232-retrieved-context-and-the-discovery-of-semantic-structure.pdf">pdf</a></p><p>Author: Vinayak Rao, Marc Howard</p><p>Abstract: Semantic memory refers to our knowledge of facts and relationships between concepts. A successful semantic memory depends on inferring relationships between items that are not explicitly taught. Recent mathematical modeling of episodic memory argues that episodic recall relies on retrieval of a gradually-changing representation of temporal context. We show that retrieved context enables the development of a global memory space that reﬂects relationships between all items that have been previously learned. When newly-learned information is integrated into this structure, it is placed in some relationship to all other items, even if that relationship has not been explicitly learned. We demonstrate this effect for global semantic structures shaped topologically as a ring, and as a two-dimensional sheet. We also examined the utility of this learning algorithm for learning a more realistic semantic space by training it on a large pool of synonym pairs. Retrieved context enabled the model to “infer” relationships between synonym pairs that had not yet been presented. 1</p><p>Reference: <a title="nips-2007-169-reference" href="../nips2007_reference/nips-2007-Retrieved_context_and_the_discovery_of_semantic_structure_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Retrieved context and the discovery of semantic structure Vinayak A. [sent-1, score-0.342]
</p><p>2 A successful semantic memory depends on inferring relationships between items that are not explicitly taught. [sent-9, score-0.748]
</p><p>3 Recent mathematical modeling of episodic memory argues that episodic recall relies on retrieval of a gradually-changing representation of temporal context. [sent-10, score-1.116]
</p><p>4 We show that retrieved context enables the development of a global memory space that reﬂects relationships between all items that have been previously learned. [sent-11, score-0.778]
</p><p>5 We demonstrate this effect for global semantic structures shaped topologically as a ring, and as a two-dimensional sheet. [sent-13, score-0.276]
</p><p>6 We also examined the utility of this learning algorithm for learning a more realistic semantic space by training it on a large pool of synonym pairs. [sent-14, score-0.408]
</p><p>7 Retrieved context enabled the model to “infer” relationships between synonym pairs that had not yet been presented. [sent-15, score-0.399]
</p><p>8 1  Introduction  Semantic memory refers to our ability to learn and retrieve facts and relationships about concepts without reference to a speciﬁc learning episode. [sent-16, score-0.346]
</p><p>9 An appropriate semantic memory for a set of stimuli as complex as, say, words in the English language, requires learning the relationships between tens of thousands of stimuli. [sent-19, score-0.566]
</p><p>10 Moreover, the relationships between these items may describe a network of non-trivial topology [16]. [sent-20, score-0.415]
</p><p>11 Put another way, semantic memory needs to not only be able to retrieve information in the absence of a memory for the details of the learning event, but also retrieve information for which there is no learning event at all. [sent-22, score-0.631]
</p><p>12 Computational models for automatic extraction of semantic content from naturally-occurring text, such as latent semantic analysis [12], and probabilistic topic models [1, 7], exploit the temporal co-occurrence structure of naturally-occurring text to estimate a semantic representation of words. [sent-23, score-0.923]
</p><p>13 Their success relies to some degree on their ability to not only learn relationships between words that occur in the same context, but also to infer relationships between words that occur in similar ∗ Vinayak Rao is now at the Gatsby Computational Neuroscience Unit, University College London. [sent-24, score-0.34]
</p><p>14 Here we show that the temporal context model (TCM), developed as a quantitative model of human performance in episodic memory tasks, can provide an on-line learning algorithm that learns appropriate semantic relationships from incomplete information. [sent-30, score-1.057]
</p><p>15 The capacity for this model of episodic memory to also construct semantic knowledge spaces of multiple distinct topologies, suggests a relatively subtle relationship between episodic and semantic memory. [sent-31, score-1.263]
</p><p>16 2  The temporal context model  Episodic memory is deﬁned as the vivid conscious recollection of information from a speciﬁc instance from one’s life [18]. [sent-32, score-0.362]
</p><p>17 Many authors describe episodic memory as the result of the recovery of some type of a contextual representation that is distinct from the items themselves. [sent-33, score-1.028]
</p><p>18 If a cue item can recover this “pointer” to an episode, this enables recovery of other items that were bound to the contextual representation without committing to lasting interitem connections between items whose occurrence may not be reliably correlated [17]. [sent-34, score-1.33]
</p><p>19 Laboratory episodic memory tasks can provide an important clue to the nature of the contextual representation that could underlie episodic memory. [sent-35, score-1.058]
</p><p>20 If episodic recall of an item is a consequence of recovering a state of context, then the transitions between recalls may tell us something about the ability of a particular state of context to cue recall of other items. [sent-37, score-0.971]
</p><p>21 Episodic memory tasks show a contiguity effect—a tendency to make transitions to items presented close together in time, but not simultaneously, with the just-recalled word. [sent-38, score-0.586]
</p><p>22 The contiguity effect shows an apparently universal form across multiple episodic recall tasks, with a characteristic asymmetry favoring forward recall transitions [11] (see Figure 1a). [sent-39, score-0.535]
</p><p>23 The temporal contiguity effect observed in episodic recall can be simply reconciled with the hypothesis that episodic recall is the result of recovery of a contextual representation if one assumes that the contextual representation changes gradually over time. [sent-40, score-1.636]
</p><p>24 The temporal context model (TCM) describes a set of rules for a gradually-changing representation of temporal context and how items can be bound to and recover states of temporal context. [sent-41, score-0.875]
</p><p>25 TCM has been applied to a number of problems in episodic recall [9]. [sent-42, score-0.359]
</p><p>26 Here we describe the model, incorporating several changes that enable TCM to describe the learning of stable semantic relationships (detailed in Section 3). [sent-43, score-0.392]
</p><p>27 In TCM, a gradually-changing state of temporal context mediates associations between items and is responsible for recency effects and contiguity effects. [sent-45, score-0.657]
</p><p>28 This means that when tj is presented as a cue, each item is activated to the extent that the probe context overlaps with its encoding contexts. [sent-49, score-0.418]
</p><p>29 We will decompose tIN into cIN , a component that does not change over the course of study of this paper, and hIN , a component 1 Previous published treatments of TCM have focused on episodic tasks in which items were presented only once. [sent-51, score-0.625]
</p><p>30 2 0  h  -5 -4 -3 -2 -1 0 1 Lag  2  3  4  5  Figure 1: Temporal recovery in episodic memory. [sent-57, score-0.347]
</p><p>31 Given that an item from a series has just been recalled, the y-axis gives the probability that the next item recalled came from each serial position relative the just-recalled item. [sent-60, score-0.433]
</p><p>32 When an item fi is presented, it evokes two inputs to t—a slowly-changing direct cortical input cIN and a more rapidly varying i hippocampal input hIN . [sent-65, score-0.323]
</p><p>33 When an item is repeated, the hippocampal component retrieves the context i in which the item was presented. [sent-66, score-0.585]
</p><p>34 While the cortical component serves as a temporally-asymmetric cue when an item is repeated, the hippocampal component provides a symmetric cue. [sent-68, score-0.55]
</p><p>35 Combining these in the right proportion enables TCM to describe temporal contiguity effects. [sent-69, score-0.299]
</p><p>36 that changes rapidly to retrieve the contexts in which an item was presented. [sent-70, score-0.334]
</p><p>37 We assume that the cIN s corresponding to the items presented in any particular experiment start and remain orthonormal to each other. [sent-74, score-0.324]
</p><p>38 Ai+1 Ai  (4)  It has been hypothesized that ti reﬂects the pattern of activity at extra-hippocampal medial temporal lobe (MTL) regions, in particular the entorhinal cortex [8]. [sent-76, score-0.359]
</p><p>39 According to TCM, associations between items are not formed directly, but rather are mediated by the effect that items have on the state of context which is then used to probe for recall of other items. [sent-78, score-0.801]
</p><p>40 When an item is repeated as a probe, this induces a correlation between the tIN of the probe context and the study context of items that were neighbors of the probe item when it was initially presented. [sent-79, score-1.008]
</p><p>41 The consistent part of tIN is an effective cue for items that followed the initial presentation of the probe item (open symbols, Figure 1c). [sent-80, score-0.77]
</p><p>42 In contrast, recovery of the state of context that was present before the probe item was initially presented is a symmetric cue (ﬁlled symbols, Figure 1). [sent-81, score-0.686]
</p><p>43 Combining these two components in the proper proportions provides an excellent description of contiguity effects in episodic memory [8]. [sent-82, score-0.573]
</p><p>44 3  Constructing global semantic information from local events  In each of the following simulations, we specify a to-be-learned semantic structure by imagining items as the nodes of a graph with some topology. [sent-83, score-0.85]
</p><p>45 This means that the temporal context when the second item is presented is effectively isolated from the previous pair. [sent-88, score-0.46]
</p><p>46 Associative strength between items after training (higher strength corresponds to darker cells). [sent-109, score-0.457]
</p><p>47 After training we evaluated the ability of the model to capture the topology of the graph by examining the cue strength between each item. [sent-119, score-0.488]
</p><p>48 The cue strength from item A to B is deﬁned as fB MT F tIN . [sent-120, score-0.516]
</p><p>49 If at some later time B is now A A presented as part of the sequence B-C , then because tIN is similar to tIN , item C is learned in a A B context that resembles tIN , despite the fact that A and C were not actually presented close together A in time. [sent-128, score-0.394]
</p><p>50 This ability to C A rate as similar items that were not presented together in the same context, but that were presented in similar contexts, is a key property of latent models of semantic learning [12]. [sent-130, score-0.655]
</p><p>51 To isolate the importance of retrieved context for the ability to extract global structure, we will compare a version of the model with γ = 0 to one with γ > 0. [sent-131, score-0.298]
</p><p>52 4 With γ = 0, the model functions as a simple co-occurrence detector in that the cue strength between A and B is non-zero only if cIN A was part of the study contexts of B. [sent-132, score-0.38]
</p><p>53 1  1-D: Rings  For this simulation we sampled edges chosen from a ring of ten items (Fig. [sent-140, score-0.319]
</p><p>54 2-dimensional MDS solution constructed from the temporal co-occurrence version of TCM γ = 0 using the log of the associative strength as the metric. [sent-155, score-0.27]
</p><p>55 The model accurately places the items in the correct topology. [sent-159, score-0.277]
</p><p>56 Figure 2b shows the cue strength between each pair of items as a grey-scale image after training the model without contextual retrieval (γ = 0). [sent-160, score-1.045]
</p><p>57 The diagonal is shaded reﬂecting the fact that an item’s cue strength to itself is high. [sent-161, score-0.315]
</p><p>58 This reﬂects the non-zero cue strength between items that were presented as part of the same training pair. [sent-163, score-0.638]
</p><p>59 That is, the model without contextual retrieval has correctly learned the relationships described by the edges of the graph. [sent-164, score-0.602]
</p><p>60 However, without contextual retrieval the model has learned nothing about the relationships between the items that were not presented as part of the same pair (e. [sent-165, score-0.862]
</p><p>61 Figure 2c shows the cue strength between each pair of items for the model with contextual retrieval γ > 0. [sent-168, score-1.02]
</p><p>62 The effect of contextual retrieval is that pairs that were not presented together have non-zero cue strength and this cue strength falls off with the number of edges separating the items in the graph. [sent-169, score-1.48]
</p><p>63 This happens because contextual retrieval enables similarity to “spread” across the edges of the graph, reaching an equilibrium that reﬂects the global structure. [sent-170, score-0.537]
</p><p>64 Figure 2d shows a two-dimensional MDS (multi-dimensional scaling) solution conducted on the log of the cue strengths of the model with contextual retrieval. [sent-171, score-0.513]
</p><p>65 More precisely, with contextual retrieval, TCM can place the items in a space that captures the topology of the graph used to generate the training pairs. [sent-173, score-0.652]
</p><p>66 On the one hand, the relationships that result from contextual retrieval in this simulation seem intuitive and satisfying. [sent-174, score-0.543]
</p><p>67 Moreover, suppose that one’s task were simply to remember the pairs, or alternatively, to predict the next item that would be presented after presenting the ﬁrst member of a pair. [sent-177, score-0.3]
</p><p>68 Under these circumstances, the co-occurrence model performs better than the model equipped with contextual retrieval. [sent-178, score-0.307]
</p><p>69 These ﬁnding suggest that the mechanism of contextual retrieval capture an important property of how we learn in similar circumstance. [sent-183, score-0.428]
</p><p>70 2  2-D: Spatial navigation  The ring illustrated in Figure 2 demonstrates the basic idea behind contextual retrieval’s ability to extract semantic spaces, but it is hard to imagine an application where such a simple space would need to be extracted. [sent-185, score-0.626]
</p><p>71 In this simulation will illustrate the ability of retrieved context to discover relationships between stimuli arranged in a two-dimensional sheet. [sent-186, score-0.409]
</p><p>72 It has long been argued that the medial temporal lobe has a special role in our ability to store and retrieve information from a spatial map. [sent-188, score-0.396]
</p><p>73 One may think of the items as landmarks in a city with a rectangular street plan. [sent-197, score-0.283]
</p><p>74 Without contextual retrieval the model places the items in a high-dimensional structure that reﬂects their co-occurrence. [sent-201, score-0.705]
</p><p>75 Contextual retrieval enables the model to place the items on a two-dimensional sheet that preserves the topology of the graph used to generate the pairs. [sent-203, score-0.64]
</p><p>76 It is not a map—there is no sense of North nor an accurate metric between the points—but it is a semantic representation that captures something intuitive about the organization that generated the pairs. [sent-204, score-0.289]
</p><p>77 This illustrates the ability of contextual retrieval to organize isolated experiences, or episodes, into a coherent whole based on the temporal structure of experience. [sent-205, score-0.663]
</p><p>78 3  More realistic example: Synonyms  The preceding simulations showed that retrieved context enables learning of simple topologies with a few items. [sent-207, score-0.334]
</p><p>79 It is possible that the utility of the model in discovering semantic relationships is limited to these toy examples. [sent-208, score-0.389]
</p><p>80 In this subsection we demonstrate that retrieved context can provide beneﬁts in learning relationships among a large number of items with a more realistic semantic structure. [sent-210, score-0.858]
</p><p>81 We tested performance by comparing the cue strength of the cue word with its synonym to the associative strength to three lures that were synonyms of other cue words—if the correct answer had the highest cue strength, it was counted as correct. [sent-216, score-1.356]
</p><p>82 In the absence of contextual retrieval, the model learns linearly, performing perfectly on pairs that have been explicitly presented. [sent-222, score-0.352]
</p><p>83 However, contextual retrieval enables faster learning of the pairs, presumably due to the fact that it can “infer” relationships between words 5  We also observed the same results when we presented the model with complete rows and columns of the sheet as a training set rather than simply pairs. [sent-223, score-0.759]
</p><p>84 6 In instances where the cue strength was zero for all the choices, as at the beginning of training, this was counted as 1/4 of a correct answer. [sent-224, score-0.335]
</p><p>85 6  0  20 40 60 80 100 Number of pairs presented (1k)  TCM Co-occurrence 0  20 40 60 80 100 Number of pairs presented (1k)  Figure 4: Retrieved context aids in learning synonyms that have not been presented. [sent-233, score-0.366]
</p><p>86 During this period, the model without contextual retrieval does not improve its performance on the test pairs because they are not presented. [sent-242, score-0.515]
</p><p>87 In contrast, TCM with contextual retrieval shows considerable improvement during that interval. [sent-243, score-0.428]
</p><p>88 7  4  Discussion  We showed that retrieval of temporal context, an on-line learning method developed for quantitatively describing episodic recall data, can also integrate distinct learning events into a coherent and intuitive semantic representation. [sent-244, score-0.971]
</p><p>89 It would be incorrect to describe this representation as a semantic space—the cue strength between items is in general asymmetric (Figure 1c). [sent-245, score-0.86]
</p><p>90 However, one can also think of the set of tIN s corresponding to the items as a semantic representation that is also a proper space. [sent-247, score-0.565]
</p><p>91 More speciﬁcally, these algorithms form semantic associations between words by batch-processing large collections of natural text (e. [sent-249, score-0.348]
</p><p>92 The BEAGLE model [10] describes the semantic representation of a word as a superposition of the words that occurred with it in the same sentence. [sent-254, score-0.373]
</p><p>93 This enables BEAGLE to describe semantic relations beyond simple cooccurrence, but precludes the development of a representation that captures continuously-varying representations (e. [sent-255, score-0.337]
</p><p>94 The present results suggest that retrieved temporal context—previously hypothesized to be essential for episodic memory—could also be important in developing coherent semantic representations. [sent-260, score-0.881]
</p><p>95 This could reﬂect similar computational mechanisms contribute to separate systems, or it could indicate a deep connection between episodic and semantic memory. [sent-261, score-0.559]
</p><p>96 A key ﬁnding is that adult-onset amnesics with impaired episodic memory retain the ability to express previously-learned semantic knowledge but are impaired at learning new semantic knowledge [19]. [sent-262, score-1.032]
</p><p>97 The temporal context model in spatial navigation and relational learning: Toward a common explanation of medial temporal lobe function across domains. [sent-320, score-0.545]
</p><p>98 Solution to Plato’s problem : The latent semantic analysis theory of acquisition, induction, and representation of knowledge. [sent-354, score-0.289]
</p><p>99 The large scale structure of semantic networks: statistical analyses and a model of semantic growth. [sent-378, score-0.527]
</p><p>100 Names and words without meaning: incidental postmorbid semantic learning in a person with extensive bilateral medial temporal damage. [sent-393, score-0.469]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tcm', 0.384), ('episodic', 0.306), ('tin', 0.277), ('contextual', 0.265), ('items', 0.256), ('semantic', 0.253), ('hin', 0.246), ('cue', 0.227), ('item', 0.201), ('cin', 0.169), ('retrieval', 0.163), ('temporal', 0.128), ('memory', 0.124), ('contiguity', 0.123), ('retrieved', 0.123), ('relationships', 0.115), ('synonym', 0.108), ('hippocampal', 0.094), ('context', 0.089), ('strength', 0.088), ('probe', 0.086), ('pairs', 0.066), ('ti', 0.065), ('retrieve', 0.065), ('synonyms', 0.061), ('associations', 0.061), ('psychological', 0.058), ('associative', 0.054), ('hippocampus', 0.054), ('medial', 0.054), ('recall', 0.053), ('mt', 0.051), ('enables', 0.048), ('entorhinal', 0.046), ('sheet', 0.046), ('mds', 0.046), ('coherent', 0.045), ('ects', 0.044), ('spatial', 0.044), ('contexts', 0.044), ('topology', 0.044), ('presented', 0.042), ('ability', 0.042), ('recovery', 0.041), ('navigation', 0.041), ('graph', 0.041), ('lobe', 0.04), ('stimuli', 0.04), ('howard', 0.039), ('edges', 0.038), ('steyvers', 0.037), ('mtl', 0.037), ('representation', 0.036), ('shuf', 0.034), ('words', 0.034), ('beagle', 0.031), ('datey', 0.031), ('rats', 0.031), ('recalled', 0.031), ('syracuse', 0.031), ('tasa', 0.031), ('vinayak', 0.031), ('remember', 0.031), ('re', 0.03), ('simulations', 0.029), ('word', 0.029), ('corpus', 0.028), ('cortical', 0.028), ('landmarks', 0.027), ('landauer', 0.027), ('impaired', 0.027), ('hypothesized', 0.026), ('review', 0.026), ('orthonormal', 0.026), ('member', 0.026), ('training', 0.025), ('ring', 0.025), ('rings', 0.024), ('imagining', 0.024), ('ai', 0.024), ('changes', 0.024), ('argued', 0.023), ('global', 0.023), ('topologies', 0.023), ('marc', 0.023), ('integrate', 0.023), ('paths', 0.022), ('realistic', 0.022), ('connectionist', 0.022), ('experiences', 0.022), ('tasks', 0.021), ('ect', 0.021), ('place', 0.021), ('model', 0.021), ('combinatorics', 0.02), ('together', 0.02), ('counted', 0.02), ('organize', 0.02), ('proper', 0.02), ('dimension', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="169-tfidf-1" href="./nips-2007-Retrieved_context_and_the_discovery_of_semantic_structure.html">169 nips-2007-Retrieved context and the discovery of semantic structure</a></p>
<p>Author: Vinayak Rao, Marc Howard</p><p>Abstract: Semantic memory refers to our knowledge of facts and relationships between concepts. A successful semantic memory depends on inferring relationships between items that are not explicitly taught. Recent mathematical modeling of episodic memory argues that episodic recall relies on retrieval of a gradually-changing representation of temporal context. We show that retrieved context enables the development of a global memory space that reﬂects relationships between all items that have been previously learned. When newly-learned information is integrated into this structure, it is placed in some relationship to all other items, even if that relationship has not been explicitly learned. We demonstrate this effect for global semantic structures shaped topologically as a ring, and as a two-dimensional sheet. We also examined the utility of this learning algorithm for learning a more realistic semantic space by training it on a large pool of synonym pairs. Retrieved context enabled the model to “infer” relationships between synonym pairs that had not yet been presented. 1</p><p>2 0.27771685 <a title="169-tfidf-2" href="./nips-2007-Hippocampal_Contributions_to_Control%3A_The_Third_Way.html">100 nips-2007-Hippocampal Contributions to Control: The Third Way</a></p>
<p>Author: Máté Lengyel, Peter Dayan</p><p>Abstract: Recent experimental studies have focused on the specialization of different neural structures for different types of instrumental behavior. Recent theoretical work has provided normative accounts for why there should be more than one control system, and how the output of different controllers can be integrated. Two particlar controllers have been identiﬁed, one associated with a forward model and the prefrontal cortex and a second associated with computationally simpler, habitual, actor-critic methods and part of the striatum. We argue here for the normative appropriateness of an additional, but so far marginalized control system, associated with episodic memory, and involving the hippocampus and medial temporal cortices. We analyze in depth a class of simple environments to show that episodic control should be useful in a range of cases characterized by complexity and inferential noise, and most particularly at the very early stages of learning, long before habitization has set in. We interpret data on the transfer of control from the hippocampus to the striatum in the light of this hypothesis. 1</p><p>3 0.08807876 <a title="169-tfidf-3" href="./nips-2007-Infinite_State_Bayes-Nets_for_Structured_Domains.html">105 nips-2007-Infinite State Bayes-Nets for Structured Domains</a></p>
<p>Author: Max Welling, Ian Porteous, Evgeniy Bart</p><p>Abstract: A general modeling framework is proposed that uniﬁes nonparametric-Bayesian models, topic-models and Bayesian networks. This class of inﬁnite state Bayes nets (ISBN) can be viewed as directed networks of ‘hierarchical Dirichlet processes’ (HDPs) where the domain of the variables can be structured (e.g. words in documents or features in images). We show that collapsed Gibbs sampling can be done efﬁciently in these models by leveraging the structure of the Bayes net and using the forward-ﬁltering-backward-sampling algorithm for junction trees. Existing models, such as nested-DP, Pachinko allocation, mixed membership stochastic block models as well as a number of new models are described as ISBNs. Two experiments have been performed to illustrate these ideas. 1</p><p>4 0.083985686 <a title="169-tfidf-4" href="./nips-2007-Object_Recognition_by_Scene_Alignment.html">143 nips-2007-Object Recognition by Scene Alignment</a></p>
<p>Author: Bryan Russell, Antonio Torralba, Ce Liu, Rob Fergus, William T. Freeman</p><p>Abstract: Current object recognition systems can only recognize a limited number of object categories; scaling up to many categories is the next challenge. We seek to build a system to recognize and localize many different object categories in complex scenes. We achieve this through a simple approach: by matching the input image, in an appropriate representation, to images in a large training set of labeled images. Due to regularities in object identities across similar scenes, the retrieved matches provide hypotheses for object identities and locations. We build a probabilistic model to transfer the labels from the retrieval set to the input image. We demonstrate the effectiveness of this approach and study algorithm component contributions using held-out test sets from the LabelMe database. 1</p><p>5 0.074627705 <a title="169-tfidf-5" href="./nips-2007-Non-parametric_Modeling_of_Partially_Ranked_Data.html">142 nips-2007-Non-parametric Modeling of Partially Ranked Data</a></p>
<p>Author: Guy Lebanon, Yi Mao</p><p>Abstract: Statistical models on full and partial rankings of n items are often of limited practical use for large n due to computational consideration. We explore the use of non-parametric models for partially ranked data and derive efﬁcient procedures for their use for large n. The derivations are largely possible through combinatorial and algebraic manipulations based on the lattice of partial rankings. In particular, we demonstrate for the ﬁrst time a non-parametric coherent and consistent model capable of efﬁciently aggregating partially ranked data of different types. 1</p><p>6 0.063350514 <a title="169-tfidf-6" href="./nips-2007-COFI_RANK_-_Maximum_Margin_Matrix_Factorization_for_Collaborative_Ranking.html">41 nips-2007-COFI RANK - Maximum Margin Matrix Factorization for Collaborative Ranking</a></p>
<p>7 0.057723813 <a title="169-tfidf-7" href="./nips-2007-A_Bayesian_Framework_for_Cross-Situational_Word-Learning.html">1 nips-2007-A Bayesian Framework for Cross-Situational Word-Learning</a></p>
<p>8 0.055852316 <a title="169-tfidf-8" href="./nips-2007-Comparing_Bayesian_models_for_multisensory_cue_combination_without_mandatory_integration.html">51 nips-2007-Comparing Bayesian models for multisensory cue combination without mandatory integration</a></p>
<p>9 0.051414132 <a title="169-tfidf-9" href="./nips-2007-Hidden_Common_Cause_Relations_in_Relational_Learning.html">97 nips-2007-Hidden Common Cause Relations in Relational Learning</a></p>
<p>10 0.050484989 <a title="169-tfidf-10" href="./nips-2007-A_Bayesian_Model_of_Conditioned_Perception.html">3 nips-2007-A Bayesian Model of Conditioned Perception</a></p>
<p>11 0.050330523 <a title="169-tfidf-11" href="./nips-2007-The_Epoch-Greedy_Algorithm_for_Multi-armed_Bandits_with_Side_Information.html">194 nips-2007-The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information</a></p>
<p>12 0.047199652 <a title="169-tfidf-12" href="./nips-2007-Mining_Internet-Scale_Software_Repositories.html">129 nips-2007-Mining Internet-Scale Software Repositories</a></p>
<p>13 0.046952691 <a title="169-tfidf-13" href="./nips-2007-Learning_and_using_relational_theories.html">114 nips-2007-Learning and using relational theories</a></p>
<p>14 0.046433147 <a title="169-tfidf-14" href="./nips-2007-HM-BiTAM%3A_Bilingual_Topic_Exploration%2C_Word_Alignment%2C_and_Translation.html">95 nips-2007-HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation</a></p>
<p>15 0.044584297 <a title="169-tfidf-15" href="./nips-2007-Multi-Task_Learning_via_Conic_Programming.html">134 nips-2007-Multi-Task Learning via Conic Programming</a></p>
<p>16 0.044520009 <a title="169-tfidf-16" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>17 0.041008804 <a title="169-tfidf-17" href="./nips-2007-Second_Order_Bilinear_Discriminant_Analysis_for_single_trial_EEG_analysis.html">173 nips-2007-Second Order Bilinear Discriminant Analysis for single trial EEG analysis</a></p>
<p>18 0.040500581 <a title="169-tfidf-18" href="./nips-2007-Spatial_Latent_Dirichlet_Allocation.html">183 nips-2007-Spatial Latent Dirichlet Allocation</a></p>
<p>19 0.03888553 <a title="169-tfidf-19" href="./nips-2007-A_Bayesian_LDA-based_model_for_semi-supervised_part-of-speech_tagging.html">2 nips-2007-A Bayesian LDA-based model for semi-supervised part-of-speech tagging</a></p>
<p>20 0.038314551 <a title="169-tfidf-20" href="./nips-2007-Semi-Supervised_Multitask_Learning.html">175 nips-2007-Semi-Supervised Multitask Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.134), (1, 0.03), (2, 0.001), (3, -0.104), (4, 0.026), (5, 0.048), (6, 0.039), (7, 0.022), (8, 0.001), (9, -0.051), (10, -0.111), (11, -0.024), (12, 0.075), (13, -0.022), (14, 0.014), (15, -0.055), (16, -0.081), (17, -0.078), (18, 0.086), (19, -0.07), (20, -0.032), (21, 0.125), (22, -0.036), (23, -0.194), (24, 0.012), (25, -0.123), (26, 0.027), (27, 0.016), (28, 0.047), (29, -0.026), (30, 0.376), (31, 0.046), (32, -0.052), (33, -0.055), (34, -0.081), (35, 0.189), (36, -0.01), (37, 0.18), (38, 0.134), (39, 0.191), (40, -0.072), (41, -0.216), (42, 0.113), (43, 0.0), (44, 0.091), (45, -0.041), (46, 0.137), (47, 0.007), (48, -0.043), (49, 0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9706043 <a title="169-lsi-1" href="./nips-2007-Retrieved_context_and_the_discovery_of_semantic_structure.html">169 nips-2007-Retrieved context and the discovery of semantic structure</a></p>
<p>Author: Vinayak Rao, Marc Howard</p><p>Abstract: Semantic memory refers to our knowledge of facts and relationships between concepts. A successful semantic memory depends on inferring relationships between items that are not explicitly taught. Recent mathematical modeling of episodic memory argues that episodic recall relies on retrieval of a gradually-changing representation of temporal context. We show that retrieved context enables the development of a global memory space that reﬂects relationships between all items that have been previously learned. When newly-learned information is integrated into this structure, it is placed in some relationship to all other items, even if that relationship has not been explicitly learned. We demonstrate this effect for global semantic structures shaped topologically as a ring, and as a two-dimensional sheet. We also examined the utility of this learning algorithm for learning a more realistic semantic space by training it on a large pool of synonym pairs. Retrieved context enabled the model to “infer” relationships between synonym pairs that had not yet been presented. 1</p><p>2 0.83479089 <a title="169-lsi-2" href="./nips-2007-Hippocampal_Contributions_to_Control%3A_The_Third_Way.html">100 nips-2007-Hippocampal Contributions to Control: The Third Way</a></p>
<p>Author: Máté Lengyel, Peter Dayan</p><p>Abstract: Recent experimental studies have focused on the specialization of different neural structures for different types of instrumental behavior. Recent theoretical work has provided normative accounts for why there should be more than one control system, and how the output of different controllers can be integrated. Two particlar controllers have been identiﬁed, one associated with a forward model and the prefrontal cortex and a second associated with computationally simpler, habitual, actor-critic methods and part of the striatum. We argue here for the normative appropriateness of an additional, but so far marginalized control system, associated with episodic memory, and involving the hippocampus and medial temporal cortices. We analyze in depth a class of simple environments to show that episodic control should be useful in a range of cases characterized by complexity and inferential noise, and most particularly at the very early stages of learning, long before habitization has set in. We interpret data on the transfer of control from the hippocampus to the striatum in the light of this hypothesis. 1</p><p>3 0.34424022 <a title="169-lsi-3" href="./nips-2007-Non-parametric_Modeling_of_Partially_Ranked_Data.html">142 nips-2007-Non-parametric Modeling of Partially Ranked Data</a></p>
<p>Author: Guy Lebanon, Yi Mao</p><p>Abstract: Statistical models on full and partial rankings of n items are often of limited practical use for large n due to computational consideration. We explore the use of non-parametric models for partially ranked data and derive efﬁcient procedures for their use for large n. The derivations are largely possible through combinatorial and algebraic manipulations based on the lattice of partial rankings. In particular, we demonstrate for the ﬁrst time a non-parametric coherent and consistent model capable of efﬁciently aggregating partially ranked data of different types. 1</p><p>4 0.32462484 <a title="169-lsi-4" href="./nips-2007-Receding_Horizon_Differential_Dynamic_Programming.html">163 nips-2007-Receding Horizon Differential Dynamic Programming</a></p>
<p>Author: Yuval Tassa, Tom Erez, William D. Smart</p><p>Abstract: The control of high-dimensional, continuous, non-linear dynamical systems is a key problem in reinforcement learning and control. Local, trajectory-based methods, using techniques such as Differential Dynamic Programming (DDP), are not directly subject to the curse of dimensionality, but generate only local controllers. In this paper,we introduce Receding Horizon DDP (RH-DDP), an extension to the classic DDP algorithm, which allows us to construct stable and robust controllers based on a library of local-control trajectories. We demonstrate the effectiveness of our approach on a series of high-dimensional problems using a simulated multi-link swimming robot. These experiments show that our approach effectively circumvents dimensionality issues, and is capable of dealing with problems of (at least) 24 state and 9 action dimensions. 1</p><p>5 0.26707199 <a title="169-lsi-5" href="./nips-2007-Infinite_State_Bayes-Nets_for_Structured_Domains.html">105 nips-2007-Infinite State Bayes-Nets for Structured Domains</a></p>
<p>Author: Max Welling, Ian Porteous, Evgeniy Bart</p><p>Abstract: A general modeling framework is proposed that uniﬁes nonparametric-Bayesian models, topic-models and Bayesian networks. This class of inﬁnite state Bayes nets (ISBN) can be viewed as directed networks of ‘hierarchical Dirichlet processes’ (HDPs) where the domain of the variables can be structured (e.g. words in documents or features in images). We show that collapsed Gibbs sampling can be done efﬁciently in these models by leveraging the structure of the Bayes net and using the forward-ﬁltering-backward-sampling algorithm for junction trees. Existing models, such as nested-DP, Pachinko allocation, mixed membership stochastic block models as well as a number of new models are described as ISBNs. Two experiments have been performed to illustrate these ideas. 1</p><p>6 0.26456171 <a title="169-lsi-6" href="./nips-2007-A_Bayesian_Framework_for_Cross-Situational_Word-Learning.html">1 nips-2007-A Bayesian Framework for Cross-Situational Word-Learning</a></p>
<p>7 0.2348631 <a title="169-lsi-7" href="./nips-2007-Optimal_models_of_sound_localization_by_barn_owls.html">150 nips-2007-Optimal models of sound localization by barn owls</a></p>
<p>8 0.2304676 <a title="169-lsi-8" href="./nips-2007-Mining_Internet-Scale_Software_Repositories.html">129 nips-2007-Mining Internet-Scale Software Repositories</a></p>
<p>9 0.21841963 <a title="169-lsi-9" href="./nips-2007-Modeling_homophily_and_stochastic_equivalence_in_symmetric_relational_data.html">131 nips-2007-Modeling homophily and stochastic equivalence in symmetric relational data</a></p>
<p>10 0.21781361 <a title="169-lsi-10" href="./nips-2007-Active_Preference_Learning_with_Discrete_Choice_Data.html">19 nips-2007-Active Preference Learning with Discrete Choice Data</a></p>
<p>11 0.21654862 <a title="169-lsi-11" href="./nips-2007-COFI_RANK_-_Maximum_Margin_Matrix_Factorization_for_Collaborative_Ranking.html">41 nips-2007-COFI RANK - Maximum Margin Matrix Factorization for Collaborative Ranking</a></p>
<p>12 0.21136254 <a title="169-lsi-12" href="./nips-2007-Modeling_Natural_Sounds_with_Modulation_Cascade_Processes.html">130 nips-2007-Modeling Natural Sounds with Modulation Cascade Processes</a></p>
<p>13 0.21087061 <a title="169-lsi-13" href="./nips-2007-Learning_and_using_relational_theories.html">114 nips-2007-Learning and using relational theories</a></p>
<p>14 0.20920521 <a title="169-lsi-14" href="./nips-2007-The_Noisy-Logical_Distribution_and_its_Application_to_Causal_Inference.html">198 nips-2007-The Noisy-Logical Distribution and its Application to Causal Inference</a></p>
<p>15 0.2083551 <a title="169-lsi-15" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>16 0.20791265 <a title="169-lsi-16" href="./nips-2007-Object_Recognition_by_Scene_Alignment.html">143 nips-2007-Object Recognition by Scene Alignment</a></p>
<p>17 0.202672 <a title="169-lsi-17" href="./nips-2007-Temporal_Difference_Updating_without_a_Learning_Rate.html">191 nips-2007-Temporal Difference Updating without a Learning Rate</a></p>
<p>18 0.20265432 <a title="169-lsi-18" href="./nips-2007-A_Bayesian_Model_of_Conditioned_Perception.html">3 nips-2007-A Bayesian Model of Conditioned Perception</a></p>
<p>19 0.19794334 <a title="169-lsi-19" href="./nips-2007-Comparing_Bayesian_models_for_multisensory_cue_combination_without_mandatory_integration.html">51 nips-2007-Comparing Bayesian models for multisensory cue combination without mandatory integration</a></p>
<p>20 0.18970616 <a title="169-lsi-20" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.036), (13, 0.032), (16, 0.017), (18, 0.015), (19, 0.011), (21, 0.068), (27, 0.016), (29, 0.339), (31, 0.011), (34, 0.026), (35, 0.027), (47, 0.1), (49, 0.016), (83, 0.079), (85, 0.02), (87, 0.059), (90, 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80464584 <a title="169-lda-1" href="./nips-2007-Computing_Robust_Counter-Strategies.html">55 nips-2007-Computing Robust Counter-Strategies</a></p>
<p>Author: Michael Johanson, Martin Zinkevich, Michael Bowling</p><p>Abstract: Adaptation to other initially unknown agents often requires computing an effective counter-strategy. In the Bayesian paradigm, one must ﬁnd a good counterstrategy to the inferred posterior of the other agents’ behavior. In the experts paradigm, one may want to choose experts that are good counter-strategies to the other agents’ expected behavior. In this paper we introduce a technique for computing robust counter-strategies for adaptation in multiagent scenarios under a variety of paradigms. The strategies can take advantage of a suspected tendency in the decisions of the other agents, while bounding the worst-case performance when the tendency is not observed. The technique involves solving a modiﬁed game, and therefore can make use of recently developed algorithms for solving very large extensive games. We demonstrate the effectiveness of the technique in two-player Texas Hold’em. We show that the computed poker strategies are substantially more robust than best response counter-strategies, while still exploiting a suspected tendency. We also compose the generated strategies in an experts algorithm showing a dramatic improvement in performance over using simple best responses. 1</p><p>same-paper 2 0.78744578 <a title="169-lda-2" href="./nips-2007-Retrieved_context_and_the_discovery_of_semantic_structure.html">169 nips-2007-Retrieved context and the discovery of semantic structure</a></p>
<p>Author: Vinayak Rao, Marc Howard</p><p>Abstract: Semantic memory refers to our knowledge of facts and relationships between concepts. A successful semantic memory depends on inferring relationships between items that are not explicitly taught. Recent mathematical modeling of episodic memory argues that episodic recall relies on retrieval of a gradually-changing representation of temporal context. We show that retrieved context enables the development of a global memory space that reﬂects relationships between all items that have been previously learned. When newly-learned information is integrated into this structure, it is placed in some relationship to all other items, even if that relationship has not been explicitly learned. We demonstrate this effect for global semantic structures shaped topologically as a ring, and as a two-dimensional sheet. We also examined the utility of this learning algorithm for learning a more realistic semantic space by training it on a large pool of synonym pairs. Retrieved context enabled the model to “infer” relationships between synonym pairs that had not yet been presented. 1</p><p>3 0.66584927 <a title="169-lda-3" href="./nips-2007-Regret_Minimization_in_Games_with_Incomplete_Information.html">165 nips-2007-Regret Minimization in Games with Incomplete Information</a></p>
<p>Author: Martin Zinkevich, Michael Johanson, Michael Bowling, Carmelo Piccione</p><p>Abstract: Extensive games are a powerful model of multiagent decision-making scenarios with incomplete information. Finding a Nash equilibrium for very large instances of these games has received a great deal of recent attention. In this paper, we describe a new technique for solving large games based on regret minimization. In particular, we introduce the notion of counterfactual regret, which exploits the degree of incomplete information in an extensive game. We show how minimizing counterfactual regret minimizes overall regret, and therefore in self-play can be used to compute a Nash equilibrium. We demonstrate this technique in the domain of poker, showing we can solve abstractions of limit Texas Hold’em with as many as 1012 states, two orders of magnitude larger than previous methods. 1</p><p>4 0.61720192 <a title="169-lda-4" href="./nips-2007-Cooled_and_Relaxed_Survey_Propagation_for_MRFs.html">64 nips-2007-Cooled and Relaxed Survey Propagation for MRFs</a></p>
<p>Author: Hai L. Chieu, Wee S. Lee, Yee W. Teh</p><p>Abstract: We describe a new algorithm, Relaxed Survey Propagation (RSP), for ﬁnding MAP conﬁgurations in Markov random ﬁelds. We compare its performance with state-of-the-art algorithms including the max-product belief propagation, its sequential tree-reweighted variant, residual (sum-product) belief propagation, and tree-structured expectation propagation. We show that it outperforms all approaches for Ising models with mixed couplings, as well as on a web person disambiguation task formulated as a supervised clustering problem. 1</p><p>5 0.55617458 <a title="169-lda-5" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>Author: John Wright, Yangyu Tao, Zhouchen Lin, Yi Ma, Heung-yeung Shum</p><p>Abstract: We present a simple new criterion for classiﬁcation, based on principles from lossy data compression. The criterion assigns a test sample to the class that uses the minimum number of additional bits to code the test sample, subject to an allowable distortion. We prove asymptotic optimality of this criterion for Gaussian data and analyze its relationships to classical classiﬁers. Theoretical results provide new insights into relationships among popular classiﬁers such as MAP and RDA, as well as unsupervised clustering methods based on lossy compression [13]. Minimizing the lossy coding length induces a regularization effect which stabilizes the (implicit) density estimate in a small-sample setting. Compression also provides a uniform means of handling classes of varying dimension. This simple classiﬁcation criterion and its kernel and local versions perform competitively against existing classiﬁers on both synthetic examples and real imagery data such as handwritten digits and human faces, without requiring domain-speciﬁc information. 1</p><p>6 0.45169628 <a title="169-lda-6" href="./nips-2007-TrueSkill_Through_Time%3A_Revisiting_the_History_of_Chess.html">208 nips-2007-TrueSkill Through Time: Revisiting the History of Chess</a></p>
<p>7 0.43208557 <a title="169-lda-7" href="./nips-2007-Continuous_Time_Particle_Filtering_for_fMRI.html">59 nips-2007-Continuous Time Particle Filtering for fMRI</a></p>
<p>8 0.43107629 <a title="169-lda-8" href="./nips-2007-Congruence_between_model_and_human_attention_reveals_unique_signatures_of_critical_visual_events.html">57 nips-2007-Congruence between model and human attention reveals unique signatures of critical visual events</a></p>
<p>9 0.43017426 <a title="169-lda-9" href="./nips-2007-Supervised_Topic_Models.html">189 nips-2007-Supervised Topic Models</a></p>
<p>10 0.42830995 <a title="169-lda-10" href="./nips-2007-Distributed_Inference_for_Latent_Dirichlet_Allocation.html">73 nips-2007-Distributed Inference for Latent Dirichlet Allocation</a></p>
<p>11 0.42718631 <a title="169-lda-11" href="./nips-2007-A_Game-Theoretic_Approach_to_Apprenticeship_Learning.html">5 nips-2007-A Game-Theoretic Approach to Apprenticeship Learning</a></p>
<p>12 0.42707255 <a title="169-lda-12" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>13 0.42620298 <a title="169-lda-13" href="./nips-2007-Hippocampal_Contributions_to_Control%3A_The_Third_Way.html">100 nips-2007-Hippocampal Contributions to Control: The Third Way</a></p>
<p>14 0.42433482 <a title="169-lda-14" href="./nips-2007-Infinite_State_Bayes-Nets_for_Structured_Domains.html">105 nips-2007-Infinite State Bayes-Nets for Structured Domains</a></p>
<p>15 0.42330822 <a title="169-lda-15" href="./nips-2007-People_Tracking_with_the_Laplacian_Eigenmaps_Latent_Variable_Model.html">153 nips-2007-People Tracking with the Laplacian Eigenmaps Latent Variable Model</a></p>
<p>16 0.42123523 <a title="169-lda-16" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>17 0.4187437 <a title="169-lda-17" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>18 0.41645926 <a title="169-lda-18" href="./nips-2007-Scene_Segmentation_with_CRFs_Learned_from_Partially_Labeled_Images.html">172 nips-2007-Scene Segmentation with CRFs Learned from Partially Labeled Images</a></p>
<p>19 0.41587514 <a title="169-lda-19" href="./nips-2007-A_Bayesian_LDA-based_model_for_semi-supervised_part-of-speech_tagging.html">2 nips-2007-A Bayesian LDA-based model for semi-supervised part-of-speech tagging</a></p>
<p>20 0.41532525 <a title="169-lda-20" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
