<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>144 nips-2007-On Ranking in Survival Analysis: Bounds on the Concordance Index</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-144" href="#">nips2007-144</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>144 nips-2007-On Ranking in Survival Analysis: Bounds on the Concordance Index</h1>
<br/><p>Source: <a title="nips-2007-144-pdf" href="http://papers.nips.cc/paper/3375-on-ranking-in-survival-analysis-bounds-on-the-concordance-index.pdf">pdf</a></p><p>Author: Harald Steck, Balaji Krishnapuram, Cary Dehing-oberije, Philippe Lambin, Vikas C. Raykar</p><p>Abstract: In this paper, we show that classical survival analysis involving censored data can naturally be cast as a ranking problem. The concordance index (CI), which quantiﬁes the quality of rankings, is the standard performance measure for model assessment in survival analysis. In contrast, the standard approach to learning the popular proportional hazard (PH) model is based on Cox’s partial likelihood. We devise two bounds on CI–one of which emerges directly from the properties of PH models–and optimize them directly. Our experimental results suggest that all three methods perform about equally well, with our new approach giving slightly better results. We also explain why a method designed to maximize the Cox’s partial likelihood also ends up (approximately) maximizing the CI. 1</p><p>Reference: <a title="nips-2007-144-reference" href="../nips2007_reference/nips-2007-On_Ranking_in_Survival_Analysis%3A_Bounds_on_the_Concordance_Index_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 nl  Abstract In this paper, we show that classical survival analysis involving censored data can naturally be cast as a ranking problem. [sent-10, score-1.032]
</p><p>2 The concordance index (CI), which quantiﬁes the quality of rankings, is the standard performance measure for model assessment in survival analysis. [sent-11, score-1.031]
</p><p>3 In contrast, the standard approach to learning the popular proportional hazard (PH) model is based on Cox’s partial likelihood. [sent-12, score-0.409]
</p><p>4 We also explain why a method designed to maximize the Cox’s partial likelihood also ends up (approximately) maximizing the CI. [sent-15, score-0.22]
</p><p>5 The time between a well-deﬁned starting point and the occurrence of the event is called the survival time or failure time, measured in clock time or in another appropriate scale, e. [sent-20, score-0.723]
</p><p>6 Survival time data are not amenable to standard statistical methods because of its two special features–(1) the continuous survival time often follows a skewed distribution, far from normal, and (2) a large portion of the data is censored (see Sec. [sent-23, score-0.838]
</p><p>7 In this paper we take a machine learning perspective and cast survival analysis as a ranking problem–where the task is to rank the data points based on their survival times rather than to predict the actual survival times. [sent-25, score-2.069]
</p><p>8 One of the most popular performance measures for assessing learned models in survival analysis is the Concordance Index (CI), which is similar to the Wilcoxon-Mann-Whitney statistic [13, 10] used in bi-partite ranking problems. [sent-26, score-0.796]
</p><p>9 As optimization of the CI is computationally expensive, we focus on maximizing two lower bounds on the CI, namely the log-sigmoid and the exponential bounds, which are described in Sec. [sent-28, score-0.203]
</p><p>10 Interestingly, the log-sigmoid bound arises in a natural way from the Proportional Hazard (PH) model, which is the standard model used in classical survival analysis, see Sec. [sent-30, score-0.711]
</p><p>11 Moreover, as the PH models are learned by optimizing Cox’s partial likelihood in classical survival analysis, we show in Sec. [sent-33, score-0.776]
</p><p>12 8 that maximizing this likelihood also ends up (approximately) maximizing the CI. [sent-34, score-0.168]
</p><p>13 9 show that optimizing our two lower bounds and Cox’s likelihood yields very similar results with respect to the CI, with the proposed lower bounds being slightly better. [sent-36, score-0.233]
</p><p>14 A primary focus is to build statistical models for survival time Ti∗ of individual i of a population. [sent-40, score-0.644]
</p><p>15 1  Censored data  ∗ A major problem is the fact that the period of observation Ci can be censored for many individuals i. [sent-42, score-0.208]
</p><p>16 For such cases the exact survival time may be longer than the observation period. [sent-45, score-0.628]
</p><p>17 Let xi ∈ Rd be the associated d-dimensional vector of covariates (explanatory variables) for the ith individual. [sent-50, score-0.168]
</p><p>18 In clinical studies, the covariates typically include demographic variables, such as age, gender, or race; diagnosis information like lab tests; or treatment information, e. [sent-51, score-0.16]
</p><p>19 , the cause for censoring is independent of the survival time. [sent-56, score-0.663]
</p><p>20 With the indicator function δi , which equals ∗ ∗ 1 if failure is observed (Ti∗ ≤ Ci ) and 0 if data is censored (Ti∗ > Ci ), the available training data N can be summarized as D = {Ti , xi , δi }i=1 for N patients. [sent-57, score-0.336]
</p><p>21 The objective is to learn a predictive model for the survival time as a function of the covariates. [sent-58, score-0.628]
</p><p>22 This distribution is characterized by the survival function S(t) = Pr[T > t] for t > 0, which is the probability that the individual is still alive at time t. [sent-61, score-0.644]
</p><p>23 A related function commonly used is the hazard function. [sent-62, score-0.305]
</p><p>24 If T has density function p, then the hazard function is deﬁned by λ(t) = lim∆t→0 Pr[t < T ≤ t + ∆t|T > t]/∆t = p(t)/S(t). [sent-63, score-0.287]
</p><p>25 The hazard function measures the instantaneous rate of failure, and provides more insight into the failure t mechanisms. [sent-64, score-0.337]
</p><p>26 The function Λ(t) = 0 λ(u)du is called the cumulative hazard function, and it holds that S(t) = e−Λ(t) [4]. [sent-65, score-0.287]
</p><p>27 3  Proportional hazard model  Proportional hazard (PH) models have become the standard for studying the effect of the covariates on the survival time distributions, e. [sent-67, score-1.292]
</p><p>28 Speciﬁcally, the PH model assumes a multiplicative effect of the covariates on the hazard function, i. [sent-70, score-0.377]
</p><p>29 , λ(t|x) = λ0 (t)ew  x  ,  (1)  where λ(t|x) is the hazard function of a person with covariates x; λ0 (t) is the so-called baseline hazard function (i. [sent-72, score-0.664]
</p><p>30 , when x = 0), which is typically based on the exponential or the Weibull distributions; w is a set of unknown regression parameters, and ew x is the relative hazard function. [sent-74, score-0.517]
</p><p>31 Equivalent formulations for the cumulative hazard function and the survival function include Λ(t|x) = Λ0 (t)ew 2. [sent-75, score-0.9]
</p><p>32 4  x  ,  w  and S(t|x) = e−Λ0 (t)e  x  =e  − ew  x  λ0 (t)dt  . [sent-76, score-0.163]
</p><p>33 Only a parametric assumption concerning the effect of the covariates on the hazard function is required. [sent-80, score-0.377]
</p><p>34 Parameter estimates in the PH model are obtained by maximizing Cox’s partial likelihood (of the weights) [2, 3]: ew xi L(w) = . [sent-81, score-0.42]
</p><p>35 (3) w xj Tj ≥Ti e T uncensored i  2  2 0 −2 −4 −6 −8 −10 −10 −8 −6 −4 −2  (a)  (b)  Indicator function Log−sigmoid lower bound Exponential lower bound 0 2 4 6 8 10 z  (c)  Figure 1: Order graphs representing the ranking constraints. [sent-82, score-0.645]
</p><p>36 The points are arranged in the increasing value of their survival times with the lowest being at the bottom. [sent-85, score-0.632]
</p><p>37 (c) Two concave lower bounds on the 0-1 indicator function. [sent-86, score-0.144]
</p><p>38 Each term in the product is the probability that the ith individual failed at time Ti given that exactly one failure has occurred at time Ti and all individuals for which Tj ≥ Ti are at risk of failing. [sent-87, score-0.129]
</p><p>39 The interesting properties of the Cox’s partial likelihood include: (1) due to its parametric form, it can be optimized in a computationally efﬁcient way; (2) it depends only on the ranks of the observed survival times, cf. [sent-89, score-0.754]
</p><p>40 We outline this connection to the ranking of the times Ti –and hence the concordance index–in Sec. [sent-92, score-0.54]
</p><p>41 The set of vertices V represents all the individuals, where each ﬁlled vertex indicates an observed/uncensored survival time, while an empty circle denotes a censored observation. [sent-98, score-0.817]
</p><p>42 1  Concordance index  For these reasons, the concordance index (CI) or c-index is one of the most commonly used performance measures of survival models, e. [sent-102, score-1.129]
</p><p>43 It can be interpreted as the fraction of all pairs of subjects whose predicted survival times are correctly ordered among all subjects that can actually be ordered. [sent-105, score-0.664]
</p><p>44 In other words, it is the probability of concordance between the predicted and the observed survival. [sent-106, score-0.338]
</p><p>45 It can be written as 1 1f (xi ) Ti  This index is a generalization of the Wilcoxon-Mann-Whitney statistics [13, 10] and thus of the area under the ROC curve (AUC) to regression problems in that it can (1) be applied to continuous 3  output variables and (2) account for censoring of the data. [sent-107, score-0.145]
</p><p>46 2  Maximizing the CI—The Ranking Problem  Since we evaluate the predictive accuracy of a survival model in terms of the concordance index, it is natural to formulate the learning problem to directly maximize the concordance index. [sent-111, score-1.305]
</p><p>47 Note that, while the concordance index has been used widely to evaluate a learnt model, it is not generally used as an objective function during training. [sent-112, score-0.434]
</p><p>48 As the concordance index is invariant to any monotone transformation of the survival times, the model learnt by maximizing the c-index is actually a ranking/scoring function. [sent-113, score-1.098]
</p><p>49 Our goal is to predict whether the survival time of one individual is larger than the one of another individual. [sent-114, score-0.644]
</p><p>50 Very often the doctor would like to know whether a particular kind of treatment results in an increase in the survival time and the exact absolute value of the survival time is not important. [sent-115, score-1.275]
</p><p>51 In terms of ranking problems studied in machine learning this is an N -partite ranking problem, where every data point is a class in itself. [sent-116, score-0.366]
</p><p>52 Formulating it as a ranking problem allows us to naturally incorporate the censored data. [sent-117, score-0.374]
</p><p>53 Once we have formulated it as a ranking problem we can use various ranking algorithms proposed in the machine learning literature [5, 7, 1, 12]. [sent-118, score-0.366]
</p><p>54 More formally, we would like to learn a ranking function f from a suitable function class F, such that f (xi ) > f (xj ) implies that the survival time of patient i is larger than the one of patient j. [sent-120, score-0.911]
</p><p>55 Given the data D and the order graph G, the optimal ranking function is f = arg maxf ∈F c(D, G, f ). [sent-121, score-0.196]
</p><p>56 For ease of exposition we will consider the family of linear ranking functions 1 in this paper: F = {fw }, where for any x, w ∈ Rd , fw (x) = w x. [sent-127, score-0.314]
</p><p>57 For this reason, we resort to maximizing a differentiable and concave lower bound on the 0-1 indicator function in the concordance index, cf. [sent-129, score-0.569]
</p><p>58 We will also show how these bounds relate to the classical approaches in survival analysis: as it turns out, for the family of linear ranking functions, these two approaches are closely related to the PH model commonly used in survival analysis, cf. [sent-138, score-1.48]
</p><p>59 5  Log-sigmoid lower bound  The ﬁrst subsection discusses the lower bound on the concordance index based on the log-sigmoid function. [sent-142, score-0.657]
</p><p>60 The second subsection shows that this bound arises naturally when using proportional hazard models. [sent-143, score-0.438]
</p><p>61 1  Lower bound  The sigmoid function is deﬁned as σ(z) = 1/(1+e−z ), While it is an approximation to the indicator function, it is not a lower bound. [sent-145, score-0.196]
</p><p>62 In contrast, the scaled version of the log of the sigmoid function, log [2σ(z)]/ log 2, is a lower bound on the indicator function (Fig. [sent-146, score-0.241]
</p><p>63 The ranking function then is of the form f (x) = N αi k(x, xi ) where k is the kernel of the RHKS H. [sent-151, score-0.261]
</p><p>64 Given the linear ranking function fw (x) = w x, the bound cLS becomes cLS (w) =  1 |E|  1 + (log σ[w (xj − xi )]/log 2). [sent-157, score-0.451]
</p><p>65 2  Connection to the PH model  The concordance index can be interpreted as the probability of correct ranking (as deﬁned by the given order graph) given a function f . [sent-160, score-0.601]
</p><p>66 Under the assumption that each pair (j, i) is independent of any other pair, the log-likelihood reads L(fw , D, G) = log  Pr [fw (xi ) < fw (xj )|w] . [sent-162, score-0.169]
</p><p>67 ), it provides a lower bound on the concordance index. [sent-166, score-0.448]
</p><p>68 While the probability of correct pairwise ordering, Pr [fw (xi ) < fw (xj )|w], is often chosen to be sigmoid in the ranking literature [1], we show in the following that the sigmoid function arises naturally in the context of PH models. [sent-167, score-0.458]
</p><p>69 Let T (w x) denote the survival time for the patient with covariates x or relative log-hazard w x. [sent-168, score-0.768]
</p><p>70 A larger hazard corresponds to a smaller survival time, cf. [sent-169, score-0.9]
</p><p>71 2 of the PH model, we continue the manipulations: Pr [fw (xi ) < fw (xj )|w]  = −ew =  ∞ −Λ (t) ew 0  xi  e  e  xj +ew  xi  Λ0 (t)dt  0 w xi  ew xj  + ew  xi  = σ[w (xi − xj )]. [sent-174, score-1.226]
</p><p>72 5  6  Exponential lower bound  The exponential 1 − e−z can serve as an alternative lower bound on the step indicator function (see Fig. [sent-179, score-0.305]
</p><p>73 The concordance index can then be lower-bounded by c ≥  1 |E|  1 − e−[f (xj )−f (xi )] ≡ cE . [sent-181, score-0.418]
</p><p>74 9 (log-sigmoid bound) is given by w cLSreg (w) = −λw − |E| log 2 Eij (xi − xj )σ wT (xi − xj ) , and the gradient of Eq. [sent-189, score-0.228]
</p><p>75 Eij (xi − xj )e |E|  8  Is Cox’s partial likelihood a lower bound on the CI ? [sent-191, score-0.336]
</p><p>76 While our proposed method was formulated to explicitly maximize a lower bound on the concordance index, the Coxs method maximized the partial likelihood. [sent-194, score-0.551]
</p><p>77 One suspects whether Coxs partial likelihood itself is a lower bound on the concordance index. [sent-195, score-0.576]
</p><p>78 The argument presented below could give an indication as to why a method which maximizes the partial likelihood also ends up (approximately) maximizing the concordance index. [sent-196, score-0.542]
</p><p>79 We re-write the exponential bound on the CI for proportional hazard models from Sec. [sent-197, score-0.433]
</p><p>80 6 cE (w)  =  =  1 |E| 1−  1 − e−w  (xi −xj )  Ti uncensored Tj ≥Ti  No |E|  1 No  1/zi  ,  =1−  1 |E|  where zi =  e−w  Tj ≥Ti  Ti uncensored  [  ew  xj  ]  Tj ≥Ti  Ti uncensored  ew  xi  xi  ew  xj  ∈ [0, 1]. [sent-198, score-1.348]
</p><p>81 , no two survival times are identical, analogous to Cox’s partial likelihood approach (cf. [sent-201, score-0.76]
</p><p>82 The number of uncensored observations is denoted by No . [sent-205, score-0.144]
</p><p>83 The Cox’s partial likelihood can be written in terms of zi as L(w) = Ti uncensored zi = zi No , where zi geom denotes the geometric mean of geom the zi with uncensored Ti . [sent-206, score-0.849]
</p><p>84 Using the inequality zi ≥ min zi the concordance index can be bounded as No 1 c≥1− . [sent-207, score-0.568]
</p><p>85 (17) |E| min zi This says maximizing min zi maximizes a lower bound on the concordance index. [sent-208, score-0.649]
</p><p>86 Since max zi = 1 (because zi = 1 for the largest uncensored Ti ), maximizing min zi can be expected to approximately maximize the geometric mean of zi , and hence the Cox’s partial likelihood. [sent-210, score-0.613]
</p><p>87 2%  Experiments  In this section we compare the performance of the two different lower bounds on the CI—the logsigmoid, exponential, and Cox’s partial likelihood—on ﬁve medical data sets. [sent-224, score-0.212]
</p><p>88 A substantial amount of data is censored and also missing. [sent-227, score-0.175]
</p><p>89 The MAASTRO dataset concerns the survival time of non-small cell lung cancer patients, which we analyzed as part of our collaboration. [sent-228, score-0.628]
</p><p>90 3  Results  The performance was evaluated in terms of the concordance index and the results are tabulated in Table 2. [sent-243, score-0.418]
</p><p>91 We compare the following methods–(1) Cox’s partial likelihood method, and (2) the proposed ranking methods with log-sigmoid and exponential lower bounds. [sent-244, score-0.414]
</p><p>92 The following observations can be made–(1) The proposed linear ranking method performs slightly better than the Cox’s partial likelihood method, but the difference does not appear signiﬁcant. [sent-245, score-0.311]
</p><p>93 This agrees with our insights that Cox’s partial likelihood may also end up maximizing the CI. [sent-246, score-0.179]
</p><p>94 (2) The exponential bound shows slightly better performance than the log-sigmoid bound, which may indicate that the tightness of the bound for positive z in Fig. [sent-247, score-0.17]
</p><p>95 10  Conclusions  In this paper, we outlined several approaches for maximizing the concordance index, the standard performance measure in survival analysis when cast as a ranking problem. [sent-250, score-1.213]
</p><p>96 We showed that, for the widely-used proportional hazard models, the log-sigmoid function arises as a natural lower bound on the concordance index. [sent-251, score-0.792]
</p><p>97 We presented an approach for directly optimizing this lower bound in a computationally efﬁcient way. [sent-252, score-0.141]
</p><p>98 Apart from that, we showed that maximizing Cox’s partial likelihood can be understood as (approximately) maximizing a lower bound on the concordance index, which explains the high CI-scores of proportional hazard models observed in practice. [sent-254, score-1.0]
</p><p>99 55  MAASTRO  Cox PH log-sigmoid exponential SUPPORT-1  Cox PH log-sigmoid exponential SUPPORT-2  Cox PH log-sigmoid exponential SUPPORT-4  Cox PH log-sigmoid exponential MELANOMA  Cox PH log-sigmoid exponential  Acknowledgements We are grateful to R. [sent-342, score-0.26]
</p><p>100 The support prognostic model: Objective estimates of survival for seriously ill hospitalized adults. [sent-403, score-0.639]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('survival', 0.613), ('concordance', 0.338), ('hazard', 0.287), ('cox', 0.266), ('ph', 0.194), ('ranking', 0.183), ('censored', 0.175), ('ew', 0.163), ('ti', 0.158), ('uncensored', 0.144), ('ci', 0.131), ('fw', 0.131), ('eij', 0.117), ('xj', 0.098), ('covariates', 0.09), ('partial', 0.087), ('index', 0.08), ('xi', 0.078), ('zi', 0.075), ('tj', 0.068), ('bound', 0.059), ('cls', 0.057), ('maastro', 0.057), ('sigmoid', 0.053), ('exponential', 0.052), ('maximizing', 0.051), ('lower', 0.051), ('patient', 0.05), ('censoring', 0.05), ('failure', 0.05), ('pr', 0.047), ('coxs', 0.043), ('melanoma', 0.043), ('likelihood', 0.041), ('clinical', 0.038), ('medical', 0.038), ('bounds', 0.036), ('proportional', 0.035), ('indicator', 0.033), ('individuals', 0.033), ('cereg', 0.029), ('clsreg', 0.029), ('geom', 0.029), ('harrell', 0.029), ('maastricht', 0.029), ('raykar', 0.029), ('cg', 0.029), ('cast', 0.028), ('dt', 0.027), ('ce', 0.026), ('ends', 0.025), ('concave', 0.024), ('reads', 0.023), ('arises', 0.022), ('std', 0.02), ('holdout', 0.02), ('skewed', 0.02), ('treatment', 0.019), ('subsection', 0.019), ('times', 0.019), ('optimizing', 0.018), ('ordering', 0.018), ('regularization', 0.018), ('commonly', 0.018), ('patients', 0.018), ('gradient', 0.017), ('classical', 0.017), ('regularized', 0.017), ('learnt', 0.016), ('circle', 0.016), ('subjects', 0.016), ('missing', 0.016), ('individual', 0.016), ('naturally', 0.016), ('maximize', 0.016), ('auc', 0.015), ('time', 0.015), ('approximately', 0.015), ('regression', 0.015), ('log', 0.015), ('occurrence', 0.015), ('ve', 0.014), ('preference', 0.014), ('empty', 0.013), ('differentiable', 0.013), ('computationally', 0.013), ('maxf', 0.013), ('bharat', 0.013), ('harald', 0.013), ('death', 0.013), ('ill', 0.013), ('race', 0.013), ('imputation', 0.013), ('demographic', 0.013), ('deeds', 0.013), ('hamilton', 0.013), ('lazier', 0.013), ('renshaw', 0.013), ('shaked', 0.013), ('seriously', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="144-tfidf-1" href="./nips-2007-On_Ranking_in_Survival_Analysis%3A_Bounds_on_the_Concordance_Index.html">144 nips-2007-On Ranking in Survival Analysis: Bounds on the Concordance Index</a></p>
<p>Author: Harald Steck, Balaji Krishnapuram, Cary Dehing-oberije, Philippe Lambin, Vikas C. Raykar</p><p>Abstract: In this paper, we show that classical survival analysis involving censored data can naturally be cast as a ranking problem. The concordance index (CI), which quantiﬁes the quality of rankings, is the standard performance measure for model assessment in survival analysis. In contrast, the standard approach to learning the popular proportional hazard (PH) model is based on Cox’s partial likelihood. We devise two bounds on CI–one of which emerges directly from the properties of PH models–and optimize them directly. Our experimental results suggest that all three methods perform about equally well, with our new approach giving slightly better results. We also explain why a method designed to maximize the Cox’s partial likelihood also ends up (approximately) maximizing the CI. 1</p><p>2 0.11466832 <a title="144-tfidf-2" href="./nips-2007-Non-parametric_Modeling_of_Partially_Ranked_Data.html">142 nips-2007-Non-parametric Modeling of Partially Ranked Data</a></p>
<p>Author: Guy Lebanon, Yi Mao</p><p>Abstract: Statistical models on full and partial rankings of n items are often of limited practical use for large n due to computational consideration. We explore the use of non-parametric models for partially ranked data and derive efﬁcient procedures for their use for large n. The derivations are largely possible through combinatorial and algebraic manipulations based on the lattice of partial rankings. In particular, we demonstrate for the ﬁrst time a non-parametric coherent and consistent model capable of efﬁciently aggregating partially ranked data of different types. 1</p><p>3 0.082344912 <a title="144-tfidf-3" href="./nips-2007-A_General_Boosting_Method_and_its_Application_to_Learning_Ranking_Functions_for_Web_Search.html">6 nips-2007-A General Boosting Method and its Application to Learning Ranking Functions for Web Search</a></p>
<p>Author: Zhaohui Zheng, Hongyuan Zha, Tong Zhang, Olivier Chapelle, Keke Chen, Gordon Sun</p><p>Abstract: We present a general boosting method extending functional gradient boosting to optimize complex loss functions that are encountered in many machine learning problems. Our approach is based on optimization of quadratic upper bounds of the loss functions which allows us to present a rigorous convergence analysis of the algorithm. More importantly, this general framework enables us to use a standard regression base learner such as single regression tree for £tting any loss function. We illustrate an application of the proposed method in learning ranking functions for Web search by combining both preference data and labeled data for training. We present experimental results for Web search using data from a commercial search engine that show signi£cant improvements of our proposed methods over some existing methods. 1</p><p>4 0.071563721 <a title="144-tfidf-4" href="./nips-2007-Privacy-Preserving_Belief_Propagation_and_Sampling.html">157 nips-2007-Privacy-Preserving Belief Propagation and Sampling</a></p>
<p>Author: Michael Kearns, Jinsong Tan, Jennifer Wortman</p><p>Abstract: We provide provably privacy-preserving versions of belief propagation, Gibbs sampling, and other local algorithms — distributed multiparty protocols in which each party or vertex learns only its ﬁnal local value, and absolutely nothing else. 1</p><p>5 0.07009659 <a title="144-tfidf-5" href="./nips-2007-McRank%3A_Learning_to_Rank_Using_Multiple_Classification_and_Gradient_Boosting.html">126 nips-2007-McRank: Learning to Rank Using Multiple Classification and Gradient Boosting</a></p>
<p>Author: Ping Li, Qiang Wu, Christopher J. Burges</p><p>Abstract: We cast the ranking problem as (1) multiple classiﬁcation (“Mc”) (2) multiple ordinal classiﬁcation, which lead to computationally tractable learning algorithms for relevance ranking in Web search. We consider the DCG criterion (discounted cumulative gain), a standard quality measure in information retrieval. Our approach is motivated by the fact that perfect classiﬁcations result in perfect DCG scores and the DCG errors are bounded by classiﬁcation errors. We propose using the Expected Relevance to convert class probabilities into ranking scores. The class probabilities are learned using a gradient boosting tree algorithm. Evaluations on large-scale datasets show that our approach can improve LambdaRank [5] and the regressions-based ranker [6], in terms of the (normalized) DCG scores. An efﬁcient implementation of the boosting tree algorithm is also presented.</p><p>6 0.065440163 <a title="144-tfidf-6" href="./nips-2007-The_Distribution_Family_of_Similarity_Distances.html">193 nips-2007-The Distribution Family of Similarity Distances</a></p>
<p>7 0.063967198 <a title="144-tfidf-7" href="./nips-2007-Fixing_Max-Product%3A_Convergent_Message_Passing_Algorithms_for_MAP_LP-Relaxations.html">92 nips-2007-Fixing Max-Product: Convergent Message Passing Algorithms for MAP LP-Relaxations</a></p>
<p>8 0.059136894 <a title="144-tfidf-8" href="./nips-2007-Boosting_the_Area_under_the_ROC_Curve.html">39 nips-2007-Boosting the Area under the ROC Curve</a></p>
<p>9 0.056702729 <a title="144-tfidf-9" href="./nips-2007-COFI_RANK_-_Maximum_Margin_Matrix_Factorization_for_Collaborative_Ranking.html">41 nips-2007-COFI RANK - Maximum Margin Matrix Factorization for Collaborative Ranking</a></p>
<p>10 0.049047358 <a title="144-tfidf-10" href="./nips-2007-Regulator_Discovery_from_Gene_Expression_Time_Series_of_Malaria_Parasites%3A_a_Hierachical_Approach.html">167 nips-2007-Regulator Discovery from Gene Expression Time Series of Malaria Parasites: a Hierachical Approach</a></p>
<p>11 0.046071175 <a title="144-tfidf-11" href="./nips-2007-Ensemble_Clustering_using_Semidefinite_Programming.html">80 nips-2007-Ensemble Clustering using Semidefinite Programming</a></p>
<p>12 0.039939355 <a title="144-tfidf-12" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>13 0.037154362 <a title="144-tfidf-13" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>14 0.036617786 <a title="144-tfidf-14" href="./nips-2007-Evaluating_Search_Engines_by_Modeling_the_Relationship_Between_Relevance_and_Clicks.html">83 nips-2007-Evaluating Search Engines by Modeling the Relationship Between Relevance and Clicks</a></p>
<p>15 0.036393359 <a title="144-tfidf-15" href="./nips-2007-Stable_Dual_Dynamic_Programming.html">185 nips-2007-Stable Dual Dynamic Programming</a></p>
<p>16 0.035989188 <a title="144-tfidf-16" href="./nips-2007-Predictive_Matrix-Variate_t_Models.html">156 nips-2007-Predictive Matrix-Variate t Models</a></p>
<p>17 0.034871377 <a title="144-tfidf-17" href="./nips-2007-Catching_Up_Faster_in_Bayesian_Model_Selection_and_Model_Averaging.html">44 nips-2007-Catching Up Faster in Bayesian Model Selection and Model Averaging</a></p>
<p>18 0.03452643 <a title="144-tfidf-18" href="./nips-2007-The_Noisy-Logical_Distribution_and_its_Application_to_Causal_Inference.html">198 nips-2007-The Noisy-Logical Distribution and its Application to Causal Inference</a></p>
<p>19 0.034197114 <a title="144-tfidf-19" href="./nips-2007-Stability_Bounds_for_Non-i.i.d._Processes.html">184 nips-2007-Stability Bounds for Non-i.i.d. Processes</a></p>
<p>20 0.033906229 <a title="144-tfidf-20" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.116), (1, 0.012), (2, -0.063), (3, 0.04), (4, 0.007), (5, -0.024), (6, 0.053), (7, -0.042), (8, 0.032), (9, -0.064), (10, -0.05), (11, -0.014), (12, 0.157), (13, 0.059), (14, 0.032), (15, 0.009), (16, 0.008), (17, 0.042), (18, -0.066), (19, 0.028), (20, 0.048), (21, -0.032), (22, -0.035), (23, -0.073), (24, -0.056), (25, -0.008), (26, -0.006), (27, -0.076), (28, -0.067), (29, 0.006), (30, 0.009), (31, 0.067), (32, 0.047), (33, -0.024), (34, 0.074), (35, 0.031), (36, 0.079), (37, -0.004), (38, -0.014), (39, 0.172), (40, -0.008), (41, 0.138), (42, 0.024), (43, 0.199), (44, -0.017), (45, 0.124), (46, -0.029), (47, 0.042), (48, -0.058), (49, -0.09)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.926934 <a title="144-lsi-1" href="./nips-2007-On_Ranking_in_Survival_Analysis%3A_Bounds_on_the_Concordance_Index.html">144 nips-2007-On Ranking in Survival Analysis: Bounds on the Concordance Index</a></p>
<p>Author: Harald Steck, Balaji Krishnapuram, Cary Dehing-oberije, Philippe Lambin, Vikas C. Raykar</p><p>Abstract: In this paper, we show that classical survival analysis involving censored data can naturally be cast as a ranking problem. The concordance index (CI), which quantiﬁes the quality of rankings, is the standard performance measure for model assessment in survival analysis. In contrast, the standard approach to learning the popular proportional hazard (PH) model is based on Cox’s partial likelihood. We devise two bounds on CI–one of which emerges directly from the properties of PH models–and optimize them directly. Our experimental results suggest that all three methods perform about equally well, with our new approach giving slightly better results. We also explain why a method designed to maximize the Cox’s partial likelihood also ends up (approximately) maximizing the CI. 1</p><p>2 0.61352062 <a title="144-lsi-2" href="./nips-2007-Non-parametric_Modeling_of_Partially_Ranked_Data.html">142 nips-2007-Non-parametric Modeling of Partially Ranked Data</a></p>
<p>Author: Guy Lebanon, Yi Mao</p><p>Abstract: Statistical models on full and partial rankings of n items are often of limited practical use for large n due to computational consideration. We explore the use of non-parametric models for partially ranked data and derive efﬁcient procedures for their use for large n. The derivations are largely possible through combinatorial and algebraic manipulations based on the lattice of partial rankings. In particular, we demonstrate for the ﬁrst time a non-parametric coherent and consistent model capable of efﬁciently aggregating partially ranked data of different types. 1</p><p>3 0.53053617 <a title="144-lsi-3" href="./nips-2007-Privacy-Preserving_Belief_Propagation_and_Sampling.html">157 nips-2007-Privacy-Preserving Belief Propagation and Sampling</a></p>
<p>Author: Michael Kearns, Jinsong Tan, Jennifer Wortman</p><p>Abstract: We provide provably privacy-preserving versions of belief propagation, Gibbs sampling, and other local algorithms — distributed multiparty protocols in which each party or vertex learns only its ﬁnal local value, and absolutely nothing else. 1</p><p>4 0.46730506 <a title="144-lsi-4" href="./nips-2007-The_Distribution_Family_of_Similarity_Distances.html">193 nips-2007-The Distribution Family of Similarity Distances</a></p>
<p>Author: Gertjan Burghouts, Arnold Smeulders, Jan-mark Geusebroek</p><p>Abstract: Assessing similarity between features is a key step in object recognition and scene categorization tasks. We argue that knowledge on the distribution of distances generated by similarity functions is crucial in deciding whether features are similar or not. Intuitively one would expect that similarities between features could arise from any distribution. In this paper, we will derive the contrary, and report the theoretical result that Lp -norms –a class of commonly applied distance metrics– from one feature vector to other vectors are Weibull-distributed if the feature values are correlated and non-identically distributed. Besides these assumptions being realistic for images, we experimentally show them to hold for various popular feature extraction algorithms, for a diverse range of images. This fundamental insight opens new directions in the assessment of feature similarity, with projected improvements in object and scene recognition algorithms. 1</p><p>5 0.43681395 <a title="144-lsi-5" href="./nips-2007-A_General_Boosting_Method_and_its_Application_to_Learning_Ranking_Functions_for_Web_Search.html">6 nips-2007-A General Boosting Method and its Application to Learning Ranking Functions for Web Search</a></p>
<p>Author: Zhaohui Zheng, Hongyuan Zha, Tong Zhang, Olivier Chapelle, Keke Chen, Gordon Sun</p><p>Abstract: We present a general boosting method extending functional gradient boosting to optimize complex loss functions that are encountered in many machine learning problems. Our approach is based on optimization of quadratic upper bounds of the loss functions which allows us to present a rigorous convergence analysis of the algorithm. More importantly, this general framework enables us to use a standard regression base learner such as single regression tree for £tting any loss function. We illustrate an application of the proposed method in learning ranking functions for Web search by combining both preference data and labeled data for training. We present experimental results for Web search using data from a commercial search engine that show signi£cant improvements of our proposed methods over some existing methods. 1</p><p>6 0.43047565 <a title="144-lsi-6" href="./nips-2007-Evaluating_Search_Engines_by_Modeling_the_Relationship_Between_Relevance_and_Clicks.html">83 nips-2007-Evaluating Search Engines by Modeling the Relationship Between Relevance and Clicks</a></p>
<p>7 0.42432916 <a title="144-lsi-7" href="./nips-2007-COFI_RANK_-_Maximum_Margin_Matrix_Factorization_for_Collaborative_Ranking.html">41 nips-2007-COFI RANK - Maximum Margin Matrix Factorization for Collaborative Ranking</a></p>
<p>8 0.41180998 <a title="144-lsi-8" href="./nips-2007-Regulator_Discovery_from_Gene_Expression_Time_Series_of_Malaria_Parasites%3A_a_Hierachical_Approach.html">167 nips-2007-Regulator Discovery from Gene Expression Time Series of Malaria Parasites: a Hierachical Approach</a></p>
<p>9 0.39962763 <a title="144-lsi-9" href="./nips-2007-Fixing_Max-Product%3A_Convergent_Message_Passing_Algorithms_for_MAP_LP-Relaxations.html">92 nips-2007-Fixing Max-Product: Convergent Message Passing Algorithms for MAP LP-Relaxations</a></p>
<p>10 0.36682582 <a title="144-lsi-10" href="./nips-2007-McRank%3A_Learning_to_Rank_Using_Multiple_Classification_and_Gradient_Boosting.html">126 nips-2007-McRank: Learning to Rank Using Multiple Classification and Gradient Boosting</a></p>
<p>11 0.35332549 <a title="144-lsi-11" href="./nips-2007-Nearest-Neighbor-Based_Active_Learning_for_Rare_Category_Detection.html">139 nips-2007-Nearest-Neighbor-Based Active Learning for Rare Category Detection</a></p>
<p>12 0.34836507 <a title="144-lsi-12" href="./nips-2007-Discriminative_Keyword_Selection_Using_Support_Vector_Machines.html">71 nips-2007-Discriminative Keyword Selection Using Support Vector Machines</a></p>
<p>13 0.33306676 <a title="144-lsi-13" href="./nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">79 nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<p>14 0.31326991 <a title="144-lsi-14" href="./nips-2007-Predictive_Matrix-Variate_t_Models.html">156 nips-2007-Predictive Matrix-Variate t Models</a></p>
<p>15 0.3108831 <a title="144-lsi-15" href="./nips-2007-Learning_with_Tree-Averaged_Densities_and_Distributions.html">119 nips-2007-Learning with Tree-Averaged Densities and Distributions</a></p>
<p>16 0.30708265 <a title="144-lsi-16" href="./nips-2007-The_Infinite_Gamma-Poisson_Feature_Model.html">196 nips-2007-The Infinite Gamma-Poisson Feature Model</a></p>
<p>17 0.3056621 <a title="144-lsi-17" href="./nips-2007-A_New_View_of_Automatic_Relevance_Determination.html">8 nips-2007-A New View of Automatic Relevance Determination</a></p>
<p>18 0.29672694 <a title="144-lsi-18" href="./nips-2007-Bayesian_Agglomerative_Clustering_with_Coalescents.html">31 nips-2007-Bayesian Agglomerative Clustering with Coalescents</a></p>
<p>19 0.28627047 <a title="144-lsi-19" href="./nips-2007-Cooled_and_Relaxed_Survey_Propagation_for_MRFs.html">64 nips-2007-Cooled and Relaxed Survey Propagation for MRFs</a></p>
<p>20 0.27701136 <a title="144-lsi-20" href="./nips-2007-Heterogeneous_Component_Analysis.html">96 nips-2007-Heterogeneous Component Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.016), (13, 0.024), (16, 0.011), (18, 0.016), (21, 0.076), (31, 0.432), (34, 0.07), (35, 0.016), (47, 0.051), (49, 0.021), (83, 0.088), (85, 0.016), (90, 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83287609 <a title="144-lda-1" href="./nips-2007-Random_Sampling_of_States_in_Dynamic_Programming.html">162 nips-2007-Random Sampling of States in Dynamic Programming</a></p>
<p>Author: Chris Atkeson, Benjamin Stephens</p><p>Abstract: We combine three threads of research on approximate dynamic programming: sparse random sampling of states, value function and policy approximation using local models, and using local trajectory optimizers to globally optimize a policy and associated value function. Our focus is on ﬁnding steady state policies for deterministic time invariant discrete time control problems with continuous states and actions often found in robotics. In this paper we show that we can now solve problems we couldn’t solve previously. 1</p><p>same-paper 2 0.78100985 <a title="144-lda-2" href="./nips-2007-On_Ranking_in_Survival_Analysis%3A_Bounds_on_the_Concordance_Index.html">144 nips-2007-On Ranking in Survival Analysis: Bounds on the Concordance Index</a></p>
<p>Author: Harald Steck, Balaji Krishnapuram, Cary Dehing-oberije, Philippe Lambin, Vikas C. Raykar</p><p>Abstract: In this paper, we show that classical survival analysis involving censored data can naturally be cast as a ranking problem. The concordance index (CI), which quantiﬁes the quality of rankings, is the standard performance measure for model assessment in survival analysis. In contrast, the standard approach to learning the popular proportional hazard (PH) model is based on Cox’s partial likelihood. We devise two bounds on CI–one of which emerges directly from the properties of PH models–and optimize them directly. Our experimental results suggest that all three methods perform about equally well, with our new approach giving slightly better results. We also explain why a method designed to maximize the Cox’s partial likelihood also ends up (approximately) maximizing the CI. 1</p><p>3 0.78009617 <a title="144-lda-3" href="./nips-2007-Variational_inference_for_Markov_jump_processes.html">214 nips-2007-Variational inference for Markov jump processes</a></p>
<p>Author: Manfred Opper, Guido Sanguinetti</p><p>Abstract: Markov jump processes play an important role in a large number of application domains. However, realistic systems are analytically intractable and they have traditionally been analysed using simulation based techniques, which do not provide a framework for statistical inference. We propose a mean ﬁeld approximation to perform posterior inference and parameter estimation. The approximation allows a practical solution to the inference problem, while still retaining a good degree of accuracy. We illustrate our approach on two biologically motivated systems.</p><p>4 0.6879527 <a title="144-lda-4" href="./nips-2007-Ensemble_Clustering_using_Semidefinite_Programming.html">80 nips-2007-Ensemble Clustering using Semidefinite Programming</a></p>
<p>Author: Vikas Singh, Lopamudra Mukherjee, Jiming Peng, Jinhui Xu</p><p>Abstract: We consider the ensemble clustering problem where the task is to ‘aggregate’ multiple clustering solutions into a single consolidated clustering that maximizes the shared information among given clustering solutions. We obtain several new results for this problem. First, we note that the notion of agreement under such circumstances can be better captured using an agreement measure based on a 2D string encoding rather than voting strategy based methods proposed in literature. Using this generalization, we ﬁrst derive a nonlinear optimization model to maximize the new agreement measure. We then show that our optimization problem can be transformed into a strict 0-1 Semideﬁnite Program (SDP) via novel convexiﬁcation techniques which can subsequently be relaxed to a polynomial time solvable SDP. Our experiments indicate improvements not only in terms of the proposed agreement measure but also the existing agreement measures based on voting strategies. We discuss evaluations on clustering and image segmentation databases. 1</p><p>5 0.46409801 <a title="144-lda-5" href="./nips-2007-Receding_Horizon_Differential_Dynamic_Programming.html">163 nips-2007-Receding Horizon Differential Dynamic Programming</a></p>
<p>Author: Yuval Tassa, Tom Erez, William D. Smart</p><p>Abstract: The control of high-dimensional, continuous, non-linear dynamical systems is a key problem in reinforcement learning and control. Local, trajectory-based methods, using techniques such as Differential Dynamic Programming (DDP), are not directly subject to the curse of dimensionality, but generate only local controllers. In this paper,we introduce Receding Horizon DDP (RH-DDP), an extension to the classic DDP algorithm, which allows us to construct stable and robust controllers based on a library of local-control trajectories. We demonstrate the effectiveness of our approach on a series of high-dimensional problems using a simulated multi-link swimming robot. These experiments show that our approach effectively circumvents dimensionality issues, and is capable of dealing with problems of (at least) 24 state and 9 action dimensions. 1</p><p>6 0.42576805 <a title="144-lda-6" href="./nips-2007-Reinforcement_Learning_in_Continuous_Action_Spaces_through_Sequential_Monte_Carlo_Methods.html">168 nips-2007-Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods</a></p>
<p>7 0.41220853 <a title="144-lda-7" href="./nips-2007-Selecting_Observations_against_Adversarial_Objectives.html">174 nips-2007-Selecting Observations against Adversarial Objectives</a></p>
<p>8 0.38767871 <a title="144-lda-8" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>9 0.38128507 <a title="144-lda-9" href="./nips-2007-Measuring_Neural_Synchrony_by_Message_Passing.html">127 nips-2007-Measuring Neural Synchrony by Message Passing</a></p>
<p>10 0.38080883 <a title="144-lda-10" href="./nips-2007-New_Outer_Bounds_on_the_Marginal_Polytope.html">141 nips-2007-New Outer Bounds on the Marginal Polytope</a></p>
<p>11 0.38061726 <a title="144-lda-11" href="./nips-2007-Consistent_Minimization_of_Clustering_Objective_Functions.html">58 nips-2007-Consistent Minimization of Clustering Objective Functions</a></p>
<p>12 0.38031512 <a title="144-lda-12" href="./nips-2007-Efficient_Convex_Relaxation_for_Transductive_Support_Vector_Machine.html">76 nips-2007-Efficient Convex Relaxation for Transductive Support Vector Machine</a></p>
<p>13 0.38001186 <a title="144-lda-13" href="./nips-2007-Locality_and_low-dimensions_in_the_prediction_of_natural_experience_from_fMRI.html">122 nips-2007-Locality and low-dimensions in the prediction of natural experience from fMRI</a></p>
<p>14 0.37876225 <a title="144-lda-14" href="./nips-2007-Competition_Adds_Complexity.html">52 nips-2007-Competition Adds Complexity</a></p>
<p>15 0.37786809 <a title="144-lda-15" href="./nips-2007-Optimistic_Linear_Programming_gives_Logarithmic_Regret_for_Irreducible_MDPs.html">151 nips-2007-Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs</a></p>
<p>16 0.37727812 <a title="144-lda-16" href="./nips-2007-Infinite_State_Bayes-Nets_for_Structured_Domains.html">105 nips-2007-Infinite State Bayes-Nets for Structured Domains</a></p>
<p>17 0.37486121 <a title="144-lda-17" href="./nips-2007-A_learning_framework_for_nearest_neighbor_search.html">16 nips-2007-A learning framework for nearest neighbor search</a></p>
<p>18 0.3738257 <a title="144-lda-18" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>19 0.37174782 <a title="144-lda-19" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>20 0.36884826 <a title="144-lda-20" href="./nips-2007-Ultrafast_Monte_Carlo_for_Statistical_Summations.html">209 nips-2007-Ultrafast Monte Carlo for Statistical Summations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
