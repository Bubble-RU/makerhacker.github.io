<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>24 nips-2007-An Analysis of Inference with the Universum</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-24" href="#">nips2007-24</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>24 nips-2007-An Analysis of Inference with the Universum</h1>
<br/><p>Source: <a title="nips-2007-24-pdf" href="http://papers.nips.cc/paper/3231-an-analysis-of-inference-with-the-universum.pdf">pdf</a></p><p>Author: Olivier Chapelle, Alekh Agarwal, Fabian H. Sinz, Bernhard Schölkopf</p><p>Abstract: We study a pattern classiﬁcation algorithm which has recently been proposed by Vapnik and coworkers. It builds on a new inductive principle which assumes that in addition to positive and negative data, a third class of data is available, termed the Universum. We assay the behavior of the algorithm by establishing links with Fisher discriminant analysis and oriented PCA, as well as with an SVM in a projected subspace (or, equivalently, with a data-dependent reduced kernel). We also provide experimental results. 1</p><p>Reference: <a title="nips-2007-24-reference" href="../nips2007_reference/nips-2007-An_Analysis_of_Inference_with_the_Universum_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Bernhard Sch¨ lkopf o Max Planck Institute for biological Cybernetics Spemannstrasse 38, 72076, T¨ bingen, Germany u bs@tuebingen. [sent-8, score-0.028]
</p><p>2 We assay the behavior of the algorithm by establishing links with Fisher discriminant analysis and oriented PCA, as well as with an SVM in a projected subspace (or, equivalently, with a data-dependent reduced kernel). [sent-12, score-0.097]
</p><p>3 1  Introduction  Learning algorithms need to make assumptions about the problem domain in order to generalise well. [sent-14, score-0.029]
</p><p>4 These assumptions are usually encoded in the regulariser or the prior. [sent-15, score-0.048]
</p><p>5 More elaborate prior knowledge, often needed for a good performance, can be hard to encode in a regulariser or a prior that is computationally efﬁcient too. [sent-18, score-0.048]
</p><p>6 A prominent example of data-dependent regularisation is semisupervised learning [1], where an additional set of unlabelled data, assumed to follow the same distribution as the training inputs, is tied to the regulariser using the so-called cluster assumption. [sent-20, score-0.105]
</p><p>7 A novel form of data-dependent regularisation was recently proposed by [11]. [sent-21, score-0.036]
</p><p>8 The additional dataset for this approach is explicitly not from the same distribution as the labelled data, but represents a third — neither — class. [sent-22, score-0.127]
</p><p>9 This kind of dataset was ﬁrst proposed by Vapnik [10] under the name Universum, owing its name to the intuition that the Universum captures a general backdrop against which a problem at hand is solved. [sent-23, score-0.088]
</p><p>10 According to Vapnik, a suitable set for this purpose can be thought of as a set of examples that belong to the same problem framework, but about which the resulting decision function should not make a strong statement. [sent-24, score-0.039]
</p><p>11 Although initially proposed for transductive inference, the authors of [11] proposed an inductive classiﬁer where the decision surface is chosen such that the Universum examples are located close to it. [sent-25, score-0.039]
</p><p>12 Although the authors showed that different choices of Universa and loss functions lead to certain known regularisers as special cases of their implementation, there are still a few unanswered questions. [sent-27, score-0.056]
</p><p>13 On the other hand, except in special cases, the inﬂuence of the Universum data on the resulting decision hyperplane and therefore criteria for a good choice of a Universum is not known. [sent-29, score-0.062]
</p><p>14 In the present paper we would like to address the second question by analysing the inﬂuence of the Universum data on the resulting function in the implementation of [11] as well as in a least squares version of it which we derive in section 2. [sent-30, score-0.06]
</p><p>15 Clarifying the regularising inﬂuence of the Universum on the solution of the SVM can give valuable insight into which set of data points might be a helpful Universum and how to obtain it. [sent-31, score-0.038]
</p><p>16 After brieﬂy deriving the algorithms in section 2 we show in section 3 that the algorithm of [11] pushes the normal of the hyperplane into the orthogonal complement of the subspace spanned by the principal directions of the Universum set. [sent-33, score-0.306]
</p><p>17 Furthermore, we demonstrate that the least squares version of the Universum algorithm is equivalent to a hybrid between kernel Fisher Discriminant Analysis and kernel Oriented Principal Component Analysis. [sent-34, score-0.123]
</p><p>18 In section 4, we validate our analysis on toy experiments and give an example how to use the geometric and algorithmic intuition gained from the analysis to construct a Universum set for a real world problem. [sent-35, score-0.029]
</p><p>19 , (xm , ym )} be the set of labelled examples and let U = {z1 , . [sent-41, score-0.118]
</p><p>20 Using the hinge loss Ha [t] = max{0, a − t} and fw,b (x) = w, x + b, a standard SVM can compactly be formulated as m  min w,b  1 ||w||2 + CL H1 [yi fw,b (xi )]. [sent-45, score-0.069]
</p><p>21 2  The Least Squares U-SVM  The derivation of the least squares U-SVM starts with the same general regularised error minimisation problem 1 CL min ||w||2 + w,b 2 2  m  CU Qyi [fw,b (x)] + 2 i=1  q  Q0 [fw,b (zj )]. [sent-49, score-0.083]
</p><p>22 (2)  j=1  Instead of using the hinge loss, we employ the quadratic loss Qa [t] = ||t − a||2 which is used in the 2 least squares versions of SVMs [9]. [sent-50, score-0.11]
</p><p>23 1 CL ||w||2 + 2 2  m 2 ξi + i=1  CU 2  q  ϑ2 j  (3)  j=1  w, xi + b = yi − ξi for i = 1, . [sent-53, score-0.059]
</p><p>24 In the remaining part of this paper we denote the least squares SVM by Uls -SVM. [sent-63, score-0.041]
</p><p>25 The authors of [12] describe an algorithm for the one-vs-one strategy in multiclass learning that additionally minimises the distance of the separating hyperplane to the examples that are in neither of the classes. [sent-66, score-0.092]
</p><p>26 There are also two Bayesian algorithms that refer to non-examples or neither class in the binary classiﬁcation setting. [sent-69, score-0.029]
</p><p>27 [8] gives a probabilistic interpretation for a standard hinge loss SVM by establishing the connection between the MAP estimate of a Gaussian process with a Gaussian prior using a covariance function k and a hinge loss based noise model. [sent-70, score-0.182]
</p><p>28 To circumvent this problem, the authors of [4] introduce an additional — neither — class to introduce a stochastic dependence between the parameter and the unobserved label in the discriminative model. [sent-74, score-0.049]
</p><p>29 However, neither of the Bayesian approaches actually assigns an observed example to the introduced third class. [sent-75, score-0.029]
</p><p>30 3  Analysis of the Algorithm  The following two sections analyse the geometrical relation of the decision hyperplane learnt with one of the Universum SVMs to the Universum set. [sent-76, score-0.083]
</p><p>31 It will turn out that in both cases the optimal solutions tend to make the normal vector orthogonal to the principal directions of the Universum. [sent-77, score-0.151]
</p><p>32 The extreme case where w is completely orthogonal to U, makes the decision function deﬁned by w invariant to transformations that act on the subspace spanned by the elements of U. [sent-78, score-0.194]
</p><p>33 Therefore, the Universum should contain directions the resulting function should be invariant against. [sent-79, score-0.075]
</p><p>34 However, our results generalise to the case where the xi and zj live in an RKHS spanned by some kernel. [sent-81, score-0.284]
</p><p>35 After showing the equivalence to using a standard SVM trained on the orthogonal complement of the subspace spanned by the zj , we extend the result to the cases with soft margin on U. [sent-85, score-0.344]
</p><p>36 Lemma A U-SVM with CU = ∞, ε = 0 is equivalent to training a standard SVM with the training points projected onto the orthogonal complement of span{zj −z0 , zj ∈ U}, where z0 is an arbitrary element of U. [sent-86, score-0.324]
</p><p>37 3  Proof: Since CU = ∞ and ε = 0, any w yielding a ﬁnite value of (1) must fulﬁl w, zj + b = 0 for all j = 1, . [sent-87, score-0.17]
</p><p>38 So w, zj − z0 = 0 and w is orthogonal to span{zj − z0 , zj ∈ U}. [sent-91, score-0.402]
</p><p>39 Let PU⊥ denote the projection operator onto the orthogonal complement of that set. [sent-92, score-0.118]
</p><p>40 From the previous argument, we can replace w, xi by PU⊥ w, xi in the solution of (1) without changing it. [sent-93, score-0.068]
</p><p>41 Since PU⊥ is an orthogonal projection we have that PU⊥ = PU⊥ and hence PU⊥ w, xi = w, PU⊥ xi = w, PU⊥ xi . [sent-95, score-0.164]
</p><p>42 Therefore, the optimisation problem in (1) is the same as a standard SVM where the xi have been replaced by PU⊥ xi . [sent-96, score-0.068]
</p><p>43 Since the resulting w is orthogonal to an afﬁne space spanned by the Universum points, it is invariant against features implicitly speciﬁed by directions of large variance in that afﬁne space. [sent-98, score-0.188]
</p><p>44 Picturing the ·, zj as ﬁlters that extract certain features from given labelled or test examples x, using the Universum algorithms means suppressing the features speciﬁed by the zj . [sent-99, score-0.458]
</p><p>45 Finally, we generalise the result of the lemma by dropping the hard constraint assumption on the Universum examples, i. [sent-100, score-0.05]
</p><p>46 | w∗ , zj + b∗ | ≥ CU min  CU  b  j=1  j=1  The right hand side can be interpreted as an ”L1 variance”. [sent-105, score-0.17]
</p><p>47 2 Uls -SVM, Fisher Discriminant Analysis and Principal Component Analysis In this section we present the relation of the Uls -SVM to two classic learning algorithms: (kernel) oriented Principal Component Analysis (koPCA) and (kernel) Fisher discriminant analysis (kFDA) [5]. [sent-109, score-0.076]
</p><p>48 Since koPCA and kFDA can both be written as maximisation of a Rayleigh Quotient we start with the Rayleigh quotient of the hybrid from FDA  z w  max w  w  (CL  X  X  +  (c  −  }|  +  − c )(c  k  k  (xi − c )(xi − c )  . [sent-111, score-0.086]
</p><p>49 j=1  k=± i∈I k  |  {  −  −c ) w q X ˜ ˜ +CU (zj − c)(zj − c) )w  {z  }  {z  |  from FDA  }  from oPCA  ˜ 1 Here, c± denote the class means of the labelled examples and c = 2 (c+ + c− ) is the point between them. [sent-112, score-0.118]
</p><p>50 For optimising the quotient it can be ﬁxed to an arbitrary value while the denominator is minimised. [sent-119, score-0.059]
</p><p>51 Since the denominator might not have full rank it needs to be regularised [6]. [sent-120, score-0.045]
</p><p>52 Choosing the regulariser to be ||w||2 , the problem can be rephrased as min w  ||w||2 + w  “  CL  P  k=±  P  i∈I k (xi  − ck )(xi − ck ) + CU  Pq  j=1 (zj  ˜ ˜ − c)(zj − c)  ”  w  (4)  w (c+ − c− ) = 2  s. [sent-121, score-0.048]
</p><p>53 As we will see below this problem can further be transformed into a quadratic program min ||w||2 + CL ||ξ||2 + CU ||ϑ||2 w,b  (5)  w, xi + b = yi + ξi for all i = 1, . [sent-123, score-0.059]
</p><p>54 The following lemma establishes the relation of the Uls -SVM to kFDA and koPCA. [sent-133, score-0.042]
</p><p>55 Let us choose b = −w c, ξi = w xi + b − yi and ϑj = w zj +b. [sent-141, score-0.229]
</p><p>56 The above lemma establishes a relation of the Uls -SVM to two classic learning algorithms. [sent-145, score-0.042]
</p><p>57 Since the noise covariance matrix of koPCA is given by the covariance of the Universum points centered on the average of the labelled class means, the role of the Universum as a data-dependent speciﬁcation of principal directions of invariance is afﬁrmed. [sent-147, score-0.271]
</p><p>58 The koPCA term also shows that both the position and covariance structure are crucial to a good q q ˜ ˜ ˜ ˜ Universum. [sent-148, score-0.044]
</p><p>59 To see this, we rewrite j=1 (zj − c)(zj − c) as j=1 (zj − z)(zj − z) + q(˜ − z q 1 ˜ z ˜ ˜ c)(˜ − c) , where z = q j=1 zj is the Universum mean. [sent-149, score-0.17]
</p><p>60 The additive relationship between covariance of Universum about its mean, and the distance between Universum and training sample means projected onto w shows that either quantity can dominate depending on the data at hand. [sent-150, score-0.081]
</p><p>61 In the next section, we demonstrate the theoretical results of this section on toy problems and give an example how to use the insight gained from this section to construct an appropriate Universum. [sent-151, score-0.029]
</p><p>62 1  Experiments Toy Experiments  The theoretical results of section 3 show that the covariance structure of the Universum as well as its absolute position inﬂuence the result of the learning process. [sent-153, score-0.044]
</p><p>63 To validate this insight on toy data, we sample ten labelled sets of size 20, 50, 100 and 500 from two ﬁfty-dimensional Gaussians. [sent-154, score-0.127]
</p><p>64 Both Gaussians have a diagonal covariance that has low standard deviation (σ1,2 = 0. [sent-155, score-0.044]
</p><p>65 We construct two kinds of Universa for this toy problem. [sent-166, score-0.029]
</p><p>66 For the ﬁrst kind we use a mean zero Gaussian with the same covariance structure as the Gaussians for the labelled data (σ3,. [sent-167, score-0.168]
</p><p>67 For the second kind of Universa we use the same covariance as the labelled classes but shifted them along the line between the means of the labelled Gaussians. [sent-174, score-0.266]
</p><p>68 In the top row, the degree of isotropy increases from left to right, whereas σ = 10 refers to the complete isotropic case. [sent-177, score-0.055]
</p><p>69 As expected, performance converges to the performance of an SVM for high isotropy σ and large translations t. [sent-179, score-0.074]
</p><p>70 However, this might be due to the fact the variance along the principal components of the Universum is much larger in magnitude than the applied shift. [sent-181, score-0.032]
</p><p>71 5 SVM (q=0) q = 100 q = 500  mean error  mean error  0. [sent-215, score-0.04]
</p><p>72 1  100  200  300  m  400  500  0 0  100  200  m  300  400  500  m  Figure 1: Learning curves of linear U-SVMs for different degrees of isotropy σ and different amounts of  t translation z → z + 2 · (c+ − c− ). [sent-220, score-0.074]
</p><p>73 With increasing isotropy and translation the performance of the U-SVMs converges to the performance of a normal SVM. [sent-221, score-0.095]
</p><p>74 Note that for instance that digits 3 and 6 are the best Universum and they are also the closest to the decision boundary. [sent-255, score-0.05]
</p><p>75 2  Results on MNIST  Following the experimental work from [11], we took up the task of distinguishing between the digits 5 and 8 on MNIST data. [sent-257, score-0.031]
</p><p>76 Training sets of size 1000 were used, and other digits served as Universum data. [sent-258, score-0.031]
</p><p>77 Using different digits as universa, we recorded the test error (in percentage) of U-SVM. [sent-259, score-0.051]
</p><p>78 w, x + b) of a normal SVM trained for binary classiﬁcation between the digits 5 and 8, measured on the points from the Universum class. [sent-262, score-0.069]
</p><p>79 Another quantity of interest measured was the angle between covariance matrices of training and Universum data in the feature space. [sent-263, score-0.068]
</p><p>80 Note that for two covariance matrices CX and CY corresponding to matrices X and Y (centered about their means), the cosine of the angle is deﬁned as trace(CX CY )/ trace(C2 )trace(C2 ). [sent-264, score-0.068]
</p><p>81 3  Classiﬁcation of Imagined Movements in Brain Computer Interfaces  Brain computer interfaces (BCI) are devices that allow a user to control a computer by merely using his brain activity [3]. [sent-270, score-0.06]
</p><p>82 In this paradigm the patient imagines the movement of his left or right hand for indicating the respective state. [sent-274, score-0.045]
</p><p>83 In order to reverse the spatial blurring of the brain activity by the intermediate tissue of the skull, the signals from all sensors are demixed via 6  Algorithm SVM U-SVM LS-SVM Uls -SVM  SVM U-SVM LS-SVM Uls -SVM  U ∅ UC3 Unm ∅ UC3 Unm  FS 40. [sent-275, score-0.071]
</p><p>84 For the ﬁrst set (DATA I) we recorded the EEG activity from three healthy subjects for an imagined movement paradigm as described by [3]. [sent-376, score-0.108]
</p><p>85 The second set (DATA II) contains EEG signals from a similar paradigm [7]. [sent-377, score-0.054]
</p><p>86 The ﬁrst Universum, UC3 consists of recordings from a third condition in the experiments that is not related to imagined movements. [sent-379, score-0.044]
</p><p>87 Since variations in signals from this condition should not carry any useful information about imagined movement task, the classiﬁer should be invariant against them. [sent-380, score-0.134]
</p><p>88 Unfortunately, signals in the α-band are also related to visual activity and independent components can be found that have a strong inﬂuence from sensors over the visual cortex. [sent-383, score-0.049]
</p><p>89 In order to make the learning algorithm prefer the signals from the motor cortex, we construct a Universum Unm by projecting the labelled data onto the independent components that have a strong inﬂuence from the visual cortex. [sent-385, score-0.146]
</p><p>90 As already observed for the DATA I dataset, the Uls -SVM performs constantly worse than its hinge loss counterpart. [sent-396, score-0.069]
</p><p>91 The regularisation constant CU for the Universum points was chosen C = CU = 0. [sent-398, score-0.053]
</p><p>92 This means that the non-orthogonality of w on the Universum points was only weakly 7  penalised, but had equal priority to classifying the labelled examples correctly. [sent-400, score-0.135]
</p><p>93 We demonstrated that the U-SVM as implemented in [11] is equivalent to searching for a hyperplane which has its normal lying in the orthogonal complement of the space spanned by Universum examples. [sent-404, score-0.215]
</p><p>94 We also showed that the corresponding least squares Uls -SVM can be seen as a hybrid between the two well known learning algorithms kFDA and koPCA where the Universum points, centered between the means of the labelled classes, play the role of the noise covariance in koPCA. [sent-405, score-0.211]
</p><p>95 Ideally the covariance matrix of the Universum should thus contain some important invariant directions for the problem at hand. [sent-406, score-0.119]
</p><p>96 The position of the Universum set plays also an important role and both our theoretical and experimental analysis show that the behaviour of the algorithm depends on the difference between the means of the labelled set and of the Universum set. [sent-407, score-0.098]
</p><p>97 The question of whether the main inﬂuence of the Universum comes from the position or the covariance does not have a clear answer and is probably problem dependent. [sent-408, score-0.044]
</p><p>98 From a practical point, the main contribution of this paper is to suggest how to select a good Universum set: it should be such that it contains invariant directions and is positioned “in between” the two classes. [sent-409, score-0.075]
</p><p>99 Therefore, as can be partly seen from the BCI experiments, a good Universum dataset needs to be carefully chosen and cannot be an arbitrary backdrop as the name might suggest. [sent-410, score-0.045]
</p><p>100 Classifying EEG and ECoG signals without subject o u training for fast bci implementation: Comparison of non-paralysed and completely paralysed subjects. [sent-433, score-0.076]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('universum', 0.844), ('uls', 0.18), ('cu', 0.174), ('zj', 0.17), ('unm', 0.138), ('svm', 0.112), ('universa', 0.111), ('pu', 0.103), ('labelled', 0.098), ('kfda', 0.084), ('kopca', 0.083), ('cl', 0.079), ('orthogonal', 0.062), ('isotropy', 0.055), ('spanned', 0.051), ('vapnik', 0.049), ('regulariser', 0.048), ('bci', 0.046), ('imagined', 0.044), ('covariance', 0.044), ('hyperplane', 0.043), ('kxy', 0.041), ('hinge', 0.041), ('squares', 0.041), ('invariant', 0.039), ('complement', 0.038), ('jh', 0.036), ('quotient', 0.036), ('regularisation', 0.036), ('directions', 0.036), ('xi', 0.034), ('eeg', 0.034), ('principal', 0.032), ('digits', 0.031), ('zien', 0.031), ('signals', 0.03), ('trace', 0.03), ('discriminant', 0.03), ('neither', 0.029), ('generalise', 0.029), ('fisher', 0.029), ('toy', 0.029), ('lkopf', 0.028), ('uence', 0.028), ('chapelle', 0.028), ('hybrid', 0.028), ('loss', 0.028), ('alekh', 0.028), ('backdrop', 0.028), ('regularisers', 0.028), ('resp', 0.028), ('sch', 0.027), ('classi', 0.027), ('kernel', 0.027), ('kind', 0.026), ('yi', 0.025), ('oriented', 0.025), ('angle', 0.024), ('fda', 0.024), ('sinz', 0.024), ('paradigm', 0.024), ('dimensions', 0.023), ('subspace', 0.023), ('ica', 0.023), ('denominator', 0.023), ('maximisation', 0.022), ('regularised', 0.022), ('spemannstrasse', 0.022), ('brain', 0.022), ('normal', 0.021), ('helpful', 0.021), ('lemma', 0.021), ('relation', 0.021), ('movement', 0.021), ('cx', 0.021), ('cy', 0.021), ('mika', 0.021), ('rayleigh', 0.021), ('unlabelled', 0.021), ('error', 0.02), ('examples', 0.02), ('cybernetics', 0.02), ('discriminative', 0.02), ('clari', 0.019), ('interfaces', 0.019), ('minimising', 0.019), ('translations', 0.019), ('translation', 0.019), ('activity', 0.019), ('decision', 0.019), ('projected', 0.019), ('implementation', 0.019), ('jl', 0.018), ('bernhard', 0.018), ('planck', 0.018), ('af', 0.018), ('onto', 0.018), ('name', 0.017), ('points', 0.017), ('fs', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="24-tfidf-1" href="./nips-2007-An_Analysis_of_Inference_with_the_Universum.html">24 nips-2007-An Analysis of Inference with the Universum</a></p>
<p>Author: Olivier Chapelle, Alekh Agarwal, Fabian H. Sinz, Bernhard Schölkopf</p><p>Abstract: We study a pattern classiﬁcation algorithm which has recently been proposed by Vapnik and coworkers. It builds on a new inductive principle which assumes that in addition to positive and negative data, a third class of data is available, termed the Universum. We assay the behavior of the algorithm by establishing links with Fisher discriminant analysis and oriented PCA, as well as with an SVM in a projected subspace (or, equivalently, with a data-dependent reduced kernel). We also provide experimental results. 1</p><p>2 0.072089054 <a title="24-tfidf-2" href="./nips-2007-Invariant_Common_Spatial_Patterns%3A_Alleviating_Nonstationarities_in_Brain-Computer_Interfacing.html">106 nips-2007-Invariant Common Spatial Patterns: Alleviating Nonstationarities in Brain-Computer Interfacing</a></p>
<p>Author: Benjamin Blankertz, Motoaki Kawanabe, Ryota Tomioka, Friederike Hohlefeld, Klaus-Robert Müller, Vadim V. Nikulin</p><p>Abstract: Brain-Computer Interfaces can suffer from a large variance of the subject conditions within and across sessions. For example vigilance ﬂuctuations in the individual, variable task involvement, workload etc. alter the characteristics of EEG signals and thus challenge a stable BCI operation. In the present work we aim to deﬁne features based on a variant of the common spatial patterns (CSP) algorithm that are constructed invariant with respect to such nonstationarities. We enforce invariance properties by adding terms to the denominator of a Rayleigh coefﬁcient representation of CSP such as disturbance covariance matrices from ﬂuctuations in visual processing. In this manner physiological prior knowledge can be used to shape the classiﬁcation engine for BCI. As a proof of concept we present a BCI classiﬁer that is robust to changes in the level of parietal α -activity. In other words, the EEG decoding still works when there are lapses in vigilance.</p><p>3 0.07204251 <a title="24-tfidf-3" href="./nips-2007-Learning_with_Transformation_Invariant_Kernels.html">118 nips-2007-Learning with Transformation Invariant Kernels</a></p>
<p>Author: Christian Walder, Olivier Chapelle</p><p>Abstract: This paper considers kernels invariant to translation, rotation and dilation. We show that no non-trivial positive deﬁnite (p.d.) kernels exist which are radial and dilation invariant, only conditionally positive deﬁnite (c.p.d.) ones. Accordingly, we discuss the c.p.d. case and provide some novel analysis, including an elementary derivation of a c.p.d. representer theorem. On the practical side, we give a support vector machine (s.v.m.) algorithm for arbitrary c.p.d. kernels. For the thinplate kernel this leads to a classiﬁer with only one parameter (the amount of regularisation), which we demonstrate to be as effective as an s.v.m. with the Gaussian kernel, even though the Gaussian involves a second parameter (the length scale). 1</p><p>4 0.071848989 <a title="24-tfidf-4" href="./nips-2007-A_Randomized_Algorithm_for_Large_Scale_Support_Vector_Learning.html">10 nips-2007-A Randomized Algorithm for Large Scale Support Vector Learning</a></p>
<p>Author: Krishnan Kumar, Chiru Bhattacharya, Ramesh Hariharan</p><p>Abstract: This paper investigates the application of randomized algorithms for large scale SVM learning. The key contribution of the paper is to show that, by using ideas random projections, the minimal number of support vectors required to solve almost separable classiﬁcation problems, such that the solution obtained is near optimal with a very high probability, is given by O(log n); if on removal of properly chosen O(log n) points the data becomes linearly separable then it is called almost separable. The second contribution is a sampling based algorithm, motivated from randomized algorithms, which solves a SVM problem by considering subsets of the dataset which are greater in size than the number of support vectors for the problem. These two ideas are combined to obtain an algorithm for SVM classiﬁcation problems which performs the learning by considering only O(log n) points at a time. Experiments done on synthetic and real life datasets show that the algorithm does scale up state of the art SVM solvers in terms of memory required and execution time without loss in accuracy. It is to be noted that the algorithm presented here nicely complements existing large scale SVM learning approaches as it can be used to scale up any SVM solver. 1</p><p>5 0.05978306 <a title="24-tfidf-5" href="./nips-2007-Learning_Monotonic_Transformations_for_Classification.html">112 nips-2007-Learning Monotonic Transformations for Classification</a></p>
<p>Author: Andrew Howard, Tony Jebara</p><p>Abstract: A discriminative method is proposed for learning monotonic transformations of the training data while jointly estimating a large-margin classiﬁer. In many domains such as document classiﬁcation, image histogram classiﬁcation and gene microarray experiments, ﬁxed monotonic transformations can be useful as a preprocessing step. However, most classiﬁers only explore these transformations through manual trial and error or via prior domain knowledge. The proposed method learns monotonic transformations automatically while training a large-margin classiﬁer without any prior knowledge of the domain. A monotonic piecewise linear function is learned which transforms data for subsequent processing by a linear hyperplane classiﬁer. Two algorithmic implementations of the method are formalized. The ﬁrst solves a convergent alternating sequence of quadratic and linear programs until it obtains a locally optimal solution. An improved algorithm is then derived using a convex semideﬁnite relaxation that overcomes initialization issues in the greedy optimization problem. The eﬀectiveness of these learned transformations on synthetic problems, text data and image data is demonstrated. 1</p><p>6 0.055981923 <a title="24-tfidf-6" href="./nips-2007-Second_Order_Bilinear_Discriminant_Analysis_for_single_trial_EEG_analysis.html">173 nips-2007-Second Order Bilinear Discriminant Analysis for single trial EEG analysis</a></p>
<p>7 0.051794272 <a title="24-tfidf-7" href="./nips-2007-Unsupervised_Feature_Selection_for_Accurate_Recommendation_of_High-Dimensional_Image_Data.html">211 nips-2007-Unsupervised Feature Selection for Accurate Recommendation of High-Dimensional Image Data</a></p>
<p>8 0.051484015 <a title="24-tfidf-8" href="./nips-2007-Efficient_Convex_Relaxation_for_Transductive_Support_Vector_Machine.html">76 nips-2007-Efficient Convex Relaxation for Transductive Support Vector Machine</a></p>
<p>9 0.050103761 <a title="24-tfidf-9" href="./nips-2007-DIFFRAC%3A_a_discriminative_and_flexible_framework_for_clustering.html">65 nips-2007-DIFFRAC: a discriminative and flexible framework for clustering</a></p>
<p>10 0.049205706 <a title="24-tfidf-10" href="./nips-2007-Convex_Learning_with_Invariances.html">62 nips-2007-Convex Learning with Invariances</a></p>
<p>11 0.047581844 <a title="24-tfidf-11" href="./nips-2007-Testing_for_Homogeneity_with_Kernel_Fisher_Discriminant_Analysis.html">192 nips-2007-Testing for Homogeneity with Kernel Fisher Discriminant Analysis</a></p>
<p>12 0.044862658 <a title="24-tfidf-12" href="./nips-2007-Support_Vector_Machine_Classification_with_Indefinite_Kernels.html">190 nips-2007-Support Vector Machine Classification with Indefinite Kernels</a></p>
<p>13 0.044472672 <a title="24-tfidf-13" href="./nips-2007-EEG-Based_Brain-Computer_Interaction%3A_Improved_Accuracy_by_Automatic_Single-Trial_Error_Detection.html">74 nips-2007-EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection</a></p>
<p>14 0.037181545 <a title="24-tfidf-14" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>15 0.03546112 <a title="24-tfidf-15" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>16 0.035452411 <a title="24-tfidf-16" href="./nips-2007-Random_Features_for_Large-Scale_Kernel_Machines.html">160 nips-2007-Random Features for Large-Scale Kernel Machines</a></p>
<p>17 0.033670701 <a title="24-tfidf-17" href="./nips-2007-A_Risk_Minimization_Principle_for_a_Class_of_Parzen_Estimators.html">11 nips-2007-A Risk Minimization Principle for a Class of Parzen Estimators</a></p>
<p>18 0.032424487 <a title="24-tfidf-18" href="./nips-2007-A_Kernel_Statistical_Test_of_Independence.html">7 nips-2007-A Kernel Statistical Test of Independence</a></p>
<p>19 0.030601723 <a title="24-tfidf-19" href="./nips-2007-Hidden_Common_Cause_Relations_in_Relational_Learning.html">97 nips-2007-Hidden Common Cause Relations in Relational Learning</a></p>
<p>20 0.02935441 <a title="24-tfidf-20" href="./nips-2007-Multi-Task_Learning_via_Conic_Programming.html">134 nips-2007-Multi-Task Learning via Conic Programming</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.115), (1, 0.031), (2, -0.035), (3, 0.057), (4, -0.008), (5, 0.033), (6, 0.044), (7, 0.041), (8, -0.077), (9, 0.029), (10, 0.034), (11, -0.074), (12, -0.011), (13, -0.004), (14, 0.028), (15, -0.026), (16, 0.019), (17, -0.087), (18, -0.009), (19, 0.033), (20, -0.016), (21, -0.018), (22, 0.031), (23, 0.061), (24, -0.014), (25, -0.057), (26, -0.024), (27, -0.028), (28, 0.007), (29, 0.034), (30, 0.033), (31, -0.045), (32, 0.03), (33, -0.085), (34, -0.06), (35, -0.019), (36, -0.003), (37, -0.052), (38, 0.012), (39, -0.031), (40, -0.069), (41, -0.01), (42, -0.083), (43, 0.033), (44, -0.067), (45, -0.035), (46, 0.078), (47, 0.043), (48, 0.088), (49, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87466121 <a title="24-lsi-1" href="./nips-2007-An_Analysis_of_Inference_with_the_Universum.html">24 nips-2007-An Analysis of Inference with the Universum</a></p>
<p>Author: Olivier Chapelle, Alekh Agarwal, Fabian H. Sinz, Bernhard Schölkopf</p><p>Abstract: We study a pattern classiﬁcation algorithm which has recently been proposed by Vapnik and coworkers. It builds on a new inductive principle which assumes that in addition to positive and negative data, a third class of data is available, termed the Universum. We assay the behavior of the algorithm by establishing links with Fisher discriminant analysis and oriented PCA, as well as with an SVM in a projected subspace (or, equivalently, with a data-dependent reduced kernel). We also provide experimental results. 1</p><p>2 0.63101035 <a title="24-lsi-2" href="./nips-2007-Learning_Monotonic_Transformations_for_Classification.html">112 nips-2007-Learning Monotonic Transformations for Classification</a></p>
<p>Author: Andrew Howard, Tony Jebara</p><p>Abstract: A discriminative method is proposed for learning monotonic transformations of the training data while jointly estimating a large-margin classiﬁer. In many domains such as document classiﬁcation, image histogram classiﬁcation and gene microarray experiments, ﬁxed monotonic transformations can be useful as a preprocessing step. However, most classiﬁers only explore these transformations through manual trial and error or via prior domain knowledge. The proposed method learns monotonic transformations automatically while training a large-margin classiﬁer without any prior knowledge of the domain. A monotonic piecewise linear function is learned which transforms data for subsequent processing by a linear hyperplane classiﬁer. Two algorithmic implementations of the method are formalized. The ﬁrst solves a convergent alternating sequence of quadratic and linear programs until it obtains a locally optimal solution. An improved algorithm is then derived using a convex semideﬁnite relaxation that overcomes initialization issues in the greedy optimization problem. The eﬀectiveness of these learned transformations on synthetic problems, text data and image data is demonstrated. 1</p><p>3 0.60840422 <a title="24-lsi-3" href="./nips-2007-Invariant_Common_Spatial_Patterns%3A_Alleviating_Nonstationarities_in_Brain-Computer_Interfacing.html">106 nips-2007-Invariant Common Spatial Patterns: Alleviating Nonstationarities in Brain-Computer Interfacing</a></p>
<p>Author: Benjamin Blankertz, Motoaki Kawanabe, Ryota Tomioka, Friederike Hohlefeld, Klaus-Robert Müller, Vadim V. Nikulin</p><p>Abstract: Brain-Computer Interfaces can suffer from a large variance of the subject conditions within and across sessions. For example vigilance ﬂuctuations in the individual, variable task involvement, workload etc. alter the characteristics of EEG signals and thus challenge a stable BCI operation. In the present work we aim to deﬁne features based on a variant of the common spatial patterns (CSP) algorithm that are constructed invariant with respect to such nonstationarities. We enforce invariance properties by adding terms to the denominator of a Rayleigh coefﬁcient representation of CSP such as disturbance covariance matrices from ﬂuctuations in visual processing. In this manner physiological prior knowledge can be used to shape the classiﬁcation engine for BCI. As a proof of concept we present a BCI classiﬁer that is robust to changes in the level of parietal α -activity. In other words, the EEG decoding still works when there are lapses in vigilance.</p><p>4 0.5864982 <a title="24-lsi-4" href="./nips-2007-Convex_Learning_with_Invariances.html">62 nips-2007-Convex Learning with Invariances</a></p>
<p>Author: Choon H. Teo, Amir Globerson, Sam T. Roweis, Alex J. Smola</p><p>Abstract: Incorporating invariances into a learning algorithm is a common problem in machine learning. We provide a convex formulation which can deal with arbitrary loss functions and arbitrary losses. In addition, it is a drop-in replacement for most optimization algorithms for kernels, including solvers of the SVMStruct family. The advantage of our setting is that it relies on column generation instead of modifying the underlying optimization problem directly. 1</p><p>5 0.57575238 <a title="24-lsi-5" href="./nips-2007-Learning_with_Transformation_Invariant_Kernels.html">118 nips-2007-Learning with Transformation Invariant Kernels</a></p>
<p>Author: Christian Walder, Olivier Chapelle</p><p>Abstract: This paper considers kernels invariant to translation, rotation and dilation. We show that no non-trivial positive deﬁnite (p.d.) kernels exist which are radial and dilation invariant, only conditionally positive deﬁnite (c.p.d.) ones. Accordingly, we discuss the c.p.d. case and provide some novel analysis, including an elementary derivation of a c.p.d. representer theorem. On the practical side, we give a support vector machine (s.v.m.) algorithm for arbitrary c.p.d. kernels. For the thinplate kernel this leads to a classiﬁer with only one parameter (the amount of regularisation), which we demonstrate to be as effective as an s.v.m. with the Gaussian kernel, even though the Gaussian involves a second parameter (the length scale). 1</p><p>6 0.5452776 <a title="24-lsi-6" href="./nips-2007-A_Randomized_Algorithm_for_Large_Scale_Support_Vector_Learning.html">10 nips-2007-A Randomized Algorithm for Large Scale Support Vector Learning</a></p>
<p>7 0.5364868 <a title="24-lsi-7" href="./nips-2007-Second_Order_Bilinear_Discriminant_Analysis_for_single_trial_EEG_analysis.html">173 nips-2007-Second Order Bilinear Discriminant Analysis for single trial EEG analysis</a></p>
<p>8 0.49261087 <a title="24-lsi-8" href="./nips-2007-EEG-Based_Brain-Computer_Interaction%3A_Improved_Accuracy_by_Automatic_Single-Trial_Error_Detection.html">74 nips-2007-EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection</a></p>
<p>9 0.49069047 <a title="24-lsi-9" href="./nips-2007-Support_Vector_Machine_Classification_with_Indefinite_Kernels.html">190 nips-2007-Support Vector Machine Classification with Indefinite Kernels</a></p>
<p>10 0.48808289 <a title="24-lsi-10" href="./nips-2007-Efficient_Convex_Relaxation_for_Transductive_Support_Vector_Machine.html">76 nips-2007-Efficient Convex Relaxation for Transductive Support Vector Machine</a></p>
<p>11 0.43903819 <a title="24-lsi-11" href="./nips-2007-Parallelizing_Support_Vector_Machines_on_Distributed_Computers.html">152 nips-2007-Parallelizing Support Vector Machines on Distributed Computers</a></p>
<p>12 0.42020845 <a title="24-lsi-12" href="./nips-2007-A_Risk_Minimization_Principle_for_a_Class_of_Parzen_Estimators.html">11 nips-2007-A Risk Minimization Principle for a Class of Parzen Estimators</a></p>
<p>13 0.38015762 <a title="24-lsi-13" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>14 0.37673315 <a title="24-lsi-14" href="./nips-2007-DIFFRAC%3A_a_discriminative_and_flexible_framework_for_clustering.html">65 nips-2007-DIFFRAC: a discriminative and flexible framework for clustering</a></p>
<p>15 0.36179042 <a title="24-lsi-15" href="./nips-2007-A_Spectral_Regularization_Framework_for_Multi-Task_Structure_Learning.html">12 nips-2007-A Spectral Regularization Framework for Multi-Task Structure Learning</a></p>
<p>16 0.3572183 <a title="24-lsi-16" href="./nips-2007-Nearest-Neighbor-Based_Active_Learning_for_Rare_Category_Detection.html">139 nips-2007-Nearest-Neighbor-Based Active Learning for Rare Category Detection</a></p>
<p>17 0.35545433 <a title="24-lsi-17" href="./nips-2007-Unsupervised_Feature_Selection_for_Accurate_Recommendation_of_High-Dimensional_Image_Data.html">211 nips-2007-Unsupervised Feature Selection for Accurate Recommendation of High-Dimensional Image Data</a></p>
<p>18 0.35385942 <a title="24-lsi-18" href="./nips-2007-Bundle_Methods_for_Machine_Learning.html">40 nips-2007-Bundle Methods for Machine Learning</a></p>
<p>19 0.35122842 <a title="24-lsi-19" href="./nips-2007-Discriminative_K-means_for_Clustering.html">70 nips-2007-Discriminative K-means for Clustering</a></p>
<p>20 0.3365216 <a title="24-lsi-20" href="./nips-2007-Random_Features_for_Large-Scale_Kernel_Machines.html">160 nips-2007-Random Features for Large-Scale Kernel Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.028), (5, 0.025), (13, 0.065), (16, 0.043), (18, 0.013), (19, 0.077), (21, 0.067), (34, 0.016), (35, 0.026), (47, 0.093), (49, 0.031), (62, 0.226), (83, 0.088), (85, 0.039), (87, 0.015), (90, 0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73606777 <a title="24-lda-1" href="./nips-2007-An_Analysis_of_Inference_with_the_Universum.html">24 nips-2007-An Analysis of Inference with the Universum</a></p>
<p>Author: Olivier Chapelle, Alekh Agarwal, Fabian H. Sinz, Bernhard Schölkopf</p><p>Abstract: We study a pattern classiﬁcation algorithm which has recently been proposed by Vapnik and coworkers. It builds on a new inductive principle which assumes that in addition to positive and negative data, a third class of data is available, termed the Universum. We assay the behavior of the algorithm by establishing links with Fisher discriminant analysis and oriented PCA, as well as with an SVM in a projected subspace (or, equivalently, with a data-dependent reduced kernel). We also provide experimental results. 1</p><p>2 0.59756678 <a title="24-lda-2" href="./nips-2007-A_Unified_Near-Optimal_Estimator_For_Dimension_Reduction_in_%24l_%5Calpha%24_%28%240%3C%5Calpha%5Cleq_2%24%29_Using_Stable_Random_Projections.html">13 nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</a></p>
<p>Author: Ping Li, Trevor J. Hastie</p><p>Abstract: Many tasks (e.g., clustering) in machine learning only require the lα distances instead of the original data. For dimension reductions in the lα norm (0 < α ≤ 2), the method of stable random projections can efﬁciently compute the lα distances in massive datasets (e.g., the Web or massive data streams) in one pass of the data. The estimation task for stable random projections has been an interesting topic. We propose a simple estimator based on the fractional power of the samples (projected data), which is surprisingly near-optimal in terms of the asymptotic variance. In fact, it achieves the Cram´ r-Rao bound when α = 2 and α = 0+. This e new result will be useful when applying stable random projections to distancebased clustering, classiﬁcations, kernels, massive data streams etc.</p><p>3 0.5902006 <a title="24-lda-3" href="./nips-2007-Invariant_Common_Spatial_Patterns%3A_Alleviating_Nonstationarities_in_Brain-Computer_Interfacing.html">106 nips-2007-Invariant Common Spatial Patterns: Alleviating Nonstationarities in Brain-Computer Interfacing</a></p>
<p>Author: Benjamin Blankertz, Motoaki Kawanabe, Ryota Tomioka, Friederike Hohlefeld, Klaus-Robert Müller, Vadim V. Nikulin</p><p>Abstract: Brain-Computer Interfaces can suffer from a large variance of the subject conditions within and across sessions. For example vigilance ﬂuctuations in the individual, variable task involvement, workload etc. alter the characteristics of EEG signals and thus challenge a stable BCI operation. In the present work we aim to deﬁne features based on a variant of the common spatial patterns (CSP) algorithm that are constructed invariant with respect to such nonstationarities. We enforce invariance properties by adding terms to the denominator of a Rayleigh coefﬁcient representation of CSP such as disturbance covariance matrices from ﬂuctuations in visual processing. In this manner physiological prior knowledge can be used to shape the classiﬁcation engine for BCI. As a proof of concept we present a BCI classiﬁer that is robust to changes in the level of parietal α -activity. In other words, the EEG decoding still works when there are lapses in vigilance.</p><p>4 0.58863258 <a title="24-lda-4" href="./nips-2007-Second_Order_Bilinear_Discriminant_Analysis_for_single_trial_EEG_analysis.html">173 nips-2007-Second Order Bilinear Discriminant Analysis for single trial EEG analysis</a></p>
<p>Author: Christoforos Christoforou, Paul Sajda, Lucas C. Parra</p><p>Abstract: Traditional analysis methods for single-trial classiﬁcation of electroencephalography (EEG) focus on two types of paradigms: phase locked methods, in which the amplitude of the signal is used as the feature for classiﬁcation, e.g. event related potentials; and second order methods, in which the feature of interest is the power of the signal, e.g. event related (de)synchronization. The procedure for deciding which paradigm to use is ad hoc and is typically driven by knowledge of the underlying neurophysiology. Here we propose a principled method, based on a bilinear model, in which the algorithm simultaneously learns the best ﬁrst and second order spatial and temporal features for classiﬁcation of EEG. The method is demonstrated on simulated data as well as on EEG taken from a benchmark data used to test classiﬁcation algorithms for brain computer interfaces. 1 1.1</p><p>5 0.56085378 <a title="24-lda-5" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>Author: Lars Buesing, Wolfgang Maass</p><p>Abstract: We show that under suitable assumptions (primarily linearization) a simple and perspicuous online learning rule for Information Bottleneck optimization with spiking neurons can be derived. This rule performs on common benchmark tasks as well as a rather complex rule that has previously been proposed [1]. Furthermore, the transparency of this new learning rule makes a theoretical analysis of its convergence properties feasible. A variation of this learning rule (with sign changes) provides a theoretically founded method for performing Principal Component Analysis (PCA) with spiking neurons. By applying this rule to an ensemble of neurons, different principal components of the input can be extracted. In addition, it is possible to preferentially extract those principal components from incoming signals X that are related or are not related to some additional target signal YT . In a biological interpretation, this target signal YT (also called relevance variable) could represent proprioceptive feedback, input from other sensory modalities, or top-down signals. 1</p><p>6 0.558442 <a title="24-lda-6" href="./nips-2007-EEG-Based_Brain-Computer_Interaction%3A_Improved_Accuracy_by_Automatic_Single-Trial_Error_Detection.html">74 nips-2007-EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection</a></p>
<p>7 0.55523992 <a title="24-lda-7" href="./nips-2007-Hippocampal_Contributions_to_Control%3A_The_Third_Way.html">100 nips-2007-Hippocampal Contributions to Control: The Third Way</a></p>
<p>8 0.55462033 <a title="24-lda-8" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>9 0.55330217 <a title="24-lda-9" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<p>10 0.55148059 <a title="24-lda-10" href="./nips-2007-Blind_channel_identification_for_speech_dereverberation_using_l1-norm_sparse_learning.html">37 nips-2007-Blind channel identification for speech dereverberation using l1-norm sparse learning</a></p>
<p>11 0.55058497 <a title="24-lda-11" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>12 0.54998469 <a title="24-lda-12" href="./nips-2007-Locality_and_low-dimensions_in_the_prediction_of_natural_experience_from_fMRI.html">122 nips-2007-Locality and low-dimensions in the prediction of natural experience from fMRI</a></p>
<p>13 0.54870892 <a title="24-lda-13" href="./nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">79 nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<p>14 0.54866636 <a title="24-lda-14" href="./nips-2007-The_Generalized_FITC_Approximation.html">195 nips-2007-The Generalized FITC Approximation</a></p>
<p>15 0.54829979 <a title="24-lda-15" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>16 0.54735321 <a title="24-lda-16" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>17 0.54654866 <a title="24-lda-17" href="./nips-2007-Probabilistic_Matrix_Factorization.html">158 nips-2007-Probabilistic Matrix Factorization</a></p>
<p>18 0.54635185 <a title="24-lda-18" href="./nips-2007-Predicting_Brain_States_from_fMRI_Data%3A_Incremental_Functional_Principal_Component_Regression.html">154 nips-2007-Predicting Brain States from fMRI Data: Incremental Functional Principal Component Regression</a></p>
<p>19 0.54604238 <a title="24-lda-19" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>20 0.54515553 <a title="24-lda-20" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
