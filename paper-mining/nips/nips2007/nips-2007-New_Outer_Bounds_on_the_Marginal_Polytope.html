<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>141 nips-2007-New Outer Bounds on the Marginal Polytope</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-141" href="#">nips2007-141</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>141 nips-2007-New Outer Bounds on the Marginal Polytope</h1>
<br/><p>Source: <a title="nips-2007-141-pdf" href="http://papers.nips.cc/paper/3274-new-outer-bounds-on-the-marginal-polytope.pdf">pdf</a></p><p>Author: David Sontag, Tommi S. Jaakkola</p><p>Abstract: We give a new class of outer bounds on the marginal polytope, and propose a cutting-plane algorithm for efﬁciently optimizing over these constraints. When combined with a concave upper bound on the entropy, this gives a new variational inference algorithm for probabilistic inference in discrete Markov Random Fields (MRFs). Valid constraints on the marginal polytope are derived through a series of projections onto the cut polytope. As a result, we obtain tighter upper bounds on the log-partition function. We also show empirically that the approximations of the marginals are signiﬁcantly more accurate when using the tighter outer bounds. Finally, we demonstrate the advantage of the new constraints for ﬁnding the MAP assignment in protein structure prediction. 1</p><p>Reference: <a title="nips-2007-141-reference" href="../nips2007_reference/nips-2007-New_Outer_Bounds_on_the_Marginal_Polytope_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We give a new class of outer bounds on the marginal polytope, and propose a cutting-plane algorithm for efﬁciently optimizing over these constraints. [sent-3, score-0.401]
</p><p>2 Valid constraints on the marginal polytope are derived through a series of projections onto the cut polytope. [sent-5, score-0.968]
</p><p>3 We also show empirically that the approximations of the marginals are signiﬁcantly more accurate when using the tighter outer bounds. [sent-7, score-0.341]
</p><p>4 We focus on a particular class of variational approximation methods that cast the inference problem as a non-linear optimization over the marginal polytope, the set of valid marginal probabilities. [sent-12, score-0.503]
</p><p>5 The selection of appropriate marginals from the marginal polytope is guided by the (non-linear) entropy function. [sent-13, score-0.952]
</p><p>6 Both the marginal polytope and the entropy are difﬁcult to characterize in general, reﬂecting the hardness of exact inference calculations. [sent-14, score-0.942]
</p><p>7 For general graphs, this is an outer bound of the marginal polytope. [sent-16, score-0.365]
</p><p>8 Our goal here is to provide tighter outer bounds on the marginal polytope. [sent-19, score-0.47]
</p><p>9 The key to such approaches is to have an efﬁcient separation algorithm which, given an infeasible solution, can quickly ﬁnd a violated constraint, generally from a very large class of valid constraints on the set of integral solutions. [sent-22, score-0.365]
</p><p>10 [3] showed that the MAP problem in pairwise binary MRFs is equivalent to a linear optimization over the cut polytope, which is the convex hull of all valid graph cuts. [sent-25, score-0.42]
</p><p>11 We extend this work by deriving a new class of outer bounds on the marginal polytope for 1  non-binary and non-pairwise MRFs. [sent-27, score-0.946]
</p><p>12 The key realization is that valid constraints can be constructed by a series of projections onto the cut polytope1 . [sent-28, score-0.325]
</p><p>13 Mean ﬁeld algorithms optimize over an inner bound on the marginal polytope (which is not convex) by restricting the marginal vectors to those coming from simpler, e. [sent-51, score-0.978]
</p><p>14 Alternatively, we can relax the optimization to be over an outer bound on the marginal polytope and also bound the entropy function. [sent-55, score-1.094]
</p><p>15 Most message passing algorithms for evaluating marginal probabilities obtain locally consistent beliefs so that the pseudomarginals over the edges agree with the singleton pseudomarginals at the nodes. [sent-56, score-0.442]
</p><p>16 The solution is therefore sought within the local marginal polytope LOCAL(G) = { µ ≥ 0 |  s∈χi µi;s  = 1,  t∈χj µij;st  = µi;s }  (2)  Clearly, M ⊆ LOCAL(G) since true marginals are also locally consistent. [sent-57, score-0.865]
</p><p>17 Belief propagation can be seen as optimizing pseudomarginals over LOCAL(G) with a (non-convex) Bethe approximation to the entropy [15]. [sent-60, score-0.302]
</p><p>18 The log-determinant relaxation [12] is instead based on a semi-deﬁnite outer bound on the marginal polytope combined with a Gaussian approximation to the entropy function. [sent-62, score-1.16]
</p><p>19 Since the moment matrix M1 (µ) can be written as Eθ [(1 x)T (1 x)] for µ ∈ M, the outer bound is obtained simply by requiring only that the pseudomarginals lie in SDEF1 (Kn ) = {µ ∈ R+ | M1 (µ) 0}. [sent-63, score-0.297]
</p><p>20 The marginal polytope also plays a critical role in ﬁnding the MAP assignment. [sent-65, score-0.75]
</p><p>21 1 For reasons of clarity, our results will be given in terms of the binary marginal polytope, also called the correlation polytope, which is equivalent to the cut polytope of the suspension graph of the MRF [6]. [sent-69, score-0.989]
</p><p>22 2  Algorithm 1 Cutting-plane algorithm for probabilistic inference 1: OUTER ← LOCAL(G) 2: repeat 3: µ∗ ← argmaxµ∈OUTER { θ, µ − B ∗ (µ)} 4: Choose projection graph Gπ , e. [sent-70, score-0.28]
</p><p>23 The marginal polytope can be deﬁned by the intersection of a large number of linear inequalities. [sent-73, score-0.777]
</p><p>24 We focus on inequalities beyond those specifying LOCAL(G), in particular the cycle inequalities [4, 2, 6]. [sent-74, score-0.927]
</p><p>25 Given an assignment x ∈ {0, 1}n , (i, j) ∈ E is cut if xi = xj . [sent-76, score-0.265]
</p><p>26 The cycle inequalities arise from the observation that a cycle must have an even (possibly zero) number of cut edges. [sent-77, score-1.088]
</p><p>27 For a cycle C and any F ⊆ C such that |F | is odd, this constraint can be written as (i,j)∈C\F δ(xi = xj )+ (i,j)∈F δ(xi = xj ) ≥ 1. [sent-81, score-0.469]
</p><p>28 Thus (µij;00 + µij;11 ) ≥ 1  (µij;10 + µij;01 ) + (i,j)∈C\F  (4)  (i,j)∈F  is valid for any µ ∈ M{0,1} , the marginal polytope of a binary pairwise MRF. [sent-83, score-0.932]
</p><p>29 For a chordless circuit C, the cycle inequalities are facets of M{0,1} [4]. [sent-84, score-0.747]
</p><p>30 Although there are exponentially many cycles and cycle inequalities for a graph, Barahona and Mahjoub [4, 6] give a simple algorithm to separate the whole class of cycle inequalities. [sent-86, score-1.008]
</p><p>31 The shortest of all these paths will not use both copies of any node j (otherwise the path j1 to j2 would be shorter), and so deﬁnes a cycle in G and gives the minimum value of (i,j)∈C\F (µij;10 + µij;01 )+ (i,j)∈F (µij;00 + µij;11 ). [sent-89, score-0.46]
</p><p>32 If this is less than 1, we have found a violated cycle inequality; otherwise, µ satisﬁes all cycle inequalities. [sent-90, score-0.778]
</p><p>33 1) and tightening the outer bound on the marginal polytope by incorporating valid constraints that are violated by the current pseudomarginals. [sent-94, score-1.161]
</p><p>34 The projection graph (line 4) is not needed for binary pairwise MRFs and will be described in the next section. [sent-95, score-0.355]
</p><p>35 We start the algorithm (line 1) with the loose outer bound on the marginal polytope given by the local consistency constraints. [sent-96, score-1.028]
</p><p>36 The experiments in Section 5 use the separation algorithm for cycle inequalities. [sent-103, score-0.443]
</p><p>37 However, any class of valid constraints for the marginal polytope with an efﬁcient separation algorithm may be used in line 5. [sent-104, score-0.99]
</p><p>38 Other examples besides the cycle inequalities include the odd-wheel and bicycle odd-wheel inequalities [6], and also linear inequalities that enforce positive semi-deﬁniteness of M1 (µ). [sent-105, score-1.218]
</p><p>39 1) over a relaxation of the marginal polytope deﬁned by the intersection of all inequalities that can be returned in line 5. [sent-107, score-1.158]
</p><p>40 The log-determinant and TRW entropy approximations have two appealing fea3  Figure 1: Illustration of the projection Ψπ for one edge (i, j) ∈ E where χi = {0, 1, 2} and χj = {0, 1, 2, 3}. [sent-109, score-0.371]
</p><p>41 The projection graph Gπ , shown on the right, has 3 partitions for i and 7 for j. [sent-110, score-0.317]
</p><p>42 Since all integral vectors in the relaxation OUTER are extreme points of the marginal polytope, any integral µ∗ is the MAP assignment. [sent-117, score-0.383]
</p><p>43 4  Generalization to non-binary MRFs  In this section we give a new class of valid inequalities for the marginal polytope of non-binary and non-pairwise MRFs, and show how to efﬁciently separate this exponentially large set of inequalities. [sent-118, score-1.148]
</p><p>44 The key theoretical idea is to project the marginal polytope onto different binary marginal polytopes. [sent-119, score-1.004]
</p><p>45 Aggregation and projection are well-known techniques in polyhedral combinatorics for obtaining valid inequalities [6]. [sent-120, score-0.62]
</p><p>46 Given a linear projection Φ(x) = Ax, any valid inequality c Φ(x) ≤ b for Φ(x) also gives the valid inequality c Ax ≤ b for x. [sent-121, score-0.481]
</p><p>47 We obtain new inequalities by aggregating the values of each variable. [sent-122, score-0.291]
</p><p>48 Deﬁne q q the projection graph Gπ = (Vπ , Eπ ) so that there is a node for each πi ∈ πi , and nodes πi and r πj are connected if (i, j) ∈ E. [sent-130, score-0.361]
</p><p>49 We call the graph consisting of all possible variable partitions the full projection graph. [sent-131, score-0.361]
</p><p>50 In Figure 1 we show part of the full projection graph corresponding to one edge (i, j), where xi has three values and xj has four values. [sent-132, score-0.433]
</p><p>51 The following gives a projection of marginal vectors of non-binary MRFs onto the marginal polytope of the projection graph Gπ , which has binary variables for each partition. [sent-135, score-1.455]
</p><p>52 π (si )=π j i  j  To construct valid inequalities for each projection we need to characterize the image space. [sent-142, score-0.597]
</p><p>53 Let M{0,1} (Gπ ) denote the binary marginal polytope of the projection graph. [sent-143, score-0.944]
</p><p>54 This result allows valid inequalities for M{0,1} (Gπ ) to carry over to M. [sent-156, score-0.398]
</p><p>55 The single projection graph, 4  where |πi | = 1 for all i, has one node per variable and is surjective. [sent-159, score-0.274]
</p><p>56 The full projection graph has O(2k ) nodes per variable. [sent-160, score-0.302]
</p><p>57 A cutting-plane algorithm may begin by projecting onto a small graph, then expanding to larger graphs only after satisfying all inequalities given by the smaller one. [sent-161, score-0.375]
</p><p>58 These projections yield a new class of cycle inequalities for the marginal polytope. [sent-163, score-0.849]
</p><p>59 Consider a single projection graph Gπ , a cycle C in G, and any F ⊆ C such that |F | is odd. [sent-164, score-0.625]
</p><p>60 We obtain the following valid inequality for µ ∈ M by applying the projection Ψπ and the cycle inequality: µπ (xi = xj ) + ij (i,j)∈C\F  µπ (xi = xj ) ≥ 1, ij  (5)  (i,j)∈F  where µπ (xi = xj ) ij  =  µij;si sj  (6)  µij;si sj . [sent-166, score-1.343]
</p><p>61 πi (si )=πj (sj )  µπ (xi = xj ) ij  = si ∈χi ,sj ∈χj s. [sent-169, score-0.267]
</p><p>62 For every single projection graph Gπ and every cycle inequality arising from a chordless circuit C on Gπ , ∃µ ∈ LOCAL(G)\M such that µ violates that inequality. [sent-175, score-0.761]
</p><p>63 The polytope resulting from the projection of M onto the remaining values (e. [sent-182, score-0.784]
</p><p>64 Barahona and Mahjoub [4] showed that the cycle inequality on the chordless circuit C is facet-deﬁning for M{0,1} . [sent-185, score-0.481]
</p><p>65 Note that the theorem implies that the projected cycle inequalities are strictly tighter than LOCAL(G), but it does not characterize how much is gained. [sent-189, score-0.754]
</p><p>66 However, since for every cycle inequality in the single projection graphs there is an equivalent cycle inequality in the full projection graph, it sufﬁces to consider just the full projection graph. [sent-191, score-1.38]
</p><p>67 Thus, even though the projection is not surjective, the full projection graph, which has O(n2k ) nodes, allows us to efﬁciently obtain a tighter relaxation than any combination of projection graphs would give. [sent-192, score-0.752]
</p><p>68 In particular, the separation problem for all cycle inequalities (5) for all single projection graphs, when we allow some additional valid inequalities for M (arising from the cycle using more than one partition for some variables), can now be solved in time O(poly(n, 2k )). [sent-193, score-1.688]
</p><p>69 They used value-aggregation to show that a class of cycle inequalities (corresponding to Eq. [sent-200, score-0.636]
</p><p>70 5 for |F | = 1) are valid for this polytope, and give an algorithm to separate the inequalities for a single cycle. [sent-201, score-0.398]
</p><p>71 These results could be applied to non-pairwise MRFs by ﬁrst projecting the marginal vector onto the marginal polytope of a pairwise MRF. [sent-204, score-1.033]
</p><p>72 We can now apply the projections of the previous section, considering various partitions of each cluster variable, to obtain a tighter relaxation of the marginal polytope. [sent-208, score-0.43]
</p><p>73 5  8  2  4  6  Coupling, θ ∼ U[−x, x]  8  Figure 2: Accuracy of single node marginals on 10 node complete graph (100 trials). [sent-221, score-0.334]
</p><p>74 These trials are on Ising models, which are pairwise MRFs with xi ∈ {−1, 1} and potentials φi (x) = xi for i ∈ V and φij (x) = xi xj for (i, j) ∈ E. [sent-224, score-0.308]
</p><p>75 We used the glpkmex and YALMIP optimization packages within Matlab, and wrote the separation algorithm for the cycle inequalities in Java. [sent-228, score-0.734]
</p><p>76 However, both entropy approximations give signiﬁcantly better pseudomarginals when used by our algorithm together with the cycle inequalities (see “TRW + Cycle” and “Logdet + Cycle” in the ﬁgures). [sent-236, score-0.922]
</p><p>77 For small MRFs, we can exactly represent the marginal polytope as the convex hull of its 2n vertices. [sent-237, score-0.772]
</p><p>78 We found that the cycle inequalities give nearly as good accuracy as the exact marginal polytope (see “TRW + Marg” and “Logdet + Marg”). [sent-238, score-1.435]
</p><p>79 Our work sheds some light on the relative value of the entropy approximation compared to the relaxation of the marginal polytope. [sent-239, score-0.413]
</p><p>80 When the MRF is weakly coupled, both entropy approximations do reasonably well using the local consistency polytope. [sent-240, score-0.267]
</p><p>81 This is not surprising: the limit of weak coupling is a fully disconnected graph, for which both the entropy approximation and the marginal polytope relaxation are exact. [sent-241, score-1.034]
</p><p>82 With the local consistency polytope, both entropy approximations get steadily worse as the coupling increases. [sent-242, score-0.322]
</p><p>83 In contrast, using the exact marginal polytope, we see a peak at θ = 2, then a steady improvement in accuracy as the coupling term grows. [sent-243, score-0.288]
</p><p>84 This occurs because the limit of strong coupling is the MAP problem, for which using the exact marginal polytope will give exact results. [sent-244, score-0.855]
</p><p>85 Our algorithm seems to “solve” the part of the problem caused by the local consistency polytope relaxation: TRW’s accuracy goes from . [sent-246, score-0.687]
</p><p>86 Next, we looked at the number of iterations (in terms of the loop in Algorithm 1) the algorithm takes before all cycle inequalities are satisﬁed. [sent-272, score-0.67]
</p><p>87 In each iteration we add to OUTER at most2 n violated cycle inequalities, coming from the n shortest paths. [sent-273, score-0.487]
</p><p>88 In Figure 3 we show boxplots of the l1 error of the single node marginals for both 10x10 grid MRFs (40 trials) and 20 node complete MRFs (10 trials). [sent-274, score-0.255]
</p><p>89 For the grid MRFs, all of the cycle inequalities were satisﬁed within 10 iterations. [sent-279, score-0.666]
</p><p>90 For the complete graph MRFs, the algorithm took many more iterations before all cycle inequalities were satisﬁed. [sent-281, score-0.779]
</p><p>91 Using the k-projection graph and projected cycle inequalities, we succeeded in ﬁnding the MAP assignment for all proteins except for the protein ‘1rl6’. [sent-290, score-0.625]
</p><p>92 For the protein ‘1rl6’, after 12 cutting-plane iterations, the solution was not integral, and we could not ﬁnd any violated cycle inequalities using the k-projection graph. [sent-293, score-0.796]
</p><p>93 Figure 4 shows one of the cycle inequalities (5) in the full projection graph that was found to be violated. [sent-295, score-0.938]
</p><p>94 This example shows that the relaxation given by the full projection graph is strictly tighter than that of the k-projection graph. [sent-303, score-0.482]
</p><p>95 2  Many fewer inequalities were added, since not all cycles in G are simple cycles in G. [sent-304, score-0.345]
</p><p>96 Our results show that the new outer bounds are powerful, allowing us to ﬁnd the MAP solution for all of the MRFs, and suggesting that using them will also lead to signiﬁcantly more accurate marginals for non-binary MRFs. [sent-312, score-0.259]
</p><p>97 6  Conclusion  The facial structure of the cut polytope, equivalently, the binary marginal polytope, has been wellstudied over the last twenty years. [sent-313, score-0.314]
</p><p>98 The cycle inequalities are just one of many large classes of valid inequalities for the cut polytope for which efﬁcient separation algorithms are known. [sent-314, score-1.805]
</p><p>99 Our theoretical results can be used to derive outer bounds for the marginal polytope from any of the valid inequalities on the cut polytope. [sent-315, score-1.451]
</p><p>100 An interesting open problem is to develop new message-passing algorithms which can incorporate cycle and other inequalities, to efﬁciently do the optimization within the cutting-plane algorithm. [sent-317, score-0.345]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('polytope', 0.566), ('cycle', 0.345), ('inequalities', 0.291), ('trw', 0.245), ('mrfs', 0.197), ('marginal', 0.184), ('projection', 0.171), ('outer', 0.157), ('ij', 0.14), ('entropy', 0.139), ('pseudomarginals', 0.116), ('graph', 0.109), ('valid', 0.107), ('cut', 0.107), ('logdet', 0.099), ('separation', 0.098), ('tighter', 0.09), ('relaxation', 0.09), ('violated', 0.088), ('node', 0.081), ('si', 0.077), ('barahona', 0.072), ('protein', 0.072), ('marg', 0.066), ('marginals', 0.063), ('map', 0.06), ('assignment', 0.057), ('cutting', 0.055), ('coupling', 0.055), ('pairwise', 0.052), ('local', 0.052), ('xi', 0.051), ('sj', 0.051), ('xj', 0.05), ('chordless', 0.05), ('inequality', 0.048), ('onto', 0.047), ('consistency', 0.045), ('proteins', 0.042), ('plane', 0.042), ('partition', 0.04), ('bounds', 0.039), ('circuit', 0.038), ('graphs', 0.037), ('integral', 0.037), ('partitions', 0.037), ('constraints', 0.035), ('extreme', 0.035), ('iterations', 0.034), ('shortest', 0.034), ('althaus', 0.033), ('kingsford', 0.033), ('mahjoub', 0.033), ('lp', 0.031), ('wainwright', 0.031), ('approximations', 0.031), ('potentials', 0.031), ('mrf', 0.03), ('grid', 0.03), ('edge', 0.03), ('koster', 0.029), ('cplex', 0.029), ('rosetta', 0.029), ('polyhedral', 0.029), ('projections', 0.029), ('characterize', 0.028), ('ti', 0.028), ('variational', 0.028), ('intersection', 0.027), ('cycles', 0.027), ('jaakkola', 0.027), ('spanning', 0.026), ('propagation', 0.026), ('edges', 0.026), ('exact', 0.025), ('bethe', 0.025), ('psd', 0.025), ('yanover', 0.025), ('upper', 0.024), ('bound', 0.024), ('constraint', 0.024), ('programming', 0.024), ('accuracy', 0.024), ('facets', 0.023), ('entropies', 0.023), ('amino', 0.023), ('trees', 0.023), ('belief', 0.023), ('binary', 0.023), ('trials', 0.022), ('variable', 0.022), ('full', 0.022), ('hull', 0.022), ('combinatorics', 0.022), ('satisfaction', 0.022), ('optimizing', 0.021), ('nding', 0.021), ('cuts', 0.02), ('coming', 0.02), ('ax', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="141-tfidf-1" href="./nips-2007-New_Outer_Bounds_on_the_Marginal_Polytope.html">141 nips-2007-New Outer Bounds on the Marginal Polytope</a></p>
<p>Author: David Sontag, Tommi S. Jaakkola</p><p>Abstract: We give a new class of outer bounds on the marginal polytope, and propose a cutting-plane algorithm for efﬁciently optimizing over these constraints. When combined with a concave upper bound on the entropy, this gives a new variational inference algorithm for probabilistic inference in discrete Markov Random Fields (MRFs). Valid constraints on the marginal polytope are derived through a series of projections onto the cut polytope. As a result, we obtain tighter upper bounds on the log-partition function. We also show empirically that the approximations of the marginals are signiﬁcantly more accurate when using the tighter outer bounds. Finally, we demonstrate the advantage of the new constraints for ﬁnding the MAP assignment in protein structure prediction. 1</p><p>2 0.16177593 <a title="141-tfidf-2" href="./nips-2007-Local_Algorithms_for_Approximate_Inference_in_Minor-Excluded_Graphs.html">121 nips-2007-Local Algorithms for Approximate Inference in Minor-Excluded Graphs</a></p>
<p>Author: Kyomin Jung, Devavrat Shah</p><p>Abstract: We present a new local approximation algorithm for computing MAP and logpartition function for arbitrary exponential family distribution represented by a ﬁnite-valued pair-wise Markov random ﬁeld (MRF), say G. Our algorithm is based on decomposing G into appropriately chosen small components; computing estimates locally in each of these components and then producing a good global solution. We prove that the algorithm can provide approximate solution within arbitrary accuracy when G excludes some ﬁnite sized graph as its minor and G has bounded degree: all Planar graphs with bounded degree are examples of such graphs. The running time of the algorithm is Θ(n) (n is the number of nodes in G), with constant dependent on accuracy, degree of graph and size of the graph that is excluded as a minor (constant for Planar graphs). Our algorithm for minor-excluded graphs uses the decomposition scheme of Klein, Plotkin and Rao (1993). In general, our algorithm works with any decomposition scheme and provides quantiﬁable approximation guarantee that depends on the decomposition scheme.</p><p>3 0.14961065 <a title="141-tfidf-3" href="./nips-2007-Fixing_Max-Product%3A_Convergent_Message_Passing_Algorithms_for_MAP_LP-Relaxations.html">92 nips-2007-Fixing Max-Product: Convergent Message Passing Algorithms for MAP LP-Relaxations</a></p>
<p>Author: Amir Globerson, Tommi S. Jaakkola</p><p>Abstract: We present a novel message passing algorithm for approximating the MAP problem in graphical models. The algorithm is similar in structure to max-product but unlike max-product it always converges, and can be proven to ﬁnd the exact MAP solution in various settings. The algorithm is derived via block coordinate descent in a dual of the LP relaxation of MAP, but does not require any tunable parameters such as step size or tree weights. We also describe a generalization of the method to cluster based potentials. The new method is tested on synthetic and real-world problems, and compares favorably with previous approaches. Graphical models are an effective approach for modeling complex objects via local interactions. In such models, a distribution over a set of variables is assumed to factor according to cliques of a graph with potentials assigned to each clique. Finding the assignment with highest probability in these models is key to using them in practice, and is often referred to as the MAP (maximum aposteriori) assignment problem. In the general case the problem is NP hard, with complexity exponential in the tree-width of the underlying graph. Linear programming (LP) relaxations have proven very useful in approximating the MAP problem, and often yield satisfactory empirical results. These approaches relax the constraint that the solution is integral, and generally yield non-integral solutions. However, when the LP solution is integral, it is guaranteed to be the exact MAP. For some classes of problems the LP relaxation is provably correct. These include the minimum cut problem and maximum weight matching in bi-partite graphs [8]. Although LP relaxations can be solved using standard LP solvers, this may be computationally intensive for large problems [13]. The key problem with generic LP solvers is that they do not use the graph structure explicitly and thus may be sub-optimal in terms of computational efﬁciency. The max-product method [7] is a message passing algorithm that is often used to approximate the MAP problem. In contrast to generic LP solvers, it makes direct use of the graph structure in constructing and passing messages, and is also very simple to implement. The relation between max-product and the LP relaxation has remained largely elusive, although there are some notable exceptions: For tree-structured graphs, max-product and LP both yield the exact MAP. A recent result [1] showed that for maximum weight matching on bi-partite graphs max-product and LP also yield the exact MAP [1]. Finally, Tree-Reweighted max-product (TRMP) algorithms [5, 10] were shown to converge to the LP solution for binary xi variables, as shown in [6]. In this work, we propose the Max Product Linear Programming algorithm (MPLP) - a very simple variation on max-product that is guaranteed to converge, and has several advantageous properties. MPLP is derived from the dual of the LP relaxation, and is equivalent to block coordinate descent in the dual. Although this results in monotone improvement of the dual objective, global convergence is not always guaranteed since coordinate descent may get stuck in suboptimal points. This can be remedied using various approaches, but in practice we have found MPLP to converge to the LP 1 solution in a majority of the cases we studied. To derive MPLP we use a special form of the dual LP, which involves the introduction of redundant primal variables and constraints. We show how the dual variables corresponding to these constraints turn out to be the messages in the algorithm. We evaluate the method on Potts models and protein design problems, and show that it compares favorably with max-product (which often does not converge for these problems) and TRMP. 1 The Max-Product and MPLP Algorithms The max-product algorithm [7] is one of the most often used methods for solving MAP problems. Although it is neither guaranteed to converge to the correct solution, or in fact converge at all, it provides satisfactory results in some cases. Here we present two algorithms: EMPLP (edge based MPLP) and NMPLP (node based MPLP), which are structurally very similar to max-product, but have several key advantages: • After each iteration, the messages yield an upper bound on the MAP value, and the sequence of bounds is monotone decreasing and convergent. The messages also have a limit point that is a ﬁxed point of the update rule. • No additional parameters (e.g., tree weights as in [6]) are required. • If the ﬁxed point beliefs have a unique maximizer then they correspond to the exact MAP. • For binary variables, MPLP can be used to obtain the solution to an LP relaxation of the MAP problem. Thus, when this LP relaxation is exact and variables are binary, MPLP will ﬁnd the MAP solution. Moreover, for any variable whose beliefs are not tied, the MAP assignment can be found (i.e., the solution is partially decodable). Pseudo code for the algorithms (and for max-product) is given in Fig. 1. As we show in the next sections, MPLP is essentially a block coordinate descent algorithm in the dual of a MAP LP relaxation. Every update of the MPLP messages corresponds to exact minimization of a set of dual variables. For EMPLP minimization is over the set of variables corresponding to an edge, and for NMPLP it is over the set of variables corresponding to all the edges a given node appears in (i.e., a star). The properties of MPLP result from its relation to the LP dual. In what follows we describe the derivation of the MPLP algorithms and prove their properties. 2 The MAP Problem and its LP Relaxation We consider functions over n variables x = {x1 , . . . , xn } deﬁned as follows. Given a graph G = (V, E) with n vertices, and potentials θij (xi , xj ) for all edges ij ∈ E, deﬁne the function1 f (x; θ) = θij (xi , xj ) . (1) ij∈E The MAP problem is deﬁned as ﬁnding an assignment xM that maximizes the function f (x; θ). Below we describe the standard LP relaxation for this problem. Denote by {µij (xi , xj )}ij∈E distributions over variables corresponding to edges ij ∈ E and {µi (xi )}i∈V distributions corresponding to nodes i ∈ V . We will use µ to denote a given set of distributions over all edges and nodes. The set ML (G) is deﬁned as the set of µ where pairwise and singleton distributions are consistent x ˆ xi µij (ˆi , xj ) = µj (xj ) , ˆ xj µij (xi , xj ) = µi (xi ) ∀ij ∈ E, xi , xj ˆ ML (G) = µ ≥ 0 ∀i ∈ V xi µi (xi ) = 1 Now consider the following linear program: MAPLPR : µL∗ = arg max µ∈ML (G) µ·θ. (2) where µ·θ is shorthand for µ·θ = ij∈E xi ,xj θij (xi , xj )µij (xi , xj ). It is easy to show (see e.g., [10]) that the optimum of MAPLPR yields an upper bound on the MAP value, i.e. µL∗ ·θ ≥ f (xM ). Furthermore, when the optimal µi (xi ) have only integral values, the assignment that maximizes µi (xi ) yields the correct MAP assignment. In what follows we show how the MPLP algorithms can be derived from the dual of MAPLPR. 1 P We note that some authors also add a term i∈V θi (xi ) to f (x; θ). However, these terms can be included in the pairwise functions θij (xi , xj ), so we ignore them for simplicity. 2 3 The LP Relaxation Dual Since MAPLPR is an LP, it has an equivalent convex dual. In App. A we derive a special dual of MAPLPR using a different representation of ML (G) with redundant variables. The advantage of this dual is that it allows the derivation of simple message passing algorithms. The dual is described in the following proposition. Proposition 1 The following optimization problem is a convex dual of MAPLPR DMAPLPR : min max xi i s.t. max βki (xk , xi ) (3) k∈N (i) xk βji (xj , xi ) + βij (xi , xj ) = θij (xi , xj ) , where the dual variables are βij (xi , xj ) for all ij, ji ∈ E and values of xi and xj . The dual has an intuitive interpretation in terms of re-parameterizations. Consider the star shaped graph Gi consisting of node i and all its neighbors N (i). Assume the potential on edge ki (for k ∈ N (i)) is βki (xk , xi ). The value of the MAP assignment for this model is max max βki (xk , xi ). This is exactly the term in the objective of DMAPLPR. Thus the dual xi k∈N (i) xk corresponds to individually decoding star graphs around all nodes i ∈ V where the potentials on the graph edges should sum to the original potential. It is easy to see that this will always result in an upper bound on the MAP value. The somewhat surprising result of the duality is that there exists a β assignment such that star decoding yields the optimal value of MAPLPR. 4 Block Coordinate Descent in the Dual To obtain a convergent algorithm we use a simple block coordinate descent strategy. At every iteration, ﬁx all variables except a subset, and optimize over this subset. It turns out that this can be done in closed form for the cases we consider. We begin by deriving the EMPLP algorithm. Consider ﬁxing all the β variables except those corresponding to some edge ij ∈ E (i.e., βij and βji ), and minimizing DMAPLPR over the non-ﬁxed variables. Only two terms in the DMAPLPR objective depend on βij and βji . We can write those as f (βij , βji ) = max λ−j (xi ) + max βji (xj , xi ) + max λ−i (xj ) + max βij (xi , xj ) i j xi where we deﬁned λ−j (xi ) = i xj k∈N (i)\j xi xi (4) λki (xi ) and λki (xi ) = maxxk βki (xk , xi ) as in App. A. Note that the function f (βij , βji ) depends on the other β values only through λ−i (xj ) and λ−j (xi ). j i This implies that the optimization can be done solely in terms of λij (xj ) and there is no need to store the β values explicitly. The optimal βij , βji are obtained by minimizing f (βij , βji ) subject to the re-parameterization constraint βji (xj , xi ) + βij (xi , xj ) = θij (xi , xj ). The following proposition characterizes the minimum of f (βij , βji ). In fact, as mentioned above, we do not need to characterize the optimal βij (xi , xj ) itself, but only the new λ values. Proposition 2 Maximizing the function f (βij , βji ) yields the following λji (xi ) (and the equivalent expression for λij (xj )) 1 −j 1 λji (xi ) = − λi (xi ) + max λ−i (xj ) + θij (xi , xj ) j 2 2 xj The proposition is proved in App. B. The λ updates above result in the EMPLP algorithm, described in Fig. 1. Note that since the β optimization affects both λji (xi ) and λij (xj ), both these messages need to be updated simultaneously. We proceed to derive the NMPLP algorithm. For a given node i ∈ V , we consider all its neighbors j ∈ N (i), and wish to optimize over the variables βji (xj , xi ) for ji, ij ∈ E (i.e., all the edges in a star centered on i), while the other variables are ﬁxed. One way of doing so is to use the EMPLP algorithm for the edges in the star, and iterate it until convergence. We now show that the result of 3 Inputs: A graph G = (V, E), potential functions θij (xi , xj ) for each edge ij ∈ E. Initialization: Initialize messages to any value. Algorithm: • Iterate until a stopping criterion is satisﬁed: – Max-product: Iterate over messages and update (cji shifts the max to zero) h i mji (xi )← max m−i (xj ) + θij (xi , xj ) − cji j xj – EMPLP: For each ij ∈ E, update λji (xi ) and λij (xj ) simultaneously (the update for λij (xj ) is the same with i and j exchanged) h i 1 1 λji (xi )← − λ−j (xi ) + max λ−i (xj ) + θij (xi , xj ) j i 2 2 xj – NMPLP: Iterate over nodes i ∈ V and update all γij (xj ) where j ∈ N (i) 2 3 X 2 γij (xj )← max 4θij (xi , xj ) − γji (xi ) + γki (xi )5 xi |N (i)| + 1 k∈N(i) • Calculate node “beliefs”: Set biP i ) to be the sum of incoming messages into node i ∈ V (x (e.g., for NMPLP set bi (xi ) = k∈N(i) γki (xi )). Output: Return assignment x deﬁned as xi = arg maxxi b(ˆi ). x ˆ Figure 1: The max-product, EMPLP and NMPLP algorithms. Max-product, EMPLP and NMPLP use mesP sages mij , λij and γij respectively. We use the notation m−i (xj ) = k∈N(j)\i mkj (xj ). j this optimization can be found in closed form. The assumption about β being ﬁxed outside the star implies that λ−i (xj ) is ﬁxed. Deﬁne: γji (xi ) = maxxj θij (xi , xj ) + λ−i (xj ) . Simple algebra j j yields the following relation between λ−j (xi ) and γki (xi ) for k ∈ N (i) i 2 λ−j (xi ) = −γji (xi ) + γki (xi ) (5) i |N (i)| + 1 k∈N (i) Plugging this into the deﬁnition of γji (xi ) we obtain the NMPLP update in Fig. 1. The messages for both algorithms can be initialized to any value since it can be shown that after one iteration they will correspond to valid β values. 5 Convergence Properties The MPLP algorithm decreases the dual objective (i.e., an upper bound on the MAP value) at every iteration, and thus its dual objective values form a convergent sequence. Using arguments similar to [5] it can be shown that MPLP has a limit point that is a ﬁxed point of its updates. This in itself does not guarantee convergence to the dual optimum since coordinate descent algorithms may get stuck at a point that is not a global optimum. There are ways of overcoming this difﬁculty, for example by smoothing the objective [4] or using techniques as in [2] (see p. 636). We leave such extensions for further work. In this section we provide several results about the properties of the MPLP ﬁxed points and their relation to the corresponding LP. First, we claim that if all beliefs have unique maxima then the exact MAP assignment is obtained. Proposition 3 If the ﬁxed point of MPLP has bi (xi ) such that for all i the function bi (xi ) has a unique maximizer x∗ , then x∗ is the solution to the MAP problem and the LP relaxation is exact. i Since the dual objective is always greater than or equal to the MAP value, it sufﬁces to show that there exists a dual feasible point whose objective value is f (x∗ ). Denote by β ∗ , λ∗ the value of the corresponding dual parameters at the ﬁxed point of MPLP. Then the dual objective satisﬁes λ∗ (xi ) = ki max i xi k∈N (i) ∗ max βki (xk , x∗ ) = i i k∈N (i) xk ∗ βki (x∗ , x∗ ) = f (x∗ ) k i i 4 k∈N (i) To see why the second equality holds, note that bi (x∗ ) = maxxi ,xj λ−j (xi ) + βji (xj , xi ) and i i bj (x∗ ) = maxxi ,xj λ−i (xj ) + βij (xi , xj ). By the equalization property in Eq. 9 the arguments of j j the two max operations are equal. From the unique maximum assumption it follows that x∗ , x∗ are i j the unique maximizers of the above. It follows that βji , βij are also maximized by x∗ , x∗ . i j In the general case, the MPLP ﬁxed point may not correspond to a primal optimum because of the local optima problem with coordinate descent. However, when the variables are binary, ﬁxed points do correspond to primal solutions, as the following proposition states. Proposition 4 When xi are binary, the MPLP ﬁxed point can be used to obtain the primal optimum. The claim can be shown by constructing a primal optimal solution µ∗ . For tied bi , set µ∗ (xi ) to 0.5 i and for untied bi , set µ∗ (x∗ ) to 1. If bi , bj are not tied we set µ∗ (x∗ , x∗ ) = 1. If bi is not tied but bj i i ij i j is, we set µ∗ (x∗ , xj ) = 0.5. If bi , bj are tied then βji , βij can be shown to be maximized at either ij i x∗ , x∗ = (0, 0), (1, 1) or x∗ , x∗ = (0, 1), (1, 0). We then set µ∗ to be 0.5 at one of these assignment i j i j ij ∗ pairs. The resulting µ∗ is clearly primal feasible. Setting δi = b∗ we obtain that the dual variables i (δ ∗ , λ∗ , β ∗ ) and primal µ∗ satisfy complementary slackness for the LP in Eq. 7 and therefore µ∗ is primal optimal. The binary optimality result implies partial decodability, since [6] shows that the LP is partially decodable for binary variables. 6 Beyond pairwise potentials: Generalized MPLP In the previous sections we considered maximizing functions which factor according to the edges of the graph. A more general setting considers clusters c1 , . . . , ck ⊂ {1, . . . , n} (the set of clusters is denoted by C), and a function f (x; θ) = c θc (xc ) deﬁned via potentials over clusters θc (xc ). The MAP problem in this case also has an LP relaxation (see e.g. [11]). To deﬁne the LP we introduce the following deﬁnitions: S = {c ∩ c : c, c ∈ C, c ∩ c = ∅} is the set of intersection between clusters ˆ ˆ ˆ and S(c) = {s ∈ S : s ⊆ c} is the set of overlap sets for cluster c.We now consider marginals over the variables in c ∈ C and s ∈ S and require that cluster marginals agree on their overlap. Denote this set by ML (C). The LP relaxation is then to maximize µ · θ subject to µ ∈ ML (C). As in Sec. 4, we can derive message passing updates that result in monotone decrease of the dual LP of the above relaxation. The derivation is similar and we omit the details. The key observation is that one needs to introduce |S(c)| copies of each marginal µc (xc ) (instead of the two copies in the pairwise case). Next, as in the EMPLP derivation we assume all β are ﬁxed except those corresponding to some cluster c. The resulting messages are λc→s (xs ) from a cluster c to all of its intersection sets s ∈ S(c). The update on these messages turns out to be:   1 1 λ−c (xs ) + max  λ−c (xs ) + θc (xc ) λc→s (xs ) = − 1 − ˆ s s ˆ |S(c)| |S(c)| xc\s s∈S(c)\s ˆ where for a given c ∈ C all λc→s should be updated simultaneously for s ∈ S(c), and λ−c (xs ) is s deﬁned as the sum of messages into s that are not from c. We refer to this algorithm as Generalized EMPLP (GEMPLP). It is possible to derive an algorithm similar to NMPLP that updates several clusters simultaneously, but its structure is more involved and we do not address it here. 7 Related Work Weiss et al. [11] recently studied the ﬁxed points of a class of max-product like algorithms. Their analysis focused on properties of ﬁxed points rather than convergence guarantees. Speciﬁcally, they showed that if the counting numbers used in a generalized max-product algorithm satisfy certain properties, then its ﬁxed points will be the exact MAP if the beliefs have unique maxima, and for binary variables the solution can be partially decodable. Both these properties are obtained for the MPLP ﬁxed points, and in fact we can show that MPLP satisﬁes the conditions in [11], so that we obtain these properties as corollaries of [11]. We stress however, that [11] does not address convergence of algorithms, but rather properties of their ﬁxed points, if they converge. MPLP is similar in some aspects to Kolmogorov’s TRW-S algorithm [5]. TRW-S is also a monotone coordinate descent method in a dual of the LP relaxation and its ﬁxed points also have similar 5 guarantees to those of MPLP [6]. Furthermore, convergence to a local optimum may occur, as it does for MPLP. One advantage of MPLP lies in the simplicity of its updates and the fact that it is parameter free. The other is its simple generalization to potentials over clusters of nodes (Sec. 6). Recently, several new dual LP algorithms have been introduced, which are more closely related to our formalism. Werner [12] presented a class of algorithms which also improve the dual LP at every iteration. The simplest of those is the max-sum-diffusion algorithm, which is similar to our EMPLP algorithm, although the updates are different from ours. Independently, Johnson et al. [4] presented a class of algorithms that improve duals of the MAP-LP using coordinate descent. They decompose the model into tractable parts by replicating variables and enforce replication constraints within the Lagrangian dual. Our basic formulation in Eq. 3 could be derived from their perspective. However, the updates in the algorithm and the analysis differ. Johnson et al. also presented a method for overcoming the local optimum problem, by smoothing the objective so that it is strictly convex. Such an approach could also be used within our algorithms. Vontobel and Koetter [9] recently introduced a coordinate descent algorithm for decoding LDPC codes. Their method is speciﬁcally tailored for this case, and uses updates that are similar to our edge based updates. Finally, the concept of dual coordinate descent may be used in approximating marginals as well. In [3] we use such an approach to optimize a variational bound on the partition function. The derivation uses some of the ideas used in the MPLP dual, but importantly does not ﬁnd the minimum for each coordinate. Instead, a gradient like step is taken at every iteration to decrease the dual objective. 8 Experiments We compared NMPLP to three other message passing algorithms:2 Tree-Reweighted max-product (TRMP) [10],3 standard max-product (MP), and GEMPLP. For MP and TRMP we used the standard approach of damping messages using a factor of α = 0.5. We ran all algorithms for a maximum of 2000 iterations, and used the hit-time measure to compare their speed of convergence. This measure is deﬁned as follows: At every iteration the beliefs can be used to obtain an assignment x with value f (x). We deﬁne the hit-time as the ﬁrst iteration at which the maximum value of f (x) is achieved.4 We ﬁrst experimented with a 10 × 10 grid graph, with 5 values per state. The function f (x) was 5 a Potts model: f (x) = The values for θij and θi (xi ) ij∈E θij I(xi = xj ) + i∈V θi (xi ). were randomly drawn from [−cI , cI ] and [−cF , cF ] respectively, and we used values of cI and cF in the range range [0.1, 2.35] (with intervals of 0.25), resulting in 100 different models. The clusters for GEMPLP were the faces of the graph [14]. To see if NMPLP converges to the LP solution we also used an LP solver to solve the LP relaxation. We found that the the normalized difference between NMPLP and LP objective was at most 10−3 (median 10−7 ), suggesting that NMPLP typically converged to the LP solution. Fig. 2 (top row) shows the results for the three algorithms. It can be seen that while all non-cluster based algorithms obtain similar f (x) values, NMPLP has better hit-time (in the median) than TRMP and MP, and MP does not converge in many cases (see caption). GEMPLP converges more slowly than NMPLP, but obtains much better f (x) values. In fact, in 99% of the cases the normalized difference between the GEMPLP objective and the f (x) value was less than 10−5 , suggesting that the exact MAP solution was found. We next applied the algorithms to the real world problems of protein design. In [13], Yanover et al. show how these problems can be formalized in terms of ﬁnding a MAP in an appropriately constructed graphical model.6 We used all algorithms except GNMPLP (since there is no natural choice for clusters in this case) to approximate the MAP solution on the 97 models used in [13]. In these models the number of states per variable is 2 − 158, and there are up to 180 variables per model. Fig. 2 (bottom) shows results for all the design problems. In this case only 11% of the MP runs converged, and NMPLP was better than TRMP in terms of hit-time and comparable in f (x) value. The performance of MP was good on the runs where it converged. 2 As expected, NMPLP was faster than EMPLP so only NMPLP results are given. The edge weights for TRMP corresponded to a uniform distribution over all spanning trees. 4 This is clearly a post-hoc measure since it can only be obtained after the algorithm has exceeded its maximum number of iterations. However, it is a reasonable algorithm-independent measure of convergence. 5 The potential θi (xi ) may be folded into the pairwise potential to yield a model as in Eq. 1. 6 Data available from http://jmlr.csail.mit.edu/papers/volume7/yanover06a/Rosetta Design Dataset.tgz 3 6 (a) (b) (c) 100 (d) 0.6 2000 0.04 0.4 0.02 −50 0 −0.02 −0.04 ∆(Value) 0 1000 ∆(Hit Time) ∆(Value) ∆(Hit Time) 50 0 MP TRMP GMPLP 0 −0.2 −1000 −0.4 −0.06 −100 0.2 MP TRMP GMPLP MP TRMP MP TRMP Figure 2: Evaluation of message passing algorithms on Potts models and protein design problems. (a,c): Convergence time results for the Potts models (a) and protein design problems (c). The box-plots (horiz. red line indicates median) show the difference between the hit-time for the other algorithms and NMPLP. (b,d): Value of integer solutions for the Potts models (b) and protein design problems (d). The box-plots show the normalized difference between the value of f (x) for NMPLP and the other algorithms. All ﬁgures are such that better MPLP performance yields positive Y axis values. Max-product converged on 58% of the cases for the Potts models, and on 11% of the protein problems. Only convergent max-product runs are shown. 9 Conclusion We have presented a convergent algorithm for MAP approximation that is based on block coordinate descent of the MAP-LP relaxation dual. The algorithm can also be extended to cluster based functions, which result empirically in improved MAP estimates. This is in line with the observations in [14] that generalized belief propagation algorithms can result in signiﬁcant performance improvements. However generalized max-product algorithms [14] are not guaranteed to converge whereas GMPLP is. Furthermore, the GMPLP algorithm does not require a region graph and only involves intersection between pairs of clusters. In conclusion, MPLP has the advantage of resolving the convergence problems of max-product while retaining its simplicity, and offering the theoretical guarantees of LP relaxations. We thus believe it should be useful in a wide array of applications. A Derivation of the dual Before deriving the dual, we ﬁrst express the constraint set ML (G) in a slightly different way. The deﬁnition of ML (G) in Sec. 2 uses a single distribution µij (xi , xj ) for every ij ∈ E. In what follows, we use two copies of this pairwise distribution for every edge, which we denote µij (xi , xj ) ¯ and µji (xj , xi ), and we add the constraint that these two copies both equal the original µij (xi , xj ). ¯ For this extended set of pairwise marginals, we consider the following set of constraints which is clearly equivalent to ML (G). On the rightmost column we give the dual variables that will correspond to each constraint (we omit non-negativity constraints). µij (xi , xj ) = µij (xi , xj ) ¯ µji (xj , xi ) = µij (xi , xj ) ¯ x xi µij (ˆi , xj ) = µj (xj ) ˆ ¯ µji (ˆj , xi ) = µi (xi ) ¯ x xj ˆ xi µi (xi ) = 1 ∀ij ∈ E, xi , xj ∀ij ∈ E, xi , xj ∀ij ∈ E, xj ∀ji ∈ E, xi ∀i ∈ V βij (xi , xj ) βji (xj , xi ) λij (xj ) λji (xi ) δi (6) ¯ We denote the set of (µ, µ) satisfying these constraints by ML (G). We can now state an LP that ¯ is equivalent to MAPLPR, only with an extended set of variables and constraints. The equivalent ¯ problem is to maximize µ · θ subject to (µ, µ) ∈ ML (G) (note that the objective uses the original ¯ µ copy). LP duality transformation of the extended problem yields the following LP min i δi s.t. λij (xj ) − βij (xi , xj ) ≥ 0 βij (xi , xj ) + βji (xj , xi ) = θij (xi , xj ) − k∈N (i) λki (xi ) + δi ≥ 0 ∀ij, ji ∈ E, xi , xj ∀ij ∈ E, xi , xj ∀i ∈ V, xi (7) We next simplify the above LP by eliminating some of its constraints and variables. Since each variable δi appears in only one constraint, and the objective minimizes δi it follows that δi = maxxi k∈N (i) λki (xi ) and the constraints with δi can be discarded. Similarly, since λij (xj ) appears in a single constraint, we have that for all ij ∈ E, ji ∈ E, xi , xj λij (xj ) = maxxi βij (xi , xj ) and the constraints with λij (xj ), λji (xi ) can also be discarded. Using the eliminated δi and λji (xi ) 7 variables, we obtain that the LP in Eq. 7 is equivalent to that in Eq. 3. Note that the objective in Eq. 3 is convex since it is a sum of point-wise maxima of convex functions. B Proof of Proposition 2 We wish to minimize f in Eq. 4 subject to the constraint that βij + βji = θij . Rewrite f as f (βij , βji ) = max λ−j (xi ) + βji (xj , xi ) + max λ−i (xj ) + βij (xi , xj ) j i xi ,xj xi ,xj (8) The sum of the two arguments in the max is λ−j (xi ) + λ−i (xj ) + θij (xi , xj ) i j (because of the constraints on β). Thus the minimum must be greater than −j −i 1 2 maxxi ,xj λi (xi ) + λj (xj ) + θij (xi , xj ) . One assignment to β that achieves this minimum is obtained by requiring an equalization condition:7 λ−i (xj ) + βij (xi , xj ) = λ−j (xi ) + βji (xj , xi ) = j i 1 θij (xi , xj ) + λ−j (xi ) + λ−i (xj ) i j 2 (9) which implies βij (xi , xj ) = 1 θij (xi , xj ) + λ−j (xi ) − λ−i (xj ) and a similar expression for βji . i j 2 The resulting λij (xj ) = maxxi βij (xi , xj ) are then the ones in Prop. 2. Acknowledgments The authors acknowledge support from the Defense Advanced Research Projects Agency (Transfer Learning program). Amir Globerson was also supported by the Rothschild Yad-Hanadiv fellowship. References [1] M. Bayati, D. Shah, and M. Sharma. Maximum weight matching via max-product belief propagation. IEEE Trans. on Information Theory (to appear), 2007. [2] D. P. Bertsekas, editor. Nonlinear Programming. Athena Scientiﬁc, Belmont, MA, 1995. [3] A. Globerson and T. Jaakkola. Convergent propagation algorithms via oriented trees. In UAI. 2007. [4] J.K. Johnson, D.M. Malioutov, and A.S. Willsky. Lagrangian relaxation for map estimation in graphical models. In Allerton Conf. Communication, Control and Computing, 2007. [5] V. Kolmogorov. Convergent tree-reweighted message passing for energy minimization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(10):1568–1583, 2006. [6] V. Kolmogorov and M. Wainwright. On the optimality of tree-reweighted max-product message passing. In 21st Conference on Uncertainty in Artiﬁcial Intelligence (UAI). 2005. [7] J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, 1988. [8] B. Taskar, S. Lacoste-Julien, and M. Jordan. Structured prediction, dual extragradient and bregman projections. Journal of Machine Learning Research, pages 1627–1653, 2006. [9] P.O. Vontobel and R. Koetter. Towards low-complexity linear-programming decoding. In Proc. 4th Int. Symposium on Turbo Codes and Related Topics, 2006. [10] M. J. Wainwright, T. Jaakkola, and A. S. Willsky. Map estimation via agreement on trees: messagepassing and linear programming. IEEE Trans. on Information Theory, 51(11):1120–1146, 2005. [11] Y. Weiss, C. Yanover, and T. Meltzer. Map estimation, linear programming and belief propagation with convex free energies. In UAI. 2007. [12] T. Werner. A linear programming approach to max-sum, a review. IEEE Trans. on PAMI, 2007. [13] C. Yanover, T. Meltzer, and Y. Weiss. Linear programming relaxations and belief propagation – an empirical study. Jourmal of Machine Learning Research, 7:1887–1907, 2006. [14] J.S. Yedidia, W.T. W.T. Freeman, and Y. Weiss. Constructing free-energy approximations and generalized belief propagation algorithms. IEEE Trans. on Information Theory, 51(7):2282–2312, 2005. 7 Other solutions are possible but may not yield some of the properties of MPLP. 8</p><p>4 0.12331227 <a title="141-tfidf-4" href="./nips-2007-Structured_Learning_with_Approximate_Inference.html">187 nips-2007-Structured Learning with Approximate Inference</a></p>
<p>Author: Alex Kulesza, Fernando Pereira</p><p>Abstract: In many structured prediction problems, the highest-scoring labeling is hard to compute exactly, leading to the use of approximate inference methods. However, when inference is used in a learning algorithm, a good approximation of the score may not be sufﬁcient. We show in particular that learning can fail even with an approximate inference method with rigorous approximation guarantees. There are two reasons for this. First, approximate methods can effectively reduce the expressivity of an underlying model by making it impossible to choose parameters that reliably give good predictions. Second, approximations can respond to parameter changes in such a way that standard learning algorithms are misled. In contrast, we give two positive results in the form of learning bounds for the use of LP-relaxed inference in structured perceptron and empirical risk minimization settings. We argue that without understanding combinations of inference and learning, such as these, that are appropriately compatible, learning performance under approximate inference cannot be guaranteed. 1</p><p>5 0.11897756 <a title="141-tfidf-5" href="./nips-2007-Loop_Series_and_Bethe_Variational_Bounds_in_Attractive_Graphical_Models.html">123 nips-2007-Loop Series and Bethe Variational Bounds in Attractive Graphical Models</a></p>
<p>Author: Alan S. Willsky, Erik B. Sudderth, Martin J. Wainwright</p><p>Abstract: Variational methods are frequently used to approximate or bound the partition or likelihood function of a Markov random ﬁeld. Methods based on mean ﬁeld theory are guaranteed to provide lower bounds, whereas certain types of convex relaxations provide upper bounds. In general, loopy belief propagation (BP) provides often accurate approximations, but not bounds. We prove that for a class of attractive binary models, the so–called Bethe approximation associated with any ﬁxed point of loopy BP always lower bounds the true likelihood. Empirically, this bound is much tighter than the naive mean ﬁeld bound, and requires no further work than running BP. We establish these lower bounds using a loop series expansion due to Chertkov and Chernyak, which we show can be derived as a consequence of the tree reparameterization characterization of BP ﬁxed points. 1</p><p>6 0.11051696 <a title="141-tfidf-6" href="./nips-2007-An_Analysis_of_Convex_Relaxations_for_MAP_Estimation.html">23 nips-2007-An Analysis of Convex Relaxations for MAP Estimation</a></p>
<p>7 0.10145524 <a title="141-tfidf-7" href="./nips-2007-Efficient_Bayesian_Inference_for_Dynamically_Changing_Graphs.html">75 nips-2007-Efficient Bayesian Inference for Dynamically Changing Graphs</a></p>
<p>8 0.096925594 <a title="141-tfidf-8" href="./nips-2007-Message_Passing_for_Max-weight_Independent_Set.html">128 nips-2007-Message Passing for Max-weight Independent Set</a></p>
<p>9 0.090168215 <a title="141-tfidf-9" href="./nips-2007-Linear_programming_analysis_of_loopy_belief_propagation_for_weighted_matching.html">120 nips-2007-Linear programming analysis of loopy belief propagation for weighted matching</a></p>
<p>10 0.083479419 <a title="141-tfidf-10" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>11 0.07784421 <a title="141-tfidf-11" href="./nips-2007-Cooled_and_Relaxed_Survey_Propagation_for_MRFs.html">64 nips-2007-Cooled and Relaxed Survey Propagation for MRFs</a></p>
<p>12 0.07455761 <a title="141-tfidf-12" href="./nips-2007-Efficient_Inference_for_Distributions_on_Permutations.html">77 nips-2007-Efficient Inference for Distributions on Permutations</a></p>
<p>13 0.067957416 <a title="141-tfidf-13" href="./nips-2007-Privacy-Preserving_Belief_Propagation_and_Sampling.html">157 nips-2007-Privacy-Preserving Belief Propagation and Sampling</a></p>
<p>14 0.063239992 <a title="141-tfidf-14" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>15 0.061666161 <a title="141-tfidf-15" href="./nips-2007-DIFFRAC%3A_a_discriminative_and_flexible_framework_for_clustering.html">65 nips-2007-DIFFRAC: a discriminative and flexible framework for clustering</a></p>
<p>16 0.061452925 <a title="141-tfidf-16" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>17 0.058772013 <a title="141-tfidf-17" href="./nips-2007-Adaptive_Embedded_Subgraph_Algorithms_using_Walk-Sum_Analysis.html">20 nips-2007-Adaptive Embedded Subgraph Algorithms using Walk-Sum Analysis</a></p>
<p>18 0.056110017 <a title="141-tfidf-18" href="./nips-2007-Learning_the_structure_of_manifolds_using_random_projections.html">116 nips-2007-Learning the structure of manifolds using random projections</a></p>
<p>19 0.054088239 <a title="141-tfidf-19" href="./nips-2007-Feature_Selection_Methods_for_Improving_Protein_Structure_Prediction_with_Rosetta.html">89 nips-2007-Feature Selection Methods for Improving Protein Structure Prediction with Rosetta</a></p>
<p>20 0.05256474 <a title="141-tfidf-20" href="./nips-2007-Hidden_Common_Cause_Relations_in_Relational_Learning.html">97 nips-2007-Hidden Common Cause Relations in Relational Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.181), (1, -0.003), (2, -0.101), (3, 0.025), (4, -0.082), (5, -0.259), (6, 0.011), (7, 0.163), (8, 0.071), (9, -0.074), (10, -0.054), (11, -0.002), (12, -0.07), (13, 0.066), (14, 0.067), (15, -0.002), (16, 0.024), (17, 0.065), (18, -0.075), (19, 0.01), (20, -0.047), (21, 0.02), (22, 0.011), (23, 0.021), (24, -0.025), (25, 0.016), (26, 0.081), (27, 0.025), (28, 0.029), (29, -0.041), (30, 0.017), (31, 0.067), (32, 0.011), (33, 0.023), (34, -0.098), (35, 0.044), (36, 0.158), (37, 0.02), (38, 0.01), (39, -0.015), (40, -0.001), (41, 0.004), (42, -0.003), (43, -0.194), (44, -0.036), (45, -0.011), (46, 0.04), (47, -0.078), (48, -0.017), (49, -0.074)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96592635 <a title="141-lsi-1" href="./nips-2007-New_Outer_Bounds_on_the_Marginal_Polytope.html">141 nips-2007-New Outer Bounds on the Marginal Polytope</a></p>
<p>Author: David Sontag, Tommi S. Jaakkola</p><p>Abstract: We give a new class of outer bounds on the marginal polytope, and propose a cutting-plane algorithm for efﬁciently optimizing over these constraints. When combined with a concave upper bound on the entropy, this gives a new variational inference algorithm for probabilistic inference in discrete Markov Random Fields (MRFs). Valid constraints on the marginal polytope are derived through a series of projections onto the cut polytope. As a result, we obtain tighter upper bounds on the log-partition function. We also show empirically that the approximations of the marginals are signiﬁcantly more accurate when using the tighter outer bounds. Finally, we demonstrate the advantage of the new constraints for ﬁnding the MAP assignment in protein structure prediction. 1</p><p>2 0.87700075 <a title="141-lsi-2" href="./nips-2007-Local_Algorithms_for_Approximate_Inference_in_Minor-Excluded_Graphs.html">121 nips-2007-Local Algorithms for Approximate Inference in Minor-Excluded Graphs</a></p>
<p>Author: Kyomin Jung, Devavrat Shah</p><p>Abstract: We present a new local approximation algorithm for computing MAP and logpartition function for arbitrary exponential family distribution represented by a ﬁnite-valued pair-wise Markov random ﬁeld (MRF), say G. Our algorithm is based on decomposing G into appropriately chosen small components; computing estimates locally in each of these components and then producing a good global solution. We prove that the algorithm can provide approximate solution within arbitrary accuracy when G excludes some ﬁnite sized graph as its minor and G has bounded degree: all Planar graphs with bounded degree are examples of such graphs. The running time of the algorithm is Θ(n) (n is the number of nodes in G), with constant dependent on accuracy, degree of graph and size of the graph that is excluded as a minor (constant for Planar graphs). Our algorithm for minor-excluded graphs uses the decomposition scheme of Klein, Plotkin and Rao (1993). In general, our algorithm works with any decomposition scheme and provides quantiﬁable approximation guarantee that depends on the decomposition scheme.</p><p>3 0.65284252 <a title="141-lsi-3" href="./nips-2007-Loop_Series_and_Bethe_Variational_Bounds_in_Attractive_Graphical_Models.html">123 nips-2007-Loop Series and Bethe Variational Bounds in Attractive Graphical Models</a></p>
<p>Author: Alan S. Willsky, Erik B. Sudderth, Martin J. Wainwright</p><p>Abstract: Variational methods are frequently used to approximate or bound the partition or likelihood function of a Markov random ﬁeld. Methods based on mean ﬁeld theory are guaranteed to provide lower bounds, whereas certain types of convex relaxations provide upper bounds. In general, loopy belief propagation (BP) provides often accurate approximations, but not bounds. We prove that for a class of attractive binary models, the so–called Bethe approximation associated with any ﬁxed point of loopy BP always lower bounds the true likelihood. Empirically, this bound is much tighter than the naive mean ﬁeld bound, and requires no further work than running BP. We establish these lower bounds using a loop series expansion due to Chertkov and Chernyak, which we show can be derived as a consequence of the tree reparameterization characterization of BP ﬁxed points. 1</p><p>4 0.60742646 <a title="141-lsi-4" href="./nips-2007-Message_Passing_for_Max-weight_Independent_Set.html">128 nips-2007-Message Passing for Max-weight Independent Set</a></p>
<p>Author: Sujay Sanghavi, Devavrat Shah, Alan S. Willsky</p><p>Abstract: We investigate the use of message-passing algorithms for the problem of ﬁnding the max-weight independent set (MWIS) in a graph. First, we study the performance of loopy max-product belief propagation. We show that, if it converges, the quality of the estimate is closely related to the tightness of an LP relaxation of the MWIS problem. We use this relationship to obtain sufﬁcient conditions for correctness of the estimate. We then develop a modiﬁcation of max-product – one that converges to an optimal solution of the dual of the MWIS problem. We also develop a simple iterative algorithm for estimating the max-weight independent set from this dual solution. We show that the MWIS estimate obtained using these two algorithms in conjunction is correct when the graph is bipartite and the MWIS is unique. Finally, we show that any problem of MAP estimation for probability distributions over ﬁnite domains can be reduced to an MWIS problem. We believe this reduction will yield new insights and algorithms for MAP estimation. 1</p><p>5 0.56168044 <a title="141-lsi-5" href="./nips-2007-Structured_Learning_with_Approximate_Inference.html">187 nips-2007-Structured Learning with Approximate Inference</a></p>
<p>Author: Alex Kulesza, Fernando Pereira</p><p>Abstract: In many structured prediction problems, the highest-scoring labeling is hard to compute exactly, leading to the use of approximate inference methods. However, when inference is used in a learning algorithm, a good approximation of the score may not be sufﬁcient. We show in particular that learning can fail even with an approximate inference method with rigorous approximation guarantees. There are two reasons for this. First, approximate methods can effectively reduce the expressivity of an underlying model by making it impossible to choose parameters that reliably give good predictions. Second, approximations can respond to parameter changes in such a way that standard learning algorithms are misled. In contrast, we give two positive results in the form of learning bounds for the use of LP-relaxed inference in structured perceptron and empirical risk minimization settings. We argue that without understanding combinations of inference and learning, such as these, that are appropriately compatible, learning performance under approximate inference cannot be guaranteed. 1</p><p>6 0.55093294 <a title="141-lsi-6" href="./nips-2007-Fixing_Max-Product%3A_Convergent_Message_Passing_Algorithms_for_MAP_LP-Relaxations.html">92 nips-2007-Fixing Max-Product: Convergent Message Passing Algorithms for MAP LP-Relaxations</a></p>
<p>7 0.53656447 <a title="141-lsi-7" href="./nips-2007-An_Analysis_of_Convex_Relaxations_for_MAP_Estimation.html">23 nips-2007-An Analysis of Convex Relaxations for MAP Estimation</a></p>
<p>8 0.51649976 <a title="141-lsi-8" href="./nips-2007-Cooled_and_Relaxed_Survey_Propagation_for_MRFs.html">64 nips-2007-Cooled and Relaxed Survey Propagation for MRFs</a></p>
<p>9 0.49138227 <a title="141-lsi-9" href="./nips-2007-Linear_programming_analysis_of_loopy_belief_propagation_for_weighted_matching.html">120 nips-2007-Linear programming analysis of loopy belief propagation for weighted matching</a></p>
<p>10 0.47100115 <a title="141-lsi-10" href="./nips-2007-Efficient_Bayesian_Inference_for_Dynamically_Changing_Graphs.html">75 nips-2007-Efficient Bayesian Inference for Dynamically Changing Graphs</a></p>
<p>11 0.4700276 <a title="141-lsi-11" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>12 0.4530251 <a title="141-lsi-12" href="./nips-2007-Efficient_Inference_for_Distributions_on_Permutations.html">77 nips-2007-Efficient Inference for Distributions on Permutations</a></p>
<p>13 0.4184027 <a title="141-lsi-13" href="./nips-2007-Adaptive_Embedded_Subgraph_Algorithms_using_Walk-Sum_Analysis.html">20 nips-2007-Adaptive Embedded Subgraph Algorithms using Walk-Sum Analysis</a></p>
<p>14 0.37103581 <a title="141-lsi-14" href="./nips-2007-Efficient_Principled_Learning_of_Thin_Junction_Trees.html">78 nips-2007-Efficient Principled Learning of Thin Junction Trees</a></p>
<p>15 0.36442399 <a title="141-lsi-15" href="./nips-2007-Privacy-Preserving_Belief_Propagation_and_Sampling.html">157 nips-2007-Privacy-Preserving Belief Propagation and Sampling</a></p>
<p>16 0.36098343 <a title="141-lsi-16" href="./nips-2007-Hidden_Common_Cause_Relations_in_Relational_Learning.html">97 nips-2007-Hidden Common Cause Relations in Relational Learning</a></p>
<p>17 0.35468355 <a title="141-lsi-17" href="./nips-2007-Discovering_Weakly-Interacting_Factors_in_a_Complex_Stochastic_Process.html">68 nips-2007-Discovering Weakly-Interacting Factors in a Complex Stochastic Process</a></p>
<p>18 0.3448346 <a title="141-lsi-18" href="./nips-2007-Fast_Variational_Inference_for_Large-scale_Internet_Diagnosis.html">87 nips-2007-Fast Variational Inference for Large-scale Internet Diagnosis</a></p>
<p>19 0.33396 <a title="141-lsi-19" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>20 0.33039305 <a title="141-lsi-20" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.061), (13, 0.038), (16, 0.014), (18, 0.014), (21, 0.054), (31, 0.045), (34, 0.053), (35, 0.034), (47, 0.065), (54, 0.242), (83, 0.136), (85, 0.079), (90, 0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79567575 <a title="141-lda-1" href="./nips-2007-New_Outer_Bounds_on_the_Marginal_Polytope.html">141 nips-2007-New Outer Bounds on the Marginal Polytope</a></p>
<p>Author: David Sontag, Tommi S. Jaakkola</p><p>Abstract: We give a new class of outer bounds on the marginal polytope, and propose a cutting-plane algorithm for efﬁciently optimizing over these constraints. When combined with a concave upper bound on the entropy, this gives a new variational inference algorithm for probabilistic inference in discrete Markov Random Fields (MRFs). Valid constraints on the marginal polytope are derived through a series of projections onto the cut polytope. As a result, we obtain tighter upper bounds on the log-partition function. We also show empirically that the approximations of the marginals are signiﬁcantly more accurate when using the tighter outer bounds. Finally, we demonstrate the advantage of the new constraints for ﬁnding the MAP assignment in protein structure prediction. 1</p><p>2 0.73681247 <a title="141-lda-2" href="./nips-2007-A_Bayesian_LDA-based_model_for_semi-supervised_part-of-speech_tagging.html">2 nips-2007-A Bayesian LDA-based model for semi-supervised part-of-speech tagging</a></p>
<p>Author: Kristina Toutanova, Mark Johnson</p><p>Abstract: We present a novel Bayesian model for semi-supervised part-of-speech tagging. Our model extends the Latent Dirichlet Allocation model and incorporates the intuition that words’ distributions over tags, p(t|w), are sparse. In addition we introduce a model for determining the set of possible tags of a word which captures important dependencies in the ambiguity classes of words. Our model outperforms the best previously proposed model for this task on a standard dataset. 1</p><p>3 0.68466765 <a title="141-lda-3" href="./nips-2007-Bundle_Methods_for_Machine_Learning.html">40 nips-2007-Bundle Methods for Machine Learning</a></p>
<p>Author: Quoc V. Le, Alex J. Smola, S.v.n. Vishwanathan</p><p>Abstract: We present a globally convergent method for regularized risk minimization problems. Our method applies to Support Vector estimation, regression, Gaussian Processes, and any other regularized risk minimization setting which leads to a convex optimization problem. SVMPerf can be shown to be a special case of our approach. In addition to the uniﬁed framework we present tight convergence bounds, which show that our algorithm converges in O(1/ ) steps to precision for general convex problems and in O(log(1/ )) steps for continuously differentiable problems. We demonstrate in experiments the performance of our approach. 1</p><p>4 0.62029845 <a title="141-lda-4" href="./nips-2007-Efficient_Inference_for_Distributions_on_Permutations.html">77 nips-2007-Efficient Inference for Distributions on Permutations</a></p>
<p>Author: Jonathan Huang, Carlos Guestrin, Leonidas Guibas</p><p>Abstract: Permutations are ubiquitous in many real world problems, such as voting, rankings and data association. Representing uncertainty over permutations is challenging, since there are n! possibilities, and typical compact representations such as graphical models cannot efﬁciently capture the mutual exclusivity constraints associated with permutations. In this paper, we use the “low-frequency” terms of a Fourier decomposition to represent such distributions compactly. We present Kronecker conditioning, a general and efﬁcient approach for maintaining these distributions directly in the Fourier domain. Low order Fourier-based approximations can lead to functions that do not correspond to valid distributions. To address this problem, we present an efﬁcient quadratic program deﬁned directly in the Fourier domain to project the approximation onto a relaxed form of the marginal polytope. We demonstrate the effectiveness of our approach on a real camera-based multi-people tracking setting. 1</p><p>5 0.61956662 <a title="141-lda-5" href="./nips-2007-Learning_Monotonic_Transformations_for_Classification.html">112 nips-2007-Learning Monotonic Transformations for Classification</a></p>
<p>Author: Andrew Howard, Tony Jebara</p><p>Abstract: A discriminative method is proposed for learning monotonic transformations of the training data while jointly estimating a large-margin classiﬁer. In many domains such as document classiﬁcation, image histogram classiﬁcation and gene microarray experiments, ﬁxed monotonic transformations can be useful as a preprocessing step. However, most classiﬁers only explore these transformations through manual trial and error or via prior domain knowledge. The proposed method learns monotonic transformations automatically while training a large-margin classiﬁer without any prior knowledge of the domain. A monotonic piecewise linear function is learned which transforms data for subsequent processing by a linear hyperplane classiﬁer. Two algorithmic implementations of the method are formalized. The ﬁrst solves a convergent alternating sequence of quadratic and linear programs until it obtains a locally optimal solution. An improved algorithm is then derived using a convex semideﬁnite relaxation that overcomes initialization issues in the greedy optimization problem. The eﬀectiveness of these learned transformations on synthetic problems, text data and image data is demonstrated. 1</p><p>6 0.61352408 <a title="141-lda-6" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>7 0.60723019 <a title="141-lda-7" href="./nips-2007-Multi-task_Gaussian_Process_Prediction.html">135 nips-2007-Multi-task Gaussian Process Prediction</a></p>
<p>8 0.60143596 <a title="141-lda-8" href="./nips-2007-Structured_Learning_with_Approximate_Inference.html">187 nips-2007-Structured Learning with Approximate Inference</a></p>
<p>9 0.60094279 <a title="141-lda-9" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>10 0.60056144 <a title="141-lda-10" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>11 0.59932882 <a title="141-lda-11" href="./nips-2007-Efficient_Bayesian_Inference_for_Dynamically_Changing_Graphs.html">75 nips-2007-Efficient Bayesian Inference for Dynamically Changing Graphs</a></p>
<p>12 0.59836346 <a title="141-lda-12" href="./nips-2007-Message_Passing_for_Max-weight_Independent_Set.html">128 nips-2007-Message Passing for Max-weight Independent Set</a></p>
<p>13 0.59705585 <a title="141-lda-13" href="./nips-2007-Multi-Task_Learning_via_Conic_Programming.html">134 nips-2007-Multi-Task Learning via Conic Programming</a></p>
<p>14 0.59057903 <a title="141-lda-14" href="./nips-2007-Fixing_Max-Product%3A_Convergent_Message_Passing_Algorithms_for_MAP_LP-Relaxations.html">92 nips-2007-Fixing Max-Product: Convergent Message Passing Algorithms for MAP LP-Relaxations</a></p>
<p>15 0.59045273 <a title="141-lda-15" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>16 0.59016043 <a title="141-lda-16" href="./nips-2007-Density_Estimation_under_Independent_Similarly_Distributed_Sampling_Assumptions.html">66 nips-2007-Density Estimation under Independent Similarly Distributed Sampling Assumptions</a></p>
<p>17 0.59015453 <a title="141-lda-17" href="./nips-2007-Efficient_Convex_Relaxation_for_Transductive_Support_Vector_Machine.html">76 nips-2007-Efficient Convex Relaxation for Transductive Support Vector Machine</a></p>
<p>18 0.58980948 <a title="141-lda-18" href="./nips-2007-On_higher-order_perceptron_algorithms.html">146 nips-2007-On higher-order perceptron algorithms</a></p>
<p>19 0.58921516 <a title="141-lda-19" href="./nips-2007-Linear_programming_analysis_of_loopy_belief_propagation_for_weighted_matching.html">120 nips-2007-Linear programming analysis of loopy belief propagation for weighted matching</a></p>
<p>20 0.58893728 <a title="141-lda-20" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
