<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>107 nips-2007-Iterative Non-linear Dimensionality Reduction with Manifold Sculpting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-107" href="#">nips2007-107</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>107 nips-2007-Iterative Non-linear Dimensionality Reduction with Manifold Sculpting</h1>
<br/><p>Source: <a title="nips-2007-107-pdf" href="http://papers.nips.cc/paper/3241-iterative-non-linear-dimensionality-reduction-with-manifold-sculpting.pdf">pdf</a></p><p>Author: Michael Gashler, Dan Ventura, Tony Martinez</p><p>Abstract: Many algorithms have been recently developed for reducing dimensionality by projecting data onto an intrinsic non-linear manifold. Unfortunately, existing algorithms often lose signiﬁcant precision in this transformation. Manifold Sculpting is a new algorithm that iteratively reduces dimensionality by simulating surface tension in local neighborhoods. We present several experiments that show Manifold Sculpting yields more accurate results than existing algorithms with both generated and natural data-sets. Manifold Sculpting is also able to beneﬁt from both prior dimensionality reduction efforts. 1</p><p>Reference: <a title="nips-2007-107-reference" href="../nips2007_reference/nips-2007-Iterative_Non-linear_Dimensionality_Reduction_with_Manifold_Sculpting_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Iterative Non-linear Dimensionality Reduction by Manifold Sculpting  Mike Gashler, Dan Ventura, and Tony Martinez ∗ Brigham Young University Provo, UT 84604  Abstract Many algorithms have been recently developed for reducing dimensionality by projecting data onto an intrinsic non-linear manifold. [sent-1, score-0.196]
</p><p>2 Manifold Sculpting is a new algorithm that iteratively reduces dimensionality by simulating surface tension in local neighborhoods. [sent-3, score-0.216]
</p><p>3 We present several experiments that show Manifold Sculpting yields more accurate results than existing algorithms with both generated and natural data-sets. [sent-4, score-0.067]
</p><p>4 Manifold Sculpting is also able to beneﬁt from both prior dimensionality reduction efforts. [sent-5, score-0.125]
</p><p>5 1  Introduction  Dimensionality reduction is a two-step process: 1) Transform the data so that more information will survive the projection, and 2) project the data into fewer dimensions. [sent-6, score-0.075]
</p><p>6 The more relationships between data points that the transformation step is required to preserve, the less ﬂexibility it will have to position the points in a manner that will cause information to survive the projection step. [sent-7, score-0.302]
</p><p>7 Due to this inverse relationship, dimensionality reduction algorithms must seek a balance that preserves information in the transformation without losing it in the projection. [sent-8, score-0.243]
</p><p>8 Nonlinear dimensionality reduction (NLDR) algorithms seek this balance by assuming that the relationships between neighboring points contain more informational content than the relationships between distant points. [sent-10, score-0.382]
</p><p>9 Although non-linear transformations have more potential than do linear transformations to lose information in the structure of the data, they also have more potential to position the data to cause more information to survive the projection. [sent-11, score-0.088]
</p><p>10 In this process, NLDR algorithms expose patterns and structures of lower dimensionality (manifolds) that exist in the original data. [sent-12, score-0.119]
</p><p>11 NLDR algorithms, or manifold learning algorithms, have potential to make the high-level concepts embedded in multidimensional data accessible to both humans and machines. [sent-13, score-0.61]
</p><p>12 This paper introduces a new algorithm for manifold learning called Manifold Sculpting, which discovers manifolds through a process of progressive reﬁnement. [sent-14, score-0.65]
</p><p>13 Experiments show that it yields more accurate results than other algorithms in many cases. [sent-15, score-0.067]
</p><p>14 Additionally, it can be used as a postprocessing step to enhance the transformation of other manifold learning algorithms. [sent-16, score-0.677]
</p><p>15 2  Related Work  Many algorithms have been developed for performing non-linear dimensionality reduction. [sent-17, score-0.119]
</p><p>16 Recent works include Isomap [1], which solves for an isometric embedding of data into fewer dimensions with an algebraic technique. [sent-18, score-0.074]
</p><p>17 edu  1  Figure 1: Comparison of several manifold learners on a Swiss Roll manifold. [sent-24, score-0.61]
</p><p>18 Color is used to indicate how points in the results correspond to points on the manifold. [sent-25, score-0.07]
</p><p>19 ) Locally Linear Embedding (LLE) [2] is able to perform a similar computation using a sparse matrix by using a metric that measures only relationships between vectors in local neighborhoods. [sent-31, score-0.104]
</p><p>20 Hessian LLE preserves the manifold structure better than the other algorithms but is, unfortunately, computationally expensive. [sent-39, score-0.671]
</p><p>21 This algorithm iteratively transforms data by balancing two opposing heuristics, one that scales information out of unwanted dimensions, and one that preserves local structure in the data. [sent-44, score-0.085]
</p><p>22 Experimental results show that this technique preserves information into fewer dimensions with more accuracy than existing manifold learning algorithms. [sent-45, score-0.684]
</p><p>23 Figure 2: δ and θ deﬁne the relationships that Manifold Sculpting attempts to preserve. [sent-49, score-0.074]
</p><p>24 2  Step 1: Find the k nearest neighbors of each point. [sent-50, score-0.075]
</p><p>25 For each data point pi in P (where P is the set of all data points represented as vectors in Rn ), ﬁnd the k-nearest neighbors Ni (such that nij ∈ Ni is the j th neighbor of point pi ). [sent-51, score-0.407]
</p><p>26 For each j (where 0 < j ≤ k) compute the Euclidean distance δij between pi and each nij ∈ Ni . [sent-53, score-0.21]
</p><p>27 Also compute the angle θij formed by the two line segments (pi to nij ) and (nij to mij ), where mij is the most colinear neighbor of nij with pi . [sent-54, score-0.492]
</p><p>28 ) The most colinear neighbor is the neighbor point that forms the angle closest to π. [sent-56, score-0.187]
</p><p>29 The values of δ and θ are the relationships that the algorithm will attempt to preserve during transformation. [sent-57, score-0.074]
</p><p>30 The global average distance between all the neighbors of all points δave is also computed. [sent-58, score-0.134]
</p><p>31 The data may optionally be preprocessed with the transformation step of Principle Component Analysis (PCA), or another efﬁcient algorithm. [sent-60, score-0.094]
</p><p>32 To the extent that there is a linear component in the manifold, PCA will move the information in the data into as few dimensions as possible, thus leaving less work to be done in step 4 (which handles the non-linear component). [sent-62, score-0.072]
</p><p>33 This step is performed by computing the ﬁrst |Dpres | principle components of the data (where Dpres is the set of dimensions that will be preserved in the projection), and rotating the dimensional axes to align with these principle components. [sent-63, score-0.247]
</p><p>34 For each pi ∈ P , the values in Dpres are adjusted to recover the relationships that are distorted by scaling. [sent-75, score-0.198]
</p><p>35 Intuitively, this step simulates tension on the manifold surface. [sent-76, score-0.678]
</p><p>36 The denominator values were chosen as normalizing factors because the value of the angle term can range from 0 to π, and the value of the distance term will tend to have a mean of about δave with some variance in both directions. [sent-78, score-0.097]
</p><p>37 The order in which points are adjusted has some impact on the rate of convergence. [sent-80, score-0.084]
</p><p>38 To further speed convergence, higher weight, wij , is given to the component of the error contributed by neighbors that have already been adjusted in the current iteration. [sent-84, score-0.185]
</p><p>39 For all of our experiments, we use wij = 1 if ni has not yet been adjusted in this iteration, and wij = 10, if nij has been adjusted in this iteration. [sent-85, score-0.346]
</p><p>40 Unfortunately the equation for the true gradient of the error surface deﬁned by this heuristic is complex, and is in O(|D|3 ). [sent-86, score-0.063]
</p><p>41 Since the error surface is not necessarily convex, the algorithm may potentially converge to local minima. [sent-88, score-0.093]
</p><p>42 Even if a local 3  Figure 3: The mean squared error of four algorithms with a Swiss Roll manifold using a varying number of neighbors k. [sent-90, score-0.839]
</p><p>43 When k > 57, neighbor paths cut across the manifold. [sent-91, score-0.111]
</p><p>44 minimum exists so close to the globally optimal state, it may have a sufﬁciently small error as to be acceptable. [sent-94, score-0.071]
</p><p>45 Even if one point becomes temporarily stuck in a local minimum, its neighbors are likely to pull it out, or change the topology of its error surface when their values are adjusted. [sent-96, score-0.197]
</p><p>46 Third, by gradually scaling the values in Dscaled (instead of directly setting them to 0), the system always remains in a state very close to the current globally optimal state. [sent-98, score-0.068]
</p><p>47 As long as it stays close to the current optimal state, it is unlikely for the error surface to change in a manner that permanently separates it from being able to reach the globally optimal state. [sent-99, score-0.11]
</p><p>48 (This is why all the dimensions need to be preserved in the PCA pre-processing step. [sent-100, score-0.083]
</p><p>49 4  Empirical Results  Figure 1 shows that Manifold Sculpting appears visually to produce results of higher quality than LLE and Isomap with the Swiss Roll manifold, a common visual test for manifold learning algorithms. [sent-105, score-0.655]
</p><p>50 Since the actual structure of this manifold is known prior to using any manifold learner, we can use this prior information to quantitatively measure the accuracy of each algorithm. [sent-107, score-1.22]
</p><p>51 We deﬁne a Swiss Roll in 3D space with n points (xi , yi , zi ) for each 0 ≤ i < n, such that xi = t sin(t), yi is a random number −6 ≤ yi < 6, and zi = t cos(t), √ where t = 8i/n + 2. [sent-110, score-0.18]
</p><p>52 manifold coordinates, the point is (ui , vi ), such that ui = 2 We created a Swiss Roll with 2000 data points and reduced the dimensionality to 2 with each of four algorithms. [sent-112, score-0.846]
</p><p>53 Next we tested how well these results align with the expected values by measuring the mean squared distance from each point to its expected value. [sent-113, score-0.068]
</p><p>54 Results are shown with a varying number of neighbors k. [sent-117, score-0.103]
</p><p>55 4  Figure 4: The mean squared error of points from an S-Curve manifold for four algorithms with a varying number of data points. [sent-120, score-0.769]
</p><p>56 We deﬁned the S-Curve points in 3D space with n points (xi , yi , zi ) for each 0 ≤ i < n, such that xi = t, yi = sin(t), and zi is a random number 0 ≤ zi < 2, where t = (2. [sent-127, score-0.22]
</p><p>57 In 2D manifold coordinates, the point is n t  cos2 (w) + 1 dw and vi = yi . [sent-130, score-0.667]
</p><p>58 (ui , vi ), such that ui = 0  Figure 4 shows the mean squared error of the transformed points from their expected values using the same regression technique described for the experiment with the Swiss Roll problem. [sent-131, score-0.142]
</p><p>59 A trend can be observed in this data that as the number of sample points increases, the quality of results from Manifold Sculpting also increases. [sent-133, score-0.08]
</p><p>60 This experiment was also performed with 6 neighbors, but Manifold Sculpting did not always converge within a reasonable time when so few neighbors were used. [sent-136, score-0.075]
</p><p>61 The other three algorithms do not have this limitation, but the quality of their results still tend to be poor when very few neighbors are used. [sent-137, score-0.157]
</p><p>62 In cases where the manifold has an intrinsic dimensionality of exactly 1, a path from neighbor to neighbor provides an accurate estimate of isolinear distance. [sent-143, score-0.975]
</p><p>63 Thus an algorithm that seeks to globally optimize isolinear distances will be less susceptible to the noise from cutting across local corners. [sent-144, score-0.15]
</p><p>64 When the intrinsic dimensionality is higher than 1, however, paths that follow from neighbor to neighbor produce a zig-zag pattern that introduces excessive noise into the isolinear distance measurement. [sent-145, score-0.416]
</p><p>65 In these cases, preserving local neighborhood relationships with precision yields better overall results than globally optimizing an error-prone metric. [sent-146, score-0.172]
</p><p>66 Consistent with this intuition, Isomap is the closest competitor to Manifold Sculpting in other experiments that involved a manifold with a single intrinsic dimension, and yields the poorest results of the four algorithms when the intrinsic dimensionality is larger than one. [sent-147, score-0.927]
</p><p>67 5  Figure 5: Mean squared error for four algorithms with an Entwined Spirals manifold. [sent-148, score-0.096]
</p><p>68 Unfortunately, the manifold structure represented by most real-world problems is not known a priori. [sent-152, score-0.61]
</p><p>69 The accuracy of a manifold learner, however, can still be estimated when the problem involves a video sequence by simply counting the percentage of frames that are sorted into the same order as the video sequence. [sent-153, score-0.9]
</p><p>70 Figure 6 shows several frames from a video sequence of a person turning his head while gradually smiling. [sent-154, score-0.185]
</p><p>71 ) The one preserved dimension could then characterize each frame according to the high-level concepts that were previously encoded in many dimensions. [sent-158, score-0.079]
</p><p>72 The dot below each image corresponds to the single-dimensional value in the preserved dimension for that image. [sent-159, score-0.065]
</p><p>73 In this case, the ordering of every frame was consistent with the video sequence. [sent-160, score-0.16]
</p><p>74 Figure 7 shows a comparison of results obtained from a manifold generated by translating an image over a background of random noise. [sent-163, score-0.681]
</p><p>75 Because two variables (horizontal position and vertical position) were used to generate the dataset, this data creates a manifold with an intrinsic dimensionality of two in a space with an extrinsic dimensionality of 2,401 (the total number of pixels in each image). [sent-167, score-0.951]
</p><p>76 Because the background is random, the average distance between neighboring points in the input space is uniform, so the ideal result is known to be a square. [sent-168, score-0.086]
</p><p>77 The distortions produced by Manifold Sculpting tend to be local in nature, while the distortions produced by other algorithms tend to be more global. [sent-169, score-0.175]
</p><p>78 Note that the points are spread nearly uniformly across the manifold in the results from Manifold Sculpting. [sent-170, score-0.645]
</p><p>79 6  Figure 7: A comparison of results with a manifold generated by translating an image over a background of noise. [sent-174, score-0.681]
</p><p>80 Manifold Sculpting tends to produce less global distortion, while other algorithms tend to produce less local distortion. [sent-175, score-0.161]
</p><p>81 Perhaps more signiﬁcantly, it also tends to keep the intrinsic variables in the dataset more linearly separable. [sent-181, score-0.102]
</p><p>82 This is particularly important when the dimensionality reduction is used as a pre-processing step for a supervised learning algorithm. [sent-182, score-0.159]
</p><p>83 We created four video sequences designed to show various types of manifold topologies and measured the accuracy of each manifold learning algorithm. [sent-183, score-1.394]
</p><p>84 Since the background pixels remain nearly constant while the pixels on the rotating object change in value, the manifold corresponding to the vector encoding of this video will contain both smooth and changing areas. [sent-186, score-0.931]
</p><p>85 The second video was made by moving a camera down a hallway. [sent-187, score-0.126]
</p><p>86 This produces a manifold with a continuous range of variability, since pixels near the center of the frame change slowly while pixels near the edges change rapidly. [sent-188, score-0.752]
</p><p>87 Unlike the video of the rotating stuffed animal, there are no background pixels that remain constant. [sent-190, score-0.318]
</p><p>88 Unlike the ﬁrst video, however, the high-contrast texture of the object used in this video results in a topology with much more variation. [sent-192, score-0.155]
</p><p>89 As the black spots shift across the pixels, a manifold is created that swings wildly in the respective dimensions. [sent-193, score-0.635]
</p><p>90 Due to the large hills and valleys in the topology of this manifold, the nearest neighbors of a frame frequently create paths that cut across the manifold. [sent-194, score-0.188]
</p><p>91 In all four cases, Manifold Sculpting produced results competitive with Isomap, which does particularly well with manifolds that have an intrinsic dimensionality of  Figure 8: Four video sequences were created with varying properties in the corresponding manfolds. [sent-195, score-0.413]
</p><p>92 Dimensionality was reduced to one with each of four manifold learning algorithms. [sent-196, score-0.633]
</p><p>93 7  one, but Manifold Sculpting is not limited by the intrinsic dimensionality as shown in the previous experiments. [sent-198, score-0.171]
</p><p>94 5  Discussion  The experiments tested in this paper show that Manifold Sculpting yields more accurate results than other well-known manifold learning algorithms. [sent-199, score-0.652]
</p><p>95 Manifold Sculpting is more accurate than other algorithms when the manifold is sparsely sampled, and the gap is even wider with higher sampling densities. [sent-201, score-0.656]
</p><p>96 Manifold Sculpting has difﬁculty when the selected number of neighbors is too small but consistently outperforms other algorithms when it is larger. [sent-202, score-0.1]
</p><p>97 Manifold Sculpting beneﬁts signiﬁcantly when the data is pre-processed with the transformation step of PCA. [sent-206, score-0.067]
</p><p>98 The transformation step of any algorithm may be used in place of this step. [sent-207, score-0.067]
</p><p>99 Current research seeks to identify which algorithms work best with Manifold Sculpting to efﬁciently produce high quality results. [sent-208, score-0.092]
</p><p>100 Laplacian eigenmaps and spectral techniques for embedding and clustering. [sent-231, score-0.07]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sculpting', 0.612), ('manifold', 0.61), ('isomap', 0.151), ('nij', 0.136), ('video', 0.126), ('dimensionality', 0.094), ('lle', 0.088), ('roll', 0.081), ('intrinsic', 0.077), ('neighbors', 0.075), ('relationships', 0.074), ('dpres', 0.068), ('dscal', 0.068), ('hlle', 0.068), ('nldr', 0.068), ('swiss', 0.065), ('neighbor', 0.061), ('rotating', 0.06), ('pixels', 0.054), ('entwined', 0.051), ('isolinear', 0.051), ('spirals', 0.051), ('stuffed', 0.051), ('pi', 0.05), ('adjusted', 0.049), ('globally', 0.047), ('preserved', 0.045), ('sam', 0.044), ('survive', 0.044), ('ave', 0.041), ('manifolds', 0.04), ('surface', 0.039), ('dimensions', 0.038), ('frames', 0.038), ('ni', 0.038), ('angle', 0.038), ('wij', 0.037), ('preserves', 0.036), ('embedding', 0.036), ('tend', 0.035), ('points', 0.035), ('step', 0.034), ('charting', 0.034), ('tension', 0.034), ('vin', 0.034), ('eigenmaps', 0.034), ('frame', 0.034), ('transformation', 0.033), ('ij', 0.032), ('zi', 0.032), ('pca', 0.031), ('reduction', 0.031), ('vi', 0.03), ('local', 0.03), ('ventura', 0.03), ('martinez', 0.03), ('ui', 0.029), ('topology', 0.029), ('varying', 0.028), ('background', 0.027), ('yi', 0.027), ('optionally', 0.027), ('trouble', 0.027), ('colinear', 0.027), ('unfortunately', 0.027), ('created', 0.025), ('cut', 0.025), ('distortions', 0.025), ('informational', 0.025), ('distorted', 0.025), ('projection', 0.025), ('principle', 0.025), ('algorithms', 0.025), ('paths', 0.025), ('tends', 0.025), ('distance', 0.024), ('balance', 0.024), ('parzen', 0.024), ('yoshua', 0.024), ('translating', 0.024), ('squared', 0.024), ('hessian', 0.024), ('error', 0.024), ('produce', 0.023), ('trend', 0.023), ('locally', 0.023), ('four', 0.023), ('position', 0.022), ('quality', 0.022), ('joshua', 0.022), ('seeks', 0.022), ('silva', 0.022), ('lose', 0.022), ('mij', 0.022), ('gradually', 0.021), ('yields', 0.021), ('accurate', 0.021), ('align', 0.02), ('image', 0.02), ('iteratively', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="107-tfidf-1" href="./nips-2007-Iterative_Non-linear_Dimensionality_Reduction_with_Manifold_Sculpting.html">107 nips-2007-Iterative Non-linear Dimensionality Reduction with Manifold Sculpting</a></p>
<p>Author: Michael Gashler, Dan Ventura, Tony Martinez</p><p>Abstract: Many algorithms have been recently developed for reducing dimensionality by projecting data onto an intrinsic non-linear manifold. Unfortunately, existing algorithms often lose signiﬁcant precision in this transformation. Manifold Sculpting is a new algorithm that iteratively reduces dimensionality by simulating surface tension in local neighborhoods. We present several experiments that show Manifold Sculpting yields more accurate results than existing algorithms with both generated and natural data-sets. Manifold Sculpting is also able to beneﬁt from both prior dimensionality reduction efforts. 1</p><p>2 0.41902623 <a title="107-tfidf-2" href="./nips-2007-Random_Projections_for_Manifold_Learning.html">161 nips-2007-Random Projections for Manifold Learning</a></p>
<p>Author: Chinmay Hegde, Michael Wakin, Richard Baraniuk</p><p>Abstract: We propose a novel method for linear dimensionality reduction of manifold modeled data. First, we show that with a small number M of random projections of sample points in RN belonging to an unknown K-dimensional Euclidean manifold, the intrinsic dimension (ID) of the sample set can be estimated to high accuracy. Second, we rigorously prove that using only this set of random projections, we can estimate the structure of the underlying manifold. In both cases, the number of random projections required is linear in K and logarithmic in N , meaning that K < M ≪ N . To handle practical situations, we develop a greedy algorithm to estimate the smallest size of the projection space required to perform manifold learning. Our method is particularly relevant in distributed sensing systems and leads to signiﬁcant potential savings in data acquisition, storage and transmission costs.</p><p>3 0.2269952 <a title="107-tfidf-3" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>Author: Nicolas L. Roux, Yoshua Bengio, Pascal Lamblin, Marc Joliveau, Balázs Kégl</p><p>Abstract: We study the following question: is the two-dimensional structure of images a very strong prior or is it something that can be learned with a few examples of natural images? If someone gave us a learning task involving images for which the two-dimensional topology of pixels was not known, could we discover it automatically and exploit it? For example suppose that the pixels had been permuted in a ﬁxed but unknown way, could we recover the relative two-dimensional location of pixels on images? The surprising result presented here is that not only the answer is yes, but that about as few as a thousand images are enough to approximately recover the relative locations of about a thousand pixels. This is achieved using a manifold learning algorithm applied to pixels associated with a measure of distributional similarity between pixel intensities. We compare different topologyextraction approaches and show how having the two-dimensional topology can be exploited.</p><p>4 0.17201467 <a title="107-tfidf-4" href="./nips-2007-Statistical_Analysis_of_Semi-Supervised_Regression.html">186 nips-2007-Statistical Analysis of Semi-Supervised Regression</a></p>
<p>Author: Larry Wasserman, John D. Lafferty</p><p>Abstract: Semi-supervised methods use unlabeled data in addition to labeled data to construct predictors. While existing semi-supervised methods have shown some promising empirical performance, their development has been based largely based on heuristics. In this paper we study semi-supervised learning from the viewpoint of minimax theory. Our ﬁrst result shows that some common methods based on regularization using graph Laplacians do not lead to faster minimax rates of convergence. Thus, the estimators that use the unlabeled data do not have smaller risk than the estimators that use only labeled data. We then develop several new approaches that provably lead to improved performance. The statistical tools of minimax analysis are thus used to offer some new perspective on the problem of semi-supervised learning. 1</p><p>5 0.11555367 <a title="107-tfidf-5" href="./nips-2007-Learning_the_structure_of_manifolds_using_random_projections.html">116 nips-2007-Learning the structure of manifolds using random projections</a></p>
<p>Author: Yoav Freund, Sanjoy Dasgupta, Mayank Kabra, Nakul Verma</p><p>Abstract: We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data. 1</p><p>6 0.075280517 <a title="107-tfidf-6" href="./nips-2007-People_Tracking_with_the_Laplacian_Eigenmaps_Latent_Variable_Model.html">153 nips-2007-People Tracking with the Laplacian Eigenmaps Latent Variable Model</a></p>
<p>7 0.06896849 <a title="107-tfidf-7" href="./nips-2007-Semi-Supervised_Multitask_Learning.html">175 nips-2007-Semi-Supervised Multitask Learning</a></p>
<p>8 0.065569252 <a title="107-tfidf-8" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>9 0.060346633 <a title="107-tfidf-9" href="./nips-2007-Regularized_Boost_for_Semi-Supervised_Learning.html">166 nips-2007-Regularized Boost for Semi-Supervised Learning</a></p>
<p>10 0.044469241 <a title="107-tfidf-10" href="./nips-2007-Kernels_on_Attributed_Pointsets_with_Applications.html">109 nips-2007-Kernels on Attributed Pointsets with Applications</a></p>
<p>11 0.043296453 <a title="107-tfidf-11" href="./nips-2007-Random_Sampling_of_States_in_Dynamic_Programming.html">162 nips-2007-Random Sampling of States in Dynamic Programming</a></p>
<p>12 0.041513294 <a title="107-tfidf-12" href="./nips-2007-Colored_Maximum_Variance_Unfolding.html">49 nips-2007-Colored Maximum Variance Unfolding</a></p>
<p>13 0.04056878 <a title="107-tfidf-13" href="./nips-2007-A_New_View_of_Automatic_Relevance_Determination.html">8 nips-2007-A New View of Automatic Relevance Determination</a></p>
<p>14 0.040135235 <a title="107-tfidf-14" href="./nips-2007-Locality_and_low-dimensions_in_the_prediction_of_natural_experience_from_fMRI.html">122 nips-2007-Locality and low-dimensions in the prediction of natural experience from fMRI</a></p>
<p>15 0.038693864 <a title="107-tfidf-15" href="./nips-2007-Subspace-Based_Face_Recognition_in_Analog_VLSI.html">188 nips-2007-Subspace-Based Face Recognition in Analog VLSI</a></p>
<p>16 0.036296081 <a title="107-tfidf-16" href="./nips-2007-Receding_Horizon_Differential_Dynamic_Programming.html">163 nips-2007-Receding Horizon Differential Dynamic Programming</a></p>
<p>17 0.036057547 <a title="107-tfidf-17" href="./nips-2007-Convex_Clustering_with_Exemplar-Based_Models.html">61 nips-2007-Convex Clustering with Exemplar-Based Models</a></p>
<p>18 0.033568643 <a title="107-tfidf-18" href="./nips-2007-A_Randomized_Algorithm_for_Large_Scale_Support_Vector_Learning.html">10 nips-2007-A Randomized Algorithm for Large Scale Support Vector Learning</a></p>
<p>19 0.032514878 <a title="107-tfidf-19" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>20 0.029284751 <a title="107-tfidf-20" href="./nips-2007-Modeling_Natural_Sounds_with_Modulation_Cascade_Processes.html">130 nips-2007-Modeling Natural Sounds with Modulation Cascade Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.139), (1, 0.051), (2, -0.065), (3, 0.043), (4, -0.039), (5, 0.112), (6, -0.08), (7, 0.139), (8, 0.012), (9, 0.311), (10, -0.134), (11, 0.42), (12, -0.078), (13, 0.093), (14, 0.171), (15, -0.116), (16, -0.132), (17, 0.048), (18, -0.041), (19, 0.036), (20, -0.038), (21, -0.056), (22, -0.151), (23, -0.015), (24, -0.008), (25, 0.045), (26, 0.077), (27, 0.061), (28, 0.119), (29, 0.049), (30, 0.041), (31, -0.07), (32, -0.037), (33, -0.058), (34, 0.065), (35, -0.033), (36, 0.088), (37, -0.007), (38, 0.062), (39, -0.02), (40, 0.004), (41, 0.044), (42, -0.059), (43, 0.039), (44, -0.045), (45, 0.031), (46, 0.009), (47, -0.016), (48, 0.049), (49, 0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9743076 <a title="107-lsi-1" href="./nips-2007-Iterative_Non-linear_Dimensionality_Reduction_with_Manifold_Sculpting.html">107 nips-2007-Iterative Non-linear Dimensionality Reduction with Manifold Sculpting</a></p>
<p>Author: Michael Gashler, Dan Ventura, Tony Martinez</p><p>Abstract: Many algorithms have been recently developed for reducing dimensionality by projecting data onto an intrinsic non-linear manifold. Unfortunately, existing algorithms often lose signiﬁcant precision in this transformation. Manifold Sculpting is a new algorithm that iteratively reduces dimensionality by simulating surface tension in local neighborhoods. We present several experiments that show Manifold Sculpting yields more accurate results than existing algorithms with both generated and natural data-sets. Manifold Sculpting is also able to beneﬁt from both prior dimensionality reduction efforts. 1</p><p>2 0.89073551 <a title="107-lsi-2" href="./nips-2007-Random_Projections_for_Manifold_Learning.html">161 nips-2007-Random Projections for Manifold Learning</a></p>
<p>Author: Chinmay Hegde, Michael Wakin, Richard Baraniuk</p><p>Abstract: We propose a novel method for linear dimensionality reduction of manifold modeled data. First, we show that with a small number M of random projections of sample points in RN belonging to an unknown K-dimensional Euclidean manifold, the intrinsic dimension (ID) of the sample set can be estimated to high accuracy. Second, we rigorously prove that using only this set of random projections, we can estimate the structure of the underlying manifold. In both cases, the number of random projections required is linear in K and logarithmic in N , meaning that K < M ≪ N . To handle practical situations, we develop a greedy algorithm to estimate the smallest size of the projection space required to perform manifold learning. Our method is particularly relevant in distributed sensing systems and leads to signiﬁcant potential savings in data acquisition, storage and transmission costs.</p><p>3 0.72638845 <a title="107-lsi-3" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>Author: Nicolas L. Roux, Yoshua Bengio, Pascal Lamblin, Marc Joliveau, Balázs Kégl</p><p>Abstract: We study the following question: is the two-dimensional structure of images a very strong prior or is it something that can be learned with a few examples of natural images? If someone gave us a learning task involving images for which the two-dimensional topology of pixels was not known, could we discover it automatically and exploit it? For example suppose that the pixels had been permuted in a ﬁxed but unknown way, could we recover the relative two-dimensional location of pixels on images? The surprising result presented here is that not only the answer is yes, but that about as few as a thousand images are enough to approximately recover the relative locations of about a thousand pixels. This is achieved using a manifold learning algorithm applied to pixels associated with a measure of distributional similarity between pixel intensities. We compare different topologyextraction approaches and show how having the two-dimensional topology can be exploited.</p><p>4 0.42479423 <a title="107-lsi-4" href="./nips-2007-Statistical_Analysis_of_Semi-Supervised_Regression.html">186 nips-2007-Statistical Analysis of Semi-Supervised Regression</a></p>
<p>Author: Larry Wasserman, John D. Lafferty</p><p>Abstract: Semi-supervised methods use unlabeled data in addition to labeled data to construct predictors. While existing semi-supervised methods have shown some promising empirical performance, their development has been based largely based on heuristics. In this paper we study semi-supervised learning from the viewpoint of minimax theory. Our ﬁrst result shows that some common methods based on regularization using graph Laplacians do not lead to faster minimax rates of convergence. Thus, the estimators that use the unlabeled data do not have smaller risk than the estimators that use only labeled data. We then develop several new approaches that provably lead to improved performance. The statistical tools of minimax analysis are thus used to offer some new perspective on the problem of semi-supervised learning. 1</p><p>5 0.38876167 <a title="107-lsi-5" href="./nips-2007-Learning_the_structure_of_manifolds_using_random_projections.html">116 nips-2007-Learning the structure of manifolds using random projections</a></p>
<p>Author: Yoav Freund, Sanjoy Dasgupta, Mayank Kabra, Nakul Verma</p><p>Abstract: We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data. 1</p><p>6 0.35522676 <a title="107-lsi-6" href="./nips-2007-Colored_Maximum_Variance_Unfolding.html">49 nips-2007-Colored Maximum Variance Unfolding</a></p>
<p>7 0.29040596 <a title="107-lsi-7" href="./nips-2007-People_Tracking_with_the_Laplacian_Eigenmaps_Latent_Variable_Model.html">153 nips-2007-People Tracking with the Laplacian Eigenmaps Latent Variable Model</a></p>
<p>8 0.26463714 <a title="107-lsi-8" href="./nips-2007-Subspace-Based_Face_Recognition_in_Analog_VLSI.html">188 nips-2007-Subspace-Based Face Recognition in Analog VLSI</a></p>
<p>9 0.2480571 <a title="107-lsi-9" href="./nips-2007-A_Unified_Near-Optimal_Estimator_For_Dimension_Reduction_in_%24l_%5Calpha%24_%28%240%3C%5Calpha%5Cleq_2%24%29_Using_Stable_Random_Projections.html">13 nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</a></p>
<p>10 0.21567115 <a title="107-lsi-10" href="./nips-2007-A_learning_framework_for_nearest_neighbor_search.html">16 nips-2007-A learning framework for nearest neighbor search</a></p>
<p>11 0.20586756 <a title="107-lsi-11" href="./nips-2007-Semi-Supervised_Multitask_Learning.html">175 nips-2007-Semi-Supervised Multitask Learning</a></p>
<p>12 0.19641581 <a title="107-lsi-12" href="./nips-2007-Regularized_Boost_for_Semi-Supervised_Learning.html">166 nips-2007-Regularized Boost for Semi-Supervised Learning</a></p>
<p>13 0.1855565 <a title="107-lsi-13" href="./nips-2007-A_Randomized_Algorithm_for_Large_Scale_Support_Vector_Learning.html">10 nips-2007-A Randomized Algorithm for Large Scale Support Vector Learning</a></p>
<p>14 0.17556219 <a title="107-lsi-14" href="./nips-2007-Receding_Horizon_Differential_Dynamic_Programming.html">163 nips-2007-Receding Horizon Differential Dynamic Programming</a></p>
<p>15 0.16318023 <a title="107-lsi-15" href="./nips-2007-Nearest-Neighbor-Based_Active_Learning_for_Rare_Category_Detection.html">139 nips-2007-Nearest-Neighbor-Based Active Learning for Rare Category Detection</a></p>
<p>16 0.16043438 <a title="107-lsi-16" href="./nips-2007-A_Constraint_Generation_Approach_to_Learning_Stable_Linear_Dynamical_Systems.html">4 nips-2007-A Constraint Generation Approach to Learning Stable Linear Dynamical Systems</a></p>
<p>17 0.1525014 <a title="107-lsi-17" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>18 0.14684287 <a title="107-lsi-18" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>19 0.14587329 <a title="107-lsi-19" href="./nips-2007-Kernels_on_Attributed_Pointsets_with_Applications.html">109 nips-2007-Kernels on Attributed Pointsets with Applications</a></p>
<p>20 0.14520712 <a title="107-lsi-20" href="./nips-2007-A_New_View_of_Automatic_Relevance_Determination.html">8 nips-2007-A New View of Automatic Relevance Determination</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.041), (13, 0.037), (16, 0.02), (18, 0.012), (19, 0.012), (21, 0.049), (31, 0.04), (34, 0.019), (35, 0.018), (36, 0.25), (39, 0.011), (47, 0.093), (81, 0.025), (83, 0.152), (85, 0.025), (87, 0.035), (90, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86196524 <a title="107-lda-1" href="./nips-2007-Non-parametric_Modeling_of_Partially_Ranked_Data.html">142 nips-2007-Non-parametric Modeling of Partially Ranked Data</a></p>
<p>Author: Guy Lebanon, Yi Mao</p><p>Abstract: Statistical models on full and partial rankings of n items are often of limited practical use for large n due to computational consideration. We explore the use of non-parametric models for partially ranked data and derive efﬁcient procedures for their use for large n. The derivations are largely possible through combinatorial and algebraic manipulations based on the lattice of partial rankings. In particular, we demonstrate for the ﬁrst time a non-parametric coherent and consistent model capable of efﬁciently aggregating partially ranked data of different types. 1</p><p>2 0.81038958 <a title="107-lda-2" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>Author: Guenther Zeck, Matthias Bethge, Jakob H. Macke</p><p>Abstract: S timulus selectivity of sensory neurons is often characterized by estimating their receptive ﬁeld properties such as orientation selectivity. Receptive ﬁelds are usually derived from the mean (or covariance) of the spike-triggered stimulus ensemble. This approach treats each spike as an independent message but does not take into account that information might be conveyed through patterns of neural activity that are distributed across space or time. Can we ﬁnd a concise description for the processing of a whole population of neurons analogous to the receptive ﬁeld for single neurons? Here, we present a generalization of the linear receptive ﬁeld which is not bound to be triggered on individual spikes but can be meaningfully linked to distributed response patterns. More precisely, we seek to identify those stimulus features and the corresponding patterns of neural activity that are most reliably coupled. We use an extension of reverse-correlation methods based on canonical correlation analysis. The resulting population receptive ﬁelds span the subspace of stimuli that is most informative about the population response. We evaluate our approach using both neuronal models and multi-electrode recordings from rabbit retinal ganglion cells. We show how the model can be extended to capture nonlinear stimulus-response relationships using kernel canonical correlation analysis, which makes it possible to test different coding mechanisms. Our technique can also be used to calculate receptive ﬁelds from multi-dimensional neural measurements such as those obtained from dynamic imaging methods. 1</p><p>same-paper 3 0.79675519 <a title="107-lda-3" href="./nips-2007-Iterative_Non-linear_Dimensionality_Reduction_with_Manifold_Sculpting.html">107 nips-2007-Iterative Non-linear Dimensionality Reduction with Manifold Sculpting</a></p>
<p>Author: Michael Gashler, Dan Ventura, Tony Martinez</p><p>Abstract: Many algorithms have been recently developed for reducing dimensionality by projecting data onto an intrinsic non-linear manifold. Unfortunately, existing algorithms often lose signiﬁcant precision in this transformation. Manifold Sculpting is a new algorithm that iteratively reduces dimensionality by simulating surface tension in local neighborhoods. We present several experiments that show Manifold Sculpting yields more accurate results than existing algorithms with both generated and natural data-sets. Manifold Sculpting is also able to beneﬁt from both prior dimensionality reduction efforts. 1</p><p>4 0.72822863 <a title="107-lda-4" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>Author: Lars Buesing, Wolfgang Maass</p><p>Abstract: We show that under suitable assumptions (primarily linearization) a simple and perspicuous online learning rule for Information Bottleneck optimization with spiking neurons can be derived. This rule performs on common benchmark tasks as well as a rather complex rule that has previously been proposed [1]. Furthermore, the transparency of this new learning rule makes a theoretical analysis of its convergence properties feasible. A variation of this learning rule (with sign changes) provides a theoretically founded method for performing Principal Component Analysis (PCA) with spiking neurons. By applying this rule to an ensemble of neurons, different principal components of the input can be extracted. In addition, it is possible to preferentially extract those principal components from incoming signals X that are related or are not related to some additional target signal YT . In a biological interpretation, this target signal YT (also called relevance variable) could represent proprioceptive feedback, input from other sensory modalities, or top-down signals. 1</p><p>5 0.62971675 <a title="107-lda-5" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>Author: Nicolas L. Roux, Yoshua Bengio, Pascal Lamblin, Marc Joliveau, Balázs Kégl</p><p>Abstract: We study the following question: is the two-dimensional structure of images a very strong prior or is it something that can be learned with a few examples of natural images? If someone gave us a learning task involving images for which the two-dimensional topology of pixels was not known, could we discover it automatically and exploit it? For example suppose that the pixels had been permuted in a ﬁxed but unknown way, could we recover the relative two-dimensional location of pixels on images? The surprising result presented here is that not only the answer is yes, but that about as few as a thousand images are enough to approximately recover the relative locations of about a thousand pixels. This is achieved using a manifold learning algorithm applied to pixels associated with a measure of distributional similarity between pixel intensities. We compare different topologyextraction approaches and show how having the two-dimensional topology can be exploited.</p><p>6 0.62530297 <a title="107-lda-6" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>7 0.61952692 <a title="107-lda-7" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>8 0.61734563 <a title="107-lda-8" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>9 0.61660886 <a title="107-lda-9" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>10 0.61489421 <a title="107-lda-10" href="./nips-2007-Random_Projections_for_Manifold_Learning.html">161 nips-2007-Random Projections for Manifold Learning</a></p>
<p>11 0.61436194 <a title="107-lda-11" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>12 0.61425459 <a title="107-lda-12" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>13 0.61423123 <a title="107-lda-13" href="./nips-2007-Distributed_Inference_for_Latent_Dirichlet_Allocation.html">73 nips-2007-Distributed Inference for Latent Dirichlet Allocation</a></p>
<p>14 0.61219215 <a title="107-lda-14" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>15 0.61174333 <a title="107-lda-15" href="./nips-2007-DIFFRAC%3A_a_discriminative_and_flexible_framework_for_clustering.html">65 nips-2007-DIFFRAC: a discriminative and flexible framework for clustering</a></p>
<p>16 0.61172479 <a title="107-lda-16" href="./nips-2007-Efficient_Convex_Relaxation_for_Transductive_Support_Vector_Machine.html">76 nips-2007-Efficient Convex Relaxation for Transductive Support Vector Machine</a></p>
<p>17 0.61164623 <a title="107-lda-17" href="./nips-2007-A_Bayesian_LDA-based_model_for_semi-supervised_part-of-speech_tagging.html">2 nips-2007-A Bayesian LDA-based model for semi-supervised part-of-speech tagging</a></p>
<p>18 0.61069727 <a title="107-lda-18" href="./nips-2007-Supervised_Topic_Models.html">189 nips-2007-Supervised Topic Models</a></p>
<p>19 0.61044502 <a title="107-lda-19" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<p>20 0.6096887 <a title="107-lda-20" href="./nips-2007-Structured_Learning_with_Approximate_Inference.html">187 nips-2007-Structured Learning with Approximate Inference</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
