<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-33" href="#">nips2007-33</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</h1>
<br/><p>Source: <a title="nips-2007-33-pdf" href="http://papers.nips.cc/paper/3300-bayesian-inference-for-spiking-neuron-models-with-a-sparsity-prior.pdf">pdf</a></p><p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><p>Reference: <a title="nips-2007-33-reference" href="../nips2007_reference/nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. [sent-4, score-0.248]
</p><p>2 Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. [sent-6, score-0.19]
</p><p>3 Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. [sent-8, score-0.339]
</p><p>4 This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. [sent-9, score-0.436]
</p><p>5 The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. [sent-10, score-0.176]
</p><p>6 In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. [sent-12, score-0.175]
</p><p>7 We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. [sent-13, score-0.559]
</p><p>8 Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. [sent-14, score-0.233]
</p><p>9 Given an arbitrary stimulus we would like to predict the neural response as well as possible. [sent-16, score-0.309]
</p><p>10 In order to achieve this goal with limited amount of data, it is essential to combine the information in the data with prior knowledge about neural function. [sent-17, score-0.149]
</p><p>11 The GLM neuron model consists of a linear ﬁlter, a static nonlinear transfer function and a Poisson spike generating mechanism. [sent-19, score-0.716]
</p><p>12 To determine the neural response to a given stimulus, the stimulus is ﬁrst convolved with the linear ﬁlter (i. [sent-20, score-0.346]
</p><p>13 Subsequently, the ﬁlter output is converted into an instantaneous ﬁring rate via a static nonlinear transfer function, and ﬁnally spikes are generated from an inhomogeneous Poisson-process according to this ﬁring rate. [sent-23, score-0.232]
</p><p>14 Note, however, that the GLM neuron model is not limited to describe neurons with Poisson ﬁring statistics. [sent-24, score-0.542]
</p><p>15 Rather, it is possible to incorporate inﬂuences of its own spiking history on the neural response. [sent-25, score-0.255]
</p><p>16 That is, the ﬁring rate is then determined by a combination of both the external 1  stimulus and the spiking-history of the neuron. [sent-26, score-0.251]
</p><p>17 Thus, the model can account for typical effects such as refractory periods, bursting behavior or spike-frequency adaptation. [sent-27, score-0.181]
</p><p>18 Last but not least, the GLM neuron model can also be applied for populations of coupled neurons by making each neuron dependent not only on its own spiking activity but also on the spiking history of all the other neurons. [sent-28, score-1.176]
</p><p>19 In particular, the posterior determines conﬁdence intervals for every linear weight, which facilitates the interpretation of the model and its parameters. [sent-34, score-0.306]
</p><p>20 In this way, we can readily distinguish statistical signiﬁcant interactions between neurons from spurious couplings. [sent-36, score-0.131]
</p><p>21 Another application of the Bayesian GLM neuron model arises in the context of spike-triggered covariance analysis. [sent-37, score-0.431]
</p><p>22 Spike-triggered covariance basically employs a quadratic expansion of the external stimulus parameter space and is often used in order to determine the most informative subspace. [sent-38, score-0.265]
</p><p>23 Feature selection in the GLM neuron model can be done by the assumption of a Laplace prior over the linear weights, which naturally leads to sparse posterior solutions. [sent-40, score-0.628]
</p><p>24 Consequently, all weights are equally strongly pushed to zero. [sent-41, score-0.131]
</p><p>25 This contrasts the Gaussian prior which pushes weights to zero proportional to their absolute value. [sent-42, score-0.167]
</p><p>26 In this sense, the Laplace prior can also be seen as an efﬁcient regularizer, which is well suited for the situation when a large range of alternative explanations for the neural response shall be compared on the basis of limited data. [sent-43, score-0.209]
</p><p>27 In section 3, we estimate the receptive ﬁelds, spike-history effects and functional couplings of a small population of retinal ganglion cells. [sent-46, score-0.556]
</p><p>28 We use the conﬁdence intervals to test whether the functional couplings between the neurons are signiﬁcant. [sent-48, score-0.419]
</p><p>29 In section 4, we use the GLM neuron model to describe a complex cell response recorded in macaque primary visual cortex: After computing the spike-triggered covariance (STC) we determine the relevant stimulus subspace via feature selection in our model. [sent-49, score-0.947]
</p><p>30 In contrast to the usual approach, the selection of the subspace in our case becomes directly linked to an explicit neuron model which also takes into account the spike-history dependence of the spike generation. [sent-50, score-0.672]
</p><p>31 1  Generalized Linear Models and Expectation Propagation Generalized Linear Models  Let Xt ∈ Rd , t ∈ [0, T ] denote a time-varying stimulus and Di = {ti,j } the spike-times of i = 1, . [sent-52, score-0.211]
</p><p>32 We assume that the stimulus can only change at distinct time points, but can be evaluated at continous time t. [sent-57, score-0.211]
</p><p>33 We would like to incorporate spike-history effects, couplings between neurons and dependence on nonlinear features of the stimulus. [sent-58, score-0.401]
</p><p>34 Therefore, we describe the effective input to a neuron via the following feature-map: ψsp ({ti,j ∈ Di : ti,j < t}),  ψ(t) = ψst (Xt ) i  where ψsp represents the spike time history and ψst the possibly nonlinear feature map for the stimulus. [sent-59, score-0.697]
</p><p>35 That is, the complete feature vector ψ contains possibly nonlinear features of the stimulus and the spike history of every neuron. [sent-60, score-0.565]
</p><p>36 We model the spike history dependence by a set of small time 2  windows [t − τl , t − τl ) in which occuring spikes are counted. [sent-62, score-0.46]
</p><p>37 (ψsp,i ({ti,j ∈ Di : ti,j < t}))l  1[t−τl ,t−τl ) (ti,j ) ,  = j:ti,j  < t1 < · · · < tj < · · · < T , where each feature ψ i (t) is constant in [tj−1 , tj ), attaining the value ψi,j . [sent-63, score-0.222]
</p><p>38 In the GLM neuron model setting the instantanious ﬁring rate of neuron i is obtained by a linear ﬁlter of the feature map: p(spike|Xt , {ti,j ∈ D : ti,j < t})  T = λ(wi ψ(t)),  (1)  where λ is the nonlinear transfer function. [sent-64, score-0.912]
</p><p>39 ˜ The sum therfore is 1 iff a spike of neuron i occurs at changepoint tj . [sent-68, score-0.641]
</p><p>40 Note that the changepoints ˜ tj depend on the spikes and therefore, the process is not Poissonian, as it might be suggested by the functional form of the likelihood. [sent-69, score-0.288]
</p><p>41 We use a Laplace prior distribution over the weights in order to favor sparse solutions over those which explain the data equally well but require more weights different from zero (c. [sent-79, score-0.3]
</p><p>42 (2) k ρk 2  Thus, prior factors have the form φi,k (ui,k ) = exp(−ρk |ui,k |) with ψk = (1l=k )l and ui,k = T wi ψk as above. [sent-82, score-0.181]
</p><p>43 The posterior takes the form: P (w|D) ∝  φi,j (ui,j ), i,j  where each φi,j individually instantiates a Generalized Linear Model (either corresponding to a likelihood factor or to a prior factor). [sent-84, score-0.171]
</p><p>44 As the posterior factorizes over neurons, we can perform our analysis for each neuron seperately. [sent-85, score-0.437]
</p><p>45 Our model does not assume or require any speciﬁc stimulus distribution. [sent-87, score-0.245]
</p><p>46 In particular, it is not limited to white noise stimuli or elliptically contoured distributions but it can be used without modiﬁcation for other stimulus distributions such as natural image sequences. [sent-88, score-0.245]
</p><p>47 Finally, this framework allows exact sampling of spike trains due to the piecewise constant rate. [sent-89, score-0.221]
</p><p>48 In this approximation technique, each factor (also called site) φj of the posterior is replaced by an unnormalised Gaussian: N U (uj |bj , πj )  =  1 ˆ exp(− πj u2 + bj uj ) =: φ(uj ), j 2  πj ≥ 0  where the bj , πj are called the site parameters. [sent-97, score-0.301]
</p><p>49 3  Modeling retinal ganglion cells: Which cells are functionally coupled? [sent-111, score-0.308]
</p><p>50 We applied the GLM neuron model to multi-electrode recordings of three rabbit retinal ganglion cells. [sent-112, score-0.646]
</p><p>51 The stimulus consisted of 32767 frames each of which showing a random 16×16 checkerboard pattern with a refresh rate of 50 Hz (data provided by G. [sent-113, score-0.251]
</p><p>52 log likelihood score on test-set  First, in order to investigate the role of the Laplace prior, we trained a single cell GLM neuron model on datasets of different sizes with either a Laplace prior or a Gaussian prior. [sent-117, score-0.523]
</p><p>53 As can be seen on the right the choice of prior becomes less important for large training sets as the weights are sufﬁciently constrained by the data. [sent-119, score-0.167]
</p><p>54 1 shows the spatiotemporal receptive ﬁeld of each neuron, as well as the ﬁlters describing the inﬂuence of spiking history and input from other cells. [sent-123, score-0.341]
</p><p>55 For conciseness, we only plot the ﬁlters for 80 and 120 ms time lags, but the ﬁtted model included 60 and 140 ms time lags as well. [sent-124, score-0.306]
</p><p>56 The strongly positive weights on the diagonal of ﬁgure 1(c) for the spiking history can be interpreted as “self-excitation”. [sent-125, score-0.348]
</p><p>57 In this way, it is possible to model the bursting behavior exhibited by the cells in our recordings (see also Fig. [sent-126, score-0.232]
</p><p>58 The strongly negative weights at small time lags represent refractory periods. [sent-128, score-0.294]
</p><p>59 The ﬁrst neuron seems to elicit ”bursts” at lower frequencies. [sent-130, score-0.343]
</p><p>60 As our prior assumption is that the couplings are 0, this interaction-term is not merely a consequence of our choice of prior. [sent-133, score-0.236]
</p><p>61 As a result of our crossvalidation it turns out that the prior variance for spike history weights should be set to 1 very large values (ρ= 0. [sent-134, score-0.516]
</p><p>62 In contrast, prior variances for the stimulus weights should be more strongly biased towards zero (ρ = 150). [sent-136, score-0.419]
</p><p>63 (a) GLM  (b) STA  (c)  Figure 1: (a): Stimulus dependence inferred by the GLM for the three neurons (columns) at different time lags (rows). [sent-137, score-0.31]
</p><p>64 2 of 4 time lags are plotted (60, 140 ms not shown). [sent-138, score-0.199]
</p><p>65 (b): Spike-triggered average for the same neurons and time lags as in (a). [sent-139, score-0.257]
</p><p>66 Entries correspond to the correlation coefﬁcient between the predicted rate of each model and spikes on a test set. [sent-159, score-0.151]
</p><p>67 Because of the regularization by the prior the spatio-temporal receptive ﬁelds are much smoother than the spike-triggered average ones, see Fig. [sent-162, score-0.162]
</p><p>68 The receptive ﬁelds of the STA seems to be 5  Figure 2: Predicted rate for the GLM neuron model with and without any spike history and the predicted rate for the STA for the same neurons as in the other plots. [sent-164, score-0.969]
</p><p>69 Rate for the GLM with spike dependence is obtained by averaging over 1000 sampled spike-trains. [sent-166, score-0.24]
</p><p>70 The more conservative estimate of the sparse neuron model should increase the prediction performance. [sent-169, score-0.42]
</p><p>71 To verify this, we calculated the linear response from the spike-triggered average and the rate of our GLM neuron model. [sent-170, score-0.48]
</p><p>72 As a model free performance measure we used the correlation coefﬁcient between the spike trains and the rates (each are binned in 5 ms bins). [sent-172, score-0.367]
</p><p>73 For the GLM with couplings, rates were estimated by sampling 1000 spike trains with the posterior mean as linear weights. [sent-173, score-0.352]
</p><p>74 The prediction performance can be increased even further by modeling couplings between neurons as summarized in Tab. [sent-176, score-0.29]
</p><p>75 Complex cells in primary visual cortex exhibit strongly nonlinear response properties which cannot be well described by a single linear ﬁlter, but rather requires a set of ﬁlters. [sent-179, score-0.314]
</p><p>76 A common approach for ﬁnding these ﬁlters is based on the covariance of the spike-triggered ensemble: Eigenvectors of eigenvalues that are much bigger (or smaller) than the eigenvalues of the whole stimulus ensemble indicate directions in stimulus space to which the cell is sensitive to. [sent-180, score-0.693]
</p><p>77 Usually, a statistical hypothesis test on the eigenvalue-spectrum is used to decide how many of the eigenvectors ei are needed to model the cells (Simoncelli et al. [sent-181, score-0.213]
</p><p>78 Here, we take a different approach: We use the conﬁdence intervals of our GLM neuron model to determine the relevant dimensions within the subspace revealed by STC. [sent-185, score-0.55]
</p><p>79 The ﬁlters are ordered according to their logratio of their eigenvalue to the corresponding eigenvalue of the complete stimulus ensemble (from left to right). [sent-189, score-0.257]
</p><p>80 Lower: Predicted rate on a test set for STC and for the GLM neuron model with spike history dependence on a test set. [sent-194, score-0.766]
</p><p>81 As the model is linear in the weights wi , we can use the GLM neuron model to ﬁt these weights and obtain conﬁdence intervals. [sent-195, score-0.732]
</p><p>82 If a ﬁlter fi (t) is not needed for explaining the cells response, its corresponding weight wi will automatically be set to zero by the model due to the sparsity prior. [sent-196, score-0.286]
</p><p>83 As in the previous application, we can model the spike history effects with an additional feature vector ψsp to take into account temporal dynamics of single neurons or couplings. [sent-199, score-0.502]
</p><p>84 Before applying our method to real data, we tested it on data generated from an artiﬁcial complex cell similar to the one in (Rust et al. [sent-200, score-0.131]
</p><p>85 We then ﬁtted this GLM neuron model to data recorded from a complex cell in primary visual cortex of an anesthetized macaque monkey (same data as in (Rust et al. [sent-203, score-0.593]
</p><p>86 We ﬁrst extracted 40 ﬁlters which eigenvalues were most different to their corresponding eigenvalues of the complete stimulus ensemble. [sent-205, score-0.313]
</p><p>87 Having ﬁxed the ﬁrst nonlinearity we approximated the posterior as above. [sent-208, score-0.143]
</p><p>88 The resulting conﬁdence intervals for the linear weights are plotted in Fig. [sent-209, score-0.209]
</p><p>89 conﬁdence intervals 9 excitatory and 8 inhibitory ﬁlters turned out to be signiﬁcant in our model. [sent-216, score-0.19]
</p><p>90 , who regarded 7 excitatory and 7 inhibitory ﬁlters as signiﬁcant (Rust et al. [sent-218, score-0.17]
</p><p>91 The rank order of the linear weights is closely related but not identical to the order of eigenvalues, as can be seen in Fig. [sent-220, score-0.127]
</p><p>92 5  Summary and Conclusions  We have shown how approximate Bayesian inference within the framework of generalized linear models can be used to address the problem of identifying relevant features of neural data. [sent-222, score-0.146]
</p><p>93 More precisely, the use of a sparsity prior favors sparse posterior solutions: non-zero weights are assigned only to those features which which are critical for explaining the data. [sent-223, score-0.368]
</p><p>94 Furthermore, the explicit 7  uncertainty information obtained from the posterior distribution enables us to identify ranges of statistical signiﬁcance and therefore facilitates the interpretation of the solution. [sent-224, score-0.153]
</p><p>95 We used this technique to determine couplings between neurons in a multi-cell recording and demonstrated an increase in prediction performance due to regularization by the sparsity prior. [sent-225, score-0.354]
</p><p>96 Also, in the context of spiketriggered covariance analysis, we used our method to determine the relevant stimulus subspace within the space spanned by the eigenvectors. [sent-226, score-0.388]
</p><p>97 Our subspace selection method is directly linked to an explicit neuron model which also takes into account the spike-history dependence of the spike generation. [sent-227, score-0.672]
</p><p>98 Analyzing functional connectivity using a network likelihood model of ensemble neural spiking activity. [sent-253, score-0.273]
</p><p>99 Prediction and decoding of retinal ganglion cell responses with a probabilistic spiking model. [sent-274, score-0.401]
</p><p>100 The spatial ﬁltering properties of local edge detectors and brisk-sustained retinal ganglion cells. [sent-345, score-0.224]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('glm', 0.484), ('neuron', 0.343), ('stimulus', 0.211), ('spike', 0.187), ('couplings', 0.159), ('lters', 0.143), ('rust', 0.137), ('neurons', 0.131), ('lags', 0.126), ('ganglion', 0.118), ('sta', 0.115), ('tj', 0.111), ('history', 0.109), ('spiking', 0.108), ('retinal', 0.106), ('wi', 0.104), ('posterior', 0.094), ('seeger', 0.093), ('zeck', 0.092), ('weights', 0.09), ('laplace', 0.088), ('receptive', 0.085), ('cells', 0.084), ('intervals', 0.082), ('lter', 0.08), ('winther', 0.079), ('paninski', 0.079), ('prior', 0.077), ('spikes', 0.077), ('pillow', 0.074), ('dence', 0.074), ('ms', 0.073), ('simoncelli', 0.071), ('uj', 0.071), ('bursting', 0.069), ('stc', 0.069), ('cell', 0.069), ('xt', 0.066), ('ring', 0.064), ('sparsity', 0.064), ('inhibitory', 0.063), ('et', 0.062), ('response', 0.06), ('facilitates', 0.059), ('ep', 0.058), ('nonlinear', 0.058), ('propagation', 0.058), ('transfer', 0.057), ('opper', 0.056), ('subspace', 0.055), ('covariance', 0.054), ('dependence', 0.053), ('changepoints', 0.053), ('chornoboy', 0.053), ('snyder', 0.053), ('crossvalidation', 0.053), ('sp', 0.053), ('bj', 0.052), ('eigenvalues', 0.051), ('macaque', 0.051), ('minka', 0.051), ('cance', 0.049), ('nonlinearity', 0.049), ('functional', 0.047), ('okatan', 0.046), ('steveninck', 0.046), ('touryan', 0.046), ('ensemble', 0.046), ('excitatory', 0.045), ('recordings', 0.045), ('ni', 0.044), ('sparse', 0.043), ('di', 0.041), ('strongly', 0.041), ('effects', 0.041), ('bayesian', 0.041), ('rate', 0.04), ('binned', 0.039), ('miller', 0.039), ('schwartz', 0.039), ('spatiotemporal', 0.039), ('neural', 0.038), ('expectation', 0.038), ('bialek', 0.037), ('refractory', 0.037), ('linear', 0.037), ('relevant', 0.036), ('generalized', 0.035), ('harris', 0.035), ('model', 0.034), ('limited', 0.034), ('neurosci', 0.034), ('trains', 0.034), ('visual', 0.034), ('con', 0.033), ('eigenvectors', 0.033), ('site', 0.032), ('spanned', 0.032), ('highlighted', 0.032), ('matthias', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="33-tfidf-1" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><p>2 0.38082916 <a title="33-tfidf-2" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>Author: Jonathan W. Pillow, Peter E. Latham</p><p>Abstract: Point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, owing in part to the fact that they do not take into account multiple stages of processing. Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more ﬂexible for ﬁtting neural data. More importantly, it allows us to estimate connectivity patterns among neurons (both observed and unobserved), and may provide insight into how networks process sensory input. We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model’s performance using a simulated example network consisting of two coupled neurons.</p><p>3 0.33765444 <a title="33-tfidf-3" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>Author: Guenther Zeck, Matthias Bethge, Jakob H. Macke</p><p>Abstract: S timulus selectivity of sensory neurons is often characterized by estimating their receptive ﬁeld properties such as orientation selectivity. Receptive ﬁelds are usually derived from the mean (or covariance) of the spike-triggered stimulus ensemble. This approach treats each spike as an independent message but does not take into account that information might be conveyed through patterns of neural activity that are distributed across space or time. Can we ﬁnd a concise description for the processing of a whole population of neurons analogous to the receptive ﬁeld for single neurons? Here, we present a generalization of the linear receptive ﬁeld which is not bound to be triggered on individual spikes but can be meaningfully linked to distributed response patterns. More precisely, we seek to identify those stimulus features and the corresponding patterns of neural activity that are most reliably coupled. We use an extension of reverse-correlation methods based on canonical correlation analysis. The resulting population receptive ﬁelds span the subspace of stimuli that is most informative about the population response. We evaluate our approach using both neuronal models and multi-electrode recordings from rabbit retinal ganglion cells. We show how the model can be extended to capture nonlinear stimulus-response relationships using kernel canonical correlation analysis, which makes it possible to test different coding mechanisms. Our technique can also be used to calculate receptive ﬁelds from multi-dimensional neural measurements such as those obtained from dynamic imaging methods. 1</p><p>4 0.24871138 <a title="33-tfidf-4" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>Author: Tatyana Sharpee</p><p>Abstract: This paper compares a family of methods for characterizing neural feature selectivity with natural stimuli in the framework of the linear-nonlinear model. In this model, the neural ﬁring rate is a nonlinear function of a small number of relevant stimulus components. The relevant stimulus dimensions can be found by maximizing one of the family of objective functions, R´ nyi divergences of different e orders [1, 2]. We show that maximizing one of them, R´ nyi divergence of ore der 2, is equivalent to least-square ﬁtting of the linear-nonlinear model to neural data. Next, we derive reconstruction errors in relevant dimensions found by maximizing R´ nyi divergences of arbitrary order in the asymptotic limit of large spike e numbers. We ﬁnd that the smallest errors are obtained with R´ nyi divergence of e order 1, also known as Kullback-Leibler divergence. This corresponds to ﬁnding relevant dimensions by maximizing mutual information [2]. We numerically test how these optimization schemes perform in the regime of low signal-to-noise ratio (small number of spikes and increasing neural noise) for model visual neurons. We ﬁnd that optimization schemes based on either least square ﬁtting or information maximization perform well even when number of spikes is small. Information maximization provides slightly, but signiﬁcantly, better reconstructions than least square ﬁtting. This makes the problem of ﬁnding relevant dimensions, together with the problem of lossy compression [3], one of examples where informationtheoretic measures are no more data limited than those derived from least squares. 1</p><p>5 0.20303735 <a title="33-tfidf-5" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>Author: Lars Buesing, Wolfgang Maass</p><p>Abstract: We show that under suitable assumptions (primarily linearization) a simple and perspicuous online learning rule for Information Bottleneck optimization with spiking neurons can be derived. This rule performs on common benchmark tasks as well as a rather complex rule that has previously been proposed [1]. Furthermore, the transparency of this new learning rule makes a theoretical analysis of its convergence properties feasible. A variation of this learning rule (with sign changes) provides a theoretically founded method for performing Principal Component Analysis (PCA) with spiking neurons. By applying this rule to an ensemble of neurons, different principal components of the input can be extracted. In addition, it is possible to preferentially extract those principal components from incoming signals X that are related or are not related to some additional target signal YT . In a biological interpretation, this target signal YT (also called relevance variable) could represent proprioceptive feedback, input from other sensory modalities, or top-down signals. 1</p><p>6 0.19878863 <a title="33-tfidf-6" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>7 0.18963268 <a title="33-tfidf-7" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>8 0.18587677 <a title="33-tfidf-8" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>9 0.1704758 <a title="33-tfidf-9" href="./nips-2007-Contraction_Properties_of_VLSI_Cooperative_Competitive_Neural_Networks_of_Spiking_Neurons.html">60 nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</a></p>
<p>10 0.16907938 <a title="33-tfidf-10" href="./nips-2007-Learning_to_classify_complex_patterns_using_a_VLSI_network_of_spiking_neurons.html">117 nips-2007-Learning to classify complex patterns using a VLSI network of spiking neurons</a></p>
<p>11 0.1534138 <a title="33-tfidf-11" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>12 0.12955183 <a title="33-tfidf-12" href="./nips-2007-Supervised_Topic_Models.html">189 nips-2007-Supervised Topic Models</a></p>
<p>13 0.11094251 <a title="33-tfidf-13" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>14 0.10751615 <a title="33-tfidf-14" href="./nips-2007-A_configurable_analog_VLSI_neural_network_with_spiking_neurons_and_self-regulating_plastic_synapses.html">14 nips-2007-A configurable analog VLSI neural network with spiking neurons and self-regulating plastic synapses</a></p>
<p>15 0.10245055 <a title="33-tfidf-15" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>16 0.082805663 <a title="33-tfidf-16" href="./nips-2007-Estimating_disparity_with_confidence_from_energy_neurons.html">81 nips-2007-Estimating disparity with confidence from energy neurons</a></p>
<p>17 0.082253791 <a title="33-tfidf-17" href="./nips-2007-The_rat_as_particle_filter.html">203 nips-2007-The rat as particle filter</a></p>
<p>18 0.082024358 <a title="33-tfidf-18" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>19 0.081258155 <a title="33-tfidf-19" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<p>20 0.078762829 <a title="33-tfidf-20" href="./nips-2007-On_Sparsity_and_Overcompleteness_in_Image_Models.html">145 nips-2007-On Sparsity and Overcompleteness in Image Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.278), (1, 0.206), (2, 0.454), (3, 0.052), (4, 0.012), (5, -0.104), (6, 0.029), (7, -0.035), (8, 0.019), (9, 0.02), (10, 0.007), (11, -0.018), (12, 0.039), (13, 0.036), (14, 0.047), (15, 0.059), (16, 0.15), (17, 0.102), (18, 0.139), (19, 0.131), (20, -0.069), (21, 0.02), (22, 0.018), (23, 0.058), (24, -0.014), (25, 0.001), (26, 0.06), (27, -0.004), (28, -0.061), (29, 0.064), (30, -0.094), (31, -0.016), (32, -0.082), (33, -0.079), (34, 0.05), (35, 0.048), (36, 0.046), (37, 0.078), (38, 0.008), (39, 0.007), (40, 0.019), (41, 0.017), (42, 0.005), (43, -0.006), (44, 0.04), (45, 0.031), (46, 0.054), (47, 0.037), (48, 0.037), (49, -0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95485234 <a title="33-lsi-1" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><p>2 0.89545369 <a title="33-lsi-2" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>Author: Guenther Zeck, Matthias Bethge, Jakob H. Macke</p><p>Abstract: S timulus selectivity of sensory neurons is often characterized by estimating their receptive ﬁeld properties such as orientation selectivity. Receptive ﬁelds are usually derived from the mean (or covariance) of the spike-triggered stimulus ensemble. This approach treats each spike as an independent message but does not take into account that information might be conveyed through patterns of neural activity that are distributed across space or time. Can we ﬁnd a concise description for the processing of a whole population of neurons analogous to the receptive ﬁeld for single neurons? Here, we present a generalization of the linear receptive ﬁeld which is not bound to be triggered on individual spikes but can be meaningfully linked to distributed response patterns. More precisely, we seek to identify those stimulus features and the corresponding patterns of neural activity that are most reliably coupled. We use an extension of reverse-correlation methods based on canonical correlation analysis. The resulting population receptive ﬁelds span the subspace of stimuli that is most informative about the population response. We evaluate our approach using both neuronal models and multi-electrode recordings from rabbit retinal ganglion cells. We show how the model can be extended to capture nonlinear stimulus-response relationships using kernel canonical correlation analysis, which makes it possible to test different coding mechanisms. Our technique can also be used to calculate receptive ﬁelds from multi-dimensional neural measurements such as those obtained from dynamic imaging methods. 1</p><p>3 0.89217204 <a title="33-lsi-3" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>Author: Jonathan W. Pillow, Peter E. Latham</p><p>Abstract: Point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, owing in part to the fact that they do not take into account multiple stages of processing. Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more ﬂexible for ﬁtting neural data. More importantly, it allows us to estimate connectivity patterns among neurons (both observed and unobserved), and may provide insight into how networks process sensory input. We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model’s performance using a simulated example network consisting of two coupled neurons.</p><p>4 0.80630362 <a title="33-lsi-4" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>Author: Tatyana Sharpee</p><p>Abstract: This paper compares a family of methods for characterizing neural feature selectivity with natural stimuli in the framework of the linear-nonlinear model. In this model, the neural ﬁring rate is a nonlinear function of a small number of relevant stimulus components. The relevant stimulus dimensions can be found by maximizing one of the family of objective functions, R´ nyi divergences of different e orders [1, 2]. We show that maximizing one of them, R´ nyi divergence of ore der 2, is equivalent to least-square ﬁtting of the linear-nonlinear model to neural data. Next, we derive reconstruction errors in relevant dimensions found by maximizing R´ nyi divergences of arbitrary order in the asymptotic limit of large spike e numbers. We ﬁnd that the smallest errors are obtained with R´ nyi divergence of e order 1, also known as Kullback-Leibler divergence. This corresponds to ﬁnding relevant dimensions by maximizing mutual information [2]. We numerically test how these optimization schemes perform in the regime of low signal-to-noise ratio (small number of spikes and increasing neural noise) for model visual neurons. We ﬁnd that optimization schemes based on either least square ﬁtting or information maximization perform well even when number of spikes is small. Information maximization provides slightly, but signiﬁcantly, better reconstructions than least square ﬁtting. This makes the problem of ﬁnding relevant dimensions, together with the problem of lossy compression [3], one of examples where informationtheoretic measures are no more data limited than those derived from least squares. 1</p><p>5 0.66376704 <a title="33-lsi-5" href="./nips-2007-Contraction_Properties_of_VLSI_Cooperative_Competitive_Neural_Networks_of_Spiking_Neurons.html">60 nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</a></p>
<p>Author: Emre Neftci, Elisabetta Chicca, Giacomo Indiveri, Jean-jeacques Slotine, Rodney J. Douglas</p><p>Abstract: A non–linear dynamic system is called contracting if initial conditions are forgotten exponentially fast, so that all trajectories converge to a single trajectory. We use contraction theory to derive an upper bound for the strength of recurrent connections that guarantees contraction for complex neural networks. Speciﬁcally, we apply this theory to a special class of recurrent networks, often called Cooperative Competitive Networks (CCNs), which are an abstract representation of the cooperative-competitive connectivity observed in cortex. This speciﬁc type of network is believed to play a major role in shaping cortical responses and selecting the relevant signal among distractors and noise. In this paper, we analyze contraction of combined CCNs of linear threshold units and verify the results of our analysis in a hybrid analog/digital VLSI CCN comprising spiking neurons and dynamic synapses. 1</p><p>6 0.63762903 <a title="33-lsi-6" href="./nips-2007-Estimating_disparity_with_confidence_from_energy_neurons.html">81 nips-2007-Estimating disparity with confidence from energy neurons</a></p>
<p>7 0.62022007 <a title="33-lsi-7" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>8 0.60957462 <a title="33-lsi-8" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>9 0.6080907 <a title="33-lsi-9" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>10 0.59047657 <a title="33-lsi-10" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>11 0.54908943 <a title="33-lsi-11" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>12 0.46502084 <a title="33-lsi-12" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>13 0.43354547 <a title="33-lsi-13" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>14 0.43040004 <a title="33-lsi-14" href="./nips-2007-Learning_to_classify_complex_patterns_using_a_VLSI_network_of_spiking_neurons.html">117 nips-2007-Learning to classify complex patterns using a VLSI network of spiking neurons</a></p>
<p>15 0.40689346 <a title="33-lsi-15" href="./nips-2007-An_online_Hebbian_learning_rule_that_performs_Independent_Component_Analysis.html">26 nips-2007-An online Hebbian learning rule that performs Independent Component Analysis</a></p>
<p>16 0.37892193 <a title="33-lsi-16" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>17 0.36233109 <a title="33-lsi-17" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<p>18 0.32674426 <a title="33-lsi-18" href="./nips-2007-Augmented_Functional_Time_Series_Representation_and_Forecasting_with_Gaussian_Processes.html">28 nips-2007-Augmented Functional Time Series Representation and Forecasting with Gaussian Processes</a></p>
<p>19 0.32094392 <a title="33-lsi-19" href="./nips-2007-Continuous_Time_Particle_Filtering_for_fMRI.html">59 nips-2007-Continuous Time Particle Filtering for fMRI</a></p>
<p>20 0.31267205 <a title="33-lsi-20" href="./nips-2007-The_rat_as_particle_filter.html">203 nips-2007-The rat as particle filter</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.041), (13, 0.019), (16, 0.486), (18, 0.022), (19, 0.015), (21, 0.052), (34, 0.021), (35, 0.017), (36, 0.027), (47, 0.072), (83, 0.081), (85, 0.015), (90, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89834809 <a title="33-lda-1" href="./nips-2007-Contraction_Properties_of_VLSI_Cooperative_Competitive_Neural_Networks_of_Spiking_Neurons.html">60 nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</a></p>
<p>Author: Emre Neftci, Elisabetta Chicca, Giacomo Indiveri, Jean-jeacques Slotine, Rodney J. Douglas</p><p>Abstract: A non–linear dynamic system is called contracting if initial conditions are forgotten exponentially fast, so that all trajectories converge to a single trajectory. We use contraction theory to derive an upper bound for the strength of recurrent connections that guarantees contraction for complex neural networks. Speciﬁcally, we apply this theory to a special class of recurrent networks, often called Cooperative Competitive Networks (CCNs), which are an abstract representation of the cooperative-competitive connectivity observed in cortex. This speciﬁc type of network is believed to play a major role in shaping cortical responses and selecting the relevant signal among distractors and noise. In this paper, we analyze contraction of combined CCNs of linear threshold units and verify the results of our analysis in a hybrid analog/digital VLSI CCN comprising spiking neurons and dynamic synapses. 1</p><p>same-paper 2 0.89045793 <a title="33-lda-2" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><p>3 0.84677464 <a title="33-lda-3" href="./nips-2007-Robust_Regression_with_Twinned_Gaussian_Processes.html">170 nips-2007-Robust Regression with Twinned Gaussian Processes</a></p>
<p>Author: Andrew Naish-guzman, Sean Holden</p><p>Abstract: We propose a Gaussian process (GP) framework for robust inference in which a GP prior on the mixing weights of a two-component noise model augments the standard process over latent function values. This approach is a generalization of the mixture likelihood used in traditional robust GP regression, and a specialization of the GP mixture models suggested by Tresp [1] and Rasmussen and Ghahramani [2]. The value of this restriction is in its tractable expectation propagation updates, which allow for faster inference and model selection, and better convergence than the standard mixture. An additional beneﬁt over the latter method lies in our ability to incorporate knowledge of the noise domain to inﬂuence predictions, and to recover with the predictive distribution information about the outlier distribution via the gating process. The model has asymptotic complexity equal to that of conventional robust methods, but yields more conﬁdent predictions on benchmark problems than classical heavy-tailed models and exhibits improved stability for data with clustered corruptions, for which they fail altogether. We show further how our approach can be used without adjustment for more smoothly heteroscedastic data, and suggest how it could be extended to more general noise models. We also address similarities with the work of Goldberg et al. [3].</p><p>4 0.84647638 <a title="33-lda-4" href="./nips-2007-Topmoumoute_Online_Natural_Gradient_Algorithm.html">206 nips-2007-Topmoumoute Online Natural Gradient Algorithm</a></p>
<p>Author: Nicolas L. Roux, Pierre-antoine Manzagol, Yoshua Bengio</p><p>Abstract: Guided by the goal of obtaining an optimization algorithm that is both fast and yields good generalization, we study the descent direction maximizing the decrease in generalization error or the probability of not increasing generalization error. The surprising result is that from both the Bayesian and frequentist perspectives this can yield the natural gradient direction. Although that direction can be very expensive to compute we develop an efﬁcient, general, online approximation to the natural gradient descent which is suited to large scale problems. We report experimental results showing much faster convergence in computation time and in number of iterations with TONGA (Topmoumoute Online natural Gradient Algorithm) than with stochastic gradient descent, even on very large datasets.</p><p>5 0.67143619 <a title="33-lda-5" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>Author: Jonathan W. Pillow, Peter E. Latham</p><p>Abstract: Point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, owing in part to the fact that they do not take into account multiple stages of processing. Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more ﬂexible for ﬁtting neural data. More importantly, it allows us to estimate connectivity patterns among neurons (both observed and unobserved), and may provide insight into how networks process sensory input. We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model’s performance using a simulated example network consisting of two coupled neurons.</p><p>6 0.66044152 <a title="33-lda-6" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>7 0.63594884 <a title="33-lda-7" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>8 0.58039248 <a title="33-lda-8" href="./nips-2007-The_Generalized_FITC_Approximation.html">195 nips-2007-The Generalized FITC Approximation</a></p>
<p>9 0.57765913 <a title="33-lda-9" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>10 0.548154 <a title="33-lda-10" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>11 0.516316 <a title="33-lda-11" href="./nips-2007-Learning_to_classify_complex_patterns_using_a_VLSI_network_of_spiking_neurons.html">117 nips-2007-Learning to classify complex patterns using a VLSI network of spiking neurons</a></p>
<p>12 0.49410388 <a title="33-lda-12" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>13 0.48171526 <a title="33-lda-13" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<p>14 0.46675116 <a title="33-lda-14" href="./nips-2007-Variational_Inference_for_Diffusion_Processes.html">213 nips-2007-Variational Inference for Diffusion Processes</a></p>
<p>15 0.46521199 <a title="33-lda-15" href="./nips-2007-Fast_Variational_Inference_for_Large-scale_Internet_Diagnosis.html">87 nips-2007-Fast Variational Inference for Large-scale Internet Diagnosis</a></p>
<p>16 0.45929298 <a title="33-lda-16" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>17 0.45511919 <a title="33-lda-17" href="./nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">79 nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<p>18 0.42987201 <a title="33-lda-18" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>19 0.4192439 <a title="33-lda-19" href="./nips-2007-Adaptive_Online_Gradient_Descent.html">21 nips-2007-Adaptive Online Gradient Descent</a></p>
<p>20 0.41881949 <a title="33-lda-20" href="./nips-2007-Selecting_Observations_against_Adversarial_Objectives.html">174 nips-2007-Selecting Observations against Adversarial Objectives</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
