<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>37 nips-2007-Blind channel identification for speech dereverberation using l1-norm sparse learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-37" href="#">nips2007-37</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>37 nips-2007-Blind channel identification for speech dereverberation using l1-norm sparse learning</h1>
<br/><p>Source: <a title="nips-2007-37-pdf" href="http://papers.nips.cc/paper/3262-blind-channel-identification-for-speech-dereverberation-using-l1-norm-sparse-learning.pdf">pdf</a></p><p>Author: Yuanqing Lin, Jingdong Chen, Youngmoo Kim, Daniel D. Lee</p><p>Abstract: Speech dereverberation remains an open problem after more than three decades of research. The most challenging step in speech dereverberation is blind channel identiﬁcation (BCI). Although many BCI approaches have been developed, their performance is still far from satisfactory for practical applications. The main difﬁculty in BCI lies in ﬁnding an appropriate acoustic model, which not only can effectively resolve solution degeneracies due to the lack of knowledge of the source, but also robustly models real acoustic environments. This paper proposes a sparse acoustic room impulse response (RIR) model for BCI, that is, an acoustic RIR can be modeled by a sparse FIR ﬁlter. Under this model, we show how to formulate the BCI of a single-input multiple-output (SIMO) system into a l1 norm regularized least squares (LS) problem, which is convex and can be solved efﬁciently with guaranteed global convergence. The sparseness of solutions is controlled by l1 -norm regularization parameters. We propose a sparse learning scheme that infers the optimal l1 -norm regularization parameters directly from microphone observations under a Bayesian framework. Our results show that the proposed approach is effective and robust, and it yields source estimates in real acoustic environments with high ﬁdelity to anechoic chamber measurements.</p><p>Reference: <a title="nips-2007-37-reference" href="../nips2007_reference/nips-2007-Blind_channel_identification_for_speech_dereverberation_using_l1-norm_sparse_learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Blind channel identiﬁcation for speech dereverberation using l1-norm sparse learning  †  Yuanqing Lin† , Jingdong Chen‡ , Youngmoo Kim♯ , Daniel D. [sent-1, score-0.584]
</p><p>2 The most challenging step in speech dereverberation is blind channel identiﬁcation (BCI). [sent-3, score-0.643]
</p><p>3 The main difﬁculty in BCI lies in ﬁnding an appropriate acoustic model, which not only can effectively resolve solution degeneracies due to the lack of knowledge of the source, but also robustly models real acoustic environments. [sent-5, score-0.763]
</p><p>4 This paper proposes a sparse acoustic room impulse response (RIR) model for BCI, that is, an acoustic RIR can be modeled by a sparse FIR ﬁlter. [sent-6, score-0.97]
</p><p>5 We propose a sparse learning scheme that infers the optimal l1 -norm regularization parameters directly from microphone observations under a Bayesian framework. [sent-9, score-0.461]
</p><p>6 Our results show that the proposed approach is effective and robust, and it yields source estimates in real acoustic environments with high ﬁdelity to anechoic chamber measurements. [sent-10, score-0.825]
</p><p>7 1 Introduction Speech dereverberation, which may be viewed as a denoising technique, is crucial for many speech related applications, such as hands-free teleconferencing and automatic speech recognition. [sent-11, score-0.258]
</p><p>8 Although many approaches [1] have been developed for speech dereverberation, blind channel identiﬁcation (BCI) is believed to be the key to thoroughly solving the dereverberation problem. [sent-13, score-0.666]
</p><p>9 Most BCI approaches rely on source statistics (higher order statistics [2] or statistics of LPC coefﬁcients [3]), or spatial difference among multiple channels [4] for resolving solution degeneracies due to the lack of knowledge of the source. [sent-14, score-0.37]
</p><p>10 The performance of these approaches depends on how well they model real acoustic systems (mainly sources and channels). [sent-15, score-0.35]
</p><p>11 The BCI approaches using source statistics need a long sequence of data to build up the statistics, and their performance often degrades signiﬁcantly in real acoustic environments where acoustic systems are time-varying and only approximately time-invariant during a short time window. [sent-16, score-0.916]
</p><p>12 Besides the data efﬁciency issue, there are some other difﬁculties in the BCI approaches using source statistics, for example, non-stationarity of a speech source, whitening side effect, and non-minimum phase of a ﬁlter [2]. [sent-17, score-0.351]
</p><p>13 In contrast, the BCI approaches exploiting channel spatial difference are blind to the source, and thus they avoid those difﬁculties arising in assuming source statistics. [sent-18, score-0.541]
</p><p>14 In general, BCI for speech dereverberation is an active research area, and the main challenge is how to build an effective acoustic model that not only can resolve solution degeneracies due to the lack of knowledge of the source, but also robustly models real acoustic environments. [sent-20, score-1.097]
</p><p>15 1  To address the challenge, this paper proposes a sparse acoustic room impulse response (RIR) model for BCI, that is, an acoustic RIR can be modeled by a sparse FIR ﬁlter. [sent-21, score-0.97]
</p><p>16 The sparse RIR model is theoretically sound [5], and it has been shown to be useful for estimating RIRs in real acoustic environments when the source is given a priori [6]. [sent-22, score-0.702]
</p><p>17 In this paper, the sparse RIR model is incorporated with channel spatial difference, resulting a blind sparse channel identiﬁcation (BSCI) approach for a single-input multiple-output (SIMO) acoustic system. [sent-23, score-0.998]
</p><p>18 It is blind to the source and therefore avoids the difﬁculties arising in assuming source statistics. [sent-25, score-0.6]
</p><p>19 It has been shown that, when the source is given a priori [7], the prior knowledge about sparse RIRs plays an important role in robustly estimating RIRs in noisy acoustic environments. [sent-27, score-0.69]
</p><p>20 Furthermore, the statistics describing the sparseness of RIRs are governed by acoustic room characteristics, and thus they are close to be stationary with respect to a speciﬁc room. [sent-28, score-0.459]
</p><p>21 Based on the cross relation formulation [4] of BCI, this paper develops a BSCI algorithm that incorporates the sparse RIR model. [sent-30, score-0.263]
</p><p>22 Our choice for enforcing sparsity is l1 -norm regularization [8], which has been the driving force for many emerging ﬁelds in signal processing, such as sparse coding and compressive sensing. [sent-31, score-0.271]
</p><p>23 First, the existing cross relation formulation for BCI is nonconvex, and directly enforcing l1 -norm regularization will result in an intractable optimization. [sent-33, score-0.225]
</p><p>24 We evaluate the proposed BSCI approach using both simulations and experiments in real acoustic environments. [sent-36, score-0.381]
</p><p>25 Simulation results illustrate the effectiveness of the proposed sparse RIR model in resolving solution degeneracies, and they show that the BSCI approach is able to robustly and accurately identify ﬁlters from noisy microphone observations. [sent-37, score-0.487]
</p><p>26 When applied to speech dereverberation in real acoustic environments, the BSCI approach yields source estimates with high ﬁdelity to anechoic chamber measurements. [sent-38, score-1.116]
</p><p>27 All of these demonstrate that the BSCI approach has the potential for solving the difﬁcult speech dereverberation problem. [sent-39, score-0.358]
</p><p>28 1 Previous work Our BSCI approach is based on the cross relation formulation for blind SIMO channel identiﬁcation [4]. [sent-41, score-0.473]
</p><p>29 The cross relation formulation is based on a clever observation, x2 (k) ∗ h1 = x1 (k) ∗ h2 = s(k) ∗ h1 ∗ h2 , if the microphone signals are noiseless [4]. [sent-43, score-0.472]
</p><p>30 Then, without requiring any knowledge from the source signal, the channel ﬁlters can be identiﬁed by minimizing the squared cross relation error. [sent-44, score-0.439]
</p><p>31 , 0]T , respectively, N is the microphone signal length, L is the ﬁlter length, hi (i = 1, 2) are L × 1 vectors representing the ﬁlters, · denotes l2 -norm, and the constraint is to avoid the trivial zero solution. [sent-54, score-0.35]
</p><p>32 It is easy to see that the above optimization is a minimum eigenvalue problem, and it can be solved by eigenvalue decomposition. [sent-55, score-0.202]
</p><p>33 As shown in [4], the eigenvalue decomposition approach ﬁnds the true solution within a constant time delay and a constant scalar factor when 1) the system is noiseless; 2) the two 2  ﬁlters are co-prime (namely, no common zeros); and 3) the system is sufﬁciently excited (i. [sent-56, score-0.271]
</p><p>34 Unfortunately, the eigenvalue decomposition approach has not been demonstrated to be useful for speech dereverberation in real acoustic environments. [sent-59, score-0.839]
</p><p>35 First, microphone signals in real acoustic environments are always immersed in excessive ambient noise (such as air-conditioning noise), and thus the noiseless assumption is never true. [sent-61, score-0.867]
</p><p>36 As a result, eigenvalue decomposition approach is often ill-conditioned and very sensitive to even a very small amount of ambient noise. [sent-63, score-0.3]
</p><p>37 Under the sparse RIR model, sparsity regularization automatically determines ﬁlter order since surplus ﬁlter coefﬁcients are forced to be zero. [sent-65, score-0.213]
</p><p>38 Furthermore, previous work [7] has demonstrated that, when the source is given a priori, sparsity regularization plays an important role in robustly estimating RIRs in noisy acoustic environments. [sent-66, score-0.657]
</p><p>39 In order to exploit the sparse RIR model, we ﬁrst formulate the BCI using cross relation into a convex optimization, which will provide a ﬂexible platform for enforcing l1 -norm sparsity regularization. [sent-67, score-0.331]
</p><p>40 It is easy to see that, when microphone signals are noiseless, the optimizations in Eqs. [sent-72, score-0.294]
</p><p>41 Furthermore, the new LS formulation is more robust to ambient noise than the eigenvalue decomposition approach in Eq. [sent-80, score-0.38]
</p><p>42 2 may be ﬁlled with less signiﬁcant frequency bands which contribute little to the source and are weighted less in the objective function. [sent-85, score-0.242]
</p><p>43 As a result, the eigenvalue decomposition approach is very sensitive to noise. [sent-86, score-0.178]
</p><p>44 3 Bayesian l1 -norm sparse learning for blind channel identiﬁcation The l1 -norm regularization parameter λ′ in Eq. [sent-98, score-0.495]
</p><p>45 However, it is not easy to obtain extra data for crossvalidation in BCI since real acoustic environments are often time-varying. [sent-103, score-0.37]
</p><p>46 A similar Bayesian framework can be found in [7], where the source was assumed to be known a priori. [sent-106, score-0.209]
</p><p>47 zero-mean Gaussian with variance σ 2 , and the ﬁlter coefﬁcients are governed by a Laplacian sparse prior with the scalar parameter λ. [sent-111, score-0.188]
</p><p>48 (7) When the ambient noise [n1 (k) and n2 (k) in Eq. [sent-114, score-0.165]
</p><p>49 The above minimization is solved by coordinate descent alternatively with respect to the source and the ﬁlters. [sent-123, score-0.209]
</p><p>50 These statistics are closely related j=0 to acoustic room characteristics and may be computed from them if they are known a priori. [sent-130, score-0.414]
</p><p>51 More generally, we choose to compute the statistics directly from microphone observations in the Baysian framework by maximizing the marginal likelihood, P (X2 h1 − X1 h2 |σ 2 , λ) = h1 (l)=1 P (X2 h1 − X1 h2 , h1 , h2 |σ 2 , λ)dh1 dh2 . [sent-132, score-0.297]
</p><p>52 After the ﬁlters are identiﬁed by BCI approaches, the source can be computed by various methods [12]. [sent-138, score-0.209]
</p><p>53 We choose to estimate the source by the following optimization 2 N −1  s∗ = argmin s  xi (k) − s(k) ∗ hi  2  ,  (13)  i=1 k=0  which will yield maximum-likelihood (ML) estimation if the ﬁlter estimates are accurate. [sent-139, score-0.35]
</p><p>54 In the simulation, we used a speech sequence of 1024 samples (with 16 kHz sampling rate) as the source (s) and simulated two 16-sample FIR ﬁlters (h1 and h2 ). [sent-144, score-0.361]
</p><p>55 Then the simulated microphone observations (x1 and x2 ) were computed by Eq. [sent-152, score-0.308]
</p><p>56 1 with the ambient noise being real noise recorded in a classroom. [sent-153, score-0.269]
</p><p>57 The noise was scaled so that the signal-to-noise ratio (SNR) of the microphone signals was approximately 20 dB. [sent-154, score-0.337]
</p><p>58 Because a big portion of the noise (mainly air-conditioning noise) was at low frequency, the microphone observations were high-passed with a cut-off frequency of 100 Hz before they were fed to BCI algorithms. [sent-155, score-0.378]
</p><p>59 3) is more robust to ambient noise and yielded better ﬁlter estimates even though the estimates still seem to be convolved by a common ﬁlter. [sent-161, score-0.273]
</p><p>60 It is evident that the proposed sparse RIR model played a crucial role in robustly and accurately identifying ﬁlters in blind manners. [sent-164, score-0.371]
</p><p>61 The robustness and accuracy gained by the BSCI approach will become essential when the ﬁlters are thousands of taps long in real acoustic environments. [sent-165, score-0.374]
</p><p>62 2 Simulations with measured RIRs Here we employ simulations using RIRs measured in real rooms to demonstrate the effectiveness of the proposed BSCI approach for speech dereverberation. [sent-168, score-0.208]
</p><p>63 In the simulation, the source sequence (s) was a sentence of speech (approximately 1. [sent-172, score-0.328]
</p><p>64 With the simulated source and ﬁlters, we then computed microphone observations using Eq. [sent-183, score-0.517]
</p><p>65 1 with ambient noise being real noise recorded in a classroom. [sent-184, score-0.269]
</p><p>66 3, and the blind sparse channel identiﬁcation (BSCI) approach in Eq. [sent-190, score-0.456]
</p><p>67 The ﬁlters were identiﬁed by three different approaches: the eigenvalue decomposition approach (denoted as eigen-decomp) in Eq. [sent-196, score-0.178]
</p><p>68 3, and the blind sparse channel identiﬁcation (BSCI) approach in Eq. [sent-198, score-0.456]
</p><p>69 After the ﬁlters were identiﬁed, the source was estimated by Eq. [sent-200, score-0.227]
</p><p>70 The source estimated by beamforming is also presented as a baseline reference. [sent-202, score-0.321]
</p><p>71 After ﬁlters were identiﬁed, the source was estimated using Eq. [sent-206, score-0.227]
</p><p>72 Because both ﬁlter and source estimates by BCI algorithms are within a constant time delay and a constant scalar factor, we use normalized correlation for evaluating the estimates. [sent-208, score-0.424]
</p><p>73 Let ˆ and s0 s denote an estimated source and the true source, respectively, then the normalized correlation C(ˆ, s0 ) s is deﬁned as s(k − m)s0 (k) ˆ C(ˆ, s0 ) = max k s (14) m ˆ s0 s where m and k are sample indices, and · denotes l2 -norm. [sent-209, score-0.315]
</p><p>74 It is easy to see that, the normalized correlation is between 0% and 100%: it is equal to 0% when the two signals are uncorrelated, and it is equal to 100% only when the two signal are identical within a constant time delay and a constant scalar factor. [sent-210, score-0.256]
</p><p>75 3) shows signiﬁcant improvement in both ﬁlter and source estimation compared to the eigenvalue decomposition approach (Eq. [sent-216, score-0.387]
</p><p>76 In fact, the eigenvalue decomposition 6  Normalized correlation (%)  100  Beamforming Eig−decomp LS BSCI  90 80 70 60 50 40 30 1  2  3  4  5  6 7 Experiments  8  9  10  Figure 3: The source estimates of 10 experiments in real acoustic environments. [sent-218, score-0.772]
</p><p>77 The normalized correlation was with respect to their anechoic chamber measurement. [sent-219, score-0.276]
</p><p>78 The ﬁlters were identiﬁed by three different BCI approaches: the eigenvalue decomposition approach (denoted as eig-decomp) in Eq. [sent-220, score-0.178]
</p><p>79 3, and the blind sparse channel identiﬁcation (BSCI) approach in Eq. [sent-222, score-0.456]
</p><p>80 Left: the ﬁlters estimated by the proposed blind sparse channel identiﬁcation (BSCI) approach. [sent-231, score-0.45]
</p><p>81 Right: a segment of source estimate (shown in C) using the BSCI approach. [sent-233, score-0.209]
</p><p>82 It is compared with its anechoic measurement (shown in A) and its microphone recording (shown in B). [sent-234, score-0.405]
</p><p>83 The remarkable performance came from the BSCI approach, which incorporates the convex LS formulation with the sparse RIR model. [sent-236, score-0.196]
</p><p>84 In particular, the BSCI approach yielded higher than 90% normalized correlation in source estimates when SNR was better than 20 dB, and it yielded higher than 99% normalized correlation in the low noise limit. [sent-237, score-0.566]
</p><p>85 2 Experiments We also evaluated the proposed BSCI approach using signals recorded in real acoustic environments. [sent-240, score-0.416]
</p><p>86 5 seconds, and the same for all experiments) was played through a loudspeaker (NSW2-326-8A, Aura Sound) and recorded by a matched omnidirectional microphone pair (M30MP, Earthworks). [sent-243, score-0.281]
</p><p>87 We also had recordings in the anechoic chamber at Bell Labs using the same instruments and settings, and the anechoic measurement served as the approximated ground truth for evaluating the performance of different BCI approaches. [sent-250, score-0.338]
</p><p>88 7  Figure 3 shows the source estimates in the 10 experiments in terms of their normalized correlation to the anechoic measurement. [sent-251, score-0.452]
</p><p>89 First, the convex LS approach yielded signiﬁcantly better source estimates than the eigenvalue decomposition method. [sent-256, score-0.497]
</p><p>90 Figure 4 shows one instance of ﬁlter and source estimates. [sent-258, score-0.209]
</p><p>91 This observation experimentally validates our hypothesis of the sparse RIR models, namely, an acoustic RIR can be modeled by a sparse FIR ﬁlter. [sent-260, score-0.538]
</p><p>92 4 Discussion We propose a blind sparse channel identiﬁcation (BSCI) approach for speech dereverberation. [sent-265, score-0.575]
</p><p>93 The ﬁrst is the sparse RIR model, which effectively resolves solution degeneracies and robustly models real acoustic environments. [sent-267, score-0.569]
</p><p>94 And the third is the Bayesian l1 -norm sparse learning scheme that infers the optimal regularization parameters for deriving optimally sparse solutions. [sent-269, score-0.331]
</p><p>95 The results demonstrate that the proposed BSCI approach holds the potential to solve the speech dereverberation problem in real acoustic environments, which has been recognized as a very difﬁcult problem in signal processing. [sent-270, score-0.721]
</p><p>96 The acoustic data used in this paper are available at http://www. [sent-271, score-0.292]
</p><p>97 Our future work includes side-by-side comparison between our BSCI approach and existing source statistics based BCI approaches. [sent-276, score-0.255]
</p><p>98 Our goal is to build a uniform framework that combines various prior knowledge about acoustic systems for best solving the speech dereverberation problem. [sent-277, score-0.626]
</p><p>99 Kinoshita, “One microphone blind dereverberation based on quasiperiodicity of speech signals,” in NIPS 16. [sent-281, score-0.771]
</p><p>100 Deng, “Speech denoising and dereverberation using probabilistic models,” in NIPS 13, 2000. [sent-292, score-0.235]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bsci', 0.511), ('acoustic', 0.292), ('bci', 0.278), ('microphone', 0.255), ('rir', 0.242), ('dereverberation', 0.215), ('source', 0.209), ('ls', 0.207), ('blind', 0.182), ('lters', 0.151), ('rirs', 0.148), ('channel', 0.127), ('sparse', 0.123), ('ambient', 0.122), ('anechoic', 0.121), ('speech', 0.119), ('lter', 0.114), ('room', 0.1), ('beamforming', 0.094), ('eigenvalue', 0.09), ('identi', 0.075), ('chamber', 0.067), ('robustly', 0.066), ('decomposition', 0.064), ('regularization', 0.063), ('relation', 0.055), ('decomp', 0.054), ('fir', 0.054), ('simo', 0.054), ('degeneracies', 0.053), ('delay', 0.049), ('cross', 0.048), ('correlation', 0.048), ('scalar', 0.044), ('environments', 0.043), ('noise', 0.043), ('db', 0.043), ('impulse', 0.04), ('yielded', 0.04), ('normalized', 0.04), ('hi', 0.04), ('signals', 0.039), ('noiseless', 0.038), ('formulation', 0.037), ('convex', 0.036), ('amplitude', 0.036), ('signal', 0.036), ('real', 0.035), ('amplitudes', 0.035), ('khz', 0.035), ('estimates', 0.034), ('frequency', 0.033), ('simulated', 0.033), ('simulations', 0.03), ('singleton', 0.03), ('measurement', 0.029), ('coef', 0.028), ('convolution', 0.028), ('argmin', 0.027), ('sparsity', 0.027), ('eig', 0.027), ('miyoshi', 0.027), ('acoustics', 0.027), ('fed', 0.027), ('cients', 0.027), ('recorded', 0.026), ('culties', 0.025), ('resolve', 0.025), ('approach', 0.024), ('sparseness', 0.024), ('echoes', 0.023), ('eigen', 0.023), ('reverberant', 0.023), ('taps', 0.023), ('approaches', 0.023), ('bayesian', 0.023), ('deriving', 0.022), ('enforcing', 0.022), ('statistics', 0.022), ('optimization', 0.022), ('governed', 0.021), ('simulation', 0.021), ('nonzero', 0.021), ('observations', 0.02), ('denoising', 0.02), ('meanwhile', 0.02), ('platform', 0.02), ('snr', 0.02), ('hz', 0.02), ('constraint', 0.019), ('decades', 0.019), ('delity', 0.019), ('resolving', 0.019), ('estimated', 0.018), ('cutoff', 0.018), ('totally', 0.018), ('solutions', 0.018), ('yield', 0.018), ('zeros', 0.017), ('nonconvex', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="37-tfidf-1" href="./nips-2007-Blind_channel_identification_for_speech_dereverberation_using_l1-norm_sparse_learning.html">37 nips-2007-Blind channel identification for speech dereverberation using l1-norm sparse learning</a></p>
<p>Author: Yuanqing Lin, Jingdong Chen, Youngmoo Kim, Daniel D. Lee</p><p>Abstract: Speech dereverberation remains an open problem after more than three decades of research. The most challenging step in speech dereverberation is blind channel identiﬁcation (BCI). Although many BCI approaches have been developed, their performance is still far from satisfactory for practical applications. The main difﬁculty in BCI lies in ﬁnding an appropriate acoustic model, which not only can effectively resolve solution degeneracies due to the lack of knowledge of the source, but also robustly models real acoustic environments. This paper proposes a sparse acoustic room impulse response (RIR) model for BCI, that is, an acoustic RIR can be modeled by a sparse FIR ﬁlter. Under this model, we show how to formulate the BCI of a single-input multiple-output (SIMO) system into a l1 norm regularized least squares (LS) problem, which is convex and can be solved efﬁciently with guaranteed global convergence. The sparseness of solutions is controlled by l1 -norm regularization parameters. We propose a sparse learning scheme that infers the optimal l1 -norm regularization parameters directly from microphone observations under a Bayesian framework. Our results show that the proposed approach is effective and robust, and it yields source estimates in real acoustic environments with high ﬁdelity to anechoic chamber measurements.</p><p>2 0.15192758 <a title="37-tfidf-2" href="./nips-2007-Invariant_Common_Spatial_Patterns%3A_Alleviating_Nonstationarities_in_Brain-Computer_Interfacing.html">106 nips-2007-Invariant Common Spatial Patterns: Alleviating Nonstationarities in Brain-Computer Interfacing</a></p>
<p>Author: Benjamin Blankertz, Motoaki Kawanabe, Ryota Tomioka, Friederike Hohlefeld, Klaus-Robert Müller, Vadim V. Nikulin</p><p>Abstract: Brain-Computer Interfaces can suffer from a large variance of the subject conditions within and across sessions. For example vigilance ﬂuctuations in the individual, variable task involvement, workload etc. alter the characteristics of EEG signals and thus challenge a stable BCI operation. In the present work we aim to deﬁne features based on a variant of the common spatial patterns (CSP) algorithm that are constructed invariant with respect to such nonstationarities. We enforce invariance properties by adding terms to the denominator of a Rayleigh coefﬁcient representation of CSP such as disturbance covariance matrices from ﬂuctuations in visual processing. In this manner physiological prior knowledge can be used to shape the classiﬁcation engine for BCI. As a proof of concept we present a BCI classiﬁer that is robust to changes in the level of parietal α -activity. In other words, the EEG decoding still works when there are lapses in vigilance.</p><p>3 0.11942774 <a title="37-tfidf-3" href="./nips-2007-Learning_Bounds_for_Domain_Adaptation.html">110 nips-2007-Learning Bounds for Domain Adaptation</a></p>
<p>Author: John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, Jennifer Wortman</p><p>Abstract: Empirical risk minimization offers well-known learning guarantees when training and test data come from the same domain. In the real world, though, we often wish to adapt a classiﬁer from a source domain with a large amount of training data to different target domain with very little training data. In this work we give uniform convergence bounds for algorithms that minimize a convex combination of source and target empirical risk. The bounds explicitly model the inherent trade-off between training on a large but inaccurate source data set and a small but accurate target training set. Our theory also gives results when we have multiple source domains, each of which may have a different number of instances, and we exhibit cases in which minimizing a non-uniform combination of source risks can achieve much lower target error than standard empirical risk minimization. 1</p><p>4 0.097794712 <a title="37-tfidf-4" href="./nips-2007-EEG-Based_Brain-Computer_Interaction%3A_Improved_Accuracy_by_Automatic_Single-Trial_Error_Detection.html">74 nips-2007-EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection</a></p>
<p>Author: Pierre Ferrez, José Millán</p><p>Abstract: Brain-computer interfaces (BCIs), as any other interaction modality based on physiological signals and body channels (e.g., muscular activity, speech and gestures), are prone to errors in the recognition of subject’s intent. An elegant approach to improve the accuracy of BCIs consists in a veriﬁcation procedure directly based on the presence of error-related potentials (ErrP) in the EEG recorded right after the occurrence of an error. Six healthy volunteer subjects with no prior BCI experience participated in a new human-robot interaction experiment where they were asked to mentally move a cursor towards a target that can be reached within a few steps using motor imagination. This experiment conﬁrms the previously reported presence of a new kind of ErrP. These “Interaction ErrP” exhibit a ﬁrst sharp negative peak followed by a positive peak and a second broader negative peak (∼290, ∼350 and ∼470 ms after the feedback, respectively). But in order to exploit these ErrP we need to detect them in each single trial using a short window following the feedback associated to the response of the classiﬁer embedded in the BCI. We have achieved an average recognition rate of correct and erroneous single trials of 81.8% and 76.2%, respectively. Furthermore, we have achieved an average recognition rate of the subject’s intent while trying to mentally drive the cursor of 73.1%. These results show that it’s possible to simultaneously extract useful information for mental control to operate a brain-actuated device as well as cognitive states such as error potentials to improve the quality of the braincomputer interaction. Finally, using a well-known inverse model (sLORETA), we show that the main focus of activity at the occurrence of the ErrP are, as expected, in the pre-supplementary motor area and in the anterior cingulate cortex. 1</p><p>5 0.092440739 <a title="37-tfidf-5" href="./nips-2007-Second_Order_Bilinear_Discriminant_Analysis_for_single_trial_EEG_analysis.html">173 nips-2007-Second Order Bilinear Discriminant Analysis for single trial EEG analysis</a></p>
<p>Author: Christoforos Christoforou, Paul Sajda, Lucas C. Parra</p><p>Abstract: Traditional analysis methods for single-trial classiﬁcation of electroencephalography (EEG) focus on two types of paradigms: phase locked methods, in which the amplitude of the signal is used as the feature for classiﬁcation, e.g. event related potentials; and second order methods, in which the feature of interest is the power of the signal, e.g. event related (de)synchronization. The procedure for deciding which paradigm to use is ad hoc and is typically driven by knowledge of the underlying neurophysiology. Here we propose a principled method, based on a bilinear model, in which the algorithm simultaneously learns the best ﬁrst and second order spatial and temporal features for classiﬁcation of EEG. The method is demonstrated on simulated data as well as on EEG taken from a benchmark data used to test classiﬁcation algorithms for brain computer interfaces. 1 1.1</p><p>6 0.066598095 <a title="37-tfidf-6" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>7 0.061649177 <a title="37-tfidf-7" href="./nips-2007-On_Sparsity_and_Overcompleteness_in_Image_Models.html">145 nips-2007-On Sparsity and Overcompleteness in Image Models</a></p>
<p>8 0.060167979 <a title="37-tfidf-8" href="./nips-2007-A_New_View_of_Automatic_Relevance_Determination.html">8 nips-2007-A New View of Automatic Relevance Determination</a></p>
<p>9 0.059334867 <a title="37-tfidf-9" href="./nips-2007-Mining_Internet-Scale_Software_Repositories.html">129 nips-2007-Mining Internet-Scale Software Repositories</a></p>
<p>10 0.057536468 <a title="37-tfidf-10" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>11 0.04966142 <a title="37-tfidf-11" href="./nips-2007-DIFFRAC%3A_a_discriminative_and_flexible_framework_for_clustering.html">65 nips-2007-DIFFRAC: a discriminative and flexible framework for clustering</a></p>
<p>12 0.048826609 <a title="37-tfidf-12" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>13 0.04774598 <a title="37-tfidf-13" href="./nips-2007-Expectation_Maximization_and_Posterior_Constraints.html">84 nips-2007-Expectation Maximization and Posterior Constraints</a></p>
<p>14 0.0476515 <a title="37-tfidf-14" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>15 0.046259817 <a title="37-tfidf-15" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>16 0.044900354 <a title="37-tfidf-16" href="./nips-2007-Discriminative_Keyword_Selection_Using_Support_Vector_Machines.html">71 nips-2007-Discriminative Keyword Selection Using Support Vector Machines</a></p>
<p>17 0.043389436 <a title="37-tfidf-17" href="./nips-2007-Random_Projections_for_Manifold_Learning.html">161 nips-2007-Random Projections for Manifold Learning</a></p>
<p>18 0.03879514 <a title="37-tfidf-18" href="./nips-2007-Modeling_Natural_Sounds_with_Modulation_Cascade_Processes.html">130 nips-2007-Modeling Natural Sounds with Modulation Cascade Processes</a></p>
<p>19 0.038411289 <a title="37-tfidf-19" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>20 0.038191069 <a title="37-tfidf-20" href="./nips-2007-People_Tracking_with_the_Laplacian_Eigenmaps_Latent_Variable_Model.html">153 nips-2007-People Tracking with the Laplacian Eigenmaps Latent Variable Model</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.123), (1, 0.053), (2, 0.022), (3, -0.009), (4, 0.008), (5, 0.074), (6, 0.003), (7, 0.072), (8, -0.076), (9, -0.032), (10, 0.077), (11, -0.066), (12, 0.01), (13, -0.076), (14, 0.162), (15, -0.07), (16, 0.001), (17, -0.081), (18, -0.067), (19, -0.092), (20, -0.151), (21, -0.09), (22, 0.072), (23, 0.099), (24, 0.009), (25, -0.041), (26, 0.188), (27, -0.088), (28, 0.021), (29, -0.048), (30, -0.034), (31, -0.119), (32, -0.034), (33, -0.074), (34, 0.076), (35, 0.021), (36, 0.043), (37, 0.08), (38, -0.125), (39, 0.106), (40, -0.084), (41, 0.094), (42, -0.041), (43, -0.056), (44, 0.026), (45, 0.018), (46, -0.075), (47, -0.143), (48, 0.04), (49, -0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95112389 <a title="37-lsi-1" href="./nips-2007-Blind_channel_identification_for_speech_dereverberation_using_l1-norm_sparse_learning.html">37 nips-2007-Blind channel identification for speech dereverberation using l1-norm sparse learning</a></p>
<p>Author: Yuanqing Lin, Jingdong Chen, Youngmoo Kim, Daniel D. Lee</p><p>Abstract: Speech dereverberation remains an open problem after more than three decades of research. The most challenging step in speech dereverberation is blind channel identiﬁcation (BCI). Although many BCI approaches have been developed, their performance is still far from satisfactory for practical applications. The main difﬁculty in BCI lies in ﬁnding an appropriate acoustic model, which not only can effectively resolve solution degeneracies due to the lack of knowledge of the source, but also robustly models real acoustic environments. This paper proposes a sparse acoustic room impulse response (RIR) model for BCI, that is, an acoustic RIR can be modeled by a sparse FIR ﬁlter. Under this model, we show how to formulate the BCI of a single-input multiple-output (SIMO) system into a l1 norm regularized least squares (LS) problem, which is convex and can be solved efﬁciently with guaranteed global convergence. The sparseness of solutions is controlled by l1 -norm regularization parameters. We propose a sparse learning scheme that infers the optimal l1 -norm regularization parameters directly from microphone observations under a Bayesian framework. Our results show that the proposed approach is effective and robust, and it yields source estimates in real acoustic environments with high ﬁdelity to anechoic chamber measurements.</p><p>2 0.6365515 <a title="37-lsi-2" href="./nips-2007-Invariant_Common_Spatial_Patterns%3A_Alleviating_Nonstationarities_in_Brain-Computer_Interfacing.html">106 nips-2007-Invariant Common Spatial Patterns: Alleviating Nonstationarities in Brain-Computer Interfacing</a></p>
<p>Author: Benjamin Blankertz, Motoaki Kawanabe, Ryota Tomioka, Friederike Hohlefeld, Klaus-Robert Müller, Vadim V. Nikulin</p><p>Abstract: Brain-Computer Interfaces can suffer from a large variance of the subject conditions within and across sessions. For example vigilance ﬂuctuations in the individual, variable task involvement, workload etc. alter the characteristics of EEG signals and thus challenge a stable BCI operation. In the present work we aim to deﬁne features based on a variant of the common spatial patterns (CSP) algorithm that are constructed invariant with respect to such nonstationarities. We enforce invariance properties by adding terms to the denominator of a Rayleigh coefﬁcient representation of CSP such as disturbance covariance matrices from ﬂuctuations in visual processing. In this manner physiological prior knowledge can be used to shape the classiﬁcation engine for BCI. As a proof of concept we present a BCI classiﬁer that is robust to changes in the level of parietal α -activity. In other words, the EEG decoding still works when there are lapses in vigilance.</p><p>3 0.61902142 <a title="37-lsi-3" href="./nips-2007-EEG-Based_Brain-Computer_Interaction%3A_Improved_Accuracy_by_Automatic_Single-Trial_Error_Detection.html">74 nips-2007-EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection</a></p>
<p>Author: Pierre Ferrez, José Millán</p><p>Abstract: Brain-computer interfaces (BCIs), as any other interaction modality based on physiological signals and body channels (e.g., muscular activity, speech and gestures), are prone to errors in the recognition of subject’s intent. An elegant approach to improve the accuracy of BCIs consists in a veriﬁcation procedure directly based on the presence of error-related potentials (ErrP) in the EEG recorded right after the occurrence of an error. Six healthy volunteer subjects with no prior BCI experience participated in a new human-robot interaction experiment where they were asked to mentally move a cursor towards a target that can be reached within a few steps using motor imagination. This experiment conﬁrms the previously reported presence of a new kind of ErrP. These “Interaction ErrP” exhibit a ﬁrst sharp negative peak followed by a positive peak and a second broader negative peak (∼290, ∼350 and ∼470 ms after the feedback, respectively). But in order to exploit these ErrP we need to detect them in each single trial using a short window following the feedback associated to the response of the classiﬁer embedded in the BCI. We have achieved an average recognition rate of correct and erroneous single trials of 81.8% and 76.2%, respectively. Furthermore, we have achieved an average recognition rate of the subject’s intent while trying to mentally drive the cursor of 73.1%. These results show that it’s possible to simultaneously extract useful information for mental control to operate a brain-actuated device as well as cognitive states such as error potentials to improve the quality of the braincomputer interaction. Finally, using a well-known inverse model (sLORETA), we show that the main focus of activity at the occurrence of the ErrP are, as expected, in the pre-supplementary motor area and in the anterior cingulate cortex. 1</p><p>4 0.53173709 <a title="37-lsi-4" href="./nips-2007-Learning_Bounds_for_Domain_Adaptation.html">110 nips-2007-Learning Bounds for Domain Adaptation</a></p>
<p>Author: John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, Jennifer Wortman</p><p>Abstract: Empirical risk minimization offers well-known learning guarantees when training and test data come from the same domain. In the real world, though, we often wish to adapt a classiﬁer from a source domain with a large amount of training data to different target domain with very little training data. In this work we give uniform convergence bounds for algorithms that minimize a convex combination of source and target empirical risk. The bounds explicitly model the inherent trade-off between training on a large but inaccurate source data set and a small but accurate target training set. Our theory also gives results when we have multiple source domains, each of which may have a different number of instances, and we exhibit cases in which minimizing a non-uniform combination of source risks can achieve much lower target error than standard empirical risk minimization. 1</p><p>5 0.50708961 <a title="37-lsi-5" href="./nips-2007-Second_Order_Bilinear_Discriminant_Analysis_for_single_trial_EEG_analysis.html">173 nips-2007-Second Order Bilinear Discriminant Analysis for single trial EEG analysis</a></p>
<p>Author: Christoforos Christoforou, Paul Sajda, Lucas C. Parra</p><p>Abstract: Traditional analysis methods for single-trial classiﬁcation of electroencephalography (EEG) focus on two types of paradigms: phase locked methods, in which the amplitude of the signal is used as the feature for classiﬁcation, e.g. event related potentials; and second order methods, in which the feature of interest is the power of the signal, e.g. event related (de)synchronization. The procedure for deciding which paradigm to use is ad hoc and is typically driven by knowledge of the underlying neurophysiology. Here we propose a principled method, based on a bilinear model, in which the algorithm simultaneously learns the best ﬁrst and second order spatial and temporal features for classiﬁcation of EEG. The method is demonstrated on simulated data as well as on EEG taken from a benchmark data used to test classiﬁcation algorithms for brain computer interfaces. 1 1.1</p><p>6 0.4646973 <a title="37-lsi-6" href="./nips-2007-Mining_Internet-Scale_Software_Repositories.html">129 nips-2007-Mining Internet-Scale Software Repositories</a></p>
<p>7 0.3823337 <a title="37-lsi-7" href="./nips-2007-Optimal_models_of_sound_localization_by_barn_owls.html">150 nips-2007-Optimal models of sound localization by barn owls</a></p>
<p>8 0.36116514 <a title="37-lsi-8" href="./nips-2007-A_New_View_of_Automatic_Relevance_Determination.html">8 nips-2007-A New View of Automatic Relevance Determination</a></p>
<p>9 0.34816742 <a title="37-lsi-9" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>10 0.3158673 <a title="37-lsi-10" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>11 0.31109372 <a title="37-lsi-11" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>12 0.3055017 <a title="37-lsi-12" href="./nips-2007-Discriminative_Keyword_Selection_Using_Support_Vector_Machines.html">71 nips-2007-Discriminative Keyword Selection Using Support Vector Machines</a></p>
<p>13 0.28887063 <a title="37-lsi-13" href="./nips-2007-Simulated_Annealing%3A_Rigorous_finite-time_guarantees_for_optimization_on_continuous_domains.html">178 nips-2007-Simulated Annealing: Rigorous finite-time guarantees for optimization on continuous domains</a></p>
<p>14 0.26399627 <a title="37-lsi-14" href="./nips-2007-An_online_Hebbian_learning_rule_that_performs_Independent_Component_Analysis.html">26 nips-2007-An online Hebbian learning rule that performs Independent Component Analysis</a></p>
<p>15 0.26117995 <a title="37-lsi-15" href="./nips-2007-An_Analysis_of_Inference_with_the_Universum.html">24 nips-2007-An Analysis of Inference with the Universum</a></p>
<p>16 0.25332749 <a title="37-lsi-16" href="./nips-2007-Regulator_Discovery_from_Gene_Expression_Time_Series_of_Malaria_Parasites%3A_a_Hierachical_Approach.html">167 nips-2007-Regulator Discovery from Gene Expression Time Series of Malaria Parasites: a Hierachical Approach</a></p>
<p>17 0.2519955 <a title="37-lsi-17" href="./nips-2007-Heterogeneous_Component_Analysis.html">96 nips-2007-Heterogeneous Component Analysis</a></p>
<p>18 0.24856433 <a title="37-lsi-18" href="./nips-2007-On_Sparsity_and_Overcompleteness_in_Image_Models.html">145 nips-2007-On Sparsity and Overcompleteness in Image Models</a></p>
<p>19 0.24543603 <a title="37-lsi-19" href="./nips-2007-Sparse_Overcomplete_Latent_Variable_Decomposition_of_Counts_Data.html">181 nips-2007-Sparse Overcomplete Latent Variable Decomposition of Counts Data</a></p>
<p>20 0.2331876 <a title="37-lsi-20" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.047), (13, 0.034), (16, 0.035), (18, 0.022), (19, 0.081), (21, 0.052), (34, 0.014), (35, 0.015), (47, 0.073), (68, 0.316), (83, 0.113), (85, 0.028), (90, 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76137042 <a title="37-lda-1" href="./nips-2007-Blind_channel_identification_for_speech_dereverberation_using_l1-norm_sparse_learning.html">37 nips-2007-Blind channel identification for speech dereverberation using l1-norm sparse learning</a></p>
<p>Author: Yuanqing Lin, Jingdong Chen, Youngmoo Kim, Daniel D. Lee</p><p>Abstract: Speech dereverberation remains an open problem after more than three decades of research. The most challenging step in speech dereverberation is blind channel identiﬁcation (BCI). Although many BCI approaches have been developed, their performance is still far from satisfactory for practical applications. The main difﬁculty in BCI lies in ﬁnding an appropriate acoustic model, which not only can effectively resolve solution degeneracies due to the lack of knowledge of the source, but also robustly models real acoustic environments. This paper proposes a sparse acoustic room impulse response (RIR) model for BCI, that is, an acoustic RIR can be modeled by a sparse FIR ﬁlter. Under this model, we show how to formulate the BCI of a single-input multiple-output (SIMO) system into a l1 norm regularized least squares (LS) problem, which is convex and can be solved efﬁciently with guaranteed global convergence. The sparseness of solutions is controlled by l1 -norm regularization parameters. We propose a sparse learning scheme that infers the optimal l1 -norm regularization parameters directly from microphone observations under a Bayesian framework. Our results show that the proposed approach is effective and robust, and it yields source estimates in real acoustic environments with high ﬁdelity to anechoic chamber measurements.</p><p>2 0.65469646 <a title="37-lda-2" href="./nips-2007-Regret_Minimization_in_Games_with_Incomplete_Information.html">165 nips-2007-Regret Minimization in Games with Incomplete Information</a></p>
<p>Author: Martin Zinkevich, Michael Johanson, Michael Bowling, Carmelo Piccione</p><p>Abstract: Extensive games are a powerful model of multiagent decision-making scenarios with incomplete information. Finding a Nash equilibrium for very large instances of these games has received a great deal of recent attention. In this paper, we describe a new technique for solving large games based on regret minimization. In particular, we introduce the notion of counterfactual regret, which exploits the degree of incomplete information in an extensive game. We show how minimizing counterfactual regret minimizes overall regret, and therefore in self-play can be used to compute a Nash equilibrium. We demonstrate this technique in the domain of poker, showing we can solve abstractions of limit Texas Hold’em with as many as 1012 states, two orders of magnitude larger than previous methods. 1</p><p>3 0.57699597 <a title="37-lda-3" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>Author: Matthias Bethge, Philipp Berens</p><p>Abstract: Maximum entropy analysis of binary variables provides an elegant way for studying the role of pairwise correlations in neural populations. Unfortunately, these approaches suffer from their poor scalability to high dimensions. In sensory coding, however, high-dimensional data is ubiquitous. Here, we introduce a new approach using a near-maximum entropy model, that makes this type of analysis feasible for very high-dimensional data—the model parameters can be derived in closed form and sampling is easy. Therefore, our NearMaxEnt approach can serve as a tool for testing predictions from a pairwise maximum entropy model not only for low-dimensional marginals, but also for high dimensional measurements of more than thousand units. We demonstrate its usefulness by studying natural images with dichotomized pixel intensities. Our results indicate that the statistics of such higher-dimensional measurements exhibit additional structure that are not predicted by pairwise correlations, despite the fact that pairwise correlations explain the lower-dimensional marginal statistics surprisingly well up to the limit of dimensionality where estimation of the full joint distribution is feasible. 1</p><p>4 0.49412861 <a title="37-lda-4" href="./nips-2007-Second_Order_Bilinear_Discriminant_Analysis_for_single_trial_EEG_analysis.html">173 nips-2007-Second Order Bilinear Discriminant Analysis for single trial EEG analysis</a></p>
<p>Author: Christoforos Christoforou, Paul Sajda, Lucas C. Parra</p><p>Abstract: Traditional analysis methods for single-trial classiﬁcation of electroencephalography (EEG) focus on two types of paradigms: phase locked methods, in which the amplitude of the signal is used as the feature for classiﬁcation, e.g. event related potentials; and second order methods, in which the feature of interest is the power of the signal, e.g. event related (de)synchronization. The procedure for deciding which paradigm to use is ad hoc and is typically driven by knowledge of the underlying neurophysiology. Here we propose a principled method, based on a bilinear model, in which the algorithm simultaneously learns the best ﬁrst and second order spatial and temporal features for classiﬁcation of EEG. The method is demonstrated on simulated data as well as on EEG taken from a benchmark data used to test classiﬁcation algorithms for brain computer interfaces. 1 1.1</p><p>5 0.49331477 <a title="37-lda-5" href="./nips-2007-A_Unified_Near-Optimal_Estimator_For_Dimension_Reduction_in_%24l_%5Calpha%24_%28%240%3C%5Calpha%5Cleq_2%24%29_Using_Stable_Random_Projections.html">13 nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</a></p>
<p>Author: Ping Li, Trevor J. Hastie</p><p>Abstract: Many tasks (e.g., clustering) in machine learning only require the lα distances instead of the original data. For dimension reductions in the lα norm (0 < α ≤ 2), the method of stable random projections can efﬁciently compute the lα distances in massive datasets (e.g., the Web or massive data streams) in one pass of the data. The estimation task for stable random projections has been an interesting topic. We propose a simple estimator based on the fractional power of the samples (projected data), which is surprisingly near-optimal in terms of the asymptotic variance. In fact, it achieves the Cram´ r-Rao bound when α = 2 and α = 0+. This e new result will be useful when applying stable random projections to distancebased clustering, classiﬁcations, kernels, massive data streams etc.</p><p>6 0.48574775 <a title="37-lda-6" href="./nips-2007-Invariant_Common_Spatial_Patterns%3A_Alleviating_Nonstationarities_in_Brain-Computer_Interfacing.html">106 nips-2007-Invariant Common Spatial Patterns: Alleviating Nonstationarities in Brain-Computer Interfacing</a></p>
<p>7 0.46810147 <a title="37-lda-7" href="./nips-2007-EEG-Based_Brain-Computer_Interaction%3A_Improved_Accuracy_by_Automatic_Single-Trial_Error_Detection.html">74 nips-2007-EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection</a></p>
<p>8 0.46725589 <a title="37-lda-8" href="./nips-2007-An_Analysis_of_Inference_with_the_Universum.html">24 nips-2007-An Analysis of Inference with the Universum</a></p>
<p>9 0.46081537 <a title="37-lda-9" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>10 0.45445335 <a title="37-lda-10" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>11 0.4529908 <a title="37-lda-11" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>12 0.45253623 <a title="37-lda-12" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>13 0.45172644 <a title="37-lda-13" href="./nips-2007-Probabilistic_Matrix_Factorization.html">158 nips-2007-Probabilistic Matrix Factorization</a></p>
<p>14 0.45156652 <a title="37-lda-14" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>15 0.45118859 <a title="37-lda-15" href="./nips-2007-Locality_and_low-dimensions_in_the_prediction_of_natural_experience_from_fMRI.html">122 nips-2007-Locality and low-dimensions in the prediction of natural experience from fMRI</a></p>
<p>16 0.45074713 <a title="37-lda-16" href="./nips-2007-Random_Projections_for_Manifold_Learning.html">161 nips-2007-Random Projections for Manifold Learning</a></p>
<p>17 0.45032927 <a title="37-lda-17" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>18 0.44907999 <a title="37-lda-18" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<p>19 0.44868922 <a title="37-lda-19" href="./nips-2007-Selecting_Observations_against_Adversarial_Objectives.html">174 nips-2007-Selecting Observations against Adversarial Objectives</a></p>
<p>20 0.4485099 <a title="37-lda-20" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
