<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>39 nips-2007-Boosting the Area under the ROC Curve</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-39" href="#">nips2007-39</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>39 nips-2007-Boosting the Area under the ROC Curve</h1>
<br/><p>Source: <a title="nips-2007-39-pdf" href="http://papers.nips.cc/paper/3247-boosting-the-area-under-the-roc-curve.pdf">pdf</a></p><p>Author: Phil Long, Rocco Servedio</p><p>Abstract: We show that any weak ranker that can achieve an area under the ROC curve slightly better than 1/2 (which can be achieved by random guessing) can be efﬁciently boosted to achieve an area under the ROC curve arbitrarily close to 1. We further show that this boosting can be performed even in the presence of independent misclassiﬁcation noise, given access to a noise-tolerant weak ranker.</p><p>Reference: <a title="nips-2007-39-reference" href="../nips2007_reference/nips-2007-Boosting_the_Area_under_the_ROC_Curve_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We show that any weak ranker that can achieve an area under the ROC curve slightly better than 1/2 (which can be achieved by random guessing) can be efﬁciently boosted to achieve an area under the ROC curve arbitrarily close to 1. [sent-6, score-1.178]
</p><p>2 We further show that this boosting can be performed even in the presence of independent misclassiﬁcation noise, given access to a noise-tolerant weak ranker. [sent-7, score-0.637]
</p><p>3 This can be formulated as a ranking problem, where the algorithm takes a input a list of examples of members and non-members of the class, and outputs a function that can be used to rank candidates. [sent-10, score-0.396]
</p><p>4 ROC curves [12, 3] are often used to evaluate the quality of a ranking function. [sent-12, score-0.222]
</p><p>5 A point on an ROC curve is obtained by cutting off the ranked list, and checking how many items above the cutoff are members of the target class (“true positives”), and how many are not (“false positives”). [sent-13, score-0.157]
</p><p>6 It is obtained by rescaling the axes so the true positives and false positives vary between 0 and 1, and, as the name implies, examining the area under the resulting curve. [sent-15, score-0.151]
</p><p>7 The AUC measures the ability of a ranker to identify regions in feature space that are unusually densely populated with members of a given class. [sent-16, score-0.471]
</p><p>8 A ranker can succeed according to this criterion even if positive examples are less dense than negative examples everywhere, but, in order to succeed, it must identify where the positive examples tend to be. [sent-17, score-0.757]
</p><p>9 We show that any weak ranker can be boosted to a strong ranker that achieves AUC arbitrarily close to the best possible value of 1. [sent-21, score-1.352]
</p><p>10 We show that in this setting, given a noise-tolerant weak ranker (that achieves nontrivial AUC in the presence of noisy data as described above), we can boost to a strong ranker that achieves AUC at least 1 − ǫ, for any η < 1/2 and any ǫ > 0. [sent-23, score-1.521]
</p><p>11 Freund, Iyer, Schapire and Singer [4] introduced RankBoost, which performs ranking with more ﬁne-grained control over preferences between pairs of items than we consider here. [sent-25, score-0.222]
</p><p>12 They performed an analysis that implies a bound on the AUC of the boosted ranking function in terms of a different measure of the quality of weak rankers. [sent-26, score-0.643]
</p><p>13 Kalai and Servedio [7] showed that, if data is corrupted  with noise at a rate η, it is possible to boost the accuracy of any noise-tolerant weak learner arbitrarily close to 1 − η, and they showed that it is impossible to boost beyond 1 − η. [sent-30, score-0.82]
</p><p>14 In contrast, we show that, in the presence of noise at a rate arbitrarily close to 1/2, the AUC can be boosted arbitrarily close to 1. [sent-31, score-0.352]
</p><p>15 Our noise tolerant boosting algorithm uses as a subroutine the “martingale booster” for classiﬁcation of Long and Servedio [9]. [sent-32, score-0.235]
</p><p>16 The key observation is that a weak ranker can be used to ﬁnd a “two-sided” weak classiﬁer (Lemma 4), which achieves accuracy slightly better than random guessing on both positive and negative examples. [sent-34, score-1.353]
</p><p>17 Two-sided weak classiﬁers can be boosted to obtain accuracy arbitrarily close to 1, also on both the positive examples and the negative examples; a proof of this is implicit in the analysis of [9]. [sent-35, score-0.712]
</p><p>18 Known approaches to noise-tolerant boosting [7, 9] force the weak learner to provide a two-sided weak hypothesis by balancing the distributions that are constructed so that both classes are equally likely. [sent-38, score-0.978]
</p><p>19 (We note that the lower bound from [7] is proved using a construction in which the class probability of positive examples is less than the noise rate; the essence of that proof is to show that in that situation it is impossible to balance the distribution given access to noisy examples. [sent-40, score-0.355]
</p><p>20 ) In contrast, having a weak ranker provides enough leverage to yield a two-sided weak classiﬁer without needing any rebalancing. [sent-41, score-1.135]
</p><p>21 In Section 3, we analyze boosting the AUC when there is no noise in an abstract model where the weak learner is given a distribution and returns a weak ranker, and sampling issues are abstracted away. [sent-44, score-1.058]
</p><p>22 In Section 4, we consider boosting in the presence of noise in a similarly abstract model. [sent-45, score-0.302]
</p><p>23 We say that D is nontrivial (for c) if D assigns nonzero probability to both positive and negative examples. [sent-49, score-0.17]
</p><p>24 We write D+ to denote the marginal distribution over positive examples and D− to denote the marginal distribution over negative examples, so D is a mixture of the distributions D+ and D− . [sent-50, score-0.167]
</p><p>25 As has been previously pointed out, we may view any function h : X → R as a ranking of X. [sent-51, score-0.222]
</p><p>26 Note that if h(x1 ) = h(x2 ) then the ranking does not order x1 relative to x2 . [sent-52, score-0.222]
</p><p>27 Given a ranking function h : X → R, for each value θ ∈ R there is a point (αθ , βθ ) on the ROC curve of h, where αθ is the false positive rate and βθ is the true positive rate of the classiﬁer obtained by thresholding h at θ: αθ = D− [h(x) ≥ θ] and βθ = D+ [h(x) ≥ θ]. [sent-53, score-0.569]
</p><p>28 This motivates the following deﬁnition: Deﬁnition 1 A weak ranker with advantage γ is an algorithm that, given any nontrivial distribution 1 D, returns a function h : X → R that has AUC(h; D) ≥ 2 + γ. [sent-62, score-0.921]
</p><p>29 In the rest of the paper we show how boosting algorithms originally designed for classiﬁcation can be adapted to convert weak rankers into “strong” rankers (that achieve AUC at least 1 − ǫ) in a range of different settings. [sent-63, score-0.732]
</p><p>30 3 From weak to strong AUC The main result of this section is a simple proof that the AUC can be boosted. [sent-64, score-0.396]
</p><p>31 We achieve this in a relatively straightforward way by using the standard AdaBoost algorithm for boosting classiﬁers. [sent-65, score-0.178]
</p><p>32 to a weak ranker which returns ranking functions h1 , h2 , . [sent-69, score-1.029]
</p><p>33 In this model we prove the following: Theorem 2 There is an algorithm AUCBoost that, given access to a weak ranker with advantage γ as an oracle, for any nontrivial distribution D, outputs a ranking function with AUC at least 1 − ǫ. [sent-74, score-1.252]
</p><p>34 The AUCBoost algorithm makes T = O( log(1/ǫ) ) many calls to the weak ranker. [sent-75, score-0.398]
</p><p>35 As can be seen from the observation that it does not depend on the relative frequency of positive and negative examples, the AUC requires a learner to perform well on both positive and negative examples. [sent-77, score-0.306]
</p><p>36 When such a requirement is imposed on a base classiﬁer, it has been called two-sided weak learning. [sent-78, score-0.355]
</p><p>37 The key to boosting the AUC is the observation (Lemma 4 below) that a weak ranker can be used to generate a two-sided weak learner. [sent-79, score-1.286]
</p><p>38 Deﬁnition 3 A γ two-sided weak learner is an algorithm that, given a nontrivial distribution D, 1 outputs a hypothesis h that satisﬁes both Prx∈D+ [h(x) = 1] ≥ 2 + γ and Prx∈D− [h(x) = −1] ≥ 1 2 + γ. [sent-80, score-0.577]
</p><p>39 Lemma 4 Let A be a weak ranking algorithm with advantage γ. [sent-82, score-0.631]
</p><p>40 Then there is a γ/4 two-sided weak learner A′ based on A that always returns classiﬁers with equal error rate on positive and negative examples. [sent-83, score-0.621]
</p><p>41 Proof: Algorithm A′ ﬁrst runs A to get a real-valued ranking function h : X → R. [sent-84, score-0.222]
</p><p>42 Thus, for the classiﬁer obtained by def def thresholding h at θ, the class conditional error rates p+ = D+ [h(x) < θ] and p− = D− [h(x) ≥ θ] γ 1 1 satisfy p+ + p− ≤ 1 − γ. [sent-88, score-0.202]
</p><p>43 We will also need the following simple lemma which shows that a classiﬁer that is good on both the positive and the negative examples, when viewed as a ranking function, achieves a good AUC. [sent-101, score-0.43]
</p><p>44 8  1  false positive rate  Figure 1: The curved line represents the ROC curve for ranking function h. [sent-110, score-0.427]
</p><p>45 16) represents the ROC curve for a ranker h′ which agrees with h on those x for which h(x) ≥ θ but randomly ranks those x for which h(x) < θ. [sent-114, score-0.506]
</p><p>46 In round t, each copy 2 of AdaBoost passes its reweighted distribution Dt to the weak ranker, and then uses the process of Lemma 4 to convert the resulting weak ranking function to a classiﬁer ht with two-sided advantage + γ/4. [sent-121, score-1.112]
</p><p>47 This can be done by sorting the examples using the weak ranking function (in O(m log m) time steps) and processing the examples in the resulting order, keeping running counts of the number of errors of each type. [sent-126, score-0.691]
</p><p>48 4 Boosting weak rankers in the presence of misclassiﬁcation noise The noise model: independent misclassiﬁcation noise. [sent-127, score-0.676]
</p><p>49 In this framework there is a noise rate η < 1/2, and each example (positive or negative) drawn from distribution D has its true label c(x) independently ﬂipped with probability η before it is given to the learner. [sent-129, score-0.155]
</p><p>50 Boosting weak rankers in the presence of independent misclassiﬁcation noise. [sent-131, score-0.508]
</p><p>51 We now show how the AUC can be boosted arbitrarily close to 1 even if the data given to the booster is corrupted with independent misclassiﬁcation noise, using weak rankers that are able to tolerate independent misclassiﬁcation noise. [sent-132, score-0.68]
</p><p>52 v0,3  Figure 2: The branching program produced by the boosting algorithm. [sent-146, score-0.419]
</p><p>53 Each node vi,t is labeled with a weak classiﬁer hi,t ; left edges correspond to -1 and right edges to 1. [sent-147, score-0.505]
</p><p>54 v0,T +1  v1,T +1  output -1  vT −1,T +1 vT,T +1  output 1  As in the previous section we begin by abstracting away sampling issues and using a model in which the booster passes a distribution to a weak ranker. [sent-151, score-0.471]
</p><p>55 Deﬁnition 6 A noise-tolerant weak ranker with advantage γ is an algorithm with the following property: for any noise rate η < 1/2, given a noisy distribution Dη , the algorithm outputs a ranking 1 function h : X → R such that AUC(h; D) ≥ 2 + γ. [sent-153, score-1.256]
</p><p>56 Our algorithm for boosting the AUC in the presence of noise uses the Basic MartiBoost algorithm (see Section 4 of [9]). [sent-154, score-0.302]
</p><p>57 This algorithm boosts any two-sided weak learner to arbitrarily high accuracy and works in a series of rounds. [sent-155, score-0.524]
</p><p>58 It then calls a two-sided weak learner t times using each of D0,t , . [sent-168, score-0.484]
</p><p>59 The output of Basic MartiBoost is a layered branching program deﬁned as follows. [sent-177, score-0.268]
</p><p>60 There is a node vi,t for each round 1 ≤ t ≤ T + 1 and each index 0 ≤ i < t (that is, for each bin constructed during training). [sent-178, score-0.165]
</p><p>61 An item x is routed through the branching program the same way a labeled example (x, y) would have been routed during the training phase: it starts in node v0,1 , and from each node vi,t it goes to vi,t+1 if hi,t (x) = −1, and to vi+1,t+1 otherwise. [sent-179, score-0.622]
</p><p>62 When the item x arrives at a terminal node of the branching program in layer T + 1, it is at some node vj,T +1 . [sent-180, score-0.504]
</p><p>63 The prediction is 1 if j ≥ T /2 and is −1 if j < T /2; in other words, the prediction is according to the majority vote of the weak classiﬁers that were encountered along the path through the branching program that the example followed. [sent-181, score-0.623]
</p><p>64 (The crux of the proof is the observation that positive (respectively, negative) examples are routed through the branching program according to a random walk that is biased to the right (respectively, left); hence the name “martingale boosting. [sent-184, score-0.491]
</p><p>65 Then for T = O(log(1/ǫ)/γ 2), Basic MartiBoost constructs a branching program H such that D+ [H(x) = −1] ≤ ǫ and D− [H(x) = 1] ≤ ǫ. [sent-189, score-0.305]
</p><p>66 Given access to a noise-tolerant weak ranker A with advantage γ, at each node vi,t the Basic MartiRank algorithm runs A and proceeds as described in Lemma 4 to obtain a weak classiﬁer hi,t . [sent-191, score-1.371]
</p><p>67 Basic MartiRank runs Basic MartiBoost with T = O(log(1/ǫ)/γ 2) and simply uses the resulting classiﬁer H as its ranking function. [sent-192, score-0.222]
</p><p>68 The following theorem shows that Basic MartiRank is an effective AUC booster in the presence of independent misclassiﬁcation noise: Theorem 8 Fix any η < 1/2 and any ǫ > 0. [sent-193, score-0.178]
</p><p>69 Given access to Dη and a noise-tolerant weak ranker A with advantage γ, Basic MartiRank outputs a branching program H such that AUC(H; D) ≥ 1 − ǫ. [sent-194, score-1.211]
</p><p>70 Proof: Fix any node vi,t in the branching program. [sent-195, score-0.319]
</p><p>71 The crux of the proof is the following simple observation: for a labeled example (x, y), the route through the branching program that is taken  by (x, y) is determined completely by the predictions of the base classiﬁers, i. [sent-196, score-0.372]
</p><p>72 So each time the noisetolerant weak ranker A is invoked at a node vi,t , it is indeed the case that the distribution that it is given is an independent misclassiﬁcation noise distribution. [sent-204, score-0.982]
</p><p>73 Consequently A does construct weak rankers with AUC at least 1/2 + γ, and the conversion of Lemma 4 yields weak classiﬁers that have advantage γ/4 with respect to the underlying distribution Di,t . [sent-205, score-0.877]
</p><p>74 Given this, Lemma 7 implies that the ﬁnal classiﬁer H has error at most ǫ on both positive and negative examples drawn from the original distribution D, and Lemma 5 then implies that H, viewed a ranker, achieves AUC at least 1 − ǫ. [sent-206, score-0.229]
</p><p>75 In [9], a more complex variant of Basic MartiBoost, called Noise-Tolerant SMartiBoost, is presented and is shown to boost any noise-tolerant weak learning algorithm to any accuracy less than 1 − η in the presence of independent misclassiﬁcation noise. [sent-207, score-0.523]
</p><p>76 Formally, we assume that the weak ranker is given access to an oracle for the noisy distribution Dη . [sent-210, score-0.932]
</p><p>77 We thus now view a noise-tolerant weak ranker with advantage γ as an algorithm A with the following property: for any noise rate η < 1/2, given access to an oracle for Dη , the algorithm outputs a ranking function h : X → R 1 such that AUC(h; D) ≥ 2 + γ. [sent-211, score-1.352]
</p><p>78 We let mA denote the number of examples from each class that sufﬁce for A to construct a ranking function as described above. [sent-212, score-0.309]
</p><p>79 In other words, if A is provided with a sample of draws from Dη such that each class, positive and negative, has at least mA points in the sample with that true label, then algorithm A outputs a γ-advantage weak ranking function. [sent-213, score-0.785]
</p><p>80 (Note that for simplicity we are assuming here that the weak ranker always constructs a weak ranking function with the desired advantage, i. [sent-214, score-1.394]
</p><p>81 We prove that SMartiRank is computationally efﬁcient, has moderate sample complexity, and efﬁciently generates a high-accuracy ﬁnal ranking function with respect to the underlying distribution D. [sent-218, score-0.222]
</p><p>82 The main challenge in [9] is the following: for each node vi,t in the branching program, the boosting algorithm considered there must simulate a balanced version of the induced distribution Di,t which puts equal weight on positive and negative examples. [sent-220, score-0.659]
</p><p>83 The solution in [9] is to “freeze” any such node and simply classify any example that reaches it as negative; the analysis argues that since only a tiny fraction of positive examples reach such nodes, this freezing only mildly degrades the accuracy of the ﬁnal hypothesis. [sent-222, score-0.392]
</p><p>84 In the ranking scenario that we now consider, we do not need to construct balanced distributions, but we do need to obtain a non-negligible number of examples from each class in order to run the weak learner at a given node. [sent-223, score-0.794]
</p><p>85 So as in [9] we still freeze some nodes, but with a twist: we now freeze nodes which have the property that for some class label (positive or negative), only a tiny fraction of examples from D with that class label reach the node. [sent-224, score-0.469]
</p><p>86 With this criterion for freezing we can prove that the ﬁnal classiﬁer constructed has high accuracy both on positive and negative examples, which is what we need to achieve good AUC. [sent-225, score-0.205]
</p><p>87 Given a node vi,t and a bit b ∈ {−1, 1}, let pb denote D[x reaches vi,t and c(x) = b]. [sent-227, score-0.238]
</p><p>88 The i,t SMartiRank algorithm is like Basic MartiBoost but with the following difference: for each node vi,t  and each value b ∈ {−1, 1}, if ǫ · D[c(x) = b] (2) T (T + 1) then the node vi,t is “frozen,” i. [sent-228, score-0.236]
</p><p>89 (If this condition holds for both values of b at a particular node vi,t then the node is frozen and either output value may be used as the label. [sent-231, score-0.346]
</p><p>90 Then for T = O(log(1/ǫ)/γ 2 ), the ﬁnal branching program hypothesis H that SMartiRank constructs will have D+ [H(x) = −1] ≤ ǫ and D− [H(x) = 1] ≤ ǫ. [sent-233, score-0.336]
</p><p>91 Given an unlabeled instance x ∈ X, we say that x freezes at node vi,t if x’s path through the branching program causes it to terminate at a node vi,t with t < T + 1 (i. [sent-235, score-0.598]
</p><p>92 We have D[x freezes and c(x) = 1] = i,t D[x freezes at vi,t and c(x) = 1] ≤  i,t  ǫ·D[c(x)=1] T (T +1)  ≤  ǫ 2  · D[c(x) = 1]. [sent-238, score-0.188]
</p><p>93 To allow for the fact that they are just estimates, it will be more conservative, and freeze when the estimate of ǫ pb is at most 4T (T +1) times the estimate of D[c(x) = b]. [sent-248, score-0.175]
</p><p>94 Now in order to determine whether node vi,t should be frozen, we must compare this estimate with a similarly accurate estimate of pb (arguments similar to those of, e. [sent-251, score-0.178]
</p><p>95 We have pb i,t  =  D[x reaches vi,t ] · D[c(x) = b | x reaches vi,t ] = Dη [x reaches vi,t ] · Di,t [c(x) = b]  =  Dη [x reaches vi,t ] ·  η Di,t [y = b] − η 1 − 2η  . [sent-255, score-0.3]
</p><p>96 ) Thus for each vi,t , we can determine whether to freeze it in the execution of SMartiRank using poly(T, 1/ǫ, 1/p, 1/(1 − 2η)) draws from Dη . [sent-259, score-0.2]
</p><p>97 For each of the nodes that are not frozen, we must run the noise-tolerant weak ranker A using the η distribution Di,t . [sent-260, score-0.808]
</p><p>98 Thus, O(T 2 mA /(ǫp)) many draws from Dη are required in order to run the weak learner A at any particular node. [sent-263, score-0.526]
</p><p>99 Since there are O(T 2 ) many nodes overall, we have that all in all O(T 4 mA /(ǫp)) many draws from Dη are required, in addition to the poly(T, 1/ǫ, 1/p, 1/(1 − 2η)) draws required to identify which nodes to freeze. [sent-264, score-0.226]
</p><p>100 Given access to an oracle for Dη and a noise-tolerant weak ranker A with advantage 1 1 1 γ, the SMartiRank algorithm makes mA · poly( 1 , γ , 1−2η , p ) calls to Dη , and and with probability ǫ 1 − δ outputs a branching program H such that AUC(h; D) ≥ 1 − ǫ. [sent-266, score-1.314]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ranker', 0.425), ('auc', 0.398), ('weak', 0.355), ('ranking', 0.222), ('branching', 0.201), ('smartirank', 0.181), ('martiboost', 0.163), ('boosting', 0.151), ('roc', 0.15), ('martirank', 0.126), ('misclassi', 0.12), ('node', 0.118), ('freeze', 0.115), ('frozen', 0.11), ('prx', 0.094), ('freezes', 0.094), ('er', 0.091), ('booster', 0.086), ('rankers', 0.086), ('learner', 0.086), ('draws', 0.085), ('noise', 0.084), ('classi', 0.083), ('curve', 0.081), ('aucboost', 0.072), ('program', 0.067), ('presence', 0.067), ('boosted', 0.066), ('boost', 0.064), ('access', 0.064), ('lemma', 0.063), ('servedio', 0.063), ('def', 0.062), ('adaboost', 0.062), ('pb', 0.06), ('nontrivial', 0.06), ('oracle', 0.06), ('reaches', 0.06), ('negative', 0.059), ('examples', 0.057), ('advantage', 0.054), ('positive', 0.051), ('ht', 0.049), ('poly', 0.048), ('thresholding', 0.048), ('dt', 0.048), ('round', 0.047), ('cortes', 0.046), ('members', 0.046), ('arbitrarily', 0.046), ('basic', 0.045), ('ers', 0.045), ('outputs', 0.045), ('balanced', 0.044), ('routed', 0.043), ('rate', 0.043), ('calls', 0.043), ('positives', 0.043), ('corrupted', 0.041), ('proof', 0.041), ('mohri', 0.04), ('kalai', 0.038), ('martingale', 0.038), ('tiny', 0.038), ('accuracy', 0.037), ('constructs', 0.037), ('guessing', 0.036), ('everywhere', 0.036), ('achieves', 0.035), ('simulate', 0.035), ('area', 0.035), ('rhs', 0.035), ('labeled', 0.032), ('rudin', 0.031), ('crux', 0.031), ('freezing', 0.031), ('pru', 0.031), ('hypothesis', 0.031), ('fix', 0.031), ('passes', 0.03), ('class', 0.03), ('false', 0.03), ('ma', 0.029), ('rocco', 0.029), ('nodes', 0.028), ('label', 0.028), ('noisy', 0.028), ('schapire', 0.028), ('returns', 0.027), ('least', 0.027), ('additive', 0.027), ('achieve', 0.027), ('rankboost', 0.027), ('ipped', 0.027), ('pr', 0.027), ('freund', 0.026), ('list', 0.026), ('cation', 0.026), ('theorem', 0.025), ('nal', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="39-tfidf-1" href="./nips-2007-Boosting_the_Area_under_the_ROC_Curve.html">39 nips-2007-Boosting the Area under the ROC Curve</a></p>
<p>Author: Phil Long, Rocco Servedio</p><p>Abstract: We show that any weak ranker that can achieve an area under the ROC curve slightly better than 1/2 (which can be achieved by random guessing) can be efﬁciently boosted to achieve an area under the ROC curve arbitrarily close to 1. We further show that this boosting can be performed even in the presence of independent misclassiﬁcation noise, given access to a noise-tolerant weak ranker.</p><p>2 0.18971844 <a title="39-tfidf-2" href="./nips-2007-One-Pass_Boosting.html">147 nips-2007-One-Pass Boosting</a></p>
<p>Author: Zafer Barutcuoglu, Phil Long, Rocco Servedio</p><p>Abstract: This paper studies boosting algorithms that make a single pass over a set of base classiﬁers. We ﬁrst analyze a one-pass algorithm in the setting of boosting with diverse base classiﬁers. Our guarantee is the same as the best proved for any boosting algorithm, but our one-pass algorithm is much faster than previous approaches. We next exhibit a random source of examples for which a “picky” variant of AdaBoost that skips poor base classiﬁers can outperform the standard AdaBoost algorithm, which uses every base classiﬁer, by an exponential factor. Experiments with Reuters and synthetic data show that one-pass boosting can substantially improve on the accuracy of Naive Bayes, and that picky boosting can sometimes lead to a further improvement in accuracy.</p><p>3 0.17670812 <a title="39-tfidf-3" href="./nips-2007-McRank%3A_Learning_to_Rank_Using_Multiple_Classification_and_Gradient_Boosting.html">126 nips-2007-McRank: Learning to Rank Using Multiple Classification and Gradient Boosting</a></p>
<p>Author: Ping Li, Qiang Wu, Christopher J. Burges</p><p>Abstract: We cast the ranking problem as (1) multiple classiﬁcation (“Mc”) (2) multiple ordinal classiﬁcation, which lead to computationally tractable learning algorithms for relevance ranking in Web search. We consider the DCG criterion (discounted cumulative gain), a standard quality measure in information retrieval. Our approach is motivated by the fact that perfect classiﬁcations result in perfect DCG scores and the DCG errors are bounded by classiﬁcation errors. We propose using the Expected Relevance to convert class probabilities into ranking scores. The class probabilities are learned using a gradient boosting tree algorithm. Evaluations on large-scale datasets show that our approach can improve LambdaRank [5] and the regressions-based ranker [6], in terms of the (normalized) DCG scores. An efﬁcient implementation of the boosting tree algorithm is also presented.</p><p>4 0.16222952 <a title="39-tfidf-4" href="./nips-2007-Optimal_ROC_Curve_for_a_Combination_of_Classifiers.html">149 nips-2007-Optimal ROC Curve for a Combination of Classifiers</a></p>
<p>Author: Marco Barreno, Alvaro Cardenas, J. D. Tygar</p><p>Abstract: We present a new analysis for the combination of binary classiﬁers. Our analysis makes use of the Neyman-Pearson lemma as a theoretical basis to analyze combinations of classiﬁers. We give a method for ﬁnding the optimal decision rule for a combination of classiﬁers and prove that it has the optimal ROC curve. We show how our method generalizes and improves previous work on combining classiﬁers and generating ROC curves. 1</p><p>5 0.16203326 <a title="39-tfidf-5" href="./nips-2007-FilterBoost%3A_Regression_and_Classification_on_Large_Datasets.html">90 nips-2007-FilterBoost: Regression and Classification on Large Datasets</a></p>
<p>Author: Joseph K. Bradley, Robert E. Schapire</p><p>Abstract: We study boosting in the ﬁltering setting, where the booster draws examples from an oracle instead of using a ﬁxed training set and so may train efﬁciently on very large datasets. Our algorithm, which is based on a logistic regression technique proposed by Collins, Schapire, & Singer, requires fewer assumptions to achieve bounds equivalent to or better than previous work. Moreover, we give the ﬁrst proof that the algorithm of Collins et al. is a strong PAC learner, albeit within the ﬁltering setting. Our proofs demonstrate the algorithm’s strong theoretical properties for both classiﬁcation and conditional probability estimation, and we validate these results through extensive experiments. Empirically, our algorithm proves more robust to noise and overﬁtting than batch boosters in conditional probability estimation and proves competitive in classiﬁcation. 1</p><p>6 0.14326903 <a title="39-tfidf-6" href="./nips-2007-Regularized_Boost_for_Semi-Supervised_Learning.html">166 nips-2007-Regularized Boost for Semi-Supervised Learning</a></p>
<p>7 0.11510673 <a title="39-tfidf-7" href="./nips-2007-A_General_Boosting_Method_and_its_Application_to_Learning_Ranking_Functions_for_Web_Search.html">6 nips-2007-A General Boosting Method and its Application to Learning Ranking Functions for Web Search</a></p>
<p>8 0.10454079 <a title="39-tfidf-8" href="./nips-2007-Multiple-Instance_Pruning_For_Learning_Efficient_Cascade_Detectors.html">137 nips-2007-Multiple-Instance Pruning For Learning Efficient Cascade Detectors</a></p>
<p>9 0.09108945 <a title="39-tfidf-9" href="./nips-2007-Non-parametric_Modeling_of_Partially_Ranked_Data.html">142 nips-2007-Non-parametric Modeling of Partially Ranked Data</a></p>
<p>10 0.077889055 <a title="39-tfidf-10" href="./nips-2007-COFI_RANK_-_Maximum_Margin_Matrix_Factorization_for_Collaborative_Ranking.html">41 nips-2007-COFI RANK - Maximum Margin Matrix Factorization for Collaborative Ranking</a></p>
<p>11 0.070529453 <a title="39-tfidf-11" href="./nips-2007-Boosting_Algorithms_for_Maximizing_the_Soft_Margin.html">38 nips-2007-Boosting Algorithms for Maximizing the Soft Margin</a></p>
<p>12 0.063092545 <a title="39-tfidf-12" href="./nips-2007-Hidden_Common_Cause_Relations_in_Relational_Learning.html">97 nips-2007-Hidden Common Cause Relations in Relational Learning</a></p>
<p>13 0.061902571 <a title="39-tfidf-13" href="./nips-2007-Semi-Supervised_Multitask_Learning.html">175 nips-2007-Semi-Supervised Multitask Learning</a></p>
<p>14 0.059136894 <a title="39-tfidf-14" href="./nips-2007-On_Ranking_in_Survival_Analysis%3A_Bounds_on_the_Concordance_Index.html">144 nips-2007-On Ranking in Survival Analysis: Bounds on the Concordance Index</a></p>
<p>15 0.055596489 <a title="39-tfidf-15" href="./nips-2007-Learning_Bounds_for_Domain_Adaptation.html">110 nips-2007-Learning Bounds for Domain Adaptation</a></p>
<p>16 0.053733364 <a title="39-tfidf-16" href="./nips-2007-Second_Order_Bilinear_Discriminant_Analysis_for_single_trial_EEG_analysis.html">173 nips-2007-Second Order Bilinear Discriminant Analysis for single trial EEG analysis</a></p>
<p>17 0.053443369 <a title="39-tfidf-17" href="./nips-2007-Anytime_Induction_of_Cost-sensitive_Trees.html">27 nips-2007-Anytime Induction of Cost-sensitive Trees</a></p>
<p>18 0.051972967 <a title="39-tfidf-18" href="./nips-2007-Bayesian_Co-Training.html">32 nips-2007-Bayesian Co-Training</a></p>
<p>19 0.051585771 <a title="39-tfidf-19" href="./nips-2007-Testing_for_Homogeneity_with_Kernel_Fisher_Discriminant_Analysis.html">192 nips-2007-Testing for Homogeneity with Kernel Fisher Discriminant Analysis</a></p>
<p>20 0.046369743 <a title="39-tfidf-20" href="./nips-2007-Mining_Internet-Scale_Software_Repositories.html">129 nips-2007-Mining Internet-Scale Software Repositories</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.169), (1, 0.012), (2, -0.096), (3, 0.109), (4, 0.105), (5, 0.143), (6, 0.2), (7, -0.13), (8, 0.168), (9, -0.178), (10, -0.032), (11, 0.08), (12, 0.039), (13, -0.019), (14, 0.019), (15, -0.057), (16, 0.085), (17, 0.054), (18, -0.142), (19, 0.011), (20, -0.074), (21, 0.156), (22, -0.187), (23, 0.098), (24, -0.039), (25, -0.028), (26, 0.051), (27, 0.018), (28, 0.018), (29, 0.004), (30, 0.006), (31, 0.019), (32, 0.075), (33, 0.095), (34, -0.023), (35, -0.034), (36, 0.016), (37, 0.032), (38, 0.055), (39, 0.01), (40, -0.001), (41, -0.031), (42, 0.004), (43, 0.026), (44, 0.078), (45, -0.048), (46, -0.001), (47, 0.091), (48, -0.024), (49, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96294302 <a title="39-lsi-1" href="./nips-2007-Boosting_the_Area_under_the_ROC_Curve.html">39 nips-2007-Boosting the Area under the ROC Curve</a></p>
<p>Author: Phil Long, Rocco Servedio</p><p>Abstract: We show that any weak ranker that can achieve an area under the ROC curve slightly better than 1/2 (which can be achieved by random guessing) can be efﬁciently boosted to achieve an area under the ROC curve arbitrarily close to 1. We further show that this boosting can be performed even in the presence of independent misclassiﬁcation noise, given access to a noise-tolerant weak ranker.</p><p>2 0.82358772 <a title="39-lsi-2" href="./nips-2007-One-Pass_Boosting.html">147 nips-2007-One-Pass Boosting</a></p>
<p>Author: Zafer Barutcuoglu, Phil Long, Rocco Servedio</p><p>Abstract: This paper studies boosting algorithms that make a single pass over a set of base classiﬁers. We ﬁrst analyze a one-pass algorithm in the setting of boosting with diverse base classiﬁers. Our guarantee is the same as the best proved for any boosting algorithm, but our one-pass algorithm is much faster than previous approaches. We next exhibit a random source of examples for which a “picky” variant of AdaBoost that skips poor base classiﬁers can outperform the standard AdaBoost algorithm, which uses every base classiﬁer, by an exponential factor. Experiments with Reuters and synthetic data show that one-pass boosting can substantially improve on the accuracy of Naive Bayes, and that picky boosting can sometimes lead to a further improvement in accuracy.</p><p>3 0.75410414 <a title="39-lsi-3" href="./nips-2007-Optimal_ROC_Curve_for_a_Combination_of_Classifiers.html">149 nips-2007-Optimal ROC Curve for a Combination of Classifiers</a></p>
<p>Author: Marco Barreno, Alvaro Cardenas, J. D. Tygar</p><p>Abstract: We present a new analysis for the combination of binary classiﬁers. Our analysis makes use of the Neyman-Pearson lemma as a theoretical basis to analyze combinations of classiﬁers. We give a method for ﬁnding the optimal decision rule for a combination of classiﬁers and prove that it has the optimal ROC curve. We show how our method generalizes and improves previous work on combining classiﬁers and generating ROC curves. 1</p><p>4 0.7289862 <a title="39-lsi-4" href="./nips-2007-FilterBoost%3A_Regression_and_Classification_on_Large_Datasets.html">90 nips-2007-FilterBoost: Regression and Classification on Large Datasets</a></p>
<p>Author: Joseph K. Bradley, Robert E. Schapire</p><p>Abstract: We study boosting in the ﬁltering setting, where the booster draws examples from an oracle instead of using a ﬁxed training set and so may train efﬁciently on very large datasets. Our algorithm, which is based on a logistic regression technique proposed by Collins, Schapire, & Singer, requires fewer assumptions to achieve bounds equivalent to or better than previous work. Moreover, we give the ﬁrst proof that the algorithm of Collins et al. is a strong PAC learner, albeit within the ﬁltering setting. Our proofs demonstrate the algorithm’s strong theoretical properties for both classiﬁcation and conditional probability estimation, and we validate these results through extensive experiments. Empirically, our algorithm proves more robust to noise and overﬁtting than batch boosters in conditional probability estimation and proves competitive in classiﬁcation. 1</p><p>5 0.63762671 <a title="39-lsi-5" href="./nips-2007-McRank%3A_Learning_to_Rank_Using_Multiple_Classification_and_Gradient_Boosting.html">126 nips-2007-McRank: Learning to Rank Using Multiple Classification and Gradient Boosting</a></p>
<p>Author: Ping Li, Qiang Wu, Christopher J. Burges</p><p>Abstract: We cast the ranking problem as (1) multiple classiﬁcation (“Mc”) (2) multiple ordinal classiﬁcation, which lead to computationally tractable learning algorithms for relevance ranking in Web search. We consider the DCG criterion (discounted cumulative gain), a standard quality measure in information retrieval. Our approach is motivated by the fact that perfect classiﬁcations result in perfect DCG scores and the DCG errors are bounded by classiﬁcation errors. We propose using the Expected Relevance to convert class probabilities into ranking scores. The class probabilities are learned using a gradient boosting tree algorithm. Evaluations on large-scale datasets show that our approach can improve LambdaRank [5] and the regressions-based ranker [6], in terms of the (normalized) DCG scores. An efﬁcient implementation of the boosting tree algorithm is also presented.</p><p>6 0.60218692 <a title="39-lsi-6" href="./nips-2007-Multiple-Instance_Pruning_For_Learning_Efficient_Cascade_Detectors.html">137 nips-2007-Multiple-Instance Pruning For Learning Efficient Cascade Detectors</a></p>
<p>7 0.55702096 <a title="39-lsi-7" href="./nips-2007-Boosting_Algorithms_for_Maximizing_the_Soft_Margin.html">38 nips-2007-Boosting Algorithms for Maximizing the Soft Margin</a></p>
<p>8 0.55050439 <a title="39-lsi-8" href="./nips-2007-Regularized_Boost_for_Semi-Supervised_Learning.html">166 nips-2007-Regularized Boost for Semi-Supervised Learning</a></p>
<p>9 0.42657363 <a title="39-lsi-9" href="./nips-2007-A_General_Boosting_Method_and_its_Application_to_Learning_Ranking_Functions_for_Web_Search.html">6 nips-2007-A General Boosting Method and its Application to Learning Ranking Functions for Web Search</a></p>
<p>10 0.40380621 <a title="39-lsi-10" href="./nips-2007-COFI_RANK_-_Maximum_Margin_Matrix_Factorization_for_Collaborative_Ranking.html">41 nips-2007-COFI RANK - Maximum Margin Matrix Factorization for Collaborative Ranking</a></p>
<p>11 0.39322937 <a title="39-lsi-11" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>12 0.38900879 <a title="39-lsi-12" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>13 0.38709214 <a title="39-lsi-13" href="./nips-2007-Anytime_Induction_of_Cost-sensitive_Trees.html">27 nips-2007-Anytime Induction of Cost-sensitive Trees</a></p>
<p>14 0.37921721 <a title="39-lsi-14" href="./nips-2007-Non-parametric_Modeling_of_Partially_Ranked_Data.html">142 nips-2007-Non-parametric Modeling of Partially Ranked Data</a></p>
<p>15 0.30038804 <a title="39-lsi-15" href="./nips-2007-Evaluating_Search_Engines_by_Modeling_the_Relationship_Between_Relevance_and_Clicks.html">83 nips-2007-Evaluating Search Engines by Modeling the Relationship Between Relevance and Clicks</a></p>
<p>16 0.28815791 <a title="39-lsi-16" href="./nips-2007-Bayesian_Co-Training.html">32 nips-2007-Bayesian Co-Training</a></p>
<p>17 0.28002852 <a title="39-lsi-17" href="./nips-2007-Subspace-Based_Face_Recognition_in_Analog_VLSI.html">188 nips-2007-Subspace-Based Face Recognition in Analog VLSI</a></p>
<p>18 0.27991122 <a title="39-lsi-18" href="./nips-2007-Hidden_Common_Cause_Relations_in_Relational_Learning.html">97 nips-2007-Hidden Common Cause Relations in Relational Learning</a></p>
<p>19 0.27867585 <a title="39-lsi-19" href="./nips-2007-On_Ranking_in_Survival_Analysis%3A_Bounds_on_the_Concordance_Index.html">144 nips-2007-On Ranking in Survival Analysis: Bounds on the Concordance Index</a></p>
<p>20 0.27678052 <a title="39-lsi-20" href="./nips-2007-Second_Order_Bilinear_Discriminant_Analysis_for_single_trial_EEG_analysis.html">173 nips-2007-Second Order Bilinear Discriminant Analysis for single trial EEG analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.298), (5, 0.036), (13, 0.044), (16, 0.023), (21, 0.042), (31, 0.013), (34, 0.074), (35, 0.034), (46, 0.011), (47, 0.06), (49, 0.034), (83, 0.117), (85, 0.035), (87, 0.016), (88, 0.023), (90, 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74089426 <a title="39-lda-1" href="./nips-2007-Boosting_the_Area_under_the_ROC_Curve.html">39 nips-2007-Boosting the Area under the ROC Curve</a></p>
<p>Author: Phil Long, Rocco Servedio</p><p>Abstract: We show that any weak ranker that can achieve an area under the ROC curve slightly better than 1/2 (which can be achieved by random guessing) can be efﬁciently boosted to achieve an area under the ROC curve arbitrarily close to 1. We further show that this boosting can be performed even in the presence of independent misclassiﬁcation noise, given access to a noise-tolerant weak ranker.</p><p>2 0.71698487 <a title="39-lda-2" href="./nips-2007-EEG-Based_Brain-Computer_Interaction%3A_Improved_Accuracy_by_Automatic_Single-Trial_Error_Detection.html">74 nips-2007-EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection</a></p>
<p>Author: Pierre Ferrez, José Millán</p><p>Abstract: Brain-computer interfaces (BCIs), as any other interaction modality based on physiological signals and body channels (e.g., muscular activity, speech and gestures), are prone to errors in the recognition of subject’s intent. An elegant approach to improve the accuracy of BCIs consists in a veriﬁcation procedure directly based on the presence of error-related potentials (ErrP) in the EEG recorded right after the occurrence of an error. Six healthy volunteer subjects with no prior BCI experience participated in a new human-robot interaction experiment where they were asked to mentally move a cursor towards a target that can be reached within a few steps using motor imagination. This experiment conﬁrms the previously reported presence of a new kind of ErrP. These “Interaction ErrP” exhibit a ﬁrst sharp negative peak followed by a positive peak and a second broader negative peak (∼290, ∼350 and ∼470 ms after the feedback, respectively). But in order to exploit these ErrP we need to detect them in each single trial using a short window following the feedback associated to the response of the classiﬁer embedded in the BCI. We have achieved an average recognition rate of correct and erroneous single trials of 81.8% and 76.2%, respectively. Furthermore, we have achieved an average recognition rate of the subject’s intent while trying to mentally drive the cursor of 73.1%. These results show that it’s possible to simultaneously extract useful information for mental control to operate a brain-actuated device as well as cognitive states such as error potentials to improve the quality of the braincomputer interaction. Finally, using a well-known inverse model (sLORETA), we show that the main focus of activity at the occurrence of the ErrP are, as expected, in the pre-supplementary motor area and in the anterior cingulate cortex. 1</p><p>3 0.68208402 <a title="39-lda-3" href="./nips-2007-SpAM%3A_Sparse_Additive_Models.html">179 nips-2007-SpAM: Sparse Additive Models</a></p>
<p>Author: Han Liu, Larry Wasserman, John D. Lafferty, Pradeep K. Ravikumar</p><p>Abstract: We present a new class of models for high-dimensional nonparametric regression and classiﬁcation called sparse additive models (SpAM). Our methods combine ideas from sparse linear modeling and additive nonparametric regression. We derive a method for ﬁtting the models that is effective even when the number of covariates is larger than the sample size. A statistical analysis of the properties of SpAM is given together with empirical results on synthetic and real data, showing that SpAM can be effective in ﬁtting sparse nonparametric models in high dimensional data. 1</p><p>4 0.49595451 <a title="39-lda-4" href="./nips-2007-Catching_Up_Faster_in_Bayesian_Model_Selection_and_Model_Averaging.html">44 nips-2007-Catching Up Faster in Bayesian Model Selection and Model Averaging</a></p>
<p>Author: Tim V. Erven, Steven D. Rooij, Peter Grünwald</p><p>Abstract: Bayesian model averaging, model selection and their approximations such as BIC are generally statistically consistent, but sometimes achieve slower rates of convergence than other methods such as AIC and leave-one-out cross-validation. On the other hand, these other methods can be inconsistent. We identify the catch-up phenomenon as a novel explanation for the slow convergence of Bayesian methods. Based on this analysis we deﬁne the switch-distribution, a modiﬁcation of the Bayesian model averaging distribution. We prove that in many situations model selection and prediction based on the switch-distribution is both consistent and achieves optimal convergence rates, thereby resolving the AIC-BIC dilemma. The method is practical; we give an efﬁcient algorithm. 1</p><p>5 0.48977703 <a title="39-lda-5" href="./nips-2007-COFI_RANK_-_Maximum_Margin_Matrix_Factorization_for_Collaborative_Ranking.html">41 nips-2007-COFI RANK - Maximum Margin Matrix Factorization for Collaborative Ranking</a></p>
<p>Author: Markus Weimer, Alexandros Karatzoglou, Quoc V. Le, Alex J. Smola</p><p>Abstract: In this paper, we consider collaborative ﬁltering as a ranking problem. We present a method which uses Maximum Margin Matrix Factorization and optimizes ranking instead of rating. We employ structured output prediction to optimize directly for ranking scores. Experimental results show that our method gives very good ranking scores and scales well on collaborative ﬁltering tasks. 1</p><p>6 0.48873484 <a title="39-lda-6" href="./nips-2007-Regulator_Discovery_from_Gene_Expression_Time_Series_of_Malaria_Parasites%3A_a_Hierachical_Approach.html">167 nips-2007-Regulator Discovery from Gene Expression Time Series of Malaria Parasites: a Hierachical Approach</a></p>
<p>7 0.48364046 <a title="39-lda-7" href="./nips-2007-McRank%3A_Learning_to_Rank_Using_Multiple_Classification_and_Gradient_Boosting.html">126 nips-2007-McRank: Learning to Rank Using Multiple Classification and Gradient Boosting</a></p>
<p>8 0.48257756 <a title="39-lda-8" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>9 0.48244381 <a title="39-lda-9" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>10 0.4762046 <a title="39-lda-10" href="./nips-2007-Compressed_Regression.html">53 nips-2007-Compressed Regression</a></p>
<p>11 0.4754265 <a title="39-lda-11" href="./nips-2007-Message_Passing_for_Max-weight_Independent_Set.html">128 nips-2007-Message Passing for Max-weight Independent Set</a></p>
<p>12 0.4753519 <a title="39-lda-12" href="./nips-2007-One-Pass_Boosting.html">147 nips-2007-One-Pass Boosting</a></p>
<p>13 0.4745509 <a title="39-lda-13" href="./nips-2007-Local_Algorithms_for_Approximate_Inference_in_Minor-Excluded_Graphs.html">121 nips-2007-Local Algorithms for Approximate Inference in Minor-Excluded Graphs</a></p>
<p>14 0.47325006 <a title="39-lda-14" href="./nips-2007-Colored_Maximum_Variance_Unfolding.html">49 nips-2007-Colored Maximum Variance Unfolding</a></p>
<p>15 0.47305593 <a title="39-lda-15" href="./nips-2007-On_higher-order_perceptron_algorithms.html">146 nips-2007-On higher-order perceptron algorithms</a></p>
<p>16 0.47290409 <a title="39-lda-16" href="./nips-2007-Fixing_Max-Product%3A_Convergent_Message_Passing_Algorithms_for_MAP_LP-Relaxations.html">92 nips-2007-Fixing Max-Product: Convergent Message Passing Algorithms for MAP LP-Relaxations</a></p>
<p>17 0.47280851 <a title="39-lda-17" href="./nips-2007-New_Outer_Bounds_on_the_Marginal_Polytope.html">141 nips-2007-New Outer Bounds on the Marginal Polytope</a></p>
<p>18 0.47230324 <a title="39-lda-18" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>19 0.47224462 <a title="39-lda-19" href="./nips-2007-Structured_Learning_with_Approximate_Inference.html">187 nips-2007-Structured Learning with Approximate Inference</a></p>
<p>20 0.47134107 <a title="39-lda-20" href="./nips-2007-Multi-Task_Learning_via_Conic_Programming.html">134 nips-2007-Multi-Task Learning via Conic Programming</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
