<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-17" href="#">nips2007-17</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</h1>
<br/><p>Source: <a title="nips-2007-17-pdf" href="http://papers.nips.cc/paper/3299-a-neural-network-implementing-optimal-state-estimation-based-on-dynamic-spike-train-decoding.pdf">pdf</a></p><p>Author: Omer Bobrowski, Ron Meir, Shy Shoham, Yonina Eldar</p><p>Abstract: It is becoming increasingly evident that organisms acting in uncertain dynamical environments often employ exact or approximate Bayesian statistical calculations in order to continuously estimate the environmental state, integrate information from multiple sensory modalities, form predictions and choose actions. What is less clear is how these putative computations are implemented by cortical neural networks. An additional level of complexity is introduced because these networks observe the world through spike trains received from primary sensory afferents, rather than directly. A recent line of research has described mechanisms by which such computations can be implemented using a network of neurons whose activity directly represents a probability distribution across the possible “world states”. Much of this work, however, uses various approximations, which severely restrict the domain of applicability of these implementations. Here we make use of rigorous mathematical results from the theory of continuous time point process ﬁltering, and show how optimal real-time state estimation and prediction may be implemented in a general setting using linear neural networks. We demonstrate the applicability of the approach with several examples, and relate the required network properties to the statistical nature of the environment, thereby quantifying the compatibility of a given network with its environment. 1</p><p>Reference: <a title="nips-2007-17-reference" href="../nips2007_reference/nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A neural network implementing optimal state estimation based on dynamic spike train decoding  Omer Bobrowski1 , Ron Meir1 , Shy Shoham2 and Yonina C. [sent-1, score-0.522]
</p><p>2 An additional level of complexity is introduced because these networks observe the world through spike trains received from primary sensory afferents, rather than directly. [sent-7, score-0.764]
</p><p>3 A recent line of research has described mechanisms by which such computations can be implemented using a network of neurons whose activity directly represents a probability distribution across the possible “world states”. [sent-8, score-0.258]
</p><p>4 Here we make use of rigorous mathematical results from the theory of continuous time point process ﬁltering, and show how optimal real-time state estimation and prediction may be implemented in a general setting using linear neural networks. [sent-10, score-0.398]
</p><p>5 We demonstrate the applicability of the approach with several examples, and relate the required network properties to the statistical nature of the environment, thereby quantifying the compatibility of a given network with its environment. [sent-11, score-0.193]
</p><p>6 1  Introduction  A key requirement of biological or artiﬁcial agents acting in a random dynamical environment is estimating the state of the environment based on noisy observations. [sent-12, score-0.253]
</p><p>7 The present work continues a line of research attempting to provide a probabilistic Bayesian framework for optimal dynamical state estimation by biological neural networks. [sent-15, score-0.251]
</p><p>8 , [8, 9]), the time-varying probability distributions are represented in the neurons’ activity patterns, while the network’s connectivity structure and intrinsic dynamics are responsible for performing the required computation. [sent-18, score-0.196]
</p><p>9 Rao’s networks use linear dynamics and discrete time to approximately compute the log-posterior distributions from noisy continuous inputs (rather than actual spike trains). [sent-19, score-0.402]
</p><p>10 , [4, 11, 13]) where approximations of optimal dynamical estima1  tors from spike-train based inputs are calculated, however, without addressing the question of neural implementation. [sent-23, score-0.21]
</p><p>11 Our approach is formulated within a continuous time point process framework, circumventing many of the difﬁculties encountered in previous work based on discrete time approximations and input smoothing. [sent-24, score-0.174]
</p><p>12 , [3]), we are able to show that a linear system sufﬁces to yield the exact posterior distribution for the state. [sent-27, score-0.174]
</p><p>13 The key element in the approach is switching from posterior distributions to a new set of functions which are simply non-normalized forms of the posterior distribution. [sent-28, score-0.192]
</p><p>14 While posterior distributions generally obey non-linear differential equations, these non-normalized functions obey a linear set of equations, known as the Zakai equations [15]. [sent-29, score-0.436]
</p><p>15 Intriguingly, these linear equations contain the full information required to reconstruct the optimal posterior distribution! [sent-30, score-0.239]
</p><p>16 Since the input observations appear directly as spike trains, no temporal information is lost. [sent-33, score-0.313]
</p><p>17 Inherent differences between the modalities, related to temporal delays and different shapes of tuning curves can be incorporated and quantiﬁed within the formalism. [sent-35, score-0.298]
</p><p>18 One of the ﬁrst papers presenting a mathematically rigorous approach to nonlinear ﬁltering in continuous time based on point process observations was [12], where the exact nonlinear differential equations for the posterior distributions are derived. [sent-37, score-0.65]
</p><p>19 2  A neural network as an optimal ﬁlter  Consider a dynamic environment characterized at time t by a state Xt , belonging to a set of N states, namely Xt ∈ {s1 , s2 , . [sent-39, score-0.341]
</p><p>20 We assume the state dynamics is Markovian with generator matrix Q. [sent-43, score-0.293]
</p><p>21 The matrix Q, [Q]ij = qij , is deﬁned [5] by requiring that for small values of h, Pr[Xt+h = si |Xt = si ] = 1 + qii h + o(h) and Pr[Xt+h = sj |Xt = si ] = qij h + o(h) for j = i. [sent-44, score-0.908]
</p><p>22 This matrix controls the process’ inﬁnitesimal progress according to π(t) = π(t)Q, where πi (t) = Pr[Xt = si ]. [sent-46, score-0.242]
</p><p>23 ˙ The state Xt is not directly observable, but is only sensed through a set of M random state-dependent (k) (k) observation point processes {Nt }M . [sent-47, score-0.177]
</p><p>24 We take each point process Nt to represent the spiking k=1 activity of the k-th sensory cell, and assume these processes to be doubly stochastic Poisson counting processes1 with state-dependent rates λk (Xt ). [sent-48, score-0.908]
</p><p>25 These processes are assumed to be independent, given the current state Xt . [sent-49, score-0.177]
</p><p>26 nonlinear ﬁltering) is to obtain a differential equation for the posterior probabilities (1)  (M )  pi (t) = Pr Xt = si N[0,t] , . [sent-53, score-0.541]
</p><p>27 , N[0,t] , and refer the  We interpret the rate λk as providing the tuning curve for the k-th sensory input. [sent-61, score-0.624]
</p><p>28 In other words, the k-th sensory cell responds with strength λk (si ) when the input state is Xt = si . [sent-62, score-0.919]
</p><p>29 The required differential equations for pi (t) are considerably simpliﬁed, with no loss of information [3], by conN sidering a set of non-normalized ‘probability functions’ ρi (t), such that pi (t) = ρi (t)/ j=1 ρj (t). [sent-63, score-0.37]
</p><p>30 2  Based on the theory presented in Section 4 we obtain N  ρi (t) = ˙  M  Qji ρj (t) + j=1  δ(t − tk ) − 1 ρi (t), n  (λk (si ) − 1)  (2)  n  k=1  where {tk } denote the spiking times of the k-th sensory cell. [sent-65, score-0.577]
</p><p>31 Assuming that the tuning functions λk are unimodal, decreasing in all directions from some maximal value (e. [sent-74, score-0.188]
</p><p>32 , Gaussian or truncated cosine functions), we observe from (2) that the impact of an input spike at time t is strongest on cell i for which λk (si ) is maximal, and decreases signiﬁcantly for cells j for which sj is ‘far’ from si . [sent-76, score-0.738]
</p><p>33 This effect can be modelled using excitatory/inhibitory connections, where neurons representing similar states excite each other, while neurons corresponding to very different states inhibit each other (e. [sent-77, score-0.256]
</p><p>34 (i) The solution of (4) provides the optimal posterior state estimator given the spike train observations, i. [sent-82, score-0.451]
</p><p>35 (ii) The equations are linear even though the equations obeyed by the posterior probabilities pi (t) are nonlinear. [sent-85, score-0.379]
</p><p>36 (iii) The temporal evolution breaks up neatly into an observationindependent term, which can be conceived of as implementing a Bayesian dynamic prior, and an observation-dependent term, which contributes each time a spike occurs. [sent-86, score-0.342]
</p><p>37 First, input processes with strong spiking activity, affect the activity more strongly. [sent-89, score-0.336]
</p><p>38 Second, the k-th input affects most strongly the Figure 1: A graphical depiction of components of ρ(t) corresponding to states with large values the network implementing optimal of the tuning curve λk (si ). [sent-90, score-0.49]
</p><p>39 (v) At this point we assume that the ﬁltering of M spike train inputs. [sent-91, score-0.214]
</p><p>40 Multi-modal inputs A multi-modal scenario may be envisaged as one in which a subset of the sensory inputs arises from one modality (e. [sent-95, score-0.588]
</p><p>41 , visual) while the remaining inputs arise from a different sensory modality (e. [sent-97, score-0.524]
</p><p>42 n  (5)  n  Prediction The framework can easily be extended to prediction, deﬁned as the problem of calculating the future posterior distribution ph (t) = Pr[Xt+h = si |Y0t ]. [sent-103, score-0.294]
</p><p>43 It is easy to show that the i 3  non-normalized probabilities ρh (t) can be calculated using the vector differential equation M  ˜ ρh (t) = (Q − Λ) ρh (t) + ˙  ˜ Λk k=1  δ(t − tk )ρh (t), n  (6)  n  ˜ with the initial condition ρh (0) = ehQ ρ(0), and where Λk = ehQ Λk e−hQ . [sent-104, score-0.224]
</p><p>44 Interestingly, the equations obtained are identical to (4), except that the system parameters are modiﬁed. [sent-105, score-0.185]
</p><p>45 Simpliﬁed equation When the tuning curves of the sensory cells are uniformly distributed Gaussians (e. [sent-106, score-0.759]
</p><p>46 , spatial receptive ﬁelds), namely λk (x) = λmax exp(−(x−k∆x)2 /2σ 2 ), it can be shown M [13] that for small enough ∆x, and a large number of sensory cells, k=1 λk (x) ≈ β for all x, implying that Λ = k Λk ≈ (β − M )I. [sent-108, score-0.432]
</p><p>47 Therefore the matrix Λ has no effect on the solution of (4), except for an exponential attenuation that is applied to all the cells simultaneously. [sent-109, score-0.193]
</p><p>48 Therefore, in cases where the number of sensory cells is large, Λ can be omitted from (4). [sent-110, score-0.5]
</p><p>49 This means that between spike arrivals, the system behaves solely according to the a-priori knowledge about the world, and when a spike arrives, this information is reshaped according to the ﬁring cell’s tuning curve. [sent-111, score-0.748]
</p><p>50 3  Theoretical Implications and Applications  Viewing (4) we note that between spike arrivals, the input has no effect on the system. [sent-112, score-0.318]
</p><p>51 Deﬁning tn as the n-th arrival time of a ˙ spike from any one of the sensors, the solution in the interval (tn , tn+1 ) is ρ(t) = e(t−tn )(Q−Λ) ρ(tn ). [sent-114, score-0.376]
</p><p>52 When a new spike arrives from the k-th sensory neuron at time tn the system is modiﬁed within an inﬁnitesimal window of time as ρi (t+ ) = ρi (t− ) + ρi (t− )(λk (si ) − 1) = ρi (t− )λk (si ). [sent-115, score-0.789]
</p><p>53 n n n n  (7)  Thus, at the exact time of a spike arrival from the k-th sensory cell, the vector ρ is reshaped according to the tuning curve of the input cell that ﬁred this spike. [sent-116, score-1.173]
</p><p>54 First consider a small object moving back and forth on a line, jumping between a set of discrete states, and being observed by a retina with M sensory cells. [sent-120, score-0.459]
</p><p>55 Each world state si describes the particle’s position, and each sensory cell k generates a Poisson spike train with rate λk (Xt ), taken to be a Gaussian λmax exp (−(x − xk )2 /2σ 2 ). [sent-121, score-1.171]
</p><p>56 Figure 2(a) displays the motion of the particle for a speciﬁc choice of matrix Q, and 2(b) presents the spiking activity of 10 position sensitive sensory cells. [sent-122, score-0.643]
</p><p>57 Note that the system is able to distinguish between 25 different states rather well with only 10 sensory cells. [sent-124, score-0.517]
</p><p>58 We add a population of sensory cells that respond differently to different movement directions. [sent-127, score-0.54]
</p><p>59 As can be seen in Figure 2(d)-(f), when for some reason the input of the sensory cells is blocked (and the sensory cells ﬁre spontaneously) the system estimates a movement that continues in the same direction. [sent-129, score-1.221]
</p><p>60 It can be seen that even during periods where sensory input is absent, the general trend is well predicted, even though the estimated uncertainty is increased. [sent-131, score-0.455]
</p><p>61 4  By expanding the state space it is also possible for the system to track multiple objects simultaneously. [sent-132, score-0.183]
</p><p>62 This is done simply by creating a new state space, sij = (s1 , s2 ), where sk denotes the state (location) of the i j i k-th object. [sent-134, score-0.25]
</p><p>63 Each of the 10 sensory cells has a Gaussian tuning curve of width σ = 2 and maximal ﬁring rate λmax = 25. [sent-139, score-0.729]
</p><p>64 The 1 2 network activity in (i) represents Pr Xt = si ∨ Xt = si |Y0t . [sent-144, score-0.592]
</p><p>65 2  Behavior Characterization  The solution of the ﬁltering equations (4) depends on two processes, namely the recurrent dynamics due to the ﬁrst term, and the sensory input arising from the second term. [sent-146, score-0.757]
</p><p>66 Recall that the connectivity matrix Q is essentially the generator matrix of the state transition process, and as such, incorporates prior knowledge about the world dynamics. [sent-147, score-0.406]
</p><p>67 The second term, consisting of the sensory input, contributes to the state estimator update every time a spike occurs. [sent-148, score-0.714]
</p><p>68 Thus, a major question relates to the interplay between the a-priori knowledge embedded in the network through Q and the incoming sensory input. [sent-149, score-0.524]
</p><p>69 , the tuning curves λk ), to the properties of the external world. [sent-152, score-0.259]
</p><p>70 The diagonal term qii is related to the average time spent in state i through E[Ti ] = −1/qii [5], and thus we deﬁne −1 −1 τ (Q) = − q11 + · · · + qN N /N , as a measure of the transition frequency of the process, where small values of τ correspond to a rapidly changing process. [sent-154, score-0.205]
</p><p>71 The entropy of qi is a measure for the state transition irregularity from state i, and we deﬁne H(Q) as the average of this entropy over all states. [sent-157, score-0.332]
</p><p>72 The sensory input inﬂuence on the system is controlled by the tuning curves. [sent-160, score-0.721]
</p><p>73 To simplify the analysis we assume uniformly placed Gaussian tuning curves, λk (x) = λmax exp (−(x − k∆x)2 /2σ 2 ), which can be characterized by two parameters - the maximum value λmax and the width σ. [sent-161, score-0.188]
</p><p>74 Note, however that our model does not require any special constraints on the tuning curves. [sent-162, score-0.188]
</p><p>75 Figure 3 examines the system performance under different world setups. [sent-163, score-0.176]
</p><p>76 The MAP estimator is obtained by selecting the cell with the highest ﬁring activity ρi (t), is optimal under the present setting (leading to the minimal probability of error), and can be easily implemented in a neural network by a Winner-Take-All circuit. [sent-165, score-0.439]
</p><p>77 The choice of the L1 error is justiﬁed in this case since the states {si } represent locations on a line, 5  thereby endowing the state space with a distance measure. [sent-166, score-0.192]
</p><p>78 In ﬁgure 3(a) we can see that as τ (Q) increases, the error diminishes, an expected result, since slower world dynamics are easier to analyze. [sent-167, score-0.173]
</p><p>79 The last issue we examine relates to the behavior of the system when an incorrect Q matrix is used (i. [sent-171, score-0.176]
</p><p>80 It is clear from ﬁgure 3(c) that for low values of M (the number of sensory cells), using the wrong Q matrix increases the error level signiﬁcantly. [sent-174, score-0.476]
</p><p>81 5  0 0  4  100  200  H(Q)  (a) Effect of state rapidity  300  400  500  M  (b) Effect of transition entropy  (c) Effect of misspeciﬁcation  Figure 3: State estimation error for different world dynamics and model misspeciﬁcation. [sent-188, score-0.416]
</p><p>82 In ﬁgure 4 we examine the effect of the tuning curve parameters on the system’s performance. [sent-190, score-0.273]
</p><p>83 Given a ﬁxed number of input cells, if the tuning curves are too narrow (ﬁg. [sent-191, score-0.319]
</p><p>84 On the other hand, if the tuning curves are too wide (ﬁg. [sent-193, score-0.259]
</p><p>85 The effect of different values of λmax is obvious - higher values of λmax lead to more spikes per sensory cell which increases the system’s accuracy. [sent-198, score-0.6]
</p><p>86 , sN } with a generator matrix Q that is being observed via a set of (doubly stochastic) Poisson processes with statedependent rate functions λk (Xt ), k = 1, . [sent-205, score-0.185]
</p><p>87 We denote the joint probability law for the state and observation process by P1 . [sent-210, score-0.17]
</p><p>88 The objective is to derive a differential equation for the posterior probabilities (1). [sent-211, score-0.221]
</p><p>89 More generally, the problem can be phrased as computing E1 [f (Xt )|N0 ], where, in the case of (1), f is a vector function, with components fi (x) = [x = si ]. [sent-215, score-0.242]
</p><p>90 4) that {ρi (t)} obey the stochastic differential equation (SDE) N  dρi (t) =  Qji ρj (t)dt + (λ(si ) − 1)ρi (t)(dNt − dt). [sent-246, score-0.22]
</p><p>91 If at time t, no jump occurred in the counting process Nt , then ρ(t + dt) − ρ(t) ≈ a(t)dt, where dt denotes an inﬁnitesimal time interval. [sent-248, score-0.269]
</p><p>92 n 7  5  Discussion  In this work we have introduced a linear recurrent neural network model capable of exactly implementing Bayesian state estimation and prediction from input spike trains in real time. [sent-259, score-0.647]
</p><p>93 The framework is mathematically rigorous and requires few assumptions, is naturally formulated in continuous time, and is based directly on spike train inputs, thereby sacriﬁcing no temporal resolution. [sent-260, score-0.447]
</p><p>94 The setup is ideally suited to the integration of several sensory modalities, and retains its optimality in this setting as well. [sent-261, score-0.395]
</p><p>95 The linearity of the system renders an analytic solution possible, and a full characterization in terms of a-priori knowledge and online sensory input. [sent-262, score-0.523]
</p><p>96 (i) It is important to ﬁnd a natural mapping between the current abstract neural model and more standard biological neural network models. [sent-264, score-0.167]
</p><p>97 Additionally, the implementation of the estimation network (namely, the variables ρi (t)) using realistic spiking neurons is still open. [sent-266, score-0.22]
</p><p>98 Combining approaches to learning Q and adapting the tuning curves λk in real time will lend further plausibility and robustness to the system. [sent-268, score-0.259]
</p><p>99 (iv) Currently, each world state is represented by a single neuron (a grandmother cell). [sent-270, score-0.203]
</p><p>100 Some applications of stochastic differential equations to optimal nonlinear ﬁltering. [sent-375, score-0.362]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sensory', 0.395), ('xt', 0.305), ('spike', 0.214), ('si', 0.198), ('tuning', 0.188), ('cell', 0.161), ('nt', 0.144), ('modalities', 0.133), ('sec', 0.131), ('differential', 0.125), ('cm', 0.124), ('activity', 0.121), ('ltering', 0.111), ('dnt', 0.108), ('qij', 0.108), ('equations', 0.107), ('cells', 0.105), ('state', 0.105), ('dt', 0.103), ('tn', 0.102), ('tk', 0.099), ('world', 0.098), ('lt', 0.096), ('posterior', 0.096), ('poisson', 0.089), ('spiking', 0.083), ('system', 0.078), ('doubly', 0.076), ('dynamics', 0.075), ('network', 0.075), ('processes', 0.072), ('curves', 0.071), ('qji', 0.071), ('generator', 0.069), ('ti', 0.069), ('pi', 0.069), ('ring', 0.065), ('process', 0.065), ('modality', 0.065), ('object', 0.064), ('inputs', 0.064), ('dynamical', 0.064), ('sn', 0.064), ('neurons', 0.062), ('tracking', 0.061), ('arrival', 0.06), ('nitesimal', 0.06), ('input', 0.06), ('pr', 0.059), ('rigorous', 0.058), ('trains', 0.057), ('counting', 0.055), ('blockade', 0.054), ('ehq', 0.054), ('pouget', 0.054), ('qii', 0.054), ('rapidity', 0.054), ('reshaped', 0.054), ('yonina', 0.054), ('obey', 0.054), ('relates', 0.054), ('nonlinear', 0.053), ('trajectory', 0.05), ('characterization', 0.05), ('rao', 0.05), ('continuous', 0.049), ('arrivals', 0.047), ('misspeci', 0.047), ('zakai', 0.047), ('jump', 0.046), ('neural', 0.046), ('transition', 0.046), ('implementing', 0.046), ('effect', 0.044), ('fi', 0.044), ('max', 0.044), ('matrix', 0.044), ('states', 0.044), ('recurrent', 0.044), ('mathematically', 0.044), ('thereby', 0.043), ('blocked', 0.043), ('sde', 0.043), ('evolution', 0.043), ('environment', 0.042), ('stochastic', 0.041), ('curve', 0.041), ('movement', 0.04), ('beck', 0.04), ('sij', 0.04), ('arising', 0.039), ('mathematical', 0.039), ('temporal', 0.039), ('organisms', 0.038), ('derivation', 0.038), ('entropy', 0.038), ('wrong', 0.037), ('namely', 0.037), ('ee', 0.036), ('optimal', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="17-tfidf-1" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>Author: Omer Bobrowski, Ron Meir, Shy Shoham, Yonina Eldar</p><p>Abstract: It is becoming increasingly evident that organisms acting in uncertain dynamical environments often employ exact or approximate Bayesian statistical calculations in order to continuously estimate the environmental state, integrate information from multiple sensory modalities, form predictions and choose actions. What is less clear is how these putative computations are implemented by cortical neural networks. An additional level of complexity is introduced because these networks observe the world through spike trains received from primary sensory afferents, rather than directly. A recent line of research has described mechanisms by which such computations can be implemented using a network of neurons whose activity directly represents a probability distribution across the possible “world states”. Much of this work, however, uses various approximations, which severely restrict the domain of applicability of these implementations. Here we make use of rigorous mathematical results from the theory of continuous time point process ﬁltering, and show how optimal real-time state estimation and prediction may be implemented in a general setting using linear neural networks. We demonstrate the applicability of the approach with several examples, and relate the required network properties to the statistical nature of the environment, thereby quantifying the compatibility of a given network with its environment. 1</p><p>2 0.22873992 <a title="17-tfidf-2" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>Author: Jonathan W. Pillow, Peter E. Latham</p><p>Abstract: Point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, owing in part to the fact that they do not take into account multiple stages of processing. Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more ﬂexible for ﬁtting neural data. More importantly, it allows us to estimate connectivity patterns among neurons (both observed and unobserved), and may provide insight into how networks process sensory input. We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model’s performance using a simulated example network consisting of two coupled neurons.</p><p>3 0.20031244 <a title="17-tfidf-3" href="./nips-2007-Variational_Inference_for_Diffusion_Processes.html">213 nips-2007-Variational Inference for Diffusion Processes</a></p>
<p>Author: Cédric Archambeau, Manfred Opper, Yuan Shen, Dan Cornford, John S. Shawe-taylor</p><p>Abstract: Diffusion processes are a family of continuous-time continuous-state stochastic processes that are in general only partially observed. The joint estimation of the forcing parameters and the system noise (volatility) in these dynamical systems is a crucial, but non-trivial task, especially when the system is nonlinear and multimodal. We propose a variational treatment of diffusion processes, which allows us to compute type II maximum likelihood estimates of the parameters by simple gradient techniques and which is computationally less demanding than most MCMC approaches. We also show how a cheap estimate of the posterior over the parameters can be constructed based on the variational free energy. 1</p><p>4 0.1924119 <a title="17-tfidf-4" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>Author: Maneesh Sahani, Byron M. Yu, John P. Cunningham, Krishna V. Shenoy</p><p>Abstract: Neural spike trains present challenges to analytical efforts due to their noisy, spiking nature. Many studies of neuroscientiﬁc and neural prosthetic importance rely on a smoothed, denoised estimate of the spike train’s underlying ﬁring rate. Current techniques to ﬁnd time-varying ﬁring rates require ad hoc choices of parameters, offer no conﬁdence intervals on their estimates, and can obscure potentially important single trial variability. We present a new method, based on a Gaussian Process prior, for inferring probabilistically optimal estimates of ﬁring rate functions underlying single or multiple neural spike trains. We test the performance of the method on simulated data and experimentally gathered neural spike trains, and we demonstrate improvements over conventional estimators. 1</p><p>5 0.18587677 <a title="17-tfidf-5" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><p>6 0.17152722 <a title="17-tfidf-6" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>7 0.17011672 <a title="17-tfidf-7" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>8 0.16334063 <a title="17-tfidf-8" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>9 0.16195245 <a title="17-tfidf-9" href="./nips-2007-Continuous_Time_Particle_Filtering_for_fMRI.html">59 nips-2007-Continuous Time Particle Filtering for fMRI</a></p>
<p>10 0.14683871 <a title="17-tfidf-10" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>11 0.14002956 <a title="17-tfidf-11" href="./nips-2007-On_higher-order_perceptron_algorithms.html">146 nips-2007-On higher-order perceptron algorithms</a></p>
<p>12 0.13846695 <a title="17-tfidf-12" href="./nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</a></p>
<p>13 0.13515013 <a title="17-tfidf-13" href="./nips-2007-The_Price_of_Bandit_Information_for_Online_Optimization.html">199 nips-2007-The Price of Bandit Information for Online Optimization</a></p>
<p>14 0.13292795 <a title="17-tfidf-14" href="./nips-2007-Adaptive_Online_Gradient_Descent.html">21 nips-2007-Adaptive Online Gradient Descent</a></p>
<p>15 0.13040024 <a title="17-tfidf-15" href="./nips-2007-Optimistic_Linear_Programming_gives_Logarithmic_Regret_for_Irreducible_MDPs.html">151 nips-2007-Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs</a></p>
<p>16 0.12369066 <a title="17-tfidf-16" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>17 0.1213284 <a title="17-tfidf-17" href="./nips-2007-Fitted_Q-iteration_in_continuous_action-space_MDPs.html">91 nips-2007-Fitted Q-iteration in continuous action-space MDPs</a></p>
<p>18 0.11527874 <a title="17-tfidf-18" href="./nips-2007-Collective_Inference_on_Markov_Models_for_Modeling_Bird_Migration.html">48 nips-2007-Collective Inference on Markov Models for Modeling Bird Migration</a></p>
<p>19 0.11172944 <a title="17-tfidf-19" href="./nips-2007-A_Bayesian_Model_of_Conditioned_Perception.html">3 nips-2007-A Bayesian Model of Conditioned Perception</a></p>
<p>20 0.10645666 <a title="17-tfidf-20" href="./nips-2007-Learning_to_classify_complex_patterns_using_a_VLSI_network_of_spiking_neurons.html">117 nips-2007-Learning to classify complex patterns using a VLSI network of spiking neurons</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.318), (1, -0.004), (2, 0.342), (3, 0.036), (4, 0.044), (5, -0.07), (6, 0.011), (7, -0.039), (8, -0.093), (9, -0.178), (10, 0.047), (11, 0.152), (12, -0.053), (13, 0.079), (14, -0.024), (15, 0.081), (16, -0.028), (17, 0.045), (18, 0.135), (19, 0.018), (20, 0.028), (21, 0.011), (22, 0.008), (23, 0.031), (24, 0.01), (25, -0.118), (26, 0.014), (27, 0.001), (28, -0.022), (29, -0.005), (30, 0.083), (31, 0.032), (32, 0.052), (33, 0.027), (34, -0.018), (35, 0.03), (36, 0.056), (37, -0.052), (38, 0.042), (39, -0.017), (40, 0.016), (41, 0.035), (42, -0.052), (43, 0.045), (44, 0.042), (45, 0.02), (46, -0.105), (47, -0.078), (48, 0.046), (49, -0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97340912 <a title="17-lsi-1" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>Author: Omer Bobrowski, Ron Meir, Shy Shoham, Yonina Eldar</p><p>Abstract: It is becoming increasingly evident that organisms acting in uncertain dynamical environments often employ exact or approximate Bayesian statistical calculations in order to continuously estimate the environmental state, integrate information from multiple sensory modalities, form predictions and choose actions. What is less clear is how these putative computations are implemented by cortical neural networks. An additional level of complexity is introduced because these networks observe the world through spike trains received from primary sensory afferents, rather than directly. A recent line of research has described mechanisms by which such computations can be implemented using a network of neurons whose activity directly represents a probability distribution across the possible “world states”. Much of this work, however, uses various approximations, which severely restrict the domain of applicability of these implementations. Here we make use of rigorous mathematical results from the theory of continuous time point process ﬁltering, and show how optimal real-time state estimation and prediction may be implemented in a general setting using linear neural networks. We demonstrate the applicability of the approach with several examples, and relate the required network properties to the statistical nature of the environment, thereby quantifying the compatibility of a given network with its environment. 1</p><p>2 0.67983341 <a title="17-lsi-2" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>Author: Jonathan W. Pillow, Peter E. Latham</p><p>Abstract: Point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, owing in part to the fact that they do not take into account multiple stages of processing. Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more ﬂexible for ﬁtting neural data. More importantly, it allows us to estimate connectivity patterns among neurons (both observed and unobserved), and may provide insight into how networks process sensory input. We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model’s performance using a simulated example network consisting of two coupled neurons.</p><p>3 0.66412765 <a title="17-lsi-3" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><p>4 0.65461332 <a title="17-lsi-4" href="./nips-2007-Variational_Inference_for_Diffusion_Processes.html">213 nips-2007-Variational Inference for Diffusion Processes</a></p>
<p>Author: Cédric Archambeau, Manfred Opper, Yuan Shen, Dan Cornford, John S. Shawe-taylor</p><p>Abstract: Diffusion processes are a family of continuous-time continuous-state stochastic processes that are in general only partially observed. The joint estimation of the forcing parameters and the system noise (volatility) in these dynamical systems is a crucial, but non-trivial task, especially when the system is nonlinear and multimodal. We propose a variational treatment of diffusion processes, which allows us to compute type II maximum likelihood estimates of the parameters by simple gradient techniques and which is computationally less demanding than most MCMC approaches. We also show how a cheap estimate of the posterior over the parameters can be constructed based on the variational free energy. 1</p><p>5 0.64580798 <a title="17-lsi-5" href="./nips-2007-Continuous_Time_Particle_Filtering_for_fMRI.html">59 nips-2007-Continuous Time Particle Filtering for fMRI</a></p>
<p>Author: Lawrence Murray, Amos J. Storkey</p><p>Abstract: We construct a biologically motivated stochastic differential model of the neural and hemodynamic activity underlying the observed Blood Oxygen Level Dependent (BOLD) signal in Functional Magnetic Resonance Imaging (fMRI). The model poses a difﬁcult parameter estimation problem, both theoretically due to the nonlinearity and divergence of the differential system, and computationally due to its time and space complexity. We adapt a particle ﬁlter and smoother to the task, and discuss some of the practical approaches used to tackle the difﬁculties, including use of sparse matrices and parallelisation. Results demonstrate the tractability of the approach in its application to an effective connectivity study. 1</p><p>6 0.63039607 <a title="17-lsi-6" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>7 0.62263471 <a title="17-lsi-7" href="./nips-2007-Collective_Inference_on_Markov_Models_for_Modeling_Bird_Migration.html">48 nips-2007-Collective Inference on Markov Models for Modeling Bird Migration</a></p>
<p>8 0.60249621 <a title="17-lsi-8" href="./nips-2007-Variational_inference_for_Markov_jump_processes.html">214 nips-2007-Variational inference for Markov jump processes</a></p>
<p>9 0.56331515 <a title="17-lsi-9" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>10 0.55494845 <a title="17-lsi-10" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>11 0.55083996 <a title="17-lsi-11" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>12 0.55060506 <a title="17-lsi-12" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>13 0.50295717 <a title="17-lsi-13" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>14 0.50160021 <a title="17-lsi-14" href="./nips-2007-Scan_Strategies_for_Meteorological_Radars.html">171 nips-2007-Scan Strategies for Meteorological Radars</a></p>
<p>15 0.49226493 <a title="17-lsi-15" href="./nips-2007-The_rat_as_particle_filter.html">203 nips-2007-The rat as particle filter</a></p>
<p>16 0.48843467 <a title="17-lsi-16" href="./nips-2007-Augmented_Functional_Time_Series_Representation_and_Forecasting_with_Gaussian_Processes.html">28 nips-2007-Augmented Functional Time Series Representation and Forecasting with Gaussian Processes</a></p>
<p>17 0.47306973 <a title="17-lsi-17" href="./nips-2007-Contraction_Properties_of_VLSI_Cooperative_Competitive_Neural_Networks_of_Spiking_Neurons.html">60 nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</a></p>
<p>18 0.46803445 <a title="17-lsi-18" href="./nips-2007-Modeling_Natural_Sounds_with_Modulation_Cascade_Processes.html">130 nips-2007-Modeling Natural Sounds with Modulation Cascade Processes</a></p>
<p>19 0.46780288 <a title="17-lsi-19" href="./nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</a></p>
<p>20 0.46683359 <a title="17-lsi-20" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.034), (13, 0.038), (16, 0.081), (18, 0.023), (19, 0.016), (21, 0.068), (31, 0.019), (34, 0.394), (35, 0.018), (47, 0.07), (49, 0.016), (83, 0.087), (85, 0.012), (87, 0.011), (90, 0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86653644 <a title="17-lda-1" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>Author: Omer Bobrowski, Ron Meir, Shy Shoham, Yonina Eldar</p><p>Abstract: It is becoming increasingly evident that organisms acting in uncertain dynamical environments often employ exact or approximate Bayesian statistical calculations in order to continuously estimate the environmental state, integrate information from multiple sensory modalities, form predictions and choose actions. What is less clear is how these putative computations are implemented by cortical neural networks. An additional level of complexity is introduced because these networks observe the world through spike trains received from primary sensory afferents, rather than directly. A recent line of research has described mechanisms by which such computations can be implemented using a network of neurons whose activity directly represents a probability distribution across the possible “world states”. Much of this work, however, uses various approximations, which severely restrict the domain of applicability of these implementations. Here we make use of rigorous mathematical results from the theory of continuous time point process ﬁltering, and show how optimal real-time state estimation and prediction may be implemented in a general setting using linear neural networks. We demonstrate the applicability of the approach with several examples, and relate the required network properties to the statistical nature of the environment, thereby quantifying the compatibility of a given network with its environment. 1</p><p>2 0.80260968 <a title="17-lda-2" href="./nips-2007-McRank%3A_Learning_to_Rank_Using_Multiple_Classification_and_Gradient_Boosting.html">126 nips-2007-McRank: Learning to Rank Using Multiple Classification and Gradient Boosting</a></p>
<p>Author: Ping Li, Qiang Wu, Christopher J. Burges</p><p>Abstract: We cast the ranking problem as (1) multiple classiﬁcation (“Mc”) (2) multiple ordinal classiﬁcation, which lead to computationally tractable learning algorithms for relevance ranking in Web search. We consider the DCG criterion (discounted cumulative gain), a standard quality measure in information retrieval. Our approach is motivated by the fact that perfect classiﬁcations result in perfect DCG scores and the DCG errors are bounded by classiﬁcation errors. We propose using the Expected Relevance to convert class probabilities into ranking scores. The class probabilities are learned using a gradient boosting tree algorithm. Evaluations on large-scale datasets show that our approach can improve LambdaRank [5] and the regressions-based ranker [6], in terms of the (normalized) DCG scores. An efﬁcient implementation of the boosting tree algorithm is also presented.</p><p>3 0.79315668 <a title="17-lda-3" href="./nips-2007-Local_Algorithms_for_Approximate_Inference_in_Minor-Excluded_Graphs.html">121 nips-2007-Local Algorithms for Approximate Inference in Minor-Excluded Graphs</a></p>
<p>Author: Kyomin Jung, Devavrat Shah</p><p>Abstract: We present a new local approximation algorithm for computing MAP and logpartition function for arbitrary exponential family distribution represented by a ﬁnite-valued pair-wise Markov random ﬁeld (MRF), say G. Our algorithm is based on decomposing G into appropriately chosen small components; computing estimates locally in each of these components and then producing a good global solution. We prove that the algorithm can provide approximate solution within arbitrary accuracy when G excludes some ﬁnite sized graph as its minor and G has bounded degree: all Planar graphs with bounded degree are examples of such graphs. The running time of the algorithm is Θ(n) (n is the number of nodes in G), with constant dependent on accuracy, degree of graph and size of the graph that is excluded as a minor (constant for Planar graphs). Our algorithm for minor-excluded graphs uses the decomposition scheme of Klein, Plotkin and Rao (1993). In general, our algorithm works with any decomposition scheme and provides quantiﬁable approximation guarantee that depends on the decomposition scheme.</p><p>4 0.76891506 <a title="17-lda-4" href="./nips-2007-Catching_Up_Faster_in_Bayesian_Model_Selection_and_Model_Averaging.html">44 nips-2007-Catching Up Faster in Bayesian Model Selection and Model Averaging</a></p>
<p>Author: Tim V. Erven, Steven D. Rooij, Peter Grünwald</p><p>Abstract: Bayesian model averaging, model selection and their approximations such as BIC are generally statistically consistent, but sometimes achieve slower rates of convergence than other methods such as AIC and leave-one-out cross-validation. On the other hand, these other methods can be inconsistent. We identify the catch-up phenomenon as a novel explanation for the slow convergence of Bayesian methods. Based on this analysis we deﬁne the switch-distribution, a modiﬁcation of the Bayesian model averaging distribution. We prove that in many situations model selection and prediction based on the switch-distribution is both consistent and achieves optimal convergence rates, thereby resolving the AIC-BIC dilemma. The method is practical; we give an efﬁcient algorithm. 1</p><p>5 0.51746899 <a title="17-lda-5" href="./nips-2007-Evaluating_Search_Engines_by_Modeling_the_Relationship_Between_Relevance_and_Clicks.html">83 nips-2007-Evaluating Search Engines by Modeling the Relationship Between Relevance and Clicks</a></p>
<p>Author: Ben Carterette, Rosie Jones</p><p>Abstract: We propose a model that leverages the millions of clicks received by web search engines to predict document relevance. This allows the comparison of ranking functions when clicks are available but complete relevance judgments are not. After an initial training phase using a set of relevance judgments paired with click data, we show that our model can predict the relevance score of documents that have not been judged. These predictions can be used to evaluate the performance of a search engine, using our novel formalization of the conﬁdence of the standard evaluation metric discounted cumulative gain (DCG), so comparisons can be made across time and datasets. This contrasts with previous methods which can provide only pair-wise relevance judgments between results shown for the same query. When no relevance judgments are available, we can identify the better of two ranked lists up to 82% of the time, and with only two relevance judgments for each query, we can identify the better ranking up to 94% of the time. While our experiments are on sponsored search results, which is the ﬁnancial backbone of web search, our method is general enough to be applicable to algorithmic web search results as well. Furthermore, we give an algorithm to guide the selection of additional documents to judge to improve conﬁdence. 1</p><p>6 0.50264126 <a title="17-lda-6" href="./nips-2007-Fixing_Max-Product%3A_Convergent_Message_Passing_Algorithms_for_MAP_LP-Relaxations.html">92 nips-2007-Fixing Max-Product: Convergent Message Passing Algorithms for MAP LP-Relaxations</a></p>
<p>7 0.49951777 <a title="17-lda-7" href="./nips-2007-Loop_Series_and_Bethe_Variational_Bounds_in_Attractive_Graphical_Models.html">123 nips-2007-Loop Series and Bethe Variational Bounds in Attractive Graphical Models</a></p>
<p>8 0.47126448 <a title="17-lda-8" href="./nips-2007-COFI_RANK_-_Maximum_Margin_Matrix_Factorization_for_Collaborative_Ranking.html">41 nips-2007-COFI RANK - Maximum Margin Matrix Factorization for Collaborative Ranking</a></p>
<p>9 0.46944827 <a title="17-lda-9" href="./nips-2007-Message_Passing_for_Max-weight_Independent_Set.html">128 nips-2007-Message Passing for Max-weight Independent Set</a></p>
<p>10 0.46798494 <a title="17-lda-10" href="./nips-2007-A_General_Boosting_Method_and_its_Application_to_Learning_Ranking_Functions_for_Web_Search.html">6 nips-2007-A General Boosting Method and its Application to Learning Ranking Functions for Web Search</a></p>
<p>11 0.46455222 <a title="17-lda-11" href="./nips-2007-Scan_Strategies_for_Meteorological_Radars.html">171 nips-2007-Scan Strategies for Meteorological Radars</a></p>
<p>12 0.46388006 <a title="17-lda-12" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>13 0.46284419 <a title="17-lda-13" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>14 0.45847467 <a title="17-lda-14" href="./nips-2007-On_higher-order_perceptron_algorithms.html">146 nips-2007-On higher-order perceptron algorithms</a></p>
<p>15 0.45414361 <a title="17-lda-15" href="./nips-2007-New_Outer_Bounds_on_the_Marginal_Polytope.html">141 nips-2007-New Outer Bounds on the Marginal Polytope</a></p>
<p>16 0.44554818 <a title="17-lda-16" href="./nips-2007-Boosting_the_Area_under_the_ROC_Curve.html">39 nips-2007-Boosting the Area under the ROC Curve</a></p>
<p>17 0.44493496 <a title="17-lda-17" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>18 0.44480744 <a title="17-lda-18" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>19 0.44336423 <a title="17-lda-19" href="./nips-2007-Optimal_ROC_Curve_for_a_Combination_of_Classifiers.html">149 nips-2007-Optimal ROC Curve for a Combination of Classifiers</a></p>
<p>20 0.44316652 <a title="17-lda-20" href="./nips-2007-Structured_Learning_with_Approximate_Inference.html">187 nips-2007-Structured Learning with Approximate Inference</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
