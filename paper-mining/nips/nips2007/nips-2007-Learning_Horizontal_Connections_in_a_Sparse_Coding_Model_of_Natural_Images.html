<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-111" href="#">nips2007-111</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</h1>
<br/><p>Source: <a title="nips-2007-111-pdf" href="http://papers.nips.cc/paper/3237-learning-horizontal-connections-in-a-sparse-coding-model-of-natural-images.pdf">pdf</a></p><p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: It has been shown that adapting a dictionary of basis functions to the statistics of natural images so as to maximize sparsity in the coefﬁcients results in a set of dictionary elements whose spatial properties resemble those of V1 (primary visual cortex) receptive ﬁelds. However, the resulting sparse coefﬁcients still exhibit pronounced statistical dependencies, thus violating the independence assumption of the sparse coding model. Here, we propose a model that attempts to capture the dependencies among the basis function coefﬁcients by including a pairwise coupling term in the prior over the coefﬁcient activity states. When adapted to the statistics of natural images, the coupling terms learn a combination of facilitatory and inhibitory interactions among neighboring basis functions. These learned interactions may offer an explanation for the function of horizontal connections in V1 in terms of a prior over natural images.</p><p>Reference: <a title="nips-2007-111-reference" href="../nips2007_reference/nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract It has been shown that adapting a dictionary of basis functions to the statistics of natural images so as to maximize sparsity in the coefﬁcients results in a set of dictionary elements whose spatial properties resemble those of V1 (primary visual cortex) receptive ﬁelds. [sent-9, score-0.747]
</p><p>2 However, the resulting sparse coefﬁcients still exhibit pronounced statistical dependencies, thus violating the independence assumption of the sparse coding model. [sent-10, score-0.607]
</p><p>3 Here, we propose a model that attempts to capture the dependencies among the basis function coefﬁcients by including a pairwise coupling term in the prior over the coefﬁcient activity states. [sent-11, score-0.489]
</p><p>4 When adapted to the statistics of natural images, the coupling terms learn a combination of facilitatory and inhibitory interactions among neighboring basis functions. [sent-12, score-0.499]
</p><p>5 These learned interactions may offer an explanation for the function of horizontal connections in V1 in terms of a prior over natural images. [sent-13, score-0.506]
</p><p>6 1 Introduction Over the last decade, mathematical explorations into the statistics of natural scenes have led to the observation that these scenes, as complex and varied as they appear, have an underlying structure that is sparse [1]. [sent-14, score-0.301]
</p><p>7 That is, one can learn a possibly overcomplete basis set such that only a small fraction of the basis functions is necessary to describe a given image, where the operation to infer this sparse representation is non-linear. [sent-15, score-0.818]
</p><p>8 Exploiting this structure has led to advances in our understanding of how information is represented in the visual cortex, since the learned basis set is a collection of oriented, Gabor-like ﬁlters that resemble the receptive ﬁelds in primary visual cortex (V1). [sent-17, score-0.61]
</p><p>9 The approach of using sparse coding to infer sparse representations of unlabeled data is useful for classiﬁcation as shown in the framework of self-taught learning [2]. [sent-18, score-0.702]
</p><p>10 An assumption of the sparse coding model is that the coefﬁcients of the representation are independent. [sent-20, score-0.453]
</p><p>11 This has been shown and modeled in the early work of [3], in the case of the responses of model complex cells [4], feedforward responses of wavelet coefﬁcients [5, 6, 7] or basis functions learned using independent component analysis [8, 9]. [sent-23, score-0.429]
</p><p>12 We develop here a generative model of image patches that does not make the independence assumption. [sent-25, score-0.203]
</p><p>13 The prior over the coefﬁcients is a mixture of a Gaussian when the corresponding basis 1  function is active, and a delta function centered at zero when it is silent as in [10]. [sent-26, score-0.296]
</p><p>14 We model the binary variables or “spins” that control the activation of the basis functions with an Ising model, whose coupling weights model the dependencies among the coefﬁcients. [sent-27, score-0.618]
</p><p>15 The representations inferred by this model are also “hard-sparse”, which is a desirable feature [2]. [sent-28, score-0.179]
</p><p>16 Our model is motivated in part by the architecture of the visual cortex, namely the extensive network of horizontal connections among neurons in V1 [11]. [sent-29, score-0.424]
</p><p>17 It has been hypothesized that they facilitate contour integration [12] and are involved in computing border ownership [13]. [sent-30, score-0.175]
</p><p>18 In both of these models the connections are set a priori based on geometrical properties of the receptive ﬁelds. [sent-31, score-0.26]
</p><p>19 We hope with our model to gain insight into the the computations performed by this extensive collateral system and compare our ﬁndings to known physiological properties of these horizontal connections. [sent-33, score-0.231]
</p><p>20 Furthermore, a recent trend in neuroscience is to model networks of neurons using Ising models, and it has been shown to predict remarkably well the statistics of groups of neurons in the retina [14]. [sent-34, score-0.191]
</p><p>21 2 A non-factorial sparse coding model Let x ∈ Rn be an image patch, where the xi ’s are the pixel values. [sent-36, score-0.514]
</p><p>22 We propose the following generative model: m  x = Φa + ν =  ai ϕi + ν, i=1  where Φ = [ϕ1 . [sent-37, score-0.179]
</p><p>23 ϕm ] ∈ Rn×m is an overcomplete transform or basis set, and the columns ϕi +1 are its basis functions. [sent-40, score-0.52]
</p><p>24 Each coefﬁcient ai = si2 ui is a Gaussian scale mixture (GSM). [sent-42, score-0.179]
</p><p>25 We model the multiplier s with an Ising model, i. [sent-43, score-0.139]
</p><p>26 If the spin si is down (si = −1), then ai = 0 and the basis function ϕi is silent. [sent-46, score-0.802]
</p><p>27 If the spin si is up (si = 1), then the basis function is active and the analog value of the coefﬁcient ai is drawn from a 2 Gaussian distribution with ui ∼ N (0, σi ). [sent-47, score-0.802]
</p><p>28 It bears similarities to [15], which however does not have the intermediate layer a and is not a sparse coding model. [sent-51, score-0.402]
</p><p>29 W1m s1  s2  a1  sm  a2  W2m am  Φ  x1  x2  xn  Figure 1: Proposed graphical model 2 The parameters of the model to be learned from data are θ = (Φ, (σi )i=1. [sent-53, score-0.151]
</p><p>30 A local magnetic ﬁeld bi < 0 favors the spin si to be down, which in turn makes the basis function ϕi mostly silent. [sent-59, score-0.696]
</p><p>31 1 Coefﬁcient estimation We describe here how to infer the representation a of an image patch x in our model. [sent-61, score-0.171]
</p><p>32 Indeed, a GSM model reduces to a linear-Gaussian model conditioned on the multiplier s, and therefore the estimation of a is easy once s is known. [sent-64, score-0.19]
</p><p>33 Given s = s, let Γ = {i : si = 1} be the set of active basis functions. [sent-65, score-0.53]
</p><p>34 Given s, x has a Gaussian ˆ 2 distribution N (0, Σ), where Σ = ǫ2 In + i : si =1 σi ϕi ϕT . [sent-73, score-0.291]
</p><p>35 In the Gibbs sampling procedure, the probability that node i changes its value from si to si given x, all the ¯ other nodes s¬i and at temperature T is given by Ex (s) =  p(si → si |s¬i , x) = ¯  1 + exp −  ∆Ex T  −1  ,  where ∆Ex = Ex (si , s¬i ) − Ex (si , s¬i ). [sent-76, score-0.873]
</p><p>36 Therefore, to compute ∆Ex we can take advantage ¯ i of the Sherman-Morrison formula ¯ Σ−1 = Σ−1 − αΣ−1 ϕi (1 + αϕT Σ−1 ϕi )−1 ϕT Σ−1 (1) i i and of a similar formula for the log det term ¯ log det Σ = log det Σ + log 1 + αϕT Σ−1 ϕi . [sent-82, score-0.444]
</p><p>37 i  (2)  Using (1) and (2) ∆Ex can be written as T  ∆Ex =  −1    2  1 1 α(x Σ ϕi ) − log 1 + αϕT Σ−1 ϕi + (si − si )  ¯ i 2 1 + αϕT Σ−1 ϕi 2 i    j=i  Wij sj + bi  . [sent-83, score-0.438]
</p><p>38 In the ﬁrst phase, we infer the coefﬁcients in the usual sparse coding model where the prior over a is factorial, i. [sent-103, score-0.551]
</p><p>39 (4)  i  With S(ai ) = |ai |, (4) is known as basis pursuit denoising (BPDN) whose solution has been shown to be such that many coefﬁcient of a are exactly zero [17]. [sent-107, score-0.309]
</p><p>40 This allows us to recover the sparsity ˆ pattern s, where si = 2. [sent-108, score-0.433]
</p><p>41 After several iterations of coordinate ascent and convergence of θ using the above approximation, we enter the second phase of the algorithm and reﬁne θ by using the GSM inference described in Section 3. [sent-112, score-0.206]
</p><p>42 Note that LΦ is the same objective function as in the standard sparse coding problem when the coefﬁcients a are ﬁxed. [sent-121, score-0.402]
</p><p>43 2  We add the constraint that ϕi 2 ≤ 1 to avoid the spurious solution where the norm of the basis functions grows and the coefﬁcients tend to 0. [sent-124, score-0.291]
</p><p>44 The problem of estimating σi is a standard variance estimation problem for a 0-mean Gaussian random variable, where we only consider the samples ai such that the spin si is ˆ ˆ equal to 1, i. [sent-127, score-0.563]
</p><p>45 ˆ card{k : si (k) = 1} k : s (k) =1 ˆ ˆ i  Maximization of LW,b . [sent-130, score-0.291]
</p><p>46 We do gradient ascent in LW,b , ∂LW,b ∂LW,b where the gradients are given by ∂Wij = −Ep∗ [si sj ] + Ep [si sj ] and ∂bi = −Ep∗ [si ] + Ep [si ]. [sent-132, score-0.187]
</p><p>47 4  Note that since computing the parameters (ˆ, s) of the variational posterior in phase 1 only depends a ˆ on Φ, we ﬁrst perform several steps of coordinate ascent in (Φ, q) until Φ has converged, which is the same as in the usual sparse coding algorithm. [sent-134, score-0.635]
</p><p>48 Let η be the coherence parameter of the basis set which equals the maximum absolute inner product between two distinct basis functions. [sent-140, score-0.478]
</p><p>49 It has been shown that given a signal that is a sparse linear combination of p basis functions, BP and OMP will identify the optimal basis functions and their 1 coefﬁcients provided that p < 2 (η −1 + 1), and the sparsest representation of the signal is unique [19]. [sent-141, score-0.735]
</p><p>50 A data point x(i) ∈ D thus has a high probability of yielding a unique sparse representation in the basis set Φ. [sent-145, score-0.444]
</p><p>51 Provided that we have a good estimate of Φ we can recover its sparse representation using OMP or BP, and therefore identify s(i) that was used to originally sample x(i) . [sent-146, score-0.259]
</p><p>52 That is we recover with high probability all the samples from the Ising model used to generate D, which allows us to recover (W0 , b0 ). [sent-147, score-0.159]
</p><p>53 2 such that the model is sufﬁciently sparse as shown by the histogram of s ↑ in Figure 2, and the weights W0 are sampled according to a Gaussian distribution. [sent-158, score-0.4]
</p><p>54 We found that the basis functions are recovered exactly (not shown), and that the parameters of the Ising model are recovered with high accuracy as shown in Figure 2. [sent-162, score-0.342]
</p><p>55 4  14  x 10  b0  sparsity histogram  W0  0  12  0. [sent-163, score-0.149]
</p><p>56 5 Results for natural images We build our training set by randomly selecting 16 × 16 image patches from a standard set of 10 512 × 512 whitened images as in [1]. [sent-174, score-0.332]
</p><p>57 As our goal is to uncover this structure, we subtract from each patch its own mean and divide it by its standard deviation such that our dataset is contrast normalized (we do not consider the patches whose variance is below a small threshold). [sent-176, score-0.16]
</p><p>58 In the second phase of the algorithm we only update Φ, and we have found that the basis functions do not change dramatically after the ﬁrst phase. [sent-178, score-0.364]
</p><p>59 The basis functions resemble Gabor ﬁlters at a variety of orientations, positions and scales. [sent-180, score-0.345]
</p><p>60 5 −1 0  50  100  Figure 3: On the left is shown the entire set of basis functions Φ learned on natural images. [sent-182, score-0.392]
</p><p>61 the spatial properties (position, orientation, length) of the basis functions that are linked together by them. [sent-186, score-0.291]
</p><p>62 Each basis function is denoted by a bar that indicates its position, orientation, and length within the 16 × 16 patch. [sent-187, score-0.239]
</p><p>63 ϕi  (a) 10 most positive weights  ϕj  ϕk  Wij < 0 Wik > 0  (b) 10 most negative weights (c) Weights visualization  (d) Association ﬁelds  Figure 4: (a) (resp. [sent-188, score-0.166]
</p><p>64 (b)) shows the basis function pairs that share the strongest positive (resp. [sent-189, score-0.239]
</p><p>65 Each subplot in (d) shows the association ﬁeld for a basis function ϕi whose position and orientation are denoted by the black bar. [sent-191, score-0.409]
</p><p>66 The horizontal connections (Wij )j=i are displayed by a set of colored bars whose orientation and position denote those of the basis functions ϕj to which they correspond, and the color denotes the connection strength (see (c)). [sent-192, score-0.745]
</p><p>67 We observe that the connections are mainly local and connect basis functions at a variety of orientations. [sent-198, score-0.44]
</p><p>68 The histogram of the weights (see Figure 5) shows a long positive tail corresponding to a bias toward facilitatory connections. [sent-199, score-0.207]
</p><p>69 We compute for a basis function the average number of basis functions sharing with it a weight larger than 0. [sent-201, score-0.53]
</p><p>70 The resulting orientation proﬁle is consistent with what has been observed in physiological experiments [24, 25]. [sent-204, score-0.19]
</p><p>71 We also show in Figure 5 the tradeoff between the signal to noise ratio (SNR) of an image patch x and its reconstruction Φˆ, and the ℓ0 norm of the representation a 0 . [sent-205, score-0.182]
</p><p>72 We vary λ (see Equation (4)) and ǫ respectively, and average over 1000 patches to obtain the two tradeoff curves. [sent-207, score-0.143]
</p><p>73 We see that at similar SNR the representations inferred by our model are more sparse by about a factor of 2, which bodes well for compression. [sent-208, score-0.384]
</p><p>74 We have also compared our prior for tasks such as denoising and ﬁlling-in, and have found its performance to be similar to the factorial Laplacian prior even though it does not exploit the dependencies of the code. [sent-209, score-0.383]
</p><p>75 One possible explanation is that the greater sparsity of our inferred representations makes them less robust to noise. [sent-210, score-0.256]
</p><p>76 T  (W,Φ Φ) correlation  coupling weights histogram  tradeoff SNR−sparsity  orientation profile 14  110  0. [sent-212, score-0.405]
</p><p>77 05  Laplacian prior proposed prior  90  10  sparsity  6000  average # of connections  0. [sent-228, score-0.351]
</p><p>78 12  7000  0  π/4  π/2  1  2  3  4  orientation bins  30 20 5  6  7  8  9  10  11  12  13  SNR  Figure 5: Properties of the weight matrix W and comparison of the tradeoff curve SNR - ℓ0 norm between a Laplacian prior over the coefﬁcients and our proposed prior. [sent-229, score-0.244]
</p><p>79 To access how much information is captured by the second-order statistics, we isolate a group (ϕi )i∈Λ of 10 basis functions sharing strong weights. [sent-230, score-0.291]
</p><p>80 Given a collection of image patches that we sparsify using (4), we obtain a number of spins (ˆi )i∈Λ from which we can estimate the ems pirical distribution pemp , the Boltzmann-Gibbs distribution pIsing consistent with ﬁrst and second order correlations, and the factorial distribution pf act (i. [sent-231, score-0.61]
</p><p>81 We can see in Figure 6 that the Ising model produces better estimates of the empirical distribution, and results in better coding efﬁciency since KL(pemp ||pIsing ) = . [sent-234, score-0.248]
</p><p>82 −1  10  factorial model Ising model −2  10  Empirical probaility  all spins down  −3  10  all spins up 3 spins up  −4  10  −5  10  −5  10  −4  10  −3  10  −2  −1  10  10  Model probability  Figure 6: Model validation for a group of 10 basis functions (right). [sent-237, score-1.097]
</p><p>83 The empirical probabilities of the 210 patterns of activation are plotted against the probabilities predicted by the Ising model (red), the factorial model (blue), and their own values (black). [sent-238, score-0.233]
</p><p>84 These patterns having exactly three spins up are circled. [sent-239, score-0.191]
</p><p>85 The prediction of the Ising model is noticably better than that of the factorial model. [sent-240, score-0.182]
</p><p>86 6 Discussion In this paper, we proposed a new sparse coding model where we include pairwise coupling terms among the coefﬁcients to capture their dependencies. [sent-241, score-0.527]
</p><p>87 We derived a new learning algorithm to adapt the parameters of the model given a data set of natural images, and we were able to discover the dependencies among the basis functions coefﬁcients. [sent-242, score-0.462]
</p><p>88 We showed that the learned connection weights are consistent with physiological data. [sent-243, score-0.232]
</p><p>89 Furthermore, the representations inferred in our model have greater sparsity than when they are inferred using the Laplacian prior as in the standard sparse coding model. [sent-244, score-0.8]
</p><p>90 Note however that we have not found evidence that these horizontal connections facilitate contour integration, as they do not primarily connect colinear basis functions. [sent-245, score-0.64]
</p><p>91 Previous models in the literature simply assume these weights according to prior intuitions about the function of horizontal connections [12, 13]. [sent-246, score-0.414]
</p><p>92 It is of great interest to develop new models and unsupervised learning schemes possibly involving attention that will help us understand the computational principles underlying contour integration in the visual cortex. [sent-247, score-0.14]
</p><p>93 Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. [sent-253, score-0.324]
</p><p>94 A multi-layer sparse coding network learns contour coding from natural a images. [sent-276, score-0.728]
</p><p>95 The functional organization of local circuits in visual cortex: insights from the study of tree shrew striate cortex. [sent-324, score-0.201]
</p><p>96 Geometrical computations explain projection patterns of long-range horizontal connections in visual cortex. [sent-329, score-0.337]
</p><p>97 Border ownership from intracortical interactions in visual area v2. [sent-333, score-0.151]
</p><p>98 Just relax: convex programming methods for identifying sparse signals in noise. [sent-404, score-0.205]
</p><p>99 Relationship between intrinsic connections and functional architecture revealed by optical imaging and in vivo targeted biocytin injections in primate striate cortex. [sent-422, score-0.224]
</p><p>100 Orientation selectivity and the arrangement of horizontal connections in the tree shrew striate cortex. [sent-436, score-0.412]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('si', 0.291), ('ising', 0.271), ('basis', 0.239), ('sparse', 0.205), ('coding', 0.197), ('spins', 0.191), ('ai', 0.179), ('ex', 0.173), ('coef', 0.169), ('connections', 0.149), ('cients', 0.142), ('ep', 0.138), ('orientation', 0.135), ('factorial', 0.131), ('horizontal', 0.125), ('omp', 0.125), ('snr', 0.116), ('wij', 0.101), ('gsm', 0.094), ('pemp', 0.094), ('spin', 0.093), ('patches', 0.091), ('sparsity', 0.088), ('multiplier', 0.088), ('det', 0.088), ('weights', 0.083), ('contour', 0.077), ('cortex', 0.075), ('striate', 0.075), ('coupling', 0.074), ('inferred', 0.074), ('phase', 0.073), ('laplacian', 0.072), ('denoising', 0.07), ('patch', 0.069), ('dependencies', 0.068), ('neuroscience', 0.068), ('receptive', 0.067), ('olshausen', 0.066), ('images', 0.064), ('sj', 0.064), ('visual', 0.063), ('maximization', 0.063), ('variational', 0.063), ('bovik', 0.063), ('bpdn', 0.063), ('facilitatory', 0.063), ('pising', 0.063), ('redwood', 0.063), ('shrew', 0.063), ('histogram', 0.061), ('image', 0.061), ('ascent', 0.059), ('prior', 0.057), ('physiological', 0.055), ('ownership', 0.054), ('representations', 0.054), ('recover', 0.054), ('resemble', 0.054), ('functions', 0.052), ('tradeoff', 0.052), ('natural', 0.052), ('model', 0.051), ('recovery', 0.05), ('raina', 0.05), ('colinear', 0.05), ('battle', 0.05), ('hoyer', 0.05), ('hyv', 0.05), ('learned', 0.049), ('berkeley', 0.048), ('connection', 0.045), ('log', 0.045), ('scenes', 0.044), ('geometrical', 0.044), ('border', 0.044), ('biases', 0.042), ('overcomplete', 0.042), ('pf', 0.042), ('infer', 0.041), ('arg', 0.04), ('gabor', 0.04), ('explanation', 0.04), ('lters', 0.039), ('wavelet', 0.038), ('comput', 0.038), ('bi', 0.038), ('gibbs', 0.038), ('coordinate', 0.038), ('elds', 0.037), ('inhibitory', 0.037), ('bp', 0.036), ('enter', 0.036), ('neurons', 0.036), ('association', 0.035), ('favors', 0.035), ('interactions', 0.034), ('eld', 0.034), ('correlations', 0.034), ('dictionary', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="111-tfidf-1" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: It has been shown that adapting a dictionary of basis functions to the statistics of natural images so as to maximize sparsity in the coefﬁcients results in a set of dictionary elements whose spatial properties resemble those of V1 (primary visual cortex) receptive ﬁelds. However, the resulting sparse coefﬁcients still exhibit pronounced statistical dependencies, thus violating the independence assumption of the sparse coding model. Here, we propose a model that attempts to capture the dependencies among the basis function coefﬁcients by including a pairwise coupling term in the prior over the coefﬁcient activity states. When adapted to the statistics of natural images, the coupling terms learn a combination of facilitatory and inhibitory interactions among neighboring basis functions. These learned interactions may offer an explanation for the function of horizontal connections in V1 in terms of a prior over natural images.</p><p>2 0.20464022 <a title="111-tfidf-2" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>Author: Honglak Lee, Chaitanya Ekanadham, Andrew Y. Ng</p><p>Abstract: Motivated in part by the hierarchical organization of the cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or “deep,” structure from unlabeled data. While several authors have formally or informally compared their algorithms to computations performed in visual area V1 (and the cochlea), little attempt has been made thus far to evaluate these algorithms in terms of their ﬁdelity for mimicking computations at deeper levels in the cortical hierarchy. This paper presents an unsupervised learning model that faithfully mimics certain properties of visual area V2. Speciﬁcally, we develop a sparse variant of the deep belief networks of Hinton et al. (2006). We learn two layers of nodes in the network, and demonstrate that the ﬁrst layer, similar to prior work on sparse coding and ICA, results in localized, oriented, edge ﬁlters, similar to the Gabor functions known to model V1 cell receptive ﬁelds. Further, the second layer in our model encodes correlations of the ﬁrst layer responses in the data. Speciﬁcally, it picks up both colinear (“contour”) features as well as corners and junctions. More interestingly, in a quantitative comparison, the encoding of these more complex “corner” features matches well with the results from the Ito & Komatsu’s study of biological V2 responses. This suggests that our sparse variant of deep belief networks holds promise for modeling more higher-order features. 1</p><p>3 0.19276202 <a title="111-tfidf-3" href="./nips-2007-On_Sparsity_and_Overcompleteness_in_Image_Models.html">145 nips-2007-On Sparsity and Overcompleteness in Image Models</a></p>
<p>Author: Pietro Berkes, Richard Turner, Maneesh Sahani</p><p>Abstract: Computational models of visual cortex, and in particular those based on sparse coding, have enjoyed much recent attention. Despite this currency, the question of how sparse or how over-complete a sparse representation should be, has gone without principled answer. Here, we use Bayesian model-selection methods to address these questions for a sparse-coding model based on a Student-t prior. Having validated our methods on toy data, we ﬁnd that natural images are indeed best modelled by extremely sparse distributions; although for the Student-t prior, the associated optimal basis size is only modestly over-complete. 1</p><p>4 0.18960644 <a title="111-tfidf-4" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>Author: Matthias Bethge, Philipp Berens</p><p>Abstract: Maximum entropy analysis of binary variables provides an elegant way for studying the role of pairwise correlations in neural populations. Unfortunately, these approaches suffer from their poor scalability to high dimensions. In sensory coding, however, high-dimensional data is ubiquitous. Here, we introduce a new approach using a near-maximum entropy model, that makes this type of analysis feasible for very high-dimensional data—the model parameters can be derived in closed form and sampling is easy. Therefore, our NearMaxEnt approach can serve as a tool for testing predictions from a pairwise maximum entropy model not only for low-dimensional marginals, but also for high dimensional measurements of more than thousand units. We demonstrate its usefulness by studying natural images with dichotomized pixel intensities. Our results indicate that the statistics of such higher-dimensional measurements exhibit additional structure that are not predicted by pairwise correlations, despite the fact that pairwise correlations explain the lower-dimensional marginal statistics surprisingly well up to the limit of dimensionality where estimation of the full joint distribution is feasible. 1</p><p>5 0.15551662 <a title="111-tfidf-5" href="./nips-2007-Sparse_Overcomplete_Latent_Variable_Decomposition_of_Counts_Data.html">181 nips-2007-Sparse Overcomplete Latent Variable Decomposition of Counts Data</a></p>
<p>Author: Madhusudana Shashanka, Bhiksha Raj, Paris Smaragdis</p><p>Abstract: An important problem in many ﬁelds is the analysis of counts data to extract meaningful latent components. Methods like Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA) have been proposed for this purpose. However, they are limited in the number of components they can extract and lack an explicit provision to control the “expressiveness” of the extracted components. In this paper, we present a learning formulation to address these limitations by employing the notion of sparsity. We start with the PLSA framework and use an entropic prior in a maximum a posteriori formulation to enforce sparsity. We show that this allows the extraction of overcomplete sets of latent components which better characterize the data. We present experimental evidence of the utility of such representations.</p><p>6 0.12701955 <a title="111-tfidf-6" href="./nips-2007-A_New_View_of_Automatic_Relevance_Determination.html">8 nips-2007-A New View of Automatic Relevance Determination</a></p>
<p>7 0.1144815 <a title="111-tfidf-7" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>8 0.11094251 <a title="111-tfidf-8" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>9 0.11037119 <a title="111-tfidf-9" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>10 0.10546398 <a title="111-tfidf-10" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>11 0.097279325 <a title="111-tfidf-11" href="./nips-2007-A_Bayesian_LDA-based_model_for_semi-supervised_part-of-speech_tagging.html">2 nips-2007-A Bayesian LDA-based model for semi-supervised part-of-speech tagging</a></p>
<p>12 0.092221469 <a title="111-tfidf-12" href="./nips-2007-Modeling_image_patches_with_a_directed_hierarchy_of_Markov_random_fields.html">132 nips-2007-Modeling image patches with a directed hierarchy of Markov random fields</a></p>
<p>13 0.084881149 <a title="111-tfidf-13" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>14 0.082969278 <a title="111-tfidf-14" href="./nips-2007-Spatial_Latent_Dirichlet_Allocation.html">183 nips-2007-Spatial Latent Dirichlet Allocation</a></p>
<p>15 0.080772571 <a title="111-tfidf-15" href="./nips-2007-The_Generalized_FITC_Approximation.html">195 nips-2007-The Generalized FITC Approximation</a></p>
<p>16 0.079486452 <a title="111-tfidf-16" href="./nips-2007-Scene_Segmentation_with_CRFs_Learned_from_Partially_Labeled_Images.html">172 nips-2007-Scene Segmentation with CRFs Learned from Partially Labeled Images</a></p>
<p>17 0.073781453 <a title="111-tfidf-17" href="./nips-2007-Robust_Regression_with_Twinned_Gaussian_Processes.html">170 nips-2007-Robust Regression with Twinned Gaussian Processes</a></p>
<p>18 0.073258296 <a title="111-tfidf-18" href="./nips-2007-Regulator_Discovery_from_Gene_Expression_Time_Series_of_Malaria_Parasites%3A_a_Hierachical_Approach.html">167 nips-2007-Regulator Discovery from Gene Expression Time Series of Malaria Parasites: a Hierachical Approach</a></p>
<p>19 0.072136573 <a title="111-tfidf-19" href="./nips-2007-Using_Deep_Belief_Nets_to_Learn_Covariance_Kernels_for_Gaussian_Processes.html">212 nips-2007-Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes</a></p>
<p>20 0.070922337 <a title="111-tfidf-20" href="./nips-2007-Object_Recognition_by_Scene_Alignment.html">143 nips-2007-Object Recognition by Scene Alignment</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.27), (1, 0.122), (2, 0.068), (3, -0.11), (4, -0.004), (5, 0.021), (6, -0.135), (7, 0.132), (8, 0.097), (9, -0.011), (10, 0.161), (11, -0.001), (12, 0.1), (13, -0.006), (14, 0.05), (15, 0.047), (16, 0.103), (17, 0.138), (18, -0.085), (19, -0.099), (20, -0.124), (21, 0.081), (22, 0.152), (23, -0.044), (24, -0.091), (25, -0.011), (26, 0.07), (27, 0.061), (28, -0.104), (29, 0.023), (30, 0.019), (31, 0.104), (32, 0.038), (33, -0.129), (34, -0.101), (35, -0.025), (36, 0.054), (37, -0.001), (38, -0.084), (39, 0.049), (40, 0.024), (41, 0.067), (42, -0.095), (43, -0.081), (44, 0.073), (45, -0.006), (46, -0.032), (47, -0.039), (48, 0.102), (49, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97066373 <a title="111-lsi-1" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: It has been shown that adapting a dictionary of basis functions to the statistics of natural images so as to maximize sparsity in the coefﬁcients results in a set of dictionary elements whose spatial properties resemble those of V1 (primary visual cortex) receptive ﬁelds. However, the resulting sparse coefﬁcients still exhibit pronounced statistical dependencies, thus violating the independence assumption of the sparse coding model. Here, we propose a model that attempts to capture the dependencies among the basis function coefﬁcients by including a pairwise coupling term in the prior over the coefﬁcient activity states. When adapted to the statistics of natural images, the coupling terms learn a combination of facilitatory and inhibitory interactions among neighboring basis functions. These learned interactions may offer an explanation for the function of horizontal connections in V1 in terms of a prior over natural images.</p><p>2 0.81451869 <a title="111-lsi-2" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>Author: Matthias Bethge, Philipp Berens</p><p>Abstract: Maximum entropy analysis of binary variables provides an elegant way for studying the role of pairwise correlations in neural populations. Unfortunately, these approaches suffer from their poor scalability to high dimensions. In sensory coding, however, high-dimensional data is ubiquitous. Here, we introduce a new approach using a near-maximum entropy model, that makes this type of analysis feasible for very high-dimensional data—the model parameters can be derived in closed form and sampling is easy. Therefore, our NearMaxEnt approach can serve as a tool for testing predictions from a pairwise maximum entropy model not only for low-dimensional marginals, but also for high dimensional measurements of more than thousand units. We demonstrate its usefulness by studying natural images with dichotomized pixel intensities. Our results indicate that the statistics of such higher-dimensional measurements exhibit additional structure that are not predicted by pairwise correlations, despite the fact that pairwise correlations explain the lower-dimensional marginal statistics surprisingly well up to the limit of dimensionality where estimation of the full joint distribution is feasible. 1</p><p>3 0.75092047 <a title="111-lsi-3" href="./nips-2007-Sparse_Overcomplete_Latent_Variable_Decomposition_of_Counts_Data.html">181 nips-2007-Sparse Overcomplete Latent Variable Decomposition of Counts Data</a></p>
<p>Author: Madhusudana Shashanka, Bhiksha Raj, Paris Smaragdis</p><p>Abstract: An important problem in many ﬁelds is the analysis of counts data to extract meaningful latent components. Methods like Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA) have been proposed for this purpose. However, they are limited in the number of components they can extract and lack an explicit provision to control the “expressiveness” of the extracted components. In this paper, we present a learning formulation to address these limitations by employing the notion of sparsity. We start with the PLSA framework and use an entropic prior in a maximum a posteriori formulation to enforce sparsity. We show that this allows the extraction of overcomplete sets of latent components which better characterize the data. We present experimental evidence of the utility of such representations.</p><p>4 0.7465483 <a title="111-lsi-4" href="./nips-2007-On_Sparsity_and_Overcompleteness_in_Image_Models.html">145 nips-2007-On Sparsity and Overcompleteness in Image Models</a></p>
<p>Author: Pietro Berkes, Richard Turner, Maneesh Sahani</p><p>Abstract: Computational models of visual cortex, and in particular those based on sparse coding, have enjoyed much recent attention. Despite this currency, the question of how sparse or how over-complete a sparse representation should be, has gone without principled answer. Here, we use Bayesian model-selection methods to address these questions for a sparse-coding model based on a Student-t prior. Having validated our methods on toy data, we ﬁnd that natural images are indeed best modelled by extremely sparse distributions; although for the Student-t prior, the associated optimal basis size is only modestly over-complete. 1</p><p>5 0.63802421 <a title="111-lsi-5" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>Author: Honglak Lee, Chaitanya Ekanadham, Andrew Y. Ng</p><p>Abstract: Motivated in part by the hierarchical organization of the cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or “deep,” structure from unlabeled data. While several authors have formally or informally compared their algorithms to computations performed in visual area V1 (and the cochlea), little attempt has been made thus far to evaluate these algorithms in terms of their ﬁdelity for mimicking computations at deeper levels in the cortical hierarchy. This paper presents an unsupervised learning model that faithfully mimics certain properties of visual area V2. Speciﬁcally, we develop a sparse variant of the deep belief networks of Hinton et al. (2006). We learn two layers of nodes in the network, and demonstrate that the ﬁrst layer, similar to prior work on sparse coding and ICA, results in localized, oriented, edge ﬁlters, similar to the Gabor functions known to model V1 cell receptive ﬁelds. Further, the second layer in our model encodes correlations of the ﬁrst layer responses in the data. Speciﬁcally, it picks up both colinear (“contour”) features as well as corners and junctions. More interestingly, in a quantitative comparison, the encoding of these more complex “corner” features matches well with the results from the Ito & Komatsu’s study of biological V2 responses. This suggests that our sparse variant of deep belief networks holds promise for modeling more higher-order features. 1</p><p>6 0.56902665 <a title="111-lsi-6" href="./nips-2007-A_New_View_of_Automatic_Relevance_Determination.html">8 nips-2007-A New View of Automatic Relevance Determination</a></p>
<p>7 0.56200719 <a title="111-lsi-7" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>8 0.49090371 <a title="111-lsi-8" href="./nips-2007-The_Generalized_FITC_Approximation.html">195 nips-2007-The Generalized FITC Approximation</a></p>
<p>9 0.48922792 <a title="111-lsi-9" href="./nips-2007-Regulator_Discovery_from_Gene_Expression_Time_Series_of_Malaria_Parasites%3A_a_Hierachical_Approach.html">167 nips-2007-Regulator Discovery from Gene Expression Time Series of Malaria Parasites: a Hierachical Approach</a></p>
<p>10 0.47637084 <a title="111-lsi-10" href="./nips-2007-Blind_channel_identification_for_speech_dereverberation_using_l1-norm_sparse_learning.html">37 nips-2007-Blind channel identification for speech dereverberation using l1-norm sparse learning</a></p>
<p>11 0.47346181 <a title="111-lsi-11" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>12 0.47065857 <a title="111-lsi-12" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>13 0.45678881 <a title="111-lsi-13" href="./nips-2007-Estimating_disparity_with_confidence_from_energy_neurons.html">81 nips-2007-Estimating disparity with confidence from energy neurons</a></p>
<p>14 0.45473912 <a title="111-lsi-14" href="./nips-2007-Modeling_Natural_Sounds_with_Modulation_Cascade_Processes.html">130 nips-2007-Modeling Natural Sounds with Modulation Cascade Processes</a></p>
<p>15 0.44469008 <a title="111-lsi-15" href="./nips-2007-The_Infinite_Gamma-Poisson_Feature_Model.html">196 nips-2007-The Infinite Gamma-Poisson Feature Model</a></p>
<p>16 0.42849734 <a title="111-lsi-16" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>17 0.42299417 <a title="111-lsi-17" href="./nips-2007-Heterogeneous_Component_Analysis.html">96 nips-2007-Heterogeneous Component Analysis</a></p>
<p>18 0.41857937 <a title="111-lsi-18" href="./nips-2007-SpAM%3A_Sparse_Additive_Models.html">179 nips-2007-SpAM: Sparse Additive Models</a></p>
<p>19 0.40246552 <a title="111-lsi-19" href="./nips-2007-Scene_Segmentation_with_CRFs_Learned_from_Partially_Labeled_Images.html">172 nips-2007-Scene Segmentation with CRFs Learned from Partially Labeled Images</a></p>
<p>20 0.3997716 <a title="111-lsi-20" href="./nips-2007-An_online_Hebbian_learning_rule_that_performs_Independent_Component_Analysis.html">26 nips-2007-An online Hebbian learning rule that performs Independent Component Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.471), (13, 0.02), (16, 0.037), (18, 0.015), (21, 0.051), (34, 0.019), (35, 0.016), (47, 0.091), (83, 0.098), (85, 0.017), (87, 0.025), (90, 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94067657 <a title="111-lda-1" href="./nips-2007-Discovering_Weakly-Interacting_Factors_in_a_Complex_Stochastic_Process.html">68 nips-2007-Discovering Weakly-Interacting Factors in a Complex Stochastic Process</a></p>
<p>Author: Charlie Frogner, Avi Pfeffer</p><p>Abstract: Dynamic Bayesian networks are structured representations of stochastic processes. Despite their structure, exact inference in DBNs is generally intractable. One approach to approximate inference involves grouping the variables in the process into smaller factors and keeping independent beliefs over these factors. In this paper we present several techniques for decomposing a dynamic Bayesian network automatically to enable factored inference. We examine a number of features of a DBN that capture different types of dependencies that will cause error in factored inference. An empirical comparison shows that the most useful of these is a heuristic that estimates the mutual information introduced between factors by one step of belief propagation. In addition to features computed over entire factors, for efﬁciency we explored scores computed over pairs of variables. We present search methods that use these features, pairwise and not, to ﬁnd a factorization, and we compare their results on several datasets. Automatic factorization extends the applicability of factored inference to large, complex models that are undesirable to factor by hand. Moreover, tests on real DBNs show that automatic factorization can achieve signiﬁcantly lower error in some cases. 1</p><p>same-paper 2 0.90047359 <a title="111-lda-2" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: It has been shown that adapting a dictionary of basis functions to the statistics of natural images so as to maximize sparsity in the coefﬁcients results in a set of dictionary elements whose spatial properties resemble those of V1 (primary visual cortex) receptive ﬁelds. However, the resulting sparse coefﬁcients still exhibit pronounced statistical dependencies, thus violating the independence assumption of the sparse coding model. Here, we propose a model that attempts to capture the dependencies among the basis function coefﬁcients by including a pairwise coupling term in the prior over the coefﬁcient activity states. When adapted to the statistics of natural images, the coupling terms learn a combination of facilitatory and inhibitory interactions among neighboring basis functions. These learned interactions may offer an explanation for the function of horizontal connections in V1 in terms of a prior over natural images.</p><p>3 0.8926394 <a title="111-lda-3" href="./nips-2007-Anytime_Induction_of_Cost-sensitive_Trees.html">27 nips-2007-Anytime Induction of Cost-sensitive Trees</a></p>
<p>Author: Saher Esmeir, Shaul Markovitch</p><p>Abstract: Machine learning techniques are increasingly being used to produce a wide-range of classiﬁers for complex real-world applications that involve nonuniform testing costs and misclassiﬁcation costs. As the complexity of these applications grows, the management of resources during the learning and classiﬁcation processes becomes a challenging task. In this work we introduce ACT (Anytime Cost-sensitive Trees), a novel framework for operating in such environments. ACT is an anytime algorithm that allows trading computation time for lower classiﬁcation costs. It builds a tree top-down and exploits additional time resources to obtain better estimations for the utility of the different candidate splits. Using sampling techniques ACT approximates for each candidate split the cost of the subtree under it and favors the one with a minimal cost. Due to its stochastic nature ACT is expected to be able to escape local minima, into which greedy methods may be trapped. Experiments with a variety of datasets were conducted to compare the performance of ACT to that of the state of the art cost-sensitive tree learners. The results show that for most domains ACT produces trees of signiﬁcantly lower costs. ACT is also shown to exhibit good anytime behavior with diminishing returns.</p><p>4 0.87791687 <a title="111-lda-4" href="./nips-2007-Sparse_Overcomplete_Latent_Variable_Decomposition_of_Counts_Data.html">181 nips-2007-Sparse Overcomplete Latent Variable Decomposition of Counts Data</a></p>
<p>Author: Madhusudana Shashanka, Bhiksha Raj, Paris Smaragdis</p><p>Abstract: An important problem in many ﬁelds is the analysis of counts data to extract meaningful latent components. Methods like Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA) have been proposed for this purpose. However, they are limited in the number of components they can extract and lack an explicit provision to control the “expressiveness” of the extracted components. In this paper, we present a learning formulation to address these limitations by employing the notion of sparsity. We start with the PLSA framework and use an entropic prior in a maximum a posteriori formulation to enforce sparsity. We show that this allows the extraction of overcomplete sets of latent components which better characterize the data. We present experimental evidence of the utility of such representations.</p><p>5 0.58699679 <a title="111-lda-5" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>Author: Matthias Bethge, Philipp Berens</p><p>Abstract: Maximum entropy analysis of binary variables provides an elegant way for studying the role of pairwise correlations in neural populations. Unfortunately, these approaches suffer from their poor scalability to high dimensions. In sensory coding, however, high-dimensional data is ubiquitous. Here, we introduce a new approach using a near-maximum entropy model, that makes this type of analysis feasible for very high-dimensional data—the model parameters can be derived in closed form and sampling is easy. Therefore, our NearMaxEnt approach can serve as a tool for testing predictions from a pairwise maximum entropy model not only for low-dimensional marginals, but also for high dimensional measurements of more than thousand units. We demonstrate its usefulness by studying natural images with dichotomized pixel intensities. Our results indicate that the statistics of such higher-dimensional measurements exhibit additional structure that are not predicted by pairwise correlations, despite the fact that pairwise correlations explain the lower-dimensional marginal statistics surprisingly well up to the limit of dimensionality where estimation of the full joint distribution is feasible. 1</p><p>6 0.51900035 <a title="111-lda-6" href="./nips-2007-A_Kernel_Statistical_Test_of_Independence.html">7 nips-2007-A Kernel Statistical Test of Independence</a></p>
<p>7 0.51862198 <a title="111-lda-7" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>8 0.51625085 <a title="111-lda-8" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>9 0.51330453 <a title="111-lda-9" href="./nips-2007-Heterogeneous_Component_Analysis.html">96 nips-2007-Heterogeneous Component Analysis</a></p>
<p>10 0.50799692 <a title="111-lda-10" href="./nips-2007-On_higher-order_perceptron_algorithms.html">146 nips-2007-On higher-order perceptron algorithms</a></p>
<p>11 0.50152278 <a title="111-lda-11" href="./nips-2007-Structured_Learning_with_Approximate_Inference.html">187 nips-2007-Structured Learning with Approximate Inference</a></p>
<p>12 0.49533051 <a title="111-lda-12" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>13 0.49532503 <a title="111-lda-13" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>14 0.49134645 <a title="111-lda-14" href="./nips-2007-Scene_Segmentation_with_CRFs_Learned_from_Partially_Labeled_Images.html">172 nips-2007-Scene Segmentation with CRFs Learned from Partially Labeled Images</a></p>
<p>15 0.48902938 <a title="111-lda-15" href="./nips-2007-The_discriminant_center-surround_hypothesis_for_bottom-up_saliency.html">202 nips-2007-The discriminant center-surround hypothesis for bottom-up saliency</a></p>
<p>16 0.48646095 <a title="111-lda-16" href="./nips-2007-Colored_Maximum_Variance_Unfolding.html">49 nips-2007-Colored Maximum Variance Unfolding</a></p>
<p>17 0.48630095 <a title="111-lda-17" href="./nips-2007-HM-BiTAM%3A_Bilingual_Topic_Exploration%2C_Word_Alignment%2C_and_Translation.html">95 nips-2007-HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation</a></p>
<p>18 0.48572659 <a title="111-lda-18" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>19 0.48158178 <a title="111-lda-19" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>20 0.48065999 <a title="111-lda-20" href="./nips-2007-Learning_Visual_Attributes.html">113 nips-2007-Learning Visual Attributes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
