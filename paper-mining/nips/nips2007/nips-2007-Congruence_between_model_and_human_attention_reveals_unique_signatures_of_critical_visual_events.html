<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>57 nips-2007-Congruence between model and human attention reveals unique signatures of critical visual events</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-57" href="#">nips2007-57</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>57 nips-2007-Congruence between model and human attention reveals unique signatures of critical visual events</h1>
<br/><p>Source: <a title="nips-2007-57-pdf" href="http://papers.nips.cc/paper/3360-congruence-between-model-and-human-attention-reveals-unique-signatures-of-critical-visual-events.pdf">pdf</a></p><p>Author: Robert Peters, Laurent Itti</p><p>Abstract: Current computational models of bottom-up and top-down components of attention are predictive of eye movements across a range of stimuli and of simple, ﬁxed visual tasks (such as visual search for a target among distractors). However, to date there exists no computational framework which can reliably mimic human gaze behavior in more complex environments and tasks, such as driving a vehicle through traﬃc. Here, we develop a hybrid computational/behavioral framework, combining simple models for bottom-up salience and top-down relevance, and looking for changes in the predictive power of these components at diﬀerent critical event times during 4.7 hours (500,000 video frames) of observers playing car racing and ﬂight combat video games. This approach is motivated by our observation that the predictive strengths of the salience and relevance models exhibit reliable temporal signatures during critical event windows in the task sequence—for example, when the game player directly engages an enemy plane in a ﬂight combat game, the predictive strength of the salience model increases signiﬁcantly, while that of the relevance model decreases signiﬁcantly. Our new framework combines these temporal signatures to implement several event detectors. Critically, we ﬁnd that an event detector based on fused behavioral and stimulus information (in the form of the model’s predictive strength) is much stronger than detectors based on behavioral information alone (eye position) or image information alone (model prediction maps). This approach to event detection, based on eye tracking combined with computational models applied to the visual input, may have useful applications as a less-invasive alternative to other event detection approaches based on neural signatures derived from EEG or fMRI recordings. 1</p><p>Reference: <a title="nips-2007-57-reference" href="../nips2007_reference/nips-2007-Congruence_between_model_and_human_attention_reveals_unique_signatures_of_critical_visual_events_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Congruence between model and human attention reveals unique signatures of critical visual events  Robert J. [sent-1, score-0.483]
</p><p>2 edu  Abstract Current computational models of bottom-up and top-down components of attention are predictive of eye movements across a range of stimuli and of simple, ﬁxed visual tasks (such as visual search for a target among distractors). [sent-4, score-0.595]
</p><p>3 Here, we develop a hybrid computational/behavioral framework, combining simple models for bottom-up salience and top-down relevance, and looking for changes in the predictive power of these components at diﬀerent critical event times during 4. [sent-6, score-0.289]
</p><p>4 7 hours (500,000 video frames) of observers playing car racing and ﬂight combat video games. [sent-7, score-0.295]
</p><p>5 Our new framework combines these temporal signatures to implement several event detectors. [sent-9, score-0.303]
</p><p>6 Critically, we ﬁnd that an event detector based on fused behavioral and stimulus information (in the form of the model’s predictive strength) is much stronger than detectors based on behavioral information alone (eye position) or image information alone (model prediction maps). [sent-10, score-0.372]
</p><p>7 This approach to event detection, based on eye tracking combined with computational models applied to the visual input, may have useful applications as a less-invasive alternative to other event detection approaches based on neural signatures derived from EEG or fMRI recordings. [sent-11, score-0.911]
</p><p>8 A human game player interacts with a video game, generating a sequence of video frames from the game, and a sequence of eye position samples from the game player. [sent-20, score-1.299]
</p><p>9 The video frames feed into computational models for predicting bottom-up (BU) salience and top-down (TD) relevance inﬂuences on attention. [sent-21, score-0.306]
</p><p>10 These predictions are then compared with the observed eye position using a “normalized scanpath saliency” (NSS) metric. [sent-22, score-0.469]
</p><p>11 Finally, the video game sequence is annotated with key event times, and these are used to generate event-locked templates from each of the game-related signals. [sent-23, score-0.525]
</p><p>12 These templates are used to try to detect the events in the original game sequences, and the results are quantiﬁed with metrics from signal detection theory. [sent-24, score-0.538]
</p><p>13 Here we address that question in the context of playing challenging video games, by comparing eye movements recorded during game play with the predictions of a combined salience/relevance computational model. [sent-29, score-0.752]
</p><p>14 The important factor in our approach is that we identify key game events (such as destroying an enemy plane, or crashing the car during driving race) which can be used as proxy indicators of likely transitions in the observer’s task set. [sent-31, score-0.636]
</p><p>15 Then we align subsequent analysis on these event times, such that we can detect repeatable changes in model predictive strength within temporal windows around the key events. [sent-32, score-0.358]
</p><p>16 Indeed, we ﬁnd significant changes in the predictive strength of both salience and relevance models within these windows, including more than 8-fold increases in predictive strength as well as complete shifts from predictive to anti-predictive behavior. [sent-33, score-0.419]
</p><p>17 Finally we show that the predictive strength signatures formed in these windows can be used to detect the occurrence of the events themselves. [sent-34, score-0.551]
</p><p>18 2  Psychophysics and eye tracking  Five subjects (four male, one female) participated under a protocol approved by the Institutional Review Board of the University of Southern California. [sent-35, score-0.407]
</p><p>19 All of the subjects had at least some prior experience with playing video games in general, but none of the subjects had prior experience with the particular games involved in our experiment. [sent-37, score-0.243]
</p><p>20 For each game, subjects ﬁrst practiced the game for several one-hour sessions on diﬀerent days until reaching a success criterion (deﬁnition follows), and then returned for a one-hour eye tracking session with that game. [sent-38, score-0.672]
</p><p>21 Within each game, subjects learned to play three game levels, and during eye tracking, each subject played each game level twice. [sent-39, score-0.815]
</p><p>22 Thus, in total, our recorded data set consists of video frames and eye tracking data from 60 clips (5 subjects × 2 games per subject × 3 levels per game × 2 clips per level) covering 4. [sent-40, score-0.951]
</p><p>23 In this game, players control a car in a race against three other computer-controlled racers in a three-lap race, with a diﬀerent race course for each game level. [sent-43, score-0.399]
</p><p>24 The game controller joystick is used simply to steer the vehicle, and a pair of controller buttons are used to apply acceleration or brakes. [sent-45, score-0.256]
</p><p>25 During eye tracking, the average length of an NFSU level was 4. [sent-48, score-0.345]
</p><p>26 In this game, players control a simulated ﬁghter plane with a success criterion of destroying 12 speciﬁc enemy targets in 10 minutes or less. [sent-53, score-0.237]
</p><p>27 The game controller provides a simple set of ﬂight controls: the joystick controls pitch (forward–backward axis) and combined yaw/roll (left– right axis), a pair of buttons controls thrust level up and down, and another button triggers missile ﬁrings toward enemy targets. [sent-54, score-0.532]
</p><p>28 Two onscreen displays aid the players in ﬁnding enemy targets: one is a radar map with enemy locations indicated by red triangles, and another is a direction ﬁnder running along the bottom screen showing the player’s current compass heading along with the headings to each enemy target. [sent-55, score-0.506]
</p><p>29 During eye tracking, the average length of a TG level was 5. [sent-57, score-0.345]
</p><p>30 Subjects were seated at a viewing distance of 80 cm and used a chin-rest to stabilize their head position during eye tracking. [sent-63, score-0.469]
</p><p>31 Finally, subjects’ eye position was recorded at 240Hz with a hardware-based eye-tracking system (ISCAN, Inc. [sent-66, score-0.49]
</p><p>32 In total, we obtained roughly 500,000 video game frames and 4,000,000 eye position samples during 4. [sent-68, score-0.863]
</p><p>33 3  Computational attention prediction models  We developed a computational model which uses existing building blocks for bottom-up and topdown components of attention to generate new eye position prediction maps for each of the recorded video game frames. [sent-70, score-1.063]
</p><p>34 Then, for each frame, we quantiﬁed the degree of correspondence between those maps and the actual eye position recorded from the game player. [sent-71, score-0.801]
</p><p>35 This model has been previously reported to be signiﬁcantly predictive of eye positions across a range of stimuli and tasks [5, 6, 7, 8]. [sent-75, score-0.382]
</p><p>36 Top-down task-relevance maps were generated using a model [9] which is trained to associate low-level “gist” signatures with relevant eye positions (see also [3]). [sent-77, score-0.579]
</p><p>37 We trained the taskrelevance model with a leave-one-out approach: for each of the 60 game clips, the task-relevance model used for testing against that clip was trained on the video frames and eye position samples from the remaining 59 clips. [sent-78, score-0.863]
</p><p>38 26s  −5  100  missile fired (discrete events)  131. [sent-81, score-0.222]
</p><p>39 27s  time (s) relative to event  TD  BU  time (s) in video game session  missile fired (# events = 658)  50  TD model prediction strength  BU model prediction strength  vertical eye position  horizontal eye position  # missiles fired (continuous signal)  BU @ 130. [sent-87, score-2.235]
</p><p>40 26s  eye position eye position  10  0  100  TD @ 216. [sent-89, score-0.938]
</p><p>41 28s  time (s) relative to event  BU  300  # targets destroyed (continuous signal)  217. [sent-96, score-0.303]
</p><p>42 28s  time (s) in video game session  target destroyed (# events = 328)  50  TD model prediction strength  BU model prediction strength  vertical eye position  horizontal eye position  216. [sent-97, score-1.907]
</p><p>43 08s  −10  0  1  2  0  2  4  0  2  4  360  240  120  160  320  480  2  4  6  8  10  12  eye position eye position  10  0  TD @ 113. [sent-98, score-0.938]
</p><p>44 42s  time (s) relative to event  start speed−up (# events = 522) −5  100  start speed−up (discrete events)  115. [sent-103, score-0.408]
</p><p>45 42s  time (s) in video game session  TD model prediction strength  BU model prediction strength  vertical eye position  50  114. [sent-104, score-1.076]
</p><p>46 41s  speedometer (continuous signal)  horizontal eye position  BU @ 113. [sent-105, score-0.505]
</p><p>47 41s  eye position eye position  Figure 2: Event-locked analysis of agreement between model-predicted attention maps and observed human eye position. [sent-109, score-1.454]
</p><p>48 2  (b) ROC for "missile fired" events  non−events  0. [sent-112, score-0.21]
</p><p>49 8  1  recall (f) precision/recall for "target destroyed " events  1  1 0. [sent-142, score-0.326]
</p><p>50 9  eye position template match strength frequency  0. [sent-185, score-0.718]
</p><p>51 4  (e) ROC for "target destroyed " events  non−events events  0. [sent-186, score-0.536]
</p><p>52 2  false positives  (g) detector histograms for "start speed−up" events  (h) ROC for "start speed−up" events  0. [sent-193, score-0.558]
</p><p>53 8  1  (i) precision/recall for "start speed−up" events  1  1  0. [sent-196, score-0.21]
</p><p>54 7  1  BU&TD; NSS template match strength  precision  frequency  0. [sent-240, score-0.249]
</p><p>55 2  false positives  (d) detector histograms for "target destroyed " events 0. [sent-241, score-0.464]
</p><p>56 7  1  BU&TD; maps template match strength  frequency  1  1  true positives  frequency  0. [sent-258, score-0.397]
</p><p>57 278  eye position template match strength frequency  1  0. [sent-260, score-0.718]
</p><p>58 9  BU&TD; NSS template match strength  frequency  (c) precision/recall for "missile fired" events  1  events  precision  frequency  (a) detector histograms for "missile fired" events  0  0. [sent-261, score-1.01]
</p><p>59 8  1  recall  Figure 3: Signal detection results of using event-locked signatures to detect visual events in video game frame sequences. [sent-270, score-0.805]
</p><p>60 Computing the NSS simply involves normalizing the model prediction map to have a mean of zero and a variance of one, and then ﬁnding the value in that normalized map at the location of the human eye position. [sent-273, score-0.405]
</p><p>61 An NSS value of 0 would represent a model at chance in predicting human eye position, while an NSS value of 1 would represent a model for which human eye positions fell at locations with salience (or relevance) one standard deviation above average. [sent-274, score-0.868]
</p><p>62 Previous studies have typically used the NSS as a summary statistic to describe the predictive strength of a model across an entire sequence of ﬁxations [6, 12]; here, we use it instead as a continuous measure of the instantaneous predictive strength of the models. [sent-275, score-0.271]
</p><p>63 4  Event-locked analyses  We annotated the video game clips with several pieces of additional information that we could use to identify interesting events (see Figure 2) which would serve as the basis for event-locked analyses. [sent-276, score-0.593]
</p><p>64 These events were selected on the basis of representing transitions between diﬀerent task phases. [sent-277, score-0.257]
</p><p>65 We hypothesized that such events should correlate with changes in the relative strengths of diﬀerent inﬂuences on visual attention, and that we should be able to detect such changes using the previously described models as diagnostic tools. [sent-278, score-0.284]
</p><p>66 Therefore, after annotating the video clips with the times of each event of interest, we subsequently aligned further analyses on a temporal window of -5s/+5s  around each event (shaded background regions in Figure 2, rows b–e). [sent-279, score-0.519]
</p><p>67 From those windows we extract the time courses of NSS scores from the salience and relevance models and then compute the average time course across all of the windows, giving an event-locked template showing the NSS signature of that event type (Figure 2e). [sent-280, score-0.472]
</p><p>68 In the TG game we looked for times when the player ﬁred missiles (Figure 2, column 1). [sent-282, score-0.33]
</p><p>69 We selected these events because they represent transitions into a unique task phase, namely the phase of direct engagement with an enemy plane. [sent-283, score-0.421]
</p><p>70 During most of the TG game playing time, the player’s primary task involves actively using the radar and direction ﬁnder to locate enemy targets; however, during the time when a missile is in ﬂight the player’s only task is to await visual conﬁrmation of the missile destroying its target. [sent-284, score-0.841]
</p><p>71 Figure 2, column 1, row a illustrates one of the “missile ﬁred” events with captured video frames at -1500ms, 0ms, and +1500ms relative to the event time. [sent-285, score-0.576]
</p><p>72 Row b uses one of the 30 TG clips to show how the event times represent transitions in a continuous signal (number of missiles ﬁred); a -5s/+5s window around each event is highlighted by the shaded background regions. [sent-286, score-0.535]
</p><p>73 These windows then propagate through our model-based analysis, where we compare the eye position traces (row c) with the maps predicted by the BU salience (row f) and TD relevance (row g) to generate a continuous sequence of NSS values for each model (row d). [sent-287, score-0.818]
</p><p>74 Finally, all of the 658 event windows are pooled and we compute the average NSS value along with a 98% conﬁdence interval at each time point in the window, giving event-locked template NSS signatures for the “missile ﬁred” event type (row e). [sent-288, score-0.605]
</p><p>75 Those signatures show a strong divergence in the predictiveness of the BU and TD models: outside the event window, both models are signiﬁcantly but weakly predictive of observers’ eye positions, with NSS values around 0. [sent-289, score-0.685]
</p><p>76 In general, the TD model has learned that the radar screen and direction ﬁnder (toward the bottom left of the game screens) are usually the relevant locations, as illustrated by the elevated activity at those locations in the sample TD maps in row g. [sent-293, score-0.419]
</p><p>77 Most of the time, that is indeed a good prediction of eye position, reﬂected by the fact that the TD NSS scores are typically higher than the BU NSS scores outside the event window. [sent-294, score-0.609]
</p><p>78 However, within the event window, players shift their attention away from the target search task to instead follow the salient objects on the display (enemy target, the missile in ﬂight), which is reﬂected in the transient upswing in BU NSS scores. [sent-295, score-0.48]
</p><p>79 In the TG game we also considered times when enemy targets were destroyed (Figure 2, column 2). [sent-297, score-0.479]
</p><p>80 This is reﬂected in the sample frames shown in Figure 2, column 2, row a, where leading up to the event (at -600ms and 0ms) the player is watching the enemy target, but by +600ms after the event the player has switched back to looking at the direction ﬁnder to ﬁnd a new target. [sent-299, score-0.68]
</p><p>81 The analysis proceeds as before, using -5s/+5s windows around each of the 328 events to generate average event-locked NSS signatures for the two models (row e). [sent-300, score-0.402]
</p><p>82 Again we computed average event-locked NSS signatures for the BU and TD models from -5s/+5s windows around each of the 522 events, giving the traces in  row e. [sent-309, score-0.28]
</p><p>83 These traces reveal a signiﬁcant drop in TD NSS scores during the event window, but no signiﬁcant change in BU NSS scores. [sent-310, score-0.254]
</p><p>84 5  Event detectors  Having seen that critical game events are linked with highly signiﬁcant signatures in the time course of BU and TD model predictiveness, we next asked whether these signatures could be used in turn to predict the events themselves. [sent-313, score-0.945]
</p><p>85 For each of these detector types and for each event type, we compute event-locked signatures just as described in the previous section. [sent-315, score-0.391]
</p><p>86 For the BU and TD prediction maps, we compute the event-locked average BU and TD prediction map at each time point within the event window, and for the eye position traces we compute the event-locked average x and y eye position coordinate at each time point. [sent-317, score-1.21]
</p><p>87 Thus we have signatures for how each of these detector signals is expected to look during the critical event intervals. [sent-318, score-0.391]
</p><p>88 Next, we go back to the original detector traces (that is, the raw eye position traces as in Figure 2 row c, or the raw BU and TD maps as in rows f and g, or the raw BU and TD NSS scores as in row d). [sent-319, score-0.937]
</p><p>89 To combine each pair of correlation coeﬃcients (from BU and TD maps, or from BU and TD NSS, or from x and y eye position) into a single match strength, we scale the individual correlation coeﬃcients to a range of [0. [sent-321, score-0.388]
</p><p>90 This yields continuous traces of match strength between the event detector templates and the current signal values, for each video game frame in the data set. [sent-331, score-0.876]
</p><p>91 For each event type, we label every video frame as “during event” if it falls within a -500ms/+500ms window around the event instant, and label it as “during non-event” otherwise. [sent-333, score-0.493]
</p><p>92 Then we ask how well the match strengths can predict the label, for each of the three detector types (BU and TD maps alone, eye position alone, or BU and TD NSS). [sent-334, score-0.691]
</p><p>93 The ﬁrst column (panels a, d, and g) shows the histograms of the match strength values during events and during nonevents, for each of the three detector types; this gives a qualitative sense for how well each detector can distinguish events from non-events. [sent-337, score-0.745]
</p><p>94 The strongest separation between events and non-events is clearly obtained by the BU&TD; NSS and eye position detectors for the “missile ﬁred” and “target destroyed” events. [sent-338, score-0.698]
</p><p>95 Within each event type, the highest scores are obtained by the BU&TD; NSS detector (representing fused image/behavioral information), followed by the eye position detector (behavioral information only) and then the BU&TD; maps detector (image information only). [sent-342, score-1.022]
</p><p>96 6  Discussion and Conclusion  Our contributions here are twofold: First, we reported several instances in which the degree of correspondence between computational models of attention and human eye position varies systematically as a function of the current task phase. [sent-343, score-0.574]
</p><p>97 Second, we reported that variations in the predictive strength of the salience and relevance models are systematic enough that the signals can be used to form template-based detectors of the key game events. [sent-345, score-0.496]
</p><p>98 Conversely, the approach of deriving human behavioral information only from eye movements has the advantage of being less invasive and cumbersome than other neurally-based event-detection approaches using EEG or fMRI [13]. [sent-348, score-0.426]
</p><p>99 Further, although an eye tracker’s x/y traces amounts to less raw information than EEG’s dozens of leads or fMRI’s 10,000s of voxels, the eye-tracking signals also contain a denser and less redundant representation of cognitive information, as they are a manifestation of whole-brain output. [sent-349, score-0.426]
</p><p>100 Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search. [sent-369, score-0.422]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nss', 0.508), ('bu', 0.373), ('eye', 0.345), ('td', 0.306), ('game', 0.22), ('events', 0.21), ('missile', 0.16), ('event', 0.16), ('signatures', 0.143), ('position', 0.124), ('destroyed', 0.116), ('enemy', 0.116), ('video', 0.11), ('template', 0.093), ('salience', 0.092), ('maps', 0.091), ('detector', 0.088), ('strength', 0.088), ('tg', 0.08), ('player', 0.074), ('frames', 0.064), ('fired', 0.062), ('traces', 0.056), ('race', 0.054), ('clips', 0.053), ('nfsu', 0.053), ('players', 0.051), ('visual', 0.05), ('windows', 0.049), ('attention', 0.048), ('auc', 0.048), ('nder', 0.047), ('session', 0.045), ('crash', 0.045), ('ight', 0.044), ('match', 0.043), ('pos', 0.043), ('vehicle', 0.043), ('relevance', 0.04), ('gaze', 0.04), ('scores', 0.038), ('predictive', 0.037), ('window', 0.036), ('non', 0.036), ('target', 0.036), ('missiles', 0.036), ('speedometer', 0.036), ('templates', 0.035), ('radar', 0.035), ('itti', 0.033), ('tracking', 0.032), ('row', 0.032), ('human', 0.032), ('positives', 0.032), ('red', 0.031), ('uences', 0.031), ('subjects', 0.03), ('movements', 0.029), ('prediction', 0.028), ('combat', 0.028), ('signal', 0.028), ('playing', 0.027), ('engagement', 0.027), ('southern', 0.027), ('frame', 0.027), ('targets', 0.027), ('erent', 0.026), ('raw', 0.025), ('task', 0.025), ('frequency', 0.025), ('detect', 0.024), ('destroying', 0.023), ('games', 0.023), ('speed', 0.023), ('vision', 0.023), ('observer', 0.023), ('coe', 0.023), ('transitions', 0.022), ('locations', 0.022), ('peters', 0.022), ('attentional', 0.021), ('overt', 0.021), ('detection', 0.021), ('phase', 0.021), ('recorded', 0.021), ('di', 0.021), ('continuous', 0.021), ('behavioral', 0.02), ('minutes', 0.02), ('car', 0.02), ('shaded', 0.019), ('fmri', 0.019), ('screen', 0.019), ('detectors', 0.019), ('start', 0.019), ('histograms', 0.018), ('buttons', 0.018), ('gamecube', 0.018), ('joystick', 0.018), ('nssbu', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="57-tfidf-1" href="./nips-2007-Congruence_between_model_and_human_attention_reveals_unique_signatures_of_critical_visual_events.html">57 nips-2007-Congruence between model and human attention reveals unique signatures of critical visual events</a></p>
<p>Author: Robert Peters, Laurent Itti</p><p>Abstract: Current computational models of bottom-up and top-down components of attention are predictive of eye movements across a range of stimuli and of simple, ﬁxed visual tasks (such as visual search for a target among distractors). However, to date there exists no computational framework which can reliably mimic human gaze behavior in more complex environments and tasks, such as driving a vehicle through traﬃc. Here, we develop a hybrid computational/behavioral framework, combining simple models for bottom-up salience and top-down relevance, and looking for changes in the predictive power of these components at diﬀerent critical event times during 4.7 hours (500,000 video frames) of observers playing car racing and ﬂight combat video games. This approach is motivated by our observation that the predictive strengths of the salience and relevance models exhibit reliable temporal signatures during critical event windows in the task sequence—for example, when the game player directly engages an enemy plane in a ﬂight combat game, the predictive strength of the salience model increases signiﬁcantly, while that of the relevance model decreases signiﬁcantly. Our new framework combines these temporal signatures to implement several event detectors. Critically, we ﬁnd that an event detector based on fused behavioral and stimulus information (in the form of the model’s predictive strength) is much stronger than detectors based on behavioral information alone (eye position) or image information alone (model prediction maps). This approach to event detection, based on eye tracking combined with computational models applied to the visual input, may have useful applications as a less-invasive alternative to other event detection approaches based on neural signatures derived from EEG or fMRI recordings. 1</p><p>2 0.11170844 <a title="57-tfidf-2" href="./nips-2007-Regret_Minimization_in_Games_with_Incomplete_Information.html">165 nips-2007-Regret Minimization in Games with Incomplete Information</a></p>
<p>Author: Martin Zinkevich, Michael Johanson, Michael Bowling, Carmelo Piccione</p><p>Abstract: Extensive games are a powerful model of multiagent decision-making scenarios with incomplete information. Finding a Nash equilibrium for very large instances of these games has received a great deal of recent attention. In this paper, we describe a new technique for solving large games based on regret minimization. In particular, we introduce the notion of counterfactual regret, which exploits the degree of incomplete information in an extensive game. We show how minimizing counterfactual regret minimizes overall regret, and therefore in self-play can be used to compute a Nash equilibrium. We demonstrate this technique in the domain of poker, showing we can solve abstractions of limit Texas Hold’em with as many as 1012 states, two orders of magnitude larger than previous methods. 1</p><p>3 0.099700525 <a title="57-tfidf-3" href="./nips-2007-Computing_Robust_Counter-Strategies.html">55 nips-2007-Computing Robust Counter-Strategies</a></p>
<p>Author: Michael Johanson, Martin Zinkevich, Michael Bowling</p><p>Abstract: Adaptation to other initially unknown agents often requires computing an effective counter-strategy. In the Bayesian paradigm, one must ﬁnd a good counterstrategy to the inferred posterior of the other agents’ behavior. In the experts paradigm, one may want to choose experts that are good counter-strategies to the other agents’ expected behavior. In this paper we introduce a technique for computing robust counter-strategies for adaptation in multiagent scenarios under a variety of paradigms. The strategies can take advantage of a suspected tendency in the decisions of the other agents, while bounding the worst-case performance when the tendency is not observed. The technique involves solving a modiﬁed game, and therefore can make use of recently developed algorithms for solving very large extensive games. We demonstrate the effectiveness of the technique in two-player Texas Hold’em. We show that the computed poker strategies are substantially more robust than best response counter-strategies, while still exploiting a suspected tendency. We also compose the generated strategies in an experts algorithm showing a dramatic improvement in performance over using simple best responses. 1</p><p>4 0.083443619 <a title="57-tfidf-4" href="./nips-2007-Predicting_human_gaze_using_low-level_saliency_combined_with_face_detection.html">155 nips-2007-Predicting human gaze using low-level saliency combined with face detection</a></p>
<p>Author: Moran Cerf, Jonathan Harel, Wolfgang Einhaeuser, Christof Koch</p><p>Abstract: Under natural viewing conditions, human observers shift their gaze to allocate processing resources to subsets of the visual input. Many computational models try to predict such voluntary eye and attentional shifts. Although the important role of high level stimulus properties (e.g., semantic information) in search stands undisputed, most models are based on low-level image properties. We here demonstrate that a combined model of face detection and low-level saliency signiﬁcantly outperforms a low-level model in predicting locations humans ﬁxate on, based on eye-movement recordings of humans observing photographs of natural scenes, most of which contained at least one person. Observers, even when not instructed to look for anything particular, ﬁxate on a face with a probability of over 80% within their ﬁrst two ﬁxations; furthermore, they exhibit more similar scanpaths when faces are present. Remarkably, our model’s predictive performance in images that do not contain faces is not impaired, and is even improved in some cases by spurious face detector responses. 1</p><p>5 0.080481723 <a title="57-tfidf-5" href="./nips-2007-Temporal_Difference_Updating_without_a_Learning_Rate.html">191 nips-2007-Temporal Difference Updating without a Learning Rate</a></p>
<p>Author: Marcus Hutter, Shane Legg</p><p>Abstract: We derive an equation for temporal difference learning from statistical principles. Speciﬁcally, we start with the variational principle and then bootstrap to produce an updating rule for discounted state value estimates. The resulting equation is similar to the standard equation for temporal difference learning with eligibility traces, so called TD(λ), however it lacks the parameter α that speciﬁes the learning rate. In the place of this free parameter there is now an equation for the learning rate that is speciﬁc to each state transition. We experimentally test this new learning rule against TD(λ) and ﬁnd that it offers superior performance in various settings. Finally, we make some preliminary investigations into how to extend our new temporal difference algorithm to reinforcement learning. To do this we combine our update equation with both Watkins’ Q(λ) and Sarsa(λ) and ﬁnd that it again offers superior performance without a learning rate parameter. 1</p><p>6 0.071558654 <a title="57-tfidf-6" href="./nips-2007-Incremental_Natural_Actor-Critic_Algorithms.html">102 nips-2007-Incremental Natural Actor-Critic Algorithms</a></p>
<p>7 0.068034396 <a title="57-tfidf-7" href="./nips-2007-TrueSkill_Through_Time%3A_Revisiting_the_History_of_Chess.html">208 nips-2007-TrueSkill Through Time: Revisiting the History of Chess</a></p>
<p>8 0.066768147 <a title="57-tfidf-8" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>9 0.062572822 <a title="57-tfidf-9" href="./nips-2007-Measuring_Neural_Synchrony_by_Message_Passing.html">127 nips-2007-Measuring Neural Synchrony by Message Passing</a></p>
<p>10 0.060378544 <a title="57-tfidf-10" href="./nips-2007-Configuration_Estimates_Improve_Pedestrian_Finding.html">56 nips-2007-Configuration Estimates Improve Pedestrian Finding</a></p>
<p>11 0.059342962 <a title="57-tfidf-11" href="./nips-2007-Computational_Equivalence_of_Fixed_Points_and_No_Regret_Algorithms%2C_and_Convergence_to_Equilibria.html">54 nips-2007-Computational Equivalence of Fixed Points and No Regret Algorithms, and Convergence to Equilibria</a></p>
<p>12 0.058474973 <a title="57-tfidf-12" href="./nips-2007-A_Game-Theoretic_Approach_to_Apprenticeship_Learning.html">5 nips-2007-A Game-Theoretic Approach to Apprenticeship Learning</a></p>
<p>13 0.056534663 <a title="57-tfidf-13" href="./nips-2007-The_discriminant_center-surround_hypothesis_for_bottom-up_saliency.html">202 nips-2007-The discriminant center-surround hypothesis for bottom-up saliency</a></p>
<p>14 0.052239127 <a title="57-tfidf-14" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>15 0.051489245 <a title="57-tfidf-15" href="./nips-2007-Experience-Guided_Search%3A_A_Theory_of_Attentional_Control.html">85 nips-2007-Experience-Guided Search: A Theory of Attentional Control</a></p>
<p>16 0.051213071 <a title="57-tfidf-16" href="./nips-2007-Multiple-Instance_Pruning_For_Learning_Efficient_Cascade_Detectors.html">137 nips-2007-Multiple-Instance Pruning For Learning Efficient Cascade Detectors</a></p>
<p>17 0.046559956 <a title="57-tfidf-17" href="./nips-2007-Object_Recognition_by_Scene_Alignment.html">143 nips-2007-Object Recognition by Scene Alignment</a></p>
<p>18 0.045409907 <a title="57-tfidf-18" href="./nips-2007-Second_Order_Bilinear_Discriminant_Analysis_for_single_trial_EEG_analysis.html">173 nips-2007-Second Order Bilinear Discriminant Analysis for single trial EEG analysis</a></p>
<p>19 0.042089351 <a title="57-tfidf-19" href="./nips-2007-Predicting_Brain_States_from_fMRI_Data%3A_Incremental_Functional_Principal_Component_Regression.html">154 nips-2007-Predicting Brain States from fMRI Data: Incremental Functional Principal Component Regression</a></p>
<p>20 0.039817527 <a title="57-tfidf-20" href="./nips-2007-A_Bayesian_Model_of_Conditioned_Perception.html">3 nips-2007-A Bayesian Model of Conditioned Perception</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.103), (1, -0.041), (2, 0.042), (3, -0.051), (4, 0.111), (5, 0.125), (6, 0.014), (7, 0.136), (8, -0.01), (9, -0.008), (10, -0.145), (11, -0.113), (12, 0.005), (13, 0.009), (14, -0.015), (15, 0.006), (16, -0.042), (17, 0.084), (18, -0.087), (19, 0.066), (20, 0.035), (21, 0.008), (22, 0.069), (23, 0.06), (24, 0.061), (25, 0.099), (26, 0.018), (27, 0.07), (28, 0.043), (29, -0.072), (30, -0.003), (31, 0.024), (32, -0.078), (33, 0.033), (34, 0.078), (35, 0.02), (36, -0.067), (37, -0.106), (38, 0.107), (39, 0.038), (40, -0.05), (41, -0.104), (42, -0.014), (43, 0.031), (44, 0.075), (45, 0.041), (46, -0.085), (47, 0.004), (48, 0.045), (49, 0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97129858 <a title="57-lsi-1" href="./nips-2007-Congruence_between_model_and_human_attention_reveals_unique_signatures_of_critical_visual_events.html">57 nips-2007-Congruence between model and human attention reveals unique signatures of critical visual events</a></p>
<p>Author: Robert Peters, Laurent Itti</p><p>Abstract: Current computational models of bottom-up and top-down components of attention are predictive of eye movements across a range of stimuli and of simple, ﬁxed visual tasks (such as visual search for a target among distractors). However, to date there exists no computational framework which can reliably mimic human gaze behavior in more complex environments and tasks, such as driving a vehicle through traﬃc. Here, we develop a hybrid computational/behavioral framework, combining simple models for bottom-up salience and top-down relevance, and looking for changes in the predictive power of these components at diﬀerent critical event times during 4.7 hours (500,000 video frames) of observers playing car racing and ﬂight combat video games. This approach is motivated by our observation that the predictive strengths of the salience and relevance models exhibit reliable temporal signatures during critical event windows in the task sequence—for example, when the game player directly engages an enemy plane in a ﬂight combat game, the predictive strength of the salience model increases signiﬁcantly, while that of the relevance model decreases signiﬁcantly. Our new framework combines these temporal signatures to implement several event detectors. Critically, we ﬁnd that an event detector based on fused behavioral and stimulus information (in the form of the model’s predictive strength) is much stronger than detectors based on behavioral information alone (eye position) or image information alone (model prediction maps). This approach to event detection, based on eye tracking combined with computational models applied to the visual input, may have useful applications as a less-invasive alternative to other event detection approaches based on neural signatures derived from EEG or fMRI recordings. 1</p><p>2 0.62114006 <a title="57-lsi-2" href="./nips-2007-Computing_Robust_Counter-Strategies.html">55 nips-2007-Computing Robust Counter-Strategies</a></p>
<p>Author: Michael Johanson, Martin Zinkevich, Michael Bowling</p><p>Abstract: Adaptation to other initially unknown agents often requires computing an effective counter-strategy. In the Bayesian paradigm, one must ﬁnd a good counterstrategy to the inferred posterior of the other agents’ behavior. In the experts paradigm, one may want to choose experts that are good counter-strategies to the other agents’ expected behavior. In this paper we introduce a technique for computing robust counter-strategies for adaptation in multiagent scenarios under a variety of paradigms. The strategies can take advantage of a suspected tendency in the decisions of the other agents, while bounding the worst-case performance when the tendency is not observed. The technique involves solving a modiﬁed game, and therefore can make use of recently developed algorithms for solving very large extensive games. We demonstrate the effectiveness of the technique in two-player Texas Hold’em. We show that the computed poker strategies are substantially more robust than best response counter-strategies, while still exploiting a suspected tendency. We also compose the generated strategies in an experts algorithm showing a dramatic improvement in performance over using simple best responses. 1</p><p>3 0.61082774 <a title="57-lsi-3" href="./nips-2007-TrueSkill_Through_Time%3A_Revisiting_the_History_of_Chess.html">208 nips-2007-TrueSkill Through Time: Revisiting the History of Chess</a></p>
<p>Author: Pierre Dangauthier, Ralf Herbrich, Tom Minka, Thore Graepel</p><p>Abstract: We extend the Bayesian skill rating system TrueSkill to infer entire time series of skills of players by smoothing through time instead of ﬁltering. The skill of each participating player, say, every year is represented by a latent skill variable which is aﬀected by the relevant game outcomes that year, and coupled with the skill variables of the previous and subsequent year. Inference in the resulting factor graph is carried out by approximate message passing (EP) along the time series of skills. As before the system tracks the uncertainty about player skills, explicitly models draws, can deal with any number of competing entities and can infer individual skills from team results. We extend the system to estimate player-speciﬁc draw margins. Based on these models we present an analysis of the skill curves of important players in the history of chess over the past 150 years. Results include plots of players’ lifetime skill development as well as the ability to compare the skills of diﬀerent players across time. Our results indicate that a) the overall playing strength has increased over the past 150 years, and b) that modelling a player’s ability to force a draw provides signiﬁcantly better predictive power. 1</p><p>4 0.52050591 <a title="57-lsi-4" href="./nips-2007-Regret_Minimization_in_Games_with_Incomplete_Information.html">165 nips-2007-Regret Minimization in Games with Incomplete Information</a></p>
<p>Author: Martin Zinkevich, Michael Johanson, Michael Bowling, Carmelo Piccione</p><p>Abstract: Extensive games are a powerful model of multiagent decision-making scenarios with incomplete information. Finding a Nash equilibrium for very large instances of these games has received a great deal of recent attention. In this paper, we describe a new technique for solving large games based on regret minimization. In particular, we introduce the notion of counterfactual regret, which exploits the degree of incomplete information in an extensive game. We show how minimizing counterfactual regret minimizes overall regret, and therefore in self-play can be used to compute a Nash equilibrium. We demonstrate this technique in the domain of poker, showing we can solve abstractions of limit Texas Hold’em with as many as 1012 states, two orders of magnitude larger than previous methods. 1</p><p>5 0.46914828 <a title="57-lsi-5" href="./nips-2007-Temporal_Difference_Updating_without_a_Learning_Rate.html">191 nips-2007-Temporal Difference Updating without a Learning Rate</a></p>
<p>Author: Marcus Hutter, Shane Legg</p><p>Abstract: We derive an equation for temporal difference learning from statistical principles. Speciﬁcally, we start with the variational principle and then bootstrap to produce an updating rule for discounted state value estimates. The resulting equation is similar to the standard equation for temporal difference learning with eligibility traces, so called TD(λ), however it lacks the parameter α that speciﬁes the learning rate. In the place of this free parameter there is now an equation for the learning rate that is speciﬁc to each state transition. We experimentally test this new learning rule against TD(λ) and ﬁnd that it offers superior performance in various settings. Finally, we make some preliminary investigations into how to extend our new temporal difference algorithm to reinforcement learning. To do this we combine our update equation with both Watkins’ Q(λ) and Sarsa(λ) and ﬁnd that it again offers superior performance without a learning rate parameter. 1</p><p>6 0.4089708 <a title="57-lsi-6" href="./nips-2007-Predicting_human_gaze_using_low-level_saliency_combined_with_face_detection.html">155 nips-2007-Predicting human gaze using low-level saliency combined with face detection</a></p>
<p>7 0.38669991 <a title="57-lsi-7" href="./nips-2007-Measuring_Neural_Synchrony_by_Message_Passing.html">127 nips-2007-Measuring Neural Synchrony by Message Passing</a></p>
<p>8 0.3630048 <a title="57-lsi-8" href="./nips-2007-Experience-Guided_Search%3A_A_Theory_of_Attentional_Control.html">85 nips-2007-Experience-Guided Search: A Theory of Attentional Control</a></p>
<p>9 0.35564664 <a title="57-lsi-9" href="./nips-2007-The_discriminant_center-surround_hypothesis_for_bottom-up_saliency.html">202 nips-2007-The discriminant center-surround hypothesis for bottom-up saliency</a></p>
<p>10 0.34963709 <a title="57-lsi-10" href="./nips-2007-Competition_Adds_Complexity.html">52 nips-2007-Competition Adds Complexity</a></p>
<p>11 0.34708804 <a title="57-lsi-11" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>12 0.34488809 <a title="57-lsi-12" href="./nips-2007-Configuration_Estimates_Improve_Pedestrian_Finding.html">56 nips-2007-Configuration Estimates Improve Pedestrian Finding</a></p>
<p>13 0.3257719 <a title="57-lsi-13" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>14 0.32536298 <a title="57-lsi-14" href="./nips-2007-Scan_Strategies_for_Meteorological_Radars.html">171 nips-2007-Scan Strategies for Meteorological Radars</a></p>
<p>15 0.30986026 <a title="57-lsi-15" href="./nips-2007-Multiple-Instance_Pruning_For_Learning_Efficient_Cascade_Detectors.html">137 nips-2007-Multiple-Instance Pruning For Learning Efficient Cascade Detectors</a></p>
<p>16 0.30909878 <a title="57-lsi-16" href="./nips-2007-Object_Recognition_by_Scene_Alignment.html">143 nips-2007-Object Recognition by Scene Alignment</a></p>
<p>17 0.2970961 <a title="57-lsi-17" href="./nips-2007-Optimal_models_of_sound_localization_by_barn_owls.html">150 nips-2007-Optimal models of sound localization by barn owls</a></p>
<p>18 0.29275125 <a title="57-lsi-18" href="./nips-2007-Learning_Visual_Attributes.html">113 nips-2007-Learning Visual Attributes</a></p>
<p>19 0.27443263 <a title="57-lsi-19" href="./nips-2007-Incremental_Natural_Actor-Critic_Algorithms.html">102 nips-2007-Incremental Natural Actor-Critic Algorithms</a></p>
<p>20 0.25264275 <a title="57-lsi-20" href="./nips-2007-Second_Order_Bilinear_Discriminant_Analysis_for_single_trial_EEG_analysis.html">173 nips-2007-Second Order Bilinear Discriminant Analysis for single trial EEG analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.014), (5, 0.032), (13, 0.023), (16, 0.019), (18, 0.026), (19, 0.033), (21, 0.034), (26, 0.013), (29, 0.01), (34, 0.025), (35, 0.012), (47, 0.052), (49, 0.017), (66, 0.374), (83, 0.1), (87, 0.036), (90, 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78733087 <a title="57-lda-1" href="./nips-2007-Congruence_between_model_and_human_attention_reveals_unique_signatures_of_critical_visual_events.html">57 nips-2007-Congruence between model and human attention reveals unique signatures of critical visual events</a></p>
<p>Author: Robert Peters, Laurent Itti</p><p>Abstract: Current computational models of bottom-up and top-down components of attention are predictive of eye movements across a range of stimuli and of simple, ﬁxed visual tasks (such as visual search for a target among distractors). However, to date there exists no computational framework which can reliably mimic human gaze behavior in more complex environments and tasks, such as driving a vehicle through traﬃc. Here, we develop a hybrid computational/behavioral framework, combining simple models for bottom-up salience and top-down relevance, and looking for changes in the predictive power of these components at diﬀerent critical event times during 4.7 hours (500,000 video frames) of observers playing car racing and ﬂight combat video games. This approach is motivated by our observation that the predictive strengths of the salience and relevance models exhibit reliable temporal signatures during critical event windows in the task sequence—for example, when the game player directly engages an enemy plane in a ﬂight combat game, the predictive strength of the salience model increases signiﬁcantly, while that of the relevance model decreases signiﬁcantly. Our new framework combines these temporal signatures to implement several event detectors. Critically, we ﬁnd that an event detector based on fused behavioral and stimulus information (in the form of the model’s predictive strength) is much stronger than detectors based on behavioral information alone (eye position) or image information alone (model prediction maps). This approach to event detection, based on eye tracking combined with computational models applied to the visual input, may have useful applications as a less-invasive alternative to other event detection approaches based on neural signatures derived from EEG or fMRI recordings. 1</p><p>2 0.74728471 <a title="57-lda-2" href="./nips-2007-Scan_Strategies_for_Meteorological_Radars.html">171 nips-2007-Scan Strategies for Meteorological Radars</a></p>
<p>Author: Victoria Manfredi, Jim Kurose</p><p>Abstract: We address the problem of adaptive sensor control in dynamic resourceconstrained sensor networks. We focus on a meteorological sensing network comprising radars that can perform sector scanning rather than always scanning 360◦ . We compare three sector scanning strategies. The sit-and-spin strategy always scans 360◦ . The limited lookahead strategy additionally uses the expected environmental state K decision epochs in the future, as predicted from Kalman ﬁlters, in its decision-making. The full lookahead strategy uses all expected future states by casting the problem as a Markov decision process and using reinforcement learning to estimate the optimal scan strategy. We show that the main beneﬁts of using a lookahead strategy are when there are multiple meteorological phenomena in the environment, and when the maximum radius of any phenomenon is sufﬁciently smaller than the radius of the radars. We also show that there is a trade-off between the average quality with which a phenomenon is scanned and the number of decision epochs before which a phenomenon is rescanned. 1</p><p>3 0.35139012 <a title="57-lda-3" href="./nips-2007-EEG-Based_Brain-Computer_Interaction%3A_Improved_Accuracy_by_Automatic_Single-Trial_Error_Detection.html">74 nips-2007-EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection</a></p>
<p>Author: Pierre Ferrez, José Millán</p><p>Abstract: Brain-computer interfaces (BCIs), as any other interaction modality based on physiological signals and body channels (e.g., muscular activity, speech and gestures), are prone to errors in the recognition of subject’s intent. An elegant approach to improve the accuracy of BCIs consists in a veriﬁcation procedure directly based on the presence of error-related potentials (ErrP) in the EEG recorded right after the occurrence of an error. Six healthy volunteer subjects with no prior BCI experience participated in a new human-robot interaction experiment where they were asked to mentally move a cursor towards a target that can be reached within a few steps using motor imagination. This experiment conﬁrms the previously reported presence of a new kind of ErrP. These “Interaction ErrP” exhibit a ﬁrst sharp negative peak followed by a positive peak and a second broader negative peak (∼290, ∼350 and ∼470 ms after the feedback, respectively). But in order to exploit these ErrP we need to detect them in each single trial using a short window following the feedback associated to the response of the classiﬁer embedded in the BCI. We have achieved an average recognition rate of correct and erroneous single trials of 81.8% and 76.2%, respectively. Furthermore, we have achieved an average recognition rate of the subject’s intent while trying to mentally drive the cursor of 73.1%. These results show that it’s possible to simultaneously extract useful information for mental control to operate a brain-actuated device as well as cognitive states such as error potentials to improve the quality of the braincomputer interaction. Finally, using a well-known inverse model (sLORETA), we show that the main focus of activity at the occurrence of the ErrP are, as expected, in the pre-supplementary motor area and in the anterior cingulate cortex. 1</p><p>4 0.35087919 <a title="57-lda-4" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>Author: John Wright, Yangyu Tao, Zhouchen Lin, Yi Ma, Heung-yeung Shum</p><p>Abstract: We present a simple new criterion for classiﬁcation, based on principles from lossy data compression. The criterion assigns a test sample to the class that uses the minimum number of additional bits to code the test sample, subject to an allowable distortion. We prove asymptotic optimality of this criterion for Gaussian data and analyze its relationships to classical classiﬁers. Theoretical results provide new insights into relationships among popular classiﬁers such as MAP and RDA, as well as unsupervised clustering methods based on lossy compression [13]. Minimizing the lossy coding length induces a regularization effect which stabilizes the (implicit) density estimate in a small-sample setting. Compression also provides a uniform means of handling classes of varying dimension. This simple classiﬁcation criterion and its kernel and local versions perform competitively against existing classiﬁers on both synthetic examples and real imagery data such as handwritten digits and human faces, without requiring domain-speciﬁc information. 1</p><p>5 0.34965959 <a title="57-lda-5" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>Author: Marc'aurelio Ranzato, Y-lan Boureau, Yann L. Cun</p><p>Abstract: Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation. We describe a novel and efﬁcient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machine trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input observed variables can be captured. 1</p><p>6 0.34803703 <a title="57-lda-6" href="./nips-2007-Distributed_Inference_for_Latent_Dirichlet_Allocation.html">73 nips-2007-Distributed Inference for Latent Dirichlet Allocation</a></p>
<p>7 0.34661293 <a title="57-lda-7" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>8 0.34654844 <a title="57-lda-8" href="./nips-2007-Density_Estimation_under_Independent_Similarly_Distributed_Sampling_Assumptions.html">66 nips-2007-Density Estimation under Independent Similarly Distributed Sampling Assumptions</a></p>
<p>9 0.3463563 <a title="57-lda-9" href="./nips-2007-Supervised_Topic_Models.html">189 nips-2007-Supervised Topic Models</a></p>
<p>10 0.34611461 <a title="57-lda-10" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>11 0.34593368 <a title="57-lda-11" href="./nips-2007-Collapsed_Variational_Inference_for_HDP.html">47 nips-2007-Collapsed Variational Inference for HDP</a></p>
<p>12 0.34573811 <a title="57-lda-12" href="./nips-2007-People_Tracking_with_the_Laplacian_Eigenmaps_Latent_Variable_Model.html">153 nips-2007-People Tracking with the Laplacian Eigenmaps Latent Variable Model</a></p>
<p>13 0.34512416 <a title="57-lda-13" href="./nips-2007-Catching_Change-points_with_Lasso.html">43 nips-2007-Catching Change-points with Lasso</a></p>
<p>14 0.34411389 <a title="57-lda-14" href="./nips-2007-The_discriminant_center-surround_hypothesis_for_bottom-up_saliency.html">202 nips-2007-The discriminant center-surround hypothesis for bottom-up saliency</a></p>
<p>15 0.34388572 <a title="57-lda-15" href="./nips-2007-Locality_and_low-dimensions_in_the_prediction_of_natural_experience_from_fMRI.html">122 nips-2007-Locality and low-dimensions in the prediction of natural experience from fMRI</a></p>
<p>16 0.34378973 <a title="57-lda-16" href="./nips-2007-Predicting_Brain_States_from_fMRI_Data%3A_Incremental_Functional_Principal_Component_Regression.html">154 nips-2007-Predicting Brain States from fMRI Data: Incremental Functional Principal Component Regression</a></p>
<p>17 0.34378111 <a title="57-lda-17" href="./nips-2007-Colored_Maximum_Variance_Unfolding.html">49 nips-2007-Colored Maximum Variance Unfolding</a></p>
<p>18 0.34329835 <a title="57-lda-18" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>19 0.34326071 <a title="57-lda-19" href="./nips-2007-Predictive_Matrix-Variate_t_Models.html">156 nips-2007-Predictive Matrix-Variate t Models</a></p>
<p>20 0.34250778 <a title="57-lda-20" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
