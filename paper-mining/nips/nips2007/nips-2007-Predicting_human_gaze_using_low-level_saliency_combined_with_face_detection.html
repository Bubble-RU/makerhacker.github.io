<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>155 nips-2007-Predicting human gaze using low-level saliency combined with face detection</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-155" href="#">nips2007-155</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>155 nips-2007-Predicting human gaze using low-level saliency combined with face detection</h1>
<br/><p>Source: <a title="nips-2007-155-pdf" href="http://papers.nips.cc/paper/3169-predicting-human-gaze-using-low-level-saliency-combined-with-face-detection.pdf">pdf</a></p><p>Author: Moran Cerf, Jonathan Harel, Wolfgang Einhaeuser, Christof Koch</p><p>Abstract: Under natural viewing conditions, human observers shift their gaze to allocate processing resources to subsets of the visual input. Many computational models try to predict such voluntary eye and attentional shifts. Although the important role of high level stimulus properties (e.g., semantic information) in search stands undisputed, most models are based on low-level image properties. We here demonstrate that a combined model of face detection and low-level saliency signiﬁcantly outperforms a low-level model in predicting locations humans ﬁxate on, based on eye-movement recordings of humans observing photographs of natural scenes, most of which contained at least one person. Observers, even when not instructed to look for anything particular, ﬁxate on a face with a probability of over 80% within their ﬁrst two ﬁxations; furthermore, they exhibit more similar scanpaths when faces are present. Remarkably, our model’s predictive performance in images that do not contain faces is not impaired, and is even improved in some cases by spurious face detector responses. 1</p><p>Reference: <a title="nips-2007-155-reference" href="../nips2007_reference/nips-2007-Predicting_human_gaze_using_low-level_saliency_combined_with_face_detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Predicting human gaze using low-level saliency combined with face detection Jonathan Harel Electrical Engineering California Institute of Technology Pasadena, CA 91125 harel@klab. [sent-1, score-0.973]
</p><p>2 edu  Abstract Under natural viewing conditions, human observers shift their gaze to allocate processing resources to subsets of the visual input. [sent-10, score-0.227]
</p><p>3 We here demonstrate that a combined model of face detection and low-level saliency signiﬁcantly outperforms a low-level model in predicting locations humans ﬁxate on, based on eye-movement recordings of humans observing photographs of natural scenes, most of which contained at least one person. [sent-15, score-0.916]
</p><p>4 Observers, even when not instructed to look for anything particular, ﬁxate on a face with a probability of over 80% within their ﬁrst two ﬁxations; furthermore, they exhibit more similar scanpaths when faces are present. [sent-16, score-0.74]
</p><p>5 Remarkably, our model’s predictive performance in images that do not contain faces is not impaired, and is even improved in some cases by spurious face detector responses. [sent-17, score-0.826]
</p><p>6 One accessible correlate of human attention is the ﬁxation pattern in scanpaths [1], which has long been of interest to the vision community [2]. [sent-19, score-0.226]
</p><p>7 Such a bottom-up saliency model works well when higher order semantics are reﬂected in low-level features (as is often the case for isolated objects, and even for reasonably cluttered scenes), but tends to fail if other factors dominate: e. [sent-25, score-0.456]
</p><p>8 , in search tasks [7, 8], strong contextual effects [9], or in free-viewing of images without clearly isolated objects, such as 1  forest scenes or foliage [10]. [sent-27, score-0.231]
</p><p>9 Here, we test how images containing faces - ecologically highly relevant objects - inﬂuence variability of scanpaths across subjects. [sent-28, score-0.549]
</p><p>10 In a second step, we improve the standard saliency model by adding a “face channel” based on an established face detector algorithm. [sent-29, score-0.857]
</p><p>11 Although there is an ongoing debate regarding the exact mechanisms which underlie face detection, there is no argument that a normal subject (in contrast to autistic patients) will not interpret a face purely as a reddish blob with four lines, but as a much more signiﬁcant entity ([11, 12]. [sent-30, score-0.703]
</p><p>12 In fact, there is mounting evidence of infants’ preference for face-like patterns before they can even consciously perceive the category of faces [13], which is crucial for emotion and social processing ([13, 14, 15, 16]). [sent-31, score-0.315]
</p><p>13 There are numerous computer-vision models for face detection with good results ([17, 18, 19, 20]). [sent-33, score-0.441]
</p><p>14 One widely used model for face recognition is the Viola & Jones [21] feature-based template matching algorithm (VJ). [sent-34, score-0.348]
</p><p>15 There have been previous attempts to incorporate face detection into a saliency model. [sent-35, score-0.874]
</p><p>16 However, they have either relied on biasing a color channel toward skin hue [22] - and thus being ineffective in many cases nor being face-selective per se - or they have suffered from lack of generality [23]. [sent-36, score-0.136]
</p><p>17 We here propose a system which combines the bottom-up saliency map model of Itti et al. [sent-37, score-0.533]
</p><p>18 The contributions of this study are: (1) Experimental data showing that subjects exhibit signiﬁcantly less variable scanpaths when viewing natural images containing faces, marked by a strong tendency to ﬁxate on faces early. [sent-39, score-0.711]
</p><p>19 (2) A novel saliency model which combines a face detector with intensity, color, and orientation information. [sent-40, score-0.899]
</p><p>20 (3) Quantitative results on two versions of this saliency model, including one extended from a recent graph-based approach, which show that, compared to previous approaches, it better predicts subjects’ ﬁxations on images with faces, and predicts as well otherwise. [sent-41, score-0.616]
</p><p>21 1  Methods Experimental procedures  Seven subjects viewed a set of 250 images (1024 × 768 pixels) in a three phase experiment. [sent-43, score-0.315]
</p><p>22 200 of the images included frontal faces of various people; 50 images contained no faces but were otherwise identical, allowing a comparison of viewing a particular scene with and without a face. [sent-44, score-0.963]
</p><p>23 In the ﬁrst (“free-viewing”) phase of the experiment, 200 of these images (the same subset for each subject) were presented to subjects for 2 s, after which they were instructed to answer “How interesting was the image? [sent-45, score-0.344]
</p><p>24 In the second (“search”) phase, subjects viewed another 200 image subset in the same setup, only this time they were initially presented with a probe image (either a face, or an object in the scene: banana, cell phone, toy car, etc. [sent-48, score-0.344]
</p><p>25 ) for 600 ms after which one of the 200 images appeared for 2 s. [sent-49, score-0.179]
</p><p>26 We used the second task to test if there are any differences in the ﬁxation orders and viewing patterns between freeviewing and task-dependent viewing of images with faces. [sent-54, score-0.303]
</p><p>27 In the third phase, subjects performed a 100 images recognition memory task where they had to answer with y/n whether they had seen the image before. [sent-55, score-0.381]
</p><p>28 50 of the images were taken from the experimental set and 50 were new. [sent-56, score-0.16]
</p><p>29 The images were introduced as “regular images that one can expect to ﬁnd in an everyday personal photo album”. [sent-59, score-0.32]
</p><p>30 Scenes were indoors and outdoors still images (see examples in Fig. [sent-60, score-0.16]
</p><p>31 Images included faces in various skin colors, age groups, and positions (no image had the face at the center as this was the starting ﬁxation location in all trials). [sent-62, score-0.691]
</p><p>32 A few images had face-like objects (see balloon in Fig. [sent-63, score-0.186]
</p><p>33 1, panel 3), animal faces, and objects that had irregular faces in them (masks, the Egyptian sphinx face, etc. [sent-64, score-0.291]
</p><p>34 ) of the entire image - between 1◦ to 5◦ of the visual ﬁeld; we also varied the number of faces in the image between 1-6, with a mean of 1. [sent-69, score-0.47]
</p><p>35 Image order was randomized throughout, and subjects were na¨ve to the purpose of the experiment. [sent-72, score-0.127]
</p><p>36 The images were presented on a CRT 2  screen (120 Hz), using Matlab’s Psychophysics and eyelink toolbox extensions ([25, 26]). [sent-75, score-0.241]
</p><p>37 The distance between the screen and the subject was 80 cm, giving a total visual angle for each image of 28◦ × 21◦ . [sent-77, score-0.178]
</p><p>38 The trend of visiting the faces ﬁrst - typically within the 1st or 2nd ﬁxation - is evident. [sent-86, score-0.29]
</p><p>39 2  Combining face detection with various saliency algorithms  We tried to predict the attentional allocation via ﬁxation patterns of the subjects using various saliency maps. [sent-92, score-1.551]
</p><p>40 Each saliency map was represented as a positive valued heat map over the image plane. [sent-94, score-0.684]
</p><p>41 This promotes feature maps with one conspicuous location to the detriment of maps presenting numerous conspicuous locations. [sent-97, score-0.177]
</p><p>42 The graphbased saliency map model (GBSM) employs spectral techniques in lieu of center surround subtraction and “Maxnorm” normalization, using only local computations. [sent-98, score-0.533]
</p><p>43 For face detection, we used the Intel Open Source Computer Vision Library (“OpenCV”) [28] implementation of [21]. [sent-100, score-0.328]
</p><p>44 This implementation rapidly processes images while achieving high detection rates. [sent-101, score-0.25]
</p><p>45 We used it to form a “Faces conspicuity map”, or “Face channel” 3  by convolving delta functions at the (x,y) detected facial centers with 2D Gaussians having standard deviation equal to estimated facial radius. [sent-109, score-0.142]
</p><p>46 Face detection  Color  Intensity  Orientation  False  False positive  Saliency Map with face  detection  Saliency Map  Figure 2: Modiﬁed saliency model. [sent-113, score-0.456]
</p><p>47 An image is processed through standard [5] color, orientation and intensity multi-scale channels, as well as through a trained template-matching face detection mechanism. [sent-114, score-0.577]
</p><p>48 Face coordinates and radius from the face detector are used to form a face conspicuity map (F), with peaks at facial centers. [sent-115, score-0.912]
</p><p>49 All four maps are normalized to the same dynamic range, and added with equal weights to a ﬁnal saliency map (SM+VJ, or GBSM+VJ). [sent-116, score-0.586]
</p><p>50 This is compared to a saliency map which only uses the three bottom-up features maps (SM or GBSM). [sent-117, score-0.586]
</p><p>51 1  Results Psychophysical results  To evaluate the results of the 7 subjects’ viewing of the images, we manually deﬁned minimally sized rectangular regions-of-interest (ROIs) around each face in the entire image collection. [sent-119, score-0.463]
</p><p>52 In 972 out of the 1050 (7 subjects x 150 images with faces) trials (92. [sent-121, score-0.334]
</p><p>53 4%) trials, a 4  face was ﬁxated on within the ﬁrst ﬁxation, and of the remaining 405 trials, a face was ﬁxated on in the second ﬁxation in 71. [sent-124, score-0.656]
</p><p>54 Given that the face ROIs were chosen very conservatively (i. [sent-130, score-0.328]
</p><p>55 ﬁxations just next to a face do not count as ﬁxations on the face), this shows that faces, if present, are typically ﬁxated on within the ﬁrst two ﬁxations (327 ms ± 95 ms on average). [sent-132, score-0.366]
</p><p>56 Furthermore, in addition to ﬁnding early ﬁxations on faces, we found that inter-subject scanpath consistency on images with faces was higher. [sent-133, score-0.499]
</p><p>57 24 pixels on images without faces (different with p < 10−6 ). [sent-136, score-0.457]
</p><p>58 The null hypothesis that we would see the same fraction of ﬁrst ﬁxations on a face at random is rejected at p < 10−20 (t-test). [sent-140, score-0.353]
</p><p>59 To test for the hypothesis that face saliency is not due to top-down preference for faces in the absence of other interesting things, we examined the results of the “search” task, in which subjects were presented with a non-face target probe in 50% of the trials. [sent-141, score-1.22]
</p><p>60 Provided the short amount of time for the search (2 s), subjects should have attempted to tune their internal saliency weights to adjust color, intensity, and orientation optimally for the searched target [30]. [sent-142, score-0.656]
</p><p>61 Nevertheless, subjects still tended to ﬁxate on the faces early. [sent-143, score-0.392]
</p><p>62 A face was ﬁxated on within the ﬁrst ﬁxation in 24% of trials, within the ﬁrst two ﬁxations in 52% of trials, and within the three ﬁxations in 77% of the trials. [sent-144, score-0.328]
</p><p>63 Overall, we found that in both experimental conditions (“free-viewing” and “search”), faces were powerful attractors of attention, accounting for a strong majority of early ﬁxations when present. [sent-147, score-0.29]
</p><p>64 This trend allowed us to easily improve standard saliency models, as discussed below. [sent-148, score-0.481]
</p><p>65 Figure 3: Extent of ﬁxation on face regions-of-interest (ROIs) during the “free-viewing” phase . [sent-149, score-0.356]
</p><p>66 Right: Bars depict percentage of trials, which reach a face the ﬁrst time in the ﬁrst, second, third, . [sent-152, score-0.356]
</p><p>67 the fraction of trials in which faces were ﬁxated on at least once up to and including the nth ﬁxation. [sent-158, score-0.337]
</p><p>68 2  Assessing the saliency map models  We ran VJ on each of the 200 images used in the free viewing task, and found at least one face detection on 176 of these images, 148 of which actually contained faces (only two images with faces were missed). [sent-160, score-1.882]
</p><p>69 For each of these 176 images, we computed four saliency maps (SM, GBSM, SM+VJ, GBSM+VJ) as discussed above, and quantiﬁed the compatibility of each with our scanpath recordings, in particular ﬁxations, using the area under an ROC curve. [sent-161, score-0.558]
</p><p>70 The ROC curves were generated by sweeping over saliency value thresholds, and treating the fraction of non-ﬁxated pixels 5  on a map above threshold as false alarms, and the fraction of ﬁxated pixels above threshold as hits [29, 31]. [sent-162, score-0.687]
</p><p>71 4, all models predict above chance (50%): SM performs worst, and GBSM+VJ best, since including the face detector substantially improves performance in both cases. [sent-164, score-0.42]
</p><p>72 Subjects’ scanpaths shown on the left panels of ﬁgure 1). [sent-166, score-0.125]
</p><p>73 Top panel: image with the 49 ﬁxations of the 7 subjects (red). [sent-167, score-0.201]
</p><p>74 From left to right, saliency map model of Itti et al. [sent-169, score-0.533]
</p><p>75 (SM), saliency map with the VJ face detection map (SM+VJ), the graph-based saliency map (GBSM), and the graph-based saliency map with face detection channel (GBSM+VJ). [sent-170, score-2.582]
</p><p>76 5): ﬁrst, all models perform better than chance, even over the 28 images without faces. [sent-175, score-0.16]
</p><p>77 For the 148/176 images with faces, SM+VJ was better than SM alone for 144/148 images (p < 10−29 ), whereas VJ alone (equal to the face conspicuity map) was better than SM alone for 83/148 images, a fraction that fails to reach signiﬁcance. [sent-180, score-0.743]
</p><p>78 Thus, although the face conspicuity map was surprisingly predictive on its own, ﬁxation predictions were much better when it was combined with the full saliency model. [sent-181, score-0.953]
</p><p>79 For the 28 images without faces, SM (better than SM+VJ for 18) and SM+VJ (better than SM for 10) did not show a signiﬁcant difference, nor did GBSM vs. [sent-182, score-0.16]
</p><p>80 However, in a recent follow-up study with more non-face images, we found preliminary results indicating that the mean ROC score of VJ-enhanced saliency maps is higher on such non-face images, although the median is slightly lower, i. [sent-184, score-0.509]
</p><p>81 performance is much improved when improved at all indicating that VJ false positives can sometimes enhance saliency maps. [sent-186, score-0.521]
</p><p>82 In summary, we found that adding a face detector channel improves ﬁxation prediction in images with faces dramatically, while it does not impair prediction in images without faces, even though the face detector has false alarms in those cases. [sent-187, score-1.521]
</p><p>83 4  Discussion  First, we demonstrated that in natural scenes containing frontal shots of people, faces were ﬁxated on within the ﬁrst few ﬁxations, whether subjects had to grade an image on interest value or search it for a speciﬁc possibly non-face target. [sent-188, score-0.537]
</p><p>84 This powerful trend motivated the introduction of a new saliency 6  **  ***  * 2 14  70  4 15  60  34  22 60  0  0 1  60  0  0 0. [sent-189, score-0.481]
</p><p>85 Scatterplots depict the area under ROC curves (AUC) for the 176 images in which VJ found a face. [sent-210, score-0.188]
</p><p>86 Points above the diagonal indicate better prediction of the model including face detection compared to the models without face channel. [sent-212, score-0.746]
</p><p>87 Blue markers denote images with faces; red markers images without faces (i. [sent-213, score-0.649]
</p><p>88 In attempting to predict the ﬁxations of human subjects, we found that this additional face channel improved the performance of both a standard and a more recent graph-based saliency model (almost all blue points in Fig. [sent-220, score-0.879]
</p><p>89 In the few images without faces, we found that the false positives represented in the face-detection channel did not signiﬁcantly alter the performance of the saliency maps – although in a preliminary follow-up on a larger image pool we found that they boost mean performance. [sent-222, score-0.878]
</p><p>90 Together, these ﬁndings point towards a specialized “face channel” in our vision system, which is subject to current debate in the attention literature [11, 12, 32]. [sent-223, score-0.15]
</p><p>91 This suggests that faces always attract attention and gaze, relatively independent of the task. [sent-225, score-0.316]
</p><p>92 They should therefore be considered as part of the bottom-up saliency pathway. [sent-226, score-0.456]
</p><p>93 A model of saliency-based visual attention for rapid scene analysis. [sent-261, score-0.14]
</p><p>94 The role of top-down and bottom-up processes in guiding eye movements during visual search. [sent-284, score-0.149]
</p><p>95 Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search. [sent-297, score-0.168]
</p><p>96 Does luminance-contrast contribute to a saliency map for overt visual attena o tion? [sent-302, score-0.59]
</p><p>97 Statistical method for 3 D object detection applied to faces and cars. [sent-352, score-0.38]
</p><p>98 Rapid object detection using a boosted cascade of simple features. [sent-370, score-0.138]
</p><p>99 Interactions of visual attention and object recognition: computational modeling, algorithms, and psychophysics. [sent-374, score-0.133]
</p><p>100 With a careful look: Still no low-level confound to face pop-out Authors’ reply. [sent-440, score-0.328]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('saliency', 0.456), ('gbsm', 0.349), ('face', 0.328), ('xations', 0.289), ('faces', 0.265), ('sm', 0.257), ('vj', 0.24), ('xation', 0.178), ('images', 0.16), ('xated', 0.154), ('subjects', 0.127), ('scanpaths', 0.098), ('detection', 0.09), ('map', 0.077), ('res', 0.075), ('image', 0.074), ('detector', 0.073), ('conspicuity', 0.07), ('channel', 0.07), ('eye', 0.062), ('viewing', 0.061), ('visual', 0.057), ('rois', 0.056), ('xate', 0.056), ('maps', 0.053), ('vision', 0.052), ('gaze', 0.052), ('itti', 0.052), ('attention', 0.051), ('scanpath', 0.049), ('trials', 0.047), ('roc', 0.045), ('auc', 0.045), ('viola', 0.044), ('attentional', 0.044), ('probe', 0.044), ('intensity', 0.043), ('color', 0.042), ('orientation', 0.042), ('eyelink', 0.042), ('false', 0.04), ('scenes', 0.04), ('koch', 0.039), ('facial', 0.036), ('jones', 0.034), ('vis', 0.033), ('harel', 0.033), ('scene', 0.032), ('observers', 0.032), ('pixels', 0.032), ('pasadena', 0.031), ('search', 0.031), ('movements', 0.03), ('instructed', 0.029), ('social', 0.029), ('allocation', 0.029), ('cerf', 0.028), ('einh', 0.028), ('hershler', 0.028), ('psychophysics', 0.028), ('depict', 0.028), ('phase', 0.028), ('technology', 0.027), ('panels', 0.027), ('subject', 0.027), ('objects', 0.026), ('peters', 0.026), ('trend', 0.025), ('early', 0.025), ('fraction', 0.025), ('human', 0.025), ('positives', 0.025), ('object', 0.025), ('tunes', 0.024), ('skin', 0.024), ('alarms', 0.024), ('conspicuous', 0.024), ('maxnorm', 0.024), ('moran', 0.024), ('red', 0.024), ('people', 0.023), ('numerous', 0.023), ('cascade', 0.023), ('castelhano', 0.022), ('navalpakkam', 0.022), ('combined', 0.022), ('patterns', 0.021), ('recognition', 0.02), ('institute', 0.02), ('contained', 0.02), ('debate', 0.02), ('markers', 0.02), ('screen', 0.02), ('look', 0.02), ('california', 0.02), ('ms', 0.019), ('chance', 0.019), ('understanding', 0.019), ('toolbox', 0.019), ('rectangles', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999863 <a title="155-tfidf-1" href="./nips-2007-Predicting_human_gaze_using_low-level_saliency_combined_with_face_detection.html">155 nips-2007-Predicting human gaze using low-level saliency combined with face detection</a></p>
<p>Author: Moran Cerf, Jonathan Harel, Wolfgang Einhaeuser, Christof Koch</p><p>Abstract: Under natural viewing conditions, human observers shift their gaze to allocate processing resources to subsets of the visual input. Many computational models try to predict such voluntary eye and attentional shifts. Although the important role of high level stimulus properties (e.g., semantic information) in search stands undisputed, most models are based on low-level image properties. We here demonstrate that a combined model of face detection and low-level saliency signiﬁcantly outperforms a low-level model in predicting locations humans ﬁxate on, based on eye-movement recordings of humans observing photographs of natural scenes, most of which contained at least one person. Observers, even when not instructed to look for anything particular, ﬁxate on a face with a probability of over 80% within their ﬁrst two ﬁxations; furthermore, they exhibit more similar scanpaths when faces are present. Remarkably, our model’s predictive performance in images that do not contain faces is not impaired, and is even improved in some cases by spurious face detector responses. 1</p><p>2 0.47753453 <a title="155-tfidf-2" href="./nips-2007-The_discriminant_center-surround_hypothesis_for_bottom-up_saliency.html">202 nips-2007-The discriminant center-surround hypothesis for bottom-up saliency</a></p>
<p>Author: Dashan Gao, Vijay Mahadevan, Nuno Vasconcelos</p><p>Abstract: The classical hypothesis, that bottom-up saliency is a center-surround process, is combined with a more recent hypothesis that all saliency decisions are optimal in a decision-theoretic sense. The combined hypothesis is denoted as discriminant center-surround saliency, and the corresponding optimal saliency architecture is derived. This architecture equates the saliency of each image location to the discriminant power of a set of features with respect to the classiﬁcation problem that opposes stimuli at center and surround, at that location. It is shown that the resulting saliency detector makes accurate quantitative predictions for various aspects of the psychophysics of human saliency, including non-linear properties beyond the reach of previous saliency models. Furthermore, it is shown that discriminant center-surround saliency can be easily generalized to various stimulus modalities (such as color, orientation and motion), and provides optimal solutions for many other saliency problems of interest for computer vision. Optimal solutions, under this hypothesis, are derived for a number of the former (including static natural images, dense motion ﬁelds, and even dynamic textures), and applied to a number of the latter (the prediction of human eye ﬁxations, motion-based saliency in the presence of ego-motion, and motion-based saliency in the presence of highly dynamic backgrounds). In result, discriminant saliency is shown to predict eye ﬁxations better than previous models, and produces background subtraction algorithms that outperform the state-of-the-art in computer vision. 1</p><p>3 0.1578909 <a title="155-tfidf-3" href="./nips-2007-Experience-Guided_Search%3A_A_Theory_of_Attentional_Control.html">85 nips-2007-Experience-Guided Search: A Theory of Attentional Control</a></p>
<p>Author: David Baldwin, Michael C. Mozer</p><p>Abstract: People perform a remarkable range of tasks that require search of the visual environment for a target item among distractors. The Guided Search model (Wolfe, 1994, 2007), or GS, is perhaps the best developed psychological account of human visual search. To prioritize search, GS assigns saliency to locations in the visual ﬁeld. Saliency is a linear combination of activations from retinotopic maps representing primitive visual features. GS includes heuristics for setting the gain coefﬁcient associated with each map. Variants of GS have formalized the notion of optimization as a principle of attentional control (e.g., Baldwin & Mozer, 2006; Cave, 1999; Navalpakkam & Itti, 2006; Rao et al., 2002), but every GS-like model must be ’dumbed down’ to match human data, e.g., by corrupting the saliency map with noise and by imposing arbitrary restrictions on gain modulation. We propose a principled probabilistic formulation of GS, called Experience-Guided Search (EGS), based on a generative model of the environment that makes three claims: (1) Feature detectors produce Poisson spike trains whose rates are conditioned on feature type and whether the feature belongs to a target or distractor; (2) the environment and/or task is nonstationary and can change over a sequence of trials; and (3) a prior speciﬁes that features are more likely to be present for target than for distractors. Through experience, EGS infers latent environment variables that determine the gains for guiding search. Control is thus cast as probabilistic inference, not optimization. We show that EGS can replicate a range of human data from visual search, including data that GS does not address. 1</p><p>4 0.14351042 <a title="155-tfidf-4" href="./nips-2007-Kernels_on_Attributed_Pointsets_with_Applications.html">109 nips-2007-Kernels on Attributed Pointsets with Applications</a></p>
<p>Author: Mehul Parsana, Sourangshu Bhattacharya, Chiru Bhattacharya, K. Ramakrishnan</p><p>Abstract: This paper introduces kernels on attributed pointsets, which are sets of vectors embedded in an euclidean space. The embedding gives the notion of neighborhood, which is used to deﬁne positive semideﬁnite kernels on pointsets. Two novel kernels on neighborhoods are proposed, one evaluating the attribute similarity and the other evaluating shape similarity. Shape similarity function is motivated from spectral graph matching techniques. The kernels are tested on three real life applications: face recognition, photo album tagging, and shot annotation in video sequences, with encouraging results. 1</p><p>5 0.13455682 <a title="155-tfidf-5" href="./nips-2007-Multiple-Instance_Pruning_For_Learning_Efficient_Cascade_Detectors.html">137 nips-2007-Multiple-Instance Pruning For Learning Efficient Cascade Detectors</a></p>
<p>Author: Cha Zhang, Paul A. Viola</p><p>Abstract: Cascade detectors have been shown to operate extremely rapidly, with high accuracy, and have important applications such as face detection. Driven by this success, cascade learning has been an area of active research in recent years. Nevertheless, there are still challenging technical problems during the training process of cascade detectors. In particular, determining the optimal target detection rate for each stage of the cascade remains an unsolved issue. In this paper, we propose the multiple instance pruning (MIP) algorithm for soft cascades. This algorithm computes a set of thresholds which aggressively terminate computation with no reduction in detection rate or increase in false positive rate on the training dataset. The algorithm is based on two key insights: i) examples that are destined to be rejected by the complete classiﬁer can be safely pruned early; ii) face detection is a multiple instance learning problem. The MIP process is fully automatic and requires no assumptions of probability distributions, statistical independence, or ad hoc intermediate rejection targets. Experimental results on the MIT+CMU dataset demonstrate signiﬁcant performance advantages. 1</p><p>6 0.10966396 <a title="155-tfidf-6" href="./nips-2007-Object_Recognition_by_Scene_Alignment.html">143 nips-2007-Object Recognition by Scene Alignment</a></p>
<p>7 0.10039869 <a title="155-tfidf-7" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>8 0.09410838 <a title="155-tfidf-8" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>9 0.083443619 <a title="155-tfidf-9" href="./nips-2007-Congruence_between_model_and_human_attention_reveals_unique_signatures_of_critical_visual_events.html">57 nips-2007-Congruence between model and human attention reveals unique signatures of critical visual events</a></p>
<p>10 0.080249064 <a title="155-tfidf-10" href="./nips-2007-Subspace-Based_Face_Recognition_in_Analog_VLSI.html">188 nips-2007-Subspace-Based Face Recognition in Analog VLSI</a></p>
<p>11 0.070634812 <a title="155-tfidf-11" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>12 0.067244165 <a title="155-tfidf-12" href="./nips-2007-Configuration_Estimates_Improve_Pedestrian_Finding.html">56 nips-2007-Configuration Estimates Improve Pedestrian Finding</a></p>
<p>13 0.066908158 <a title="155-tfidf-13" href="./nips-2007-Locality_and_low-dimensions_in_the_prediction_of_natural_experience_from_fMRI.html">122 nips-2007-Locality and low-dimensions in the prediction of natural experience from fMRI</a></p>
<p>14 0.065839559 <a title="155-tfidf-14" href="./nips-2007-Spatial_Latent_Dirichlet_Allocation.html">183 nips-2007-Spatial Latent Dirichlet Allocation</a></p>
<p>15 0.060127176 <a title="155-tfidf-15" href="./nips-2007-Markov_Chain_Monte_Carlo_with_People.html">125 nips-2007-Markov Chain Monte Carlo with People</a></p>
<p>16 0.059633508 <a title="155-tfidf-16" href="./nips-2007-A_Bayesian_Model_of_Conditioned_Perception.html">3 nips-2007-A Bayesian Model of Conditioned Perception</a></p>
<p>17 0.056441788 <a title="155-tfidf-17" href="./nips-2007-EEG-Based_Brain-Computer_Interaction%3A_Improved_Accuracy_by_Automatic_Single-Trial_Error_Detection.html">74 nips-2007-EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection</a></p>
<p>18 0.056190122 <a title="155-tfidf-18" href="./nips-2007-Sparse_Overcomplete_Latent_Variable_Decomposition_of_Counts_Data.html">181 nips-2007-Sparse Overcomplete Latent Variable Decomposition of Counts Data</a></p>
<p>19 0.053804737 <a title="155-tfidf-19" href="./nips-2007-Predicting_Brain_States_from_fMRI_Data%3A_Incremental_Functional_Principal_Component_Regression.html">154 nips-2007-Predicting Brain States from fMRI Data: Incremental Functional Principal Component Regression</a></p>
<p>20 0.053127751 <a title="155-tfidf-20" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.14), (1, 0.097), (2, 0.009), (3, -0.087), (4, 0.047), (5, 0.309), (6, 0.068), (7, 0.31), (8, -0.004), (9, -0.109), (10, -0.132), (11, -0.078), (12, -0.134), (13, 0.16), (14, -0.365), (15, -0.034), (16, 0.083), (17, 0.176), (18, -0.09), (19, -0.013), (20, -0.02), (21, -0.238), (22, -0.108), (23, -0.055), (24, 0.185), (25, 0.04), (26, 0.016), (27, 0.057), (28, -0.083), (29, -0.023), (30, 0.006), (31, -0.0), (32, 0.008), (33, -0.031), (34, -0.062), (35, -0.037), (36, 0.05), (37, 0.015), (38, -0.039), (39, 0.009), (40, -0.007), (41, 0.008), (42, -0.002), (43, -0.015), (44, -0.001), (45, -0.028), (46, 0.029), (47, -0.001), (48, -0.016), (49, -0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96348143 <a title="155-lsi-1" href="./nips-2007-Predicting_human_gaze_using_low-level_saliency_combined_with_face_detection.html">155 nips-2007-Predicting human gaze using low-level saliency combined with face detection</a></p>
<p>Author: Moran Cerf, Jonathan Harel, Wolfgang Einhaeuser, Christof Koch</p><p>Abstract: Under natural viewing conditions, human observers shift their gaze to allocate processing resources to subsets of the visual input. Many computational models try to predict such voluntary eye and attentional shifts. Although the important role of high level stimulus properties (e.g., semantic information) in search stands undisputed, most models are based on low-level image properties. We here demonstrate that a combined model of face detection and low-level saliency signiﬁcantly outperforms a low-level model in predicting locations humans ﬁxate on, based on eye-movement recordings of humans observing photographs of natural scenes, most of which contained at least one person. Observers, even when not instructed to look for anything particular, ﬁxate on a face with a probability of over 80% within their ﬁrst two ﬁxations; furthermore, they exhibit more similar scanpaths when faces are present. Remarkably, our model’s predictive performance in images that do not contain faces is not impaired, and is even improved in some cases by spurious face detector responses. 1</p><p>2 0.95668876 <a title="155-lsi-2" href="./nips-2007-The_discriminant_center-surround_hypothesis_for_bottom-up_saliency.html">202 nips-2007-The discriminant center-surround hypothesis for bottom-up saliency</a></p>
<p>Author: Dashan Gao, Vijay Mahadevan, Nuno Vasconcelos</p><p>Abstract: The classical hypothesis, that bottom-up saliency is a center-surround process, is combined with a more recent hypothesis that all saliency decisions are optimal in a decision-theoretic sense. The combined hypothesis is denoted as discriminant center-surround saliency, and the corresponding optimal saliency architecture is derived. This architecture equates the saliency of each image location to the discriminant power of a set of features with respect to the classiﬁcation problem that opposes stimuli at center and surround, at that location. It is shown that the resulting saliency detector makes accurate quantitative predictions for various aspects of the psychophysics of human saliency, including non-linear properties beyond the reach of previous saliency models. Furthermore, it is shown that discriminant center-surround saliency can be easily generalized to various stimulus modalities (such as color, orientation and motion), and provides optimal solutions for many other saliency problems of interest for computer vision. Optimal solutions, under this hypothesis, are derived for a number of the former (including static natural images, dense motion ﬁelds, and even dynamic textures), and applied to a number of the latter (the prediction of human eye ﬁxations, motion-based saliency in the presence of ego-motion, and motion-based saliency in the presence of highly dynamic backgrounds). In result, discriminant saliency is shown to predict eye ﬁxations better than previous models, and produces background subtraction algorithms that outperform the state-of-the-art in computer vision. 1</p><p>3 0.77651322 <a title="155-lsi-3" href="./nips-2007-Experience-Guided_Search%3A_A_Theory_of_Attentional_Control.html">85 nips-2007-Experience-Guided Search: A Theory of Attentional Control</a></p>
<p>Author: David Baldwin, Michael C. Mozer</p><p>Abstract: People perform a remarkable range of tasks that require search of the visual environment for a target item among distractors. The Guided Search model (Wolfe, 1994, 2007), or GS, is perhaps the best developed psychological account of human visual search. To prioritize search, GS assigns saliency to locations in the visual ﬁeld. Saliency is a linear combination of activations from retinotopic maps representing primitive visual features. GS includes heuristics for setting the gain coefﬁcient associated with each map. Variants of GS have formalized the notion of optimization as a principle of attentional control (e.g., Baldwin & Mozer, 2006; Cave, 1999; Navalpakkam & Itti, 2006; Rao et al., 2002), but every GS-like model must be ’dumbed down’ to match human data, e.g., by corrupting the saliency map with noise and by imposing arbitrary restrictions on gain modulation. We propose a principled probabilistic formulation of GS, called Experience-Guided Search (EGS), based on a generative model of the environment that makes three claims: (1) Feature detectors produce Poisson spike trains whose rates are conditioned on feature type and whether the feature belongs to a target or distractor; (2) the environment and/or task is nonstationary and can change over a sequence of trials; and (3) a prior speciﬁes that features are more likely to be present for target than for distractors. Through experience, EGS infers latent environment variables that determine the gains for guiding search. Control is thus cast as probabilistic inference, not optimization. We show that EGS can replicate a range of human data from visual search, including data that GS does not address. 1</p><p>4 0.41143382 <a title="155-lsi-4" href="./nips-2007-Congruence_between_model_and_human_attention_reveals_unique_signatures_of_critical_visual_events.html">57 nips-2007-Congruence between model and human attention reveals unique signatures of critical visual events</a></p>
<p>Author: Robert Peters, Laurent Itti</p><p>Abstract: Current computational models of bottom-up and top-down components of attention are predictive of eye movements across a range of stimuli and of simple, ﬁxed visual tasks (such as visual search for a target among distractors). However, to date there exists no computational framework which can reliably mimic human gaze behavior in more complex environments and tasks, such as driving a vehicle through traﬃc. Here, we develop a hybrid computational/behavioral framework, combining simple models for bottom-up salience and top-down relevance, and looking for changes in the predictive power of these components at diﬀerent critical event times during 4.7 hours (500,000 video frames) of observers playing car racing and ﬂight combat video games. This approach is motivated by our observation that the predictive strengths of the salience and relevance models exhibit reliable temporal signatures during critical event windows in the task sequence—for example, when the game player directly engages an enemy plane in a ﬂight combat game, the predictive strength of the salience model increases signiﬁcantly, while that of the relevance model decreases signiﬁcantly. Our new framework combines these temporal signatures to implement several event detectors. Critically, we ﬁnd that an event detector based on fused behavioral and stimulus information (in the form of the model’s predictive strength) is much stronger than detectors based on behavioral information alone (eye position) or image information alone (model prediction maps). This approach to event detection, based on eye tracking combined with computational models applied to the visual input, may have useful applications as a less-invasive alternative to other event detection approaches based on neural signatures derived from EEG or fMRI recordings. 1</p><p>5 0.39226133 <a title="155-lsi-5" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>Author: Michael Ross, Andrew Cohen</p><p>Abstract: This paper describes a new model for human visual classiﬁcation that enables the recovery of image features that explain human subjects’ performance on different visual classiﬁcation tasks. Unlike previous methods, this algorithm does not model their performance with a single linear classiﬁer operating on raw image pixels. Instead, it represents classiﬁcation as the combination of multiple feature detectors. This approach extracts more information about human visual classiﬁcation than previous methods and provides a foundation for further exploration. 1</p><p>6 0.33614084 <a title="155-lsi-6" href="./nips-2007-Kernels_on_Attributed_Pointsets_with_Applications.html">109 nips-2007-Kernels on Attributed Pointsets with Applications</a></p>
<p>7 0.32406551 <a title="155-lsi-7" href="./nips-2007-Multiple-Instance_Pruning_For_Learning_Efficient_Cascade_Detectors.html">137 nips-2007-Multiple-Instance Pruning For Learning Efficient Cascade Detectors</a></p>
<p>8 0.28215426 <a title="155-lsi-8" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>9 0.26415846 <a title="155-lsi-9" href="./nips-2007-Subspace-Based_Face_Recognition_in_Analog_VLSI.html">188 nips-2007-Subspace-Based Face Recognition in Analog VLSI</a></p>
<p>10 0.26370612 <a title="155-lsi-10" href="./nips-2007-Object_Recognition_by_Scene_Alignment.html">143 nips-2007-Object Recognition by Scene Alignment</a></p>
<p>11 0.25941178 <a title="155-lsi-11" href="./nips-2007-Configuration_Estimates_Improve_Pedestrian_Finding.html">56 nips-2007-Configuration Estimates Improve Pedestrian Finding</a></p>
<p>12 0.2448193 <a title="155-lsi-12" href="./nips-2007-Learning_Visual_Attributes.html">113 nips-2007-Learning Visual Attributes</a></p>
<p>13 0.22352025 <a title="155-lsi-13" href="./nips-2007-A_Bayesian_Model_of_Conditioned_Perception.html">3 nips-2007-A Bayesian Model of Conditioned Perception</a></p>
<p>14 0.22287799 <a title="155-lsi-14" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>15 0.22118515 <a title="155-lsi-15" href="./nips-2007-Unsupervised_Feature_Selection_for_Accurate_Recommendation_of_High-Dimensional_Image_Data.html">211 nips-2007-Unsupervised Feature Selection for Accurate Recommendation of High-Dimensional Image Data</a></p>
<p>16 0.20132233 <a title="155-lsi-16" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>17 0.19303684 <a title="155-lsi-17" href="./nips-2007-EEG-Based_Brain-Computer_Interaction%3A_Improved_Accuracy_by_Automatic_Single-Trial_Error_Detection.html">74 nips-2007-EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection</a></p>
<p>18 0.1880458 <a title="155-lsi-18" href="./nips-2007-Feature_Selection_Methods_for_Improving_Protein_Structure_Prediction_with_Rosetta.html">89 nips-2007-Feature Selection Methods for Improving Protein Structure Prediction with Rosetta</a></p>
<p>19 0.18312247 <a title="155-lsi-19" href="./nips-2007-The_Distribution_Family_of_Similarity_Distances.html">193 nips-2007-The Distribution Family of Similarity Distances</a></p>
<p>20 0.18095198 <a title="155-lsi-20" href="./nips-2007-Scene_Segmentation_with_CRFs_Learned_from_Partially_Labeled_Images.html">172 nips-2007-Scene Segmentation with CRFs Learned from Partially Labeled Images</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.014), (5, 0.039), (13, 0.03), (16, 0.02), (18, 0.041), (19, 0.026), (21, 0.052), (26, 0.021), (35, 0.028), (47, 0.067), (49, 0.011), (76, 0.011), (82, 0.284), (83, 0.145), (85, 0.019), (87, 0.03), (90, 0.074)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80190653 <a title="155-lda-1" href="./nips-2007-Predicting_human_gaze_using_low-level_saliency_combined_with_face_detection.html">155 nips-2007-Predicting human gaze using low-level saliency combined with face detection</a></p>
<p>Author: Moran Cerf, Jonathan Harel, Wolfgang Einhaeuser, Christof Koch</p><p>Abstract: Under natural viewing conditions, human observers shift their gaze to allocate processing resources to subsets of the visual input. Many computational models try to predict such voluntary eye and attentional shifts. Although the important role of high level stimulus properties (e.g., semantic information) in search stands undisputed, most models are based on low-level image properties. We here demonstrate that a combined model of face detection and low-level saliency signiﬁcantly outperforms a low-level model in predicting locations humans ﬁxate on, based on eye-movement recordings of humans observing photographs of natural scenes, most of which contained at least one person. Observers, even when not instructed to look for anything particular, ﬁxate on a face with a probability of over 80% within their ﬁrst two ﬁxations; furthermore, they exhibit more similar scanpaths when faces are present. Remarkably, our model’s predictive performance in images that do not contain faces is not impaired, and is even improved in some cases by spurious face detector responses. 1</p><p>2 0.69097531 <a title="155-lda-2" href="./nips-2007-Probabilistic_Matrix_Factorization.html">158 nips-2007-Probabilistic Matrix Factorization</a></p>
<p>Author: Andriy Mnih, Ruslan Salakhutdinov</p><p>Abstract: Many existing approaches to collaborative ﬁltering can neither handle very large datasets nor easily deal with users who have very few ratings. In this paper we present the Probabilistic Matrix Factorization (PMF) model which scales linearly with the number of observations and, more importantly, performs well on the large, sparse, and very imbalanced Netﬂix dataset. We further extend the PMF model to include an adaptive prior on the model parameters and show how the model capacity can be controlled automatically. Finally, we introduce a constrained version of the PMF model that is based on the assumption that users who have rated similar sets of movies are likely to have similar preferences. The resulting model is able to generalize considerably better for users with very few ratings. When the predictions of multiple PMF models are linearly combined with the predictions of Restricted Boltzmann Machines models, we achieve an error rate of 0.8861, that is nearly 7% better than the score of Netﬂix’s own system.</p><p>3 0.67544532 <a title="155-lda-3" href="./nips-2007-Expectation_Maximization_and_Posterior_Constraints.html">84 nips-2007-Expectation Maximization and Posterior Constraints</a></p>
<p>Author: Kuzman Ganchev, Ben Taskar, João Gama</p><p>Abstract: The expectation maximization (EM) algorithm is a widely used maximum likelihood estimation procedure for statistical models when the values of some of the variables in the model are not observed. Very often, however, our aim is primarily to ﬁnd a model that assigns values to the latent variables that have intended meaning for our data and maximizing expected likelihood only sometimes accomplishes this. Unfortunately, it is typically difﬁcult to add even simple a-priori information about latent variables in graphical models without making the models overly complex or intractable. In this paper, we present an efﬁcient, principled way to inject rich constraints on the posteriors of latent variables into the EM algorithm. Our method can be used to learn tractable graphical models that satisfy additional, otherwise intractable constraints. Focusing on clustering and the alignment problem for statistical machine translation, we show that simple, intuitive posterior constraints can greatly improve the performance over standard baselines and be competitive with more complex, intractable models. 1</p><p>4 0.55259293 <a title="155-lda-4" href="./nips-2007-The_discriminant_center-surround_hypothesis_for_bottom-up_saliency.html">202 nips-2007-The discriminant center-surround hypothesis for bottom-up saliency</a></p>
<p>Author: Dashan Gao, Vijay Mahadevan, Nuno Vasconcelos</p><p>Abstract: The classical hypothesis, that bottom-up saliency is a center-surround process, is combined with a more recent hypothesis that all saliency decisions are optimal in a decision-theoretic sense. The combined hypothesis is denoted as discriminant center-surround saliency, and the corresponding optimal saliency architecture is derived. This architecture equates the saliency of each image location to the discriminant power of a set of features with respect to the classiﬁcation problem that opposes stimuli at center and surround, at that location. It is shown that the resulting saliency detector makes accurate quantitative predictions for various aspects of the psychophysics of human saliency, including non-linear properties beyond the reach of previous saliency models. Furthermore, it is shown that discriminant center-surround saliency can be easily generalized to various stimulus modalities (such as color, orientation and motion), and provides optimal solutions for many other saliency problems of interest for computer vision. Optimal solutions, under this hypothesis, are derived for a number of the former (including static natural images, dense motion ﬁelds, and even dynamic textures), and applied to a number of the latter (the prediction of human eye ﬁxations, motion-based saliency in the presence of ego-motion, and motion-based saliency in the presence of highly dynamic backgrounds). In result, discriminant saliency is shown to predict eye ﬁxations better than previous models, and produces background subtraction algorithms that outperform the state-of-the-art in computer vision. 1</p><p>5 0.549905 <a title="155-lda-5" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>Author: Yuhong Guo, Dale Schuurmans</p><p>Abstract: We investigate a new, convex relaxation of an expectation-maximization (EM) variant that approximates a standard objective while eliminating local minima. First, a cautionary result is presented, showing that any convex relaxation of EM over hidden variables must give trivial results if any dependence on the missing values is retained. Although this appears to be a strong negative outcome, we then demonstrate how the problem can be bypassed by using equivalence relations instead of value assignments over hidden variables. In particular, we develop new algorithms for estimating exponential conditional models that only require equivalence relation information over the variable values. This reformulation leads to an exact expression for EM variants in a wide range of problems. We then develop a semideﬁnite relaxation that yields global training by eliminating local minima. 1</p><p>6 0.54939044 <a title="155-lda-6" href="./nips-2007-Density_Estimation_under_Independent_Similarly_Distributed_Sampling_Assumptions.html">66 nips-2007-Density Estimation under Independent Similarly Distributed Sampling Assumptions</a></p>
<p>7 0.54801631 <a title="155-lda-7" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>8 0.54594374 <a title="155-lda-8" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>9 0.54516625 <a title="155-lda-9" href="./nips-2007-Markov_Chain_Monte_Carlo_with_People.html">125 nips-2007-Markov Chain Monte Carlo with People</a></p>
<p>10 0.54500866 <a title="155-lda-10" href="./nips-2007-Predictive_Matrix-Variate_t_Models.html">156 nips-2007-Predictive Matrix-Variate t Models</a></p>
<p>11 0.54496408 <a title="155-lda-11" href="./nips-2007-Cluster_Stability_for_Finite_Samples.html">46 nips-2007-Cluster Stability for Finite Samples</a></p>
<p>12 0.543706 <a title="155-lda-12" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>13 0.54212928 <a title="155-lda-13" href="./nips-2007-Distributed_Inference_for_Latent_Dirichlet_Allocation.html">73 nips-2007-Distributed Inference for Latent Dirichlet Allocation</a></p>
<p>14 0.54132301 <a title="155-lda-14" href="./nips-2007-Catching_Change-points_with_Lasso.html">43 nips-2007-Catching Change-points with Lasso</a></p>
<p>15 0.54038656 <a title="155-lda-15" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>16 0.54038173 <a title="155-lda-16" href="./nips-2007-People_Tracking_with_the_Laplacian_Eigenmaps_Latent_Variable_Model.html">153 nips-2007-People Tracking with the Laplacian Eigenmaps Latent Variable Model</a></p>
<p>17 0.54024565 <a title="155-lda-17" href="./nips-2007-Collapsed_Variational_Inference_for_HDP.html">47 nips-2007-Collapsed Variational Inference for HDP</a></p>
<p>18 0.53978956 <a title="155-lda-18" href="./nips-2007-Statistical_Analysis_of_Semi-Supervised_Regression.html">186 nips-2007-Statistical Analysis of Semi-Supervised Regression</a></p>
<p>19 0.53907317 <a title="155-lda-19" href="./nips-2007-Colored_Maximum_Variance_Unfolding.html">49 nips-2007-Colored Maximum Variance Unfolding</a></p>
<p>20 0.53896481 <a title="155-lda-20" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
