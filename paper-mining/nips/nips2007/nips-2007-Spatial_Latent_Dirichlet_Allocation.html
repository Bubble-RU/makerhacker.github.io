<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>183 nips-2007-Spatial Latent Dirichlet Allocation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-183" href="#">nips2007-183</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>183 nips-2007-Spatial Latent Dirichlet Allocation</h1>
<br/><p>Source: <a title="nips-2007-183-pdf" href="http://papers.nips.cc/paper/3278-spatial-latent-dirichlet-allocation.pdf">pdf</a></p><p>Author: Xiaogang Wang, Eric Grimson</p><p>Abstract: In recent years, the language model Latent Dirichlet Allocation (LDA), which clusters co-occurring words into topics, has been widely applied in the computer vision ﬁeld. However, many of these applications have difﬁculty with modeling the spatial and temporal structure among visual words, since LDA assumes that a document is a “bag-of-words”. It is also critical to properly design “words” and “documents” when using a language model to solve vision problems. In this paper, we propose a topic model Spatial Latent Dirichlet Allocation (SLDA), which better encodes spatial structures among visual words that are essential for solving many vision problems. The spatial information is not encoded in the values of visual words but in the design of documents. Instead of knowing the partition of words into documents a priori, the word-document assignment becomes a random hidden variable in SLDA. There is a generative procedure, where knowledge of spatial structure can be ﬂexibly added as a prior, grouping visual words which are close in space into the same document. We use SLDA to discover objects from a collection of images, and show it achieves better performance than LDA. 1</p><p>Reference: <a title="nips-2007-183-reference" href="../nips2007_reference/nips-2007-Spatial_Latent_Dirichlet_Allocation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract In recent years, the language model Latent Dirichlet Allocation (LDA), which clusters co-occurring words into topics, has been widely applied in the computer vision ﬁeld. [sent-5, score-0.27]
</p><p>2 However, many of these applications have difﬁculty with modeling the spatial and temporal structure among visual words, since LDA assumes that a document is a “bag-of-words”. [sent-6, score-0.589]
</p><p>3 In this paper, we propose a topic model Spatial Latent Dirichlet Allocation (SLDA), which better encodes spatial structures among visual words that are essential for solving many vision problems. [sent-8, score-0.706]
</p><p>4 The spatial information is not encoded in the values of visual words but in the design of documents. [sent-9, score-0.423]
</p><p>5 Instead of knowing the partition of words into documents a priori, the word-document assignment becomes a random hidden variable in SLDA. [sent-10, score-0.267]
</p><p>6 There is a generative procedure, where knowledge of spatial structure can be ﬂexibly added as a prior, grouping visual words which are close in space into the same document. [sent-11, score-0.459]
</p><p>7 For example, LDA was used to discover objects from a collection of images [2, 3, 4] and to classify images into different scene categories [5]. [sent-15, score-0.393]
</p><p>8 First, LDA assumes that a document is a bag of words, such that spatial and temporal structures among visual words, which are meaningless in a language model but important in many computer vision problems, are ignored. [sent-20, score-0.678]
</p><p>9 For example, in order to cluster image patches, which are treated as words, into classes of objects, researchers treated images as documents [2]. [sent-23, score-0.496]
</p><p>10 This assumes that if two types of patches are from the same object class, they often appear in the same images. [sent-24, score-0.336]
</p><p>11 As an example shown in Figure 1, even though sky is far from vehicles, if they often exist in the same images in some data set, they would be clustered into the same topic by LDA. [sent-26, score-0.42]
</p><p>12 Furthermore, since in this image most of the patches are sky and building, a patch on a vehicle is likely to be labeled as building or sky as well. [sent-27, score-0.762]
</p><p>13 These problems could be solved if the document of a patch, such as the yellow patch in Figure 1, only includes other 1  Figure 1: There will be some problems (see text) if the whole image is treated as one document when using LDA to discover classes of objects. [sent-28, score-1.056]
</p><p>14 patches falling within its neighborhood, marked by the red dashed window in Figure 1, instead of the whole image. [sent-29, score-0.341]
</p><p>15 So a better assumption is that if two types of image patches are from the same object class, they are not only often in the same images but also close in space. [sent-30, score-0.639]
</p><p>16 We expect to utilize spatial information in a ﬂexible way when designing documents for solving vision problems. [sent-31, score-0.31]
</p><p>17 In this paper, we propose a Spatial Latent Dirichlet Allocation (SLDA) model which encodes the spatial structure among visual words. [sent-32, score-0.282]
</p><p>18 an eye patch and a nose patch), which often occur in the same images and are close in space, into one topic (e. [sent-35, score-0.549]
</p><p>19 However the spatial or temporal information is not encoded in the values of visual words, but in the design of documents. [sent-40, score-0.323]
</p><p>20 LDA and its extensions, such as the author-topic model [8], the dynamic topic model [9], and the correlated topic model [10], all assume that the partition of words into documents is known a priori. [sent-41, score-0.691]
</p><p>21 When visual words are close in space or time, they have a high probability to be grouped into the same document. [sent-44, score-0.298]
</p><p>22 Some approaches such as [11, 3, 12, 4] could also capture some spatial structures among visual words. [sent-45, score-0.282]
</p><p>23 [11] assumed that the spatial distribution of an object class could be modeled as Gaussian and the number of objects in the image was known. [sent-46, score-0.518]
</p><p>24 [12] modeled the spatial dependency among image patches as Markov random ﬁelds. [sent-48, score-0.58]
</p><p>25 And an image usually contains several objects of different classes. [sent-52, score-0.291]
</p><p>26 The goal is to segment objects from images, and at the same time, to label these segments as different object classes in an unsupervised way. [sent-53, score-0.306]
</p><p>27 A local descriptor is computed for each image patch and quantized into a visual word. [sent-56, score-0.582]
</p><p>28 Using topic models, the visual words are clustered into topics which correspond to object classes. [sent-57, score-0.723]
</p><p>29 Thus an image patch can be labeled as one of the object classes. [sent-58, score-0.504]
</p><p>30 Instead of only computing visual words at interest points as in [2], we divide an image into local patches on a grid and densely sample a local descriptor for each patch. [sent-63, score-0.853]
</p><p>31 Each local patch is quantized into a visual word according to the codebook. [sent-65, score-0.551]
</p><p>32 In the next step, these visual words (image patches) will be further clustered into classes of objects. [sent-66, score-0.343]
</p><p>33 2  Figure 2: Given a collection of images as shown in the ﬁrst row (which are selected from the MSRC image dataset [13]), the goal is to segment images into objects and cluster these objects into different classes. [sent-68, score-0.676]
</p><p>34 Under the same labeling approach, image patches marked in the same color are in one object cluster, but the meaning of colors changes across different labeling methods. [sent-71, score-0.64]
</p><p>35 3  LDA  When LDA is used to solve our problem, we treat local patches of images as words and the whole image as a document. [sent-72, score-0.723]
</p><p>36 wji is the observed value of word i in document j. [sent-76, score-0.534]
</p><p>37 All the words in the corpus will be clustered into K topics (classes of objects). [sent-77, score-0.307]
</p><p>38 Each topic k is modeled as a multinomial distribution over the codebook. [sent-78, score-0.278]
</p><p>39 For a topic k, a multinomial parameter k is sampled from Dirichlet prior k Dir(φ). [sent-82, score-0.332]
</p><p>40 For a document j, a multinomial parameter j over the K topics is sampled from Dirichlet prior j Dir( ). [sent-84, score-0.46]
</p><p>41 For a word i in document j, a topic label zji is sampled from discrete distribution zji Discrete( j ). [sent-86, score-1.04]
</p><p>42 The value wji of word i in document j is sampled from the discrete distribution of topic zji , wji Discrete( zji ). [sent-88, score-1.211]
</p><p>43 zji can be sampled through a Gibbs sampling procedure which integrates out p(zji = k z where n  (k) ji w  ji  w  φ)  (k) n ji wji W w=1  + φwji  (k) n ji w  + φw  j  (j) n ji k + (j) K k =1 n ji k  and  k  k  +  [14]. [sent-89, score-0.779]
</p><p>44 (1)  k  is the number of words in the corpus with value w assigned to topic k excluding word (j)  i in document j, and n ji k is the number of words in document j assigned to topic k excluding word i in document j. [sent-90, score-2.221]
</p><p>45 Eq 1 is the product of two ratios: the probability of word wji under topic k and the probability of topic k in document j. [sent-91, score-0.984]
</p><p>46 So LDA clusters the visual words often co-occurring in the same images into one object class. [sent-92, score-0.489]
</p><p>47 Although LDA assumes that one image contains multiple topics, from experimental results we observe that the patches in the same image are likely to have the same labels. [sent-97, score-0.598]
</p><p>48 Since the whole image is treated as one document, if one object class, e. [sent-98, score-0.349]
</p><p>49 car in Figure 2, is dominant in the image, the second ratio in Eq 1 will lead to a large bias towards the car class, and thus the patches of street are also likely to be labeled as car. [sent-100, score-0.269]
</p><p>50 This problem could be solved if a local patch only considers its neighboring patches as being in the same document. [sent-101, score-0.471]
</p><p>51 4  SLDA  We assume that if visual words are from the same class of objects, they not only often co-occur in the same images but also are close in space. [sent-102, score-0.389]
</p><p>52 So we try to group image patches which are close in space into the same documents. [sent-103, score-0.452]
</p><p>53 Each region is treated as a document instead of the whole image. [sent-105, score-0.374]
</p><p>54 In Figure 4 (a), patch A on the cow is likely to be labeled as grass, since most other patches in its document are grass. [sent-107, score-0.732]
</p><p>55 Any two patches whose distance is smaller than the region size “could” belong to the same document if the regions are placed densely enough. [sent-110, score-0.675]
</p><p>56 We use the word “could” because each local patch is covered by several regions, so we have to decide to which document it belongs. [sent-111, score-0.665]
</p><p>57 If two patches are closer in space, they have a higher probability to be assigned to the same document since there are more regions covering both of them. [sent-113, score-0.627]
</p><p>58 As shown in Figure 4 (c), each document can be represented by a point (marked by magenta circle) in the image, assuming its region covers the whole image. [sent-115, score-0.365]
</p><p>59 If an image patch is close to a document, it has a high probability to be assigned to that document. [sent-116, score-0.493]
</p><p>60 A hidden variable di indicates which document word i is assigned to. [sent-119, score-0.648]
</p><p>61 For each document d d d j there is a hyperparameter cd = gj , xd , yj known a priori. [sent-120, score-0.41]
</p><p>62 gj is the index of the image where j j d d document j is placed and xj , yj is the location of the document. [sent-121, score-0.516]
</p><p>63 For a word i, in addition to the observed word value wi , its location (xi , yi ) and image index gi are also observed and stored in variable ci = (gi , xi , yi ). [sent-122, score-0.684]
</p><p>64 For a topic k, a multinomial parameter φk is sampled from Dirichlet prior φk ∼ Dir(β). [sent-124, score-0.332]
</p><p>65 4  Figure 4: There are several ways to add spatial information among image patches when designing documents. [sent-125, score-0.607]
</p><p>66 Image patches inside the region are assigned to the corresponding document. [sent-128, score-0.358]
</p><p>67 (c): Each document is associated with a point (marked in magenta color). [sent-131, score-0.29]
</p><p>68 If a image patch is close to a document, it has a high probability to be assigned to that document. [sent-133, score-0.493]
</p><p>69 For a document j, a multinomial parameter j over the K topics is sampled from Dirichlet prior j Dir( ). [sent-135, score-0.46]
</p><p>70 For a word (image patch) i, a random variable di is sampled from prior p(di σ) indicating to which document word i is assigned. [sent-137, score-0.792]
</p><p>71 The image index and location of word i is sampled from distribution p(ci cdi ). [sent-140, score-0.442]
</p><p>72 2 2 d xdi xi + ydi yi d d d d ) πgd (gi ) exp p((gi xi yi ) gdi xdi ydi d 2 i  p(ci cdi ) = 0 if the word and the document are not in the same image. [sent-142, score-0.624]
</p><p>73 The topic label zi of word i is sampled from the discrete distribution of document di , zi Discrete( di ). [sent-144, score-1.24]
</p><p>74 The value wi of word i is sampled from the discrete distribution of topic zi , wi Discrete( zi ). [sent-146, score-0.756]
</p><p>75 1  Gibbs Sampling  zi and di can be sampled through a Gibbs sampling procedure integrating out the conditional distribution of zi given di is the same as in LDA. [sent-148, score-0.52]
</p><p>76 In SLDA  k  (2)  +  k  is the number of words in the corpus with value w assigned to topic k excluding word  i, and is the number of words in document j assigned to topic k excluding word i. [sent-150, score-1.613]
</p><p>77 p di = j zi = k z i d i ci cd φ σ j p (di = j σ) p ci cd j p (zi = k z  i  di = j d  i  p (zi = k z  ) is obtained by integrating out  j  i  di = j d  )  i  . [sent-153, score-0.857]
</p><p>78 M  p (zi = k z  i  di = j d  i  p(  ) =  j  )p(zj  ji )d j  j =1 K k =1  M  = j =1  K k =1  5  (  k  K k =1  k  K k =1  )  (j )  nk (j ) nk  +  +  k  K k =1  k  We choose p (di = j|η) as a uniform prior and p ci |cd , σ as a Gaussian kernel. [sent-154, score-0.287]
</p><p>79 From Eq 2 and 3, we observed that a word tends to have the same topic label as other words in its document and words closer in space are more likely to be assigned to the same documents. [sent-157, score-1.057]
</p><p>80 So essentially under SLDA a word tends to be labeled as the same topic as other words close to it. [sent-158, score-0.602]
</p><p>81 This satisﬁes our assumption that visual words from the same object class are closer in space. [sent-159, score-0.361]
</p><p>82 Since we densely place many documents over one image, during Gibbs sampling some documents are only assigned a few words and the distributions cannot be well estimated. [sent-160, score-0.477]
</p><p>83 To solve this problem we replicate each image patch to get many particles. [sent-161, score-0.379]
</p><p>84 These particles have the same word value and location but can be assigned to different documents and have different labels. [sent-162, score-0.349]
</p><p>85 Thus each document will have enough samples of words to estimate the distributions. [sent-163, score-0.407]
</p><p>86 2  Discussion  SLDA is a ﬂexible model intended to encode spatial structure among image patches and design documents. [sent-165, score-0.607]
</p><p>87 If there is only one document placed over one image, SLDA simply reduces to LDA. [sent-166, score-0.297]
</p><p>88 However the object class model φk , simply a multinomial distribution over the codebook, has no spatial structure. [sent-171, score-0.28]
</p><p>89 By simply adding a time stamp to ci and cd , it is easy for SLDA to encode temporal structure j among visual words. [sent-173, score-0.374]
</p><p>90 Our codebook size is 200 and the topic number is 15. [sent-176, score-0.263]
</p><p>91 The results of LDA are noisy and within one image most of the patches are labeled as one topic. [sent-179, score-0.448]
</p><p>92 We treat all the frames in the sequence as an image collection and ignore their temporal order. [sent-186, score-0.282]
</p><p>93 SLDA clusters image patches into tigers, rock, water, and grass. [sent-192, score-0.456]
</p><p>94 If we choose the topic of tiger, as shown in the last row of Figure 5, all the tigers in the video can be segmented out. [sent-193, score-0.357]
</p><p>95 6  Conclusion  We propose a novel Spatial Latent Dirichlet Allocation model which clusters co-occurring and spatially neighboring visual words into the same topic. [sent-194, score-0.302]
</p><p>96 Instead of knowing word-document assignment a priori, SLDA has a generative procedure partitioning visual words which are close in space into the same documents. [sent-195, score-0.354]
</p><p>97 In the second column, we label the patches in the two frames as different topics using LDA. [sent-199, score-0.391]
</p><p>98 In the fourth column, we segment tigers out by choosing the topic marked in red. [sent-202, score-0.376]
</p><p>99 Using multiple segmentations to discover objects and their extent in image collections. [sent-250, score-0.337]
</p><p>100 Spatially coherent latent topic model for concurrent object segmentation and classiﬁcation. [sent-256, score-0.418]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('slda', 0.515), ('lda', 0.32), ('document', 0.263), ('patches', 0.24), ('topic', 0.225), ('patch', 0.2), ('image', 0.179), ('word', 0.171), ('words', 0.144), ('zji', 0.133), ('di', 0.133), ('spatial', 0.131), ('visual', 0.121), ('objects', 0.112), ('cd', 0.107), ('wji', 0.1), ('zi', 0.1), ('dirichlet', 0.099), ('documents', 0.097), ('object', 0.096), ('images', 0.091), ('topics', 0.09), ('ji', 0.082), ('assigned', 0.081), ('ci', 0.072), ('overlapped', 0.066), ('msrc', 0.066), ('marked', 0.063), ('densely', 0.058), ('segmentation', 0.058), ('bicycles', 0.057), ('cows', 0.057), ('video', 0.057), ('sky', 0.057), ('vision', 0.055), ('gi', 0.054), ('sampled', 0.054), ('dir', 0.053), ('multinomial', 0.053), ('tigers', 0.05), ('clustered', 0.047), ('allocation', 0.047), ('discover', 0.046), ('temporal', 0.044), ('iccv', 0.044), ('regions', 0.043), ('cars', 0.042), ('excluding', 0.041), ('gj', 0.04), ('latent', 0.039), ('cdi', 0.038), ('xdi', 0.038), ('ydi', 0.038), ('fa', 0.038), ('codebook', 0.038), ('segment', 0.038), ('whole', 0.038), ('wi', 0.037), ('clusters', 0.037), ('cvpr', 0.037), ('region', 0.037), ('treated', 0.036), ('language', 0.034), ('placed', 0.034), ('close', 0.033), ('sivic', 0.033), ('efros', 0.033), ('discrete', 0.032), ('frames', 0.032), ('classes', 0.031), ('labeling', 0.031), ('local', 0.031), ('gibbs', 0.031), ('tiger', 0.03), ('generative', 0.03), ('activities', 0.03), ('among', 0.03), ('label', 0.029), ('eq', 0.029), ('labeled', 0.029), ('blei', 0.028), ('quantized', 0.028), ('wang', 0.028), ('collection', 0.027), ('designing', 0.027), ('design', 0.027), ('russell', 0.027), ('magenta', 0.027), ('corpus', 0.026), ('scene', 0.026), ('assignment', 0.026), ('divide', 0.026), ('faces', 0.026), ('cluster', 0.026), ('segmented', 0.025), ('atomic', 0.024), ('human', 0.024), ('discovering', 0.023), ('alarm', 0.023), ('descriptor', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="183-tfidf-1" href="./nips-2007-Spatial_Latent_Dirichlet_Allocation.html">183 nips-2007-Spatial Latent Dirichlet Allocation</a></p>
<p>Author: Xiaogang Wang, Eric Grimson</p><p>Abstract: In recent years, the language model Latent Dirichlet Allocation (LDA), which clusters co-occurring words into topics, has been widely applied in the computer vision ﬁeld. However, many of these applications have difﬁculty with modeling the spatial and temporal structure among visual words, since LDA assumes that a document is a “bag-of-words”. It is also critical to properly design “words” and “documents” when using a language model to solve vision problems. In this paper, we propose a topic model Spatial Latent Dirichlet Allocation (SLDA), which better encodes spatial structures among visual words that are essential for solving many vision problems. The spatial information is not encoded in the values of visual words but in the design of documents. Instead of knowing the partition of words into documents a priori, the word-document assignment becomes a random hidden variable in SLDA. There is a generative procedure, where knowledge of spatial structure can be ﬂexibly added as a prior, grouping visual words which are close in space into the same document. We use SLDA to discover objects from a collection of images, and show it achieves better performance than LDA. 1</p><p>2 0.54716897 <a title="183-tfidf-2" href="./nips-2007-Supervised_Topic_Models.html">189 nips-2007-Supervised Topic Models</a></p>
<p>Author: Jon D. Mcauliffe, David M. Blei</p><p>Abstract: We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive a maximum-likelihood procedure for parameter estimation, which relies on variational approximations to handle intractable posterior expectations. Prediction problems motivate this research: we use the ﬁtted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and web page popularity predicted from text descriptions. We illustrate the beneﬁts of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression. 1</p><p>3 0.28658086 <a title="183-tfidf-3" href="./nips-2007-Distributed_Inference_for_Latent_Dirichlet_Allocation.html">73 nips-2007-Distributed Inference for Latent Dirichlet Allocation</a></p>
<p>Author: David Newman, Padhraic Smyth, Max Welling, Arthur U. Asuncion</p><p>Abstract: We investigate the problem of learning a widely-used latent-variable model – the Latent Dirichlet Allocation (LDA) or “topic” model – using distributed computation, where each of processors only sees of the total data set. We propose two distributed inference schemes that are motivated from different perspectives. The ﬁrst scheme uses local Gibbs sampling on each processor with periodic updates—it is simple to implement and can be viewed as an approximation to a single processor implementation of Gibbs sampling. The second scheme relies on a hierarchical Bayesian extension of the standard LDA model to directly account for the fact that data are distributed across processors—it has a theoretical guarantee of convergence but is more complex to implement than the approximate method. Using ﬁve real-world text corpora we show that distributed learning works very well for LDA models, i.e., perplexity and precision-recall scores for distributed learning are indistinguishable from those obtained with single-processor learning. Our extensive experimental results include large-scale distributed computation on 1000 virtual processors; and speedup experiments of learning topics in a 100-million word corpus using 16 processors. ¢ ¤ ¦¥£ ¢ ¢</p><p>4 0.2027465 <a title="183-tfidf-4" href="./nips-2007-Object_Recognition_by_Scene_Alignment.html">143 nips-2007-Object Recognition by Scene Alignment</a></p>
<p>Author: Bryan Russell, Antonio Torralba, Ce Liu, Rob Fergus, William T. Freeman</p><p>Abstract: Current object recognition systems can only recognize a limited number of object categories; scaling up to many categories is the next challenge. We seek to build a system to recognize and localize many different object categories in complex scenes. We achieve this through a simple approach: by matching the input image, in an appropriate representation, to images in a large training set of labeled images. Due to regularities in object identities across similar scenes, the retrieved matches provide hypotheses for object identities and locations. We build a probabilistic model to transfer the labels from the retrieval set to the input image. We demonstrate the effectiveness of this approach and study algorithm component contributions using held-out test sets from the LabelMe database. 1</p><p>5 0.19787015 <a title="183-tfidf-5" href="./nips-2007-Infinite_State_Bayes-Nets_for_Structured_Domains.html">105 nips-2007-Infinite State Bayes-Nets for Structured Domains</a></p>
<p>Author: Max Welling, Ian Porteous, Evgeniy Bart</p><p>Abstract: A general modeling framework is proposed that uniﬁes nonparametric-Bayesian models, topic-models and Bayesian networks. This class of inﬁnite state Bayes nets (ISBN) can be viewed as directed networks of ‘hierarchical Dirichlet processes’ (HDPs) where the domain of the variables can be structured (e.g. words in documents or features in images). We show that collapsed Gibbs sampling can be done efﬁciently in these models by leveraging the structure of the Bayes net and using the forward-ﬁltering-backward-sampling algorithm for junction trees. Existing models, such as nested-DP, Pachinko allocation, mixed membership stochastic block models as well as a number of new models are described as ISBNs. Two experiments have been performed to illustrate these ideas. 1</p><p>6 0.18230729 <a title="183-tfidf-6" href="./nips-2007-Scene_Segmentation_with_CRFs_Learned_from_Partially_Labeled_Images.html">172 nips-2007-Scene Segmentation with CRFs Learned from Partially Labeled Images</a></p>
<p>7 0.16038993 <a title="183-tfidf-7" href="./nips-2007-A_Bayesian_LDA-based_model_for_semi-supervised_part-of-speech_tagging.html">2 nips-2007-A Bayesian LDA-based model for semi-supervised part-of-speech tagging</a></p>
<p>8 0.15765879 <a title="183-tfidf-8" href="./nips-2007-HM-BiTAM%3A_Bilingual_Topic_Exploration%2C_Word_Alignment%2C_and_Translation.html">95 nips-2007-HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation</a></p>
<p>9 0.13781855 <a title="183-tfidf-9" href="./nips-2007-Collapsed_Variational_Inference_for_HDP.html">47 nips-2007-Collapsed Variational Inference for HDP</a></p>
<p>10 0.12845188 <a title="183-tfidf-10" href="./nips-2007-Mining_Internet-Scale_Software_Repositories.html">129 nips-2007-Mining Internet-Scale Software Repositories</a></p>
<p>11 0.09997955 <a title="183-tfidf-11" href="./nips-2007-Subspace-Based_Face_Recognition_in_Analog_VLSI.html">188 nips-2007-Subspace-Based Face Recognition in Analog VLSI</a></p>
<p>12 0.096647382 <a title="183-tfidf-12" href="./nips-2007-Sparse_Overcomplete_Latent_Variable_Decomposition_of_Counts_Data.html">181 nips-2007-Sparse Overcomplete Latent Variable Decomposition of Counts Data</a></p>
<p>13 0.09417887 <a title="183-tfidf-13" href="./nips-2007-Learning_Visual_Attributes.html">113 nips-2007-Learning Visual Attributes</a></p>
<p>14 0.090255208 <a title="183-tfidf-14" href="./nips-2007-Unsupervised_Feature_Selection_for_Accurate_Recommendation_of_High-Dimensional_Image_Data.html">211 nips-2007-Unsupervised Feature Selection for Accurate Recommendation of High-Dimensional Image Data</a></p>
<p>15 0.0899745 <a title="183-tfidf-15" href="./nips-2007-The_Infinite_Markov_Model.html">197 nips-2007-The Infinite Markov Model</a></p>
<p>16 0.088195145 <a title="183-tfidf-16" href="./nips-2007-A_Bayesian_Framework_for_Cross-Situational_Word-Learning.html">1 nips-2007-A Bayesian Framework for Cross-Situational Word-Learning</a></p>
<p>17 0.082969278 <a title="183-tfidf-17" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>18 0.078255959 <a title="183-tfidf-18" href="./nips-2007-Expectation_Maximization_and_Posterior_Constraints.html">84 nips-2007-Expectation Maximization and Posterior Constraints</a></p>
<p>19 0.074582696 <a title="183-tfidf-19" href="./nips-2007-Discriminative_Keyword_Selection_Using_Support_Vector_Machines.html">71 nips-2007-Discriminative Keyword Selection Using Support Vector Machines</a></p>
<p>20 0.073684998 <a title="183-tfidf-20" href="./nips-2007-A_Probabilistic_Approach_to_Language_Change.html">9 nips-2007-A Probabilistic Approach to Language Change</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.211), (1, 0.176), (2, -0.102), (3, -0.523), (4, 0.159), (5, 0.001), (6, 0.128), (7, -0.102), (8, 0.035), (9, 0.191), (10, -0.007), (11, -0.003), (12, -0.101), (13, 0.201), (14, -0.023), (15, 0.183), (16, 0.181), (17, -0.112), (18, 0.042), (19, 0.006), (20, -0.002), (21, -0.01), (22, -0.001), (23, 0.001), (24, -0.063), (25, -0.038), (26, 0.016), (27, -0.057), (28, 0.019), (29, -0.009), (30, -0.031), (31, 0.064), (32, -0.012), (33, 0.051), (34, -0.037), (35, 0.002), (36, -0.029), (37, -0.087), (38, 0.034), (39, 0.02), (40, -0.0), (41, 0.043), (42, -0.023), (43, -0.013), (44, -0.012), (45, 0.084), (46, -0.046), (47, 0.101), (48, -0.021), (49, 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97261113 <a title="183-lsi-1" href="./nips-2007-Spatial_Latent_Dirichlet_Allocation.html">183 nips-2007-Spatial Latent Dirichlet Allocation</a></p>
<p>Author: Xiaogang Wang, Eric Grimson</p><p>Abstract: In recent years, the language model Latent Dirichlet Allocation (LDA), which clusters co-occurring words into topics, has been widely applied in the computer vision ﬁeld. However, many of these applications have difﬁculty with modeling the spatial and temporal structure among visual words, since LDA assumes that a document is a “bag-of-words”. It is also critical to properly design “words” and “documents” when using a language model to solve vision problems. In this paper, we propose a topic model Spatial Latent Dirichlet Allocation (SLDA), which better encodes spatial structures among visual words that are essential for solving many vision problems. The spatial information is not encoded in the values of visual words but in the design of documents. Instead of knowing the partition of words into documents a priori, the word-document assignment becomes a random hidden variable in SLDA. There is a generative procedure, where knowledge of spatial structure can be ﬂexibly added as a prior, grouping visual words which are close in space into the same document. We use SLDA to discover objects from a collection of images, and show it achieves better performance than LDA. 1</p><p>2 0.83444583 <a title="183-lsi-2" href="./nips-2007-Supervised_Topic_Models.html">189 nips-2007-Supervised Topic Models</a></p>
<p>Author: Jon D. Mcauliffe, David M. Blei</p><p>Abstract: We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive a maximum-likelihood procedure for parameter estimation, which relies on variational approximations to handle intractable posterior expectations. Prediction problems motivate this research: we use the ﬁtted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and web page popularity predicted from text descriptions. We illustrate the beneﬁts of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression. 1</p><p>3 0.81981856 <a title="183-lsi-3" href="./nips-2007-Distributed_Inference_for_Latent_Dirichlet_Allocation.html">73 nips-2007-Distributed Inference for Latent Dirichlet Allocation</a></p>
<p>Author: David Newman, Padhraic Smyth, Max Welling, Arthur U. Asuncion</p><p>Abstract: We investigate the problem of learning a widely-used latent-variable model – the Latent Dirichlet Allocation (LDA) or “topic” model – using distributed computation, where each of processors only sees of the total data set. We propose two distributed inference schemes that are motivated from different perspectives. The ﬁrst scheme uses local Gibbs sampling on each processor with periodic updates—it is simple to implement and can be viewed as an approximation to a single processor implementation of Gibbs sampling. The second scheme relies on a hierarchical Bayesian extension of the standard LDA model to directly account for the fact that data are distributed across processors—it has a theoretical guarantee of convergence but is more complex to implement than the approximate method. Using ﬁve real-world text corpora we show that distributed learning works very well for LDA models, i.e., perplexity and precision-recall scores for distributed learning are indistinguishable from those obtained with single-processor learning. Our extensive experimental results include large-scale distributed computation on 1000 virtual processors; and speedup experiments of learning topics in a 100-million word corpus using 16 processors. ¢ ¤ ¦¥£ ¢ ¢</p><p>4 0.56006414 <a title="183-lsi-4" href="./nips-2007-Collapsed_Variational_Inference_for_HDP.html">47 nips-2007-Collapsed Variational Inference for HDP</a></p>
<p>Author: Yee W. Teh, Kenichi Kurihara, Max Welling</p><p>Abstract: A wide variety of Dirichlet-multinomial ‘topic’ models have found interesting applications in recent years. While Gibbs sampling remains an important method of inference in such models, variational techniques have certain advantages such as easy assessment of convergence, easy optimization without the need to maintain detailed balance, a bound on the marginal likelihood, and side-stepping of issues with topic-identiﬁability. The most accurate variational technique thus far, namely collapsed variational latent Dirichlet allocation, did not deal with model selection nor did it include inference for hyperparameters. We address both issues by generalizing the technique, obtaining the ﬁrst variational algorithm to deal with the hierarchical Dirichlet process and to deal with hyperparameters of Dirichlet variables. Experiments show a signiﬁcant improvement in accuracy. 1</p><p>5 0.55922139 <a title="183-lsi-5" href="./nips-2007-Infinite_State_Bayes-Nets_for_Structured_Domains.html">105 nips-2007-Infinite State Bayes-Nets for Structured Domains</a></p>
<p>Author: Max Welling, Ian Porteous, Evgeniy Bart</p><p>Abstract: A general modeling framework is proposed that uniﬁes nonparametric-Bayesian models, topic-models and Bayesian networks. This class of inﬁnite state Bayes nets (ISBN) can be viewed as directed networks of ‘hierarchical Dirichlet processes’ (HDPs) where the domain of the variables can be structured (e.g. words in documents or features in images). We show that collapsed Gibbs sampling can be done efﬁciently in these models by leveraging the structure of the Bayes net and using the forward-ﬁltering-backward-sampling algorithm for junction trees. Existing models, such as nested-DP, Pachinko allocation, mixed membership stochastic block models as well as a number of new models are described as ISBNs. Two experiments have been performed to illustrate these ideas. 1</p><p>6 0.50486213 <a title="183-lsi-6" href="./nips-2007-Object_Recognition_by_Scene_Alignment.html">143 nips-2007-Object Recognition by Scene Alignment</a></p>
<p>7 0.48949307 <a title="183-lsi-7" href="./nips-2007-HM-BiTAM%3A_Bilingual_Topic_Exploration%2C_Word_Alignment%2C_and_Translation.html">95 nips-2007-HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation</a></p>
<p>8 0.48102349 <a title="183-lsi-8" href="./nips-2007-Scene_Segmentation_with_CRFs_Learned_from_Partially_Labeled_Images.html">172 nips-2007-Scene Segmentation with CRFs Learned from Partially Labeled Images</a></p>
<p>9 0.46245354 <a title="183-lsi-9" href="./nips-2007-Mining_Internet-Scale_Software_Repositories.html">129 nips-2007-Mining Internet-Scale Software Repositories</a></p>
<p>10 0.41441044 <a title="183-lsi-10" href="./nips-2007-Learning_Visual_Attributes.html">113 nips-2007-Learning Visual Attributes</a></p>
<p>11 0.41273594 <a title="183-lsi-11" href="./nips-2007-The_Infinite_Gamma-Poisson_Feature_Model.html">196 nips-2007-The Infinite Gamma-Poisson Feature Model</a></p>
<p>12 0.38509592 <a title="183-lsi-12" href="./nips-2007-A_Bayesian_LDA-based_model_for_semi-supervised_part-of-speech_tagging.html">2 nips-2007-A Bayesian LDA-based model for semi-supervised part-of-speech tagging</a></p>
<p>13 0.37772647 <a title="183-lsi-13" href="./nips-2007-Subspace-Based_Face_Recognition_in_Analog_VLSI.html">188 nips-2007-Subspace-Based Face Recognition in Analog VLSI</a></p>
<p>14 0.33957946 <a title="183-lsi-14" href="./nips-2007-A_Bayesian_Framework_for_Cross-Situational_Word-Learning.html">1 nips-2007-A Bayesian Framework for Cross-Situational Word-Learning</a></p>
<p>15 0.32065687 <a title="183-lsi-15" href="./nips-2007-Unsupervised_Feature_Selection_for_Accurate_Recommendation_of_High-Dimensional_Image_Data.html">211 nips-2007-Unsupervised Feature Selection for Accurate Recommendation of High-Dimensional Image Data</a></p>
<p>16 0.29287735 <a title="183-lsi-16" href="./nips-2007-The_Distribution_Family_of_Similarity_Distances.html">193 nips-2007-The Distribution Family of Similarity Distances</a></p>
<p>17 0.28232443 <a title="183-lsi-17" href="./nips-2007-Sparse_Overcomplete_Latent_Variable_Decomposition_of_Counts_Data.html">181 nips-2007-Sparse Overcomplete Latent Variable Decomposition of Counts Data</a></p>
<p>18 0.2815772 <a title="183-lsi-18" href="./nips-2007-A_Probabilistic_Approach_to_Language_Change.html">9 nips-2007-A Probabilistic Approach to Language Change</a></p>
<p>19 0.27825946 <a title="183-lsi-19" href="./nips-2007-Discriminative_Keyword_Selection_Using_Support_Vector_Machines.html">71 nips-2007-Discriminative Keyword Selection Using Support Vector Machines</a></p>
<p>20 0.25354856 <a title="183-lsi-20" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.028), (13, 0.019), (21, 0.062), (26, 0.016), (31, 0.035), (34, 0.011), (35, 0.017), (47, 0.052), (49, 0.011), (83, 0.089), (85, 0.012), (87, 0.508), (90, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92635697 <a title="183-lda-1" href="./nips-2007-Spatial_Latent_Dirichlet_Allocation.html">183 nips-2007-Spatial Latent Dirichlet Allocation</a></p>
<p>Author: Xiaogang Wang, Eric Grimson</p><p>Abstract: In recent years, the language model Latent Dirichlet Allocation (LDA), which clusters co-occurring words into topics, has been widely applied in the computer vision ﬁeld. However, many of these applications have difﬁculty with modeling the spatial and temporal structure among visual words, since LDA assumes that a document is a “bag-of-words”. It is also critical to properly design “words” and “documents” when using a language model to solve vision problems. In this paper, we propose a topic model Spatial Latent Dirichlet Allocation (SLDA), which better encodes spatial structures among visual words that are essential for solving many vision problems. The spatial information is not encoded in the values of visual words but in the design of documents. Instead of knowing the partition of words into documents a priori, the word-document assignment becomes a random hidden variable in SLDA. There is a generative procedure, where knowledge of spatial structure can be ﬂexibly added as a prior, grouping visual words which are close in space into the same document. We use SLDA to discover objects from a collection of images, and show it achieves better performance than LDA. 1</p><p>2 0.9186067 <a title="183-lda-2" href="./nips-2007-Mining_Internet-Scale_Software_Repositories.html">129 nips-2007-Mining Internet-Scale Software Repositories</a></p>
<p>Author: Erik Linstead, Paul Rigor, Sushil Bajracharya, Cristina Lopes, Pierre F. Baldi</p><p>Abstract: Large repositories of source code create new challenges and opportunities for statistical machine learning. Here we ﬁrst develop Sourcerer, an infrastructure for the automated crawling, parsing, and database storage of open source software. Sourcerer allows us to gather Internet-scale source code. For instance, in one experiment, we gather 4,632 java projects from SourceForge and Apache totaling over 38 million lines of code from 9,250 developers. Simple statistical analyses of the data ﬁrst reveal robust power-law behavior for package, SLOC, and lexical containment distributions. We then develop and apply unsupervised author-topic, probabilistic models to automatically discover the topics embedded in the code and extract topic-word and author-topic distributions. In addition to serving as a convenient summary for program function and developer activities, these and other related distributions provide a statistical and information-theoretic basis for quantifying and analyzing developer similarity and competence, topic scattering, and document tangling, with direct applications to software engineering. Finally, by combining software textual content with structural information captured by our CodeRank approach, we are able to signiﬁcantly improve software retrieval performance, increasing the AUC metric to 0.84– roughly 10-30% better than previous approaches based on text alone. Supplementary material may be found at: http://sourcerer.ics.uci.edu/nips2007/nips07.html. 1</p><p>3 0.82936013 <a title="183-lda-3" href="./nips-2007-Combined_discriminative_and_generative_articulated_pose_and_non-rigid_shape_estimation.html">50 nips-2007-Combined discriminative and generative articulated pose and non-rigid shape estimation</a></p>
<p>Author: Leonid Sigal, Alexandru Balan, Michael J. Black</p><p>Abstract: Estimation of three-dimensional articulated human pose and motion from images is a central problem in computer vision. Much of the previous work has been limited by the use of crude generative models of humans represented as articulated collections of simple parts such as cylinders. Automatic initialization of such models has proved difﬁcult and most approaches assume that the size and shape of the body parts are known a priori. In this paper we propose a method for automatically recovering a detailed parametric model of non-rigid body shape and pose from monocular imagery. Speciﬁcally, we represent the body using a parameterized triangulated mesh model that is learned from a database of human range scans. We demonstrate a discriminative method to directly recover the model parameters from monocular images using a conditional mixture of kernel regressors. This predicted pose and shape are used to initialize a generative model for more detailed pose and shape estimation. The resulting approach allows fully automatic pose and shape recovery from monocular and multi-camera imagery. Experimental results show that our method is capable of robustly recovering articulated pose, shape and biometric measurements (e.g. height, weight, etc.) in both calibrated and uncalibrated camera environments. 1</p><p>4 0.78076887 <a title="183-lda-4" href="./nips-2007-Continuous_Time_Particle_Filtering_for_fMRI.html">59 nips-2007-Continuous Time Particle Filtering for fMRI</a></p>
<p>Author: Lawrence Murray, Amos J. Storkey</p><p>Abstract: We construct a biologically motivated stochastic differential model of the neural and hemodynamic activity underlying the observed Blood Oxygen Level Dependent (BOLD) signal in Functional Magnetic Resonance Imaging (fMRI). The model poses a difﬁcult parameter estimation problem, both theoretically due to the nonlinearity and divergence of the differential system, and computationally due to its time and space complexity. We adapt a particle ﬁlter and smoother to the task, and discuss some of the practical approaches used to tackle the difﬁculties, including use of sparse matrices and parallelisation. Results demonstrate the tractability of the approach in its application to an effective connectivity study. 1</p><p>5 0.54675996 <a title="183-lda-5" href="./nips-2007-Supervised_Topic_Models.html">189 nips-2007-Supervised Topic Models</a></p>
<p>Author: Jon D. Mcauliffe, David M. Blei</p><p>Abstract: We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive a maximum-likelihood procedure for parameter estimation, which relies on variational approximations to handle intractable posterior expectations. Prediction problems motivate this research: we use the ﬁtted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and web page popularity predicted from text descriptions. We illustrate the beneﬁts of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression. 1</p><p>6 0.49677375 <a title="183-lda-6" href="./nips-2007-Distributed_Inference_for_Latent_Dirichlet_Allocation.html">73 nips-2007-Distributed Inference for Latent Dirichlet Allocation</a></p>
<p>7 0.47012183 <a title="183-lda-7" href="./nips-2007-HM-BiTAM%3A_Bilingual_Topic_Exploration%2C_Word_Alignment%2C_and_Translation.html">95 nips-2007-HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation</a></p>
<p>8 0.45000041 <a title="183-lda-8" href="./nips-2007-Object_Recognition_by_Scene_Alignment.html">143 nips-2007-Object Recognition by Scene Alignment</a></p>
<p>9 0.44462302 <a title="183-lda-9" href="./nips-2007-A_Bayesian_LDA-based_model_for_semi-supervised_part-of-speech_tagging.html">2 nips-2007-A Bayesian LDA-based model for semi-supervised part-of-speech tagging</a></p>
<p>10 0.44171256 <a title="183-lda-10" href="./nips-2007-Infinite_State_Bayes-Nets_for_Structured_Domains.html">105 nips-2007-Infinite State Bayes-Nets for Structured Domains</a></p>
<p>11 0.42417517 <a title="183-lda-11" href="./nips-2007-Collapsed_Variational_Inference_for_HDP.html">47 nips-2007-Collapsed Variational Inference for HDP</a></p>
<p>12 0.41213316 <a title="183-lda-12" href="./nips-2007-A_Bayesian_Framework_for_Cross-Situational_Word-Learning.html">1 nips-2007-A Bayesian Framework for Cross-Situational Word-Learning</a></p>
<p>13 0.41129991 <a title="183-lda-13" href="./nips-2007-Scene_Segmentation_with_CRFs_Learned_from_Partially_Labeled_Images.html">172 nips-2007-Scene Segmentation with CRFs Learned from Partially Labeled Images</a></p>
<p>14 0.40710947 <a title="183-lda-14" href="./nips-2007-Learning_Visual_Attributes.html">113 nips-2007-Learning Visual Attributes</a></p>
<p>15 0.39744076 <a title="183-lda-15" href="./nips-2007-Configuration_Estimates_Improve_Pedestrian_Finding.html">56 nips-2007-Configuration Estimates Improve Pedestrian Finding</a></p>
<p>16 0.39701653 <a title="183-lda-16" href="./nips-2007-Unsupervised_Feature_Selection_for_Accurate_Recommendation_of_High-Dimensional_Image_Data.html">211 nips-2007-Unsupervised Feature Selection for Accurate Recommendation of High-Dimensional Image Data</a></p>
<p>17 0.39485571 <a title="183-lda-17" href="./nips-2007-People_Tracking_with_the_Laplacian_Eigenmaps_Latent_Variable_Model.html">153 nips-2007-People Tracking with the Laplacian Eigenmaps Latent Variable Model</a></p>
<p>18 0.38365659 <a title="183-lda-18" href="./nips-2007-Retrieved_context_and_the_discovery_of_semantic_structure.html">169 nips-2007-Retrieved context and the discovery of semantic structure</a></p>
<p>19 0.37613338 <a title="183-lda-19" href="./nips-2007-Predicting_Brain_States_from_fMRI_Data%3A_Incremental_Functional_Principal_Component_Regression.html">154 nips-2007-Predicting Brain States from fMRI Data: Incremental Functional Principal Component Regression</a></p>
<p>20 0.37512746 <a title="183-lda-20" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
