<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-36" href="#">nips2007-36</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</h1>
<br/><p>Source: <a title="nips-2007-36-pdf" href="http://papers.nips.cc/paper/3242-better-than-least-squares-comparison-of-objective-functions-for-estimating-linear-nonlinear-models.pdf">pdf</a></p><p>Author: Tatyana Sharpee</p><p>Abstract: This paper compares a family of methods for characterizing neural feature selectivity with natural stimuli in the framework of the linear-nonlinear model. In this model, the neural ﬁring rate is a nonlinear function of a small number of relevant stimulus components. The relevant stimulus dimensions can be found by maximizing one of the family of objective functions, R´ nyi divergences of different e orders [1, 2]. We show that maximizing one of them, R´ nyi divergence of ore der 2, is equivalent to least-square ﬁtting of the linear-nonlinear model to neural data. Next, we derive reconstruction errors in relevant dimensions found by maximizing R´ nyi divergences of arbitrary order in the asymptotic limit of large spike e numbers. We ﬁnd that the smallest errors are obtained with R´ nyi divergence of e order 1, also known as Kullback-Leibler divergence. This corresponds to ﬁnding relevant dimensions by maximizing mutual information [2]. We numerically test how these optimization schemes perform in the regime of low signal-to-noise ratio (small number of spikes and increasing neural noise) for model visual neurons. We ﬁnd that optimization schemes based on either least square ﬁtting or information maximization perform well even when number of spikes is small. Information maximization provides slightly, but signiﬁcantly, better reconstructions than least square ﬁtting. This makes the problem of ﬁnding relevant dimensions, together with the problem of lossy compression [3], one of examples where informationtheoretic measures are no more data limited than those derived from least squares. 1</p><p>Reference: <a title="nips-2007-36-reference" href="../nips2007_reference/nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this model, the neural ﬁring rate is a nonlinear function of a small number of relevant stimulus components. [sent-4, score-0.438]
</p><p>2 The relevant stimulus dimensions can be found by maximizing one of the family of objective functions, R´ nyi divergences of different e orders [1, 2]. [sent-5, score-1.323]
</p><p>3 We show that maximizing one of them, R´ nyi divergence of ore der 2, is equivalent to least-square ﬁtting of the linear-nonlinear model to neural data. [sent-6, score-0.637]
</p><p>4 Next, we derive reconstruction errors in relevant dimensions found by maximizing R´ nyi divergences of arbitrary order in the asymptotic limit of large spike e numbers. [sent-7, score-1.804]
</p><p>5 We ﬁnd that the smallest errors are obtained with R´ nyi divergence of e order 1, also known as Kullback-Leibler divergence. [sent-8, score-0.592]
</p><p>6 This corresponds to ﬁnding relevant dimensions by maximizing mutual information [2]. [sent-9, score-0.589]
</p><p>7 We numerically test how these optimization schemes perform in the regime of low signal-to-noise ratio (small number of spikes and increasing neural noise) for model visual neurons. [sent-10, score-0.277]
</p><p>8 We ﬁnd that optimization schemes based on either least square ﬁtting or information maximization perform well even when number of spikes is small. [sent-11, score-0.302]
</p><p>9 This makes the problem of ﬁnding relevant dimensions, together with the problem of lossy compression [3], one of examples where informationtheoretic measures are no more data limited than those derived from least squares. [sent-13, score-0.315]
</p><p>10 One family of approaches employs the dimensionality reduction idea: while inputs are typically very high-dimensional, not all dimensions are equally important for eliciting a neural response [4, 5, 6, 7, 8]. [sent-15, score-0.401]
</p><p>11 The aim is then to ﬁnd a small set of dimensions {ˆ1 , e2 , . [sent-16, score-0.208]
</p><p>12 } in the stimulus e ˆ space that are relevant for neural response, without imposing, however, a particular functional dependence between the neural response and the stimulus components {s1 , s2 , . [sent-19, score-0.585]
</p><p>13 } along the relevant dimensions: P (spike|s) = P (spike)g(s1 , s2 , . [sent-22, score-0.258]
</p><p>14 , sK ), (1) If the inputs are Gaussian, the last requirement is not important, because relevant dimensions can be found without knowing a correct functional form for the nonlinear function g in Eq. [sent-25, score-0.586]
</p><p>15 However, for non-Gaussian inputs a wrong assumption for the form of the nonlinearity g will lead to systematic errors in the estimate of the relevant dimensions themselves [9, 5, 1, 2]. [sent-27, score-0.565]
</p><p>16 The larger the deviations of the stimulus distribution from a Gaussian, the larger will be the effect of errors in the presumed form of the nonlinearity function g on estimating the relevant dimensions. [sent-28, score-0.441]
</p><p>17 To ﬁnd the relevant dimensions for neural responses probed with non-Gaussian inputs, Hunter and Korenberg proposed an iterative scheme [5] where the relevant dimensions are ﬁrst found by assuming that the input–output function g is linear. [sent-30, score-1.011]
</p><p>18 Its functional form is then updated given the current estimate of the relevant dimensions. [sent-31, score-0.231]
</p><p>19 The inverse of g is then used to improve the estimate of the relevant dimensions. [sent-32, score-0.231]
</p><p>20 This procedure can be improved not to rely on inverting the nonlinear function g by formulating optimization problem exclusively with respect to relevant dimensions [1, 2], where the nonlinear function g is taken into account in the objective function to be optimized. [sent-33, score-0.586]
</p><p>21 Here we show that the optimization problem based on the R´ nyi divergence of order 2 e corresponds to least square ﬁtting of the linear-nonlinear model to neural spike trains. [sent-35, score-1.18]
</p><p>22 The KullbackLeibler divergence also belongs to this family and is the R´ nyi divergence of order 1. [sent-36, score-0.648]
</p><p>23 It quantiﬁes e the amount of mutual information between the neural response and the stimulus components along the relevant dimension [2]. [sent-37, score-0.615]
</p><p>24 We then show in numerical simulations on model cells that this trend persists even for very low spike numbers. [sent-40, score-0.617]
</p><p>25 (1) [multiple dimensions could also be used, see below]. [sent-42, score-0.208]
</p><p>26 (3) P (s) P (s · v) P (s) Pv (x)  In the last integral averaging has been carried out with respect to all stimulus components except for those along the trial direction v, so that integration variable x = s · v. [sent-44, score-0.2]
</p><p>27 Note that if there multiple spikes are sometimes elicited, the probability distribution P (x|spike) can be constructed by weighting the contribution from each stimulus according to the number of spikes it elicited. [sent-47, score-0.346]
</p><p>28 If neural spikes are indeed based on one relevant dimension, then this dimension will explain all of the variance, leading to χ2 = 0. [sent-48, score-0.495]
</p><p>29 e For optimization strategy based on R´ nyi divergences of order α, the relevant dimensions are found e by maximizing: α Pv (x|spike) 1 dxPv (x) . [sent-52, score-1.054]
</p><p>30 Pv (x)  (7)  Returning to the variance optimization, the maximal value for F [v] that can be achieved by any dimension v is: 2 [P (s|spike)] . [sent-54, score-0.2]
</p><p>31 (8) Fmax = ds P (s) It corresponds to the variance in the ﬁring rate averaged across different inputs (see Eq. [sent-55, score-0.192]
</p><p>32 Computation of the mutual information carried by the individual spike about the stimulus relies on similar integrals. [sent-57, score-0.749]
</p><p>33 The fact that F [v] < Fmax can be seen either by simply noting that χ2 [v] ≥ 0, or from the data processing inequality, which applies not only to Kullback-Leibler divergence, but also to R´ nyi e divergences [12, 13, 1]. [sent-61, score-0.547]
</p><p>34 In other words, the variance in the ﬁring rate explained by a given dimension F [v] cannot be greater than the overall variance in the ﬁring rate Fmax . [sent-62, score-0.286]
</p><p>35 This is because we have averaged over all of the variations in the ﬁring rate that correspond to inputs with the same projection value on the dimension v and differ only in projections onto other dimensions. [sent-63, score-0.285]
</p><p>36 Optimization scheme based on R´ nyi divergences of different orders have very similar structure. [sent-64, score-0.569]
</p><p>37 In e particular, gradient could be evaluated in a similar way: vF  (α)  =  α α−1  dxPv (x|spike) [ s|x, spike − s|x ]  d dx  Pv (x|spike) Pv (x)  α−1  ,  (10)  where s|x, spike = ds sδ(x−s·v)P (s|spike)/P (x|spike), and similarly for s|x . [sent-65, score-1.198]
</p><p>38 The gradient is thus given by a weighted sum of spike-triggered averages s|x, spike − s|x conditional upon projection values of stimuli onto the dimension v for which the gradient of information is being evaluated. [sent-66, score-0.868]
</p><p>39 The similarity of the structure of both the objective functions and their gradients for different R´ nyi divergences means that numeric algorithms can be used for optimization of R´ nyi e e divergences of different orders. [sent-67, score-1.174]
</p><p>40 First, as was proved in the case of information maximization based on Kullback-Leibler divergence [2], the merit function F (α) [v] does not change with the length of the vector v. [sent-70, score-0.278]
</p><p>41 (10), because v · s|x, spike = x and v · s|x = x. [sent-72, score-0.551]
</p><p>42 This is because for the true relevant dimension according to which spikes were generated, s|s1 , spike = s|s1 , a consequence of the fact that relevant projections completely determine the spike probability. [sent-74, score-1.839]
</p><p>43 Third, merit functions, including variance and information, can be computed with respect to multiple dimensions by keeping track of stimulus  projections on all the relevant dimensions when forming probability distributions (4). [sent-75, score-0.968]
</p><p>44 For example, in the case of two dimensions v1 and v2 , we would use Pv1 ,v2 (x1 , x2 |spike) = Pv1 ,v2 (x1 , x2 ) = to  ds δ(x1 − s · v1 )δ(x2 − s · v2 )P (s|spike),  ds δ(x1 − s · v1 )δ(x2 − s · v2 )P (s),  compute the variance with respect 2 dx1 dx2 [P (x1 , x2 |spike)] /P (x1 , x2 ). [sent-76, score-0.37]
</p><p>45 to  the  two  dimensions  (11) as  F [v1 , v2 ]  =  If multiple stimulus dimensions are relevant for eliciting the neural response, they can always be found (provided sufﬁcient number of responses have been recorded) by optimizing the variance according to Eq. [sent-77, score-0.993]
</p><p>46 In practice this involves ﬁnding a single relevant dimension ﬁrst, and then iteratively increasing the number of relevant dimensions considered while adjusting the previously found relevant dimensions. [sent-79, score-1.05]
</p><p>47 The amount by which relevant dimensions need to be adjusted is proportional to the contribution of subsequent relevant dimensions to neural spiking (the corresponding expression has the same functional form as that for relevant dimensions found by maximizing information, cf. [sent-80, score-1.489]
</p><p>48 If stimuli are either uncorrelated or correlated but Gaussian, then the previously found dimensions do not need to be adjusted when additional dimensions are introduced. [sent-82, score-0.494]
</p><p>49 All of the relevant dimensions can be found one by one, by always searching only for a single relevant dimension in the subspace orthogonal to the relevant dimensions already found. [sent-83, score-1.258]
</p><p>50 Our goal is to reconstruct relevant dimensions of neurons probed with inputs of arbitrary statistics. [sent-85, score-0.561]
</p><p>51 Advantage of doing so is that the relevant dimensions are known. [sent-88, score-0.439]
</p><p>52 It has a single relevant dimension, which we will denote as e1 . [sent-90, score-0.231]
</p><p>53 In what follows we will measure these parameters in units of the standard deviation of stimulus projections along the relevant dimension. [sent-95, score-0.484]
</p><p>54 Figure 1 shows that it is possible to obtain a good estimate of the relevant dimension e1 by maxiˆ mizing either information, as shown in panel (b), or variance, as shown in panel(c). [sent-97, score-0.408]
</p><p>55 1 there were ≈ 50, 000 spikes with average probability of spike ≈ 0. [sent-100, score-0.662]
</p><p>56 Having estimated the relevant dimension, one can proceed to sample the nonlinear input– output function. [sent-103, score-0.275]
</p><p>57 This is done by constructing histograms for P (s · vmax ) and P (s · vmax |spike) of ˆ ˆ projections onto vector vmax found by maximizing either information or variance, and taking their ˆ ratio. [sent-104, score-0.574]
</p><p>58 1(d) the spike probability of the reconstructed neuron P (spike|s · vmax ) (crosses) is compared ˆ with the probability P (spike|s1 ) used in the model (solid line). [sent-108, score-0.697]
</p><p>59 In actuality, reconstructing even just one relevant dimension from neural responses to correlated non-Gaussian inputs, such as those derived from real-world, is not an easy problem. [sent-110, score-0.42]
</p><p>60 This fact can be appreciated by considering the estimates of relevant dimension obtained from the spike-triggered average (STA) shown in panel (e). [sent-111, score-0.408]
</p><p>61 4  decorrelated STA (x) regularized decorrelated STA (x)  0. [sent-122, score-0.381]
</p><p>62 0  -6 -4 -2 0 2 4 6 filtered stimulus (sd=1)  Figure 1: Analysis of a model visual neuron with one relevant dimension shown in (a). [sent-124, score-0.588]
</p><p>63 In panel (h), the spike probabilities as a function of stimulus projections onto the dimensions obtained as decorrelated STA (blue crosses) and regularized decorrelated STA (red crosses) are compared to a spike probability used to generate spikes (solid line). [sent-130, score-2.039]
</p><p>64 Attempt to regularize the inverse of covariance matrix results in a closer match to the true relevant dimension [15, 16, 17, 18, 19] and has a projection value of 0. [sent-134, score-0.398]
</p><p>65 While it appears to be less noisy, the regularized decorrelated STA can have systematic deviations from the true relevant dimensions [9, 20, 2, 11]. [sent-136, score-0.677]
</p><p>66 4  Comparison of Performance with Finite Data  In the limit of inﬁnite data the relevant dimensions can be found by maximizing variance, information, or other objective functions [1]. [sent-139, score-0.598]
</p><p>67 In a real experiment, with a dataset of ﬁnite size, the optimal vector found by any of the R´ nyi divergences v will deviate from the true relevant dimension e1 . [sent-140, score-0.927]
</p><p>68 e ˆ ˆ In this section we compare the robustness of optimization strategies based on R´ nyi divergences of e various orders, including least squares ﬁtting (α = 2) and information maximization (α = 1), as the dataset size decreases and/or neural noise increases. [sent-141, score-0.81]
</p><p>69 The deviation from the true relevant dimension δv = v − e1 arises because the probability distriˆ ˆ butions (4) are estimated from experimental histograms and differ from the distributions found in the limit of inﬁnite data size. [sent-142, score-0.431]
</p><p>70 The effects of noise on the reconstruction can be characterized by taking the dot product between the relevant dimension and the optimal vector for a particular data 1 sample: v · e1 = 1 − 2 δv2 , where both v and e1 are normalized, and δv is by deﬁnition orthogoˆ ˆ ˆ ˆ nal to e1 . [sent-143, score-0.404]
</p><p>71 Subscript (α) denotes the order of the R´ nyi divergence used ˆ e as an objective function. [sent-146, score-0.526]
</p><p>72 Next, in order to measure the expected spread of optimal dimensions around the true one e1 , we need to evaluate ˆ 2 (α) (α)T (α) −2 δv = Tr F F H , and therefore need to know the variance of the gradient of F averaged across different equivalent datasets. [sent-150, score-0.324]
</p><p>73 Assuming that the probability of generating a (α) (α) (α) spike is independent for different bins, we ﬁnd that Fi Fj = Bij /Nspike , where (α)  Bij = α2  dxP (x|spike)Cij (x)  P (x|spike) P (x)  2α−4  d P (x|spike) dx P (x)  2  . [sent-151, score-0.579]
</p><p>74 (13)  Therefore an expected error in the reconstruction of the optimal ﬁlter by maximizing variance is inversely proportional to the number of spikes: v · e1 ≈ 1 − ˆ ˆ  1 Tr [BH −2 ] δv2 = 1 − , 2 2Nspike  (14)  where we omitted superscripts (α) for clarity. [sent-152, score-0.217]
</p><p>75 Tr denotes the trace taken in the subspace orthogonal to the relevant dimension (deviations along the relevant dimension have no meaning [2], which mathematically manifests itself in dimension e1 being an eigenvector of matrices H and B with the ˆ zero eigenvalue). [sent-153, score-0.853]
</p><p>76 The asymptotic errors in this case are completely determined by the trace of the Hessian of information, δv2 ∝ Tr A−1 , reproducing the previously published result for maximally informative dimensions [2]. [sent-155, score-0.337]
</p><p>77 This dependence is in common with expected errors of relevant dimensions found by maximizing information [2], as well as methods based on computing the spike-triggered average both for white noise [1, 21, 22] and correlated Gaussian inputs [2]. [sent-157, score-0.748]
</p><p>78 Next we examine which of the R´ nyi divergences provides the smallest asymptotic error (14) for e estimating relevant dimensions. [sent-158, score-0.855]
</p><p>79 Thus, among the various optimization strategies based on R´ nyi divergences, Kullback-Leibler divergence (α = 1) e has the smallest asymptotic errors. [sent-165, score-0.653]
</p><p>80 The least square ﬁtting corresponds to optimization based on R´ nyi divergence with α = 2, and is expected to have larger errors than optimization based on e Kullback-Leibler divergence (α = 1) implementing information maximization. [sent-166, score-0.821]
</p><p>81 Below we use numerical simulations with model cells to compare the performance of information (α = 1) and variance (α = 2) maximization strategies in the regime of relatively small numbers  of spikes. [sent-168, score-0.358]
</p><p>82 The relevant dimension for each simulated spike train was obtained as an average of 4 jackknife estimates computed by setting aside 1/4 of the data as a test set. [sent-179, score-0.919]
</p><p>83 We ﬁnd that reconstruction errors are comparable for both information and variance maximization strategies, and are better or equal (at very low spike numbers) than STA-based methods. [sent-183, score-0.829]
</p><p>84 Information maximization achieves signiﬁcantly smaller errors than the least-square ﬁtting, when we analyze results for all simulations for four different models cells and spike numbers (p < 10−4 , paired t-test). [sent-184, score-0.774]
</p><p>85 0  maximizing information maximizing variance regularized decorrelated STA  projection on true dimension  0. [sent-187, score-0.683]
</p><p>86 5  Figure 2: Projection of vector vmax obtained by maximizing information (red ﬁlled symbols) or ˆ variance (blue open symbols) on the true relevant dimension e1 is plotted as a function of ratio beˆ tween stimulus dimensionality D and the number of spikes Nspike , with D = 900. [sent-211, score-0.902]
</p><p>87 Simulations were carried out for model visual neurons with one relevant dimension from Fig. [sent-212, score-0.452]
</p><p>88 The left panel also shows results obtained using spike-triggered average (STA, gray) and decorrelated STA (dSTA, black). [sent-220, score-0.234]
</p><p>89 In the right panel, we replot results for information and variance optimization together with those for regularized decorrelated STA (RdSTA, green open symbols). [sent-221, score-0.353]
</p><p>90 5  Conclusions  In this paper we compared accuracy of a family of optimization strategies for analyzing neural responses to natural stimuli based on R´ nyi divergences. [sent-223, score-0.633]
</p><p>91 Finding relevant dimensions by maximizing e one of the merit functions, R´ nyi divergence of order 2, corresponds to ﬁtting the linear-nonlinear e model in the least-square sense to neural spike trains. [sent-224, score-1.688]
</p><p>92 We derived errors expected for relevant dimensions computed by maximizing R´ nyi divergences of are bitrary order in the asymptotic regime of large spike numbers. [sent-226, score-1.776]
</p><p>93 The smallest errors were achieved not in the case of (nonlinear) least square ﬁtting of the linear-nonlinear model to the neural spike trains (R´ nyi divergence of order 2), but with information maximization (based on Kullback-Leibler die vergence). [sent-227, score-1.34]
</p><p>94 Numeric simulations on the performance of both information and variance maximization strategies showed that both algorithms performed well even when the number of spikes is very small. [sent-228, score-0.372]
</p><p>95 With small numbers of spikes, reconstructions based on information maximization had also slightly, but signiﬁcantly, smaller errors those of least-square ﬁtting. [sent-229, score-0.212]
</p><p>96 This makes the problem of ﬁnding relevant dimensions, together with the problem of lossy compression [23, 3], one of examples where  information-theoretic measures are no more data limited than those derived from least squares. [sent-230, score-0.315]
</p><p>97 It remains possible, however, that other merit functions based on non-polynomial divergence measures could provide even smaller reconstruction errors than information maximization. [sent-231, score-0.292]
</p><p>98 Real-time performance of a movement-sensitive neuron in the blowﬂy visual system: coding and information transfer in short spike sequences. [sent-277, score-0.661]
</p><p>99 Spectral-temporal receptive ﬁelds of nonlinear auditory neurons obtained using natural sounds. [sent-363, score-0.186]
</p><p>100 Estimating spatio-temporal receptive ﬁelds of auditory and visual neurons from their responses to natural stimuli. [sent-379, score-0.232]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spike', 0.551), ('nyi', 0.384), ('pv', 0.26), ('sta', 0.256), ('relevant', 0.231), ('dimensions', 0.208), ('decorrelated', 0.171), ('divergences', 0.163), ('stimulus', 0.124), ('divergence', 0.116), ('dimension', 0.114), ('vmax', 0.114), ('spikes', 0.111), ('maximizing', 0.098), ('variance', 0.086), ('dxp', 0.082), ('dxpv', 0.082), ('fmax', 0.078), ('maximization', 0.077), ('inputs', 0.068), ('dsp', 0.065), ('sharpee', 0.065), ('panel', 0.063), ('crosses', 0.061), ('merit', 0.061), ('errors', 0.058), ('visual', 0.054), ('projection', 0.053), ('receptive', 0.053), ('projections', 0.05), ('theunissen', 0.049), ('tting', 0.046), ('ring', 0.046), ('nonlinear', 0.044), ('ruyter', 0.043), ('stimuli', 0.043), ('strategies', 0.043), ('asymptotic', 0.043), ('cij', 0.04), ('regime', 0.04), ('tr', 0.039), ('neural', 0.039), ('regularized', 0.039), ('hessian', 0.038), ('ds', 0.038), ('rust', 0.036), ('bij', 0.036), ('lossy', 0.036), ('square', 0.036), ('responses', 0.036), ('found', 0.035), ('auditory', 0.035), ('cells', 0.035), ('smallest', 0.034), ('reconstruction', 0.033), ('optimization', 0.033), ('ergodic', 0.033), ('filtered', 0.033), ('nspike', 0.033), ('ringach', 0.033), ('neuron', 0.032), ('family', 0.032), ('reconstructions', 0.031), ('simulations', 0.031), ('neurons', 0.031), ('symbols', 0.03), ('gradient', 0.03), ('bins', 0.029), ('hunter', 0.028), ('sen', 0.028), ('mutual', 0.028), ('dx', 0.028), ('deviations', 0.028), ('maximally', 0.028), ('response', 0.028), ('compression', 0.027), ('along', 0.027), ('averaging', 0.027), ('objective', 0.026), ('noise', 0.026), ('units', 0.026), ('eliciting', 0.026), ('deviation', 0.026), ('histograms', 0.025), ('information', 0.024), ('bh', 0.024), ('schwartz', 0.024), ('triggered', 0.024), ('natural', 0.023), ('probed', 0.023), ('aside', 0.023), ('bialek', 0.023), ('averages', 0.023), ('orders', 0.022), ('carried', 0.022), ('numbers', 0.022), ('checked', 0.022), ('matrices', 0.022), ('least', 0.021), ('numeric', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="36-tfidf-1" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>Author: Tatyana Sharpee</p><p>Abstract: This paper compares a family of methods for characterizing neural feature selectivity with natural stimuli in the framework of the linear-nonlinear model. In this model, the neural ﬁring rate is a nonlinear function of a small number of relevant stimulus components. The relevant stimulus dimensions can be found by maximizing one of the family of objective functions, R´ nyi divergences of different e orders [1, 2]. We show that maximizing one of them, R´ nyi divergence of ore der 2, is equivalent to least-square ﬁtting of the linear-nonlinear model to neural data. Next, we derive reconstruction errors in relevant dimensions found by maximizing R´ nyi divergences of arbitrary order in the asymptotic limit of large spike e numbers. We ﬁnd that the smallest errors are obtained with R´ nyi divergence of e order 1, also known as Kullback-Leibler divergence. This corresponds to ﬁnding relevant dimensions by maximizing mutual information [2]. We numerically test how these optimization schemes perform in the regime of low signal-to-noise ratio (small number of spikes and increasing neural noise) for model visual neurons. We ﬁnd that optimization schemes based on either least square ﬁtting or information maximization perform well even when number of spikes is small. Information maximization provides slightly, but signiﬁcantly, better reconstructions than least square ﬁtting. This makes the problem of ﬁnding relevant dimensions, together with the problem of lossy compression [3], one of examples where informationtheoretic measures are no more data limited than those derived from least squares. 1</p><p>2 0.31813487 <a title="36-tfidf-2" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>Author: Maneesh Sahani, Byron M. Yu, John P. Cunningham, Krishna V. Shenoy</p><p>Abstract: Neural spike trains present challenges to analytical efforts due to their noisy, spiking nature. Many studies of neuroscientiﬁc and neural prosthetic importance rely on a smoothed, denoised estimate of the spike train’s underlying ﬁring rate. Current techniques to ﬁnd time-varying ﬁring rates require ad hoc choices of parameters, offer no conﬁdence intervals on their estimates, and can obscure potentially important single trial variability. We present a new method, based on a Gaussian Process prior, for inferring probabilistically optimal estimates of ﬁring rate functions underlying single or multiple neural spike trains. We test the performance of the method on simulated data and experimentally gathered neural spike trains, and we demonstrate improvements over conventional estimators. 1</p><p>3 0.25493959 <a title="36-tfidf-3" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>Author: Jonathan W. Pillow, Peter E. Latham</p><p>Abstract: Point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, owing in part to the fact that they do not take into account multiple stages of processing. Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more ﬂexible for ﬁtting neural data. More importantly, it allows us to estimate connectivity patterns among neurons (both observed and unobserved), and may provide insight into how networks process sensory input. We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model’s performance using a simulated example network consisting of two coupled neurons.</p><p>4 0.24871138 <a title="36-tfidf-4" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><p>5 0.18387602 <a title="36-tfidf-5" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>Author: Guenther Zeck, Matthias Bethge, Jakob H. Macke</p><p>Abstract: S timulus selectivity of sensory neurons is often characterized by estimating their receptive ﬁeld properties such as orientation selectivity. Receptive ﬁelds are usually derived from the mean (or covariance) of the spike-triggered stimulus ensemble. This approach treats each spike as an independent message but does not take into account that information might be conveyed through patterns of neural activity that are distributed across space or time. Can we ﬁnd a concise description for the processing of a whole population of neurons analogous to the receptive ﬁeld for single neurons? Here, we present a generalization of the linear receptive ﬁeld which is not bound to be triggered on individual spikes but can be meaningfully linked to distributed response patterns. More precisely, we seek to identify those stimulus features and the corresponding patterns of neural activity that are most reliably coupled. We use an extension of reverse-correlation methods based on canonical correlation analysis. The resulting population receptive ﬁelds span the subspace of stimuli that is most informative about the population response. We evaluate our approach using both neuronal models and multi-electrode recordings from rabbit retinal ganglion cells. We show how the model can be extended to capture nonlinear stimulus-response relationships using kernel canonical correlation analysis, which makes it possible to test different coding mechanisms. Our technique can also be used to calculate receptive ﬁelds from multi-dimensional neural measurements such as those obtained from dynamic imaging methods. 1</p><p>6 0.17152722 <a title="36-tfidf-6" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>7 0.16608268 <a title="36-tfidf-7" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>8 0.15741165 <a title="36-tfidf-8" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>9 0.13753286 <a title="36-tfidf-9" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>10 0.10993429 <a title="36-tfidf-10" href="./nips-2007-Learning_to_classify_complex_patterns_using_a_VLSI_network_of_spiking_neurons.html">117 nips-2007-Learning to classify complex patterns using a VLSI network of spiking neurons</a></p>
<p>11 0.086447924 <a title="36-tfidf-11" href="./nips-2007-Modelling_motion_primitives_and_their_timing_in_biologically_executed_movements.html">133 nips-2007-Modelling motion primitives and their timing in biologically executed movements</a></p>
<p>12 0.077098548 <a title="36-tfidf-12" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>13 0.073025458 <a title="36-tfidf-13" href="./nips-2007-A_configurable_analog_VLSI_neural_network_with_spiking_neurons_and_self-regulating_plastic_synapses.html">14 nips-2007-A configurable analog VLSI neural network with spiking neurons and self-regulating plastic synapses</a></p>
<p>14 0.070600837 <a title="36-tfidf-14" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>15 0.063690908 <a title="36-tfidf-15" href="./nips-2007-Contraction_Properties_of_VLSI_Cooperative_Competitive_Neural_Networks_of_Spiking_Neurons.html">60 nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</a></p>
<p>16 0.059672952 <a title="36-tfidf-16" href="./nips-2007-Random_Projections_for_Manifold_Learning.html">161 nips-2007-Random Projections for Manifold Learning</a></p>
<p>17 0.057738446 <a title="36-tfidf-17" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>18 0.048725337 <a title="36-tfidf-18" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>19 0.048626989 <a title="36-tfidf-19" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<p>20 0.047226433 <a title="36-tfidf-20" href="./nips-2007-Learning_the_structure_of_manifolds_using_random_projections.html">116 nips-2007-Learning the structure of manifolds using random projections</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.198), (1, 0.164), (2, 0.354), (3, 0.103), (4, 0.002), (5, -0.088), (6, 0.02), (7, -0.065), (8, 0.001), (9, 0.072), (10, -0.065), (11, 0.029), (12, 0.034), (13, 0.008), (14, 0.005), (15, -0.006), (16, 0.128), (17, 0.2), (18, 0.157), (19, 0.154), (20, -0.077), (21, -0.06), (22, -0.026), (23, 0.118), (24, 0.024), (25, -0.086), (26, 0.049), (27, -0.055), (28, -0.014), (29, -0.003), (30, 0.035), (31, -0.086), (32, -0.048), (33, 0.073), (34, 0.024), (35, 0.075), (36, -0.003), (37, 0.033), (38, -0.046), (39, -0.021), (40, -0.037), (41, -0.046), (42, 0.057), (43, 0.105), (44, -0.02), (45, -0.094), (46, 0.024), (47, 0.01), (48, -0.1), (49, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97843295 <a title="36-lsi-1" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>Author: Tatyana Sharpee</p><p>Abstract: This paper compares a family of methods for characterizing neural feature selectivity with natural stimuli in the framework of the linear-nonlinear model. In this model, the neural ﬁring rate is a nonlinear function of a small number of relevant stimulus components. The relevant stimulus dimensions can be found by maximizing one of the family of objective functions, R´ nyi divergences of different e orders [1, 2]. We show that maximizing one of them, R´ nyi divergence of ore der 2, is equivalent to least-square ﬁtting of the linear-nonlinear model to neural data. Next, we derive reconstruction errors in relevant dimensions found by maximizing R´ nyi divergences of arbitrary order in the asymptotic limit of large spike e numbers. We ﬁnd that the smallest errors are obtained with R´ nyi divergence of e order 1, also known as Kullback-Leibler divergence. This corresponds to ﬁnding relevant dimensions by maximizing mutual information [2]. We numerically test how these optimization schemes perform in the regime of low signal-to-noise ratio (small number of spikes and increasing neural noise) for model visual neurons. We ﬁnd that optimization schemes based on either least square ﬁtting or information maximization perform well even when number of spikes is small. Information maximization provides slightly, but signiﬁcantly, better reconstructions than least square ﬁtting. This makes the problem of ﬁnding relevant dimensions, together with the problem of lossy compression [3], one of examples where informationtheoretic measures are no more data limited than those derived from least squares. 1</p><p>2 0.81173718 <a title="36-lsi-2" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>Author: Dominik Endres, Mike Oram, Johannes Schindelin, Peter Foldiak</p><p>Abstract: The peristimulus time histogram (PSTH) and its more continuous cousin, the spike density function (SDF) are staples in the analytic toolkit of neurophysiologists. The former is usually obtained by binning spike trains, whereas the standard method for the latter is smoothing with a Gaussian kernel. Selection of a bin width or a kernel size is often done in an relatively arbitrary fashion, even though there have been recent attempts to remedy this situation [1, 2]. We develop an exact Bayesian, generative model approach to estimating PSTHs and demonstate its superiority to competing methods. Further advantages of our scheme include automatic complexity control and error bars on its predictions. 1</p><p>3 0.76558971 <a title="36-lsi-3" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>Author: Jonathan W. Pillow, Peter E. Latham</p><p>Abstract: Point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, owing in part to the fact that they do not take into account multiple stages of processing. Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more ﬂexible for ﬁtting neural data. More importantly, it allows us to estimate connectivity patterns among neurons (both observed and unobserved), and may provide insight into how networks process sensory input. We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model’s performance using a simulated example network consisting of two coupled neurons.</p><p>4 0.75803471 <a title="36-lsi-4" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>Author: Maneesh Sahani, Byron M. Yu, John P. Cunningham, Krishna V. Shenoy</p><p>Abstract: Neural spike trains present challenges to analytical efforts due to their noisy, spiking nature. Many studies of neuroscientiﬁc and neural prosthetic importance rely on a smoothed, denoised estimate of the spike train’s underlying ﬁring rate. Current techniques to ﬁnd time-varying ﬁring rates require ad hoc choices of parameters, offer no conﬁdence intervals on their estimates, and can obscure potentially important single trial variability. We present a new method, based on a Gaussian Process prior, for inferring probabilistically optimal estimates of ﬁring rate functions underlying single or multiple neural spike trains. We test the performance of the method on simulated data and experimentally gathered neural spike trains, and we demonstrate improvements over conventional estimators. 1</p><p>5 0.73720556 <a title="36-lsi-5" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><p>6 0.67001539 <a title="36-lsi-6" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>7 0.5258038 <a title="36-lsi-7" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>8 0.51449311 <a title="36-lsi-8" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>9 0.45294344 <a title="36-lsi-9" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>10 0.43450227 <a title="36-lsi-10" href="./nips-2007-Contraction_Properties_of_VLSI_Cooperative_Competitive_Neural_Networks_of_Spiking_Neurons.html">60 nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</a></p>
<p>11 0.41699961 <a title="36-lsi-11" href="./nips-2007-Modelling_motion_primitives_and_their_timing_in_biologically_executed_movements.html">133 nips-2007-Modelling motion primitives and their timing in biologically executed movements</a></p>
<p>12 0.37400362 <a title="36-lsi-12" href="./nips-2007-Estimating_disparity_with_confidence_from_energy_neurons.html">81 nips-2007-Estimating disparity with confidence from energy neurons</a></p>
<p>13 0.31481147 <a title="36-lsi-13" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<p>14 0.28440449 <a title="36-lsi-14" href="./nips-2007-Learning_to_classify_complex_patterns_using_a_VLSI_network_of_spiking_neurons.html">117 nips-2007-Learning to classify complex patterns using a VLSI network of spiking neurons</a></p>
<p>15 0.27722228 <a title="36-lsi-15" href="./nips-2007-Estimating_divergence_functionals_and_the_likelihood_ratio_by_penalized_convex_risk_minimization.html">82 nips-2007-Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization</a></p>
<p>16 0.27570471 <a title="36-lsi-16" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>17 0.25014341 <a title="36-lsi-17" href="./nips-2007-Augmented_Functional_Time_Series_Representation_and_Forecasting_with_Gaussian_Processes.html">28 nips-2007-Augmented Functional Time Series Representation and Forecasting with Gaussian Processes</a></p>
<p>18 0.24599119 <a title="36-lsi-18" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>19 0.24491447 <a title="36-lsi-19" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>20 0.24407592 <a title="36-lsi-20" href="./nips-2007-Experience-Guided_Search%3A_A_Theory_of_Attentional_Control.html">85 nips-2007-Experience-Guided Search: A Theory of Attentional Control</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.04), (13, 0.021), (16, 0.166), (18, 0.019), (19, 0.022), (21, 0.042), (31, 0.023), (34, 0.022), (35, 0.03), (42, 0.216), (47, 0.078), (49, 0.015), (83, 0.128), (85, 0.024), (87, 0.011), (90, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84840757 <a title="36-lda-1" href="./nips-2007-TrueSkill_Through_Time%3A_Revisiting_the_History_of_Chess.html">208 nips-2007-TrueSkill Through Time: Revisiting the History of Chess</a></p>
<p>Author: Pierre Dangauthier, Ralf Herbrich, Tom Minka, Thore Graepel</p><p>Abstract: We extend the Bayesian skill rating system TrueSkill to infer entire time series of skills of players by smoothing through time instead of ﬁltering. The skill of each participating player, say, every year is represented by a latent skill variable which is aﬀected by the relevant game outcomes that year, and coupled with the skill variables of the previous and subsequent year. Inference in the resulting factor graph is carried out by approximate message passing (EP) along the time series of skills. As before the system tracks the uncertainty about player skills, explicitly models draws, can deal with any number of competing entities and can infer individual skills from team results. We extend the system to estimate player-speciﬁc draw margins. Based on these models we present an analysis of the skill curves of important players in the history of chess over the past 150 years. Results include plots of players’ lifetime skill development as well as the ability to compare the skills of diﬀerent players across time. Our results indicate that a) the overall playing strength has increased over the past 150 years, and b) that modelling a player’s ability to force a draw provides signiﬁcantly better predictive power. 1</p><p>same-paper 2 0.83642209 <a title="36-lda-2" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>Author: Tatyana Sharpee</p><p>Abstract: This paper compares a family of methods for characterizing neural feature selectivity with natural stimuli in the framework of the linear-nonlinear model. In this model, the neural ﬁring rate is a nonlinear function of a small number of relevant stimulus components. The relevant stimulus dimensions can be found by maximizing one of the family of objective functions, R´ nyi divergences of different e orders [1, 2]. We show that maximizing one of them, R´ nyi divergence of ore der 2, is equivalent to least-square ﬁtting of the linear-nonlinear model to neural data. Next, we derive reconstruction errors in relevant dimensions found by maximizing R´ nyi divergences of arbitrary order in the asymptotic limit of large spike e numbers. We ﬁnd that the smallest errors are obtained with R´ nyi divergence of e order 1, also known as Kullback-Leibler divergence. This corresponds to ﬁnding relevant dimensions by maximizing mutual information [2]. We numerically test how these optimization schemes perform in the regime of low signal-to-noise ratio (small number of spikes and increasing neural noise) for model visual neurons. We ﬁnd that optimization schemes based on either least square ﬁtting or information maximization perform well even when number of spikes is small. Information maximization provides slightly, but signiﬁcantly, better reconstructions than least square ﬁtting. This makes the problem of ﬁnding relevant dimensions, together with the problem of lossy compression [3], one of examples where informationtheoretic measures are no more data limited than those derived from least squares. 1</p><p>3 0.73513931 <a title="36-lda-3" href="./nips-2007-Robust_Regression_with_Twinned_Gaussian_Processes.html">170 nips-2007-Robust Regression with Twinned Gaussian Processes</a></p>
<p>Author: Andrew Naish-guzman, Sean Holden</p><p>Abstract: We propose a Gaussian process (GP) framework for robust inference in which a GP prior on the mixing weights of a two-component noise model augments the standard process over latent function values. This approach is a generalization of the mixture likelihood used in traditional robust GP regression, and a specialization of the GP mixture models suggested by Tresp [1] and Rasmussen and Ghahramani [2]. The value of this restriction is in its tractable expectation propagation updates, which allow for faster inference and model selection, and better convergence than the standard mixture. An additional beneﬁt over the latter method lies in our ability to incorporate knowledge of the noise domain to inﬂuence predictions, and to recover with the predictive distribution information about the outlier distribution via the gating process. The model has asymptotic complexity equal to that of conventional robust methods, but yields more conﬁdent predictions on benchmark problems than classical heavy-tailed models and exhibits improved stability for data with clustered corruptions, for which they fail altogether. We show further how our approach can be used without adjustment for more smoothly heteroscedastic data, and suggest how it could be extended to more general noise models. We also address similarities with the work of Goldberg et al. [3].</p><p>4 0.73456585 <a title="36-lda-4" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><p>5 0.73352706 <a title="36-lda-5" href="./nips-2007-Topmoumoute_Online_Natural_Gradient_Algorithm.html">206 nips-2007-Topmoumoute Online Natural Gradient Algorithm</a></p>
<p>Author: Nicolas L. Roux, Pierre-antoine Manzagol, Yoshua Bengio</p><p>Abstract: Guided by the goal of obtaining an optimization algorithm that is both fast and yields good generalization, we study the descent direction maximizing the decrease in generalization error or the probability of not increasing generalization error. The surprising result is that from both the Bayesian and frequentist perspectives this can yield the natural gradient direction. Although that direction can be very expensive to compute we develop an efﬁcient, general, online approximation to the natural gradient descent which is suited to large scale problems. We report experimental results showing much faster convergence in computation time and in number of iterations with TONGA (Topmoumoute Online natural Gradient Algorithm) than with stochastic gradient descent, even on very large datasets.</p><p>6 0.73150921 <a title="36-lda-6" href="./nips-2007-Contraction_Properties_of_VLSI_Cooperative_Competitive_Neural_Networks_of_Spiking_Neurons.html">60 nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</a></p>
<p>7 0.6975283 <a title="36-lda-7" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>8 0.69302881 <a title="36-lda-8" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>9 0.67915338 <a title="36-lda-9" href="./nips-2007-Message_Passing_for_Max-weight_Independent_Set.html">128 nips-2007-Message Passing for Max-weight Independent Set</a></p>
<p>10 0.66464287 <a title="36-lda-10" href="./nips-2007-The_Generalized_FITC_Approximation.html">195 nips-2007-The Generalized FITC Approximation</a></p>
<p>11 0.63708264 <a title="36-lda-11" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>12 0.62588948 <a title="36-lda-12" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>13 0.62073785 <a title="36-lda-13" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>14 0.61512935 <a title="36-lda-14" href="./nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">79 nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<p>15 0.61160582 <a title="36-lda-15" href="./nips-2007-Fast_Variational_Inference_for_Large-scale_Internet_Diagnosis.html">87 nips-2007-Fast Variational Inference for Large-scale Internet Diagnosis</a></p>
<p>16 0.60488379 <a title="36-lda-16" href="./nips-2007-Selecting_Observations_against_Adversarial_Objectives.html">174 nips-2007-Selecting Observations against Adversarial Objectives</a></p>
<p>17 0.6011613 <a title="36-lda-17" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>18 0.59599072 <a title="36-lda-18" href="./nips-2007-Adaptive_Online_Gradient_Descent.html">21 nips-2007-Adaptive Online Gradient Descent</a></p>
<p>19 0.59194744 <a title="36-lda-19" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>20 0.59021908 <a title="36-lda-20" href="./nips-2007-Probabilistic_Matrix_Factorization.html">158 nips-2007-Probabilistic Matrix Factorization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
