<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>4 nips-2007-A Constraint Generation Approach to Learning Stable Linear Dynamical Systems</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-4" href="#">nips2007-4</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>4 nips-2007-A Constraint Generation Approach to Learning Stable Linear Dynamical Systems</h1>
<br/><p>Source: <a title="nips-2007-4-pdf" href="http://papers.nips.cc/paper/3358-a-constraint-generation-approach-to-learning-stable-linear-dynamical-systems.pdf">pdf</a></p><p>Author: Byron Boots, Geoffrey J. Gordon, Sajid M. Siddiqi</p><p>Abstract: Stability is a desirable characteristic for linear dynamical systems, but it is often ignored by algorithms that learn these systems from data. We propose a novel method for learning stable linear dynamical systems: we formulate an approximation of the problem as a convex program, start with a solution to a relaxed version of the program, and incrementally add constraints to improve stability. Rather than continuing to generate constraints until we reach a feasible solution, we test stability at each step; because the convex program is only an approximation of the desired problem, this early stopping rule can yield a higher-quality solution. We apply our algorithm to the task of learning dynamic textures from image sequences as well as to modeling biosurveillance drug-sales data. The constraint generation approach leads to noticeable improvement in the quality of simulated sequences. We compare our method to those of Lacy and Bernstein [1, 2], with positive results in terms of accuracy, quality of simulated sequences, and efﬁciency. 1</p><p>Reference: <a title="nips-2007-4-reference" href="../nips2007_reference/nips-2007-A_Constraint_Generation_Approach_to_Learning_Stable_Linear_Dynamical_Systems_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We propose a novel method for learning stable linear dynamical systems: we formulate an approximation of the problem as a convex program, start with a solution to a relaxed version of the program, and incrementally add constraints to improve stability. [sent-10, score-0.605]
</p><p>2 Rather than continuing to generate constraints until we reach a feasible solution, we test stability at each step; because the convex program is only an approximation of the desired problem, this early stopping rule can yield a higher-quality solution. [sent-11, score-0.403]
</p><p>3 We apply our algorithm to the task of learning dynamic textures from image sequences as well as to modeling biosurveillance drug-sales data. [sent-12, score-0.492]
</p><p>4 The constraint generation approach leads to noticeable improvement in the quality of simulated sequences. [sent-13, score-0.577]
</p><p>5 In the case where the state is real-valued and the noise terms are assumed to be Gaussian, the resulting model is called a linear dynamical system (LDS), also known as a Kalman Filter [3]. [sent-17, score-0.238]
</p><p>6 An LDS with dynamics matrix A is stable if all of A’s eigenvalues have magnitude at most 1, i. [sent-20, score-0.449]
</p><p>7 However, when learning from ﬁnite data samples, the least squares solution may be unstable even if the system is stable [6]. [sent-25, score-0.816]
</p><p>8 The drawback of ignoring stability is most apparent when simulating long sequences from the system in order to generate representative data or infer stretches of missing values. [sent-26, score-0.305]
</p><p>9 We propose a convex optimization algorithm for learning the dynamics matrix while guaranteeing stability. [sent-27, score-0.204]
</p><p>10 An estimate of the underlying state sequence is ﬁrst obtained using subspace identiﬁcation. [sent-28, score-0.28]
</p><p>11 We then formulate the least-squares problem for the dynamics matrix as a quadratic program ˆ (QP) [7], initially without constraints. [sent-29, score-0.294]
</p><p>12 However, any unstable solution allows us to derive a linear constraint which we then add  to our original QP and re-solve. [sent-31, score-0.522]
</p><p>13 The above two steps are iterated until we reach a stable solution, which is then reﬁned by a simple interpolation to obtain the best possible stable estimate. [sent-32, score-0.592]
</p><p>14 Our method can be viewed as constraint generation for an underlying convex program with a feasible set of all matrices with singular values at most 1, similar to work in control systems [1]. [sent-33, score-0.989]
</p><p>15 However, we terminate before reaching feasibility in the convex program, by checking for matrix stability after each new constraint. [sent-34, score-0.268]
</p><p>16 This makes our algorithm less conservative than previous methods for enforcing stability since it chooses the best of a larger set of stable dynamics matrices. [sent-35, score-0.538]
</p><p>17 The difference in the resulting stable systems is noticeable when simulating data. [sent-36, score-0.296]
</p><p>18 The constraint generation approach also achieves much greater efﬁciency than previous methods in our experiments. [sent-37, score-0.51]
</p><p>19 One application of LDSs in computer vision is learning dynamic textures from video data [8]. [sent-38, score-0.346]
</p><p>20 An advantage of learning dynamic textures is the ability to play back a realistic-looking generated sequence of any desired duration. [sent-39, score-0.367]
</p><p>21 In practice, however, videos synthesized from dynamic texture models can quickly degenerate because of instability in the underlying LDS. [sent-40, score-0.457]
</p><p>22 In contrast, sequences generated from dynamic textures learned by our method remain “sane” even after arbitrarily long durations. [sent-41, score-0.419]
</p><p>23 We also apply our algorithm to learning baseline dynamic models of over-the-counter (OTC) drug sales for biosurveillance, and sunspot numbers from the UCR archive [9]. [sent-42, score-0.611]
</p><p>24 Within subspace methods, techniques have been developed to enforce stability by augmenting the extended observability matrix with zeros [6] or adding a regularization term to the least squares objective [11]. [sent-47, score-0.655]
</p><p>25 They formulate the problem as a semideﬁnite program (SDP) whose objective minimizes the state sequence reconstruction error, and whose constraint bounds the largest singular value by 1. [sent-49, score-0.697]
</p><p>26 This convex constraint is obtained by rewriting the nonlinear matrix inequality In − AAT 0 as a linear matrix inequality [12], where In is the n × n identity matrix. [sent-50, score-0.414]
</p><p>27 The existence of this constraint also proves the convexity of the σ1 ≤ 1 region. [sent-52, score-0.235]
</p><p>28 In our experiments, this causes LB-2 to perform worse than LB-1 (for any δ) in terms of the state sequence reconstruction error, even while obtaining solutions outside the feasible region of LB-1. [sent-56, score-0.424]
</p><p>29 To summarize the distinction between constraint generation, LB-1 and LB-2: it is hard to have both the right objective function (reconstruction error) and the right feasible region (the set of stable matrices). [sent-59, score-0.764]
</p><p>30 LB-1 optimizes the right objective but over the wrong feasible region (the set of matrices with σ1 ≤ 1). [sent-60, score-0.374]
</p><p>31 LB-2 has a feasible region close to the right one, but at the cost of distorting its objective function to an extent that it fares worse than LB-1 in nearly all cases. [sent-61, score-0.233]
</p><p>32 In contrast, our method optimizes the right objective over a less conservative feasible region than that of any previous algorithm with the right objective, and this combination is shown to work the best in practice. [sent-62, score-0.233]
</p><p>33 3  Linear Dynamical Systems  The evolution of a linear dynamical system can be described by the following two equations: xt+1 = Axt + wt yt = Cxt + vt  (1)  Time is indexed by the discrete variable t. [sent-63, score-0.267]
</p><p>34 The parameters of the system are the dynamics matrix A ∈ Rn×n , the observation model C ∈ Rm×n , and the noise covariance matrices Q and R. [sent-76, score-0.359]
</p><p>35 Note that we are learning uncontrolled linear dynamical systems, though, as in previous work, control inputs can easily be incorporated into the objective function and convex program. [sent-77, score-0.29]
</p><p>36 We follow the subspace identiﬁcation literature in estimating all parameters other than the dynamics matrix. [sent-80, score-0.23]
</p><p>37 We use subspace identiﬁcation methods in our experiments for uniformity with previous work we are building on (in the control systems literature) and with work we are comparing to ([8] on the dynamic textures data). [sent-83, score-0.482]
</p><p>38 X is referred to as the extended observability matrix in the control systems literature; the tth column ˆ of X represents an estimate of the state of our LDS at time t. [sent-103, score-0.213]
</p><p>39 yd  yd+1  yd+2  ···  yd+τ −1  md×τ  A matrix of this form, with each block of rows equal to the previous block but shifted by a constant number of columns, is called a block Hankel matrix [4]. [sent-125, score-0.212]
</p><p>40 10  Sλ  A  S λ (stable matrices)  * Afinal  Sσ  A generated constraint  unstable matrices  β0  A LB-1  *  stable matrices  unstable matrices  Sσ n2  R  -10  −10  0 α  10  Figure 2: (A): Conceptual depiction of the space of n × n matrices. [sent-129, score-1.428]
</p><p>41 The region of stability (Sλ ) is non-convex while the smaller region of matrices with σ1 ≤ 1 (Sσ ) is convex. [sent-130, score-0.456]
</p><p>42 One iteration of constraint generation yields the constraint indicated by the line labeled ‘generated constraint’, and (in this case) leads to a stable solution A∗ . [sent-134, score-1.091]
</p><p>43 (B): The actual stable f and unstable regions for the space of 2×2 matrices Eα,β = [ 0. [sent-136, score-0.674]
</p><p>44 Constraint generation is able to learn a nearly optimal model from a noisy state sequence of length 7 simulated from E0,10 , with better state reconstruction error than either LB-1 or LB-2. [sent-139, score-0.63]
</p><p>45 Having multiple observations per column in D is particularly helpful when the underlying dynamical system is known to have periodicity. [sent-144, score-0.205]
</p><p>46 To account for stability, we ﬁrst formulate the dynamics matrix learning problem as a quadratic program with a feasible set that includes the set of stable dynamics matrices. [sent-149, score-0.772]
</p><p>47 Then we demonstrate how instability in its solutions can be used to generate constraints that restrict this feasible set appropriately. [sent-150, score-0.267]
</p><p>48 2 Generating Constraints The quadratic objective function above is equivalent to the least squares problem of Eq. [sent-160, score-0.299]
</p><p>49 When its solution yields ˆ ˆ an unstable matrix, the spectral radius of A (i. [sent-163, score-0.354]
</p><p>50 Ideally we would like to ˆ to calculate a convex constraint on the spectral radius. [sent-166, score-0.321]
</p><p>51 The matrices E10,0 and E0,10 are stable with λ1 = 0. [sent-170, score-0.437]
</p><p>52 3, but  their convex combination γE10,0 + (1 − γ)E0,10 is unstable for (e. [sent-171, score-0.288]
</p><p>53 This shows that the set of stable matrices is non-convex for n = 2, and in fact this is true for all n > 1. [sent-175, score-0.437]
</p><p>54 , n  [15]  Therefore every unstable matrix has a singular value greater than one, but the converse is not necessarily true. [sent-179, score-0.383]
</p><p>55 Figure 2(A) conceptually depicts the non-convex region of stability Sλ and the convex region Sσ with σ1 ≤ 1 in the space of all n × n matrices for some ﬁxed n. [sent-181, score-0.507]
</p><p>56 The stable matrices E10,0 and E0,10 reside at the edges of the ﬁgure. [sent-184, score-0.437]
</p><p>57 While results for this class of matrices vary, the constraint generation algorithm described below is able to learn a nearly optimal model from a noisy state sequence of τ = 7 simulated from E0,10 , with better state reconstruction error than LB-1 and LB-2. [sent-185, score-1.006]
</p><p>58 The QP is invoked repeatedly until the stable region, i. [sent-199, score-0.296]
</p><p>59 At each iteration, we calculate a linear constraint of the form in Eq. [sent-202, score-0.235]
</p><p>60 Once a stable matrix is obtained, it is possible to reﬁne this solution. [sent-205, score-0.36]
</p><p>61 We know that the last constraint caused our solution to cross the boundary of Sλ , so we interpolate between the last solution and the previous iteration’s solution using binary search to look for a boundary of the stable region, in order to obtain a better objective value while remaining stable. [sent-206, score-0.74]
</p><p>62 An interpolation could be attempted between the least squares solution and any stable solution. [sent-207, score-0.548]
</p><p>63 However, the stable region can be highly complex, and there may be several folds and boundaries of the stable region in the interpolated area. [sent-208, score-0.754]
</p><p>64 5  Experiments  For learning the dynamics matrix, we implemented1 least squares, constraint generation (using quadprog), LB-1 [1] and LB-2 [2] (using CVX with SeDuMi) in Matlab on a 3. [sent-210, score-0.65]
</p><p>65 However, least-squares LDSs trained in scarce-data scenarios are unstable for almost any domain, and some domains lead to unstable models up to the limit of available data (e. [sent-213, score-0.474]
</p><p>66 The goals of our experiments are to: (1) examine the state evolution and simulated observations of models learned using our method, and compare them to previous work; and (2) compare the algorithms in terms of reconstruction error and efﬁciency. [sent-217, score-0.301]
</p><p>67 We apply these algorithms to learning dynamic textures from the vision domain (Section 5. [sent-222, score-0.298]
</p><p>68 1), as well as OTC drug sales counts and sunspot numbers (Section 5. [sent-223, score-0.479]
</p><p>69 Samples from the original steam sequence and the fountain sequence. [sent-234, score-0.504]
</p><p>70 State evolution of synthesized sequences over 1000 frames (steam top, fountain bottom). [sent-236, score-0.497]
</p><p>71 The least squares solutions display instability as time progresses. [sent-237, score-0.339]
</p><p>72 The solutions obtained using LB-1 remain stable for the full 1000 frame image sequence. [sent-238, score-0.376]
</p><p>73 The constraint generation solutions, however, yield state sequences that are stable over the full 1000 frame image sequence without signiﬁcant damping. [sent-239, score-1.114]
</p><p>74 Samples drawn from a least squares synthesized sequences (top), and samples drawn from a constraint generation synthesized sequence (bottom). [sent-241, score-1.172]
</p><p>75 The constraint generation synthesized steam sequence is qualitatively better looking than the steam sequence generated by LB-1, although there is little qualitative difference between the two synthesized fountain sequences. [sent-243, score-1.637]
</p><p>76 53  Table 1: Quantitative results on the dynamic textures data for different numbers of states n. [sent-324, score-0.337]
</p><p>77 1 Stable Dynamic Textures Dynamic textures in vision can intuitively be described as models for sequences of images that exhibit some form of low-dimensional structure and recurrent (though not necessarily repeating) characteristics, e. [sent-330, score-0.352]
</p><p>78 An LDS model of a dynamic texture may synthesize an “inﬁnitely” long sequence of images by driving the model with zero mean Gaussian noise. [sent-336, score-0.281]
</p><p>79 To better visualize the difference between image sequences generated by least-squares, LB-1, and constraint generation, the evolution of each method’s state is plotted over the course of the synthesized sequences (Figure 3(B)). [sent-338, score-0.73]
</p><p>80 Sequences generated by the least squares models appear to be unstable, and this was in fact the case; both the steam and the fountain sequences resulted in unstable dynamics matrices. [sent-339, score-1.084]
</p><p>81 Conversely, the constrained subspace identiﬁcation algorithms all produced well-behaved sequences of states and stable dynamics matrices (Table 1), although constraint generation demonstrates the fastest runtime, best scalability, and lowest error of any stability-enforcing approach. [sent-340, score-1.331]
</p><p>82 A qualitative comparison of images generated by constraint generation and least squares (Figure 3(C)) indicates the effect of instability in synthesized sequences generated from dynamic texture models. [sent-341, score-1.327]
</p><p>83 While the unstable least-squares model demonstrates a dramatic increase in image contrast over time, the constraint generation model continues to generate qualitatively reasonable images. [sent-342, score-0.78]
</p><p>84 Qualitative comparisons between constraint generation and LB-1 indicate that constraint generation learns models that generate more natural-looking video sequences2 than LB-1. [sent-343, score-1.068]
</p><p>85 Table 1 demonstrates that constraint generation always has the lowest error as well as the fastest runtime. [sent-344, score-0.543]
</p><p>86 The running time of constraint generation depends on the number of constraints needed to reach a stable solution. [sent-345, score-0.843]
</p><p>87 Note that LB-1 is more efﬁcient and scalable when simulated using constraint generation (by adding constraints until Sσ is reached) than it is in its original SDP formulation. [sent-346, score-0.647]
</p><p>88 2 Stable Baseline Models for Biosurveillance We examine daily counts of OTC drug sales in pharmacies, obtained from the National Data Retail Monitor (NDRM) collection [17]. [sent-348, score-0.295]
</p><p>89 We isolate a 60-day subsequence where the data dynamics remain relatively stationary, and attempt to learn LDS parameters to be able to simulate sequences of baseline values for use in detecting anomalies. [sent-352, score-0.242]
</p><p>90 Figure 4(A) plots 22 different drug categories aggregated over all zipcodes, and Figure 4(B) plots a single drug category (cough/cold) in 29 different zipcodes separately. [sent-354, score-0.407]
</p><p>91 In both cases, constraint generation is able to use very little training data to learn a stable model that captures the periodicity in the data, while the least squares model is unstable and its predictions diverge over time. [sent-355, score-1.281]
</p><p>92 LB-1 learns a model that is stable but overconstrained, and the simulated observations quickly drift from the correct magnitudes. [sent-356, score-0.4]
</p><p>93 6  Discussion  We have introduced a novel method for learning stable linear dynamical systems. [sent-359, score-0.433]
</p><p>94 Our constraint generation algorithm is more powerful than previous methods in the sense of optimizing over a larger set of stable matrices with a suitable objective function. [sent-360, score-1.006]
</p><p>95 The constraint generation approach also has the beneﬁt of being faster than previous methods in nearly all of our experiments. [sent-361, score-0.51]
</p><p>96 One possible extension is to modify the EM algorithm for LDSs to incorporate constraint generation into the M-step in order to learn stable systems that locally maximize the observed data likelihood. [sent-362, score-0.806]
</p><p>97 400  0 1500  0 400  0 300  0 1500  0 400  0 300  0 1500  0 400  Sunspot numbers  300  0 300  LB-1  Least Constraint Training Squares Generation Data  1500  0  0  0  0  30  60  0  30  60  0  100  200  Figure 4: (A): 60 days of data for 22 drug categories aggregated over all zipcodes in the city. [sent-371, score-0.32]
</p><p>98 (B): 60 days of data for a single drug category (cough/cold) for all 29 zipcodes in the city. [sent-372, score-0.247]
</p><p>99 The training data (top), simulated output from constraint generation, output from the unstable least squares model, and output from the over-damped LB-1 model (bottom). [sent-374, score-0.741]
</p><p>100 Identiﬁcation of stable models in subspace identiﬁcation by using regularization. [sent-440, score-0.437]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stable', 0.296), ('generation', 0.275), ('steam', 0.242), ('unstable', 0.237), ('constraint', 0.235), ('fountain', 0.193), ('textures', 0.192), ('lds', 0.173), ('stability', 0.153), ('squares', 0.151), ('sunspot', 0.145), ('subspace', 0.141), ('matrices', 0.141), ('dynamical', 0.137), ('synthesized', 0.135), ('drug', 0.126), ('sequences', 0.121), ('sales', 0.121), ('zipcodes', 0.121), ('dynamic', 0.106), ('instability', 0.105), ('lacy', 0.097), ('ldss', 0.097), ('otc', 0.097), ('feasible', 0.093), ('dynamics', 0.089), ('yd', 0.084), ('identi', 0.083), ('singular', 0.082), ('region', 0.081), ('reconstruction', 0.079), ('hankel', 0.077), ('beb', 0.073), ('biosurveillance', 0.073), ('siddiqi', 0.073), ('state', 0.07), ('program', 0.069), ('sequence', 0.069), ('simulated', 0.067), ('ex', 0.067), ('texture', 0.067), ('matrix', 0.064), ('objective', 0.059), ('qp', 0.057), ('convex', 0.051), ('yt', 0.051), ('least', 0.051), ('solution', 0.05), ('atp', 0.048), ('boots', 0.048), ('mina', 0.048), ('retail', 0.048), ('sajid', 0.048), ('sms', 0.048), ('soatto', 0.048), ('ucr', 0.048), ('cg', 0.048), ('svd', 0.048), ('frame', 0.048), ('video', 0.048), ('counts', 0.048), ('evolution', 0.048), ('geoffrey', 0.046), ('vec', 0.046), ('videos', 0.044), ('pittsburgh', 0.044), ('control', 0.043), ('archive', 0.042), ('byron', 0.042), ('seth', 0.042), ('qualitative', 0.042), ('numbers', 0.039), ('quantitative', 0.039), ('images', 0.039), ('dennis', 0.038), ('quadratic', 0.038), ('constraints', 0.037), ('rm', 0.037), ('observations', 0.037), ('observability', 0.036), ('periodicity', 0.036), ('spectral', 0.035), ('aggregated', 0.034), ('ap', 0.034), ('bernstein', 0.034), ('ta', 0.034), ('observation', 0.034), ('formulate', 0.034), ('author', 0.033), ('inequalities', 0.033), ('scalable', 0.033), ('ut', 0.033), ('tr', 0.033), ('demonstrates', 0.033), ('pa', 0.033), ('interpolating', 0.032), ('radius', 0.032), ('solutions', 0.032), ('baseline', 0.032), ('system', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999887 <a title="4-tfidf-1" href="./nips-2007-A_Constraint_Generation_Approach_to_Learning_Stable_Linear_Dynamical_Systems.html">4 nips-2007-A Constraint Generation Approach to Learning Stable Linear Dynamical Systems</a></p>
<p>Author: Byron Boots, Geoffrey J. Gordon, Sajid M. Siddiqi</p><p>Abstract: Stability is a desirable characteristic for linear dynamical systems, but it is often ignored by algorithms that learn these systems from data. We propose a novel method for learning stable linear dynamical systems: we formulate an approximation of the problem as a convex program, start with a solution to a relaxed version of the program, and incrementally add constraints to improve stability. Rather than continuing to generate constraints until we reach a feasible solution, we test stability at each step; because the convex program is only an approximation of the desired problem, this early stopping rule can yield a higher-quality solution. We apply our algorithm to the task of learning dynamic textures from image sequences as well as to modeling biosurveillance drug-sales data. The constraint generation approach leads to noticeable improvement in the quality of simulated sequences. We compare our method to those of Lacy and Bernstein [1, 2], with positive results in terms of accuracy, quality of simulated sequences, and efﬁciency. 1</p><p>2 0.13918617 <a title="4-tfidf-2" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>Author: Gwenn Englebienne, Tim Cootes, Magnus Rattray</p><p>Abstract: The present work aims to model the correspondence between facial motion and speech. The face and sound are modelled separately, with phonemes being the link between both. We propose a sequential model and evaluate its suitability for the generation of the facial animation from a sequence of phonemes, which we obtain from speech. We evaluate the results both by computing the error between generated sequences and real video, as well as with a rigorous double-blind test with human subjects. Experiments show that our model compares favourably to other existing methods and that the sequences generated are comparable to real video sequences. 1</p><p>3 0.11871229 <a title="4-tfidf-3" href="./nips-2007-Cluster_Stability_for_Finite_Samples.html">46 nips-2007-Cluster Stability for Finite Samples</a></p>
<p>Author: Ohad Shamir, Naftali Tishby</p><p>Abstract: Over the past few years, the notion of stability in data clustering has received growing attention as a cluster validation criterion in a sample-based framework. However, recent work has shown that as the sample size increases, any clustering model will usually become asymptotically stable. This led to the conclusion that stability is lacking as a theoretical and practical tool. The discrepancy between this conclusion and the success of stability in practice has remained an open question, which we attempt to address. Our theoretical approach is that stability, as used by cluster validation algorithms, is similar in certain respects to measures of generalization in a model-selection framework. In such cases, the model chosen governs the convergence rate of generalization bounds. By arguing that these rates are more important than the sample size, we are led to the prediction that stability-based cluster validation algorithms should not degrade with increasing sample size, despite the asymptotic universal stability. This prediction is substantiated by a theoretical analysis as well as some empirical results. We conclude that stability remains a meaningful cluster validation criterion over ﬁnite samples. 1</p><p>4 0.10784312 <a title="4-tfidf-4" href="./nips-2007-DIFFRAC%3A_a_discriminative_and_flexible_framework_for_clustering.html">65 nips-2007-DIFFRAC: a discriminative and flexible framework for clustering</a></p>
<p>Author: Francis R. Bach, Zaïd Harchaoui</p><p>Abstract: We present a novel linear clustering framework (D IFFRAC) which relies on a linear discriminative cost function and a convex relaxation of a combinatorial optimization problem. The large convex optimization problem is solved through a sequence of lower dimensional singular value decompositions. This framework has several attractive properties: (1) although apparently similar to K-means, it exhibits superior clustering performance than K-means, in particular in terms of robustness to noise. (2) It can be readily extended to non linear clustering if the discriminative cost function is based on positive deﬁnite kernels, and can then be seen as an alternative to spectral clustering. (3) Prior information on the partition is easily incorporated, leading to state-of-the-art performance for semi-supervised learning, for clustering or classiﬁcation. We present empirical evaluations of our algorithms on synthetic and real medium-scale datasets.</p><p>5 0.076228611 <a title="4-tfidf-5" href="./nips-2007-Stability_Bounds_for_Non-i.i.d._Processes.html">184 nips-2007-Stability Bounds for Non-i.i.d. Processes</a></p>
<p>Author: Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: The notion of algorithmic stability has been used effectively in the past to derive tight generalization bounds. A key advantage of these bounds is that they are designed for speciﬁc learning algorithms, exploiting their particular properties. But, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed (i.i.d.). In many machine learning applications, however, this assumption does not hold. The observations received by the learning algorithm often have some inherent temporal dependence, which is clear in system diagnosis or time series prediction problems. This paper studies the scenario where the observations are drawn from a stationary mixing sequence, which implies a dependence between observations that weaken over time. It proves novel stability-based generalization bounds that hold even with this more general setting. These bounds strictly generalize the bounds given in the i.i.d. case. It also illustrates their application in the case of several general classes of learning algorithms, including Support Vector Regression and Kernel Ridge Regression.</p><p>6 0.067334473 <a title="4-tfidf-6" href="./nips-2007-A_Unified_Near-Optimal_Estimator_For_Dimension_Reduction_in_%24l_%5Calpha%24_%28%240%3C%5Calpha%5Cleq_2%24%29_Using_Stable_Random_Projections.html">13 nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</a></p>
<p>7 0.06662076 <a title="4-tfidf-7" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>8 0.066027865 <a title="4-tfidf-8" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>9 0.06418132 <a title="4-tfidf-9" href="./nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</a></p>
<p>10 0.064039335 <a title="4-tfidf-10" href="./nips-2007-Expectation_Maximization_and_Posterior_Constraints.html">84 nips-2007-Expectation Maximization and Posterior Constraints</a></p>
<p>11 0.062658437 <a title="4-tfidf-11" href="./nips-2007-Stable_Dual_Dynamic_Programming.html">185 nips-2007-Stable Dual Dynamic Programming</a></p>
<p>12 0.060763076 <a title="4-tfidf-12" href="./nips-2007-Random_Sampling_of_States_in_Dynamic_Programming.html">162 nips-2007-Random Sampling of States in Dynamic Programming</a></p>
<p>13 0.059429154 <a title="4-tfidf-13" href="./nips-2007-Support_Vector_Machine_Classification_with_Indefinite_Kernels.html">190 nips-2007-Support Vector Machine Classification with Indefinite Kernels</a></p>
<p>14 0.058737852 <a title="4-tfidf-14" href="./nips-2007-Receding_Horizon_Differential_Dynamic_Programming.html">163 nips-2007-Receding Horizon Differential Dynamic Programming</a></p>
<p>15 0.057808239 <a title="4-tfidf-15" href="./nips-2007-Multi-Task_Learning_via_Conic_Programming.html">134 nips-2007-Multi-Task Learning via Conic Programming</a></p>
<p>16 0.055156294 <a title="4-tfidf-16" href="./nips-2007-A_Spectral_Regularization_Framework_for_Multi-Task_Structure_Learning.html">12 nips-2007-A Spectral Regularization Framework for Multi-Task Structure Learning</a></p>
<p>17 0.054310706 <a title="4-tfidf-17" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<p>18 0.054191347 <a title="4-tfidf-18" href="./nips-2007-Topmoumoute_Online_Natural_Gradient_Algorithm.html">206 nips-2007-Topmoumoute Online Natural Gradient Algorithm</a></p>
<p>19 0.05370931 <a title="4-tfidf-19" href="./nips-2007-Predictive_Matrix-Variate_t_Models.html">156 nips-2007-Predictive Matrix-Variate t Models</a></p>
<p>20 0.052212834 <a title="4-tfidf-20" href="./nips-2007-An_Analysis_of_Convex_Relaxations_for_MAP_Estimation.html">23 nips-2007-An Analysis of Convex Relaxations for MAP Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.199), (1, -0.007), (2, -0.003), (3, 0.005), (4, -0.018), (5, -0.015), (6, 0.016), (7, 0.067), (8, -0.123), (9, 0.036), (10, 0.076), (11, 0.022), (12, 0.058), (13, -0.04), (14, -0.099), (15, 0.029), (16, -0.1), (17, -0.052), (18, 0.052), (19, -0.017), (20, -0.055), (21, 0.022), (22, -0.033), (23, 0.02), (24, -0.062), (25, 0.082), (26, 0.118), (27, 0.025), (28, 0.018), (29, -0.099), (30, -0.003), (31, -0.098), (32, -0.098), (33, 0.125), (34, 0.031), (35, -0.032), (36, 0.072), (37, -0.179), (38, 0.042), (39, 0.079), (40, -0.047), (41, -0.06), (42, 0.023), (43, -0.009), (44, 0.003), (45, -0.011), (46, 0.14), (47, 0.024), (48, 0.057), (49, -0.107)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95854807 <a title="4-lsi-1" href="./nips-2007-A_Constraint_Generation_Approach_to_Learning_Stable_Linear_Dynamical_Systems.html">4 nips-2007-A Constraint Generation Approach to Learning Stable Linear Dynamical Systems</a></p>
<p>Author: Byron Boots, Geoffrey J. Gordon, Sajid M. Siddiqi</p><p>Abstract: Stability is a desirable characteristic for linear dynamical systems, but it is often ignored by algorithms that learn these systems from data. We propose a novel method for learning stable linear dynamical systems: we formulate an approximation of the problem as a convex program, start with a solution to a relaxed version of the program, and incrementally add constraints to improve stability. Rather than continuing to generate constraints until we reach a feasible solution, we test stability at each step; because the convex program is only an approximation of the desired problem, this early stopping rule can yield a higher-quality solution. We apply our algorithm to the task of learning dynamic textures from image sequences as well as to modeling biosurveillance drug-sales data. The constraint generation approach leads to noticeable improvement in the quality of simulated sequences. We compare our method to those of Lacy and Bernstein [1, 2], with positive results in terms of accuracy, quality of simulated sequences, and efﬁciency. 1</p><p>2 0.71804219 <a title="4-lsi-2" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>Author: Gwenn Englebienne, Tim Cootes, Magnus Rattray</p><p>Abstract: The present work aims to model the correspondence between facial motion and speech. The face and sound are modelled separately, with phonemes being the link between both. We propose a sequential model and evaluate its suitability for the generation of the facial animation from a sequence of phonemes, which we obtain from speech. We evaluate the results both by computing the error between generated sequences and real video, as well as with a rigorous double-blind test with human subjects. Experiments show that our model compares favourably to other existing methods and that the sequences generated are comparable to real video sequences. 1</p><p>3 0.57126147 <a title="4-lsi-3" href="./nips-2007-Augmented_Functional_Time_Series_Representation_and_Forecasting_with_Gaussian_Processes.html">28 nips-2007-Augmented Functional Time Series Representation and Forecasting with Gaussian Processes</a></p>
<p>Author: Nicolas Chapados, Yoshua Bengio</p><p>Abstract: We introduce a functional representation of time series which allows forecasts to be performed over an unspeciﬁed horizon with progressively-revealed information sets. By virtue of using Gaussian processes, a complete covariance matrix between forecasts at several time-steps is available. This information is put to use in an application to actively trade price spreads between commodity futures contracts. The approach delivers impressive out-of-sample risk-adjusted returns after transaction costs on a portfolio of 30 spreads. 1</p><p>4 0.5282535 <a title="4-lsi-4" href="./nips-2007-Cluster_Stability_for_Finite_Samples.html">46 nips-2007-Cluster Stability for Finite Samples</a></p>
<p>Author: Ohad Shamir, Naftali Tishby</p><p>Abstract: Over the past few years, the notion of stability in data clustering has received growing attention as a cluster validation criterion in a sample-based framework. However, recent work has shown that as the sample size increases, any clustering model will usually become asymptotically stable. This led to the conclusion that stability is lacking as a theoretical and practical tool. The discrepancy between this conclusion and the success of stability in practice has remained an open question, which we attempt to address. Our theoretical approach is that stability, as used by cluster validation algorithms, is similar in certain respects to measures of generalization in a model-selection framework. In such cases, the model chosen governs the convergence rate of generalization bounds. By arguing that these rates are more important than the sample size, we are led to the prediction that stability-based cluster validation algorithms should not degrade with increasing sample size, despite the asymptotic universal stability. This prediction is substantiated by a theoretical analysis as well as some empirical results. We conclude that stability remains a meaningful cluster validation criterion over ﬁnite samples. 1</p><p>5 0.51085389 <a title="4-lsi-5" href="./nips-2007-Selecting_Observations_against_Adversarial_Objectives.html">174 nips-2007-Selecting Observations against Adversarial Objectives</a></p>
<p>Author: Andreas Krause, Brendan Mcmahan, Carlos Guestrin, Anupam Gupta</p><p>Abstract: In many applications, one has to actively select among a set of expensive observations before making an informed decision. Often, we want to select observations which perform well when evaluated with an objective function chosen by an adversary. Examples include minimizing the maximum posterior variance in Gaussian Process regression, robust experimental design, and sensor placement for outbreak detection. In this paper, we present the Submodular Saturation algorithm, a simple and efﬁcient algorithm with strong theoretical approximation guarantees for the case where the possible objective functions exhibit submodularity, an intuitive diminishing returns property. Moreover, we prove that better approximation algorithms do not exist unless NP-complete problems admit efﬁcient algorithms. We evaluate our algorithm on several real-world problems. For Gaussian Process regression, our algorithm compares favorably with state-of-the-art heuristics described in the geostatistics literature, while being simpler, faster and providing theoretical guarantees. For robust experimental design, our algorithm performs favorably compared to SDP-based algorithms. 1</p><p>6 0.50943601 <a title="4-lsi-6" href="./nips-2007-Stability_Bounds_for_Non-i.i.d._Processes.html">184 nips-2007-Stability Bounds for Non-i.i.d. Processes</a></p>
<p>7 0.50465232 <a title="4-lsi-7" href="./nips-2007-DIFFRAC%3A_a_discriminative_and_flexible_framework_for_clustering.html">65 nips-2007-DIFFRAC: a discriminative and flexible framework for clustering</a></p>
<p>8 0.45581254 <a title="4-lsi-8" href="./nips-2007-Unconstrained_On-line_Handwriting_Recognition_with_Recurrent_Neural_Networks.html">210 nips-2007-Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks</a></p>
<p>9 0.45135868 <a title="4-lsi-9" href="./nips-2007-Receding_Horizon_Differential_Dynamic_Programming.html">163 nips-2007-Receding Horizon Differential Dynamic Programming</a></p>
<p>10 0.44199377 <a title="4-lsi-10" href="./nips-2007-Discriminative_K-means_for_Clustering.html">70 nips-2007-Discriminative K-means for Clustering</a></p>
<p>11 0.43531233 <a title="4-lsi-11" href="./nips-2007-A_Spectral_Regularization_Framework_for_Multi-Task_Structure_Learning.html">12 nips-2007-A Spectral Regularization Framework for Multi-Task Structure Learning</a></p>
<p>12 0.42519981 <a title="4-lsi-12" href="./nips-2007-Modeling_Natural_Sounds_with_Modulation_Cascade_Processes.html">130 nips-2007-Modeling Natural Sounds with Modulation Cascade Processes</a></p>
<p>13 0.41130269 <a title="4-lsi-13" href="./nips-2007-Random_Sampling_of_States_in_Dynamic_Programming.html">162 nips-2007-Random Sampling of States in Dynamic Programming</a></p>
<p>14 0.40332207 <a title="4-lsi-14" href="./nips-2007-A_Unified_Near-Optimal_Estimator_For_Dimension_Reduction_in_%24l_%5Calpha%24_%28%240%3C%5Calpha%5Cleq_2%24%29_Using_Stable_Random_Projections.html">13 nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</a></p>
<p>15 0.40085632 <a title="4-lsi-15" href="./nips-2007-Predictive_Matrix-Variate_t_Models.html">156 nips-2007-Predictive Matrix-Variate t Models</a></p>
<p>16 0.38766196 <a title="4-lsi-16" href="./nips-2007-Learning_Monotonic_Transformations_for_Classification.html">112 nips-2007-Learning Monotonic Transformations for Classification</a></p>
<p>17 0.38663903 <a title="4-lsi-17" href="./nips-2007-Simulated_Annealing%3A_Rigorous_finite-time_guarantees_for_optimization_on_continuous_domains.html">178 nips-2007-Simulated Annealing: Rigorous finite-time guarantees for optimization on continuous domains</a></p>
<p>18 0.38498861 <a title="4-lsi-18" href="./nips-2007-Heterogeneous_Component_Analysis.html">96 nips-2007-Heterogeneous Component Analysis</a></p>
<p>19 0.37506366 <a title="4-lsi-19" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>20 0.37264511 <a title="4-lsi-20" href="./nips-2007-Ensemble_Clustering_using_Semidefinite_Programming.html">80 nips-2007-Ensemble Clustering using Semidefinite Programming</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.057), (13, 0.043), (16, 0.024), (19, 0.014), (21, 0.038), (31, 0.017), (34, 0.024), (35, 0.05), (47, 0.081), (49, 0.02), (53, 0.294), (83, 0.116), (85, 0.049), (87, 0.019), (90, 0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.7946856 <a title="4-lda-1" href="./nips-2007-Receding_Horizon_Differential_Dynamic_Programming.html">163 nips-2007-Receding Horizon Differential Dynamic Programming</a></p>
<p>Author: Yuval Tassa, Tom Erez, William D. Smart</p><p>Abstract: The control of high-dimensional, continuous, non-linear dynamical systems is a key problem in reinforcement learning and control. Local, trajectory-based methods, using techniques such as Differential Dynamic Programming (DDP), are not directly subject to the curse of dimensionality, but generate only local controllers. In this paper,we introduce Receding Horizon DDP (RH-DDP), an extension to the classic DDP algorithm, which allows us to construct stable and robust controllers based on a library of local-control trajectories. We demonstrate the effectiveness of our approach on a series of high-dimensional problems using a simulated multi-link swimming robot. These experiments show that our approach effectively circumvents dimensionality issues, and is capable of dealing with problems of (at least) 24 state and 9 action dimensions. 1</p><p>same-paper 2 0.74719673 <a title="4-lda-2" href="./nips-2007-A_Constraint_Generation_Approach_to_Learning_Stable_Linear_Dynamical_Systems.html">4 nips-2007-A Constraint Generation Approach to Learning Stable Linear Dynamical Systems</a></p>
<p>Author: Byron Boots, Geoffrey J. Gordon, Sajid M. Siddiqi</p><p>Abstract: Stability is a desirable characteristic for linear dynamical systems, but it is often ignored by algorithms that learn these systems from data. We propose a novel method for learning stable linear dynamical systems: we formulate an approximation of the problem as a convex program, start with a solution to a relaxed version of the program, and incrementally add constraints to improve stability. Rather than continuing to generate constraints until we reach a feasible solution, we test stability at each step; because the convex program is only an approximation of the desired problem, this early stopping rule can yield a higher-quality solution. We apply our algorithm to the task of learning dynamic textures from image sequences as well as to modeling biosurveillance drug-sales data. The constraint generation approach leads to noticeable improvement in the quality of simulated sequences. We compare our method to those of Lacy and Bernstein [1, 2], with positive results in terms of accuracy, quality of simulated sequences, and efﬁciency. 1</p><p>3 0.52837902 <a title="4-lda-3" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>Author: Yuhong Guo, Dale Schuurmans</p><p>Abstract: We investigate a new, convex relaxation of an expectation-maximization (EM) variant that approximates a standard objective while eliminating local minima. First, a cautionary result is presented, showing that any convex relaxation of EM over hidden variables must give trivial results if any dependence on the missing values is retained. Although this appears to be a strong negative outcome, we then demonstrate how the problem can be bypassed by using equivalence relations instead of value assignments over hidden variables. In particular, we develop new algorithms for estimating exponential conditional models that only require equivalence relation information over the variable values. This reformulation leads to an exact expression for EM variants in a wide range of problems. We then develop a semideﬁnite relaxation that yields global training by eliminating local minima. 1</p><p>4 0.51804376 <a title="4-lda-4" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>Author: Matthias Bethge, Philipp Berens</p><p>Abstract: Maximum entropy analysis of binary variables provides an elegant way for studying the role of pairwise correlations in neural populations. Unfortunately, these approaches suffer from their poor scalability to high dimensions. In sensory coding, however, high-dimensional data is ubiquitous. Here, we introduce a new approach using a near-maximum entropy model, that makes this type of analysis feasible for very high-dimensional data—the model parameters can be derived in closed form and sampling is easy. Therefore, our NearMaxEnt approach can serve as a tool for testing predictions from a pairwise maximum entropy model not only for low-dimensional marginals, but also for high dimensional measurements of more than thousand units. We demonstrate its usefulness by studying natural images with dichotomized pixel intensities. Our results indicate that the statistics of such higher-dimensional measurements exhibit additional structure that are not predicted by pairwise correlations, despite the fact that pairwise correlations explain the lower-dimensional marginal statistics surprisingly well up to the limit of dimensionality where estimation of the full joint distribution is feasible. 1</p><p>5 0.51424587 <a title="4-lda-5" href="./nips-2007-Multi-Task_Learning_via_Conic_Programming.html">134 nips-2007-Multi-Task Learning via Conic Programming</a></p>
<p>Author: Tsuyoshi Kato, Hisashi Kashima, Masashi Sugiyama, Kiyoshi Asai</p><p>Abstract: When we have several related tasks, solving them simultaneously is shown to be more effective than solving them individually. This approach is called multi-task learning (MTL) and has been studied extensively. Existing approaches to MTL often treat all the tasks as uniformly related to each other and the relatedness of the tasks is controlled globally. For this reason, the existing methods can lead to undesired solutions when some tasks are not highly related to each other, and some pairs of related tasks can have signiﬁcantly different solutions. In this paper, we propose a novel MTL algorithm that can overcome these problems. Our method makes use of a task network, which describes the relation structure among tasks. This allows us to deal with intricate relation structures in a systematic way. Furthermore, we control the relatedness of the tasks locally, so all pairs of related tasks are guaranteed to have similar solutions. We apply the above idea to support vector machines (SVMs) and show that the optimization problem can be cast as a second order cone program, which is convex and can be solved efﬁciently. The usefulness of our approach is demonstrated through simulations with protein super-family classiﬁcation and ordinal regression problems.</p><p>6 0.51224899 <a title="4-lda-6" href="./nips-2007-Predictive_Matrix-Variate_t_Models.html">156 nips-2007-Predictive Matrix-Variate t Models</a></p>
<p>7 0.51056921 <a title="4-lda-7" href="./nips-2007-Colored_Maximum_Variance_Unfolding.html">49 nips-2007-Colored Maximum Variance Unfolding</a></p>
<p>8 0.510355 <a title="4-lda-8" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>9 0.51022106 <a title="4-lda-9" href="./nips-2007-Heterogeneous_Component_Analysis.html">96 nips-2007-Heterogeneous Component Analysis</a></p>
<p>10 0.50937772 <a title="4-lda-10" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>11 0.50711662 <a title="4-lda-11" href="./nips-2007-A_Kernel_Statistical_Test_of_Independence.html">7 nips-2007-A Kernel Statistical Test of Independence</a></p>
<p>12 0.50705582 <a title="4-lda-12" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>13 0.50674337 <a title="4-lda-13" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<p>14 0.50622839 <a title="4-lda-14" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>15 0.50578022 <a title="4-lda-15" href="./nips-2007-Learning_Monotonic_Transformations_for_Classification.html">112 nips-2007-Learning Monotonic Transformations for Classification</a></p>
<p>16 0.50551617 <a title="4-lda-16" href="./nips-2007-Stable_Dual_Dynamic_Programming.html">185 nips-2007-Stable Dual Dynamic Programming</a></p>
<p>17 0.50527948 <a title="4-lda-17" href="./nips-2007-A_learning_framework_for_nearest_neighbor_search.html">16 nips-2007-A learning framework for nearest neighbor search</a></p>
<p>18 0.50454891 <a title="4-lda-18" href="./nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">79 nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<p>19 0.50428891 <a title="4-lda-19" href="./nips-2007-Density_Estimation_under_Independent_Similarly_Distributed_Sampling_Assumptions.html">66 nips-2007-Density Estimation under Independent Similarly Distributed Sampling Assumptions</a></p>
<p>20 0.50341475 <a title="4-lda-20" href="./nips-2007-Selecting_Observations_against_Adversarial_Objectives.html">174 nips-2007-Selecting Observations against Adversarial Objectives</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
