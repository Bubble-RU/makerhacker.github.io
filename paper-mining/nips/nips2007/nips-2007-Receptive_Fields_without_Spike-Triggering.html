<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>164 nips-2007-Receptive Fields without Spike-Triggering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-164" href="#">nips2007-164</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>164 nips-2007-Receptive Fields without Spike-Triggering</h1>
<br/><p>Source: <a title="nips-2007-164-pdf" href="http://papers.nips.cc/paper/3220-receptive-fields-without-spike-triggering.pdf">pdf</a></p><p>Author: Guenther Zeck, Matthias Bethge, Jakob H. Macke</p><p>Abstract: S timulus selectivity of sensory neurons is often characterized by estimating their receptive ﬁeld properties such as orientation selectivity. Receptive ﬁelds are usually derived from the mean (or covariance) of the spike-triggered stimulus ensemble. This approach treats each spike as an independent message but does not take into account that information might be conveyed through patterns of neural activity that are distributed across space or time. Can we ﬁnd a concise description for the processing of a whole population of neurons analogous to the receptive ﬁeld for single neurons? Here, we present a generalization of the linear receptive ﬁeld which is not bound to be triggered on individual spikes but can be meaningfully linked to distributed response patterns. More precisely, we seek to identify those stimulus features and the corresponding patterns of neural activity that are most reliably coupled. We use an extension of reverse-correlation methods based on canonical correlation analysis. The resulting population receptive ﬁelds span the subspace of stimuli that is most informative about the population response. We evaluate our approach using both neuronal models and multi-electrode recordings from rabbit retinal ganglion cells. We show how the model can be extended to capture nonlinear stimulus-response relationships using kernel canonical correlation analysis, which makes it possible to test different coding mechanisms. Our technique can also be used to calculate receptive ﬁelds from multi-dimensional neural measurements such as those obtained from dynamic imaging methods. 1</p><p>Reference: <a title="nips-2007-164-reference" href="../nips2007_reference/nips-2007-Receptive_Fields_without_Spike-Triggering_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de Max Planck Institute for Biological Cybernetics S pemannstrasse 41 ¨ 72076 T u bingen, Germany  Abstract S timulus selectivity of sensory neurons is often characterized by estimating their receptive ﬁeld properties such as orientation selectivity. [sent-7, score-0.744]
</p><p>2 Receptive ﬁelds are usually derived from the mean (or covariance) of the spike-triggered stimulus ensemble. [sent-8, score-0.239]
</p><p>3 This approach treats each spike as an independent message but does not take into account that information might be conveyed through patterns of neural activity that are distributed across space or time. [sent-9, score-0.36]
</p><p>4 Can we ﬁnd a concise description for the processing of a whole population of neurons analogous to the receptive ﬁeld for single neurons? [sent-10, score-0.903]
</p><p>5 Here, we present a generalization of the linear receptive ﬁeld which is not bound to be triggered on individual spikes but can be meaningfully linked to distributed response patterns. [sent-11, score-0.787]
</p><p>6 More precisely, we seek to identify those stimulus features and the corresponding patterns of neural activity that are most reliably coupled. [sent-12, score-0.465]
</p><p>7 We use an extension of reverse-correlation methods based on canonical correlation analysis. [sent-13, score-0.248]
</p><p>8 The resulting population receptive ﬁelds span the subspace of stimuli that is most informative about the population response. [sent-14, score-1.197]
</p><p>9 We evaluate our approach using both neuronal models and multi-electrode recordings from rabbit retinal ganglion cells. [sent-15, score-0.501]
</p><p>10 We show how the model can be extended to capture nonlinear stimulus-response relationships using kernel canonical correlation analysis, which makes it possible to test different coding mechanisms. [sent-16, score-0.397]
</p><p>11 Our technique can also be used to calculate receptive ﬁelds from multi-dimensional neural measurements such as those obtained from dynamic imaging methods. [sent-17, score-0.641]
</p><p>12 The interpretation of these patterns constitutes a challenging problem: for computational tasks like object recognition, it is not clear what information about the image should be extracted and in which format it should be represented. [sent-19, score-0.11]
</p><p>13 S imilarly, it is difﬁcult to assess what information is conveyed by the multitude of neurons in the visual pathway. [sent-20, score-0.152]
</p><p>14 Right from the ﬁrst synapse, the information of an individual photoreceptor is signaled to many different cells with different temporal ﬁltering properties, each of which is only a small unit within a complex neural network [ 20] . [sent-21, score-0.165]
</p><p>15 Even if we leave the difﬁculties imposed by nonlinearities and feedback aside, it is hard to judge what the contribution of any particular neuron is to the information transmitted. [sent-22, score-0.156]
</p><p>16 1  The prevalent tool for characterizing the behavior of sensory neurons, the spike triggered average, is based on a quasi-linear model of neural responses [ 1 5] . [sent-23, score-0.302]
</p><p>17 , yN ) T denotes the vector of neural responses, x the stimulus parameters, W = ( w 1 , . [sent-27, score-0.291]
</p><p>18 , w N ) T the ﬁlter matrix with row ‘ k’ containing the receptive ﬁeld w k of neuron k, and ξ is the noise. [sent-30, score-0.669]
</p><p>19 In order to understand the collective behavior of a neuronal population, we rather have to understand the behavior of the matrix W, and the structure of the noise correlations Σ ξ : Both of them inﬂuence the feature selectivity of the population. [sent-34, score-0.226]
</p><p>20 Can we ﬁnd a compact description of the features that a neural ensemble is most sensitive to? [sent-35, score-0.165]
</p><p>21 In the case of a single cell, the receptive ﬁeld provides such a description: It can be interpreted as the “favorite stimulus” of the neuron, in the sense that the more similar an input is to the receptive ﬁeld, the higher is the spiking probability, and thus the ﬁring rate of the neuron. [sent-36, score-1.073]
</p><p>22 In addition, the receptive ﬁeld can easily be estimated using a spike-triggered average, which, under certain assumptions, yields the optimal estimate of the receptive ﬁeld in a linear-nonlinear cascade model [ 1 1 ] . [sent-37, score-1.026]
</p><p>23 If we are considering an ensemble of neurons rather than a single neuron, it is not obvious what to trigger on: This requires assumptions about what patterns of spikes or modulations in ﬁring rates across the population carry information about the stimulus. [sent-38, score-0.613]
</p><p>24 Rather than addressing the question “what features of the stimulus are correlated with the occurence of spikes”, the question now is: “What stimulus features are correlated with what patterns of spiking activity? [sent-39, score-0.711]
</p><p>25 Phrased in the language of information theory, we are searching for the subspace that contains most of the mutual information between sensory inputs and neuronal responses. [sent-41, score-0.294]
</p><p>26 As an efﬁcient implementation of this strategy, we present an extension of reverse-correlation methods based on canonical correlation analysis. [sent-43, score-0.248]
</p><p>27 The resulting population receptive ﬁelds (PRFs) are not bound to be triggered on individual spikes but are linked to response patterns that are simultaneously determined by the algorithm. [sent-44, score-1.132]
</p><p>28 We calculate the PRF for a population consisting of uniformly spaced cells with center-surround receptive ﬁelds and noise correlations, and estimate the PRF of a population of rabbit retinal ganglion cells from multi-electrode recordings. [sent-45, score-1.669]
</p><p>29 In addition, we show how our method can be extended to explore different hypotheses about the neural code, such as spike latencies or interval coding, which require nonlinear read out mechanisms. [sent-46, score-0.222]
</p><p>30 2  From reverse correlation to canonical correlation  We regard the stimulus at time t as a random variable Xt ∈ R n , and the neural response as Yt ∈ R m . [sent-47, score-0.778]
</p><p>31 For simplicity, we assume that the stimulus consists of Gaussian white noise, i. [sent-48, score-0.268]
</p><p>32 The spike-triggered average a of a neuron can be motivated by the fact that it is the direction in stimulus-space maximizing the correlation-coefﬁcient ρ= �  Cov( a T X, Y1 ) Var( a T X) Var( Y1 )  . [sent-51, score-0.182]
</p><p>33 (2)  between the ﬁltered stimulus a T X and a univariate neural response Y1 . [sent-52, score-0.383]
</p><p>34 In the case of a neural population, we are not only looking for the stimulus feature a, but also need to determine what pattern of spiking activity b it is coupled with. [sent-53, score-0.433]
</p><p>35 (3 ) ρ1 = � Var( a T X) Var( b T Y) 1 1 We interpret a 1 as the stimulus ﬁlter whose output is maximally correlated with the output of the “response ﬁlter” b 1 . [sent-55, score-0.304]
</p><p>36 Thus, we are simultaneously searching for features of the stimulus that the neural system is selective for, and the patterns of activity that it uses to signal the presence or absence 2  of this feature. [sent-56, score-0.499]
</p><p>37 We refer to the vector a 1 as the (ﬁrst) population receptive ﬁeld of the population, and b 1 is the response f eature corresponding to a 1 . [sent-57, score-0.84]
</p><p>38 If a hypothetical neuron receives input from the population, and wants to decode the presence of the stimulus a 1 , the weights of the optimal linear readout [ 1 6] could be derived from b 1 . [sent-58, score-0.464]
</p><p>39 The population receptive ﬁelds and the characteristic patterns are found by a joint optimization in stimulus and response space. [sent-67, score-1.189]
</p><p>40 Therefore, one does not need to know—or assume—a priori what features the population is sensitive to, or what spike patterns convey the information. [sent-68, score-0.508]
</p><p>41 The ﬁrst K PRFs form a basis for the subspace of stimuli that the neural population is most sensitive to, and the individual basis vectors a k are sorted according to their “informativeness” [ 1 3 , 1 7] . [sent-69, score-0.524]
</p><p>42 The mutual information between two one-dimensional Gaussian Variables with correlation ρ is given 1 by MI G a us s = − 2 log( 1 − ρ 2 ) , so maximizing correlation coefﬁcients is equivalent to maximizing mutual information [ 3 ] . [sent-70, score-0.508]
</p><p>43 Assuming the neural response Y to be Gaussian, the subspace spanned by the ﬁrst K vectors B K = ( b 1 , . [sent-71, score-0.327]
</p><p>44 , b K ) is also the K-subspace of stimuli that contains the maximal amount of mutual information between stimuli and neural response. [sent-74, score-0.271]
</p><p>45 In contrast to oriented PCA, however, CCA does not require one to know explicitly how the response covariance Σ y = Σ s + Σ ξ splits into signal Σ s and noise Σ ξ covariance. [sent-77, score-0.196]
</p><p>46 Instead, it uses the cross-covariance Σ x y which is directly available from reverse correlation experiments. [sent-78, score-0.147]
</p><p>47 b K but also the most predictive stimulus components A K = ( a 1 , . [sent-82, score-0.239]
</p><p>48 S ince for elliptically contoured distributions J( A T X) does not depend on A, the PRFs can be seen as the solution of a variational approach, maximizing a lower bound to the mutual information. [sent-87, score-0.189]
</p><p>49 Maximizing mutual information directly is hard, requires extensive amounts of data, and usually multiple repetitions of the same stimulus sequence. [sent-88, score-0.32]
</p><p>50 3  The receptive ﬁeld of a population of neurons  3. [sent-89, score-0.863]
</p><p>51 the neuron is not sensitive to the mean luminance of the stimulus. [sent-96, score-0.189]
</p><p>52 S peciﬁcally, we assume exponentially decaying noise correlation with Σ ξ ( s ) = exp( − | s | / λ) . [sent-99, score-0.189]
</p><p>53 That is, the ﬁrst PRF can be used to estimate the passband of the population transfer function. [sent-101, score-0.282]
</p><p>54 (8 )  The passband of the ﬁrst population ﬁlter moves as a function of both parameters A and λ. [sent-103, score-0.282]
</p><p>55 In this case, the mean intensity is the stimulus property that is most faithfully signaled by the ensemble. [sent-109, score-0.286]
</p><p>56 2  The receptive ﬁeld of an ensemble of retinal ganglion cells  We mapped the population receptive ﬁelds of rabbit retinal ganglion cells recorded with a wholemount preparation. [sent-124, score-2.204]
</p><p>57 The neurons were stimulated with a 1 6 × 1 6 checkerboard consisting of binary white noise which was updated every 20ms. [sent-126, score-0.211]
</p><p>58 After spikesorting, spike trains from 32 neurons were binned at 2 0ms resolution, and the response of a neuron to a stimulus at time t was deﬁned to consist of the the spike-counts in the 1 0 bins between 40ms and 240ms after t. [sent-128, score-0.729]
</p><p>59 Thus, each population response Yt is a 3 20 dimensional vector. [sent-129, score-0.327]
</p><p>60 2 A) displays the ﬁrst 6 PRFs, the corresponding patterns of neural activity (B) and their correlation coefﬁcients ρ k (which were calculated using a cross-validation procedure). [sent-131, score-0.404]
</p><p>61 It can be seen that the PRFs look very different to the usual center-surrond structure of retinal ganglion. [sent-132, score-0.155]
</p><p>62 For comparison, we also plotted the single-cell receptive ﬁelds in Figure 3 . [sent-134, score-0.513]
</p><p>63 2 C), and their projections into the spaced spanned by the ﬁrst 6 PRFs. [sent-135, score-0.114]
</p><p>64 These plots suggest that a small number of PRFs might 4  be sufﬁcient to approximate each of the receptive ﬁelds. [sent-136, score-0.513]
</p><p>65 The Gaussian Mutual Information �K MI G a us s = − 1 k = 1 log( 1 − ρ 2 ) is an estimate of the information contained in the subspace k 2 spanned by the ﬁrst K PRFs. [sent-138, score-0.183]
</p><p>66 RF  RF  C)  Figure 2: The population receptive ﬁelds of a group of 32 retinal ganglion cells: A) the ﬁrst 6 PRFs, as sorted by the correlation coefﬁcient ρ k B) the response features b k coupled with the PRFs. [sent-153, score-1.37]
</p><p>67 It can be seen that only a subset of neurons contributed to the ﬁrst 6 PRFs. [sent-156, score-0.115]
</p><p>68 C) The single-cell receptive ﬁelds of 2 4 neurons from our population, and their projections into the space spanned by the 6 PRFs. [sent-157, score-0.714]
</p><p>69 B) Gaussian-MI of the subspace spanned by the ﬁrst K PRFs. [sent-167, score-0.183]
</p><p>70 4  Nonlinear extensions using Kernel Canonical Correlation Analysis  Thus far, our model is completely linear: We assume that the stimulus is linearly related to the neural responses, and we also assume a linear readout of the response. [sent-168, score-0.332]
</p><p>71 It is worth mentioning that the space of patterns Y itself does not have to be a vector space. [sent-175, score-0.11]
</p><p>72 We illustrate the concept on simulated data: We will use a similarity measure based on the metric D interval [ 1 9] to estimate the receptive ﬁeld of a neuron which does not use its ﬁring rate, but rather the occurrence of speciﬁc interspike intervals to convey information about the stimulus. [sent-185, score-0.768]
</p><p>73 ) If we consider coding-schemes that are based on patterns of spikes, the methods described here become useful even for the analysis of single neurons. [sent-189, score-0.11]
</p><p>74 Our hypothetical neuron encodes information in a pattern consisting of three spikes: The relative timing of the second spike is informative about the stimulus: The bigger the correlation between receptive ﬁeld and stimulus � r, s t � , the shorter is the interval. [sent-191, score-1.291]
</p><p>75 If the receptive ﬁeld is very dissimilar to the stimulus, the interval is long. [sent-192, score-0.546]
</p><p>76 While the timing of the spikes relative to each other is precise, there is jitter in the timing of the pattern relative to the stimulus. [sent-193, score-0.189]
</p><p>77 6  A)  B)  Spike trains  C)  D)  0  50  100 Time →  150  200  Figure 4: Coding by spike patterns: A) Receptive ﬁeld of neuron described in S ection 4. [sent-196, score-0.283]
</p><p>78 B) A subset of the simulated spike-trains, sorted with respect to the similarity between the shown stimulus and the receptive ﬁeld of the model. [sent-197, score-0.823]
</p><p>79 The interval between the ﬁrst two informative spikes in each trial is highlighted in red. [sent-198, score-0.194]
</p><p>80 C) Receptive ﬁeld recovered by Kernel CCA, the correlation coefﬁcient between real and estimated receptive ﬁeld is 0. [sent-199, score-0.66]
</p><p>81 D) Receptive ﬁeld derived using linear decoding, correlation coefﬁcient is 0. [sent-201, score-0.147]
</p><p>82 Using these spike-trains, we tried to recover the receptive ﬁeld r without telling the algorithm what the indicating pattern was. [sent-203, score-0.513]
</p><p>83 Each stimulus was shown only once, and therefore, that every spikepattern occurred only once. [sent-204, score-0.239]
</p><p>84 We simulated 5 000 stimulus presentations for this model, and applied Kernel CCA with a linear kernel on the stimuli, and the alignment-score on the spike-trains. [sent-205, score-0.344]
</p><p>85 As many kernels on spike trains are computationally expensive, this trick can result in substantial speed-ups of the computation. [sent-207, score-0.127]
</p><p>86 The receptive ﬁeld was recovered (see Figure 4), despite the highly nonlinear encoding mechanism of the neuron. [sent-208, score-0.553]
</p><p>87 For comparison, we also show what receptive ﬁeld would be obtained using linear decoding on the indicated bins. [sent-209, score-0.549]
</p><p>88 Although this neuron model may seem slightly contrived, it is a good proof of concept that, in principle, receptive ﬁelds can be estimated even if the ﬁring rate gives no information at all about the stimulus, and the encoding is highly nonlinear. [sent-210, score-0.669]
</p><p>89 Our algorithm does not only look at patterns that occur more often than expected by chance, but also takes into account to what extent their occurrence is correlated to the sensory input. [sent-211, score-0.189]
</p><p>90 5  Conclusions  We set out to ﬁnd a useful description of the stimulus-response relationship of an ensemble of neurons akin to the concept of receptive ﬁeld for single neurons. [sent-212, score-0.708]
</p><p>91 The population receptive ﬁelds are found by a joint optimization over stimuli and spike-patterns, and are thus not bound to be triggered by single spikes. [sent-213, score-0.886]
</p><p>92 We estimated the PRFs of a group of retinal ganglion cells, and found that the ﬁrst PRF had most spectral power in the low-frequency bands, consistent with our theoretical analysis. [sent-214, score-0.371]
</p><p>93 The stimulus we used was a white-noise sequence—it will be interesting to see how the informative subspace and its spectral properties change for different stimuli such as colored noise. [sent-215, score-0.479]
</p><p>94 The ganglion cell layer of the retina is a system that is relatively well understood at the level of single neurons. [sent-216, score-0.266]
</p><p>95 However, our approach has the potential to be especially useful in systems in which the functional signiﬁcance of single cell receptive ﬁelds is difﬁcult to interpret. [sent-218, score-0.556]
</p><p>96 Using CCA, receptive ﬁelds can readily be estimated from these kinds of representations without limiting attention to single channels or extracting neural events. [sent-227, score-0.565]
</p><p>97 Prediction and decoding of retinal ganglion cell responses with a probabilistic spiking model. [sent-311, score-0.514]
</p><p>98 Multineuronal ﬁring patterns in the signal from eye to brain. [sent-324, score-0.144]
</p><p>99 Analyzing neural responses to natural signals: maximally informative dimensions. [sent-346, score-0.17]
</p><p>100 The spatial ﬁltering properties of local edge detectors and brisk-sustained retinal ganglion cells. [sent-369, score-0.345]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('receptive', 0.513), ('prfs', 0.256), ('stimulus', 0.239), ('population', 0.235), ('cca', 0.222), ('prf', 0.21), ('ganglion', 0.19), ('neuron', 0.156), ('retinal', 0.155), ('eld', 0.152), ('correlation', 0.147), ('neurons', 0.115), ('spikes', 0.113), ('patterns', 0.11), ('elds', 0.105), ('canonical', 0.101), ('subspace', 0.097), ('spike', 0.097), ('response', 0.092), ('spanned', 0.086), ('rabbit', 0.081), ('rf', 0.081), ('mutual', 0.081), ('neuronal', 0.075), ('kernel', 0.072), ('cho', 0.07), ('mpg', 0.07), ('ofmachine', 0.07), ('stimuli', 0.069), ('triggered', 0.069), ('cells', 0.066), ('activity', 0.064), ('dc', 0.059), ('neural', 0.052), ('correlations', 0.05), ('pillow', 0.049), ('det', 0.049), ('nr', 0.049), ('informative', 0.048), ('spiking', 0.047), ('ring', 0.047), ('eichhorn', 0.047), ('passband', 0.047), ('pemannstrasse', 0.047), ('signaled', 0.047), ('var', 0.045), ('planck', 0.045), ('neurosci', 0.045), ('cell', 0.043), ('imaging', 0.043), ('responses', 0.043), ('noise', 0.042), ('mi', 0.041), ('sensory', 0.041), ('contoured', 0.041), ('elliptically', 0.041), ('zeck', 0.041), ('phys', 0.041), ('readout', 0.041), ('ensemble', 0.04), ('description', 0.04), ('nonlinear', 0.04), ('lter', 0.039), ('cov', 0.039), ('correlated', 0.038), ('sorted', 0.038), ('timing', 0.038), ('vis', 0.037), ('conveyed', 0.037), ('coding', 0.037), ('decoding', 0.036), ('dimensionality', 0.035), ('rust', 0.035), ('rev', 0.035), ('signal', 0.034), ('sensitive', 0.033), ('interval', 0.033), ('coef', 0.033), ('retina', 0.033), ('convey', 0.033), ('simulated', 0.033), ('calculate', 0.033), ('calculated', 0.031), ('germany', 0.031), ('feature', 0.031), ('trains', 0.03), ('white', 0.029), ('hypothetical', 0.028), ('oriented', 0.028), ('spaced', 0.028), ('selectivity', 0.028), ('tuning', 0.027), ('bingen', 0.027), ('maximally', 0.027), ('cients', 0.026), ('maximizing', 0.026), ('frequency', 0.026), ('spectral', 0.026), ('consisting', 0.025), ('fields', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="164-tfidf-1" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>Author: Guenther Zeck, Matthias Bethge, Jakob H. Macke</p><p>Abstract: S timulus selectivity of sensory neurons is often characterized by estimating their receptive ﬁeld properties such as orientation selectivity. Receptive ﬁelds are usually derived from the mean (or covariance) of the spike-triggered stimulus ensemble. This approach treats each spike as an independent message but does not take into account that information might be conveyed through patterns of neural activity that are distributed across space or time. Can we ﬁnd a concise description for the processing of a whole population of neurons analogous to the receptive ﬁeld for single neurons? Here, we present a generalization of the linear receptive ﬁeld which is not bound to be triggered on individual spikes but can be meaningfully linked to distributed response patterns. More precisely, we seek to identify those stimulus features and the corresponding patterns of neural activity that are most reliably coupled. We use an extension of reverse-correlation methods based on canonical correlation analysis. The resulting population receptive ﬁelds span the subspace of stimuli that is most informative about the population response. We evaluate our approach using both neuronal models and multi-electrode recordings from rabbit retinal ganglion cells. We show how the model can be extended to capture nonlinear stimulus-response relationships using kernel canonical correlation analysis, which makes it possible to test different coding mechanisms. Our technique can also be used to calculate receptive ﬁelds from multi-dimensional neural measurements such as those obtained from dynamic imaging methods. 1</p><p>2 0.33765444 <a title="164-tfidf-2" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><p>3 0.28331333 <a title="164-tfidf-3" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>Author: Jonathan W. Pillow, Peter E. Latham</p><p>Abstract: Point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, owing in part to the fact that they do not take into account multiple stages of processing. Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more ﬂexible for ﬁtting neural data. More importantly, it allows us to estimate connectivity patterns among neurons (both observed and unobserved), and may provide insight into how networks process sensory input. We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model’s performance using a simulated example network consisting of two coupled neurons.</p><p>4 0.18861519 <a title="164-tfidf-4" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>Author: Honglak Lee, Chaitanya Ekanadham, Andrew Y. Ng</p><p>Abstract: Motivated in part by the hierarchical organization of the cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or “deep,” structure from unlabeled data. While several authors have formally or informally compared their algorithms to computations performed in visual area V1 (and the cochlea), little attempt has been made thus far to evaluate these algorithms in terms of their ﬁdelity for mimicking computations at deeper levels in the cortical hierarchy. This paper presents an unsupervised learning model that faithfully mimics certain properties of visual area V2. Speciﬁcally, we develop a sparse variant of the deep belief networks of Hinton et al. (2006). We learn two layers of nodes in the network, and demonstrate that the ﬁrst layer, similar to prior work on sparse coding and ICA, results in localized, oriented, edge ﬁlters, similar to the Gabor functions known to model V1 cell receptive ﬁelds. Further, the second layer in our model encodes correlations of the ﬁrst layer responses in the data. Speciﬁcally, it picks up both colinear (“contour”) features as well as corners and junctions. More interestingly, in a quantitative comparison, the encoding of these more complex “corner” features matches well with the results from the Ito & Komatsu’s study of biological V2 responses. This suggests that our sparse variant of deep belief networks holds promise for modeling more higher-order features. 1</p><p>5 0.18387602 <a title="164-tfidf-5" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>Author: Tatyana Sharpee</p><p>Abstract: This paper compares a family of methods for characterizing neural feature selectivity with natural stimuli in the framework of the linear-nonlinear model. In this model, the neural ﬁring rate is a nonlinear function of a small number of relevant stimulus components. The relevant stimulus dimensions can be found by maximizing one of the family of objective functions, R´ nyi divergences of different e orders [1, 2]. We show that maximizing one of them, R´ nyi divergence of ore der 2, is equivalent to least-square ﬁtting of the linear-nonlinear model to neural data. Next, we derive reconstruction errors in relevant dimensions found by maximizing R´ nyi divergences of arbitrary order in the asymptotic limit of large spike e numbers. We ﬁnd that the smallest errors are obtained with R´ nyi divergence of e order 1, also known as Kullback-Leibler divergence. This corresponds to ﬁnding relevant dimensions by maximizing mutual information [2]. We numerically test how these optimization schemes perform in the regime of low signal-to-noise ratio (small number of spikes and increasing neural noise) for model visual neurons. We ﬁnd that optimization schemes based on either least square ﬁtting or information maximization perform well even when number of spikes is small. Information maximization provides slightly, but signiﬁcantly, better reconstructions than least square ﬁtting. This makes the problem of ﬁnding relevant dimensions, together with the problem of lossy compression [3], one of examples where informationtheoretic measures are no more data limited than those derived from least squares. 1</p><p>6 0.14683871 <a title="164-tfidf-6" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>7 0.13828681 <a title="164-tfidf-7" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>8 0.13364787 <a title="164-tfidf-8" href="./nips-2007-Estimating_disparity_with_confidence_from_energy_neurons.html">81 nips-2007-Estimating disparity with confidence from energy neurons</a></p>
<p>9 0.12399457 <a title="164-tfidf-9" href="./nips-2007-Learning_to_classify_complex_patterns_using_a_VLSI_network_of_spiking_neurons.html">117 nips-2007-Learning to classify complex patterns using a VLSI network of spiking neurons</a></p>
<p>10 0.11430823 <a title="164-tfidf-10" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>11 0.11037119 <a title="164-tfidf-11" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>12 0.10937389 <a title="164-tfidf-12" href="./nips-2007-Contraction_Properties_of_VLSI_Cooperative_Competitive_Neural_Networks_of_Spiking_Neurons.html">60 nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</a></p>
<p>13 0.10720682 <a title="164-tfidf-13" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>14 0.098857462 <a title="164-tfidf-14" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>15 0.083414696 <a title="164-tfidf-15" href="./nips-2007-Predicting_Brain_States_from_fMRI_Data%3A_Incremental_Functional_Principal_Component_Regression.html">154 nips-2007-Predicting Brain States from fMRI Data: Incremental Functional Principal Component Regression</a></p>
<p>16 0.077466071 <a title="164-tfidf-16" href="./nips-2007-A_configurable_analog_VLSI_neural_network_with_spiking_neurons_and_self-regulating_plastic_synapses.html">14 nips-2007-A configurable analog VLSI neural network with spiking neurons and self-regulating plastic synapses</a></p>
<p>17 0.076077484 <a title="164-tfidf-17" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<p>18 0.07450752 <a title="164-tfidf-18" href="./nips-2007-Second_Order_Bilinear_Discriminant_Analysis_for_single_trial_EEG_analysis.html">173 nips-2007-Second Order Bilinear Discriminant Analysis for single trial EEG analysis</a></p>
<p>19 0.072825633 <a title="164-tfidf-19" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>20 0.065964043 <a title="164-tfidf-20" href="./nips-2007-Locality_and_low-dimensions_in_the_prediction_of_natural_experience_from_fMRI.html">122 nips-2007-Locality and low-dimensions in the prediction of natural experience from fMRI</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.219), (1, 0.184), (2, 0.36), (3, 0.055), (4, 0.007), (5, -0.036), (6, 0.035), (7, 0.058), (8, 0.023), (9, 0.046), (10, 0.042), (11, -0.022), (12, 0.081), (13, -0.003), (14, 0.07), (15, 0.006), (16, 0.151), (17, 0.08), (18, 0.141), (19, 0.181), (20, -0.005), (21, 0.02), (22, 0.014), (23, 0.057), (24, 0.019), (25, -0.023), (26, 0.054), (27, -0.001), (28, -0.073), (29, 0.061), (30, -0.078), (31, 0.022), (32, -0.076), (33, -0.107), (34, 0.032), (35, -0.034), (36, 0.079), (37, 0.088), (38, 0.084), (39, 0.07), (40, 0.037), (41, -0.055), (42, -0.067), (43, -0.026), (44, -0.037), (45, -0.063), (46, 0.039), (47, 0.032), (48, 0.087), (49, -0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97396803 <a title="164-lsi-1" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>Author: Guenther Zeck, Matthias Bethge, Jakob H. Macke</p><p>Abstract: S timulus selectivity of sensory neurons is often characterized by estimating their receptive ﬁeld properties such as orientation selectivity. Receptive ﬁelds are usually derived from the mean (or covariance) of the spike-triggered stimulus ensemble. This approach treats each spike as an independent message but does not take into account that information might be conveyed through patterns of neural activity that are distributed across space or time. Can we ﬁnd a concise description for the processing of a whole population of neurons analogous to the receptive ﬁeld for single neurons? Here, we present a generalization of the linear receptive ﬁeld which is not bound to be triggered on individual spikes but can be meaningfully linked to distributed response patterns. More precisely, we seek to identify those stimulus features and the corresponding patterns of neural activity that are most reliably coupled. We use an extension of reverse-correlation methods based on canonical correlation analysis. The resulting population receptive ﬁelds span the subspace of stimuli that is most informative about the population response. We evaluate our approach using both neuronal models and multi-electrode recordings from rabbit retinal ganglion cells. We show how the model can be extended to capture nonlinear stimulus-response relationships using kernel canonical correlation analysis, which makes it possible to test different coding mechanisms. Our technique can also be used to calculate receptive ﬁelds from multi-dimensional neural measurements such as those obtained from dynamic imaging methods. 1</p><p>2 0.84311086 <a title="164-lsi-2" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><p>3 0.81720734 <a title="164-lsi-3" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>Author: Jonathan W. Pillow, Peter E. Latham</p><p>Abstract: Point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, owing in part to the fact that they do not take into account multiple stages of processing. Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more ﬂexible for ﬁtting neural data. More importantly, it allows us to estimate connectivity patterns among neurons (both observed and unobserved), and may provide insight into how networks process sensory input. We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model’s performance using a simulated example network consisting of two coupled neurons.</p><p>4 0.78619516 <a title="164-lsi-4" href="./nips-2007-Estimating_disparity_with_confidence_from_energy_neurons.html">81 nips-2007-Estimating disparity with confidence from energy neurons</a></p>
<p>Author: Eric K. Tsang, Bertram E. Shi</p><p>Abstract: The peak location in a population of phase-tuned neurons has been shown to be a more reliable estimator for disparity than the peak location in a population of position-tuned neurons. Unfortunately, the disparity range covered by a phasetuned population is limited by phase wraparound. Thus, a single population cannot cover the large range of disparities encountered in natural scenes unless the scale of the receptive fields is chosen to be very large, which results in very low resolution depth estimates. Here we describe a biologically plausible measure of the confidence that the stimulus disparity is inside the range covered by a population of phase-tuned neurons. Based upon this confidence measure, we propose an algorithm for disparity estimation that uses many populations of high-resolution phase-tuned neurons that are biased to different disparity ranges via position shifts between the left and right eye receptive fields. The population with the highest confidence is used to estimate the stimulus disparity. We show that this algorithm outperforms a previously proposed coarse-to-fine algorithm for disparity estimation, which uses disparity estimates from coarse scales to select the populations used at finer scales and can effectively detect occlusions.</p><p>5 0.69355196 <a title="164-lsi-5" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>Author: Tatyana Sharpee</p><p>Abstract: This paper compares a family of methods for characterizing neural feature selectivity with natural stimuli in the framework of the linear-nonlinear model. In this model, the neural ﬁring rate is a nonlinear function of a small number of relevant stimulus components. The relevant stimulus dimensions can be found by maximizing one of the family of objective functions, R´ nyi divergences of different e orders [1, 2]. We show that maximizing one of them, R´ nyi divergence of ore der 2, is equivalent to least-square ﬁtting of the linear-nonlinear model to neural data. Next, we derive reconstruction errors in relevant dimensions found by maximizing R´ nyi divergences of arbitrary order in the asymptotic limit of large spike e numbers. We ﬁnd that the smallest errors are obtained with R´ nyi divergence of e order 1, also known as Kullback-Leibler divergence. This corresponds to ﬁnding relevant dimensions by maximizing mutual information [2]. We numerically test how these optimization schemes perform in the regime of low signal-to-noise ratio (small number of spikes and increasing neural noise) for model visual neurons. We ﬁnd that optimization schemes based on either least square ﬁtting or information maximization perform well even when number of spikes is small. Information maximization provides slightly, but signiﬁcantly, better reconstructions than least square ﬁtting. This makes the problem of ﬁnding relevant dimensions, together with the problem of lossy compression [3], one of examples where informationtheoretic measures are no more data limited than those derived from least squares. 1</p><p>6 0.63553518 <a title="164-lsi-6" href="./nips-2007-Contraction_Properties_of_VLSI_Cooperative_Competitive_Neural_Networks_of_Spiking_Neurons.html">60 nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</a></p>
<p>7 0.48819622 <a title="164-lsi-7" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>8 0.47778097 <a title="164-lsi-8" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>9 0.45129597 <a title="164-lsi-9" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>10 0.45111609 <a title="164-lsi-10" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>11 0.43967825 <a title="164-lsi-11" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>12 0.43596983 <a title="164-lsi-12" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>13 0.41870573 <a title="164-lsi-13" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>14 0.3955217 <a title="164-lsi-14" href="./nips-2007-An_online_Hebbian_learning_rule_that_performs_Independent_Component_Analysis.html">26 nips-2007-An online Hebbian learning rule that performs Independent Component Analysis</a></p>
<p>15 0.39432201 <a title="164-lsi-15" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<p>16 0.36243734 <a title="164-lsi-16" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>17 0.31252381 <a title="164-lsi-17" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>18 0.31023848 <a title="164-lsi-18" href="./nips-2007-Learning_to_classify_complex_patterns_using_a_VLSI_network_of_spiking_neurons.html">117 nips-2007-Learning to classify complex patterns using a VLSI network of spiking neurons</a></p>
<p>19 0.30338672 <a title="164-lsi-19" href="./nips-2007-Augmented_Functional_Time_Series_Representation_and_Forecasting_with_Gaussian_Processes.html">28 nips-2007-Augmented Functional Time Series Representation and Forecasting with Gaussian Processes</a></p>
<p>20 0.30192739 <a title="164-lsi-20" href="./nips-2007-Continuous_Time_Particle_Filtering_for_fMRI.html">59 nips-2007-Continuous Time Particle Filtering for fMRI</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.06), (13, 0.034), (16, 0.119), (18, 0.019), (19, 0.03), (21, 0.082), (31, 0.01), (34, 0.028), (35, 0.024), (36, 0.263), (47, 0.086), (49, 0.016), (83, 0.074), (87, 0.016), (90, 0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81839836 <a title="164-lda-1" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>Author: Guenther Zeck, Matthias Bethge, Jakob H. Macke</p><p>Abstract: S timulus selectivity of sensory neurons is often characterized by estimating their receptive ﬁeld properties such as orientation selectivity. Receptive ﬁelds are usually derived from the mean (or covariance) of the spike-triggered stimulus ensemble. This approach treats each spike as an independent message but does not take into account that information might be conveyed through patterns of neural activity that are distributed across space or time. Can we ﬁnd a concise description for the processing of a whole population of neurons analogous to the receptive ﬁeld for single neurons? Here, we present a generalization of the linear receptive ﬁeld which is not bound to be triggered on individual spikes but can be meaningfully linked to distributed response patterns. More precisely, we seek to identify those stimulus features and the corresponding patterns of neural activity that are most reliably coupled. We use an extension of reverse-correlation methods based on canonical correlation analysis. The resulting population receptive ﬁelds span the subspace of stimuli that is most informative about the population response. We evaluate our approach using both neuronal models and multi-electrode recordings from rabbit retinal ganglion cells. We show how the model can be extended to capture nonlinear stimulus-response relationships using kernel canonical correlation analysis, which makes it possible to test different coding mechanisms. Our technique can also be used to calculate receptive ﬁelds from multi-dimensional neural measurements such as those obtained from dynamic imaging methods. 1</p><p>2 0.71137381 <a title="164-lda-2" href="./nips-2007-Non-parametric_Modeling_of_Partially_Ranked_Data.html">142 nips-2007-Non-parametric Modeling of Partially Ranked Data</a></p>
<p>Author: Guy Lebanon, Yi Mao</p><p>Abstract: Statistical models on full and partial rankings of n items are often of limited practical use for large n due to computational consideration. We explore the use of non-parametric models for partially ranked data and derive efﬁcient procedures for their use for large n. The derivations are largely possible through combinatorial and algebraic manipulations based on the lattice of partial rankings. In particular, we demonstrate for the ﬁrst time a non-parametric coherent and consistent model capable of efﬁciently aggregating partially ranked data of different types. 1</p><p>3 0.63136584 <a title="164-lda-3" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>Author: Lars Buesing, Wolfgang Maass</p><p>Abstract: We show that under suitable assumptions (primarily linearization) a simple and perspicuous online learning rule for Information Bottleneck optimization with spiking neurons can be derived. This rule performs on common benchmark tasks as well as a rather complex rule that has previously been proposed [1]. Furthermore, the transparency of this new learning rule makes a theoretical analysis of its convergence properties feasible. A variation of this learning rule (with sign changes) provides a theoretically founded method for performing Principal Component Analysis (PCA) with spiking neurons. By applying this rule to an ensemble of neurons, different principal components of the input can be extracted. In addition, it is possible to preferentially extract those principal components from incoming signals X that are related or are not related to some additional target signal YT . In a biological interpretation, this target signal YT (also called relevance variable) could represent proprioceptive feedback, input from other sensory modalities, or top-down signals. 1</p><p>4 0.62215143 <a title="164-lda-4" href="./nips-2007-Iterative_Non-linear_Dimensionality_Reduction_with_Manifold_Sculpting.html">107 nips-2007-Iterative Non-linear Dimensionality Reduction with Manifold Sculpting</a></p>
<p>Author: Michael Gashler, Dan Ventura, Tony Martinez</p><p>Abstract: Many algorithms have been recently developed for reducing dimensionality by projecting data onto an intrinsic non-linear manifold. Unfortunately, existing algorithms often lose signiﬁcant precision in this transformation. Manifold Sculpting is a new algorithm that iteratively reduces dimensionality by simulating surface tension in local neighborhoods. We present several experiments that show Manifold Sculpting yields more accurate results than existing algorithms with both generated and natural data-sets. Manifold Sculpting is also able to beneﬁt from both prior dimensionality reduction efforts. 1</p><p>5 0.59686404 <a title="164-lda-5" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><p>6 0.58118492 <a title="164-lda-6" href="./nips-2007-Robust_Regression_with_Twinned_Gaussian_Processes.html">170 nips-2007-Robust Regression with Twinned Gaussian Processes</a></p>
<p>7 0.58037609 <a title="164-lda-7" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>8 0.57533085 <a title="164-lda-8" href="./nips-2007-Topmoumoute_Online_Natural_Gradient_Algorithm.html">206 nips-2007-Topmoumoute Online Natural Gradient Algorithm</a></p>
<p>9 0.57213205 <a title="164-lda-9" href="./nips-2007-Contraction_Properties_of_VLSI_Cooperative_Competitive_Neural_Networks_of_Spiking_Neurons.html">60 nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</a></p>
<p>10 0.56310809 <a title="164-lda-10" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>11 0.55192131 <a title="164-lda-11" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>12 0.55072552 <a title="164-lda-12" href="./nips-2007-The_Generalized_FITC_Approximation.html">195 nips-2007-The Generalized FITC Approximation</a></p>
<p>13 0.52004057 <a title="164-lda-13" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>14 0.51761746 <a title="164-lda-14" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>15 0.5157218 <a title="164-lda-15" href="./nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">79 nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<p>16 0.51254362 <a title="164-lda-16" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>17 0.51029497 <a title="164-lda-17" href="./nips-2007-Fast_Variational_Inference_for_Large-scale_Internet_Diagnosis.html">87 nips-2007-Fast Variational Inference for Large-scale Internet Diagnosis</a></p>
<p>18 0.50895363 <a title="164-lda-18" href="./nips-2007-Augmented_Functional_Time_Series_Representation_and_Forecasting_with_Gaussian_Processes.html">28 nips-2007-Augmented Functional Time Series Representation and Forecasting with Gaussian Processes</a></p>
<p>19 0.50408375 <a title="164-lda-19" href="./nips-2007-Probabilistic_Matrix_Factorization.html">158 nips-2007-Probabilistic Matrix Factorization</a></p>
<p>20 0.50376678 <a title="164-lda-20" href="./nips-2007-Selecting_Observations_against_Adversarial_Objectives.html">174 nips-2007-Selecting Observations against Adversarial Objectives</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
