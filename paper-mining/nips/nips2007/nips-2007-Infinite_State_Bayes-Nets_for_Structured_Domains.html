<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>105 nips-2007-Infinite State Bayes-Nets for Structured Domains</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-105" href="#">nips2007-105</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>105 nips-2007-Infinite State Bayes-Nets for Structured Domains</h1>
<br/><p>Source: <a title="nips-2007-105-pdf" href="http://papers.nips.cc/paper/3310-infinite-state-bayes-nets-for-structured-domains.pdf">pdf</a></p><p>Author: Max Welling, Ian Porteous, Evgeniy Bart</p><p>Abstract: A general modeling framework is proposed that uniﬁes nonparametric-Bayesian models, topic-models and Bayesian networks. This class of inﬁnite state Bayes nets (ISBN) can be viewed as directed networks of ‘hierarchical Dirichlet processes’ (HDPs) where the domain of the variables can be structured (e.g. words in documents or features in images). We show that collapsed Gibbs sampling can be done efﬁciently in these models by leveraging the structure of the Bayes net and using the forward-ﬁltering-backward-sampling algorithm for junction trees. Existing models, such as nested-DP, Pachinko allocation, mixed membership stochastic block models as well as a number of new models are described as ISBNs. Two experiments have been performed to illustrate these ideas. 1</p><p>Reference: <a title="nips-2007-105-reference" href="../nips2007_reference/nips-2007-Infinite_State_Bayes-Nets_for_Structured_Domains_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This class of inﬁnite state Bayes nets (ISBN) can be viewed as directed networks of ‘hierarchical Dirichlet processes’ (HDPs) where the domain of the variables can be structured (e. [sent-5, score-0.191]
</p><p>2 We show that collapsed Gibbs sampling can be done efﬁciently in these models by leveraging the structure of the Bayes net and using the forward-ﬁltering-backward-sampling algorithm for junction trees. [sent-8, score-0.278]
</p><p>3 A recent development in this area is a class of Bayes nets known as topic models (e. [sent-13, score-0.198]
</p><p>4 A recent statistical sophistication of topic models is a nonparametric extension known as HDP [2], which adaptively infers the number of topics based on the available data. [sent-16, score-0.275]
</p><p>5 We consider models where the variables may have the nested structure of documents and images, may have inﬁnite discrete state spaces, and where the random variables are related through the intuitive causal dependencies of a Bayes net. [sent-19, score-0.35]
</p><p>6 Inference in these networks is achieved through a two-stage Gibbs sampler, which combines the “forward-ﬁltering-backward-sampling algorithm” [3] extended to junction trees and the direct assignment sampler for HDPs [2]. [sent-21, score-0.182]
</p><p>7 2  Bayes Net Structure for ISBN  Consider observed random variables xA {xa }, a = 1. [sent-22, score-0.093]
</p><p>8 These variables can take values in an arbitrary domain. [sent-25, score-0.093]
</p><p>9 In the following we will assume that xa is sampled from a (conditional) distribution in the exponential family. [sent-26, score-0.206]
</p><p>10 We will also introduce hidden (unobserved, latent) variables {zb }, b = 1. [sent-27, score-0.14]
</p><p>11 The indices a, b thus index the nodes of the Bayesian network. [sent-30, score-0.117]
</p><p>12 however also be interested in more structured data, such as words in documents, where the index n can be decomposed into e. [sent-42, score-0.122]
</p><p>13 In this notation we think of j as labelling a document and ij as labelling a word in document j. [sent-45, score-0.23]
</p><p>14 The unobserved structure is labelled with the discrete “assignment variables” za which assign the object indexed by n to latent groups (a. [sent-53, score-0.331]
</p><p>15 The assignment variables z together with the observed variables x are organized into a Bayes net, where dependencies are encoded by the usual “conditional probability tables” (CPT), which we denote with φaa |℘a for observed variables and π b b |℘b for latent variables1 . [sent-57, score-0.413]
</p><p>16 Here, ℘a denotes the x z joint state of all the parent variables of xa or zb . [sent-58, score-0.766]
</p><p>17 When a vertical bar is present we normalize over the variables to the left of it, e. [sent-59, score-0.093]
</p><p>18 Note that CPTs are considered random x variables and may themselves be indexed by (a subset of) n, e. [sent-62, score-0.144]
</p><p>19 π zb |℘b ∼ D[αb τ zb ] independently and identically for all values of ℘b . [sent-67, score-0.598]
</p><p>20 The distribution τ itself is Dirichlet distributed, τ za ∼ D[γ a /K a ], where K a is the number of states for variable za . [sent-68, score-0.256]
</p><p>21 We can put gamma priors on αa , γ a and consider them as random variables as well, but to keep things simple we will consider them ﬁxed variables here. [sent-69, score-0.186]
</p><p>22 However, it is always possible to infer the number of times variables in the BN are replicated by looking at its indices. [sent-73, score-0.093]
</p><p>23 Here φ is a distribution over words, one for each topic value z, and is often referred to a “topic distribution”. [sent-78, score-0.157]
</p><p>24 Topic values are generated from a document speciﬁc distribution π which in turn are generated from a “mother distribution” over topics τ . [sent-79, score-0.164]
</p><p>25 One of the key features of the HDP is that topics are shared across all documents indexed by j. [sent-87, score-0.281]
</p><p>26 We could add this node to the BN as a parent of z (since π depends on it) and reinterpret the statement of sharing topics as a fully connected transition matrix between states of ι to states of z. [sent-91, score-0.386]
</p><p>27 This idea can be extended to a combination of fully observed parent variables and multiple unobserved parent variables, e. [sent-92, score-0.356]
</p><p>28 Moreover, the child variables do not have to be observed either, so we can also replace x → z. [sent-95, score-0.093]
</p><p>29 In this fashion we can connect together multiple vertical stacks τ → φ → z where each such module is part of a “virtual-HDP” where the joint child states act as virtual data and the joint parent states act as virtual document labels. [sent-96, score-0.323]
</p><p>30 1a (inﬁnite extension of a Bayes net with IID data items) and 3a (inﬁnite extension of Pachinko Allocation). [sent-98, score-0.101]
</p><p>31 In this case data is unstructured, assumed IID and indexed by a ﬂat index n = i. [sent-101, score-0.127]
</p><p>32 There is a considerable body of empirical evidence which conﬁrms that marginalizing out the variables π, φ will result in improved inference (e. [sent-103, score-0.093]
</p><p>33 In this collapsed space, we sample two sets of variables alternatingly, {z} on the hand and {τ } on the other. [sent-106, score-0.154]
</p><p>34 We then draw a number Nk|l Bernoulli random variables with probability of success given by the t elements of v, which we call3 st and compute their sum across t: Sk|l = t sk|l . [sent-112, score-0.093]
</p><p>35 This prok|l cedure is equivalent to running a Chinese restaurant process (CRP) with Nk|l customers and only keeping track of how many tables become occupied. [sent-113, score-0.168]
</p><p>36 If the state corresponding to γ is picked, a new state is created and we increment K a ← K a + 1. [sent-119, score-0.114]
</p><p>37 E[π zb |℘b ] = (α τ b + Nzb |℘b )/(αb + N℘b ) and similarly for φ. [sent-122, score-0.299]
</p><p>38 z 3 These variables are so called auxiliary variables to facilitate sampling τ . [sent-123, score-0.292]
</p><p>39 b  3  z1  1  3  2  1  2  z2  x |z 1  z 1|z 2 j  z 2|z 3 j  x ji  1 z ji  3  2 z ji  2  1  z3  z 3|j  x |z 1  z 1|z 2  z 2|z 3  xn  zn1  zn2  3 z ji  (a)  3  z3  zn3  (b)  Figure 3: Graphical representation for (a) Pachinko Allocation and (b) Nested DP. [sent-124, score-0.964]
</p><p>40 This will allow assignment variables to add or remove states adaptively4 . [sent-126, score-0.228]
</p><p>41 Hence, we can use the structure of the Bayes net to sample the assignment variables jointly across the BN (for data-case i). [sent-129, score-0.311]
</p><p>42 5  ISBN for Structured Data  In section 2 we introduced an index n to label the known structure of the data. [sent-137, score-0.076]
</p><p>43 1c where n = (kji) is labelling for instance words (i) in chapters (j) in books (k). [sent-143, score-0.228]
</p><p>44 The ﬁrst level CPT π z|kj is speciﬁc to chapters (and hence books) and is sampled from a Dirichlet distribution with mean given 4 We adopted the name ‘inﬁnite state Bayesian network’ because the (K a + 1)th state actually represents an inﬁnite pool of indistinguishable states. [sent-144, score-0.189]
</p><p>45 To sample from ρ, τ we compute counts Nu|m,jk which is the number of times words were assigned in chapter j and book k to the joint state z = u, ℘ = m. [sent-147, score-0.103]
</p><p>46 We then work our way up the stack, sampling new count arrays S, R as we go, and then down again sampling the CPTs (τ , ρ) using these count arrays5 . [sent-148, score-0.132]
</p><p>47 If all z variables carry the same index n, sampling zn given the hierarchical priors is very similar to the FFBS procedure described in the previous section, except that the count arrays may carry ¬ijk a subset of the indices from n, e. [sent-151, score-0.331]
</p><p>48 If two neighboring z variables carry different subsets of n labels, e. [sent-155, score-0.093]
</p><p>49 The general rule is to identify and remove all z variables that are impacted by changing the value for z under consideration, e. [sent-159, score-0.143]
</p><p>50 To j jw jf compute the conditional probability we set z = k and add the impacted variables z back into the system, one-by-one in an arbitrary order and assigning them to their old values. [sent-163, score-0.277]
</p><p>51 In taking the inﬁnite limit the conditional distribution for existing states zb becomes directly proportional to Nzb |℘b (the αb τ zb term is missing). [sent-165, score-0.656]
</p><p>52 This has the effect that a new state z b = k that was discovered for some parent state ℘b = l will not be available to other parent states, simply because Nk|l = 0, l = l. [sent-166, score-0.336]
</p><p>53 A DP prior is certainly appropriate for nodes zb with CPTs that do not depend on other parents or additional labels, e. [sent-172, score-0.299]
</p><p>54 It consists of a single topic node and a single observation node. [sent-180, score-0.198]
</p><p>55 If we make φ depend on j instead and add a prior: ψ x|z → φx|z,j , we obtain an “HDP with random effects” [12] which has the beneﬁt that shared topics across documents can vary slightly relative to each other. [sent-184, score-0.23]
</p><p>56 Example: Inﬁnite State Chains The ‘Pachinko allocation model’ (PAM) [13] consists of a linear chain of assignment variables with document speciﬁc transition probabilities, see Fig. [sent-185, score-0.267]
</p><p>57 Here, images are modeled as mixtures over parts and parts were modeled as mixtures over visual words. [sent-191, score-0.171]
</p><p>58 Finally, a visual word is a distribution over features. [sent-192, score-0.097]
</p><p>59 2a has a data variable xji and two parent topic variables z1 and z2 . [sent-196, score-0.411]
</p><p>60 One can think of j as the customer index and i as the product index (and no IID ji ji repeated index). [sent-197, score-0.783]
</p><p>61 The value of x is the rating of that customer for that product. [sent-198, score-0.149]
</p><p>62 The hidden variables 5  Teh’s code npbayes-r21, (available from his web-site) does in fact implement this sampling process. [sent-199, score-0.206]
</p><p>63 5  z1 and z2 represent product groups and customer groups. [sent-200, score-0.232]
</p><p>64 Every data entry is assigned to both a ji ji customer group and a product group which together determine the factor from which we sample the rating. [sent-201, score-0.753]
</p><p>65 Note that the difference between the assignment variables is that their corresponding CPTs π z1 ,j and π z2 ,i depend on j and i respectively. [sent-202, score-0.17]
</p><p>66 Also, single topics can be generalized to hierarchies of topics, so every branch becomes a PAM. [sent-207, score-0.161]
</p><p>67 Note that for unobserved xji values (not all products have been rated by all customers) the corresponding za , zb are “dangling” and can be integrated out. [sent-208, score-0.489]
</p><p>68 The result is that we should skip that variable in ji ji the Gibbs sampler. [sent-209, score-0.482]
</p><p>69 The main difference with HDP is that (like BiHDP) π depends on two parent states zi→j and zj→i by which we mean that item i has chosen topic zi→j to interact with item j and vice versa. [sent-212, score-0.612]
</p><p>70 However, (unlike BiHDP) those topic states share a common distribution π. [sent-213, score-0.215]
</p><p>71 The hidden variables jointly label the type of interaction that was used to generate ‘matrix-element’ xij . [sent-216, score-0.18]
</p><p>72 2c has two observed variables and an assignment variable that is not repeated over items. [sent-219, score-0.17]
</p><p>73 The left branch can then model words on the web-page while the right branch can model visual features on the web-page. [sent-221, score-0.183]
</p><p>74 Market Basket Data In this experiment we investigate the performance of BiHDP on a synthetic market basket dataset. [sent-226, score-0.199]
</p><p>75 The generated data consists of purchases from simulated groups of customers who have similar buying habits. [sent-229, score-0.239]
</p><p>76 Similarity of buying habits refers to the fact that customers within a group buy similar groups of items. [sent-230, score-0.3]
</p><p>77 For example, items like strawberries and cream are likely to be in the same item group and thus are likely to be purchased together in the same market basket. [sent-231, score-0.407]
</p><p>78 The following parameters were used to generate data for our experiments: 1M transactions, 10K customers, 1K different items, 4 items per transaction on average, 4 item groups per customer group on average, 50 market basket patterns, 50 customer patterns. [sent-232, score-0.888]
</p><p>79 The two assignment variables correspond to customers and items respectively. [sent-234, score-0.39]
</p><p>80 For a given pair of customer and item groups, a binomial distribution was used to model the probability of a customer group making a purchase from that item group. [sent-235, score-0.645]
</p><p>81 A collapsed Gibbs sampler was used to ﬁt the model. [sent-236, score-0.116]
</p><p>82 After 1000 epochs the system converged to 278 customer groups and 39 item factors. [sent-237, score-0.375]
</p><p>83 As can be seen, most item groups correspond directly to the hidden ground truth data. [sent-240, score-0.273]
</p><p>84 The visual vocabulary is usually determined in a preprocessing step where k-means is run to cluster features collected from the training set. [sent-243, score-0.094]
</p><p>85 In [15] a different approach was proposed in which the visual word vocabulary was learned jointly with ﬁtting the parameters of the model. [sent-244, score-0.237]
</p><p>86 This can have the beneﬁt that the vocabulary is better adapted to suit the needs of the model. [sent-245, score-0.086]
</p><p>87 3a with 2 hidden variables (instead of 3) and π z1 |z2 independent of j. [sent-247, score-0.14]
</p><p>88 x is modeled as a Gaussian-distributed random variable over feature values, z1 represents the word identity and z2 is the topic index. [sent-248, score-0.203]
</p><p>89 Ground truth items have no associated weight; therefore, they were ordered to facilitate comparison with the left row. [sent-254, score-0.144]
</p><p>90 8  1  Figure 5: Precision Recall curves for Caltech-4 dataset (left) and turntable dataset (right). [sent-272, score-0.1]
</p><p>91 LDA was ﬁt using 500 visual words and 50 parts (which we found to give the best results). [sent-277, score-0.097]
</p><p>92 The turntable database contains images of 15 toy objects. [sent-278, score-0.14]
</p><p>93 The objects were placed on a turntable and photographed every 5 degrees. [sent-279, score-0.1]
</p><p>94 LDA used 15 topics and 200 visual words (which again was optimal). [sent-281, score-0.215]
</p><p>95 We are not interested in claiming superiority of ISBNs, but rather hope to convey that ISBNs are a convenient tool to design models and to facilitate the search for the number of latent states. [sent-289, score-0.097]
</p><p>96 Not every topic model naturally ﬁts the suit of an ISBN. [sent-292, score-0.2]
</p><p>97 Also, in [10, 20] models were studied where a word can be emitted at 7  any node corresponding to a topic variable z. [sent-296, score-0.244]
</p><p>98 A collapsed variational bayesian inference algorithm for latent dirichlet allocation. [sent-349, score-0.335]
</p><p>99 Hierarchical topic models and the nested chinese restaurant process. [sent-366, score-0.299]
</p><p>100 Mining associations between sets of items in massive databases. [sent-405, score-0.104]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('zb', 0.299), ('hdp', 0.282), ('ji', 0.241), ('xa', 0.206), ('bihdp', 0.174), ('dirichlet', 0.162), ('topic', 0.157), ('pachinko', 0.152), ('nzb', 0.149), ('pom', 0.149), ('customer', 0.149), ('item', 0.143), ('isbn', 0.139), ('cpts', 0.125), ('bayes', 0.119), ('topics', 0.118), ('customers', 0.116), ('parent', 0.111), ('items', 0.104), ('net', 0.101), ('basket', 0.1), ('cpt', 0.1), ('turntable', 0.1), ('market', 0.099), ('za', 0.099), ('variables', 0.093), ('nk', 0.089), ('groups', 0.083), ('assignment', 0.077), ('index', 0.076), ('lda', 0.076), ('chapters', 0.075), ('isbns', 0.075), ('jf', 0.075), ('jkm', 0.075), ('kji', 0.075), ('gibbs', 0.07), ('su', 0.069), ('iid', 0.067), ('sampling', 0.066), ('hdps', 0.065), ('zi', 0.065), ('sk', 0.065), ('documents', 0.063), ('books', 0.061), ('collapsed', 0.061), ('group', 0.061), ('jw', 0.059), ('states', 0.058), ('nite', 0.058), ('state', 0.057), ('latent', 0.057), ('learned', 0.057), ('bayesian', 0.055), ('sampler', 0.055), ('hierarchical', 0.055), ('teh', 0.054), ('xb', 0.052), ('restaurant', 0.052), ('ru', 0.052), ('allocation', 0.051), ('indexed', 0.051), ('visual', 0.051), ('downstream', 0.05), ('ffbs', 0.05), ('impacted', 0.05), ('nxa', 0.05), ('stacks', 0.05), ('vocabularies', 0.05), ('xji', 0.05), ('junction', 0.05), ('shared', 0.049), ('hidden', 0.047), ('word', 0.046), ('document', 0.046), ('words', 0.046), ('chinese', 0.046), ('aa', 0.046), ('bn', 0.046), ('labelling', 0.046), ('membership', 0.046), ('nested', 0.044), ('suit', 0.043), ('ijk', 0.043), ('pam', 0.043), ('depicted', 0.043), ('vocabulary', 0.043), ('branch', 0.043), ('nets', 0.041), ('indices', 0.041), ('unobserved', 0.041), ('node', 0.041), ('facilitate', 0.04), ('mixtures', 0.04), ('jointly', 0.04), ('images', 0.04), ('buying', 0.04), ('bart', 0.04), ('nu', 0.04), ('modules', 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="105-tfidf-1" href="./nips-2007-Infinite_State_Bayes-Nets_for_Structured_Domains.html">105 nips-2007-Infinite State Bayes-Nets for Structured Domains</a></p>
<p>Author: Max Welling, Ian Porteous, Evgeniy Bart</p><p>Abstract: A general modeling framework is proposed that uniﬁes nonparametric-Bayesian models, topic-models and Bayesian networks. This class of inﬁnite state Bayes nets (ISBN) can be viewed as directed networks of ‘hierarchical Dirichlet processes’ (HDPs) where the domain of the variables can be structured (e.g. words in documents or features in images). We show that collapsed Gibbs sampling can be done efﬁciently in these models by leveraging the structure of the Bayes net and using the forward-ﬁltering-backward-sampling algorithm for junction trees. Existing models, such as nested-DP, Pachinko allocation, mixed membership stochastic block models as well as a number of new models are described as ISBNs. Two experiments have been performed to illustrate these ideas. 1</p><p>2 0.24094228 <a title="105-tfidf-2" href="./nips-2007-Collapsed_Variational_Inference_for_HDP.html">47 nips-2007-Collapsed Variational Inference for HDP</a></p>
<p>Author: Yee W. Teh, Kenichi Kurihara, Max Welling</p><p>Abstract: A wide variety of Dirichlet-multinomial ‘topic’ models have found interesting applications in recent years. While Gibbs sampling remains an important method of inference in such models, variational techniques have certain advantages such as easy assessment of convergence, easy optimization without the need to maintain detailed balance, a bound on the marginal likelihood, and side-stepping of issues with topic-identiﬁability. The most accurate variational technique thus far, namely collapsed variational latent Dirichlet allocation, did not deal with model selection nor did it include inference for hyperparameters. We address both issues by generalizing the technique, obtaining the ﬁrst variational algorithm to deal with the hierarchical Dirichlet process and to deal with hyperparameters of Dirichlet variables. Experiments show a signiﬁcant improvement in accuracy. 1</p><p>3 0.19787015 <a title="105-tfidf-3" href="./nips-2007-Spatial_Latent_Dirichlet_Allocation.html">183 nips-2007-Spatial Latent Dirichlet Allocation</a></p>
<p>Author: Xiaogang Wang, Eric Grimson</p><p>Abstract: In recent years, the language model Latent Dirichlet Allocation (LDA), which clusters co-occurring words into topics, has been widely applied in the computer vision ﬁeld. However, many of these applications have difﬁculty with modeling the spatial and temporal structure among visual words, since LDA assumes that a document is a “bag-of-words”. It is also critical to properly design “words” and “documents” when using a language model to solve vision problems. In this paper, we propose a topic model Spatial Latent Dirichlet Allocation (SLDA), which better encodes spatial structures among visual words that are essential for solving many vision problems. The spatial information is not encoded in the values of visual words but in the design of documents. Instead of knowing the partition of words into documents a priori, the word-document assignment becomes a random hidden variable in SLDA. There is a generative procedure, where knowledge of spatial structure can be ﬂexibly added as a prior, grouping visual words which are close in space into the same document. We use SLDA to discover objects from a collection of images, and show it achieves better performance than LDA. 1</p><p>4 0.19786006 <a title="105-tfidf-4" href="./nips-2007-The_Infinite_Markov_Model.html">197 nips-2007-The Infinite Markov Model</a></p>
<p>Author: Daichi Mochihashi, Eiichiro Sumita</p><p>Abstract: We present a nonparametric Bayesian method of estimating variable order Markov processes up to a theoretically inﬁnite order. By extending a stick-breaking prior, which is usually deﬁned on a unit interval, “vertically” to the trees of inﬁnite depth associated with a hierarchical Chinese restaurant process, our model directly infers the hidden orders of Markov dependencies from which each symbol originated. Experiments on character and word sequences in natural language showed that the model has a comparative performance with an exponentially large full-order model, while computationally much efﬁcient in both time and space. We expect that this basic model will also extend to the variable order hierarchical clustering of general data. 1</p><p>5 0.16923836 <a title="105-tfidf-5" href="./nips-2007-Distributed_Inference_for_Latent_Dirichlet_Allocation.html">73 nips-2007-Distributed Inference for Latent Dirichlet Allocation</a></p>
<p>Author: David Newman, Padhraic Smyth, Max Welling, Arthur U. Asuncion</p><p>Abstract: We investigate the problem of learning a widely-used latent-variable model – the Latent Dirichlet Allocation (LDA) or “topic” model – using distributed computation, where each of processors only sees of the total data set. We propose two distributed inference schemes that are motivated from different perspectives. The ﬁrst scheme uses local Gibbs sampling on each processor with periodic updates—it is simple to implement and can be viewed as an approximation to a single processor implementation of Gibbs sampling. The second scheme relies on a hierarchical Bayesian extension of the standard LDA model to directly account for the fact that data are distributed across processors—it has a theoretical guarantee of convergence but is more complex to implement than the approximate method. Using ﬁve real-world text corpora we show that distributed learning works very well for LDA models, i.e., perplexity and precision-recall scores for distributed learning are indistinguishable from those obtained with single-processor learning. Our extensive experimental results include large-scale distributed computation on 1000 virtual processors; and speedup experiments of learning topics in a 100-million word corpus using 16 processors. ¢ ¤ ¦¥£ ¢ ¢</p><p>6 0.15097485 <a title="105-tfidf-6" href="./nips-2007-Supervised_Topic_Models.html">189 nips-2007-Supervised Topic Models</a></p>
<p>7 0.11979784 <a title="105-tfidf-7" href="./nips-2007-HM-BiTAM%3A_Bilingual_Topic_Exploration%2C_Word_Alignment%2C_and_Translation.html">95 nips-2007-HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation</a></p>
<p>8 0.11825617 <a title="105-tfidf-8" href="./nips-2007-Comparing_Bayesian_models_for_multisensory_cue_combination_without_mandatory_integration.html">51 nips-2007-Comparing Bayesian models for multisensory cue combination without mandatory integration</a></p>
<p>9 0.11333051 <a title="105-tfidf-9" href="./nips-2007-Mining_Internet-Scale_Software_Repositories.html">129 nips-2007-Mining Internet-Scale Software Repositories</a></p>
<p>10 0.10486672 <a title="105-tfidf-10" href="./nips-2007-Fixing_Max-Product%3A_Convergent_Message_Passing_Algorithms_for_MAP_LP-Relaxations.html">92 nips-2007-Fixing Max-Product: Convergent Message Passing Algorithms for MAP LP-Relaxations</a></p>
<p>11 0.08807876 <a title="105-tfidf-11" href="./nips-2007-Retrieved_context_and_the_discovery_of_semantic_structure.html">169 nips-2007-Retrieved context and the discovery of semantic structure</a></p>
<p>12 0.084829755 <a title="105-tfidf-12" href="./nips-2007-Hidden_Common_Cause_Relations_in_Relational_Learning.html">97 nips-2007-Hidden Common Cause Relations in Relational Learning</a></p>
<p>13 0.080043793 <a title="105-tfidf-13" href="./nips-2007-Object_Recognition_by_Scene_Alignment.html">143 nips-2007-Object Recognition by Scene Alignment</a></p>
<p>14 0.074632779 <a title="105-tfidf-14" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>15 0.073402472 <a title="105-tfidf-15" href="./nips-2007-A_Bayesian_LDA-based_model_for_semi-supervised_part-of-speech_tagging.html">2 nips-2007-A Bayesian LDA-based model for semi-supervised part-of-speech tagging</a></p>
<p>16 0.072852843 <a title="105-tfidf-16" href="./nips-2007-Hierarchical_Penalization.html">99 nips-2007-Hierarchical Penalization</a></p>
<p>17 0.072694808 <a title="105-tfidf-17" href="./nips-2007-Unsupervised_Feature_Selection_for_Accurate_Recommendation_of_High-Dimensional_Image_Data.html">211 nips-2007-Unsupervised Feature Selection for Accurate Recommendation of High-Dimensional Image Data</a></p>
<p>18 0.069284901 <a title="105-tfidf-18" href="./nips-2007-An_Analysis_of_Convex_Relaxations_for_MAP_Estimation.html">23 nips-2007-An Analysis of Convex Relaxations for MAP Estimation</a></p>
<p>19 0.065604456 <a title="105-tfidf-19" href="./nips-2007-Efficient_Bayesian_Inference_for_Dynamically_Changing_Graphs.html">75 nips-2007-Efficient Bayesian Inference for Dynamically Changing Graphs</a></p>
<p>20 0.062160015 <a title="105-tfidf-20" href="./nips-2007-Sparse_Overcomplete_Latent_Variable_Decomposition_of_Counts_Data.html">181 nips-2007-Sparse Overcomplete Latent Variable Decomposition of Counts Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.228), (1, 0.074), (2, -0.089), (3, -0.329), (4, 0.059), (5, -0.112), (6, 0.054), (7, -0.081), (8, 0.026), (9, 0.024), (10, -0.049), (11, -0.046), (12, -0.009), (13, 0.105), (14, 0.027), (15, 0.04), (16, 0.091), (17, -0.079), (18, 0.031), (19, -0.051), (20, 0.022), (21, 0.042), (22, -0.064), (23, -0.005), (24, 0.087), (25, 0.057), (26, -0.085), (27, 0.071), (28, 0.138), (29, 0.041), (30, 0.072), (31, 0.03), (32, 0.017), (33, 0.008), (34, -0.018), (35, -0.107), (36, -0.032), (37, 0.079), (38, -0.008), (39, -0.019), (40, -0.03), (41, -0.102), (42, -0.059), (43, 0.13), (44, -0.108), (45, -0.019), (46, 0.147), (47, -0.093), (48, 0.086), (49, -0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94360381 <a title="105-lsi-1" href="./nips-2007-Infinite_State_Bayes-Nets_for_Structured_Domains.html">105 nips-2007-Infinite State Bayes-Nets for Structured Domains</a></p>
<p>Author: Max Welling, Ian Porteous, Evgeniy Bart</p><p>Abstract: A general modeling framework is proposed that uniﬁes nonparametric-Bayesian models, topic-models and Bayesian networks. This class of inﬁnite state Bayes nets (ISBN) can be viewed as directed networks of ‘hierarchical Dirichlet processes’ (HDPs) where the domain of the variables can be structured (e.g. words in documents or features in images). We show that collapsed Gibbs sampling can be done efﬁciently in these models by leveraging the structure of the Bayes net and using the forward-ﬁltering-backward-sampling algorithm for junction trees. Existing models, such as nested-DP, Pachinko allocation, mixed membership stochastic block models as well as a number of new models are described as ISBNs. Two experiments have been performed to illustrate these ideas. 1</p><p>2 0.75111359 <a title="105-lsi-2" href="./nips-2007-Collapsed_Variational_Inference_for_HDP.html">47 nips-2007-Collapsed Variational Inference for HDP</a></p>
<p>Author: Yee W. Teh, Kenichi Kurihara, Max Welling</p><p>Abstract: A wide variety of Dirichlet-multinomial ‘topic’ models have found interesting applications in recent years. While Gibbs sampling remains an important method of inference in such models, variational techniques have certain advantages such as easy assessment of convergence, easy optimization without the need to maintain detailed balance, a bound on the marginal likelihood, and side-stepping of issues with topic-identiﬁability. The most accurate variational technique thus far, namely collapsed variational latent Dirichlet allocation, did not deal with model selection nor did it include inference for hyperparameters. We address both issues by generalizing the technique, obtaining the ﬁrst variational algorithm to deal with the hierarchical Dirichlet process and to deal with hyperparameters of Dirichlet variables. Experiments show a signiﬁcant improvement in accuracy. 1</p><p>3 0.72000581 <a title="105-lsi-3" href="./nips-2007-Distributed_Inference_for_Latent_Dirichlet_Allocation.html">73 nips-2007-Distributed Inference for Latent Dirichlet Allocation</a></p>
<p>Author: David Newman, Padhraic Smyth, Max Welling, Arthur U. Asuncion</p><p>Abstract: We investigate the problem of learning a widely-used latent-variable model – the Latent Dirichlet Allocation (LDA) or “topic” model – using distributed computation, where each of processors only sees of the total data set. We propose two distributed inference schemes that are motivated from different perspectives. The ﬁrst scheme uses local Gibbs sampling on each processor with periodic updates—it is simple to implement and can be viewed as an approximation to a single processor implementation of Gibbs sampling. The second scheme relies on a hierarchical Bayesian extension of the standard LDA model to directly account for the fact that data are distributed across processors—it has a theoretical guarantee of convergence but is more complex to implement than the approximate method. Using ﬁve real-world text corpora we show that distributed learning works very well for LDA models, i.e., perplexity and precision-recall scores for distributed learning are indistinguishable from those obtained with single-processor learning. Our extensive experimental results include large-scale distributed computation on 1000 virtual processors; and speedup experiments of learning topics in a 100-million word corpus using 16 processors. ¢ ¤ ¦¥£ ¢ ¢</p><p>4 0.66299069 <a title="105-lsi-4" href="./nips-2007-Supervised_Topic_Models.html">189 nips-2007-Supervised Topic Models</a></p>
<p>Author: Jon D. Mcauliffe, David M. Blei</p><p>Abstract: We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive a maximum-likelihood procedure for parameter estimation, which relies on variational approximations to handle intractable posterior expectations. Prediction problems motivate this research: we use the ﬁtted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and web page popularity predicted from text descriptions. We illustrate the beneﬁts of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression. 1</p><p>5 0.61135083 <a title="105-lsi-5" href="./nips-2007-The_Infinite_Markov_Model.html">197 nips-2007-The Infinite Markov Model</a></p>
<p>Author: Daichi Mochihashi, Eiichiro Sumita</p><p>Abstract: We present a nonparametric Bayesian method of estimating variable order Markov processes up to a theoretically inﬁnite order. By extending a stick-breaking prior, which is usually deﬁned on a unit interval, “vertically” to the trees of inﬁnite depth associated with a hierarchical Chinese restaurant process, our model directly infers the hidden orders of Markov dependencies from which each symbol originated. Experiments on character and word sequences in natural language showed that the model has a comparative performance with an exponentially large full-order model, while computationally much efﬁcient in both time and space. We expect that this basic model will also extend to the variable order hierarchical clustering of general data. 1</p><p>6 0.59796089 <a title="105-lsi-6" href="./nips-2007-Spatial_Latent_Dirichlet_Allocation.html">183 nips-2007-Spatial Latent Dirichlet Allocation</a></p>
<p>7 0.45820314 <a title="105-lsi-7" href="./nips-2007-HM-BiTAM%3A_Bilingual_Topic_Exploration%2C_Word_Alignment%2C_and_Translation.html">95 nips-2007-HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation</a></p>
<p>8 0.45210963 <a title="105-lsi-8" href="./nips-2007-Modeling_homophily_and_stochastic_equivalence_in_symmetric_relational_data.html">131 nips-2007-Modeling homophily and stochastic equivalence in symmetric relational data</a></p>
<p>9 0.43814856 <a title="105-lsi-9" href="./nips-2007-Bayesian_Agglomerative_Clustering_with_Coalescents.html">31 nips-2007-Bayesian Agglomerative Clustering with Coalescents</a></p>
<p>10 0.43537807 <a title="105-lsi-10" href="./nips-2007-Unsupervised_Feature_Selection_for_Accurate_Recommendation_of_High-Dimensional_Image_Data.html">211 nips-2007-Unsupervised Feature Selection for Accurate Recommendation of High-Dimensional Image Data</a></p>
<p>11 0.42165369 <a title="105-lsi-11" href="./nips-2007-Mining_Internet-Scale_Software_Repositories.html">129 nips-2007-Mining Internet-Scale Software Repositories</a></p>
<p>12 0.39117536 <a title="105-lsi-12" href="./nips-2007-The_Infinite_Gamma-Poisson_Feature_Model.html">196 nips-2007-The Infinite Gamma-Poisson Feature Model</a></p>
<p>13 0.37177181 <a title="105-lsi-13" href="./nips-2007-Hidden_Common_Cause_Relations_in_Relational_Learning.html">97 nips-2007-Hidden Common Cause Relations in Relational Learning</a></p>
<p>14 0.36976996 <a title="105-lsi-14" href="./nips-2007-Comparing_Bayesian_models_for_multisensory_cue_combination_without_mandatory_integration.html">51 nips-2007-Comparing Bayesian models for multisensory cue combination without mandatory integration</a></p>
<p>15 0.3650724 <a title="105-lsi-15" href="./nips-2007-A_Probabilistic_Approach_to_Language_Change.html">9 nips-2007-A Probabilistic Approach to Language Change</a></p>
<p>16 0.35245925 <a title="105-lsi-16" href="./nips-2007-Discovering_Weakly-Interacting_Factors_in_a_Complex_Stochastic_Process.html">68 nips-2007-Discovering Weakly-Interacting Factors in a Complex Stochastic Process</a></p>
<p>17 0.35167 <a title="105-lsi-17" href="./nips-2007-Hierarchical_Penalization.html">99 nips-2007-Hierarchical Penalization</a></p>
<p>18 0.34202376 <a title="105-lsi-18" href="./nips-2007-Probabilistic_Matrix_Factorization.html">158 nips-2007-Probabilistic Matrix Factorization</a></p>
<p>19 0.33959723 <a title="105-lsi-19" href="./nips-2007-Retrieved_context_and_the_discovery_of_semantic_structure.html">169 nips-2007-Retrieved context and the discovery of semantic structure</a></p>
<p>20 0.3332552 <a title="105-lsi-20" href="./nips-2007-Active_Preference_Learning_with_Discrete_Choice_Data.html">19 nips-2007-Active Preference Learning with Discrete Choice Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.043), (13, 0.036), (16, 0.015), (18, 0.012), (21, 0.06), (31, 0.033), (34, 0.019), (35, 0.027), (47, 0.109), (83, 0.096), (85, 0.071), (87, 0.082), (90, 0.056), (98, 0.258)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77885985 <a title="105-lda-1" href="./nips-2007-Infinite_State_Bayes-Nets_for_Structured_Domains.html">105 nips-2007-Infinite State Bayes-Nets for Structured Domains</a></p>
<p>Author: Max Welling, Ian Porteous, Evgeniy Bart</p><p>Abstract: A general modeling framework is proposed that uniﬁes nonparametric-Bayesian models, topic-models and Bayesian networks. This class of inﬁnite state Bayes nets (ISBN) can be viewed as directed networks of ‘hierarchical Dirichlet processes’ (HDPs) where the domain of the variables can be structured (e.g. words in documents or features in images). We show that collapsed Gibbs sampling can be done efﬁciently in these models by leveraging the structure of the Bayes net and using the forward-ﬁltering-backward-sampling algorithm for junction trees. Existing models, such as nested-DP, Pachinko allocation, mixed membership stochastic block models as well as a number of new models are described as ISBNs. Two experiments have been performed to illustrate these ideas. 1</p><p>2 0.67175269 <a title="105-lda-2" href="./nips-2007-Estimating_divergence_functionals_and_the_likelihood_ratio_by_penalized_convex_risk_minimization.html">82 nips-2007-Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization</a></p>
<p>Author: Xuanlong Nguyen, Martin J. Wainwright, Michael I. Jordan</p><p>Abstract: We develop and analyze an algorithm for nonparametric estimation of divergence functionals and the density ratio of two probability distributions. Our method is based on a variational characterization of f -divergences, which turns the estimation into a penalized convex risk minimization problem. We present a derivation of our kernel-based estimation algorithm and an analysis of convergence rates for the estimator. Our simulation results demonstrate the convergence behavior of the method, which compares favorably with existing methods in the literature. 1</p><p>3 0.64820004 <a title="105-lda-3" href="./nips-2007-Efficient_Convex_Relaxation_for_Transductive_Support_Vector_Machine.html">76 nips-2007-Efficient Convex Relaxation for Transductive Support Vector Machine</a></p>
<p>Author: Zenglin Xu, Rong Jin, Jianke Zhu, Irwin King, Michael Lyu</p><p>Abstract: We consider the problem of Support Vector Machine transduction, which involves a combinatorial problem with exponential computational complexity in the number of unlabeled examples. Although several studies are devoted to Transductive SVM, they suffer either from the high computation complexity or from the solutions of local optimum. To address this problem, we propose solving Transductive SVM via a convex relaxation, which converts the NP-hard problem to a semi-deﬁnite programming. Compared with the other SDP relaxation for Transductive SVM, the proposed algorithm is computationally more efﬁcient with the number of free parameters reduced from O(n2 ) to O(n) where n is the number of examples. Empirical study with several benchmark data sets shows the promising performance of the proposed algorithm in comparison with other state-of-the-art implementations of Transductive SVM. 1</p><p>4 0.58242542 <a title="105-lda-4" href="./nips-2007-Continuous_Time_Particle_Filtering_for_fMRI.html">59 nips-2007-Continuous Time Particle Filtering for fMRI</a></p>
<p>Author: Lawrence Murray, Amos J. Storkey</p><p>Abstract: We construct a biologically motivated stochastic differential model of the neural and hemodynamic activity underlying the observed Blood Oxygen Level Dependent (BOLD) signal in Functional Magnetic Resonance Imaging (fMRI). The model poses a difﬁcult parameter estimation problem, both theoretically due to the nonlinearity and divergence of the differential system, and computationally due to its time and space complexity. We adapt a particle ﬁlter and smoother to the task, and discuss some of the practical approaches used to tackle the difﬁculties, including use of sparse matrices and parallelisation. Results demonstrate the tractability of the approach in its application to an effective connectivity study. 1</p><p>5 0.57195759 <a title="105-lda-5" href="./nips-2007-Supervised_Topic_Models.html">189 nips-2007-Supervised Topic Models</a></p>
<p>Author: Jon D. Mcauliffe, David M. Blei</p><p>Abstract: We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive a maximum-likelihood procedure for parameter estimation, which relies on variational approximations to handle intractable posterior expectations. Prediction problems motivate this research: we use the ﬁtted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and web page popularity predicted from text descriptions. We illustrate the beneﬁts of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression. 1</p><p>6 0.57156682 <a title="105-lda-6" href="./nips-2007-Distributed_Inference_for_Latent_Dirichlet_Allocation.html">73 nips-2007-Distributed Inference for Latent Dirichlet Allocation</a></p>
<p>7 0.56394988 <a title="105-lda-7" href="./nips-2007-Learning_Monotonic_Transformations_for_Classification.html">112 nips-2007-Learning Monotonic Transformations for Classification</a></p>
<p>8 0.56342131 <a title="105-lda-8" href="./nips-2007-A_Bayesian_LDA-based_model_for_semi-supervised_part-of-speech_tagging.html">2 nips-2007-A Bayesian LDA-based model for semi-supervised part-of-speech tagging</a></p>
<p>9 0.56210333 <a title="105-lda-9" href="./nips-2007-Combined_discriminative_and_generative_articulated_pose_and_non-rigid_shape_estimation.html">50 nips-2007-Combined discriminative and generative articulated pose and non-rigid shape estimation</a></p>
<p>10 0.55919683 <a title="105-lda-10" href="./nips-2007-Efficient_Inference_for_Distributions_on_Permutations.html">77 nips-2007-Efficient Inference for Distributions on Permutations</a></p>
<p>11 0.55369431 <a title="105-lda-11" href="./nips-2007-People_Tracking_with_the_Laplacian_Eigenmaps_Latent_Variable_Model.html">153 nips-2007-People Tracking with the Laplacian Eigenmaps Latent Variable Model</a></p>
<p>12 0.55181873 <a title="105-lda-12" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>13 0.5492748 <a title="105-lda-13" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>14 0.54905128 <a title="105-lda-14" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>15 0.5477041 <a title="105-lda-15" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>16 0.54693949 <a title="105-lda-16" href="./nips-2007-Scene_Segmentation_with_CRFs_Learned_from_Partially_Labeled_Images.html">172 nips-2007-Scene Segmentation with CRFs Learned from Partially Labeled Images</a></p>
<p>17 0.54688394 <a title="105-lda-17" href="./nips-2007-Unsupervised_Feature_Selection_for_Accurate_Recommendation_of_High-Dimensional_Image_Data.html">211 nips-2007-Unsupervised Feature Selection for Accurate Recommendation of High-Dimensional Image Data</a></p>
<p>18 0.54511589 <a title="105-lda-18" href="./nips-2007-Collapsed_Variational_Inference_for_HDP.html">47 nips-2007-Collapsed Variational Inference for HDP</a></p>
<p>19 0.54416782 <a title="105-lda-19" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>20 0.54248983 <a title="105-lda-20" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
