<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-103" href="#">nips2007-103</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</h1>
<br/><p>Source: <a title="nips-2007-103-pdf" href="http://papers.nips.cc/paper/3224-inferring-elapsed-time-from-stochastic-neural-processes.pdf">pdf</a></p><p>Author: Misha Ahrens, Maneesh Sahani</p><p>Abstract: Many perceptual processes and neural computations, such as speech recognition, motor control and learning, depend on the ability to measure and mark the passage of time. However, the processes that make such temporal judgements possible are unknown. A number of different hypothetical mechanisms have been advanced, all of which depend on the known, temporally predictable evolution of a neural or psychological state, possibly through oscillations or the gradual decay of a memory trace. Alternatively, judgements of elapsed time might be based on observations of temporally structured, but stochastic processes. Such processes need not be speciﬁc to the sense of time; typical neural and sensory processes contain at least some statistical structure across a range of time scales. Here, we investigate the statistical properties of an estimator of elapsed time which is based on a simple family of stochastic process. 1</p><p>Reference: <a title="nips-2007-103-reference" href="../nips2007_reference/nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract Many perceptual processes and neural computations, such as speech recognition, motor control and learning, depend on the ability to measure and mark the passage of time. [sent-5, score-0.423]
</p><p>2 However, the processes that make such temporal judgements possible are unknown. [sent-6, score-0.462]
</p><p>3 A number of different hypothetical mechanisms have been advanced, all of which depend on the known, temporally predictable evolution of a neural or psychological state, possibly through oscillations or the gradual decay of a memory trace. [sent-7, score-0.328]
</p><p>4 Alternatively, judgements of elapsed time might be based on observations of temporally structured, but stochastic processes. [sent-8, score-0.452]
</p><p>5 Such processes need not be speciﬁc to the sense of time; typical neural and sensory processes contain at least some statistical structure across a range of time scales. [sent-9, score-0.888]
</p><p>6 Here, we investigate the statistical properties of an estimator of elapsed time which is based on a simple family of stochastic process. [sent-10, score-0.402]
</p><p>7 1  Introduction  The experience of the passage of time, as well as the timing of events and intervals, has long been of interest in psychology, and has more recently attracted attention in neuroscience as well. [sent-11, score-0.514]
</p><p>8 Timing information is crucial for the correct functioning of a large number of processes, such as accurate limb movement, speech and the perception of speech (for example, the difference between “ba” and “pa” lies only in the relative timing of voice onsets), and causal learning. [sent-12, score-0.473]
</p><p>9 Neuroscientiﬁc evidence that points to a specialized neural substrate for timing is very sparse, particularly when compared to the divergent set of speciﬁc mechanisms which have been theorized. [sent-13, score-0.689]
</p><p>10 One of the most inﬂuential proposals, the scalar expectancy theory (SET) of timing [1], suggests that interval timing is based on the accumulation of activity from an internal oscillatory process. [sent-14, score-1.271]
</p><p>11 An alternative, which we explore here, is to phrase time estimation as a statistical problem, in which the elapsed time ∆t is extracted from a collection of stochastic processes whose statistics are known. [sent-18, score-0.727]
</p><p>12 Such models have 1  been related to recent psychological ﬁndings the show that the nature of the stimulus being timed affects judgments of duration [7]. [sent-20, score-0.198]
</p><p>13 Here, by contrast, we consider the properties of duration estimators that are based on more general stochastic processes. [sent-21, score-0.221]
</p><p>14 However, they may be seen as models both for internally-generated neural processes, such as (spontaneous) network activity and local ﬁeld potentials, and for sensory processes, in the form of externally-driven neural activity, or (taking a functional view) in the form of the stimuli themselves. [sent-23, score-0.386]
</p><p>15 Both neural activity and sensory input from the environment follow well-deﬁned temporal statistical patterns, but the exploitation of these statistics has thus far not been studied as a potential substrate for timing judgements, despite being potentially attractive. [sent-24, score-0.94]
</p><p>16 Such a basis for timing is consistent with recent studies that show that the statistics of external stimuli affect timing estimates [8, 7], a behavior not captured by the existing mechanistic models. [sent-25, score-1.1]
</p><p>17 In addition, there is evidence that timing mechanisms are distributed [9] but subject to local (e. [sent-26, score-0.546]
</p><p>18 Using the distributed time-varying processes which are already present in the brain is implementationally efﬁcient, and lends itself straightforwardly to a distributed implementation. [sent-29, score-0.352]
</p><p>19 At the same time, it suggests a possible origin for the modality-speciﬁcity and locality of the bias effects, as different sets of processes may be exploited for different timing purposes. [sent-30, score-0.837]
</p><p>20 Likewise, sensory information too varies over a large range of temporal scales [13]. [sent-33, score-0.289]
</p><p>21 The particular stochastic processes we use here are Gaussian Processes, whose power spectra are chosen to be broad and roughly similar to those seen in natural stimuli. [sent-34, score-0.559]
</p><p>22 2  The framework  To illustrate how random processes contain timing information, consider a random walk starting at the origin, and suppose that we see a snapshot of the random walk at another, unknown, point in time. [sent-35, score-0.95]
</p><p>23 If the walk were to end up very far from the origin, and if some statistics of the random walk were known, we would expect that the time difference between the two observations, ∆t, must be reasonably long in comparison to the diffusion time of the process. [sent-36, score-0.291]
</p><p>24 To formalize these ideas, we model the random processes as a family of independent stationary Gaussian Processes (GPs, [14]). [sent-42, score-0.311]
</p><p>25 Although this is not a necessity, we let each process evolve independently according to the same stochastic dynamics; thus the process values differ only due to the random effects. [sent-48, score-0.176]
</p><p>26 Mimicking the temporal statistics of natural scenes [15], we choose the dynamics to simultaneously contain multiple time scales — speciﬁcally, the power spectrum approximately follows a 1/f 2 power law, were f = frequency = 1/(time scale). [sent-49, score-0.888]
</p><p>27 Some instances of such processes are shown in Figure 1. [sent-50, score-0.311]
</p><p>28 Stationary Gaussian processes are fully described by the covariance function K(∆t): yi (t)yi (t + ∆t) = K(∆t) so that the probability of observing a sequence of values [yi (t1 ), yi (t2 ), . [sent-51, score-0.802]
</p><p>29 , yi (tn )] is Gaussian distributed, with zero mean and covariance matrix Σn,n′ = K(tn′ − tn ). [sent-54, score-0.369]
</p><p>30 2  y  log power  0  −5  −10 −4  time  −2 0 log frequency  2  4  Figure 1: Left: Two examples of the GPs used for inference of ∆t. [sent-55, score-0.222]
</p><p>31 This is approximately a 1/f 2 spectrum, similar to the temporal power spectrum of visual scenes. [sent-57, score-0.491]
</p><p>32 To 2 mimic a 1/f 2 spectrum, we choose the power of each component to be constant: αq = 1/Q. [sent-60, score-0.164]
</p><p>33 Figure 2 1 shows that this choice does indeed quite accurately reproduce a 1/f power spectrum. [sent-61, score-0.164]
</p><p>34 To illustrate how elapsed time is implicitly encoded in such stochastic processes, we infer the duration of an interval [t, t + ∆t] from two instantaneous observations of the processes, namely {yi (t)} and {yi (t+∆t)}. [sent-62, score-0.504]
</p><p>35 For convenience, yi is used to denote the vector [yi (t), yi (t+∆t)]. [sent-63, score-0.438]
</p><p>36 As we will see below (see Figure 2), this distribution tends to be centred on the true value of ∆t, showing that such random processes may indeed be exploited to obtain timing information. [sent-65, score-0.784]
</p><p>37 In the following section, we explore the statistical properties of timing estimates based on Φ, and show that they correspond to several experimental ﬁndings. [sent-66, score-0.534]
</p><p>38 Right: The Weber law of timing, σ ∝ ∆t, approximately holds true for this model. [sent-72, score-0.213]
</p><p>39 1  Scaling laws and behaviour Empirical demonstration of Weber’s law  Many behavioral studies have shown that the standard deviation of interval estimates is proportional to the interval being judged, σ ∝ ∆t, across a wide range of timescales and tasks (e. [sent-77, score-0.58]
</p><p>40 To compare the behaviour of the model to experimental data, we must choose a mapping from the function Φ to a single scalar value, which will model the observer’s report. [sent-81, score-0.16]
</p><p>41 To compare the statistics of this estimator to the experimental observation, we took samples {yi (t)} and {yi (t + ∆t)} from 50 GPs with identical 1/f 2 -like statistics containing time scales from 1 to 40 time units. [sent-83, score-0.386]
</p><p>42 Thus, time estimation is possible using the stochastic process framework, and the Weber law of timing holds fairly accurately. [sent-89, score-0.842]
</p><p>43 2  Fisher Information and Weber’s law  A number of questions about this Weber-like result naturally arise: Does it still hold if one changes the power spectrum of the processes? [sent-91, score-0.512]
</p><p>44 We 2 increased the noise scale σy , and found that the Weber law was still approximately satisﬁed. [sent-93, score-0.293]
</p><p>45 When changing the power spectrum of the processes from a 1/f 2 -type spectrum to a 1/f 3 -type spectrum 2 2 (by letting αi ∝ li instead of αi ∝ 1), the Weber law was still approximately satisﬁed (Figure 3). [sent-94, score-1.284]
</p><p>46 To ﬁnd reasons for this behaviour, it would useful to have an analytical expression for the relationship between the variability of the estimated duration and the true duration. [sent-96, score-0.204]
</p><p>47 This is a lower bound on the asymptotic variance of an unbiased Maximum Likelihood estimator of ∆t and is given by the inverse Fisher Information: 4  y  standard deviation  2. [sent-98, score-0.169]
</p><p>48 5 0  0  time  5  ∆t  10  15  2 Figure 3: Left: Two examples of GPs with a different power spectrum (αi ∝ li , for li ∝ i, which 3 approximates a 1/f power spectrum, resulting in much smoother dynamics). [sent-101, score-0.743]
</p><p>49 The Weber law still approximately holds, even though the dynamics is different from the initial case. [sent-106, score-0.253]
</p><p>50 (The power spectrum 2 is then shaped as exp(−f 2 li /2). [sent-115, score-0.426]
</p><p>51 ) The likelihood of observing the processes at two instances is now  P ({yi (t)}, {yi (t + ∆t)}|∆t)  ∝  i  1 T |Ci |−1/2 exp − yi C−1 yi i 2  (2)  This model shows very similar behaviour to the original model, but is somewhat less natural. [sent-116, score-0.918]
</p><p>52 In particular, we tested both linear spacing of time scales (li ∝ i) and quadratic spacing (li ∝ i2 ), and we tested a constant power distribution 5  power ~ time scale lengthscales spaced linearly  power ~ time scale lengthscales spaced quadratically l=0. [sent-118, score-1.299]
</p><p>53 −Rao bound  0  10  ∆t  20  30  0  power = constant lengthscales spaced linearly  20  30  l=0. [sent-129, score-0.365]
</p><p>54 −Rao bound  F  F  I and (I )−1/2  −1/2  ∆t  power = constant lengthscales spaced quadratically  l=6 l=11,. [sent-136, score-0.399]
</p><p>55 1, and the time scales are 2 either li = i, i = 1, 2, . [sent-143, score-0.25]
</p><p>56 The power of each 2 2 process is either αi = 1 (constant) or αi = li . [sent-150, score-0.305]
</p><p>57 The graphs show that each time scale contributes to the estimation of a wide range of ∆t, and that the Cramer-Rao bounds are all fairly linear, leading to a robust Weber-like behaviour of the estimator of elapsed time. [sent-151, score-0.544]
</p><p>58 2 (αi = 1) and a power distribution where slower processes have more power (αi ∝ li ). [sent-152, score-0.734]
</p><p>59 Next, we can evaluate the contribution of each time scale to the accuracy of estimates of ∆t, by inspecting the Fisher Information IF,i of a given process yi . [sent-154, score-0.431]
</p><p>60 This lies at the heart of the robust Weber-like behaviour: the details of the distribution of time scales do not matter much, because each time scale contributes to the estimation of a wide range of ∆t. [sent-156, score-0.311]
</p><p>61 For similar reasons, the distribution of power does not drastically affect the Cramer-Rao bound. [sent-157, score-0.164]
</p><p>62 From the graphs of IF,i , it is evident that the Weber law arises from an accumulation of high values of Fisher Information at low values of ∆t. [sent-158, score-0.257]
</p><p>63 Very small values of ∆t may be an exception, if the instantaneous noise dominates the subtle changes that the processes undergo during very short periods; for these ∆t, the standard deviation may rise. [sent-159, score-0.475]
</p><p>64 However, it may be assumed that the shortest times that neural systems can evaluate are no shorter than the scale of the fastest process within the system, making these small ∆t’s irrelevant. [sent-161, score-0.164]
</p><p>65 3  Dependence of timing variability on the number of processes  Increasing the number of processes, say Nprocesses , will add more terms to the likelihood and make the estimated ∆t more accurate. [sent-163, score-0.826]
</p><p>66 The Fisher Information (equation 1) scales with Nprocesses , which suggests that the standard deviation of ∆tMAP is proportional to 1/ Nprocesses ; this was conﬁrmed empirically (data not shown). [sent-164, score-0.144]
</p><p>67 6  Psychologically and neurally, increasing the number of processes would correspond to adding more perceptual processes, or expanding the size of the network that is being monitored for timing estimation. [sent-165, score-0.836]
</p><p>68 Although experimental data on this issue is sparse, in [9], it is shown that unimanual rhythm tapping results in a higher variability of tapping times than bimanual rhythm tapping, and that tapping with two hands and a foot results in even lower variability. [sent-166, score-0.436]
</p><p>69 This correlates well with the theoretical scaling behaviour of the estimator ∆tMAP . [sent-167, score-0.266]
</p><p>70 Note that a similar scaling law is obtained from the Multiple Timer Model [16]. [sent-168, score-0.232]
</p><p>71 √ Experimentally, a slower decrease in variability than a 1/ N law was observed. [sent-170, score-0.223]
</p><p>72 This can be accounted for by assuming that the processes governing the right and left hands are dependent, so that the number of effectively independent processes grows more slowly than the number of effectors. [sent-171, score-0.656]
</p><p>73 4  Conclusion  We have shown that timing information is present in random processes, and can be extracted probabilistically if certain statistics of the processes are known. [sent-172, score-0.827]
</p><p>74 A neural implementation of such a framework of time estimation could use both internally generated population activity as well as external stimuli to drive its processes. [sent-173, score-0.268]
</p><p>75 The timing estimators considered were based on the full probability distribution of the process values at times t and t′ , but simpler estimators could also be constructed. [sent-174, score-0.669]
</p><p>76 There are two reasons for considering simpler estimators: First, simpler estimators might be more easily implemented in neural systems. [sent-175, score-0.283]
</p><p>77 Second, to calculate Φ(∆t), one needs all of {yi (t), yi (t′ )}, so that (at least) {yi (t)} has to be stored in memory. [sent-176, score-0.219]
</p><p>78 One way to construct a simpler estimator might be to select a particular class (say, a linear function of {yi }) and optimize over its parameters. [sent-177, score-0.162]
</p><p>79 Brownian motion, being translationally invariant, would require only {yi (t′ ) − yi (t)} instead of {yi (t), yi (t′ )}; we have not considered such processes because they are unbounded and therefore hard to associate with sensory or neural processes). [sent-181, score-0.923]
</p><p>80 We have not addressed how a memory mechanism might be combined with the stochastic process framework; this will be explored in the future. [sent-182, score-0.21]
</p><p>81 The intention of this paper is not to offer a complete theory of neural and psychological timing, but to examine the statistical properties of a hitherto neglected substrate for timing — stochastic processes that take place in the brain or in the sensory world. [sent-183, score-1.261]
</p><p>82 It was demonstrated that estimators based on such processes replicate several important behaviors of humans and animals. [sent-184, score-0.369]
</p><p>83 Full models might be based on the same substrate, thereby naturally incorporating the same behaviors, but contain more completely speciﬁed relations to external input, memory mechanisms, adaptive mechanisms, neural implementation, and importantly, (supervised) learning of the estimator. [sent-185, score-0.235]
</p><p>84 The neural and sensory processes that we assume to form the basis of time estimation are, of course, not fully random. [sent-186, score-0.543]
</p><p>85 But when the deterministic structure behind a process is unknown, they can still be treated as stochastic under certain statistical rules, and thus lead to a valid timing estimator. [sent-187, score-0.637]
</p><p>86 Would the GP likelihood still apply to real neural processes or would the correct likelihood be completely different? [sent-188, score-0.382]
</p><p>87 stochastic processes tend to Gaussian Processes — so that, when e. [sent-192, score-0.395]
</p><p>88 monitoring average neuronal activity, the correct estimator may well be based on a GP likelihood. [sent-194, score-0.17]
</p><p>89 there is evidence for a spatial 7  code for temporal frequency in V2 [17]), so that neural and stimulus ﬂuctuations cannot always be treated on the same footing. [sent-199, score-0.209]
</p><p>90 The framework presented here has some similarities with the very interesting and more explicitly physiological model proposed by Buonomano and colleagues [5, 18], in which time is implicitly encoded in deterministic2 neural networks through slow neuronal time constants. [sent-201, score-0.27]
</p><p>91 However, temporal information in the network model is lost when there are stimulus-independent ﬂuctuations in the network activity, and the network can only be used as a reliable timer when it starts from a ﬁxed resting state, and if the stimulus is identical on every trial. [sent-202, score-0.384]
</p><p>92 The difference in our scheme is that here timing estimates are based on statistics, rather than deterministic structure, so that it is fundamentally robust to noise, internal ﬂuctuations, and stimulus changes. [sent-203, score-0.665]
</p><p>93 The stochastic process framework is, however, more abstract and farther removed from physiology, and a neural implementation may well share some features of the network model of timing. [sent-204, score-0.253]
</p><p>94 Scalar expectancy theory and Weber’s law in animal timing. [sent-207, score-0.233]
</p><p>95 Judging the duration of time intervals: A process of remembering segments of experience. [sent-222, score-0.183]
</p><p>96 Neural mechanisms for timing visual events are spatially selective in real-world coordinates. [sent-235, score-0.585]
</p><p>97 Fractal characted of the neural spike train in the visual system of the cat. [sent-238, score-0.153]
</p><p>98 Spatial and temporal frequency selectivity of neurones in visual cortical areas v1 and v2 of the macaque monkey. [sent-256, score-0.161]
</p><p>99 Timing in the absence of clocks: encoding time in neural network states. [sent-259, score-0.181]
</p><p>100 2 While this model and some other previous models might also contain neuronal noise, it is the deterministic (and known) element of their behaviour which encodes time. [sent-261, score-0.32]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('timing', 0.473), ('processes', 0.311), ('yi', 0.219), ('weber', 0.203), ('law', 0.181), ('elapsed', 0.173), ('spectrum', 0.167), ('power', 0.164), ('fisher', 0.149), ('tmap', 0.129), ('behaviour', 0.128), ('sensory', 0.103), ('tn', 0.097), ('scales', 0.097), ('uctuations', 0.095), ('li', 0.095), ('tapping', 0.09), ('lengthscales', 0.09), ('timer', 0.09), ('temporal', 0.089), ('activity', 0.089), ('estimator', 0.087), ('gps', 0.086), ('stochastic', 0.084), ('neuronal', 0.083), ('duration', 0.079), ('buonomano', 0.078), ('nprocesses', 0.078), ('spaced', 0.076), ('mechanisms', 0.073), ('substrate', 0.072), ('neural', 0.071), ('psychological', 0.07), ('walk', 0.066), ('rao', 0.063), ('judgements', 0.062), ('estimates', 0.061), ('interval', 0.059), ('estimators', 0.058), ('time', 0.058), ('origin', 0.053), ('covariance', 0.053), ('network', 0.052), ('ahrens', 0.052), ('expectancy', 0.052), ('ivry', 0.052), ('evolving', 0.052), ('lj', 0.052), ('contributes', 0.051), ('instantaneous', 0.051), ('scaling', 0.051), ('external', 0.05), ('neurosci', 0.05), ('ci', 0.049), ('stimulus', 0.049), ('internal', 0.048), ('scale', 0.047), ('deviation', 0.047), ('process', 0.046), ('rhythm', 0.045), ('accumulation', 0.045), ('timescales', 0.045), ('reasons', 0.045), ('spike', 0.043), ('gp', 0.043), ('statistics', 0.043), ('variability', 0.042), ('lq', 0.041), ('oscillations', 0.041), ('psychol', 0.041), ('passage', 0.041), ('might', 0.041), ('exp', 0.041), ('brain', 0.041), ('dynamics', 0.04), ('visual', 0.039), ('memory', 0.039), ('spacing', 0.038), ('snapshots', 0.038), ('maneesh', 0.038), ('analytical', 0.038), ('neglected', 0.036), ('proposals', 0.036), ('blue', 0.035), ('bound', 0.035), ('hands', 0.034), ('quadratically', 0.034), ('temporally', 0.034), ('motion', 0.034), ('intervals', 0.034), ('deterministic', 0.034), ('simpler', 0.034), ('contain', 0.034), ('subtle', 0.033), ('macaque', 0.033), ('noise', 0.033), ('approximately', 0.032), ('scalar', 0.032), ('theories', 0.031), ('evident', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="103-tfidf-1" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>Author: Misha Ahrens, Maneesh Sahani</p><p>Abstract: Many perceptual processes and neural computations, such as speech recognition, motor control and learning, depend on the ability to measure and mark the passage of time. However, the processes that make such temporal judgements possible are unknown. A number of different hypothetical mechanisms have been advanced, all of which depend on the known, temporally predictable evolution of a neural or psychological state, possibly through oscillations or the gradual decay of a memory trace. Alternatively, judgements of elapsed time might be based on observations of temporally structured, but stochastic processes. Such processes need not be speciﬁc to the sense of time; typical neural and sensory processes contain at least some statistical structure across a range of time scales. Here, we investigate the statistical properties of an estimator of elapsed time which is based on a simple family of stochastic process. 1</p><p>2 0.16334063 <a title="103-tfidf-2" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>Author: Omer Bobrowski, Ron Meir, Shy Shoham, Yonina Eldar</p><p>Abstract: It is becoming increasingly evident that organisms acting in uncertain dynamical environments often employ exact or approximate Bayesian statistical calculations in order to continuously estimate the environmental state, integrate information from multiple sensory modalities, form predictions and choose actions. What is less clear is how these putative computations are implemented by cortical neural networks. An additional level of complexity is introduced because these networks observe the world through spike trains received from primary sensory afferents, rather than directly. A recent line of research has described mechanisms by which such computations can be implemented using a network of neurons whose activity directly represents a probability distribution across the possible “world states”. Much of this work, however, uses various approximations, which severely restrict the domain of applicability of these implementations. Here we make use of rigorous mathematical results from the theory of continuous time point process ﬁltering, and show how optimal real-time state estimation and prediction may be implemented in a general setting using linear neural networks. We demonstrate the applicability of the approach with several examples, and relate the required network properties to the statistical nature of the environment, thereby quantifying the compatibility of a given network with its environment. 1</p><p>3 0.16073851 <a title="103-tfidf-3" href="./nips-2007-Modelling_motion_primitives_and_their_timing_in_biologically_executed_movements.html">133 nips-2007-Modelling motion primitives and their timing in biologically executed movements</a></p>
<p>Author: Ben Williams, Marc Toussaint, Amos J. Storkey</p><p>Abstract: Biological movement is built up of sub-blocks or motion primitives. Such primitives provide a compact representation of movement which is also desirable in robotic control applications. We analyse handwriting data to gain a better understanding of primitives and their timings in biological movements. Inference of the shape and the timing of primitives can be done using a factorial HMM based model, allowing the handwriting to be represented in primitive timing space. This representation provides a distribution of spikes corresponding to the primitive activations, which can also be modelled using HMM architectures. We show how the coupling of the low level primitive model, and the higher level timing model during inference can produce good reconstructions of handwriting, with shared primitives for all characters modelled. This coupled model also captures the variance proﬁle of the dataset which is accounted for by spike timing jitter. The timing code provides a compact representation of the movement while generating a movement without an explicit timing model produces a scribbling style of output. 1</p><p>4 0.11491485 <a title="103-tfidf-4" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>Author: Maneesh Sahani, Byron M. Yu, John P. Cunningham, Krishna V. Shenoy</p><p>Abstract: Neural spike trains present challenges to analytical efforts due to their noisy, spiking nature. Many studies of neuroscientiﬁc and neural prosthetic importance rely on a smoothed, denoised estimate of the spike train’s underlying ﬁring rate. Current techniques to ﬁnd time-varying ﬁring rates require ad hoc choices of parameters, offer no conﬁdence intervals on their estimates, and can obscure potentially important single trial variability. We present a new method, based on a Gaussian Process prior, for inferring probabilistically optimal estimates of ﬁring rate functions underlying single or multiple neural spike trains. We test the performance of the method on simulated data and experimentally gathered neural spike trains, and we demonstrate improvements over conventional estimators. 1</p><p>5 0.11166693 <a title="103-tfidf-5" href="./nips-2007-Continuous_Time_Particle_Filtering_for_fMRI.html">59 nips-2007-Continuous Time Particle Filtering for fMRI</a></p>
<p>Author: Lawrence Murray, Amos J. Storkey</p><p>Abstract: We construct a biologically motivated stochastic differential model of the neural and hemodynamic activity underlying the observed Blood Oxygen Level Dependent (BOLD) signal in Functional Magnetic Resonance Imaging (fMRI). The model poses a difﬁcult parameter estimation problem, both theoretically due to the nonlinearity and divergence of the differential system, and computationally due to its time and space complexity. We adapt a particle ﬁlter and smoother to the task, and discuss some of the practical approaches used to tackle the difﬁculties, including use of sparse matrices and parallelisation. Results demonstrate the tractability of the approach in its application to an effective connectivity study. 1</p><p>6 0.098857462 <a title="103-tfidf-6" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>7 0.09359847 <a title="103-tfidf-7" href="./nips-2007-Second_Order_Bilinear_Discriminant_Analysis_for_single_trial_EEG_analysis.html">173 nips-2007-Second Order Bilinear Discriminant Analysis for single trial EEG analysis</a></p>
<p>8 0.092051476 <a title="103-tfidf-8" href="./nips-2007-A_Unified_Near-Optimal_Estimator_For_Dimension_Reduction_in_%24l_%5Calpha%24_%28%240%3C%5Calpha%5Cleq_2%24%29_Using_Stable_Random_Projections.html">13 nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</a></p>
<p>9 0.090051576 <a title="103-tfidf-9" href="./nips-2007-Testing_for_Homogeneity_with_Kernel_Fisher_Discriminant_Analysis.html">192 nips-2007-Testing for Homogeneity with Kernel Fisher Discriminant Analysis</a></p>
<p>10 0.088802449 <a title="103-tfidf-10" href="./nips-2007-Bayesian_Co-Training.html">32 nips-2007-Bayesian Co-Training</a></p>
<p>11 0.087524325 <a title="103-tfidf-11" href="./nips-2007-A_Bayesian_Model_of_Conditioned_Perception.html">3 nips-2007-A Bayesian Model of Conditioned Perception</a></p>
<p>12 0.082024358 <a title="103-tfidf-12" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>13 0.081192903 <a title="103-tfidf-13" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>14 0.078087844 <a title="103-tfidf-14" href="./nips-2007-Robust_Regression_with_Twinned_Gaussian_Processes.html">170 nips-2007-Robust Regression with Twinned Gaussian Processes</a></p>
<p>15 0.07495138 <a title="103-tfidf-15" href="./nips-2007-Statistical_Analysis_of_Semi-Supervised_Regression.html">186 nips-2007-Statistical Analysis of Semi-Supervised Regression</a></p>
<p>16 0.0728423 <a title="103-tfidf-16" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>17 0.072179511 <a title="103-tfidf-17" href="./nips-2007-Managing_Power_Consumption_and_Performance_of_Computing_Systems_Using_Reinforcement_Learning.html">124 nips-2007-Managing Power Consumption and Performance of Computing Systems Using Reinforcement Learning</a></p>
<p>18 0.071569957 <a title="103-tfidf-18" href="./nips-2007-Hidden_Common_Cause_Relations_in_Relational_Learning.html">97 nips-2007-Hidden Common Cause Relations in Relational Learning</a></p>
<p>19 0.071216337 <a title="103-tfidf-19" href="./nips-2007-Measuring_Neural_Synchrony_by_Message_Passing.html">127 nips-2007-Measuring Neural Synchrony by Message Passing</a></p>
<p>20 0.068810321 <a title="103-tfidf-20" href="./nips-2007-Multi-task_Gaussian_Process_Prediction.html">135 nips-2007-Multi-task Gaussian Process Prediction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.229), (1, 0.079), (2, 0.107), (3, 0.051), (4, -0.025), (5, 0.059), (6, -0.028), (7, -0.05), (8, -0.097), (9, -0.095), (10, -0.074), (11, -0.023), (12, -0.033), (13, 0.03), (14, 0.112), (15, -0.007), (16, 0.007), (17, 0.023), (18, 0.104), (19, 0.059), (20, 0.047), (21, 0.0), (22, 0.102), (23, -0.089), (24, 0.015), (25, -0.112), (26, 0.022), (27, -0.017), (28, 0.055), (29, -0.102), (30, 0.199), (31, 0.031), (32, 0.11), (33, 0.07), (34, -0.074), (35, -0.041), (36, -0.058), (37, -0.173), (38, 0.195), (39, -0.109), (40, 0.128), (41, 0.006), (42, -0.084), (43, 0.019), (44, -0.046), (45, -0.052), (46, -0.007), (47, 0.015), (48, 0.063), (49, -0.088)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97091013 <a title="103-lsi-1" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>Author: Misha Ahrens, Maneesh Sahani</p><p>Abstract: Many perceptual processes and neural computations, such as speech recognition, motor control and learning, depend on the ability to measure and mark the passage of time. However, the processes that make such temporal judgements possible are unknown. A number of different hypothetical mechanisms have been advanced, all of which depend on the known, temporally predictable evolution of a neural or psychological state, possibly through oscillations or the gradual decay of a memory trace. Alternatively, judgements of elapsed time might be based on observations of temporally structured, but stochastic processes. Such processes need not be speciﬁc to the sense of time; typical neural and sensory processes contain at least some statistical structure across a range of time scales. Here, we investigate the statistical properties of an estimator of elapsed time which is based on a simple family of stochastic process. 1</p><p>2 0.65973431 <a title="103-lsi-2" href="./nips-2007-Modelling_motion_primitives_and_their_timing_in_biologically_executed_movements.html">133 nips-2007-Modelling motion primitives and their timing in biologically executed movements</a></p>
<p>Author: Ben Williams, Marc Toussaint, Amos J. Storkey</p><p>Abstract: Biological movement is built up of sub-blocks or motion primitives. Such primitives provide a compact representation of movement which is also desirable in robotic control applications. We analyse handwriting data to gain a better understanding of primitives and their timings in biological movements. Inference of the shape and the timing of primitives can be done using a factorial HMM based model, allowing the handwriting to be represented in primitive timing space. This representation provides a distribution of spikes corresponding to the primitive activations, which can also be modelled using HMM architectures. We show how the coupling of the low level primitive model, and the higher level timing model during inference can produce good reconstructions of handwriting, with shared primitives for all characters modelled. This coupled model also captures the variance proﬁle of the dataset which is accounted for by spike timing jitter. The timing code provides a compact representation of the movement while generating a movement without an explicit timing model produces a scribbling style of output. 1</p><p>3 0.60034484 <a title="103-lsi-3" href="./nips-2007-Continuous_Time_Particle_Filtering_for_fMRI.html">59 nips-2007-Continuous Time Particle Filtering for fMRI</a></p>
<p>Author: Lawrence Murray, Amos J. Storkey</p><p>Abstract: We construct a biologically motivated stochastic differential model of the neural and hemodynamic activity underlying the observed Blood Oxygen Level Dependent (BOLD) signal in Functional Magnetic Resonance Imaging (fMRI). The model poses a difﬁcult parameter estimation problem, both theoretically due to the nonlinearity and divergence of the differential system, and computationally due to its time and space complexity. We adapt a particle ﬁlter and smoother to the task, and discuss some of the practical approaches used to tackle the difﬁculties, including use of sparse matrices and parallelisation. Results demonstrate the tractability of the approach in its application to an effective connectivity study. 1</p><p>4 0.56394321 <a title="103-lsi-4" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>Author: Omer Bobrowski, Ron Meir, Shy Shoham, Yonina Eldar</p><p>Abstract: It is becoming increasingly evident that organisms acting in uncertain dynamical environments often employ exact or approximate Bayesian statistical calculations in order to continuously estimate the environmental state, integrate information from multiple sensory modalities, form predictions and choose actions. What is less clear is how these putative computations are implemented by cortical neural networks. An additional level of complexity is introduced because these networks observe the world through spike trains received from primary sensory afferents, rather than directly. A recent line of research has described mechanisms by which such computations can be implemented using a network of neurons whose activity directly represents a probability distribution across the possible “world states”. Much of this work, however, uses various approximations, which severely restrict the domain of applicability of these implementations. Here we make use of rigorous mathematical results from the theory of continuous time point process ﬁltering, and show how optimal real-time state estimation and prediction may be implemented in a general setting using linear neural networks. We demonstrate the applicability of the approach with several examples, and relate the required network properties to the statistical nature of the environment, thereby quantifying the compatibility of a given network with its environment. 1</p><p>5 0.470853 <a title="103-lsi-5" href="./nips-2007-A_Bayesian_Model_of_Conditioned_Perception.html">3 nips-2007-A Bayesian Model of Conditioned Perception</a></p>
<p>Author: Alan Stocker, Eero P. Simoncelli</p><p>Abstract: unkown-abstract</p><p>6 0.43918711 <a title="103-lsi-6" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>7 0.43864012 <a title="103-lsi-7" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<p>8 0.42092985 <a title="103-lsi-8" href="./nips-2007-Augmented_Functional_Time_Series_Representation_and_Forecasting_with_Gaussian_Processes.html">28 nips-2007-Augmented Functional Time Series Representation and Forecasting with Gaussian Processes</a></p>
<p>9 0.4146384 <a title="103-lsi-9" href="./nips-2007-Structured_Learning_with_Approximate_Inference.html">187 nips-2007-Structured Learning with Approximate Inference</a></p>
<p>10 0.4140397 <a title="103-lsi-10" href="./nips-2007-Testing_for_Homogeneity_with_Kernel_Fisher_Discriminant_Analysis.html">192 nips-2007-Testing for Homogeneity with Kernel Fisher Discriminant Analysis</a></p>
<p>11 0.39764002 <a title="103-lsi-11" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>12 0.39551085 <a title="103-lsi-12" href="./nips-2007-Fast_and_Scalable_Training_of_Semi-Supervised_CRFs_with_Application_to_Activity_Recognition.html">88 nips-2007-Fast and Scalable Training of Semi-Supervised CRFs with Application to Activity Recognition</a></p>
<p>13 0.39351133 <a title="103-lsi-13" href="./nips-2007-A_Unified_Near-Optimal_Estimator_For_Dimension_Reduction_in_%24l_%5Calpha%24_%28%240%3C%5Calpha%5Cleq_2%24%29_Using_Stable_Random_Projections.html">13 nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</a></p>
<p>14 0.38276005 <a title="103-lsi-14" href="./nips-2007-An_Analysis_of_Inference_with_the_Universum.html">24 nips-2007-An Analysis of Inference with the Universum</a></p>
<p>15 0.381385 <a title="103-lsi-15" href="./nips-2007-Managing_Power_Consumption_and_Performance_of_Computing_Systems_Using_Reinforcement_Learning.html">124 nips-2007-Managing Power Consumption and Performance of Computing Systems Using Reinforcement Learning</a></p>
<p>16 0.37328207 <a title="103-lsi-16" href="./nips-2007-Unconstrained_On-line_Handwriting_Recognition_with_Recurrent_Neural_Networks.html">210 nips-2007-Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks</a></p>
<p>17 0.37214607 <a title="103-lsi-17" href="./nips-2007-Hidden_Common_Cause_Relations_in_Relational_Learning.html">97 nips-2007-Hidden Common Cause Relations in Relational Learning</a></p>
<p>18 0.35738489 <a title="103-lsi-18" href="./nips-2007-Statistical_Analysis_of_Semi-Supervised_Regression.html">186 nips-2007-Statistical Analysis of Semi-Supervised Regression</a></p>
<p>19 0.35108984 <a title="103-lsi-19" href="./nips-2007-Second_Order_Bilinear_Discriminant_Analysis_for_single_trial_EEG_analysis.html">173 nips-2007-Second Order Bilinear Discriminant Analysis for single trial EEG analysis</a></p>
<p>20 0.34968752 <a title="103-lsi-20" href="./nips-2007-Variational_inference_for_Markov_jump_processes.html">214 nips-2007-Variational inference for Markov jump processes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.026), (13, 0.021), (16, 0.034), (18, 0.026), (19, 0.018), (21, 0.536), (31, 0.012), (34, 0.025), (35, 0.021), (47, 0.084), (83, 0.069), (87, 0.018), (90, 0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96034473 <a title="103-lda-1" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>Author: Misha Ahrens, Maneesh Sahani</p><p>Abstract: Many perceptual processes and neural computations, such as speech recognition, motor control and learning, depend on the ability to measure and mark the passage of time. However, the processes that make such temporal judgements possible are unknown. A number of different hypothetical mechanisms have been advanced, all of which depend on the known, temporally predictable evolution of a neural or psychological state, possibly through oscillations or the gradual decay of a memory trace. Alternatively, judgements of elapsed time might be based on observations of temporally structured, but stochastic processes. Such processes need not be speciﬁc to the sense of time; typical neural and sensory processes contain at least some statistical structure across a range of time scales. Here, we investigate the statistical properties of an estimator of elapsed time which is based on a simple family of stochastic process. 1</p><p>2 0.9468658 <a title="103-lda-2" href="./nips-2007-Bayesian_Co-Training.html">32 nips-2007-Bayesian Co-Training</a></p>
<p>Author: Shipeng Yu, Balaji Krishnapuram, Harald Steck, R. B. Rao, Rómer Rosales</p><p>Abstract: We propose a Bayesian undirected graphical model for co-training, or more generally for semi-supervised multi-view learning. This makes explicit the previously unstated assumptions of a large class of co-training type algorithms, and also clariﬁes the circumstances under which these assumptions fail. Building upon new insights from this model, we propose an improved method for co-training, which is a novel co-training kernel for Gaussian process classiﬁers. The resulting approach is convex and avoids local-maxima problems, unlike some previous multi-view learning methods. Furthermore, it can automatically estimate how much each view should be trusted, and thus accommodate noisy or unreliable views. Experiments on toy data and real world data sets illustrate the beneﬁts of this approach. 1</p><p>3 0.93673992 <a title="103-lda-3" href="./nips-2007-The_Distribution_Family_of_Similarity_Distances.html">193 nips-2007-The Distribution Family of Similarity Distances</a></p>
<p>Author: Gertjan Burghouts, Arnold Smeulders, Jan-mark Geusebroek</p><p>Abstract: Assessing similarity between features is a key step in object recognition and scene categorization tasks. We argue that knowledge on the distribution of distances generated by similarity functions is crucial in deciding whether features are similar or not. Intuitively one would expect that similarities between features could arise from any distribution. In this paper, we will derive the contrary, and report the theoretical result that Lp -norms –a class of commonly applied distance metrics– from one feature vector to other vectors are Weibull-distributed if the feature values are correlated and non-identically distributed. Besides these assumptions being realistic for images, we experimentally show them to hold for various popular feature extraction algorithms, for a diverse range of images. This fundamental insight opens new directions in the assessment of feature similarity, with projected improvements in object and scene recognition algorithms. 1</p><p>4 0.90261489 <a title="103-lda-4" href="./nips-2007-Active_Preference_Learning_with_Discrete_Choice_Data.html">19 nips-2007-Active Preference Learning with Discrete Choice Data</a></p>
<p>Author: Brochu Eric, Nando D. Freitas, Abhijeet Ghosh</p><p>Abstract: We propose an active learning algorithm that learns a continuous valuation model from discrete preferences. The algorithm automatically decides what items are best presented to an individual in order to ﬁnd the item that they value highly in as few trials as possible, and exploits quirks of human psychology to minimize time and cognitive burden. To do this, our algorithm maximizes the expected improvement at each query without accurately modelling the entire valuation surface, which would be needlessly expensive. The problem is particularly difﬁcult because the space of choices is inﬁnite. We demonstrate the effectiveness of the new algorithm compared to related active learning methods. We also embed the algorithm within a decision making tool for assisting digital artists in rendering materials. The tool ﬁnds the best parameters while minimizing the number of queries. 1</p><p>5 0.61505312 <a title="103-lda-5" href="./nips-2007-Scene_Segmentation_with_CRFs_Learned_from_Partially_Labeled_Images.html">172 nips-2007-Scene Segmentation with CRFs Learned from Partially Labeled Images</a></p>
<p>Author: Bill Triggs, Jakob J. Verbeek</p><p>Abstract: Conditional Random Fields (CRFs) are an effective tool for a variety of different data segmentation and labeling tasks including visual scene interpretation, which seeks to partition images into their constituent semantic-level regions and assign appropriate class labels to each region. For accurate labeling it is important to capture the global context of the image as well as local information. We introduce a CRF based scene labeling model that incorporates both local features and features aggregated over the whole image or large sections of it. Secondly, traditional CRF learning requires fully labeled datasets which can be costly and troublesome to produce. We introduce a method for learning CRFs from datasets with many unlabeled nodes by marginalizing out the unknown labels so that the log-likelihood of the known ones can be maximized by gradient ascent. Loopy Belief Propagation is used to approximate the marginals needed for the gradient and log-likelihood calculations and the Bethe free-energy approximation to the log-likelihood is monitored to control the step size. Our experimental results show that effective models can be learned from fragmentary labelings and that incorporating top-down aggregate features signiﬁcantly improves the segmentations. The resulting segmentations are compared to the state-of-the-art on three different image datasets. 1</p><p>6 0.60150331 <a title="103-lda-6" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>7 0.59625107 <a title="103-lda-7" href="./nips-2007-Discriminative_Batch_Mode_Active_Learning.html">69 nips-2007-Discriminative Batch Mode Active Learning</a></p>
<p>8 0.58476704 <a title="103-lda-8" href="./nips-2007-Configuration_Estimates_Improve_Pedestrian_Finding.html">56 nips-2007-Configuration Estimates Improve Pedestrian Finding</a></p>
<p>9 0.58082342 <a title="103-lda-9" href="./nips-2007-Fast_and_Scalable_Training_of_Semi-Supervised_CRFs_with_Application_to_Activity_Recognition.html">88 nips-2007-Fast and Scalable Training of Semi-Supervised CRFs with Application to Activity Recognition</a></p>
<p>10 0.58045125 <a title="103-lda-10" href="./nips-2007-Augmented_Functional_Time_Series_Representation_and_Forecasting_with_Gaussian_Processes.html">28 nips-2007-Augmented Functional Time Series Representation and Forecasting with Gaussian Processes</a></p>
<p>11 0.56921756 <a title="103-lda-11" href="./nips-2007-Using_Deep_Belief_Nets_to_Learn_Covariance_Kernels_for_Gaussian_Processes.html">212 nips-2007-Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes</a></p>
<p>12 0.56514049 <a title="103-lda-12" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>13 0.5634281 <a title="103-lda-13" href="./nips-2007-Multiple-Instance_Active_Learning.html">136 nips-2007-Multiple-Instance Active Learning</a></p>
<p>14 0.55908769 <a title="103-lda-14" href="./nips-2007-Hidden_Common_Cause_Relations_in_Relational_Learning.html">97 nips-2007-Hidden Common Cause Relations in Relational Learning</a></p>
<p>15 0.55567122 <a title="103-lda-15" href="./nips-2007-Hippocampal_Contributions_to_Control%3A_The_Third_Way.html">100 nips-2007-Hippocampal Contributions to Control: The Third Way</a></p>
<p>16 0.5555163 <a title="103-lda-16" href="./nips-2007-Semi-Supervised_Multitask_Learning.html">175 nips-2007-Semi-Supervised Multitask Learning</a></p>
<p>17 0.55511016 <a title="103-lda-17" href="./nips-2007-Privacy-Preserving_Belief_Propagation_and_Sampling.html">157 nips-2007-Privacy-Preserving Belief Propagation and Sampling</a></p>
<p>18 0.55480444 <a title="103-lda-18" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>19 0.55240083 <a title="103-lda-19" href="./nips-2007-Evaluating_Search_Engines_by_Modeling_the_Relationship_Between_Relevance_and_Clicks.html">83 nips-2007-Evaluating Search Engines by Modeling the Relationship Between Relevance and Clicks</a></p>
<p>20 0.54792845 <a title="103-lda-20" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
