<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>194 nips-2007-The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-194" href="#">nips2007-194</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>194 nips-2007-The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information</h1>
<br/><p>Source: <a title="nips-2007-194-pdf" href="http://papers.nips.cc/paper/3178-the-epoch-greedy-algorithm-for-multi-armed-bandits-with-side-information.pdf">pdf</a></p><p>Author: John Langford, Tong Zhang</p><p>Abstract: We present Epoch-Greedy, an algorithm for contextual multi-armed bandits (also known as bandits with side information). Epoch-Greedy has the following properties: 1. No knowledge of a time horizon T is necessary. 2. The regret incurred by Epoch-Greedy is controlled by a sample complexity bound for a hypothesis class. 3. The regret scales as O(T 2/3 S 1/3 ) or better (sometimes, much better). Here S is the complexity term in a sample complexity bound for standard supervised learning. 1</p><p>Reference: <a title="nips-2007-194-reference" href="../nips2007_reference/nips-2007-The_Epoch-Greedy_Algorithm_for_Multi-armed_Bandits_with_Side_Information_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We present Epoch-Greedy, an algorithm for contextual multi-armed bandits (also known as bandits with side information). [sent-5, score-1.292]
</p><p>2 The regret incurred by Epoch-Greedy is controlled by a sample complexity bound for a hypothesis class. [sent-9, score-0.731]
</p><p>3 The regret scales as O(T 2/3 S 1/3 ) or better (sometimes, much better). [sent-11, score-0.456]
</p><p>4 1  Introduction  The standard k-armed bandits problem has been well-studied in the literature (Lai & Robbins, 1985; Auer et al. [sent-13, score-0.586]
</p><p>5 , rk ∈ [0, 1]; the player chooses an arm i ∈ {1, k} without knowledge of the world’s chosen rewards, and then observes the reward ri . [sent-19, score-0.462]
</p><p>6 The contextual bandits setting considered in this paper is the same except for a modiﬁcation of the ﬁrst step, in which the player also observes context information x which can be used to determine which arm to pull. [sent-20, score-1.059]
</p><p>7 The contextual bandits problem has many applications and is often more suitable than the standard bandits problem, because settings with no context information are rare in practice. [sent-21, score-1.303]
</p><p>8 Although one may potentially put multiple ads on each web-page, we focus on the problem that only one ad is placed on each page (which is like pulling an arm given context information). [sent-27, score-0.454]
</p><p>9 The problem of bandits with context has been analyzed previously (Pandey et al. [sent-30, score-0.627]
</p><p>10 Epoch-Greedy has a worse regret bound in T (O(T 2/3 ) rather than O(T 1/2 )) and is only 1  analyzed under an IID assumption. [sent-39, score-0.552]
</p><p>11 In the situation where the number of predictors is inﬁnite but with ﬁnite VC-Dimension d, Exp4 has a vacuous regret bound while Epoch-Greedy has a regret bound no worse than O(T 2/3 (ln m)1/3 ). [sent-41, score-1.119]
</p><p>12 Sometimes we can achieve much better dependence on T , depending on the structure of the hypothesis space. [sent-42, score-0.181]
</p><p>13 For example, we will show that it is possible to achieve O(ln T ) regret bound using Epoch-Greedy, while this is not possible with Exp4 or any simple modiﬁcation of it. [sent-43, score-0.554]
</p><p>14 Exp4 on the other hand requires computation proportional to the explicit count of hypotheses in a hypothesis space. [sent-46, score-0.216]
</p><p>15 Epoch-Greedy takes advantage of this idea, but it also has analysis which states that it trades off the number of exploration and exploitation steps so as to maximize the sum of rewards incurred during both exploration and exploitation. [sent-51, score-0.757]
</p><p>16 We present and analyze the Epoch-Greedy algorithm for multiarmed bandits with context. [sent-53, score-0.558]
</p><p>17 Section 3 presents the Epoch-Greedy algorithm along with a regret bound analysis which holds without knowledge of T . [sent-59, score-0.533]
</p><p>18 2  Contextual bandits  We ﬁrst formally deﬁne contextual bandit problems and algorithms to solve them. [sent-62, score-0.847]
</p><p>19 1 (Contextual bandit problem) In a contextual bandits problem, there is a distribution P over (x, r1 , . [sent-64, score-0.847]
</p><p>20 , k} is one of the k arms to be pulled, and ra ∈ [0, 1] is the reward for arm a. [sent-70, score-0.708]
</p><p>21 , rk ) is drawn from P , the context x is announced, and then for precisely one arm a chosen by the player, its reward ra is revealed. [sent-74, score-0.684]
</p><p>22 2 (Contextual bandit algorithm) A contextual bandits algorithm B determines an arm a ∈ {1, . [sent-76, score-1.107]
</p><p>23 T  Our goal is to maximize the expected total reward t=1 E(xt ,rt )∼P [ra,t ]. [sent-83, score-0.197]
</p><p>24 Each hypothesis maps side information x to an arm a. [sent-89, score-0.45]
</p><p>25 A natural goal is to choose arms to compete with the best hypothesis in H. [sent-90, score-0.251]
</p><p>26 3 (Regret) The expected reward of a hypothesis h is R(h) = E(x,r)∼D rh(x) . [sent-93, score-0.325]
</p><p>27 , (xT , rT )}, and the expected regret of B with respect to a hypothesis h be: T  ∆R(B, h, T ) = T R(h) − EZ T ∼P T  rB(x),t . [sent-98, score-0.659]
</p><p>28 t=1  2  The expected regret of B up to time T with respect to hypothesis space H is deﬁned as ∆R(B, H, T ) = sup ∆R(B, h, T ). [sent-99, score-0.681]
</p><p>29 h∈H  The main challenge of the contextual bandits problem is that when we pull an arm, rewards of other arms are not observed. [sent-100, score-0.928]
</p><p>30 In this context, methods we investigate in the paper make explicit distinctions between exploration and exploitation steps. [sent-102, score-0.438]
</p><p>31 In an exploration step, the goal is to form unbiased samples by randomly pulling all arms to improve the accuracy of learning. [sent-103, score-0.351]
</p><p>32 Because it does not focus on the best arm, this step leads to large immediate regret but can potentially reduce regret for the future exploitation steps. [sent-104, score-1.231]
</p><p>33 In an exploitation step, the learning algorithm suggests the best hypothesis learned from the samples formed in the exploration steps, and the arm given by the hypothesis is pulled: the goal is to maximize immediate reward (or minimize immediate regret). [sent-105, score-1.232]
</p><p>34 Since the samples in the exploitation steps are biased (toward the arm suggested by the learning algorithm using previous exploration samples), we do not use them to learn the hypothesis for the future steps. [sent-106, score-0.927]
</p><p>35 That is, in methods we consider, exploitation does not help us to improve learning accuracy for the future. [sent-107, score-0.261]
</p><p>36 More speciﬁcally, in an exploration step, in order to form unbiased samples, we pull an arm a ∈ {1, . [sent-108, score-0.538]
</p><p>37 Therefore the expected regret comparing to the best hypothesis in H can be as large as O(1). [sent-112, score-0.659]
</p><p>38 In an exploitation step, the expected regret can be much smaller. [sent-113, score-0.76]
</p><p>39 Therefore a central theme we examine in this paper is to balance the trade-off between exploration and exploitation, so as to achieve a small overall expected regret up to some time horizon T . [sent-114, score-0.752]
</p><p>40 Note that if we decide to pull a speciﬁc arm a with side information x, we do not observe rewards ra for a = a. [sent-115, score-0.658]
</p><p>41 In order to apply standard sample complexity analysis, we ﬁrst show that exploration samples, where a is picked uniformly at random, can create a standard learning problem without missing observations. [sent-116, score-0.215]
</p><p>42 This is simply achieved by setting fully observed rewards r such that ra (ra ) = kI(a = a)ra ,  (1)  where I(·) is the indicator function. [sent-117, score-0.285]
</p><p>43 Therefore for any hypothesis h(x), we have R(h) = E(x,r)∼P,a∼U (1,. [sent-126, score-0.16]
</p><p>44 1 implies that we can estimate reward R(h) of any hypothesis h(x) using expectation with respect to exploration samples (x, a, ra ). [sent-135, score-0.743]
</p><p>45 The right hand side can then be replaced by empirical samples as t I(h(xt ) = at )ra,t for hypotheses in a hypothesis space H. [sent-136, score-0.274]
</p><p>46 3  Exploration with the Epoch-Greedy algorithm  The problem of treating contextual bandits as standard bandits is that the information in x is lost. [sent-138, score-1.262]
</p><p>47 That is, the optimal arm to pull should be a function of the context x, but this is not captured by the 3  standard bandits setting. [sent-139, score-0.942]
</p><p>48 An alternative approach is to regard each hypothesis h as a separate artiﬁcial “arm”, and then apply a standard bandits algorithm to these artiﬁcial arms. [sent-140, score-0.718]
</p><p>49 However, this solution ignores the fact that many hypotheses can share the same arm so that choosing an arm yields information for many hypotheses. [sent-142, score-0.576]
</p><p>50 For this reason, with a simple algorithm, we can get a bound that depends on m logarithmically, instead of O(m) as would be the case for the standard bandits solution discussed above. [sent-143, score-0.635]
</p><p>51 If we are given the time horizon T in advance, and would like to optimize performance with the given T , then it is always advantageous to perform a ﬁrst phase of exploration steps, followed by a second phase of exploitation steps (until time step T ). [sent-145, score-0.618]
</p><p>52 The reason that there is no advantage to take any exploitation step before the last exploration step is: by switching the two steps, we can more accurately pick the optimal hypothesis in the exploitation step due to more samples from exploration. [sent-146, score-1.005]
</p><p>53 With ﬁxed T , assume that we have taken n steps of exploration, and obtain an average regret bound of n for each exploitation step at the point, then we can bound the regret of the exploration phase as n, and the exploitation phase as n (T − n). [sent-147, score-1.89]
</p><p>54 Using this bound, we shall switch from exploration to exploitation at the point n that minimizes the sum. [sent-149, score-0.438]
</p><p>55 Without knowing T in advance, but with the same generalization bound, we can run exploration/exploitation in epochs, where at the beginning of each epoch , we perform one step of exploration, followed by 1/ n steps of exploitation. [sent-150, score-0.179]
</p><p>56 After epoch L, the L total average regret is no more than n=1 (1 + n 1/ n ) ≤ 3L. [sent-152, score-0.576]
</p><p>57 Moreover, the epoch L∗ containing T is no more than the optimal regret bound minn [n + (T − n) n ] (with known T and optimal stopping point). [sent-153, score-0.701]
</p><p>58 n In Figure 1, s(Z1 ) is a sample-dependent (integer valued) exploitation step count. [sent-166, score-0.295]
</p><p>59 1 n n n suggests that choosing s(Z1 ) = 1/ n (Z1 ) , where n (Z1 ) is a sample dependent average generalization bound, yields performance comparable to the optimal bound with known time horizon T. [sent-168, score-0.151]
</p><p>60 1 (Epoch-Greedy Exploitation Cost) Consider a hypothesis space H consisting of hypotheses that take values in {1, 2, . [sent-170, score-0.216]
</p><p>61 t=1 n n [0, 1], and observation Z1 , we denote by s(Z1 ) a data-dependent exploitation h∈H  Given any ﬁxed n, δ ∈ step count. [sent-185, score-0.295]
</p><p>62 Then the per-epoch exploitation cost is deﬁned as: n µn (H, s) = EZ1  n ˆ n sup R(h) − R(h(Z1 )) s(Z1 ). [sent-186, score-0.283]
</p><p>63 h∈H  4  Epoch-Greedy (s(W )) /*parameter s(W ): exploitation steps*/ initialize: exploration samples W0 = {} and t1 = 1 iterate = 1, 2, . [sent-187, score-0.484]
</p><p>64 t = t , and observe xt /*do one-step exploration*/ select an arm at ∈ {1, . [sent-190, score-0.324]
</p><p>65 1 For all T, n , L such that: T ≤ L + in Figure 1 is bounded by  L =1  n , the expected regret of Epoch-Greedy  L  ∆R(Epoch-Greedy, H, T ) ≤ L +  L  µ (H, s) + T =1  P [s(Z1 ) < n ]. [sent-194, score-0.499]
</p><p>66 Then the expected regret of Epoch-Greedy in Figure 1 is bounded by ∆R(Epoch-Greedy, H, T ) ≤ 2LT . [sent-201, score-0.499]
</p><p>67 If event A occurs, then since each reward is in [0,1], up to time T , regret cannot be larger than T . [sent-211, score-0.578]
</p><p>68 Thus the total expected contribution of A to the regret ∆R(B, H, T ) is at most L  T P (A) ≤ T  P [s(Z1 ) < n ]. [sent-212, score-0.515]
</p><p>69 Therefore the expected contribution of B to the regret ∆R(B, H, T ) is at most the sum of expected regret in the ﬁrst L epochs. [sent-217, score-0.998]
</p><p>70 By deﬁnition and construction, after the ﬁrst step of epoch , W consists of random observations Zj = (xj , aj , ra,j ) where aj is drawn uniformly at random from {1, . [sent-218, score-0.195]
</p><p>71 This is independent of the number of exploitation steps before epoch . [sent-225, score-0.406]
</p><p>72 This means that the expected regret associated with exploitation steps in epoch is µ (H, s). [sent-227, score-0.905]
</p><p>73 Since the exploration step in each epoch contributes at most 1 to the 5  expected regret, the total expected regret for each epoch is at most 1 + µ (H, s). [sent-228, score-0.977]
</p><p>74 Therefore the L total expected regret for epochs = 1, . [sent-229, score-0.548]
</p><p>75 In the theorem, we bound the expected regret of each exploration step by one. [sent-234, score-0.787]
</p><p>76 1  Finite hypothesis space worst case bound  Consider the ﬁnite hypothesis space situation, with m = |H| < ∞. [sent-241, score-0.427]
</p><p>77 i=1  i=1  It follows that there exists a universal constant c > 0 such that µn (H, 1) ≤ c−1  k ln m/n. [sent-248, score-0.228]
</p><p>78 Therefore in Figure 1, if we choose s(Z1 ) = c  /(k ln m) ,  then µ (H, s) ≤ 1: this is consistent with the choice recommended in Proposition 3. [sent-249, score-0.179]
</p><p>79 It implies that there exists a universal constant c > 0 such that for any given T , we can take L = c T 2/3 (k ln m)1/3 in Theorem 3. [sent-255, score-0.249]
</p><p>80 In summary, if we choose s(Z1 ) = c  /(k ln m) in Figure 1, then  ∆(Epoch-Greedy, H, T ) ≤ 2L ≤ 2c T 2/3 (k ln m)1/3 . [sent-257, score-0.358]
</p><p>81 Reducing the problem to standard bandits, as discussed at the beginning of Section 3, leads to a bound of O(m ln T ) (Lai & Robbins, 1985; Auer et al. [sent-258, score-0.284]
</p><p>82 For many hypothesis classes, the ln m factor can be improved for Epoch-Greedy. [sent-264, score-0.339]
</p><p>83 Moreover, as we will see next, under additional assumptions, it is possible to obtain much better bounds in terms of T for Epoch-Greedy, such as O(k ln m + k ln T ). [sent-266, score-0.378]
</p><p>84 2  Finite hypothesis space with unknown expected reward gap  This example illustrates the importance of allowing sample-dependent s(Z1 ). [sent-269, score-0.381]
</p><p>85 We still assume a ﬁnite hypothesis space, with m = |H| < ∞. [sent-270, score-0.16]
</p><p>86 In particular we note that the standard bandits problem has regret of the form O(ln T ) while in the worst case, our method for the contextual bandits problem has regret O(T 2/3 ). [sent-272, score-2.204]
</p><p>87 A natural question is then: what are the assumptions we can impose so that the Epoch-Greedy algorithm can have a regret of the form O(ln T ). [sent-273, score-0.473]
</p><p>88 In this example we show that a similar assumption for contextual bandits problems leads to a similar regret bound of O(ln T ) for the Epoch-Greedy algorithm. [sent-275, score-1.237]
</p><p>89 n t=1 n ˆ ˆ Let h1 be the hypothesis with highest empirical reward on Z1 , and h2 be the hypothesis with second highest empirical reward. [sent-283, score-0.476]
</p><p>90 ˆ Let h1 be the hypothesis with the highest true expected reward, then we suffer a regret when h1 = h1 . [sent-285, score-0.676]
</p><p>91 Again, the standard large deviation bound implies that there exists a universal constant c > 0 such that for all j ≥ 1: −1 2 2 ˆ ˆ n P (∆(Z1 ) ≥ (j − 1)∆, h1 = h1 ) ≤me−ck n(1+j )∆  ˆ n P (∆(Z1 ) ≤ 0. [sent-286, score-0.147]
</p><p>92 c∆2  The regret for this choice is O(ln T ), which is better than O(T 2/3 ) of Section 4. [sent-296, score-0.456]
</p><p>93 1 when ∆(Z1 ) is small) and obtain bounds that not only work well when the gap ∆ is large, but also not much worse than the bound of Section 4. [sent-300, score-0.172]
</p><p>94 As a special case, we can apply the method in this section to solve the standard bandits problem. [sent-302, score-0.558]
</p><p>95 The O(k ln T ) bound of the Epoch-Greedy method matches those more specialized algorithms for the standard bandits problem, although our algorithm has a larger constant. [sent-303, score-0.814]
</p><p>96 5  Conclusion  We consider a generalization of the multi-armed bandits problem, where observable context can be used to determine which arm to pull and investigate the sample complexity of the exploration/exploitation trade-off for the Epoch-Greedy algorithm. [sent-304, score-0.961]
</p><p>97 Epoch-Greedy is much better at dealing with large hypothesis spaces or hypothesis spaces with special structures due to its ability to employ any data-dependent sample complexity bound. [sent-306, score-0.339]
</p><p>98 However, for ﬁnite hypothesis space, in the worst case scenario, Exp4 has better dependency on T . [sent-307, score-0.19]
</p><p>99 Gambling in a rigged casino: The adversarial multi-armed bandit problem. [sent-321, score-0.159]
</p><p>100 Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. [sent-327, score-0.192]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bandits', 0.558), ('regret', 0.456), ('exploitation', 0.261), ('arm', 0.26), ('ra', 0.235), ('ln', 0.179), ('exploration', 0.177), ('hypothesis', 0.16), ('contextual', 0.146), ('bandit', 0.143), ('reward', 0.122), ('epoch', 0.104), ('arms', 0.091), ('pull', 0.083), ('bound', 0.077), ('lai', 0.074), ('auer', 0.068), ('xt', 0.064), ('hypotheses', 0.056), ('gap', 0.056), ('ck', 0.056), ('ads', 0.056), ('horizon', 0.055), ('rewards', 0.05), ('expected', 0.043), ('robbins', 0.041), ('context', 0.041), ('steps', 0.041), ('cnj', 0.037), ('exi', 0.037), ('heckman', 0.037), ('minn', 0.037), ('pandey', 0.037), ('pulling', 0.037), ('visitor', 0.037), ('yakowitz', 0.037), ('step', 0.034), ('finite', 0.033), ('epochs', 0.033), ('rh', 0.032), ('er', 0.031), ('player', 0.031), ('worst', 0.03), ('side', 0.03), ('pulled', 0.03), ('samples', 0.028), ('et', 0.028), ('strehl', 0.028), ('nl', 0.028), ('stopping', 0.027), ('theorem', 0.026), ('mansour', 0.026), ('rk', 0.026), ('universal', 0.025), ('phase', 0.025), ('page', 0.025), ('immediate', 0.024), ('exists', 0.024), ('hm', 0.023), ('observes', 0.023), ('proposition', 0.022), ('associative', 0.022), ('sup', 0.022), ('reinforcement', 0.022), ('regarded', 0.021), ('kearns', 0.021), ('implies', 0.021), ('achieve', 0.021), ('cn', 0.021), ('iid', 0.02), ('bounds', 0.02), ('incurred', 0.019), ('uniformly', 0.019), ('worse', 0.019), ('sample', 0.019), ('aj', 0.019), ('ad', 0.019), ('nition', 0.018), ('situation', 0.018), ('game', 0.018), ('iterate', 0.018), ('wang', 0.018), ('unbiased', 0.018), ('advance', 0.018), ('receive', 0.017), ('assumptions', 0.017), ('highest', 0.017), ('placed', 0.016), ('advantage', 0.016), ('rat', 0.016), ('vacuous', 0.016), ('dates', 0.016), ('revenue', 0.016), ('modelbased', 0.016), ('maxh', 0.016), ('casino', 0.016), ('gambling', 0.016), ('rigged', 0.016), ('maximize', 0.016), ('total', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="194-tfidf-1" href="./nips-2007-The_Epoch-Greedy_Algorithm_for_Multi-armed_Bandits_with_Side_Information.html">194 nips-2007-The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information</a></p>
<p>Author: John Langford, Tong Zhang</p><p>Abstract: We present Epoch-Greedy, an algorithm for contextual multi-armed bandits (also known as bandits with side information). Epoch-Greedy has the following properties: 1. No knowledge of a time horizon T is necessary. 2. The regret incurred by Epoch-Greedy is controlled by a sample complexity bound for a hypothesis class. 3. The regret scales as O(T 2/3 S 1/3 ) or better (sometimes, much better). Here S is the complexity term in a sample complexity bound for standard supervised learning. 1</p><p>2 0.3211194 <a title="194-tfidf-2" href="./nips-2007-Computational_Equivalence_of_Fixed_Points_and_No_Regret_Algorithms%2C_and_Convergence_to_Equilibria.html">54 nips-2007-Computational Equivalence of Fixed Points and No Regret Algorithms, and Convergence to Equilibria</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We study the relation between notions of game-theoretic equilibria which are based on stability under a set of deviations, and empirical equilibria which are reached by rational players. Rational players are modeled by players using no regret algorithms, which guarantee that their payoff in the long run is close to the maximum they could hope to achieve by consistently deviating from the algorithm’s suggested action. We show that for a given set of deviations over the strategy set of a player, it is possible to efﬁciently approximate ﬁxed points of a given deviation if and only if there exist efﬁcient no regret algorithms resistant to the deviations. Further, we show that if all players use a no regret algorithm, then the empirical distribution of their plays converges to an equilibrium. 1</p><p>3 0.28367138 <a title="194-tfidf-3" href="./nips-2007-The_Price_of_Bandit_Information_for_Online_Optimization.html">199 nips-2007-The Price of Bandit Information for Online Optimization</a></p>
<p>Author: Varsha Dani, Sham M. Kakade, Thomas P. Hayes</p><p>Abstract: In the online linear optimization problem, a learner must choose, in each round, a decision from a set D ⊂ Rn in order to minimize an (unknown and changing) linear cost function. We present sharp rates of convergence (with respect to additive regret) for both the full information setting (where the cost function is revealed at the end of each round) and the bandit setting (where only the scalar cost incurred is revealed). In particular, this paper is concerned with the price of bandit information, by which we mean the ratio of the best achievable regret in the bandit setting to that in the full-information setting. For the full informa√ tion case, the upper bound on the regret is O∗ ( nT ), where n is the ambient dimension and T is the time horizon. For the bandit case, we present an algorithm √ which achieves O∗ (n3/2 T ) regret — all previous (nontrivial) bounds here were O(poly(n)T 2/3 ) or worse. It is striking that the convergence rate for the bandit setting is only a factor of n worse than in the full information case — in stark contrast to the K-arm bandit setting, where the gap in the dependence on K is √ √ exponential ( T K√ vs. T log K). We also present lower bounds showing that this gap is at least n, which we conjecture to be the correct order. The bandit algorithm we present can be implemented efﬁciently in special cases of particular interest, such as path planning and Markov Decision Problems. 1</p><p>4 0.22260118 <a title="194-tfidf-4" href="./nips-2007-Regret_Minimization_in_Games_with_Incomplete_Information.html">165 nips-2007-Regret Minimization in Games with Incomplete Information</a></p>
<p>Author: Martin Zinkevich, Michael Johanson, Michael Bowling, Carmelo Piccione</p><p>Abstract: Extensive games are a powerful model of multiagent decision-making scenarios with incomplete information. Finding a Nash equilibrium for very large instances of these games has received a great deal of recent attention. In this paper, we describe a new technique for solving large games based on regret minimization. In particular, we introduce the notion of counterfactual regret, which exploits the degree of incomplete information in an extensive game. We show how minimizing counterfactual regret minimizes overall regret, and therefore in self-play can be used to compute a Nash equilibrium. We demonstrate this technique in the domain of poker, showing we can solve abstractions of limit Texas Hold’em with as many as 1012 states, two orders of magnitude larger than previous methods. 1</p><p>5 0.20257983 <a title="194-tfidf-5" href="./nips-2007-Adaptive_Online_Gradient_Descent.html">21 nips-2007-Adaptive Online Gradient Descent</a></p>
<p>Author: Elad Hazan, Alexander Rakhlin, Peter L. Bartlett</p><p>Abstract: We study the rates of growth of the regret in online convex optimization. First, we show that a simple extension of the algorithm of Hazan et al eliminates the need for a priori knowledge of the lower bound on the second derivatives of the observed functions. We then provide an algorithm, Adaptive Online Gradient Descent, which interpolates between the results of Zinkevich for linear functions and of Hazan et al for strongly convex functions, achieving intermediate rates √ between T and log T . Furthermore, we show strong optimality of the algorithm. Finally, we provide an extension of our results to general norms. 1</p><p>6 0.17374414 <a title="194-tfidf-6" href="./nips-2007-Optimistic_Linear_Programming_gives_Logarithmic_Regret_for_Irreducible_MDPs.html">151 nips-2007-Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs</a></p>
<p>7 0.099713713 <a title="194-tfidf-7" href="./nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</a></p>
<p>8 0.068447754 <a title="194-tfidf-8" href="./nips-2007-A_Game-Theoretic_Approach_to_Apprenticeship_Learning.html">5 nips-2007-A Game-Theoretic Approach to Apprenticeship Learning</a></p>
<p>9 0.064439341 <a title="194-tfidf-9" href="./nips-2007-Computing_Robust_Counter-Strategies.html">55 nips-2007-Computing Robust Counter-Strategies</a></p>
<p>10 0.057376616 <a title="194-tfidf-10" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>11 0.055333856 <a title="194-tfidf-11" href="./nips-2007-Learning_Bounds_for_Domain_Adaptation.html">110 nips-2007-Learning Bounds for Domain Adaptation</a></p>
<p>12 0.054927919 <a title="194-tfidf-12" href="./nips-2007-A_Bayesian_Model_of_Conditioned_Perception.html">3 nips-2007-A Bayesian Model of Conditioned Perception</a></p>
<p>13 0.052824069 <a title="194-tfidf-13" href="./nips-2007-Fitted_Q-iteration_in_continuous_action-space_MDPs.html">91 nips-2007-Fitted Q-iteration in continuous action-space MDPs</a></p>
<p>14 0.050330523 <a title="194-tfidf-14" href="./nips-2007-Retrieved_context_and_the_discovery_of_semantic_structure.html">169 nips-2007-Retrieved context and the discovery of semantic structure</a></p>
<p>15 0.04949934 <a title="194-tfidf-15" href="./nips-2007-One-Pass_Boosting.html">147 nips-2007-One-Pass Boosting</a></p>
<p>16 0.049013108 <a title="194-tfidf-16" href="./nips-2007-Stability_Bounds_for_Non-i.i.d._Processes.html">184 nips-2007-Stability Bounds for Non-i.i.d. Processes</a></p>
<p>17 0.045480751 <a title="194-tfidf-17" href="./nips-2007-Hippocampal_Contributions_to_Control%3A_The_Third_Way.html">100 nips-2007-Hippocampal Contributions to Control: The Third Way</a></p>
<p>18 0.044994708 <a title="194-tfidf-18" href="./nips-2007-Predictive_Matrix-Variate_t_Models.html">156 nips-2007-Predictive Matrix-Variate t Models</a></p>
<p>19 0.044728696 <a title="194-tfidf-19" href="./nips-2007-Boosting_Algorithms_for_Maximizing_the_Soft_Margin.html">38 nips-2007-Boosting Algorithms for Maximizing the Soft Margin</a></p>
<p>20 0.044008169 <a title="194-tfidf-20" href="./nips-2007-Testing_for_Homogeneity_with_Kernel_Fisher_Discriminant_Analysis.html">192 nips-2007-Testing for Homogeneity with Kernel Fisher Discriminant Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.147), (1, -0.257), (2, 0.04), (3, 0.067), (4, 0.416), (5, -0.024), (6, -0.097), (7, 0.042), (8, -0.003), (9, 0.028), (10, -0.021), (11, 0.002), (12, -0.007), (13, -0.001), (14, -0.007), (15, -0.024), (16, 0.006), (17, -0.037), (18, 0.028), (19, -0.057), (20, 0.004), (21, -0.0), (22, -0.076), (23, -0.121), (24, -0.031), (25, -0.065), (26, -0.01), (27, -0.088), (28, -0.016), (29, 0.066), (30, 0.071), (31, 0.008), (32, 0.054), (33, -0.011), (34, -0.021), (35, 0.036), (36, 0.011), (37, 0.072), (38, -0.094), (39, -0.0), (40, 0.1), (41, 0.094), (42, 0.097), (43, -0.038), (44, -0.09), (45, -0.005), (46, 0.118), (47, 0.001), (48, 0.11), (49, -0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95017809 <a title="194-lsi-1" href="./nips-2007-The_Epoch-Greedy_Algorithm_for_Multi-armed_Bandits_with_Side_Information.html">194 nips-2007-The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information</a></p>
<p>Author: John Langford, Tong Zhang</p><p>Abstract: We present Epoch-Greedy, an algorithm for contextual multi-armed bandits (also known as bandits with side information). Epoch-Greedy has the following properties: 1. No knowledge of a time horizon T is necessary. 2. The regret incurred by Epoch-Greedy is controlled by a sample complexity bound for a hypothesis class. 3. The regret scales as O(T 2/3 S 1/3 ) or better (sometimes, much better). Here S is the complexity term in a sample complexity bound for standard supervised learning. 1</p><p>2 0.8416695 <a title="194-lsi-2" href="./nips-2007-Computational_Equivalence_of_Fixed_Points_and_No_Regret_Algorithms%2C_and_Convergence_to_Equilibria.html">54 nips-2007-Computational Equivalence of Fixed Points and No Regret Algorithms, and Convergence to Equilibria</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We study the relation between notions of game-theoretic equilibria which are based on stability under a set of deviations, and empirical equilibria which are reached by rational players. Rational players are modeled by players using no regret algorithms, which guarantee that their payoff in the long run is close to the maximum they could hope to achieve by consistently deviating from the algorithm’s suggested action. We show that for a given set of deviations over the strategy set of a player, it is possible to efﬁciently approximate ﬁxed points of a given deviation if and only if there exist efﬁcient no regret algorithms resistant to the deviations. Further, we show that if all players use a no regret algorithm, then the empirical distribution of their plays converges to an equilibrium. 1</p><p>3 0.75145727 <a title="194-lsi-3" href="./nips-2007-The_Price_of_Bandit_Information_for_Online_Optimization.html">199 nips-2007-The Price of Bandit Information for Online Optimization</a></p>
<p>Author: Varsha Dani, Sham M. Kakade, Thomas P. Hayes</p><p>Abstract: In the online linear optimization problem, a learner must choose, in each round, a decision from a set D ⊂ Rn in order to minimize an (unknown and changing) linear cost function. We present sharp rates of convergence (with respect to additive regret) for both the full information setting (where the cost function is revealed at the end of each round) and the bandit setting (where only the scalar cost incurred is revealed). In particular, this paper is concerned with the price of bandit information, by which we mean the ratio of the best achievable regret in the bandit setting to that in the full-information setting. For the full informa√ tion case, the upper bound on the regret is O∗ ( nT ), where n is the ambient dimension and T is the time horizon. For the bandit case, we present an algorithm √ which achieves O∗ (n3/2 T ) regret — all previous (nontrivial) bounds here were O(poly(n)T 2/3 ) or worse. It is striking that the convergence rate for the bandit setting is only a factor of n worse than in the full information case — in stark contrast to the K-arm bandit setting, where the gap in the dependence on K is √ √ exponential ( T K√ vs. T log K). We also present lower bounds showing that this gap is at least n, which we conjecture to be the correct order. The bandit algorithm we present can be implemented efﬁciently in special cases of particular interest, such as path planning and Markov Decision Problems. 1</p><p>4 0.5725854 <a title="194-lsi-4" href="./nips-2007-Regret_Minimization_in_Games_with_Incomplete_Information.html">165 nips-2007-Regret Minimization in Games with Incomplete Information</a></p>
<p>Author: Martin Zinkevich, Michael Johanson, Michael Bowling, Carmelo Piccione</p><p>Abstract: Extensive games are a powerful model of multiagent decision-making scenarios with incomplete information. Finding a Nash equilibrium for very large instances of these games has received a great deal of recent attention. In this paper, we describe a new technique for solving large games based on regret minimization. In particular, we introduce the notion of counterfactual regret, which exploits the degree of incomplete information in an extensive game. We show how minimizing counterfactual regret minimizes overall regret, and therefore in self-play can be used to compute a Nash equilibrium. We demonstrate this technique in the domain of poker, showing we can solve abstractions of limit Texas Hold’em with as many as 1012 states, two orders of magnitude larger than previous methods. 1</p><p>5 0.5473398 <a title="194-lsi-5" href="./nips-2007-Optimistic_Linear_Programming_gives_Logarithmic_Regret_for_Irreducible_MDPs.html">151 nips-2007-Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs</a></p>
<p>Author: Ambuj Tewari, Peter L. Bartlett</p><p>Abstract: We present an algorithm called Optimistic Linear Programming (OLP) for learning to optimize average reward in an irreducible but otherwise unknown Markov decision process (MDP). OLP uses its experience so far to estimate the MDP. It chooses actions by optimistically maximizing estimated future rewards over a set of next-state transition probabilities that are close to the estimates, a computation that corresponds to solving linear programs. We show that the total expected reward obtained by OLP up to time T is within C(P ) log T of the reward obtained by the optimal policy, where C(P ) is an explicit, MDP-dependent constant. OLP is closely related to an algorithm proposed by Burnetas and Katehakis with four key differences: OLP is simpler, it does not require knowledge of the supports of transition probabilities, the proof of the regret bound is simpler, but our regret bound is a constant factor larger than the regret of their algorithm. OLP is also similar in ﬂavor to an algorithm recently proposed by Auer and Ortner. But OLP is simpler and its regret bound has a better dependence on the size of the MDP. 1</p><p>6 0.54647827 <a title="194-lsi-6" href="./nips-2007-Adaptive_Online_Gradient_Descent.html">21 nips-2007-Adaptive Online Gradient Descent</a></p>
<p>7 0.30790925 <a title="194-lsi-7" href="./nips-2007-Stability_Bounds_for_Non-i.i.d._Processes.html">184 nips-2007-Stability Bounds for Non-i.i.d. Processes</a></p>
<p>8 0.27842909 <a title="194-lsi-8" href="./nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</a></p>
<p>9 0.26826522 <a title="194-lsi-9" href="./nips-2007-Sequential_Hypothesis_Testing_under_Stochastic_Deadlines.html">176 nips-2007-Sequential Hypothesis Testing under Stochastic Deadlines</a></p>
<p>10 0.25407377 <a title="194-lsi-10" href="./nips-2007-Computing_Robust_Counter-Strategies.html">55 nips-2007-Computing Robust Counter-Strategies</a></p>
<p>11 0.24657971 <a title="194-lsi-11" href="./nips-2007-A_general_agnostic_active_learning_algorithm.html">15 nips-2007-A general agnostic active learning algorithm</a></p>
<p>12 0.24108681 <a title="194-lsi-12" href="./nips-2007-Fitted_Q-iteration_in_continuous_action-space_MDPs.html">91 nips-2007-Fitted Q-iteration in continuous action-space MDPs</a></p>
<p>13 0.24089037 <a title="194-lsi-13" href="./nips-2007-FilterBoost%3A_Regression_and_Classification_on_Large_Datasets.html">90 nips-2007-FilterBoost: Regression and Classification on Large Datasets</a></p>
<p>14 0.23453905 <a title="194-lsi-14" href="./nips-2007-A_Game-Theoretic_Approach_to_Apprenticeship_Learning.html">5 nips-2007-A Game-Theoretic Approach to Apprenticeship Learning</a></p>
<p>15 0.23387896 <a title="194-lsi-15" href="./nips-2007-A_Bayesian_Model_of_Conditioned_Perception.html">3 nips-2007-A Bayesian Model of Conditioned Perception</a></p>
<p>16 0.2215562 <a title="194-lsi-16" href="./nips-2007-Progressive_mixture_rules_are_deviation_suboptimal.html">159 nips-2007-Progressive mixture rules are deviation suboptimal</a></p>
<p>17 0.21529981 <a title="194-lsi-17" href="./nips-2007-Boosting_Algorithms_for_Maximizing_the_Soft_Margin.html">38 nips-2007-Boosting Algorithms for Maximizing the Soft Margin</a></p>
<p>18 0.21300867 <a title="194-lsi-18" href="./nips-2007-Nearest-Neighbor-Based_Active_Learning_for_Rare_Category_Detection.html">139 nips-2007-Nearest-Neighbor-Based Active Learning for Rare Category Detection</a></p>
<p>19 0.20809463 <a title="194-lsi-19" href="./nips-2007-Ultrafast_Monte_Carlo_for_Statistical_Summations.html">209 nips-2007-Ultrafast Monte Carlo for Statistical Summations</a></p>
<p>20 0.19298443 <a title="194-lsi-20" href="./nips-2007-Markov_Chain_Monte_Carlo_with_People.html">125 nips-2007-Markov Chain Monte Carlo with People</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.032), (13, 0.048), (16, 0.011), (18, 0.012), (19, 0.014), (21, 0.046), (33, 0.344), (34, 0.025), (35, 0.025), (46, 0.034), (47, 0.067), (49, 0.013), (83, 0.126), (85, 0.011), (87, 0.018), (90, 0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80028856 <a title="194-lda-1" href="./nips-2007-Measuring_Neural_Synchrony_by_Message_Passing.html">127 nips-2007-Measuring Neural Synchrony by Message Passing</a></p>
<p>Author: Justin Dauwels, François Vialatte, Tomasz Rutkowski, Andrzej S. Cichocki</p><p>Abstract: A novel approach to measure the interdependence of two time series is proposed, referred to as “stochastic event synchrony” (SES); it quantiﬁes the alignment of two point processes by means of the following parameters: time delay, variance of the timing jitter, fraction of “spurious” events, and average similarity of events. SES may be applied to generic one-dimensional and multi-dimensional point processes, however, the paper mainly focusses on point processes in time-frequency domain. The average event similarity is in that case described by two parameters: the average frequency offset between events in the time-frequency plane, and the variance of the frequency offset (“frequency jitter”); SES then consists of ﬁve parameters in total. Those parameters quantify the synchrony of oscillatory events, and hence, they provide an alternative to existing synchrony measures that quantify amplitude or phase synchrony. The pairwise alignment of point processes is cast as a statistical inference problem, which is solved by applying the maxproduct algorithm on a graphical model. The SES parameters are determined from the resulting pairwise alignment by maximum a posteriori (MAP) estimation. The proposed interdependence measure is applied to the problem of detecting anomalies in EEG synchrony of Mild Cognitive Impairment (MCI) patients; the results indicate that SES signiﬁcantly improves the sensitivity of EEG in detecting MCI.</p><p>same-paper 2 0.70474768 <a title="194-lda-2" href="./nips-2007-The_Epoch-Greedy_Algorithm_for_Multi-armed_Bandits_with_Side_Information.html">194 nips-2007-The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information</a></p>
<p>Author: John Langford, Tong Zhang</p><p>Abstract: We present Epoch-Greedy, an algorithm for contextual multi-armed bandits (also known as bandits with side information). Epoch-Greedy has the following properties: 1. No knowledge of a time horizon T is necessary. 2. The regret incurred by Epoch-Greedy is controlled by a sample complexity bound for a hypothesis class. 3. The regret scales as O(T 2/3 S 1/3 ) or better (sometimes, much better). Here S is the complexity term in a sample complexity bound for standard supervised learning. 1</p><p>3 0.44977617 <a title="194-lda-3" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>Author: Yuhong Guo, Dale Schuurmans</p><p>Abstract: We investigate a new, convex relaxation of an expectation-maximization (EM) variant that approximates a standard objective while eliminating local minima. First, a cautionary result is presented, showing that any convex relaxation of EM over hidden variables must give trivial results if any dependence on the missing values is retained. Although this appears to be a strong negative outcome, we then demonstrate how the problem can be bypassed by using equivalence relations instead of value assignments over hidden variables. In particular, we develop new algorithms for estimating exponential conditional models that only require equivalence relation information over the variable values. This reformulation leads to an exact expression for EM variants in a wide range of problems. We then develop a semideﬁnite relaxation that yields global training by eliminating local minima. 1</p><p>4 0.44247371 <a title="194-lda-4" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>Author: Marc'aurelio Ranzato, Y-lan Boureau, Yann L. Cun</p><p>Abstract: Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation. We describe a novel and efﬁcient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machine trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input observed variables can be captured. 1</p><p>5 0.44087937 <a title="194-lda-5" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>Author: John Wright, Yangyu Tao, Zhouchen Lin, Yi Ma, Heung-yeung Shum</p><p>Abstract: We present a simple new criterion for classiﬁcation, based on principles from lossy data compression. The criterion assigns a test sample to the class that uses the minimum number of additional bits to code the test sample, subject to an allowable distortion. We prove asymptotic optimality of this criterion for Gaussian data and analyze its relationships to classical classiﬁers. Theoretical results provide new insights into relationships among popular classiﬁers such as MAP and RDA, as well as unsupervised clustering methods based on lossy compression [13]. Minimizing the lossy coding length induces a regularization effect which stabilizes the (implicit) density estimate in a small-sample setting. Compression also provides a uniform means of handling classes of varying dimension. This simple classiﬁcation criterion and its kernel and local versions perform competitively against existing classiﬁers on both synthetic examples and real imagery data such as handwritten digits and human faces, without requiring domain-speciﬁc information. 1</p><p>6 0.43948397 <a title="194-lda-6" href="./nips-2007-Structured_Learning_with_Approximate_Inference.html">187 nips-2007-Structured Learning with Approximate Inference</a></p>
<p>7 0.43921787 <a title="194-lda-7" href="./nips-2007-Message_Passing_for_Max-weight_Independent_Set.html">128 nips-2007-Message Passing for Max-weight Independent Set</a></p>
<p>8 0.43825346 <a title="194-lda-8" href="./nips-2007-Expectation_Maximization_and_Posterior_Constraints.html">84 nips-2007-Expectation Maximization and Posterior Constraints</a></p>
<p>9 0.43811905 <a title="194-lda-9" href="./nips-2007-Predictive_Matrix-Variate_t_Models.html">156 nips-2007-Predictive Matrix-Variate t Models</a></p>
<p>10 0.43807864 <a title="194-lda-10" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<p>11 0.43781841 <a title="194-lda-11" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>12 0.43753314 <a title="194-lda-12" href="./nips-2007-Density_Estimation_under_Independent_Similarly_Distributed_Sampling_Assumptions.html">66 nips-2007-Density Estimation under Independent Similarly Distributed Sampling Assumptions</a></p>
<p>13 0.43737754 <a title="194-lda-13" href="./nips-2007-A_Kernel_Statistical_Test_of_Independence.html">7 nips-2007-A Kernel Statistical Test of Independence</a></p>
<p>14 0.43729803 <a title="194-lda-14" href="./nips-2007-Statistical_Analysis_of_Semi-Supervised_Regression.html">186 nips-2007-Statistical Analysis of Semi-Supervised Regression</a></p>
<p>15 0.43710205 <a title="194-lda-15" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>16 0.43703932 <a title="194-lda-16" href="./nips-2007-Kernel_Measures_of_Conditional_Dependence.html">108 nips-2007-Kernel Measures of Conditional Dependence</a></p>
<p>17 0.43666333 <a title="194-lda-17" href="./nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</a></p>
<p>18 0.43664271 <a title="194-lda-18" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>19 0.43657055 <a title="194-lda-19" href="./nips-2007-Colored_Maximum_Variance_Unfolding.html">49 nips-2007-Colored Maximum Variance Unfolding</a></p>
<p>20 0.43646252 <a title="194-lda-20" href="./nips-2007-Supervised_Topic_Models.html">189 nips-2007-Supervised Topic Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
