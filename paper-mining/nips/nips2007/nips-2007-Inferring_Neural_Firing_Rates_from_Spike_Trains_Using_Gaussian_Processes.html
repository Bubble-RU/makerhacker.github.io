<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-104" href="#">nips2007-104</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</h1>
<br/><p>Source: <a title="nips-2007-104-pdf" href="http://papers.nips.cc/paper/3229-inferring-neural-firing-rates-from-spike-trains-using-gaussian-processes.pdf">pdf</a></p><p>Author: Maneesh Sahani, Byron M. Yu, John P. Cunningham, Krishna V. Shenoy</p><p>Abstract: Neural spike trains present challenges to analytical efforts due to their noisy, spiking nature. Many studies of neuroscientiﬁc and neural prosthetic importance rely on a smoothed, denoised estimate of the spike train’s underlying ﬁring rate. Current techniques to ﬁnd time-varying ﬁring rates require ad hoc choices of parameters, offer no conﬁdence intervals on their estimates, and can obscure potentially important single trial variability. We present a new method, based on a Gaussian Process prior, for inferring probabilistically optimal estimates of ﬁring rate functions underlying single or multiple neural spike trains. We test the performance of the method on simulated data and experimentally gathered neural spike trains, and we demonstrate improvements over conventional estimators. 1</p><p>Reference: <a title="nips-2007-104-reference" href="../nips2007_reference/nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk 3  Abstract Neural spike trains present challenges to analytical efforts due to their noisy, spiking nature. [sent-8, score-0.821]
</p><p>2 Many studies of neuroscientiﬁc and neural prosthetic importance rely on a smoothed, denoised estimate of the spike train’s underlying ﬁring rate. [sent-9, score-0.622]
</p><p>3 Current techniques to ﬁnd time-varying ﬁring rates require ad hoc choices of parameters, offer no conﬁdence intervals on their estimates, and can obscure potentially important single trial variability. [sent-10, score-0.207]
</p><p>4 We present a new method, based on a Gaussian Process prior, for inferring probabilistically optimal estimates of ﬁring rate functions underlying single or multiple neural spike trains. [sent-11, score-0.713]
</p><p>5 We test the performance of the method on simulated data and experimentally gathered neural spike trains, and we demonstrate improvements over conventional estimators. [sent-12, score-0.677]
</p><p>6 Even when experimental conditions are repeated closely, the same neuron may produce quite different spike trains from trial to trial. [sent-14, score-0.812]
</p><p>7 This variability may be due to both randomness in the spiking process and to differences in cognitive processing on different experimental trials. [sent-15, score-0.089]
</p><p>8 One common view is that a spike train is generated from a smooth underlying function of time (the ﬁring rate) and that this function carries a signiﬁcant portion of the neural information. [sent-16, score-0.66]
</p><p>9 If this is the case, questions of neuroscientiﬁc and neural prosthetic importance may require an accurate estimate of the ﬁring rate. [sent-17, score-0.074]
</p><p>10 Unfortunately, these estimates are complicated by the fact that spike data gives only a sparse observation of its underlying rate. [sent-18, score-0.571]
</p><p>11 Typically, researchers average across many trials to ﬁnd a smooth estimate (averaging out spiking noise). [sent-19, score-0.118]
</p><p>12 However, averaging across many roughly similar trials can obscure important temporal features [1]. [sent-20, score-0.094]
</p><p>13 Thus, estimating the underlying rate from only one spike train (or a small number of spike trains believed to be generated from the same underlying rate) is an important but challenging problem. [sent-21, score-1.509]
</p><p>14 The most common approach to the problem has been to collect spikes from multiple trials in a peristimulus-time histogram (PSTH), which is then sometimes smoothed by convolution or splines [2], [3]. [sent-22, score-0.164]
</p><p>15 Bin sizes and smoothness parameters are typically chosen ad hoc (but see [4], [5]) and the result is fundamentally a multi-trial analysis. [sent-23, score-0.077]
</p><p>16 An alternative is to convolve a single spike train with a kernel. [sent-24, score-0.584]
</p><p>17 Again, the kernel shape and time scale are frequently ad hoc. [sent-25, score-0.099]
</p><p>18 1  More recently, point process likelihood methods have been adapted to spike data [6]–[8]. [sent-28, score-0.542]
</p><p>19 These methods optimize (implicitly or explicitly) the conditional intensity function λ(t|x(t), H(t)) — which gives the probability of a spike in [t, t + dt), given an underlying rate function x(t) and the history of previous spikes H(t) — with respect to x(t). [sent-29, score-0.713]
</p><p>20 In a regression setting, this rate x(t) may be learned as a function of an observed covariate, such as a sensory stimulus or limb movement. [sent-30, score-0.082]
</p><p>21 In other ﬁelds, a theory of generalized Cox processes has been developed, where the point process is conditionally Poisson, and x(t) is obtained by applying a link function to a draw from a random process, often a Gaussian process (GP) (e. [sent-39, score-0.105]
</p><p>22 However, the link function, which ensures a nonnegative intensity, introduces possibly undesirable artifacts. [sent-43, score-0.09]
</p><p>23 For instance, an exponential link leads to a process that grows less smooth as the intensity increases. [sent-44, score-0.12]
</p><p>24 First, we adapt the theory of GP-driven point processes to incorporate a history-dependent conditional likelihood, suitable for spike trains. [sent-46, score-0.509]
</p><p>25 We show that GP methods employing evidence optimization outperform both kernel smoothing and maximum-likelihood point process models. [sent-49, score-0.104]
</p><p>26 2  Gaussian Process Model For Spike Trains  Spike trains can often be well modelled by gamma-interval point processes [6], [10]. [sent-50, score-0.256]
</p><p>27 We assume the underlying nonnegative ﬁring rate x(t) : t ∈ [0, T ] is a draw from a GP, and then we assume that our spike train is a conditionally inhomogeneous gamma-interval process (IGIP), given x(t). [sent-51, score-0.894]
</p><p>28 The spike train is represented by a list of spike times y = {y0 , . [sent-52, score-1.093]
</p><p>29 [6]) can be written as: p(yi | yi−1 , x(t)) =  γx(yi ) γ Γ(γ)  γ−1  yi  x(u)du yi−1  yi  exp −γ  x(u)du . [sent-58, score-0.15]
</p><p>30 (2)  yi−1  The true p0 (·) and pT (·) under this gamma-interval spiking model are not closed form, so we simplify these distributions as intervals of an inhomogeneous Poisson process (IP). [sent-59, score-0.242]
</p><p>31 The discretized IGIP output process is now (ignoring terms that scale with ∆): 1 The IGIP is one of a class of renewal models that works well for spike data (much better than inhomogeneous Poisson; see [6], [10]). [sent-67, score-0.738]
</p><p>32 Other log-concave renewal models such as the inhomogeneous inverse-Gaussian interval can be chosen, and the implementation details remain unchanged. [sent-68, score-0.196]
</p><p>33 2  N  p(y | x)  = i=1  γxyi γ Γ(γ)  yi −1  yi −1  γ−1  exp −γ  xk ∆ k=yi−1  y0 −1  · xy0 exp −  xk ∆ k=yi−1  n−1  xk ∆ · exp − k=0  (3)  xk ∆ , k=yN  where the ﬁnal two terms are p0 (·) and pT (·), respectively [11]. [sent-69, score-0.354]
</p><p>34 Our goal is to estimate a smoothly varying ﬁring rate function from spike times. [sent-70, score-0.591]
</p><p>35 Since our intensity function is nonnegative, however, it is sensible to treat µ instead as a hyperparameter and let it be optimized to a positive value. [sent-79, score-0.105]
</p><p>36 As written, the model assumes only one observed spike train; it may be that we have m trials believed to be generated from the same ﬁring rate proﬁle. [sent-82, score-0.653]
</p><p>37 Our method naturally incorporates this case: deﬁne m p(y(i) | x), where y(i) denotes the ith spike train observed. [sent-83, score-0.584]
</p><p>38 1  Finding an Optimal Firing Rate Estimate Algorithmic Approach  Ideally, we would calculate the posterior on ﬁring rate p(x | y) = θ p(x | y, θ)p(θ)dθ (integrating over the hyperparameters θ), but this problem is intractable. [sent-86, score-0.146]
</p><p>39 We consider two approximations: replacing the integral by evaluation at the modal θ, and replacing the integral with a sum over a discrete grid of θ values. [sent-87, score-0.099]
</p><p>40 We ﬁrst consider choosing a modal hyperparameter set (ML-II model selection, see [12]), i. [sent-88, score-0.13]
</p><p>41 p(x | y) ≈ q(x | y, θ ∗ ) where q(·) is some approximate posterior, and  θ∗ = argmax p(θ | y) = argmax p(θ)p(y | θ) = argmax p(θ) θ  θ  θ  p(y | x, θ)p(x | θ)dx. [sent-90, score-0.135]
</p><p>42 Below we show this integrand is log concave in x. [sent-98, score-0.071]
</p><p>43 Further, since we are modelling a non-zero mean GP, most of the Laplace approximated probability mass lies in the nonnegative orthant (as is the case with the true posterior). [sent-100, score-0.117]
</p><p>44 Accordingly, we write: 2 Another reasonable approach would consider each trial as having a different rate function x that is a draw from a GP with a nonstationary mean function µ(t). [sent-101, score-0.156]
</p><p>45 Instead of inferring a mean rate function x ∗ , we would learn a distribution of means. [sent-102, score-0.105]
</p><p>46 The Laplace approximation also naturally provides conﬁdence intervals from the approximated posterior covariance (Σ −1 + Λ∗ )−1 . [sent-109, score-0.072]
</p><p>47 The approximate integrated posterior can be written as p(x | y) = Eθ|y [p(x | y, θ)] ≈ j q(x | y, θj )q(θj | y) for some choice of samples θj (which again gives conﬁdence intervals on the estimates). [sent-113, score-0.072]
</p><p>48 This approximate integration consistently yields better results than a modal hyperparameter set, so we will only consider approximate integration for the remainder of this report. [sent-115, score-0.18]
</p><p>49 For the Laplace approximation at any value of θ, we require the modal estimate of ﬁring rate x ∗ , which is simply the MAP estimator: x∗ = argmax p(x | y) = argmax p(y | x)p(x). [sent-116, score-0.245]
</p><p>50 Typically a link or squashing function would be included to enforce nonnegativity in x, but this can distort the intensity space in unintended ways. [sent-118, score-0.116]
</p><p>51 Accordingly, we can use a log barrier Newton method to efﬁciently solve for the global MAP estimator of ﬁring rate x∗ [15]. [sent-145, score-0.112]
</p><p>52 In the case of multiple spike train observations, we need only add extra terms of negative log likelihood from the observation model. [sent-146, score-0.614]
</p><p>53 With 1 ms time resolution (or similar), this method would be restricted to spike trains lasting less than a second, and even this problem would be burdensome. [sent-158, score-0.837]
</p><p>54 Since we have evenly spaced resolution of our data x in time indices ti , Σ is Toeplitz; thus multiplication by Σ can be done using Fast Fourier Transform (FFT) methods [16]. [sent-165, score-0.084]
</p><p>55 For the modal hyperparameter scheme (as opposed to approximately integrating over the hyperparameters), gradients of Eq. [sent-168, score-0.153]
</p><p>56 These data sizes translate to seconds of spike data at 1 ms resolution, long enough for most electrophysiological trials. [sent-172, score-0.551]
</p><p>57 4  Results  We tested the methods developed here using both simulated neural data, where the true ﬁring rate was known by construction, and in real neural spike trains, where the true ﬁring rate was estimated by a PSTH that averaged many similar trials. [sent-174, score-0.828]
</p><p>58 We compared the IGIP-likelihood GP method (hereafter, GP IGIP) to other rate estimators (kernel smoothers, Bayesian Adaptive Regressions Splines or BARS [3], and variants of the GP method) using root mean squared difference (RMS) to the true ﬁring rate. [sent-177, score-0.082]
</p><p>59 PSTH and kernel methods approximate the mean conditional intensity λ(t) = EH(t) [λ(t|x(t), H(t))]. [sent-178, score-0.119]
</p><p>60 For a renewal process, we know (by the time rescaling theorem [7], [11]) that λ(t) = x(t), and thus we can compare the GP IGIP (which ﬁnds x(t)) directly to the kernel methods. [sent-179, score-0.162]
</p><p>61 To conﬁrm that hyperparameter optimization improves performance, we also compared GP IGIP results to maximum likelihood (ML) estimates of x(t) using ﬁxed hyperparameters θ. [sent-180, score-0.12]
</p><p>62 To evaluate the importance of an observation model with spike history dependence (the IGIP of Eq. [sent-182, score-0.509]
</p><p>63 3), we also compared GP IGIP to an inhomogeneous Poisson (GP IP) observation model (again with a GP prior on x(t); simply γ = 1 in Eq. [sent-183, score-0.105]
</p><p>64 For the approximate integration, we chose a grid consisting of the empirical mean rate for µ (that is, total spike count N divided by total time T ) 2 and (γ, log(σf ), log(κ)) ∈ [1, 2, 4] × [4, . [sent-193, score-0.617]
</p><p>65 1; 4 spike trains  50  16 14  40  Firing Rate (spikes/sec)  Firing Rate (spikes/sec)  45  35 30 25 20 15 10  10 8 6 4 2  5 0  12  0  0. [sent-219, score-0.765]
</p><p>66 3; 1 spike train  Figure 1: Sample GP ﬁring rate estimate. [sent-237, score-0.666]
</p><p>67 1 represent experimentally gathered ﬁring rate proﬁles (according to the methods in [17]). [sent-240, score-0.129]
</p><p>68 In each of the plots, the empirical average ﬁring rate of the spike trains is shown in bold red. [sent-241, score-0.887]
</p><p>69 For simulated spike trains, the spike trains were generated from each of these empirical average ﬁring rates using an IGIP (γ = 4, comparable to ﬁts to real neural data). [sent-242, score-1.42]
</p><p>70 For real neural data, the spike train(s) were selected as a subset of the roughly 200 experimentally recorded spike trains that were used to construct the ﬁring rate proﬁle. [sent-243, score-1.447]
</p><p>71 These spike trains are shown as a train of black dots, each dot indicating a spike event time (the y-axis position is not meaningful). [sent-244, score-1.349]
</p><p>72 This spike train or group of spike trains is the only input given to each of the ﬁtting models. [sent-245, score-1.349]
</p><p>73 In thin green and magenta, we have two kernel smoothed estimates of ﬁring rates; each represents the spike trains convolved with a normal distribution of a speciﬁed standard deviation (50 and 100 ms). [sent-246, score-0.894]
</p><p>74 We also smoothed these spike trains with adaptive kernel [18], ﬁxed ML (as described above), BARS [3], and 150 ms kernel smoothers. [sent-247, score-1.015]
</p><p>75 The light blue envelopes around the bold blue GP ﬁring rate estimate represent the 95% conﬁdence intervals. [sent-252, score-0.172]
</p><p>76 We then ran all methods 100 times on each ﬁring rate proﬁle, using (separately) simulated and real neural spike trains. [sent-256, score-0.709]
</p><p>77 The four panels correspond to the same rate proﬁles shown in Fig. [sent-262, score-0.108]
</p><p>78 In each panel, the top, middle, and bottom bar graphs correspond to the method on 1, 4, and 8 spike trains, respectively. [sent-264, score-0.509]
</p><p>79 6  50  GP Methods GP IP Fixed ML  short  Kernel Smoothers medium long adaptive  BARS  50  %  0 50  %  %  0  BARS  %  0 50  Kernel Smoothers medium long adaptive  0 50  %  short  %  0 50  GP Methods GP IP Fixed ML  0  (a) L20061107. [sent-271, score-0.208]
</p><p>80 1; 1,4,8 spike trains 50  GP Methods GP IP Fixed ML  short  Kernel Smoothers medium long adaptive  BARS  (b) L20061107. [sent-273, score-0.869]
</p><p>81 1; 1,4,8 spike trains 50  %  0 50  %  %  0  BARS  %  0 50  Kernel Smoothers medium long adaptive  0 50  %  short  %  0 50  GP Methods GP IP Fixed ML  0  (c) L20061107. [sent-275, score-0.869]
</p><p>82 3; 1,4,8 spike trains  Figure 2: Average percent RMS improvement of GP IGIP method (with model selection) vs. [sent-279, score-0.765]
</p><p>83 Blue improvement bars are for simulated spike trains; red improvement bars are for real neural spike trains. [sent-282, score-1.252]
</p><p>84 the long bandwidth kernel in panel (b), real spike trains, 4 and 8 spike trains), outperforming the GP IGIP method requires an optimal kernel choice, which can not be judged from the data alone. [sent-286, score-1.218]
</p><p>85 In particular, the adaptive kernel method generally performed more poorly than GP IGIP. [sent-287, score-0.102]
</p><p>86 5  Discussion  We have demonstrated a new method that accurately estimates underlying neural ﬁring rate functions and provides conﬁdence intervals, given one or a few spike trains as input. [sent-291, score-0.946]
</p><p>87 Estimating underlying ﬁring rates is especially challenging due to the inherent noise in spike trains. [sent-293, score-0.576]
</p><p>88 Having only a few spike trains deprives the method of many trials to reduce spiking noise. [sent-294, score-0.883]
</p><p>89 It is important here to remember why we care about single trial or small number of trial estimates, since we believe that in general the neural processing on repeated trials is not identical. [sent-295, score-0.193]
</p><p>90 In this study we show both simulated and real neural spike trains. [sent-297, score-0.627]
</p><p>91 Simulated data provides a good test environment for this method, since the underlying ﬁring rate is known, but it lacks the experimental proof of real neural spike trains (where spiking does not exactly follow a gamma-interval process). [sent-298, score-1.009]
</p><p>92 Taken together, however, we believe the real and simulated data give good evidence of the general improvements offered by this method. [sent-300, score-0.114]
</p><p>93 In simulation, GP IGIP generally outperforms the other smoothers (though, by considerably less than in other panels). [sent-304, score-0.087]
</p><p>94 This may indicate that, in the low ﬁring rate regime, the IGIP is a poor model for real neural spiking. [sent-306, score-0.149]
</p><p>95 Furthermore, some neural spike trains may be inherently ill-suited to analysis. [sent-309, score-0.802]
</p><p>96 With spike trains of only a few spikes/sec, it will be impossible for any method to ﬁnd interesting structure in the ﬁring rate. [sent-311, score-0.765]
</p><p>97 Several studies have investigated the inhomogeneous gamma and other more general models (e. [sent-313, score-0.105]
</p><p>98 [6], [19]), including the inhomogeneous inverse gaussian (IIG) interval and inhomogeneous Markov interval (IMI) processes. [sent-315, score-0.21]
</p><p>99 The methods of this paper apply immediately to any log-concave inhomogeneous renewal process in which inhomogeneity is generated by time-rescaling (this includes the IIG and several others). [sent-316, score-0.229]
</p><p>100 Another direction for this work is to consider signiﬁcant nonstationarity in the spike data. [sent-318, score-0.509]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spike', 0.509), ('gp', 0.455), ('igip', 0.402), ('trains', 0.256), ('ring', 0.219), ('inhomogeneous', 0.105), ('laplace', 0.101), ('firing', 0.095), ('renewal', 0.091), ('smoothers', 0.087), ('rate', 0.082), ('yi', 0.075), ('train', 0.075), ('ip', 0.074), ('modal', 0.073), ('kernel', 0.071), ('trials', 0.062), ('bars', 0.058), ('hyperparameter', 0.057), ('spiking', 0.056), ('ml', 0.054), ('simulated', 0.051), ('xk', 0.051), ('nonnegative', 0.051), ('yn', 0.049), ('intervals', 0.048), ('intensity', 0.048), ('kass', 0.048), ('inversions', 0.048), ('ryu', 0.048), ('trial', 0.047), ('medium', 0.047), ('fixed', 0.047), ('argmax', 0.045), ('psth', 0.044), ('se', 0.043), ('hessian', 0.042), ('ms', 0.042), ('integrand', 0.041), ('bold', 0.04), ('hyperparameters', 0.04), ('sec', 0.039), ('underlying', 0.039), ('link', 0.039), ('neural', 0.037), ('pro', 0.037), ('barbieri', 0.037), ('gilja', 0.037), ('iig', 0.037), ('imi', 0.037), ('matern', 0.037), ('orthant', 0.037), ('prosthetic', 0.037), ('xyi', 0.037), ('pt', 0.036), ('smoothed', 0.035), ('spikes', 0.035), ('poisson', 0.034), ('improvements', 0.033), ('process', 0.033), ('dence', 0.032), ('determinants', 0.032), ('neuroscienti', 0.032), ('afshar', 0.032), ('obscure', 0.032), ('santhanam', 0.032), ('splines', 0.032), ('ventura', 0.032), ('adaptive', 0.031), ('ti', 0.031), ('rms', 0.03), ('log', 0.03), ('resolution', 0.03), ('real', 0.03), ('nonnegativity', 0.029), ('modelling', 0.029), ('tj', 0.029), ('panel', 0.028), ('rates', 0.028), ('ad', 0.028), ('nonstationary', 0.027), ('maneesh', 0.027), ('panels', 0.026), ('grid', 0.026), ('short', 0.026), ('meaningful', 0.026), ('comp', 0.026), ('spline', 0.026), ('integration', 0.025), ('smoothness', 0.025), ('blue', 0.025), ('hoc', 0.024), ('experimentally', 0.024), ('posterior', 0.024), ('multiplication', 0.023), ('gathered', 0.023), ('inferring', 0.023), ('gradients', 0.023), ('estimates', 0.023), ('sacri', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="104-tfidf-1" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>Author: Maneesh Sahani, Byron M. Yu, John P. Cunningham, Krishna V. Shenoy</p><p>Abstract: Neural spike trains present challenges to analytical efforts due to their noisy, spiking nature. Many studies of neuroscientiﬁc and neural prosthetic importance rely on a smoothed, denoised estimate of the spike train’s underlying ﬁring rate. Current techniques to ﬁnd time-varying ﬁring rates require ad hoc choices of parameters, offer no conﬁdence intervals on their estimates, and can obscure potentially important single trial variability. We present a new method, based on a Gaussian Process prior, for inferring probabilistically optimal estimates of ﬁring rate functions underlying single or multiple neural spike trains. We test the performance of the method on simulated data and experimentally gathered neural spike trains, and we demonstrate improvements over conventional estimators. 1</p><p>2 0.31813487 <a title="104-tfidf-2" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>Author: Tatyana Sharpee</p><p>Abstract: This paper compares a family of methods for characterizing neural feature selectivity with natural stimuli in the framework of the linear-nonlinear model. In this model, the neural ﬁring rate is a nonlinear function of a small number of relevant stimulus components. The relevant stimulus dimensions can be found by maximizing one of the family of objective functions, R´ nyi divergences of different e orders [1, 2]. We show that maximizing one of them, R´ nyi divergence of ore der 2, is equivalent to least-square ﬁtting of the linear-nonlinear model to neural data. Next, we derive reconstruction errors in relevant dimensions found by maximizing R´ nyi divergences of arbitrary order in the asymptotic limit of large spike e numbers. We ﬁnd that the smallest errors are obtained with R´ nyi divergence of e order 1, also known as Kullback-Leibler divergence. This corresponds to ﬁnding relevant dimensions by maximizing mutual information [2]. We numerically test how these optimization schemes perform in the regime of low signal-to-noise ratio (small number of spikes and increasing neural noise) for model visual neurons. We ﬁnd that optimization schemes based on either least square ﬁtting or information maximization perform well even when number of spikes is small. Information maximization provides slightly, but signiﬁcantly, better reconstructions than least square ﬁtting. This makes the problem of ﬁnding relevant dimensions, together with the problem of lossy compression [3], one of examples where informationtheoretic measures are no more data limited than those derived from least squares. 1</p><p>3 0.29059827 <a title="104-tfidf-3" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>Author: Kai Yu, Wei Chu</p><p>Abstract: This paper aims to model relational data on edges of networks. We describe appropriate Gaussian Processes (GPs) for directed, undirected, and bipartite networks. The inter-dependencies of edges can be effectively modeled by adapting the GP hyper-parameters. The framework suggests an intimate connection between link prediction and transfer learning, which were traditionally two separate research topics. We develop an efﬁcient learning algorithm that can handle a large number of observations. The experimental results on several real-world data sets verify superior learning capacity. 1</p><p>4 0.2158999 <a title="104-tfidf-4" href="./nips-2007-Robust_Regression_with_Twinned_Gaussian_Processes.html">170 nips-2007-Robust Regression with Twinned Gaussian Processes</a></p>
<p>Author: Andrew Naish-guzman, Sean Holden</p><p>Abstract: We propose a Gaussian process (GP) framework for robust inference in which a GP prior on the mixing weights of a two-component noise model augments the standard process over latent function values. This approach is a generalization of the mixture likelihood used in traditional robust GP regression, and a specialization of the GP mixture models suggested by Tresp [1] and Rasmussen and Ghahramani [2]. The value of this restriction is in its tractable expectation propagation updates, which allow for faster inference and model selection, and better convergence than the standard mixture. An additional beneﬁt over the latter method lies in our ability to incorporate knowledge of the noise domain to inﬂuence predictions, and to recover with the predictive distribution information about the outlier distribution via the gating process. The model has asymptotic complexity equal to that of conventional robust methods, but yields more conﬁdent predictions on benchmark problems than classical heavy-tailed models and exhibits improved stability for data with clustered corruptions, for which they fail altogether. We show further how our approach can be used without adjustment for more smoothly heteroscedastic data, and suggest how it could be extended to more general noise models. We also address similarities with the work of Goldberg et al. [3].</p><p>5 0.21198839 <a title="104-tfidf-5" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>Author: Jonathan W. Pillow, Peter E. Latham</p><p>Abstract: Point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, owing in part to the fact that they do not take into account multiple stages of processing. Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more ﬂexible for ﬁtting neural data. More importantly, it allows us to estimate connectivity patterns among neurons (both observed and unobserved), and may provide insight into how networks process sensory input. We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model’s performance using a simulated example network consisting of two coupled neurons.</p><p>6 0.1924119 <a title="104-tfidf-6" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>7 0.18963268 <a title="104-tfidf-7" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>8 0.18780947 <a title="104-tfidf-8" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>9 0.18464299 <a title="104-tfidf-9" href="./nips-2007-Bayesian_Co-Training.html">32 nips-2007-Bayesian Co-Training</a></p>
<p>10 0.17035876 <a title="104-tfidf-10" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>11 0.16931045 <a title="104-tfidf-11" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>12 0.16636334 <a title="104-tfidf-12" href="./nips-2007-Using_Deep_Belief_Nets_to_Learn_Covariance_Kernels_for_Gaussian_Processes.html">212 nips-2007-Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes</a></p>
<p>13 0.14357729 <a title="104-tfidf-13" href="./nips-2007-Multi-task_Gaussian_Process_Prediction.html">135 nips-2007-Multi-task Gaussian Process Prediction</a></p>
<p>14 0.13008668 <a title="104-tfidf-14" href="./nips-2007-Learning_to_classify_complex_patterns_using_a_VLSI_network_of_spiking_neurons.html">117 nips-2007-Learning to classify complex patterns using a VLSI network of spiking neurons</a></p>
<p>15 0.11491485 <a title="104-tfidf-15" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>16 0.11057498 <a title="104-tfidf-16" href="./nips-2007-Random_Projections_for_Manifold_Learning.html">161 nips-2007-Random Projections for Manifold Learning</a></p>
<p>17 0.10860728 <a title="104-tfidf-17" href="./nips-2007-The_Generalized_FITC_Approximation.html">195 nips-2007-The Generalized FITC Approximation</a></p>
<p>18 0.10720682 <a title="104-tfidf-18" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>19 0.076859549 <a title="104-tfidf-19" href="./nips-2007-A_configurable_analog_VLSI_neural_network_with_spiking_neurons_and_self-regulating_plastic_synapses.html">14 nips-2007-A configurable analog VLSI neural network with spiking neurons and self-regulating plastic synapses</a></p>
<p>20 0.076489076 <a title="104-tfidf-20" href="./nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">79 nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.246), (1, 0.188), (2, 0.286), (3, 0.183), (4, -0.045), (5, -0.072), (6, -0.178), (7, -0.258), (8, -0.008), (9, 0.055), (10, -0.251), (11, -0.047), (12, -0.074), (13, 0.096), (14, -0.071), (15, -0.036), (16, 0.05), (17, 0.06), (18, 0.083), (19, 0.094), (20, -0.084), (21, -0.1), (22, 0.014), (23, 0.149), (24, 0.006), (25, -0.106), (26, 0.032), (27, -0.068), (28, -0.008), (29, -0.002), (30, 0.077), (31, -0.083), (32, 0.013), (33, 0.162), (34, 0.011), (35, 0.037), (36, -0.053), (37, 0.009), (38, -0.039), (39, -0.002), (40, -0.052), (41, 0.037), (42, 0.025), (43, 0.033), (44, 0.014), (45, 0.027), (46, -0.028), (47, -0.026), (48, -0.014), (49, 0.062)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96858817 <a title="104-lsi-1" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>Author: Maneesh Sahani, Byron M. Yu, John P. Cunningham, Krishna V. Shenoy</p><p>Abstract: Neural spike trains present challenges to analytical efforts due to their noisy, spiking nature. Many studies of neuroscientiﬁc and neural prosthetic importance rely on a smoothed, denoised estimate of the spike train’s underlying ﬁring rate. Current techniques to ﬁnd time-varying ﬁring rates require ad hoc choices of parameters, offer no conﬁdence intervals on their estimates, and can obscure potentially important single trial variability. We present a new method, based on a Gaussian Process prior, for inferring probabilistically optimal estimates of ﬁring rate functions underlying single or multiple neural spike trains. We test the performance of the method on simulated data and experimentally gathered neural spike trains, and we demonstrate improvements over conventional estimators. 1</p><p>2 0.74147731 <a title="104-lsi-2" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>Author: Tatyana Sharpee</p><p>Abstract: This paper compares a family of methods for characterizing neural feature selectivity with natural stimuli in the framework of the linear-nonlinear model. In this model, the neural ﬁring rate is a nonlinear function of a small number of relevant stimulus components. The relevant stimulus dimensions can be found by maximizing one of the family of objective functions, R´ nyi divergences of different e orders [1, 2]. We show that maximizing one of them, R´ nyi divergence of ore der 2, is equivalent to least-square ﬁtting of the linear-nonlinear model to neural data. Next, we derive reconstruction errors in relevant dimensions found by maximizing R´ nyi divergences of arbitrary order in the asymptotic limit of large spike e numbers. We ﬁnd that the smallest errors are obtained with R´ nyi divergence of e order 1, also known as Kullback-Leibler divergence. This corresponds to ﬁnding relevant dimensions by maximizing mutual information [2]. We numerically test how these optimization schemes perform in the regime of low signal-to-noise ratio (small number of spikes and increasing neural noise) for model visual neurons. We ﬁnd that optimization schemes based on either least square ﬁtting or information maximization perform well even when number of spikes is small. Information maximization provides slightly, but signiﬁcantly, better reconstructions than least square ﬁtting. This makes the problem of ﬁnding relevant dimensions, together with the problem of lossy compression [3], one of examples where informationtheoretic measures are no more data limited than those derived from least squares. 1</p><p>3 0.68802035 <a title="104-lsi-3" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>Author: Dominik Endres, Mike Oram, Johannes Schindelin, Peter Foldiak</p><p>Abstract: The peristimulus time histogram (PSTH) and its more continuous cousin, the spike density function (SDF) are staples in the analytic toolkit of neurophysiologists. The former is usually obtained by binning spike trains, whereas the standard method for the latter is smoothing with a Gaussian kernel. Selection of a bin width or a kernel size is often done in an relatively arbitrary fashion, even though there have been recent attempts to remedy this situation [1, 2]. We develop an exact Bayesian, generative model approach to estimating PSTHs and demonstate its superiority to competing methods. Further advantages of our scheme include automatic complexity control and error bars on its predictions. 1</p><p>4 0.56386352 <a title="104-lsi-4" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><p>5 0.5629217 <a title="104-lsi-5" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>Author: Kai Yu, Wei Chu</p><p>Abstract: This paper aims to model relational data on edges of networks. We describe appropriate Gaussian Processes (GPs) for directed, undirected, and bipartite networks. The inter-dependencies of edges can be effectively modeled by adapting the GP hyper-parameters. The framework suggests an intimate connection between link prediction and transfer learning, which were traditionally two separate research topics. We develop an efﬁcient learning algorithm that can handle a large number of observations. The experimental results on several real-world data sets verify superior learning capacity. 1</p><p>6 0.53193909 <a title="104-lsi-6" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>7 0.52304244 <a title="104-lsi-7" href="./nips-2007-Robust_Regression_with_Twinned_Gaussian_Processes.html">170 nips-2007-Robust Regression with Twinned Gaussian Processes</a></p>
<p>8 0.50680202 <a title="104-lsi-8" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>9 0.50112879 <a title="104-lsi-9" href="./nips-2007-Bayesian_Co-Training.html">32 nips-2007-Bayesian Co-Training</a></p>
<p>10 0.47115475 <a title="104-lsi-10" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>11 0.43047786 <a title="104-lsi-11" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>12 0.40397337 <a title="104-lsi-12" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>13 0.39244717 <a title="104-lsi-13" href="./nips-2007-The_Generalized_FITC_Approximation.html">195 nips-2007-The Generalized FITC Approximation</a></p>
<p>14 0.38240743 <a title="104-lsi-14" href="./nips-2007-Using_Deep_Belief_Nets_to_Learn_Covariance_Kernels_for_Gaussian_Processes.html">212 nips-2007-Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes</a></p>
<p>15 0.37895557 <a title="104-lsi-15" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>16 0.37578917 <a title="104-lsi-16" href="./nips-2007-Modelling_motion_primitives_and_their_timing_in_biologically_executed_movements.html">133 nips-2007-Modelling motion primitives and their timing in biologically executed movements</a></p>
<p>17 0.35190928 <a title="104-lsi-17" href="./nips-2007-Multi-task_Gaussian_Process_Prediction.html">135 nips-2007-Multi-task Gaussian Process Prediction</a></p>
<p>18 0.30994812 <a title="104-lsi-18" href="./nips-2007-Hidden_Common_Cause_Relations_in_Relational_Learning.html">97 nips-2007-Hidden Common Cause Relations in Relational Learning</a></p>
<p>19 0.29698098 <a title="104-lsi-19" href="./nips-2007-Learning_to_classify_complex_patterns_using_a_VLSI_network_of_spiking_neurons.html">117 nips-2007-Learning to classify complex patterns using a VLSI network of spiking neurons</a></p>
<p>20 0.2849856 <a title="104-lsi-20" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.036), (13, 0.028), (16, 0.176), (18, 0.027), (19, 0.017), (21, 0.106), (25, 0.133), (31, 0.023), (34, 0.029), (35, 0.041), (46, 0.013), (47, 0.076), (49, 0.019), (83, 0.089), (85, 0.031), (87, 0.018), (90, 0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84584492 <a title="104-lda-1" href="./nips-2007-Robust_Regression_with_Twinned_Gaussian_Processes.html">170 nips-2007-Robust Regression with Twinned Gaussian Processes</a></p>
<p>Author: Andrew Naish-guzman, Sean Holden</p><p>Abstract: We propose a Gaussian process (GP) framework for robust inference in which a GP prior on the mixing weights of a two-component noise model augments the standard process over latent function values. This approach is a generalization of the mixture likelihood used in traditional robust GP regression, and a specialization of the GP mixture models suggested by Tresp [1] and Rasmussen and Ghahramani [2]. The value of this restriction is in its tractable expectation propagation updates, which allow for faster inference and model selection, and better convergence than the standard mixture. An additional beneﬁt over the latter method lies in our ability to incorporate knowledge of the noise domain to inﬂuence predictions, and to recover with the predictive distribution information about the outlier distribution via the gating process. The model has asymptotic complexity equal to that of conventional robust methods, but yields more conﬁdent predictions on benchmark problems than classical heavy-tailed models and exhibits improved stability for data with clustered corruptions, for which they fail altogether. We show further how our approach can be used without adjustment for more smoothly heteroscedastic data, and suggest how it could be extended to more general noise models. We also address similarities with the work of Goldberg et al. [3].</p><p>same-paper 2 0.84342498 <a title="104-lda-2" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>Author: Maneesh Sahani, Byron M. Yu, John P. Cunningham, Krishna V. Shenoy</p><p>Abstract: Neural spike trains present challenges to analytical efforts due to their noisy, spiking nature. Many studies of neuroscientiﬁc and neural prosthetic importance rely on a smoothed, denoised estimate of the spike train’s underlying ﬁring rate. Current techniques to ﬁnd time-varying ﬁring rates require ad hoc choices of parameters, offer no conﬁdence intervals on their estimates, and can obscure potentially important single trial variability. We present a new method, based on a Gaussian Process prior, for inferring probabilistically optimal estimates of ﬁring rate functions underlying single or multiple neural spike trains. We test the performance of the method on simulated data and experimentally gathered neural spike trains, and we demonstrate improvements over conventional estimators. 1</p><p>3 0.83556062 <a title="104-lda-3" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><p>4 0.8290894 <a title="104-lda-4" href="./nips-2007-Contraction_Properties_of_VLSI_Cooperative_Competitive_Neural_Networks_of_Spiking_Neurons.html">60 nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</a></p>
<p>Author: Emre Neftci, Elisabetta Chicca, Giacomo Indiveri, Jean-jeacques Slotine, Rodney J. Douglas</p><p>Abstract: A non–linear dynamic system is called contracting if initial conditions are forgotten exponentially fast, so that all trajectories converge to a single trajectory. We use contraction theory to derive an upper bound for the strength of recurrent connections that guarantees contraction for complex neural networks. Speciﬁcally, we apply this theory to a special class of recurrent networks, often called Cooperative Competitive Networks (CCNs), which are an abstract representation of the cooperative-competitive connectivity observed in cortex. This speciﬁc type of network is believed to play a major role in shaping cortical responses and selecting the relevant signal among distractors and noise. In this paper, we analyze contraction of combined CCNs of linear threshold units and verify the results of our analysis in a hybrid analog/digital VLSI CCN comprising spiking neurons and dynamic synapses. 1</p><p>5 0.82706857 <a title="104-lda-5" href="./nips-2007-Topmoumoute_Online_Natural_Gradient_Algorithm.html">206 nips-2007-Topmoumoute Online Natural Gradient Algorithm</a></p>
<p>Author: Nicolas L. Roux, Pierre-antoine Manzagol, Yoshua Bengio</p><p>Abstract: Guided by the goal of obtaining an optimization algorithm that is both fast and yields good generalization, we study the descent direction maximizing the decrease in generalization error or the probability of not increasing generalization error. The surprising result is that from both the Bayesian and frequentist perspectives this can yield the natural gradient direction. Although that direction can be very expensive to compute we develop an efﬁcient, general, online approximation to the natural gradient descent which is suited to large scale problems. We report experimental results showing much faster convergence in computation time and in number of iterations with TONGA (Topmoumoute Online natural Gradient Algorithm) than with stochastic gradient descent, even on very large datasets.</p><p>6 0.79786009 <a title="104-lda-6" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>7 0.76615256 <a title="104-lda-7" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>8 0.75302935 <a title="104-lda-8" href="./nips-2007-The_Generalized_FITC_Approximation.html">195 nips-2007-The Generalized FITC Approximation</a></p>
<p>9 0.74001187 <a title="104-lda-9" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>10 0.6995827 <a title="104-lda-10" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>11 0.6921683 <a title="104-lda-11" href="./nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">79 nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<p>12 0.68410277 <a title="104-lda-12" href="./nips-2007-Fast_Variational_Inference_for_Large-scale_Internet_Diagnosis.html">87 nips-2007-Fast Variational Inference for Large-scale Internet Diagnosis</a></p>
<p>13 0.67660952 <a title="104-lda-13" href="./nips-2007-Variational_Inference_for_Diffusion_Processes.html">213 nips-2007-Variational Inference for Diffusion Processes</a></p>
<p>14 0.66854197 <a title="104-lda-14" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>15 0.6678499 <a title="104-lda-15" href="./nips-2007-Augmented_Functional_Time_Series_Representation_and_Forecasting_with_Gaussian_Processes.html">28 nips-2007-Augmented Functional Time Series Representation and Forecasting with Gaussian Processes</a></p>
<p>16 0.66781098 <a title="104-lda-16" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>17 0.66729975 <a title="104-lda-17" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>18 0.66647416 <a title="104-lda-18" href="./nips-2007-Selecting_Observations_against_Adversarial_Objectives.html">174 nips-2007-Selecting Observations against Adversarial Objectives</a></p>
<p>19 0.66604447 <a title="104-lda-19" href="./nips-2007-Learning_to_classify_complex_patterns_using_a_VLSI_network_of_spiking_neurons.html">117 nips-2007-Learning to classify complex patterns using a VLSI network of spiking neurons</a></p>
<p>20 0.66584796 <a title="104-lda-20" href="./nips-2007-Markov_Chain_Monte_Carlo_with_People.html">125 nips-2007-Markov Chain Monte Carlo with People</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
