<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>133 nips-2007-Modelling motion primitives and their timing in biologically executed movements</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-133" href="#">nips2007-133</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>133 nips-2007-Modelling motion primitives and their timing in biologically executed movements</h1>
<br/><p>Source: <a title="nips-2007-133-pdf" href="http://papers.nips.cc/paper/3204-modelling-motion-primitives-and-their-timing-in-biologically-executed-movements.pdf">pdf</a></p><p>Author: Ben Williams, Marc Toussaint, Amos J. Storkey</p><p>Abstract: Biological movement is built up of sub-blocks or motion primitives. Such primitives provide a compact representation of movement which is also desirable in robotic control applications. We analyse handwriting data to gain a better understanding of primitives and their timings in biological movements. Inference of the shape and the timing of primitives can be done using a factorial HMM based model, allowing the handwriting to be represented in primitive timing space. This representation provides a distribution of spikes corresponding to the primitive activations, which can also be modelled using HMM architectures. We show how the coupling of the low level primitive model, and the higher level timing model during inference can produce good reconstructions of handwriting, with shared primitives for all characters modelled. This coupled model also captures the variance proﬁle of the dataset which is accounted for by spike timing jitter. The timing code provides a compact representation of the movement while generating a movement without an explicit timing model produces a scribbling style of output. 1</p><p>Reference: <a title="nips-2007-133-reference" href="../nips2007_reference/nips-2007-Modelling_motion_primitives_and_their_timing_in_biologically_executed_movements_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Modelling motion primitives and their timing in biologically executed movements  Ben H Williams School of Informatics University of Edinburgh 5 Forrest Hill, EH1 2QL, UK ben. [sent-1, score-0.922]
</p><p>2 uk  Abstract Biological movement is built up of sub-blocks or motion primitives. [sent-10, score-0.238]
</p><p>3 Such primitives provide a compact representation of movement which is also desirable in robotic control applications. [sent-11, score-0.86]
</p><p>4 We analyse handwriting data to gain a better understanding of primitives and their timings in biological movements. [sent-12, score-0.899]
</p><p>5 Inference of the shape and the timing of primitives can be done using a factorial HMM based model, allowing the handwriting to be represented in primitive timing space. [sent-13, score-1.76]
</p><p>6 This representation provides a distribution of spikes corresponding to the primitive activations, which can also be modelled using HMM architectures. [sent-14, score-0.517]
</p><p>7 We show how the coupling of the low level primitive model, and the higher level timing model during inference can produce good reconstructions of handwriting, with shared primitives for all characters modelled. [sent-15, score-1.475]
</p><p>8 This coupled model also captures the variance proﬁle of the dataset which is accounted for by spike timing jitter. [sent-16, score-0.429]
</p><p>9 The timing code provides a compact representation of the movement while generating a movement without an explicit timing model produces a scribbling style of output. [sent-17, score-1.0]
</p><p>10 There is much evidence to suggest that biological movement generation is based upon motor primitives, with discrete muscle synergies found in frog spines, (Bizzi et al. [sent-21, score-0.462]
</p><p>11 , 2002), evidence of primitives being locally ﬁxed (Kargo & Giszter, 2000), and modularity in human motor learning and adaption (Wolpert et al. [sent-24, score-0.702]
</p><p>12 Compact forms of representation for any biologically produced data should therefore also be based upon primitive sub-blocks. [sent-26, score-0.445]
</p><p>13 1  (A)  (B)  ¯t Figure 1: (A) A factorial HMM of a handwriting trajectory Yt . [sent-27, score-0.271]
</p><p>14 The parameters λm indicate the probability of triggering a primitive in the mth factor at time t and are learnt for one speciﬁc character. [sent-28, score-0.589]
</p><p>15 (B) A hierarchical generative model of handwriting where the random variable c indicates the currently written character and deﬁnes a distribution over random variables λm via a Markov t model over Gm . [sent-29, score-0.53]
</p><p>16 There are several approaches to use this idea of motion primitives for more eﬃcient robotic movement control. [sent-30, score-0.877]
</p><p>17 , 2004) use non-linear attractor dynamics as a motion primitive and train them to generate motion that solves a speciﬁc task. [sent-33, score-0.626]
</p><p>18 (Amit & Matari´, 2002) use a single attractor system and generate non-linear motion by c modulating the attractor point. [sent-34, score-0.211]
</p><p>19 These approaches deﬁne a primitive as a segment of movement rather than understanding movement as a superposition of concurrent primitives. [sent-35, score-0.837]
</p><p>20 The goal of analysing and better understanding biological data is to extract a generative model of complex movement based on concurrent primitives which may serve as an eﬃcient representation for robotic movement control. [sent-36, score-1.182]
</p><p>21 This is in contrast to previous studies of handwriting which usually focus on the problem of character classiﬁcation rather than generation (Singer & Tishby, 1994; Hinton & Nair, 2005). [sent-37, score-0.394]
</p><p>22 We investigate handwriting data and analyse whether it can be modelled as a superposition of sparsely activated motion primitives. [sent-38, score-0.376]
</p><p>23 Just as piano music can (approximately) be modelled as a superposition of the sounds emitted by each key we follow the idea that biological movement is a superposition of pre-learnt motion primitives. [sent-41, score-0.518]
</p><p>24 This implies that the whole movement can be compactly represented by the timing of each primitive in analogy to a score of music. [sent-42, score-0.846]
</p><p>25 On the lower level a factorial Hidden Markov Model (fHMM, Ghahramani & Jordan, 1997) is used to model the output as a combination of signals emitted from independent primitives (each primitives corresponds to a factor in the fHMM). [sent-44, score-1.297]
</p><p>26 On the higher level we formulate a model for the primitive timing dependent upon character class. [sent-45, score-0.966]
</p><p>27 The same motion primitives are shared across characters, only their timings diﬀer. [sent-46, score-0.71]
</p><p>28 We train this model on handwriting data using an EM-algorithm and thereby infer the primitives and the primitive timings inherent in this data. [sent-47, score-1.252]
</p><p>29 We ﬁnd that the inferred timing posterior for a speciﬁc character is indeed a compact representation for the speciﬁc character which allows for a good reproduction of this character using the learnt primitives. [sent-48, score-1.17]
</p><p>30 Further, using the timing model learnt on the higher level we can generate new movement – new samples of characters (in the same writing style as the data), and also scribblings that exhibit local similarity to written characters when the higher level timing control is omitted. [sent-49, score-1.181]
</p><p>31 Finally in section 4 we present results on handwriting data recorded with a digitisation tablet, show the primitives and timing code we extract, and demonstrate how the learnt model can be used to generate new samples of characters. [sent-52, score-1.3]
</p><p>32 2  2  Model  Our analysis of primitives and primitive timings in handwriting is based on formulating a corresponding probabilistic generative model. [sent-53, score-1.295]
</p><p>33 On the lower level (Figure 1(A)) we consider a factorial Hidden Markov Model (fHMM) where each factor produces the signal of a single primitive and the linear combination of factors generates the observed movement Yt . [sent-55, score-0.642]
</p><p>34 It allows the learning and identiﬁcation of primitives in the data but does not include a model of their timing. [sent-59, score-0.606]
</p><p>35 In this paper we introduce the full generative model (Figure 1(B)) which includes a generative model for the primitive timing conditioned on the current character. [sent-60, score-0.92]
</p><p>36 1  Modelling primitives in data  Let M be the number of primitives we allow for. [sent-62, score-1.15]
</p><p>37 We describe a primitive as a strongly constrained Markov process which remains in a zero state most of the time but with some ¯ probability λ ∈ [0, 1] enters the 1 state and then rigorously runs through all states 2, . [sent-63, score-0.403]
</p><p>38 (1) P (St = b | St−1 = a, λm ) = t 1 for a = 0 and b = (a + 1) mod Km   0 otherwise  ¯ This process is parameterised by the onset probability λm of the mth primitive at time t. [sent-71, score-0.535]
</p><p>39 , WKm ) is what we call a primitive and – to stay in the analogy ¯ – can be compared to the sound of a piano key. [sent-76, score-0.462]
</p><p>40 We will describe below how we learn the primitives m Ws and also adapt the primitive lengths Km using an EM-algorithm. [sent-78, score-0.978]
</p><p>41 2  A timing model  ¯ Considering the λ’s to be ﬁxed parameters is not a suitable model of biological movement. [sent-80, score-0.378]
</p><p>42 The usage and timing of primitives depends on the character that is written and the timing ¯ varies from character to character. [sent-81, score-1.557]
</p><p>43 Our model takes a diﬀerent approach to parameterise the primitive activations. [sent-83, score-0.463]
</p><p>44 For instance, if a primitive is activated twice in the course of the movement we assume that there have been two signals (“spikes”) emitted from a higher level process which encode the activation times. [sent-84, score-0.682]
</p><p>45 More formally, let c be a discrete random variable indicating the character to be written, see Figure 1(B). [sent-85, score-0.215]
</p><p>46 We assume that for each primitive we have another Markovian process which generates a length-L sequence of states Gm ∈ {1, . [sent-86, score-0.403]
</p><p>47 l l−1  (3)  l=2  The states Gm encode which primitives are activated and how they are timed, as seen in l Figure 2(b). [sent-89, score-0.601]
</p><p>48 (b) Scatter plot of the MAP onsets of a single t primitive for diﬀerent samples of the same character ‘p’. [sent-99, score-0.681]
</p><p>49 of a primitive at time t, which we call a “spike”. [sent-101, score-0.403]
</p><p>50 We can observe at l most L spikes in one primitive, the spike times between diﬀerent primitives are dependent, but we have a Markovian dependency between the presence and timing of spikes within a primitive. [sent-113, score-1.115]
</p><p>51 The whole process is parameterised by the initial state distribution P (Gm | c), 1 m the transition probabilities P (Gm | Gm , c), the spike means µm and the variances σr . [sent-114, score-0.182]
</p><p>52 l  3  Inference and learning  In the experiments we will compare both the fHMM without the timing model (Figure 1(A)) and the full model including the timing model (Figure 1(B)). [sent-121, score-0.676]
</p><p>53 5  5  Distance /mm  (c)  Figure 3: (a) Reconstruction of a character from a training dataset, using a subset of the primitives. [sent-168, score-0.215]
</p><p>54 The posterior probability of primitive onset is shown on the left, highlighting why a spike timing representation is appropriate. [sent-170, score-0.882]
</p><p>55 (c) Generative samples using a ﬂat primitive onset prior, showing scribbling behaviour of uncoupled model. [sent-172, score-0.528]
</p><p>56 In the full model, inference is an iterative process of inference in the timing model and inference in the fHMM. [sent-174, score-0.416]
</p><p>57 We couple this iteration to inference in the timing model in both directions: In each iteration, m the posterior over St deﬁnes observation likelihoods for inference in the Markov models Gm . [sent-176, score-0.359]
</p><p>58 Standard M-steps are then used t to train all parameters of the fHMM and the timing model. [sent-178, score-0.276]
</p><p>59 In addition, we use heuristics to adapt the length Km of each primitive: we increase or decrease Km depending on whether the learnt primitive is signiﬁcantly diﬀerent to zero in the last time steps. [sent-179, score-0.554]
</p><p>60 By this we mean that we take a trained model, use inference to compute the MAP spikes λ for a speciﬁc data sample, then we use these λ’s and the deﬁnition of our generative model (including the learnt primitives W ) to generate a trajectory which can be compared to the original data sample. [sent-182, score-0.991]
</p><p>61 1  Results Primitive and timing analysis using the fHMM-only  We ﬁrst consider a data set of 300 handwritten ‘p’s recorded using an INTUOS 3 WACOM digitisation tablet http://www. [sent-185, score-0.334]
</p><p>62 Our choice of parameter was M = 10 primitives and we initialised all Km = 20 and constrained them to be smaller than 100 throughout learning. [sent-192, score-0.575]
</p><p>63 This clean posterior is the motivation for introducing a model of the spike timings as a compact representation 5  −15 −20 −25 −30 −35 −40 −45 −50  4  2  −20  −10  0  Distance /mm  (a)  10  0  x 10  x position y position pressure  1. [sent-195, score-0.294]
</p><p>64 (b) Histogram of the reconstruction error, which is 3-dimensional pen movement velocity space. [sent-204, score-0.303]
</p><p>65 Equally the reconstruction (using the Viterbi aligned MAP spikes) shows the suﬃciency of the spike code to generate the character. [sent-208, score-0.216]
</p><p>66 Figure 3(b) shows the primitives W m (translated back into pen-space) that were learnt and implicitly used for the reconstruction of the ‘p’. [sent-209, score-0.798]
</p><p>67 These primitives can be seen to represent typical parts of the ‘p’ character; the arrows in the reconstruction indicate when they are activated. [sent-210, score-0.647]
</p><p>68 To show the importance of this spike timing information, we can demonstrate the eﬀects of removing it. [sent-212, score-0.398]
</p><p>69 When using the fHMM-only model as a generative model with ¯ the learnt stationary spike probabilities λm the result is a form of primitive babbling, as can be seen in Figure 3(c). [sent-213, score-0.834]
</p><p>70 Since these scribblings are generated by random expression of the learnt primitives they locally resemble parts of the ‘p’ character. [sent-214, score-0.755]
</p><p>71 The primitives generalise to other characters if the training dataset contained suﬃcient variation. [sent-215, score-0.65]
</p><p>72 Further investigation has shown that 20 primitives learnt from 12 character types are suﬃciently generalised to represent all remaining novel character types without further learning, by using a single E-step to ﬁt the pre-learnt parameters to a novel dataset. [sent-216, score-1.156]
</p><p>73 2  Generating new characters using the full generative model  Next we trained the full model on the same ‘p’-dataset. [sent-218, score-0.273]
</p><p>74 To the right we see the reconstruction errors in velocity space showing at many time points a perfect reconstruction was attained. [sent-220, score-0.185]
</p><p>75 Since the full model includes a timing model it can also be run autonomously as a generative model for new character samples. [sent-221, score-0.727]
</p><p>76 Figure 4(c) displays such new samples of the character ‘p’ generated by the learnt model. [sent-222, score-0.403]
</p><p>77 As a more challenging problem we collected a data set of over 450 character samples of the letters a, b and c. [sent-223, score-0.252]
</p><p>78 The full model includes the written character class as a random variable and can thus be trained on multi-character data sets. [sent-224, score-0.277]
</p><p>79 Note that we restrict the total number of primitives to M = 10 which will require a sharing of primitives across characters. [sent-225, score-1.15]
</p><p>80 Coupling the timing and the primitive model during learning has the eﬀect of trying to learn primitives from data that are usually in the same place. [sent-229, score-1.285]
</p><p>81 Thus, using the full model the inferred spikes are more compactly clustered at the Gaussian components due to the prior imposed from the timing model (the thick black lines correspond to Equation (4)). [sent-230, score-0.44]
</p><p>82 (b) Reconstruction of dataset using 10 primitives learnt from the dataset in (a). [sent-232, score-0.726]
</p><p>83 8  Time /ms  (a)  (b)  Figure 6: (a) Scatter plot of primitive onset spikes for a single character type across all samples and primitives, showing the clustering of certain primitives in particular parts of a character. [sent-250, score-1.36]
</p><p>84 The thick black lines displays the prior over λ’s imposed from the timing model via Equation (4). [sent-253, score-0.307]
</p><p>85 Finally, we run the full model autonomously to generate new character samples, see Figure 5(c). [sent-254, score-0.337]
</p><p>86 Here the character class, c is ﬁrst sampled uniform randomly and then all learnt parameters are used to eventually sample a trajectory Yt . [sent-255, score-0.407]
</p><p>87 5  Conclusions  In this paper we have shown that it is possible to represent handwriting using a primitive based model. [sent-257, score-0.582]
</p><p>88 The timing of activations is crucial to the accurate reproduction of the character. [sent-260, score-0.32]
</p><p>89 With a small amount of timing variation, a distorted version of the original character is reproduced, whilst large (and coordinated) diﬀerences in the timing pattern produce diﬀerent character types. [sent-261, score-0.982]
</p><p>90 The spike code provides a compact representation of movement, unlike that which has previously been explored in the domain of robotic control. [sent-262, score-0.24]
</p><p>91 We have proposed to use Markov processes conditioned on the character as a model for these spike emissions. [sent-263, score-0.368]
</p><p>92 7  An assumption made in this work is that the primitives are learnt velocity proﬁles. [sent-267, score-0.767]
</p><p>93 We have not included any feedback control systems in the primitive production, however the presence of low-level feedback, such as in a spring system (Hinton & Nair, 2005) or dynamic motor primitives (Ijspeert et al. [sent-268, score-1.156]
</p><p>94 We make no assumptions about how the primitives are learnt in biology. [sent-271, score-0.726]
</p><p>95 It would be interesting to study the evolution of the primitives during human learning of a new character set. [sent-272, score-0.79]
</p><p>96 This could be related to a more accurate and eﬃcient use of primitives already available. [sent-274, score-0.575]
</p><p>97 However, it might also be the case that new primitives are learnt, or old ones adapted. [sent-275, score-0.575]
</p><p>98 Shared and speciﬁc muscle synergies in natural motor behaviors. [sent-311, score-0.179]
</p><p>99 Combinations of muscle synergies in the construction of a natural motor behavior. [sent-317, score-0.179]
</p><p>100 A primitive based generative model to infer timing information in unpartitioned handwriting data. [sent-374, score-0.963]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('primitives', 0.575), ('primitive', 0.403), ('timing', 0.276), ('gm', 0.234), ('character', 0.215), ('handwriting', 0.179), ('fhmm', 0.176), ('movement', 0.167), ('learnt', 0.151), ('spike', 0.122), ('bizzi', 0.103), ('motor', 0.1), ('erent', 0.078), ('characters', 0.075), ('di', 0.074), ('generative', 0.074), ('avella', 0.073), ('reconstruction', 0.072), ('motion', 0.071), ('spikes', 0.071), ('km', 0.069), ('robotic', 0.064), ('timings', 0.064), ('attractor', 0.059), ('piano', 0.059), ('onset', 0.059), ('superposition', 0.058), ('factorial', 0.051), ('schaal', 0.051), ('ijspeert', 0.051), ('wolpert', 0.051), ('reconstructions', 0.047), ('giszter', 0.044), ('reproduction', 0.044), ('saltiel', 0.044), ('synergies', 0.044), ('emitted', 0.044), ('distance', 0.042), ('velocity', 0.041), ('trajectory', 0.041), ('williams', 0.041), ('biological', 0.04), ('parameterised', 0.038), ('storkey', 0.038), ('autonomously', 0.038), ('nair', 0.038), ('samples', 0.037), ('yt', 0.036), ('muscle', 0.035), ('mth', 0.035), ('st', 0.033), ('compact', 0.032), ('model', 0.031), ('full', 0.031), ('cemgil', 0.029), ('digitisation', 0.029), ('frog', 0.029), ('kargo', 0.029), ('kawato', 0.029), ('matari', 0.029), ('nakanishi', 0.029), ('parameterise', 0.029), ('scribbling', 0.029), ('scribblings', 0.029), ('spring', 0.029), ('tablet', 0.029), ('markovian', 0.028), ('scatter', 0.028), ('et', 0.027), ('inference', 0.026), ('activated', 0.026), ('toussaint', 0.026), ('spinal', 0.026), ('onsets', 0.026), ('forrest', 0.026), ('ghahramani', 0.025), ('su', 0.024), ('markov', 0.023), ('amit', 0.023), ('emits', 0.023), ('pen', 0.023), ('pressure', 0.023), ('emit', 0.023), ('map', 0.022), ('probabilities', 0.022), ('generate', 0.022), ('hmm', 0.022), ('hinton', 0.022), ('feedback', 0.022), ('concurrent', 0.022), ('marc', 0.022), ('representation', 0.022), ('activation', 0.021), ('modelled', 0.021), ('level', 0.021), ('centred', 0.021), ('analyse', 0.021), ('edinburgh', 0.021), ('upon', 0.02), ('understanding', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="133-tfidf-1" href="./nips-2007-Modelling_motion_primitives_and_their_timing_in_biologically_executed_movements.html">133 nips-2007-Modelling motion primitives and their timing in biologically executed movements</a></p>
<p>Author: Ben Williams, Marc Toussaint, Amos J. Storkey</p><p>Abstract: Biological movement is built up of sub-blocks or motion primitives. Such primitives provide a compact representation of movement which is also desirable in robotic control applications. We analyse handwriting data to gain a better understanding of primitives and their timings in biological movements. Inference of the shape and the timing of primitives can be done using a factorial HMM based model, allowing the handwriting to be represented in primitive timing space. This representation provides a distribution of spikes corresponding to the primitive activations, which can also be modelled using HMM architectures. We show how the coupling of the low level primitive model, and the higher level timing model during inference can produce good reconstructions of handwriting, with shared primitives for all characters modelled. This coupled model also captures the variance proﬁle of the dataset which is accounted for by spike timing jitter. The timing code provides a compact representation of the movement while generating a movement without an explicit timing model produces a scribbling style of output. 1</p><p>2 0.21768571 <a title="133-tfidf-2" href="./nips-2007-On_Sparsity_and_Overcompleteness_in_Image_Models.html">145 nips-2007-On Sparsity and Overcompleteness in Image Models</a></p>
<p>Author: Pietro Berkes, Richard Turner, Maneesh Sahani</p><p>Abstract: Computational models of visual cortex, and in particular those based on sparse coding, have enjoyed much recent attention. Despite this currency, the question of how sparse or how over-complete a sparse representation should be, has gone without principled answer. Here, we use Bayesian model-selection methods to address these questions for a sparse-coding model based on a Student-t prior. Having validated our methods on toy data, we ﬁnd that natural images are indeed best modelled by extremely sparse distributions; although for the Student-t prior, the associated optimal basis size is only modestly over-complete. 1</p><p>3 0.16073851 <a title="133-tfidf-3" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>Author: Misha Ahrens, Maneesh Sahani</p><p>Abstract: Many perceptual processes and neural computations, such as speech recognition, motor control and learning, depend on the ability to measure and mark the passage of time. However, the processes that make such temporal judgements possible are unknown. A number of different hypothetical mechanisms have been advanced, all of which depend on the known, temporally predictable evolution of a neural or psychological state, possibly through oscillations or the gradual decay of a memory trace. Alternatively, judgements of elapsed time might be based on observations of temporally structured, but stochastic processes. Such processes need not be speciﬁc to the sense of time; typical neural and sensory processes contain at least some statistical structure across a range of time scales. Here, we investigate the statistical properties of an estimator of elapsed time which is based on a simple family of stochastic process. 1</p><p>4 0.096388385 <a title="133-tfidf-4" href="./nips-2007-Unconstrained_On-line_Handwriting_Recognition_with_Recurrent_Neural_Networks.html">210 nips-2007-Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks</a></p>
<p>Author: Alex Graves, Marcus Liwicki, Horst Bunke, Jürgen Schmidhuber, Santiago Fernández</p><p>Abstract: In online handwriting recognition the trajectory of the pen is recorded during writing. Although the trajectory provides a compact and complete representation of the written output, it is hard to transcribe directly, because each letter is spread over many pen locations. Most recognition systems therefore employ sophisticated preprocessing techniques to put the inputs into a more localised form. However these techniques require considerable human effort, and are speciﬁc to particular languages and alphabets. This paper describes a system capable of directly transcribing raw online handwriting data. The system consists of an advanced recurrent neural network with an output layer designed for sequence labelling, combined with a probabilistic language model. In experiments on an unconstrained online database, we record excellent results using either raw or preprocessed data, well outperforming a state-of-the-art HMM based system in both cases. 1</p><p>5 0.086447924 <a title="133-tfidf-5" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>Author: Tatyana Sharpee</p><p>Abstract: This paper compares a family of methods for characterizing neural feature selectivity with natural stimuli in the framework of the linear-nonlinear model. In this model, the neural ﬁring rate is a nonlinear function of a small number of relevant stimulus components. The relevant stimulus dimensions can be found by maximizing one of the family of objective functions, R´ nyi divergences of different e orders [1, 2]. We show that maximizing one of them, R´ nyi divergence of ore der 2, is equivalent to least-square ﬁtting of the linear-nonlinear model to neural data. Next, we derive reconstruction errors in relevant dimensions found by maximizing R´ nyi divergences of arbitrary order in the asymptotic limit of large spike e numbers. We ﬁnd that the smallest errors are obtained with R´ nyi divergence of e order 1, also known as Kullback-Leibler divergence. This corresponds to ﬁnding relevant dimensions by maximizing mutual information [2]. We numerically test how these optimization schemes perform in the regime of low signal-to-noise ratio (small number of spikes and increasing neural noise) for model visual neurons. We ﬁnd that optimization schemes based on either least square ﬁtting or information maximization perform well even when number of spikes is small. Information maximization provides slightly, but signiﬁcantly, better reconstructions than least square ﬁtting. This makes the problem of ﬁnding relevant dimensions, together with the problem of lossy compression [3], one of examples where informationtheoretic measures are no more data limited than those derived from least squares. 1</p><p>6 0.075996891 <a title="133-tfidf-6" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>7 0.072535478 <a title="133-tfidf-7" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>8 0.070181124 <a title="133-tfidf-8" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>9 0.063453466 <a title="133-tfidf-9" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>10 0.057700213 <a title="133-tfidf-10" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>11 0.053672526 <a title="133-tfidf-11" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>12 0.049823612 <a title="133-tfidf-12" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>13 0.047895283 <a title="133-tfidf-13" href="./nips-2007-The_Infinite_Markov_Model.html">197 nips-2007-The Infinite Markov Model</a></p>
<p>14 0.042324923 <a title="133-tfidf-14" href="./nips-2007-A_Bayesian_Model_of_Conditioned_Perception.html">3 nips-2007-A Bayesian Model of Conditioned Perception</a></p>
<p>15 0.041135177 <a title="133-tfidf-15" href="./nips-2007-People_Tracking_with_the_Laplacian_Eigenmaps_Latent_Variable_Model.html">153 nips-2007-People Tracking with the Laplacian Eigenmaps Latent Variable Model</a></p>
<p>16 0.041113302 <a title="133-tfidf-16" href="./nips-2007-Receding_Horizon_Differential_Dynamic_Programming.html">163 nips-2007-Receding Horizon Differential Dynamic Programming</a></p>
<p>17 0.040601194 <a title="133-tfidf-17" href="./nips-2007-EEG-Based_Brain-Computer_Interaction%3A_Improved_Accuracy_by_Automatic_Single-Trial_Error_Detection.html">74 nips-2007-EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection</a></p>
<p>18 0.040582187 <a title="133-tfidf-18" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>19 0.039805949 <a title="133-tfidf-19" href="./nips-2007-Second_Order_Bilinear_Discriminant_Analysis_for_single_trial_EEG_analysis.html">173 nips-2007-Second Order Bilinear Discriminant Analysis for single trial EEG analysis</a></p>
<p>20 0.037664175 <a title="133-tfidf-20" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.126), (1, 0.049), (2, 0.122), (3, -0.039), (4, -0.013), (5, 0.014), (6, -0.026), (7, 0.003), (8, -0.04), (9, -0.044), (10, 0.011), (11, 0.006), (12, -0.006), (13, 0.019), (14, 0.031), (15, 0.001), (16, -0.042), (17, 0.099), (18, 0.028), (19, 0.009), (20, -0.046), (21, -0.001), (22, 0.133), (23, 0.061), (24, 0.003), (25, -0.083), (26, 0.041), (27, 0.049), (28, -0.031), (29, -0.179), (30, 0.227), (31, -0.176), (32, 0.149), (33, 0.118), (34, -0.053), (35, -0.048), (36, -0.154), (37, -0.205), (38, -0.013), (39, -0.105), (40, 0.193), (41, 0.097), (42, -0.047), (43, 0.148), (44, -0.148), (45, -0.152), (46, 0.111), (47, 0.058), (48, -0.117), (49, 0.124)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95524216 <a title="133-lsi-1" href="./nips-2007-Modelling_motion_primitives_and_their_timing_in_biologically_executed_movements.html">133 nips-2007-Modelling motion primitives and their timing in biologically executed movements</a></p>
<p>Author: Ben Williams, Marc Toussaint, Amos J. Storkey</p><p>Abstract: Biological movement is built up of sub-blocks or motion primitives. Such primitives provide a compact representation of movement which is also desirable in robotic control applications. We analyse handwriting data to gain a better understanding of primitives and their timings in biological movements. Inference of the shape and the timing of primitives can be done using a factorial HMM based model, allowing the handwriting to be represented in primitive timing space. This representation provides a distribution of spikes corresponding to the primitive activations, which can also be modelled using HMM architectures. We show how the coupling of the low level primitive model, and the higher level timing model during inference can produce good reconstructions of handwriting, with shared primitives for all characters modelled. This coupled model also captures the variance proﬁle of the dataset which is accounted for by spike timing jitter. The timing code provides a compact representation of the movement while generating a movement without an explicit timing model produces a scribbling style of output. 1</p><p>2 0.5144161 <a title="133-lsi-2" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>Author: Misha Ahrens, Maneesh Sahani</p><p>Abstract: Many perceptual processes and neural computations, such as speech recognition, motor control and learning, depend on the ability to measure and mark the passage of time. However, the processes that make such temporal judgements possible are unknown. A number of different hypothetical mechanisms have been advanced, all of which depend on the known, temporally predictable evolution of a neural or psychological state, possibly through oscillations or the gradual decay of a memory trace. Alternatively, judgements of elapsed time might be based on observations of temporally structured, but stochastic processes. Such processes need not be speciﬁc to the sense of time; typical neural and sensory processes contain at least some statistical structure across a range of time scales. Here, we investigate the statistical properties of an estimator of elapsed time which is based on a simple family of stochastic process. 1</p><p>3 0.50265473 <a title="133-lsi-3" href="./nips-2007-On_Sparsity_and_Overcompleteness_in_Image_Models.html">145 nips-2007-On Sparsity and Overcompleteness in Image Models</a></p>
<p>Author: Pietro Berkes, Richard Turner, Maneesh Sahani</p><p>Abstract: Computational models of visual cortex, and in particular those based on sparse coding, have enjoyed much recent attention. Despite this currency, the question of how sparse or how over-complete a sparse representation should be, has gone without principled answer. Here, we use Bayesian model-selection methods to address these questions for a sparse-coding model based on a Student-t prior. Having validated our methods on toy data, we ﬁnd that natural images are indeed best modelled by extremely sparse distributions; although for the Student-t prior, the associated optimal basis size is only modestly over-complete. 1</p><p>4 0.44954839 <a title="133-lsi-4" href="./nips-2007-Unconstrained_On-line_Handwriting_Recognition_with_Recurrent_Neural_Networks.html">210 nips-2007-Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks</a></p>
<p>Author: Alex Graves, Marcus Liwicki, Horst Bunke, Jürgen Schmidhuber, Santiago Fernández</p><p>Abstract: In online handwriting recognition the trajectory of the pen is recorded during writing. Although the trajectory provides a compact and complete representation of the written output, it is hard to transcribe directly, because each letter is spread over many pen locations. Most recognition systems therefore employ sophisticated preprocessing techniques to put the inputs into a more localised form. However these techniques require considerable human effort, and are speciﬁc to particular languages and alphabets. This paper describes a system capable of directly transcribing raw online handwriting data. The system consists of an advanced recurrent neural network with an output layer designed for sequence labelling, combined with a probabilistic language model. In experiments on an unconstrained online database, we record excellent results using either raw or preprocessed data, well outperforming a state-of-the-art HMM based system in both cases. 1</p><p>5 0.4174608 <a title="133-lsi-5" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>Author: Dominik Endres, Mike Oram, Johannes Schindelin, Peter Foldiak</p><p>Abstract: The peristimulus time histogram (PSTH) and its more continuous cousin, the spike density function (SDF) are staples in the analytic toolkit of neurophysiologists. The former is usually obtained by binning spike trains, whereas the standard method for the latter is smoothing with a Gaussian kernel. Selection of a bin width or a kernel size is often done in an relatively arbitrary fashion, even though there have been recent attempts to remedy this situation [1, 2]. We develop an exact Bayesian, generative model approach to estimating PSTHs and demonstate its superiority to competing methods. Further advantages of our scheme include automatic complexity control and error bars on its predictions. 1</p><p>6 0.35596868 <a title="133-lsi-6" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>7 0.33103928 <a title="133-lsi-7" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>8 0.30096045 <a title="133-lsi-8" href="./nips-2007-A_Bayesian_Model_of_Conditioned_Perception.html">3 nips-2007-A Bayesian Model of Conditioned Perception</a></p>
<p>9 0.27288011 <a title="133-lsi-9" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>10 0.27006984 <a title="133-lsi-10" href="./nips-2007-Augmented_Functional_Time_Series_Representation_and_Forecasting_with_Gaussian_Processes.html">28 nips-2007-Augmented Functional Time Series Representation and Forecasting with Gaussian Processes</a></p>
<p>11 0.26446733 <a title="133-lsi-11" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<p>12 0.26433444 <a title="133-lsi-12" href="./nips-2007-Measuring_Neural_Synchrony_by_Message_Passing.html">127 nips-2007-Measuring Neural Synchrony by Message Passing</a></p>
<p>13 0.25148579 <a title="133-lsi-13" href="./nips-2007-A_Probabilistic_Approach_to_Language_Change.html">9 nips-2007-A Probabilistic Approach to Language Change</a></p>
<p>14 0.24963984 <a title="133-lsi-14" href="./nips-2007-Modeling_Natural_Sounds_with_Modulation_Cascade_Processes.html">130 nips-2007-Modeling Natural Sounds with Modulation Cascade Processes</a></p>
<p>15 0.24948606 <a title="133-lsi-15" href="./nips-2007-Receding_Horizon_Differential_Dynamic_Programming.html">163 nips-2007-Receding Horizon Differential Dynamic Programming</a></p>
<p>16 0.24583964 <a title="133-lsi-16" href="./nips-2007-The_Infinite_Markov_Model.html">197 nips-2007-The Infinite Markov Model</a></p>
<p>17 0.2249773 <a title="133-lsi-17" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>18 0.21467575 <a title="133-lsi-18" href="./nips-2007-Random_Sampling_of_States_in_Dynamic_Programming.html">162 nips-2007-Random Sampling of States in Dynamic Programming</a></p>
<p>19 0.20225102 <a title="133-lsi-19" href="./nips-2007-People_Tracking_with_the_Laplacian_Eigenmaps_Latent_Variable_Model.html">153 nips-2007-People Tracking with the Laplacian Eigenmaps Latent Variable Model</a></p>
<p>20 0.20121504 <a title="133-lsi-20" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.034), (13, 0.043), (16, 0.042), (18, 0.017), (19, 0.018), (21, 0.07), (31, 0.021), (34, 0.013), (35, 0.02), (47, 0.073), (83, 0.07), (85, 0.017), (87, 0.033), (90, 0.04), (97, 0.391)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7540251 <a title="133-lda-1" href="./nips-2007-Modelling_motion_primitives_and_their_timing_in_biologically_executed_movements.html">133 nips-2007-Modelling motion primitives and their timing in biologically executed movements</a></p>
<p>Author: Ben Williams, Marc Toussaint, Amos J. Storkey</p><p>Abstract: Biological movement is built up of sub-blocks or motion primitives. Such primitives provide a compact representation of movement which is also desirable in robotic control applications. We analyse handwriting data to gain a better understanding of primitives and their timings in biological movements. Inference of the shape and the timing of primitives can be done using a factorial HMM based model, allowing the handwriting to be represented in primitive timing space. This representation provides a distribution of spikes corresponding to the primitive activations, which can also be modelled using HMM architectures. We show how the coupling of the low level primitive model, and the higher level timing model during inference can produce good reconstructions of handwriting, with shared primitives for all characters modelled. This coupled model also captures the variance proﬁle of the dataset which is accounted for by spike timing jitter. The timing code provides a compact representation of the movement while generating a movement without an explicit timing model produces a scribbling style of output. 1</p><p>2 0.7361933 <a title="133-lda-2" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>Author: Dominik Endres, Mike Oram, Johannes Schindelin, Peter Foldiak</p><p>Abstract: The peristimulus time histogram (PSTH) and its more continuous cousin, the spike density function (SDF) are staples in the analytic toolkit of neurophysiologists. The former is usually obtained by binning spike trains, whereas the standard method for the latter is smoothing with a Gaussian kernel. Selection of a bin width or a kernel size is often done in an relatively arbitrary fashion, even though there have been recent attempts to remedy this situation [1, 2]. We develop an exact Bayesian, generative model approach to estimating PSTHs and demonstate its superiority to competing methods. Further advantages of our scheme include automatic complexity control and error bars on its predictions. 1</p><p>3 0.65788686 <a title="133-lda-3" href="./nips-2007-The_discriminant_center-surround_hypothesis_for_bottom-up_saliency.html">202 nips-2007-The discriminant center-surround hypothesis for bottom-up saliency</a></p>
<p>Author: Dashan Gao, Vijay Mahadevan, Nuno Vasconcelos</p><p>Abstract: The classical hypothesis, that bottom-up saliency is a center-surround process, is combined with a more recent hypothesis that all saliency decisions are optimal in a decision-theoretic sense. The combined hypothesis is denoted as discriminant center-surround saliency, and the corresponding optimal saliency architecture is derived. This architecture equates the saliency of each image location to the discriminant power of a set of features with respect to the classiﬁcation problem that opposes stimuli at center and surround, at that location. It is shown that the resulting saliency detector makes accurate quantitative predictions for various aspects of the psychophysics of human saliency, including non-linear properties beyond the reach of previous saliency models. Furthermore, it is shown that discriminant center-surround saliency can be easily generalized to various stimulus modalities (such as color, orientation and motion), and provides optimal solutions for many other saliency problems of interest for computer vision. Optimal solutions, under this hypothesis, are derived for a number of the former (including static natural images, dense motion ﬁelds, and even dynamic textures), and applied to a number of the latter (the prediction of human eye ﬁxations, motion-based saliency in the presence of ego-motion, and motion-based saliency in the presence of highly dynamic backgrounds). In result, discriminant saliency is shown to predict eye ﬁxations better than previous models, and produces background subtraction algorithms that outperform the state-of-the-art in computer vision. 1</p><p>4 0.34131441 <a title="133-lda-4" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>Author: Michael Ross, Andrew Cohen</p><p>Abstract: This paper describes a new model for human visual classiﬁcation that enables the recovery of image features that explain human subjects’ performance on different visual classiﬁcation tasks. Unlike previous methods, this algorithm does not model their performance with a single linear classiﬁer operating on raw image pixels. Instead, it represents classiﬁcation as the combination of multiple feature detectors. This approach extracts more information about human visual classiﬁcation than previous methods and provides a foundation for further exploration. 1</p><p>5 0.33588749 <a title="133-lda-5" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>Author: Jonathan W. Pillow, Peter E. Latham</p><p>Abstract: Point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, owing in part to the fact that they do not take into account multiple stages of processing. Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more ﬂexible for ﬁtting neural data. More importantly, it allows us to estimate connectivity patterns among neurons (both observed and unobserved), and may provide insight into how networks process sensory input. We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model’s performance using a simulated example network consisting of two coupled neurons.</p><p>6 0.33568275 <a title="133-lda-6" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>7 0.33507681 <a title="133-lda-7" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>8 0.33454973 <a title="133-lda-8" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>9 0.3344734 <a title="133-lda-9" href="./nips-2007-Hippocampal_Contributions_to_Control%3A_The_Third_Way.html">100 nips-2007-Hippocampal Contributions to Control: The Third Way</a></p>
<p>10 0.33413801 <a title="133-lda-10" href="./nips-2007-The_Generalized_FITC_Approximation.html">195 nips-2007-The Generalized FITC Approximation</a></p>
<p>11 0.33362994 <a title="133-lda-11" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>12 0.33340287 <a title="133-lda-12" href="./nips-2007-People_Tracking_with_the_Laplacian_Eigenmaps_Latent_Variable_Model.html">153 nips-2007-People Tracking with the Laplacian Eigenmaps Latent Variable Model</a></p>
<p>13 0.33306581 <a title="133-lda-13" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<p>14 0.3318921 <a title="133-lda-14" href="./nips-2007-Scene_Segmentation_with_CRFs_Learned_from_Partially_Labeled_Images.html">172 nips-2007-Scene Segmentation with CRFs Learned from Partially Labeled Images</a></p>
<p>15 0.33173779 <a title="133-lda-15" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>16 0.33159387 <a title="133-lda-16" href="./nips-2007-Distributed_Inference_for_Latent_Dirichlet_Allocation.html">73 nips-2007-Distributed Inference for Latent Dirichlet Allocation</a></p>
<p>17 0.33016536 <a title="133-lda-17" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>18 0.32985672 <a title="133-lda-18" href="./nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">79 nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<p>19 0.3296681 <a title="133-lda-19" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>20 0.32965398 <a title="133-lda-20" href="./nips-2007-Locality_and_low-dimensions_in_the_prediction_of_natural_experience_from_fMRI.html">122 nips-2007-Locality and low-dimensions in the prediction of natural experience from fMRI</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
