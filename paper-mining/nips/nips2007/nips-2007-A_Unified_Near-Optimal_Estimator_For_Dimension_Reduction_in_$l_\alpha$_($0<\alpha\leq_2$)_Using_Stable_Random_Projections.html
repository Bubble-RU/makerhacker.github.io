<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>13 nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-13" href="#">nips2007-13</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>13 nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</h1>
<br/><p>Source: <a title="nips-2007-13-pdf" href="http://papers.nips.cc/paper/3225-a-unified-near-optimal-estimator-for-dimension-reduction-in-l_alpha-0alphaleq-2-using-stable-random-projections.pdf">pdf</a></p><p>Author: Ping Li, Trevor J. Hastie</p><p>Abstract: Many tasks (e.g., clustering) in machine learning only require the lα distances instead of the original data. For dimension reductions in the lα norm (0 < α ≤ 2), the method of stable random projections can efﬁciently compute the lα distances in massive datasets (e.g., the Web or massive data streams) in one pass of the data. The estimation task for stable random projections has been an interesting topic. We propose a simple estimator based on the fractional power of the samples (projected data), which is surprisingly near-optimal in terms of the asymptotic variance. In fact, it achieves the Cram´ r-Rao bound when α = 2 and α = 0+. This e new result will be useful when applying stable random projections to distancebased clustering, classiﬁcations, kernels, massive data streams etc.</p><p>Reference: <a title="nips-2007-13-reference" href="../nips2007_reference/nips-2007-A_Unified_Near-Optimal_Estimator_For_Dimension_Reduction_in_%24l_%5Calpha%24_%28%240%3C%5Calpha%5Cleq_2%24%29_Using_Stable_Random_Projections_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 , clustering) in machine learning only require the lα distances instead of the original data. [sent-6, score-0.147]
</p><p>2 For dimension reductions in the lα norm (0 < α ≤ 2), the method of stable random projections can efﬁciently compute the lα distances in massive datasets (e. [sent-7, score-0.884]
</p><p>3 , the Web or massive data streams) in one pass of the data. [sent-9, score-0.165]
</p><p>4 The estimation task for stable random projections has been an interesting topic. [sent-10, score-0.372]
</p><p>5 We propose a simple estimator based on the fractional power of the samples (projected data), which is surprisingly near-optimal in terms of the asymptotic variance. [sent-11, score-1.098]
</p><p>6 This e new result will be useful when applying stable random projections to distancebased clustering, classiﬁcations, kernels, massive data streams etc. [sent-13, score-0.632]
</p><p>7 1 Introduction Dimension reductions in the lα norm (0 < α ≤ 2) have numerous applications in data mining, information retrieval, and machine learning. [sent-14, score-0.176]
</p><p>8 In modern applications, the data can be way too large for the physical memory or even the disk; and sometimes only one pass of the data can be afforded for building statistical learning models [1, 2, 5]. [sent-15, score-0.172]
</p><p>9 The method of stable random projections [9, 18, 22] is a useful tool for efﬁciently computing the lα (0 < α ≤ 2) properties in massive data using a small (memory) space. [sent-18, score-0.532]
</p><p>10 d(α) =  (1)  i=1  The choice of α is beyond the scope of this study; but basically, we can treat α as a tuning parameter. [sent-21, score-0.061]
</p><p>11 For example, the l1 norm has become popular recently, e. [sent-30, score-0.11]
</p><p>12 The lα norm with α < 1, which may initially appear strange, is now well-understood to be a natural measure of sparsity [6]. [sent-35, score-0.081]
</p><p>13 In the extreme case, when α → 0+, the lα norm approaches the Hamming norm (i. [sent-36, score-0.162]
</p><p>14 Therefore, there is the natural demand in science and engineering for dimension reductions in the lα norm other than l2 . [sent-39, score-0.229]
</p><p>15 While the method of normal random projections for the l2 norm [22] has become very popular recently, we have to resort to more general methodologies for 0 < α < 2. [sent-40, score-0.336]
</p><p>16 The idea of stable random projections is to multiply A with a random projection matrix R ∈ RD×k (k ≪ D). [sent-41, score-0.402]
</p><p>17 samples from a symmetric α-stable distribution [24], denoted by S(α, 1), where α is the index and 1 is the scale. [sent-46, score-0.086]
</p><p>18 We can then discard the original data matrix A because the projected matrix B now contains enough information to recover the original lα properties approximately. [sent-47, score-0.109]
</p><p>19 A symmetric α-stable random variable is denoted by S(α, d), where d is the scale parameter. [sent-48, score-0.088]
</p><p>20 Applying stable random projections on u1 ∈ RD , u2 ∈ RD yields, respectively, v1 = RT u1 ∈ Rk and v2 = RT u2 ∈ Rk . [sent-54, score-0.372]
</p><p>21 samples of the stable distribution S(α, d(α) ), i. [sent-61, score-0.197]
</p><p>22 Because no closed-form density functions are available except for α = 1, 2, the estimation task is challenging when we seek estimators that are both accurate and computationally efﬁcient. [sent-71, score-0.117]
</p><p>23 For general 0 < α < 2, a widely used estimator is based on the sample inter-quantiles [7,20], which can be simpliﬁed to be the sample median estimator by choosing the 0. [sent-72, score-0.978]
</p><p>24 d(α),me = median{S(α, 1)}α  (4)  It has been well-known that the sample median estimator is not accurate, especially when the sample size k is not too large. [sent-79, score-0.599]
</p><p>25 Recently, [13] proposed various estimators based on the geometric mean and the harmonic mean of the samples. [sent-80, score-0.48]
</p><p>26 The harmonic mean estimator only works for small α. [sent-81, score-0.59]
</p><p>27 The geometric mean estimator has nice properties including closed-form variances, closed-form tail bounds in exponential forms, and very importantly, an analog of the Johnson-Lindenstrauss (JL) Lemma [10] for dimension reduction in lα . [sent-82, score-0.76]
</p><p>28 The geometric mean estimator, however, can still be improved for certain α, especially for large samples (e. [sent-83, score-0.204]
</p><p>29 1 Our Contribution: the Fractional Power Estimator The fractional power estimator, with a simple uniﬁed format for all 0 < α ≤ 2, is (surprisingly) near-optimal in the Cram´ r-Rao sense (i. [sent-87, score-0.582]
</p><p>30 We ﬁrst obtain an unbiased estimator of dλ , denoted by R(α),λ . [sent-92, score-0.437]
</p><p>31 We choose λ = λ (α) to minimize the theoretical asymptotic variance. [sent-94, score-0.112]
</p><p>32 The main computation  involves only  k j=1  |xj |λ  ∗  α  1/λ∗  ; and hence this estimator is also computationally efﬁcient. [sent-98, score-0.379]
</p><p>33 2 Applications The method of stable random projections is useful for efﬁciently computing the lα properties (norms or distances) in massive data, using a small (memory) space. [sent-100, score-0.532]
</p><p>34 • Data stream computations Massive data streams are fundamental in many modern data processing application [1, 2, 5, 9]. [sent-101, score-0.201]
</p><p>35 It is common practice to store only a very small sketch of the streams to efﬁciently compute the lα norms of the individual streams or the lα distances between a pair of streams. [sent-102, score-0.54]
</p><p>36 One interesting special case is to estimate the Hamming norms (or distances) using the D α fact that, when α → 0+, d(α) = i=1 |u1,i − u2,i | approaches the total number of D non-zeros in {|u1,i − u2,i |}i=1 , i. [sent-104, score-0.091]
</p><p>37 One may ask why not just (binary) quantize the data and then apply normal random projections to the binary data. [sent-107, score-0.226]
</p><p>38 With stable random projections, we only need to update the corresponding sketches whenever the data are updated. [sent-111, score-0.242]
</p><p>39 • Computing all pairwise distances In many applications including distanced-based clustering, classiﬁcations and kernels (e. [sent-112, score-0.191]
</p><p>40 Computing all pairwise distances of A ∈ Rn×D would cost O(n2 D), which can be significantly reduced to O(nDk + n2 k) by stable random projections. [sent-115, score-0.366]
</p><p>41 The cost reduction will be more considerable when the original datasets are too large for the physical memory. [sent-116, score-0.121]
</p><p>42 • Estimating lα distances online While it is often infeasible to store the original matrix A in the memory, it is also often infeasible to materialize all pairwise distances in A. [sent-117, score-0.425]
</p><p>43 Thus, in applications such as online learning, databases, search engines, online recommendation systems, and online market-basket analysis, it is often more efﬁcient if we store B ∈ Rn×k in the memory and estimate any pairwise distance in A on the ﬂy only when it is necessary. [sent-118, score-0.247]
</p><p>44 , re-computing the lα distances for many different α, stable random projections will be even more desirable as a cost-saving device. [sent-121, score-0.495]
</p><p>45 • The geometric mean estimator is recommended in [13] for α < 2. [sent-130, score-0.59]
</p><p>46 ˆ d(α),gm =  k j=1  2 Γ π  ˆ Var d(α),gm = d2 (α) = d2 (α)  α k  |xj |α/k  Γ 1− 2 Γ π 2 Γ π 2  2α k α k  1 k  2 k  Γ 1− Γ 1−  1π α2 + 2 k 12  k  π α 2 k  sin 1 k  . [sent-131, score-0.374]
</p><p>47 (5)  sin π α k sin  +O  π α 2 k  1 k2  k 2k  −1  (6)  . [sent-132, score-0.748]
</p><p>48 (7)  • The harmonic mean estimator is recommended in [13] for 0 < α ≤ 0. [sent-133, score-0.625]
</p><p>49 − 2 Γ(−α) sin π α 2 ˆ d(α),hm = π k −α j=1 |xj | 1 ˆ Var d(α),hm = d2 (α) k  −πΓ(−2α) sin (πα)  k−  Γ(−α) sin  −πΓ(−2α) sin (πα) Γ(−α) sin  π α 2  2  −1  +O  π α 2  1 k2  2  −1  ,  . [sent-135, score-1.87]
</p><p>50 (8)  (9)  1 • For α = 2, the arithmetic mean estimator, k k |xj |2 , is commonly used, which has j=1 2 variance = k d2 . [sent-136, score-0.176]
</p><p>51 It can be improved by taking advantage of the marginal l2 norms [17]. [sent-137, score-0.091]
</p><p>52 (2)  3 The Fractional Power Estimator The fractional power estimator takes advantage of the following statistical result in Lemma 1. [sent-138, score-0.929]
</p><p>53 , x ∼ S(2, d(2) ) = N (0, 2d(2) ), then for λ > −1, λ/2  E |x|λ = d(2)  2 λ Γ 1− π 2  Γ(λ) sin  Proof: For 0 < α ≤ 2 and −1 < λ < α, (10) can be inferred directly from [24, Theorem 2. [sent-144, score-0.374]
</p><p>54 The Euler’s reﬂection formula √ π Γ(1 − z)Γ(z) = sin(πz) and the duplication formula Γ(z)Γ z + 1 = 21−2z πΓ(2z) are handy. [sent-151, score-0.072]
</p><p>55 2  The fractional power estimator is deﬁned in Lemma 2. [sent-152, score-0.929]
</p><p>56 8 2  α  Figure 1: Left panel plots the variance factor g (λ; α) as functions of λ for different α, illustrating g (λ; α) is a convex function of λ and the optimal solution (lowest points on the curves) are between -1 and 0. [sent-209, score-0.172]
</p><p>57 When α = 2, since λ∗ (2) = 1, the fractional power estimator becomes arithmetic mean estimator. [sent-218, score-1.044]
</p><p>58 , the  when α → 0+, since λ∗ (0+) = −1, the fractional power estimator approaches the harmonic mean estimator, which is asymptotically optimal when α = 0+ [13]. [sent-222, score-1.181]
</p><p>59 When α → 1, since λ∗ (1) = 0 in the limit, the fractional power estimator has the same asymptotic variance as the geometric mean estimator. [sent-223, score-1.278]
</p><p>60 2 The Asymptotic (Cram´ r-Rao) Efﬁciency e ˆ For an estimator d(α) , its variance, under certain regularity condition, is lower-bounded by the Inforˆ mation inequality (also known as the Cram´ r-Rao bound) [11, Chapter 2], i. [sent-225, score-0.379]
</p><p>61 When α = 2, it is well-known that the arithmetic mean estimator attains the Cram´ r-Rao bound. [sent-229, score-0.494]
</p><p>62 e When α = 0+, [13] has shown that the harmonic mean estimator is also asymptotically optimal. [sent-230, score-0.631]
</p><p>63 Therefore, our fractional power estimator achieves the Cram´ r-Rao bound, exactly when α = 2, e and asymptotically when α = 0+. [sent-231, score-0.998]
</p><p>64 1 The asymptotic (Cram´ r-Rao) efﬁciency is deﬁned as the ratio of kI(α) to the asymptotic variance of e ˆ d(α) (d(α) = 1 for simplicity). [sent-232, score-0.285]
</p><p>65 Figure 2 plots the efﬁciencies for all estimators we have mentioned, illustrating that the fractional power estimator is near-optimal in a wide range of α. [sent-233, score-1.099]
</p><p>66 8 2 α  Figure 2: The asymptotic Cram´ r-Rao efﬁciencies of various estimators for 0 < α < 2, which are e 1 the ratios of kI(α) to the asymptotic variances of the estimators. [sent-248, score-0.347]
</p><p>67 The asymptotic variance of the sample ˆ median estimator d(α),me is computed from known statistical theory for sample quantiles. [sent-250, score-0.772]
</p><p>68 We can ˆ see that the fractional power estimator d(α),f p is close to be optimal in a wide range of α; and it always outperforms both the geometric mean and the harmonic mean estimators. [sent-251, score-1.316]
</p><p>69 3 Theoretical Properties ˆ We can show that, when computing the fractional power estimator d(α),f p , to ﬁnd the optimal λ∗ only involves searching for the minimum on a convex curve in the narrow range λ∗ ∈ 1 max −1, − 2α , 0. [sent-254, score-0.963]
</p><p>70 These properties theoretically ensure that the new estimator is well-deﬁned and is numerically easy to compute. [sent-256, score-0.406]
</p><p>71 2 π Γ(1 − 2λ)Γ(2λα) sin (πλα) Lemma 3 Part 1: g (λ; α) = 1 (16) 2 −1 , 2 λ2 Γ(1 − λ)Γ(λα) sin π λα π  2  is a convex function of λ. [sent-258, score-0.782]
</p><p>72 Figure 3 plots the empirical mean square errors (MSE) from simulations for the fractional power estimator, the harmonic mean estimator, and the sample median estimator. [sent-263, score-1.111]
</p><p>73 The MSE for the geometric mean estimators can be computed exactly without simulations. [sent-264, score-0.269]
</p><p>74 ˆ Figure 3 indicates that the fractional power estimator d(α),f p also has good small sample perforˆ mance unless α is close to 2. [sent-265, score-0.965]
</p><p>75 It is also clear that the sample median estimator has poor small sample performance; but even at very large k, its performance is not that good except when α is about 1. [sent-267, score-0.647]
</p><p>76 6  Mean square error (MSE)  Mean square error (MSE)  0. [sent-269, score-0.096]
</p><p>77 8 2  α  Mean square error (MSE)  Mean square error (MSE)  0 0 0. [sent-294, score-0.096]
</p><p>78 8 2  α  Figure 3: We simulate the mean square errors (MSE) (106 simulations at every α and k) for the harmonic mean estimator (0 < α ≤ 0. [sent-330, score-0.712]
</p><p>79 We compute the MSE exactly for the geometric mean estimator (for 0. [sent-332, score-0.555]
</p><p>80 The fractional power has good accuracy (small MSE) at reasonable sample sizes (e. [sent-334, score-0.586]
</p><p>81 4 Discussion ˆ The fractional power estimator d(α),f p ∝ ∗  k j=1  ∗  |xj |λ  α  1/λ∗  can be treated as a linear estimator k j=1  ∗  in because the power 1/λ is just a constant. [sent-340, score-1.46]
</p><p>82 Thus our result does not conﬂict the celebrated impossibility result [3], which proved that there is no hope to recover the original l1 distances using linear projections and linear estimators without incurring large errors. [sent-342, score-0.463]
</p><p>83 Although the fractional power estimator achieves near-optimal asymptotic variance, analyzing its tail bounds does not appear straightforward. [sent-343, score-1.155]
</p><p>84 In fact, when α approaches 2, this estimator does not have ﬁnite moments much higher than the second order, suggesting poor tail behavior. [sent-344, score-0.489]
</p><p>85 Our ˆ additional simulations (not included in this paper) indicate that d(α),f p still has comparable tail probability behavior as the geometric mean estimator, when α ≤ 1. [sent-345, score-0.286]
</p><p>86 Finally, we should mention that the method of stable random projections does not take advantage of the data sparsity while high-dimensional data (e. [sent-346, score-0.372]
</p><p>87 While there are already many papers on dimension reductions in the l2 norm, this paper focuses on the lα norm for 0 < α ≤ 2 using stable random projections, as it has become increasingly popular in machine learning to consider the lα norm other than l2 . [sent-351, score-0.538]
</p><p>88 Our main contribution is the fractional power estimator for stable random projections. [sent-353, score-1.128]
</p><p>89 This estimator, with a uniﬁed format for all 0 < α ≤ 2, is computationally efﬁcient and (surprisingly) is also near-optimal in terms of the asymptotic variance. [sent-354, score-0.144]
</p><p>90 We expect that this work will help advance the state-of-the-art of dimension reductions in the lα norms. [sent-357, score-0.148]
</p><p>91 The asymptotic variance would be (α) ˆ Var d(α),f p,λ  1 λ  ˆ = Var R(α),λ = d2 (α)  1/λ−1  dλ (α)  2  1 k2  +O  2 π Γ(1 − 2λ)Γ(2λα) sin (πλα) 2 2 π π Γ(1 − λ)Γ(λα) sin 2 λα  1 λ2 k  −1  +O  1 k2  . [sent-366, score-0.921]
</p><p>92 The optimal λ, denoted by λ∗ , is then 2 Γ(1 − 2λ)Γ(2λα) sin (πλα) π 2 2 Γ(1 − λ)Γ(λα) sin π λα π 2  1 λ2  λ∗ = argmin 1 1 − 2α λ< 2  −1  . [sent-367, score-0.82]
</p><p>93 ∂ log fs ∂λ  2  −  4 λ  ∞  s=1  ∂ log fs ∂λ  −  6 . [sent-373, score-0.418]
</p><p>94 ∂ 4 log fs ∂λ4 −12 96 4 + + 6α (s − λ)4 (s − 2λ)4  To show  ∂2g ∂λ2  2  ∂ > 0, it sufﬁces to show λ4 ∂λg > 0, which can shown based on its own second deriva2 4  log tive (and hence we need ∞ ∂ ∂λ4 fs ). [sent-376, score-0.418]
</p><p>95 Next, we show that λ∗ < −1 does not satisfy h(λ∗ ) = M(λ∗ )  ∂g(λ;α) ∂λ λ∗  1−  λ∗ 2  ∞  s=1  = 0, which is equivalent to h(λ∗ ) = 1,  ∂ log fs ∂λ  = 1, λ∗  ∂h We show that when λ < −1, ∂λ > 0, i. [sent-379, score-0.209]
</p><p>96 On the impossibility of dimension reduction in l1 . [sent-400, score-0.142]
</p><p>97 Comparing data streams using hamming norms (how to zero in). [sent-414, score-0.297]
</p><p>98 Estimators and tail bounds for dimension reduction in lα (0 < α ≤ 2) using stable random projections. [sent-454, score-0.377]
</p><p>99 Nonlinear estimators and tail bounds for dimensional reduction in l1 using cauchy random projections. [sent-482, score-0.291]
</p><p>100 Some improvements in numerical evaluation of symmetric stable density and its derivatives. [sent-487, score-0.193]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fractional', 0.398), ('estimator', 0.379), ('sin', 0.374), ('cram', 0.25), ('fs', 0.183), ('projections', 0.173), ('stable', 0.169), ('mse', 0.167), ('harmonic', 0.161), ('power', 0.152), ('median', 0.148), ('massive', 0.133), ('streams', 0.127), ('geometric', 0.126), ('distances', 0.123), ('asymptotic', 0.112), ('var', 0.097), ('reductions', 0.095), ('xj', 0.095), ('estimators', 0.093), ('norms', 0.091), ('tail', 0.086), ('norm', 0.081), ('hamming', 0.079), ('lemma', 0.075), ('arithmetic', 0.065), ('variance', 0.061), ('hastie', 0.059), ('li', 0.055), ('dimension', 0.053), ('mean', 0.05), ('discontinuity', 0.05), ('impossibility', 0.05), ('stream', 0.05), ('square', 0.048), ('memory', 0.045), ('plots', 0.044), ('pairwise', 0.044), ('cauchy', 0.043), ('sketches', 0.043), ('asymptotically', 0.041), ('ciencies', 0.04), ('afforded', 0.04), ('reduction', 0.039), ('argmin', 0.038), ('providence', 0.037), ('datar', 0.037), ('disk', 0.037), ('sample', 0.036), ('sketch', 0.036), ('formula', 0.036), ('store', 0.036), ('recommendation', 0.035), ('recommended', 0.035), ('convex', 0.034), ('projected', 0.034), ('denoted', 0.034), ('rn', 0.034), ('illustrating', 0.033), ('uni', 0.032), ('web', 0.032), ('ki', 0.032), ('format', 0.032), ('pass', 0.032), ('treat', 0.032), ('rd', 0.031), ('physical', 0.031), ('cations', 0.031), ('clustering', 0.031), ('variances', 0.03), ('random', 0.03), ('basically', 0.029), ('radial', 0.029), ('surprisingly', 0.029), ('tuning', 0.029), ('online', 0.029), ('popular', 0.029), ('communications', 0.029), ('fourier', 0.029), ('proof', 0.028), ('achieves', 0.028), ('samples', 0.028), ('datasets', 0.027), ('properties', 0.027), ('fisher', 0.026), ('bias', 0.026), ('log', 0.026), ('taylor', 0.025), ('svm', 0.025), ('symmetric', 0.024), ('modern', 0.024), ('simulations', 0.024), ('kernels', 0.024), ('poor', 0.024), ('original', 0.024), ('unbiased', 0.024), ('except', 0.024), ('normal', 0.023), ('rk', 0.023), ('infeasible', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="13-tfidf-1" href="./nips-2007-A_Unified_Near-Optimal_Estimator_For_Dimension_Reduction_in_%24l_%5Calpha%24_%28%240%3C%5Calpha%5Cleq_2%24%29_Using_Stable_Random_Projections.html">13 nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</a></p>
<p>Author: Ping Li, Trevor J. Hastie</p><p>Abstract: Many tasks (e.g., clustering) in machine learning only require the lα distances instead of the original data. For dimension reductions in the lα norm (0 < α ≤ 2), the method of stable random projections can efﬁciently compute the lα distances in massive datasets (e.g., the Web or massive data streams) in one pass of the data. The estimation task for stable random projections has been an interesting topic. We propose a simple estimator based on the fractional power of the samples (projected data), which is surprisingly near-optimal in terms of the asymptotic variance. In fact, it achieves the Cram´ r-Rao bound when α = 2 and α = 0+. This e new result will be useful when applying stable random projections to distancebased clustering, classiﬁcations, kernels, massive data streams etc.</p><p>2 0.1536445 <a title="13-tfidf-2" href="./nips-2007-Statistical_Analysis_of_Semi-Supervised_Regression.html">186 nips-2007-Statistical Analysis of Semi-Supervised Regression</a></p>
<p>Author: Larry Wasserman, John D. Lafferty</p><p>Abstract: Semi-supervised methods use unlabeled data in addition to labeled data to construct predictors. While existing semi-supervised methods have shown some promising empirical performance, their development has been based largely based on heuristics. In this paper we study semi-supervised learning from the viewpoint of minimax theory. Our ﬁrst result shows that some common methods based on regularization using graph Laplacians do not lead to faster minimax rates of convergence. Thus, the estimators that use the unlabeled data do not have smaller risk than the estimators that use only labeled data. We then develop several new approaches that provably lead to improved performance. The statistical tools of minimax analysis are thus used to offer some new perspective on the problem of semi-supervised learning. 1</p><p>3 0.11575973 <a title="13-tfidf-3" href="./nips-2007-Random_Projections_for_Manifold_Learning.html">161 nips-2007-Random Projections for Manifold Learning</a></p>
<p>Author: Chinmay Hegde, Michael Wakin, Richard Baraniuk</p><p>Abstract: We propose a novel method for linear dimensionality reduction of manifold modeled data. First, we show that with a small number M of random projections of sample points in RN belonging to an unknown K-dimensional Euclidean manifold, the intrinsic dimension (ID) of the sample set can be estimated to high accuracy. Second, we rigorously prove that using only this set of random projections, we can estimate the structure of the underlying manifold. In both cases, the number of random projections required is linear in K and logarithmic in N , meaning that K < M ≪ N . To handle practical situations, we develop a greedy algorithm to estimate the smallest size of the projection space required to perform manifold learning. Our method is particularly relevant in distributed sensing systems and leads to signiﬁcant potential savings in data acquisition, storage and transmission costs.</p><p>4 0.092051476 <a title="13-tfidf-4" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>Author: Misha Ahrens, Maneesh Sahani</p><p>Abstract: Many perceptual processes and neural computations, such as speech recognition, motor control and learning, depend on the ability to measure and mark the passage of time. However, the processes that make such temporal judgements possible are unknown. A number of different hypothetical mechanisms have been advanced, all of which depend on the known, temporally predictable evolution of a neural or psychological state, possibly through oscillations or the gradual decay of a memory trace. Alternatively, judgements of elapsed time might be based on observations of temporally structured, but stochastic processes. Such processes need not be speciﬁc to the sense of time; typical neural and sensory processes contain at least some statistical structure across a range of time scales. Here, we investigate the statistical properties of an estimator of elapsed time which is based on a simple family of stochastic process. 1</p><p>5 0.078103699 <a title="13-tfidf-5" href="./nips-2007-Testing_for_Homogeneity_with_Kernel_Fisher_Discriminant_Analysis.html">192 nips-2007-Testing for Homogeneity with Kernel Fisher Discriminant Analysis</a></p>
<p>Author: Moulines Eric, Francis R. Bach, Zaïd Harchaoui</p><p>Abstract: We propose to investigate test statistics for testing homogeneity based on kernel Fisher discriminant analysis. Asymptotic null distributions under null hypothesis are derived, and consistency against ﬁxed alternatives is assessed. Finally, experimental evidence of the performance of the proposed approach on both artiﬁcial and real datasets is provided. 1</p><p>6 0.076609083 <a title="13-tfidf-6" href="./nips-2007-A_Randomized_Algorithm_for_Large_Scale_Support_Vector_Learning.html">10 nips-2007-A Randomized Algorithm for Large Scale Support Vector Learning</a></p>
<p>7 0.076359682 <a title="13-tfidf-7" href="./nips-2007-Collective_Inference_on_Markov_Models_for_Modeling_Bird_Migration.html">48 nips-2007-Collective Inference on Markov Models for Modeling Bird Migration</a></p>
<p>8 0.072135985 <a title="13-tfidf-8" href="./nips-2007-Random_Features_for_Large-Scale_Kernel_Machines.html">160 nips-2007-Random Features for Large-Scale Kernel Machines</a></p>
<p>9 0.071901023 <a title="13-tfidf-9" href="./nips-2007-The_Distribution_Family_of_Similarity_Distances.html">193 nips-2007-The Distribution Family of Similarity Distances</a></p>
<p>10 0.067334473 <a title="13-tfidf-10" href="./nips-2007-A_Constraint_Generation_Approach_to_Learning_Stable_Linear_Dynamical_Systems.html">4 nips-2007-A Constraint Generation Approach to Learning Stable Linear Dynamical Systems</a></p>
<p>11 0.066820599 <a title="13-tfidf-11" href="./nips-2007-Managing_Power_Consumption_and_Performance_of_Computing_Systems_Using_Reinforcement_Learning.html">124 nips-2007-Managing Power Consumption and Performance of Computing Systems Using Reinforcement Learning</a></p>
<p>12 0.064920045 <a title="13-tfidf-12" href="./nips-2007-Compressed_Regression.html">53 nips-2007-Compressed Regression</a></p>
<p>13 0.064742073 <a title="13-tfidf-13" href="./nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</a></p>
<p>14 0.062498763 <a title="13-tfidf-14" href="./nips-2007-Estimating_divergence_functionals_and_the_likelihood_ratio_by_penalized_convex_risk_minimization.html">82 nips-2007-Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization</a></p>
<p>15 0.061895791 <a title="13-tfidf-15" href="./nips-2007-Learning_the_structure_of_manifolds_using_random_projections.html">116 nips-2007-Learning the structure of manifolds using random projections</a></p>
<p>16 0.061685845 <a title="13-tfidf-16" href="./nips-2007-SpAM%3A_Sparse_Additive_Models.html">179 nips-2007-SpAM: Sparse Additive Models</a></p>
<p>17 0.059616327 <a title="13-tfidf-17" href="./nips-2007-Cluster_Stability_for_Finite_Samples.html">46 nips-2007-Cluster Stability for Finite Samples</a></p>
<p>18 0.059399977 <a title="13-tfidf-18" href="./nips-2007-A_learning_framework_for_nearest_neighbor_search.html">16 nips-2007-A learning framework for nearest neighbor search</a></p>
<p>19 0.055589296 <a title="13-tfidf-19" href="./nips-2007-Catching_Change-points_with_Lasso.html">43 nips-2007-Catching Change-points with Lasso</a></p>
<p>20 0.05430856 <a title="13-tfidf-20" href="./nips-2007-Learning_with_Transformation_Invariant_Kernels.html">118 nips-2007-Learning with Transformation Invariant Kernels</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.174), (1, 0.001), (2, -0.058), (3, 0.074), (4, -0.039), (5, -0.002), (6, 0.008), (7, 0.033), (8, -0.12), (9, 0.102), (10, 0.015), (11, 0.108), (12, 0.045), (13, 0.007), (14, 0.085), (15, -0.007), (16, 0.059), (17, 0.014), (18, -0.028), (19, -0.034), (20, 0.039), (21, -0.036), (22, 0.026), (23, -0.09), (24, 0.084), (25, -0.073), (26, 0.078), (27, -0.053), (28, -0.004), (29, -0.01), (30, 0.055), (31, 0.036), (32, -0.036), (33, 0.141), (34, 0.103), (35, -0.077), (36, -0.016), (37, -0.085), (38, 0.093), (39, -0.048), (40, -0.041), (41, -0.166), (42, 0.143), (43, -0.12), (44, 0.103), (45, -0.057), (46, -0.116), (47, 0.13), (48, 0.102), (49, -0.151)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97165036 <a title="13-lsi-1" href="./nips-2007-A_Unified_Near-Optimal_Estimator_For_Dimension_Reduction_in_%24l_%5Calpha%24_%28%240%3C%5Calpha%5Cleq_2%24%29_Using_Stable_Random_Projections.html">13 nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</a></p>
<p>Author: Ping Li, Trevor J. Hastie</p><p>Abstract: Many tasks (e.g., clustering) in machine learning only require the lα distances instead of the original data. For dimension reductions in the lα norm (0 < α ≤ 2), the method of stable random projections can efﬁciently compute the lα distances in massive datasets (e.g., the Web or massive data streams) in one pass of the data. The estimation task for stable random projections has been an interesting topic. We propose a simple estimator based on the fractional power of the samples (projected data), which is surprisingly near-optimal in terms of the asymptotic variance. In fact, it achieves the Cram´ r-Rao bound when α = 2 and α = 0+. This e new result will be useful when applying stable random projections to distancebased clustering, classiﬁcations, kernels, massive data streams etc.</p><p>2 0.57595778 <a title="13-lsi-2" href="./nips-2007-Statistical_Analysis_of_Semi-Supervised_Regression.html">186 nips-2007-Statistical Analysis of Semi-Supervised Regression</a></p>
<p>Author: Larry Wasserman, John D. Lafferty</p><p>Abstract: Semi-supervised methods use unlabeled data in addition to labeled data to construct predictors. While existing semi-supervised methods have shown some promising empirical performance, their development has been based largely based on heuristics. In this paper we study semi-supervised learning from the viewpoint of minimax theory. Our ﬁrst result shows that some common methods based on regularization using graph Laplacians do not lead to faster minimax rates of convergence. Thus, the estimators that use the unlabeled data do not have smaller risk than the estimators that use only labeled data. We then develop several new approaches that provably lead to improved performance. The statistical tools of minimax analysis are thus used to offer some new perspective on the problem of semi-supervised learning. 1</p><p>3 0.51557738 <a title="13-lsi-3" href="./nips-2007-Compressed_Regression.html">53 nips-2007-Compressed Regression</a></p>
<p>Author: Shuheng Zhou, Larry Wasserman, John D. Lafferty</p><p>Abstract: Recent research has studied the role of sparsity in high dimensional regression and signal reconstruction, establishing theoretical limits for recovering sparse models from sparse data. In this paper we study a variant of this problem where the original n input variables are compressed by a random linear transformation to m n examples in p dimensions, and establish conditions under which a sparse linear model can be successfully recovered from the compressed data. A primary motivation for this compression procedure is to anonymize the data and preserve privacy by revealing little information about the original data. We characterize the number of random projections that are required for 1 -regularized compressed regression to identify the nonzero coefﬁcients in the true model with probability approaching one, a property called “sparsistence.” In addition, we show that 1 -regularized compressed regression asymptotically predicts as well as an oracle linear model, a property called “persistence.” Finally, we characterize the privacy properties of the compression procedure in information-theoretic terms, establishing upper bounds on the rate of information communicated between the compressed and uncompressed data that decay to zero. 1</p><p>4 0.49821615 <a title="13-lsi-4" href="./nips-2007-Testing_for_Homogeneity_with_Kernel_Fisher_Discriminant_Analysis.html">192 nips-2007-Testing for Homogeneity with Kernel Fisher Discriminant Analysis</a></p>
<p>Author: Moulines Eric, Francis R. Bach, Zaïd Harchaoui</p><p>Abstract: We propose to investigate test statistics for testing homogeneity based on kernel Fisher discriminant analysis. Asymptotic null distributions under null hypothesis are derived, and consistency against ﬁxed alternatives is assessed. Finally, experimental evidence of the performance of the proposed approach on both artiﬁcial and real datasets is provided. 1</p><p>5 0.46971884 <a title="13-lsi-5" href="./nips-2007-Catching_Change-points_with_Lasso.html">43 nips-2007-Catching Change-points with Lasso</a></p>
<p>Author: Céline Levy-leduc, Zaïd Harchaoui</p><p>Abstract: We propose a new approach for dealing with the estimation of the location of change-points in one-dimensional piecewise constant signals observed in white noise. Our approach consists in reframing this task in a variable selection context. We use a penalized least-squares criterion with a 1 -type penalty for this purpose. We prove some theoretical results on the estimated change-points and on the underlying piecewise constant estimated function. Then, we explain how to implement this method in practice by combining the LAR algorithm and a reduced version of the dynamic programming algorithm and we apply it to synthetic and real data. 1</p><p>6 0.43873194 <a title="13-lsi-6" href="./nips-2007-Random_Projections_for_Manifold_Learning.html">161 nips-2007-Random Projections for Manifold Learning</a></p>
<p>7 0.41263816 <a title="13-lsi-7" href="./nips-2007-A_learning_framework_for_nearest_neighbor_search.html">16 nips-2007-A learning framework for nearest neighbor search</a></p>
<p>8 0.40903476 <a title="13-lsi-8" href="./nips-2007-Estimating_divergence_functionals_and_the_likelihood_ratio_by_penalized_convex_risk_minimization.html">82 nips-2007-Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization</a></p>
<p>9 0.39061564 <a title="13-lsi-9" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>10 0.38802183 <a title="13-lsi-10" href="./nips-2007-SpAM%3A_Sparse_Additive_Models.html">179 nips-2007-SpAM: Sparse Additive Models</a></p>
<p>11 0.38512316 <a title="13-lsi-11" href="./nips-2007-Managing_Power_Consumption_and_Performance_of_Computing_Systems_Using_Reinforcement_Learning.html">124 nips-2007-Managing Power Consumption and Performance of Computing Systems Using Reinforcement Learning</a></p>
<p>12 0.37779754 <a title="13-lsi-12" href="./nips-2007-Ultrafast_Monte_Carlo_for_Statistical_Summations.html">209 nips-2007-Ultrafast Monte Carlo for Statistical Summations</a></p>
<p>13 0.37417993 <a title="13-lsi-13" href="./nips-2007-Random_Features_for_Large-Scale_Kernel_Machines.html">160 nips-2007-Random Features for Large-Scale Kernel Machines</a></p>
<p>14 0.36138558 <a title="13-lsi-14" href="./nips-2007-Cluster_Stability_for_Finite_Samples.html">46 nips-2007-Cluster Stability for Finite Samples</a></p>
<p>15 0.360807 <a title="13-lsi-15" href="./nips-2007-A_Constraint_Generation_Approach_to_Learning_Stable_Linear_Dynamical_Systems.html">4 nips-2007-A Constraint Generation Approach to Learning Stable Linear Dynamical Systems</a></p>
<p>16 0.35229388 <a title="13-lsi-16" href="./nips-2007-A_Randomized_Algorithm_for_Large_Scale_Support_Vector_Learning.html">10 nips-2007-A Randomized Algorithm for Large Scale Support Vector Learning</a></p>
<p>17 0.35163102 <a title="13-lsi-17" href="./nips-2007-The_Distribution_Family_of_Similarity_Distances.html">193 nips-2007-The Distribution Family of Similarity Distances</a></p>
<p>18 0.34384957 <a title="13-lsi-18" href="./nips-2007-Selecting_Observations_against_Adversarial_Objectives.html">174 nips-2007-Selecting Observations against Adversarial Objectives</a></p>
<p>19 0.3317872 <a title="13-lsi-19" href="./nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</a></p>
<p>20 0.32155475 <a title="13-lsi-20" href="./nips-2007-Fitted_Q-iteration_in_continuous_action-space_MDPs.html">91 nips-2007-Fitted Q-iteration in continuous action-space MDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.031), (13, 0.025), (16, 0.018), (18, 0.011), (19, 0.411), (21, 0.075), (34, 0.034), (35, 0.033), (47, 0.093), (49, 0.013), (83, 0.108), (87, 0.019), (90, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90102601 <a title="13-lda-1" href="./nips-2007-Invariant_Common_Spatial_Patterns%3A_Alleviating_Nonstationarities_in_Brain-Computer_Interfacing.html">106 nips-2007-Invariant Common Spatial Patterns: Alleviating Nonstationarities in Brain-Computer Interfacing</a></p>
<p>Author: Benjamin Blankertz, Motoaki Kawanabe, Ryota Tomioka, Friederike Hohlefeld, Klaus-Robert Müller, Vadim V. Nikulin</p><p>Abstract: Brain-Computer Interfaces can suffer from a large variance of the subject conditions within and across sessions. For example vigilance ﬂuctuations in the individual, variable task involvement, workload etc. alter the characteristics of EEG signals and thus challenge a stable BCI operation. In the present work we aim to deﬁne features based on a variant of the common spatial patterns (CSP) algorithm that are constructed invariant with respect to such nonstationarities. We enforce invariance properties by adding terms to the denominator of a Rayleigh coefﬁcient representation of CSP such as disturbance covariance matrices from ﬂuctuations in visual processing. In this manner physiological prior knowledge can be used to shape the classiﬁcation engine for BCI. As a proof of concept we present a BCI classiﬁer that is robust to changes in the level of parietal α -activity. In other words, the EEG decoding still works when there are lapses in vigilance.</p><p>2 0.87970978 <a title="13-lda-2" href="./nips-2007-Second_Order_Bilinear_Discriminant_Analysis_for_single_trial_EEG_analysis.html">173 nips-2007-Second Order Bilinear Discriminant Analysis for single trial EEG analysis</a></p>
<p>Author: Christoforos Christoforou, Paul Sajda, Lucas C. Parra</p><p>Abstract: Traditional analysis methods for single-trial classiﬁcation of electroencephalography (EEG) focus on two types of paradigms: phase locked methods, in which the amplitude of the signal is used as the feature for classiﬁcation, e.g. event related potentials; and second order methods, in which the feature of interest is the power of the signal, e.g. event related (de)synchronization. The procedure for deciding which paradigm to use is ad hoc and is typically driven by knowledge of the underlying neurophysiology. Here we propose a principled method, based on a bilinear model, in which the algorithm simultaneously learns the best ﬁrst and second order spatial and temporal features for classiﬁcation of EEG. The method is demonstrated on simulated data as well as on EEG taken from a benchmark data used to test classiﬁcation algorithms for brain computer interfaces. 1 1.1</p><p>same-paper 3 0.81515175 <a title="13-lda-3" href="./nips-2007-A_Unified_Near-Optimal_Estimator_For_Dimension_Reduction_in_%24l_%5Calpha%24_%28%240%3C%5Calpha%5Cleq_2%24%29_Using_Stable_Random_Projections.html">13 nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</a></p>
<p>Author: Ping Li, Trevor J. Hastie</p><p>Abstract: Many tasks (e.g., clustering) in machine learning only require the lα distances instead of the original data. For dimension reductions in the lα norm (0 < α ≤ 2), the method of stable random projections can efﬁciently compute the lα distances in massive datasets (e.g., the Web or massive data streams) in one pass of the data. The estimation task for stable random projections has been an interesting topic. We propose a simple estimator based on the fractional power of the samples (projected data), which is surprisingly near-optimal in terms of the asymptotic variance. In fact, it achieves the Cram´ r-Rao bound when α = 2 and α = 0+. This e new result will be useful when applying stable random projections to distancebased clustering, classiﬁcations, kernels, massive data streams etc.</p><p>4 0.60676944 <a title="13-lda-4" href="./nips-2007-EEG-Based_Brain-Computer_Interaction%3A_Improved_Accuracy_by_Automatic_Single-Trial_Error_Detection.html">74 nips-2007-EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection</a></p>
<p>Author: Pierre Ferrez, José Millán</p><p>Abstract: Brain-computer interfaces (BCIs), as any other interaction modality based on physiological signals and body channels (e.g., muscular activity, speech and gestures), are prone to errors in the recognition of subject’s intent. An elegant approach to improve the accuracy of BCIs consists in a veriﬁcation procedure directly based on the presence of error-related potentials (ErrP) in the EEG recorded right after the occurrence of an error. Six healthy volunteer subjects with no prior BCI experience participated in a new human-robot interaction experiment where they were asked to mentally move a cursor towards a target that can be reached within a few steps using motor imagination. This experiment conﬁrms the previously reported presence of a new kind of ErrP. These “Interaction ErrP” exhibit a ﬁrst sharp negative peak followed by a positive peak and a second broader negative peak (∼290, ∼350 and ∼470 ms after the feedback, respectively). But in order to exploit these ErrP we need to detect them in each single trial using a short window following the feedback associated to the response of the classiﬁer embedded in the BCI. We have achieved an average recognition rate of correct and erroneous single trials of 81.8% and 76.2%, respectively. Furthermore, we have achieved an average recognition rate of the subject’s intent while trying to mentally drive the cursor of 73.1%. These results show that it’s possible to simultaneously extract useful information for mental control to operate a brain-actuated device as well as cognitive states such as error potentials to improve the quality of the braincomputer interaction. Finally, using a well-known inverse model (sLORETA), we show that the main focus of activity at the occurrence of the ErrP are, as expected, in the pre-supplementary motor area and in the anterior cingulate cortex. 1</p><p>5 0.50984985 <a title="13-lda-5" href="./nips-2007-Blind_channel_identification_for_speech_dereverberation_using_l1-norm_sparse_learning.html">37 nips-2007-Blind channel identification for speech dereverberation using l1-norm sparse learning</a></p>
<p>Author: Yuanqing Lin, Jingdong Chen, Youngmoo Kim, Daniel D. Lee</p><p>Abstract: Speech dereverberation remains an open problem after more than three decades of research. The most challenging step in speech dereverberation is blind channel identiﬁcation (BCI). Although many BCI approaches have been developed, their performance is still far from satisfactory for practical applications. The main difﬁculty in BCI lies in ﬁnding an appropriate acoustic model, which not only can effectively resolve solution degeneracies due to the lack of knowledge of the source, but also robustly models real acoustic environments. This paper proposes a sparse acoustic room impulse response (RIR) model for BCI, that is, an acoustic RIR can be modeled by a sparse FIR ﬁlter. Under this model, we show how to formulate the BCI of a single-input multiple-output (SIMO) system into a l1 norm regularized least squares (LS) problem, which is convex and can be solved efﬁciently with guaranteed global convergence. The sparseness of solutions is controlled by l1 -norm regularization parameters. We propose a sparse learning scheme that infers the optimal l1 -norm regularization parameters directly from microphone observations under a Bayesian framework. Our results show that the proposed approach is effective and robust, and it yields source estimates in real acoustic environments with high ﬁdelity to anechoic chamber measurements.</p><p>6 0.48966241 <a title="13-lda-6" href="./nips-2007-An_Analysis_of_Inference_with_the_Universum.html">24 nips-2007-An Analysis of Inference with the Universum</a></p>
<p>7 0.4549771 <a title="13-lda-7" href="./nips-2007-Random_Projections_for_Manifold_Learning.html">161 nips-2007-Random Projections for Manifold Learning</a></p>
<p>8 0.45270649 <a title="13-lda-8" href="./nips-2007-Measuring_Neural_Synchrony_by_Message_Passing.html">127 nips-2007-Measuring Neural Synchrony by Message Passing</a></p>
<p>9 0.44949824 <a title="13-lda-9" href="./nips-2007-Locality_and_low-dimensions_in_the_prediction_of_natural_experience_from_fMRI.html">122 nips-2007-Locality and low-dimensions in the prediction of natural experience from fMRI</a></p>
<p>10 0.44298267 <a title="13-lda-10" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>11 0.44063231 <a title="13-lda-11" href="./nips-2007-Predicting_Brain_States_from_fMRI_Data%3A_Incremental_Functional_Principal_Component_Regression.html">154 nips-2007-Predicting Brain States from fMRI Data: Incremental Functional Principal Component Regression</a></p>
<p>12 0.42548501 <a title="13-lda-12" href="./nips-2007-The_discriminant_center-surround_hypothesis_for_bottom-up_saliency.html">202 nips-2007-The discriminant center-surround hypothesis for bottom-up saliency</a></p>
<p>13 0.42502904 <a title="13-lda-13" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>14 0.42445797 <a title="13-lda-14" href="./nips-2007-SpAM%3A_Sparse_Additive_Models.html">179 nips-2007-SpAM: Sparse Additive Models</a></p>
<p>15 0.41795114 <a title="13-lda-15" href="./nips-2007-Compressed_Regression.html">53 nips-2007-Compressed Regression</a></p>
<p>16 0.41768909 <a title="13-lda-16" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>17 0.41691357 <a title="13-lda-17" href="./nips-2007-Fast_and_Scalable_Training_of_Semi-Supervised_CRFs_with_Application_to_Activity_Recognition.html">88 nips-2007-Fast and Scalable Training of Semi-Supervised CRFs with Application to Activity Recognition</a></p>
<p>18 0.41595292 <a title="13-lda-18" href="./nips-2007-Learning_the_structure_of_manifolds_using_random_projections.html">116 nips-2007-Learning the structure of manifolds using random projections</a></p>
<p>19 0.41492313 <a title="13-lda-19" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>20 0.41434249 <a title="13-lda-20" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
