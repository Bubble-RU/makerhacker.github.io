<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>87 nips-2007-Fast Variational Inference for Large-scale Internet Diagnosis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-87" href="#">nips2007-87</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>87 nips-2007-Fast Variational Inference for Large-scale Internet Diagnosis</h1>
<br/><p>Source: <a title="nips-2007-87-pdf" href="http://papers.nips.cc/paper/3292-fast-variational-inference-for-large-scale-internet-diagnosis.pdf">pdf</a></p><p>Author: Emre Kiciman, David Maltz, John C. Platt</p><p>Abstract: Web servers on the Internet need to maintain high reliability, but the cause of intermittent failures of web transactions is non-obvious. We use approximate Bayesian inference to diagnose problems with web services. This diagnosis problem is far larger than any previously attempted: it requires inference of 104 possible faults from 105 observations. Further, such inference must be performed in less than a second. Inference can be done at this speed by combining a mean-ﬁeld variational approximation and the use of stochastic gradient descent to optimize a variational cost function. We use this fast inference to diagnose a time series of anomalous HTTP requests taken from a real web service. The inference is fast enough to analyze network logs with billions of entries in a matter of hours. 1</p><p>Reference: <a title="nips-2007-87-reference" href="../nips2007_reference/nips-2007-Fast_Variational_Inference_for_Large-scale_Internet_Diagnosis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Maltz  Abstract Web servers on the Internet need to maintain high reliability, but the cause of intermittent failures of web transactions is non-obvious. [sent-4, score-0.28]
</p><p>2 We use approximate Bayesian inference to diagnose problems with web services. [sent-5, score-0.36]
</p><p>3 This diagnosis problem is far larger than any previously attempted: it requires inference of 104 possible faults from 105 observations. [sent-6, score-0.889]
</p><p>4 Further, such inference must be performed in less than a second. [sent-7, score-0.125]
</p><p>5 Inference can be done at this speed by combining a mean-ﬁeld variational approximation and the use of stochastic gradient descent to optimize a variational cost function. [sent-8, score-0.807]
</p><p>6 We use this fast inference to diagnose a time series of anomalous HTTP requests taken from a real web service. [sent-9, score-0.429]
</p><p>7 The inference is fast enough to analyze network logs with billions of entries in a matter of hours. [sent-10, score-0.386]
</p><p>8 1  Introduction  Internet content providers, such as MSN, Google and Yahoo, all depend on the correct functioning of the wide-area Internet to communicate with their users and provide their services. [sent-11, score-0.129]
</p><p>9 When these content providers lose network connectivity with some of their users, it is critical that they quickly resolve the problem, even if the failure lies outside their own systems. [sent-12, score-0.481]
</p><p>10 1 One challenge is that content providers have little direct visibility into the wide-area Internet infrastructure and the causes of user request failures. [sent-13, score-0.468]
</p><p>11 Requests may fail because of problems in the content provider’s systems or faults in the network infrastructure anywhere between the user and the content provider, including routers, proxies, ﬁrewalls, and DNS servers. [sent-14, score-0.821]
</p><p>12 Other failing requests may be due to denial of service attacks or bugs in the user’s software. [sent-15, score-0.188]
</p><p>13 To compound the diagnosis problem, these faults may be intermittent: we must use probabilistic inference to perform diagnosis, rather than using logic. [sent-16, score-0.889]
</p><p>14 Not only do popular Internet content providers receive billions of HTTP requests a week, but the number of potential causes of failure are numerous. [sent-18, score-0.561]
</p><p>15 In this paper, we show that approximate Bayesian inference scales to handle this high rate of observations and accurately estimates the underlying failure rates of such a large number of potential causes of failure. [sent-20, score-0.52]
</p><p>16 To scale Bayesian inference to Internet-sized problems, we must make several simplifying approximations. [sent-21, score-0.149]
</p><p>17 First, we introduce a bipartite graphical model using overlapping noisyORs, to model the interactions between faults and observations. [sent-22, score-0.573]
</p><p>18 Second, we use mean1  A loss of connectivity to users translates directly into lost revenue and a sullied reputation for content providers, even if the cause of the problem is a third-party network component. [sent-23, score-0.276]
</p><p>19 1  ﬁeld variational inference to map the diagnosis problem to a reasonably-sized optimization problem. [sent-24, score-0.653]
</p><p>20 Third, we further approximate the integral in the variational method. [sent-25, score-0.243]
</p><p>21 Fourth, we speed up the optimization problem using stochastic gradient descent. [sent-26, score-0.296]
</p><p>22 We describe the graphical model in Section 2, and the approximate inference in that model in Section 2. [sent-29, score-0.228]
</p><p>23 We present inference results on synthetic and real data in Section 4 and then draw conclusions. [sent-31, score-0.165]
</p><p>24 1  Previous Work  The original application of Bayesian diagnosis was medicine. [sent-33, score-0.302]
</p><p>25 One of the original diagnosis network was QMR-DT [14], a bipartite graphical model that used noisy-OR to model symptoms given diseases. [sent-34, score-0.535]
</p><p>26 Exact inference in such networks is intractable (exponential in the number of positive symptoms,[2]), so diﬀerent approximation and sampling algorithms were proposed. [sent-35, score-0.154]
</p><p>27 Shwe and Cooper proposed likelihood-weighted sampling [13], while Jaakkola and Jordan proposed using a variational approximation to unlink each input to the network [3]. [sent-36, score-0.295]
</p><p>28 More recently, researchers have applied Bayesian techniques for the diagnosis of computers and networks [1][12][16]. [sent-38, score-0.363]
</p><p>29 This work has tended to avoid inference in large networks, due to speed constraints. [sent-39, score-0.181]
</p><p>30 In contrast, we attack the enormous inference problem directly. [sent-40, score-0.174]
</p><p>31 2  Graphical model of diagnosis  Beta  Bernoulli  Noisy OR  Figure 1: The full graphical model for the diagnosis of Internet faults The initial graphical model for diagnosis is shown in Figure 1. [sent-41, score-1.482]
</p><p>32 The matrix rij is typically very sparse, because there are only a small number of possible causes for the failure of any request. [sent-44, score-0.326]
</p><p>33 The ri0 parameter models the probability of a spontaneous failure without any known cause. [sent-45, score-0.116]
</p><p>34 The rij are set by elicitation of probabilities from an expert. [sent-46, score-0.153]
</p><p>35 All of these underlying causes are modeled independently for each request, because possible faults in the system can be intermittent. [sent-49, score-0.545]
</p><p>36 Each of the Bernoulli variables Dij depends on an underlying continuous fault rate variable Fj ∈ [0, 1]: d  P (Dij |Fj = µj ) = µj ij (1 − µj )1−dij ,  (2)  where µj is the probability of a fault manifesting at any time. [sent-50, score-0.592]
</p><p>37 We model the Fj as independent Beta distributions, one for each fault: p(Fj = µj ) =  1 α0 −1 j (1 0 , β 0 ) µj B(αj j  0  − µj )βj −1 ,  (3)  where B is the beta function. [sent-51, score-0.131]
</p><p>38 The fan-out for each of these fault rates can be diﬀerent: some of these fault rates are connected to many observations, while less common ones are connected to fewer. [sent-52, score-0.61]
</p><p>39 Our goal is to model the posterior distribution P (F |V ) in order to identify hidden faults and track them through time. [sent-53, score-0.536]
</p><p>40 The model is now completely analogous to the QMR-DT mode [14], but instead of the noisy-OR combining binary random variables, they combine rate variables: P (Vi = fail|Fj = µj ) = 1 − (1 − ri0 )  (1 − rij µj ). [sent-58, score-0.183]
</p><p>41 1  Approximations to make inference tractable  In order to scale inference up to 104 hidden variables, and 105 observations, we choose a simple, robust approximate inference algorithm: mean-ﬁeld variational inference [4]. [sent-61, score-0.796]
</p><p>42 Meanﬁeld variational inference approximates the posterior P (F |V ) with a factorized distribution. [sent-62, score-0.367]
</p><p>43 For inferring fault rates, we choose to approximate P with a product of beta distributions q(Fj |V ) =  Q(F |V ) = j  j  1 α −1 µ j (1 − µj )βj −1 . [sent-63, score-0.445]
</p><p>44 B(αj , βj ) j 3  (5)  Mean-ﬁeld variational inference maximizes a lower bound on the evidence of the model: max L =  Q(µ|V ) log  α,β  P (V |µ)p(µ) Q(µ|V )  dµ. [sent-64, score-0.357]
</p><p>45 (6)  This integral can be broken into two terms: a cross-entropy between the approximate posterior and the prior, and an expected log-likelihood of the observations: max L = −  Q(µ|V ) log  α,β  Q(µ|V ) dµ + log P (V |F ) p(µ)  Q  . [sent-65, score-0.197]
</p><p>46 The second term then becomes the sum of a set of log likelihoods, one per observation: L(Vi ) =  log 1 − (1 − ri0 ) log(1 − ri0 ) +  j  j [1  − rij αj /(αj + βj )]  log[1 − rij αj /(αj + βj )]  if Vi = 1 (failure); if Vi = 0 (success). [sent-69, score-0.376]
</p><p>47 (9)  For the Internet diagnosis case, the MF(0) approximation is reasonable: we expect the posterior distribution to be concentrated around its mean, due to the large amount of data that is available. [sent-70, score-0.347]
</p><p>48 (10)  i  Variational inference by stochastic gradient descent  In order to apply unconstrained optimization algorithms to minimize (10), we need transform the variables: only positive αj and βj are valid, so we parameterize them by αj = eaj ,  βj = ebj . [sent-73, score-0.486]
</p><p>49 and the gradient computation becomes  ∂C ∂DKL (qj ||pj ) = αj  − ∂aj ∂αj j  i  (11)  ∂L(Vi )  . [sent-74, score-0.127]
</p><p>50 Note that this gradient computation can be quite computationally expensive, given that i sums over all of the observations. [sent-76, score-0.127]
</p><p>51 For Internet diagnosis, we can decompose the observation stream into blocks, where the size of the block is determined by how quickly the underlying rates of faults change, and how ﬁnely we want to sample those rates. [sent-77, score-0.63]
</p><p>52 We typically use blocks of 100,000 observations, which can make the computation of the gradient expensive. [sent-78, score-0.168]
</p><p>53 Further, we repeat the inference over and over again, on thousands of blocks of data: we prefer a fast optimization procedure over a highly accurate one. [sent-79, score-0.22]
</p><p>54 Therefore, we investigated the use of stochastic gradient descent for optimizing the variational cost function. [sent-80, score-0.554]
</p><p>55 The details of stochastic gradient descent are shown in Algorithm 1. [sent-83, score-0.332]
</p><p>56 For example, the sign of a single L(Vi ) gradient term depends only on the sign of Vi . [sent-85, score-0.127]
</p><p>57 In order to reduce the noise in the estimate, we use momentum [15]: we exponentially smooth the gradient with a ﬁrst-order ﬁlter before applying it to the state variables. [sent-86, score-0.224]
</p><p>58 99), in order to both react quickly to changes in the fault rate and to smooth out noise. [sent-90, score-0.327]
</p><p>59 Stochastic gradient descent can be used as a purely on-line method (where each data point is seen only once), setting the “number of epochs” in Algorithm 1 to 1. [sent-91, score-0.248]
</p><p>60 1  Other possible approaches  We considered and tested several other approaches to solving the approximate inference problem. [sent-94, score-0.171]
</p><p>61 Jaakkola and Jordan propose a variational inference method for bipartite noisy-OR networks [3], where one variational parameter is introduced to unlink one observation from the network. [sent-95, score-0.689]
</p><p>62 We typically have far more observations than possible faults: this previous approach would have forced us to solve very large optimization problems (with 100,000 parameters). [sent-96, score-0.112]
</p><p>63 We originally optimized the variational cost function (10) with both BFGS and the trustregion algorithm in the Matlab optimization toolbox. [sent-98, score-0.251]
</p><p>64 This turned out to be far worse than stochastic gradient descent. [sent-99, score-0.211]
</p><p>65 We report on the L-BFGS performance, below: it is within 4x the speed of the stochastic gradient descent. [sent-101, score-0.267]
</p><p>66 5  We experimented with Metropolis-Hastings to sample from the posterior, using a Gaussian random walk in (aj , bj ). [sent-102, score-0.133]
</p><p>67 In the Internet diagnosis network, the fan-out is quite high (because a single fault aﬀects many observations). [sent-105, score-0.57]
</p><p>68 Finally, we did not try the idea of learning to predict the posterior from the observations by sampling from the generative model and learning the reverse mapping [7]. [sent-109, score-0.128]
</p><p>69 For Internet diagnosis, we do not know the structure of graphical model for a block of data ahead of time: the structure depends on the metadata for the requests in the log. [sent-110, score-0.227]
</p><p>70 4  Results  We test the approximations and optimization methods used for Internet diagnosis on both synthetic and real data. [sent-112, score-0.371]
</p><p>71 1  Synthetic data with known hidden state  Testing the accuracy of approximate inference is very diﬃcult, because, for large graphical models, the true posterior distribution is intractable. [sent-114, score-0.329]
</p><p>72 We start by generating fault rates from a prior (here, 2000 faults drawn from Beta(5e3,1)). [sent-116, score-0.767]
</p><p>73 We randomly generate connections from faults to observations, with probability 5 × 10−3 . [sent-117, score-0.462]
</p><p>74 Each connection has a strength rij drawn randomly from [0, 1]. [sent-118, score-0.153]
</p><p>75 Given that the number of observations is much larger than the number of faults, we expect that the posterior distribution should tightly cluster around the rate that generated the observations. [sent-121, score-0.158]
</p><p>76 Diﬀerence between the true rate and the mean of the approximate posterior should reﬂect inaccuracies in the estimation. [sent-122, score-0.121]
</p><p>77 There is a slight systematic bias in the stochastic gradient descent, as compared to L-BFGS. [sent-146, score-0.211]
</p><p>78 However, the improvement in speed shown in Table 1 is worth the loss of accuracy: we need inference to 6  be as fast as possible to scale to billions of samples. [sent-147, score-0.32]
</p><p>79 2  Real data from web server logs  We then tested the algorithm on real data from a major web service. [sent-155, score-0.445]
</p><p>80 Each observation consists of a success or failure of a single HTTP request. [sent-156, score-0.154]
</p><p>81 We selected 18848 possible faults that occur frequently in the dataset, including the web server that received the request, which autonomous system that originated the request, and which “user agent” (brower or robot) generated the request. [sent-157, score-0.775]
</p><p>82 We have been analyzing HTTP logs collected over several months with the stochastic gradient descent algorithm. [sent-158, score-0.429]
</p><p>83 5 hour window containing an anomalously high rate of failures, in order to demonstrate that our algorithm can help us understand the cause of failures based on observations in a real-world environment. [sent-160, score-0.228]
</p><p>84 We the the the  broke the time series of observations into blocks of 100,000 observations, and inferred hidden rates for each block. [sent-161, score-0.19]
</p><p>85 Thus, for stochastic gradient descent, momentum variables were carried forward from block to block. [sent-163, score-0.346]
</p><p>86 1 0  8:00 PM 8:29 PM 8:57 PM 9:26 PM 9:55 PM 10:24 PM 10:53 PM 11:21 PM  Figure 4: The inferred fault rate for two Autonomous Systems, as a function of time. [sent-173, score-0.298]
</p><p>87 In this ﬁgure, we used stochastic gradient descent and a Beta(0. [sent-176, score-0.332]
</p><p>88 The ﬁgure shows the only two faults whose probability went higher than 0. [sent-178, score-0.462]
</p><p>89 This could be due to a router that is in common between them, or perhaps an denial of service attack that originated in that city. [sent-180, score-0.232]
</p><p>90 This allows us to go through logs containing billions of entries in a matter of hours. [sent-183, score-0.187]
</p><p>91 7  5  Conclusions  This paper presents high-speed variational inference to diagnose problems on the scale of the Internet. [sent-184, score-0.419]
</p><p>92 Given observations at a web server, the diagnosis can determine whether a web server needs rebooting, whether part of the Internet is broken, or whether the web server is compatible with a browser or user agent. [sent-185, score-1.044]
</p><p>93 In order to scale inference up to Internet-sized diagnosis problems, we make several approximations. [sent-186, score-0.451]
</p><p>94 First, we use mean-ﬁeld variational inference to approximate the posterior distribution. [sent-187, score-0.413]
</p><p>95 The expected log likelihood inside of the variational cost function is approximated with the MF(0) approximation. [sent-188, score-0.257]
</p><p>96 Finally, we use stochastic gradient descent to perform the variational optimization. [sent-189, score-0.529]
</p><p>97 We are currently using variational stochastic gradient descent to analyze logs that contain billions of requests. [sent-190, score-0.716]
</p><p>98 We are not aware of any other applications of variational inference at this scale. [sent-191, score-0.322]
</p><p>99 Future publications will include conclusions of such analysis, and implications for web services and the Internet at large. [sent-192, score-0.116]
</p><p>100 Probabilistic diagnosis using a reformulation of the INTERNIST1/QMR knowledge base. [sent-298, score-0.302]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('faults', 0.462), ('diagnosis', 0.302), ('fault', 0.268), ('internet', 0.233), ('variational', 0.197), ('rij', 0.153), ('vi', 0.151), ('dij', 0.146), ('aj', 0.136), ('bj', 0.133), ('beta', 0.131), ('gradient', 0.127), ('inference', 0.125), ('providers', 0.122), ('descent', 0.121), ('pm', 0.119), ('request', 0.119), ('server', 0.116), ('web', 0.116), ('failure', 0.116), ('logs', 0.097), ('momentum', 0.097), ('requests', 0.09), ('billions', 0.09), ('content', 0.086), ('qj', 0.085), ('stochastic', 0.084), ('observations', 0.083), ('dkl', 0.081), ('zj', 0.08), ('pj', 0.078), ('diagnose', 0.073), ('provider', 0.073), ('shwe', 0.073), ('symptoms', 0.073), ('fj', 0.072), ('failures', 0.065), ('mf', 0.065), ('sgd', 0.064), ('yj', 0.06), ('causes', 0.057), ('graphical', 0.057), ('speed', 0.056), ('bipartite', 0.054), ('fail', 0.054), ('jaakkola', 0.052), ('user', 0.052), ('jordan', 0.051), ('cause', 0.05), ('network', 0.049), ('ases', 0.049), ('attack', 0.049), ('denial', 0.049), ('intermittent', 0.049), ('router', 0.049), ('unlink', 0.049), ('service', 0.049), ('connectivity', 0.048), ('approximate', 0.046), ('posterior', 0.045), ('autonomous', 0.045), ('epochs', 0.043), ('users', 0.043), ('metadata', 0.042), ('blocks', 0.041), ('synthetic', 0.04), ('eld', 0.04), ('uai', 0.039), ('observation', 0.038), ('block', 0.038), ('di', 0.038), ('rates', 0.037), ('originated', 0.036), ('broken', 0.036), ('log', 0.035), ('computers', 0.032), ('infrastructure', 0.032), ('reliability', 0.032), ('optimizer', 0.031), ('lose', 0.031), ('propagation', 0.03), ('rate', 0.03), ('nocedal', 0.03), ('bayesian', 0.029), ('networks', 0.029), ('erent', 0.029), ('quickly', 0.029), ('optimization', 0.029), ('hidden', 0.029), ('cpu', 0.027), ('compatible', 0.027), ('accuracy', 0.027), ('http', 0.027), ('belief', 0.027), ('ng', 0.027), ('underlying', 0.026), ('cost', 0.025), ('fast', 0.025), ('loopy', 0.025), ('scale', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="87-tfidf-1" href="./nips-2007-Fast_Variational_Inference_for_Large-scale_Internet_Diagnosis.html">87 nips-2007-Fast Variational Inference for Large-scale Internet Diagnosis</a></p>
<p>Author: Emre Kiciman, David Maltz, John C. Platt</p><p>Abstract: Web servers on the Internet need to maintain high reliability, but the cause of intermittent failures of web transactions is non-obvious. We use approximate Bayesian inference to diagnose problems with web services. This diagnosis problem is far larger than any previously attempted: it requires inference of 104 possible faults from 105 observations. Further, such inference must be performed in less than a second. Inference can be done at this speed by combining a mean-ﬁeld variational approximation and the use of stochastic gradient descent to optimize a variational cost function. We use this fast inference to diagnose a time series of anomalous HTTP requests taken from a real web service. The inference is fast enough to analyze network logs with billions of entries in a matter of hours. 1</p><p>2 0.12767012 <a title="87-tfidf-2" href="./nips-2007-Convex_Clustering_with_Exemplar-Based_Models.html">61 nips-2007-Convex Clustering with Exemplar-Based Models</a></p>
<p>Author: Danial Lashkari, Polina Golland</p><p>Abstract: Clustering is often formulated as the maximum likelihood estimation of a mixture model that explains the data. The EM algorithm widely used to solve the resulting optimization problem is inherently a gradient-descent method and is sensitive to initialization. The resulting solution is a local optimum in the neighborhood of the initial guess. This sensitivity to initialization presents a signiﬁcant challenge in clustering large data sets into many clusters. In this paper, we present a different approach to approximate mixture ﬁtting for clustering. We introduce an exemplar-based likelihood function that approximates the exact likelihood. This formulation leads to a convex minimization problem and an efﬁcient algorithm with guaranteed convergence to the globally optimal solution. The resulting clustering can be thought of as a probabilistic mapping of the data points to the set of exemplars that minimizes the average distance and the information-theoretic cost of mapping. We present experimental results illustrating the performance of our algorithm and its comparison with the conventional approach to mixture model clustering. 1</p><p>3 0.096013263 <a title="87-tfidf-3" href="./nips-2007-Collapsed_Variational_Inference_for_HDP.html">47 nips-2007-Collapsed Variational Inference for HDP</a></p>
<p>Author: Yee W. Teh, Kenichi Kurihara, Max Welling</p><p>Abstract: A wide variety of Dirichlet-multinomial ‘topic’ models have found interesting applications in recent years. While Gibbs sampling remains an important method of inference in such models, variational techniques have certain advantages such as easy assessment of convergence, easy optimization without the need to maintain detailed balance, a bound on the marginal likelihood, and side-stepping of issues with topic-identiﬁability. The most accurate variational technique thus far, namely collapsed variational latent Dirichlet allocation, did not deal with model selection nor did it include inference for hyperparameters. We address both issues by generalizing the technique, obtaining the ﬁrst variational algorithm to deal with the hierarchical Dirichlet process and to deal with hyperparameters of Dirichlet variables. Experiments show a signiﬁcant improvement in accuracy. 1</p><p>4 0.094413236 <a title="87-tfidf-4" href="./nips-2007-Topmoumoute_Online_Natural_Gradient_Algorithm.html">206 nips-2007-Topmoumoute Online Natural Gradient Algorithm</a></p>
<p>Author: Nicolas L. Roux, Pierre-antoine Manzagol, Yoshua Bengio</p><p>Abstract: Guided by the goal of obtaining an optimization algorithm that is both fast and yields good generalization, we study the descent direction maximizing the decrease in generalization error or the probability of not increasing generalization error. The surprising result is that from both the Bayesian and frequentist perspectives this can yield the natural gradient direction. Although that direction can be very expensive to compute we develop an efﬁcient, general, online approximation to the natural gradient descent which is suited to large scale problems. We report experimental results showing much faster convergence in computation time and in number of iterations with TONGA (Topmoumoute Online natural Gradient Algorithm) than with stochastic gradient descent, even on very large datasets.</p><p>5 0.089454323 <a title="87-tfidf-5" href="./nips-2007-Variational_Inference_for_Diffusion_Processes.html">213 nips-2007-Variational Inference for Diffusion Processes</a></p>
<p>Author: Cédric Archambeau, Manfred Opper, Yuan Shen, Dan Cornford, John S. Shawe-taylor</p><p>Abstract: Diffusion processes are a family of continuous-time continuous-state stochastic processes that are in general only partially observed. The joint estimation of the forcing parameters and the system noise (volatility) in these dynamical systems is a crucial, but non-trivial task, especially when the system is nonlinear and multimodal. We propose a variational treatment of diffusion processes, which allows us to compute type II maximum likelihood estimates of the parameters by simple gradient techniques and which is computationally less demanding than most MCMC approaches. We also show how a cheap estimate of the posterior over the parameters can be constructed based on the variational free energy. 1</p><p>6 0.085167661 <a title="87-tfidf-6" href="./nips-2007-Modeling_image_patches_with_a_directed_hierarchy_of_Markov_random_fields.html">132 nips-2007-Modeling image patches with a directed hierarchy of Markov random fields</a></p>
<p>7 0.083823226 <a title="87-tfidf-7" href="./nips-2007-Agreement-Based_Learning.html">22 nips-2007-Agreement-Based Learning</a></p>
<p>8 0.077031821 <a title="87-tfidf-8" href="./nips-2007-Structured_Learning_with_Approximate_Inference.html">187 nips-2007-Structured Learning with Approximate Inference</a></p>
<p>9 0.068980351 <a title="87-tfidf-9" href="./nips-2007-Supervised_Topic_Models.html">189 nips-2007-Supervised Topic Models</a></p>
<p>10 0.06706398 <a title="87-tfidf-10" href="./nips-2007-Variational_inference_for_Markov_jump_processes.html">214 nips-2007-Variational inference for Markov jump processes</a></p>
<p>11 0.058337048 <a title="87-tfidf-11" href="./nips-2007-Density_Estimation_under_Independent_Similarly_Distributed_Sampling_Assumptions.html">66 nips-2007-Density Estimation under Independent Similarly Distributed Sampling Assumptions</a></p>
<p>12 0.057666626 <a title="87-tfidf-12" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>13 0.057613578 <a title="87-tfidf-13" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>14 0.056523561 <a title="87-tfidf-14" href="./nips-2007-Efficient_Bayesian_Inference_for_Dynamically_Changing_Graphs.html">75 nips-2007-Efficient Bayesian Inference for Dynamically Changing Graphs</a></p>
<p>15 0.054977484 <a title="87-tfidf-15" href="./nips-2007-On_Sparsity_and_Overcompleteness_in_Image_Models.html">145 nips-2007-On Sparsity and Overcompleteness in Image Models</a></p>
<p>16 0.054191578 <a title="87-tfidf-16" href="./nips-2007-Expectation_Maximization_and_Posterior_Constraints.html">84 nips-2007-Expectation Maximization and Posterior Constraints</a></p>
<p>17 0.052148126 <a title="87-tfidf-17" href="./nips-2007-Hidden_Common_Cause_Relations_in_Relational_Learning.html">97 nips-2007-Hidden Common Cause Relations in Relational Learning</a></p>
<p>18 0.051623546 <a title="87-tfidf-18" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>19 0.04923543 <a title="87-tfidf-19" href="./nips-2007-Unsupervised_Feature_Selection_for_Accurate_Recommendation_of_High-Dimensional_Image_Data.html">211 nips-2007-Unsupervised Feature Selection for Accurate Recommendation of High-Dimensional Image Data</a></p>
<p>20 0.047070954 <a title="87-tfidf-20" href="./nips-2007-Managing_Power_Consumption_and_Performance_of_Computing_Systems_Using_Reinforcement_Learning.html">124 nips-2007-Managing Power Consumption and Performance of Computing Systems Using Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.168), (1, 0.026), (2, -0.022), (3, -0.069), (4, -0.007), (5, -0.084), (6, -0.076), (7, -0.036), (8, 0.005), (9, -0.079), (10, 0.058), (11, 0.001), (12, 0.028), (13, -0.021), (14, 0.012), (15, 0.05), (16, -0.052), (17, 0.05), (18, -0.055), (19, 0.042), (20, 0.063), (21, -0.064), (22, -0.097), (23, 0.032), (24, -0.03), (25, 0.053), (26, -0.036), (27, -0.002), (28, 0.132), (29, -0.012), (30, -0.055), (31, -0.105), (32, 0.118), (33, -0.154), (34, -0.033), (35, 0.085), (36, 0.051), (37, -0.087), (38, 0.182), (39, -0.106), (40, -0.016), (41, 0.001), (42, 0.169), (43, -0.084), (44, 0.009), (45, 0.007), (46, 0.001), (47, -0.181), (48, 0.045), (49, 0.104)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94943249 <a title="87-lsi-1" href="./nips-2007-Fast_Variational_Inference_for_Large-scale_Internet_Diagnosis.html">87 nips-2007-Fast Variational Inference for Large-scale Internet Diagnosis</a></p>
<p>Author: Emre Kiciman, David Maltz, John C. Platt</p><p>Abstract: Web servers on the Internet need to maintain high reliability, but the cause of intermittent failures of web transactions is non-obvious. We use approximate Bayesian inference to diagnose problems with web services. This diagnosis problem is far larger than any previously attempted: it requires inference of 104 possible faults from 105 observations. Further, such inference must be performed in less than a second. Inference can be done at this speed by combining a mean-ﬁeld variational approximation and the use of stochastic gradient descent to optimize a variational cost function. We use this fast inference to diagnose a time series of anomalous HTTP requests taken from a real web service. The inference is fast enough to analyze network logs with billions of entries in a matter of hours. 1</p><p>2 0.52132767 <a title="87-lsi-2" href="./nips-2007-Collapsed_Variational_Inference_for_HDP.html">47 nips-2007-Collapsed Variational Inference for HDP</a></p>
<p>Author: Yee W. Teh, Kenichi Kurihara, Max Welling</p><p>Abstract: A wide variety of Dirichlet-multinomial ‘topic’ models have found interesting applications in recent years. While Gibbs sampling remains an important method of inference in such models, variational techniques have certain advantages such as easy assessment of convergence, easy optimization without the need to maintain detailed balance, a bound on the marginal likelihood, and side-stepping of issues with topic-identiﬁability. The most accurate variational technique thus far, namely collapsed variational latent Dirichlet allocation, did not deal with model selection nor did it include inference for hyperparameters. We address both issues by generalizing the technique, obtaining the ﬁrst variational algorithm to deal with the hierarchical Dirichlet process and to deal with hyperparameters of Dirichlet variables. Experiments show a signiﬁcant improvement in accuracy. 1</p><p>3 0.48243645 <a title="87-lsi-3" href="./nips-2007-Agreement-Based_Learning.html">22 nips-2007-Agreement-Based Learning</a></p>
<p>Author: Percy Liang, Dan Klein, Michael I. Jordan</p><p>Abstract: The learning of probabilistic models with many hidden variables and nondecomposable dependencies is an important and challenging problem. In contrast to traditional approaches based on approximate inference in a single intractable model, our approach is to train a set of tractable submodels by encouraging them to agree on the hidden variables. This allows us to capture non-decomposable aspects of the data while still maintaining tractability. We propose an objective function for our approach, derive EM-style algorithms for parameter estimation, and demonstrate their effectiveness on three challenging real-world learning tasks. 1</p><p>4 0.47107047 <a title="87-lsi-4" href="./nips-2007-Topmoumoute_Online_Natural_Gradient_Algorithm.html">206 nips-2007-Topmoumoute Online Natural Gradient Algorithm</a></p>
<p>Author: Nicolas L. Roux, Pierre-antoine Manzagol, Yoshua Bengio</p><p>Abstract: Guided by the goal of obtaining an optimization algorithm that is both fast and yields good generalization, we study the descent direction maximizing the decrease in generalization error or the probability of not increasing generalization error. The surprising result is that from both the Bayesian and frequentist perspectives this can yield the natural gradient direction. Although that direction can be very expensive to compute we develop an efﬁcient, general, online approximation to the natural gradient descent which is suited to large scale problems. We report experimental results showing much faster convergence in computation time and in number of iterations with TONGA (Topmoumoute Online natural Gradient Algorithm) than with stochastic gradient descent, even on very large datasets.</p><p>5 0.46821359 <a title="87-lsi-5" href="./nips-2007-Variational_inference_for_Markov_jump_processes.html">214 nips-2007-Variational inference for Markov jump processes</a></p>
<p>Author: Manfred Opper, Guido Sanguinetti</p><p>Abstract: Markov jump processes play an important role in a large number of application domains. However, realistic systems are analytically intractable and they have traditionally been analysed using simulation based techniques, which do not provide a framework for statistical inference. We propose a mean ﬁeld approximation to perform posterior inference and parameter estimation. The approximation allows a practical solution to the inference problem, while still retaining a good degree of accuracy. We illustrate our approach on two biologically motivated systems.</p><p>6 0.46617967 <a title="87-lsi-6" href="./nips-2007-Variational_Inference_for_Diffusion_Processes.html">213 nips-2007-Variational Inference for Diffusion Processes</a></p>
<p>7 0.4175818 <a title="87-lsi-7" href="./nips-2007-Density_Estimation_under_Independent_Similarly_Distributed_Sampling_Assumptions.html">66 nips-2007-Density Estimation under Independent Similarly Distributed Sampling Assumptions</a></p>
<p>8 0.37896606 <a title="87-lsi-8" href="./nips-2007-Bundle_Methods_for_Machine_Learning.html">40 nips-2007-Bundle Methods for Machine Learning</a></p>
<p>9 0.37455776 <a title="87-lsi-9" href="./nips-2007-Feature_Selection_Methods_for_Improving_Protein_Structure_Prediction_with_Rosetta.html">89 nips-2007-Feature Selection Methods for Improving Protein Structure Prediction with Rosetta</a></p>
<p>10 0.36702794 <a title="87-lsi-10" href="./nips-2007-Convex_Clustering_with_Exemplar-Based_Models.html">61 nips-2007-Convex Clustering with Exemplar-Based Models</a></p>
<p>11 0.36104435 <a title="87-lsi-11" href="./nips-2007-Expectation_Maximization_and_Posterior_Constraints.html">84 nips-2007-Expectation Maximization and Posterior Constraints</a></p>
<p>12 0.35882288 <a title="87-lsi-12" href="./nips-2007-Structured_Learning_with_Approximate_Inference.html">187 nips-2007-Structured Learning with Approximate Inference</a></p>
<p>13 0.34625694 <a title="87-lsi-13" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>14 0.33098054 <a title="87-lsi-14" href="./nips-2007-Active_Preference_Learning_with_Discrete_Choice_Data.html">19 nips-2007-Active Preference Learning with Discrete Choice Data</a></p>
<p>15 0.33053192 <a title="87-lsi-15" href="./nips-2007-The_rat_as_particle_filter.html">203 nips-2007-The rat as particle filter</a></p>
<p>16 0.32095596 <a title="87-lsi-16" href="./nips-2007-Estimating_divergence_functionals_and_the_likelihood_ratio_by_penalized_convex_risk_minimization.html">82 nips-2007-Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization</a></p>
<p>17 0.32083809 <a title="87-lsi-17" href="./nips-2007-Continuous_Time_Particle_Filtering_for_fMRI.html">59 nips-2007-Continuous Time Particle Filtering for fMRI</a></p>
<p>18 0.31153613 <a title="87-lsi-18" href="./nips-2007-Modeling_image_patches_with_a_directed_hierarchy_of_Markov_random_fields.html">132 nips-2007-Modeling image patches with a directed hierarchy of Markov random fields</a></p>
<p>19 0.31138712 <a title="87-lsi-19" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<p>20 0.30666268 <a title="87-lsi-20" href="./nips-2007-New_Outer_Bounds_on_the_Marginal_Polytope.html">141 nips-2007-New Outer Bounds on the Marginal Polytope</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.042), (6, 0.294), (13, 0.044), (16, 0.056), (18, 0.034), (21, 0.072), (31, 0.036), (34, 0.022), (35, 0.031), (47, 0.082), (83, 0.117), (85, 0.021), (87, 0.021), (90, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7630595 <a title="87-lda-1" href="./nips-2007-Fast_Variational_Inference_for_Large-scale_Internet_Diagnosis.html">87 nips-2007-Fast Variational Inference for Large-scale Internet Diagnosis</a></p>
<p>Author: Emre Kiciman, David Maltz, John C. Platt</p><p>Abstract: Web servers on the Internet need to maintain high reliability, but the cause of intermittent failures of web transactions is non-obvious. We use approximate Bayesian inference to diagnose problems with web services. This diagnosis problem is far larger than any previously attempted: it requires inference of 104 possible faults from 105 observations. Further, such inference must be performed in less than a second. Inference can be done at this speed by combining a mean-ﬁeld variational approximation and the use of stochastic gradient descent to optimize a variational cost function. We use this fast inference to diagnose a time series of anomalous HTTP requests taken from a real web service. The inference is fast enough to analyze network logs with billions of entries in a matter of hours. 1</p><p>2 0.5265944 <a title="87-lda-2" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>Author: Maneesh Sahani, Byron M. Yu, John P. Cunningham, Krishna V. Shenoy</p><p>Abstract: Neural spike trains present challenges to analytical efforts due to their noisy, spiking nature. Many studies of neuroscientiﬁc and neural prosthetic importance rely on a smoothed, denoised estimate of the spike train’s underlying ﬁring rate. Current techniques to ﬁnd time-varying ﬁring rates require ad hoc choices of parameters, offer no conﬁdence intervals on their estimates, and can obscure potentially important single trial variability. We present a new method, based on a Gaussian Process prior, for inferring probabilistically optimal estimates of ﬁring rate functions underlying single or multiple neural spike trains. We test the performance of the method on simulated data and experimentally gathered neural spike trains, and we demonstrate improvements over conventional estimators. 1</p><p>3 0.52243876 <a title="87-lda-3" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>Author: Matthew Hoffman, Arnaud Doucet, Nando D. Freitas, Ajay Jasra</p><p>Abstract: A recently proposed formulation of the stochastic planning and control problem as one of parameter estimation for suitable artiﬁcial statistical models has led to the adoption of inference algorithms for this notoriously hard problem. At the algorithmic level, the focus has been on developing Expectation-Maximization (EM) algorithms. In this paper, we begin by making the crucial observation that the stochastic control problem can be reinterpreted as one of trans-dimensional inference. With this new interpretation, we are able to propose a novel reversible jump Markov chain Monte Carlo (MCMC) algorithm that is more efﬁcient than its EM counterparts. Moreover, it enables us to implement full Bayesian policy search, without the need for gradients and with one single Markov chain. The new approach involves sampling directly from a distribution that is proportional to the reward and, consequently, performs better than classic simulations methods in situations where the reward is a rare event.</p><p>4 0.52204341 <a title="87-lda-4" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>Author: Kai Yu, Wei Chu</p><p>Abstract: This paper aims to model relational data on edges of networks. We describe appropriate Gaussian Processes (GPs) for directed, undirected, and bipartite networks. The inter-dependencies of edges can be effectively modeled by adapting the GP hyper-parameters. The framework suggests an intimate connection between link prediction and transfer learning, which were traditionally two separate research topics. We develop an efﬁcient learning algorithm that can handle a large number of observations. The experimental results on several real-world data sets verify superior learning capacity. 1</p><p>5 0.51988721 <a title="87-lda-5" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>Author: Matthias Bethge, Philipp Berens</p><p>Abstract: Maximum entropy analysis of binary variables provides an elegant way for studying the role of pairwise correlations in neural populations. Unfortunately, these approaches suffer from their poor scalability to high dimensions. In sensory coding, however, high-dimensional data is ubiquitous. Here, we introduce a new approach using a near-maximum entropy model, that makes this type of analysis feasible for very high-dimensional data—the model parameters can be derived in closed form and sampling is easy. Therefore, our NearMaxEnt approach can serve as a tool for testing predictions from a pairwise maximum entropy model not only for low-dimensional marginals, but also for high dimensional measurements of more than thousand units. We demonstrate its usefulness by studying natural images with dichotomized pixel intensities. Our results indicate that the statistics of such higher-dimensional measurements exhibit additional structure that are not predicted by pairwise correlations, despite the fact that pairwise correlations explain the lower-dimensional marginal statistics surprisingly well up to the limit of dimensionality where estimation of the full joint distribution is feasible. 1</p><p>6 0.51972163 <a title="87-lda-6" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>7 0.51963252 <a title="87-lda-7" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>8 0.51816583 <a title="87-lda-8" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>9 0.5177182 <a title="87-lda-9" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>10 0.51729822 <a title="87-lda-10" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>11 0.51709765 <a title="87-lda-11" href="./nips-2007-People_Tracking_with_the_Laplacian_Eigenmaps_Latent_Variable_Model.html">153 nips-2007-People Tracking with the Laplacian Eigenmaps Latent Variable Model</a></p>
<p>12 0.51707315 <a title="87-lda-12" href="./nips-2007-The_Generalized_FITC_Approximation.html">195 nips-2007-The Generalized FITC Approximation</a></p>
<p>13 0.51661587 <a title="87-lda-13" href="./nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">79 nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<p>14 0.51625192 <a title="87-lda-14" href="./nips-2007-Ultrafast_Monte_Carlo_for_Statistical_Summations.html">209 nips-2007-Ultrafast Monte Carlo for Statistical Summations</a></p>
<p>15 0.51541817 <a title="87-lda-15" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<p>16 0.51283151 <a title="87-lda-16" href="./nips-2007-Selecting_Observations_against_Adversarial_Objectives.html">174 nips-2007-Selecting Observations against Adversarial Objectives</a></p>
<p>17 0.511168 <a title="87-lda-17" href="./nips-2007-Probabilistic_Matrix_Factorization.html">158 nips-2007-Probabilistic Matrix Factorization</a></p>
<p>18 0.5110386 <a title="87-lda-18" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>19 0.51053631 <a title="87-lda-19" href="./nips-2007-Predictive_Matrix-Variate_t_Models.html">156 nips-2007-Predictive Matrix-Variate t Models</a></p>
<p>20 0.50979507 <a title="87-lda-20" href="./nips-2007-Reinforcement_Learning_in_Continuous_Action_Spaces_through_Sequential_Monte_Carlo_Methods.html">168 nips-2007-Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
