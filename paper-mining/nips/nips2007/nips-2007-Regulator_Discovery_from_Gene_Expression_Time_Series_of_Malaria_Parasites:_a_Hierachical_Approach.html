<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>167 nips-2007-Regulator Discovery from Gene Expression Time Series of Malaria Parasites: a Hierachical Approach</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-167" href="#">nips2007-167</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>167 nips-2007-Regulator Discovery from Gene Expression Time Series of Malaria Parasites: a Hierachical Approach</h1>
<br/><p>Source: <a title="nips-2007-167-pdf" href="http://papers.nips.cc/paper/3362-regulator-discovery-from-gene-expression-time-series-of-malaria-parasites-a-hierachical-approach.pdf">pdf</a></p><p>Author: José M. Hernández-lobato, Tjeerd Dijkstra, Tom Heskes</p><p>Abstract: We introduce a hierarchical Bayesian model for the discovery of putative regulators from gene expression data only. The hierarchy incorporates the knowledge that there are just a few regulators that by themselves only regulate a handful of genes. This is implemented through a so-called spike-and-slab prior, a mixture of Gaussians with different widths, with mixing weights from a hierarchical Bernoulli model. For efﬁcient inference we implemented expectation propagation. Running the model on a malaria parasite data set, we found four genes with signiﬁcant homology to transcription factors in an amoebe, one RNA regulator and three genes of unknown function (out of the top ten genes considered).</p><p>Reference: <a title="nips-2007-167-reference" href="../nips2007_reference/nips-2007-Regulator_Discovery_from_Gene_Expression_Time_Series_of_Malaria_Parasites%3A_a_Hierachical_Approach_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 nl  Abstract We introduce a hierarchical Bayesian model for the discovery of putative regulators from gene expression data only. [sent-8, score-0.877]
</p><p>2 The hierarchy incorporates the knowledge that there are just a few regulators that by themselves only regulate a handful of genes. [sent-9, score-0.225]
</p><p>3 This is implemented through a so-called spike-and-slab prior, a mixture of Gaussians with different widths, with mixing weights from a hierarchical Bernoulli model. [sent-10, score-0.061]
</p><p>4 Running the model on a malaria parasite data set, we found four genes with signiﬁcant homology to transcription factors in an amoebe, one RNA regulator and three genes of unknown function (out of the top ten genes considered). [sent-12, score-1.856]
</p><p>5 Especially the elucidation of regulatory networks underlying gene expression has lead to a cornucopia of approaches: see [1] for review. [sent-14, score-0.6]
</p><p>6 Here we focus on one aspect of network elucidation, the identiﬁcation of the regulators of the causative agent of severe malaria, Plasmodium falciparum. [sent-15, score-0.225]
</p><p>7 Several properties of the parasite necessitate a tailored algorithm for regulator identiﬁcation: • In most species gene regulation takes place at the ﬁrst stage of gene expression when a DNA template is transcribed into mRNA. [sent-16, score-1.433]
</p><p>8 This transcriptional control is mediated by speciﬁc transcription factors. [sent-17, score-0.328]
</p><p>9 Few speciﬁc transcription factors have been identiﬁed in Plasmodium based on sequence homology with other species [2, 3]. [sent-18, score-0.4]
</p><p>10 This could be due to Plasmodium possessing a unique set of transcription factors or due to other mechanisms of gene regulation, e. [sent-19, score-0.694]
</p><p>11 • Compared with yeast, gene expression in Plasmodium is hardly changed by perturbations e. [sent-22, score-0.515]
</p><p>12 The biological interpretation of this ﬁnding is that the parasite is so narrowly adapted to its environment inside a red blood cell that it follows a stereotyped gene expression program. [sent-25, score-0.676]
</p><p>13 From a machine learning point of view, this ﬁnding means that network elucidation techniques relying on perturbations of gene expression cannot be used. [sent-26, score-0.566]
</p><p>14 • Similar to yeast [5], data for three different strains of the parasite with time series of gene expression are publicly available [6]. [sent-27, score-0.807]
</p><p>15 These assay all of Plasmodium’s 5,600 genes for about 50 time points. [sent-28, score-0.317]
</p><p>16 In contrast to yeast, there are no ChIP-chip data available and fewer then ten transcription factor binding motifs are known. [sent-29, score-0.453]
</p><p>17 1  Together, these properties point to a vector autoregressive model making use of the gene expression time series. [sent-30, score-0.515]
</p><p>18 2 The model We start with a semi-realistic model of transcription based on Michaelis-Menten kinetics [1] and subsequently simplify to obtain a linear model. [sent-33, score-0.272]
</p><p>19 The activator is thought to bind to DNA motifs upstream of the transcription start site and binds RNA polymerase which reads the DNA template to produce an mRNA transcript. [sent-36, score-0.478]
</p><p>20 We proceed with several simpliﬁcations: • aj (t)  Kj : activator concentration is low;  • p(t) = p0 is constant; •  dz(t) dt  ≈  z(t+Δ)−z(t) Δ  with Δ the sampling period;  • Δ ≈ τz : sampling period roughly equal to transcript life time. [sent-39, score-0.252]
</p><p>21 This is a linear model for gene expression level given the expression levels of a set of activators. [sent-41, score-0.648]
</p><p>22 1 A Bayesian model for sparse linear regression Let y be a vector with the log expression of the target gene and X = (x1 , . [sent-44, score-0.663]
</p><p>23 , xN ) a matrix whose columns contain the log expression of the candidate regulators. [sent-47, score-0.188]
</p><p>24 We specify an inverse gamma (IG) prior for σ 2 so that P(σ 2 ) = IG(σ 2 , ν/2, νλ/2), where λ is a prior estimate of σ 2 and ν is the sample size associated with that estimate. [sent-57, score-0.078]
</p><p>25 That is, we introduce binary latent variables γi , with γi = 1 if xi takes part in the regression of y and γi = 0 otherwise. [sent-59, score-0.085]
</p><p>26 Given γ, the prior on β then reads N  N  P(βi |γi ) =  P(β|γ) = i=1  N (βi , 0, v1 )γi N (βi , 0, v0 )1−γi ,  i=1  where N (x, μ, σ 2 ) denotes a Gaussian density with mean μ and variance σ 2 evaluated at x. [sent-60, score-0.073]
</p><p>27 (3)  i=1  Unfortunately, this posterior distribution cannot be computed exactly if the number N of candidate genes is larger than 25. [sent-69, score-0.372]
</p><p>28 2 A hierarchical model for gene regulation In the section above we made use of the prior information that a target gene is typically regulated by a small number of regulators. [sent-72, score-1.175]
</p><p>29 We have not yet made use of the prior information that a regulator typically regulates more than one gene. [sent-73, score-0.245]
</p><p>30 We incorporate this information by a hierarchical extension of our previous model. [sent-74, score-0.061]
</p><p>31 We introduce a vector τ of binary latent variables where τi = 1 if gene i is a regulator and τi = 0 otherwise. [sent-75, score-0.615]
</p><p>32 (4)  i=1  In this hierarchical model, γ is a matrix of binary latent variables where γj,i = 1 if gene i takes part in the regression of gene j and γj,i = 0 otherwise. [sent-77, score-0.91]
</p><p>33 The relationship between regulators and regulatees suggests that P(γj,i = 1|τi = 1) should be bigger than P(γj,i = 1|τi = 0) and thus w1 > w0 . [sent-78, score-0.268]
</p><p>34 Matrix β contains regression coefﬁcients where βj,i is the regression coefﬁcient between the expression of gene i and the delayed expression of gene j. [sent-79, score-1.146]
</p><p>35 Hyperparameter w represents the prior 2 probability of any gene being a regulator and the elements σj of the vector σ 2 contain the variance of the noise in each of the N regressions. [sent-80, score-0.627]
</p><p>36 We can identify the genes more likely to be regulators by means of the posterior distribution P(τ |X). [sent-83, score-0.542]
</p><p>37 Compared with the sparse linear regression model we expanded the number of latent variables from O(N ) to O(N 2 ). [sent-84, score-0.141]
</p><p>38 3  w0  w  τi  γji w1  λj  v0  σj  βji  v1  νj xjt+1 xit  Figure 1: The hierarchical model for gene regulation. [sent-86, score-0.443]
</p><p>39 elements can be expressed as a product of terms n  n+1  P(yi |xi , θ)P(θ) =  P(θ, D) = i=1  ti (θ) ,  (5)  i=1  where tn+1 (θ) = P(θ) is the prior distribution for θ and ti (θ) = P(yi |xi , θ) for i = 1, . [sent-94, score-0.479]
</p><p>40 Expectation propagation proceeds to approximate (5) with a product of simpler terms n+1  n+1  ti (θ) ≈ i=1  ˜ ti (θ) = Q(θ) ,  (6)  i=1  ˜ where all the term approximations ti are restricted to belong to the same family F of exponential distributions, but they do not have to integrate 1. [sent-98, score-0.831]
</p><p>41 Each term approximation ti is chosen so that ˜ ˜ tj (θ) = ti (θ)Q\i (θ)  ˜ Q(θ) = ti (θ) j=i  is as close as possible to  ˜ tj (θ) = ti (θ)Q\i (θ) ,  ti (θ) j=i  in terms of the direct Kullback-Leibler (K-L) divergence. [sent-100, score-1.136]
</p><p>42 Initialize the term approximations ti and Q to be uniform. [sent-102, score-0.336]
</p><p>43 Repeat until all t ˜ ˜ (a) Choose a ti to reﬁne and remove it from Q to get Q\i (e. [sent-104, score-0.22]
</p><p>44 \i ˜ ˜ (b) Update the term ti so that it minimizes the K-L divergence between ti Q and ti Q\i . [sent-107, score-0.696]
</p><p>45 The optimization problem in step (b) is solved by matching sufﬁcient statistics between a distribu˜ tion Q within the F family and ti Q\i , the new ti is then equal to Q /Q\i . [sent-109, score-0.44]
</p><p>46 1 EP for sparse linear regression The application of EP to the models of Section 2 introduces some nontrivial technicalities. [sent-114, score-0.114]
</p><p>47 We approximate P(γ, β, σ 2 , y|X) for sparse linear regression by means of a factorized exponential distribution: P(γ, β, σ 2 , y|X) ≈  N  Bern(γi , qi )N (βi , μi , si ) IG(σ 2 , a, b) ≡ Q(γ, β, σ 2 ) ,  i=1  4  (7)  where {qi , μi , si : i = 1, . [sent-116, score-0.175]
</p><p>48 Note that in the approximation Q(γ, β, σ 2 ) all the components of the vectors γ and β and the variable σ2 are considered to be n independent; this allows the approximation of P(γ|y, X) by i=1 Bern(γi , qi ). [sent-120, score-0.065]
</p><p>49 Such density appears in (3) as a product of T + N terms (not counting the priors) which correspond to the ti terms in (5). [sent-122, score-0.22]
</p><p>50 This way, we have T + N term approximations with the same form as (7) and which ˜ correspond to the term approximations ti in (6). [sent-123, score-0.452]
</p><p>51 The complexity is O(T N ) per iteration, because updating any of the ﬁrst T term approximations requires N operations. [sent-124, score-0.116]
</p><p>52 , N } of the Gaussians in the term approximations, we approximate a Student’s t-distribution by means of a Gaussian distribution with the same mean and variance. [sent-130, score-0.064]
</p><p>53 To achieve this, we have to perform two approximations like the one stated above. [sent-134, score-0.08]
</p><p>54 In order to improve convergence, we re-update all the N last term approximations each time one of the ﬁrst T term approximations is updated. [sent-137, score-0.232]
</p><p>55 The posterior probability P(τ |X) that indicates which genes are more likely to be regulators can then N be approximated by i=1 Bern(τi , ti ). [sent-149, score-0.762]
</p><p>56 It is trivial to adapt the EP algorithm used in the sparse linear regression model to this new case: the terms to be approximated are the same as before except for the new N (N − 1) terms for the prior on γ. [sent-151, score-0.153]
</p><p>57 As in the previous section and in order to improve convergence, we re-update all the N (N − 1) term approximations corresponding to the prior on β each time N of the N (T − 1) term approximations corresponding to regressions are updated. [sent-152, score-0.305]
</p><p>58 In order to reduce memory requirements, we associate all the N (N − 1) terms for the prior on β into a single term, which we can do because they are independent so that we only store in memory one term approximation instead of N (N − 1). [sent-153, score-0.075]
</p><p>59 We also group the N (N − 1) terms for the prior on γ into N independent terms and the N (T − 1) terms for the regressions into T − 1 independent terms. [sent-154, score-0.073]
</p><p>60 This indicates that it is feasible to analyze data sets which contain the expression pattern of thousands of genes. [sent-156, score-0.133]
</p><p>61 In the experiments for sparse linear regression we ﬁxed the hyperparameters in (3) so that ν = 3, λ is the sample variance of the target vector y, v1 = 1, δ = N −1 , v0 is chosen according to (2) and w = N −1 . [sent-159, score-0.196]
</p><p>62 In the experiment for gene regulation we ﬁxed the hyperparameters in (4) so that w = (N − 1)−1 , νi = 3 and λi is the sample variance of the vector xi , w1 = 10−1 (N − 1)−1 , w0 = 10−2 (N − 1)−1 , v1 = 1, δ = 0. [sent-160, score-0.599]
</p><p>63 , x6000 ∼ N (0, 32 I) candidate vectors and a target vector y = x1 − x2 + 0. [sent-169, score-0.121]
</p><p>64 , x500 candidate vectors so that xi = y + εi for i = 2, . [sent-182, score-0.087]
</p><p>65 Note that all the candidate vectors are highly correlated with each other and with the target vector. [sent-189, score-0.121]
</p><p>66 This is what happens in gene expression data sets where many genes show similar expression patterns. [sent-190, score-0.965]
</p><p>67 However, w1 obtained the highest value on 54 of the runs and it was among the three ws with highest value on 87 of the runs. [sent-195, score-0.09]
</p><p>68 Both techniques produced results that are statistically indistinguishable (the approximations obtained through EP fall within the variation of the MCMC method), for EP within a fraction of the time of MCMC. [sent-197, score-0.08]
</p><p>69 2 Gene regulation In this experiment we set T = 50 and generated a vector z with T + 1 values from a sinusoid. [sent-199, score-0.169]
</p><p>70 We ran the EP algorithm for gene regulation over 100 different realizations of x1 , . [sent-222, score-0.551]
</p><p>71 The algorithm assigned t1 the highest value on 33 of the runs and x1 was ranked among the top ﬁve on 74 of the runs. [sent-226, score-0.076]
</p><p>72 This indicates that the EP algorithm can successfully detect small differences in correlations and should be able to ﬁnd new regulators in real microarray data. [sent-227, score-0.265]
</p><p>73 The ﬁrst is a yeast cell-cycle data set from [5] which is commonly used as a benchmark for regulator discovery. [sent-229, score-0.326]
</p><p>74 The yeast cdc15 data set contains 23 measurements of 6178 genes. [sent-232, score-0.162]
</p><p>75 We singled out 751 genes which met a minimum criterion for cell cycle regulation [5]. [sent-233, score-0.556]
</p><p>76 The top ten genes with the highest values for τ along with their annotation from the Saccharomyces Genome database are listed in table 5: the top two genes are speciﬁc transcription factors and IOC2 is associated with transcription regulation. [sent-234, score-1.418]
</p><p>77 As 4% of the yeast genome is associated with transcription the probability of this occurring by chance is 0. [sent-235, score-0.461]
</p><p>78 However, although the result is statistically signiﬁcant, we were disappointed to ﬁnd none of the known cell-cycle regulators (like ACE2, FKH* or SWI*) among the top ten. [sent-237, score-0.256]
</p><p>79 0  Expression  Gene PF11_321 Genes positively regulated Genes negatively regulated  −1. [sent-244, score-0.272]
</p><p>80 The vector x1 contains the expression of a regulator which would determine the expressions in x2 , . [sent-249, score-0.37]
</p><p>81 Right: Expressions of gene PF11 321 (black) and the 100 genes which are more likely to be regulated by it (light and dark grey). [sent-253, score-0.807]
</p><p>82 Two clusters of positively and negatively regulated genes can be appreciated. [sent-254, score-0.481]
</p><p>83 We singled out 751 genes who showed the highest variation as quantiﬁed by the interquartile range of the expression measurements. [sent-257, score-0.533]
</p><p>84 The top ten genes with the highest values for τ along with their annotation from PlasmoDB are listed in table 5. [sent-258, score-0.486]
</p><p>85 Recalling the motivation for our approach, the paucity of known transcription factors, we cannot expect to ﬁnd many annotated regulators in PlasmoDB version 5. [sent-259, score-0.497]
</p><p>86 These hits were the highest scoring ones outside of the genus Plasmodium. [sent-262, score-0.193]
</p><p>87 We ﬁnd four genes with a large identity to transcription factors in Dictyostelium (a recently sequenced social amoebe) and one annotated helicase which typically functions in post-transcriptional regulation. [sent-263, score-0.767]
</p><p>88 Interestingly three genes have no known function and could be regulators. [sent-264, score-0.317]
</p><p>89 rank  standard name  annotation or selected BLASTP hits  1 2 3 4 5 6 7 8 9 10  PFC0950c PF11 0321 PFI1210w MAL6P1. [sent-265, score-0.119]
</p><p>90 Somewhat disappointing, we found only one putative regulator (a helicase) among the top ten genes for Dd2. [sent-270, score-0.672]
</p><p>91 Our main contributions are: a hierarchical model to discover regulators, a tractable algorithm for fast approximate inference in models with many interacting variables, and the application to malaria. [sent-272, score-0.089]
</p><p>92 Arguably most related is the hierarchical model in [15]. [sent-273, score-0.061]
</p><p>93 The covariates in this model are a dozen external variables, coding experimental conditions, instead of the hundreds of expression levels of other genes as in our model. [sent-274, score-0.45]
</p><p>94 Furthermore, the prior in [15] enforces sparsity on the “columns” of β to implement the idea that some genes are not inﬂuenced by any of the experimental conditions. [sent-275, score-0.399]
</p><p>95 Comparative genomics of transcriptional control in the human malaria parasite Plasmodium falciparum. [sent-292, score-0.346]
</p><p>96 Discovery of the principal speciﬁc transcription factors of apicomplexa and their implication for the evolution of the ap2-integrase dna binding domains. [sent-302, score-0.509]
</p><p>97 Comprehensive identiﬁcation of cell cycle-regulated genes of the yeast Saccharomyces cerevisiae by microarray hybridization. [sent-322, score-0.509]
</p><p>98 Comparative whole genome transcriptome analysis of three Plasmodium falciparum strains. [sent-333, score-0.069]
</p><p>99 Bayesian sparse hidden components analysis for transcription regulation networks. [sent-343, score-0.497]
</p><p>100 Vannucci, editors, Bayesian u inference for gene expression and proteomics. [sent-397, score-0.515]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gene', 0.382), ('genes', 0.317), ('transcription', 0.272), ('plasmodium', 0.237), ('regulators', 0.225), ('ti', 0.22), ('regulator', 0.206), ('ep', 0.191), ('bern', 0.188), ('regulation', 0.169), ('ig', 0.154), ('expression', 0.133), ('malaria', 0.129), ('parasite', 0.129), ('yeast', 0.12), ('blastp', 0.108), ('dictyostelium', 0.108), ('regulated', 0.108), ('dna', 0.101), ('binding', 0.096), ('activator', 0.086), ('approximations', 0.08), ('putative', 0.076), ('genus', 0.075), ('genome', 0.069), ('gata', 0.065), ('helicase', 0.065), ('plasmodb', 0.065), ('hierarchical', 0.061), ('regression', 0.058), ('mcmc', 0.057), ('homology', 0.056), ('mrna', 0.056), ('transcriptional', 0.056), ('sparse', 0.056), ('candidate', 0.055), ('tf', 0.053), ('rna', 0.051), ('elucidation', 0.051), ('annotation', 0.051), ('hyperparameters', 0.048), ('highest', 0.045), ('bayesian', 0.044), ('aj', 0.044), ('concentration', 0.044), ('sparsity', 0.043), ('amoebe', 0.043), ('leiden', 0.043), ('madrid', 0.043), ('motifs', 0.043), ('polymerase', 0.043), ('regulatees', 0.043), ('sequenced', 0.043), ('slab', 0.043), ('strains', 0.043), ('transferase', 0.043), ('ten', 0.042), ('measurements', 0.042), ('hits', 0.041), ('factors', 0.04), ('life', 0.04), ('microarray', 0.04), ('prior', 0.039), ('mn', 0.038), ('singled', 0.038), ('saccharomyces', 0.038), ('transcript', 0.038), ('nijmegen', 0.038), ('dubious', 0.038), ('george', 0.038), ('term', 0.036), ('regressions', 0.034), ('regulatory', 0.034), ('heskes', 0.034), ('reads', 0.034), ('target', 0.034), ('qi', 0.033), ('cell', 0.032), ('genomics', 0.032), ('species', 0.032), ('molecular', 0.032), ('vectors', 0.032), ('outside', 0.032), ('expressions', 0.031), ('bioinformatics', 0.031), ('top', 0.031), ('dz', 0.03), ('nucleic', 0.03), ('acids', 0.03), ('identity', 0.03), ('reading', 0.029), ('netherlands', 0.029), ('negatively', 0.029), ('approximate', 0.028), ('vn', 0.027), ('grey', 0.027), ('positively', 0.027), ('latent', 0.027), ('propagation', 0.027), ('name', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="167-tfidf-1" href="./nips-2007-Regulator_Discovery_from_Gene_Expression_Time_Series_of_Malaria_Parasites%3A_a_Hierachical_Approach.html">167 nips-2007-Regulator Discovery from Gene Expression Time Series of Malaria Parasites: a Hierachical Approach</a></p>
<p>Author: José M. Hernández-lobato, Tjeerd Dijkstra, Tom Heskes</p><p>Abstract: We introduce a hierarchical Bayesian model for the discovery of putative regulators from gene expression data only. The hierarchy incorporates the knowledge that there are just a few regulators that by themselves only regulate a handful of genes. This is implemented through a so-called spike-and-slab prior, a mixture of Gaussians with different widths, with mixing weights from a hierarchical Bernoulli model. For efﬁcient inference we implemented expectation propagation. Running the model on a malaria parasite data set, we found four genes with signiﬁcant homology to transcription factors in an amoebe, one RNA regulator and three genes of unknown function (out of the top ten genes considered).</p><p>2 0.08655639 <a title="167-tfidf-2" href="./nips-2007-Robust_Regression_with_Twinned_Gaussian_Processes.html">170 nips-2007-Robust Regression with Twinned Gaussian Processes</a></p>
<p>Author: Andrew Naish-guzman, Sean Holden</p><p>Abstract: We propose a Gaussian process (GP) framework for robust inference in which a GP prior on the mixing weights of a two-component noise model augments the standard process over latent function values. This approach is a generalization of the mixture likelihood used in traditional robust GP regression, and a specialization of the GP mixture models suggested by Tresp [1] and Rasmussen and Ghahramani [2]. The value of this restriction is in its tractable expectation propagation updates, which allow for faster inference and model selection, and better convergence than the standard mixture. An additional beneﬁt over the latter method lies in our ability to incorporate knowledge of the noise domain to inﬂuence predictions, and to recover with the predictive distribution information about the outlier distribution via the gating process. The model has asymptotic complexity equal to that of conventional robust methods, but yields more conﬁdent predictions on benchmark problems than classical heavy-tailed models and exhibits improved stability for data with clustered corruptions, for which they fail altogether. We show further how our approach can be used without adjustment for more smoothly heteroscedastic data, and suggest how it could be extended to more general noise models. We also address similarities with the work of Goldberg et al. [3].</p><p>3 0.077637009 <a title="167-tfidf-3" href="./nips-2007-Variational_inference_for_Markov_jump_processes.html">214 nips-2007-Variational inference for Markov jump processes</a></p>
<p>Author: Manfred Opper, Guido Sanguinetti</p><p>Abstract: Markov jump processes play an important role in a large number of application domains. However, realistic systems are analytically intractable and they have traditionally been analysed using simulation based techniques, which do not provide a framework for statistical inference. We propose a mean ﬁeld approximation to perform posterior inference and parameter estimation. The approximation allows a practical solution to the inference problem, while still retaining a good degree of accuracy. We illustrate our approach on two biologically motivated systems.</p><p>4 0.073258296 <a title="167-tfidf-4" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: It has been shown that adapting a dictionary of basis functions to the statistics of natural images so as to maximize sparsity in the coefﬁcients results in a set of dictionary elements whose spatial properties resemble those of V1 (primary visual cortex) receptive ﬁelds. However, the resulting sparse coefﬁcients still exhibit pronounced statistical dependencies, thus violating the independence assumption of the sparse coding model. Here, we propose a model that attempts to capture the dependencies among the basis function coefﬁcients by including a pairwise coupling term in the prior over the coefﬁcient activity states. When adapted to the statistics of natural images, the coupling terms learn a combination of facilitatory and inhibitory interactions among neighboring basis functions. These learned interactions may offer an explanation for the function of horizontal connections in V1 in terms of a prior over natural images.</p><p>5 0.068872146 <a title="167-tfidf-5" href="./nips-2007-Heterogeneous_Component_Analysis.html">96 nips-2007-Heterogeneous Component Analysis</a></p>
<p>Author: Shigeyuki Oba, Motoaki Kawanabe, Klaus-Robert Müller, Shin Ishii</p><p>Abstract: In bioinformatics it is often desirable to combine data from various measurement sources and thus structured feature vectors are to be analyzed that possess different intrinsic blocking characteristics (e.g., different patterns of missing values, observation noise levels, effective intrinsic dimensionalities). We propose a new machine learning tool, heterogeneous component analysis (HCA), for feature extraction in order to better understand the factors that underlie such complex structured heterogeneous data. HCA is a linear block-wise sparse Bayesian PCA based not only on a probabilistic model with block-wise residual variance terms but also on a Bayesian treatment of a block-wise sparse factor-loading matrix. We study various algorithms that implement our HCA concept extracting sparse heterogeneous structure by obtaining common components for the blocks and speciﬁc components within each block. Simulations on toy and bioinformatics data underline the usefulness of the proposed structured matrix factorization concept. 1</p><p>6 0.062954649 <a title="167-tfidf-6" href="./nips-2007-Hierarchical_Penalization.html">99 nips-2007-Hierarchical Penalization</a></p>
<p>7 0.062378377 <a title="167-tfidf-7" href="./nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">79 nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<p>8 0.059552781 <a title="167-tfidf-8" href="./nips-2007-Variational_Inference_for_Diffusion_Processes.html">213 nips-2007-Variational Inference for Diffusion Processes</a></p>
<p>9 0.057996552 <a title="167-tfidf-9" href="./nips-2007-On_Sparsity_and_Overcompleteness_in_Image_Models.html">145 nips-2007-On Sparsity and Overcompleteness in Image Models</a></p>
<p>10 0.057148471 <a title="167-tfidf-10" href="./nips-2007-Bayesian_Agglomerative_Clustering_with_Coalescents.html">31 nips-2007-Bayesian Agglomerative Clustering with Coalescents</a></p>
<p>11 0.056042433 <a title="167-tfidf-11" href="./nips-2007-The_Generalized_FITC_Approximation.html">195 nips-2007-The Generalized FITC Approximation</a></p>
<p>12 0.054609463 <a title="167-tfidf-12" href="./nips-2007-The_Distribution_Family_of_Similarity_Distances.html">193 nips-2007-The Distribution Family of Similarity Distances</a></p>
<p>13 0.053937551 <a title="167-tfidf-13" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>14 0.052602015 <a title="167-tfidf-14" href="./nips-2007-Learning_Monotonic_Transformations_for_Classification.html">112 nips-2007-Learning Monotonic Transformations for Classification</a></p>
<p>15 0.050157379 <a title="167-tfidf-15" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>16 0.049047358 <a title="167-tfidf-16" href="./nips-2007-On_Ranking_in_Survival_Analysis%3A_Bounds_on_the_Concordance_Index.html">144 nips-2007-On Ranking in Survival Analysis: Bounds on the Concordance Index</a></p>
<p>17 0.048709132 <a title="167-tfidf-17" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>18 0.047930773 <a title="167-tfidf-18" href="./nips-2007-Predictive_Matrix-Variate_t_Models.html">156 nips-2007-Predictive Matrix-Variate t Models</a></p>
<p>19 0.046700101 <a title="167-tfidf-19" href="./nips-2007-Catching_Up_Faster_in_Bayesian_Model_Selection_and_Model_Averaging.html">44 nips-2007-Catching Up Faster in Bayesian Model Selection and Model Averaging</a></p>
<p>20 0.046438929 <a title="167-tfidf-20" href="./nips-2007-A_New_View_of_Automatic_Relevance_Determination.html">8 nips-2007-A New View of Automatic Relevance Determination</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.149), (1, 0.032), (2, 0.003), (3, -0.033), (4, -0.023), (5, -0.03), (6, -0.063), (7, -0.027), (8, -0.048), (9, -0.068), (10, 0.003), (11, 0.005), (12, 0.041), (13, -0.012), (14, 0.041), (15, 0.04), (16, -0.004), (17, 0.087), (18, -0.034), (19, -0.112), (20, 0.017), (21, 0.007), (22, 0.001), (23, 0.009), (24, -0.028), (25, 0.056), (26, -0.02), (27, 0.116), (28, -0.118), (29, -0.026), (30, 0.018), (31, -0.001), (32, 0.031), (33, -0.061), (34, 0.041), (35, -0.023), (36, 0.008), (37, 0.07), (38, -0.074), (39, 0.058), (40, -0.108), (41, -0.051), (42, 0.004), (43, 0.126), (44, 0.053), (45, 0.205), (46, -0.017), (47, 0.001), (48, 0.12), (49, -0.188)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93313944 <a title="167-lsi-1" href="./nips-2007-Regulator_Discovery_from_Gene_Expression_Time_Series_of_Malaria_Parasites%3A_a_Hierachical_Approach.html">167 nips-2007-Regulator Discovery from Gene Expression Time Series of Malaria Parasites: a Hierachical Approach</a></p>
<p>Author: José M. Hernández-lobato, Tjeerd Dijkstra, Tom Heskes</p><p>Abstract: We introduce a hierarchical Bayesian model for the discovery of putative regulators from gene expression data only. The hierarchy incorporates the knowledge that there are just a few regulators that by themselves only regulate a handful of genes. This is implemented through a so-called spike-and-slab prior, a mixture of Gaussians with different widths, with mixing weights from a hierarchical Bernoulli model. For efﬁcient inference we implemented expectation propagation. Running the model on a malaria parasite data set, we found four genes with signiﬁcant homology to transcription factors in an amoebe, one RNA regulator and three genes of unknown function (out of the top ten genes considered).</p><p>2 0.59111387 <a title="167-lsi-2" href="./nips-2007-Bayesian_Agglomerative_Clustering_with_Coalescents.html">31 nips-2007-Bayesian Agglomerative Clustering with Coalescents</a></p>
<p>Author: Yee W. Teh, Daniel J. Hsu, Hal Daume</p><p>Abstract: We introduce a new Bayesian model for hierarchical clustering based on a prior over trees called Kingman’s coalescent. We develop novel greedy and sequential Monte Carlo inferences which operate in a bottom-up agglomerative fashion. We show experimentally the superiority of our algorithms over the state-of-the-art, and demonstrate our approach in document clustering and phylolinguistics. 1</p><p>3 0.53930384 <a title="167-lsi-3" href="./nips-2007-Heterogeneous_Component_Analysis.html">96 nips-2007-Heterogeneous Component Analysis</a></p>
<p>Author: Shigeyuki Oba, Motoaki Kawanabe, Klaus-Robert Müller, Shin Ishii</p><p>Abstract: In bioinformatics it is often desirable to combine data from various measurement sources and thus structured feature vectors are to be analyzed that possess different intrinsic blocking characteristics (e.g., different patterns of missing values, observation noise levels, effective intrinsic dimensionalities). We propose a new machine learning tool, heterogeneous component analysis (HCA), for feature extraction in order to better understand the factors that underlie such complex structured heterogeneous data. HCA is a linear block-wise sparse Bayesian PCA based not only on a probabilistic model with block-wise residual variance terms but also on a Bayesian treatment of a block-wise sparse factor-loading matrix. We study various algorithms that implement our HCA concept extracting sparse heterogeneous structure by obtaining common components for the blocks and speciﬁc components within each block. Simulations on toy and bioinformatics data underline the usefulness of the proposed structured matrix factorization concept. 1</p><p>4 0.48895139 <a title="167-lsi-4" href="./nips-2007-Variational_inference_for_Markov_jump_processes.html">214 nips-2007-Variational inference for Markov jump processes</a></p>
<p>Author: Manfred Opper, Guido Sanguinetti</p><p>Abstract: Markov jump processes play an important role in a large number of application domains. However, realistic systems are analytically intractable and they have traditionally been analysed using simulation based techniques, which do not provide a framework for statistical inference. We propose a mean ﬁeld approximation to perform posterior inference and parameter estimation. The approximation allows a practical solution to the inference problem, while still retaining a good degree of accuracy. We illustrate our approach on two biologically motivated systems.</p><p>5 0.48336306 <a title="167-lsi-5" href="./nips-2007-A_New_View_of_Automatic_Relevance_Determination.html">8 nips-2007-A New View of Automatic Relevance Determination</a></p>
<p>Author: David P. Wipf, Srikantan S. Nagarajan</p><p>Abstract: Automatic relevance determination (ARD) and the closely-related sparse Bayesian learning (SBL) framework are effective tools for pruning large numbers of irrelevant features leading to a sparse explanatory subset. However, popular update rules used for ARD are either difﬁcult to extend to more general problems of interest or are characterized by non-ideal convergence properties. Moreover, it remains unclear exactly how ARD relates to more traditional MAP estimation-based methods for learning sparse representations (e.g., the Lasso). This paper furnishes an alternative means of expressing the ARD cost function using auxiliary functions that naturally addresses both of these issues. First, the proposed reformulation of ARD can naturally be optimized by solving a series of re-weighted 1 problems. The result is an efﬁcient, extensible algorithm that can be implemented using standard convex programming toolboxes and is guaranteed to converge to a local minimum (or saddle point). Secondly, the analysis reveals that ARD is exactly equivalent to performing standard MAP estimation in weight space using a particular feature- and noise-dependent, non-factorial weight prior. We then demonstrate that this implicit prior maintains several desirable advantages over conventional priors with respect to feature selection. Overall these results suggest alternative cost functions and update procedures for selecting features and promoting sparse solutions in a variety of general situations. In particular, the methodology readily extends to handle problems such as non-negative sparse coding and covariance component estimation. 1</p><p>6 0.4404912 <a title="167-lsi-6" href="./nips-2007-On_Ranking_in_Survival_Analysis%3A_Bounds_on_the_Concordance_Index.html">144 nips-2007-On Ranking in Survival Analysis: Bounds on the Concordance Index</a></p>
<p>7 0.42688176 <a title="167-lsi-7" href="./nips-2007-Predictive_Matrix-Variate_t_Models.html">156 nips-2007-Predictive Matrix-Variate t Models</a></p>
<p>8 0.42314434 <a title="167-lsi-8" href="./nips-2007-Scan_Strategies_for_Meteorological_Radars.html">171 nips-2007-Scan Strategies for Meteorological Radars</a></p>
<p>9 0.42240804 <a title="167-lsi-9" href="./nips-2007-Robust_Regression_with_Twinned_Gaussian_Processes.html">170 nips-2007-Robust Regression with Twinned Gaussian Processes</a></p>
<p>10 0.41923061 <a title="167-lsi-10" href="./nips-2007-Augmented_Functional_Time_Series_Representation_and_Forecasting_with_Gaussian_Processes.html">28 nips-2007-Augmented Functional Time Series Representation and Forecasting with Gaussian Processes</a></p>
<p>11 0.41329324 <a title="167-lsi-11" href="./nips-2007-Continuous_Time_Particle_Filtering_for_fMRI.html">59 nips-2007-Continuous Time Particle Filtering for fMRI</a></p>
<p>12 0.4019441 <a title="167-lsi-12" href="./nips-2007-Variational_Inference_for_Diffusion_Processes.html">213 nips-2007-Variational Inference for Diffusion Processes</a></p>
<p>13 0.38757911 <a title="167-lsi-13" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>14 0.38599366 <a title="167-lsi-14" href="./nips-2007-The_Distribution_Family_of_Similarity_Distances.html">193 nips-2007-The Distribution Family of Similarity Distances</a></p>
<p>15 0.37615299 <a title="167-lsi-15" href="./nips-2007-The_Infinite_Gamma-Poisson_Feature_Model.html">196 nips-2007-The Infinite Gamma-Poisson Feature Model</a></p>
<p>16 0.37483814 <a title="167-lsi-16" href="./nips-2007-Modeling_Natural_Sounds_with_Modulation_Cascade_Processes.html">130 nips-2007-Modeling Natural Sounds with Modulation Cascade Processes</a></p>
<p>17 0.36944759 <a title="167-lsi-17" href="./nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">79 nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<p>18 0.35896081 <a title="167-lsi-18" href="./nips-2007-The_Generalized_FITC_Approximation.html">195 nips-2007-The Generalized FITC Approximation</a></p>
<p>19 0.35676074 <a title="167-lsi-19" href="./nips-2007-Receding_Horizon_Differential_Dynamic_Programming.html">163 nips-2007-Receding Horizon Differential Dynamic Programming</a></p>
<p>20 0.35255149 <a title="167-lsi-20" href="./nips-2007-Simulated_Annealing%3A_Rigorous_finite-time_guarantees_for_optimization_on_continuous_domains.html">178 nips-2007-Simulated Annealing: Rigorous finite-time guarantees for optimization on continuous domains</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.059), (5, 0.031), (13, 0.021), (16, 0.055), (18, 0.023), (21, 0.059), (31, 0.027), (34, 0.022), (35, 0.02), (47, 0.086), (49, 0.01), (50, 0.296), (83, 0.107), (85, 0.017), (87, 0.025), (90, 0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78488451 <a title="167-lda-1" href="./nips-2007-Regulator_Discovery_from_Gene_Expression_Time_Series_of_Malaria_Parasites%3A_a_Hierachical_Approach.html">167 nips-2007-Regulator Discovery from Gene Expression Time Series of Malaria Parasites: a Hierachical Approach</a></p>
<p>Author: José M. Hernández-lobato, Tjeerd Dijkstra, Tom Heskes</p><p>Abstract: We introduce a hierarchical Bayesian model for the discovery of putative regulators from gene expression data only. The hierarchy incorporates the knowledge that there are just a few regulators that by themselves only regulate a handful of genes. This is implemented through a so-called spike-and-slab prior, a mixture of Gaussians with different widths, with mixing weights from a hierarchical Bernoulli model. For efﬁcient inference we implemented expectation propagation. Running the model on a malaria parasite data set, we found four genes with signiﬁcant homology to transcription factors in an amoebe, one RNA regulator and three genes of unknown function (out of the top ten genes considered).</p><p>2 0.71243477 <a title="167-lda-2" href="./nips-2007-Predicting_Brain_States_from_fMRI_Data%3A_Incremental_Functional_Principal_Component_Regression.html">154 nips-2007-Predicting Brain States from fMRI Data: Incremental Functional Principal Component Regression</a></p>
<p>Author: Sennay Ghebreab, Arnold Smeulders, Pieter Adriaans</p><p>Abstract: We propose a method for reconstruction of human brain states directly from functional neuroimaging data. The method extends the traditional multivariate regression analysis of discretized fMRI data to the domain of stochastic functional measurements, facilitating evaluation of brain responses to complex stimuli and boosting the power of functional imaging. The method searches for sets of voxel time courses that optimize a multivariate functional linear model in terms of R2 statistic. Population based incremental learning is used to identify spatially distributed brain responses to complex stimuli without attempting to localize function ﬁrst. Variation in hemodynamic lag across brain areas and among subjects is taken into account by voxel-wise non-linear registration of stimulus pattern to fMRI data. Application of the method on an international test benchmark for prediction of naturalistic stimuli from new and unknown fMRI data shows that the method successfully uncovers spatially distributed parts of the brain that are highly predictive of a given stimulus. 1</p><p>3 0.50687236 <a title="167-lda-3" href="./nips-2007-SpAM%3A_Sparse_Additive_Models.html">179 nips-2007-SpAM: Sparse Additive Models</a></p>
<p>Author: Han Liu, Larry Wasserman, John D. Lafferty, Pradeep K. Ravikumar</p><p>Abstract: We present a new class of models for high-dimensional nonparametric regression and classiﬁcation called sparse additive models (SpAM). Our methods combine ideas from sparse linear modeling and additive nonparametric regression. We derive a method for ﬁtting the models that is effective even when the number of covariates is larger than the sample size. A statistical analysis of the properties of SpAM is given together with empirical results on synthetic and real data, showing that SpAM can be effective in ﬁtting sparse nonparametric models in high dimensional data. 1</p><p>4 0.49039122 <a title="167-lda-4" href="./nips-2007-EEG-Based_Brain-Computer_Interaction%3A_Improved_Accuracy_by_Automatic_Single-Trial_Error_Detection.html">74 nips-2007-EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection</a></p>
<p>Author: Pierre Ferrez, José Millán</p><p>Abstract: Brain-computer interfaces (BCIs), as any other interaction modality based on physiological signals and body channels (e.g., muscular activity, speech and gestures), are prone to errors in the recognition of subject’s intent. An elegant approach to improve the accuracy of BCIs consists in a veriﬁcation procedure directly based on the presence of error-related potentials (ErrP) in the EEG recorded right after the occurrence of an error. Six healthy volunteer subjects with no prior BCI experience participated in a new human-robot interaction experiment where they were asked to mentally move a cursor towards a target that can be reached within a few steps using motor imagination. This experiment conﬁrms the previously reported presence of a new kind of ErrP. These “Interaction ErrP” exhibit a ﬁrst sharp negative peak followed by a positive peak and a second broader negative peak (∼290, ∼350 and ∼470 ms after the feedback, respectively). But in order to exploit these ErrP we need to detect them in each single trial using a short window following the feedback associated to the response of the classiﬁer embedded in the BCI. We have achieved an average recognition rate of correct and erroneous single trials of 81.8% and 76.2%, respectively. Furthermore, we have achieved an average recognition rate of the subject’s intent while trying to mentally drive the cursor of 73.1%. These results show that it’s possible to simultaneously extract useful information for mental control to operate a brain-actuated device as well as cognitive states such as error potentials to improve the quality of the braincomputer interaction. Finally, using a well-known inverse model (sLORETA), we show that the main focus of activity at the occurrence of the ErrP are, as expected, in the pre-supplementary motor area and in the anterior cingulate cortex. 1</p><p>5 0.48782754 <a title="167-lda-5" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>Author: Maneesh Sahani, Byron M. Yu, John P. Cunningham, Krishna V. Shenoy</p><p>Abstract: Neural spike trains present challenges to analytical efforts due to their noisy, spiking nature. Many studies of neuroscientiﬁc and neural prosthetic importance rely on a smoothed, denoised estimate of the spike train’s underlying ﬁring rate. Current techniques to ﬁnd time-varying ﬁring rates require ad hoc choices of parameters, offer no conﬁdence intervals on their estimates, and can obscure potentially important single trial variability. We present a new method, based on a Gaussian Process prior, for inferring probabilistically optimal estimates of ﬁring rate functions underlying single or multiple neural spike trains. We test the performance of the method on simulated data and experimentally gathered neural spike trains, and we demonstrate improvements over conventional estimators. 1</p><p>6 0.48345989 <a title="167-lda-6" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>7 0.48328853 <a title="167-lda-7" href="./nips-2007-The_Generalized_FITC_Approximation.html">195 nips-2007-The Generalized FITC Approximation</a></p>
<p>8 0.48269641 <a title="167-lda-8" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>9 0.48123905 <a title="167-lda-9" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>10 0.48122627 <a title="167-lda-10" href="./nips-2007-Boosting_the_Area_under_the_ROC_Curve.html">39 nips-2007-Boosting the Area under the ROC Curve</a></p>
<p>11 0.47977665 <a title="167-lda-11" href="./nips-2007-People_Tracking_with_the_Laplacian_Eigenmaps_Latent_Variable_Model.html">153 nips-2007-People Tracking with the Laplacian Eigenmaps Latent Variable Model</a></p>
<p>12 0.47869307 <a title="167-lda-12" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>13 0.47824094 <a title="167-lda-13" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>14 0.47764888 <a title="167-lda-14" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>15 0.47660875 <a title="167-lda-15" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>16 0.47571349 <a title="167-lda-16" href="./nips-2007-Selecting_Observations_against_Adversarial_Objectives.html">174 nips-2007-Selecting Observations against Adversarial Objectives</a></p>
<p>17 0.47457731 <a title="167-lda-17" href="./nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">79 nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<p>18 0.4744308 <a title="167-lda-18" href="./nips-2007-Ultrafast_Monte_Carlo_for_Statistical_Summations.html">209 nips-2007-Ultrafast Monte Carlo for Statistical Summations</a></p>
<p>19 0.47428766 <a title="167-lda-19" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>20 0.47414762 <a title="167-lda-20" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
