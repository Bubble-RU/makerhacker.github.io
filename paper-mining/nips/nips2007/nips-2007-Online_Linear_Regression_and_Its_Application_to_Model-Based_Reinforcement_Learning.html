<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-148" href="#">nips2007-148</a> knowledge-graph by maker-knowledge-mining</p><h1>148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2007-148-pdf" href="http://papers.nips.cc/paper/3197-online-linear-regression-and-its-application-to-model-based-reinforcement-learning.pdf">pdf</a></p><p>Author: Alexander L. Strehl, Michael L. Littman</p><p>Abstract: We provide a provably efﬁcient algorithm for learning Markov Decision Processes (MDPs) with continuous state and action spaces in the online setting. Speciﬁcally, we take a model-based approach and show that a special type of online linear regression allows us to learn MDPs with (possibly kernalized) linearly parameterized dynamics. This result builds on Kearns and Singh’s work that provides a provably efﬁcient algorithm for ﬁnite state MDPs. Our approach is not restricted to the linear setting, and is applicable to other classes of continuous MDPs.</p><p>Reference: <a title="nips-2007-148-reference" href="../nips2007_reference/nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mdps', 0.279), ('rl', 0.25), ('xt', 0.236), ('mdp', 0.234), ('klrp', 0.22), ('kwik', 0.22), ('timestep', 0.201), ('rn', 0.181), ('policy', 0.155), ('reward', 0.15), ('ns', 0.149), ('kearn', 0.144), ('yt', 0.143), ('regress', 0.13), ('admiss', 0.126), ('polynom', 0.124), ('abbeel', 0.12), ('ith', 0.115), ('reinforc', 0.113), ('kakad', 0.11)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="148-tfidf-1" href="./nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</a></p>
<p>2 0.25031769 <a title="148-tfidf-2" href="./nips-2007-A_Game-Theoretic_Approach_to_Apprenticeship_Learning.html">5 nips-2007-A Game-Theoretic Approach to Apprenticeship Learning</a></p>
<p>3 0.22543575 <a title="148-tfidf-3" href="./nips-2007-Fitted_Q-iteration_in_continuous_action-space_MDPs.html">91 nips-2007-Fitted Q-iteration in continuous action-space MDPs</a></p>
<p>4 0.21652006 <a title="148-tfidf-4" href="./nips-2007-Reinforcement_Learning_in_Continuous_Action_Spaces_through_Sequential_Monte_Carlo_Methods.html">168 nips-2007-Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods</a></p>
<p>5 0.19137059 <a title="148-tfidf-5" href="./nips-2007-Hierarchical_Apprenticeship_Learning_with_Application_to_Quadruped_Locomotion.html">98 nips-2007-Hierarchical Apprenticeship Learning with Application to Quadruped Locomotion</a></p>
<p>6 0.18735038 <a title="148-tfidf-6" href="./nips-2007-Managing_Power_Consumption_and_Performance_of_Computing_Systems_Using_Reinforcement_Learning.html">124 nips-2007-Managing Power Consumption and Performance of Computing Systems Using Reinforcement Learning</a></p>
<p>7 0.17640361 <a title="148-tfidf-7" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>8 0.17382447 <a title="148-tfidf-8" href="./nips-2007-Incremental_Natural_Actor-Critic_Algorithms.html">102 nips-2007-Incremental Natural Actor-Critic Algorithms</a></p>
<p>9 0.17129622 <a title="148-tfidf-9" href="./nips-2007-On_higher-order_perceptron_algorithms.html">146 nips-2007-On higher-order perceptron algorithms</a></p>
<p>10 0.14939523 <a title="148-tfidf-10" href="./nips-2007-Adaptive_Online_Gradient_Descent.html">21 nips-2007-Adaptive Online Gradient Descent</a></p>
<p>11 0.14824912 <a title="148-tfidf-11" href="./nips-2007-Optimistic_Linear_Programming_gives_Logarithmic_Regret_for_Irreducible_MDPs.html">151 nips-2007-Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs</a></p>
<p>12 0.14655897 <a title="148-tfidf-12" href="./nips-2007-Stable_Dual_Dynamic_Programming.html">185 nips-2007-Stable Dual Dynamic Programming</a></p>
<p>13 0.13979374 <a title="148-tfidf-13" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>14 0.1334313 <a title="148-tfidf-14" href="./nips-2007-Competition_Adds_Complexity.html">52 nips-2007-Competition Adds Complexity</a></p>
<p>15 0.1220879 <a title="148-tfidf-15" href="./nips-2007-Discovering_Weakly-Interacting_Factors_in_a_Complex_Stochastic_Process.html">68 nips-2007-Discovering Weakly-Interacting Factors in a Complex Stochastic Process</a></p>
<p>16 0.11265115 <a title="148-tfidf-16" href="./nips-2007-Random_Sampling_of_States_in_Dynamic_Programming.html">162 nips-2007-Random Sampling of States in Dynamic Programming</a></p>
<p>17 0.10694774 <a title="148-tfidf-17" href="./nips-2007-Bayes-Adaptive_POMDPs.html">30 nips-2007-Bayes-Adaptive POMDPs</a></p>
<p>18 0.10665008 <a title="148-tfidf-18" href="./nips-2007-A_Risk_Minimization_Principle_for_a_Class_of_Parzen_Estimators.html">11 nips-2007-A Risk Minimization Principle for a Class of Parzen Estimators</a></p>
<p>19 0.10561606 <a title="148-tfidf-19" href="./nips-2007-What_makes_some_POMDP_problems_easy_to_approximate%3F.html">215 nips-2007-What makes some POMDP problems easy to approximate?</a></p>
<p>20 0.10494892 <a title="148-tfidf-20" href="./nips-2007-Hippocampal_Contributions_to_Control%3A_The_Third_Way.html">100 nips-2007-Hippocampal Contributions to Control: The Third Way</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.294), (1, -0.113), (2, -0.342), (3, 0.044), (4, 0.092), (5, 0.015), (6, 0.027), (7, 0.005), (8, -0.007), (9, -0.062), (10, 0.077), (11, 0.085), (12, 0.023), (13, 0.012), (14, 0.101), (15, -0.045), (16, -0.03), (17, -0.052), (18, 0.011), (19, -0.067), (20, -0.013), (21, -0.006), (22, 0.021), (23, 0.007), (24, -0.067), (25, 0.094), (26, 0.038), (27, -0.007), (28, 0.048), (29, 0.136), (30, 0.009), (31, 0.045), (32, -0.067), (33, -0.053), (34, -0.179), (35, 0.099), (36, 0.016), (37, -0.008), (38, -0.01), (39, 0.058), (40, 0.012), (41, -0.012), (42, 0.062), (43, -0.03), (44, 0.121), (45, -0.044), (46, 0.013), (47, 0.042), (48, -0.078), (49, -0.113)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90821701 <a title="148-lsi-1" href="./nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</a></p>
<p>2 0.8266443 <a title="148-lsi-2" href="./nips-2007-Fitted_Q-iteration_in_continuous_action-space_MDPs.html">91 nips-2007-Fitted Q-iteration in continuous action-space MDPs</a></p>
<p>3 0.73665184 <a title="148-lsi-3" href="./nips-2007-A_Game-Theoretic_Approach_to_Apprenticeship_Learning.html">5 nips-2007-A Game-Theoretic Approach to Apprenticeship Learning</a></p>
<p>4 0.69414812 <a title="148-lsi-4" href="./nips-2007-Hierarchical_Apprenticeship_Learning_with_Application_to_Quadruped_Locomotion.html">98 nips-2007-Hierarchical Apprenticeship Learning with Application to Quadruped Locomotion</a></p>
<p>5 0.670609 <a title="148-lsi-5" href="./nips-2007-Competition_Adds_Complexity.html">52 nips-2007-Competition Adds Complexity</a></p>
<p>6 0.66490096 <a title="148-lsi-6" href="./nips-2007-Managing_Power_Consumption_and_Performance_of_Computing_Systems_Using_Reinforcement_Learning.html">124 nips-2007-Managing Power Consumption and Performance of Computing Systems Using Reinforcement Learning</a></p>
<p>7 0.61898696 <a title="148-lsi-7" href="./nips-2007-On_higher-order_perceptron_algorithms.html">146 nips-2007-On higher-order perceptron algorithms</a></p>
<p>8 0.58885336 <a title="148-lsi-8" href="./nips-2007-Reinforcement_Learning_in_Continuous_Action_Spaces_through_Sequential_Monte_Carlo_Methods.html">168 nips-2007-Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods</a></p>
<p>9 0.50309658 <a title="148-lsi-9" href="./nips-2007-Stable_Dual_Dynamic_Programming.html">185 nips-2007-Stable Dual Dynamic Programming</a></p>
<p>10 0.49838835 <a title="148-lsi-10" href="./nips-2007-Incremental_Natural_Actor-Critic_Algorithms.html">102 nips-2007-Incremental Natural Actor-Critic Algorithms</a></p>
<p>11 0.49785736 <a title="148-lsi-11" href="./nips-2007-Discovering_Weakly-Interacting_Factors_in_a_Complex_Stochastic_Process.html">68 nips-2007-Discovering Weakly-Interacting Factors in a Complex Stochastic Process</a></p>
<p>12 0.49725848 <a title="148-lsi-12" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>13 0.49213284 <a title="148-lsi-13" href="./nips-2007-Adaptive_Online_Gradient_Descent.html">21 nips-2007-Adaptive Online Gradient Descent</a></p>
<p>14 0.48757496 <a title="148-lsi-14" href="./nips-2007-Optimistic_Linear_Programming_gives_Logarithmic_Regret_for_Irreducible_MDPs.html">151 nips-2007-Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs</a></p>
<p>15 0.45730758 <a title="148-lsi-15" href="./nips-2007-Random_Sampling_of_States_in_Dynamic_Programming.html">162 nips-2007-Random Sampling of States in Dynamic Programming</a></p>
<p>16 0.43299717 <a title="148-lsi-16" href="./nips-2007-The_Epoch-Greedy_Algorithm_for_Multi-armed_Bandits_with_Side_Information.html">194 nips-2007-The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information</a></p>
<p>17 0.4057205 <a title="148-lsi-17" href="./nips-2007-Temporal_Difference_Updating_without_a_Learning_Rate.html">191 nips-2007-Temporal Difference Updating without a Learning Rate</a></p>
<p>18 0.39799559 <a title="148-lsi-18" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>19 0.38829106 <a title="148-lsi-19" href="./nips-2007-Receding_Horizon_Differential_Dynamic_Programming.html">163 nips-2007-Receding Horizon Differential Dynamic Programming</a></p>
<p>20 0.37652072 <a title="148-lsi-20" href="./nips-2007-Variational_Inference_for_Diffusion_Processes.html">213 nips-2007-Variational Inference for Diffusion Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.109), (30, 0.037), (36, 0.378), (45, 0.074), (46, 0.117), (56, 0.015), (60, 0.125), (62, 0.011), (90, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.73031014 <a title="148-lda-1" href="./nips-2007-Incremental_Natural_Actor-Critic_Algorithms.html">102 nips-2007-Incremental Natural Actor-Critic Algorithms</a></p>
<p>same-paper 2 0.70717204 <a title="148-lda-2" href="./nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</a></p>
<p>3 0.65530694 <a title="148-lda-3" href="./nips-2007-Modeling_Natural_Sounds_with_Modulation_Cascade_Processes.html">130 nips-2007-Modeling Natural Sounds with Modulation Cascade Processes</a></p>
<p>4 0.64676934 <a title="148-lda-4" href="./nips-2007-A_Game-Theoretic_Approach_to_Apprenticeship_Learning.html">5 nips-2007-A Game-Theoretic Approach to Apprenticeship Learning</a></p>
<p>5 0.59549588 <a title="148-lda-5" href="./nips-2007-Reinforcement_Learning_in_Continuous_Action_Spaces_through_Sequential_Monte_Carlo_Methods.html">168 nips-2007-Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods</a></p>
<p>6 0.59212846 <a title="148-lda-6" href="./nips-2007-Optimistic_Linear_Programming_gives_Logarithmic_Regret_for_Irreducible_MDPs.html">151 nips-2007-Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs</a></p>
<p>7 0.57485908 <a title="148-lda-7" href="./nips-2007-Hierarchical_Apprenticeship_Learning_with_Application_to_Quadruped_Locomotion.html">98 nips-2007-Hierarchical Apprenticeship Learning with Application to Quadruped Locomotion</a></p>
<p>8 0.55614769 <a title="148-lda-8" href="./nips-2007-Fitted_Q-iteration_in_continuous_action-space_MDPs.html">91 nips-2007-Fitted Q-iteration in continuous action-space MDPs</a></p>
<p>9 0.54834795 <a title="148-lda-9" href="./nips-2007-Managing_Power_Consumption_and_Performance_of_Computing_Systems_Using_Reinforcement_Learning.html">124 nips-2007-Managing Power Consumption and Performance of Computing Systems Using Reinforcement Learning</a></p>
<p>10 0.54673535 <a title="148-lda-10" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>11 0.53340322 <a title="148-lda-11" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<p>12 0.52332044 <a title="148-lda-12" href="./nips-2007-Bayes-Adaptive_POMDPs.html">30 nips-2007-Bayes-Adaptive POMDPs</a></p>
<p>13 0.52092153 <a title="148-lda-13" href="./nips-2007-The_Epoch-Greedy_Algorithm_for_Multi-armed_Bandits_with_Side_Information.html">194 nips-2007-The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information</a></p>
<p>14 0.52013689 <a title="148-lda-14" href="./nips-2007-Theoretical_Analysis_of_Heuristic_Search_Methods_for_Online_POMDPs.html">204 nips-2007-Theoretical Analysis of Heuristic Search Methods for Online POMDPs</a></p>
<p>15 0.51670116 <a title="148-lda-15" href="./nips-2007-Stable_Dual_Dynamic_Programming.html">185 nips-2007-Stable Dual Dynamic Programming</a></p>
<p>16 0.51443803 <a title="148-lda-16" href="./nips-2007-Temporal_Difference_Updating_without_a_Learning_Rate.html">191 nips-2007-Temporal Difference Updating without a Learning Rate</a></p>
<p>17 0.51411384 <a title="148-lda-17" href="./nips-2007-Sequential_Hypothesis_Testing_under_Stochastic_Deadlines.html">176 nips-2007-Sequential Hypothesis Testing under Stochastic Deadlines</a></p>
<p>18 0.50724375 <a title="148-lda-18" href="./nips-2007-What_makes_some_POMDP_problems_easy_to_approximate%3F.html">215 nips-2007-What makes some POMDP problems easy to approximate?</a></p>
<p>19 0.50445861 <a title="148-lda-19" href="./nips-2007-Receding_Horizon_Differential_Dynamic_Programming.html">163 nips-2007-Receding Horizon Differential Dynamic Programming</a></p>
<p>20 0.50418329 <a title="148-lda-20" href="./nips-2007-Hippocampal_Contributions_to_Control%3A_The_Third_Way.html">100 nips-2007-Hippocampal Contributions to Control: The Third Way</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
