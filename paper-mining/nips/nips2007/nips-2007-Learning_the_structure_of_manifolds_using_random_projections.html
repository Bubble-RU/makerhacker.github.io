<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>116 nips-2007-Learning the structure of manifolds using random projections</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-116" href="#">nips2007-116</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>116 nips-2007-Learning the structure of manifolds using random projections</h1>
<br/><p>Source: <a title="nips-2007-116-pdf" href="http://papers.nips.cc/paper/3195-learning-the-structure-of-manifolds-using-random-projections.pdf">pdf</a></p><p>Author: Yoav Freund, Sanjoy Dasgupta, Mayank Kabra, Nakul Verma</p><p>Abstract: We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data. 1</p><p>Reference: <a title="nips-2007-116-reference" href="../nips2007_reference/nips-2007-Learning_the_structure_of_manifolds_using_random_projections_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In such cases data of low intrinsic dimension is embedded in a space of high extrinsic dimension. [sent-4, score-0.413]
</p><p>2 Some of this work (for instance, [2]) exploits the low intrinsic dimension to improve the convergence rate of supervised learning algorithms. [sent-13, score-0.307]
</p><p>3 In this paper, we describe a new way of modeling data that resides in RD but has lower intrinsic dimension d < D. [sent-15, score-0.299]
</p><p>4 We call this spatial data structure a random projection tree (RP tree). [sent-18, score-0.535]
</p><p>5 It can be thought of as a variant of the k-d tree that is provably manifoldadaptive. [sent-19, score-0.346]
</p><p>6 k-d trees, RP trees, and vector quantization Recall that a k-d tree [3] partitions RD into hyperrectangular cells. [sent-20, score-0.419]
</p><p>7 It is built in a recursive manner, splitting along one coordinate direction at a time. [sent-21, score-0.277]
</p><p>8 The succession of splits corresponds to a binary tree whose leaves contain the individual cells in RD . [sent-22, score-0.563]
</p><p>9 These trees are among the most widely-used methods for spatial partitioning in machine learning and computer vision. [sent-23, score-0.336]
</p><p>10 1  Figure 1: Left: A spatial partitioning of R2 induced by a k-d tree with three levels. [sent-28, score-0.425]
</p><p>11 On the left part of Figure 1 we illustrate a k-d tree for a set of vectors in R2 . [sent-31, score-0.367]
</p><p>12 The leaves of the tree partition RD into cells; given a query point q, the cell containing q is identiﬁed by traversing down the k-d tree. [sent-32, score-0.546]
</p><p>13 Each cell can be thought of as having a representative vector: its mean, depicted in the ﬁgure by a circle. [sent-33, score-0.222]
</p><p>14 The partitioning together with these mean vectors deﬁne a vector quantization (VQ) of R2 : a mapping from R2 to a ﬁnite set of representative vectors (called a “codebook” in the context of lossy compression methods). [sent-34, score-0.238]
</p><p>15 This error is closely related (in fact, proportional) to the average diameter of cells, that is, the average squared distance between pairs of points in a cell. [sent-38, score-0.286]
</p><p>16 1 As the depth of the k-d tree increases the diameter of the cells decreases and so does the VQ error. [sent-39, score-0.676]
</p><p>17 However, in high dimension, the rate of decrease of the average diameter can be very slow. [sent-40, score-0.264]
</p><p>18 In fact, as we show in the supplementary material, there are data sets in RD for which a k-d tree requires D levels in order to halve the diameter. [sent-41, score-0.468]
</p><p>19 This slow rate of decrease of cell diameter is ﬁne if D = 2 as in Figure 1, but it is disastrous if D = 1000. [sent-42, score-0.454]
</p><p>20 Constructing 1000 levels of the tree requires 21000 data points! [sent-43, score-0.426]
</p><p>21 This problem is a real one that has been observed empirically: k-d trees are prone to a curse of dimensionality. [sent-44, score-0.261]
</p><p>22 In general, k-d trees will not be able to beneﬁt from this; in fact the bad example mentioned above has intrinsic dimension d = 1. [sent-46, score-0.5]
</p><p>23 But we show that a simple variant of the k-d tree does indeed decrease cell diameters much more quickly. [sent-47, score-0.589]
</p><p>24 Instead of splitting along coordinate directions, we use randomly chosen unit vectors, and instead of splitting data exactly at the median, we use a more carefully chosen split point. [sent-48, score-0.489]
</p><p>25 We call the resulting data structure a random projection tree (Figure 1, right) and we show that it admits the following theoretical guarantee (formal statement is in the next section). [sent-49, score-0.477]
</p><p>26 Pick any cell C in the RP tree, and suppose the data in C have intrinsic dimension d. [sent-50, score-0.489]
</p><p>27 Pick a descendant cell ≥ d levels below; then with constant probability, this descendant has average diameter at most half that of C. [sent-51, score-0.565]
</p><p>28 We thus have a vector quantization construction method for which the diameter of the cells depends on the intrinsic dimension, rather than the extrinsic dimension of the data. [sent-53, score-0.729]
</p><p>29 A large part of the beneﬁt of RP trees comes from the use of random unit directions, which is rather like running k-d trees with a preprocessing step in which the data are projected into a random 1 2  This is in contrast to the max diameter, the maximum distance between two vectors in a cell. [sent-54, score-0.7]
</p><p>30 In fact, a recent experimental study of nearest neighbor algorithms [8] observes that a similar pre-processing step improves the performance of nearest neighbor schemes based on spatial data structures. [sent-57, score-0.418]
</p><p>31 The explanation we provide is based on the assumption that the data has low intrinsic dimension. [sent-59, score-0.24]
</p><p>32 Another spatial data structure based on random projections is the locality sensitive hashing scheme [6]. [sent-60, score-0.292]
</p><p>33 Manifold learning and near neighbor search The fast rate of diameter decrease in random projection trees has many consequences beyond the quality of vector quantization. [sent-61, score-0.744]
</p><p>34 In particular, the statistical theory of tree-based statistical estimators — whether used for classiﬁcation or regression — is centered around the rate of diameter decrease; for details, see for instance Chapter 20 of [7]. [sent-62, score-0.251]
</p><p>35 Thus RP trees generically exhibit faster convergence in all these contexts. [sent-63, score-0.231]
</p><p>36 If the diameter of cells is small, then it is reasonable to classify a query point according to the majority label in its cell. [sent-65, score-0.351]
</p><p>37 The classical work of Cover and Hart [5] on the Bayes risk of nearest neighbor methods applies equally to the majority vote in a small enough cell. [sent-67, score-0.197]
</p><p>38 The left ﬁgure depicts data concentrated near a one-dimensional manifold. [sent-70, score-0.155]
</p><p>39 Our goal is to partition data into small diameter regions so that the data in each region is well-approximated by its mean+PCA. [sent-72, score-0.307]
</p><p>40 Some of the data lies close to a one-dimensional manifold, some of the data spans two dimensions, and some of the data (represented by the red dot) is concentrated around a single point (a zero-dimensional manifold). [sent-74, score-0.159]
</p><p>41 In the literature, the most common way to capture this manifold structure is to create a graph in which nodes represent data points and edges connect pairs of nearby points. [sent-76, score-0.155]
</p><p>42 Once these individual cells are small enough, the data in them can be well-approximated by an afﬁne subspace, for instance that given by principal component analysis. [sent-80, score-0.271]
</p><p>43 1  The RP tree algorithm Spatial data structures  In what follows, we assume the data lie in RD , and we consider spatial data structures built by recursive binary splits. [sent-83, score-0.54]
</p><p>44 procedure C HOOSE RULE(S) comment: PCA tree version let u be the principal eigenvector of the covariance of S Rule(x) := x · u ≤ median({z · u : z ∈ S}) return (Rule) This method will do a good job of adapting to low intrinsic dimension (details omitted). [sent-87, score-0.901]
</p><p>45 First, estimating the principal eigenvector requires a signiﬁcant amount of data; recall that only about 1/2k fraction of the data winds up at a cell at level k of the tree. [sent-89, score-0.439]
</p><p>46 Second, when the extrinsic dimension is high, the amount of memory and computation required to compute the dot product between the data vectors and the eigenvectors becomes the dominant part of the computation. [sent-90, score-0.282]
</p><p>47 As each node in the tree is likely to have a different eigenvector this severely limits the feasible tree depth. [sent-91, score-0.749]
</p><p>48 We now show that using random projections overcomes these problems while maintaining the adaptivity to low intrinsic dimension. [sent-92, score-0.316]
</p><p>49 2  Random projection trees  We shall see that the key beneﬁts of PCA-based splits can be realized much more simply, by picking random directions. [sent-94, score-0.493]
</p><p>50 PCA will of course correctly identify this subspace, and a split along the principal eigenvector u will do a good job of reducing the diameter of the data. [sent-96, score-0.622]
</p><p>51 But a random direction v will also have some component in the direction of u, and splitting along the median of v will not be all that different from splitting along u. [sent-97, score-0.479]
</p><p>52 Figure 3: Intuition: a random direction is almost as good as the principal eigenvector. [sent-98, score-0.174]
</p><p>53 Also, we can use the same random projection in different places in the tree; all we need is to choose a large enough set of projections that, with high probability, there is be a good projection direction for each node in the tree. [sent-100, score-0.395]
</p><p>54 In our experience setting the number of projections equal to the depth of the tree is sufﬁcient. [sent-101, score-0.434]
</p><p>55 Thus, for a tree of depth k, we use only k projection vectors v, as opposed to 2k with a PCA tree. [sent-102, score-0.502]
</p><p>56 When preparing data to train a tree we can compute the k projection values before building the tree. [sent-103, score-0.448]
</p><p>57 For a cell containing points S, let ∆(S) be the diameter of S (the distance between the two furthest points in the set), and ∆A (S) the average diameter, that is, the 4  average distance between points of S: ∆2 (S) = A  1 |S|2  x−y  2  x,y∈S  =  2 |S|  x − mean(S) 2 . [sent-106, score-0.58]
</p><p>58 x∈S  2  We use two different types of splits: if ∆ (S) is less than c∆2 (S) (for some constant c) then we A use the hyperplane split discussed above. [sent-107, score-0.183]
</p><p>59 Otherwise, we split S into two groups based on distance from the mean. [sent-108, score-0.229]
</p><p>60 procedure C HOOSE RULE(S) comment: RP tree version if ∆2 (S) ≤ c · ∆2 (S) A  choose a random unit direction v  sort projection values: a(x) = v · x ∀x ∈ S, generating the list a1 ≤ a2 ≤ · · · ≤ an    for i = 1, . [sent-109, score-0.499]
</p><p>61 4, for instance, splitting the bottom cell at the median would lead to a messy partition, whereas the RP tree split produces two clean, connected clusters. [sent-115, score-0.877]
</p><p>62 3: The two PCA ellipses corresponding to the two cells after the ﬁrst split. [sent-119, score-0.197]
</p><p>63 5: The four PCA ellipses for the cells at the third level. [sent-121, score-0.197]
</p><p>64 As the cells get smaller, their individual PCAs reveal 1D manifold structure. [sent-123, score-0.204]
</p><p>65 Note: the ellipses are for comparison only; the RP tree algorithm does not look at them. [sent-124, score-0.409]
</p><p>66 The second type of split, based on distance from the mean of the cell, is needed to deal with cases in which the cell contains data at very different scales. [sent-125, score-0.266]
</p><p>67 If only splits by projection were allowed, then a large number of splits would be devoted to uselessly subdividing this point mass. [sent-127, score-0.368]
</p><p>68 The second type of split separates it from the rest of the data in one go. [sent-128, score-0.213]
</p><p>69 A large fraction of them might be “empty” background patches, in which case they’d fall near the center of the cell in a very tight cluster. [sent-130, score-0.251]
</p><p>70 The effect of the split is then to separate out these two clusters. [sent-132, score-0.183]
</p><p>71 3  Theoretical foundations  In analyzing RP trees, we consider a statistical notion of dimension: we say set S has local covariance dimension (d, ) if (1 − ) fraction of the variance is concentrated in a d-dimensional subspace. [sent-134, score-0.237]
</p><p>72 Deﬁnition 1 S ⊂ RD has local covariance dimension (d, ) if the largest d eigenvalues of its 2 2 2 2 2 2 covariance matrix satisfy σ1 + · · · + σd ≥ (1 − ) · (σ1 + · · · + σD ). [sent-136, score-0.206]
</p><p>73 ) Now, suppose an RP tree is built from a data set X ⊂ RD , not necessarily ﬁnite. [sent-138, score-0.395]
</p><p>74 Recall that there are two different types of splits; let’s call them splits by distance and splits by projection. [sent-139, score-0.316]
</p><p>75 Suppose an RP tree is built using data set X ⊂ RD . [sent-141, score-0.395]
</p><p>76 Consider any cell C for which X ∩ C has local covariance dimension (d, ), where < c1 . [sent-142, score-0.327]
</p><p>77 Pick a point x ∈ S ∩ C at random, and let C be the cell that contains it at the next level down. [sent-143, score-0.19]
</p><p>78 • If C is split by distance then E [∆(S ∩ C )] ≤ c2 ∆(S ∩ C). [sent-144, score-0.229]
</p><p>79 • If C is split by projection, then E ∆2 (S ∩ C ) ≤ 1 − A  c3 ∆2 (S ∩ C). [sent-145, score-0.183]
</p><p>80 As a consequence, the expected average diameter of cells is halved every O(d) levels. [sent-147, score-0.319]
</p><p>81 First of all, both splits operate on the projected data; for the second type of split (split by distance), data that fall in an interval around the median are separated from data outside that interval. [sent-151, score-0.523]
</p><p>82 Second, the tree is built in a streaming manner: that is, the data arrive one at a time, and are processed (to update the tree) and immediately discarded. [sent-152, score-0.439]
</p><p>83 This is managed by maintaining simple statistics at each internal node of the tree and updating them appropriately as the data streams by (more details in the supplementary matter). [sent-153, score-0.433]
</p><p>84 Finally, instead of choosing a new random projection in each cell, a dictionary of a few random projections is chosen at the outset. [sent-155, score-0.233]
</p><p>85 We will see that RP trees adapt well to such cases. [sent-160, score-0.231]
</p><p>86 We compare the performance of different trees according to the average VQ error they incur at various levels. [sent-172, score-0.231]
</p><p>87 We consider four types of trees: (1) k-d trees in which the coordinate for a split is chosen at random; (2) k-d trees in which at each split, the best coordinate is chosen (the one that most improves VQ error); (3) RP trees; and (4) for reference, PCA trees. [sent-173, score-0.747]
</p><p>88 In both cases, RP trees outperform both k-d tree variants and are close to the performance of PCA trees without having to explicitly compute any principal components. [sent-175, score-0.875]
</p><p>89 3  MNIST dataset  We next demonstrate RP trees on the all-familiar MNIST dataset of handwritten digits. [sent-177, score-0.321]
</p><p>90 This dataset consists of 28 × 28 grayscale images of the digits zero through nine, and is believed to have low intrinsic dimension (for instance, see [10]). [sent-178, score-0.381]
</p><p>91 Figure 6 (top) shows the ﬁrst few levels of the RP tree for the images of digit 1. [sent-180, score-0.464]
</p><p>92 The bar underneath each node shows the fraction of points going to the left and to the right, to give a sense of how balanced each split is. [sent-183, score-0.284]
</p><p>93 Alongside each mean, we also show a histogram of the 20 largest eigenvalues of the covariance matrix, which reveal how closely the data in the cell is concentrated near a low-dimensional subspace. [sent-184, score-0.388]
</p><p>94 Hence, very quickly, the cell means become good representatives of the dataset: an experimental corroboration that RP trees adapt to the low intrinsic dimension of the data. [sent-188, score-0.728]
</p><p>95 This is also brought out in Figure 6 (bottom), where the images are shown projected onto the plane deﬁned by their top two principal components. [sent-189, score-0.179]
</p><p>96 ) The left image shows how the data was split at the topmost level (dark versus light). [sent-191, score-0.257]
</p><p>97 Observe that this random cut is actually quite close to what the PCA split would have been, corroborating our earlier intuition (recall Figure 3). [sent-192, score-0.212]
</p><p>98 7  Figure 6: Top: Three levels of the RP tree for MNIST digit 1. [sent-194, score-0.435]
</p><p>99 Colors represent different cells in the RP tree, after just one split (left) or after two levels of the tree (right). [sent-196, score-0.687]
</p><p>100 A fast nearest neighbor algorithm based on a principal axis search tree. [sent-245, score-0.258]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rp', 0.526), ('tree', 0.32), ('trees', 0.231), ('diameter', 0.211), ('cell', 0.19), ('split', 0.183), ('intrinsic', 0.172), ('vq', 0.162), ('splits', 0.135), ('ree', 0.13), ('rd', 0.112), ('pca', 0.11), ('cells', 0.108), ('ake', 0.102), ('coord', 0.102), ('projection', 0.098), ('dimension', 0.097), ('manifold', 0.096), ('splitting', 0.096), ('principal', 0.093), ('neighbor', 0.092), ('hoose', 0.089), ('ellipses', 0.089), ('median', 0.088), ('projections', 0.077), ('levels', 0.076), ('extrinsic', 0.076), ('nearest', 0.073), ('rule', 0.073), ('concentrated', 0.069), ('eigenvector', 0.068), ('diego', 0.068), ('quantization', 0.065), ('spatial', 0.058), ('projected', 0.057), ('decrease', 0.053), ('aj', 0.052), ('direction', 0.052), ('dimensionality', 0.051), ('coordinate', 0.051), ('rightt', 0.051), ('partitioning', 0.047), ('vectors', 0.047), ('uc', 0.046), ('distance', 0.046), ('built', 0.045), ('dataset', 0.045), ('verma', 0.044), ('streaming', 0.044), ('topmost', 0.044), ('descendant', 0.044), ('san', 0.043), ('supplementary', 0.042), ('node', 0.041), ('comment', 0.04), ('hashing', 0.04), ('lef', 0.04), ('covariance', 0.04), ('instance', 0.04), ('mnist', 0.04), ('return', 0.039), ('digit', 0.039), ('low', 0.038), ('depth', 0.037), ('partition', 0.036), ('manner', 0.036), ('job', 0.034), ('avg', 0.034), ('thing', 0.034), ('tt', 0.034), ('partitions', 0.034), ('synthetic', 0.033), ('along', 0.033), ('dasgupta', 0.032), ('eigenvectors', 0.032), ('representative', 0.032), ('subspace', 0.032), ('majority', 0.032), ('locality', 0.031), ('pick', 0.031), ('fraction', 0.031), ('data', 0.03), ('curse', 0.03), ('colors', 0.03), ('belkin', 0.03), ('manifolds', 0.03), ('near', 0.03), ('eigenvalues', 0.029), ('images', 0.029), ('random', 0.029), ('points', 0.029), ('directions', 0.028), ('datasets', 0.028), ('sensitive', 0.027), ('recall', 0.027), ('lie', 0.027), ('variant', 0.026), ('depicts', 0.026), ('areas', 0.026), ('var', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="116-tfidf-1" href="./nips-2007-Learning_the_structure_of_manifolds_using_random_projections.html">116 nips-2007-Learning the structure of manifolds using random projections</a></p>
<p>Author: Yoav Freund, Sanjoy Dasgupta, Mayank Kabra, Nakul Verma</p><p>Abstract: We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data. 1</p><p>2 0.17306174 <a title="116-tfidf-2" href="./nips-2007-A_learning_framework_for_nearest_neighbor_search.html">16 nips-2007-A learning framework for nearest neighbor search</a></p>
<p>Author: Lawrence Cayton, Sanjoy Dasgupta</p><p>Abstract: Can we leverage learning techniques to build a fast nearest-neighbor (ANN) retrieval data structure? We present a general learning framework for the NN problem in which sample queries are used to learn the parameters of a data structure that minimize the retrieval time and/or the miss rate. We explore the potential of this novel framework through two popular NN data structures: KD-trees and the rectilinear structures employed by locality sensitive hashing. We derive a generalization theory for these data structure classes and present simple learning algorithms for both. Experimental results reveal that learning often improves on the already strong performance of these data structures. 1</p><p>3 0.17092717 <a title="116-tfidf-3" href="./nips-2007-Random_Projections_for_Manifold_Learning.html">161 nips-2007-Random Projections for Manifold Learning</a></p>
<p>Author: Chinmay Hegde, Michael Wakin, Richard Baraniuk</p><p>Abstract: We propose a novel method for linear dimensionality reduction of manifold modeled data. First, we show that with a small number M of random projections of sample points in RN belonging to an unknown K-dimensional Euclidean manifold, the intrinsic dimension (ID) of the sample set can be estimated to high accuracy. Second, we rigorously prove that using only this set of random projections, we can estimate the structure of the underlying manifold. In both cases, the number of random projections required is linear in K and logarithmic in N , meaning that K < M ≪ N . To handle practical situations, we develop a greedy algorithm to estimate the smallest size of the projection space required to perform manifold learning. Our method is particularly relevant in distributed sensing systems and leads to signiﬁcant potential savings in data acquisition, storage and transmission costs.</p><p>4 0.14403895 <a title="116-tfidf-4" href="./nips-2007-Efficient_Bayesian_Inference_for_Dynamically_Changing_Graphs.html">75 nips-2007-Efficient Bayesian Inference for Dynamically Changing Graphs</a></p>
<p>Author: Ozgur Sumer, Umut Acar, Alexander T. Ihler, Ramgopal R. Mettu</p><p>Abstract: Motivated by stochastic systems in which observed evidence and conditional dependencies between states of the network change over time, and certain quantities of interest (marginal distributions, likelihood estimates etc.) must be updated, we study the problem of adaptive inference in tree-structured Bayesian networks. We describe an algorithm for adaptive inference that handles a broad range of changes to the network and is able to maintain marginal distributions, MAP estimates, and data likelihoods in all expected logarithmic time. We give an implementation of our algorithm and provide experiments that show that the algorithm can yield up to two orders of magnitude speedups on answering queries and responding to dynamic changes over the sum-product algorithm. 1</p><p>5 0.11555367 <a title="116-tfidf-5" href="./nips-2007-Iterative_Non-linear_Dimensionality_Reduction_with_Manifold_Sculpting.html">107 nips-2007-Iterative Non-linear Dimensionality Reduction with Manifold Sculpting</a></p>
<p>Author: Michael Gashler, Dan Ventura, Tony Martinez</p><p>Abstract: Many algorithms have been recently developed for reducing dimensionality by projecting data onto an intrinsic non-linear manifold. Unfortunately, existing algorithms often lose signiﬁcant precision in this transformation. Manifold Sculpting is a new algorithm that iteratively reduces dimensionality by simulating surface tension in local neighborhoods. We present several experiments that show Manifold Sculpting yields more accurate results than existing algorithms with both generated and natural data-sets. Manifold Sculpting is also able to beneﬁt from both prior dimensionality reduction efforts. 1</p><p>6 0.11343576 <a title="116-tfidf-6" href="./nips-2007-Efficient_Principled_Learning_of_Thin_Junction_Trees.html">78 nips-2007-Efficient Principled Learning of Thin Junction Trees</a></p>
<p>7 0.10604264 <a title="116-tfidf-7" href="./nips-2007-The_Infinite_Markov_Model.html">197 nips-2007-The Infinite Markov Model</a></p>
<p>8 0.099784374 <a title="116-tfidf-8" href="./nips-2007-Anytime_Induction_of_Cost-sensitive_Trees.html">27 nips-2007-Anytime Induction of Cost-sensitive Trees</a></p>
<p>9 0.098741151 <a title="116-tfidf-9" href="./nips-2007-Adaptive_Embedded_Subgraph_Algorithms_using_Walk-Sum_Analysis.html">20 nips-2007-Adaptive Embedded Subgraph Algorithms using Walk-Sum Analysis</a></p>
<p>10 0.086619459 <a title="116-tfidf-10" href="./nips-2007-Subspace-Based_Face_Recognition_in_Analog_VLSI.html">188 nips-2007-Subspace-Based Face Recognition in Analog VLSI</a></p>
<p>11 0.084642 <a title="116-tfidf-11" href="./nips-2007-Theoretical_Analysis_of_Heuristic_Search_Methods_for_Online_POMDPs.html">204 nips-2007-Theoretical Analysis of Heuristic Search Methods for Online POMDPs</a></p>
<p>12 0.083449259 <a title="116-tfidf-12" href="./nips-2007-McRank%3A_Learning_to_Rank_Using_Multiple_Classification_and_Gradient_Boosting.html">126 nips-2007-McRank: Learning to Rank Using Multiple Classification and Gradient Boosting</a></p>
<p>13 0.083308786 <a title="116-tfidf-13" href="./nips-2007-A_Randomized_Algorithm_for_Large_Scale_Support_Vector_Learning.html">10 nips-2007-A Randomized Algorithm for Large Scale Support Vector Learning</a></p>
<p>14 0.08089038 <a title="116-tfidf-14" href="./nips-2007-Bayesian_Agglomerative_Clustering_with_Coalescents.html">31 nips-2007-Bayesian Agglomerative Clustering with Coalescents</a></p>
<p>15 0.073264785 <a title="116-tfidf-15" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>16 0.069763243 <a title="116-tfidf-16" href="./nips-2007-Random_Features_for_Large-Scale_Kernel_Machines.html">160 nips-2007-Random Features for Large-Scale Kernel Machines</a></p>
<p>17 0.067083403 <a title="116-tfidf-17" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>18 0.066432022 <a title="116-tfidf-18" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>19 0.064055651 <a title="116-tfidf-19" href="./nips-2007-Statistical_Analysis_of_Semi-Supervised_Regression.html">186 nips-2007-Statistical Analysis of Semi-Supervised Regression</a></p>
<p>20 0.061895791 <a title="116-tfidf-20" href="./nips-2007-A_Unified_Near-Optimal_Estimator_For_Dimension_Reduction_in_%24l_%5Calpha%24_%28%240%3C%5Calpha%5Cleq_2%24%29_Using_Stable_Random_Projections.html">13 nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.205), (1, 0.042), (2, -0.041), (3, 0.005), (4, -0.038), (5, -0.034), (6, 0.018), (7, 0.127), (8, 0.03), (9, 0.073), (10, -0.138), (11, 0.328), (12, 0.108), (13, -0.046), (14, 0.068), (15, -0.101), (16, 0.002), (17, -0.045), (18, 0.018), (19, 0.001), (20, -0.002), (21, 0.067), (22, 0.028), (23, 0.072), (24, 0.229), (25, -0.128), (26, -0.101), (27, 0.059), (28, -0.042), (29, -0.032), (30, -0.135), (31, 0.003), (32, -0.083), (33, -0.117), (34, -0.14), (35, -0.028), (36, -0.1), (37, 0.018), (38, -0.085), (39, -0.081), (40, 0.069), (41, -0.044), (42, 0.079), (43, 0.021), (44, 0.041), (45, 0.031), (46, 0.007), (47, 0.065), (48, 0.071), (49, 0.001)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97777396 <a title="116-lsi-1" href="./nips-2007-Learning_the_structure_of_manifolds_using_random_projections.html">116 nips-2007-Learning the structure of manifolds using random projections</a></p>
<p>Author: Yoav Freund, Sanjoy Dasgupta, Mayank Kabra, Nakul Verma</p><p>Abstract: We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data. 1</p><p>2 0.77130222 <a title="116-lsi-2" href="./nips-2007-A_learning_framework_for_nearest_neighbor_search.html">16 nips-2007-A learning framework for nearest neighbor search</a></p>
<p>Author: Lawrence Cayton, Sanjoy Dasgupta</p><p>Abstract: Can we leverage learning techniques to build a fast nearest-neighbor (ANN) retrieval data structure? We present a general learning framework for the NN problem in which sample queries are used to learn the parameters of a data structure that minimize the retrieval time and/or the miss rate. We explore the potential of this novel framework through two popular NN data structures: KD-trees and the rectilinear structures employed by locality sensitive hashing. We derive a generalization theory for these data structure classes and present simple learning algorithms for both. Experimental results reveal that learning often improves on the already strong performance of these data structures. 1</p><p>3 0.6936267 <a title="116-lsi-3" href="./nips-2007-Efficient_Principled_Learning_of_Thin_Junction_Trees.html">78 nips-2007-Efficient Principled Learning of Thin Junction Trees</a></p>
<p>Author: Anton Chechetka, Carlos Guestrin</p><p>Abstract: We present the ﬁrst truly polynomial algorithm for PAC-learning the structure of bounded-treewidth junction trees – an attractive subclass of probabilistic graphical models that permits both the compact representation of probability distributions and efﬁcient exact inference. For a constant treewidth, our algorithm has polynomial time and sample complexity. If a junction tree with sufﬁciently strong intraclique dependencies exists, we provide strong theoretical guarantees in terms of KL divergence of the result from the true distribution. We also present a lazy extension of our approach that leads to very signiﬁcant speed ups in practice, and demonstrate the viability of our method empirically, on several real world datasets. One of our key new theoretical insights is a method for bounding the conditional mutual information of arbitrarily large sets of variables with only polynomially many mutual information computations on ﬁxed-size subsets of variables, if the underlying distribution can be approximated by a bounded-treewidth junction tree. 1</p><p>4 0.67511874 <a title="116-lsi-4" href="./nips-2007-Anytime_Induction_of_Cost-sensitive_Trees.html">27 nips-2007-Anytime Induction of Cost-sensitive Trees</a></p>
<p>Author: Saher Esmeir, Shaul Markovitch</p><p>Abstract: Machine learning techniques are increasingly being used to produce a wide-range of classiﬁers for complex real-world applications that involve nonuniform testing costs and misclassiﬁcation costs. As the complexity of these applications grows, the management of resources during the learning and classiﬁcation processes becomes a challenging task. In this work we introduce ACT (Anytime Cost-sensitive Trees), a novel framework for operating in such environments. ACT is an anytime algorithm that allows trading computation time for lower classiﬁcation costs. It builds a tree top-down and exploits additional time resources to obtain better estimations for the utility of the different candidate splits. Using sampling techniques ACT approximates for each candidate split the cost of the subtree under it and favors the one with a minimal cost. Due to its stochastic nature ACT is expected to be able to escape local minima, into which greedy methods may be trapped. Experiments with a variety of datasets were conducted to compare the performance of ACT to that of the state of the art cost-sensitive tree learners. The results show that for most domains ACT produces trees of signiﬁcantly lower costs. ACT is also shown to exhibit good anytime behavior with diminishing returns.</p><p>5 0.5510686 <a title="116-lsi-5" href="./nips-2007-Random_Projections_for_Manifold_Learning.html">161 nips-2007-Random Projections for Manifold Learning</a></p>
<p>Author: Chinmay Hegde, Michael Wakin, Richard Baraniuk</p><p>Abstract: We propose a novel method for linear dimensionality reduction of manifold modeled data. First, we show that with a small number M of random projections of sample points in RN belonging to an unknown K-dimensional Euclidean manifold, the intrinsic dimension (ID) of the sample set can be estimated to high accuracy. Second, we rigorously prove that using only this set of random projections, we can estimate the structure of the underlying manifold. In both cases, the number of random projections required is linear in K and logarithmic in N , meaning that K < M ≪ N . To handle practical situations, we develop a greedy algorithm to estimate the smallest size of the projection space required to perform manifold learning. Our method is particularly relevant in distributed sensing systems and leads to signiﬁcant potential savings in data acquisition, storage and transmission costs.</p><p>6 0.50167167 <a title="116-lsi-6" href="./nips-2007-Adaptive_Embedded_Subgraph_Algorithms_using_Walk-Sum_Analysis.html">20 nips-2007-Adaptive Embedded Subgraph Algorithms using Walk-Sum Analysis</a></p>
<p>7 0.48553792 <a title="116-lsi-7" href="./nips-2007-Bayesian_Agglomerative_Clustering_with_Coalescents.html">31 nips-2007-Bayesian Agglomerative Clustering with Coalescents</a></p>
<p>8 0.48518732 <a title="116-lsi-8" href="./nips-2007-Efficient_Bayesian_Inference_for_Dynamically_Changing_Graphs.html">75 nips-2007-Efficient Bayesian Inference for Dynamically Changing Graphs</a></p>
<p>9 0.46674162 <a title="116-lsi-9" href="./nips-2007-The_Infinite_Markov_Model.html">197 nips-2007-The Infinite Markov Model</a></p>
<p>10 0.45474631 <a title="116-lsi-10" href="./nips-2007-Iterative_Non-linear_Dimensionality_Reduction_with_Manifold_Sculpting.html">107 nips-2007-Iterative Non-linear Dimensionality Reduction with Manifold Sculpting</a></p>
<p>11 0.41276777 <a title="116-lsi-11" href="./nips-2007-Feature_Selection_Methods_for_Improving_Protein_Structure_Prediction_with_Rosetta.html">89 nips-2007-Feature Selection Methods for Improving Protein Structure Prediction with Rosetta</a></p>
<p>12 0.3996979 <a title="116-lsi-12" href="./nips-2007-Discriminative_Log-Linear_Grammars_with_Latent_Variables.html">72 nips-2007-Discriminative Log-Linear Grammars with Latent Variables</a></p>
<p>13 0.38816839 <a title="116-lsi-13" href="./nips-2007-Subspace-Based_Face_Recognition_in_Analog_VLSI.html">188 nips-2007-Subspace-Based Face Recognition in Analog VLSI</a></p>
<p>14 0.3260304 <a title="116-lsi-14" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>15 0.31786516 <a title="116-lsi-15" href="./nips-2007-Theoretical_Analysis_of_Heuristic_Search_Methods_for_Online_POMDPs.html">204 nips-2007-Theoretical Analysis of Heuristic Search Methods for Online POMDPs</a></p>
<p>16 0.3153401 <a title="116-lsi-16" href="./nips-2007-McRank%3A_Learning_to_Rank_Using_Multiple_Classification_and_Gradient_Boosting.html">126 nips-2007-McRank: Learning to Rank Using Multiple Classification and Gradient Boosting</a></p>
<p>17 0.30605641 <a title="116-lsi-17" href="./nips-2007-Learning_with_Tree-Averaged_Densities_and_Distributions.html">119 nips-2007-Learning with Tree-Averaged Densities and Distributions</a></p>
<p>18 0.30335239 <a title="116-lsi-18" href="./nips-2007-A_Randomized_Algorithm_for_Large_Scale_Support_Vector_Learning.html">10 nips-2007-A Randomized Algorithm for Large Scale Support Vector Learning</a></p>
<p>19 0.30291066 <a title="116-lsi-19" href="./nips-2007-Transfer_Learning_using_Kolmogorov_Complexity%3A_Basic_Theory_and_Empirical_Evaluations.html">207 nips-2007-Transfer Learning using Kolmogorov Complexity: Basic Theory and Empirical Evaluations</a></p>
<p>20 0.30253452 <a title="116-lsi-20" href="./nips-2007-A_Unified_Near-Optimal_Estimator_For_Dimension_Reduction_in_%24l_%5Calpha%24_%28%240%3C%5Calpha%5Cleq_2%24%29_Using_Stable_Random_Projections.html">13 nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.054), (13, 0.035), (16, 0.037), (19, 0.033), (21, 0.058), (31, 0.025), (34, 0.036), (35, 0.067), (47, 0.105), (49, 0.013), (51, 0.184), (83, 0.176), (85, 0.031), (87, 0.018), (90, 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85032439 <a title="116-lda-1" href="./nips-2007-Learning_the_structure_of_manifolds_using_random_projections.html">116 nips-2007-Learning the structure of manifolds using random projections</a></p>
<p>Author: Yoav Freund, Sanjoy Dasgupta, Mayank Kabra, Nakul Verma</p><p>Abstract: We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data. 1</p><p>2 0.77686751 <a title="116-lda-2" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>Author: Yuhong Guo, Dale Schuurmans</p><p>Abstract: We investigate a new, convex relaxation of an expectation-maximization (EM) variant that approximates a standard objective while eliminating local minima. First, a cautionary result is presented, showing that any convex relaxation of EM over hidden variables must give trivial results if any dependence on the missing values is retained. Although this appears to be a strong negative outcome, we then demonstrate how the problem can be bypassed by using equivalence relations instead of value assignments over hidden variables. In particular, we develop new algorithms for estimating exponential conditional models that only require equivalence relation information over the variable values. This reformulation leads to an exact expression for EM variants in a wide range of problems. We then develop a semideﬁnite relaxation that yields global training by eliminating local minima. 1</p><p>3 0.77178526 <a title="116-lda-3" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>Author: Marc'aurelio Ranzato, Y-lan Boureau, Yann L. Cun</p><p>Abstract: Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation. We describe a novel and efﬁcient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machine trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input observed variables can be captured. 1</p><p>4 0.77071589 <a title="116-lda-4" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>Author: Matthias Bethge, Philipp Berens</p><p>Abstract: Maximum entropy analysis of binary variables provides an elegant way for studying the role of pairwise correlations in neural populations. Unfortunately, these approaches suffer from their poor scalability to high dimensions. In sensory coding, however, high-dimensional data is ubiquitous. Here, we introduce a new approach using a near-maximum entropy model, that makes this type of analysis feasible for very high-dimensional data—the model parameters can be derived in closed form and sampling is easy. Therefore, our NearMaxEnt approach can serve as a tool for testing predictions from a pairwise maximum entropy model not only for low-dimensional marginals, but also for high dimensional measurements of more than thousand units. We demonstrate its usefulness by studying natural images with dichotomized pixel intensities. Our results indicate that the statistics of such higher-dimensional measurements exhibit additional structure that are not predicted by pairwise correlations, despite the fact that pairwise correlations explain the lower-dimensional marginal statistics surprisingly well up to the limit of dimensionality where estimation of the full joint distribution is feasible. 1</p><p>5 0.77009523 <a title="116-lda-5" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>Author: Nicolas L. Roux, Yoshua Bengio, Pascal Lamblin, Marc Joliveau, Balázs Kégl</p><p>Abstract: We study the following question: is the two-dimensional structure of images a very strong prior or is it something that can be learned with a few examples of natural images? If someone gave us a learning task involving images for which the two-dimensional topology of pixels was not known, could we discover it automatically and exploit it? For example suppose that the pixels had been permuted in a ﬁxed but unknown way, could we recover the relative two-dimensional location of pixels on images? The surprising result presented here is that not only the answer is yes, but that about as few as a thousand images are enough to approximately recover the relative locations of about a thousand pixels. This is achieved using a manifold learning algorithm applied to pixels associated with a measure of distributional similarity between pixel intensities. We compare different topologyextraction approaches and show how having the two-dimensional topology can be exploited.</p><p>6 0.76827282 <a title="116-lda-6" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>7 0.76766664 <a title="116-lda-7" href="./nips-2007-DIFFRAC%3A_a_discriminative_and_flexible_framework_for_clustering.html">65 nips-2007-DIFFRAC: a discriminative and flexible framework for clustering</a></p>
<p>8 0.76718426 <a title="116-lda-8" href="./nips-2007-Multi-Task_Learning_via_Conic_Programming.html">134 nips-2007-Multi-Task Learning via Conic Programming</a></p>
<p>9 0.76617324 <a title="116-lda-9" href="./nips-2007-A_learning_framework_for_nearest_neighbor_search.html">16 nips-2007-A learning framework for nearest neighbor search</a></p>
<p>10 0.76547873 <a title="116-lda-10" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>11 0.76272762 <a title="116-lda-11" href="./nips-2007-Statistical_Analysis_of_Semi-Supervised_Regression.html">186 nips-2007-Statistical Analysis of Semi-Supervised Regression</a></p>
<p>12 0.76243079 <a title="116-lda-12" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>13 0.76093531 <a title="116-lda-13" href="./nips-2007-An_Analysis_of_Inference_with_the_Universum.html">24 nips-2007-An Analysis of Inference with the Universum</a></p>
<p>14 0.7598291 <a title="116-lda-14" href="./nips-2007-Probabilistic_Matrix_Factorization.html">158 nips-2007-Probabilistic Matrix Factorization</a></p>
<p>15 0.75871122 <a title="116-lda-15" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>16 0.75705451 <a title="116-lda-16" href="./nips-2007-Ultrafast_Monte_Carlo_for_Statistical_Summations.html">209 nips-2007-Ultrafast Monte Carlo for Statistical Summations</a></p>
<p>17 0.75587398 <a title="116-lda-17" href="./nips-2007-Structured_Learning_with_Approximate_Inference.html">187 nips-2007-Structured Learning with Approximate Inference</a></p>
<p>18 0.75557047 <a title="116-lda-18" href="./nips-2007-Colored_Maximum_Variance_Unfolding.html">49 nips-2007-Colored Maximum Variance Unfolding</a></p>
<p>19 0.75556463 <a title="116-lda-19" href="./nips-2007-Efficient_Convex_Relaxation_for_Transductive_Support_Vector_Machine.html">76 nips-2007-Efficient Convex Relaxation for Transductive Support Vector Machine</a></p>
<p>20 0.75521523 <a title="116-lda-20" href="./nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
