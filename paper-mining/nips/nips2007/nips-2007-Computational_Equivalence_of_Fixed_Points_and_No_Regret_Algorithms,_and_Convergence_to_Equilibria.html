<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>54 nips-2007-Computational Equivalence of Fixed Points and No Regret Algorithms, and Convergence to Equilibria</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-54" href="#">nips2007-54</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>54 nips-2007-Computational Equivalence of Fixed Points and No Regret Algorithms, and Convergence to Equilibria</h1>
<br/><p>Source: <a title="nips-2007-54-pdf" href="http://papers.nips.cc/paper/3249-computational-equivalence-of-fixed-points-and-no-regret-algorithms-and-convergence-to-equilibria.pdf">pdf</a></p><p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We study the relation between notions of game-theoretic equilibria which are based on stability under a set of deviations, and empirical equilibria which are reached by rational players. Rational players are modeled by players using no regret algorithms, which guarantee that their payoff in the long run is close to the maximum they could hope to achieve by consistently deviating from the algorithm’s suggested action. We show that for a given set of deviations over the strategy set of a player, it is possible to efﬁciently approximate ﬁxed points of a given deviation if and only if there exist efﬁcient no regret algorithms resistant to the deviations. Further, we show that if all players use a no regret algorithm, then the empirical distribution of their plays converges to an equilibrium. 1</p><p>Reference: <a title="nips-2007-54-reference" href="../nips2007_reference/nips-2007-Computational_Equivalence_of_Fixed_Points_and_No_Regret_Algorithms%2C_and_Convergence_to_Equilibria_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We study the relation between notions of game-theoretic equilibria which are based on stability under a set of deviations, and empirical equilibria which are reached by rational players. [sent-6, score-0.335]
</p><p>2 Rational players are modeled by players using no regret algorithms, which guarantee that their payoff in the long run is close to the maximum they could hope to achieve by consistently deviating from the algorithm’s suggested action. [sent-7, score-1.482]
</p><p>3 We show that for a given set of deviations over the strategy set of a player, it is possible to efﬁciently approximate ﬁxed points of a given deviation if and only if there exist efﬁcient no regret algorithms resistant to the deviations. [sent-8, score-0.88]
</p><p>4 Further, we show that if all players use a no regret algorithm, then the empirical distribution of their plays converges to an equilibrium. [sent-9, score-0.847]
</p><p>5 1  Introduction  We consider a setting where a number of agents need to repeatedly make decisions in the face of uncertainty. [sent-10, score-0.135]
</p><p>6 In each round, the agent obtains a payoff based on the decision she chose. [sent-11, score-0.65]
</p><p>7 We imagine that the agent has a choice of several well-deﬁned ways to change her decision, and now the agent aims to maximize her payoff relative to what she could have obtained had she changed her decisions in a consistent manner. [sent-17, score-0.681]
</p><p>8 As an example of what we mean by consistent changes, a possible objective could be to maximize her payoff relative to the most she could have achieved by choosing some ﬁxed decision in all the rounds. [sent-18, score-0.553]
</p><p>9 The difference between these payoffs is known as external regret in the game theory literature. [sent-19, score-0.929]
</p><p>10 Another notion is that of internal regret, which arises when the possible ways to change are the ones that switch from some decision i to another, j, whenever the agent chose decision i, leaving all other decisions unchanged. [sent-20, score-0.385]
</p><p>11 Based on what set of decision modiﬁers are under consideration, various no regret algorithms are known (for e. [sent-22, score-0.792]
</p><p>12 Hannan [10] gave algorithms to minimize external regret, and Hart and Mas-Collel [11] give algorithms to minimize internal regret). [sent-24, score-0.189]
</p><p>13 1  The reason no regret algorithms are so appealing, apart from the fact that they model rational behavior of agents in the face of uncertainty, is that in various cases it can be shown that using no regret algorithms guides the overall play towards a game theoretic equilibrium. [sent-25, score-1.729]
</p><p>14 For example, Freund and Schapire [7] show that in a zero-sum game, if all agents use a no external regret algorithm, then the empirical distribution of the play converges to the set of minimax equilibria. [sent-26, score-0.881]
</p><p>15 Similarly, Hart and Mas-Collel [11] show that if all agents use a no internal regret algorithm, then the empirical distribution of the play converges to the set of correlated equilibria. [sent-27, score-0.915]
</p><p>16 In general, given a set of decision modiﬁers Φ, we can deﬁne a notion of game theoretic equilibrium that is based on the property of being stable under deviations speciﬁed by Φ. [sent-28, score-0.475]
</p><p>17 This is a joint distribution on the agents’ decisions that ensures that the expected payoff to any agent is no less than the most she could achieve if she decided to unilaterally (and consistently) decided to deviate from her suggested action using any decision modiﬁer in Φ. [sent-29, score-0.835]
</p><p>18 One can then show that if all agents use a Φ-no regret algorithm, then the empirical distribution of the play converges to the set of Φ-equilibria. [sent-30, score-0.816]
</p><p>19 This brings us to the question of whether it is possible to design no regret algorithms for various sets of decision modiﬁers Φ. [sent-31, score-0.792]
</p><p>20 In this paper, we design algorithms which achieve no regret with respect to Φ for a very general setting of arbitrary convex compact decision spaces, arbitrary concave payoff functions, and arbitrary continuous decision modiﬁers. [sent-32, score-1.561]
</p><p>21 Our method works as long as it is possible to compute approximate ﬁxed points for (convex combinations) of decision modiﬁers in Φ. [sent-33, score-0.157]
</p><p>22 [18]) and we show how to apply known learning algorithms to obtain Φ-no regret algorithms. [sent-36, score-0.692]
</p><p>23 The generality of our connection allows us to use various sophisticated Online Convex Optimization algorithms which can exploit various structural properties of the utility functions and guarantee a faster rate of convergence to the equilibrium. [sent-37, score-0.177]
</p><p>24 Previous work by Greenwald and Jafari [9] gave algorithms for the case when the decision space is the simplex of probability distributions over the agents’ decisions, the payoff functions are linear, and the decision modiﬁers are also linear. [sent-38, score-0.778]
</p><p>25 Their algorithm, based on the work of Hart and MasCollel [11], uses a version of Blackwell’s Approachability Theorem, and also needs to computes ﬁxed points of the decision modiﬁers. [sent-39, score-0.157]
</p><p>26 Computing Brouwer ﬁxed points of continuous functions is in general a very hard problem (it is PPAD-complete, as shown by Papadimitriou [15]). [sent-41, score-0.133]
</p><p>27 Most common notions of equilibria in game theory are deﬁned as the set of ﬁxed points of a certain mapping. [sent-43, score-0.388]
</p><p>28 The fact that Brouwer ﬁxed points are hard to compute in general is no reason why computing speciﬁc ﬁxed points should be hard (for instance, as mentioned earlier, computing ﬁxed points of linear functions is easy via eigenvector computations). [sent-45, score-0.258]
</p><p>29 These hopes were dashed by the work of [6, 3] who showed that computing NE is as computationally difﬁcult as ﬁnding ﬁxed points in a general mapping: they show that computing NE in a two-player game is PPAD-complete. [sent-47, score-0.265]
</p><p>30 Since our algorithms (and all previous ones as well) depend on computing (approximate) ﬁxed points of various decision modiﬁers, the above discussion leads us to question whether this is necessary. [sent-49, score-0.214]
</p><p>31 We show in this paper that indeed it is: a Φ-no-regret algorithm can be efﬁciently used to compute approximate ﬁxed points of any convex combination of decision modiﬁers. [sent-50, score-0.232]
</p><p>32 This establishes an equivalence theorem, which is the main contribution of this paper: there exist efﬁcient Φ-no-regret algorithms if and only it is possible to efﬁciently compute ﬁxed points of convex combinations of decision modiﬁers in Φ. [sent-51, score-0.337]
</p><p>33 This equivalence theorem allows us to translate complexity theoretic lower bounds on computing ﬁxed points to designing no regret algorithms. [sent-52, score-0.882]
</p><p>34 For instance, a Nash equilibrium can be obtained by applying Brouwer’s ﬁxed point theorem to an appropriately deﬁned continuous mapping from the compact convex set of pairs of the players’ mixed strategies to itself. [sent-53, score-0.345]
</p><p>35 They also show how to design them  2  from ﬁxed-point oracles, and proved convergence to equilibria under even more general conditions than we consider. [sent-56, score-0.164]
</p><p>36 First, the set of strategies for the players of the game is a convex compact set. [sent-61, score-0.448]
</p><p>37 Second, the utility functions for the players are concave over their strategy sets. [sent-62, score-0.346]
</p><p>38 Formally, for i = 1, 2, player i plays points from a convex compact set Ki ⊆ Rni . [sent-64, score-0.286]
</p><p>39 Her payoff is given by function ui : K1 × K2 → R, i. [sent-65, score-0.489]
</p><p>40 if x1 , x2 is the pair of strategies played by the two players, then the payoff to player i is given by ui (x1 , x2 ). [sent-67, score-0.655]
</p><p>41 We assume that u1 is a concave function of x1 for any ﬁxed x2 , and similarly u2 is a concave function of x2 for any ﬁxed x1 . [sent-68, score-0.128]
</p><p>42 We now deﬁne a notion of game theoretic equilibrium based on the property of being stable with respect to consistent deviations. [sent-69, score-0.288]
</p><p>43 By this, we mean an online game-playing strategy for the players that will guarantee that neither stands to gain if they decided to unilaterally, and consistently, deviate from their suggested moves. [sent-70, score-0.336]
</p><p>44 To model this, assume that each player i has a set of possible deviations Φi which is a ﬁnite1 set of continuous mappings φi : Ki → Ki . [sent-71, score-0.282]
</p><p>45 If it is the case that for any deviation φ1 ∈ Φ1 , player 1’s expected payoff obtained by sampling x1 using Ψ is always larger than her expected payoff obtained by deviating to φ1 (x1 ), then we call Ψ stable under deviations in Φ1 . [sent-74, score-1.188]
</p><p>46 Intuitively, we imagine a repeated game between the two players, where at equilibrium, the players’ moves are correlated by a signal, which could be the past history of the play, and various external factors. [sent-79, score-0.326]
</p><p>47 This signal samples a pair of moves from an equilibrium joint distribution over all pairs of moves, and suggests to each player individually only the move she is supposed to play. [sent-80, score-0.226]
</p><p>48 If no player stands to gain if she unilaterally, but consistently, used a deviation from her suggested move, then the distribution of the correlating signal is stable under the set of deviations, and is hence an equilibrium. [sent-81, score-0.203]
</p><p>49 A standard 2-player game is obtained when the Ki are the simplices of distributions over some base sets of actions Ai and the utility functions ui are bilinear in x1 , x2 . [sent-83, score-0.314]
</p><p>50 Consider the following setting: there are two investors (the generalization to many investors is straightforward), who invest their wealth in n stocks. [sent-87, score-0.184]
</p><p>51 In each period, they choose portfolios x1 and x2 over the n stocks, and observe the stock returns. [sent-88, score-0.139]
</p><p>52 We model the stock returns as a function r of the portfolios x1 , x2 chosen by the investors, and it maps the portfolios to the vector of stock returns. [sent-89, score-0.278]
</p><p>53 We make the assumption that each player has a small inﬂuence on the market, and thus the function r is insensitive to the small perturbations in the input. [sent-90, score-0.133]
</p><p>54 We can now deﬁne the utility functions as ui (x1 , x2 ) = log(r(x1 , x2 ) · xi ). [sent-94, score-0.152]
</p><p>55 Intuitively, this game models the setting in which the market prices are affected by the investments of the players. [sent-95, score-0.242]
</p><p>56 A natural goal for a good investment strategy would be to compare the wealth gain to that of the best ﬁxed portfolio, i. [sent-96, score-0.121]
</p><p>57 In Section 3, we show that the stock market game admits algorithms that converge to an ε-equilibrium in O( 1 log 1 ) rounds, whereas all previous algorithms need O( ε1 ) rounds. [sent-101, score-0.408]
</p><p>58 2  No regret algorithms  The online learning framework we consider is called online convex optimization [18], in which there is a ﬁxed convex compact feasible set K ⊂ Rn and an arbitrary, unknown sequence of concave payoff functions f (1) , f (2) , . [sent-103, score-1.596]
</p><p>59 The decision maker must make a sequence of decisions, where the tth decision is a selection of a point x(t) ∈ K and obtains a payoff of f (t) (x(t) ) on period t. [sent-107, score-0.772]
</p><p>60 The decision maker can only use the previous points x(1) , . [sent-108, score-0.21]
</p><p>61 , x(t−1) , and the previous payoff functions f (1) , . [sent-111, score-0.494]
</p><p>62 The decision maker has a ﬁnite set of N decision modiﬁers Φ which, as before, is a set of continuous mappings from K → K. [sent-116, score-0.336]
</p><p>63 Then the regret for not using some deviation φ ∈ Φ is the excess payoff the decision maker could have obtained if she had changed her points in each round by applying φ. [sent-117, score-1.388]
</p><p>64 Given a set of T concave utility functions f1 , . [sent-120, score-0.18]
</p><p>65 This implies that the average per iteration payoff of the algorithm converges to the average payoff of a clairvoyant algorithm that uses the best deviation in hindsight to change the point in every round. [sent-128, score-0.996]
</p><p>66 that the regret is polynomially sublinear as a function of T . [sent-130, score-0.685]
</p><p>67 A no Φ-regret algorithm is one which, given any sequence of concave payoff functions f (1) , f (2) , . [sent-132, score-0.558]
</p><p>68 In this paper, we will use Exponentiated Gradient (EG) algorithm ([14], [1]), which has the following (external) regret bound: Theorem 1. [sent-148, score-0.658]
</p><p>69 Let G∞ be an upper bound on the L∞ norm of the gradients of the payoff functions, i. [sent-150, score-0.453]
</p><p>70 , x(T ) such that T x∈K  T  f (t) (x) −  max t=1  f (t) (x(t) ) ≤ O(G∞  log(n)T )  t=1  If the utility functions are strictly concave rather than linear, even stronger regret bounds, which √ depend on log(T ) rather than T , are known [13]. [sent-156, score-0.838]
</p><p>71 While most of the literature on online convex optimization focuses on external regret, it was observed that any Online Convex Optimization algorithm for external regret can be converted to an internal regret algorithm (for example, see [2], [16]). [sent-157, score-1.654]
</p><p>72 3  Fixed Points  As mentioned in the introduction, our no regret algorithms depend on computing ﬁxed points of the relevant mappings. [sent-159, score-0.772]
</p><p>73 For a given set of deviations Φ, denote by CH(Φ) the set of all convex combinations of deviations in Φ, i. [sent-160, score-0.275]
</p><p>74 Since each map φ ∈ CH(Φ) is a continuous function from K → K, and K is a convex compact domain, by Brouwer’s ﬁxed theorem, φ has a ﬁxed point in K, i. [sent-163, score-0.175]
</p><p>75 ε  3  Convergence of no Φ-regret algorithms to Φ-equilibria  In this section we prove that if the players use no Φ-regret algorithms, then the empirical distribution of the moves converges to a Φ-equilibrium. [sent-170, score-0.279]
</p><p>76 [11] shows that if players use no internal regret algorithms, then the empirical distribution of the moves converges to a correlated equilibrium. [sent-171, score-1.002]
</p><p>77 The advantage of this general setting is that the connection to online convex optimization allows for faster rates of convergence using recent online learning techniques. [sent-175, score-0.256]
</p><p>78 We give an example of a natural game theoretic setting with faster convergence rate below. [sent-176, score-0.23]
</p><p>79 If each player i chooses moves using a no Φi -regret algorithms, then the empirical game distribution of the players’ moves converges to a Φ-equilibrium. [sent-178, score-0.431]
</p><p>80 In each game iteration t, let (x1 (t) , x2 (t) ) be the pair of moves played by the two players. [sent-182, score-0.247]
</p><p>81 From player 1’s point of view, the payoff function she obtains, f (t) , is the following: ∀x ∈ K1 : f (t) (x) u1 (x, x2 (t) ). [sent-183, score-0.588]
</p><p>82 Now assume that both players use no regret algorithms, which ensure that RegretΦi (T ) ≤ O(T 1−c ) for some constant c > 0. [sent-195, score-0.802]
</p><p>83 A corollary of Theorem 2 is that we can obtain faster rates of convergence using recent online learning techniques, when the payoff functions are non-linear. [sent-201, score-0.598]
</p><p>84 For the stock market game as deﬁned in section 2. [sent-204, score-0.314]
</p><p>85 1, there exists no regret algorithms which guarantee convergence to an ε-equilibrium in O( 1 log 1 ) iterations. [sent-205, score-0.745]
</p><p>86 The utility functions observed by the investor i in the stock market game are of the form ui (x1 , x2 ) = log(r(x1 , x2 ) · xi ). [sent-207, score-0.495]
</p><p>87 ε ε  4  Computational Equivalence of Fixed Points and No Regret algorithms  In this section we prove our main result on the computational equivalence of computing ﬁxed points and designing no regret algorithms. [sent-212, score-0.841]
</p><p>88 By the result of the previous section, players using no regret algorithms converge to equilibria. [sent-213, score-0.836]
</p><p>89 We assume that the payoff functions f (t) are scaled so that the (L2 ) norm of their gradients is bounded by 1, i. [sent-214, score-0.494]
</p><p>90 The ﬁrst direction of the theorem is proved by designing utility functions for which the no regret property will imply convergence to an approximate ﬁxed point of the corresponding transformations. [sent-220, score-0.882]
</p><p>91 The proof crucially depends on the fact that no regret algorithms have the stringent requirement that their worst case regret, against arbitrary adversarially chosen payoff functions, is sublinear as a function of the number of the rounds. [sent-221, score-1.197]
</p><p>92 Else, supply A with the following payoff function: (φ0 (x(t) ) − x(t) ) (x − x(t) ) φ0 (x(t) ) − x(t)  f (t) (x)  This is a linear function, with f (t) (x) = 1. [sent-229, score-0.453]
</p><p>93 Thus, when T = Ω( ε1/c ) the lower bound (2) on the regret cannot hold unless we have already found an ε-approximate ﬁxed point of φ0 . [sent-234, score-0.681]
</p><p>94 The second direction is on the lines of the algorithms of [2] and [16] which use ﬁxed point computations to obtain no internal regret algorithms. [sent-235, score-0.771]
</p><p>95 We use a no external regret algorithm for the inner OCO problem to generate points in K for the outer one, and use the payoff functions obtained in the outer OCO problem to generate appropriate payoff functions for the inner one. [sent-242, score-1.916]
</p><p>96 Let x(t) ∈ K be the point used in the outer OCO problem in the tth round, and let f (t) be the obtained payoff function. [sent-250, score-0.536]
</p><p>97 Then the payoff functions for the inner OCO problem is the function g (t) : ∆N → R deﬁned as follows: ∀α ∈ ∆N :  g (t) (α)  f (t) (φα (x(t) )). [sent-251, score-0.53]
</p><p>98 t  2 In the full version of the paper, we improve the regret bound to O(log T ) under some stronger concavity assumptions on the payoff functions. [sent-264, score-1.133]
</p><p>99 7  Now, using the deﬁnition of the g (t) functions, and by the regret bound for the EG algorithm, we have that for any ﬁxed distribution α ∈ ∆N , T  T  f (t) (φα (x(t) ))− t=1  T  f (t) (φα(t) (x(t) )) = t=1  f  Since  (t)  T  g (t) (α)− t=1  g (t) (α(t) ) ≤ O( log(N )T ). [sent-265, score-0.658]
</p><p>100 Learning correlated equilibria in games with compact sets of strategies. [sent-371, score-0.275]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('regret', 0.658), ('payoff', 0.453), ('oco', 0.168), ('game', 0.162), ('players', 0.144), ('equilibria', 0.137), ('player', 0.112), ('fptas', 0.101), ('decision', 0.1), ('hart', 0.088), ('deviations', 0.087), ('agents', 0.082), ('market', 0.08), ('ch', 0.078), ('online', 0.077), ('agent', 0.076), ('utility', 0.075), ('convex', 0.075), ('stock', 0.072), ('brouwer', 0.067), ('greenwald', 0.067), ('investors', 0.067), ('portfolios', 0.067), ('eg', 0.067), ('external', 0.065), ('concave', 0.064), ('nash', 0.059), ('hazan', 0.058), ('equilibrium', 0.058), ('modi', 0.057), ('points', 0.057), ('internal', 0.056), ('moves', 0.056), ('maker', 0.053), ('decisions', 0.053), ('games', 0.053), ('ers', 0.052), ('xed', 0.051), ('unilaterally', 0.05), ('wealth', 0.05), ('simplex', 0.05), ('mappings', 0.048), ('converges', 0.045), ('equivalence', 0.045), ('portfolio', 0.044), ('payoffs', 0.044), ('ki', 0.044), ('correlated', 0.043), ('fixed', 0.043), ('compact', 0.042), ('functions', 0.041), ('theoretic', 0.041), ('outer', 0.038), ('exponentiated', 0.037), ('ui', 0.036), ('inner', 0.036), ('continuous', 0.035), ('algorithms', 0.034), ('theorem', 0.034), ('deviating', 0.034), ('hannan', 0.034), ('stoltz', 0.034), ('poly', 0.034), ('notions', 0.032), ('mapping', 0.032), ('play', 0.031), ('decided', 0.03), ('investment', 0.029), ('kale', 0.029), ('satyen', 0.029), ('investor', 0.029), ('rational', 0.029), ('played', 0.029), ('consistently', 0.027), ('stable', 0.027), ('convergence', 0.027), ('focs', 0.027), ('sublinear', 0.027), ('combinations', 0.026), ('log', 0.026), ('strategies', 0.025), ('adversarially', 0.025), ('designing', 0.024), ('changed', 0.023), ('point', 0.023), ('computing', 0.023), ('tth', 0.022), ('concavity', 0.022), ('strategy', 0.022), ('deviation', 0.022), ('round', 0.022), ('nition', 0.022), ('suggested', 0.022), ('logarithmic', 0.021), ('economic', 0.021), ('deviate', 0.021), ('perturbations', 0.021), ('appropriately', 0.021), ('obtains', 0.021), ('gain', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="54-tfidf-1" href="./nips-2007-Computational_Equivalence_of_Fixed_Points_and_No_Regret_Algorithms%2C_and_Convergence_to_Equilibria.html">54 nips-2007-Computational Equivalence of Fixed Points and No Regret Algorithms, and Convergence to Equilibria</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We study the relation between notions of game-theoretic equilibria which are based on stability under a set of deviations, and empirical equilibria which are reached by rational players. Rational players are modeled by players using no regret algorithms, which guarantee that their payoff in the long run is close to the maximum they could hope to achieve by consistently deviating from the algorithm’s suggested action. We show that for a given set of deviations over the strategy set of a player, it is possible to efﬁciently approximate ﬁxed points of a given deviation if and only if there exist efﬁcient no regret algorithms resistant to the deviations. Further, we show that if all players use a no regret algorithm, then the empirical distribution of their plays converges to an equilibrium. 1</p><p>2 0.44213694 <a title="54-tfidf-2" href="./nips-2007-Regret_Minimization_in_Games_with_Incomplete_Information.html">165 nips-2007-Regret Minimization in Games with Incomplete Information</a></p>
<p>Author: Martin Zinkevich, Michael Johanson, Michael Bowling, Carmelo Piccione</p><p>Abstract: Extensive games are a powerful model of multiagent decision-making scenarios with incomplete information. Finding a Nash equilibrium for very large instances of these games has received a great deal of recent attention. In this paper, we describe a new technique for solving large games based on regret minimization. In particular, we introduce the notion of counterfactual regret, which exploits the degree of incomplete information in an extensive game. We show how minimizing counterfactual regret minimizes overall regret, and therefore in self-play can be used to compute a Nash equilibrium. We demonstrate this technique in the domain of poker, showing we can solve abstractions of limit Texas Hold’em with as many as 1012 states, two orders of magnitude larger than previous methods. 1</p><p>3 0.3211194 <a title="54-tfidf-3" href="./nips-2007-The_Epoch-Greedy_Algorithm_for_Multi-armed_Bandits_with_Side_Information.html">194 nips-2007-The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information</a></p>
<p>Author: John Langford, Tong Zhang</p><p>Abstract: We present Epoch-Greedy, an algorithm for contextual multi-armed bandits (also known as bandits with side information). Epoch-Greedy has the following properties: 1. No knowledge of a time horizon T is necessary. 2. The regret incurred by Epoch-Greedy is controlled by a sample complexity bound for a hypothesis class. 3. The regret scales as O(T 2/3 S 1/3 ) or better (sometimes, much better). Here S is the complexity term in a sample complexity bound for standard supervised learning. 1</p><p>4 0.31088647 <a title="54-tfidf-4" href="./nips-2007-The_Price_of_Bandit_Information_for_Online_Optimization.html">199 nips-2007-The Price of Bandit Information for Online Optimization</a></p>
<p>Author: Varsha Dani, Sham M. Kakade, Thomas P. Hayes</p><p>Abstract: In the online linear optimization problem, a learner must choose, in each round, a decision from a set D ⊂ Rn in order to minimize an (unknown and changing) linear cost function. We present sharp rates of convergence (with respect to additive regret) for both the full information setting (where the cost function is revealed at the end of each round) and the bandit setting (where only the scalar cost incurred is revealed). In particular, this paper is concerned with the price of bandit information, by which we mean the ratio of the best achievable regret in the bandit setting to that in the full-information setting. For the full informa√ tion case, the upper bound on the regret is O∗ ( nT ), where n is the ambient dimension and T is the time horizon. For the bandit case, we present an algorithm √ which achieves O∗ (n3/2 T ) regret — all previous (nontrivial) bounds here were O(poly(n)T 2/3 ) or worse. It is striking that the convergence rate for the bandit setting is only a factor of n worse than in the full information case — in stark contrast to the K-arm bandit setting, where the gap in the dependence on K is √ √ exponential ( T K√ vs. T log K). We also present lower bounds showing that this gap is at least n, which we conjecture to be the correct order. The bandit algorithm we present can be implemented efﬁciently in special cases of particular interest, such as path planning and Markov Decision Problems. 1</p><p>5 0.28450239 <a title="54-tfidf-5" href="./nips-2007-Adaptive_Online_Gradient_Descent.html">21 nips-2007-Adaptive Online Gradient Descent</a></p>
<p>Author: Elad Hazan, Alexander Rakhlin, Peter L. Bartlett</p><p>Abstract: We study the rates of growth of the regret in online convex optimization. First, we show that a simple extension of the algorithm of Hazan et al eliminates the need for a priori knowledge of the lower bound on the second derivatives of the observed functions. We then provide an algorithm, Adaptive Online Gradient Descent, which interpolates between the results of Zinkevich for linear functions and of Hazan et al for strongly convex functions, achieving intermediate rates √ between T and log T . Furthermore, we show strong optimality of the algorithm. Finally, we provide an extension of our results to general norms. 1</p><p>6 0.19820496 <a title="54-tfidf-6" href="./nips-2007-Optimistic_Linear_Programming_gives_Logarithmic_Regret_for_Irreducible_MDPs.html">151 nips-2007-Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs</a></p>
<p>7 0.18744199 <a title="54-tfidf-7" href="./nips-2007-Computing_Robust_Counter-Strategies.html">55 nips-2007-Computing Robust Counter-Strategies</a></p>
<p>8 0.098471224 <a title="54-tfidf-8" href="./nips-2007-Competition_Adds_Complexity.html">52 nips-2007-Competition Adds Complexity</a></p>
<p>9 0.092368506 <a title="54-tfidf-9" href="./nips-2007-TrueSkill_Through_Time%3A_Revisiting_the_History_of_Chess.html">208 nips-2007-TrueSkill Through Time: Revisiting the History of Chess</a></p>
<p>10 0.065961398 <a title="54-tfidf-10" href="./nips-2007-A_Game-Theoretic_Approach_to_Apprenticeship_Learning.html">5 nips-2007-A Game-Theoretic Approach to Apprenticeship Learning</a></p>
<p>11 0.059342962 <a title="54-tfidf-11" href="./nips-2007-Congruence_between_model_and_human_attention_reveals_unique_signatures_of_critical_visual_events.html">57 nips-2007-Congruence between model and human attention reveals unique signatures of critical visual events</a></p>
<p>12 0.053062562 <a title="54-tfidf-12" href="./nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</a></p>
<p>13 0.04282929 <a title="54-tfidf-13" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>14 0.042214047 <a title="54-tfidf-14" href="./nips-2007-COFI_RANK_-_Maximum_Margin_Matrix_Factorization_for_Collaborative_Ranking.html">41 nips-2007-COFI RANK - Maximum Margin Matrix Factorization for Collaborative Ranking</a></p>
<p>15 0.040962089 <a title="54-tfidf-15" href="./nips-2007-Reinforcement_Learning_in_Continuous_Action_Spaces_through_Sequential_Monte_Carlo_Methods.html">168 nips-2007-Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods</a></p>
<p>16 0.04024132 <a title="54-tfidf-16" href="./nips-2007-DIFFRAC%3A_a_discriminative_and_flexible_framework_for_clustering.html">65 nips-2007-DIFFRAC: a discriminative and flexible framework for clustering</a></p>
<p>17 0.036138721 <a title="54-tfidf-17" href="./nips-2007-Progressive_mixture_rules_are_deviation_suboptimal.html">159 nips-2007-Progressive mixture rules are deviation suboptimal</a></p>
<p>18 0.032437261 <a title="54-tfidf-18" href="./nips-2007-Convex_Clustering_with_Exemplar-Based_Models.html">61 nips-2007-Convex Clustering with Exemplar-Based Models</a></p>
<p>19 0.03223021 <a title="54-tfidf-19" href="./nips-2007-One-Pass_Boosting.html">147 nips-2007-One-Pass Boosting</a></p>
<p>20 0.032158528 <a title="54-tfidf-20" href="./nips-2007-What_makes_some_POMDP_problems_easy_to_approximate%3F.html">215 nips-2007-What makes some POMDP problems easy to approximate?</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.155), (1, -0.283), (2, 0.029), (3, 0.095), (4, 0.578), (5, -0.069), (6, -0.148), (7, 0.116), (8, 0.007), (9, 0.118), (10, -0.102), (11, -0.095), (12, 0.042), (13, -0.012), (14, 0.037), (15, -0.016), (16, 0.001), (17, 0.002), (18, -0.009), (19, -0.025), (20, 0.021), (21, -0.036), (22, -0.008), (23, -0.003), (24, -0.034), (25, -0.01), (26, -0.001), (27, -0.016), (28, -0.009), (29, 0.037), (30, 0.041), (31, 0.027), (32, 0.04), (33, 0.016), (34, -0.052), (35, -0.001), (36, -0.004), (37, 0.028), (38, -0.033), (39, -0.024), (40, 0.023), (41, 0.092), (42, 0.081), (43, -0.02), (44, -0.0), (45, -0.015), (46, 0.034), (47, 0.011), (48, 0.078), (49, -0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95502716 <a title="54-lsi-1" href="./nips-2007-Computational_Equivalence_of_Fixed_Points_and_No_Regret_Algorithms%2C_and_Convergence_to_Equilibria.html">54 nips-2007-Computational Equivalence of Fixed Points and No Regret Algorithms, and Convergence to Equilibria</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We study the relation between notions of game-theoretic equilibria which are based on stability under a set of deviations, and empirical equilibria which are reached by rational players. Rational players are modeled by players using no regret algorithms, which guarantee that their payoff in the long run is close to the maximum they could hope to achieve by consistently deviating from the algorithm’s suggested action. We show that for a given set of deviations over the strategy set of a player, it is possible to efﬁciently approximate ﬁxed points of a given deviation if and only if there exist efﬁcient no regret algorithms resistant to the deviations. Further, we show that if all players use a no regret algorithm, then the empirical distribution of their plays converges to an equilibrium. 1</p><p>2 0.84844238 <a title="54-lsi-2" href="./nips-2007-Regret_Minimization_in_Games_with_Incomplete_Information.html">165 nips-2007-Regret Minimization in Games with Incomplete Information</a></p>
<p>Author: Martin Zinkevich, Michael Johanson, Michael Bowling, Carmelo Piccione</p><p>Abstract: Extensive games are a powerful model of multiagent decision-making scenarios with incomplete information. Finding a Nash equilibrium for very large instances of these games has received a great deal of recent attention. In this paper, we describe a new technique for solving large games based on regret minimization. In particular, we introduce the notion of counterfactual regret, which exploits the degree of incomplete information in an extensive game. We show how minimizing counterfactual regret minimizes overall regret, and therefore in self-play can be used to compute a Nash equilibrium. We demonstrate this technique in the domain of poker, showing we can solve abstractions of limit Texas Hold’em with as many as 1012 states, two orders of magnitude larger than previous methods. 1</p><p>3 0.80659825 <a title="54-lsi-3" href="./nips-2007-The_Epoch-Greedy_Algorithm_for_Multi-armed_Bandits_with_Side_Information.html">194 nips-2007-The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information</a></p>
<p>Author: John Langford, Tong Zhang</p><p>Abstract: We present Epoch-Greedy, an algorithm for contextual multi-armed bandits (also known as bandits with side information). Epoch-Greedy has the following properties: 1. No knowledge of a time horizon T is necessary. 2. The regret incurred by Epoch-Greedy is controlled by a sample complexity bound for a hypothesis class. 3. The regret scales as O(T 2/3 S 1/3 ) or better (sometimes, much better). Here S is the complexity term in a sample complexity bound for standard supervised learning. 1</p><p>4 0.65972418 <a title="54-lsi-4" href="./nips-2007-The_Price_of_Bandit_Information_for_Online_Optimization.html">199 nips-2007-The Price of Bandit Information for Online Optimization</a></p>
<p>Author: Varsha Dani, Sham M. Kakade, Thomas P. Hayes</p><p>Abstract: In the online linear optimization problem, a learner must choose, in each round, a decision from a set D ⊂ Rn in order to minimize an (unknown and changing) linear cost function. We present sharp rates of convergence (with respect to additive regret) for both the full information setting (where the cost function is revealed at the end of each round) and the bandit setting (where only the scalar cost incurred is revealed). In particular, this paper is concerned with the price of bandit information, by which we mean the ratio of the best achievable regret in the bandit setting to that in the full-information setting. For the full informa√ tion case, the upper bound on the regret is O∗ ( nT ), where n is the ambient dimension and T is the time horizon. For the bandit case, we present an algorithm √ which achieves O∗ (n3/2 T ) regret — all previous (nontrivial) bounds here were O(poly(n)T 2/3 ) or worse. It is striking that the convergence rate for the bandit setting is only a factor of n worse than in the full information case — in stark contrast to the K-arm bandit setting, where the gap in the dependence on K is √ √ exponential ( T K√ vs. T log K). We also present lower bounds showing that this gap is at least n, which we conjecture to be the correct order. The bandit algorithm we present can be implemented efﬁciently in special cases of particular interest, such as path planning and Markov Decision Problems. 1</p><p>5 0.58236098 <a title="54-lsi-5" href="./nips-2007-Computing_Robust_Counter-Strategies.html">55 nips-2007-Computing Robust Counter-Strategies</a></p>
<p>Author: Michael Johanson, Martin Zinkevich, Michael Bowling</p><p>Abstract: Adaptation to other initially unknown agents often requires computing an effective counter-strategy. In the Bayesian paradigm, one must ﬁnd a good counterstrategy to the inferred posterior of the other agents’ behavior. In the experts paradigm, one may want to choose experts that are good counter-strategies to the other agents’ expected behavior. In this paper we introduce a technique for computing robust counter-strategies for adaptation in multiagent scenarios under a variety of paradigms. The strategies can take advantage of a suspected tendency in the decisions of the other agents, while bounding the worst-case performance when the tendency is not observed. The technique involves solving a modiﬁed game, and therefore can make use of recently developed algorithms for solving very large extensive games. We demonstrate the effectiveness of the technique in two-player Texas Hold’em. We show that the computed poker strategies are substantially more robust than best response counter-strategies, while still exploiting a suspected tendency. We also compose the generated strategies in an experts algorithm showing a dramatic improvement in performance over using simple best responses. 1</p><p>6 0.50837362 <a title="54-lsi-6" href="./nips-2007-Adaptive_Online_Gradient_Descent.html">21 nips-2007-Adaptive Online Gradient Descent</a></p>
<p>7 0.44646892 <a title="54-lsi-7" href="./nips-2007-Optimistic_Linear_Programming_gives_Logarithmic_Regret_for_Irreducible_MDPs.html">151 nips-2007-Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs</a></p>
<p>8 0.26474255 <a title="54-lsi-8" href="./nips-2007-Competition_Adds_Complexity.html">52 nips-2007-Competition Adds Complexity</a></p>
<p>9 0.24937519 <a title="54-lsi-9" href="./nips-2007-Congruence_between_model_and_human_attention_reveals_unique_signatures_of_critical_visual_events.html">57 nips-2007-Congruence between model and human attention reveals unique signatures of critical visual events</a></p>
<p>10 0.2463399 <a title="54-lsi-10" href="./nips-2007-TrueSkill_Through_Time%3A_Revisiting_the_History_of_Chess.html">208 nips-2007-TrueSkill Through Time: Revisiting the History of Chess</a></p>
<p>11 0.19429733 <a title="54-lsi-11" href="./nips-2007-A_Game-Theoretic_Approach_to_Apprenticeship_Learning.html">5 nips-2007-A Game-Theoretic Approach to Apprenticeship Learning</a></p>
<p>12 0.1939843 <a title="54-lsi-12" href="./nips-2007-Progressive_mixture_rules_are_deviation_suboptimal.html">159 nips-2007-Progressive mixture rules are deviation suboptimal</a></p>
<p>13 0.19386727 <a title="54-lsi-13" href="./nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</a></p>
<p>14 0.17552266 <a title="54-lsi-14" href="./nips-2007-Stability_Bounds_for_Non-i.i.d._Processes.html">184 nips-2007-Stability Bounds for Non-i.i.d. Processes</a></p>
<p>15 0.1737389 <a title="54-lsi-15" href="./nips-2007-COFI_RANK_-_Maximum_Margin_Matrix_Factorization_for_Collaborative_Ranking.html">41 nips-2007-COFI RANK - Maximum Margin Matrix Factorization for Collaborative Ranking</a></p>
<p>16 0.16862369 <a title="54-lsi-16" href="./nips-2007-Simulated_Annealing%3A_Rigorous_finite-time_guarantees_for_optimization_on_continuous_domains.html">178 nips-2007-Simulated Annealing: Rigorous finite-time guarantees for optimization on continuous domains</a></p>
<p>17 0.15606341 <a title="54-lsi-17" href="./nips-2007-Sequential_Hypothesis_Testing_under_Stochastic_Deadlines.html">176 nips-2007-Sequential Hypothesis Testing under Stochastic Deadlines</a></p>
<p>18 0.1531281 <a title="54-lsi-18" href="./nips-2007-Estimating_divergence_functionals_and_the_likelihood_ratio_by_penalized_convex_risk_minimization.html">82 nips-2007-Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization</a></p>
<p>19 0.15302296 <a title="54-lsi-19" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>20 0.15268001 <a title="54-lsi-20" href="./nips-2007-Modeling_homophily_and_stochastic_equivalence_in_symmetric_relational_data.html">131 nips-2007-Modeling homophily and stochastic equivalence in symmetric relational data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.026), (13, 0.026), (16, 0.022), (21, 0.068), (29, 0.049), (30, 0.312), (31, 0.017), (34, 0.032), (35, 0.043), (46, 0.013), (47, 0.063), (49, 0.018), (83, 0.145), (90, 0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70298696 <a title="54-lda-1" href="./nips-2007-Computational_Equivalence_of_Fixed_Points_and_No_Regret_Algorithms%2C_and_Convergence_to_Equilibria.html">54 nips-2007-Computational Equivalence of Fixed Points and No Regret Algorithms, and Convergence to Equilibria</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We study the relation between notions of game-theoretic equilibria which are based on stability under a set of deviations, and empirical equilibria which are reached by rational players. Rational players are modeled by players using no regret algorithms, which guarantee that their payoff in the long run is close to the maximum they could hope to achieve by consistently deviating from the algorithm’s suggested action. We show that for a given set of deviations over the strategy set of a player, it is possible to efﬁciently approximate ﬁxed points of a given deviation if and only if there exist efﬁcient no regret algorithms resistant to the deviations. Further, we show that if all players use a no regret algorithm, then the empirical distribution of their plays converges to an equilibrium. 1</p><p>2 0.52337188 <a title="54-lda-2" href="./nips-2007-Cooled_and_Relaxed_Survey_Propagation_for_MRFs.html">64 nips-2007-Cooled and Relaxed Survey Propagation for MRFs</a></p>
<p>Author: Hai L. Chieu, Wee S. Lee, Yee W. Teh</p><p>Abstract: We describe a new algorithm, Relaxed Survey Propagation (RSP), for ﬁnding MAP conﬁgurations in Markov random ﬁelds. We compare its performance with state-of-the-art algorithms including the max-product belief propagation, its sequential tree-reweighted variant, residual (sum-product) belief propagation, and tree-structured expectation propagation. We show that it outperforms all approaches for Ising models with mixed couplings, as well as on a web person disambiguation task formulated as a supervised clustering problem. 1</p><p>3 0.52317941 <a title="54-lda-3" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>Author: John Wright, Yangyu Tao, Zhouchen Lin, Yi Ma, Heung-yeung Shum</p><p>Abstract: We present a simple new criterion for classiﬁcation, based on principles from lossy data compression. The criterion assigns a test sample to the class that uses the minimum number of additional bits to code the test sample, subject to an allowable distortion. We prove asymptotic optimality of this criterion for Gaussian data and analyze its relationships to classical classiﬁers. Theoretical results provide new insights into relationships among popular classiﬁers such as MAP and RDA, as well as unsupervised clustering methods based on lossy compression [13]. Minimizing the lossy coding length induces a regularization effect which stabilizes the (implicit) density estimate in a small-sample setting. Compression also provides a uniform means of handling classes of varying dimension. This simple classiﬁcation criterion and its kernel and local versions perform competitively against existing classiﬁers on both synthetic examples and real imagery data such as handwritten digits and human faces, without requiring domain-speciﬁc information. 1</p><p>4 0.50936699 <a title="54-lda-4" href="./nips-2007-Statistical_Analysis_of_Semi-Supervised_Regression.html">186 nips-2007-Statistical Analysis of Semi-Supervised Regression</a></p>
<p>Author: Larry Wasserman, John D. Lafferty</p><p>Abstract: Semi-supervised methods use unlabeled data in addition to labeled data to construct predictors. While existing semi-supervised methods have shown some promising empirical performance, their development has been based largely based on heuristics. In this paper we study semi-supervised learning from the viewpoint of minimax theory. Our ﬁrst result shows that some common methods based on regularization using graph Laplacians do not lead to faster minimax rates of convergence. Thus, the estimators that use the unlabeled data do not have smaller risk than the estimators that use only labeled data. We then develop several new approaches that provably lead to improved performance. The statistical tools of minimax analysis are thus used to offer some new perspective on the problem of semi-supervised learning. 1</p><p>5 0.50641531 <a title="54-lda-5" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>Author: Kai Yu, Wei Chu</p><p>Abstract: This paper aims to model relational data on edges of networks. We describe appropriate Gaussian Processes (GPs) for directed, undirected, and bipartite networks. The inter-dependencies of edges can be effectively modeled by adapting the GP hyper-parameters. The framework suggests an intimate connection between link prediction and transfer learning, which were traditionally two separate research topics. We develop an efﬁcient learning algorithm that can handle a large number of observations. The experimental results on several real-world data sets verify superior learning capacity. 1</p><p>6 0.50464457 <a title="54-lda-6" href="./nips-2007-Ultrafast_Monte_Carlo_for_Statistical_Summations.html">209 nips-2007-Ultrafast Monte Carlo for Statistical Summations</a></p>
<p>7 0.50352752 <a title="54-lda-7" href="./nips-2007-Predictive_Matrix-Variate_t_Models.html">156 nips-2007-Predictive Matrix-Variate t Models</a></p>
<p>8 0.50311983 <a title="54-lda-8" href="./nips-2007-A_learning_framework_for_nearest_neighbor_search.html">16 nips-2007-A learning framework for nearest neighbor search</a></p>
<p>9 0.50259501 <a title="54-lda-9" href="./nips-2007-Semi-Supervised_Multitask_Learning.html">175 nips-2007-Semi-Supervised Multitask Learning</a></p>
<p>10 0.50184715 <a title="54-lda-10" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>11 0.50136721 <a title="54-lda-11" href="./nips-2007-Using_Deep_Belief_Nets_to_Learn_Covariance_Kernels_for_Gaussian_Processes.html">212 nips-2007-Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes</a></p>
<p>12 0.50097406 <a title="54-lda-12" href="./nips-2007-A_General_Boosting_Method_and_its_Application_to_Learning_Ranking_Functions_for_Web_Search.html">6 nips-2007-A General Boosting Method and its Application to Learning Ranking Functions for Web Search</a></p>
<p>13 0.50068492 <a title="54-lda-13" href="./nips-2007-Structured_Learning_with_Approximate_Inference.html">187 nips-2007-Structured Learning with Approximate Inference</a></p>
<p>14 0.49976093 <a title="54-lda-14" href="./nips-2007-Fitted_Q-iteration_in_continuous_action-space_MDPs.html">91 nips-2007-Fitted Q-iteration in continuous action-space MDPs</a></p>
<p>15 0.49969202 <a title="54-lda-15" href="./nips-2007-Fixing_Max-Product%3A_Convergent_Message_Passing_Algorithms_for_MAP_LP-Relaxations.html">92 nips-2007-Fixing Max-Product: Convergent Message Passing Algorithms for MAP LP-Relaxations</a></p>
<p>16 0.4994022 <a title="54-lda-16" href="./nips-2007-Cluster_Stability_for_Finite_Samples.html">46 nips-2007-Cluster Stability for Finite Samples</a></p>
<p>17 0.49890396 <a title="54-lda-17" href="./nips-2007-Message_Passing_for_Max-weight_Independent_Set.html">128 nips-2007-Message Passing for Max-weight Independent Set</a></p>
<p>18 0.49789351 <a title="54-lda-18" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>19 0.49786437 <a title="54-lda-19" href="./nips-2007-Efficient_Convex_Relaxation_for_Transductive_Support_Vector_Machine.html">76 nips-2007-Efficient Convex Relaxation for Transductive Support Vector Machine</a></p>
<p>20 0.49777266 <a title="54-lda-20" href="./nips-2007-Simulated_Annealing%3A_Rigorous_finite-time_guarantees_for_optimization_on_continuous_domains.html">178 nips-2007-Simulated Annealing: Rigorous finite-time guarantees for optimization on continuous domains</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
