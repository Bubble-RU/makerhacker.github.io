<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>16 nips-2007-A learning framework for nearest neighbor search</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-16" href="#">nips2007-16</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>16 nips-2007-A learning framework for nearest neighbor search</h1>
<br/><p>Source: <a title="nips-2007-16-pdf" href="http://papers.nips.cc/paper/3357-a-learning-framework-for-nearest-neighbor-search.pdf">pdf</a></p><p>Author: Lawrence Cayton, Sanjoy Dasgupta</p><p>Abstract: Can we leverage learning techniques to build a fast nearest-neighbor (ANN) retrieval data structure? We present a general learning framework for the NN problem in which sample queries are used to learn the parameters of a data structure that minimize the retrieval time and/or the miss rate. We explore the potential of this novel framework through two popular NN data structures: KD-trees and the rectilinear structures employed by locality sensitive hashing. We derive a generalization theory for these data structure classes and present simple learning algorithms for both. Experimental results reveal that learning often improves on the already strong performance of these data structures. 1</p><p>Reference: <a title="nips-2007-16-reference" href="../nips2007_reference/nips-2007-A_learning_framework_for_nearest_neighbor_search_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We present a general learning framework for the NN problem in which sample queries are used to learn the parameters of a data structure that minimize the retrieval time and/or the miss rate. [sent-6, score-0.574]
</p><p>2 We explore the potential of this novel framework through two popular NN data structures: KD-trees and the rectilinear structures employed by locality sensitive hashing. [sent-7, score-0.305]
</p><p>3 We derive a generalization theory for these data structure classes and present simple learning algorithms for both. [sent-8, score-0.153]
</p><p>4 1  Introduction  Nearest neighbor (NN) searching is a fundamental operation in machine learning, databases, signal processing, and a variety of other disciplines. [sent-10, score-0.132]
</p><p>5 , xn }, and on an input query q, we hope to return the nearest (or approximately nearest, or k-nearest) point(s) to q in X using some similarity measure. [sent-14, score-0.342]
</p><p>6 A tremendous amount of research has been devoted to designing data structures for fast NN retrieval. [sent-15, score-0.17]
</p><p>7 Most of these structures are based on some clever partitioning of the space and a few have bounds (typically worst-case) on the number of distance calculations necessary to query it. [sent-16, score-0.44]
</p><p>8 In contrast to the various data structures built using geometric intuitions, this learning framework allows one to construct a data structure by directly minimizing the cost of querying it. [sent-18, score-0.34]
</p><p>9 In our framework, a sample query set guides the construction of the data structure containing the database. [sent-19, score-0.26]
</p><p>10 In the absence of a sample query set, the database itself may be used as a reasonable prior. [sent-20, score-0.349]
</p><p>11 The problem of building a NN data structure can then be cast as a learning problem: Learn a data structure that yields efﬁcient retrieval times on the sample queries and is simple enough to generalize well. [sent-21, score-0.451]
</p><p>12 A major beneﬁt of this framework is that one can seamlessly handle situations where the query distribution is substantially different from the distribution of the database. [sent-22, score-0.212]
</p><p>13 We consider two different function classes that have performed well in NN searching: KD-trees and the cell structures employed by locality sensitive hashing. [sent-23, score-0.39]
</p><p>14 The known algorithms for these data structures do not, of course, use learning to choose the parameters. [sent-24, score-0.139]
</p><p>15 Nevertheless, we can examine the generalization properties of a data structure learned from one of these classes. [sent-25, score-0.157]
</p><p>16 1  2  Related work  There is a voluminous literature on data structures for nearest neighbor search, spanning several academic communities. [sent-29, score-0.36]
</p><p>17 Work on efﬁcient NN data structures can be classiﬁed according to two criteria: whether they return exact or approximate answers to queries; and whether they merely assume the distance function is a metric or make a stronger assumption (usually that the data are Euclidean). [sent-30, score-0.2]
</p><p>18 The framework we describe in this paper applies to all these methods, though we focus in particular on data structures for RD . [sent-31, score-0.177]
</p><p>19 Perhaps the most popular data structure for nearest neighbor search in RD is the simple and convenient KD-tree [1], which has enjoyed success in a vast range of applications. [sent-32, score-0.323]
</p><p>20 One recent line of work suggests randomly projecting points in the database down to a low-dimensional space, and then using KD-trees [3, 4]. [sent-35, score-0.172]
</p><p>21 Locality sensitive hashing (LSH) has emerged as a promising option for high-dimensional NN search in RD [5]. [sent-36, score-0.131]
</p><p>22 A recent trend is to look for data structures that are attuned to the intrinsic dimension, e. [sent-41, score-0.139]
</p><p>23 There has been some work on building a data structure for a particular query distribution [11]; this line of work is perhaps most similar to ours. [sent-45, score-0.255]
</p><p>24 Nevertheless, the learning theoretic approach in this paper is novel; the study of NN data structures through the lens of generalization ability provides a fundamentally different theoretical basis for NN search with important practical implications. [sent-47, score-0.225]
</p><p>25 , xn } denote the database and Q the space from which queries are drawn. [sent-53, score-0.359]
</p><p>26 We take a nearest neighbor data structure to be a mapping f : Q → 2X ; the interpretation is we compute distances only to f (q), not all of X. [sent-55, score-0.325]
</p><p>27 For example, the structure underlying LSH partitions RD into cells and a query is assigned to the subset of X that falls into the same cell. [sent-56, score-0.298]
</p><p>28 We want to only compute distances to a small fraction of the database on a query; and, in the case of probabilistic algorithms, we want a high probability of success. [sent-58, score-0.226]
</p><p>29 More precisely, we hope to minimize the following two quantities for a data structure f : • The fraction of X that we need to compute distances to: sizef (q) ≡  |f (q)| . [sent-59, score-0.567]
</p><p>30 n  • The fraction of a query’s k nearest neighbors that are missed: missf (q) ≡  |Γk (q) \ f (q)| k  (Γk (q) denotes the k nearest neighbors of q in X). [sent-60, score-0.781]
</p><p>31 2  In -approximate NN search, we only require a point x such that d(q, x) ≤ (1 + )d(q, X), so we instead use an approximate miss rate: missf (q) ≡ 1 [ x ∈ f (q) such that d(q, x) ≤ (1 + )d(q, X)] . [sent-61, score-0.619]
</p><p>32 None of the previously discussed data structures are built by explicitly minimizing these quantities, though there are known bounds for some. [sent-62, score-0.231]
</p><p>33 One reason is that research has typically focused on worst-case sizef and missf rates, which require minimizing these functions over all q ∈ Q. [sent-64, score-0.884]
</p><p>34 In this work, we instead focus on average-case sizef and missf rates—i. [sent-66, score-0.851]
</p><p>35 To do so, we assume that we are given a sample query set Q = {q1 , . [sent-69, score-0.206]
</p><p>36 We attempt to build f minimizing the empirical size and miss rates, then resort to generalization bounds to relate these rates to the true ones. [sent-73, score-0.238]
</p><p>37 The ﬁrst is based on a splitting rule for KD-trees designed to minimize a greedy surrogate for the empirical sizef function. [sent-75, score-0.461]
</p><p>38 The second is a algorithm that determines the boundary locations of the cell structure used in LSH that minimize a tradeoff of the empirical sizef and missf functions. [sent-76, score-1.131]
</p><p>39 1  KD-trees  KD-trees are a popular cell partitioning scheme for RD based on the binary search paradigm. [sent-78, score-0.194]
</p><p>40 The data structure is built by picking a dimension, splitting the database along the median value in that dimension, and then recursing on both halves. [sent-79, score-0.414]
</p><p>41 To ﬁnd a NN for a query q, one ﬁrst computes distances to all points in the same cell, then traverses up the tree. [sent-86, score-0.253]
</p><p>42 Explore right subtree:  Do not explore:  Typically the cells contain only a few points; a query is expensive because it lies close to many of the cell boundaries and much of the tree must be explored. [sent-89, score-0.419]
</p><p>43 Learning method Rather than picking the median split at each level, we use the training queries qi to pick a split that greedily minimizes the expected cost. [sent-90, score-0.633]
</p><p>44 A split s divides the sample queries (that are in the cell being split) into three sets: Qtc , those q that are “too close” to s—i. [sent-91, score-0.453]
</p><p>45 The split also divides the database points (that are in the cell being split) into Xl and Xr . [sent-95, score-0.377]
</p><p>46 3  cost(s) is a greedy surrogate for i sizef (qi ); evaluating the true average size would require a potentially costly recursion. [sent-97, score-0.399]
</p><p>47 Using a sample set led us to a very simple, natural cost function that can be used to pick splits in a principled manner. [sent-99, score-0.167]
</p><p>48 2  Locality sensitive hashing  LSH was a tremendous breakthrough in NN search as it led to data structures with provably sublinear (in the database size) retrieval time for approximate NN searching. [sent-101, score-0.512]
</p><p>49 It is built on an extremely simple space partitioning scheme which we refer to as a rectilinear cell structure (RCS). [sent-104, score-0.275]
</p><p>50 1 Project database down to O(log n) dimensions: xi → Rxi . [sent-106, score-0.143]
</p><p>51 On query q, one simply ﬁnds the cell that q belongs to, and returns the nearest x in that cell. [sent-109, score-0.414]
</p><p>52 We wish to select boundary locations minimizing the cost i missf (qi ) + λsizef (qi ), where λ is a tradeoff parameter (alternatively, one could ﬁx a miss rate that is reasonable, say 5%, and minimize the size). [sent-117, score-0.817]
</p><p>53 The cost of placing the boundaries at p1 , p2 , pB+1 can be decomposed as c[p1 , p2 ] + · · · + c[pB , pB+1 ], where c[pi , pi+1 ] =  |{x ∈ [pi , pi+1 ]}| . [sent-121, score-0.153]
</p><p>54 missf (q) + λ q∈[pi ,pi+1 ]  q∈[pi ,pi+1 ]  Let D be our dynamic programming table where D[p, i] is deﬁned as the cost of putting the ith boundary at position p and the remaining B + 1 − i to the right. [sent-122, score-0.564]
</p><p>55 Generalization theory2  5  In our framework, a nearest neighbor data structure is learned by speciﬁcally designing it to perform well on a set of sample queries. [sent-124, score-0.372]
</p><p>56 Recall the setting: there is a database X = {x1 , . [sent-126, score-0.143]
</p><p>57 , qm } drawn iid from some distribution D on Q, and we wish to learn a data structure f : Q → 2X drawn from a d  Dp is p-stable if for any v ∈ Rd and Z, X1 , . [sent-132, score-0.27]
</p><p>58 We are interested in the generalization of sizef (q) ≡ |f (q)| , and missf (q) ≡ n |Γk (q)\f (q)| , both of which have range [0, 1] ( missf (q) can be substituted for missf (q) throughout k this section). [sent-139, score-1.849]
</p><p>59 Suppose a data structure f is chosen from some class F, so as to have low empirical cost 1 m  m  sizef (qi ) and i=1  1 m  m  missf (qi ). [sent-140, score-0.95]
</p><p>60 i=1  Can we then conclude that data structure f will continue to perform well for subsequent queries drawn from the underlying distribution on Q? [sent-141, score-0.307]
</p><p>61 In other words, are the empirical estimates above necessarily close to the true expected values Eq∼D sizef (q) and Eq∼D missf (q) ? [sent-142, score-0.851]
</p><p>62 , ud , B)-rectilinear cell structure (RCS) in RD is a partition of RD into B d cells given by x → (h1 (x · u1 ), . [sent-162, score-0.33]
</p><p>63 , ud ∈ RD , and, for some positive integer B, let the set of data structures F consist of all (u1 , . [sent-173, score-0.278]
</p><p>64 Suppose there is an underlying distribution over queries in RD , from which m sample queries q1 , . [sent-178, score-0.464]
</p><p>65 Then sup E[missf ] −  f ∈F  1 m  m  2d(B − 1) log(m + n) + m  missf (qi ) ≤ 2 i=1  log(2/δ) m  and likewise for sizef . [sent-182, score-0.917]
</p><p>66 Along each axis ui there are B − 1 boundaries to be chosen and only m + n distinct locations for each of these (as far as partitioning of the xi ’s and qi ’s is concerned). [sent-195, score-0.286]
</p><p>67 , ud are chosen randomly, but, more remarkably, even if they are chosen based on X (for instance, by running PCA on X). [sent-202, score-0.139]
</p><p>68 , ud , B)-rectilinear cell structures for all choices of u1 , . [sent-208, score-0.384]
</p><p>69 Then with probability at least 1 − δ, sup E[missf ] −  f ∈F  1 m  m  missf (qi ) ≤ 2 i=1  2 + 2d(D + B − 2) log(m + n) + m  and likewise for sizef . [sent-212, score-0.917]
</p><p>70 Let F be the set of all depth η KD-trees in RD and X ⊂ RD be a database of points. [sent-220, score-0.185]
</p><p>71 Suppose there is an underlying distribution over queries in RD from which q1 , . [sent-221, score-0.216]
</p><p>72 Then with probability at least 1 − δ, sup E[missf ] −  f ∈F  1 m  m  missf (qi ) ≤ 2 i=1  (2η+1 − 2) log (D(3m + n)) + m  log (2/δ) m  A KD-tree utilizing median splits has depth η ≤ log n. [sent-225, score-0.857]
</p><p>73 The depth of a KD-tree with learned splits can be higher, though we found empirically that the depth was always much less than 2 log n (and can of course be restricted manually). [sent-226, score-0.241]
</p><p>74 1  KD-trees  First let us look at a simple example comparing the learned splits to median splits. [sent-229, score-0.279]
</p><p>75 Figure 1 shows a 2-dimensional dataset and the cell partitions produced by the learned splits and the median splits. [sent-230, score-0.424]
</p><p>76 The KD-tree constructed with the median splitting rule places nearly all of the boundaries running right through the queries. [sent-231, score-0.294]
</p><p>77 As a result, nearly the entire database will have to be searched for queries drawn from the center cluster distribution. [sent-232, score-0.396]
</p><p>78 The KD-tree with the learned splits places most of the boundaries right around the actual database points, ensuring that fewer leaves will need to be examined for each query. [sent-233, score-0.379]
</p><p>79 For this set of experiments, we used a randomly selected subset of the dataset as the database and a separate small subset as the test queries. [sent-237, score-0.143]
</p><p>80 For the sample queries, we used the database itself—i. [sent-238, score-0.175]
</p><p>81 We compare performance in terms of the average number of database points we have to compute distances to on a test set. [sent-242, score-0.222]
</p><p>82 data set  DB size  test pts  dim  Corel (UCI) Covertype (UCI) Letter (UCI) Pen digits (UCI) Bio (KDD) Physics (KDD)  32k 100k 18k 9k 100k 100k  5k 10k 2k 1k 10k 10k  # distance calculations median split learned split 1035. [sent-243, score-0.403]
</p><p>83 3  Figure 2: Percentage of DB examined as a function of (the approximation factor) for various query distributions. [sent-292, score-0.174]
</p><p>84 Random boundaries Random boundaries  Tuned boundaries Tuned boundaries  Figure 3: Example RCSs. [sent-293, score-0.432]
</p><p>85 This dataset allows us to explore the effect of differing query and database distributions in a natural setting. [sent-304, score-0.317]
</p><p>86 Figure 2 shows the results of running KD-trees using median and learned splits. [sent-306, score-0.216]
</p><p>87 In each case, 4000 images were chosen for the database (from across all the classes) and images from select classes were chosen for the queries. [sent-307, score-0.204]
</p><p>88 The “All” queries were chosen from all classes; the “Animals” were chosen from the 11 animal classes; the “N. [sent-308, score-0.243]
</p><p>89 Standard KD-trees are performing somewhat better than brute force in these experiments; the learned KD-trees yield much faster retrieval times across a range of approximation errors. [sent-310, score-0.133]
</p><p>90 Note also that the performance of the learned KD-tree seems to improve as the query distribution becomes simpler whereas the performance for the standard KD-tree actually degrades. [sent-311, score-0.239]
</p><p>91 The queries and DB are drawn from the same distribution. [sent-314, score-0.253]
</p><p>92 The learning algorithm adjusts the bin boundaries to the regions of density. [sent-315, score-0.149]
</p><p>93 Experimenting with RCS structures is somewhat challenging since there are two parameters to set (number of projections and boundaries), an approximation factor , and two quantities to compare (size and miss). [sent-316, score-0.209]
</p><p>94 Rather than minimizing a tradeoff between sizef and missf , we constrained the miss rate and optimized the sizef . [sent-319, score-1.421]
</p><p>95 We suspect that the learning algorithm helps substantially for the physics data because the one-dimensional projections are highly nonuniform whereas the MNIST one-dimensional projections are much more uniform. [sent-325, score-0.141]
</p><p>96 We used this framework to develop algorithms that learn RCSs and KD-trees optimized for a query distribution. [sent-346, score-0.212]
</p><p>97 Possible future work includes applying the learning framework to other data structures, though we expect that even stronger results may be obtained by using this framework to develop a novel data structure from the ground up. [sent-347, score-0.13]
</p><p>98 An algorithm for ﬁnding nearest neighbours in (approximately) constant average time. [sent-398, score-0.134]
</p><p>99 The analysis of a probabilistic approach to nearest neighbor searching. [sent-419, score-0.221]
</p><p>100 Analysis of approximate nearest neighbor searching with clustered point sets. [sent-424, score-0.266]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('missf', 0.48), ('sizef', 0.371), ('nn', 0.249), ('queries', 0.216), ('query', 0.174), ('rcss', 0.153), ('median', 0.151), ('database', 0.143), ('structures', 0.139), ('miss', 0.139), ('ud', 0.139), ('nearest', 0.134), ('lsh', 0.131), ('lshp', 0.131), ('rcs', 0.131), ('rd', 0.13), ('qi', 0.111), ('qtc', 0.109), ('boundaries', 0.108), ('cell', 0.106), ('qm', 0.095), ('neighbor', 0.087), ('uild', 0.087), ('db', 0.07), ('retrieval', 0.068), ('mount', 0.065), ('learned', 0.065), ('physics', 0.065), ('split', 0.064), ('splits', 0.063), ('classes', 0.061), ('animals', 0.058), ('mnist', 0.057), ('pi', 0.056), ('structure', 0.054), ('locality', 0.053), ('hashing', 0.052), ('distances', 0.05), ('uci', 0.048), ('search', 0.048), ('iid', 0.047), ('databases', 0.047), ('searching', 0.045), ('cost', 0.045), ('eq', 0.044), ('arya', 0.044), ('lefttree', 0.044), ('maneewongvatana', 0.044), ('rasiwasia', 0.044), ('rectilinear', 0.044), ('righttree', 0.044), ('pb', 0.044), ('depth', 0.042), ('gm', 0.042), ('dasgupta', 0.042), ('ree', 0.042), ('bin', 0.041), ('partitioning', 0.04), ('bears', 0.04), ('boundary', 0.039), ('partitions', 0.039), ('framework', 0.038), ('generalization', 0.038), ('ql', 0.038), ('projections', 0.038), ('fix', 0.037), ('drawn', 0.037), ('kdd', 0.035), ('splitting', 0.035), ('divides', 0.035), ('qr', 0.035), ('xr', 0.035), ('sup', 0.034), ('return', 0.034), ('minimizing', 0.033), ('fraction', 0.033), ('likewise', 0.032), ('quantities', 0.032), ('sample', 0.032), ('calculations', 0.032), ('sensitive', 0.031), ('built', 0.031), ('cells', 0.031), ('tremendous', 0.031), ('trees', 0.03), ('ann', 0.029), ('diego', 0.029), ('zm', 0.029), ('log', 0.029), ('points', 0.029), ('surrogate', 0.028), ('bounds', 0.028), ('dimension', 0.028), ('distance', 0.027), ('tradeoff', 0.027), ('locations', 0.027), ('animal', 0.027), ('pick', 0.027), ('minimize', 0.027), ('building', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="16-tfidf-1" href="./nips-2007-A_learning_framework_for_nearest_neighbor_search.html">16 nips-2007-A learning framework for nearest neighbor search</a></p>
<p>Author: Lawrence Cayton, Sanjoy Dasgupta</p><p>Abstract: Can we leverage learning techniques to build a fast nearest-neighbor (ANN) retrieval data structure? We present a general learning framework for the NN problem in which sample queries are used to learn the parameters of a data structure that minimize the retrieval time and/or the miss rate. We explore the potential of this novel framework through two popular NN data structures: KD-trees and the rectilinear structures employed by locality sensitive hashing. We derive a generalization theory for these data structure classes and present simple learning algorithms for both. Experimental results reveal that learning often improves on the already strong performance of these data structures. 1</p><p>2 0.17306174 <a title="16-tfidf-2" href="./nips-2007-Learning_the_structure_of_manifolds_using_random_projections.html">116 nips-2007-Learning the structure of manifolds using random projections</a></p>
<p>Author: Yoav Freund, Sanjoy Dasgupta, Mayank Kabra, Nakul Verma</p><p>Abstract: We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data. 1</p><p>3 0.078235917 <a title="16-tfidf-3" href="./nips-2007-Multiple-Instance_Active_Learning.html">136 nips-2007-Multiple-Instance Active Learning</a></p>
<p>Author: Burr Settles, Mark Craven, Soumya Ray</p><p>Abstract: We present a framework for active learning in the multiple-instance (MI) setting. In an MI learning problem, instances are naturally organized into bags and it is the bags, instead of individual instances, that are labeled for training. MI learners assume that every instance in a bag labeled negative is actually negative, whereas at least one instance in a bag labeled positive is actually positive. We consider the particular case in which an MI learner is allowed to selectively query unlabeled instances from positive bags. This approach is well motivated in domains in which it is inexpensive to acquire bag labels and possible, but expensive, to acquire instance labels. We describe a method for learning from labels at mixed levels of granularity, and introduce two active query selection strategies motivated by the MI setting. Our experiments show that learning from instance labels can signiﬁcantly improve performance of a basic MI learning algorithm in two multiple-instance domains: content-based image retrieval and text classiﬁcation. 1</p><p>4 0.077684842 <a title="16-tfidf-4" href="./nips-2007-Efficient_Bayesian_Inference_for_Dynamically_Changing_Graphs.html">75 nips-2007-Efficient Bayesian Inference for Dynamically Changing Graphs</a></p>
<p>Author: Ozgur Sumer, Umut Acar, Alexander T. Ihler, Ramgopal R. Mettu</p><p>Abstract: Motivated by stochastic systems in which observed evidence and conditional dependencies between states of the network change over time, and certain quantities of interest (marginal distributions, likelihood estimates etc.) must be updated, we study the problem of adaptive inference in tree-structured Bayesian networks. We describe an algorithm for adaptive inference that handles a broad range of changes to the network and is able to maintain marginal distributions, MAP estimates, and data likelihoods in all expected logarithmic time. We give an implementation of our algorithm and provide experiments that show that the algorithm can yield up to two orders of magnitude speedups on answering queries and responding to dynamic changes over the sum-product algorithm. 1</p><p>5 0.070096955 <a title="16-tfidf-5" href="./nips-2007-A_General_Boosting_Method_and_its_Application_to_Learning_Ranking_Functions_for_Web_Search.html">6 nips-2007-A General Boosting Method and its Application to Learning Ranking Functions for Web Search</a></p>
<p>Author: Zhaohui Zheng, Hongyuan Zha, Tong Zhang, Olivier Chapelle, Keke Chen, Gordon Sun</p><p>Abstract: We present a general boosting method extending functional gradient boosting to optimize complex loss functions that are encountered in many machine learning problems. Our approach is based on optimization of quadratic upper bounds of the loss functions which allows us to present a rigorous convergence analysis of the algorithm. More importantly, this general framework enables us to use a standard regression base learner such as single regression tree for £tting any loss function. We illustrate an application of the proposed method in learning ranking functions for Web search by combining both preference data and labeled data for training. We present experimental results for Web search using data from a commercial search engine that show signi£cant improvements of our proposed methods over some existing methods. 1</p><p>6 0.065661773 <a title="16-tfidf-6" href="./nips-2007-Random_Features_for_Large-Scale_Kernel_Machines.html">160 nips-2007-Random Features for Large-Scale Kernel Machines</a></p>
<p>7 0.060724668 <a title="16-tfidf-7" href="./nips-2007-Random_Projections_for_Manifold_Learning.html">161 nips-2007-Random Projections for Manifold Learning</a></p>
<p>8 0.060614567 <a title="16-tfidf-8" href="./nips-2007-Object_Recognition_by_Scene_Alignment.html">143 nips-2007-Object Recognition by Scene Alignment</a></p>
<p>9 0.059399977 <a title="16-tfidf-9" href="./nips-2007-A_Unified_Near-Optimal_Estimator_For_Dimension_Reduction_in_%24l_%5Calpha%24_%28%240%3C%5Calpha%5Cleq_2%24%29_Using_Stable_Random_Projections.html">13 nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</a></p>
<p>10 0.058685053 <a title="16-tfidf-10" href="./nips-2007-Active_Preference_Learning_with_Discrete_Choice_Data.html">19 nips-2007-Active Preference Learning with Discrete Choice Data</a></p>
<p>11 0.055649448 <a title="16-tfidf-11" href="./nips-2007-Consistent_Minimization_of_Clustering_Objective_Functions.html">58 nips-2007-Consistent Minimization of Clustering Objective Functions</a></p>
<p>12 0.055144332 <a title="16-tfidf-12" href="./nips-2007-McRank%3A_Learning_to_Rank_Using_Multiple_Classification_and_Gradient_Boosting.html">126 nips-2007-McRank: Learning to Rank Using Multiple Classification and Gradient Boosting</a></p>
<p>13 0.053705454 <a title="16-tfidf-13" href="./nips-2007-Random_Sampling_of_States_in_Dynamic_Programming.html">162 nips-2007-Random Sampling of States in Dynamic Programming</a></p>
<p>14 0.050567895 <a title="16-tfidf-14" href="./nips-2007-Learning_with_Transformation_Invariant_Kernels.html">118 nips-2007-Learning with Transformation Invariant Kernels</a></p>
<p>15 0.049078442 <a title="16-tfidf-15" href="./nips-2007-A_Randomized_Algorithm_for_Large_Scale_Support_Vector_Learning.html">10 nips-2007-A Randomized Algorithm for Large Scale Support Vector Learning</a></p>
<p>16 0.047634054 <a title="16-tfidf-16" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>17 0.04551468 <a title="16-tfidf-17" href="./nips-2007-Nearest-Neighbor-Based_Active_Learning_for_Rare_Category_Detection.html">139 nips-2007-Nearest-Neighbor-Based Active Learning for Rare Category Detection</a></p>
<p>18 0.044103868 <a title="16-tfidf-18" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>19 0.043678295 <a title="16-tfidf-19" href="./nips-2007-The_Infinite_Markov_Model.html">197 nips-2007-The Infinite Markov Model</a></p>
<p>20 0.043537151 <a title="16-tfidf-20" href="./nips-2007-Subspace-Based_Face_Recognition_in_Analog_VLSI.html">188 nips-2007-Subspace-Based Face Recognition in Analog VLSI</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.158), (1, 0.024), (2, -0.053), (3, 0.006), (4, -0.006), (5, 0.019), (6, 0.038), (7, 0.053), (8, 0.013), (9, 0.031), (10, -0.063), (11, 0.18), (12, 0.101), (13, -0.077), (14, -0.009), (15, 0.022), (16, 0.017), (17, -0.005), (18, 0.013), (19, -0.002), (20, 0.053), (21, 0.011), (22, 0.09), (23, 0.037), (24, 0.13), (25, -0.181), (26, -0.023), (27, 0.019), (28, -0.055), (29, -0.02), (30, -0.085), (31, -0.065), (32, -0.003), (33, -0.144), (34, -0.064), (35, 0.003), (36, -0.093), (37, -0.008), (38, -0.134), (39, -0.127), (40, -0.026), (41, -0.179), (42, 0.102), (43, -0.03), (44, -0.093), (45, -0.006), (46, -0.034), (47, 0.029), (48, 0.074), (49, -0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94456059 <a title="16-lsi-1" href="./nips-2007-A_learning_framework_for_nearest_neighbor_search.html">16 nips-2007-A learning framework for nearest neighbor search</a></p>
<p>Author: Lawrence Cayton, Sanjoy Dasgupta</p><p>Abstract: Can we leverage learning techniques to build a fast nearest-neighbor (ANN) retrieval data structure? We present a general learning framework for the NN problem in which sample queries are used to learn the parameters of a data structure that minimize the retrieval time and/or the miss rate. We explore the potential of this novel framework through two popular NN data structures: KD-trees and the rectilinear structures employed by locality sensitive hashing. We derive a generalization theory for these data structure classes and present simple learning algorithms for both. Experimental results reveal that learning often improves on the already strong performance of these data structures. 1</p><p>2 0.76980537 <a title="16-lsi-2" href="./nips-2007-Learning_the_structure_of_manifolds_using_random_projections.html">116 nips-2007-Learning the structure of manifolds using random projections</a></p>
<p>Author: Yoav Freund, Sanjoy Dasgupta, Mayank Kabra, Nakul Verma</p><p>Abstract: We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data. 1</p><p>3 0.53110486 <a title="16-lsi-3" href="./nips-2007-Efficient_Principled_Learning_of_Thin_Junction_Trees.html">78 nips-2007-Efficient Principled Learning of Thin Junction Trees</a></p>
<p>Author: Anton Chechetka, Carlos Guestrin</p><p>Abstract: We present the ﬁrst truly polynomial algorithm for PAC-learning the structure of bounded-treewidth junction trees – an attractive subclass of probabilistic graphical models that permits both the compact representation of probability distributions and efﬁcient exact inference. For a constant treewidth, our algorithm has polynomial time and sample complexity. If a junction tree with sufﬁciently strong intraclique dependencies exists, we provide strong theoretical guarantees in terms of KL divergence of the result from the true distribution. We also present a lazy extension of our approach that leads to very signiﬁcant speed ups in practice, and demonstrate the viability of our method empirically, on several real world datasets. One of our key new theoretical insights is a method for bounding the conditional mutual information of arbitrarily large sets of variables with only polynomially many mutual information computations on ﬁxed-size subsets of variables, if the underlying distribution can be approximated by a bounded-treewidth junction tree. 1</p><p>4 0.52558637 <a title="16-lsi-4" href="./nips-2007-Anytime_Induction_of_Cost-sensitive_Trees.html">27 nips-2007-Anytime Induction of Cost-sensitive Trees</a></p>
<p>Author: Saher Esmeir, Shaul Markovitch</p><p>Abstract: Machine learning techniques are increasingly being used to produce a wide-range of classiﬁers for complex real-world applications that involve nonuniform testing costs and misclassiﬁcation costs. As the complexity of these applications grows, the management of resources during the learning and classiﬁcation processes becomes a challenging task. In this work we introduce ACT (Anytime Cost-sensitive Trees), a novel framework for operating in such environments. ACT is an anytime algorithm that allows trading computation time for lower classiﬁcation costs. It builds a tree top-down and exploits additional time resources to obtain better estimations for the utility of the different candidate splits. Using sampling techniques ACT approximates for each candidate split the cost of the subtree under it and favors the one with a minimal cost. Due to its stochastic nature ACT is expected to be able to escape local minima, into which greedy methods may be trapped. Experiments with a variety of datasets were conducted to compare the performance of ACT to that of the state of the art cost-sensitive tree learners. The results show that for most domains ACT produces trees of signiﬁcantly lower costs. ACT is also shown to exhibit good anytime behavior with diminishing returns.</p><p>5 0.43850508 <a title="16-lsi-5" href="./nips-2007-Bayesian_Agglomerative_Clustering_with_Coalescents.html">31 nips-2007-Bayesian Agglomerative Clustering with Coalescents</a></p>
<p>Author: Yee W. Teh, Daniel J. Hsu, Hal Daume</p><p>Abstract: We introduce a new Bayesian model for hierarchical clustering based on a prior over trees called Kingman’s coalescent. We develop novel greedy and sequential Monte Carlo inferences which operate in a bottom-up agglomerative fashion. We show experimentally the superiority of our algorithms over the state-of-the-art, and demonstrate our approach in document clustering and phylolinguistics. 1</p><p>6 0.41650218 <a title="16-lsi-6" href="./nips-2007-Feature_Selection_Methods_for_Improving_Protein_Structure_Prediction_with_Rosetta.html">89 nips-2007-Feature Selection Methods for Improving Protein Structure Prediction with Rosetta</a></p>
<p>7 0.41530135 <a title="16-lsi-7" href="./nips-2007-Active_Preference_Learning_with_Discrete_Choice_Data.html">19 nips-2007-Active Preference Learning with Discrete Choice Data</a></p>
<p>8 0.41117814 <a title="16-lsi-8" href="./nips-2007-Random_Features_for_Large-Scale_Kernel_Machines.html">160 nips-2007-Random Features for Large-Scale Kernel Machines</a></p>
<p>9 0.384076 <a title="16-lsi-9" href="./nips-2007-Nearest-Neighbor-Based_Active_Learning_for_Rare_Category_Detection.html">139 nips-2007-Nearest-Neighbor-Based Active Learning for Rare Category Detection</a></p>
<p>10 0.37981328 <a title="16-lsi-10" href="./nips-2007-Efficient_Bayesian_Inference_for_Dynamically_Changing_Graphs.html">75 nips-2007-Efficient Bayesian Inference for Dynamically Changing Graphs</a></p>
<p>11 0.37623626 <a title="16-lsi-11" href="./nips-2007-A_Unified_Near-Optimal_Estimator_For_Dimension_Reduction_in_%24l_%5Calpha%24_%28%240%3C%5Calpha%5Cleq_2%24%29_Using_Stable_Random_Projections.html">13 nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</a></p>
<p>12 0.3702288 <a title="16-lsi-12" href="./nips-2007-Multiple-Instance_Active_Learning.html">136 nips-2007-Multiple-Instance Active Learning</a></p>
<p>13 0.35414115 <a title="16-lsi-13" href="./nips-2007-A_Randomized_Algorithm_for_Large_Scale_Support_Vector_Learning.html">10 nips-2007-A Randomized Algorithm for Large Scale Support Vector Learning</a></p>
<p>14 0.35108846 <a title="16-lsi-14" href="./nips-2007-Discriminative_Log-Linear_Grammars_with_Latent_Variables.html">72 nips-2007-Discriminative Log-Linear Grammars with Latent Variables</a></p>
<p>15 0.34857234 <a title="16-lsi-15" href="./nips-2007-Subspace-Based_Face_Recognition_in_Analog_VLSI.html">188 nips-2007-Subspace-Based Face Recognition in Analog VLSI</a></p>
<p>16 0.33959737 <a title="16-lsi-16" href="./nips-2007-Parallelizing_Support_Vector_Machines_on_Distributed_Computers.html">152 nips-2007-Parallelizing Support Vector Machines on Distributed Computers</a></p>
<p>17 0.32800922 <a title="16-lsi-17" href="./nips-2007-The_Infinite_Markov_Model.html">197 nips-2007-The Infinite Markov Model</a></p>
<p>18 0.32451788 <a title="16-lsi-18" href="./nips-2007-Consistent_Minimization_of_Clustering_Objective_Functions.html">58 nips-2007-Consistent Minimization of Clustering Objective Functions</a></p>
<p>19 0.32418966 <a title="16-lsi-19" href="./nips-2007-Random_Projections_for_Manifold_Learning.html">161 nips-2007-Random Projections for Manifold Learning</a></p>
<p>20 0.31864828 <a title="16-lsi-20" href="./nips-2007-The_Distribution_Family_of_Similarity_Distances.html">193 nips-2007-The Distribution Family of Similarity Distances</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.057), (13, 0.036), (16, 0.029), (21, 0.086), (31, 0.039), (34, 0.03), (35, 0.069), (47, 0.077), (49, 0.016), (75, 0.27), (83, 0.112), (85, 0.014), (87, 0.025), (90, 0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80573958 <a title="16-lda-1" href="./nips-2007-Learning_Visual_Attributes.html">113 nips-2007-Learning Visual Attributes</a></p>
<p>Author: Vittorio Ferrari, Andrew Zisserman</p><p>Abstract: We present a probabilistic generative model of visual attributes, together with an efﬁcient learning algorithm. Attributes are visual qualities of objects, such as ‘red’, ‘striped’, or ‘spotted’. The model sees attributes as patterns of image segments, repeatedly sharing some characteristic properties. These can be any combination of appearance, shape, or the layout of segments within the pattern. Moreover, attributes with general appearance are taken into account, such as the pattern of alternation of any two colors which is characteristic for stripes. To enable learning from unsegmented training images, the model is learnt discriminatively, by optimizing a likelihood ratio. As demonstrated in the experimental evaluation, our model can learn in a weakly supervised setting and encompasses a broad range of attributes. We show that attributes can be learnt starting from a text query to Google image search, and can then be used to recognize the attribute and determine its spatial extent in novel real-world images.</p><p>same-paper 2 0.74464691 <a title="16-lda-2" href="./nips-2007-A_learning_framework_for_nearest_neighbor_search.html">16 nips-2007-A learning framework for nearest neighbor search</a></p>
<p>Author: Lawrence Cayton, Sanjoy Dasgupta</p><p>Abstract: Can we leverage learning techniques to build a fast nearest-neighbor (ANN) retrieval data structure? We present a general learning framework for the NN problem in which sample queries are used to learn the parameters of a data structure that minimize the retrieval time and/or the miss rate. We explore the potential of this novel framework through two popular NN data structures: KD-trees and the rectilinear structures employed by locality sensitive hashing. We derive a generalization theory for these data structure classes and present simple learning algorithms for both. Experimental results reveal that learning often improves on the already strong performance of these data structures. 1</p><p>3 0.56377172 <a title="16-lda-3" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>Author: Matthias Bethge, Philipp Berens</p><p>Abstract: Maximum entropy analysis of binary variables provides an elegant way for studying the role of pairwise correlations in neural populations. Unfortunately, these approaches suffer from their poor scalability to high dimensions. In sensory coding, however, high-dimensional data is ubiquitous. Here, we introduce a new approach using a near-maximum entropy model, that makes this type of analysis feasible for very high-dimensional data—the model parameters can be derived in closed form and sampling is easy. Therefore, our NearMaxEnt approach can serve as a tool for testing predictions from a pairwise maximum entropy model not only for low-dimensional marginals, but also for high dimensional measurements of more than thousand units. We demonstrate its usefulness by studying natural images with dichotomized pixel intensities. Our results indicate that the statistics of such higher-dimensional measurements exhibit additional structure that are not predicted by pairwise correlations, despite the fact that pairwise correlations explain the lower-dimensional marginal statistics surprisingly well up to the limit of dimensionality where estimation of the full joint distribution is feasible. 1</p><p>4 0.5632031 <a title="16-lda-4" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>Author: Kai Yu, Wei Chu</p><p>Abstract: This paper aims to model relational data on edges of networks. We describe appropriate Gaussian Processes (GPs) for directed, undirected, and bipartite networks. The inter-dependencies of edges can be effectively modeled by adapting the GP hyper-parameters. The framework suggests an intimate connection between link prediction and transfer learning, which were traditionally two separate research topics. We develop an efﬁcient learning algorithm that can handle a large number of observations. The experimental results on several real-world data sets verify superior learning capacity. 1</p><p>5 0.55847138 <a title="16-lda-5" href="./nips-2007-Predictive_Matrix-Variate_t_Models.html">156 nips-2007-Predictive Matrix-Variate t Models</a></p>
<p>Author: Shenghuo Zhu, Kai Yu, Yihong Gong</p><p>Abstract: It is becoming increasingly important to learn from a partially-observed random matrix and predict its missing elements. We assume that the entire matrix is a single sample drawn from a matrix-variate t distribution and suggest a matrixvariate t model (MVTM) to predict those missing elements. We show that MVTM generalizes a range of known probabilistic models, and automatically performs model selection to encourage sparse predictive models. Due to the non-conjugacy of its prior, it is difﬁcult to make predictions by computing the mode or mean of the posterior distribution. We suggest an optimization method that sequentially minimizes a convex upper-bound of the log-likelihood, which is very efﬁcient and scalable. The experiments on a toy data and EachMovie dataset show a good predictive accuracy of the model. 1</p><p>6 0.55567819 <a title="16-lda-6" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>7 0.55392879 <a title="16-lda-7" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>8 0.5535832 <a title="16-lda-8" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>9 0.55332589 <a title="16-lda-9" href="./nips-2007-Semi-Supervised_Multitask_Learning.html">175 nips-2007-Semi-Supervised Multitask Learning</a></p>
<p>10 0.5520972 <a title="16-lda-10" href="./nips-2007-Scene_Segmentation_with_CRFs_Learned_from_Partially_Labeled_Images.html">172 nips-2007-Scene Segmentation with CRFs Learned from Partially Labeled Images</a></p>
<p>11 0.55120814 <a title="16-lda-11" href="./nips-2007-Probabilistic_Matrix_Factorization.html">158 nips-2007-Probabilistic Matrix Factorization</a></p>
<p>12 0.55017233 <a title="16-lda-12" href="./nips-2007-Discriminative_Batch_Mode_Active_Learning.html">69 nips-2007-Discriminative Batch Mode Active Learning</a></p>
<p>13 0.54981172 <a title="16-lda-13" href="./nips-2007-Ultrafast_Monte_Carlo_for_Statistical_Summations.html">209 nips-2007-Ultrafast Monte Carlo for Statistical Summations</a></p>
<p>14 0.54929042 <a title="16-lda-14" href="./nips-2007-Selecting_Observations_against_Adversarial_Objectives.html">174 nips-2007-Selecting Observations against Adversarial Objectives</a></p>
<p>15 0.54923046 <a title="16-lda-15" href="./nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">79 nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<p>16 0.54836881 <a title="16-lda-16" href="./nips-2007-Multi-Task_Learning_via_Conic_Programming.html">134 nips-2007-Multi-Task Learning via Conic Programming</a></p>
<p>17 0.54788166 <a title="16-lda-17" href="./nips-2007-Hidden_Common_Cause_Relations_in_Relational_Learning.html">97 nips-2007-Hidden Common Cause Relations in Relational Learning</a></p>
<p>18 0.54591298 <a title="16-lda-18" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<p>19 0.54582667 <a title="16-lda-19" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>20 0.54560894 <a title="16-lda-20" href="./nips-2007-A_Kernel_Statistical_Test_of_Independence.html">7 nips-2007-A Kernel Statistical Test of Independence</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
