<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>67 nips-2007-Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-67" href="#">nips2007-67</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>67 nips-2007-Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation</h1>
<br/><p>Source: <a title="nips-2007-67-pdf" href="http://papers.nips.cc/paper/3248-direct-importance-estimation-with-model-selection-and-its-application-to-covariate-shift-adaptation.pdf">pdf</a></p><p>Author: Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul V. Buenau, Motoaki Kawanabe</p><p>Abstract: A situation where training and test samples follow different input distributions is called covariate shift. Under covariate shift, standard learning methods such as maximum likelihood estimation are no longer consistent—weighted variants according to the ratio of test and training input densities are consistent. Therefore, accurately estimating the density ratio, called the importance, is one of the key issues in covariate shift adaptation. A naive approach to this task is to ﬁrst estimate training and test input densities separately and then estimate the importance by taking the ratio of the estimated densities. However, this naive approach tends to perform poorly since density estimation is a hard task particularly in high dimensional cases. In this paper, we propose a direct importance estimation method that does not involve density estimation. Our method is equipped with a natural cross validation procedure and hence tuning parameters such as the kernel width can be objectively optimized. Simulations illustrate the usefulness of our approach. 1</p><p>Reference: <a title="nips-2007-67-reference" href="../nips2007_reference/nips-2007-Direct_Importance_Estimation_with_Model_Selection_and_Its_Application_to_Covariate_Shift_Adaptation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract A situation where training and test samples follow different input distributions is called covariate shift. [sent-13, score-0.273]
</p><p>2 Under covariate shift, standard learning methods such as maximum likelihood estimation are no longer consistent—weighted variants according to the ratio of test and training input densities are consistent. [sent-14, score-0.3]
</p><p>3 Therefore, accurately estimating the density ratio, called the importance, is one of the key issues in covariate shift adaptation. [sent-15, score-0.265]
</p><p>4 A naive approach to this task is to ﬁrst estimate training and test input densities separately and then estimate the importance by taking the ratio of the estimated densities. [sent-16, score-0.373]
</p><p>5 However, this naive approach tends to perform poorly since density estimation is a hard task particularly in high dimensional cases. [sent-17, score-0.127]
</p><p>6 In this paper, we propose a direct importance estimation method that does not involve density estimation. [sent-18, score-0.241]
</p><p>7 Our method is equipped with a natural cross validation procedure and hence tuning parameters such as the kernel width can be objectively optimized. [sent-19, score-0.176]
</p><p>8 1  Introduction  A common assumption in supervised learning is that training and test samples follow the same distribution. [sent-21, score-0.1]
</p><p>9 A situation where the input distribution P (x) is different in the training and test phases but the conditional distribution of output values, P (y|x), remains unchanged is called covariate shift [8]. [sent-23, score-0.325]
</p><p>10 In many real-world applications such as robot control [10], bioinformatics [1], spam ﬁltering [3], brain-computer interfacing [9], or econometrics [5], covariate shift is conceivable and thus learning under covariate shift is gathering a lot of attention these days. [sent-24, score-0.434]
</p><p>11 The inﬂuence of covariate shift could be alleviated by weighting the log likelihood terms according to the importance [8]: w(x) = pte (x)/ptr (x), where pte (x) and ptr (x) are test and training input densities. [sent-25, score-1.181]
</p><p>12 Since the importance is usually unknown, the key issue of covariate shift adaptation is how to accurately estimate the importance. [sent-26, score-0.406]
</p><p>13 A naive approach to importance estimation would be to ﬁrst estimate the training and test densities separately from training and test input samples, and then estimate the importance by taking the ratio of the estimated densities. [sent-27, score-0.63]
</p><p>14 Therefore, this naive approach may not be effective—directly estimating the importance without estimating the densities would be more promising. [sent-29, score-0.247]
</p><p>15 Following this spirit, the kernel mean matching (KMM) method has been proposed recently [6], which directly gives importance estimates without going through density estimation. [sent-30, score-0.268]
</p><p>16 KMM is shown 1  to work well, given that tuning parameters such as the kernel width are chosen appropriately. [sent-31, score-0.122]
</p><p>17 Intuitively, model selection of importance estimation algorithms (such as KMM) is straightforward by cross validation (CV) over the performance of subsequent learning algorithms. [sent-32, score-0.265]
</p><p>18 However, this is highly unreliable since the ordinary CV score is heavily biased under covariate shift—for unbiased estimation of the prediction performance of subsequent learning algorithms, the CV procedure itself needs to be importance-weighted [9]. [sent-33, score-0.18]
</p><p>19 Since the importance weight has to have been ﬁxed when model selection is carried out by importance weighted CV, it can not be used for model selection of importance estimation algorithms. [sent-34, score-0.636]
</p><p>20 The above fact implies that model selection of importance estimation algorithms should be performed within the importance estimation step in an unsupervised manner. [sent-35, score-0.43]
</p><p>21 However, since KMM can only estimate the values of the importance at training input points, it can not be directly applied in the CV framework; an out-of-sample extension is needed, but this seems to be an open research issue currently. [sent-36, score-0.256]
</p><p>22 In this paper, we propose a new importance estimation method which can overcome the above problems, i. [sent-37, score-0.195]
</p><p>23 , the proposed method directly estimates the importance without density estimation and is equipped with a natural model selection procedure. [sent-39, score-0.299]
</p><p>24 Our basic idea is to ﬁnd an importance estimate w(x) such that the Kullback-Leibler divergence from the true test input density p te (x) to its estimate pte (x) = w(x)ptr (x) is minimized. [sent-40, score-0.672]
</p><p>25 We propose an algorithm that can carry out this minimization without explicitly modeling ptr (x) and pte (x). [sent-41, score-0.414]
</p><p>26 Furthermore, the solution tends to be sparse, which contributes to reducing the computational cost in the test phase. [sent-44, score-0.087]
</p><p>27 Since KLIEP is based on the minimization of the Kullback-Leibler divergence, its model selection can be naturally carried out through a variant of likelihood CV, which is a standard model selection technique in density estimation. [sent-45, score-0.136]
</p><p>28 A key advantage of our CV procedure is that, not the training samples, but the test input samples are cross-validated. [sent-46, score-0.166]
</p><p>29 This highly contributes to improving the model selection accuracy since the number of training samples is typically limited while test input samples are abundantly available. [sent-47, score-0.261]
</p><p>30 The simulation studies show that KLIEP tends to outperform existing approaches in importance estimation including the logistic regression based method [2], and it contributes to improving the prediction performance in covariate shift scenarios. [sent-48, score-0.5]
</p><p>31 2  New Importance Estimation Method  In this section, we propose a new importance estimation method. [sent-49, score-0.195]
</p><p>32 training input samples {x tr }ntr i i=1 from a training input distribution with density ptr (x) and i. [sent-54, score-0.428]
</p><p>33 test input samples {xte }nte from a j j=1 test input distribution with density pte (x). [sent-57, score-0.502]
</p><p>34 Typically, the number ntr of training samples is rather small, while the number nte of test input samples is very large. [sent-59, score-0.918]
</p><p>35 The goal of this paper is to develop a method of estimating the importance w(x) from {xtr }ntr and {xte }nte : i i=1 j j=1 pte (x) w(x) = . [sent-60, score-0.436]
</p><p>36 ptr (x) Our key restriction is that we avoid estimating densities pte (x) and ptr (x) when estimating the importance w(x). [sent-61, score-0.801]
</p><p>37 Using the model w(x), we can estimate the test input density pte (x) by pte (x) = w(x)ptr (x). [sent-71, score-0.652]
</p><p>38 }b=1  We determine the parameters {α pte (x) to pte (x) is minimized:  in the model (1) so that the Kullback-Leibler divergence from  pte (x) dx w(x)ptr (x) D pte (x) = pte (x) log dx − pte (x) log w(x)dx. [sent-72, score-1.603]
</p><p>39 ptr (x) D D  KL[pte (x) pte (x)] =  pte (x) log  Since the ﬁrst term in the last equation is independent of {α }b=1 , we ignore it and focus on the second term. [sent-73, score-0.68]
</p><p>40 We denote it by J: J=  pte (x) log w(x)dx  (2)  D  1 ≈ nte  nte  log w(xte ) j j=1  1 = nte  nte  b  α ϕ (xte ) , j  log =1  j=1  where the empirical approximation based on the test input samples {xte }nte is used from the ﬁrst j j=1 line to the second line above. [sent-74, score-1.683]
</p><p>41 Note that the above objective function only involves the test input samples {xte }nte , i. [sent-76, score-0.121]
</p><p>42 , we did not use the training input samples {xtr }ntr yet. [sent-78, score-0.115]
</p><p>43 i i=1  w(x) is an estimate of the importance w(x) which is non-negative by deﬁnition. [sent-80, score-0.179]
</p><p>44   nte  maximize  {α }b=1  b  log  j=1  =1  ntr  α ϕ (xte )  j b  subject to i=1 =1  α ϕ (xtr ) = ntr and α1 , α2 , . [sent-87, score-1.159]
</p><p>45 Note that the solution {α }b=1 tends to be sparse [4], which contributes to reducing the computational cost in the test phase. [sent-95, score-0.087]
</p><p>46 /Aα); α ←− α + (1 − b α)b/(b b); α ←− max(0, α); α ←− α/(b α); end P w(x) ←− b=1 α ϕ (x); b  te Split {xte }nte into R disjoint subsets {Xr }R ; r=1 j j=1 for each model m ∈ M for each split r = 1, . [sent-98, score-0.113]
</p><p>47 The expectation over pte (x) involved in J can be numerically approximated by likelihood cross validation (LCV) as follows: First, divide the test samte ples {xte }nte into R disjoint subsets {Xr }R . [sent-110, score-0.328]
</p><p>48 Then obtain an importance estimate wr (x) from r=1 j j=1 te te {Xj }j=r and approximate the score J using Xr as Jr =  1 te |Xr |  log wr (x). [sent-111, score-0.552]
</p><p>49 te x∈Xr  We repeat this procedure for r = 1, 2, . [sent-112, score-0.121]
</p><p>50 On the other hand, in our CV procedure, the data splitting is performed over the test input samples, not over the training samples. [sent-118, score-0.122]
</p><p>51 Since we typically have a large number of test input samples, our CV procedure does not suffer from the small sample problem. [sent-119, score-0.1]
</p><p>52 As model candidates, we propose using a Gaussian kernel model centered at the test input points {xte }nte , i. [sent-121, score-0.154]
</p><p>53 , j j=1 nte  α Kσ (x, xte ),  w(x) = =1  where Kσ (x, x ) is the Gaussian kernel with kernel width σ: Kσ (x, x ) = exp − 4  x−x 2σ 2  2  . [sent-123, score-0.721]
</p><p>54 (5)  The reason why we chose the test input points {xte }nte as the Gaussian centers, not the training j j=1 input points {xtr }ntr , is as follows. [sent-124, score-0.16]
</p><p>55 By deﬁnition, the importance w(x) tends to take large values i i=1 if the training input density ptr (x) is small and the test input density pte (x) is large; conversely, w(x) tends to be small (i. [sent-125, score-0.901]
</p><p>56 , close to zero) if ptr (x) is large and pte (x) is small. [sent-127, score-0.414]
</p><p>57 When a function is approximated by a Gaussian kernel model, many kernels may be needed in the region where the output of the target function is large; on the other hand, only a small number of kernels would be enough in the region where the output of the target function is close to zero. [sent-128, score-0.111]
</p><p>58 Following this heuristic, we decided to allocate many kernels at high test input density regions, which can be achieved by setting the Gaussian centers at the test input points {xte }nte . [sent-129, score-0.249]
</p><p>59 Since nte is typically very large, just using all the test input points {xte }nte j j=1 as Gaussian centers is already computationally rather demanding. [sent-132, score-0.418]
</p><p>60 , j b  w(x) =  α Kσ (x, c ),  (6)  =1  where c is a template point randomly chosen from {xte }nte and b (≤ nte ) is a preﬁxed number. [sent-135, score-0.347]
</p><p>61 j j=1 In the rest of this paper, we ﬁx the number of template points at b = min(100, nte ), and optimize the kernel width σ by the above CV procedure. [sent-136, score-0.435]
</p><p>62 1  Importance Estimation for Artiﬁcial Data Sets  Let ptr (x) be the d-dimensional Gaussian density with mean (0, 0, . [sent-139, score-0.208]
</p><p>63 , 0) and covariance identity and pte (x) be the d-dimensional Gaussian density with mean (1, 0, . [sent-142, score-0.298]
</p><p>64 The task is to estimate the importance at training input points: wi = w(xtr ) = i  pte (xtr ) i ptr (xtr ) i  for i = 1, 2, . [sent-146, score-0.706]
</p><p>65 Since the peri=1 formance of KLIEP is dependent on the kernel width σ, we test several different values of σ. [sent-151, score-0.147]
</p><p>66 KLIEP(CV): The kernel width σ in KLIEP is chosen based on 5-fold LCV (see Section 2. [sent-152, score-0.11]
</p><p>67 KDE(CV): {wi }ntr are estimated through the kernel density estimator (KDE) with the Gaussian i=1 kernel. [sent-154, score-0.099]
</p><p>68 The kernel widths for the training and test densities are chosen separately based on 5-fold likelihood cross-validation. [sent-155, score-0.186]
</p><p>69 We set B = 1000 and = ( ntr − √ 1)/ ntr following the paper [6], and test several different values of σ. [sent-158, score-0.862]
</p><p>70 Since the performance of LogReg is dependent on the kernel width σ, we test several different values of σ. [sent-162, score-0.147]
</p><p>71 LogReg(CV): The kernel width σ in LogReg is chosen based on 5-fold CV. [sent-164, score-0.11]
</p><p>72 We ﬁxed the number of test input points at nte = 1000 and consider the following two settings for the number ntr of training samples and the input dimension d: (a) ntr = 100 and d = 1, 2, . [sent-172, score-1.343]
</p><p>73 We run the experiments 100 times for each d, each ntr , and each method, and evaluate the quality of the importance estimates {wi }ntr by the normalized mean squared error (NMSE): i=1 NMSE =  1 ntr  ntr i=1  wi ntr i =1  wi  −  wi ntr i =1  2  wi  . [sent-179, score-2.383]
</p><p>74 Figure 2(a) shows that the error of KDE(CV) sharply increases as the input dimension grows, while KLIEP, KMM, and LogReg with appropriate kernel widths tend to give smaller errors than KDE(CV). [sent-181, score-0.144]
</p><p>75 This would be the fruit of directly estimating the importance without going through density estimation. [sent-182, score-0.23]
</p><p>76 The graph also show that the performance of KLIEP, KMM, and LogReg is dependent on the kernel width σ—the results of KLIEP(CV) and LogReg(CV) show that model selection is carried out reasonably well and KLIEP(CV) works signiﬁcantly better than LogReg(CV). [sent-183, score-0.174]
</p><p>77 Figure 2(b) shows that the errors of all methods tend to decrease as the number of training samples grows. [sent-184, score-0.086]
</p><p>78 Again, KLIEP, KMM, and LogReg with appropriate kernel widths tend to give smaller errors than KDE(CV). [sent-185, score-0.095]
</p><p>79 Overall, KLIEP(CV) is shown to be a useful method in importance estimation. [sent-187, score-0.169]
</p><p>80 2  Covariate Shift Adaptation with Regression and Classiﬁcation Benchmark Data Sets  Here we employ importance estimation methods for covariate shift adaptation in regression and classiﬁcation benchmark problems (see Table 1). [sent-189, score-0.439]
</p><p>81 We normalize all the input samples k=1 te {xk }n into [0, 1]d and choose the test samples {(xte , yj )}nte from the pool {(xk , yk )}n as j j=1 k=1 k=1 follows. [sent-191, score-0.293]
</p><p>82 We choose the training samples tr {(xtr , yi )}ntr uniformly from the rest. [sent-193, score-0.094]
</p><p>83 Intuitively, in this experiment, the test input density tends i i=1 6  (c)  to be lower than the training input density when xk is small. [sent-194, score-0.317]
</p><p>84 We set the number of samples at tr ntr = 100 and nte = 500 for all data sets. [sent-195, score-0.797]
</p><p>85 Note that we only use {(xtr , yi )}ntr and {xte }nte i j j=1 i=1 nte te for training regressors or classiﬁers; the test output values {yj }j=1 are used only for evaluating the generalization performance. [sent-196, score-0.481]
</p><p>86 We use the following kernel model for regression or classiﬁcation: t  f (x; θ) =  θ Kh (x, m ), =1  where Kh (x, x ) is the Gaussian kernel (5) and m is a template point randomly chosen from {xte }nte . [sent-197, score-0.162]
</p><p>87 We learn the parameter θ by importancej j=1 weighted regularized least squares (IWRLS) [9]: ntr  θ IW RLS ≡ argmin θ  i=1  tr w(xtr ) f (xtr ; θ) − yi i i  2  +λ θ  2  . [sent-199, score-0.455]
</p><p>88 The kernel width h and the regularization parameter λ in IWRLS (7) are chosen by 5-fold importance weighted CV (IWCV) [9]. [sent-207, score-0.292]
</p><p>89 2  We run the experiments 100 times for each data set and evaluate the mean test error: 1 nte  nte te L f (xte ), yj . [sent-209, score-0.772]
</p><p>90 The table shows that KLIEP(CV) compares favorably with Uniform, implying that the importance weighted methods combined with KLIEP(CV) are useful for improving the prediction performance under covariate shift. [sent-213, score-0.32]
</p><p>91 We tested 10 different values of the kernel width σ for KMM and described three representative results in the table. [sent-215, score-0.099]
</p><p>92 Overall, we conclude that the proposed KLIEP(CV) is a promising method for covariate shift adaptation. [sent-218, score-0.216]
</p><p>93 4  Conclusions  In this paper, we addressed the problem of estimating the importance for covariate shift adaptation. [sent-219, score-0.388]
</p><p>94 The proposed method, called KLIEP, does not involve density estimation so it is more advantageous than a naive KDE-based approach particularly in high-dimensional problems. [sent-220, score-0.094]
</p><p>95 All the error values are normalized so that the mean error by ‘Uniform’ (uniform weighting, or equivalently no importance weighting) is one. [sent-223, score-0.169]
</p><p>96 37)  which also directly gives importance estimates, KLIEP is practically more useful since it is equipped with a model selection procedure. [sent-369, score-0.227]
</p><p>97 Our experiments highlighted these advantages and therefore KLIEP is shown to be a promising method for covariate shift adaptation. [sent-370, score-0.216]
</p><p>98 In KLIEP, we modeled the importance function by a linear (or kernel) model, which resulted in a convex optimization problem with a sparse solution. [sent-371, score-0.169]
</p><p>99 Finally, the range of application of importance weights is not limited to covariate shift adaptation. [sent-374, score-0.373]
</p><p>100 Improving predictive inference under covariate shift by weighting the log-likelihood function. [sent-432, score-0.221]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kliep', 0.454), ('ntr', 0.414), ('nte', 0.317), ('cv', 0.31), ('pte', 0.252), ('xte', 0.252), ('kmm', 0.242), ('logreg', 0.222), ('xtr', 0.222), ('importance', 0.169), ('ptr', 0.162), ('covariate', 0.124), ('te', 0.092), ('shift', 0.08), ('kde', 0.075), ('kernel', 0.053), ('input', 0.049), ('density', 0.046), ('width', 0.046), ('jr', 0.044), ('xr', 0.04), ('samples', 0.038), ('wi', 0.036), ('nmse', 0.035), ('wr', 0.035), ('test', 0.034), ('tends', 0.033), ('xk', 0.032), ('selection', 0.031), ('lcv', 0.03), ('training', 0.028), ('tr', 0.028), ('estimation', 0.026), ('densities', 0.026), ('adaptation', 0.023), ('widths', 0.022), ('kh', 0.022), ('dx', 0.022), ('naive', 0.022), ('iwcv', 0.02), ('iwrls', 0.02), ('nmses', 0.02), ('rls', 0.02), ('contributes', 0.02), ('kernels', 0.019), ('template', 0.019), ('equipped', 0.018), ('centers', 0.018), ('sugiyama', 0.018), ('pseudo', 0.017), ('weighting', 0.017), ('procedure', 0.017), ('logistic', 0.017), ('regression', 0.017), ('cross', 0.017), ('candidates', 0.016), ('zr', 0.016), ('spam', 0.016), ('yk', 0.016), ('estimating', 0.015), ('log', 0.014), ('gaussian', 0.014), ('iw', 0.014), ('improving', 0.014), ('pool', 0.014), ('dependent', 0.014), ('trials', 0.014), ('uniform', 0.013), ('ratio', 0.013), ('score', 0.013), ('weighted', 0.013), ('bickel', 0.013), ('validation', 0.013), ('separately', 0.012), ('yj', 0.012), ('hoffman', 0.012), ('accept', 0.012), ('promising', 0.012), ('repeat', 0.012), ('tuning', 0.012), ('disjoint', 0.012), ('mk', 0.012), ('summarized', 0.011), ('reasonably', 0.011), ('chosen', 0.011), ('splitting', 0.011), ('tend', 0.01), ('ibm', 0.01), ('code', 0.01), ('carried', 0.01), ('estimate', 0.01), ('output', 0.01), ('sch', 0.01), ('divergence', 0.01), ('bioinformatics', 0.01), ('errors', 0.01), ('cambridge', 0.01), ('platt', 0.01), ('basis', 0.009), ('model', 0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="67-tfidf-1" href="./nips-2007-Direct_Importance_Estimation_with_Model_Selection_and_Its_Application_to_Covariate_Shift_Adaptation.html">67 nips-2007-Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation</a></p>
<p>Author: Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul V. Buenau, Motoaki Kawanabe</p><p>Abstract: A situation where training and test samples follow different input distributions is called covariate shift. Under covariate shift, standard learning methods such as maximum likelihood estimation are no longer consistent—weighted variants according to the ratio of test and training input densities are consistent. Therefore, accurately estimating the density ratio, called the importance, is one of the key issues in covariate shift adaptation. A naive approach to this task is to ﬁrst estimate training and test input densities separately and then estimate the importance by taking the ratio of the estimated densities. However, this naive approach tends to perform poorly since density estimation is a hard task particularly in high dimensional cases. In this paper, we propose a direct importance estimation method that does not involve density estimation. Our method is equipped with a natural cross validation procedure and hence tuning parameters such as the kernel width can be objectively optimized. Simulations illustrate the usefulness of our approach. 1</p><p>2 0.045352645 <a title="67-tfidf-2" href="./nips-2007-Topmoumoute_Online_Natural_Gradient_Algorithm.html">206 nips-2007-Topmoumoute Online Natural Gradient Algorithm</a></p>
<p>Author: Nicolas L. Roux, Pierre-antoine Manzagol, Yoshua Bengio</p><p>Abstract: Guided by the goal of obtaining an optimization algorithm that is both fast and yields good generalization, we study the descent direction maximizing the decrease in generalization error or the probability of not increasing generalization error. The surprising result is that from both the Bayesian and frequentist perspectives this can yield the natural gradient direction. Although that direction can be very expensive to compute we develop an efﬁcient, general, online approximation to the natural gradient descent which is suited to large scale problems. We report experimental results showing much faster convergence in computation time and in number of iterations with TONGA (Topmoumoute Online natural Gradient Algorithm) than with stochastic gradient descent, even on very large datasets.</p><p>3 0.034684308 <a title="67-tfidf-3" href="./nips-2007-Linear_programming_analysis_of_loopy_belief_propagation_for_weighted_matching.html">120 nips-2007-Linear programming analysis of loopy belief propagation for weighted matching</a></p>
<p>Author: Sujay Sanghavi, Dmitry Malioutov, Alan S. Willsky</p><p>Abstract: Loopy belief propagation has been employed in a wide variety of applications with great empirical success, but it comes with few theoretical guarantees. In this paper we investigate the use of the max-product form of belief propagation for weighted matching problems on general graphs. We show that max-product converges to the correct answer if the linear programming (LP) relaxation of the weighted matching problem is tight and does not converge if the LP relaxation is loose. This provides an exact characterization of max-product performance and reveals connections to the widely used optimization technique of LP relaxation. In addition, we demonstrate that max-product is effective in solving practical weighted matching problems in a distributed fashion by applying it to the problem of self-organization in sensor networks. 1</p><p>4 0.03286849 <a title="67-tfidf-4" href="./nips-2007-Efficient_Bayesian_Inference_for_Dynamically_Changing_Graphs.html">75 nips-2007-Efficient Bayesian Inference for Dynamically Changing Graphs</a></p>
<p>Author: Ozgur Sumer, Umut Acar, Alexander T. Ihler, Ramgopal R. Mettu</p><p>Abstract: Motivated by stochastic systems in which observed evidence and conditional dependencies between states of the network change over time, and certain quantities of interest (marginal distributions, likelihood estimates etc.) must be updated, we study the problem of adaptive inference in tree-structured Bayesian networks. We describe an algorithm for adaptive inference that handles a broad range of changes to the network and is able to maintain marginal distributions, MAP estimates, and data likelihoods in all expected logarithmic time. We give an implementation of our algorithm and provide experiments that show that the algorithm can yield up to two orders of magnitude speedups on answering queries and responding to dynamic changes over the sum-product algorithm. 1</p><p>5 0.031751078 <a title="67-tfidf-5" href="./nips-2007-Random_Features_for_Large-Scale_Kernel_Machines.html">160 nips-2007-Random Features for Large-Scale Kernel Machines</a></p>
<p>Author: Ali Rahimi, Benjamin Recht</p><p>Abstract: To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user speciﬁed shiftinvariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classiﬁcation and regression tasks linear machine learning algorithms applied to these features outperform state-of-the-art large-scale kernel machines. 1</p><p>6 0.029806551 <a title="67-tfidf-6" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>7 0.029629793 <a title="67-tfidf-7" href="./nips-2007-Support_Vector_Machine_Classification_with_Indefinite_Kernels.html">190 nips-2007-Support Vector Machine Classification with Indefinite Kernels</a></p>
<p>8 0.028833214 <a title="67-tfidf-8" href="./nips-2007-Reinforcement_Learning_in_Continuous_Action_Spaces_through_Sequential_Monte_Carlo_Methods.html">168 nips-2007-Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods</a></p>
<p>9 0.027418787 <a title="67-tfidf-9" href="./nips-2007-Multi-Task_Learning_via_Conic_Programming.html">134 nips-2007-Multi-Task Learning via Conic Programming</a></p>
<p>10 0.026613489 <a title="67-tfidf-10" href="./nips-2007-Learning_with_Transformation_Invariant_Kernels.html">118 nips-2007-Learning with Transformation Invariant Kernels</a></p>
<p>11 0.026415005 <a title="67-tfidf-11" href="./nips-2007-SpAM%3A_Sparse_Additive_Models.html">179 nips-2007-SpAM: Sparse Additive Models</a></p>
<p>12 0.025916191 <a title="67-tfidf-12" href="./nips-2007-Statistical_Analysis_of_Semi-Supervised_Regression.html">186 nips-2007-Statistical Analysis of Semi-Supervised Regression</a></p>
<p>13 0.025888504 <a title="67-tfidf-13" href="./nips-2007-On_Sparsity_and_Overcompleteness_in_Image_Models.html">145 nips-2007-On Sparsity and Overcompleteness in Image Models</a></p>
<p>14 0.025274925 <a title="67-tfidf-14" href="./nips-2007-Testing_for_Homogeneity_with_Kernel_Fisher_Discriminant_Analysis.html">192 nips-2007-Testing for Homogeneity with Kernel Fisher Discriminant Analysis</a></p>
<p>15 0.024860954 <a title="67-tfidf-15" href="./nips-2007-The_rat_as_particle_filter.html">203 nips-2007-The rat as particle filter</a></p>
<p>16 0.023246739 <a title="67-tfidf-16" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<p>17 0.022978045 <a title="67-tfidf-17" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>18 0.022789454 <a title="67-tfidf-18" href="./nips-2007-A_Kernel_Statistical_Test_of_Independence.html">7 nips-2007-A Kernel Statistical Test of Independence</a></p>
<p>19 0.022515127 <a title="67-tfidf-19" href="./nips-2007-Learning_Bounds_for_Domain_Adaptation.html">110 nips-2007-Learning Bounds for Domain Adaptation</a></p>
<p>20 0.021979962 <a title="67-tfidf-20" href="./nips-2007-Using_Deep_Belief_Nets_to_Learn_Covariance_Kernels_for_Gaussian_Processes.html">212 nips-2007-Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.074), (1, 0.009), (2, -0.017), (3, 0.024), (4, -0.012), (5, 0.005), (6, -0.004), (7, -0.004), (8, -0.02), (9, 0.021), (10, 0.032), (11, -0.021), (12, 0.001), (13, -0.005), (14, -0.021), (15, -0.023), (16, 0.016), (17, 0.002), (18, -0.005), (19, -0.024), (20, 0.034), (21, 0.012), (22, 0.017), (23, -0.008), (24, 0.043), (25, -0.018), (26, -0.017), (27, 0.013), (28, -0.029), (29, -0.007), (30, -0.002), (31, -0.044), (32, 0.016), (33, -0.031), (34, 0.017), (35, 0.032), (36, -0.028), (37, 0.054), (38, 0.005), (39, -0.072), (40, -0.06), (41, 0.011), (42, 0.095), (43, -0.07), (44, 0.042), (45, 0.012), (46, -0.034), (47, 0.055), (48, -0.177), (49, 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86934149 <a title="67-lsi-1" href="./nips-2007-Direct_Importance_Estimation_with_Model_Selection_and_Its_Application_to_Covariate_Shift_Adaptation.html">67 nips-2007-Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation</a></p>
<p>Author: Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul V. Buenau, Motoaki Kawanabe</p><p>Abstract: A situation where training and test samples follow different input distributions is called covariate shift. Under covariate shift, standard learning methods such as maximum likelihood estimation are no longer consistent—weighted variants according to the ratio of test and training input densities are consistent. Therefore, accurately estimating the density ratio, called the importance, is one of the key issues in covariate shift adaptation. A naive approach to this task is to ﬁrst estimate training and test input densities separately and then estimate the importance by taking the ratio of the estimated densities. However, this naive approach tends to perform poorly since density estimation is a hard task particularly in high dimensional cases. In this paper, we propose a direct importance estimation method that does not involve density estimation. Our method is equipped with a natural cross validation procedure and hence tuning parameters such as the kernel width can be objectively optimized. Simulations illustrate the usefulness of our approach. 1</p><p>2 0.46320635 <a title="67-lsi-2" href="./nips-2007-Topmoumoute_Online_Natural_Gradient_Algorithm.html">206 nips-2007-Topmoumoute Online Natural Gradient Algorithm</a></p>
<p>Author: Nicolas L. Roux, Pierre-antoine Manzagol, Yoshua Bengio</p><p>Abstract: Guided by the goal of obtaining an optimization algorithm that is both fast and yields good generalization, we study the descent direction maximizing the decrease in generalization error or the probability of not increasing generalization error. The surprising result is that from both the Bayesian and frequentist perspectives this can yield the natural gradient direction. Although that direction can be very expensive to compute we develop an efﬁcient, general, online approximation to the natural gradient descent which is suited to large scale problems. We report experimental results showing much faster convergence in computation time and in number of iterations with TONGA (Topmoumoute Online natural Gradient Algorithm) than with stochastic gradient descent, even on very large datasets.</p><p>3 0.43361109 <a title="67-lsi-3" href="./nips-2007-Feature_Selection_Methods_for_Improving_Protein_Structure_Prediction_with_Rosetta.html">89 nips-2007-Feature Selection Methods for Improving Protein Structure Prediction with Rosetta</a></p>
<p>Author: Ben Blum, Rhiju Das, Philip Bradley, David Baker, Michael I. Jordan, David Tax</p><p>Abstract: Rosetta is one of the leading algorithms for protein structure prediction today. It is a Monte Carlo energy minimization method requiring many random restarts to ﬁnd structures with low energy. In this paper we present a resampling technique for structure prediction of small alpha/beta proteins using Rosetta. From an initial round of Rosetta sampling, we learn properties of the energy landscape that guide a subsequent round of sampling toward lower-energy structures. Rather than attempt to ﬁt the full energy landscape, we use feature selection methods—both L1-regularized linear regression and decision trees—to identify structural features that give rise to low energy. We then enrich these structural features in the second sampling round. Results are presented across a benchmark set of nine small alpha/beta proteins demonstrating that our methods seldom impair, and frequently improve, Rosetta’s performance. 1</p><p>4 0.40112936 <a title="67-lsi-4" href="./nips-2007-Anytime_Induction_of_Cost-sensitive_Trees.html">27 nips-2007-Anytime Induction of Cost-sensitive Trees</a></p>
<p>Author: Saher Esmeir, Shaul Markovitch</p><p>Abstract: Machine learning techniques are increasingly being used to produce a wide-range of classiﬁers for complex real-world applications that involve nonuniform testing costs and misclassiﬁcation costs. As the complexity of these applications grows, the management of resources during the learning and classiﬁcation processes becomes a challenging task. In this work we introduce ACT (Anytime Cost-sensitive Trees), a novel framework for operating in such environments. ACT is an anytime algorithm that allows trading computation time for lower classiﬁcation costs. It builds a tree top-down and exploits additional time resources to obtain better estimations for the utility of the different candidate splits. Using sampling techniques ACT approximates for each candidate split the cost of the subtree under it and favors the one with a minimal cost. Due to its stochastic nature ACT is expected to be able to escape local minima, into which greedy methods may be trapped. Experiments with a variety of datasets were conducted to compare the performance of ACT to that of the state of the art cost-sensitive tree learners. The results show that for most domains ACT produces trees of signiﬁcantly lower costs. ACT is also shown to exhibit good anytime behavior with diminishing returns.</p><p>5 0.39508778 <a title="67-lsi-5" href="./nips-2007-Ultrafast_Monte_Carlo_for_Statistical_Summations.html">209 nips-2007-Ultrafast Monte Carlo for Statistical Summations</a></p>
<p>Author: Charles L. Isbell, Michael P. Holmes, Alexander G. Gray</p><p>Abstract: Machine learning contains many computational bottlenecks in the form of nested summations over datasets. Kernel estimators and other methods are burdened by these expensive computations. Exact evaluation is typically O(n2 ) or higher, which severely limits application to large datasets. We present a multi-stage stratiﬁed Monte Carlo method for approximating such summations with probabilistic relative error control. The essential idea is fast approximation by sampling in trees. This method differs from many previous scalability techniques (such as standard multi-tree methods) in that its error is stochastic, but we derive conditions for error control and demonstrate that they work. Further, we give a theoretical sample complexity for the method that is independent of dataset size, and show that this appears to hold in experiments, where speedups reach as high as 1014 , many orders of magnitude beyond the previous state of the art. 1</p><p>6 0.39482716 <a title="67-lsi-6" href="./nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">79 nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<p>7 0.35453796 <a title="67-lsi-7" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>8 0.35168734 <a title="67-lsi-8" href="./nips-2007-Random_Features_for_Large-Scale_Kernel_Machines.html">160 nips-2007-Random Features for Large-Scale Kernel Machines</a></p>
<p>9 0.35021457 <a title="67-lsi-9" href="./nips-2007-Estimating_divergence_functionals_and_the_likelihood_ratio_by_penalized_convex_risk_minimization.html">82 nips-2007-Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization</a></p>
<p>10 0.34933114 <a title="67-lsi-10" href="./nips-2007-SpAM%3A_Sparse_Additive_Models.html">179 nips-2007-SpAM: Sparse Additive Models</a></p>
<p>11 0.34275928 <a title="67-lsi-11" href="./nips-2007-Learning_with_Transformation_Invariant_Kernels.html">118 nips-2007-Learning with Transformation Invariant Kernels</a></p>
<p>12 0.34252283 <a title="67-lsi-12" href="./nips-2007-Markov_Chain_Monte_Carlo_with_People.html">125 nips-2007-Markov Chain Monte Carlo with People</a></p>
<p>13 0.34151584 <a title="67-lsi-13" href="./nips-2007-How_SVMs_can_estimate_quantiles_and_the_median.html">101 nips-2007-How SVMs can estimate quantiles and the median</a></p>
<p>14 0.33472762 <a title="67-lsi-14" href="./nips-2007-Support_Vector_Machine_Classification_with_Indefinite_Kernels.html">190 nips-2007-Support Vector Machine Classification with Indefinite Kernels</a></p>
<p>15 0.33165818 <a title="67-lsi-15" href="./nips-2007-A_Kernel_Statistical_Test_of_Independence.html">7 nips-2007-A Kernel Statistical Test of Independence</a></p>
<p>16 0.33085507 <a title="67-lsi-16" href="./nips-2007-Simulated_Annealing%3A_Rigorous_finite-time_guarantees_for_optimization_on_continuous_domains.html">178 nips-2007-Simulated Annealing: Rigorous finite-time guarantees for optimization on continuous domains</a></p>
<p>17 0.32925326 <a title="67-lsi-17" href="./nips-2007-Testing_for_Homogeneity_with_Kernel_Fisher_Discriminant_Analysis.html">192 nips-2007-Testing for Homogeneity with Kernel Fisher Discriminant Analysis</a></p>
<p>18 0.32787067 <a title="67-lsi-18" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>19 0.31993815 <a title="67-lsi-19" href="./nips-2007-Colored_Maximum_Variance_Unfolding.html">49 nips-2007-Colored Maximum Variance Unfolding</a></p>
<p>20 0.30973151 <a title="67-lsi-20" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.03), (13, 0.029), (16, 0.02), (18, 0.011), (19, 0.018), (21, 0.039), (34, 0.011), (35, 0.029), (47, 0.078), (49, 0.01), (64, 0.435), (83, 0.088), (85, 0.01), (87, 0.016), (90, 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70807457 <a title="67-lda-1" href="./nips-2007-Direct_Importance_Estimation_with_Model_Selection_and_Its_Application_to_Covariate_Shift_Adaptation.html">67 nips-2007-Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation</a></p>
<p>Author: Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul V. Buenau, Motoaki Kawanabe</p><p>Abstract: A situation where training and test samples follow different input distributions is called covariate shift. Under covariate shift, standard learning methods such as maximum likelihood estimation are no longer consistent—weighted variants according to the ratio of test and training input densities are consistent. Therefore, accurately estimating the density ratio, called the importance, is one of the key issues in covariate shift adaptation. A naive approach to this task is to ﬁrst estimate training and test input densities separately and then estimate the importance by taking the ratio of the estimated densities. However, this naive approach tends to perform poorly since density estimation is a hard task particularly in high dimensional cases. In this paper, we propose a direct importance estimation method that does not involve density estimation. Our method is equipped with a natural cross validation procedure and hence tuning parameters such as the kernel width can be objectively optimized. Simulations illustrate the usefulness of our approach. 1</p><p>2 0.58114886 <a title="67-lda-2" href="./nips-2007-Modeling_homophily_and_stochastic_equivalence_in_symmetric_relational_data.html">131 nips-2007-Modeling homophily and stochastic equivalence in symmetric relational data</a></p>
<p>Author: Peter Hoff</p><p>Abstract: This article discusses a latent variable model for inference and prediction of symmetric relational data. The model, based on the idea of the eigenvalue decomposition, represents the relationship between two nodes as the weighted inner-product of node-speciﬁc vectors of latent characteristics. This “eigenmodel” generalizes other popular latent variable models, such as latent class and distance models: It is shown mathematically that any latent class or distance model has a representation as an eigenmodel, but not vice-versa. The practical implications of this are examined in the context of three real datasets, for which the eigenmodel has as good or better out-of-sample predictive performance than the other two models. 1</p><p>3 0.54455686 <a title="67-lda-3" href="./nips-2007-Compressed_Regression.html">53 nips-2007-Compressed Regression</a></p>
<p>Author: Shuheng Zhou, Larry Wasserman, John D. Lafferty</p><p>Abstract: Recent research has studied the role of sparsity in high dimensional regression and signal reconstruction, establishing theoretical limits for recovering sparse models from sparse data. In this paper we study a variant of this problem where the original n input variables are compressed by a random linear transformation to m n examples in p dimensions, and establish conditions under which a sparse linear model can be successfully recovered from the compressed data. A primary motivation for this compression procedure is to anonymize the data and preserve privacy by revealing little information about the original data. We characterize the number of random projections that are required for 1 -regularized compressed regression to identify the nonzero coefﬁcients in the true model with probability approaching one, a property called “sparsistence.” In addition, we show that 1 -regularized compressed regression asymptotically predicts as well as an oracle linear model, a property called “persistence.” Finally, we characterize the privacy properties of the compression procedure in information-theoretic terms, establishing upper bounds on the rate of information communicated between the compressed and uncompressed data that decay to zero. 1</p><p>4 0.30766246 <a title="67-lda-4" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>Author: Yuhong Guo, Dale Schuurmans</p><p>Abstract: We investigate a new, convex relaxation of an expectation-maximization (EM) variant that approximates a standard objective while eliminating local minima. First, a cautionary result is presented, showing that any convex relaxation of EM over hidden variables must give trivial results if any dependence on the missing values is retained. Although this appears to be a strong negative outcome, we then demonstrate how the problem can be bypassed by using equivalence relations instead of value assignments over hidden variables. In particular, we develop new algorithms for estimating exponential conditional models that only require equivalence relation information over the variable values. This reformulation leads to an exact expression for EM variants in a wide range of problems. We then develop a semideﬁnite relaxation that yields global training by eliminating local minima. 1</p><p>5 0.30495703 <a title="67-lda-5" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>Author: Gwenn Englebienne, Tim Cootes, Magnus Rattray</p><p>Abstract: The present work aims to model the correspondence between facial motion and speech. The face and sound are modelled separately, with phonemes being the link between both. We propose a sequential model and evaluate its suitability for the generation of the facial animation from a sequence of phonemes, which we obtain from speech. We evaluate the results both by computing the error between generated sequences and real video, as well as with a rigorous double-blind test with human subjects. Experiments show that our model compares favourably to other existing methods and that the sequences generated are comparable to real video sequences. 1</p><p>6 0.30454609 <a title="67-lda-6" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>7 0.30391645 <a title="67-lda-7" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<p>8 0.30348876 <a title="67-lda-8" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>9 0.30329639 <a title="67-lda-9" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>10 0.3025496 <a title="67-lda-10" href="./nips-2007-An_Analysis_of_Inference_with_the_Universum.html">24 nips-2007-An Analysis of Inference with the Universum</a></p>
<p>11 0.30254394 <a title="67-lda-11" href="./nips-2007-Probabilistic_Matrix_Factorization.html">158 nips-2007-Probabilistic Matrix Factorization</a></p>
<p>12 0.30236188 <a title="67-lda-12" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>13 0.30176008 <a title="67-lda-13" href="./nips-2007-People_Tracking_with_the_Laplacian_Eigenmaps_Latent_Variable_Model.html">153 nips-2007-People Tracking with the Laplacian Eigenmaps Latent Variable Model</a></p>
<p>14 0.30162913 <a title="67-lda-14" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>15 0.30155474 <a title="67-lda-15" href="./nips-2007-Predictive_Matrix-Variate_t_Models.html">156 nips-2007-Predictive Matrix-Variate t Models</a></p>
<p>16 0.30093637 <a title="67-lda-16" href="./nips-2007-Locality_and_low-dimensions_in_the_prediction_of_natural_experience_from_fMRI.html">122 nips-2007-Locality and low-dimensions in the prediction of natural experience from fMRI</a></p>
<p>17 0.30085817 <a title="67-lda-17" href="./nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</a></p>
<p>18 0.30052394 <a title="67-lda-18" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>19 0.3004877 <a title="67-lda-19" href="./nips-2007-Heterogeneous_Component_Analysis.html">96 nips-2007-Heterogeneous Component Analysis</a></p>
<p>20 0.30000013 <a title="67-lda-20" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
