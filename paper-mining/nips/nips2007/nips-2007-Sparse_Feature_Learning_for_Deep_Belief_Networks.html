<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>180 nips-2007-Sparse Feature Learning for Deep Belief Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-180" href="#">nips2007-180</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>180 nips-2007-Sparse Feature Learning for Deep Belief Networks</h1>
<br/><p>Source: <a title="nips-2007-180-pdf" href="http://papers.nips.cc/paper/3363-sparse-feature-learning-for-deep-belief-networks.pdf">pdf</a></p><p>Author: Marc'aurelio Ranzato, Y-lan Boureau, Yann L. Cun</p><p>Abstract: Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation. We describe a novel and efﬁcient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machine trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input observed variables can be captured. 1</p><p>Reference: <a title="nips-2007-180-reference" href="../nips2007_reference/nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu}  Abstract Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. [sent-3, score-0.217]
</p><p>2 Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e. [sent-4, score-0.21]
</p><p>3 We describe a novel and efﬁcient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machine trained probabilistically, namely a Restricted Boltzmann Machine. [sent-8, score-0.16]
</p><p>4 We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. [sent-9, score-0.318]
</p><p>5 We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input observed variables can be captured. [sent-11, score-0.208]
</p><p>6 1  Introduction  One of the main purposes of unsupervised learning is to produce good representations for data, that can be used for detection, recognition, prediction, or visualization. [sent-12, score-0.189]
</p><p>7 One cause for the recent resurgence of interest in unsupervised learning is the ability to produce deep feature hierarchies by stacking unsupervised modules on top of each other, as proposed by Hinton et al. [sent-14, score-0.367]
</p><p>8 The unsupervised module at one level in the hierarchy is fed with the representation vectors produced by the level below. [sent-17, score-0.215]
</p><p>9 An encoder transforms the input into the representation (also known as the code or the feature vector), and a decoder reconstructs the input (perhaps stochastically) from the representation. [sent-21, score-1.056]
</p><p>10 PCA, Auto-encoder neural nets, Restricted Boltzmann Machines (RBMs), our previous sparse energy-based model [3], and the model proposed in [6] for noisy overcomplete channels are just examples of this kind of architecture. [sent-22, score-0.215]
</p><p>11 after training, computing the code is a very fast process that merely consists in running the input through the encoder; 2. [sent-24, score-0.383]
</p><p>12 reconstructing the input with the decoder provides a way to check that the code has captured the relevant information in the data. [sent-25, score-0.671]
</p><p>13 Some learning algorithms [7] do not have a decoder and must resort to computationally expensive Markov Chain Monte Carlo (MCMC) sampling methods in order to provide reconstructions. [sent-26, score-0.255]
</p><p>14 Other learning algorithms [8, 9] lack an encoder, which makes it necessary to run an expensive optimization algorithm to ﬁnd the code associated with each new input sample. [sent-27, score-0.383]
</p><p>15 The energy function includes the reconstruction error, and perhaps other terms as well. [sent-30, score-0.262]
</p><p>16 Training the machine to model the input distribution is performed by ﬁnding the encoder and decoder parameters that minimize a loss function equal to the negative log likelihood of the training data under the model. [sent-32, score-0.868]
</p><p>17 For a single training sample Y , the loss function is L(W, Y ) = −  1 log β  e−βE(Y,z) + z  1 log β  e−βE(y,z)  (2)  y,z  The ﬁrst term is the free energy Fβ (Y ). [sent-33, score-0.42]
</p><p>18 It ensures that the system produces low energy only for input vectors that have high probability in the (true) data distribution, and produces higher energies for all other input vectors [5]. [sent-36, score-0.313]
</p><p>19 Regardless of whether only Z ∗ or the whole distribution over Z is considered, the main difﬁculty with this framework is that it can be very hard to compute the gradient of the log partition function in equation 2 or 3 with respect to the parameters W . [sent-38, score-0.197]
</p><p>20 For instance, Restricted Boltzmann Machines (RBM) [10] approximate the gradient of the log partition function in equation 2 by sampling values of Y whose energy will be pulled up using an MCMC technique. [sent-40, score-0.412]
</p><p>21 By running the MCMC for a short time, those samples are chosen in the vicinity of the training samples, thereby ensuring that the energy surface forms a ravine around the manifold of the training samples. [sent-41, score-0.419]
</p><p>22 The role of the log partition function is merely to ensure that the energy surface is lower around training samples than anywhere else. [sent-43, score-0.449]
</p><p>23 The method proposed here eliminates the log partition function from the loss, and replaces it by a term that limits the volume of the input space over which the energy surface can take a low value. [sent-44, score-0.442]
</p><p>24 This is performed by adding a penalty term on the code rather than on the input. [sent-45, score-0.39]
</p><p>25 To understand the method, we ﬁrst note that if for each vector Y , there exists a corresponding optimal code Z ∗ (Y ) that makes the reconstruction error (or energy) F∞ (Y ) zero (or near zero), the model can perfectly reconstruct any input vector. [sent-47, score-0.52]
</p><p>26 On the other hand, if Z can only take a small number of different values (low entropy code), then the energy F∞ (Y ) can only be low in a limited number of places (the Y ’s that are reconstructed from this small number of Z values), and the energy cannot be ﬂat. [sent-49, score-0.471]
</p><p>27 More generally, a convenient method through which ﬂat energy surfaces can be avoided is to limit the maximum information content of the code. [sent-50, score-0.257]
</p><p>28 Hence, minimizing the energy F∞ (Y ) together with the information content of the code is a good substitute for minimizing the log partition function. [sent-51, score-0.708]
</p><p>29 2  A popular way to minimize the information content in the code is to make the code sparse or lowdimensional [5]. [sent-52, score-0.82]
</p><p>30 This technique is used in a number of unsupervised learning methods, including PCA, auto-encoders neural network, and sparse coding methods [6, 3, 8, 9]. [sent-53, score-0.289]
</p><p>31 In sparse methods, the code is forced to have only a few non-zero units while most code units are zero most of the time. [sent-54, score-0.876]
</p><p>32 This may explain why biology seems to like sparse representations [11]. [sent-58, score-0.156]
</p><p>33 In our context, the main advantage of sparsity constraints is to allow us to replace a marginalization by a minimization, and to free ourselves from the need to minimize the log partition function explicitly. [sent-59, score-0.29]
</p><p>34 2 and 3, we consider a loss function which is a weighted sum of the reconstruction error and a sparsity penalty, as in many other unsupervised learning algorithms [13, 14, 8]. [sent-62, score-0.391]
</p><p>35 Encoder and decoder are constrained to be symmetric, and share a set of linear ﬁlters. [sent-63, score-0.255]
</p><p>36 Although we only consider linear ﬁlters in this paper, the method allows the use of any differentiable function for encoder and decoder. [sent-64, score-0.329]
</p><p>37 The ﬁrst step computes the optimal code by minimizing the energy for the given input. [sent-66, score-0.528]
</p><p>38 Following [15], we evaluate these methods by measuring the reconstruction error for a given entropy of the code. [sent-70, score-0.174]
</p><p>39 The representational power of this hierarchical non-linear feature extraction is demonstrated through the unsupervised discovery of the numeral class labels in the high-level code. [sent-79, score-0.168]
</p><p>40 2  Architecture  In this section we describe a Sparse Encoding Symmetric Machine (SESM) having a set of linear ﬁlters in both encoder and decoder. [sent-80, score-0.329]
</p><p>41 However, everything can be easily extended to any other choice of parameterized functions as long as these are differentiable and maintain symmetry between encoder and decoder. [sent-81, score-0.329]
</p><p>42 Let us denote with Y the input deﬁned in RN , and with Z the code deﬁned in RM , where M is in general greater than N (for overcomplete representations). [sent-82, score-0.496]
</p><p>43 Let the ﬁlters in encoder and decoder be the columns of matrix W ∈ RN ×M , and let the biases in the encoder and decoder be denoted by benc ∈ RM and bdec ∈ RN , respectively. [sent-83, score-1.301]
</p><p>44 Then, encoder and decoder compute: fenc (Y ) = W T Y + benc ,  fdec (Z) = W l(Z) + bdec  (5)  where the function l is a point-wise logistic non-linearity of the form: l(x) = 1/(1 + exp(−gx)),  (6)  with g ﬁxed gain. [sent-84, score-0.897]
</p><p>45 The system is characterized by an energy measuring the compatibility between pairs of input Y and latent code Z, E(Y, Z) [16]. [sent-85, score-0.568]
</p><p>46 The second term is the mean-squared error between the input Y and the reconstruction provided by the decoder. [sent-88, score-0.214]
</p><p>47 For instance, the ﬁrst two terms appear in our previous model [3], but in that work, the weights of encoder and decoder were not tied and the parameters in the logistic were updated using running averages. [sent-92, score-0.631]
</p><p>48 Because of the sparsity penalty and the linear reconstruction, code units become tiny and are compensated by the ﬁlters in the decoder that grow without bound. [sent-97, score-0.784]
</p><p>49 In this work, we propose to enforce symmetry between encoder and decoder (through weight sharing) so as to have automatic scaling of ﬁlters. [sent-101, score-0.584]
</p><p>50 Their norm cannot possibly be large because code units, produced by the encoder weights, would have large values as well, producing bad reconstructions and increasing the energy (the second term in equation 7 and 8). [sent-102, score-1.004]
</p><p>51 3  Learning Algorithm  Learning consists of determining the parameters in W , benc , and bdec that minimize the loss in equation 8. [sent-103, score-0.281]
</p><p>52 As indicated in the introduction, the energy augmented with the sparsity constraint is minimized with respect to the code to ﬁnd the optimal code. [sent-104, score-0.61]
</p><p>53 Instead, we rely on the code sparsity constraints to ensure that the energy surface is not ﬂat. [sent-108, score-0.654]
</p><p>54 Since the second term in equation 8 couples both Z and W and bdec , it is not straightforward to minimize this energy with respect to both. [sent-109, score-0.387]
</p><p>55 Vice versa, if the parameters W are ﬁxed, the optimal code Z ∗ that minimizes L can be computed easily through gradient descent. [sent-111, score-0.349]
</p><p>56 for a given sample Y and parameter setting, minimize the loss in equation 8 with respect to Z by gradient descent to obtain the optimal code Z ∗ 2. [sent-113, score-0.528]
</p><p>57 clamping both the input Y and the optimal code Z ∗ found at the previous step, do one step of gradient descent to update the parameters. [sent-114, score-0.444]
</p><p>58 After training, the system converges to a state where the decoder produces good reconstructions from a sparse code, and the optimal code is predicted by a simple feed-forward propagation through the encoder. [sent-117, score-0.712]
</p><p>59 An RBM is a binary stochastic symmetric machine deﬁned 4  by an energy function of the form: E(Y, Z) = −Z T W T Y − bT Z − bT Y . [sent-120, score-0.217]
</p><p>60 Although this is not enc dec obvious at ﬁrst glance, this energy can be seen as a special case of the encoder-decoder architecture that pertains to binary data vectors and code vectors [5]. [sent-121, score-0.554]
</p><p>61 Training an RBM minimizes an approximation of the negative log likelihood loss function 2, averaged over the training set, through a gradient descent procedure. [sent-122, score-0.243]
</p><p>62 Instead of estimating the gradient of the log partition function, RBM training uses contrastive divergence [10], which takes random samples drawn over a limited region Ω around the training samples. [sent-123, score-0.374]
</p><p>63 This means that only the energy of points around training samples is pulled up. [sent-126, score-0.321]
</p><p>64 However, the code vector in an RBM is binary and noisy, and one may wonder whether this does not have the effect of surreptitiously limiting the information content of the code, thereby further minimizing the log partition function as a bonus. [sent-128, score-0.522]
</p><p>65 SESM RBM and SESM have almost the same architecture because they both have a symmetric encoder and decoder, and a logistic non-linearity on the top of the encoder. [sent-129, score-0.458]
</p><p>66 However, RBM is trained using (approximate) maximum likelihood, while SESM is trained by simply minimizing the average energy F∞ (Y ) of equation 4 with an additional code sparsity term. [sent-130, score-0.803]
</p><p>67 SESM relies on the sparsity term to prevent ﬂat energy surfaces, while RBM relies on an explicit contrastive term in the loss, an approximation of the log partition function. [sent-131, score-0.538]
</p><p>68 Also, the coding strategy is very different because code units are “noisy” and binary in RBM, while they are quasi-binary and sparse in SESM. [sent-132, score-0.563]
</p><p>69 1  Experimental Comparison  In the ﬁrst experiment we have trained SESM, RBM, and PCA on the ﬁrst 20000 digits in the MNIST training dataset [18] in order to produce codes with 200 components. [sent-135, score-0.278]
</p><p>70 Similarly to [15] we have collected test image codes after the logistic non linearity (except for PCA which is linear), and we have measured the root mean square error (RMSE) and the entropy. [sent-136, score-0.247]
</p><p>71 SESM was run for different values of the sparsity coefﬁcient αs in equation 8 (while all other parameters are left unchanged, see 1 ¯ ¯ next section for details). [sent-137, score-0.159]
</p><p>72 The RMSE is deﬁned as 1 Y − fdec (Z) 2 , where Z is the uniformly σ  PN  2  quantized code produced by the encoder, P is the number of test samples, and σ is the estimated variance of units in the input Y . [sent-138, score-0.616]
</p><p>73 Assuming to encode the (quantized) code units independently and with the same distribution, the lower bound on the number of bits required to encode each of them Q i i is given by: Hc. [sent-139, score-0.429]
</p><p>74 Unlike N in [15, 12], the reconstruction is done taking the quantized code in order to measure the robustness of the code to the quantization noise. [sent-145, score-0.865]
</p><p>75 1-C, RBM is very robust to noise in the code because it is trained by sampling. [sent-147, score-0.377]
</p><p>76 Despite the similarities in the architecture, ﬁlters look quite different in general, revealing two different coding strategies: distributed for RBM, and sparse for SESM. [sent-152, score-0.176]
</p><p>77 Since we have available also the labels in the MNIST, we have used the codes (produced by these machines trained unsupervised) as input to the same linear classiﬁer. [sent-154, score-0.259]
</p><p>78 1-A/B show the result of these experiments in addition to what can be achieved by a linear classiﬁer trained on the raw pixel data. [sent-157, score-0.161]
</p><p>79 45 PCA: quantization in 5 bins PCA: quantization in 256 bins RBM: quantization in 5 bins RBM: quantization in 256 bins Sparse Coding: quantization in 5 bins Sparse Coding: quantization in 256 bins  0. [sent-165, score-0.948]
</p><p>80 5  2  (D)  (E)  (F)  (G)  (H)  Figure 1: (A)-(B) Error rate on MNIST training (with 10, 100 and 1000 samples per class) and test set produced by a linear classiﬁer trained on the codes produced by SESM, RBM, and PCA. [sent-175, score-0.378]
</p><p>81 The entropy and RMSE refers to a quantization into 256 bins. [sent-176, score-0.167]
</p><p>82 The comparison has been extended also to the same classiﬁer trained on raw pixel data (showing the advantage of extracting features). [sent-177, score-0.161]
</p><p>83 (C) Comparison between SESM, RBM, and PCA when quantizing the code into 5 and 256 bins. [sent-187, score-0.319]
</p><p>84 (E) Some pairs of original and reconstructed digit from the code produced by the encoder in SESM (feed-forward propagation through encoder and decoder). [sent-190, score-1.066]
</p><p>85 (G) Back-projection in image space of the ﬁlters learned in the second stage of the hierarchical feature extractor. [sent-192, score-0.167]
</p><p>86 The second stage was trained on the non linearly transformed codes produced by the ﬁrst stage machine. [sent-193, score-0.426]
</p><p>87 The back-projection has been performed by using a 1-of-10 code in the second stage machine, and propagating this through the second stage decoder and ﬁrst stage decoder. [sent-194, score-0.82]
</p><p>88 Even if the code unit activation has a very sparse distribution, reconstructions are very good (no minimization in code space was performed). [sent-209, score-0.776]
</p><p>89 1  Hierarchical Features  A hierarchical feature extractor can be trained layer-by-layer similarly to what has been proposed in [19, 1] for training deep belief nets (DBNs). [sent-212, score-0.287]
</p><p>90 We have trained a second (higher) stage machine on the non linearly transformed codes produced by the ﬁrst (lower) stage machine described in the previous example. [sent-213, score-0.426]
</p><p>91 Since we aimed to ﬁnd a 1-of-10 code we increased the sparsity level (in the second stage machine) by setting αs to 1. [sent-215, score-0.507]
</p><p>92 Despite the completely unsupervised training procedure, the feature detectors in the second stage machine look like digit prototypes as can be seen in ﬁg. [sent-216, score-0.306]
</p><p>93 The hierarchical unsupervised feature extractor is able to capture higher order correlations among the input pixel intensities, and to discover the highly non-linear mapping from raw pixel data to the class labels. [sent-218, score-0.446]
</p><p>94 For comparison, when training a DBN, prototypes are not recovered because the learned code is distributed among units. [sent-221, score-0.405]
</p><p>95 2  Natural Image Patches  A SESM with about the same set up was trained on a dataset of 30000 8x8 natural image patches randomly extracted from the Berkeley segmentation dataset [20]. [sent-223, score-0.159]
</p><p>96 We have considered a 2 times overcomplete code with 128 units. [sent-226, score-0.432]
</p><p>97 6  Conclusions  There are two strategies to train unsupervised machines: 1) having a contrastive term in the loss function minimized during training, 2) constraining the internal representation in such a way that training samples can be better reconstructed than other points in input space. [sent-233, score-0.512]
</p><p>98 We have proposed an evaluation protocol to compare different machines which is based on RMSE, entropy and, eventually, error rate when also 7  labels are available. [sent-236, score-0.176]
</p><p>99 A theoretical analysis of robust coding over noisy overcomplete channels. [sent-286, score-0.187]
</p><p>100 Sparse coding with an overcomplete basis set: a strategy employed by v1? [sent-303, score-0.187]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sesm', 0.472), ('rbm', 0.37), ('encoder', 0.329), ('code', 0.319), ('decoder', 0.255), ('energy', 0.185), ('rmse', 0.162), ('overcomplete', 0.113), ('unsupervised', 0.113), ('lters', 0.11), ('quantization', 0.108), ('sparsity', 0.106), ('sparse', 0.102), ('codes', 0.089), ('pca', 0.082), ('stage', 0.082), ('reconstruction', 0.077), ('bdec', 0.076), ('fdec', 0.076), ('coding', 0.074), ('partition', 0.073), ('units', 0.068), ('input', 0.064), ('contrastive', 0.063), ('raw', 0.062), ('training', 0.061), ('entropy', 0.059), ('deep', 0.059), ('trained', 0.058), ('loss', 0.057), ('benc', 0.057), ('fenc', 0.057), ('hinton', 0.056), ('representations', 0.054), ('equation', 0.053), ('architecture', 0.05), ('bins', 0.05), ('chopra', 0.049), ('ranzato', 0.049), ('machines', 0.048), ('produced', 0.047), ('logistic', 0.047), ('handwritten', 0.047), ('samples', 0.045), ('surface', 0.044), ('non', 0.043), ('quantized', 0.042), ('reconstructed', 0.042), ('content', 0.042), ('pixel', 0.041), ('log', 0.041), ('mnist', 0.039), ('bengio', 0.039), ('minimize', 0.038), ('error', 0.038), ('numerals', 0.038), ('pcm', 0.038), ('discover', 0.037), ('reconstructions', 0.036), ('penalty', 0.036), ('term', 0.035), ('stacking', 0.035), ('reconstructing', 0.033), ('mcmc', 0.033), ('extractor', 0.033), ('rbms', 0.033), ('marginalization', 0.032), ('symmetric', 0.032), ('train', 0.032), ('descent', 0.031), ('rate', 0.031), ('hierarchy', 0.03), ('boltzmann', 0.03), ('gradient', 0.03), ('surfaces', 0.03), ('pulled', 0.03), ('image', 0.03), ('hierarchical', 0.03), ('encoding', 0.029), ('patches', 0.027), ('recognition', 0.027), ('olshausen', 0.026), ('classi', 0.026), ('digits', 0.026), ('prototypes', 0.025), ('fed', 0.025), ('osindero', 0.025), ('feature', 0.025), ('transformed', 0.025), ('minimizing', 0.024), ('stochastically', 0.024), ('localized', 0.023), ('likelihood', 0.023), ('thereby', 0.023), ('dataset', 0.022), ('produce', 0.022), ('understand', 0.022), ('encode', 0.021), ('er', 0.021), ('nets', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="180-tfidf-1" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>Author: Marc'aurelio Ranzato, Y-lan Boureau, Yann L. Cun</p><p>Abstract: Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation. We describe a novel and efﬁcient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machine trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input observed variables can be captured. 1</p><p>2 0.23857762 <a title="180-tfidf-2" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>Author: Honglak Lee, Chaitanya Ekanadham, Andrew Y. Ng</p><p>Abstract: Motivated in part by the hierarchical organization of the cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or “deep,” structure from unlabeled data. While several authors have formally or informally compared their algorithms to computations performed in visual area V1 (and the cochlea), little attempt has been made thus far to evaluate these algorithms in terms of their ﬁdelity for mimicking computations at deeper levels in the cortical hierarchy. This paper presents an unsupervised learning model that faithfully mimics certain properties of visual area V2. Speciﬁcally, we develop a sparse variant of the deep belief networks of Hinton et al. (2006). We learn two layers of nodes in the network, and demonstrate that the ﬁrst layer, similar to prior work on sparse coding and ICA, results in localized, oriented, edge ﬁlters, similar to the Gabor functions known to model V1 cell receptive ﬁelds. Further, the second layer in our model encodes correlations of the ﬁrst layer responses in the data. Speciﬁcally, it picks up both colinear (“contour”) features as well as corners and junctions. More interestingly, in a quantitative comparison, the encoding of these more complex “corner” features matches well with the results from the Ito & Komatsu’s study of biological V2 responses. This suggests that our sparse variant of deep belief networks holds promise for modeling more higher-order features. 1</p><p>3 0.22007081 <a title="180-tfidf-3" href="./nips-2007-Modeling_image_patches_with_a_directed_hierarchy_of_Markov_random_fields.html">132 nips-2007-Modeling image patches with a directed hierarchy of Markov random fields</a></p>
<p>Author: Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We describe an efﬁcient learning procedure for multilayer generative models that combine the best aspects of Markov random ﬁelds and deep, directed belief nets. The generative models can be learned one layer at a time and when learning is complete they have a very fast inference procedure for computing a good approximation to the posterior distribution in all of the hidden layers. Each hidden layer has its own MRF whose energy function is modulated by the top-down directed connections from the layer above. To generate from the model, each layer in turn must settle to equilibrium given its top-down input. We show that this type of model is good at capturing the statistics of patches of natural images. 1</p><p>4 0.18255731 <a title="180-tfidf-4" href="./nips-2007-Using_Deep_Belief_Nets_to_Learn_Covariance_Kernels_for_Gaussian_Processes.html">212 nips-2007-Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes</a></p>
<p>Author: Geoffrey E. Hinton, Ruslan Salakhutdinov</p><p>Abstract: We show how to use unlabeled data and a deep belief net (DBN) to learn a good covariance kernel for a Gaussian process. We ﬁrst learn a deep generative model of the unlabeled data using the fast, greedy algorithm introduced by [7]. If the data is high-dimensional and highly-structured, a Gaussian kernel applied to the top layer of features in the DBN works much better than a similar kernel applied to the raw input. Performance at both regression and classiﬁcation can then be further improved by using backpropagation through the DBN to discriminatively ﬁne-tune the covariance kernel.</p><p>5 0.11916678 <a title="180-tfidf-5" href="./nips-2007-Sparse_Overcomplete_Latent_Variable_Decomposition_of_Counts_Data.html">181 nips-2007-Sparse Overcomplete Latent Variable Decomposition of Counts Data</a></p>
<p>Author: Madhusudana Shashanka, Bhiksha Raj, Paris Smaragdis</p><p>Abstract: An important problem in many ﬁelds is the analysis of counts data to extract meaningful latent components. Methods like Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA) have been proposed for this purpose. However, they are limited in the number of components they can extract and lack an explicit provision to control the “expressiveness” of the extracted components. In this paper, we present a learning formulation to address these limitations by employing the notion of sparsity. We start with the PLSA framework and use an entropic prior in a maximum a posteriori formulation to enforce sparsity. We show that this allows the extraction of overcomplete sets of latent components which better characterize the data. We present experimental evidence of the utility of such representations.</p><p>6 0.11593682 <a title="180-tfidf-6" href="./nips-2007-Mining_Internet-Scale_Software_Repositories.html">129 nips-2007-Mining Internet-Scale Software Repositories</a></p>
<p>7 0.1144815 <a title="180-tfidf-7" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>8 0.11177319 <a title="180-tfidf-8" href="./nips-2007-On_Sparsity_and_Overcompleteness_in_Image_Models.html">145 nips-2007-On Sparsity and Overcompleteness in Image Models</a></p>
<p>9 0.072394706 <a title="180-tfidf-9" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>10 0.069290906 <a title="180-tfidf-10" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<p>11 0.054828353 <a title="180-tfidf-11" href="./nips-2007-A_New_View_of_Automatic_Relevance_Determination.html">8 nips-2007-A New View of Automatic Relevance Determination</a></p>
<p>12 0.053372301 <a title="180-tfidf-12" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>13 0.053277716 <a title="180-tfidf-13" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>14 0.050086271 <a title="180-tfidf-14" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>15 0.049924411 <a title="180-tfidf-15" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>16 0.049100693 <a title="180-tfidf-16" href="./nips-2007-Convex_Learning_with_Invariances.html">62 nips-2007-Convex Learning with Invariances</a></p>
<p>17 0.048660606 <a title="180-tfidf-17" href="./nips-2007-Subspace-Based_Face_Recognition_in_Analog_VLSI.html">188 nips-2007-Subspace-Based Face Recognition in Analog VLSI</a></p>
<p>18 0.047947451 <a title="180-tfidf-18" href="./nips-2007-Topmoumoute_Online_Natural_Gradient_Algorithm.html">206 nips-2007-Topmoumoute Online Natural Gradient Algorithm</a></p>
<p>19 0.046259817 <a title="180-tfidf-19" href="./nips-2007-Blind_channel_identification_for_speech_dereverberation_using_l1-norm_sparse_learning.html">37 nips-2007-Blind channel identification for speech dereverberation using l1-norm sparse learning</a></p>
<p>20 0.044686992 <a title="180-tfidf-20" href="./nips-2007-DIFFRAC%3A_a_discriminative_and_flexible_framework_for_clustering.html">65 nips-2007-DIFFRAC: a discriminative and flexible framework for clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.194), (1, 0.106), (2, 0.016), (3, -0.063), (4, -0.016), (5, 0.073), (6, -0.228), (7, 0.08), (8, 0.217), (9, 0.028), (10, 0.258), (11, -0.034), (12, 0.116), (13, -0.036), (14, 0.022), (15, -0.003), (16, -0.025), (17, -0.073), (18, -0.121), (19, 0.02), (20, -0.006), (21, 0.003), (22, -0.061), (23, -0.033), (24, 0.018), (25, -0.092), (26, -0.015), (27, 0.074), (28, 0.023), (29, -0.039), (30, 0.044), (31, -0.083), (32, -0.034), (33, 0.04), (34, 0.014), (35, 0.037), (36, 0.038), (37, -0.015), (38, -0.066), (39, -0.057), (40, -0.067), (41, -0.009), (42, -0.005), (43, -0.041), (44, 0.003), (45, -0.042), (46, -0.081), (47, -0.006), (48, -0.064), (49, -0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93424863 <a title="180-lsi-1" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>Author: Marc'aurelio Ranzato, Y-lan Boureau, Yann L. Cun</p><p>Abstract: Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation. We describe a novel and efﬁcient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machine trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input observed variables can be captured. 1</p><p>2 0.82162684 <a title="180-lsi-2" href="./nips-2007-Modeling_image_patches_with_a_directed_hierarchy_of_Markov_random_fields.html">132 nips-2007-Modeling image patches with a directed hierarchy of Markov random fields</a></p>
<p>Author: Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We describe an efﬁcient learning procedure for multilayer generative models that combine the best aspects of Markov random ﬁelds and deep, directed belief nets. The generative models can be learned one layer at a time and when learning is complete they have a very fast inference procedure for computing a good approximation to the posterior distribution in all of the hidden layers. Each hidden layer has its own MRF whose energy function is modulated by the top-down directed connections from the layer above. To generate from the model, each layer in turn must settle to equilibrium given its top-down input. We show that this type of model is good at capturing the statistics of patches of natural images. 1</p><p>3 0.80874008 <a title="180-lsi-3" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>Author: Honglak Lee, Chaitanya Ekanadham, Andrew Y. Ng</p><p>Abstract: Motivated in part by the hierarchical organization of the cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or “deep,” structure from unlabeled data. While several authors have formally or informally compared their algorithms to computations performed in visual area V1 (and the cochlea), little attempt has been made thus far to evaluate these algorithms in terms of their ﬁdelity for mimicking computations at deeper levels in the cortical hierarchy. This paper presents an unsupervised learning model that faithfully mimics certain properties of visual area V2. Speciﬁcally, we develop a sparse variant of the deep belief networks of Hinton et al. (2006). We learn two layers of nodes in the network, and demonstrate that the ﬁrst layer, similar to prior work on sparse coding and ICA, results in localized, oriented, edge ﬁlters, similar to the Gabor functions known to model V1 cell receptive ﬁelds. Further, the second layer in our model encodes correlations of the ﬁrst layer responses in the data. Speciﬁcally, it picks up both colinear (“contour”) features as well as corners and junctions. More interestingly, in a quantitative comparison, the encoding of these more complex “corner” features matches well with the results from the Ito & Komatsu’s study of biological V2 responses. This suggests that our sparse variant of deep belief networks holds promise for modeling more higher-order features. 1</p><p>4 0.58755541 <a title="180-lsi-4" href="./nips-2007-Using_Deep_Belief_Nets_to_Learn_Covariance_Kernels_for_Gaussian_Processes.html">212 nips-2007-Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes</a></p>
<p>Author: Geoffrey E. Hinton, Ruslan Salakhutdinov</p><p>Abstract: We show how to use unlabeled data and a deep belief net (DBN) to learn a good covariance kernel for a Gaussian process. We ﬁrst learn a deep generative model of the unlabeled data using the fast, greedy algorithm introduced by [7]. If the data is high-dimensional and highly-structured, a Gaussian kernel applied to the top layer of features in the DBN works much better than a similar kernel applied to the raw input. Performance at both regression and classiﬁcation can then be further improved by using backpropagation through the DBN to discriminatively ﬁne-tune the covariance kernel.</p><p>5 0.46765584 <a title="180-lsi-5" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: It has been shown that adapting a dictionary of basis functions to the statistics of natural images so as to maximize sparsity in the coefﬁcients results in a set of dictionary elements whose spatial properties resemble those of V1 (primary visual cortex) receptive ﬁelds. However, the resulting sparse coefﬁcients still exhibit pronounced statistical dependencies, thus violating the independence assumption of the sparse coding model. Here, we propose a model that attempts to capture the dependencies among the basis function coefﬁcients by including a pairwise coupling term in the prior over the coefﬁcient activity states. When adapted to the statistics of natural images, the coupling terms learn a combination of facilitatory and inhibitory interactions among neighboring basis functions. These learned interactions may offer an explanation for the function of horizontal connections in V1 in terms of a prior over natural images.</p><p>6 0.44561508 <a title="180-lsi-6" href="./nips-2007-Sparse_Overcomplete_Latent_Variable_Decomposition_of_Counts_Data.html">181 nips-2007-Sparse Overcomplete Latent Variable Decomposition of Counts Data</a></p>
<p>7 0.42082852 <a title="180-lsi-7" href="./nips-2007-On_Sparsity_and_Overcompleteness_in_Image_Models.html">145 nips-2007-On Sparsity and Overcompleteness in Image Models</a></p>
<p>8 0.41655728 <a title="180-lsi-8" href="./nips-2007-Mining_Internet-Scale_Software_Repositories.html">129 nips-2007-Mining Internet-Scale Software Repositories</a></p>
<p>9 0.41173163 <a title="180-lsi-9" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<p>10 0.38358095 <a title="180-lsi-10" href="./nips-2007-Direct_Importance_Estimation_with_Model_Selection_and_Its_Application_to_Covariate_Shift_Adaptation.html">67 nips-2007-Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation</a></p>
<p>11 0.38312191 <a title="180-lsi-11" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>12 0.37268972 <a title="180-lsi-12" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>13 0.35744393 <a title="180-lsi-13" href="./nips-2007-Unconstrained_On-line_Handwriting_Recognition_with_Recurrent_Neural_Networks.html">210 nips-2007-Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks</a></p>
<p>14 0.35724509 <a title="180-lsi-14" href="./nips-2007-Feature_Selection_Methods_for_Improving_Protein_Structure_Prediction_with_Rosetta.html">89 nips-2007-Feature Selection Methods for Improving Protein Structure Prediction with Rosetta</a></p>
<p>15 0.33431983 <a title="180-lsi-15" href="./nips-2007-Blind_channel_identification_for_speech_dereverberation_using_l1-norm_sparse_learning.html">37 nips-2007-Blind channel identification for speech dereverberation using l1-norm sparse learning</a></p>
<p>16 0.31895158 <a title="180-lsi-16" href="./nips-2007-Unsupervised_Feature_Selection_for_Accurate_Recommendation_of_High-Dimensional_Image_Data.html">211 nips-2007-Unsupervised Feature Selection for Accurate Recommendation of High-Dimensional Image Data</a></p>
<p>17 0.31425956 <a title="180-lsi-17" href="./nips-2007-Subspace-Based_Face_Recognition_in_Analog_VLSI.html">188 nips-2007-Subspace-Based Face Recognition in Analog VLSI</a></p>
<p>18 0.30692345 <a title="180-lsi-18" href="./nips-2007-Topmoumoute_Online_Natural_Gradient_Algorithm.html">206 nips-2007-Topmoumoute Online Natural Gradient Algorithm</a></p>
<p>19 0.30242667 <a title="180-lsi-19" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>20 0.29134363 <a title="180-lsi-20" href="./nips-2007-Convex_Learning_with_Invariances.html">62 nips-2007-Convex Learning with Invariances</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.078), (10, 0.145), (13, 0.042), (16, 0.041), (18, 0.011), (19, 0.017), (21, 0.057), (34, 0.035), (35, 0.054), (47, 0.093), (49, 0.012), (83, 0.183), (85, 0.016), (87, 0.062), (90, 0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90106654 <a title="180-lda-1" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>Author: Marc'aurelio Ranzato, Y-lan Boureau, Yann L. Cun</p><p>Abstract: Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation. We describe a novel and efﬁcient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machine trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input observed variables can be captured. 1</p><p>2 0.83319861 <a title="180-lda-2" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>Author: Nicolas L. Roux, Yoshua Bengio, Pascal Lamblin, Marc Joliveau, Balázs Kégl</p><p>Abstract: We study the following question: is the two-dimensional structure of images a very strong prior or is it something that can be learned with a few examples of natural images? If someone gave us a learning task involving images for which the two-dimensional topology of pixels was not known, could we discover it automatically and exploit it? For example suppose that the pixels had been permuted in a ﬁxed but unknown way, could we recover the relative two-dimensional location of pixels on images? The surprising result presented here is that not only the answer is yes, but that about as few as a thousand images are enough to approximately recover the relative locations of about a thousand pixels. This is achieved using a manifold learning algorithm applied to pixels associated with a measure of distributional similarity between pixel intensities. We compare different topologyextraction approaches and show how having the two-dimensional topology can be exploited.</p><p>3 0.83183086 <a title="180-lda-3" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>Author: Yuhong Guo, Dale Schuurmans</p><p>Abstract: We investigate a new, convex relaxation of an expectation-maximization (EM) variant that approximates a standard objective while eliminating local minima. First, a cautionary result is presented, showing that any convex relaxation of EM over hidden variables must give trivial results if any dependence on the missing values is retained. Although this appears to be a strong negative outcome, we then demonstrate how the problem can be bypassed by using equivalence relations instead of value assignments over hidden variables. In particular, we develop new algorithms for estimating exponential conditional models that only require equivalence relation information over the variable values. This reformulation leads to an exact expression for EM variants in a wide range of problems. We then develop a semideﬁnite relaxation that yields global training by eliminating local minima. 1</p><p>4 0.82899094 <a title="180-lda-4" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>Author: Matthias Bethge, Philipp Berens</p><p>Abstract: Maximum entropy analysis of binary variables provides an elegant way for studying the role of pairwise correlations in neural populations. Unfortunately, these approaches suffer from their poor scalability to high dimensions. In sensory coding, however, high-dimensional data is ubiquitous. Here, we introduce a new approach using a near-maximum entropy model, that makes this type of analysis feasible for very high-dimensional data—the model parameters can be derived in closed form and sampling is easy. Therefore, our NearMaxEnt approach can serve as a tool for testing predictions from a pairwise maximum entropy model not only for low-dimensional marginals, but also for high dimensional measurements of more than thousand units. We demonstrate its usefulness by studying natural images with dichotomized pixel intensities. Our results indicate that the statistics of such higher-dimensional measurements exhibit additional structure that are not predicted by pairwise correlations, despite the fact that pairwise correlations explain the lower-dimensional marginal statistics surprisingly well up to the limit of dimensionality where estimation of the full joint distribution is feasible. 1</p><p>5 0.82878679 <a title="180-lda-5" href="./nips-2007-Supervised_Topic_Models.html">189 nips-2007-Supervised Topic Models</a></p>
<p>Author: Jon D. Mcauliffe, David M. Blei</p><p>Abstract: We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive a maximum-likelihood procedure for parameter estimation, which relies on variational approximations to handle intractable posterior expectations. Prediction problems motivate this research: we use the ﬁtted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and web page popularity predicted from text descriptions. We illustrate the beneﬁts of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression. 1</p><p>6 0.82874173 <a title="180-lda-6" href="./nips-2007-Distributed_Inference_for_Latent_Dirichlet_Allocation.html">73 nips-2007-Distributed Inference for Latent Dirichlet Allocation</a></p>
<p>7 0.82592678 <a title="180-lda-7" href="./nips-2007-Colored_Maximum_Variance_Unfolding.html">49 nips-2007-Colored Maximum Variance Unfolding</a></p>
<p>8 0.82027215 <a title="180-lda-8" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>9 0.8190223 <a title="180-lda-9" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>10 0.81842631 <a title="180-lda-10" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>11 0.8170045 <a title="180-lda-11" href="./nips-2007-A_learning_framework_for_nearest_neighbor_search.html">16 nips-2007-A learning framework for nearest neighbor search</a></p>
<p>12 0.81651646 <a title="180-lda-12" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>13 0.81515151 <a title="180-lda-13" href="./nips-2007-Structured_Learning_with_Approximate_Inference.html">187 nips-2007-Structured Learning with Approximate Inference</a></p>
<p>14 0.81486148 <a title="180-lda-14" href="./nips-2007-People_Tracking_with_the_Laplacian_Eigenmaps_Latent_Variable_Model.html">153 nips-2007-People Tracking with the Laplacian Eigenmaps Latent Variable Model</a></p>
<p>15 0.81368601 <a title="180-lda-15" href="./nips-2007-DIFFRAC%3A_a_discriminative_and_flexible_framework_for_clustering.html">65 nips-2007-DIFFRAC: a discriminative and flexible framework for clustering</a></p>
<p>16 0.81240016 <a title="180-lda-16" href="./nips-2007-A_Bayesian_LDA-based_model_for_semi-supervised_part-of-speech_tagging.html">2 nips-2007-A Bayesian LDA-based model for semi-supervised part-of-speech tagging</a></p>
<p>17 0.80902886 <a title="180-lda-17" href="./nips-2007-Statistical_Analysis_of_Semi-Supervised_Regression.html">186 nips-2007-Statistical Analysis of Semi-Supervised Regression</a></p>
<p>18 0.80895102 <a title="180-lda-18" href="./nips-2007-Predictive_Matrix-Variate_t_Models.html">156 nips-2007-Predictive Matrix-Variate t Models</a></p>
<p>19 0.80882078 <a title="180-lda-19" href="./nips-2007-A_Kernel_Statistical_Test_of_Independence.html">7 nips-2007-A Kernel Statistical Test of Independence</a></p>
<p>20 0.80865073 <a title="180-lda-20" href="./nips-2007-Learning_the_structure_of_manifolds_using_random_projections.html">116 nips-2007-Learning the structure of manifolds using random projections</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
