<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>90 nips-2007-FilterBoost: Regression and Classification on Large Datasets</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-90" href="#">nips2007-90</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>90 nips-2007-FilterBoost: Regression and Classification on Large Datasets</h1>
<br/><p>Source: <a title="nips-2007-90-pdf" href="http://papers.nips.cc/paper/3321-filterboost-regression-and-classification-on-large-datasets.pdf">pdf</a></p><p>Author: Joseph K. Bradley, Robert E. Schapire</p><p>Abstract: We study boosting in the ﬁltering setting, where the booster draws examples from an oracle instead of using a ﬁxed training set and so may train efﬁciently on very large datasets. Our algorithm, which is based on a logistic regression technique proposed by Collins, Schapire, & Singer, requires fewer assumptions to achieve bounds equivalent to or better than previous work. Moreover, we give the ﬁrst proof that the algorithm of Collins et al. is a strong PAC learner, albeit within the ﬁltering setting. Our proofs demonstrate the algorithm’s strong theoretical properties for both classiﬁcation and conditional probability estimation, and we validate these results through extensive experiments. Empirically, our algorithm proves more robust to noise and overﬁtting than batch boosters in conditional probability estimation and proves competitive in classiﬁcation. 1</p><p>Reference: <a title="nips-2007-90-reference" href="../nips2007_reference/nips-2007-FilterBoost%3A_Regression_and_Classification_on_Large_Datasets_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We study boosting in the ﬁltering setting, where the booster draws examples from an oracle instead of using a ﬁxed training set and so may train efﬁciently on very large datasets. [sent-7, score-0.472]
</p><p>2 Empirically, our algorithm proves more robust to noise and overﬁtting than batch boosters in conditional probability estimation and proves competitive in classiﬁcation. [sent-12, score-0.622]
</p><p>3 Taking a weaker learner as input, boosters use the weak learner to generate weak hypotheses which are combined into a classiﬁcation rule more accurate than the weak hypotheses themselves. [sent-14, score-1.214]
</p><p>4 Most boosters are designed for the batch setting where the learner trains on a ﬁxed example set. [sent-16, score-0.703]
</p><p>5 Moreover, most batch boosters maintain distributions over the entire training set, making them computationally costly for very large datasets. [sent-18, score-0.596]
</p><p>6 To make boosting feasible on larger datasets, learners can be designed for the ﬁltering setting. [sent-19, score-0.157]
</p><p>7 The batch setting provides the learner with a ﬁxed training set, but the ﬁltering setting provides an oracle which can produce an unlimited number of labeled examples, one at a time. [sent-20, score-0.423]
</p><p>8 By using new training examples each round, ﬁltering boosters avoid maintaining a distribution over a training set and so may use large datasets much more efﬁciently than batch boosters. [sent-22, score-0.707]
</p><p>9 Later ﬁltering boosters included two more efﬁcient ones proposed by Freund, but both are non-adaptive, requiring a priori bounds on weak hypothesis error rates and combining weak hypotheses via unweighted majority votes [3,4]. [sent-24, score-0.898]
</p><p>10 Domingo & Watanabe’s MadaBoost is competitive with AdaBoost empirically but theoretically requires weak hypotheses’ error rates to be monotonically increasing, an assumption we found to be violated often in practice [5]. [sent-25, score-0.221]
</p><p>11 Bshouty & Gavinsky proposed another, but, like Freund’s, their algorithm requires an a priori bound on weak hypothesis error rates [6]. [sent-26, score-0.263]
</p><p>12 Gavinsky’s AdaFlatﬁlt algorithm and Hatano’s GiniBoost do not have these limitations, but the former has worse bounds than other adaptive algorithms while the latter explicitly requires ﬁnite weak hypothesis spaces [7,8]. [sent-27, score-0.263]
</p><p>13 In Section 2, we describe the algorithm, after which we interpret it as a stepwise method for ﬁtting an additive logistic regression model for conditional probabilities. [sent-30, score-0.157]
</p><p>14 We then bound the number of rounds and examples required to achieve any target error in (0, 1). [sent-31, score-0.193]
</p><p>15 These bounds match or improve upon those for previous ﬁltering boosters but require fewer assumptions. [sent-32, score-0.405]
</p><p>16 We also show that FilterBoost can use the conﬁdence-rated predictions from weak hypotheses described by Schapire & Singer [9]. [sent-33, score-0.25]
</p><p>17 For conditional probability estimation, we show that FilterBoost often outperforms batch boosters, which prove less robust to overﬁtting. [sent-35, score-0.215]
</p><p>18 For classiﬁcation, we show that ﬁltering boosters’ efﬁciency on large datasets allows them to achieve higher accuracies faster than batch boosters in many cases. [sent-36, score-0.652]
</p><p>19 Their batch algorithm has yet to be shown to achieve arbitrarily low test error, but we use techniques similar to those of MadaBoost to adapt the algorithm to the ﬁltering setting and prove generalization bounds. [sent-38, score-0.214]
</p><p>20 We assume there exists an unknown target distribution D over labeled examples (x, y) ∈ X × Y from which training and test examples are generated. [sent-43, score-0.162]
</p><p>21 In the batch setting, a booster is given a ﬁxed training set S and a weak learner which, given any distribution Dt over training examples S, is guaranteed to return a weak hypothesis ht : X → R such that the error t ≡ PrDt [sign(ht (x)) = y] < 1/2. [sent-45, score-1.173]
</p><p>22 For T rounds t, the booster builds a distribution Dt over S, runs the weak learner on S and Dt , and receives ht . [sent-46, score-0.695]
</p><p>23 The booster usually then estimates t using S and weights ht with αt = αt ( t ). [sent-47, score-0.372]
</p><p>24 After T rounds, the booster outputs a ﬁnal hypothesis H which is a linear combination of the weak hypotheses (e. [sent-48, score-0.427]
</p><p>25 ˆ Two key elements of boosting are constructing Dt over S and weighting weak hypotheses. [sent-52, score-0.301]
</p><p>26 Dt is built such that misclassiﬁed examples receive higher weights than in Dt−1 , eventually forcing the weak learner to classify previously poorly classiﬁed examples correctly. [sent-53, score-0.377]
</p><p>27 Weak hypotheses ht are generally weighted such that hypotheses with lower errors receive higher weights. [sent-54, score-0.332]
</p><p>28 The ﬁltering setting assumes the learner has access to an example oracle, allowing it to use entirely new examples sampled i. [sent-57, score-0.159]
</p><p>29 However, while maintaining the distribution Dt is straightforward in the batch setting, there is no ﬁxed set S on which to deﬁne Dt in ﬁltering. [sent-61, score-0.195]
</p><p>30 Instead, the booster simulates examples drawn from Dt by drawing examples from D via the oracle and reweighting them according to Dt . [sent-62, score-0.342]
</p><p>31 Filtering boosters generally accept each example (x, y) from the oracle for training on round t with probability proportional to the example’s weight Dt (x, y). [sent-63, score-0.561]
</p><p>32 The mechanism which accepts examples from the oracle with some probability is called the ﬁlter. [sent-64, score-0.188]
</p><p>33 Thus, on each round, a boosting-by-ﬁltering algorithm draws a set of examples from Dt via the ﬁlter, trains the weak learner on this set, and receives a weak hypothesis ht . [sent-65, score-0.794]
</p><p>34 Though a batch booster would estimate t using the ﬁxed set S, ﬁltering boosters may use new examples from the ﬁlter. [sent-66, score-0.783]
</p><p>35 Like batch boosters, ﬁltering boosters may weight ht using αt = αt ( t ), and they output a linear combination of h1 , . [sent-67, score-0.788]
</p><p>36 2  t−1  The ﬁltering setting allows the learner Deﬁne Ft (x) ≡ t =1 αt ht (x) to estimate the error of Ht to arbitrary Algorithm F ilterBoost accepts Oracle(), ε, δ, τ : precision by sampling from D via the For t = 1, 2, 3, . [sent-71, score-0.403]
</p><p>37 2  Call F ilter(t, δt , ε) to get mt examples to train WL; get ht γt ←− getEdge(t, τ, δt , ε) ˆ  FilterBoost  FilterBoost, given in Figure 1, is mod1/2+ˆ γ 1 eled after the aforementioned algoαt ←− 2 ln 1/2−ˆt γt rithm by Collins et al. [sent-76, score-0.373]
</p><p>38 Given an example oracle, weak learner, target error ε ∈ (0, 1), (Algorithm exits from F ilter() function. [sent-78, score-0.215]
</p><p>39 δt δt ←− r(r+1) On round t, FilterBoost draws mt ex1 For (i = 0; i < 2 ln( δ ); i = i + 1): amples from the ﬁlter to train the weak ε t learner and get ht . [sent-80, score-0.617]
</p><p>40 The number mt (x, y) ←− Oracle() must be large enough to ensure ht has 1 qt (x, y) ←− 1+eyFt (x) error t < 1/2 with high probability. [sent-81, score-0.39]
</p><p>41 The edge of ht is γt = 1/2 − t , Return (x, y) with probability qt (x, y) and this edge is estimated by the funcEnd algorithm; return Ht−1 tion getEdge(), discussed below, and Function getEdge(t, τ, δt , ε) returns γt ˆ is used to set ht ’s weight αt . [sent-82, score-0.632]
</p><p>42 The current combined hypothesis is deﬁned as Let m ←− 0, n ←− 0, u ←− 0, α ←− ∞ t Ht = sign( t =1 αt ht ). [sent-83, score-0.257]
</p><p>43 While (|u| < α(1 + 1/τ )): The F ilter() function generates (x, y) (x, y) ←− F ilter(t, δt , ε) from Dt by repeatedly drawing (x, y) n ←− n + 1 from the oracle, calculating the weight m ←− m + I(ht (x) = y) qt (x, y) ∝ Dt (x, y), and accepting u ←− m/n − 1/2 (x, y) with probability qt (x, y). [sent-84, score-0.244]
</p><p>44 Their algorithm draws an adaptively chosen number of examples from the ﬁlter and returns an estimate γt of the edge of ht within relative error τ of the true edge γt with ˆ high probability. [sent-87, score-0.396]
</p><p>45 Such models take the form 1 Pr[y = 1|x] = ft (x) = F (x), which implies Pr[y = 1|x] = log Pr[y = −1|x] 1 + e−F (x) t where, for FilterBoost, ft (x) = αt ht (x). [sent-91, score-0.299]
</p><p>46 Dropping subscripts, we can write the expected negative log likelihood of example (x, y) after round t as 1  = E ln 1 + e−y(F (x)+αh(x)) . [sent-92, score-0.14]
</p><p>47 π(Ft + αt ht ) = π(F + αh) = E − ln  e−y(F (x)+αh(x))  3  Theorem 1 Deﬁne the expected negative log likelihood π(F + αh) as above. [sent-95, score-0.295]
</p><p>48 Note that FilterBoost uses weak learners which are simple classiﬁers to perform regression. [sent-100, score-0.206]
</p><p>49 4  Analysis: Classiﬁcation  In this section, we interpret FilterBoost as a traditional boosting algorithm for classiﬁcation and prove bounds on its generalization error. [sent-103, score-0.144]
</p><p>50 We ﬁrst give a theorem relating errt , the error rate of Ht over the target distribution D, to pt , the probability with which the ﬁlter accepts a random example generated by the oracle on round t. [sent-104, score-0.479]
</p><p>51 Theorem 2 Let errt = PrD [Ht (x) = y], and let pt = ED [qt (x, y)]. [sent-105, score-0.192]
</p><p>52 Proof: errt  = PrD [Ht (x) = y] = PrD [yFt−1 (x) ≤ 0] = PrD [qt (x, y) ≥ 1/2] ≤ 2 · ED [qt (x, y)] = 2pt (using Markov’s inequality above)  We next use the expected negative log likelihood π from Section 2. [sent-107, score-0.143]
</p><p>53 3 as an auxiliary function to aid in bounding the required number of boosting rounds. [sent-108, score-0.135]
</p><p>54 Viewing π as a function of the boosting round t, we can write πt = − (x,y) D(x, y) ln(1 − qt (x, y)). [sent-109, score-0.299]
</p><p>55 Combining Theorem 2, which bounds the error of the current combined hypothesis in terms of pt , with Lemma 1 gives the following upper bound on the required rounds T . [sent-119, score-0.238]
</p><p>56 Theorem 3 shows FilterBoost can reduce generalization error to any ε ∈ (0, 1), but we have thus far overlooked the probabilities of failure introduced by three steps: training the weak learner, deciding when to stop boosting, and estimating edges. [sent-134, score-0.231]
</p><p>57 We bound the probability of each of these steps failing δ on round t with a conﬁdence parameter δt = 3t(t+1) so that a simple union bound ensures the probability of some step failing to be at most FilterBoost’s conﬁdence parameter δ. [sent-135, score-0.149]
</p><p>58 The number mt of examples the weak learner trains on must be large enough to ensure weak hypothesis ht has a non-zero edge and should be set according to the choice of weak learner. [sent-137, score-1.007]
</p><p>59 when errt ≤ ε), we can use Theorem 2, which upper-bounds the error of the current combined hypothesis Ht in terms of the probability pt that F ilter() accepts a random example from the oracle. [sent-140, score-0.316]
</p><p>60 Theorem 4 In a single call to F ilter(t), if n examples have been rejected, where n ≥ then errt ≤ ε with probability at least 1 − δt . [sent-143, score-0.175]
</p><p>61 We estimate weak hypotheses’ edges γt using the Nonmonotonic Adaptive Sampling (NAS) algorithm [11,12] used by MadaBoost. [sent-147, score-0.19]
</p><p>62 The bounds from Theorems 3 and 5 show FilterBoost requires at most O(ε−1 γ −2 ) boosting rounds. [sent-156, score-0.144]
</p><p>63 MadaBoost [5], which we test in our experiments, resembles FilterBoost but uses truncated exponential weights qt (x, y) = min{1, exp(yFt−1 (x))} instead of the logistic weights qt (x, y) = (1 + exp(yFt (x)))−1 used by FilterBoost. [sent-157, score-0.296]
</p><p>64 The non-adaptive ﬁltering boosters of Freund [3,4] and of Bshouty & Gavinsky [6] and the batch booster AdaBoost [1] have smaller bounds on T , proportional to log(ε−1 ). [sent-159, score-0.777]
</p><p>65 However, we can use boosting tandems, a technique used by Freund [4] and Gavinsky [7], to create a ﬁltering booster with T bounded by O(log(ε−1 )γ −2 ). [sent-160, score-0.276]
</p><p>66 Following Gavinsky, we can use FilterBoost to boost the accuracy of the weak learner to some constant and, in turn, treat FilterBoost as a weak learner and use an algorithm from Freund to achieve any target error. [sent-161, score-0.591]
</p><p>67 As with AdaFlatﬁlt , boosting tandems turn FilterBoost into an adaptive booster with a bound on T proportional to log(ε−1 ). [sent-162, score-0.399]
</p><p>68 ) Note, however, that boosting tandems result in more complicated ﬁnal hypotheses. [sent-164, score-0.186]
</p><p>69 Schapire & Singer [9] show AdaBoost beneﬁts from conﬁdence-rated predictions, where weak hypotheses return predictions whose absolute values indicate conﬁdence. [sent-168, score-0.25]
</p><p>70 3  Experiments  Vanilla FilterBoost accepts examples (x, y) from the oracle with probability qt (x, y), but it may instead accept all examples and weight each with qt (x, y). [sent-173, score-0.483]
</p><p>71 Weighting instead of ﬁltering examples increases accuracy but also increases the size of the training set passed to the weak learner. [sent-174, score-0.24]
</p><p>72 For efﬁciency, we choose to ﬁlter when training the weak learner but weight when estimating edges γt . [sent-175, score-0.32]
</p><p>73 For simplicity, we train weak learners with Cn log(t + 1) examples as well. [sent-180, score-0.275]
</p><p>74 In practice, ﬁltering boosters can achieve higher accuracy by cycling through training sets again instead of stopping once examples are depleted, and we use this “recycling” in our tests. [sent-183, score-0.489]
</p><p>75 We compare FilterBoost against MadaBoost [5], which does not require an a priori bound on weak hypotheses’ edges and has similar bounds without the complication of boosting tandems. [sent-185, score-0.361]
</p><p>76 We test FilterBoost against two batch boosters: the well-studied and historically successful AdaBoost [1] and the algorithm from Collins et al. [sent-187, score-0.195]
</p><p>77 [10] which is essentially a batch version of FilterBoost (labeled “AdaBoost-LOG”). [sent-188, score-0.195]
</p><p>78 In resampling, the booster trains weak learners on small sets of examples sampled from the distribution Dt over the training set S rather than on the entire set S, and this technique often increases efﬁciency with little effect on accuracy. [sent-190, score-0.458]
</p><p>79 Our batch boosters use sets of size Cm log(t + 1) for training, like the ﬁltering boosters, but use all of S to estimate edges γt since this can be done efﬁciently. [sent-191, score-0.597]
</p><p>80 We test the batch boosters using conﬁdence-rated predictions and resampling in order to compare FilterBoost with batch algorithms optimized for the efﬁciency which boosting-by-ﬁltering claims as its goal. [sent-192, score-0.836]
</p><p>81 We test each booster using decision stumps and decision trees as weak learners to discern the effects of simple and complicated weak hypotheses. [sent-193, score-0.756]
</p><p>82 The decision stumps minimize training error, and the decision trees greedily maximize information gain and are pruned using 1/3 of the data. [sent-194, score-0.247]
</p><p>83 Both weak learners minimize exponential loss when outputing conﬁdence-rated predictions. [sent-195, score-0.224]
</p><p>84 As expected, ﬁltering boosters run slower per round than batch boosters on small datasets but much faster on large ones. [sent-203, score-1.05]
</p><p>85 Interestingly, ﬁltering boosters take longer on very small datasets in some cases (not shown), for the probability the ﬁlter accepts an example quickly shrinks when the booster has seen that example many times. [sent-204, score-0.63]
</p><p>86 As Figure 3 shows, both FilterBoost variants are competitive with batch algorithms when boosting decision stumps. [sent-212, score-0.392]
</p><p>87 In each plot, we compare FilterBoost with the best of AdaBoost and AdaBoost-LOG: AdaBoost was best with decision stumps and AdaBoost-LOG with de- Figure 3: Log (base e) loss & root mean squared error cisions trees. [sent-214, score-0.156]
</p><p>88 logistic regression via gradient de- Left two: WL = stumps (FilterBoost vs. [sent-217, score-0.154]
</p><p>89 On Adult and Twonorm, FilterBoost generally outperforms the batch boosters, which tend to overﬁt when boosting decision trees, though AdaBoost slightly outperforms FilterBoost on smaller datasets when boosting decision stumps. [sent-223, score-0.571]
</p><p>90 The Covertype dataset is an exception to our results and highlights a danger in ﬁltering and in resampling for batch learning: the complicated structure of some datasets seems to require decision trees to train on the entire dataset. [sent-224, score-0.389]
</p><p>91 With decision stumps, the ﬁltering boosters are competitive, yet only the non-resampling batch boosters achieve high accuracies with decision trees. [sent-225, score-1.095]
</p><p>92 The ﬁrst decision tree trained on the entire training set achieves about 94% accuracy, which is unachievable by any of the ﬁltering or resampling batch boosters when using Cm = 300 as the base number of examples for training the weak learner. [sent-226, score-0.93]
</p><p>93 To compete with non-resampling batch boosters, the other boosters must use Cm on the order of 105 , by which point they become very inefﬁcient. [sent-227, score-0.574]
</p><p>94 Conﬁdence-rated predictions allow FilterBoost to outperform MadaBoost when using decision stumps but sometimes cause FilterBoost to perform poorly with decision trees. [sent-230, score-0.205]
</p><p>95 Figure 5 compares FilterBoost with the best batch booster for each weak learner. [sent-231, score-0.52]
</p><p>96 With decision stumps, all boosters achieve higher accuracies with the larger dataset, on which ﬁltering algorithms are much more efﬁcient. [sent-232, score-0.47]
</p><p>97 Majority is represented well as a linear combination of decision stumps, so the boosters all learn more slowly 7  Figure 4: FilterBoost vs. [sent-233, score-0.43]
</p><p>98 However, this problem generally affects ﬁltering boosters less than most batch variants, especially on larger datasets. [sent-239, score-0.574]
</p><p>99 1, ﬁltering and resampling batch boosters perform poorly on Covertype. [sent-242, score-0.617]
</p><p>100 (2003) Optimally-smooth adaptive boosting and application to agnostic learning. [sent-275, score-0.145]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('filterboost', 0.671), ('boosters', 0.379), ('ht', 0.214), ('batch', 0.195), ('weak', 0.167), ('booster', 0.158), ('ltering', 0.132), ('errt', 0.124), ('qt', 0.122), ('madaboost', 0.119), ('boosting', 0.118), ('adaboost', 0.114), ('ilter', 0.112), ('learner', 0.108), ('gavinsky', 0.087), ('getedge', 0.087), ('oracle', 0.082), ('dt', 0.08), ('stumps', 0.079), ('pt', 0.068), ('watanabe', 0.065), ('schapire', 0.063), ('ln', 0.062), ('collins', 0.061), ('hypotheses', 0.059), ('round', 0.059), ('accepts', 0.055), ('prd', 0.054), ('singer', 0.052), ('logistic', 0.052), ('examples', 0.051), ('decision', 0.051), ('nas', 0.05), ('tandems', 0.05), ('rounds', 0.048), ('freund', 0.048), ('domingo', 0.043), ('stepwise', 0.043), ('hypothesis', 0.043), ('theorem', 0.043), ('resampling', 0.043), ('edge', 0.041), ('lemma', 0.04), ('learners', 0.039), ('datasets', 0.038), ('adaflat', 0.037), ('bshouty', 0.037), ('nonmonotonic', 0.037), ('yft', 0.037), ('wl', 0.035), ('ft', 0.033), ('classi', 0.033), ('majority', 0.031), ('adult', 0.03), ('lter', 0.029), ('competitive', 0.028), ('mt', 0.028), ('con', 0.027), ('adaptive', 0.027), ('bound', 0.027), ('error', 0.026), ('bounds', 0.026), ('twonorm', 0.026), ('trees', 0.026), ('galvad', 0.025), ('hatano', 0.025), ('predictions', 0.024), ('draws', 0.023), ('regression', 0.023), ('edges', 0.023), ('training', 0.022), ('target', 0.022), ('lt', 0.022), ('trains', 0.021), ('accuracies', 0.021), ('cn', 0.021), ('conditional', 0.02), ('covertype', 0.02), ('mint', 0.02), ('log', 0.019), ('cm', 0.019), ('proportional', 0.019), ('additive', 0.019), ('sign', 0.019), ('achieve', 0.019), ('vanilla', 0.018), ('failing', 0.018), ('pr', 0.018), ('stopping', 0.018), ('train', 0.018), ('complicated', 0.018), ('minimize', 0.018), ('aid', 0.017), ('dence', 0.017), ('ciency', 0.016), ('stop', 0.016), ('weighting', 0.016), ('modi', 0.016), ('labeled', 0.016), ('discovery', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="90-tfidf-1" href="./nips-2007-FilterBoost%3A_Regression_and_Classification_on_Large_Datasets.html">90 nips-2007-FilterBoost: Regression and Classification on Large Datasets</a></p>
<p>Author: Joseph K. Bradley, Robert E. Schapire</p><p>Abstract: We study boosting in the ﬁltering setting, where the booster draws examples from an oracle instead of using a ﬁxed training set and so may train efﬁciently on very large datasets. Our algorithm, which is based on a logistic regression technique proposed by Collins, Schapire, & Singer, requires fewer assumptions to achieve bounds equivalent to or better than previous work. Moreover, we give the ﬁrst proof that the algorithm of Collins et al. is a strong PAC learner, albeit within the ﬁltering setting. Our proofs demonstrate the algorithm’s strong theoretical properties for both classiﬁcation and conditional probability estimation, and we validate these results through extensive experiments. Empirically, our algorithm proves more robust to noise and overﬁtting than batch boosters in conditional probability estimation and proves competitive in classiﬁcation. 1</p><p>2 0.17675285 <a title="90-tfidf-2" href="./nips-2007-One-Pass_Boosting.html">147 nips-2007-One-Pass Boosting</a></p>
<p>Author: Zafer Barutcuoglu, Phil Long, Rocco Servedio</p><p>Abstract: This paper studies boosting algorithms that make a single pass over a set of base classiﬁers. We ﬁrst analyze a one-pass algorithm in the setting of boosting with diverse base classiﬁers. Our guarantee is the same as the best proved for any boosting algorithm, but our one-pass algorithm is much faster than previous approaches. We next exhibit a random source of examples for which a “picky” variant of AdaBoost that skips poor base classiﬁers can outperform the standard AdaBoost algorithm, which uses every base classiﬁer, by an exponential factor. Experiments with Reuters and synthetic data show that one-pass boosting can substantially improve on the accuracy of Naive Bayes, and that picky boosting can sometimes lead to a further improvement in accuracy.</p><p>3 0.16203326 <a title="90-tfidf-3" href="./nips-2007-Boosting_the_Area_under_the_ROC_Curve.html">39 nips-2007-Boosting the Area under the ROC Curve</a></p>
<p>Author: Phil Long, Rocco Servedio</p><p>Abstract: We show that any weak ranker that can achieve an area under the ROC curve slightly better than 1/2 (which can be achieved by random guessing) can be efﬁciently boosted to achieve an area under the ROC curve arbitrarily close to 1. We further show that this boosting can be performed even in the presence of independent misclassiﬁcation noise, given access to a noise-tolerant weak ranker.</p><p>4 0.13415958 <a title="90-tfidf-4" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<p>Author: David Wingate, Satinder S. Baveja</p><p>Abstract: In order to represent state in controlled, partially observable, stochastic dynamical systems, some sort of sufﬁcient statistic for history is necessary. Predictive representations of state (PSRs) capture state as statistics of the future. We introduce a new model of such systems called the “Exponential family PSR,” which deﬁnes as state the time-varying parameters of an exponential family distribution which models n sequential observations in the future. This choice of state representation explicitly connects PSRs to state-of-the-art probabilistic modeling, which allows us to take advantage of current efforts in high-dimensional density estimation, and in particular, graphical models and maximum entropy models. We present a parameter learning algorithm based on maximum likelihood, and we show how a variety of current approximate inference methods apply. We evaluate the quality of our model with reinforcement learning by directly evaluating the control performance of the model. 1</p><p>5 0.11809663 <a title="90-tfidf-5" href="./nips-2007-Regularized_Boost_for_Semi-Supervised_Learning.html">166 nips-2007-Regularized Boost for Semi-Supervised Learning</a></p>
<p>Author: Ke Chen, Shihai Wang</p><p>Abstract: Semi-supervised inductive learning concerns how to learn a decision rule from a data set containing both labeled and unlabeled data. Several boosting algorithms have been extended to semi-supervised learning with various strategies. To our knowledge, however, none of them takes local smoothness constraints among data into account during ensemble learning. In this paper, we introduce a local smoothness regularizer to semi-supervised boosting algorithms based on the universal optimization framework of margin cost functionals. Our regularizer is applicable to existing semi-supervised boosting algorithms to improve their generalization and speed up their training. Comparative results on synthetic, benchmark and real world tasks demonstrate the effectiveness of our local smoothness regularizer. We discuss relevant issues and relate our regularizer to previous work. 1</p><p>6 0.10997864 <a title="90-tfidf-6" href="./nips-2007-Boosting_Algorithms_for_Maximizing_the_Soft_Margin.html">38 nips-2007-Boosting Algorithms for Maximizing the Soft Margin</a></p>
<p>7 0.096772745 <a title="90-tfidf-7" href="./nips-2007-Adaptive_Online_Gradient_Descent.html">21 nips-2007-Adaptive Online Gradient Descent</a></p>
<p>8 0.089633018 <a title="90-tfidf-8" href="./nips-2007-Discriminative_Batch_Mode_Active_Learning.html">69 nips-2007-Discriminative Batch Mode Active Learning</a></p>
<p>9 0.077085517 <a title="90-tfidf-9" href="./nips-2007-The_Price_of_Bandit_Information_for_Online_Optimization.html">199 nips-2007-The Price of Bandit Information for Online Optimization</a></p>
<p>10 0.067878082 <a title="90-tfidf-10" href="./nips-2007-Learning_Bounds_for_Domain_Adaptation.html">110 nips-2007-Learning Bounds for Domain Adaptation</a></p>
<p>11 0.063406058 <a title="90-tfidf-11" href="./nips-2007-Sequential_Hypothesis_Testing_under_Stochastic_Deadlines.html">176 nips-2007-Sequential Hypothesis Testing under Stochastic Deadlines</a></p>
<p>12 0.059454467 <a title="90-tfidf-12" href="./nips-2007-Automatic_Generation_of_Social_Tags_for_Music_Recommendation.html">29 nips-2007-Automatic Generation of Social Tags for Music Recommendation</a></p>
<p>13 0.054582123 <a title="90-tfidf-13" href="./nips-2007-Optimistic_Linear_Programming_gives_Logarithmic_Regret_for_Irreducible_MDPs.html">151 nips-2007-Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs</a></p>
<p>14 0.050480932 <a title="90-tfidf-14" href="./nips-2007-Multiple-Instance_Pruning_For_Learning_Efficient_Cascade_Detectors.html">137 nips-2007-Multiple-Instance Pruning For Learning Efficient Cascade Detectors</a></p>
<p>15 0.044845141 <a title="90-tfidf-15" href="./nips-2007-Theoretical_Analysis_of_Heuristic_Search_Methods_for_Online_POMDPs.html">204 nips-2007-Theoretical Analysis of Heuristic Search Methods for Online POMDPs</a></p>
<p>16 0.044321001 <a title="90-tfidf-16" href="./nips-2007-Collective_Inference_on_Markov_Models_for_Modeling_Bird_Migration.html">48 nips-2007-Collective Inference on Markov Models for Modeling Bird Migration</a></p>
<p>17 0.040932156 <a title="90-tfidf-17" href="./nips-2007-Variational_Inference_for_Diffusion_Processes.html">213 nips-2007-Variational Inference for Diffusion Processes</a></p>
<p>18 0.040504854 <a title="90-tfidf-18" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>19 0.040379863 <a title="90-tfidf-19" href="./nips-2007-A_General_Boosting_Method_and_its_Application_to_Learning_Ranking_Functions_for_Web_Search.html">6 nips-2007-A General Boosting Method and its Application to Learning Ranking Functions for Web Search</a></p>
<p>20 0.037056506 <a title="90-tfidf-20" href="./nips-2007-The_Epoch-Greedy_Algorithm_for_Multi-armed_Bandits_with_Side_Information.html">194 nips-2007-The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.131), (1, -0.062), (2, -0.04), (3, 0.102), (4, 0.131), (5, 0.073), (6, 0.112), (7, -0.101), (8, 0.102), (9, -0.165), (10, 0.061), (11, 0.12), (12, -0.132), (13, -0.077), (14, 0.018), (15, -0.023), (16, 0.129), (17, 0.046), (18, -0.07), (19, 0.017), (20, -0.099), (21, 0.147), (22, -0.138), (23, 0.056), (24, -0.07), (25, 0.037), (26, -0.044), (27, -0.042), (28, -0.007), (29, -0.021), (30, -0.019), (31, -0.067), (32, -0.032), (33, 0.046), (34, 0.016), (35, -0.052), (36, 0.011), (37, 0.065), (38, -0.022), (39, -0.013), (40, -0.049), (41, -0.127), (42, -0.034), (43, -0.071), (44, -0.062), (45, 0.11), (46, 0.074), (47, -0.008), (48, -0.126), (49, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94837123 <a title="90-lsi-1" href="./nips-2007-FilterBoost%3A_Regression_and_Classification_on_Large_Datasets.html">90 nips-2007-FilterBoost: Regression and Classification on Large Datasets</a></p>
<p>Author: Joseph K. Bradley, Robert E. Schapire</p><p>Abstract: We study boosting in the ﬁltering setting, where the booster draws examples from an oracle instead of using a ﬁxed training set and so may train efﬁciently on very large datasets. Our algorithm, which is based on a logistic regression technique proposed by Collins, Schapire, & Singer, requires fewer assumptions to achieve bounds equivalent to or better than previous work. Moreover, we give the ﬁrst proof that the algorithm of Collins et al. is a strong PAC learner, albeit within the ﬁltering setting. Our proofs demonstrate the algorithm’s strong theoretical properties for both classiﬁcation and conditional probability estimation, and we validate these results through extensive experiments. Empirically, our algorithm proves more robust to noise and overﬁtting than batch boosters in conditional probability estimation and proves competitive in classiﬁcation. 1</p><p>2 0.76332003 <a title="90-lsi-2" href="./nips-2007-One-Pass_Boosting.html">147 nips-2007-One-Pass Boosting</a></p>
<p>Author: Zafer Barutcuoglu, Phil Long, Rocco Servedio</p><p>Abstract: This paper studies boosting algorithms that make a single pass over a set of base classiﬁers. We ﬁrst analyze a one-pass algorithm in the setting of boosting with diverse base classiﬁers. Our guarantee is the same as the best proved for any boosting algorithm, but our one-pass algorithm is much faster than previous approaches. We next exhibit a random source of examples for which a “picky” variant of AdaBoost that skips poor base classiﬁers can outperform the standard AdaBoost algorithm, which uses every base classiﬁer, by an exponential factor. Experiments with Reuters and synthetic data show that one-pass boosting can substantially improve on the accuracy of Naive Bayes, and that picky boosting can sometimes lead to a further improvement in accuracy.</p><p>3 0.7209748 <a title="90-lsi-3" href="./nips-2007-Boosting_Algorithms_for_Maximizing_the_Soft_Margin.html">38 nips-2007-Boosting Algorithms for Maximizing the Soft Margin</a></p>
<p>Author: Gunnar Rätsch, Manfred K. Warmuth, Karen A. Glocer</p><p>Abstract: We present a novel boosting algorithm, called SoftBoost, designed for sets of binary labeled examples that are not necessarily separable by convex combinations of base hypotheses. Our algorithm achieves robustness by capping the distributions on the examples. Our update of the distribution is motivated by minimizing a relative entropy subject to the capping constraints and constraints on the edges of the obtained base hypotheses. The capping constraints imply a soft margin in the dual optimization problem. Our algorithm produces a convex combination of hypotheses whose soft margin is within δ of its maximum. We employ relative enN tropy projection methods to prove an O( ln 2 ) iteration bound for our algorithm, δ where N is number of examples. We compare our algorithm with other approaches including LPBoost, BrownBoost, and SmoothBoost. We show that there exist cases where the number of iterations required by LPBoost grows linearly in N instead of the logarithmic growth for SoftBoost. In simulation studies we show that our algorithm converges about as fast as LPBoost, faster than BrownBoost, and much faster than SmoothBoost. In a benchmark comparison we illustrate the competitiveness of our approach.</p><p>4 0.66220027 <a title="90-lsi-4" href="./nips-2007-Boosting_the_Area_under_the_ROC_Curve.html">39 nips-2007-Boosting the Area under the ROC Curve</a></p>
<p>Author: Phil Long, Rocco Servedio</p><p>Abstract: We show that any weak ranker that can achieve an area under the ROC curve slightly better than 1/2 (which can be achieved by random guessing) can be efﬁciently boosted to achieve an area under the ROC curve arbitrarily close to 1. We further show that this boosting can be performed even in the presence of independent misclassiﬁcation noise, given access to a noise-tolerant weak ranker.</p><p>5 0.51637948 <a title="90-lsi-5" href="./nips-2007-Regularized_Boost_for_Semi-Supervised_Learning.html">166 nips-2007-Regularized Boost for Semi-Supervised Learning</a></p>
<p>Author: Ke Chen, Shihai Wang</p><p>Abstract: Semi-supervised inductive learning concerns how to learn a decision rule from a data set containing both labeled and unlabeled data. Several boosting algorithms have been extended to semi-supervised learning with various strategies. To our knowledge, however, none of them takes local smoothness constraints among data into account during ensemble learning. In this paper, we introduce a local smoothness regularizer to semi-supervised boosting algorithms based on the universal optimization framework of margin cost functionals. Our regularizer is applicable to existing semi-supervised boosting algorithms to improve their generalization and speed up their training. Comparative results on synthetic, benchmark and real world tasks demonstrate the effectiveness of our local smoothness regularizer. We discuss relevant issues and relate our regularizer to previous work. 1</p><p>6 0.42489266 <a title="90-lsi-6" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<p>7 0.35794976 <a title="90-lsi-7" href="./nips-2007-Optimal_ROC_Curve_for_a_Combination_of_Classifiers.html">149 nips-2007-Optimal ROC Curve for a Combination of Classifiers</a></p>
<p>8 0.33072129 <a title="90-lsi-8" href="./nips-2007-Sequential_Hypothesis_Testing_under_Stochastic_Deadlines.html">176 nips-2007-Sequential Hypothesis Testing under Stochastic Deadlines</a></p>
<p>9 0.32414892 <a title="90-lsi-9" href="./nips-2007-Multiple-Instance_Pruning_For_Learning_Efficient_Cascade_Detectors.html">137 nips-2007-Multiple-Instance Pruning For Learning Efficient Cascade Detectors</a></p>
<p>10 0.32375833 <a title="90-lsi-10" href="./nips-2007-Learning_Bounds_for_Domain_Adaptation.html">110 nips-2007-Learning Bounds for Domain Adaptation</a></p>
<p>11 0.31440973 <a title="90-lsi-11" href="./nips-2007-Adaptive_Online_Gradient_Descent.html">21 nips-2007-Adaptive Online Gradient Descent</a></p>
<p>12 0.29300779 <a title="90-lsi-12" href="./nips-2007-Anytime_Induction_of_Cost-sensitive_Trees.html">27 nips-2007-Anytime Induction of Cost-sensitive Trees</a></p>
<p>13 0.26945156 <a title="90-lsi-13" href="./nips-2007-The_Price_of_Bandit_Information_for_Online_Optimization.html">199 nips-2007-The Price of Bandit Information for Online Optimization</a></p>
<p>14 0.26881695 <a title="90-lsi-14" href="./nips-2007-Automatic_Generation_of_Social_Tags_for_Music_Recommendation.html">29 nips-2007-Automatic Generation of Social Tags for Music Recommendation</a></p>
<p>15 0.26870114 <a title="90-lsi-15" href="./nips-2007-Fast_and_Scalable_Training_of_Semi-Supervised_CRFs_with_Application_to_Activity_Recognition.html">88 nips-2007-Fast and Scalable Training of Semi-Supervised CRFs with Application to Activity Recognition</a></p>
<p>16 0.26219165 <a title="90-lsi-16" href="./nips-2007-A_general_agnostic_active_learning_algorithm.html">15 nips-2007-A general agnostic active learning algorithm</a></p>
<p>17 0.25894096 <a title="90-lsi-17" href="./nips-2007-Stability_Bounds_for_Non-i.i.d._Processes.html">184 nips-2007-Stability Bounds for Non-i.i.d. Processes</a></p>
<p>18 0.25324664 <a title="90-lsi-18" href="./nips-2007-The_Epoch-Greedy_Algorithm_for_Multi-armed_Bandits_with_Side_Information.html">194 nips-2007-The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information</a></p>
<p>19 0.23270443 <a title="90-lsi-19" href="./nips-2007-A_General_Boosting_Method_and_its_Application_to_Learning_Ranking_Functions_for_Web_Search.html">6 nips-2007-A General Boosting Method and its Application to Learning Ranking Functions for Web Search</a></p>
<p>20 0.22831576 <a title="90-lsi-20" href="./nips-2007-Transfer_Learning_using_Kolmogorov_Complexity%3A_Basic_Theory_and_Empirical_Evaluations.html">207 nips-2007-Transfer Learning using Kolmogorov Complexity: Basic Theory and Empirical Evaluations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.031), (13, 0.028), (16, 0.021), (18, 0.039), (21, 0.027), (31, 0.011), (34, 0.038), (35, 0.035), (46, 0.037), (47, 0.056), (49, 0.013), (60, 0.349), (83, 0.127), (85, 0.01), (88, 0.033), (90, 0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.67202592 <a title="90-lda-1" href="./nips-2007-FilterBoost%3A_Regression_and_Classification_on_Large_Datasets.html">90 nips-2007-FilterBoost: Regression and Classification on Large Datasets</a></p>
<p>Author: Joseph K. Bradley, Robert E. Schapire</p><p>Abstract: We study boosting in the ﬁltering setting, where the booster draws examples from an oracle instead of using a ﬁxed training set and so may train efﬁciently on very large datasets. Our algorithm, which is based on a logistic regression technique proposed by Collins, Schapire, & Singer, requires fewer assumptions to achieve bounds equivalent to or better than previous work. Moreover, we give the ﬁrst proof that the algorithm of Collins et al. is a strong PAC learner, albeit within the ﬁltering setting. Our proofs demonstrate the algorithm’s strong theoretical properties for both classiﬁcation and conditional probability estimation, and we validate these results through extensive experiments. Empirically, our algorithm proves more robust to noise and overﬁtting than batch boosters in conditional probability estimation and proves competitive in classiﬁcation. 1</p><p>2 0.41973704 <a title="90-lda-2" href="./nips-2007-COFI_RANK_-_Maximum_Margin_Matrix_Factorization_for_Collaborative_Ranking.html">41 nips-2007-COFI RANK - Maximum Margin Matrix Factorization for Collaborative Ranking</a></p>
<p>Author: Markus Weimer, Alexandros Karatzoglou, Quoc V. Le, Alex J. Smola</p><p>Abstract: In this paper, we consider collaborative ﬁltering as a ranking problem. We present a method which uses Maximum Margin Matrix Factorization and optimizes ranking instead of rating. We employ structured output prediction to optimize directly for ranking scores. Experimental results show that our method gives very good ranking scores and scales well on collaborative ﬁltering tasks. 1</p><p>3 0.41440445 <a title="90-lda-3" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>Author: Yuhong Guo, Dale Schuurmans</p><p>Abstract: We investigate a new, convex relaxation of an expectation-maximization (EM) variant that approximates a standard objective while eliminating local minima. First, a cautionary result is presented, showing that any convex relaxation of EM over hidden variables must give trivial results if any dependence on the missing values is retained. Although this appears to be a strong negative outcome, we then demonstrate how the problem can be bypassed by using equivalence relations instead of value assignments over hidden variables. In particular, we develop new algorithms for estimating exponential conditional models that only require equivalence relation information over the variable values. This reformulation leads to an exact expression for EM variants in a wide range of problems. We then develop a semideﬁnite relaxation that yields global training by eliminating local minima. 1</p><p>4 0.41390002 <a title="90-lda-4" href="./nips-2007-What_makes_some_POMDP_problems_easy_to_approximate%3F.html">215 nips-2007-What makes some POMDP problems easy to approximate?</a></p>
<p>Author: Wee S. Lee, Nan Rong, Daniel J. Hsu</p><p>Abstract: Point-based algorithms have been surprisingly successful in computing approximately optimal solutions for partially observable Markov decision processes (POMDPs) in high dimensional belief spaces. In this work, we seek to understand the belief-space properties that allow some POMDP problems to be approximated efﬁciently and thus help to explain the point-based algorithms’ success often observed in the experiments. We show that an approximately optimal POMDP solution can be computed in time polynomial in the covering number of a reachable belief space, which is the subset of the belief space reachable from a given belief point. We also show that under the weaker condition of having a small covering number for an optimal reachable space, which is the subset of the belief space reachable under an optimal policy, computing an approximately optimal solution is NP-hard. However, given a suitable set of points that “cover” an optimal reachable space well, an approximate solution can be computed in polynomial time. The covering number highlights several interesting properties that reduce the complexity of POMDP planning in practice, e.g., fully observed state variables, beliefs with sparse support, smooth beliefs, and circulant state-transition matrices. 1</p><p>5 0.41022411 <a title="90-lda-5" href="./nips-2007-The_Price_of_Bandit_Information_for_Online_Optimization.html">199 nips-2007-The Price of Bandit Information for Online Optimization</a></p>
<p>Author: Varsha Dani, Sham M. Kakade, Thomas P. Hayes</p><p>Abstract: In the online linear optimization problem, a learner must choose, in each round, a decision from a set D ⊂ Rn in order to minimize an (unknown and changing) linear cost function. We present sharp rates of convergence (with respect to additive regret) for both the full information setting (where the cost function is revealed at the end of each round) and the bandit setting (where only the scalar cost incurred is revealed). In particular, this paper is concerned with the price of bandit information, by which we mean the ratio of the best achievable regret in the bandit setting to that in the full-information setting. For the full informa√ tion case, the upper bound on the regret is O∗ ( nT ), where n is the ambient dimension and T is the time horizon. For the bandit case, we present an algorithm √ which achieves O∗ (n3/2 T ) regret — all previous (nontrivial) bounds here were O(poly(n)T 2/3 ) or worse. It is striking that the convergence rate for the bandit setting is only a factor of n worse than in the full information case — in stark contrast to the K-arm bandit setting, where the gap in the dependence on K is √ √ exponential ( T K√ vs. T log K). We also present lower bounds showing that this gap is at least n, which we conjecture to be the correct order. The bandit algorithm we present can be implemented efﬁciently in special cases of particular interest, such as path planning and Markov Decision Problems. 1</p><p>6 0.40752119 <a title="90-lda-6" href="./nips-2007-Kernel_Measures_of_Conditional_Dependence.html">108 nips-2007-Kernel Measures of Conditional Dependence</a></p>
<p>7 0.40743339 <a title="90-lda-7" href="./nips-2007-Structured_Learning_with_Approximate_Inference.html">187 nips-2007-Structured Learning with Approximate Inference</a></p>
<p>8 0.40633622 <a title="90-lda-8" href="./nips-2007-Support_Vector_Machine_Classification_with_Indefinite_Kernels.html">190 nips-2007-Support Vector Machine Classification with Indefinite Kernels</a></p>
<p>9 0.4062891 <a title="90-lda-9" href="./nips-2007-Colored_Maximum_Variance_Unfolding.html">49 nips-2007-Colored Maximum Variance Unfolding</a></p>
<p>10 0.40535825 <a title="90-lda-10" href="./nips-2007-Boosting_Algorithms_for_Maximizing_the_Soft_Margin.html">38 nips-2007-Boosting Algorithms for Maximizing the Soft Margin</a></p>
<p>11 0.40519506 <a title="90-lda-11" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>12 0.40504879 <a title="90-lda-12" href="./nips-2007-Adaptive_Online_Gradient_Descent.html">21 nips-2007-Adaptive Online Gradient Descent</a></p>
<p>13 0.4048053 <a title="90-lda-13" href="./nips-2007-Regularized_Boost_for_Semi-Supervised_Learning.html">166 nips-2007-Regularized Boost for Semi-Supervised Learning</a></p>
<p>14 0.40387458 <a title="90-lda-14" href="./nips-2007-Markov_Chain_Monte_Carlo_with_People.html">125 nips-2007-Markov Chain Monte Carlo with People</a></p>
<p>15 0.40387288 <a title="90-lda-15" href="./nips-2007-Predictive_Matrix-Variate_t_Models.html">156 nips-2007-Predictive Matrix-Variate t Models</a></p>
<p>16 0.40342048 <a title="90-lda-16" href="./nips-2007-DIFFRAC%3A_a_discriminative_and_flexible_framework_for_clustering.html">65 nips-2007-DIFFRAC: a discriminative and flexible framework for clustering</a></p>
<p>17 0.40308264 <a title="90-lda-17" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>18 0.4030275 <a title="90-lda-18" href="./nips-2007-Efficient_Bayesian_Inference_for_Dynamically_Changing_Graphs.html">75 nips-2007-Efficient Bayesian Inference for Dynamically Changing Graphs</a></p>
<p>19 0.40298474 <a title="90-lda-19" href="./nips-2007-Fixing_Max-Product%3A_Convergent_Message_Passing_Algorithms_for_MAP_LP-Relaxations.html">92 nips-2007-Fixing Max-Product: Convergent Message Passing Algorithms for MAP LP-Relaxations</a></p>
<p>20 0.40264928 <a title="90-lda-20" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
