<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>168 nips-2007-Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-168" href="#">nips2007-168</a> knowledge-graph by maker-knowledge-mining</p><h1>168 nips-2007-Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods</h1>
<br/><p>Source: <a title="nips-2007-168-pdf" href="http://papers.nips.cc/paper/3318-reinforcement-learning-in-continuous-action-spaces-through-sequential-monte-carlo-methods.pdf">pdf</a></p><p>Author: Alessandro Lazaric, Marcello Restelli, Andrea Bonarini</p><p>Abstract: Learning in real-world domains often requires to deal with continuous state and action spaces. Although many solutions have been proposed to apply Reinforcement Learning algorithms to continuous state problems, the same techniques can be hardly extended to continuous action spaces, where, besides the computation of a good approximation of the value function, a fast method for the identiﬁcation of the highest-valued action is needed. In this paper, we propose a novel actor-critic approach in which the policy of the actor is estimated through sequential Monte Carlo methods. The importance sampling step is performed on the basis of the values learned by the critic, while the resampling step modiﬁes the actor’s policy. The proposed approach has been empirically compared to other learning algorithms into several domains; in this paper, we report results obtained in a control problem consisting of steering a boat across a river. 1</p><p>Reference: <a title="nips-2007-168-reference" href="../nips2007_reference/nips-2007-Reinforcement_Learning_in_Continuous_Action_Spaces_through_Sequential_Monte_Carlo_Methods_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sars', 0.417), ('policy', 0.366), ('resampl', 0.288), ('boat', 0.256), ('act', 0.215), ('st', 0.193), ('wi', 0.19), ('quay', 0.187), ('til', 0.184), ('continu', 0.158), ('smc', 0.149), ('ai', 0.143), ('reward', 0.118), ('reinforc', 0.116), ('ag', 0.114), ('util', 0.111), ('nef', 0.107), ('rl', 0.106), ('qt', 0.094), ('crit', 0.088)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="168-tfidf-1" href="./nips-2007-Reinforcement_Learning_in_Continuous_Action_Spaces_through_Sequential_Monte_Carlo_Methods.html">168 nips-2007-Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods</a></p>
<p>2 0.3921743 <a title="168-tfidf-2" href="./nips-2007-Incremental_Natural_Actor-Critic_Algorithms.html">102 nips-2007-Incremental Natural Actor-Critic Algorithms</a></p>
<p>3 0.25294173 <a title="168-tfidf-3" href="./nips-2007-Managing_Power_Consumption_and_Performance_of_Computing_Systems_Using_Reinforcement_Learning.html">124 nips-2007-Managing Power Consumption and Performance of Computing Systems Using Reinforcement Learning</a></p>
<p>4 0.24767552 <a title="168-tfidf-4" href="./nips-2007-Competition_Adds_Complexity.html">52 nips-2007-Competition Adds Complexity</a></p>
<p>5 0.24490581 <a title="168-tfidf-5" href="./nips-2007-A_Game-Theoretic_Approach_to_Apprenticeship_Learning.html">5 nips-2007-A Game-Theoretic Approach to Apprenticeship Learning</a></p>
<p>6 0.23582046 <a title="168-tfidf-6" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>7 0.22889499 <a title="168-tfidf-7" href="./nips-2007-Random_Sampling_of_States_in_Dynamic_Programming.html">162 nips-2007-Random Sampling of States in Dynamic Programming</a></p>
<p>8 0.21752569 <a title="168-tfidf-8" href="./nips-2007-Fitted_Q-iteration_in_continuous_action-space_MDPs.html">91 nips-2007-Fitted Q-iteration in continuous action-space MDPs</a></p>
<p>9 0.21652006 <a title="168-tfidf-9" href="./nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</a></p>
<p>10 0.1819407 <a title="168-tfidf-10" href="./nips-2007-Optimistic_Linear_Programming_gives_Logarithmic_Regret_for_Irreducible_MDPs.html">151 nips-2007-Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs</a></p>
<p>11 0.18053168 <a title="168-tfidf-11" href="./nips-2007-Temporal_Difference_Updating_without_a_Learning_Rate.html">191 nips-2007-Temporal Difference Updating without a Learning Rate</a></p>
<p>12 0.1601426 <a title="168-tfidf-12" href="./nips-2007-Stable_Dual_Dynamic_Programming.html">185 nips-2007-Stable Dual Dynamic Programming</a></p>
<p>13 0.13269077 <a title="168-tfidf-13" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<p>14 0.13267516 <a title="168-tfidf-14" href="./nips-2007-Hierarchical_Apprenticeship_Learning_with_Application_to_Quadruped_Locomotion.html">98 nips-2007-Hierarchical Apprenticeship Learning with Application to Quadruped Locomotion</a></p>
<p>15 0.12015983 <a title="168-tfidf-15" href="./nips-2007-What_makes_some_POMDP_problems_easy_to_approximate%3F.html">215 nips-2007-What makes some POMDP problems easy to approximate?</a></p>
<p>16 0.11827007 <a title="168-tfidf-16" href="./nips-2007-Hippocampal_Contributions_to_Control%3A_The_Third_Way.html">100 nips-2007-Hippocampal Contributions to Control: The Third Way</a></p>
<p>17 0.1113162 <a title="168-tfidf-17" href="./nips-2007-The_rat_as_particle_filter.html">203 nips-2007-The rat as particle filter</a></p>
<p>18 0.10487518 <a title="168-tfidf-18" href="./nips-2007-Bayes-Adaptive_POMDPs.html">30 nips-2007-Bayes-Adaptive POMDPs</a></p>
<p>19 0.09733066 <a title="168-tfidf-19" href="./nips-2007-Scan_Strategies_for_Meteorological_Radars.html">171 nips-2007-Scan Strategies for Meteorological Radars</a></p>
<p>20 0.092233047 <a title="168-tfidf-20" href="./nips-2007-Theoretical_Analysis_of_Heuristic_Search_Methods_for_Online_POMDPs.html">204 nips-2007-Theoretical Analysis of Heuristic Search Methods for Online POMDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.267), (1, -0.138), (2, -0.439), (3, -0.123), (4, 0.229), (5, 0.032), (6, 0.02), (7, -0.016), (8, 0.043), (9, -0.048), (10, -0.039), (11, -0.027), (12, 0.077), (13, 0.013), (14, 0.036), (15, 0.034), (16, 0.016), (17, -0.006), (18, -0.048), (19, 0.093), (20, 0.014), (21, -0.021), (22, 0.047), (23, 0.007), (24, -0.071), (25, -0.081), (26, 0.014), (27, 0.057), (28, -0.012), (29, 0.042), (30, 0.035), (31, -0.048), (32, -0.032), (33, 0.044), (34, -0.023), (35, -0.09), (36, 0.008), (37, 0.02), (38, 0.06), (39, -0.032), (40, -0.068), (41, 0.017), (42, -0.055), (43, -0.037), (44, 0.029), (45, -0.078), (46, -0.011), (47, -0.012), (48, -0.006), (49, 0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93841749 <a title="168-lsi-1" href="./nips-2007-Reinforcement_Learning_in_Continuous_Action_Spaces_through_Sequential_Monte_Carlo_Methods.html">168 nips-2007-Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods</a></p>
<p>2 0.87014514 <a title="168-lsi-2" href="./nips-2007-Incremental_Natural_Actor-Critic_Algorithms.html">102 nips-2007-Incremental Natural Actor-Critic Algorithms</a></p>
<p>3 0.79411495 <a title="168-lsi-3" href="./nips-2007-Competition_Adds_Complexity.html">52 nips-2007-Competition Adds Complexity</a></p>
<p>4 0.79147583 <a title="168-lsi-4" href="./nips-2007-Managing_Power_Consumption_and_Performance_of_Computing_Systems_Using_Reinforcement_Learning.html">124 nips-2007-Managing Power Consumption and Performance of Computing Systems Using Reinforcement Learning</a></p>
<p>5 0.78005266 <a title="168-lsi-5" href="./nips-2007-Temporal_Difference_Updating_without_a_Learning_Rate.html">191 nips-2007-Temporal Difference Updating without a Learning Rate</a></p>
<p>6 0.74098849 <a title="168-lsi-6" href="./nips-2007-A_Game-Theoretic_Approach_to_Apprenticeship_Learning.html">5 nips-2007-A Game-Theoretic Approach to Apprenticeship Learning</a></p>
<p>7 0.70850945 <a title="168-lsi-7" href="./nips-2007-Fitted_Q-iteration_in_continuous_action-space_MDPs.html">91 nips-2007-Fitted Q-iteration in continuous action-space MDPs</a></p>
<p>8 0.68054235 <a title="168-lsi-8" href="./nips-2007-Random_Sampling_of_States_in_Dynamic_Programming.html">162 nips-2007-Random Sampling of States in Dynamic Programming</a></p>
<p>9 0.67247093 <a title="168-lsi-9" href="./nips-2007-Optimistic_Linear_Programming_gives_Logarithmic_Regret_for_Irreducible_MDPs.html">151 nips-2007-Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs</a></p>
<p>10 0.62902182 <a title="168-lsi-10" href="./nips-2007-Stable_Dual_Dynamic_Programming.html">185 nips-2007-Stable Dual Dynamic Programming</a></p>
<p>11 0.6115008 <a title="168-lsi-11" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>12 0.60021645 <a title="168-lsi-12" href="./nips-2007-Hierarchical_Apprenticeship_Learning_with_Application_to_Quadruped_Locomotion.html">98 nips-2007-Hierarchical Apprenticeship Learning with Application to Quadruped Locomotion</a></p>
<p>13 0.56507486 <a title="168-lsi-13" href="./nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</a></p>
<p>14 0.52155745 <a title="168-lsi-14" href="./nips-2007-Receding_Horizon_Differential_Dynamic_Programming.html">163 nips-2007-Receding Horizon Differential Dynamic Programming</a></p>
<p>15 0.51841438 <a title="168-lsi-15" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<p>16 0.41463831 <a title="168-lsi-16" href="./nips-2007-Hippocampal_Contributions_to_Control%3A_The_Third_Way.html">100 nips-2007-Hippocampal Contributions to Control: The Third Way</a></p>
<p>17 0.40862042 <a title="168-lsi-17" href="./nips-2007-Scan_Strategies_for_Meteorological_Radars.html">171 nips-2007-Scan Strategies for Meteorological Radars</a></p>
<p>18 0.36923003 <a title="168-lsi-18" href="./nips-2007-Measuring_Neural_Synchrony_by_Message_Passing.html">127 nips-2007-Measuring Neural Synchrony by Message Passing</a></p>
<p>19 0.34773037 <a title="168-lsi-19" href="./nips-2007-Simulated_Annealing%3A_Rigorous_finite-time_guarantees_for_optimization_on_continuous_domains.html">178 nips-2007-Simulated Annealing: Rigorous finite-time guarantees for optimization on continuous domains</a></p>
<p>20 0.34627143 <a title="168-lsi-20" href="./nips-2007-Bayes-Adaptive_POMDPs.html">30 nips-2007-Bayes-Adaptive POMDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.104), (30, 0.039), (36, 0.126), (45, 0.072), (46, 0.083), (52, 0.02), (53, 0.027), (56, 0.015), (60, 0.168), (73, 0.211), (90, 0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77750504 <a title="168-lda-1" href="./nips-2007-Reinforcement_Learning_in_Continuous_Action_Spaces_through_Sequential_Monte_Carlo_Methods.html">168 nips-2007-Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods</a></p>
<p>2 0.77691627 <a title="168-lda-2" href="./nips-2007-Modelling_motion_primitives_and_their_timing_in_biologically_executed_movements.html">133 nips-2007-Modelling motion primitives and their timing in biologically executed movements</a></p>
<p>3 0.76516646 <a title="168-lda-3" href="./nips-2007-Discriminative_K-means_for_Clustering.html">70 nips-2007-Discriminative K-means for Clustering</a></p>
<p>4 0.74994594 <a title="168-lda-4" href="./nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</a></p>
<p>5 0.7416116 <a title="168-lda-5" href="./nips-2007-Incremental_Natural_Actor-Critic_Algorithms.html">102 nips-2007-Incremental Natural Actor-Critic Algorithms</a></p>
<p>6 0.73846877 <a title="168-lda-6" href="./nips-2007-A_Game-Theoretic_Approach_to_Apprenticeship_Learning.html">5 nips-2007-A Game-Theoretic Approach to Apprenticeship Learning</a></p>
<p>7 0.72555524 <a title="168-lda-7" href="./nips-2007-Modeling_Natural_Sounds_with_Modulation_Cascade_Processes.html">130 nips-2007-Modeling Natural Sounds with Modulation Cascade Processes</a></p>
<p>8 0.69999075 <a title="168-lda-8" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>9 0.69955158 <a title="168-lda-9" href="./nips-2007-Hierarchical_Apprenticeship_Learning_with_Application_to_Quadruped_Locomotion.html">98 nips-2007-Hierarchical Apprenticeship Learning with Application to Quadruped Locomotion</a></p>
<p>10 0.69622183 <a title="168-lda-10" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<p>11 0.69193107 <a title="168-lda-11" href="./nips-2007-Fitted_Q-iteration_in_continuous_action-space_MDPs.html">91 nips-2007-Fitted Q-iteration in continuous action-space MDPs</a></p>
<p>12 0.69118172 <a title="168-lda-12" href="./nips-2007-Managing_Power_Consumption_and_Performance_of_Computing_Systems_Using_Reinforcement_Learning.html">124 nips-2007-Managing Power Consumption and Performance of Computing Systems Using Reinforcement Learning</a></p>
<p>13 0.68496722 <a title="168-lda-13" href="./nips-2007-What_makes_some_POMDP_problems_easy_to_approximate%3F.html">215 nips-2007-What makes some POMDP problems easy to approximate?</a></p>
<p>14 0.68429381 <a title="168-lda-14" href="./nips-2007-Stable_Dual_Dynamic_Programming.html">185 nips-2007-Stable Dual Dynamic Programming</a></p>
<p>15 0.68375647 <a title="168-lda-15" href="./nips-2007-Theoretical_Analysis_of_Heuristic_Search_Methods_for_Online_POMDPs.html">204 nips-2007-Theoretical Analysis of Heuristic Search Methods for Online POMDPs</a></p>
<p>16 0.68296498 <a title="168-lda-16" href="./nips-2007-Bayes-Adaptive_POMDPs.html">30 nips-2007-Bayes-Adaptive POMDPs</a></p>
<p>17 0.68214768 <a title="168-lda-17" href="./nips-2007-Optimistic_Linear_Programming_gives_Logarithmic_Regret_for_Irreducible_MDPs.html">151 nips-2007-Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs</a></p>
<p>18 0.67822254 <a title="168-lda-18" href="./nips-2007-Learning_the_structure_of_manifolds_using_random_projections.html">116 nips-2007-Learning the structure of manifolds using random projections</a></p>
<p>19 0.67713225 <a title="168-lda-19" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>20 0.67692333 <a title="168-lda-20" href="./nips-2007-Simulated_Annealing%3A_Rigorous_finite-time_guarantees_for_optimization_on_continuous_domains.html">178 nips-2007-Simulated Annealing: Rigorous finite-time guarantees for optimization on continuous domains</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
