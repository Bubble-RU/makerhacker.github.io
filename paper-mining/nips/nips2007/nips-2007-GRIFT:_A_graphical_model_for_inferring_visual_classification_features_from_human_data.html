<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-93" href="#">nips2007-93</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</h1>
<br/><p>Source: <a title="nips-2007-93-pdf" href="http://papers.nips.cc/paper/3289-grift-a-graphical-model-for-inferring-visual-classification-features-from-human-data.pdf">pdf</a></p><p>Author: Michael Ross, Andrew Cohen</p><p>Abstract: This paper describes a new model for human visual classiﬁcation that enables the recovery of image features that explain human subjects’ performance on different visual classiﬁcation tasks. Unlike previous methods, this algorithm does not model their performance with a single linear classiﬁer operating on raw image pixels. Instead, it represents classiﬁcation as the combination of multiple feature detectors. This approach extracts more information about human visual classiﬁcation than previous methods and provides a foundation for further exploration. 1</p><p>Reference: <a title="nips-2007-93-reference" href="../nips2007_reference/nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 GRIFT: A graphical model for inferring visual classiﬁcation features from human data Michael G. [sent-1, score-0.245]
</p><p>2 edu  Abstract This paper describes a new model for human visual classiﬁcation that enables the recovery of image features that explain human subjects’ performance on different visual classiﬁcation tasks. [sent-6, score-0.526]
</p><p>3 This approach extracts more information about human visual classiﬁcation than previous methods and provides a foundation for further exploration. [sent-9, score-0.159]
</p><p>4 1  Introduction  Although a great deal is known about the low-level features computed by the human visual system, determining the information used to make high-level visual classiﬁcations is an active area of research. [sent-10, score-0.309]
</p><p>5 Instead of representing human visual discrimination as a single linear classiﬁer, GRIFT models it as the non-linear combination of multiple independently detected features. [sent-14, score-0.159]
</p><p>6 This paper describes GRIFT and the algorithms for ﬁtting it to data, demonstrates the model’s efﬁcacy on simulated and human data, and concludes with a discussion of future research directions. [sent-16, score-0.178]
</p><p>7 2  Related work  Ahumada’s classiﬁcation image algorithm [1] models an observer’s classiﬁcations of visual stimuli with a noisy linear classiﬁer — a ﬁxed set of weights and a normally distributed threshold. [sent-17, score-0.213]
</p><p>8 In a typical classiﬁcation image experiment, participants are presented with hundreds or thousands of noise-corrupted examples from two categories and asked to classify each one. [sent-19, score-0.321]
</p><p>9 GRIFT takes the next step and infers features which predict human performance directly from classiﬁcation data. [sent-28, score-0.164]
</p><p>10 [4] demonstrates that Gaussian mixture models can be used to recover features from human classiﬁcation data without specifying a ﬁxed set of possible features. [sent-31, score-0.188]
</p><p>11 Each feature detector is binary valued (1 indicates detection), as is the classiﬁcation, C (1 indicates one class and 2 the other). [sent-39, score-0.315]
</p><p>12 The stimulus only inﬂuences C through the feature detectors, therefore the joint probability of a stimulus and classiﬁcation pair is N  P (Fi |S) . [sent-41, score-0.228]
</p><p>13 The boxed region in the ﬁgure indicates the parts of the model that are replicated when N > 1 — each feature detector is represented by an independent copy of those variables and parameters. [sent-44, score-0.315]
</p><p>14 The conditional distribution of each feature detector’s value, P (Fi |S), is modeled with a logistic regression function on the pixel values of S. [sent-47, score-0.146]
</p><p>15 Humans can successfully classify images in the presence of extremely high additive noise, which suggests the use of averaging and contrast, linear computations which 2  are known to play important roles in human visual perception [9]. [sent-49, score-0.256]
</p><p>16 Just as the classiﬁcation image used a random threshold to represent uncertainty in the output of its single linear classiﬁer, logistic regression also allows GRIFT to represent uncertainty in the output of each of its feature detectors. [sent-50, score-0.206]
</p><p>17 A GRIFT model with N features applied to the classiﬁcation of images each containing |S| pixels has N (|S| + 2) + 1 parameters. [sent-58, score-0.151]
</p><p>18 2 2 2 2π This prior reﬂects the assumption that each feature detector should have a signiﬁcant impact on the classiﬁcation, but no single detector should make it deterministic — a single-feature model with λ0 = 0 and λ1 = −2 has an 88% chance of choosing class 1 if the feature is active. [sent-63, score-0.654]
</p><p>19 As mentioned previously, human visual processing is sensitive to contrasts between image regions. [sent-67, score-0.25]
</p><p>20 If one image region is assigned positive ωij s and another is assigned negative ωij s, the feature detector will be sensitive to the contrast between them. [sent-68, score-0.391]
</p><p>21 P (C, F, S, ω, β, λ) = P (C|F, λ)P (S)P (λ0 )  (1)  i=1  4  GRIFT algorithm  The goal of the algorithm is to ﬁnd the parameters that satisfy the prior distributions and best account for the (S, C) samples gathered from a human subject. [sent-74, score-0.175]
</p><p>22 The 3  algorithm is derived using the expectation-maximization (EM) method [3], a widely used optimization technique for dealing with unobserved variables, in this case F, the feature detector outputs for all the trials. [sent-76, score-0.315]
</p><p>23 In order to determine the most probable parameter assignments, the algorithm chooses random initial parameters θ∗ = (ω ∗ , β ∗ , λ∗ ) and then ﬁnds the θ that maximizes Q(θ|θ∗ ) =  P (F|S, C, θ∗ ) log P (C, F, S|θ) + log P (θ). [sent-77, score-0.148]
</p><p>24 F  ∗  Q(θ|θ ) is the expected log posterior probability of the parameters computed by using the current θ∗ to estimate the distribution of F, the unobserved feature detector activations. [sent-78, score-0.367]
</p><p>25 In each experiment, human participants classiﬁed stimuli into two classes. [sent-93, score-0.351]
</p><p>26 In each trial, the participant saw a stimulus (a sample from S) that consisted of a randomly chosen target with high levels of independent identically distributed noise added to each pixel. [sent-95, score-0.236]
</p><p>27 The noise samples were drawn from a truncated normal distribution to ensure that the stimulus pixel values remained within the display’s output range. [sent-96, score-0.185]
</p><p>28 Figure 1 shows the classes and targets from each experiment and a sample stimulus from each class. [sent-97, score-0.203]
</p><p>29 In the four-square experiment four participants were asked to distinguish between two artiﬁcial stimulus classes, one in which there were bright squares in the upper-left or upper-right corners and one in which there were bright squares in the lower-left or lower-right corners. [sent-98, score-0.632]
</p><p>30 In the light-dark experiment three participants were asked to distinguish between three strips that each had two light blobs and three strips that each had only one light blob. [sent-99, score-0.348]
</p><p>31 Finally, in the faces experiment three participants were asked to distinguish between two faces. [sent-100, score-0.406]
</p><p>32 To maintain their interest in the task, participants were given auditory feedback after each trial that indicated success or failure. [sent-104, score-0.191]
</p><p>33 05 1  2  3  4  N + Figure 2: The most probable ω parameters found for the four-square experiments for different values 4 of N and the mutual information between these feature detectors and the observed classiﬁcations. [sent-114, score-0.539]
</p><p>34 3 Fitting GRIFT models is not especially sensitive to the random initialization procedure used to start 4 ∗  2 3  four square: most probable ωi values N=2 N=3 N=4 mutual information  simulations humans  2  top v. [sent-115, score-0.328]
</p><p>35 The λ parameters were initialized by normal random samples and then half  8 9 10 3 were negated so the features would tend to start evenly assigned to the two classes, except for λ∗ , 4 0 3 which was initialized to 0. [sent-117, score-0.198]
</p><p>36 In the four-square experiments, the ω∗ parameters were initialized by 4  a mixture of normal distributions and in the light-dark experiments they were initialized from a uniform distribution. [sent-118, score-0.134]
</p><p>37 In the faces experiments the ω ∗ were initialized by adding normal noise to the 8 9 10 optimal linear classiﬁer separating the two targets. [sent-119, score-0.248]
</p><p>38 Because of the large number of pixels in the faces stimuli, the other initialization procedures frequently produced initial assignments with extremely low probabilities, which led to numerical precision problems. [sent-120, score-0.265]
</p><p>39 This performance level is high enough to keep the participants engaged in the task, but allows for sufﬁcient noise to explore their responses in a large volume of the stimulus space. [sent-125, score-0.342]
</p><p>40 The corners observer used four feature detectors, one for each bright corner, whereas the top-v. [sent-129, score-0.372]
</p><p>41 -bottom observer contrasted the brightness of the top and bottom pixels. [sent-130, score-0.231]
</p><p>42 The result of using GRIFT to recover the feature detectors are displayed in Figure 2. [sent-131, score-0.284]
</p><p>43 Dark pixels indicate negative weights and bright pixels correspond to positive weights. [sent-133, score-0.232]
</p><p>44 The presence of dark and light regions in a feature detector indicates the computation of contrasts between those areas. [sent-134, score-0.367]
</p><p>45 The sign of the weights is not signiﬁcant — given a ﬁxed number of features, there are typically several equivalent sets of feature detectors that only differ from each other in the signs of their ω terms and in the associated λ and β values. [sent-135, score-0.308]
</p><p>46 Because the optimal number of features for human subjects is unknown, GRIFT models with 1–4 features were ﬁt to the data from each subject. [sent-136, score-0.262]
</p><p>47 Instead, after recovering the parameters, we estimated the mutual information between the unobserved F variables and the observed classiﬁcations C. [sent-139, score-0.213]
</p><p>48 Mutual information measures how well the feature detector outputs can 5  predict the subject’s classiﬁcation decision. [sent-140, score-0.293]
</p><p>49 Unlike the log likelihood of the observations, which is dependent on the choice to model C with a logistic regression function, mutual information does not assume a particular relationship between F and C and does not necessarily increase with N . [sent-141, score-0.266]
</p><p>50 Plotting the mutual information as N increases can indicate if new detectors are making a substantial contribution or are overﬁtting the data. [sent-142, score-0.391]
</p><p>51 On the simulated observers’ data, for which the true values of N were known, mutual information was a more accurate model selection indicator than traditional statistics such as the Bayesian or Akaike information criteria [3]. [sent-143, score-0.208]
</p><p>52 Fitting GRIFT to the simulated observers demonstrated that if the model is accurate, the correct features can be recovered reliably. [sent-144, score-0.204]
</p><p>53 -bottom observer showed no substantial increase in mutual information as the number of features increased from 1 to 4. [sent-146, score-0.368]
</p><p>54 Each set of recovered feature detectors included a top-bottom contrast detector and other detectors with noisy ωi s that did not contribute much to predicting C. [sent-147, score-0.779]
</p><p>55 Although the observer truly used two detectors, one top-brighter detector and one bottom-brighter detector, the recovery of only one top-bottom contrast detector is a success because one contrast detector plus a suitable λ0 term is logically equivalent to the original two-feature model. [sent-148, score-0.821]
</p><p>56 The corners observer showed a substantial increase in mutual information as N increased from 1 to 4 and the ω values clearly indicate four corner-sensitive feature detectors. [sent-149, score-0.507]
</p><p>57 The corners data was also tested with a ﬁve-feature GRIFT model (ω not shown) which produced four corner detectors and one feature with noisy ωi . [sent-150, score-0.511]
</p><p>58 Its gain in mutual information was smaller than that observed on any of the previous steps. [sent-151, score-0.155]
</p><p>59 Note the corner areas in the ωi s recovered from the corners data are sometimes black and sometimes white. [sent-152, score-0.197]
</p><p>60 Recall that these are not image pixel values that the detectors are attempting to match, but positive and negative weights indicating that the brightness in the corner region is being contrasted to the brightness of the rest of the image. [sent-153, score-0.638]
</p><p>61 Even though targets consisted of four bright-corner stimuli, recovering the parameters from the topv. [sent-154, score-0.176]
</p><p>62 -bottom observer never produced ω values indicating corner-speciﬁc feature detectors. [sent-155, score-0.191]
</p><p>63 The simulations demonstrate that the recovered detectors are determined by the classiﬁcation strategy, not by the structure of the targets and classes. [sent-157, score-0.332]
</p><p>64 The data of the four human participants revealed some interesting differences. [sent-158, score-0.323]
</p><p>65 EA’s data indicated no consistent pattern of mutual information increase after two features, and the twofeature model appears to contain two top-bottom contrast detectors. [sent-161, score-0.233]
</p><p>66 At the other extreme is participant JG, whose data shows four very clear corner detectors and a steady increase in mutual information up to four features. [sent-163, score-0.597]
</p><p>67 Therefore, it seems very likely that this participant was matching corners and probably should be tested with a ﬁve-feature model to gain additional insight. [sent-164, score-0.201]
</p><p>68 AC and RS’s data suggest three corner detectors and a top-bottom contrast detector. [sent-165, score-0.292]
</p><p>69 In the light-dark and faces experiments, stair-casing was used the adjust the noise level to the 71% performance level at the beginning of each session and then the noise level was ﬁxed for the remaining trials to improve the independence of the samples. [sent-169, score-0.331]
</p><p>70 Because the noise levels were ﬁxed after the ﬁrst 101 trials, a participant with good luck at the end of that period could experience very high noise levels for the remainder of the experiment, leading to poor performance. [sent-173, score-0.236]
</p><p>71 All three participants appear to have used different classiﬁcation methods, providing a very informative contrast. [sent-174, score-0.191]
</p><p>72 The ﬂat mutual information graph and the presence of a feature detector thresholding the overall brightness for each value of N indicate that P1 pursued a one-feature, linear-classiﬁer strategy. [sent-176, score-0.579]
</p><p>73 For N = 1 and N = 2, the most interpretable feature detector is an overall brightness detector, which disappears when N = 3 and the best ﬁt model consists of three detectors looking for speciﬁc patterns, one for each position a  6  a b light-dark: most probable ωi values P1 c P2 a P3 N=1 0. [sent-178, score-0.702]
</p><p>74 6 1  P6  2  3  N=5  2  4  3  2  mutual information  3  3  0. [sent-190, score-0.155]
</p><p>75 01 0 1  2  3  N N 3 4 + Figure 3: The most probable ω parameters found for the light-dark and faces experiments for differ0. [sent-202, score-0.23]
</p><p>76 2 ent N and the mutual information between these feature detectors and the observed classiﬁcations. [sent-204, score-0.439]
</p><p>77 1  7  8  9  10  0 bright or dark spot 3can appear. [sent-206, score-0.152]
</p><p>78 Then 7when N 9 = 410the overall brightness detector reappears, added to 1 2 4 5 6 8 the three spot detectors. [sent-207, score-0.367]
</p><p>79 Apparently the spot detectors are only effective if they are all present. [sent-208, score-0.249]
</p><p>80 With only three available detectors, the overall brightness detector is excluded, but the optimal assignment includes all four detectors. [sent-209, score-0.356]
</p><p>81 This is the best-ﬁt model because increasing to N = 5 keeps the mutual information constant and adds a detector that is active for every stimulus. [sent-210, score-0.419]
</p><p>82 Always active detectors function as constant additions to λ0 , therefore this is equivalent to the N = 4 solution. [sent-211, score-0.233]
</p><p>83 The GRIFT models of participant P3 do not show a substantial increase in mutual information as the number of features rises. [sent-212, score-0.363]
</p><p>84 The clear distinction between the results for all three subjects demonstrates the effectiveness of GRIFT and the mutual information measure in distinguishing between classiﬁcation strategies. [sent-214, score-0.236]
</p><p>85 The targets were two unﬁltered faces from Gold et al. [sent-216, score-0.21]
</p><p>86 After the experiment, the stimuli were down-sampled further to 32x32 and the background surrounding the faces was removed by cropping, reducing the stimuli to 26x17. [sent-218, score-0.25]
</p><p>87 The results for three participants (P4, P5, and P6) are in Figure 3. [sent-220, score-0.191]
</p><p>88 Never active features cannot affect the classiﬁcation, and, as explained previously, always active features are also superﬂuous. [sent-223, score-0.182]
</p><p>89 Perhaps the noise level was too high and P5 was guessing rather than using image information much of the time. [sent-227, score-0.182]
</p><p>90 Participant P6’s data did produce a two-feature GRIFT model, albeit one that is difﬁcult to interpret and which only caused a small rise in mutual information. [sent-228, score-0.155]
</p><p>91 Instead of recovering independent part detectors, such as a nose detector and an eye detector, GRIFT extracted two subtly different holistic feature detectors. [sent-229, score-0.385]
</p><p>92 Given P6’s poor performance (58% accuracy) these features may, like P5’s results, be indicative of a guessing strategy that was not strongly inﬂuenced by the image information. [sent-230, score-0.17]
</p><p>93 The results on faces support the hypothesis that face classiﬁcation is holistic and conﬁgural, rather than the result of part classiﬁcations, especially when individual feature detection is difﬁcult [11]. [sent-231, score-0.261]
</p><p>94 6  Conclusion  This paper has described the GRIFT model for determining the features used in human image classiﬁcation. [sent-237, score-0.256]
</p><p>95 It provides a probabilistic model of human visual classiﬁcation that accounts for data and incorporates prior beliefs about the features. [sent-239, score-0.205]
</p><p>96 The feature detectors it ﬁnds are associated with the classiﬁcation strategy employed by the observer and are not the result of structure in the classes’ target images. [sent-240, score-0.375]
</p><p>97 GRIFT’s value has been demonstrated by modeling the performance of humans on the four-square, light-dark, and faces classiﬁcation tasks and by successfully recovering the parameters of computer simulated observers in the four-square task. [sent-241, score-0.311]
</p><p>98 Its inability to ﬁnd multiple local features when analyzing human performance on the faces task agrees with previous results. [sent-242, score-0.294]
</p><p>99 New resolution-independent feature parameterizations can be introduced, as can transformation parameters to make the features translationally and rotationally invariant. [sent-246, score-0.17]
</p><p>100 Identiﬁcation of band-pass ﬁltered letters and faces by human and ideal observers. [sent-296, score-0.23]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('grift', 0.753), ('detector', 0.215), ('detectors', 0.206), ('participants', 0.191), ('mutual', 0.155), ('classi', 0.152), ('faces', 0.13), ('brightness', 0.109), ('human', 0.1), ('corners', 0.093), ('observer', 0.091), ('participant', 0.086), ('targets', 0.08), ('bright', 0.078), ('feature', 0.078), ('stimulus', 0.075), ('probable', 0.072), ('image', 0.07), ('ea', 0.067), ('pixels', 0.065), ('features', 0.064), ('cation', 0.062), ('stimuli', 0.06), ('gold', 0.06), ('visual', 0.059), ('fi', 0.059), ('corner', 0.058), ('fitting', 0.054), ('cohen', 0.053), ('noise', 0.051), ('jg', 0.047), ('recovered', 0.046), ('er', 0.045), ('humans', 0.045), ('spot', 0.043), ('observers', 0.041), ('ij', 0.039), ('initialized', 0.039), ('logistic', 0.037), ('asked', 0.036), ('recovering', 0.036), ('gural', 0.036), ('onefeature', 0.036), ('pelli', 0.036), ('strips', 0.036), ('guessing', 0.036), ('em', 0.036), ('ac', 0.034), ('encourages', 0.034), ('subjects', 0.034), ('rs', 0.033), ('four', 0.032), ('simulated', 0.031), ('ahumada', 0.031), ('contrasted', 0.031), ('pixel', 0.031), ('dark', 0.031), ('tting', 0.03), ('substantial', 0.03), ('cations', 0.03), ('recovery', 0.029), ('ross', 0.029), ('templates', 0.029), ('holistic', 0.029), ('increase', 0.028), ('contrast', 0.028), ('parameters', 0.028), ('normal', 0.028), ('active', 0.027), ('additive', 0.027), ('nose', 0.027), ('experiment', 0.027), ('level', 0.025), ('amherst', 0.025), ('demonstrates', 0.024), ('trials', 0.024), ('detection', 0.024), ('prior', 0.024), ('log', 0.024), ('extremely', 0.024), ('levels', 0.024), ('initialization', 0.024), ('classify', 0.024), ('weights', 0.024), ('describes', 0.023), ('distinguishing', 0.023), ('gathered', 0.023), ('produced', 0.022), ('presence', 0.022), ('exp', 0.022), ('distinguish', 0.022), ('unobserved', 0.022), ('class', 0.022), ('model', 0.022), ('encourage', 0.021), ('contrasts', 0.021), ('near', 0.021), ('classes', 0.021), ('threshold', 0.021), ('vision', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="93-tfidf-1" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>Author: Michael Ross, Andrew Cohen</p><p>Abstract: This paper describes a new model for human visual classiﬁcation that enables the recovery of image features that explain human subjects’ performance on different visual classiﬁcation tasks. Unlike previous methods, this algorithm does not model their performance with a single linear classiﬁer operating on raw image pixels. Instead, it represents classiﬁcation as the combination of multiple feature detectors. This approach extracts more information about human visual classiﬁcation than previous methods and provides a foundation for further exploration. 1</p><p>2 0.10681415 <a title="93-tfidf-2" href="./nips-2007-Multiple-Instance_Pruning_For_Learning_Efficient_Cascade_Detectors.html">137 nips-2007-Multiple-Instance Pruning For Learning Efficient Cascade Detectors</a></p>
<p>Author: Cha Zhang, Paul A. Viola</p><p>Abstract: Cascade detectors have been shown to operate extremely rapidly, with high accuracy, and have important applications such as face detection. Driven by this success, cascade learning has been an area of active research in recent years. Nevertheless, there are still challenging technical problems during the training process of cascade detectors. In particular, determining the optimal target detection rate for each stage of the cascade remains an unsolved issue. In this paper, we propose the multiple instance pruning (MIP) algorithm for soft cascades. This algorithm computes a set of thresholds which aggressively terminate computation with no reduction in detection rate or increase in false positive rate on the training dataset. The algorithm is based on two key insights: i) examples that are destined to be rejected by the complete classiﬁer can be safely pruned early; ii) face detection is a multiple instance learning problem. The MIP process is fully automatic and requires no assumptions of probability distributions, statistical independence, or ad hoc intermediate rejection targets. Experimental results on the MIT+CMU dataset demonstrate signiﬁcant performance advantages. 1</p><p>3 0.10407262 <a title="93-tfidf-3" href="./nips-2007-A_Bayesian_Model_of_Conditioned_Perception.html">3 nips-2007-A Bayesian Model of Conditioned Perception</a></p>
<p>Author: Alan Stocker, Eero P. Simoncelli</p><p>Abstract: unkown-abstract</p><p>4 0.10039869 <a title="93-tfidf-4" href="./nips-2007-Predicting_human_gaze_using_low-level_saliency_combined_with_face_detection.html">155 nips-2007-Predicting human gaze using low-level saliency combined with face detection</a></p>
<p>Author: Moran Cerf, Jonathan Harel, Wolfgang Einhaeuser, Christof Koch</p><p>Abstract: Under natural viewing conditions, human observers shift their gaze to allocate processing resources to subsets of the visual input. Many computational models try to predict such voluntary eye and attentional shifts. Although the important role of high level stimulus properties (e.g., semantic information) in search stands undisputed, most models are based on low-level image properties. We here demonstrate that a combined model of face detection and low-level saliency signiﬁcantly outperforms a low-level model in predicting locations humans ﬁxate on, based on eye-movement recordings of humans observing photographs of natural scenes, most of which contained at least one person. Observers, even when not instructed to look for anything particular, ﬁxate on a face with a probability of over 80% within their ﬁrst two ﬁxations; furthermore, they exhibit more similar scanpaths when faces are present. Remarkably, our model’s predictive performance in images that do not contain faces is not impaired, and is even improved in some cases by spurious face detector responses. 1</p><p>5 0.096829869 <a title="93-tfidf-5" href="./nips-2007-Configuration_Estimates_Improve_Pedestrian_Finding.html">56 nips-2007-Configuration Estimates Improve Pedestrian Finding</a></p>
<p>Author: Duan Tran, David A. Forsyth</p><p>Abstract: Fair discriminative pedestrian ﬁnders are now available. In fact, these pedestrian ﬁnders make most errors on pedestrians in conﬁgurations that are uncommon in the training data, for example, mounting a bicycle. This is undesirable. However, the human conﬁguration can itself be estimated discriminatively using structure learning. We demonstrate a pedestrian ﬁnder which ﬁrst ﬁnds the most likely human pose in the window using a discriminative procedure trained with structure learning on a small dataset. We then present features (local histogram of oriented gradient and local PCA of gradient) based on that conﬁguration to an SVM classiﬁer. We show, using the INRIA Person dataset, that estimates of conﬁguration signiﬁcantly improve the accuracy of a discriminative pedestrian ﬁnder. 1</p><p>6 0.075205989 <a title="93-tfidf-6" href="./nips-2007-The_Noisy-Logical_Distribution_and_its_Application_to_Causal_Inference.html">198 nips-2007-The Noisy-Logical Distribution and its Application to Causal Inference</a></p>
<p>7 0.069278941 <a title="93-tfidf-7" href="./nips-2007-Object_Recognition_by_Scene_Alignment.html">143 nips-2007-Object Recognition by Scene Alignment</a></p>
<p>8 0.064417101 <a title="93-tfidf-8" href="./nips-2007-Learning_the_2-D_Topology_of_Images.html">115 nips-2007-Learning the 2-D Topology of Images</a></p>
<p>9 0.064231925 <a title="93-tfidf-9" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>10 0.060107544 <a title="93-tfidf-10" href="./nips-2007-Predicting_Brain_States_from_fMRI_Data%3A_Incremental_Functional_Principal_Component_Regression.html">154 nips-2007-Predicting Brain States from fMRI Data: Incremental Functional Principal Component Regression</a></p>
<p>11 0.057853375 <a title="93-tfidf-11" href="./nips-2007-Markov_Chain_Monte_Carlo_with_People.html">125 nips-2007-Markov Chain Monte Carlo with People</a></p>
<p>12 0.056162868 <a title="93-tfidf-12" href="./nips-2007-Locality_and_low-dimensions_in_the_prediction_of_natural_experience_from_fMRI.html">122 nips-2007-Locality and low-dimensions in the prediction of natural experience from fMRI</a></p>
<p>13 0.054876588 <a title="93-tfidf-13" href="./nips-2007-Kernels_on_Attributed_Pointsets_with_Applications.html">109 nips-2007-Kernels on Attributed Pointsets with Applications</a></p>
<p>14 0.052239127 <a title="93-tfidf-14" href="./nips-2007-Congruence_between_model_and_human_attention_reveals_unique_signatures_of_critical_visual_events.html">57 nips-2007-Congruence between model and human attention reveals unique signatures of critical visual events</a></p>
<p>15 0.052158669 <a title="93-tfidf-15" href="./nips-2007-The_Distribution_Family_of_Similarity_Distances.html">193 nips-2007-The Distribution Family of Similarity Distances</a></p>
<p>16 0.051919471 <a title="93-tfidf-16" href="./nips-2007-Discriminative_Batch_Mode_Active_Learning.html">69 nips-2007-Discriminative Batch Mode Active Learning</a></p>
<p>17 0.051257983 <a title="93-tfidf-17" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>18 0.050677214 <a title="93-tfidf-18" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>19 0.04937385 <a title="93-tfidf-19" href="./nips-2007-Second_Order_Bilinear_Discriminant_Analysis_for_single_trial_EEG_analysis.html">173 nips-2007-Second Order Bilinear Discriminant Analysis for single trial EEG analysis</a></p>
<p>20 0.049163301 <a title="93-tfidf-20" href="./nips-2007-Unsupervised_Feature_Selection_for_Accurate_Recommendation_of_High-Dimensional_Image_Data.html">211 nips-2007-Unsupervised Feature Selection for Accurate Recommendation of High-Dimensional Image Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.173), (1, 0.079), (2, 0.001), (3, -0.035), (4, 0.028), (5, 0.171), (6, 0.046), (7, 0.132), (8, 0.027), (9, -0.072), (10, -0.039), (11, -0.048), (12, -0.047), (13, -0.014), (14, -0.039), (15, 0.011), (16, 0.004), (17, 0.087), (18, 0.043), (19, 0.0), (20, -0.005), (21, 0.038), (22, -0.023), (23, 0.055), (24, -0.059), (25, 0.001), (26, -0.061), (27, 0.051), (28, 0.049), (29, 0.067), (30, -0.055), (31, 0.055), (32, -0.004), (33, 0.011), (34, 0.057), (35, -0.002), (36, -0.098), (37, 0.016), (38, 0.063), (39, 0.041), (40, -0.022), (41, 0.051), (42, 0.1), (43, -0.023), (44, -0.032), (45, -0.177), (46, -0.029), (47, -0.041), (48, 0.064), (49, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91908675 <a title="93-lsi-1" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>Author: Michael Ross, Andrew Cohen</p><p>Abstract: This paper describes a new model for human visual classiﬁcation that enables the recovery of image features that explain human subjects’ performance on different visual classiﬁcation tasks. Unlike previous methods, this algorithm does not model their performance with a single linear classiﬁer operating on raw image pixels. Instead, it represents classiﬁcation as the combination of multiple feature detectors. This approach extracts more information about human visual classiﬁcation than previous methods and provides a foundation for further exploration. 1</p><p>2 0.74133396 <a title="93-lsi-2" href="./nips-2007-Multiple-Instance_Pruning_For_Learning_Efficient_Cascade_Detectors.html">137 nips-2007-Multiple-Instance Pruning For Learning Efficient Cascade Detectors</a></p>
<p>Author: Cha Zhang, Paul A. Viola</p><p>Abstract: Cascade detectors have been shown to operate extremely rapidly, with high accuracy, and have important applications such as face detection. Driven by this success, cascade learning has been an area of active research in recent years. Nevertheless, there are still challenging technical problems during the training process of cascade detectors. In particular, determining the optimal target detection rate for each stage of the cascade remains an unsolved issue. In this paper, we propose the multiple instance pruning (MIP) algorithm for soft cascades. This algorithm computes a set of thresholds which aggressively terminate computation with no reduction in detection rate or increase in false positive rate on the training dataset. The algorithm is based on two key insights: i) examples that are destined to be rejected by the complete classiﬁer can be safely pruned early; ii) face detection is a multiple instance learning problem. The MIP process is fully automatic and requires no assumptions of probability distributions, statistical independence, or ad hoc intermediate rejection targets. Experimental results on the MIT+CMU dataset demonstrate signiﬁcant performance advantages. 1</p><p>3 0.67050952 <a title="93-lsi-3" href="./nips-2007-Configuration_Estimates_Improve_Pedestrian_Finding.html">56 nips-2007-Configuration Estimates Improve Pedestrian Finding</a></p>
<p>Author: Duan Tran, David A. Forsyth</p><p>Abstract: Fair discriminative pedestrian ﬁnders are now available. In fact, these pedestrian ﬁnders make most errors on pedestrians in conﬁgurations that are uncommon in the training data, for example, mounting a bicycle. This is undesirable. However, the human conﬁguration can itself be estimated discriminatively using structure learning. We demonstrate a pedestrian ﬁnder which ﬁrst ﬁnds the most likely human pose in the window using a discriminative procedure trained with structure learning on a small dataset. We then present features (local histogram of oriented gradient and local PCA of gradient) based on that conﬁguration to an SVM classiﬁer. We show, using the INRIA Person dataset, that estimates of conﬁguration signiﬁcantly improve the accuracy of a discriminative pedestrian ﬁnder. 1</p><p>4 0.59995073 <a title="93-lsi-4" href="./nips-2007-Estimating_disparity_with_confidence_from_energy_neurons.html">81 nips-2007-Estimating disparity with confidence from energy neurons</a></p>
<p>Author: Eric K. Tsang, Bertram E. Shi</p><p>Abstract: The peak location in a population of phase-tuned neurons has been shown to be a more reliable estimator for disparity than the peak location in a population of position-tuned neurons. Unfortunately, the disparity range covered by a phasetuned population is limited by phase wraparound. Thus, a single population cannot cover the large range of disparities encountered in natural scenes unless the scale of the receptive fields is chosen to be very large, which results in very low resolution depth estimates. Here we describe a biologically plausible measure of the confidence that the stimulus disparity is inside the range covered by a population of phase-tuned neurons. Based upon this confidence measure, we propose an algorithm for disparity estimation that uses many populations of high-resolution phase-tuned neurons that are biased to different disparity ranges via position shifts between the left and right eye receptive fields. The population with the highest confidence is used to estimate the stimulus disparity. We show that this algorithm outperforms a previously proposed coarse-to-fine algorithm for disparity estimation, which uses disparity estimates from coarse scales to select the populations used at finer scales and can effectively detect occlusions.</p><p>5 0.56069529 <a title="93-lsi-5" href="./nips-2007-Predicting_human_gaze_using_low-level_saliency_combined_with_face_detection.html">155 nips-2007-Predicting human gaze using low-level saliency combined with face detection</a></p>
<p>Author: Moran Cerf, Jonathan Harel, Wolfgang Einhaeuser, Christof Koch</p><p>Abstract: Under natural viewing conditions, human observers shift their gaze to allocate processing resources to subsets of the visual input. Many computational models try to predict such voluntary eye and attentional shifts. Although the important role of high level stimulus properties (e.g., semantic information) in search stands undisputed, most models are based on low-level image properties. We here demonstrate that a combined model of face detection and low-level saliency signiﬁcantly outperforms a low-level model in predicting locations humans ﬁxate on, based on eye-movement recordings of humans observing photographs of natural scenes, most of which contained at least one person. Observers, even when not instructed to look for anything particular, ﬁxate on a face with a probability of over 80% within their ﬁrst two ﬁxations; furthermore, they exhibit more similar scanpaths when faces are present. Remarkably, our model’s predictive performance in images that do not contain faces is not impaired, and is even improved in some cases by spurious face detector responses. 1</p><p>6 0.52501363 <a title="93-lsi-6" href="./nips-2007-Object_Recognition_by_Scene_Alignment.html">143 nips-2007-Object Recognition by Scene Alignment</a></p>
<p>7 0.51371425 <a title="93-lsi-7" href="./nips-2007-Learning_Visual_Attributes.html">113 nips-2007-Learning Visual Attributes</a></p>
<p>8 0.5025394 <a title="93-lsi-8" href="./nips-2007-The_Noisy-Logical_Distribution_and_its_Application_to_Causal_Inference.html">198 nips-2007-The Noisy-Logical Distribution and its Application to Causal Inference</a></p>
<p>9 0.47189218 <a title="93-lsi-9" href="./nips-2007-Congruence_between_model_and_human_attention_reveals_unique_signatures_of_critical_visual_events.html">57 nips-2007-Congruence between model and human attention reveals unique signatures of critical visual events</a></p>
<p>10 0.46522358 <a title="93-lsi-10" href="./nips-2007-Optimal_ROC_Curve_for_a_Combination_of_Classifiers.html">149 nips-2007-Optimal ROC Curve for a Combination of Classifiers</a></p>
<p>11 0.44936877 <a title="93-lsi-11" href="./nips-2007-A_Bayesian_Model_of_Conditioned_Perception.html">3 nips-2007-A Bayesian Model of Conditioned Perception</a></p>
<p>12 0.44848803 <a title="93-lsi-12" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>13 0.44565284 <a title="93-lsi-13" href="./nips-2007-Comparing_Bayesian_models_for_multisensory_cue_combination_without_mandatory_integration.html">51 nips-2007-Comparing Bayesian models for multisensory cue combination without mandatory integration</a></p>
<p>14 0.4424524 <a title="93-lsi-14" href="./nips-2007-Markov_Chain_Monte_Carlo_with_People.html">125 nips-2007-Markov Chain Monte Carlo with People</a></p>
<p>15 0.4306623 <a title="93-lsi-15" href="./nips-2007-Unsupervised_Feature_Selection_for_Accurate_Recommendation_of_High-Dimensional_Image_Data.html">211 nips-2007-Unsupervised Feature Selection for Accurate Recommendation of High-Dimensional Image Data</a></p>
<p>16 0.41242728 <a title="93-lsi-16" href="./nips-2007-EEG-Based_Brain-Computer_Interaction%3A_Improved_Accuracy_by_Automatic_Single-Trial_Error_Detection.html">74 nips-2007-EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection</a></p>
<p>17 0.41112736 <a title="93-lsi-17" href="./nips-2007-Selecting_Observations_against_Adversarial_Objectives.html">174 nips-2007-Selecting Observations against Adversarial Objectives</a></p>
<p>18 0.41037592 <a title="93-lsi-18" href="./nips-2007-Scene_Segmentation_with_CRFs_Learned_from_Partially_Labeled_Images.html">172 nips-2007-Scene Segmentation with CRFs Learned from Partially Labeled Images</a></p>
<p>19 0.40467456 <a title="93-lsi-19" href="./nips-2007-Hidden_Common_Cause_Relations_in_Relational_Learning.html">97 nips-2007-Hidden Common Cause Relations in Relational Learning</a></p>
<p>20 0.39342186 <a title="93-lsi-20" href="./nips-2007-The_Distribution_Family_of_Similarity_Distances.html">193 nips-2007-The Distribution Family of Similarity Distances</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.21), (5, 0.059), (13, 0.034), (16, 0.032), (18, 0.053), (19, 0.017), (21, 0.086), (31, 0.023), (34, 0.015), (35, 0.019), (47, 0.099), (49, 0.011), (83, 0.128), (85, 0.019), (87, 0.038), (90, 0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8253355 <a title="93-lda-1" href="./nips-2007-GRIFT%3A_A_graphical_model_for_inferring_visual_classification_features_from_human_data.html">93 nips-2007-GRIFT: A graphical model for inferring visual classification features from human data</a></p>
<p>Author: Michael Ross, Andrew Cohen</p><p>Abstract: This paper describes a new model for human visual classiﬁcation that enables the recovery of image features that explain human subjects’ performance on different visual classiﬁcation tasks. Unlike previous methods, this algorithm does not model their performance with a single linear classiﬁer operating on raw image pixels. Instead, it represents classiﬁcation as the combination of multiple feature detectors. This approach extracts more information about human visual classiﬁcation than previous methods and provides a foundation for further exploration. 1</p><p>2 0.68911946 <a title="93-lda-2" href="./nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</a></p>
<p>Author: Matthew Hoffman, Arnaud Doucet, Nando D. Freitas, Ajay Jasra</p><p>Abstract: A recently proposed formulation of the stochastic planning and control problem as one of parameter estimation for suitable artiﬁcial statistical models has led to the adoption of inference algorithms for this notoriously hard problem. At the algorithmic level, the focus has been on developing Expectation-Maximization (EM) algorithms. In this paper, we begin by making the crucial observation that the stochastic control problem can be reinterpreted as one of trans-dimensional inference. With this new interpretation, we are able to propose a novel reversible jump Markov chain Monte Carlo (MCMC) algorithm that is more efﬁcient than its EM counterparts. Moreover, it enables us to implement full Bayesian policy search, without the need for gradients and with one single Markov chain. The new approach involves sampling directly from a distribution that is proportional to the reward and, consequently, performs better than classic simulations methods in situations where the reward is a rare event.</p><p>3 0.68692404 <a title="93-lda-3" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>Author: Matthias Bethge, Philipp Berens</p><p>Abstract: Maximum entropy analysis of binary variables provides an elegant way for studying the role of pairwise correlations in neural populations. Unfortunately, these approaches suffer from their poor scalability to high dimensions. In sensory coding, however, high-dimensional data is ubiquitous. Here, we introduce a new approach using a near-maximum entropy model, that makes this type of analysis feasible for very high-dimensional data—the model parameters can be derived in closed form and sampling is easy. Therefore, our NearMaxEnt approach can serve as a tool for testing predictions from a pairwise maximum entropy model not only for low-dimensional marginals, but also for high dimensional measurements of more than thousand units. We demonstrate its usefulness by studying natural images with dichotomized pixel intensities. Our results indicate that the statistics of such higher-dimensional measurements exhibit additional structure that are not predicted by pairwise correlations, despite the fact that pairwise correlations explain the lower-dimensional marginal statistics surprisingly well up to the limit of dimensionality where estimation of the full joint distribution is feasible. 1</p><p>4 0.68646419 <a title="93-lda-4" href="./nips-2007-People_Tracking_with_the_Laplacian_Eigenmaps_Latent_Variable_Model.html">153 nips-2007-People Tracking with the Laplacian Eigenmaps Latent Variable Model</a></p>
<p>Author: Zhengdong Lu, Cristian Sminchisescu, Miguel Á. Carreira-Perpiñán</p><p>Abstract: Reliably recovering 3D human pose from monocular video requires models that bias the estimates towards typical human poses and motions. We construct priors for people tracking using the Laplacian Eigenmaps Latent Variable Model (LELVM). LELVM is a recently introduced probabilistic dimensionality reduction model that combines the advantages of latent variable models—a multimodal probability density for latent and observed variables, and globally differentiable nonlinear mappings for reconstruction and dimensionality reduction—with those of spectral manifold learning methods—no local optima, ability to unfold highly nonlinear manifolds, and good practical scaling to latent spaces of high dimension. LELVM is computationally efﬁcient, simple to learn from sparse training data, and compatible with standard probabilistic trackers such as particle ﬁlters. We analyze the performance of a LELVM-based probabilistic sigma point mixture tracker in several real and synthetic human motion sequences and demonstrate that LELVM not only provides sufﬁcient constraints for robust operation in the presence of missing, noisy and ambiguous image measurements, but also compares favorably with alternative trackers based on PCA or GPLVM priors. Recent research in reconstructing articulated human motion has focused on methods that can exploit available prior knowledge on typical human poses or motions in an attempt to build more reliable algorithms. The high-dimensionality of human ambient pose space—between 30-60 joint angles or joint positions depending on the desired accuracy level, makes exhaustive search prohibitively expensive. This has negative impact on existing trackers, which are often not sufﬁciently reliable at reconstructing human-like poses, self-initializing or recovering from failure. Such difﬁculties have stimulated research in algorithms and models that reduce the effective working space, either using generic search focusing methods (annealing, state space decomposition, covariance scaling) or by exploiting speciﬁc problem structure (e.g. kinematic jumps). Experience with these procedures has nevertheless shown that any search strategy, no matter how effective, can be made signiﬁcantly more reliable if restricted to low-dimensional state spaces. This permits a more thorough exploration of the typical solution space, for a given, comparatively similar computational effort as a high-dimensional method. The argument correlates well with the belief that the human pose space, although high-dimensional in its natural ambient parameterization, has a signiﬁcantly lower perceptual (latent or intrinsic) dimensionality, at least in a practical sense—many poses that are possible are so improbable in many real-world situations that it pays off to encode them with low accuracy. A perceptual representation has to be powerful enough to capture the diversity of human poses in a sufﬁciently broad domain of applicability (the task domain), yet compact and analytically tractable for search and optimization. This justiﬁes the use of models that are nonlinear and low-dimensional (able to unfold highly nonlinear manifolds with low distortion), yet probabilistically motivated and globally continuous for efﬁcient optimization. Reducing dimensionality is not the only goal: perceptual representations have to preserve critical properties of the ambient space. Reliable tracking needs locality: nearby regions in ambient space have to be mapped to nearby regions in latent space. If this does not hold, the tracker is forced to make unrealistically large, and difﬁcult to predict jumps in latent space in order to follow smooth trajectories in the joint angle ambient space. 1 In this paper we propose to model priors for articulated motion using a recently introduced probabilistic dimensionality reduction method, the Laplacian Eigenmaps Latent Variable Model (LELVM) [1]. Section 1 discusses the requirements of priors for articulated motion in the context of probabilistic and spectral methods for manifold learning, and section 2 describes LELVM and shows how it combines both types of methods in a principled way. Section 3 describes our tracking framework (using a particle ﬁlter) and section 4 shows experiments with synthetic and real human motion sequences using LELVM priors learned from motion-capture data. Related work: There is signiﬁcant work in human tracking, using both generative and discriminative methods. Due to space limitations, we will focus on the more restricted class of 3D generative algorithms based on learned state priors, and not aim at a full literature review. Deriving compact prior representations for tracking people or other articulated objects is an active research ﬁeld, steadily growing with the increased availability of human motion capture data. Howe et al. and Sidenbladh et al. [2] propose Gaussian mixture representations of short human motion fragments (snippets) and integrate them in a Bayesian MAP estimation framework that uses 2D human joint measurements, independently tracked by scaled prismatic models [3]. Brand [4] models the human pose manifold using a Gaussian mixture and uses an HMM to infer the mixture component index based on a temporal sequence of human silhouettes. Sidenbladh et al. [5] use similar dynamic priors and exploit ideas in texture synthesis—efﬁcient nearest-neighbor search for similar motion fragments at runtime—in order to build a particle-ﬁlter tracker with observation model based on contour and image intensity measurements. Sminchisescu and Jepson [6] propose a low-dimensional probabilistic model based on ﬁtting a parametric reconstruction mapping (sparse radial basis function) and a parametric latent density (Gaussian mixture) to the embedding produced with a spectral method. They track humans walking and involved in conversations using a Bayesian multiple hypotheses framework that fuses contour and intensity measurements. Urtasun et al. [7] use a dynamic MAP estimation framework based on a GPLVM and 2D human joint correspondences obtained from an independent image-based tracker. Li et al. [8] use a coordinated mixture of factor analyzers within a particle ﬁltering framework, in order to reconstruct human motion in multiple views using chamfer matching to score different conﬁguration. Wang et al. [9] learn a latent space with associated dynamics where both the dynamics and observation mapping are Gaussian processes, and Urtasun et al. [10] use it for tracking. Taylor et al. [11] also learn a binary latent space with dynamics (using an energy-based model) but apply it to synthesis, not tracking. Our work learns a static, generative low-dimensional model of poses and integrates it into a particle ﬁlter for tracking. We show its ability to work with real or partially missing data and to track multiple activities. 1 Priors for articulated human pose We consider the problem of learning a probabilistic low-dimensional model of human articulated motion. Call y ∈ RD the representation in ambient space of the articulated pose of a person. In this paper, y contains the 3D locations of anywhere between 10 and 60 markers located on the person’s joints (other representations such as joint angles are also possible). The values of y have been normalised for translation and rotation in order to remove rigid motion and leave only the articulated motion (see section 3 for how we track the rigid motion). While y is high-dimensional, the motion pattern lives in a low-dimensional manifold because most values of y yield poses that violate body constraints or are simply atypical for the motion type considered. Thus we want to model y in terms of a small number of latent variables x given a collection of poses {yn }N (recorded from a human n=1 with motion-capture technology). The model should satisfy the following: (1) It should deﬁne a probability density for x and y, to be able to deal with noise (in the image or marker measurements) and uncertainty (from missing data due to occlusion or markers that drop), and to allow integration in a sequential Bayesian estimation framework. The density model should also be ﬂexible enough to represent multimodal densities. (2) It should deﬁne mappings for dimensionality reduction F : y → x and reconstruction f : x → y that apply to any value of x and y (not just those in the training set); and such mappings should be deﬁned on a global coordinate system, be continuous (to avoid physically impossible discontinuities) and differentiable (to allow efﬁcient optimisation when tracking), yet ﬂexible enough to represent the highly nonlinear manifold of articulated poses. From a statistical machine learning point of view, this is precisely what latent variable models (LVMs) do; for example, factor analysis deﬁnes linear mappings and Gaussian densities, while the generative topographic mapping (GTM; [12]) deﬁnes nonlinear mappings and a Gaussian-mixture density in ambient space. However, factor analysis is too limited to be of practical use, and GTM— 2 while ﬂexible—has two important practical problems: (1) the latent space must be discretised to allow tractable learning and inference, which limits it to very low (2–3) latent dimensions; (2) the parameter estimation is prone to bad local optima that result in highly distorted mappings. Another dimensionality reduction method recently introduced, GPLVM [13], which uses a Gaussian process mapping f (x), partly improves this situation by deﬁning a tunable parameter xn for each data point yn . While still prone to local optima, this allows the use of a better initialisation for {xn }N (obtained from a spectral method, see later). This has prompted the application of n=1 GPLVM for tracking human motion [7]. However, GPLVM has some disadvantages: its training is very costly (each step of the gradient iteration is cubic on the number of training points N , though approximations based on using few points exist); unlike true LVMs, it deﬁnes neither a posterior distribution p(x|y) in latent space nor a dimensionality reduction mapping E {x|y}; and the latent representation it obtains is not ideal. For example, for periodic motions such as running or walking, repeated periods (identical up to small noise) can be mapped apart from each other in latent space because nothing constrains xn and xm to be close even when yn = ym (see ﬁg. 3 and [10]). There exists a different type of dimensionality reduction methods, spectral methods (such as Isomap, LLE or Laplacian eigenmaps [14]), that have advantages and disadvantages complementary to those of LVMs. They deﬁne neither mappings nor densities but just a correspondence (xn , yn ) between points in latent space xn and ambient space yn . However, the training is efﬁcient (a sparse eigenvalue problem) and has no local optima, and often yields a correspondence that successfully models highly nonlinear, convoluted manifolds such as the Swiss roll. While these attractive properties have spurred recent research in spectral methods, their lack of mappings and densities has limited their applicability in people tracking. However, a new model that combines the advantages of LVMs and spectral methods in a principled way has been recently proposed [1], which we brieﬂy describe next. 2 The Laplacian Eigenmaps Latent Variable Model (LELVM) LELVM is based on a natural way of deﬁning an out-of-sample mapping for Laplacian eigenmaps (LE) which, in addition, results in a density model. In LE, typically we ﬁrst deﬁne a k-nearestneighbour graph on the sample data {yn }N and weigh each edge yn ∼ ym by a Gaussian afﬁnity n=1 2 1 function K(yn , ym ) = wnm = exp (− 2 (yn − ym )/σ ). Then the latent points X result from: min tr XLX⊤ s.t. X ∈ RL×N , XDX⊤ = I, XD1 = 0 (1) where we deﬁne the matrix XL×N = (x1 , . . . , xN ), the symmetric afﬁnity matrix WN ×N , the deN gree matrix D = diag ( n=1 wnm ), the graph Laplacian matrix L = D−W, and 1 = (1, . . . , 1)⊤ . The constraints eliminate the two trivial solutions X = 0 (by ﬁxing an arbitrary scale) and x1 = · · · = xN (by removing 1, which is an eigenvector of L associated with a zero eigenvalue). The solution is given by the leading u2 , . . . , uL+1 eigenvectors of the normalised afﬁnity matrix 1 1 1 N = D− 2 WD− 2 , namely X = V⊤ D− 2 with VN ×L = (v2 , . . . , vL+1 ) (an a posteriori translated, rotated or uniformly scaled X is equally valid). Following [1], we now deﬁne an out-of-sample mapping F(y) = x for a new point y as a semisupervised learning problem, by recomputing the embedding as in (1) (i.e., augmenting the graph Laplacian with the new point), but keeping the old embedding ﬁxed: L K(y) X⊤ min tr ( X x ) K(y)⊤ 1⊤ K(y) (2) x⊤ x∈RL 2 where Kn (y) = K(y, yn ) = exp (− 1 (y − yn )/σ ) for n = 1, . . . , N is the kernel induced by 2 the Gaussian afﬁnity (applied only to the k nearest neighbours of y, i.e., Kn (y) = 0 if y ≁ yn ). This is one natural way of adding a new point to the embedding by keeping existing embedded points ﬁxed. We need not use the constraints from (1) because they would trivially determine x, and the uninteresting solutions X = 0 and X = constant were already removed in the old embedding anyway. The solution yields an out-of-sample dimensionality reduction mapping x = F(y): x = F(y) = X K(y) 1⊤ K(y) N K(y,yn ) PN x n=1 K(y,yn′ ) n ′ = (3) n =1 applicable to any point y (new or old). This mapping is formally identical to a Nadaraya-Watson estimator (kernel regression; [15]) using as data {(xn , yn )}N and the kernel K. We can take this n=1 a step further by deﬁning a LVM that has as joint distribution a kernel density estimate (KDE): p(x, y) = 1 N N n=1 Ky (y, yn )Kx (x, xn ) p(y) = 3 1 N N n=1 Ky (y, yn ) p(x) = 1 N N n=1 Kx (x, xn ) where Ky is proportional to K so it integrates to 1, and Kx is a pdf kernel in x–space. Consequently, the marginals in observed and latent space are also KDEs, and the dimensionality reduction and reconstruction mappings are given by kernel regression (the conditional means E {y|x}, E {x|y}): F(y) = N n=1 p(n|y)xn f (x) = N K (x,xn ) PN x y n=1 Kx (x,xn′ ) n ′ = n =1 N n=1 p(n|x)yn . (4) We allow the bandwidths to be different in the latent and ambient spaces: 2 2 1 Kx (x, xn ) ∝ exp (− 1 (x − xn )/σx ) and Ky (y, yn ) ∝ exp (− 2 (y − yn )/σy ). They 2 may be tuned to control the smoothness of the mappings and densities [1]. Thus, LELVM naturally extends a LE embedding (efﬁciently obtained as a sparse eigenvalue problem with a cost O(N 2 )) to global, continuous, differentiable mappings (NW estimators) and potentially multimodal densities having the form of a Gaussian KDE. This allows easy computation of posterior probabilities such as p(x|y) (unlike GPLVM). It can use a continuous latent space of arbitrary dimension L (unlike GTM) by simply choosing L eigenvectors in the LE embedding. It has no local optima since it is based on the LE embedding. LELVM can learn convoluted mappings (e.g. the Swiss roll) and deﬁne maps and densities for them [1]. The only parameters to set are the graph parameters (number of neighbours k, afﬁnity width σ) and the smoothing bandwidths σx , σy . 3 Tracking framework We follow the sequential Bayesian estimation framework, where for state variables s and observation variables z we have the recursive prediction and correction equations: p(st |z0:t−1 ) = p(st |st−1 ) p(st−1 |z0:t−1 ) dst−1 p(st |z0:t ) ∝ p(zt |st ) p(st |z0:t−1 ). (5) L We deﬁne the state variables as s = (x, d) where x ∈ R is the low-dim. latent space (for pose) and d ∈ R3 is the centre-of-mass location of the body (in the experiments our state also includes the orientation of the body, but for simplicity here we describe only the translation). The observed variables z consist of image features or the perspective projection of the markers on the camera plane. The mapping from state to observations is (for the markers’ case, assuming M markers): P f x ∈ RL − − → y ∈ R3M −→ ⊕ − − − z ∈ R2M −− − − −→ d ∈ R3 (6) where f is the LELVM reconstruction mapping (learnt from mocap data); ⊕ shifts each 3D marker by d; and P is the perspective projection (pinhole camera), applied to each 3D point separately. Here we use a simple observation model p(zt |st ): Gaussian with mean given by the transformation (6) and isotropic covariance (set by the user to control the inﬂuence of measurements in the tracking). We assume known correspondences and observations that are obtained either from the 3D markers (for tracking synthetic data) or 2D tracks obtained from a 2D tracker. Our dynamics model is p(st |st−1 ) ∝ pd (dt |dt−1 ) px (xt |xt−1 ) p(xt ) (7) where both dynamics models for d and x are random walks: Gaussians centred at the previous step value dt−1 and xt−1 , respectively, with isotropic covariance (set by the user to control the inﬂuence of dynamics in the tracking); and p(xt ) is the LELVM prior. Thus the overall dynamics predicts states that are both near the previous state and yield feasible poses. Of course, more complex dynamics models could be used if e.g. the speed and direction of movement are known. As tracker we use the Gaussian mixture Sigma-point particle ﬁlter (GMSPPF) [16]. This is a particle ﬁlter that uses a Gaussian mixture representation for the posterior distribution in state space and updates it with a Sigma-point Kalman ﬁlter. This Gaussian mixture will be used as proposal distribution to draw the particles. As in other particle ﬁlter implementations, the prediction step is carried out by approximating the integral (5) with particles and updating the particles’ weights. Then, a new Gaussian mixture is ﬁtted with a weighted EM algorithm to these particles. This replaces the resampling stage needed by many particle ﬁlters and mitigates the problem of sample depletion while also preventing the number of components in the Gaussian mixture from growing over time. The choice of this particular tracker is not critical; we use it to illustrate the fact that LELVM can be introduced in any probabilistic tracker for nonlinear, nongaussian models. Given the corrected distribution p(st |z0:t ), we choose its mean as recovered state (pose and location). It is also possible to choose instead the mode closest to the state at t − 1, which could be found by mean-shift or Newton algorithms [17] since we are using a Gaussian-mixture representation in state space. 4 4 Experiments We demonstrate our low-dimensional tracker on image sequences of people walking and running, both synthetic (ﬁg. 1) and real (ﬁg. 2–3). Fig. 1 shows the model copes well with persistent partial occlusion and severely subsampled training data (A,B), and quantitatively evaluates temporal reconstruction (C). For all our experiments, the LELVM parameters (number of neighbors k, Gaussian afﬁnity σ, and bandwidths σx and σy ) were set manually. We mainly considered 2D latent spaces (for pose, plus 6D for rigid motion), which were expressive enough for our experiments. More complex, higher-dimensional models are straightforward to construct. The initial state distribution p(s0 ) was chosen a broad Gaussian, the dynamics and observation covariance were set manually to control the tracking smoothness, and the GMSPPF tracker used a 5-component Gaussian mixture in latent space (and in the state space of rigid motion) and a small set of 500 particles. The 3D representation we use is a 102-D vector obtained by concatenating the 3D markers coordinates of all the body joints. These would be highly unconstrained if estimated independently, but we only use them as intermediate representation; tracking actually occurs in the latent space, tightly controlled using the LELVM prior. For the synthetic experiments and some of the real experiments (ﬁgs. 2–3) the camera parameters and the body proportions were known (for the latter, we used the 2D outputs of [6]). For the CMU mocap video (ﬁg. 2B) we roughly guessed. We used mocap data from several sources (CMU, OSU). As observations we always use 2D marker positions, which, depending on the analyzed sequence were either known (the synthetic case), or provided by an existing tracker [6] or speciﬁed manually (ﬁg. 2B). Alternatively 2D point trackers similar to the ones of [7] can be used. The forward generative model is obtained by combining the latent to ambient space mapping (this provides the position of the 3D markers) with a perspective projection transformation. The observation model is a product of Gaussians, each measuring the probability of a particular marker position given its corresponding image point track. Experiments with synthetic data: we analyze the performance of our tracker in controlled conditions (noise perturbed synthetically generated image tracks) both under regular circumstances (reasonable sampling of training data) and more severe conditions with subsampled training points and persistent partial occlusion (the man running behind a fence, with many of the 2D marker tracks obstructed). Fig. 1B,C shows both the posterior (ﬁltered) latent space distribution obtained from our tracker, and its mean (we do not show the distribution of the global rigid body motion; in all experiments this is tracked with good accuracy). In the latent space plot shown in ﬁg. 1B, the onset of running (two cycles were used) appears as a separate region external to the main loop. It does not appear in the subsampled training set in ﬁg. 1B, where only one running cycle was used for training and the onset of running was removed. In each case, one can see that the model is able to track quite competently, with a modest decrease in its temporal accuracy, shown in ﬁg. 1C, where the averages are computed per 3D joint (normalised wrt body height). Subsampling causes some ambiguity in the estimate, e.g. see the bimodality in the right plot in ﬁg. 1C. In another set of experiments (not shown) we also tracked using different subsets of 3D markers. The estimates were accurate even when about 30% of the markers were dropped. Experiments with real images: this shows our tracker’s ability to work with real motions of different people, with different body proportions, not in its latent variable model training set (ﬁgs. 2–3). We study walking, running and turns. In all cases, tracking and 3D reconstruction are reasonably accurate. We have also run comparisons against low-dimensional models based on PCA and GPLVM (ﬁg. 3). It is important to note that, for LELVM, errors in the pose estimates are primarily caused by mismatches between the mocap data used to learn the LELVM prior and the body proportions of the person in the video. For example, the body proportions of the OSU motion captured walker are quite different from those of the image in ﬁg. 2–3 (e.g. note how the legs of the stick man are shorter relative to the trunk). Likewise, the style of the runner from the OSU data (e.g. the swinging of the arms) is quite different from that of the video. Finally, the interest points tracked by the 2D tracker do not entirely correspond either in number or location to the motion capture markers, and are noisy and sometimes missing. In future work, we plan to include an optimization step to also estimate the body proportions. This would be complicated for a general, unconstrained model because the dimensions of the body couple with the pose, so either one or the other can be changed to improve the tracking error (the observation likelihood can also become singular). But for dedicated prior pose models like ours these difﬁculties should be signiﬁcantly reduced. The model simply cannot assume highly unlikely stances—these are either not representable at all, or have reduced probability—and thus avoids compensatory, unrealistic body proportion estimates. 5 n = 15 n = 40 n = 65 n = 90 n = 115 n = 140 A 1.5 1.5 1.5 1.5 1 −1 −1 −1 −1 −1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1 −1 −1 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 −1 −1 1 −1 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1.5 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0.3 0.2 0.1 −1 0.4 0.3 0.2 −1.5 −1.5 n = 60 0.4 0.3 −2 −1 −1.5 −1 0 −0.5 n = 49 0.4 0.1 −1 −1.5 1 0 −0.5 −2 −1 −1.5 −1 0.5 0 −0.5 −1.5 −1.5 0.5 1 0.5 0 −0.5 −1.5 −1.5 −1.5 1 0.5 0 −0.5 n = 37 0.2 0.1 −1 −1 −1.5 −0.5 −1 −1.5 −2 1 0.5 0 −0.5 −0.5 −1.5 −1.5 0.3 0.2 0.1 0 0.4 0.3 0.2 −0.5 n = 25 0.4 0.3 −1 0 −0.5 −1 −1.5 1 0.5 0 0 −0.5 1.5 1 0.5 0 −0.5 −2 1 0.5 0.5 0 −1.5 −1.5 −1.5 1.5 1 0.5 0 −1 −1.5 −2 1 1 0.5 −0.5 −1 −1.5 −1.5 n = 13 0.4 −1 −1 −1.5 −1.5 −1.5 n=1 B −1 −1 −1.5 −1.5 1.5 1 0.5 −0.5 0 −1 −1.5 −2 −2 1 0 −0.5 −0.5 −0.5 −1 −1.5 −2 0.5 0 0 −0.5 0.5 0 −0.5 −1 −1.5 −2 1 0.5 0.5 0 1 0.5 0 −0.5 −1 −1.5 −1 −1.5 −2 1 1 0.5 1.5 1 0.5 0 −0.5 0 −0.5 −1 −1.5 −2 1 −0.5 1.5 1.5 1 0.5 0.5 0 −0.5 −1 −1.5 −2 1.5 1 1 0.5 0 −0.5 −2 0 −0.5 −0.5 1.5 1 0.5 0 −1 −1.5 −1 −1.5 0.5 0 0 −0.5 −1.5 −1.5 −1.5 1.5 1.5 1 0.5 −0.5 0 −0.5 −2 1 0.5 0.5 0 −0.5 −1 −1.5 −1.5 1.5 1 1 0.5 0 −1 −1.5 1 1 0.5 0 −0.5 −0.5 1.5 1 −0.5 −2 1 0.5 0 0 −0.5 0.5 0 −1 −1.5 −2 1 0.5 0.5 0 −0.5 1.5 1.5 1 −0.5 −2 1 1 0.5 0.5 0 −1 −1.5 −1 −1.5 −2 −2 1 0 −0.5 1.5 1 0.5 −0.5 0 −0.5 −1 −1.5 −2 0.5 0 −0.5 0.5 0 −0.5 −1 −1.5 −2 1 0.5 0 −0.5 0.5 0 −0.5 −1 −1.5 −1 −1.5 −2 1 0.5 0 1 0.5 0 −0.5 0 −0.5 −1 −1.5 −2 1 0.5 1.5 1 0.5 0.5 0 −0.5 −1 −1.5 1 1 1 0.5 0 −0.5 −2 1.5 1.5 1 1 0.5 0 −1 −1.5 1.5 1.5 1 0.5 −0.5 0.2 0.1 0.1 0 0 0 0 0 0 −0.1 −0.1 −0.1 −0.1 −0.1 −0.1 −0.2 −0.2 −0.2 −0.2 −0.2 −0.2 −0.3 −0.3 −0.3 −0.3 −0.3 −0.3 −0.4 0.4 0.6 0.8 1.5 1 1.2 1.4 1.6 1.8 2 2.2 −0.4 0.4 2.4 1.5 0.6 0.8 1.5 1 1.2 1.4 1.6 1.8 2 2.2 −0.4 0.4 2.4 1.5 0.6 0.8 1.5 1.5 1.5 1 1.2 1.4 1.6 1.8 2 2.2 −0.4 0.4 2.4 1.5 0.6 0.8 1.5 1.5 1.5 1 1.2 1.4 1.6 1.8 2 2.2 −0.4 0.4 2.4 1.5 0.6 0.8 1.5 1.5 1.5 1 1.2 1.4 1.6 1.8 2 2.2 −0.4 0.4 2.4 1.5 0.6 0.8 1.5 1.5 1.5 1 1.2 1.4 1.6 1.8 2 2.2 2.4 1.5 1.5 1.5 1.5 1.5 1 1 0.5 0.5 0 0 1 −0.5 −0.5 0 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 0.1 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 0 −0.5 −1.5 0.5 0 0 −0.5 0 −0.5 −0.5 −0.5 −2 −1 −1 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −2 −1 −1.5 −1 −1.5 1 0.5 0.5 0 −1.5 −1 1 1 0.5 −2 −1 −1.5 −1.5 −0.5 −1 −2 1 −1 0 −1 −1.5 −2 −0.5 −2 −1.5 0.5 −0.5 −1 −1.5 0 −0.5 −1 −1.5 0 −1.5 0.5 0 −0.5 −1 −0.5 −1 −1.5 1 0.5 0 −0.5 −1 −0.5 −1 1 0.5 0 −1.5 1 0.5 0 −0.5 −2 1 0.5 −2 −1 −1.5 −1.5 −0.5 0 −1 1 −1 0 1 −1.5 −2 −0.5 −2 −1.5 1 0.5 0 0.5 −0.5 −1 −1.5 0 −0.5 −1 −1.5 0 −1.5 0.5 0 −0.5 −1 −0.5 −1 −1.5 1 0.5 0 −0.5 −1 −0.5 −1 1 0.5 0 −1.5 1 0.5 1 0.5 0 −0.5 −2 1 0.5 −2 −1 −1.5 −1.5 −0.5 0 −1 1 −1 0 1 −1.5 −2 −0.5 −2 −1.5 1 0.5 0 0.5 −0.5 −1 −1.5 0 −0.5 −1 −1.5 0 −1.5 0.5 0 −0.5 −1 −0.5 −1 −1.5 1 0.5 0 −0.5 −1 −0.5 −1 1 0.5 0 −1.5 1 0.5 1 0.5 0 −0.5 −2 1 0.5 −2 −1 −1.5 −1.5 −0.5 0 −1 1 −1 0 1 −1.5 −2 −0.5 −2 −1 −1.5 −0.5 1 0.5 0 0.5 −0.5 −1 −1.5 0 −0.5 −0.5 −1 −1 −1.5 0.5 0 0 −0.5 −2 −1.5 0 −1.5 1 0.5 0.5 0 −2 −1 −1.5 −1.5 −1 1 1 0.5 −0.5 −1 1 0.5 1 0.5 −0.5 −1 −2 1 0 −0.5 −1 −1.5 −1.5 −1.5 −2 −1.5 0.5 0 −0.5 −1 −0.5 0 −0.5 −1.5 −1 −1.5 1 0.5 0 −0.5 0 1 −1 −0.5 −1 1 0.5 0 1 0.5 0 0.5 −0.5 −1 −0.5 −2 1 0.5 1 0.5 1 0.5 0 −1 1 0 1 −1.5 −2 0.5 0 1 0.5 −0.5 −1 −1.5 1 0.5 1 0.5 −1.5 −1.5 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 −1.5 −1 −0.5 0 0.5 1 0.13 0.12 RMSE C RMSE 0.08 0.06 0.04 0.02 0.11 0.1 0.09 0.08 0.07 0.06 0 0 50 100 150 0.05 0 10 time step n 20 30 40 50 60 time step n Figure 1: OSU running man motion capture data. A: we use 217 datapoints for training LELVM (with added noise) and for tracking. Row 1: tracking in the 2D latent space. The contours (very tight in this sequence) are the posterior probability. Row 2: perspective-projection-based observations with occlusions. Row 3: each quadruplet (a, a′ , b, b′ ) show the true pose of the running man from a front and side views (a, b), and the reconstructed pose by tracking with our model (a′ , b′ ). B: we use the ﬁrst running cycle for training LELVM and the second cycle for tracking. C: RMSE errors for each frame, for the tracking of A (left plot) and B (middle plot), normalised so that 1 equals the M 1 ˆ 2 height of the stick man. RMSE(n) = M j=1 ynj − ynj −1/2 for all 3D locations of the M ˆ markers, i.e., comparison of reconstructed stick man yn with ground-truth stick man yn . Right plot: multimodal posterior distribution in pose space for the model of A (frame 42). Comparison with PCA and GPLVM (ﬁg. 3): for these models, the tracker uses the same GMSPPF setting as for LELVM (number of particles, initialisation, random-walk dynamics, etc.) but with the mapping y = f (x) provided by GPLVM or PCA, and with a uniform prior p(x) in latent space (since neither GPLVM nor the non-probabilistic PCA provide one). The LELVM-tracker uses both its f (x) and latent space prior p(x), as discussed. All methods use a 2D latent space. We ensured the best possible training of GPLVM by model selection based on multiple runs. For PCA, the latent space looks deceptively good, showing non-intersecting loops. However, (1) individual loops do not collect together as they should (for LELVM they do); (2) worse still, the mapping from 2D to pose space yields a poor observation model. The reason is that the loop in 102-D pose space is nonlinearly bent and a plane can at best intersect it at a few points, so the tracker often stays put at one of those (typically an “average” standing position), since leaving it would increase the error a lot. Using more latent dimensions would improve this, but as LELVM shows, this is not necessary. For GPLVM, we found high sensitivity to ﬁlter initialisation: the estimates have high variance across runs and are inaccurate ≈ 80% of the time. When it fails, the GPLVM tracker often freezes in latent space, like PCA. When it does succeed, it produces results that are comparable with LELVM, although somewhat less accurate visually. However, even then GPLVM’s latent space consists of continuous chunks spread apart and offset from each other; GPLVM has no incentive to place nearby two xs mapping to the same y. This effect, combined with the lack of a data-sensitive, realistic latent space density p(x), makes GPLVM jump erratically from chunk to chunk, in contrast with LELVM, which smoothly follows the 1D loop. Some GPLVM problems might be alleviated using higher-order dynamics, but our experiments suggest that such modeling sophistication is less 6 0 0.5 1 n=1 n = 15 n = 29 n = 43 n = 55 n = 69 A 100 100 100 100 100 50 50 50 50 50 0 0 0 0 0 0 −50 −50 −50 −50 −50 −50 −100 −100 −100 −100 −100 −100 50 50 40 −40 −20 −50 −30 −10 0 −40 20 −20 40 n=4 −40 10 20 −20 40 50 0 n=9 −40 −20 30 40 50 0 n = 14 −40 20 −20 40 50 30 20 10 0 0 −40 −20 −30 −20 −40 10 20 −30 −10 0 −50 30 50 n = 19 −50 −30 −10 −50 30 −10 −20 −30 −40 10 50 40 30 20 10 0 −50 −30 −10 −50 20 −10 −20 −30 −40 10 50 40 30 20 10 0 −50 −30 −10 −50 30 −10 −20 −30 −40 0 50 50 40 30 20 10 0 −50 −30 −10 −50 30 −10 −20 −30 −40 10 40 30 20 10 0 −10 −20 −30 50 40 30 20 10 0 −10 −50 100 40 −40 10 20 −50 30 50 n = 24 40 50 n = 29 B 20 20 20 20 20 40 40 40 40 40 60 60 60 60 60 80 80 80 80 80 80 100 100 100 100 100 100 120 120 120 120 120 120 140 140 140 140 140 140 160 160 160 160 160 160 180 180 180 180 180 200 200 200 200 200 220 220 220 220 220 50 100 150 200 250 300 350 50 100 150 200 250 300 350 50 100 150 200 250 300 350 50 100 150 200 250 300 350 20 40 60 180 200 220 50 100 150 200 250 300 350 50 100 150 100 100 100 100 100 50 50 50 50 200 250 300 350 100 50 50 0 0 0 0 0 0 −50 −50 −50 −50 −50 −50 −100 −100 −100 −100 −100 −100 50 50 40 −40 −20 −50 −30 −10 0 −40 10 20 −50 30 40 −40 −20 −50 −30 −10 0 −40 10 20 −50 30 40 −40 −20 −50 −30 −10 0 −40 −20 30 40 50 0 −40 10 20 −50 30 40 50 40 30 30 20 20 10 10 0 −50 −30 −10 −50 20 0 −10 −20 −30 −40 10 40 30 20 10 0 −10 −20 −30 50 50 40 30 20 10 0 −10 −20 −30 50 50 40 30 20 10 0 −10 −20 −30 50 40 30 20 10 0 −10 −50 50 −40 −20 −30 −20 −30 −10 0 −40 10 20 −50 30 40 50 −10 −50 −40 −20 −30 −20 −30 −10 0 −40 10 20 −50 30 40 50 Figure 2: A: tracking of a video from [6] (turning & walking). We use 220 datapoints (3 full walking cycles) for training LELVM. Row 1: tracking in the 2D latent space. The contours are the estimated posterior probability. Row 2: tracking based on markers. The red dots are the 2D tracks and the green stick man is the 3D reconstruction obtained using our model. Row 3: our 3D reconstruction from a different viewpoint. B: tracking of a person running straight towards the camera. Notice the scale changes and possible forward-backward ambiguities in the 3D estimates. We train the LELVM using 180 datapoints (2.5 running cycles); 2D tracks were obtained by manually marking the video. In both A–B the mocap training data was for a person different from the video’s (with different body proportions and motions), and no ground-truth estimate was available for favourable initialisation. LELVM GPLVM PCA tracking in latent space tracking in latent space tracking in latent space 2.5 0.02 30 38 2 38 0.99 0.015 20 1.5 38 0.01 1 10 0.005 0.5 0 0 0 −0.005 −0.5 −10 −0.01 −1 −0.015 −1.5 −20 −0.02 −0.025 −0.025 −2 −0.02 −0.015 −0.01 −0.005 0 0.005 0.01 0.015 0.02 0.025 −2.5 −2 −1 0 1 2 3 −30 −80 −60 −40 −20 0 20 40 60 80 Figure 3: Method comparison, frame 38. PCA and GPLVM map consecutive walking cycles to spatially distinct latent space regions. Compounded by a data independent latent prior, the resulting tracker gets easily confused: it jumps across loops and/or remains put, trapped in local optima. In contrast, LELVM is stable and follows tightly a 1D manifold (see videos). crucial if locality constraints are correctly modeled (as in LELVM). We conclude that, compared to LELVM, GPLVM is signiﬁcantly less robust for tracking, has much higher training overhead and lacks some operations (e.g. computing latent conditionals based on partly missing ambient data). 7 5 Conclusion and future work We have proposed the use of priors based on the Laplacian Eigenmaps Latent Variable Model (LELVM) for people tracking. LELVM is a probabilistic dim. red. method that combines the advantages of latent variable models and spectral manifold learning algorithms: a multimodal probability density over latent and ambient variables, globally differentiable nonlinear mappings for reconstruction and dimensionality reduction, no local optima, ability to unfold highly nonlinear manifolds, and good practical scaling to latent spaces of high dimension. LELVM is computationally efﬁcient, simple to learn from sparse training data, and compatible with standard probabilistic trackers such as particle ﬁlters. Our results using a LELVM-based probabilistic sigma point mixture tracker with several real and synthetic human motion sequences show that LELVM provides sufﬁcient constraints for robust operation in the presence of missing, noisy and ambiguous image measurements. Comparisons with PCA and GPLVM show LELVM is superior in terms of accuracy, robustness and computation time. The objective of this paper was to demonstrate the ability of the LELVM prior in a simple setting using 2D tracks obtained automatically or manually, and single-type motions (running, walking). Future work will explore more complex observation models such as silhouettes; the combination of different motion types in the same latent space (whose dimension will exceed 2); and the exploration of multimodal posterior distributions in latent space caused by ambiguities. Acknowledgments This work was partially supported by NSF CAREER award IIS–0546857 (MACP), NSF IIS–0535140 and EC MCEXT–025481 (CS). CMU data: http://mocap.cs.cmu.edu (created with funding from NSF EIA–0196217). OSU data: http://accad.osu.edu/research/mocap/mocap data.htm. References ´ [1] M. A. Carreira-Perpi˜ an and Z. Lu. The Laplacian Eigenmaps Latent Variable Model. In AISTATS, 2007. n´ [2] N. R. Howe, M. E. Leventon, and W. T. Freeman. Bayesian reconstruction of 3D human motion from single-camera video. In NIPS, volume 12, pages 820–826, 2000. [3] T.-J. Cham and J. M. Rehg. A multiple hypothesis approach to ﬁgure tracking. In CVPR, 1999. [4] M. Brand. Shadow puppetry. In ICCV, pages 1237–1244, 1999. [5] H. Sidenbladh, M. J. Black, and L. Sigal. Implicit probabilistic models of human motion for synthesis and tracking. In ECCV, volume 1, pages 784–800, 2002. [6] C. Sminchisescu and A. Jepson. Generative modeling for continuous non-linearly embedded visual inference. In ICML, pages 759–766, 2004. [7] R. Urtasun, D. J. Fleet, A. Hertzmann, and P. Fua. Priors for people tracking from small training sets. In ICCV, pages 403–410, 2005. [8] R. Li, M.-H. Yang, S. Sclaroff, and T.-P. Tian. Monocular tracking of 3D human motion with a coordinated mixture of factor analyzers. In ECCV, volume 2, pages 137–150, 2006. [9] J. M. Wang, D. Fleet, and A. Hertzmann. Gaussian process dynamical models. In NIPS, volume 18, 2006. [10] R. Urtasun, D. J. Fleet, and P. Fua. Gaussian process dynamical models for 3D people tracking. In CVPR, pages 238–245, 2006. [11] G. W. Taylor, G. E. Hinton, and S. Roweis. Modeling human motion using binary latent variables. In NIPS, volume 19, 2007. [12] C. M. Bishop, M. Svens´ n, and C. K. I. Williams. GTM: The generative topographic mapping. Neural e Computation, 10(1):215–234, January 1998. [13] N. Lawrence. Probabilistic non-linear principal component analysis with Gaussian process latent variable models. Journal of Machine Learning Research, 6:1783–1816, November 2005. [14] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15(6):1373–1396, June 2003. [15] B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chapman & Hall, 1986. [16] R. van der Merwe and E. A. Wan. Gaussian mixture sigma-point particle ﬁlters for sequential probabilistic inference in dynamic state-space models. In ICASSP, volume 6, pages 701–704, 2003. ´ [17] M. A. Carreira-Perpi˜ an. Acceleration strategies for Gaussian mean-shift image segmentation. In CVPR, n´ pages 1160–1167, 2006. 8</p><p>5 0.68369693 <a title="93-lda-5" href="./nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">18 nips-2007-A probabilistic model for generating realistic lip movements from speech</a></p>
<p>Author: Gwenn Englebienne, Tim Cootes, Magnus Rattray</p><p>Abstract: The present work aims to model the correspondence between facial motion and speech. The face and sound are modelled separately, with phonemes being the link between both. We propose a sequential model and evaluate its suitability for the generation of the facial animation from a sequence of phonemes, which we obtain from speech. We evaluate the results both by computing the error between generated sequences and real video, as well as with a rigorous double-blind test with human subjects. Experiments show that our model compares favourably to other existing methods and that the sequences generated are comparable to real video sequences. 1</p><p>6 0.68098706 <a title="93-lda-6" href="./nips-2007-Distributed_Inference_for_Latent_Dirichlet_Allocation.html">73 nips-2007-Distributed Inference for Latent Dirichlet Allocation</a></p>
<p>7 0.6791752 <a title="93-lda-7" href="./nips-2007-Locality_and_low-dimensions_in_the_prediction_of_natural_experience_from_fMRI.html">122 nips-2007-Locality and low-dimensions in the prediction of natural experience from fMRI</a></p>
<p>8 0.67910427 <a title="93-lda-8" href="./nips-2007-Gaussian_Process_Models_for_Link_Analysis_and_Transfer_Learning.html">94 nips-2007-Gaussian Process Models for Link Analysis and Transfer Learning</a></p>
<p>9 0.67785805 <a title="93-lda-9" href="./nips-2007-Exponential_Family_Predictive_Representations_of_State.html">86 nips-2007-Exponential Family Predictive Representations of State</a></p>
<p>10 0.67776722 <a title="93-lda-10" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>11 0.67602563 <a title="93-lda-11" href="./nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</a></p>
<p>12 0.67582804 <a title="93-lda-12" href="./nips-2007-Predicting_Brain_States_from_fMRI_Data%3A_Incremental_Functional_Principal_Component_Regression.html">154 nips-2007-Predicting Brain States from fMRI Data: Incremental Functional Principal Component Regression</a></p>
<p>13 0.67563051 <a title="93-lda-13" href="./nips-2007-Supervised_Topic_Models.html">189 nips-2007-Supervised Topic Models</a></p>
<p>14 0.67556751 <a title="93-lda-14" href="./nips-2007-Collapsed_Variational_Inference_for_HDP.html">47 nips-2007-Collapsed Variational Inference for HDP</a></p>
<p>15 0.67519999 <a title="93-lda-15" href="./nips-2007-Predictive_Matrix-Variate_t_Models.html">156 nips-2007-Predictive Matrix-Variate t Models</a></p>
<p>16 0.67517531 <a title="93-lda-16" href="./nips-2007-Scene_Segmentation_with_CRFs_Learned_from_Partially_Labeled_Images.html">172 nips-2007-Scene Segmentation with CRFs Learned from Partially Labeled Images</a></p>
<p>17 0.6703853 <a title="93-lda-17" href="./nips-2007-Markov_Chain_Monte_Carlo_with_People.html">125 nips-2007-Markov Chain Monte Carlo with People</a></p>
<p>18 0.6687234 <a title="93-lda-18" href="./nips-2007-Sparse_Feature_Learning_for_Deep_Belief_Networks.html">180 nips-2007-Sparse Feature Learning for Deep Belief Networks</a></p>
<p>19 0.66863859 <a title="93-lda-19" href="./nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">79 nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<p>20 0.66844058 <a title="93-lda-20" href="./nips-2007-A_Bayesian_LDA-based_model_for_semi-supervised_part-of-speech_tagging.html">2 nips-2007-A Bayesian LDA-based model for semi-supervised part-of-speech tagging</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
