<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>140 nips-2007-Neural characterization in partially observed populations of spiking neurons</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-140" href="#">nips2007-140</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>140 nips-2007-Neural characterization in partially observed populations of spiking neurons</h1>
<br/><p>Source: <a title="nips-2007-140-pdf" href="http://papers.nips.cc/paper/3256-neural-characterization-in-partially-observed-populations-of-spiking-neurons.pdf">pdf</a></p><p>Author: Jonathan W. Pillow, Peter E. Latham</p><p>Abstract: Point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, owing in part to the fact that they do not take into account multiple stages of processing. Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more ﬂexible for ﬁtting neural data. More importantly, it allows us to estimate connectivity patterns among neurons (both observed and unobserved), and may provide insight into how networks process sensory input. We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model’s performance using a simulated example network consisting of two coupled neurons.</p><p>Reference: <a title="nips-2007-140-reference" href="../nips2007_reference/nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Neural characterization in partially observed populations of spiking neurons  Jonathan W. [sent-1, score-0.548]
</p><p>2 uk  Abstract Point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli. [sent-8, score-0.513]
</p><p>3 Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, owing in part to the fact that they do not take into account multiple stages of processing. [sent-9, score-0.734]
</p><p>4 Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. [sent-10, score-0.663]
</p><p>5 The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more ﬂexible for ﬁtting neural data. [sent-11, score-0.228]
</p><p>6 More importantly, it allows us to estimate connectivity patterns among neurons (both observed and unobserved), and may provide insight into how networks process sensory input. [sent-12, score-0.45]
</p><p>7 We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model’s performance using a simulated example network consisting of two coupled neurons. [sent-13, score-0.238]
</p><p>8 1 Introduction A central goal of computational neuroscience is to understand how the brain transforms sensory input into spike trains, and considerable effort has focused on the development of statistical models that can describe this transformation. [sent-14, score-0.4]
</p><p>9 One of the most successful of these is the linear-nonlinearPoisson (LNP) cascade model, which describes a cell’s response in terms of a linear ﬁlter (or receptive ﬁeld), an output nonlinearity, and an instantaneous spiking point process [1–5]. [sent-15, score-0.403]
</p><p>10 Recent efforts have generalized this model to incorporate spike-history and multi-neuronal dependencies, which greatly enhances the model’s ﬂexibility, allowing it to capture non-Poisson spiking statistics and joint responses of an entire population of neurons [6–10]. [sent-16, score-0.644]
</p><p>11 Point process models accurately describe the spiking responses of neurons in the early visual pathway to light, and of cortical neurons to injected currents. [sent-17, score-0.816]
</p><p>12 Such failings are in some ways not surprising: the cascade model’s stimulus sensitivity is described with a single linear ﬁlter, whereas responses in the brain reﬂect multiple stages of nonlinear processing, adaptation on multiple timescales, and recurrent feedback from higher-level areas. [sent-19, score-0.493]
</p><p>13 Here we extend the point-process modeling framework to incorporate a set of unobserved or “hidden” neurons, whose spike trains are unknown and treated as hidden or latent variables. [sent-21, score-0.569]
</p><p>14 The unobserved neurons respond to the stimulus and to synaptic inputs from other neurons, and their spiking 1  activity can in turn affect the responses of the observed neurons. [sent-22, score-1.013]
</p><p>15 Although this expanded model offers considerably greater ﬂexibility in describing an observed set of neural responses, it is more difﬁcult to ﬁt to data. [sent-25, score-0.208]
</p><p>16 Computing the likelihood of an observed set of spike trains requires integrating out the probability distribution over hidden activity, and we need sophisticated algorithms to ﬁnd the maximum likelihood estimate of model parameters. [sent-26, score-0.733]
</p><p>17 Both algorithms make use of a novel proposal density to capture the dependence of hidden spikes on the observed spike trains, which allows for fast sampling of hidden neurons’ activity. [sent-28, score-0.945]
</p><p>18 We show that a single-cell model used to characterize the observed neuron performs poorly, while a coupled two-cell model estimated using the wake-sleep algorithm performs much more accurately. [sent-30, score-0.447]
</p><p>19 2 Multi-neuronal point-process encoding model We begin with a description of the encoding model, which generalizes the LNP model to incorporate non-Poisson spiking and coupling between neurons. [sent-31, score-0.526]
</p><p>20 In this section we do not distinguish between observed and unobserved spikes, but will do so in the next. [sent-34, score-0.19]
</p><p>21 Let xt denote the stimulus at time t, and y t and zt denote the number of spikes elicited by two neurons at t, where t ∈ [0, T ] is an index over time. [sent-35, score-0.917]
</p><p>22 Note that x t is a vector containing all elements of the stimulus that are causally related to the (scalar) responses y t and zt at time t. [sent-36, score-0.607]
</p><p>23 Typically ∆ is sufﬁciently small that we observe only zero or one spike in every bin: y t , zt ∈ {0, 1}. [sent-43, score-0.477]
</p><p>24 The conditional intensity (or instantaneous spike rate) of each cell depends on both the stimulus and the recent spiking history via a bank of linear ﬁlters. [sent-44, score-0.994]
</p><p>25 Let y [t−τ,t) and z[t−τ,t) denote the (vector) spike train histories at time t. [sent-45, score-0.297]
</p><p>26 The nonlinear function, f , maps the input to the instantaneous spike rate of each cell. [sent-53, score-0.306]
</p><p>27 2  =  stimulus filter  x  >  point-process model ky  stochastic nonlinearity spiking  +  ky  neuron y  kz neuron z  stimulus  y  xt  hyy  hyy  coupling filters  exp(·)  f post-spike filter  x  equivalent model diagram  neuron y spikes  hzy  x1  x2  x3  x4  . [sent-55, score-2.562]
</p><p>28 hzy  neuron z spikes  +  causal structure  +  y[t-τ,t) yt  hyz  ? [sent-64, score-0.913]
</p><p>29 time  z[t-τ,t)  z  time  hzz  Figure 1: Schematic of generalized linear point-process (glpp) encoding model. [sent-65, score-0.319]
</p><p>30 a, Diagram of model parameters for a pair of coupled neurons. [sent-66, score-0.157]
</p><p>31 For each cell, the parameters consist of a stimulus ﬁlter (e. [sent-67, score-0.272]
</p><p>32 , ky ), a spike-train history ﬁlter (hyy ), and a ﬁlter capturing coupling from the spike train history of the other cell (hzy ). [sent-69, score-0.86]
</p><p>33 The ﬁlter outputs are summed, pass through an exponential nonlinearity, and drive spiking via an instantaneous point process. [sent-70, score-0.238]
</p><p>34 b, Equivalent diagram showing just the parameters of the neuron y, as used for drawing a sample yt . [sent-71, score-0.393]
</p><p>35 Gray boxes highlight the stimulus vector xt and spike train history vectors that form the input to the model on this time step. [sent-72, score-0.71]
</p><p>36 c, Simpliﬁed graphical model of the glpp causal structure, which allows us to visualize how the likelihood factorizes. [sent-73, score-0.303]
</p><p>37 Red arrows highlight the dependency structure for a single time bin of the response y3 . [sent-76, score-0.241]
</p><p>38 Equation 1 is equivalent to f applied to a linear convolution of the stimulus and spike trains with their respective ﬁlters; a schematic is shown in ﬁgure 1. [sent-78, score-0.68]
</p><p>39 The probability of observing y t spikes in a bin of size ∆ is given by a Poisson distribution with rate parameter λyt ∆, (λyt ∆)yt −λyt ∆ e , yt ! [sent-79, score-0.387]
</p><p>40 This factorization is possible because λyt and λzt depend only on the process history up to time t, making y t and zt conditionally independent given the stimulus and spike histories up to t (see Fig. [sent-82, score-0.829]
</p><p>41 If the response at time t were to depend on both the past and future response, we would have a causal loop , preventing factorization and making both sampling and likelihood evaluation very difﬁcult. [sent-84, score-0.229]
</p><p>42 We can write the log-likelihood simply as log P (Y, Z|X, θ) =  (yt log λyt + zt log λzt − ∆λyt − ∆λzt ) + c, t  where c is a constant that does not depend on θ. [sent-87, score-0.337]
</p><p>43 3  (4)  3 Generalized Expectation-Maximization and Wake-Sleep Maximizing log P (Y, Z|X, θ) is straightforward if both Y and Z are observed, but here we are interested in the case where Y is observed and Z is “hidden”. [sent-88, score-0.155]
</p><p>44 The log-likelihood of the observed data is given by L(θ) ≡ log P (Y |θ) = log  P (Y, Z|θ),  (5)  Z  where we have dropped X to simplify notation (all probabilities can henceforth be taken to also depend on X). [sent-90, score-0.195]
</p><p>45 The variational E-step involves minimizing D KL (Q, P ) with respect to φ, which remains positive if Q does not approximate P exactly; the variational M-step is unchanged from the standard algorithm. [sent-109, score-0.188]
</p><p>46 In the “wake” step (identical to the M-step), we ﬁt the true model parameters θ by maximizing (an approximation to) the log-probability of the observed data Y . [sent-114, score-0.251]
</p><p>47 We can therefore think of the wake phase as learning a model of the data (parametrized by θ), and the sleep phase as learning a consistent internal description of that model (parametrized by φ). [sent-116, score-0.4]
</p><p>48 In the next section we show that considerations of the spike generation process can provide us with a good choice for Q. [sent-119, score-0.26]
</p><p>49 a, Conditional model schematic, which allows zt to depend on the observed response both before and after t. [sent-121, score-0.483]
</p><p>50 b, Graphical model showing causal structure of the acausal model, with arrows indicating dependency. [sent-122, score-0.314]
</p><p>51 The observed spike responses (gray circles) are no longer dependent variables, but regarded as ﬁxed, external data, which is necessary for computing Q(zt |Y, φ). [sent-123, score-0.519]
</p><p>52 Red arrows illustrate the dependency structure for a single bin of the hidden response, z3 . [sent-124, score-0.251]
</p><p>53 , the hidden neuron spikes at time t), it is highly likely that y t+1 = 1 (i. [sent-127, score-0.469]
</p><p>54 Consequently, under the true P (Z|Y, θ), which is the probability over Z in all time bins given Y in all time bins, if y t+1 = 1 there is a high probability that zt = 1. [sent-130, score-0.291]
</p><p>55 In other words, z t exhibits an acausal dependence on y t+1 . [sent-131, score-0.238]
</p><p>56 But this acausal dependence is not captured in Equation 3, which expresses the probability over z t as depending only on past events at time t, ignoring the future event y t+1 = 1. [sent-132, score-0.237]
</p><p>57 Thus we have ˜ ˜ ˜ ˜ λzt = exp(kz · xt + hzz · z[t−τ,t) + hzy · y[t−τ,t+τ ) ). [sent-134, score-0.52]
</p><p>58 (10)  ˜ ˜ ˜ ˜ As above, kz , hzz and hzy are linear ﬁlters; the important difference is that hzy · y[t−τ,t+τ ) is a sum over past and future time: from t − τ to t + τ − ∆. [sent-135, score-0.91]
</p><p>59 For this model, the parameters are ˜ ˜ ˜ φ = (kz , hzz , hzy ). [sent-136, score-0.488]
</p><p>60 We now have a straightforward way to implement the wake-sleep algorithm, using samples from Q to perform the wake phase (estimating θ), and samples from P (Y, Z|θ) to perform the sleep phase (estimating φ). [sent-138, score-0.386]
</p><p>61 The algorithm works as follows: • Wake: Draw samples {Zi } ∼ Q(Z|Y, φ), where Y are the observed spike trains and φ is the current set of parameters for the acausal point-process model Q. [sent-139, score-0.736]
</p><p>62 5  • Sleep: Draw samples {Yj , Zj } ∼ P (Y, Z|θ), the true encoding distribution with current parameters θ. [sent-143, score-0.178]
</p><p>63 To perform a variational E-step, we draw samples (as above) from Q and use them to evaluate both the KL divergence D KL Q(Z|Y, φ)||P (Z|Y, θ) and its gradient with respect to φ. [sent-150, score-0.163]
</p><p>64 Such samples can be used to evaluate the true log-likelihood, for comparison with the variational lower bound, and for noisy gradient ascent of the likelihood to examine how closely these approximate methods converge to the true ML estimate. [sent-154, score-0.238]
</p><p>65 For fully observed data, such samples also provide a useful means for measuring how much the entropy of one neuron’s response is reduced by knowing the responses of its neighbors. [sent-155, score-0.403]
</p><p>66 5 Simulations: a two-neuron example To verify the method, we applied it to a pair of neurons (as depicted in ﬁg. [sent-156, score-0.258]
</p><p>67 1), simulated using a stimulus consisting of a long presentation of white noise. [sent-157, score-0.309]
</p><p>68 We denoted one of the neurons ”observed” and the other ”hidden”. [sent-158, score-0.228]
</p><p>69 The cells have similarly-shaped biphasic stimulus ﬁlters with opposite sign, like those commonly observed in ON and OFF retinal ganglion cells. [sent-161, score-0.507]
</p><p>70 We assume that the ON-like cell is observed, while the OFF-like cell is hidden. [sent-162, score-0.362]
</p><p>71 The hidden cell has a strong positive coupling ﬁlter h zy onto the observed cell, which allows spiking activity in the hidden cell to excite the observed cell (despite the fact that the two cells receive opposite-sign stimulus input). [sent-164, score-1.691]
</p><p>72 For simplicity, we assume no coupling from the observed to the hidden cell 2 . [sent-165, score-0.551]
</p><p>73 3b shows rasters of the two cells’ responses to a repeated presentations of a 1s Gaussian whitenoise stimulus with a framerate of 100Hz. [sent-170, score-0.43]
</p><p>74 Note that the temporal structure of the observed cell’s response is strongly correlated with that of the hidden cell due to the strong coupling from hidden to observed (and the fact that the hidden cell receives slightly stronger stimulus drive). [sent-171, score-1.49]
</p><p>75 Our ﬁrst task is to examine whether a standard, single-cell glpp model can capture the mapping from stimuli to spike responses. [sent-172, score-0.55]
</p><p>76 3c shows the parameters obtained from such a ﬁt to the observed data, using 10s of the response to a non-repeating white noise stimulus (1000 samples, 251 spikes). [sent-174, score-0.53]
</p><p>77 Note that the estimated stimulus ﬁlter (red) has much lower amplitude than the stimulus ﬁlter of the true model (gray). [sent-175, score-0.571]
</p><p>78 3d shows the parameters obtained for an observed and a hidden neuron, estimated using wake-sleep as described in section 4. [sent-177, score-0.287]
</p><p>79 3e-f shows a comparison of the performance of the two models, indicating that the coupled model estimated with wake-sleep does a much better job of capturing the temporal structure of the observed neuron’s response (accounting for 60% vs. [sent-179, score-0.41]
</p><p>80 15% of 2 Although the stimulus and spike-history ﬁlters bear a rough similarity to those observed in retinal ganglion cells, the coupling used here is unlike coupling ﬁlters observed (to our knowledge) between ON and OFF cells in retinal data; it is assumed purely for demonstration purposes. [sent-180, score-0.886]
</p><p>81 2  0 -10 -20  hzy  0 -10 -20 0 0  hyy  hzz  @ coupled-model estimate using variational EM  0. [sent-182, score-0.695]
</p><p>82 05  A  single-cell model estimate  GWN stimulus  B  hidden observed raster raster  raster comparison  coupled singletrue model cell observed  observed  2 0 -2  ky  >  ? [sent-183, score-1.451]
</p><p>83 true parameters  2 0 -2  hidden  =  psth comparison true  rate (Hz)  100  0  0. [sent-184, score-0.293]
</p><p>84 The top row shows the ﬁlters determining the input to the observed cell, while the bottom row shows those inﬂuencing the hidden cell. [sent-188, score-0.261]
</p><p>85 b, Raster of spike responses of observed and hidden cells to a repeated, 1s Gaussian white noise stimulus (top). [sent-189, score-1.015]
</p><p>86 c, Parameter estimates for a single-cell glpp model ﬁt to the observed cell’s response, using just the stimulus and observed data (estimates in red; true observedcell ﬁlters in gray). [sent-190, score-0.717]
</p><p>87 d, Parameters obtained using wake-sleep to estimate a coupled glpp model, again using only the stimulus and observed spike times. [sent-191, score-0.868]
</p><p>88 e, Response raster of true observed cell (obtained by simulating the true two-cell model), estimated single-cell model and estimated coupled model. [sent-192, score-0.574]
</p><p>89 f, Peri-stimulus time histogram (PSTH) of the above rasters showing that the coupled model gives much higher accuracy predicting the true response. [sent-193, score-0.204]
</p><p>90 The single-cell model, by contrast, exhibits much worse performance, which is unsurprising given that the standard glpp encoding model can capture only quasi-linear stimulus dependencies. [sent-195, score-0.594]
</p><p>91 6 Discussion Although most statistical models of spike trains posit a direct pathway from sensory stimuli to neuronal responses, neurons are in fact embedded in highly recurrent networks that exhibit dynamics on a broad range of time-scales. [sent-196, score-0.777]
</p><p>92 To take into account the fact that neural responses are driven by both stimuli and network activity, and to understand the role of network interactions, we proposed a model incorporating both hidden and observed spikes. [sent-197, score-0.645]
</p><p>93 We regard the observed spike responses as those recorded during a typical experiment, while the responses of unobserved neurons are modeled as latent variables (unrecorded, but exerting inﬂuence on the observed responses). [sent-198, score-1.081]
</p><p>94 The resulting model is tractable, as the latent variables can be integrated out using approximate sampling methods, and optimization using variational EM or wake-sleep provides an approximate maximum likelihood estimate of the model parameters. [sent-199, score-0.225]
</p><p>95 As shown by a simple example, certain settings of model parameters necessitate the incorporation unobserved spikes, as the standard single-stage encoding model does not accurately describe the data. [sent-200, score-0.273]
</p><p>96 The model offers a promising tool for analyzing network structure and networkbased computations carried out in higher sensory areas, particularly in the context where data are only available from a restricted set of neurons recorded within a larger population. [sent-202, score-0.369]
</p><p>97 A point process framework for relating neural spiking activity to spiking history, neural ensemble and extrinsic covariate effects. [sent-257, score-0.464]
</p><p>98 Maximum likelihood estimation of cascade point-process neural encoding models. [sent-262, score-0.216]
</p><p>99 Correlations and coding with multi-neuronal spike trains in primate retina. [sent-276, score-0.348]
</p><p>100 Analyzing functional connectivity using a network likelihood model of ensemble neural spiking activity. [sent-291, score-0.411]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spike', 0.26), ('hzy', 0.254), ('stimulus', 0.246), ('neurons', 0.228), ('zt', 0.217), ('hzz', 0.208), ('cell', 0.181), ('spikes', 0.168), ('kz', 0.165), ('spiking', 0.165), ('yt', 0.164), ('acausal', 0.162), ('glpp', 0.162), ('neuron', 0.155), ('hidden', 0.146), ('lters', 0.144), ('responses', 0.144), ('hyy', 0.139), ('hyz', 0.116), ('sleep', 0.116), ('wake', 0.116), ('observed', 0.115), ('ky', 0.113), ('coupling', 0.109), ('response', 0.105), ('kl', 0.1), ('variational', 0.094), ('trains', 0.088), ('schematic', 0.086), ('coupled', 0.085), ('raster', 0.081), ('encoding', 0.08), ('lter', 0.078), ('unobserved', 0.075), ('em', 0.069), ('history', 0.069), ('cells', 0.066), ('pillow', 0.065), ('sensory', 0.061), ('capturing', 0.059), ('xt', 0.058), ('causal', 0.056), ('psth', 0.055), ('bin', 0.055), ('stimuli', 0.052), ('pathway', 0.051), ('arrows', 0.05), ('cascade', 0.05), ('diagram', 0.048), ('neural', 0.047), ('dependence', 0.046), ('lnp', 0.046), ('tractably', 0.046), ('retinal', 0.046), ('model', 0.046), ('connectivity', 0.046), ('instantaneous', 0.046), ('nonlinearity', 0.042), ('bins', 0.041), ('log', 0.04), ('rasters', 0.04), ('refractoriness', 0.04), ('activity', 0.04), ('populations', 0.04), ('likelihood', 0.039), ('samples', 0.039), ('white', 0.038), ('phase', 0.038), ('gray', 0.038), ('neuronal', 0.037), ('receptive', 0.037), ('eden', 0.037), ('glm', 0.037), ('histories', 0.037), ('paninski', 0.034), ('ganglion', 0.034), ('functional', 0.034), ('network', 0.034), ('proposal', 0.034), ('true', 0.033), ('refractory', 0.032), ('maximizing', 0.031), ('generalized', 0.031), ('highlight', 0.031), ('dkl', 0.031), ('exhibits', 0.03), ('depicted', 0.03), ('divergence', 0.03), ('capture', 0.03), ('past', 0.029), ('parametrized', 0.028), ('drive', 0.027), ('brain', 0.027), ('understand', 0.027), ('conditional', 0.027), ('red', 0.027), ('parameters', 0.026), ('stages', 0.026), ('consisting', 0.025), ('neuroscience', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="140-tfidf-1" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>Author: Jonathan W. Pillow, Peter E. Latham</p><p>Abstract: Point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, owing in part to the fact that they do not take into account multiple stages of processing. Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more ﬂexible for ﬁtting neural data. More importantly, it allows us to estimate connectivity patterns among neurons (both observed and unobserved), and may provide insight into how networks process sensory input. We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model’s performance using a simulated example network consisting of two coupled neurons.</p><p>2 0.38082916 <a title="140-tfidf-2" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><p>3 0.28331333 <a title="140-tfidf-3" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>Author: Guenther Zeck, Matthias Bethge, Jakob H. Macke</p><p>Abstract: S timulus selectivity of sensory neurons is often characterized by estimating their receptive ﬁeld properties such as orientation selectivity. Receptive ﬁelds are usually derived from the mean (or covariance) of the spike-triggered stimulus ensemble. This approach treats each spike as an independent message but does not take into account that information might be conveyed through patterns of neural activity that are distributed across space or time. Can we ﬁnd a concise description for the processing of a whole population of neurons analogous to the receptive ﬁeld for single neurons? Here, we present a generalization of the linear receptive ﬁeld which is not bound to be triggered on individual spikes but can be meaningfully linked to distributed response patterns. More precisely, we seek to identify those stimulus features and the corresponding patterns of neural activity that are most reliably coupled. We use an extension of reverse-correlation methods based on canonical correlation analysis. The resulting population receptive ﬁelds span the subspace of stimuli that is most informative about the population response. We evaluate our approach using both neuronal models and multi-electrode recordings from rabbit retinal ganglion cells. We show how the model can be extended to capture nonlinear stimulus-response relationships using kernel canonical correlation analysis, which makes it possible to test different coding mechanisms. Our technique can also be used to calculate receptive ﬁelds from multi-dimensional neural measurements such as those obtained from dynamic imaging methods. 1</p><p>4 0.26151755 <a title="140-tfidf-4" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>Author: Lars Buesing, Wolfgang Maass</p><p>Abstract: We show that under suitable assumptions (primarily linearization) a simple and perspicuous online learning rule for Information Bottleneck optimization with spiking neurons can be derived. This rule performs on common benchmark tasks as well as a rather complex rule that has previously been proposed [1]. Furthermore, the transparency of this new learning rule makes a theoretical analysis of its convergence properties feasible. A variation of this learning rule (with sign changes) provides a theoretically founded method for performing Principal Component Analysis (PCA) with spiking neurons. By applying this rule to an ensemble of neurons, different principal components of the input can be extracted. In addition, it is possible to preferentially extract those principal components from incoming signals X that are related or are not related to some additional target signal YT . In a biological interpretation, this target signal YT (also called relevance variable) could represent proprioceptive feedback, input from other sensory modalities, or top-down signals. 1</p><p>5 0.25493959 <a title="140-tfidf-5" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>Author: Tatyana Sharpee</p><p>Abstract: This paper compares a family of methods for characterizing neural feature selectivity with natural stimuli in the framework of the linear-nonlinear model. In this model, the neural ﬁring rate is a nonlinear function of a small number of relevant stimulus components. The relevant stimulus dimensions can be found by maximizing one of the family of objective functions, R´ nyi divergences of different e orders [1, 2]. We show that maximizing one of them, R´ nyi divergence of ore der 2, is equivalent to least-square ﬁtting of the linear-nonlinear model to neural data. Next, we derive reconstruction errors in relevant dimensions found by maximizing R´ nyi divergences of arbitrary order in the asymptotic limit of large spike e numbers. We ﬁnd that the smallest errors are obtained with R´ nyi divergence of e order 1, also known as Kullback-Leibler divergence. This corresponds to ﬁnding relevant dimensions by maximizing mutual information [2]. We numerically test how these optimization schemes perform in the regime of low signal-to-noise ratio (small number of spikes and increasing neural noise) for model visual neurons. We ﬁnd that optimization schemes based on either least square ﬁtting or information maximization perform well even when number of spikes is small. Information maximization provides slightly, but signiﬁcantly, better reconstructions than least square ﬁtting. This makes the problem of ﬁnding relevant dimensions, together with the problem of lossy compression [3], one of examples where informationtheoretic measures are no more data limited than those derived from least squares. 1</p><p>6 0.22873992 <a title="140-tfidf-6" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>7 0.21198839 <a title="140-tfidf-7" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>8 0.18083164 <a title="140-tfidf-8" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>9 0.1705292 <a title="140-tfidf-9" href="./nips-2007-Contraction_Properties_of_VLSI_Cooperative_Competitive_Neural_Networks_of_Spiking_Neurons.html">60 nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</a></p>
<p>10 0.1633129 <a title="140-tfidf-10" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>11 0.1487256 <a title="140-tfidf-11" href="./nips-2007-Learning_to_classify_complex_patterns_using_a_VLSI_network_of_spiking_neurons.html">117 nips-2007-Learning to classify complex patterns using a VLSI network of spiking neurons</a></p>
<p>12 0.12787089 <a title="140-tfidf-12" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>13 0.10394513 <a title="140-tfidf-13" href="./nips-2007-Estimating_disparity_with_confidence_from_energy_neurons.html">81 nips-2007-Estimating disparity with confidence from energy neurons</a></p>
<p>14 0.10128959 <a title="140-tfidf-14" href="./nips-2007-A_configurable_analog_VLSI_neural_network_with_spiking_neurons_and_self-regulating_plastic_synapses.html">14 nips-2007-A configurable analog VLSI neural network with spiking neurons and self-regulating plastic synapses</a></p>
<p>15 0.091391161 <a title="140-tfidf-15" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<p>16 0.085001536 <a title="140-tfidf-16" href="./nips-2007-Convex_Relaxations_of_Latent_Variable_Training.html">63 nips-2007-Convex Relaxations of Latent Variable Training</a></p>
<p>17 0.084881149 <a title="140-tfidf-17" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>18 0.081543542 <a title="140-tfidf-18" href="./nips-2007-Variational_Inference_for_Diffusion_Processes.html">213 nips-2007-Variational Inference for Diffusion Processes</a></p>
<p>19 0.081192903 <a title="140-tfidf-19" href="./nips-2007-Inferring_Elapsed_Time_from_Stochastic_Neural_Processes.html">103 nips-2007-Inferring Elapsed Time from Stochastic Neural Processes</a></p>
<p>20 0.079580233 <a title="140-tfidf-20" href="./nips-2007-On_higher-order_perceptron_algorithms.html">146 nips-2007-On higher-order perceptron algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.273), (1, 0.194), (2, 0.474), (3, 0.038), (4, 0.04), (5, -0.122), (6, 0.029), (7, -0.021), (8, 0.006), (9, -0.013), (10, 0.075), (11, 0.029), (12, 0.035), (13, 0.002), (14, 0.027), (15, 0.024), (16, 0.05), (17, 0.135), (18, 0.139), (19, 0.171), (20, -0.001), (21, -0.03), (22, -0.058), (23, 0.126), (24, 0.014), (25, -0.007), (26, 0.076), (27, -0.017), (28, 0.017), (29, 0.043), (30, -0.063), (31, 0.0), (32, -0.08), (33, -0.043), (34, 0.02), (35, 0.04), (36, -0.005), (37, 0.093), (38, -0.03), (39, 0.048), (40, 0.046), (41, -0.024), (42, 0.024), (43, -0.003), (44, -0.021), (45, -0.027), (46, 0.065), (47, -0.017), (48, 0.053), (49, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96597826 <a title="140-lsi-1" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>Author: Jonathan W. Pillow, Peter E. Latham</p><p>Abstract: Point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, owing in part to the fact that they do not take into account multiple stages of processing. Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more ﬂexible for ﬁtting neural data. More importantly, it allows us to estimate connectivity patterns among neurons (both observed and unobserved), and may provide insight into how networks process sensory input. We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model’s performance using a simulated example network consisting of two coupled neurons.</p><p>2 0.8906607 <a title="140-lsi-2" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><p>3 0.85582495 <a title="140-lsi-3" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>Author: Guenther Zeck, Matthias Bethge, Jakob H. Macke</p><p>Abstract: S timulus selectivity of sensory neurons is often characterized by estimating their receptive ﬁeld properties such as orientation selectivity. Receptive ﬁelds are usually derived from the mean (or covariance) of the spike-triggered stimulus ensemble. This approach treats each spike as an independent message but does not take into account that information might be conveyed through patterns of neural activity that are distributed across space or time. Can we ﬁnd a concise description for the processing of a whole population of neurons analogous to the receptive ﬁeld for single neurons? Here, we present a generalization of the linear receptive ﬁeld which is not bound to be triggered on individual spikes but can be meaningfully linked to distributed response patterns. More precisely, we seek to identify those stimulus features and the corresponding patterns of neural activity that are most reliably coupled. We use an extension of reverse-correlation methods based on canonical correlation analysis. The resulting population receptive ﬁelds span the subspace of stimuli that is most informative about the population response. We evaluate our approach using both neuronal models and multi-electrode recordings from rabbit retinal ganglion cells. We show how the model can be extended to capture nonlinear stimulus-response relationships using kernel canonical correlation analysis, which makes it possible to test different coding mechanisms. Our technique can also be used to calculate receptive ﬁelds from multi-dimensional neural measurements such as those obtained from dynamic imaging methods. 1</p><p>4 0.82038981 <a title="140-lsi-4" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>Author: Tatyana Sharpee</p><p>Abstract: This paper compares a family of methods for characterizing neural feature selectivity with natural stimuli in the framework of the linear-nonlinear model. In this model, the neural ﬁring rate is a nonlinear function of a small number of relevant stimulus components. The relevant stimulus dimensions can be found by maximizing one of the family of objective functions, R´ nyi divergences of different e orders [1, 2]. We show that maximizing one of them, R´ nyi divergence of ore der 2, is equivalent to least-square ﬁtting of the linear-nonlinear model to neural data. Next, we derive reconstruction errors in relevant dimensions found by maximizing R´ nyi divergences of arbitrary order in the asymptotic limit of large spike e numbers. We ﬁnd that the smallest errors are obtained with R´ nyi divergence of e order 1, also known as Kullback-Leibler divergence. This corresponds to ﬁnding relevant dimensions by maximizing mutual information [2]. We numerically test how these optimization schemes perform in the regime of low signal-to-noise ratio (small number of spikes and increasing neural noise) for model visual neurons. We ﬁnd that optimization schemes based on either least square ﬁtting or information maximization perform well even when number of spikes is small. Information maximization provides slightly, but signiﬁcantly, better reconstructions than least square ﬁtting. This makes the problem of ﬁnding relevant dimensions, together with the problem of lossy compression [3], one of examples where informationtheoretic measures are no more data limited than those derived from least squares. 1</p><p>5 0.64058352 <a title="140-lsi-5" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>Author: Lars Buesing, Wolfgang Maass</p><p>Abstract: We show that under suitable assumptions (primarily linearization) a simple and perspicuous online learning rule for Information Bottleneck optimization with spiking neurons can be derived. This rule performs on common benchmark tasks as well as a rather complex rule that has previously been proposed [1]. Furthermore, the transparency of this new learning rule makes a theoretical analysis of its convergence properties feasible. A variation of this learning rule (with sign changes) provides a theoretically founded method for performing Principal Component Analysis (PCA) with spiking neurons. By applying this rule to an ensemble of neurons, different principal components of the input can be extracted. In addition, it is possible to preferentially extract those principal components from incoming signals X that are related or are not related to some additional target signal YT . In a biological interpretation, this target signal YT (also called relevance variable) could represent proprioceptive feedback, input from other sensory modalities, or top-down signals. 1</p><p>6 0.62921554 <a title="140-lsi-6" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>7 0.62555748 <a title="140-lsi-7" href="./nips-2007-Contraction_Properties_of_VLSI_Cooperative_Competitive_Neural_Networks_of_Spiking_Neurons.html">60 nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</a></p>
<p>8 0.61754674 <a title="140-lsi-8" href="./nips-2007-Estimating_disparity_with_confidence_from_energy_neurons.html">81 nips-2007-Estimating disparity with confidence from energy neurons</a></p>
<p>9 0.58682358 <a title="140-lsi-9" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>10 0.58329761 <a title="140-lsi-10" href="./nips-2007-A_neural_network_implementing_optimal_state_estimation_based_on_dynamic_spike_train_decoding.html">17 nips-2007-A neural network implementing optimal state estimation based on dynamic spike train decoding</a></p>
<p>11 0.56833911 <a title="140-lsi-11" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>12 0.44618869 <a title="140-lsi-12" href="./nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">182 nips-2007-Sparse deep belief net model for visual area V2</a></p>
<p>13 0.40393272 <a title="140-lsi-13" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<p>14 0.39744517 <a title="140-lsi-14" href="./nips-2007-Learning_to_classify_complex_patterns_using_a_VLSI_network_of_spiking_neurons.html">117 nips-2007-Learning to classify complex patterns using a VLSI network of spiking neurons</a></p>
<p>15 0.37599421 <a title="140-lsi-15" href="./nips-2007-An_in-silico_Neural_Model_of_Dynamic_Routing_through_Neuronal_Coherence.html">25 nips-2007-An in-silico Neural Model of Dynamic Routing through Neuronal Coherence</a></p>
<p>16 0.37392691 <a title="140-lsi-16" href="./nips-2007-An_online_Hebbian_learning_rule_that_performs_Independent_Component_Analysis.html">26 nips-2007-An online Hebbian learning rule that performs Independent Component Analysis</a></p>
<p>17 0.36922035 <a title="140-lsi-17" href="./nips-2007-Modeling_Natural_Sounds_with_Modulation_Cascade_Processes.html">130 nips-2007-Modeling Natural Sounds with Modulation Cascade Processes</a></p>
<p>18 0.31703448 <a title="140-lsi-18" href="./nips-2007-Augmented_Functional_Time_Series_Representation_and_Forecasting_with_Gaussian_Processes.html">28 nips-2007-Augmented Functional Time Series Representation and Forecasting with Gaussian Processes</a></p>
<p>19 0.3014425 <a title="140-lsi-19" href="./nips-2007-Learning_Horizontal_Connections_in_a_Sparse_Coding_Model_of_Natural_Images.html">111 nips-2007-Learning Horizontal Connections in a Sparse Coding Model of Natural Images</a></p>
<p>20 0.29091999 <a title="140-lsi-20" href="./nips-2007-The_rat_as_particle_filter.html">203 nips-2007-The rat as particle filter</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.064), (13, 0.036), (16, 0.185), (18, 0.024), (19, 0.013), (21, 0.056), (34, 0.034), (35, 0.03), (46, 0.2), (47, 0.071), (83, 0.096), (85, 0.012), (87, 0.016), (90, 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83853221 <a title="140-lda-1" href="./nips-2007-Estimating_disparity_with_confidence_from_energy_neurons.html">81 nips-2007-Estimating disparity with confidence from energy neurons</a></p>
<p>Author: Eric K. Tsang, Bertram E. Shi</p><p>Abstract: The peak location in a population of phase-tuned neurons has been shown to be a more reliable estimator for disparity than the peak location in a population of position-tuned neurons. Unfortunately, the disparity range covered by a phasetuned population is limited by phase wraparound. Thus, a single population cannot cover the large range of disparities encountered in natural scenes unless the scale of the receptive fields is chosen to be very large, which results in very low resolution depth estimates. Here we describe a biologically plausible measure of the confidence that the stimulus disparity is inside the range covered by a population of phase-tuned neurons. Based upon this confidence measure, we propose an algorithm for disparity estimation that uses many populations of high-resolution phase-tuned neurons that are biased to different disparity ranges via position shifts between the left and right eye receptive fields. The population with the highest confidence is used to estimate the stimulus disparity. We show that this algorithm outperforms a previously proposed coarse-to-fine algorithm for disparity estimation, which uses disparity estimates from coarse scales to select the populations used at finer scales and can effectively detect occlusions.</p><p>same-paper 2 0.82993197 <a title="140-lda-2" href="./nips-2007-Neural_characterization_in_partially_observed_populations_of_spiking_neurons.html">140 nips-2007-Neural characterization in partially observed populations of spiking neurons</a></p>
<p>Author: Jonathan W. Pillow, Peter E. Latham</p><p>Abstract: Point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, owing in part to the fact that they do not take into account multiple stages of processing. Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more ﬂexible for ﬁtting neural data. More importantly, it allows us to estimate connectivity patterns among neurons (both observed and unobserved), and may provide insight into how networks process sensory input. We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model’s performance using a simulated example network consisting of two coupled neurons.</p><p>3 0.75873417 <a title="140-lda-3" href="./nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</a></p>
<p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><p>4 0.75390357 <a title="140-lda-4" href="./nips-2007-Robust_Regression_with_Twinned_Gaussian_Processes.html">170 nips-2007-Robust Regression with Twinned Gaussian Processes</a></p>
<p>Author: Andrew Naish-guzman, Sean Holden</p><p>Abstract: We propose a Gaussian process (GP) framework for robust inference in which a GP prior on the mixing weights of a two-component noise model augments the standard process over latent function values. This approach is a generalization of the mixture likelihood used in traditional robust GP regression, and a specialization of the GP mixture models suggested by Tresp [1] and Rasmussen and Ghahramani [2]. The value of this restriction is in its tractable expectation propagation updates, which allow for faster inference and model selection, and better convergence than the standard mixture. An additional beneﬁt over the latter method lies in our ability to incorporate knowledge of the noise domain to inﬂuence predictions, and to recover with the predictive distribution information about the outlier distribution via the gating process. The model has asymptotic complexity equal to that of conventional robust methods, but yields more conﬁdent predictions on benchmark problems than classical heavy-tailed models and exhibits improved stability for data with clustered corruptions, for which they fail altogether. We show further how our approach can be used without adjustment for more smoothly heteroscedastic data, and suggest how it could be extended to more general noise models. We also address similarities with the work of Goldberg et al. [3].</p><p>5 0.7536599 <a title="140-lda-5" href="./nips-2007-Kernel_Measures_of_Conditional_Dependence.html">108 nips-2007-Kernel Measures of Conditional Dependence</a></p>
<p>Author: Kenji Fukumizu, Arthur Gretton, Xiaohai Sun, Bernhard Schölkopf</p><p>Abstract: We propose a new measure of conditional dependence of random variables, based on normalized cross-covariance operators on reproducing kernel Hilbert spaces. Unlike previous kernel dependence measures, the proposed criterion does not depend on the choice of kernel in the limit of inﬁnite data, for a wide class of kernels. At the same time, it has a straightforward empirical estimate with good convergence behaviour. We discuss the theoretical properties of the measure, and demonstrate its application in experiments. 1</p><p>6 0.7536459 <a title="140-lda-6" href="./nips-2007-Contraction_Properties_of_VLSI_Cooperative_Competitive_Neural_Networks_of_Spiking_Neurons.html">60 nips-2007-Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons</a></p>
<p>7 0.74630916 <a title="140-lda-7" href="./nips-2007-Topmoumoute_Online_Natural_Gradient_Algorithm.html">206 nips-2007-Topmoumoute Online Natural Gradient Algorithm</a></p>
<p>8 0.73933423 <a title="140-lda-8" href="./nips-2007-The_Price_of_Bandit_Information_for_Online_Optimization.html">199 nips-2007-The Price of Bandit Information for Online Optimization</a></p>
<p>9 0.6880787 <a title="140-lda-9" href="./nips-2007-Better_than_least_squares%3A_comparison_of_objective_functions_for_estimating_linear-nonlinear_models.html">36 nips-2007-Better than least squares: comparison of objective functions for estimating linear-nonlinear models</a></p>
<p>10 0.68310398 <a title="140-lda-10" href="./nips-2007-Inferring_Neural_Firing_Rates_from_Spike_Trains_Using_Gaussian_Processes.html">104 nips-2007-Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes</a></p>
<p>11 0.65729427 <a title="140-lda-11" href="./nips-2007-The_Generalized_FITC_Approximation.html">195 nips-2007-The Generalized FITC Approximation</a></p>
<p>12 0.6448822 <a title="140-lda-12" href="./nips-2007-Receptive_Fields_without_Spike-Triggering.html">164 nips-2007-Receptive Fields without Spike-Triggering</a></p>
<p>13 0.63318712 <a title="140-lda-13" href="./nips-2007-Supervised_Topic_Models.html">189 nips-2007-Supervised Topic Models</a></p>
<p>14 0.6133796 <a title="140-lda-14" href="./nips-2007-Theoretical_Analysis_of_Learning_with_Reward-Modulated_Spike-Timing-Dependent_Plasticity.html">205 nips-2007-Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity</a></p>
<p>15 0.60181898 <a title="140-lda-15" href="./nips-2007-Efficient_multiple_hyperparameter_learning_for_log-linear_models.html">79 nips-2007-Efficient multiple hyperparameter learning for log-linear models</a></p>
<p>16 0.59409887 <a title="140-lda-16" href="./nips-2007-Simplified_Rules_and_Theoretical_Analysis_for_Information_Bottleneck_Optimization_and_PCA_with_Spiking_Neurons.html">177 nips-2007-Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons</a></p>
<p>17 0.59196895 <a title="140-lda-17" href="./nips-2007-Fast_Variational_Inference_for_Large-scale_Internet_Diagnosis.html">87 nips-2007-Fast Variational Inference for Large-scale Internet Diagnosis</a></p>
<p>18 0.58991307 <a title="140-lda-18" href="./nips-2007-Bayesian_binning_beats_approximate_alternatives%3A_estimating_peri-stimulus_time_histograms.html">35 nips-2007-Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</a></p>
<p>19 0.58582038 <a title="140-lda-19" href="./nips-2007-Learning_to_classify_complex_patterns_using_a_VLSI_network_of_spiking_neurons.html">117 nips-2007-Learning to classify complex patterns using a VLSI network of spiking neurons</a></p>
<p>20 0.58421427 <a title="140-lda-20" href="./nips-2007-Near-Maximum_Entropy_Models_for_Binary_Neural_Representations_of_Natural_Images.html">138 nips-2007-Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
