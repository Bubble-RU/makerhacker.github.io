<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>58 nips-2005-Divergences, surrogate loss functions and experimental design</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-58" href="#">nips2005-58</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>58 nips-2005-Divergences, surrogate loss functions and experimental design</h1>
<br/><p>Source: <a title="nips-2005-58-pdf" href="http://papers.nips.cc/paper/2905-divergences-surrogate-loss-functions-and-experimental-design.pdf">pdf</a></p><p>Author: Xuanlong Nguyen, Martin J. Wainwright, Michael I. Jordan</p><p>Abstract: In this paper, we provide a general theorem that establishes a correspondence between surrogate loss functions in classiﬁcation and the family of f -divergences. Moreover, we provide constructive procedures for determining the f -divergence induced by a given surrogate loss, and conversely for ﬁnding all surrogate loss functions that realize a given f -divergence. Next we introduce the notion of universal equivalence among loss functions and corresponding f -divergences, and provide necessary and sufﬁcient conditions for universal equivalence to hold. These ideas have applications to classiﬁcation problems that also involve a component of experiment design; in particular, we leverage our results to prove consistency of a procedure for learning a classiﬁer under decentralization requirements. 1</p><p>Reference: <a title="nips-2005-58-reference" href="../nips2005_reference/nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('surrog', 0.557), ('rbay', 0.432), ('loss', 0.364), ('cn', 0.196), ('risk', 0.179), ('convex', 0.169), ('fvar', 0.162), ('hel', 0.151), ('dn', 0.134), ('kl', 0.133), ('diverg', 0.129), ('lemm', 0.094), ('bay', 0.094), ('theorem', 0.091), ('dec', 0.086), ('nguy', 0.086), ('hing', 0.077), ('inf', 0.075), ('dist', 0.074), ('au', 0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="58-tfidf-1" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>2 0.32897371 <a title="58-tfidf-2" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>3 0.19571301 <a title="58-tfidf-3" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>4 0.14946985 <a title="58-tfidf-4" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>5 0.14481263 <a title="58-tfidf-5" href="./nips-2005-Improved_risk_tail_bounds_for_on-line_algorithms.html">95 nips-2005-Improved risk tail bounds for on-line algorithms</a></p>
<p>6 0.14071672 <a title="58-tfidf-6" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>7 0.13676603 <a title="58-tfidf-7" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>8 0.13546257 <a title="58-tfidf-8" href="./nips-2005-Variational_EM_Algorithms_for_Non-Gaussian_Latent_Variable_Models.html">202 nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</a></p>
<p>9 0.12144087 <a title="58-tfidf-9" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>10 0.10807259 <a title="58-tfidf-10" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>11 0.10750058 <a title="58-tfidf-11" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>12 0.085661866 <a title="58-tfidf-12" href="./nips-2005-TD%280%29_Leads_to_Better_Policies_than_Approximate_Value_Iteration.html">186 nips-2005-TD(0) Leads to Better Policies than Approximate Value Iteration</a></p>
<p>13 0.084289536 <a title="58-tfidf-13" href="./nips-2005-A_Bayes_Rule_for_Density_Matrices.html">2 nips-2005-A Bayes Rule for Density Matrices</a></p>
<p>14 0.083660603 <a title="58-tfidf-14" href="./nips-2005-Nonparametric_inference_of_prior_probabilities_from_Bayes-optimal_behavior.html">140 nips-2005-Nonparametric inference of prior probabilities from Bayes-optimal behavior</a></p>
<p>15 0.080881998 <a title="58-tfidf-15" href="./nips-2005-Maximum_Margin_Semi-Supervised_Learning_for_Structured_Variables.html">123 nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</a></p>
<p>16 0.079846397 <a title="58-tfidf-16" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>17 0.076544285 <a title="58-tfidf-17" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>18 0.075682744 <a title="58-tfidf-18" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>19 0.07499668 <a title="58-tfidf-19" href="./nips-2005-Metric_Learning_by_Collapsing_Classes.html">126 nips-2005-Metric Learning by Collapsing Classes</a></p>
<p>20 0.073298737 <a title="58-tfidf-20" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.197), (1, -0.116), (2, -0.053), (3, 0.119), (4, -0.207), (5, 0.001), (6, -0.185), (7, -0.038), (8, 0.158), (9, -0.037), (10, -0.281), (11, 0.034), (12, 0.17), (13, 0.011), (14, -0.112), (15, -0.143), (16, -0.07), (17, -0.048), (18, -0.117), (19, -0.03), (20, 0.114), (21, -0.076), (22, 0.021), (23, 0.142), (24, -0.102), (25, -0.07), (26, 0.014), (27, 0.002), (28, 0.068), (29, 0.036), (30, -0.021), (31, -0.003), (32, -0.033), (33, -0.034), (34, 0.088), (35, 0.125), (36, 0.065), (37, 0.058), (38, -0.01), (39, 0.036), (40, -0.027), (41, -0.114), (42, -0.005), (43, 0.073), (44, -0.01), (45, -0.076), (46, -0.034), (47, -0.091), (48, 0.058), (49, -0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92669135 <a title="58-lsi-1" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>2 0.77614796 <a title="58-lsi-2" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>3 0.6303525 <a title="58-lsi-3" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>4 0.60478061 <a title="58-lsi-4" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>5 0.55240154 <a title="58-lsi-5" href="./nips-2005-Improved_risk_tail_bounds_for_on-line_algorithms.html">95 nips-2005-Improved risk tail bounds for on-line algorithms</a></p>
<p>6 0.47489893 <a title="58-lsi-6" href="./nips-2005-Variational_EM_Algorithms_for_Non-Gaussian_Latent_Variable_Models.html">202 nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</a></p>
<p>7 0.4630104 <a title="58-lsi-7" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>8 0.44816971 <a title="58-lsi-8" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>9 0.44777519 <a title="58-lsi-9" href="./nips-2005-Learning_Minimum_Volume_Sets.html">112 nips-2005-Learning Minimum Volume Sets</a></p>
<p>10 0.44366452 <a title="58-lsi-10" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>11 0.4358837 <a title="58-lsi-11" href="./nips-2005-Using_%60%60epitomes%27%27_to_model_genetic_diversity%3A_Rational_design_of_HIV_vaccine_cocktails.html">198 nips-2005-Using ``epitomes'' to model genetic diversity: Rational design of HIV vaccine cocktails</a></p>
<p>12 0.36071 <a title="58-lsi-12" href="./nips-2005-TD%280%29_Leads_to_Better_Policies_than_Approximate_Value_Iteration.html">186 nips-2005-TD(0) Leads to Better Policies than Approximate Value Iteration</a></p>
<p>13 0.33296961 <a title="58-lsi-13" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>14 0.31322873 <a title="58-lsi-14" href="./nips-2005-Metric_Learning_by_Collapsing_Classes.html">126 nips-2005-Metric Learning by Collapsing Classes</a></p>
<p>15 0.31092748 <a title="58-lsi-15" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>16 0.30114764 <a title="58-lsi-16" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>17 0.29618585 <a title="58-lsi-17" href="./nips-2005-Robust_Fisher_Discriminant_Analysis.html">166 nips-2005-Robust Fisher Discriminant Analysis</a></p>
<p>18 0.29516274 <a title="58-lsi-18" href="./nips-2005-Nonparametric_inference_of_prior_probabilities_from_Bayes-optimal_behavior.html">140 nips-2005-Nonparametric inference of prior probabilities from Bayes-optimal behavior</a></p>
<p>19 0.29431212 <a title="58-lsi-19" href="./nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<p>20 0.29309937 <a title="58-lsi-20" href="./nips-2005-Generalized_Nonnegative_Matrix_Approximations_with_Bregman_Divergences.html">86 nips-2005-Generalized Nonnegative Matrix Approximations with Bregman Divergences</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.045), (12, 0.018), (17, 0.014), (18, 0.011), (45, 0.225), (54, 0.137), (71, 0.143), (88, 0.101), (89, 0.078), (92, 0.124)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79371595 <a title="58-lda-1" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>2 0.7480197 <a title="58-lda-2" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>3 0.72071904 <a title="58-lda-3" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>4 0.71818125 <a title="58-lda-4" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>5 0.69434971 <a title="58-lda-5" href="./nips-2005-Improved_risk_tail_bounds_for_on-line_algorithms.html">95 nips-2005-Improved risk tail bounds for on-line algorithms</a></p>
<p>6 0.69271195 <a title="58-lda-6" href="./nips-2005-Sensory_Adaptation_within_a_Bayesian_Framework_for_Perception.html">173 nips-2005-Sensory Adaptation within a Bayesian Framework for Perception</a></p>
<p>7 0.68838722 <a title="58-lda-7" href="./nips-2005-Rodeo%3A_Sparse_Nonparametric_Regression_in_High_Dimensions.html">168 nips-2005-Rodeo: Sparse Nonparametric Regression in High Dimensions</a></p>
<p>8 0.6881181 <a title="58-lda-8" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>9 0.68695092 <a title="58-lda-9" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>10 0.68668276 <a title="58-lda-10" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>11 0.68653494 <a title="58-lda-11" href="./nips-2005-Learning_Minimum_Volume_Sets.html">112 nips-2005-Learning Minimum Volume Sets</a></p>
<p>12 0.68511057 <a title="58-lda-12" href="./nips-2005-Computing_the_Solution_Path_for_the_Regularized_Support_Vector_Regression.html">44 nips-2005-Computing the Solution Path for the Regularized Support Vector Regression</a></p>
<p>13 0.68453628 <a title="58-lda-13" href="./nips-2005-Beyond_Gaussian_Processes%3A_On_the_Distributions_of_Infinite_Networks.html">38 nips-2005-Beyond Gaussian Processes: On the Distributions of Infinite Networks</a></p>
<p>14 0.68129593 <a title="58-lda-14" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>15 0.68078291 <a title="58-lda-15" href="./nips-2005-Variational_Bayesian_Stochastic_Complexity_of_Mixture_Models.html">201 nips-2005-Variational Bayesian Stochastic Complexity of Mixture Models</a></p>
<p>16 0.68013734 <a title="58-lda-16" href="./nips-2005-A_Bayes_Rule_for_Density_Matrices.html">2 nips-2005-A Bayes Rule for Density Matrices</a></p>
<p>17 0.67952323 <a title="58-lda-17" href="./nips-2005-Worst-Case_Bounds_for_Gaussian_Process_Models.html">205 nips-2005-Worst-Case Bounds for Gaussian Process Models</a></p>
<p>18 0.67867613 <a title="58-lda-18" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>19 0.67810142 <a title="58-lda-19" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>20 0.6780166 <a title="58-lda-20" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
