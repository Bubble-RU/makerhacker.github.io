<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>145 nips-2005-On Local Rewards and Scaling Distributed Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-145" href="#">nips2005-145</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>145 nips-2005-On Local Rewards and Scaling Distributed Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2005-145-pdf" href="http://papers.nips.cc/paper/2951-on-local-rewards-and-scaling-distributed-reinforcement-learning.pdf">pdf</a></p><p>Author: Drew Bagnell, Andrew Y. Ng</p><p>Abstract: We consider the scaling of the number of examples necessary to achieve good performance in distributed, cooperative, multi-agent reinforcement learning, as a function of the the number of agents n. We prove a worstcase lower bound showing that algorithms that rely solely on a global reward signal to learn policies confront a fundamental limit: They require a number of real-world examples that scales roughly linearly in the number of agents. For settings of interest with a very large number of agents, this is impractical. We demonstrate, however, that there is a class of algorithms that, by taking advantage of local reward signals in large distributed Markov Decision Processes, are able to ensure good performance with a number of samples that scales as O(log n). This makes them applicable even in settings with a very large number of agents n. 1</p><p>Reference: <a title="nips-2005-145-reference" href="../nips2005_reference/nips-2005-On_Local_Rewards_and_Scaling_Distributed_Reinforcement_Learning_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('neigh', 0.565), ('reward', 0.497), ('policy', 0.315), ('ag', 0.292), ('st', 0.198), ('mdp', 0.18), ('um', 0.157), ('cpt', 0.123), ('dbn', 0.112), ('cpts', 0.106), ('glob', 0.096), ('einforc', 0.088), ('sm', 0.078), ('reinforc', 0.078), ('backup', 0.071), ('bi', 0.068), ('tri', 0.062), ('ep', 0.057), ('traf', 0.056), ('loc', 0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="145-tfidf-1" href="./nips-2005-On_Local_Rewards_and_Scaling_Distributed_Reinforcement_Learning.html">145 nips-2005-On Local Rewards and Scaling Distributed Reinforcement Learning</a></p>
<p>2 0.44250664 <a title="145-tfidf-2" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>3 0.35900837 <a title="145-tfidf-3" href="./nips-2005-Policy-Gradient_Methods_for_Planning.html">153 nips-2005-Policy-Gradient Methods for Planning</a></p>
<p>4 0.2323719 <a title="145-tfidf-4" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>5 0.22740605 <a title="145-tfidf-5" href="./nips-2005-Bayesian_models_of_human_action_understanding.html">36 nips-2005-Bayesian models of human action understanding</a></p>
<p>6 0.22214776 <a title="145-tfidf-6" href="./nips-2005-How_fast_to_work%3A_Response_vigor%2C_motivation_and_tonic_dopamine.html">91 nips-2005-How fast to work: Response vigor, motivation and tonic dopamine</a></p>
<p>7 0.20514606 <a title="145-tfidf-7" href="./nips-2005-TD%280%29_Leads_to_Better_Policies_than_Approximate_Value_Iteration.html">186 nips-2005-TD(0) Leads to Better Policies than Approximate Value Iteration</a></p>
<p>8 0.20184958 <a title="145-tfidf-8" href="./nips-2005-Goal-Based_Imitation_as_Probabilistic_Inference_over_Graphical_Models.html">87 nips-2005-Goal-Based Imitation as Probabilistic Inference over Graphical Models</a></p>
<p>9 0.17305863 <a title="145-tfidf-9" href="./nips-2005-An_exploration-exploitation_model_based_on_norepinepherine_and_dopamine_activity.html">26 nips-2005-An exploration-exploitation model based on norepinepherine and dopamine activity</a></p>
<p>10 0.15183911 <a title="145-tfidf-10" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>11 0.14524727 <a title="145-tfidf-11" href="./nips-2005-Temporal_Abstraction_in_Temporal-difference_Networks.html">187 nips-2005-Temporal Abstraction in Temporal-difference Networks</a></p>
<p>12 0.13801938 <a title="145-tfidf-12" href="./nips-2005-Oblivious_Equilibrium%3A_A_Mean_Field_Approximation_for_Large-Scale_Dynamic_Games.html">142 nips-2005-Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games</a></p>
<p>13 0.12315561 <a title="145-tfidf-13" href="./nips-2005-Value_Function_Approximation_with_Diffusion_Wavelets_and_Laplacian_Eigenfunctions.html">199 nips-2005-Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions</a></p>
<p>14 0.1178544 <a title="145-tfidf-14" href="./nips-2005-Learning_Influence_among_Interacting_Markov_Chains.html">111 nips-2005-Learning Influence among Interacting Markov Chains</a></p>
<p>15 0.10582141 <a title="145-tfidf-15" href="./nips-2005-Modeling_Neuronal_Interactivity_using_Dynamic_Bayesian_Networks.html">130 nips-2005-Modeling Neuronal Interactivity using Dynamic Bayesian Networks</a></p>
<p>16 0.10370269 <a title="145-tfidf-16" href="./nips-2005-Cyclic_Equilibria_in_Markov_Games.html">53 nips-2005-Cyclic Equilibria in Markov Games</a></p>
<p>17 0.09272103 <a title="145-tfidf-17" href="./nips-2005-Learning_to_Control_an_Octopus_Arm_with_Gaussian_Process_Temporal_Difference_Methods.html">119 nips-2005-Learning to Control an Octopus Arm with Gaussian Process Temporal Difference Methods</a></p>
<p>18 0.055023815 <a title="145-tfidf-18" href="./nips-2005-Norepinephrine_and_Neural_Interrupts.html">141 nips-2005-Norepinephrine and Neural Interrupts</a></p>
<p>19 0.053360619 <a title="145-tfidf-19" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>20 0.046240348 <a title="145-tfidf-20" href="./nips-2005-Online_Discovery_and_Learning_of_Predictive_State_Representations.html">148 nips-2005-Online Discovery and Learning of Predictive State Representations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.175), (1, -0.044), (2, 0.566), (3, 0.212), (4, 0.096), (5, -0.097), (6, 0.007), (7, 0.028), (8, 0.0), (9, 0.031), (10, -0.025), (11, 0.114), (12, -0.029), (13, 0.05), (14, -0.041), (15, -0.004), (16, -0.033), (17, 0.002), (18, 0.044), (19, -0.006), (20, -0.021), (21, -0.025), (22, 0.037), (23, 0.018), (24, 0.023), (25, -0.034), (26, -0.018), (27, -0.084), (28, -0.053), (29, 0.087), (30, 0.045), (31, 0.026), (32, -0.158), (33, 0.1), (34, -0.006), (35, -0.001), (36, -0.029), (37, 0.029), (38, 0.026), (39, 0.09), (40, 0.011), (41, 0.016), (42, -0.034), (43, -0.007), (44, 0.049), (45, 0.013), (46, -0.016), (47, -0.021), (48, 0.083), (49, -0.0)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94639784 <a title="145-lsi-1" href="./nips-2005-On_Local_Rewards_and_Scaling_Distributed_Reinforcement_Learning.html">145 nips-2005-On Local Rewards and Scaling Distributed Reinforcement Learning</a></p>
<p>2 0.77759695 <a title="145-lsi-2" href="./nips-2005-Policy-Gradient_Methods_for_Planning.html">153 nips-2005-Policy-Gradient Methods for Planning</a></p>
<p>3 0.76780897 <a title="145-lsi-3" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>4 0.70740479 <a title="145-lsi-4" href="./nips-2005-How_fast_to_work%3A_Response_vigor%2C_motivation_and_tonic_dopamine.html">91 nips-2005-How fast to work: Response vigor, motivation and tonic dopamine</a></p>
<p>5 0.5939365 <a title="145-lsi-5" href="./nips-2005-Goal-Based_Imitation_as_Probabilistic_Inference_over_Graphical_Models.html">87 nips-2005-Goal-Based Imitation as Probabilistic Inference over Graphical Models</a></p>
<p>6 0.58205318 <a title="145-lsi-6" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>7 0.58119416 <a title="145-lsi-7" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>8 0.56310546 <a title="145-lsi-8" href="./nips-2005-Learning_to_Control_an_Octopus_Arm_with_Gaussian_Process_Temporal_Difference_Methods.html">119 nips-2005-Learning to Control an Octopus Arm with Gaussian Process Temporal Difference Methods</a></p>
<p>9 0.53077018 <a title="145-lsi-9" href="./nips-2005-An_exploration-exploitation_model_based_on_norepinepherine_and_dopamine_activity.html">26 nips-2005-An exploration-exploitation model based on norepinepherine and dopamine activity</a></p>
<p>10 0.48126596 <a title="145-lsi-10" href="./nips-2005-Bayesian_models_of_human_action_understanding.html">36 nips-2005-Bayesian models of human action understanding</a></p>
<p>11 0.46574906 <a title="145-lsi-11" href="./nips-2005-TD%280%29_Leads_to_Better_Policies_than_Approximate_Value_Iteration.html">186 nips-2005-TD(0) Leads to Better Policies than Approximate Value Iteration</a></p>
<p>12 0.42664713 <a title="145-lsi-12" href="./nips-2005-Cyclic_Equilibria_in_Markov_Games.html">53 nips-2005-Cyclic Equilibria in Markov Games</a></p>
<p>13 0.41743365 <a title="145-lsi-13" href="./nips-2005-Oblivious_Equilibrium%3A_A_Mean_Field_Approximation_for_Large-Scale_Dynamic_Games.html">142 nips-2005-Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games</a></p>
<p>14 0.39630586 <a title="145-lsi-14" href="./nips-2005-Temporal_Abstraction_in_Temporal-difference_Networks.html">187 nips-2005-Temporal Abstraction in Temporal-difference Networks</a></p>
<p>15 0.30559444 <a title="145-lsi-15" href="./nips-2005-Value_Function_Approximation_with_Diffusion_Wavelets_and_Laplacian_Eigenfunctions.html">199 nips-2005-Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions</a></p>
<p>16 0.29964539 <a title="145-lsi-16" href="./nips-2005-Learning_Influence_among_Interacting_Markov_Chains.html">111 nips-2005-Learning Influence among Interacting Markov Chains</a></p>
<p>17 0.28941816 <a title="145-lsi-17" href="./nips-2005-Modeling_Neuronal_Interactivity_using_Dynamic_Bayesian_Networks.html">130 nips-2005-Modeling Neuronal Interactivity using Dynamic Bayesian Networks</a></p>
<p>18 0.25948787 <a title="145-lsi-18" href="./nips-2005-Norepinephrine_and_Neural_Interrupts.html">141 nips-2005-Norepinephrine and Neural Interrupts</a></p>
<p>19 0.2428928 <a title="145-lsi-19" href="./nips-2005-Factorial_Switching_Kalman_Filters_for_Condition_Monitoring_in_Neonatal_Intensive_Care.html">68 nips-2005-Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care</a></p>
<p>20 0.20841262 <a title="145-lsi-20" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.03), (9, 0.011), (12, 0.175), (17, 0.056), (54, 0.011), (58, 0.01), (69, 0.012), (71, 0.108), (88, 0.086), (89, 0.018), (90, 0.219), (92, 0.138)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.73766756 <a title="145-lda-1" href="./nips-2005-Temporal_Abstraction_in_Temporal-difference_Networks.html">187 nips-2005-Temporal Abstraction in Temporal-difference Networks</a></p>
<p>same-paper 2 0.73448086 <a title="145-lda-2" href="./nips-2005-On_Local_Rewards_and_Scaling_Distributed_Reinforcement_Learning.html">145 nips-2005-On Local Rewards and Scaling Distributed Reinforcement Learning</a></p>
<p>3 0.73393285 <a title="145-lda-3" href="./nips-2005-Dual-Tree_Fast_Gauss_Transforms.html">59 nips-2005-Dual-Tree Fast Gauss Transforms</a></p>
<p>4 0.7321316 <a title="145-lda-4" href="./nips-2005-Policy-Gradient_Methods_for_Planning.html">153 nips-2005-Policy-Gradient Methods for Planning</a></p>
<p>5 0.68335778 <a title="145-lda-5" href="./nips-2005-Learning_Topology_with_the_Generative_Gaussian_Graph_and_the_EM_Algorithm.html">116 nips-2005-Learning Topology with the Generative Gaussian Graph and the EM Algorithm</a></p>
<p>6 0.66326058 <a title="145-lda-6" href="./nips-2005-Efficient_Unsupervised_Learning_for_Localization_and_Detection_in_Object_Categories.html">63 nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</a></p>
<p>7 0.64948332 <a title="145-lda-7" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>8 0.62776828 <a title="145-lda-8" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>9 0.625359 <a title="145-lda-9" href="./nips-2005-Bayesian_models_of_human_action_understanding.html">36 nips-2005-Bayesian models of human action understanding</a></p>
<p>10 0.60837579 <a title="145-lda-10" href="./nips-2005-Oblivious_Equilibrium%3A_A_Mean_Field_Approximation_for_Large-Scale_Dynamic_Games.html">142 nips-2005-Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games</a></p>
<p>11 0.60689068 <a title="145-lda-11" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>12 0.59772301 <a title="145-lda-12" href="./nips-2005-How_fast_to_work%3A_Response_vigor%2C_motivation_and_tonic_dopamine.html">91 nips-2005-How fast to work: Response vigor, motivation and tonic dopamine</a></p>
<p>13 0.592507 <a title="145-lda-13" href="./nips-2005-Logic_and_MRF_Circuitry_for_Labeling_Occluding_and_Thinline_Visual_Contours.html">122 nips-2005-Logic and MRF Circuitry for Labeling Occluding and Thinline Visual Contours</a></p>
<p>14 0.5881505 <a title="145-lda-14" href="./nips-2005-A_Connectionist_Model_for_Constructive_Modal_Reasoning.html">6 nips-2005-A Connectionist Model for Constructive Modal Reasoning</a></p>
<p>15 0.58647579 <a title="145-lda-15" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>16 0.58560258 <a title="145-lda-16" href="./nips-2005-Fast_Krylov_Methods_for_N-Body_Learning.html">71 nips-2005-Fast Krylov Methods for N-Body Learning</a></p>
<p>17 0.58300227 <a title="145-lda-17" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>18 0.58072191 <a title="145-lda-18" href="./nips-2005-Principles_of_real-time_computing_with_feedback_applied_to_cortical_microcircuit_models.html">157 nips-2005-Principles of real-time computing with feedback applied to cortical microcircuit models</a></p>
<p>19 0.57991529 <a title="145-lda-19" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>20 0.57985985 <a title="145-lda-20" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
