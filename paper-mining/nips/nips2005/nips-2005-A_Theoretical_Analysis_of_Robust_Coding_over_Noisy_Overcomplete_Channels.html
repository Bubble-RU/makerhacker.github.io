<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-15" href="#">nips2005-15</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</h1>
<br/><p>Source: <a title="nips-2005-15-pdf" href="http://papers.nips.cc/paper/2867-a-theoretical-analysis-of-robust-coding-over-noisy-overcomplete-channels.pdf">pdf</a></p><p>Author: Eizaburo Doi, Doru C. Balcan, Michael S. Lewicki</p><p>Abstract: Biological sensory systems are faced with the problem of encoding a high-ﬁdelity sensory signal with a population of noisy, low-ﬁdelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and twodimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of highdimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets. 1</p><p>Reference: <a title="nips-2005-15-reference" href="../nips2005_reference/nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mse', 0.469), ('decod', 0.294), ('snr', 0.263), ('channel', 0.261), ('capac', 0.246), ('cod', 0.236), ('reconstruct', 0.216), ('ax', 0.159), ('anisotrop', 0.156), ('overcomplet', 0.156), ('vvt', 0.15), ('encod', 0.141), ('robust', 0.124), ('isotrop', 0.116), ('zk', 0.111), ('wavelet', 0.11), ('princip', 0.107), ('deg', 0.106), ('ic', 0.105), ('aw', 0.097)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="15-tfidf-1" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>Author: Eizaburo Doi, Doru C. Balcan, Michael S. Lewicki</p><p>Abstract: Biological sensory systems are faced with the problem of encoding a high-ﬁdelity sensory signal with a population of noisy, low-ﬁdelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and twodimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of highdimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets. 1</p><p>2 0.17180327 <a title="15-tfidf-2" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>Author: Amir Navot, Lavi Shpigelman, Naftali Tishby, Eilon Vaadia</p><p>Abstract: We present a non-linear, simple, yet effective, feature subset selection method for regression and use it in analyzing cortical neural activity. Our algorithm involves a feature-weighted version of the k-nearest-neighbor algorithm. It is able to capture complex dependency of the target function on its input and makes use of the leave-one-out error as a natural regularization. We explain the characteristics of our algorithm on synthetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey. By applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data.</p><p>3 0.17155008 <a title="15-tfidf-3" href="./nips-2005-Large-scale_biophysical_parameter_estimation_in_single_neurons_via_constrained_linear_regression.html">106 nips-2005-Large-scale biophysical parameter estimation in single neurons via constrained linear regression</a></p>
<p>Author: Misha Ahrens, Liam Paninski, Quentin J. Huys</p><p>Abstract: Our understanding of the input-output function of single cells has been substantially advanced by biophysically accurate multi-compartmental models. The large number of parameters needing hand tuning in these models has, however, somewhat hampered their applicability and interpretability. Here we propose a simple and well-founded method for automatic estimation of many of these key parameters: 1) the spatial distribution of channel densities on the cell’s membrane; 2) the spatiotemporal pattern of synaptic input; 3) the channels’ reversal potentials; 4) the intercompartmental conductances; and 5) the noise level in each compartment. We assume experimental access to: a) the spatiotemporal voltage signal in the dendrite (or some contiguous subpart thereof, e.g. via voltage sensitive imaging techniques), b) an approximate kinetic description of the channels and synapses present in each compartment, and c) the morphology of the part of the neuron under investigation. The key observation is that, given data a)-c), all of the parameters 1)-4) may be simultaneously inferred by a version of constrained linear regression; this regression, in turn, is efﬁciently solved using standard algorithms, without any “local minima” problems despite the large number of parameters and complex dynamics. The noise level 5) may also be estimated by standard techniques. We demonstrate the method’s accuracy on several model datasets, and describe techniques for quantifying the uncertainty in our estimates. 1</p><p>4 0.16724016 <a title="15-tfidf-4" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>Author: Martin J. Wainwright</p><p>Abstract: Consider the problem of joint parameter estimation and prediction in a Markov random ﬁeld: i.e., the model parameters are estimated on the basis of an initial set of data, and then the ﬁtted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working in the computation-limited setting, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for ﬁtting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the “wrong” model even in the inﬁnite data limit) is provably beneﬁcial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of variational methods. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product. 1 Keywords: Markov random ﬁelds; variational method; message-passing algorithms; sum-product; belief propagation; parameter estimation; learning. 1</p><p>5 0.12931637 <a title="15-tfidf-5" href="./nips-2005-Is_Early_Vision_Optimized_for_Extracting_Higher-order_Dependencies%3F.html">101 nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: Linear implementations of the efﬁcient coding hypothesis, such as independent component analysis (ICA) and sparse coding models, have provided functional explanations for properties of simple cells in V1 [1, 2]. These models, however, ignore the non-linear behavior of neurons and fail to match individual and population properties of neural receptive ﬁelds in subtle but important ways. Hierarchical models, including Gaussian Scale Mixtures [3, 4] and other generative statistical models [5, 6], can capture higher-order regularities in natural images and explain nonlinear aspects of neural processing such as normalization and context effects [6,7]. Previously, it had been assumed that the lower level representation is independent of the hierarchy, and had been ﬁxed when training these models. Here we examine the optimal lower-level representations derived in the context of a hierarchical model and ﬁnd that the resulting representations are strikingly different from those based on linear models. Unlike the the basis functions and ﬁlters learned by ICA or sparse coding, these functions individually more closely resemble simple cell receptive ﬁelds and collectively span a broad range of spatial scales. Our work uniﬁes several related approaches and observations about natural image structure and suggests that hierarchical models might yield better representations of image structure throughout the hierarchy.</p><p>6 0.12480154 <a title="15-tfidf-6" href="./nips-2005-Message_passing_for_task_redistribution_on_sparse_graphs.html">125 nips-2005-Message passing for task redistribution on sparse graphs</a></p>
<p>7 0.11202163 <a title="15-tfidf-7" href="./nips-2005-Non-iterative_Estimation_with_Perturbed_Gaussian_Markov_Processes.html">139 nips-2005-Non-iterative Estimation with Perturbed Gaussian Markov Processes</a></p>
<p>8 0.11168951 <a title="15-tfidf-8" href="./nips-2005-Measuring_Shared_Information_and_Coordinated_Activity_in_Neuronal_Networks.html">124 nips-2005-Measuring Shared Information and Coordinated Activity in Neuronal Networks</a></p>
<p>9 0.10778766 <a title="15-tfidf-9" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>10 0.10600261 <a title="15-tfidf-10" href="./nips-2005-Inferring_Motor_Programs_from_Images_of_Handwritten_Digits.html">97 nips-2005-Inferring Motor Programs from Images of Handwritten Digits</a></p>
<p>11 0.10263998 <a title="15-tfidf-11" href="./nips-2005-Gaussian_Processes_for_Multiuser_Detection_in_CDMA_receivers.html">81 nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</a></p>
<p>12 0.095758952 <a title="15-tfidf-12" href="./nips-2005-Recovery_of_Jointly_Sparse_Signals_from_Few_Random_Projections.html">163 nips-2005-Recovery of Jointly Sparse Signals from Few Random Projections</a></p>
<p>13 0.093736723 <a title="15-tfidf-13" href="./nips-2005-Products_of_%60%60Edge-perts.html">158 nips-2005-Products of ``Edge-perts</a></p>
<p>14 0.08827091 <a title="15-tfidf-14" href="./nips-2005-A_Bayes_Rule_for_Density_Matrices.html">2 nips-2005-A Bayes Rule for Density Matrices</a></p>
<p>15 0.084699325 <a title="15-tfidf-15" href="./nips-2005-Rate_Distortion_Codes_in_Sensor_Networks%3A_A_System-level_Analysis.html">162 nips-2005-Rate Distortion Codes in Sensor Networks: A System-level Analysis</a></p>
<p>16 0.080867499 <a title="15-tfidf-16" href="./nips-2005-Kernelized_Infomax_Clustering.html">102 nips-2005-Kernelized Infomax Clustering</a></p>
<p>17 0.075377084 <a title="15-tfidf-17" href="./nips-2005-Gradient_Flow_Independent_Component_Analysis_in_Micropower_VLSI.html">88 nips-2005-Gradient Flow Independent Component Analysis in Micropower VLSI</a></p>
<p>18 0.071612664 <a title="15-tfidf-18" href="./nips-2005-Noise_and_the_two-thirds_power_Law.html">136 nips-2005-Noise and the two-thirds power Law</a></p>
<p>19 0.071456768 <a title="15-tfidf-19" href="./nips-2005-An_Approximate_Inference_Approach_for_the_PCA_Reconstruction_Error.html">24 nips-2005-An Approximate Inference Approach for the PCA Reconstruction Error</a></p>
<p>20 0.070711404 <a title="15-tfidf-20" href="./nips-2005-Value_Function_Approximation_with_Diffusion_Wavelets_and_Laplacian_Eigenfunctions.html">199 nips-2005-Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.21), (1, 0.007), (2, -0.016), (3, -0.063), (4, -0.004), (5, 0.097), (6, 0.098), (7, 0.033), (8, 0.22), (9, 0.075), (10, -0.075), (11, -0.043), (12, -0.019), (13, 0.077), (14, 0.069), (15, -0.226), (16, 0.08), (17, 0.04), (18, 0.079), (19, -0.123), (20, 0.169), (21, 0.048), (22, -0.073), (23, 0.072), (24, 0.196), (25, -0.11), (26, -0.239), (27, 0.123), (28, -0.045), (29, -0.032), (30, 0.109), (31, -0.1), (32, -0.082), (33, -0.036), (34, 0.156), (35, -0.104), (36, -0.027), (37, 0.044), (38, -0.068), (39, -0.014), (40, 0.075), (41, -0.124), (42, -0.002), (43, -0.081), (44, -0.121), (45, -0.006), (46, 0.014), (47, -0.061), (48, 0.09), (49, 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94850475 <a title="15-lsi-1" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>Author: Eizaburo Doi, Doru C. Balcan, Michael S. Lewicki</p><p>Abstract: Biological sensory systems are faced with the problem of encoding a high-ﬁdelity sensory signal with a population of noisy, low-ﬁdelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and twodimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of highdimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets. 1</p><p>2 0.56334198 <a title="15-lsi-2" href="./nips-2005-Large-scale_biophysical_parameter_estimation_in_single_neurons_via_constrained_linear_regression.html">106 nips-2005-Large-scale biophysical parameter estimation in single neurons via constrained linear regression</a></p>
<p>Author: Misha Ahrens, Liam Paninski, Quentin J. Huys</p><p>Abstract: Our understanding of the input-output function of single cells has been substantially advanced by biophysically accurate multi-compartmental models. The large number of parameters needing hand tuning in these models has, however, somewhat hampered their applicability and interpretability. Here we propose a simple and well-founded method for automatic estimation of many of these key parameters: 1) the spatial distribution of channel densities on the cell’s membrane; 2) the spatiotemporal pattern of synaptic input; 3) the channels’ reversal potentials; 4) the intercompartmental conductances; and 5) the noise level in each compartment. We assume experimental access to: a) the spatiotemporal voltage signal in the dendrite (or some contiguous subpart thereof, e.g. via voltage sensitive imaging techniques), b) an approximate kinetic description of the channels and synapses present in each compartment, and c) the morphology of the part of the neuron under investigation. The key observation is that, given data a)-c), all of the parameters 1)-4) may be simultaneously inferred by a version of constrained linear regression; this regression, in turn, is efﬁciently solved using standard algorithms, without any “local minima” problems despite the large number of parameters and complex dynamics. The noise level 5) may also be estimated by standard techniques. We demonstrate the method’s accuracy on several model datasets, and describe techniques for quantifying the uncertainty in our estimates. 1</p><p>3 0.51452088 <a title="15-lsi-3" href="./nips-2005-Gaussian_Processes_for_Multiuser_Detection_in_CDMA_receivers.html">81 nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</a></p>
<p>Author: Juan J. Murillo-fuentes, Sebastian Caro, Fernando Pérez-Cruz</p><p>Abstract: In this paper we propose a new receiver for digital communications. We focus on the application of Gaussian Processes (GPs) to the multiuser detection (MUD) in code division multiple access (CDMA) systems to solve the near-far problem. Hence, we aim to reduce the interference from other users sharing the same frequency band. While usual approaches minimize the mean square error (MMSE) to linearly retrieve the user of interest, we exploit the same criteria but in the design of a nonlinear MUD. Since the optimal solution is known to be nonlinear, the performance of this novel method clearly improves that of the MMSE detectors. Furthermore, the GP based MUD achieves excellent interference suppression even for short training sequences. We also include some experiments to illustrate that other nonlinear detectors such as those based on Support Vector Machines (SVMs) exhibit a worse performance. 1</p><p>4 0.49579668 <a title="15-lsi-4" href="./nips-2005-Recovery_of_Jointly_Sparse_Signals_from_Few_Random_Projections.html">163 nips-2005-Recovery of Jointly Sparse Signals from Few Random Projections</a></p>
<p>Author: Michael B. Wakin, Marco F. Duarte, Shriram Sarvotham, Dror Baron, Richard G. Baraniuk</p><p>Abstract: Compressed sensing is an emerging ﬁeld based on the revelation that a small group of linear projections of a sparse signal contains enough information for reconstruction. In this paper we introduce a new theory for distributed compressed sensing (DCS) that enables new distributed coding algorithms for multi-signal ensembles that exploit both intra- and inter-signal correlation structures. The DCS theory rests on a new concept that we term the joint sparsity of a signal ensemble. We study three simple models for jointly sparse signals, propose algorithms for joint recovery of multiple signals from incoherent projections, and characterize theoretically and empirically the number of measurements per sensor required for accurate reconstruction. In some sense DCS is a framework for distributed compression of sources with memory, which has remained a challenging problem in information theory for some time. DCS is immediately applicable to a range of problems in sensor networks and arrays. 1</p><p>5 0.46376631 <a title="15-lsi-5" href="./nips-2005-Is_Early_Vision_Optimized_for_Extracting_Higher-order_Dependencies%3F.html">101 nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: Linear implementations of the efﬁcient coding hypothesis, such as independent component analysis (ICA) and sparse coding models, have provided functional explanations for properties of simple cells in V1 [1, 2]. These models, however, ignore the non-linear behavior of neurons and fail to match individual and population properties of neural receptive ﬁelds in subtle but important ways. Hierarchical models, including Gaussian Scale Mixtures [3, 4] and other generative statistical models [5, 6], can capture higher-order regularities in natural images and explain nonlinear aspects of neural processing such as normalization and context effects [6,7]. Previously, it had been assumed that the lower level representation is independent of the hierarchy, and had been ﬁxed when training these models. Here we examine the optimal lower-level representations derived in the context of a hierarchical model and ﬁnd that the resulting representations are strikingly different from those based on linear models. Unlike the the basis functions and ﬁlters learned by ICA or sparse coding, these functions individually more closely resemble simple cell receptive ﬁelds and collectively span a broad range of spatial scales. Our work uniﬁes several related approaches and observations about natural image structure and suggests that hierarchical models might yield better representations of image structure throughout the hierarchy.</p><p>6 0.45384178 <a title="15-lsi-6" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>7 0.43746698 <a title="15-lsi-7" href="./nips-2005-Products_of_%60%60Edge-perts.html">158 nips-2005-Products of ``Edge-perts</a></p>
<p>8 0.43640783 <a title="15-lsi-8" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>9 0.42103335 <a title="15-lsi-9" href="./nips-2005-Rate_Distortion_Codes_in_Sensor_Networks%3A_A_System-level_Analysis.html">162 nips-2005-Rate Distortion Codes in Sensor Networks: A System-level Analysis</a></p>
<p>10 0.41864178 <a title="15-lsi-10" href="./nips-2005-Non-iterative_Estimation_with_Perturbed_Gaussian_Markov_Processes.html">139 nips-2005-Non-iterative Estimation with Perturbed Gaussian Markov Processes</a></p>
<p>11 0.40773183 <a title="15-lsi-11" href="./nips-2005-An_Approximate_Inference_Approach_for_the_PCA_Reconstruction_Error.html">24 nips-2005-An Approximate Inference Approach for the PCA Reconstruction Error</a></p>
<p>12 0.36069322 <a title="15-lsi-12" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>13 0.35548541 <a title="15-lsi-13" href="./nips-2005-Using_%60%60epitomes%27%27_to_model_genetic_diversity%3A_Rational_design_of_HIV_vaccine_cocktails.html">198 nips-2005-Using ``epitomes'' to model genetic diversity: Rational design of HIV vaccine cocktails</a></p>
<p>14 0.34930807 <a title="15-lsi-14" href="./nips-2005-Robust_design_of_biological_experiments.html">167 nips-2005-Robust design of biological experiments</a></p>
<p>15 0.3436369 <a title="15-lsi-15" href="./nips-2005-Spectral_Bounds_for_Sparse_PCA%3A_Exact_and_Greedy_Algorithms.html">180 nips-2005-Spectral Bounds for Sparse PCA: Exact and Greedy Algorithms</a></p>
<p>16 0.33047944 <a title="15-lsi-16" href="./nips-2005-Noise_and_the_two-thirds_power_Law.html">136 nips-2005-Noise and the two-thirds power Law</a></p>
<p>17 0.32840136 <a title="15-lsi-17" href="./nips-2005-Inferring_Motor_Programs_from_Images_of_Handwritten_Digits.html">97 nips-2005-Inferring Motor Programs from Images of Handwritten Digits</a></p>
<p>18 0.29621631 <a title="15-lsi-18" href="./nips-2005-Measuring_Shared_Information_and_Coordinated_Activity_in_Neuronal_Networks.html">124 nips-2005-Measuring Shared Information and Coordinated Activity in Neuronal Networks</a></p>
<p>19 0.28585169 <a title="15-lsi-19" href="./nips-2005-Message_passing_for_task_redistribution_on_sparse_graphs.html">125 nips-2005-Message passing for task redistribution on sparse graphs</a></p>
<p>20 0.28516763 <a title="15-lsi-20" href="./nips-2005-A_Bayes_Rule_for_Density_Matrices.html">2 nips-2005-A Bayes Rule for Density Matrices</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.038), (9, 0.012), (12, 0.015), (17, 0.404), (54, 0.027), (71, 0.224), (88, 0.054), (89, 0.028), (92, 0.109)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82046187 <a title="15-lda-1" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>Author: John D. Lafferty, Pradeep K. Ravikumar</p><p>Abstract: We present a family of approximation techniques for probabilistic graphical models, based on the use of graphical preconditioners developed in the scientiﬁc computing literature. Our framework yields rigorous upper and lower bounds on event probabilities and the log partition function of undirected graphical models, using non-iterative procedures that have low time complexity. As in mean ﬁeld approaches, the approximations are built upon tractable subgraphs; however, we recast the problem of optimizing the tractable distribution parameters and approximate inference in terms of the well-studied linear systems problem of obtaining a good matrix preconditioner. Experiments are presented that compare the new approximation schemes to variational methods. 1</p><p>2 0.81924909 <a title="15-lda-2" href="./nips-2005-TD%280%29_Leads_to_Better_Policies_than_Approximate_Value_Iteration.html">186 nips-2005-TD(0) Leads to Better Policies than Approximate Value Iteration</a></p>
<p>Author: Benjamin V. Roy</p><p>Abstract: We consider approximate value iteration with a parameterized approximator in which the state space is partitioned and the optimal cost-to-go function over each partition is approximated by a constant. We establish performance loss bounds for policies derived from approximations associated with ﬁxed points. These bounds identify beneﬁts to having projection weights equal to the invariant distribution of the resulting policy. Such projection weighting leads to the same ﬁxed points as TD(0). Our analysis also leads to the ﬁrst performance loss bound for approximate value iteration with an average cost objective. 1 Preliminaries Consider a discrete-time communicating Markov decision process (MDP) with a ﬁnite state space S = {1, . . . , |S|}. At each state x ∈ S, there is a ﬁnite set Ux of admissible actions. If the current state is x and an action u ∈ Ux is selected, a cost of gu (x) is incurred, and the system transitions to a state y ∈ S with probability pxy (u). For any x ∈ S and u ∈ Ux , y∈S pxy (u) = 1. Costs are discounted at a rate of α ∈ (0, 1) per period. Each instance of such an MDP is deﬁned by a quintuple (S, U, g, p, α). A (stationary deterministic) policy is a mapping µ that assigns an action u ∈ Ux to each state x ∈ S. If actions are selected based on a policy µ, the state follows a Markov process with transition matrix Pµ , where each (x, y)th entry is equal to pxy (µ(x)). The restriction to communicating MDPs ensures that it is possible to reach any state from any other state. Each policy µ is associated with a cost-to-go function Jµ ∈ |S| , deﬁned by Jµ = ∞ t t −1 gµ , where, with some abuse of notation, gµ (x) = gµ(x) (x) t=0 α Pµ gµ = (I − αPµ ) for each x ∈ S. A policy µ is said to be greedy with respect to a function J if µ(x) ∈ argmin(gu (x) + α y∈S pxy (u)J(y)) for all x ∈ S. u∈Ux The optimal cost-to-go function J ∗ ∈ |S| is deﬁned by J ∗ (x) = minµ Jµ (x), for all x ∈ S. A policy µ∗ is said to be optimal if Jµ∗ = J ∗ . It is well-known that an optimal policy exists. Further, a policy µ∗ is optimal if and only if it is greedy with respect to J ∗ . Hence, given the optimal cost-to-go function, optimal actions can computed be minimizing the right-hand side of the above inclusion. Value iteration generates a sequence J converging to J ∗ according to J +1 = T J , where T is the dynamic programming operator, deﬁned by (T J)(x) = minu∈Ux (gu (x) + α y∈S pxy (u)J(y)), for all x ∈ S and J ∈ |S| . This sequence converges to J ∗ for any initialization of J0 . 2 Approximate Value Iteration The state spaces of relevant MDPs are typically so large that computation and storage of a cost-to-go function is infeasible. One approach to dealing with this obstacle involves partitioning the state space S into a manageable number K of disjoint subsets S1 , . . . , SK and approximating the optimal cost-to-go function with a function that is constant over each partition. This can be thought of as a form of state aggregation – all states within a given partition are assumed to share a common optimal cost-to-go. To represent an approximation, we deﬁne a matrix Φ ∈ |S|×K such that each kth column is an indicator function for the kth partition Sk . Hence, for any r ∈ K , k, and x ∈ Sk , (Φr)(x) = rk . In this paper, we study variations of value iteration, each of which computes a vector r so that Φr approximates J ∗ . The use of such a policy µr which is greedy with respect to Φr is justiﬁed by the following result (see [10] for a proof): ˜ Theorem 1 If µ is a greedy policy with respect to a function J ∈ Jµ − J ∗ ≤ ∞ 2α ˜ J∗ − J 1−α |S| then ∞. One common way of approximating a function J ∈ |S| with a function of the form Φr involves projection with respect to a weighted Euclidean norm · π . The weighted Euclidean 1/2 |S| 2 norm: J 2,π = . Here, π ∈ + is a vector of weights that assign x∈S π(x)J (x) relative emphasis among states. The projection Ππ J is the function Φr that attains the minimum of J −Φr 2,π ; if there are multiple functions Φr that attain the minimum, they must form an afﬁne space, and the projection is taken to be the one with minimal norm Φr 2,π . Note that in our context, where each kth column of Φ represents an indicator function for the kth partition, for any π, J, and x ∈ Sk , (Ππ J)(x) = y∈Sk π(y)J(y)/ y∈Sk π(y). Approximate value iteration begins with a function Φr(0) and generates a sequence according to Φr( +1) = Ππ T Φr( ) . It is well-known that the dynamic programming operator T is a contraction mapping with respect to the maximum norm. Further, Ππ is maximum-norm nonexpansive [16, 7, 8]. (This is not true for general Φ, but is true in our context in which columns of Φ are indicator functions for partitions.) It follows that the composition Ππ T is a contraction mapping. By the contraction mapping theorem, Ππ T has a unique ﬁxed point Φ˜, which is the limit of the sequence Φr( ) . Further, the following result holds: r Theorem 2 For any MDP, partition, and weights π with support intersecting every partition, if Φ˜ = Ππ T Φ˜ then r r Φ˜ − J ∗ r ∞ ≤ 2 min J ∗ − Φr 1 − α r∈ K and (1 − α) Jµr − J ∗ ˜ ∞ ≤ ∞, 4α min J ∗ − Φr 1 − α r∈ K ∞. The ﬁrst inequality of the theorem is an approximation error bound, established in [16, 7, 8] for broader classes of approximators that include state aggregation as a special case. The second is a performance loss bound, derived by simply combining the approximation error bound and Theorem 1. Note that Jµr (x) ≥ J ∗ (x) for all x, so the left-hand side of the performance loss bound ˜ is the maximal increase in cost-to-go, normalized by 1 − α. This normalization is natural, since a cost-to-go function is a linear combination of expected future costs, with coefﬁcients 1, α, α2 , . . ., which sum to 1/(1 − α). Our motivation of the normalizing constant begs the question of whether, for ﬁxed MDP parameters (S, U, g, p) and ﬁxed Φ, minr J ∗ − Φr ∞ also grows with 1/(1 − α). It turns out that minr J ∗ − Φr ∞ = O(1). To see why, note that for any µ, Jµ = (I − αPµ )−1 gµ = 1 λ µ + hµ , 1−α where λµ (x) is the expected average cost if the process starts in state x and is controlled by policy µ, τ −1 1 t λµ = lim Pµ gµ , τ →∞ τ t=0 and hµ is the discounted differential cost function hµ = (I − αPµ )−1 (gµ − λµ ). Both λµ and hµ converge to ﬁnite vectors as α approaches 1 [3]. For an optimal policy µ∗ , limα↑1 λµ∗ (x) does not depend on x (in our context of a communicating MDP). Since constant functions lie in the range of Φ, lim min J ∗ − Φr α↑1 r∈ K ∞ ≤ lim hµ∗ α↑1 ∞ < ∞. The performance loss bound still exhibits an undesirable dependence on α through the coefﬁcient 4α/(1 − α). In most relevant contexts, α is close to 1; a representative value might be 0.99. Consequently, 4α/(1 − α) can be very large. Unfortunately, the bound is sharp, as expressed by the following theorem. We will denote by 1 the vector with every component equal to 1. Theorem 3 For any δ > 0, α ∈ (0, 1), and ∆ ≥ 0, there exists MDP parameters (S, U, g, p) and a partition such that minr∈ K J ∗ − Φr ∞ = ∆ and, if Φ˜ = Ππ T Φ˜ r r with π = 1, 4α min J ∗ − Φr ∞ − δ. (1 − α) Jµr − J ∗ ∞ ≥ ˜ 1 − α r∈ K This theorem is established through an example in [22]. The choice of uniform weights (π = 1) is meant to point out that even for such a simple, perhaps natural, choice of weights, the performance loss bound is sharp. Based on Theorems 2 and 3, one might expect that there exists MDP parameters (S, U, g, p) and a partition such that, with π = 1, (1 − α) Jµr − J ∗ ˜ ∞ =Θ 1 min J ∗ − Φr 1 − α r∈ K ∞ . In other words, that the performance loss is both lower and upper bounded by 1/(1 − α) times the smallest possible approximation error. It turns out that this is not true, at least if we restrict to a ﬁnite state space. However, as the following theorem establishes, the coefﬁcient multiplying minr∈ K J ∗ − Φr ∞ can grow arbitrarily large as α increases, keeping all else ﬁxed. Theorem 4 For any L and ∆ ≥ 0, there exists MDP parameters (S, U, g, p) and a partition such that limα↑1 minr∈ K J ∗ − Φr ∞ = ∆ and, if Φ˜ = Ππ T Φ˜ with π = 1, r r lim inf (1 − α) (Jµr (x) − J ∗ (x)) ≥ L lim min J ∗ − Φr ∞ , ˜ α↑1 α↑1 r∈ K for all x ∈ S. This Theorem is also established through an example [22]. For any µ and x, lim ((1 − α)Jµ (x) − λµ (x)) = lim(1 − α)hµ (x) = 0. α↑1 α↑1 Combined with Theorem 4, this yields the following corollary. Corollary 1 For any L and ∆ ≥ 0, there exists MDP parameters (S, U, g, p) and a partition such that limα↑1 minr∈ K J ∗ − Φr ∞ = ∆ and, if Φ˜ = Ππ T Φ˜ with π = 1, r r ∗ lim inf (λµr (x) − λµ∗ (x)) ≥ L lim min J − Φr ∞ , ˜ α↑1 α↑1 r∈ K for all x ∈ S. 3 Using the Invariant Distribution In the previous section, we considered an approximation Φ˜ that solves Ππ T Φ˜ = Φ˜ for r r r some arbitrary pre-selected weights π. We now turn to consider use of an invariant state distribution πr of Pµr as the weight vector.1 This leads to a circular deﬁnition: the weights ˜ ˜ are used in deﬁning r and now we are deﬁning the weights in terms of r. What we are ˜ ˜ really after here is a vector r that satisﬁes Ππr T Φ˜ = Φ˜. The following theorem captures ˜ r r ˜ the associated beneﬁts. (Due to space limitations, we omit the proof, which is provided in the full length version of this paper [22].) Theorem 5 For any MDP and partition, if Φ˜ = Ππr T Φ˜ and πr has support intersecting r r ˜ ˜ T every partition, (1 − α)πr (Jµr − J ∗ ) ≤ 2α minr∈ K J ∗ − Φr ∞ . ˜ ˜ When α is close to 1, which is typical, the right-hand side of our new performance loss bound is far less than that of Theorem 2. The primary improvement is in the omission of a factor of 1 − α from the denominator. But for the bounds to be compared in a meaningful way, we must also relate the left-hand-side expressions. A relation can be based on the fact that for all µ, limα↑1 (1 − α)Jµ − λµ ∞ = 0, as explained in Section 2. In particular, based on this, we have lim(1 − α) Jµ − J ∗ ∞ = |λµ − λ∗ | = λµ − λ∗ = lim π T (Jµ − J ∗ ), α↑1 α↑1 for all policies µ and probability distributions π. Hence, the left-hand-side expressions from the two performance bounds become directly comparable as α approaches 1. Another interesting comparison can be made by contrasting Corollary 1 against the following immediate consequence of Theorem 5. Corollary 2 For all MDP parameters (S, U, g, p) and partitions, if Φ˜ = Ππr T Φ˜ and r r ˜ lim inf α↑1 x∈Sk πr (x) > 0 for all k, ˜ lim sup λµr − λµ∗ ∞ ≤ 2 lim min J ∗ − Φr ∞ . ˜ α↑1 α↑1 r∈ K The comparison suggests that solving Φ˜ = Ππr T Φ˜ is strongly preferable to solving r r ˜ Φ˜ = Ππ T Φ˜ with π = 1. r r 1 By an invariant state distribution of a transition matrix P , we mean any probability distribution π such that π T P = π T . In the event that Pµr has multiple invariant distributions, πr denotes an ˜ ˜ arbitrary choice. 4 Exploration If a vector r solves Φ˜ = Ππr T Φ˜ and the support of πr intersects every partition, Theorem ˜ r r ˜ ˜ 5 promises a desirable bound. However, there are two signiﬁcant shortcomings to this solution concept, which we will address in this section. First, in some cases, the equation Ππr T Φ˜ = Φ˜ does not have a solution. It is easy to produce examples of this; though r r ˜ no example has been documented for the particular class of approximators we are using here, [2] offers an example involving a different linearly parameterized approximator that captures the spirit of what can happen. Second, it would be nice to relax the requirement that the support of πr intersect every partition. ˜ To address these shortcomings, we introduce stochastic policies. A stochastic policy µ maps state-action pairs to probabilities. For each x ∈ S and u ∈ Ux , µ(x, u) is the probability of taking action u when in state x. Hence, µ(x, u) ≥ 0 for all x ∈ S and u ∈ Ux , and u∈Ux µ(x, u) = 1 for all x ∈ S. Given a scalar > 0 and a function J, the -greedy Boltzmann exploration policy with respect to J is deﬁned by µ(x, u) = e−(Tu J)(x)(|Ux |−1)/ e . −(Tu J)(x)(|Ux |−1)/ e u∈Ux e For any > 0 and r, let µr denote the -greedy Boltzmann exploration policy with respect to Φr. Further, we deﬁne a modiﬁed dynamic programming operator that incorporates Boltzmann exploration: (T J)(x) = u∈Ux e−(Tu J)(x)(|Ux |−1)/ e (Tu J)(x) . −(Tu J)(x)(|Ux |−1)/ e u∈Ux e As approaches 0, -greedy Boltzmann exploration policies become greedy and the modiﬁed dynamic programming operators become the dynamic programming operator. More precisely, for all r, x, and J, lim ↓0 µr (x, µr (x)) = 1 and lim ↓1 T J = T J. These are immediate consequences of the following result (see [4] for a proof). Lemma 1 For any n, v ∈ mini vi . n , mini vi + ≥ i e−vi (n−1)/ e vi / i e−vi (n−1)/ e ≥ Because we are only concerned with communicating MDPs, there is a unique invariant state distribution associated with each -greedy Boltzmann exploration policy µr and the support of this distribution is S. Let πr denote this distribution. We consider a vector r that ˜ solves Φ˜ = Ππr T Φ˜. For any > 0, there exists a solution to this equation (this is an r r ˜ immediate extension of Theorem 5.1 from [4]). We have the following performance loss bound, which parallels Theorem 5 but with an equation for which a solution is guaranteed to exist and without any requirement on the resulting invariant distribution. (Again, we omit the proof, which is available in [22].) Theorem 6 For any MDP, partition, and > 0, if Φ˜ = Ππr T Φ˜ then (1 − r r ˜ T ∗ ∗ α)(πr ) (Jµr − J ) ≤ 2α minr∈ K J − Φr ∞ + . ˜ ˜ 5 Computation: TD(0) Though computation is not a focus of this paper, we offer a brief discussion here. First, we describe a simple algorithm from [16], which draws on ideas from temporal-difference learning [11, 12] and Q-learning [23, 24] to solve Φ˜ = Ππ T Φ˜. It requires an abilr r ity to sample a sequence of states x(0) , x(1) , x(2) , . . ., each independent and identically distributed according to π. Also required is a way to efﬁciently compute (T Φr)(x) = minu∈Ux (gu (x) + α y∈S pxy (u)(Φr)(y)), for any given x and r. This is typically possible when the action set Ux and the support of px· (u) (i.e., the set of states that can follow x if action u is selected) are not too large. The algorithm generates a sequence of vectors r( ) according to r( +1) = r( ) + γ φ(x( ) ) (T Φr( ) )(x( ) ) − (Φr( ) )(x( ) ) , where γ is a step size and φ(x) denotes the column vector made up of components from the xth row of Φ. In [16], using results from [15, 9], it is shown that under appropriate assumptions on the step size sequence, r( ) converges to a vector r that solves Φ˜ = Ππ T Φ˜. ˜ r r The equation Φ˜ = Ππ T Φ˜ may have no solution. Further, the requirement that states r r are sampled independently from the invariant distribution may be impractical. However, a natural extension of the above algorithm leads to an easily implementable version of TD(0) that aims at solving Φ˜ = Ππr T Φ˜. The algorithm requires simulation of a trajectory r r ˜ x0 , x1 , x2 , . . . of the MDP, with each action ut ∈ Uxt generated by the -greedy Boltzmann exploration policy with respect to Φr(t) . The sequence of vectors r(t) is generated according to r(t+1) = r(t) + γt φ(xt ) (T Φr(t) )(xt ) − (Φr(t) )(xt ) . Under suitable conditions on the step size sequence, if this algorithm converges, the limit satisﬁes Φ˜ = Ππr T Φ˜. Whether such an algorithm converges and whether there are r r ˜ other algorithms that can effectively solve Φ˜ = Ππr T Φ˜ for broad classes of relevant r r ˜ problems remain open issues. 6 Extensions and Open Issues Our results demonstrate that weighting a Euclidean norm projection by the invariant distribution of a greedy (or approximately greedy) policy can lead to a dramatic performance gain. It is intriguing that temporal-difference learning implicitly carries out such a projection, and consequently, any limit of convergence obeys the stronger performance loss bound. This is not the ﬁrst time that the invariant distribution has been shown to play a critical role in approximate value iteration and temporal-difference learning. In prior work involving approximation of a cost-to-go function for a ﬁxed policy (no control) and a general linearly parameterized approximator (arbitrary matrix Φ), it was shown that weighting by the invariant distribution is key to ensuring convergence and an approximation error bound [17, 18]. Earlier empirical work anticipated this [13, 14]. The temporal-difference learning algorithm presented in Section 5 is a version of TD(0), This is a special case of TD(λ), which is parameterized by λ ∈ [0, 1]. It is not known whether the results of this paper can be extended to the general case of λ ∈ [0, 1]. Prior research has suggested that larger values of λ lead to superior results. In particular, an example of [1] and the approximation error bounds of [17, 18], both of which are restricted to the case of a ﬁxed policy, suggest that approximation error is ampliﬁed by a factor of 1/(1 − α) as λ is changed from 1 to 0. The results of Sections 3 and 4 suggest that this factor vanishes if one considers a controlled process and performance loss rather than approximation error. Whether the results of this paper can be extended to accommodate approximate value iteration with general linearly parameterized approximators remains an open issue. In this broader context, error and performance loss bounds of the kind offered by Theorem 2 are unavailable, even when the invariant distribution is used to weight the projection. Such error and performance bounds are available, on the other hand, for the solution to a certain linear program [5, 6]. Whether a factor of 1/(1 − α) can similarly be eliminated from these bounds is an open issue. Our results can be extended to accommodate an average cost objective, assuming that the MDP is communicating. With Boltzmann exploration, the equation of interest becomes Φ˜ = Ππr (T Φ˜ − λ1). r r ˜ ˜ ˜ The variables include an estimate λ ∈ of the minimal average cost λ∗ ∈ and an approximation Φ˜ of the optimal differential cost function h∗ . The discount factor α is set r to 1 in computing an -greedy Boltzmann exploration policy as well as T . There is an average-cost version of temporal-difference learning for which any limit of convergence ˜ ˜ (λ, r) satisﬁes this equation [19, 20, 21]. Generalization of Theorem 2 does not lead to a useful result because the right-hand side of the bound becomes inﬁnite as α approaches 1. On the other hand, generalization of Theorem 6 yields the ﬁrst performance loss bound for approximate value iteration with an average-cost objective: Theorem 7 For any communicating MDP with an average-cost objective, partition, and r ˜ > 0, if Φ˜ = Ππr (T Φ˜ − λ1) then r ˜ λµr − λ∗ ≤ 2 min h∗ − Φr ˜ r∈ K ∞ + . Here, λµr ∈ denotes the average cost under policy µr , which is well-deﬁned because the ˜ ˜ process is irreducible under an -greedy Boltzmann exploration policy. This theorem can be proved by taking limits on the left and right-hand sides of the bound of Theorem 6. It is easy to see that the limit of the left-hand side is λµr − λ∗ . The limit of minr∈ K J ∗ − Φr ∞ ˜ on the right-hand side is minr∈ K h∗ − Φr ∞ . (This follows from the analysis of [3].) Acknowledgments This material is based upon work supported by the National Science Foundation under Grant ECS-9985229 and by the Ofﬁce of Naval Research under Grant MURI N00014-001-0637. The author’s understanding of the topic beneﬁted from collaborations with Dimitri Bertsekas, Daniela de Farias, and John Tsitsiklis. A full length version of this paper has been submitted to Mathematics of Operations Research and has beneﬁted from a number of useful comments and suggestions made by reviewers. References [1] D. P. Bertsekas. A counterexample to temporal-difference learning. Neural Computation, 7:270–279, 1994. [2] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc, Belmont, MA, 1996. [3] D. Blackwell. Discrete dynamic programming. Annals of Mathematical Statistics, 33:719–726, 1962. [4] D. P. de Farias and B. Van Roy. On the existence of ﬁxed points for approximate value iteration and temporal-difference learning. Journal of Optimization Theory and Applications, 105(3), 2000. [5] D. P. de Farias and B. Van Roy. Approximate dynamic programming via linear programming. In Advances in Neural Information Processing Systems 14. MIT Press, 2002. [6] D. P. de Farias and B. Van Roy. The linear programming approach to approximate dynamic programming. Operations Research, 51(6):850–865, 2003. [7] G. J. Gordon. Stable function approximation in dynamic programming. Technical Report CMU-CS-95-103, Carnegie Mellon University, 1995. [8] G. J. Gordon. Stable function approximation in dynamic programming. In Machine Learning: Proceedings of the Twelfth International Conference (ICML), San Francisco, CA, 1995. [9] T. Jaakkola, M. I. Jordan, and S. P. Singh. On the Convergence of Stochastic Iterative Dynamic Programming Algorithms. Neural Computation, 6:1185–1201, 1994. [10] S. P. Singh and R. C. Yee. An upper-bound on the loss from approximate optimalvalue functions. Machine Learning, 1994. [11] R. S. Sutton. Temporal Credit Assignment in Reinforcement Learning. PhD thesis, University of Massachusetts, Amherst, Amherst, MA, 1984. [12] R. S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3:9–44, 1988. [13] R. S. Sutton. On the virtues of linear learning and trajectory distributions. In Proceedings of the Workshop on Value Function Approximation, Machine Learning Conference, 1995. [14] R. S. Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse coding. In Advances in Neural Information Processing Systems 8, Cambridge, MA, 1996. MIT Press. [15] J. N. Tsitsiklis. Asynchronous stochastic approximation and Q-learning. Machine Learning, 16:185–202, 1994. [16] J. N. Tsitsiklis and B. Van Roy. Feature–based methods for large scale dynamic programming. Machine Learning, 22:59–94, 1996. [17] J. N. Tsitsiklis and B. Van Roy. An analysis of temporal–difference learning with function approximation. IEEE Transactions on Automatic Control, 42(5):674–690, 1997. [18] J. N. Tsitsiklis and B. Van Roy. Analysis of temporal-difference learning with function approximation. In Advances in Neural Information Processing Systems 9, Cambridge, MA, 1997. MIT Press. [19] J. N. Tsitsiklis and B. Van Roy. Average cost temporal-difference learning. In Proceedings of the IEEE Conference on Decision and Control, 1997. [20] J. N. Tsitsiklis and B. Van Roy. Average cost temporal-difference learning. Automatica, 35(11):1799–1808, 1999. [21] J. N. Tsitsiklis and B. Van Roy. On average versus discounted reward temporaldifference learning. Machine Learning, 49(2-3):179–191, 2002. [22] B. Van Roy. Performance loss bounds for approximate value iteration with state aggregation. Under review with Mathematics of Operations Research, available at www.stanford.edu/ bvr/psﬁles/aggregation.pdf, 2005. [23] C. J. C. H. Watkins. Learning From Delayed Rewards. PhD thesis, Cambridge University, Cambridge, UK, 1989. [24] C. J. C. H. Watkins and P. Dayan. Q-learning. Machine Learning, 8:279–292, 1992.</p><p>same-paper 3 0.7730785 <a title="15-lda-3" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>Author: Eizaburo Doi, Doru C. Balcan, Michael S. Lewicki</p><p>Abstract: Biological sensory systems are faced with the problem of encoding a high-ﬁdelity sensory signal with a population of noisy, low-ﬁdelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and twodimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of highdimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets. 1</p><p>4 0.64980525 <a title="15-lda-4" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>Author: Martin J. Wainwright</p><p>Abstract: Consider the problem of joint parameter estimation and prediction in a Markov random ﬁeld: i.e., the model parameters are estimated on the basis of an initial set of data, and then the ﬁtted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working in the computation-limited setting, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for ﬁtting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the “wrong” model even in the inﬁnite data limit) is provably beneﬁcial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of variational methods. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product. 1 Keywords: Markov random ﬁelds; variational method; message-passing algorithms; sum-product; belief propagation; parameter estimation; learning. 1</p><p>5 0.64688522 <a title="15-lda-5" href="./nips-2005-Generalization_to_Unseen_Cases.html">85 nips-2005-Generalization to Unseen Cases</a></p>
<p>Author: Teemu Roos, Peter Grünwald, Petri Myllymäki, Henry Tirri</p><p>Abstract: We analyze classiﬁcation error on unseen cases, i.e. cases that are different from those in the training set. Unlike standard generalization error, this off-training-set error may differ signiﬁcantly from the empirical error with high probability even with large sample sizes. We derive a datadependent bound on the difference between off-training-set and standard generalization error. Our result is based on a new bound on the missing mass, which for small samples is stronger than existing bounds based on Good-Turing estimators. As we demonstrate on UCI data-sets, our bound gives nontrivial generalization guarantees in many practical cases. In light of these results, we show that certain claims made in the No Free Lunch literature are overly pessimistic. 1</p><p>6 0.62367582 <a title="15-lda-6" href="./nips-2005-Oblivious_Equilibrium%3A_A_Mean_Field_Approximation_for_Large-Scale_Dynamic_Games.html">142 nips-2005-Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games</a></p>
<p>7 0.62353981 <a title="15-lda-7" href="./nips-2005-Layered_Dynamic_Textures.html">108 nips-2005-Layered Dynamic Textures</a></p>
<p>8 0.61746025 <a title="15-lda-8" href="./nips-2005-Dynamical_Synapses_Give_Rise_to_a_Power-Law_Distribution_of_Neuronal_Avalanches.html">61 nips-2005-Dynamical Synapses Give Rise to a Power-Law Distribution of Neuronal Avalanches</a></p>
<p>9 0.61737359 <a title="15-lda-9" href="./nips-2005-Active_Learning_For_Identifying_Function_Threshold_Boundaries.html">18 nips-2005-Active Learning For Identifying Function Threshold Boundaries</a></p>
<p>10 0.61309046 <a title="15-lda-10" href="./nips-2005-Variational_Bayesian_Stochastic_Complexity_of_Mixture_Models.html">201 nips-2005-Variational Bayesian Stochastic Complexity of Mixture Models</a></p>
<p>11 0.60855073 <a title="15-lda-11" href="./nips-2005-Robust_design_of_biological_experiments.html">167 nips-2005-Robust design of biological experiments</a></p>
<p>12 0.6026265 <a title="15-lda-12" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>13 0.59977144 <a title="15-lda-13" href="./nips-2005-Gaussian_Process_Dynamical_Models.html">80 nips-2005-Gaussian Process Dynamical Models</a></p>
<p>14 0.5916419 <a title="15-lda-14" href="./nips-2005-An_Approximate_Inference_Approach_for_the_PCA_Reconstruction_Error.html">24 nips-2005-An Approximate Inference Approach for the PCA Reconstruction Error</a></p>
<p>15 0.59043163 <a title="15-lda-15" href="./nips-2005-On_Local_Rewards_and_Scaling_Distributed_Reinforcement_Learning.html">145 nips-2005-On Local Rewards and Scaling Distributed Reinforcement Learning</a></p>
<p>16 0.5896697 <a title="15-lda-16" href="./nips-2005-Learning_from_Data_of_Variable_Quality.html">117 nips-2005-Learning from Data of Variable Quality</a></p>
<p>17 0.5854131 <a title="15-lda-17" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>18 0.58317506 <a title="15-lda-18" href="./nips-2005-Kernelized_Infomax_Clustering.html">102 nips-2005-Kernelized Infomax Clustering</a></p>
<p>19 0.58297533 <a title="15-lda-19" href="./nips-2005-Improved_risk_tail_bounds_for_on-line_algorithms.html">95 nips-2005-Improved risk tail bounds for on-line algorithms</a></p>
<p>20 0.58061558 <a title="15-lda-20" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
