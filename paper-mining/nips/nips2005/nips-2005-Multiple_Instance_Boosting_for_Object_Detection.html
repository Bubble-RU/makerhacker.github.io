<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>131 nips-2005-Multiple Instance Boosting for Object Detection</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-131" href="#">nips2005-131</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>131 nips-2005-Multiple Instance Boosting for Object Detection</h1>
<br/><p>Source: <a title="nips-2005-131-pdf" href="http://papers.nips.cc/paper/2926-multiple-instance-boosting-for-object-detection.pdf">pdf</a></p><p>Author: Cha Zhang, John C. Platt, Paul A. Viola</p><p>Abstract: A good image object detection algorithm is accurate, fast, and does not require exact locations of objects in a training set. We can create such an object detector by taking the architecture of the Viola-Jones detector cascade and training it with a new variant of boosting that we call MILBoost. MILBoost uses cost functions from the Multiple Instance Learning literature combined with the AnyBoost framework. We adapt the feature selection criterion of MILBoost to optimize the performance of the Viola-Jones cascade. Experiments show that the detection rate is up to 1.6 times better using MILBoost. This increased detection rate shows the advantage of simultaneously learning the locations and scales of the objects in the training set along with the parameters of the classiﬁer. 1</p><p>Reference: <a title="nips-2005-131-reference" href="../nips2005_reference/nips-2005-Multiple_Instance_Boosting_for_Object_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract A good image object detection algorithm is accurate, fast, and does not require exact locations of objects in a training set. [sent-3, score-0.562]
</p><p>2 We can create such an object detector by taking the architecture of the Viola-Jones detector cascade and training it with a new variant of boosting that we call MILBoost. [sent-4, score-0.826]
</p><p>3 Experiments show that the detection rate is up to 1. [sent-7, score-0.247]
</p><p>4 This increased detection rate shows the advantage of simultaneously learning the locations and scales of the objects in the training set along with the parameters of the classiﬁer. [sent-9, score-0.378]
</p><p>5 1  Introduction  When researchers use machine learning for object detection, they need to know the location and size of the objects, in order to generate positive examples for the classiﬁcation algorithm. [sent-10, score-0.378]
</p><p>6 In this paper, we explicitly acknowledge that object recognition is innately a Multiple Instance Learning problem: we know that objects are located in regions of the image, but we don’t know exactly where. [sent-14, score-0.246]
</p><p>7 Instead, they come in “bags”, where all of the examples in a bag share a label [4]. [sent-16, score-0.434]
</p><p>8 A positive bag means that at least one example in the bag is positive, while a negative bag means that all examples in the bag are negative. [sent-17, score-1.657]
</p><p>9 In MIL, learning must simultaneously learn which examples in the positive bags are positive, along with the parameters of the classiﬁer. [sent-18, score-0.329]
</p><p>10 We have combined MIL with the Viola-Jones method of object detection, which uses Adaboost [11] to create a cascade of detectors. [sent-19, score-0.32]
</p><p>11 In addition, we show how early stage in the detection cascade can be re-trained using information extracted from the ﬁnal MIL classiﬁer. [sent-21, score-0.383]
</p><p>12 The MIL framework is shown to produce classiﬁers with much higher detection rates and fast computation times. [sent-24, score-0.263]
</p><p>13 1  Structure of paper  We ﬁrst review the previous work in two ﬁelds: previous related work in object detection (Section 2. [sent-26, score-0.355]
</p><p>14 We derive a new MIL variant of boosting in Section 3, called MILBoost. [sent-29, score-0.228]
</p><p>15 We then adapt MILBoost to train an effective cascade using a new criterion for selecting features in the early rounds of training (Section 5). [sent-31, score-0.263]
</p><p>16 The paper concludes in Section 6 with experimental results on the problem of person detection in a teleconferencing application. [sent-32, score-0.319]
</p><p>17 The MIL framework is shown to produce classiﬁers with much higher detection rates and fast computation times. [sent-33, score-0.263]
</p><p>18 2  Relationship to previous work  This paper lies at the intersection between the subﬁelds of object detection and multiple instance learning. [sent-34, score-0.455]
</p><p>19 1  Previous work in image object detection  The task of object detection in images is quite daunting. [sent-37, score-0.854]
</p><p>20 Amongst the challenges are 1) creating a system with high accuracy and low false detection rate, 2) restricting the system to consume a reasonable amount of CPU time, and 3) creating a large training set that has low labeling error. [sent-38, score-0.427]
</p><p>21 These models can be trained using unsegmented images in which the object can appear at any location. [sent-41, score-0.257]
</p><p>22 However, hitherto, the detection accuracy has not be as good as the best methods. [sent-43, score-0.235]
</p><p>23 It is also very efﬁcient, because it uses a cascade of detectors and very simple image features. [sent-46, score-0.25]
</p><p>24 The exact location and size of the hands is approximately truthed: the neural network used MIL training to co-learn the object location and the parameters of the classiﬁer. [sent-51, score-0.292]
</p><p>25 Unlike Nowlan and Platt, we maintain a cascade of detectors for maximum speed. [sent-56, score-0.174]
</p><p>26 Diverse Density uses the Noisy OR generative model [6] to explain the bag labels. [sent-66, score-0.348]
</p><p>27 Finally, a number of researchers have modiﬁed the boosting algorithm to perform MIL. [sent-70, score-0.2]
</p><p>28 For example, Andrews and Hofmann [1] have proposed modifying the inner loop of boosting to use linear programming. [sent-71, score-0.2]
</p><p>29 This is not practically applicable to the object detection task, which can have millions of examples (pixels) and thousands of bags. [sent-72, score-0.441]
</p><p>30 A third approach is that of Xu and Frank [14], which uses a generative model that the probability of a bag being positive is the mean of the probabilities that the examples are positive. [sent-75, score-0.538]
</p><p>31 We believe that this rule is unsuited for object detection, because only a small subset of the examples in the bag are ever positive. [sent-76, score-0.58]
</p><p>32 which views boosting as a gradient descent process [9]. [sent-79, score-0.2]
</p><p>33 1  Noisy-OR Boost  Recall in boosting each example is classiﬁed by a linear combination of weak classiﬁers. [sent-83, score-0.292]
</p><p>34 The score of the example is yij = C(xij ) and C(xij ) = t t λt c (xij ) a weighted sum of weak classiﬁers. [sent-87, score-0.191]
</p><p>35 The probability of an example is positive is given by 1 pij = , 1 + exp(−yij ) the standard logistic function. [sent-88, score-0.185]
</p><p>36 The probability that the bag is positive is a “noisy OR” pi = 1 − j∈i (1 − pij ) [6] [8]. [sent-89, score-0.63]
</p><p>37 Under this model the likelihood assigned to a set of training bags is: L(C) = pti (1 − pi )(1−ti ) i i  where ti ∈ {0, 1} is the label of bag i. [sent-90, score-0.728]
</p><p>38 The derivative of the log likelihood is: ∂ log L(C) t i − pi = wij = pij . [sent-92, score-0.212]
</p><p>39 Each round of boosting is a search for a classiﬁer which maximizes ij c(xij )wij where c(xij ) is the score assigned to the example by the weak classiﬁer (for a binary classiﬁer c(xij ) ∈ {−1, +1}). [sent-95, score-0.409]
</p><p>40 Examining the criteria (1) the weight on each example is the product of two quantities: the bag weight Wbag = ti −pi and the instance weight Winstance = pij . [sent-97, score-0.754]
</p><p>41 Observe that Wbag for pi a negative bag is always −1. [sent-98, score-0.52]
</p><p>42 Thus, the weight for a negative instance, pij , is the same that would result in a non-MIL AdaBoost framework (i. [sent-99, score-0.247]
</p><p>43 The weight on the positive instances is more complex. [sent-102, score-0.167]
</p><p>44 As learning proceeds and the probability of the bag approaches the target, the weight on the entire bag is reduced. [sent-103, score-0.759]
</p><p>45 Within the bag, the examples are assigned a weight which is higher for examples with higher scores. [sent-104, score-0.279]
</p><p>46 Intuitively the algorithm selects a subset of examples to assign a higher positive weight, and these example dominate subsequent learning. [sent-105, score-0.19]
</p><p>47 The quantity Si can be interpreted as a likelihood ratio that some (at least one) instance is positive, and ﬁnally pi is the probability that some instance is positive. [sent-113, score-0.223]
</p><p>48 The example weights for the ISR framework are: ∂ log L(C) χij (2) = wij = (ti − pi ) ∂yij j∈i χij  Examining the ISR criteria reveals two key properties. [sent-114, score-0.221]
</p><p>49 The examples in the bag compete for weight, since the weight is normalized by sum of the χij ’s. [sent-116, score-0.528]
</p><p>50 Though the experimental evidence is weak, this rule perhaps leads to a very localized representation, where a single example is labeled positive and the other examples are labeled negative. [sent-117, score-0.318]
</p><p>51 The second property is that the negative examples also compete for weight. [sent-118, score-0.192]
</p><p>52 This turns out to be troublesome in the detection framework since there are many, many more negative examples than positive. [sent-119, score-0.398]
</p><p>53 In contrast, the Noisy OR criteria treats all negative examples as independent negative examples. [sent-121, score-0.271]
</p><p>54 4  Application of MIL Boost to Object Detection in Images  Each image is divided into a set of overlapping square windows that uniformly sample the space of position and scale (typically there are between 10,000 and 100,000 windows in a training image). [sent-122, score-0.352]
</p><p>55 Each training image is labeled to determine the position and scale of the object of interest. [sent-124, score-0.348]
</p><p>56 One possibility is to localize the eyes and then to determine the single positive image window in which the eyes appear at a given relative location and scale. [sent-126, score-0.303]
</p><p>57 Even for this type of object the effort in carefully labeling the images is signiﬁcant. [sent-127, score-0.214]
</p><p>58 For many other types of objects (objects which may be visible from multiple poses, or  Figure 1: Two example images with people in a wide variety of poses. [sent-128, score-0.217]
</p><p>59 It is not clear how to normalize images of people in a conference room, who may be standing, sitting upright, reclining, looking toward, or looking away from the camera. [sent-132, score-0.165]
</p><p>60 In every training image each person is labeled by hand. [sent-138, score-0.246]
</p><p>61 At the available resolution (approximately 1000x150 pixels) the head is often less than 10 pixels wide. [sent-141, score-0.186]
</p><p>62 At this resolution, even for clear frontal faces, the best face detection algorithms frequently fail. [sent-142, score-0.29]
</p><p>63 The only way to detect the head is to include the surrounding image context. [sent-144, score-0.233]
</p><p>64 Each positive head is represented, during training, by a large number of related image windows (see Figure 2). [sent-150, score-0.444]
</p><p>65 The MIL boosting algorithm is then used to simultaneously learn a detector and determine the location and scale of the appropriate image context. [sent-151, score-0.426]
</p><p>66 5  MIL Boosting a Detection Cascade  In their work on face detection Viola and Jones train a cascade of classiﬁers, each designed to achieve high detection rates and modest false positive rates. [sent-152, score-0.906]
</p><p>67 During detection almost all of the computation is performed by the early stages in the cascade, perhaps 90% in the ﬁrst 10 features. [sent-153, score-0.209]
</p><p>68 Training the initial stages of the cascade is the key to a fast and effective classiﬁer. [sent-154, score-0.2]
</p><p>69 Training and evaluating a detector in a MIL framework has a direct impact on cascade construction, both on the features selected and the appropriate thresholds. [sent-155, score-0.31]
</p><p>70 Those examples in positive bags which are assigned high weight have also high score. [sent-157, score-0.436]
</p><p>71 The remaining examples in the positive bags are assigned a low weight and have a low score. [sent-159, score-0.436]
</p><p>72 Since boosting is a greedy process, the initial weak classiﬁers do not have any knowledge of the subsequent classiﬁers. [sent-161, score-0.292]
</p><p>73 The key to efﬁcient processing, is that the initial classiﬁers have a low false negative rate on the examples determined to be positive by the ﬁnal MIL classiﬁer. [sent-163, score-0.433]
</p><p>74 Train a complete MIL boosted classiﬁer and set the detection threshold to achieve the desired false positive and false negative rates. [sent-165, score-0.679]
</p><p>75 Retrain the initial weak classiﬁer so that it has a zero false negative rate on the examples labeled positive by the full classiﬁer. [sent-166, score-0.589]
</p><p>76 The process can be repeated, so that the second classiﬁer is trained to yield a zero false negative rate on the remaining examples. [sent-168, score-0.286]
</p><p>77 In all cases the detector was trained on 7 video conferences and tested on the remaining video conference. [sent-171, score-0.215]
</p><p>78 Each was labeled by drawing a rectangle around the head of each person. [sent-173, score-0.25]
</p><p>79 Figure 3: ROC comparison between various boosting rules. [sent-180, score-0.2]
</p><p>80 For the MIL algorithms there is one bag for each labeled head, containing those positive windows which overlap that head. [sent-183, score-0.656]
</p><p>81 Additionally there is one negative bag for each image. [sent-184, score-0.423]
</p><p>82 During training a set of positive windows are generated for each labeled example. [sent-186, score-0.337]
</p><p>83 5 times the head width and whose center is within 0. [sent-189, score-0.212]
</p><p>84 5 times the head width of the center of the head are labeled positive. [sent-190, score-0.433]
</p><p>85 An exception is made for AdaBoost, which has a tighter deﬁnition on positive examples (width between 0. [sent-191, score-0.19]
</p><p>86 All windows which do not overlap with any head are considered negative. [sent-195, score-0.297]
</p><p>87 A second experiment corrupts this ground truth further, moving each head by a uniform random shift such that there is non-zero overlap with the true position. [sent-197, score-0.26]
</p><p>88 A typical example of detection results are shown in Figure 4. [sent-202, score-0.209]
</p><p>89 In order to simplify the display, signiﬁcantly overlapping detection windows are averaged into a single window. [sent-204, score-0.316]
</p><p>90 The scheme for retraining the initial classiﬁer was evaluated on the noisy OR strong classiﬁer trained above. [sent-205, score-0.171]
</p><p>91 Training a conventional cascade requires ﬁnding a small set of weak classiﬁers that can achieve zero false negative rate (or almost zero) and a low false positive rate. [sent-206, score-0.743]
</p><p>92 Using the ﬁrst weak classiﬁer yields a false positive rate of 39. [sent-207, score-0.364]
</p><p>93 Including the ﬁrst  four weak classiﬁers yields a false positive rate of 21. [sent-209, score-0.364]
</p><p>94 After retraining the ﬁrst weak classiﬁer alone yields a false positive rate of 11. [sent-211, score-0.43]
</p><p>95 This improved rejection rate has the effect of reducing computation time of the cascade by roughly a factor of three. [sent-213, score-0.212]
</p><p>96 7  Conclusions  This paper combines the truthing ﬂexibility of multiple instance learning with the high accuracy of the boosted object detector of Viola and Jones. [sent-214, score-0.411]
</p><p>97 Maximum likelihood on the output of these bag combination functions ﬁt within the AnyBoost framework, which generates boosting weights for each example. [sent-217, score-0.575]
</p><p>98 NorBoost improves the detection rate over standard AdaBoost (tight positive) by nearly 15% (at a 10% false positive rate). [sent-219, score-0.481]
</p><p>99 Using MILBoost for object detection allows the detector to ﬂexibly assign labels to the training set, which reduces label noise and improves performance. [sent-220, score-0.525]
</p><p>100 Logistic regression and boosting for labeled bags of instances. [sent-324, score-0.403]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mil', 0.458), ('bag', 0.348), ('isr', 0.219), ('milboost', 0.219), ('detection', 0.209), ('boosting', 0.2), ('cascade', 0.174), ('adaboost', 0.16), ('head', 0.157), ('object', 0.146), ('bags', 0.139), ('false', 0.13), ('classi', 0.119), ('viola', 0.111), ('keeler', 0.11), ('detector', 0.108), ('windows', 0.107), ('positive', 0.104), ('pi', 0.097), ('anyboost', 0.095), ('weak', 0.092), ('nowlan', 0.088), ('xij', 0.087), ('examples', 0.086), ('pij', 0.081), ('er', 0.08), ('image', 0.076), ('negative', 0.075), ('boost', 0.073), ('yij', 0.073), ('ers', 0.07), ('objects', 0.069), ('images', 0.068), ('retraining', 0.066), ('teleconferencing', 0.066), ('labeled', 0.064), ('instance', 0.063), ('weight', 0.063), ('training', 0.062), ('noisy', 0.062), ('jones', 0.058), ('width', 0.055), ('face', 0.053), ('ij', 0.047), ('platt', 0.046), ('assigned', 0.044), ('person', 0.044), ('auer', 0.044), ('norboost', 0.044), ('reclining', 0.044), ('schmid', 0.044), ('subwindows', 0.044), ('unavoidable', 0.044), ('wbag', 0.044), ('trained', 0.043), ('people', 0.043), ('location', 0.042), ('convolutional', 0.042), ('volume', 0.038), ('ground', 0.038), ('maron', 0.038), ('rate', 0.038), ('ti', 0.038), ('multiple', 0.037), ('criteria', 0.035), ('zip', 0.035), ('mason', 0.035), ('wij', 0.034), ('overlap', 0.033), ('pages', 0.033), ('andrews', 0.032), ('video', 0.032), ('truth', 0.032), ('nal', 0.031), ('recognition', 0.031), ('si', 0.031), ('notes', 0.031), ('boosted', 0.031), ('constellation', 0.031), ('compete', 0.031), ('sub', 0.031), ('microsoft', 0.031), ('rectangle', 0.029), ('localize', 0.029), ('pixels', 0.029), ('framework', 0.028), ('perona', 0.028), ('frontal', 0.028), ('variant', 0.028), ('weights', 0.027), ('xu', 0.027), ('indexes', 0.027), ('train', 0.027), ('al', 0.027), ('looking', 0.027), ('accuracy', 0.026), ('derivation', 0.026), ('score', 0.026), ('fast', 0.026), ('eyes', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="131-tfidf-1" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>Author: Cha Zhang, John C. Platt, Paul A. Viola</p><p>Abstract: A good image object detection algorithm is accurate, fast, and does not require exact locations of objects in a training set. We can create such an object detector by taking the architecture of the Viola-Jones detector cascade and training it with a new variant of boosting that we call MILBoost. MILBoost uses cost functions from the Multiple Instance Learning literature combined with the AnyBoost framework. We adapt the feature selection criterion of MILBoost to optimize the performance of the Viola-Jones cascade. Experiments show that the detection rate is up to 1.6 times better using MILBoost. This increased detection rate shows the advantage of simultaneously learning the locations and scales of the objects in the training set along with the parameters of the classiﬁer. 1</p><p>2 0.19884619 <a title="131-tfidf-2" href="./nips-2005-A_Computational_Model_of_Eye_Movements_during_Object_Class_Detection.html">5 nips-2005-A Computational Model of Eye Movements during Object Class Detection</a></p>
<p>Author: Wei Zhang, Hyejin Yang, Dimitris Samaras, Gregory J. Zelinsky</p><p>Abstract: We present a computational model of human eye movements in an object class detection task. The model combines state-of-the-art computer vision object class detection methods (SIFT features trained using AdaBoost) with a biologically plausible model of human eye movement to produce a sequence of simulated ﬁxations, culminating with the acquisition of a target. We validated the model by comparing its behavior to the behavior of human observers performing the identical object class detection task (looking for a teddy bear among visually complex nontarget objects). We found considerable agreement between the model and human data in multiple eye movement measures, including number of ﬁxations, cumulative probability of ﬁxating the target, and scanpath distance.</p><p>3 0.15548293 <a title="131-tfidf-3" href="./nips-2005-Efficient_Unsupervised_Learning_for_Localization_and_Detection_in_Object_Categories.html">63 nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</a></p>
<p>Author: Nicolas Loeff, Himanshu Arora, Alexander Sorokin, David Forsyth</p><p>Abstract: We describe a novel method for learning templates for recognition and localization of objects drawn from categories. A generative model represents the conﬁguration of multiple object parts with respect to an object coordinate system; these parts in turn generate image features. The complexity of the model in the number of features is low, meaning our model is much more efﬁcient to train than comparative methods. Moreover, a variational approximation is introduced that allows learning to be orders of magnitude faster than previous approaches while incorporating many more features. This results in both accuracy and localization improvements. Our model has been carefully tested on standard datasets; we compare with a number of recent template models. In particular, we demonstrate state-of-the-art results for detection and localization. 1</p><p>4 0.11284705 <a title="131-tfidf-4" href="./nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">151 nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<p>Author: Francois Fleuret, Gilles Blanchard</p><p>Abstract: We investigate the learning of the appearance of an object from a single image of it. Instead of using a large number of pictures of the object to recognize, we use a labeled reference database of pictures of other objects to learn invariance to noise and variations in pose and illumination. This acquired knowledge is then used to predict if two pictures of new objects, which do not appear on the training pictures, actually display the same object. We propose a generic scheme called chopping to address this task. It relies on hundreds of random binary splits of the training set chosen to keep together the images of any given object. Those splits are extended to the complete image space with a simple learning algorithm. Given two images, the responses of the split predictors are combined with a Bayesian rule into a posterior probability of similarity. Experiments with the COIL-100 database and with a database of 150 deA graded LTEX symbols compare our method to a classical learning with several examples of the positive class and to a direct learning of the similarity. 1</p><p>5 0.1033155 <a title="131-tfidf-5" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>Author: Kilian Q. Weinberger, John Blitzer, Lawrence K. Saul</p><p>Abstract: We show how to learn a Mahanalobis distance metric for k-nearest neighbor (kNN) classiﬁcation by semideﬁnite programming. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. On seven data sets of varying size and difﬁculty, we ﬁnd that metrics trained in this way lead to signiﬁcant improvements in kNN classiﬁcation—for example, achieving a test error rate of 1.3% on the MNIST handwritten digits. As in support vector machines (SVMs), the learning problem reduces to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our framework requires no modiﬁcation or extension for problems in multiway (as opposed to binary) classiﬁcation. 1</p><p>6 0.099669695 <a title="131-tfidf-6" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>7 0.091705605 <a title="131-tfidf-7" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>8 0.080189951 <a title="131-tfidf-8" href="./nips-2005-Fusion_of_Similarity_Data_in_Clustering.html">79 nips-2005-Fusion of Similarity Data in Clustering</a></p>
<p>9 0.078983739 <a title="131-tfidf-9" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>10 0.077767409 <a title="131-tfidf-10" href="./nips-2005-Describing_Visual_Scenes_using_Transformed_Dirichlet_Processes.html">55 nips-2005-Describing Visual Scenes using Transformed Dirichlet Processes</a></p>
<p>11 0.076882802 <a title="131-tfidf-11" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>12 0.076162927 <a title="131-tfidf-12" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>13 0.075123355 <a title="131-tfidf-13" href="./nips-2005-Inferring_Motor_Programs_from_Images_of_Handwritten_Digits.html">97 nips-2005-Inferring Motor Programs from Images of Handwritten Digits</a></p>
<p>14 0.062384289 <a title="131-tfidf-14" href="./nips-2005-Off-Road_Obstacle_Avoidance_through_End-to-End_Learning.html">143 nips-2005-Off-Road Obstacle Avoidance through End-to-End Learning</a></p>
<p>15 0.061426383 <a title="131-tfidf-15" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>16 0.059817452 <a title="131-tfidf-16" href="./nips-2005-Transfer_learning_for_text_classification.html">195 nips-2005-Transfer learning for text classification</a></p>
<p>17 0.059708778 <a title="131-tfidf-17" href="./nips-2005-An_Application_of_Markov_Random_Fields_to_Range_Sensing.html">23 nips-2005-An Application of Markov Random Fields to Range Sensing</a></p>
<p>18 0.059699398 <a title="131-tfidf-18" href="./nips-2005-Location-based_activity_recognition.html">121 nips-2005-Location-based activity recognition</a></p>
<p>19 0.059184372 <a title="131-tfidf-19" href="./nips-2005-Infinite_latent_feature_models_and_the_Indian_buffet_process.html">98 nips-2005-Infinite latent feature models and the Indian buffet process</a></p>
<p>20 0.055126984 <a title="131-tfidf-20" href="./nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">113 nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.199), (1, 0.031), (2, -0.033), (3, 0.152), (4, -0.003), (5, 0.147), (6, 0.132), (7, 0.179), (8, -0.033), (9, -0.092), (10, -0.006), (11, -0.047), (12, 0.003), (13, 0.054), (14, 0.109), (15, 0.022), (16, -0.084), (17, -0.091), (18, 0.072), (19, 0.044), (20, -0.012), (21, 0.214), (22, 0.002), (23, 0.058), (24, -0.135), (25, -0.044), (26, -0.049), (27, 0.017), (28, -0.043), (29, -0.008), (30, -0.041), (31, -0.006), (32, -0.088), (33, 0.076), (34, 0.007), (35, -0.076), (36, 0.02), (37, 0.047), (38, -0.076), (39, -0.094), (40, 0.058), (41, 0.022), (42, 0.08), (43, -0.153), (44, -0.042), (45, 0.115), (46, 0.104), (47, -0.037), (48, -0.019), (49, -0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9598521 <a title="131-lsi-1" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>Author: Cha Zhang, John C. Platt, Paul A. Viola</p><p>Abstract: A good image object detection algorithm is accurate, fast, and does not require exact locations of objects in a training set. We can create such an object detector by taking the architecture of the Viola-Jones detector cascade and training it with a new variant of boosting that we call MILBoost. MILBoost uses cost functions from the Multiple Instance Learning literature combined with the AnyBoost framework. We adapt the feature selection criterion of MILBoost to optimize the performance of the Viola-Jones cascade. Experiments show that the detection rate is up to 1.6 times better using MILBoost. This increased detection rate shows the advantage of simultaneously learning the locations and scales of the objects in the training set along with the parameters of the classiﬁer. 1</p><p>2 0.7270537 <a title="131-lsi-2" href="./nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">151 nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<p>Author: Francois Fleuret, Gilles Blanchard</p><p>Abstract: We investigate the learning of the appearance of an object from a single image of it. Instead of using a large number of pictures of the object to recognize, we use a labeled reference database of pictures of other objects to learn invariance to noise and variations in pose and illumination. This acquired knowledge is then used to predict if two pictures of new objects, which do not appear on the training pictures, actually display the same object. We propose a generic scheme called chopping to address this task. It relies on hundreds of random binary splits of the training set chosen to keep together the images of any given object. Those splits are extended to the complete image space with a simple learning algorithm. Given two images, the responses of the split predictors are combined with a Bayesian rule into a posterior probability of similarity. Experiments with the COIL-100 database and with a database of 150 deA graded LTEX symbols compare our method to a classical learning with several examples of the positive class and to a direct learning of the similarity. 1</p><p>3 0.72051769 <a title="131-lsi-3" href="./nips-2005-Efficient_Unsupervised_Learning_for_Localization_and_Detection_in_Object_Categories.html">63 nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</a></p>
<p>Author: Nicolas Loeff, Himanshu Arora, Alexander Sorokin, David Forsyth</p><p>Abstract: We describe a novel method for learning templates for recognition and localization of objects drawn from categories. A generative model represents the conﬁguration of multiple object parts with respect to an object coordinate system; these parts in turn generate image features. The complexity of the model in the number of features is low, meaning our model is much more efﬁcient to train than comparative methods. Moreover, a variational approximation is introduced that allows learning to be orders of magnitude faster than previous approaches while incorporating many more features. This results in both accuracy and localization improvements. Our model has been carefully tested on standard datasets; we compare with a number of recent template models. In particular, we demonstrate state-of-the-art results for detection and localization. 1</p><p>4 0.66177827 <a title="131-lsi-4" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>Author: Long Zhu, Alan L. Yuille</p><p>Abstract: We describe a hierarchical compositional system for detecting deformable objects in images. Objects are represented by graphical models. The algorithm uses a hierarchical tree where the root of the tree corresponds to the full object and lower-level elements of the tree correspond to simpler features. The algorithm proceeds by passing simple messages up and down the tree. The method works rapidly, in under a second, on 320 × 240 images. We demonstrate the approach on detecting cats, horses, and hands. The method works in the presence of background clutter and occlusions. Our approach is contrasted with more traditional methods such as dynamic programming and belief propagation. 1</p><p>5 0.60773891 <a title="131-lsi-5" href="./nips-2005-A_Computational_Model_of_Eye_Movements_during_Object_Class_Detection.html">5 nips-2005-A Computational Model of Eye Movements during Object Class Detection</a></p>
<p>Author: Wei Zhang, Hyejin Yang, Dimitris Samaras, Gregory J. Zelinsky</p><p>Abstract: We present a computational model of human eye movements in an object class detection task. The model combines state-of-the-art computer vision object class detection methods (SIFT features trained using AdaBoost) with a biologically plausible model of human eye movement to produce a sequence of simulated ﬁxations, culminating with the acquisition of a target. We validated the model by comparing its behavior to the behavior of human observers performing the identical object class detection task (looking for a teddy bear among visually complex nontarget objects). We found considerable agreement between the model and human data in multiple eye movement measures, including number of ﬁxations, cumulative probability of ﬁxating the target, and scanpath distance.</p><p>6 0.44108564 <a title="131-lsi-6" href="./nips-2005-Describing_Visual_Scenes_using_Transformed_Dirichlet_Processes.html">55 nips-2005-Describing Visual Scenes using Transformed Dirichlet Processes</a></p>
<p>7 0.4312993 <a title="131-lsi-7" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>8 0.42599228 <a title="131-lsi-8" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>9 0.41457912 <a title="131-lsi-9" href="./nips-2005-Inferring_Motor_Programs_from_Images_of_Handwritten_Digits.html">97 nips-2005-Inferring Motor Programs from Images of Handwritten Digits</a></p>
<p>10 0.41395125 <a title="131-lsi-10" href="./nips-2005-Identifying_Distributed_Object_Representations_in_Human_Extrastriate_Visual_Cortex.html">94 nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</a></p>
<p>11 0.40673262 <a title="131-lsi-11" href="./nips-2005-Fusion_of_Similarity_Data_in_Clustering.html">79 nips-2005-Fusion of Similarity Data in Clustering</a></p>
<p>12 0.39723328 <a title="131-lsi-12" href="./nips-2005-Gaussian_Processes_for_Multiuser_Detection_in_CDMA_receivers.html">81 nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</a></p>
<p>13 0.39417043 <a title="131-lsi-13" href="./nips-2005-Searching_for_Character_Models.html">171 nips-2005-Searching for Character Models</a></p>
<p>14 0.37803933 <a title="131-lsi-14" href="./nips-2005-Transfer_learning_for_text_classification.html">195 nips-2005-Transfer learning for text classification</a></p>
<p>15 0.36177126 <a title="131-lsi-15" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>16 0.35876301 <a title="131-lsi-16" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>17 0.34601131 <a title="131-lsi-17" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>18 0.33404341 <a title="131-lsi-18" href="./nips-2005-Tensor_Subspace_Analysis.html">189 nips-2005-Tensor Subspace Analysis</a></p>
<p>19 0.32043049 <a title="131-lsi-19" href="./nips-2005-Off-Road_Obstacle_Avoidance_through_End-to-End_Learning.html">143 nips-2005-Off-Road Obstacle Avoidance through End-to-End Learning</a></p>
<p>20 0.31689578 <a title="131-lsi-20" href="./nips-2005-Location-based_activity_recognition.html">121 nips-2005-Location-based activity recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.505), (10, 0.015), (27, 0.017), (31, 0.039), (34, 0.086), (39, 0.043), (41, 0.017), (55, 0.023), (69, 0.043), (73, 0.028), (88, 0.079), (91, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97851014 <a title="131-lda-1" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>Author: Nicolas Usunier, Massih-reza Amini, Patrick Gallinari</p><p>Abstract: In this paper we propose a general framework to study the generalization properties of binary classiﬁers trained with data which may be dependent, but are deterministically generated upon a sample of independent examples. It provides generalization bounds for binary classiﬁcation and some cases of ranking problems, and clariﬁes the relationship between these learning tasks. 1</p><p>same-paper 2 0.95710135 <a title="131-lda-2" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>Author: Cha Zhang, John C. Platt, Paul A. Viola</p><p>Abstract: A good image object detection algorithm is accurate, fast, and does not require exact locations of objects in a training set. We can create such an object detector by taking the architecture of the Viola-Jones detector cascade and training it with a new variant of boosting that we call MILBoost. MILBoost uses cost functions from the Multiple Instance Learning literature combined with the AnyBoost framework. We adapt the feature selection criterion of MILBoost to optimize the performance of the Viola-Jones cascade. Experiments show that the detection rate is up to 1.6 times better using MILBoost. This increased detection rate shows the advantage of simultaneously learning the locations and scales of the objects in the training set along with the parameters of the classiﬁer. 1</p><p>3 0.95262128 <a title="131-lda-3" href="./nips-2005-Learning_from_Data_of_Variable_Quality.html">117 nips-2005-Learning from Data of Variable Quality</a></p>
<p>Author: Koby Crammer, Michael Kearns, Jennifer Wortman</p><p>Abstract: We initiate the study of learning from multiple sources of limited data, each of which may be corrupted at a different rate. We develop a complete theory of which data sources should be used for two fundamental problems: estimating the bias of a coin, and learning a classiﬁer in the presence of label noise. In both cases, efﬁcient algorithms are provided for computing the optimal subset of data. 1</p><p>4 0.95169348 <a title="131-lda-4" href="./nips-2005-Q-Clustering.html">159 nips-2005-Q-Clustering</a></p>
<p>Author: Mukund Narasimhan, Nebojsa Jojic, Jeff A. Bilmes</p><p>Abstract: We show that Queyranne’s algorithm for minimizing symmetric submodular functions can be used for clustering with a variety of different objective functions. Two speciﬁc criteria that we consider in this paper are the single linkage and the minimum description length criteria. The ﬁrst criterion tries to maximize the minimum distance between elements of different clusters, and is inherently “discriminative”. It is known that optimal clusterings into k clusters for any given k in polynomial time for this criterion can be computed. The second criterion seeks to minimize the description length of the clusters given a probabilistic generative model. We show that the optimal partitioning into 2 clusters, and approximate partitioning (guaranteed to be within a factor of 2 of the the optimal) for more clusters can be computed. To the best of our knowledge, this is the ﬁrst time that a tractable algorithm for ﬁnding the optimal clustering with respect to the MDL criterion for 2 clusters has been given. Besides the optimality result for the MDL criterion, the chief contribution of this paper is to show that the same algorithm can be used to optimize a broad class of criteria, and hence can be used for many application speciﬁc criterion for which efﬁcient algorithm are not known.</p><p>5 0.82125843 <a title="131-lda-5" href="./nips-2005-Generalization_to_Unseen_Cases.html">85 nips-2005-Generalization to Unseen Cases</a></p>
<p>Author: Teemu Roos, Peter Grünwald, Petri Myllymäki, Henry Tirri</p><p>Abstract: We analyze classiﬁcation error on unseen cases, i.e. cases that are different from those in the training set. Unlike standard generalization error, this off-training-set error may differ signiﬁcantly from the empirical error with high probability even with large sample sizes. We derive a datadependent bound on the difference between off-training-set and standard generalization error. Our result is based on a new bound on the missing mass, which for small samples is stronger than existing bounds based on Good-Turing estimators. As we demonstrate on UCI data-sets, our bound gives nontrivial generalization guarantees in many practical cases. In light of these results, we show that certain claims made in the No Free Lunch literature are overly pessimistic. 1</p><p>6 0.79273391 <a title="131-lda-6" href="./nips-2005-Learning_Minimum_Volume_Sets.html">112 nips-2005-Learning Minimum Volume Sets</a></p>
<p>7 0.71623284 <a title="131-lda-7" href="./nips-2005-Coarse_sample_complexity_bounds_for_active_learning.html">41 nips-2005-Coarse sample complexity bounds for active learning</a></p>
<p>8 0.70623946 <a title="131-lda-8" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>9 0.70392799 <a title="131-lda-9" href="./nips-2005-Two_view_learning%3A_SVM-2K%2C_Theory_and_Practice.html">196 nips-2005-Two view learning: SVM-2K, Theory and Practice</a></p>
<p>10 0.6593048 <a title="131-lda-10" href="./nips-2005-Generalization_in_Clustering_with_Unobserved_Features.html">84 nips-2005-Generalization in Clustering with Unobserved Features</a></p>
<p>11 0.65560651 <a title="131-lda-11" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>12 0.65430439 <a title="131-lda-12" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>13 0.65076154 <a title="131-lda-13" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>14 0.64130473 <a title="131-lda-14" href="./nips-2005-The_Forgetron%3A_A_Kernel-Based_Perceptron_on_a_Fixed_Budget.html">191 nips-2005-The Forgetron: A Kernel-Based Perceptron on a Fixed Budget</a></p>
<p>15 0.63946015 <a title="131-lda-15" href="./nips-2005-On_the_Convergence_of_Eigenspaces_in_Kernel_Principal_Component_Analysis.html">147 nips-2005-On the Convergence of Eigenspaces in Kernel Principal Component Analysis</a></p>
<p>16 0.62949932 <a title="131-lda-16" href="./nips-2005-Query_by_Committee_Made_Real.html">160 nips-2005-Query by Committee Made Real</a></p>
<p>17 0.62861836 <a title="131-lda-17" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>18 0.62666541 <a title="131-lda-18" href="./nips-2005-Improved_risk_tail_bounds_for_on-line_algorithms.html">95 nips-2005-Improved risk tail bounds for on-line algorithms</a></p>
<p>19 0.62414497 <a title="131-lda-19" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>20 0.62257457 <a title="131-lda-20" href="./nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">151 nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
