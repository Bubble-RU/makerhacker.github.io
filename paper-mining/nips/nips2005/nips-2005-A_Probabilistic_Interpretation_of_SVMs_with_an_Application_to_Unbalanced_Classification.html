<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-14" href="#">nips2005-14</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</h1>
<br/><p>Source: <a title="nips-2005-14-pdf" href="http://papers.nips.cc/paper/2763-a-probabilistic-interpretation-of-svms-with-an-application-to-unbalanced-classification.pdf">pdf</a></p><p>Author: Yves Grandvalet, Johnny Mariethoz, Samy Bengio</p><p>Abstract: In this paper, we show that the hinge loss can be interpreted as the neg-log-likelihood of a semi-parametric model of posterior probabilities. From this point of view, SVMs represent the parametric component of a semi-parametric model ﬁtted by a maximum a posteriori estimation procedure. This connection enables to derive a mapping from SVM scores to estimated posterior probabilities. Unlike previous proposals, the suggested mapping is interval-valued, providing a set of posterior probabilities compatible with each SVM score. This framework offers a new way to adapt the SVM optimization problem to unbalanced classiﬁcation, when decisions result in unequal (asymmetric) losses. Experiments show improvements over state-of-the-art procedures. 1</p><p>Reference: <a title="nips-2005-14-reference" href="../nips2005_reference/nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ch  Abstract In this paper, we show that the hinge loss can be interpreted as the neg-log-likelihood of a semi-parametric model of posterior probabilities. [sent-3, score-0.507]
</p><p>2 From this point of view, SVMs represent the parametric component of a semi-parametric model ﬁtted by a maximum a posteriori estimation procedure. [sent-4, score-0.194]
</p><p>3 This connection enables to derive a mapping from SVM scores to estimated posterior probabilities. [sent-5, score-0.387]
</p><p>4 Unlike previous proposals, the suggested mapping is interval-valued, providing a set of posterior probabilities compatible with each SVM score. [sent-6, score-0.482]
</p><p>5 This framework offers a new way to adapt the SVM optimization problem to unbalanced classiﬁcation, when decisions result in unequal (asymmetric) losses. [sent-7, score-0.255]
</p><p>6 1  Introduction  In this paper, we show that support vector machines (SVMs) are the solution of a relaxed maximum a posteriori (MAP) estimation problem. [sent-9, score-0.144]
</p><p>7 This relaxed problem results from ﬁtting a semi-parametric model of posterior probabilities. [sent-10, score-0.277]
</p><p>8 This model is decomposed into two components: the parametric component, which is a function of the SVM score, and the non-parametric component which we call a nuisance function. [sent-11, score-0.402]
</p><p>9 Given a proper binding of the nuisance function adapted to the considered problem, this decomposition enables to concentrate on selected ranges of the probability spectrum. [sent-12, score-0.246]
</p><p>10 The estimation process can thus allocate model capacity to the neighborhoods of decision boundaries. [sent-13, score-0.171]
</p><p>11 The connection to semi-parametric models provides a probabilistic interpretation of SVM scores, which may have several applications, such as estimating conﬁdences over the predictions, or dealing with unbalanced losses. [sent-14, score-0.42]
</p><p>12 Several mappings relating SVM scores to probabilities have already been proposed (Sollich 2000, Platt 2000), but they are subject to arbitrary choices, which are avoided here by their integration to the nuisance function. [sent-16, score-0.568]
</p><p>13 Section 2 presents the semi-parametric modeling approach; Section 3 shows how we reformulate SVM in this framework; Section 4 proposes several outcomes of this formulation, including a new method to handle unbalanced losses, which is tested empirically in Section 5. [sent-18, score-0.259]
</p><p>14 2  Semi-Parametric Classiﬁcation  We address the binary classiﬁcation problem of estimating a decision rule from a learning set Ln = {(xi , yi )}n , where the ith example is described by the pattern xi ∈ X and i=1 the associated response yi ∈ {−1, 1}. [sent-22, score-0.407]
</p><p>15 1  Complete and Marginal Likelihood, Nuisance Functions  Let p(1|x; θ) denote the model of P (Y = 1|X = x), p(x; ψ) the model of P (X) and ti the binary response variable such that ti = 1 when yi = 1 and ti = 0 when yi = −1. [sent-27, score-0.641]
</p><p>16 For classiﬁcation purposes, the parameter ψ is not relevant, and may thus be qualiﬁed as a nuisance parameter (Lindsay 1985). [sent-29, score-0.246]
</p><p>17 When θ can be estimated independently of ψ, maximizing the marginal likelihood provides the estimate returned by maximizing the complete likelihood with respect to θ and ψ. [sent-30, score-0.33]
</p><p>18 In particular, when no assumption whatsoever is made on P (X), maximizing the conditional likelihood amounts to maximize the joint likelihood (McLachlan 1992). [sent-31, score-0.164]
</p><p>19 The density of inputs is then considered as a nuisance function. [sent-32, score-0.246]
</p><p>20 We may thus consider looking for the decision rule minimizing the empirical classiﬁcation error, but this problem is intractable for non-trivial models of discriminant functions. [sent-36, score-0.137]
</p><p>21 For this, we consider a two-component semi-parametric model of P (Y = 1|X = x), deﬁned as p(1|x; θ) = g(x; θ) + ε(x), where the parametric component g(x; θ) is the function of interest, and where the non-parametric component ε is a constrained nuisance function. [sent-38, score-0.424]
</p><p>22 Then, we address the maximum likelihood estimation of the semi-parametric model p(1|x; θ)   min − ti log(p(1|xi ; θ)) + (1 − ti ) log(1 − p(1|xi ; θ))  θ,ε   i (2) s. [sent-39, score-0.577]
</p><p>23 At the one extreme, when ε− = ε+ , one recovers a parametric maximum likelihood problem, where the estimate of posterior probabilities p(1|x; θ) is simply g(x; θ) shifted by the baseline function ε. [sent-43, score-0.638]
</p><p>24 Note that the optimization problem in ε is always ill-posed, but this is not of concern as we do not wish to estimate the nuisance function. [sent-45, score-0.356]
</p><p>25 Generally, as ε is not estimated, the estimate of posterior probabilities p(1|x; θ) is only known to lie within the interval [g(x; θ) + ε− (x), g(x; θ) + ε+ (x)]. [sent-54, score-0.485]
</p><p>26 We also require ε− (x) ≤ 0 ≤ ε+ (x), in order to ensure that g(x; θ) is an admissible value of p(1|x; θ). [sent-56, score-0.219]
</p><p>27 The two ﬁrst graphs represent ε− and ε+ designed to estimate posterior probabilities up to precision ǫ, and the corresponding ǫ-tube of admissible estimates knowing g(x). [sent-58, score-0.691]
</p><p>28 3  Estimation of the Parametric Component  The deﬁnitions of ε− and ε+ affect the estimation of the parametric component. [sent-61, score-0.176]
</p><p>29 Regarding θ, when the values of g(x; θ) + ε− (x) and g(x; θ) + ε+ (x) lie within [0, 1], problem (2) is equivalent to the following relaxed maximum likelihood problem   min − ti log(g(xi ; θ) + εi ) + (1 − ti ) log(1 − g(xi ; θ) − εi ) θ,ε (3) i  s. [sent-62, score-0.543]
</p><p>30 The problem is qualiﬁed as relaxed compared to the the maximum likelihood estimation of posterior probabilities by g(xi ; θ), because modeling posterior probabilities by g(xi ; θ) + εi is a looser objective. [sent-68, score-1.078]
</p><p>31 The monotonicity of the objective function with respect to εi implies that the constraints ε− (xi ) ≤ εi and εi ≤ ε+ (xi ) are saturated at the solution of (3) for ti = 0 or ti = 1 respectively. [sent-69, score-0.35]
</p><p>32 Thus, the loss in (3) is the neg-log-likelihood of the lower or the upper bound on p(1|xi ; θ) respectively. [sent-70, score-0.153]
</p><p>33 Provided that g, ε− and ε+ are deﬁned such that ε− (x) ≤ ε+ (x), 0 ≤ g(x) + ε− (x) ≤ 1 and 0 ≤ g(x) + ε+ (x) ≤ 1, the optimization problem with respect to θ reduces to ti log(g(xi ; θ) + ε+ (xi )) + (1 − ti ) log(1 − g(xi ; θ) − ε− (xi )) . [sent-71, score-0.394]
</p><p>34 min − θ  (4)  i  Figure 2 displays the losses for positive examples corresponding to the choices of ε− and ε+ depicted in Figure 1 (the losses are symmetrical around 0. [sent-72, score-0.72]
</p><p>35 When ε− (x) ≤ 0 ≤ ε+ (x), g(x) is an admissible estimate of P (Y = 1|x). [sent-76, score-0.251]
</p><p>36 However, the relaxed loss (4) is optimistic, below the neg-log-likelihood of g. [sent-77, score-0.19]
</p><p>37 5  1  g(x)  g(x)  Figure 2: Losses for positive examples (plain) and neg-log-likelihood of g(x) (dotted) vs. [sent-81, score-0.147]
</p><p>38 Left: for the function ε+ displayed on the left-hand side of Figure 1; right: for the function ε+ displayed on the right-hand side of Figure 1. [sent-83, score-0.19]
</p><p>39 results in a non-consistent estimation of posterior probabilities (i. [sent-84, score-0.445]
</p><p>40 This lack of consistency should not be a concern here, since the non-parametric component is purposely introduced to address a looser estimation problem. [sent-86, score-0.338]
</p><p>41 We should therefore restrict consistency requirements to the primary goal of having posterior probabilities in the ǫ-tube [g(x) + ε− (x), g(x) + ε+ (x)]. [sent-87, score-0.418]
</p><p>42 However, this similarity does not provide a proper mapping from SVM scores to posterior probabilities. [sent-89, score-0.387]
</p><p>43 1  SVMs and Gaussian Processes  In its primal Lagrangian formulation, the SVM optimization problem reads 1 [1 − yi (f (xi ) + b)]+ , min f 2 + C H f,b 2 i where H is a reproducing kernel Hilbert space with norm parameter and [f ]+ = max(f, 0). [sent-92, score-0.172]
</p><p>44 Then, the interpretation of the hinge loss as a marginal log-likelihood requires to identify an afﬁne function of the last term of (5) with the two ﬁrst terms of (1). [sent-94, score-0.408]
</p><p>45 To proceed with a probabilistic interpretation of SVMs, Sollich (2000) proposed a normalized probability model. [sent-98, score-0.168]
</p><p>46 The normalization functional was chosen arbitrarily, and the consequences of this choice on the probabilistic interpretation was not evaluated. [sent-99, score-0.203]
</p><p>47 In what follows, we derive an imprecise mapping, with interval-valued estimates of probabilities, representing the set of all admissible semi-parametric formulations of SVM scores. [sent-100, score-0.252]
</p><p>48 2, one has to identify an afﬁne function of the hinge loss with the two terms of (4). [sent-103, score-0.263]
</p><p>49 75  1  g(x)  Figure 3: Left: lower (dashed) and upper (plain) posterior probabilities [g(x) + ε− (x), g(x) + ε+ (x)] vs. [sent-115, score-0.414]
</p><p>50 SVM scores f (x) + b; center: corresponding neg-log-likelihood of g(x) for positive examples vs. [sent-116, score-0.295]
</p><p>51 right: lower (dashed) and upper (plain) posterior probabilities vs. [sent-118, score-0.414]
</p><p>52 Hence, we obtain a set of probabilistic interpretations fully compatible with SVM scores. [sent-125, score-0.222]
</p><p>53 The solutions indexed by c1 are nested, in the sense that, for any x, the length of the uncertainty interval, ε+ (x)−ε− (x), is monotonically decreasing in c1 : the interpretation of SVM scores as posterior probabilities gets tighter as c1 increases. [sent-126, score-0.673]
</p><p>54 The most restricted subset of admissible interpretations, with the shortest uncertainty intervals, obtained for c1 = log(2), is represented in the left-hand side of Figure 3. [sent-127, score-0.252]
</p><p>55 The loss incurred by a positive example is represented on the central graph, where the gray zone represents the neg-log-likelihood of all admissible solutions of g(x). [sent-128, score-0.442]
</p><p>56 Note that the hinge loss is proportional to the neg-log-likelihood of the upper posterior probability g(x) + ε+ (x), which is the loss for positive examples in the semi-parametric model in (4). [sent-129, score-0.765]
</p><p>57 Conversely, the hinge loss for negative examples is reached for g(x) + ε− (x). [sent-130, score-0.393]
</p><p>58 2 is that the neg-log-likelihood of any admissible functions g(x) is tangent to the hinge loss at f (x) + b = 0. [sent-132, score-0.515]
</p><p>59 The solution is unique in terms of the admissible interval [g + ε− , g + ε+ ], but many deﬁnitions of (ε− , ε+ , g) solve (7). [sent-133, score-0.296]
</p><p>60 For example, g may be deﬁned as  2−[1−(f (x)+b)]+ , (8) 2−[1+(f (x)+b)]+ + 2−[1−(f (x)+b)]+ which is essentially the posterior probability model proposed by Sollich (2000), represented dotted in the ﬁrst two graphs of Figure 3. [sent-134, score-0.233]
</p><p>61 g(x; θ) =  The last graph of Figure 3 displays the mapping from g(x) to admissible values of p(1|x) which results from the choice described in (8). [sent-135, score-0.296]
</p><p>62 Although the interpretation of SVM scores does not require to specify g, it may worth to list some features common to all options. [sent-136, score-0.251]
</p><p>63 Second, the estimation of posterior probabilities is accurate at 0. [sent-139, score-0.445]
</p><p>64 Hence, the training objective of SVMs is intermediate between the accurate estimation of posterior probabilities on the whole range [0, 1] and the minimization of the classiﬁcation risk. [sent-143, score-0.445]
</p><p>65 4  Outcomes of the Probabilistic Interpretation  This section gives two consequences of our probabilistic interpretation of SVMs. [sent-144, score-0.203]
</p><p>66 1  Pointwise Posterior Probabilities from SVM Scores  Platt (2000) proposed to estimate posterior probabilities from SVM scores by ﬁtting a logistic function over the SVM scores. [sent-147, score-0.611]
</p><p>67 The only logistic function compatible with the most stringent interpretation of SVMs in the semi-parametric framework, 1 g(x; θ) = , (9) 1 + 4−(f (x)+b)) is identical to the model of Sollich (2000) (8) when f (x) + b lies in the interval [−1, 1]. [sent-148, score-0.304]
</p><p>68 Other logistic functions are compatible with the looser interpretations obtained by letting c1 < log(2), but their use as pointwise estimates is questionable, since the associated conﬁdence interval is wider. [sent-149, score-0.504]
</p><p>69 In particular, the looser interpretations do not ensure that f (x) + b = 0 corresponds to g(x) = 0. [sent-150, score-0.188]
</p><p>70 Then, the decision function based on the estimated posterior probabilities by g(x) may differ from the SVM decision function. [sent-152, score-0.58]
</p><p>71 Being based on an arbitrary choice of g(x), pointwise estimates of posterior probabilities derived from SVM scores should be handled with caution. [sent-153, score-0.639]
</p><p>72 2  Unbalanced Classiﬁcation Losses  SVMs are known to perform well regarding misclassiﬁcation error, but they provide skewed decision boundaries for unbalanced classiﬁcation losses, where the losses associated with incorrect decisions differ according to the true label. [sent-157, score-0.579]
</p><p>73 The mainstream approach used to address this problem consists in using different losses for positive and negative examples (Morik et al. [sent-158, score-0.461]
</p><p>74 1 [1 − (f (xi ) + b)]+ +C − [1 + (f (xi ) + b)]+ , (10) min f 2 +C + H f,b 2 {i|yi =1}  {i|yi =−1}  where the coefﬁcients C + and C − are constants, whose ratio is equal to the ratio of the losses ℓFN and ℓFP pertaining to false negatives and false positives, respectively (Lin et al. [sent-162, score-0.457]
</p><p>75 2 Bayes’ decision theory deﬁnes the optimal decision rule by positive classiﬁcation ℓFP when P (y = 1|x) > P0 , where P0 = ℓFP +ℓFN . [sent-164, score-0.265]
</p><p>76 With such deﬁnitions, the optimization problem may be interpreted as an upper-bound on the classiﬁcation risk deﬁned from ℓFN and ℓFP . [sent-166, score-0.134]
</p><p>77 2 unveils a major problem: the SVM decision function provided by sign(f (xi ) + b) is not consistent with the probabilistic interpretation of SVM scores. [sent-168, score-0.27]
</p><p>78 We address this problem by deriving another criterion, by requiring that the neg-loglikelihood of any admissible functions g(x) is tangent to the hinge loss at f (x) + b = 0. [sent-169, score-0.555]
</p><p>79 75  1  g(x)  Figure 4: Left: lower (dashed) and upper (plain) posterior probabilities [g(x) + ε− (x), g(x) + ε+ (x)] vs. [sent-180, score-0.414]
</p><p>80 SVM scores f (x) + b obtained from (11) with P0 = 0. [sent-181, score-0.148]
</p><p>81 25; center: corresponding neg-log-likelihood of g(x) for positive examples vs. [sent-182, score-0.147]
</p><p>82 right: lower (dashed) and upper (plain) posterior probabilities vs. [sent-184, score-0.414]
</p><p>83 This loss differs from (10), in the respect that the margin for positive examples is smaller than the one for negative examples when P0 < 0. [sent-186, score-0.392]
</p><p>84 In particular, (10) does not affect the SVM solution for separable problems, while in (11), the decision boundary moves towards positive support vectors when P0 decreases. [sent-188, score-0.198]
</p><p>85 The analogue of Figure 3, displayed on Figure 4, shows that one recovers the characteristics of the standard SVM loss, except that the focus is now on the posterior probability P0 deﬁned by Bayes’ decision rule. [sent-189, score-0.406]
</p><p>86 For experimenting with difﬁcult unbalanced two-class problems, we used the Forest database, the largest available UCI dataset (http://kdd. [sent-191, score-0.211]
</p><p>87 The ratio of negative to positive examples is high, a feature commonly encountered with unbalanced classiﬁcation losses. [sent-196, score-0.402]
</p><p>88 The training set was built by random selection of size 11 000 (1000 and 10 000 examples from the positive and negative class respectively); a validation set, of size 11 000 was drawn identically among the other examples; ﬁnally, the test set, of size 99 000, was drawn among the remaining examples. [sent-197, score-0.245]
</p><p>89 1 The performance was measured by the weighted risk function R = n (NFN ℓFN +NFP ℓFP ), where NFN and NFP are the number of false negatives and false positives, respectively. [sent-198, score-0.239]
</p><p>90 We additionally considered three tuning for the bias b: ˆ is the bias returned by the algorithm; ˆv the b b bias returned by minimizing R on the validation set, which is an optimistic estimate of the bias that could be computed by cross-validation. [sent-202, score-0.647]
</p><p>91 The bias returned by the algorithm is very close to the optimal one. [sent-207, score-0.166]
</p><p>92 While the solutions returned by C + /C − can be signiﬁcantly improved  Table 1: Errors for 3 different criteria and for 3 different models over the Forest database ℓFN 1 10 100  Baseline, problem (5) ˆ b b∗ 0. [sent-209, score-0.143]
</p><p>93 289  by tuning the bias, our criterion provides results that are very close to the optimum, in the range of the performances obtained with the bias optimized on an independant validation set. [sent-233, score-0.165]
</p><p>94 The new optimization criterion can thus outperform standard approaches for highly unbalanced problems. [sent-234, score-0.255]
</p><p>95 The non-parametric component provides an intuitive means of transforming the likelihood into a decision-oriented criterion. [sent-236, score-0.135]
</p><p>96 This framework was used here to propose a new parameterization of the hinge loss, dedicated to unbalanced classiﬁcation problems, yielding signiﬁcant improvements over the classical procedure. [sent-237, score-0.359]
</p><p>97 Among other prospectives, we plan to apply the same framework to investigate hinge-like criteria for decision rules including a reject option, where the classiﬁer abstains when a pattern is ambiguous. [sent-238, score-0.153]
</p><p>98 We also aim at deﬁning losses encouraging sparsity in probabilistic models, such as kernelized logistic regression. [sent-239, score-0.38]
</p><p>99 We could thus build sparse probabilistic classiﬁers, providing an accurate estimation of posterior probabilities on a (limited) predeﬁned range of posterior probabilities. [sent-240, score-0.712]
</p><p>100 For example, minimizing classiﬁcation error only requires to ﬁnd the class with highest posterior probability, and this search does not require precise estimates of probabilities outside the interval [1/K, 1/2], where K is the number of classes. [sent-242, score-0.486]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('svm', 0.342), ('nuisance', 0.246), ('losses', 0.23), ('admissible', 0.219), ('unbalanced', 0.211), ('posterior', 0.202), ('sollich', 0.189), ('ti', 0.175), ('probabilities', 0.174), ('fn', 0.17), ('scores', 0.148), ('svms', 0.148), ('hinge', 0.148), ('plain', 0.14), ('classi', 0.121), ('fp', 0.12), ('loss', 0.115), ('xi', 0.108), ('interpretation', 0.103), ('decision', 0.102), ('looser', 0.1), ('returned', 0.092), ('interpretations', 0.088), ('examples', 0.086), ('cation', 0.083), ('likelihood', 0.082), ('pointwise', 0.082), ('interval', 0.077), ('relaxed', 0.075), ('bias', 0.074), ('false', 0.072), ('parametric', 0.072), ('compatible', 0.069), ('estimation', 0.069), ('probabilistic', 0.065), ('crystal', 0.063), ('kotz', 0.063), ('lindsay', 0.063), ('morik', 0.063), ('nfn', 0.063), ('nfp', 0.063), ('quali', 0.063), ('veropoulos', 0.063), ('log', 0.062), ('displayed', 0.062), ('positive', 0.061), ('yi', 0.058), ('logistic', 0.055), ('banks', 0.055), ('encyclopedia', 0.055), ('validation', 0.054), ('component', 0.053), ('dashed', 0.051), ('criteria', 0.051), ('forest', 0.05), ('risk', 0.048), ('outcomes', 0.048), ('negatives', 0.047), ('zone', 0.047), ('monotonically', 0.046), ('negative', 0.044), ('optimization', 0.044), ('positives', 0.044), ('optimistic', 0.044), ('platt', 0.044), ('interpreted', 0.042), ('consistency', 0.042), ('marginal', 0.042), ('estimating', 0.041), ('address', 0.04), ('read', 0.04), ('recovers', 0.04), ('displays', 0.04), ('nitions', 0.04), ('lin', 0.038), ('upper', 0.038), ('tuning', 0.037), ('choices', 0.037), ('slack', 0.037), ('mapping', 0.037), ('baseline', 0.036), ('regarding', 0.036), ('min', 0.036), ('affect', 0.035), ('consequences', 0.035), ('discriminant', 0.035), ('editors', 0.034), ('concern', 0.034), ('freedom', 0.034), ('reproducing', 0.034), ('side', 0.033), ('estimates', 0.033), ('ball', 0.033), ('tangent', 0.033), ('estimate', 0.032), ('graphs', 0.031), ('decomposed', 0.031), ('convexity', 0.031), ('wiley', 0.03), ('sparsity', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="14-tfidf-1" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>Author: Yves Grandvalet, Johnny Mariethoz, Samy Bengio</p><p>Abstract: In this paper, we show that the hinge loss can be interpreted as the neg-log-likelihood of a semi-parametric model of posterior probabilities. From this point of view, SVMs represent the parametric component of a semi-parametric model ﬁtted by a maximum a posteriori estimation procedure. This connection enables to derive a mapping from SVM scores to estimated posterior probabilities. Unlike previous proposals, the suggested mapping is interval-valued, providing a set of posterior probabilities compatible with each SVM score. This framework offers a new way to adapt the SVM optimization problem to unbalanced classiﬁcation, when decisions result in unequal (asymmetric) losses. Experiments show improvements over state-of-the-art procedures. 1</p><p>2 0.22401068 <a title="14-tfidf-2" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>Author: Régis Vert, Jean-philippe Vert</p><p>Abstract: We determine the asymptotic limit of the function computed by support vector machines (SVM) and related algorithms that minimize a regularized empirical convex loss function in the reproducing kernel Hilbert space of the Gaussian RBF kernel, in the situation where the number of examples tends to inﬁnity, the bandwidth of the Gaussian kernel tends to 0, and the regularization parameter is held ﬁxed. Non-asymptotic convergence bounds to this limit in the L2 sense are provided, together with upper bounds on the classiﬁcation error that is shown to converge to the Bayes risk, therefore proving the Bayes-consistency of a variety of methods although the regularization term does not vanish. These results are particularly relevant to the one-class SVM, for which the regularization can not vanish by construction, and which is shown for the ﬁrst time to be a consistent density level set estimator. 1</p><p>3 0.13260141 <a title="14-tfidf-3" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>Author: Sören Sonnenburg, Gunnar Rätsch, Christin Schäfer</p><p>Abstract: While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lankriet et al. (2004) considered conic combinations of kernel matrices for classiﬁcation, leading to a convex quadratically constraint quadratic program. We show that it can be rewritten as a semi-inﬁnite linear program that can be efﬁciently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classiﬁcation. Experimental results show that the proposed algorithm helps for automatic model selection, improving the interpretability of the learning result and works for hundred thousands of examples or hundreds of kernels to be combined. 1</p><p>4 0.13153906 <a title="14-tfidf-4" href="./nips-2005-Benchmarking_Non-Parametric_Statistical_Tests.html">37 nips-2005-Benchmarking Non-Parametric Statistical Tests</a></p>
<p>Author: Mikaela Keller, Samy Bengio, Siew Y. Wong</p><p>Abstract: Although non-parametric tests have already been proposed for that purpose, statistical signiﬁcance tests for non-standard measures (different from the classiﬁcation error) are less often used in the literature. This paper is an attempt at empirically verifying how these tests compare with more classical tests, on various conditions. More precisely, using a very large dataset to estimate the whole “population”, we analyzed the behavior of several statistical test, varying the class unbalance, the compared models, the performance measure, and the sample size. The main result is that providing big enough evaluation sets non-parametric tests are relatively reliable in all conditions. 1</p><p>5 0.13144776 <a title="14-tfidf-5" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>Author: Kilian Q. Weinberger, John Blitzer, Lawrence K. Saul</p><p>Abstract: We show how to learn a Mahanalobis distance metric for k-nearest neighbor (kNN) classiﬁcation by semideﬁnite programming. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. On seven data sets of varying size and difﬁculty, we ﬁnd that metrics trained in this way lead to signiﬁcant improvements in kNN classiﬁcation—for example, achieving a test error rate of 1.3% on the MNIST handwritten digits. As in support vector machines (SVMs), the learning problem reduces to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our framework requires no modiﬁcation or extension for problems in multiway (as opposed to binary) classiﬁcation. 1</p><p>6 0.13103126 <a title="14-tfidf-6" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>7 0.12370966 <a title="14-tfidf-7" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>8 0.11675643 <a title="14-tfidf-8" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>9 0.1157275 <a title="14-tfidf-9" href="./nips-2005-Nonparametric_inference_of_prior_probabilities_from_Bayes-optimal_behavior.html">140 nips-2005-Nonparametric inference of prior probabilities from Bayes-optimal behavior</a></p>
<p>10 0.11424589 <a title="14-tfidf-10" href="./nips-2005-Maximum_Margin_Semi-Supervised_Learning_for_Structured_Variables.html">123 nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</a></p>
<p>11 0.11250552 <a title="14-tfidf-11" href="./nips-2005-Unbiased_Estimator_of_Shape_Parameter_for_Spiking_Irregularities_under_Changing_Environments.html">197 nips-2005-Unbiased Estimator of Shape Parameter for Spiking Irregularities under Changing Environments</a></p>
<p>12 0.11125892 <a title="14-tfidf-12" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>13 0.10643244 <a title="14-tfidf-13" href="./nips-2005-Two_view_learning%3A_SVM-2K%2C_Theory_and_Practice.html">196 nips-2005-Two view learning: SVM-2K, Theory and Practice</a></p>
<p>14 0.10434314 <a title="14-tfidf-14" href="./nips-2005-From_Lasso_regression_to_Feature_vector_machine.html">77 nips-2005-From Lasso regression to Feature vector machine</a></p>
<p>15 0.098818146 <a title="14-tfidf-15" href="./nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">113 nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<p>16 0.096594237 <a title="14-tfidf-16" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>17 0.090719022 <a title="14-tfidf-17" href="./nips-2005-Transfer_learning_for_text_classification.html">195 nips-2005-Transfer learning for text classification</a></p>
<p>18 0.088978611 <a title="14-tfidf-18" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>19 0.088618465 <a title="14-tfidf-19" href="./nips-2005-Query_by_Committee_Made_Real.html">160 nips-2005-Query by Committee Made Real</a></p>
<p>20 0.082415834 <a title="14-tfidf-20" href="./nips-2005-Variational_EM_Algorithms_for_Non-Gaussian_Latent_Variable_Models.html">202 nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.273), (1, 0.131), (2, -0.069), (3, -0.072), (4, 0.171), (5, 0.118), (6, 0.061), (7, 0.008), (8, -0.018), (9, -0.053), (10, 0.026), (11, -0.178), (12, 0.091), (13, 0.082), (14, 0.089), (15, -0.021), (16, 0.128), (17, 0.043), (18, 0.087), (19, 0.043), (20, -0.003), (21, 0.012), (22, 0.071), (23, -0.058), (24, -0.07), (25, 0.118), (26, 0.095), (27, -0.052), (28, 0.062), (29, 0.16), (30, -0.038), (31, -0.036), (32, -0.026), (33, -0.027), (34, -0.141), (35, -0.189), (36, -0.013), (37, 0.018), (38, 0.078), (39, -0.014), (40, -0.004), (41, -0.073), (42, -0.002), (43, 0.01), (44, -0.004), (45, 0.096), (46, 0.067), (47, 0.068), (48, 0.044), (49, -0.107)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96575457 <a title="14-lsi-1" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>Author: Yves Grandvalet, Johnny Mariethoz, Samy Bengio</p><p>Abstract: In this paper, we show that the hinge loss can be interpreted as the neg-log-likelihood of a semi-parametric model of posterior probabilities. From this point of view, SVMs represent the parametric component of a semi-parametric model ﬁtted by a maximum a posteriori estimation procedure. This connection enables to derive a mapping from SVM scores to estimated posterior probabilities. Unlike previous proposals, the suggested mapping is interval-valued, providing a set of posterior probabilities compatible with each SVM score. This framework offers a new way to adapt the SVM optimization problem to unbalanced classiﬁcation, when decisions result in unequal (asymmetric) losses. Experiments show improvements over state-of-the-art procedures. 1</p><p>2 0.68264377 <a title="14-lsi-2" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>Author: Régis Vert, Jean-philippe Vert</p><p>Abstract: We determine the asymptotic limit of the function computed by support vector machines (SVM) and related algorithms that minimize a regularized empirical convex loss function in the reproducing kernel Hilbert space of the Gaussian RBF kernel, in the situation where the number of examples tends to inﬁnity, the bandwidth of the Gaussian kernel tends to 0, and the regularization parameter is held ﬁxed. Non-asymptotic convergence bounds to this limit in the L2 sense are provided, together with upper bounds on the classiﬁcation error that is shown to converge to the Bayes risk, therefore proving the Bayes-consistency of a variety of methods although the regularization term does not vanish. These results are particularly relevant to the one-class SVM, for which the regularization can not vanish by construction, and which is shown for the ﬁrst time to be a consistent density level set estimator. 1</p><p>3 0.60320449 <a title="14-lsi-3" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>Author: Sören Sonnenburg, Gunnar Rätsch, Christin Schäfer</p><p>Abstract: While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lankriet et al. (2004) considered conic combinations of kernel matrices for classiﬁcation, leading to a convex quadratically constraint quadratic program. We show that it can be rewritten as a semi-inﬁnite linear program that can be efﬁciently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classiﬁcation. Experimental results show that the proposed algorithm helps for automatic model selection, improving the interpretability of the learning result and works for hundred thousands of examples or hundreds of kernels to be combined. 1</p><p>4 0.58739269 <a title="14-lsi-4" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>Author: Malte Kuss, Carl E. Rasmussen</p><p>Abstract: Gaussian processes are attractive models for probabilistic classiﬁcation but unfortunately exact inference is analytically intractable. We compare Laplace’s method and Expectation Propagation (EP) focusing on marginal likelihood estimates and predictive performance. We explain theoretically and corroborate empirically that EP is superior to Laplace. We also compare to a sophisticated MCMC scheme and show that EP is surprisingly accurate. In recent years models based on Gaussian process (GP) priors have attracted much attention in the machine learning community. Whereas inference in the GP regression model with Gaussian noise can be done analytically, probabilistic classiﬁcation using GPs is analytically intractable. Several approaches to approximate Bayesian inference have been suggested, including Laplace’s approximation, Expectation Propagation (EP), variational approximations and Markov chain Monte Carlo (MCMC) sampling, some of these in conjunction with generalisation bounds, online learning schemes and sparse approximations. Despite the abundance of recent work on probabilistic GP classiﬁers, most experimental studies provide only anecdotal evidence, and no clear picture has yet emerged, as to when and why which algorithm should be preferred. Thus, from a practitioners point of view probabilistic GP classiﬁcation remains a jungle. In this paper, we set out to understand and compare two of the most wide-spread approximations: Laplace’s method and Expectation Propagation (EP). We also compare to a sophisticated, but computationally demanding MCMC scheme to examine how close the approximations are to ground truth. We examine two aspects of the approximation schemes: Firstly the accuracy of approximations to the marginal likelihood which is of central importance for model selection and model comparison. In any practical application of GPs in classiﬁcation (usually multiple) parameters of the covariance function (hyperparameters) have to be handled. Bayesian model selection provides a consistent framework for setting such parameters. Therefore, it is essential to evaluate the accuracy of the marginal likelihood approximations as a function of the hyperparameters, in order to assess the practical usefulness of the approach Secondly, we need to assess the quality of the approximate probabilistic predictions. In the past, the probabilistic nature of the GP predictions have not received much attention, the focus being mostly on classiﬁcation error rates. This unfortunate state of affairs is caused primarily by typical benchmarking problems being considered outside of a realistic context. The ability of a classiﬁer to produce class probabilities or conﬁdences, have obvious relevance in most areas of application, eg. medical diagnosis. We evaluate the predictive distributions of the approximate methods, and compare to the MCMC gold standard. 1 The Gaussian Process Model for Binary Classiﬁcation Let y ∈ {−1, 1} denote the class label of an input x. Gaussian process classiﬁcation (GPC) is discriminative in modelling p(y|x) for given x by a Bernoulli distribution. The probability of success p(y = 1|x) is related to an unconstrained latent function f (x) which is mapped to the unit interval by a sigmoid transformation, eg. the logit or the probit. For reasons of analytic convenience we exclusively use the probit model p(y = 1|x) = Φ(f (x)), where Φ denotes the cumulative density function of the standard Normal distribution. In the GPC model Bayesian inference is performed about the latent function f in the light of observed data D = {(yi , xi )|i = 1, . . . , m}. Let fi = f (xi ) and f = [f1 , . . . , fm ] be shorthand for the values of the latent function and y = [y1 , . . . , ym ] and X = [x1 , . . . , xm ] collect the class labels and inputs respectively. Given the latent function the class labels are independent Bernoulli variables, so the joint likelihood factories: m m p(yi |fi ) = p(y|f ) = i=1 Φ(yi fi ), i=1 and depends on f only through its value at the observed inputs. We use a zero-mean Gaussian process prior over the latent function f with a covariance function k(x, x |θ), which may depend on hyperparameters θ [1]. The functional form and parameters of the covariance function encodes assumptions about the latent function, and adaptation of these is part of the inference. The posterior distribution over latent function values f at the observed X for given hyperparameters θ becomes: m p(f |D, θ) = N (f |0, K) Φ(yi fi ), p(D|θ) i=1 where p(D|θ) = p(y|f )p(f |X, θ)df , denotes the marginal likelihood. Unfortunately neither the marginal likelihood, nor the posterior itself, or predictions can be computed analytically, so approximations are needed. 2 Approximate Bayesian Inference For the GPC model approximations are either based on a Gaussian approximation to the posterior p(f |D, θ) ≈ q(f |D, θ) = N (f |m, A) or involve Markov chain Monte Carlo (MCMC) sampling [2]. We compare Laplace’s method and Expectation Propagation (EP) which are two alternative approaches to ﬁnding parameters m and A of the Gaussian q(f |D, θ). Both methods also allow approximate evaluation of the marginal likelihood, which is useful for ML-II hyperparameter optimisation. Laplace’s approximation (LA) is found by making a second order Taylor approximation of the (un-normalised) log posterior [3]. The mean m is placed at the mode (MAP) and the covariance A equals the negative inverse Hessian of the log posterior density at m. The EP approximation [4] also gives a Gaussian approximation to the posterior. The parameters m and A are found in an iterative scheme by matching the approximate marginal moments of p(fi |D, θ) by the marginals of the approximation N (fi |mi , Aii ). Although we cannot prove the convergence of EP, we conjecture that it always converges for GPC with probit likelihood, and have never encountered an exception. A key insight is that a Gaussian approximation to the GPC posterior is equivalent to a GP approximation to the posterior distribution over latent functions. For a test input x∗ the fi 1 0.16 0.14 0.8 0.6 0.1 fj p(y|f) p(f|y) 0.12 Likelihood p(y|f) Prior p(f) Posterior p(f|y) Laplace q(f|y) EP q(f|y) 0.08 0.4 0.06 0.04 0.2 0.02 0 −4 0 4 8 0 f . (a) (b) Figure 1: Panel (a) provides a one-dimensional illustration of the approximations. The prior N (f |0, 52 ) combined with the probit likelihood (y = 1) results in a skewed posterior. The likelihood uses the right axis, all other curves use the left axis. Laplace’s approximation peaks at the posterior mode, but places far too much mass over negative values of f and too little at large positive values. The EP approximation matches the ﬁrst two posterior moments, which results in a larger mean and a more accurate placement of probability mass compared to Laplace’s approximation. In Panel (b) we caricature a high dimensional zeromean Gaussian prior as an ellipse. The gray shadow indicates that for a high dimensional Gaussian most of the mass lies in a thin shell. For large latent signals (large entries in K), the likelihood essentially cuts off regions which are incompatible with the training labels (hatched area), leaving the upper right orthant as the posterior. The dot represents the mode of the posterior, which remains close to the origin. approximate predictive latent and class probabilities are: 2 q(f∗ |D, θ, x∗ ) = N (µ∗ , σ∗ ), and 2 q(y∗ = 1|D, x∗ ) = Φ(µ∗ / 1 + σ∗ ), 2 where µ∗ = k∗ K−1 m and σ∗ = k(x∗ , x∗ )−k∗ (K−1 − K−1 AK−1 )k∗ , where the vector k∗ = [k(x1 , x∗ ), . . . , k(xm , x∗ )] collects covariances between x∗ and training inputs X. MCMC sampling has the advantage that it becomes exact in the limit of long runs and so provides a gold standard by which to measure the two analytic methods described above. Although MCMC methods can in principle be used to do inference over f and θ jointly [5], we compare to methods using ML-II optimisation over θ, thus we use MCMC to integrate over f only. Good marginal likelihood estimates are notoriously difﬁcult to obtain; in our experiments we use Annealed Importance Sampling (AIS) [6], combining several Thermodynamic Integration runs into a single (unbiased) estimate of the marginal likelihood. Both analytic approximations have a computational complexity which is cubic O(m3 ) as common among non-sparse GP models due to inversions m × m matrices. In our implementations LA and EP need similar running times, on the order of a few minutes for several hundred data-points. Making AIS work efﬁciently requires some ﬁne-tuning and a single estimate of p(D|θ) can take several hours for data sets of a few hundred examples, but this could conceivably be improved upon. 3 Structural Properties of the Posterior and its Approximations Structural properties of the posterior can best be understood by examining its construction. The prior is a correlated m-dimensional Gaussian N (f |0, K) centred at the origin. Each likelihood term p(yi |fi ) softly truncates the half-space from the prior that is incompatible with the observed label, see Figure 1. The resulting posterior is unimodal and skewed, similar to a multivariate Gaussian truncated to the orthant containing y. The mode of the posterior remains close to the origin, while the mass is placed in accordance with the observed class labels. Additionally, high dimensional Gaussian distributions exhibit the property that most probability mass is contained in a thin ellipsoidal shell – depending on the covariance structure – away from the mean [7, ch. 29.2]. Intuitively this occurs since in high dimensions the volume grows extremely rapidly with the radius. As an effect the mode becomes less representative (typical) for the prior distribution as the dimension increases. For the GPC posterior this property persists: the mode of the posterior distribution stays relatively close to the origin, still being unrepresentative for the posterior distribution, while the mean moves to the mass of the posterior making mean and mode differ signiﬁcantly. We cannot generally assume the posterior to be close to Gaussian, as in the often studied limit of low-dimensional parametric models with large amounts of data. Therefore in GPC we must be aware of making a Gaussian approximation to a non-Gaussian posterior. From the properties of the posterior it can be expected that Laplace’s method places m in the right orthant but too close to the origin, such that the approximation will overlap with regions having practically zero posterior mass. As an effect the amplitude of the approximate latent posterior GP will be underestimated systematically, leading to overly cautious predictive distributions. The EP approximation does not rely on a local expansion, but assumes that the marginal distributions can be well approximated by Gaussians. This assumption will be examined empirically below. 4 Experiments In this section we compare and inspect approximations for GPC using various benchmark data sets. The primary focus is not to optimise the absolute performance of GPC models but to compare the relative accuracy of approximations and to validate the arguments given in the previous section. In all experiments we use a covariance function of the form: k(x, x |θ) = σ 2 exp − 1 x − x 2 2 / 2 , (1) such that θ = [σ, ]. We refer to σ 2 as the signal variance and to as the characteristic length-scale. Note that for many classiﬁcation tasks it may be reasonable to use an individual length scale parameter for every input dimension (ARD) or a different kind of covariance function. Nevertheless, for the sake of presentability we use the above covariance function and we believe the conclusions about the accuracy of approximations to be independent of this choice, since it relies on arguments which are independent of the form of the covariance function. As measure of the accuracy of predictive probabilities we use the average information in bits of the predictions about the test targets in excess of that of random guessing. Let p∗ = p(y∗ = 1|D, θ, x∗ ) be the model’s prediction, then we average: I(p∗ , yi ) = i yi +1 2 log2 (p∗ ) + i 1−yi 2 log2 (1 − p∗ ) + H i (2) over all test cases, where H is the entropy of the training labels. The error rate E is equal to the percentage of erroneous class assignments if prediction is understood as a decision problem with symmetric costs. For the ﬁrst set of experiments presented here the well-known USPS digits and the Ionosphere data set were used. A binary sub-problem from the USPS digits is deﬁned by only considering 3’s vs. 5’s (which is probably the hardest of the binary sub-problems) and dividing the data into 767 cases for training and 773 for testing. The Ionosphere data is split into 200 training and 151 test cases. We do an exhaustive investigation on a ﬁne regular grid of values for the log hyperparameters. For each θ on the grid we compute the approximated log marginal likelihood by LA, EP and AIS. Additionally we compute the respective predictive performance (2) on the test set. Results are shown in Figure 2. Log marginal likelihood −150 −130 −200 Log marginal likelihood 5 −115 −105 −95 4 −115 −105 3 −130 −100 −150 2 1 log magnitude, log(σf) log magnitude, log(σf) 4 Log marginal likelihood 5 −160 4 −100 3 −130 −92 −160 2 −105 −160 −105 −200 −115 1 log magnitude, log(σf) 5 −92 −95 3 −100 −105 2−200 −115 −160 −130 −200 1 −200 0 0 0 −200 3 4 log lengthscale, log(l) 5 2 3 4 log lengthscale, log(l) (1a) 4 0.84 4 0.8 0.8 0.25 3 0.8 0.84 2 0.7 0.7 1 0.5 log magnitude, log(σf) 0.86 5 0.86 0.8 0.89 0.88 0.7 1 0.5 3 4 log lengthscale, log(l) 2 3 4 log lengthscale, log(l) (2a) Log marginal likelihood −90 −70 −100 −120 −120 0 −70 −75 −120 1 −100 1 2 3 log lengthscale, log(l) 4 0 −70 −90 −65 2 −100 −100 1 −120 −80 1 2 3 log lengthscale, log(l) 4 −1 −1 5 5 f 0.1 0.2 0.55 0 1 0.4 1 2 3 log lengthscale, log(l) 5 0.5 0.1 0 0.3 0.4 0.6 0.55 0.3 0.2 0.2 0.1 1 0 0.2 4 5 −1 −1 0.4 0.2 0.6 2 0.3 10 0 0.1 0.2 0.1 0 0 0.5 1 2 3 log lengthscale, log(l) 0.5 0.5 0.55 3 0 0.1 0 1 2 3 log lengthscale, log(l) 0.5 0.3 0.5 4 2 5 (3c) 0.5 3 4 Information about test targets in bits 4 log magnitude, log(σf) 4 2 0 (3b) Information about test targets in bits 0.3 log magnitude, log(σ ) −75 0 −1 −1 5 5 0 −120 3 −120 (3a) −1 −1 −90 −80 −65 −100 2 Information about test targets in bits 0 −75 4 0 3 5 Log marginal likelihood −90 3 −100 0 0.25 3 4 log lengthscale, log(l) 5 log magnitude, log(σf) log magnitude, log(σf) f log magnitude, log(σ ) −80 3 0.5 (2c) −75 −90 0.7 0.8 2 4 −75 −1 −1 0.86 0.84 Log marginal likelihood 4 1 0.7 1 5 5 −150 2 (2b) 5 2 0.88 3 0 5 0.84 0.89 0.25 0 0.7 0.25 0 0.86 4 0.84 3 2 5 Information about test targets in bits log magnitude, log(σf) log magnitude, log(σf) 5 −200 3 4 log lengthscale, log(l) (1c) Information about test targets in bits 5 2 2 (1b) Information about test targets in bits 0.5 5 log magnitude, log(σf) 2 4 5 −1 −1 0 1 2 3 log lengthscale, log(l) 4 5 (4a) (4b) (4c) Figure 2: Comparison of marginal likelihood approximations and predictive performances of different approximation techniques for USPS 3s vs. 5s (upper half) and the Ionosphere data (lower half). The columns correspond to LA (a), EP (b), and MCMC (c). The rows show estimates of the log marginal likelihood (rows 1 & 3) and the corresponding predictive performance (2) on the test set (rows 2 & 4) respectively. MCMC samples Laplace p(f|D) EP p(f|D) 0.2 0.15 0.45 0.1 0.4 0.05 0.3 −16 −14 −12 −10 −8 −6 f −4 −2 0 2 4 p(xi) 0 0.35 (a) 0.06 0.25 0.2 0.15 MCMC samples Laplace p(f|D) EP p(f|D) 0.1 0.05 0.04 0 0 2 0.02 xi 4 6 (c) 0 −40 −35 −30 −25 −20 −15 −10 −5 0 5 10 15 f (b) Figure 3: Panel (a) and (b) show two marginal distributions p(fi |D, θ) from a GPC posterior and its approximations. The true posterior is approximated by a normalised histogram of 9000 samples of fi obtained by MCMC sampling. Panel (c) shows a histogram of samples of a marginal distribution of a truncated high-dimensional Gaussian. The line describes a Gaussian with mean and variance estimated from the samples. For all three approximation techniques we see an agreement between marginal likelihood estimates and test performance, which justiﬁes the use of ML-II parameter estimation. But the shape of the contours and the values differ between the methods. The contours for Laplace’s method appear to be slanted compared to EP. The marginal likelihood estimates of EP and AIS agree surprisingly well1 , given that the marginal likelihood comes as a 767 respectively 200 dimensional integral. The EP predictions contain as much information about the test cases as the MCMC predictions and signiﬁcantly more than for LA. Note that for small signal variances (roughly ln(σ 2 ) < 1) LA and EP give very similar results. A possible explanation is that for small signal variances the likelihood does not truncate the prior but only down-weights the tail that disagrees with the observation. As an effect the posterior will be less skewed and both approximations will lead to similar results. For the USPS 3’s vs. 5’s we now inspect the marginal distributions p(fi |D, θ) of single latent function values under the posterior approximations for a given value of θ. We have chosen the values ln(σ) = 3.35 and ln( ) = 2.85 which are between the ML-II estimates of EP and LA. Hybrid MCMC was used to generate 9000 samples from the posterior p(f |D, θ). For LA and EP the approximate marginals are q(fi |D, θ) = N (fi |mi , Aii ) where m and A are found by the respective approximation techniques. In general we observe that the marginal distributions of MCMC samples agree very well with the respective marginal distributions of the EP approximation. For Laplace’s approximation we ﬁnd the mean to be underestimated and the marginal distributions to overlap with zero far more than the EP approximations. Figure (3a) displays the marginal distribution and its approximations for which the MCMC samples show maximal skewness. Figure (3b) shows a typical example where the EP approximation agrees very well with the MCMC samples. We show this particular example because under the EP approximation p(yi = 1|D, θ) < 0.1% but LA gives a wrong p(yi = 1|D, θ) ≈ 18%. In the experiment we saw that the marginal distributions of the posterior often agree very 1 Note that the agreement between the two seems to be limited by the accuracy of the MCMC runs, as judged by the regularity of the contour lines; the tolerance is less than one unit on a (natural) log scale. well with a Gaussian approximation. This seems to contradict the description given in the previous section were we argued that the posterior is skewed by construction. In order to inspect the marginals of a truncated high-dimensional multivariate Gaussian distribution we made an additional synthetic experiment. We constructed a 767 dimensional Gaussian N (x|0, C) with a covariance matrix having one eigenvalue of 100 with eigenvector 1, and all other eigenvalues are 1. We then truncate this distribution such that all xi ≥ 0. Note that the mode of the truncated Gaussian is still at zero, whereas the mean moves towards the remaining mass. Figure (3c) shows a normalised histogram of samples from a marginal distribution of one xi . The samples agree very well with a Gaussian approximation. In the previous section we described the somewhat surprising property, that for a truncated high-dimensional Gaussian, resembling the posterior, the mode (used by LA) may not be particularly representative of the distribution. Although the marginal is also truncated, it is still exceptionally well modelled by a Gaussian – however, the Laplace approximation centred on the origin would be completely inappropriate. In a second set of experiments we compare the predictive performance of LA and EP for GPC on several well known benchmark problems. Each data set is randomly split into 10 folds of which one at a time is left out as a test set to measure the predictive performance of a model trained (or selected) on the remaining nine folds. All performance measures are averages over the 10 folds. For GPC we implement model selection by ML-II hyperparameter estimation, reporting results given the θ that maximised the respective approximate marginal likelihoods p(D|θ). In order to get a better picture of the absolute performance we also compare to results obtained by C-SVM classiﬁcation. The kernel we used is equivalent to the covariance function (1) without the signal variance parameter. For each fold the parameters C and are found in an inner loop of 5-fold cross-validation, in which the parameter grids are reﬁned until the performance stabilises. Predictive probabilities for test cases are obtained by mapping the unthresholded output of the SVM to [0, 1] using a sigmoid function [8]. Results are summarised in Table 1. Comparing Laplace’s method to EP the latter shows to be more accurate both in terms of error rate and information. While the error rates are relatively similar the predictive distribution obtained by EP shows to be more informative about the test targets. Note that for GPC the error rate only depends of the sign of the mean µ∗ of the approximated posterior over latent functions and not the entire posterior predictive distribution. As to be expected, the length of the mean vector m shows much larger values for the EP approximations. Comparing EP and SVMs the results are mixed. For the Crabs data set all methods show the same error rate but the information content of the predictive distributions differs dramatically. For some test cases the SVM predicts the wrong class with large certainty. 5 Summary & Conclusions Our experiments reveal serious differences between Laplace’s method and EP when used in GPC models. From the structural properties of the posterior we described why LA systematically underestimates the mean m. The resulting posterior GP over latent functions will have too small amplitude, although the sign of the mean function will be mostly correct. As an effect LA gives over-conservative predictive probabilities, and diminished information about the test labels. This effect has been show empirically on several real world examples. Large resulting discrepancies in the actual posterior probabilities were found, even at the training locations, which renders the predictive class probabilities produced under this approximation grossly inaccurate. Note, the difference becomes less dramatic if we only consider the classiﬁcation error rates obtained by thresholding p∗ at 1/2. For this particular task, we’ve seen the the sign of the latent function tends to be correct (at least at the training locations). Laplace EP SVM Data Set m n E% I m E% I m E% I Ionosphere 351 34 8.84 0.591 49.96 7.99 0.661 124.94 5.69 0.681 Wisconsin 683 9 3.21 0.804 62.62 3.21 0.805 84.95 3.21 0.795 Pima Indians 768 8 22.77 0.252 29.05 22.63 0.253 47.49 23.01 0.232 Crabs 200 7 2.0 0.682 112.34 2.0 0.908 2552.97 2.0 0.047 Sonar 208 60 15.36 0.439 26.86 13.85 0.537 15678.55 11.14 0.567 USPS 3 vs 5 1540 256 2.27 0.849 163.05 2.21 0.902 22011.70 2.01 0.918 Table 1: Results for benchmark data sets. The ﬁrst three columns give the name of the data set, number of observations m and dimension of inputs n. For Laplace’s method and EP the table reports the average error rate E%, the average information I (2) and the average length m of the mean vector of the Gaussian approximation. For SVMs the error rate and the average information about the test targets are reported. Note that for the Crabs data set we use the sex (not the colour) of the crabs as class label. The EP approximation has shown to give results very close to MCMC both in terms of predictive distributions and marginal likelihood estimates. We have shown and explained why the marginal distributions of the posterior can be well approximated by Gaussians. Further, the marginal likelihood values obtained by LA and EP differ systematically which will lead to different results of ML-II hyperparameter estimation. The discrepancies are similar for different tasks. Using AIS we were able to show the accuracy of marginal likelihood estimates, which to the best of our knowledge has never been done before. In summary, we found that EP is the method of choice for approximate inference in binary GPC models, when the computational cost of MCMC is prohibitive. In contrast, the Laplace approximation is so inaccurate that we advise against its use, especially when predictive probabilities are to be taken seriously. Further experiments and a detailed description of the approximation schemes can be found in [2]. Acknowledgements Both authors acknowledge support by the German Research Foundation (DFG) through grant RA 1030/1. This work was supported in part by the IST Programme of the European Community, under the PASCAL Network of Excellence, IST2002-506778. This publication only reﬂects the authors’ views. References [1] C. K. I. Williams and C. E. Rasmussen. Gaussian processes for regression. In David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, editors, NIPS 8, pages 514–520. MIT Press, 1996. [2] M. Kuss and C. E. Rasmussen. Assessing approximate inference for binary Gaussian process classiﬁcation. Journal of Machine Learning Research, 6:1679–1704, 2005. [3] C. K. I. Williams and D. Barber. Bayesian classiﬁcation with Gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1342–1351, 1998. [4] T. P. Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD thesis, Department of Electrical Engineering and Computer Science, MIT, 2001. [5] R. M. Neal. Regression and classiﬁcation using Gaussian process priors. In J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith, editors, Bayesian Statistics 6, pages 475–501. Oxford University Press, 1998. [6] R. M. Neal. Annealed importance sampling. Statistics and Computing, 11:125–139, 2001. [7] D. J. C. MacKay. Information Theory, Inference and Learning Algorithms. CUP, 2003. [8] J. C. Platt. Probabilities for SV machines. In Advances in Large Margin Classiﬁers, pages 61–73. The MIT Press, 2000.</p><p>5 0.58265144 <a title="14-lsi-5" href="./nips-2005-Nonparametric_inference_of_prior_probabilities_from_Bayes-optimal_behavior.html">140 nips-2005-Nonparametric inference of prior probabilities from Bayes-optimal behavior</a></p>
<p>Author: Liam Paninski</p><p>Abstract: We discuss a method for obtaining a subject’s a priori beliefs from his/her behavior in a psychophysics context, under the assumption that the behavior is (nearly) optimal from a Bayesian perspective. The method is nonparametric in the sense that we do not assume that the prior belongs to any ﬁxed class of distributions (e.g., Gaussian). Despite this increased generality, the method is relatively simple to implement, being based in the simplest case on a linear programming algorithm, and more generally on a straightforward maximum likelihood or maximum a posteriori formulation, which turns out to be a convex optimization problem (with no non-global local maxima) in many important cases. In addition, we develop methods for analyzing the uncertainty of these estimates. We demonstrate the accuracy of the method in a simple simulated coin-ﬂipping setting; in particular, the method is able to precisely track the evolution of the subject’s posterior distribution as more and more data are observed. We close by brieﬂy discussing an interesting connection to recent models of neural population coding.</p><p>6 0.57616335 <a title="14-lsi-6" href="./nips-2005-Computing_the_Solution_Path_for_the_Regularized_Support_Vector_Regression.html">44 nips-2005-Computing the Solution Path for the Regularized Support Vector Regression</a></p>
<p>7 0.54990178 <a title="14-lsi-7" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>8 0.5375554 <a title="14-lsi-8" href="./nips-2005-Transfer_learning_for_text_classification.html">195 nips-2005-Transfer learning for text classification</a></p>
<p>9 0.52683181 <a title="14-lsi-9" href="./nips-2005-Two_view_learning%3A_SVM-2K%2C_Theory_and_Practice.html">196 nips-2005-Two view learning: SVM-2K, Theory and Practice</a></p>
<p>10 0.52562273 <a title="14-lsi-10" href="./nips-2005-Benchmarking_Non-Parametric_Statistical_Tests.html">37 nips-2005-Benchmarking Non-Parametric Statistical Tests</a></p>
<p>11 0.51658487 <a title="14-lsi-11" href="./nips-2005-Worst-Case_Bounds_for_Gaussian_Process_Models.html">205 nips-2005-Worst-Case Bounds for Gaussian Process Models</a></p>
<p>12 0.5137102 <a title="14-lsi-12" href="./nips-2005-Unbiased_Estimator_of_Shape_Parameter_for_Spiking_Irregularities_under_Changing_Environments.html">197 nips-2005-Unbiased Estimator of Shape Parameter for Spiking Irregularities under Changing Environments</a></p>
<p>13 0.50764436 <a title="14-lsi-13" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>14 0.49940124 <a title="14-lsi-14" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>15 0.49081808 <a title="14-lsi-15" href="./nips-2005-Query_by_Committee_Made_Real.html">160 nips-2005-Query by Committee Made Real</a></p>
<p>16 0.46857953 <a title="14-lsi-16" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>17 0.45542017 <a title="14-lsi-17" href="./nips-2005-Learning_Minimum_Volume_Sets.html">112 nips-2005-Learning Minimum Volume Sets</a></p>
<p>18 0.44305226 <a title="14-lsi-18" href="./nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">113 nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<p>19 0.44251344 <a title="14-lsi-19" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>20 0.42379999 <a title="14-lsi-20" href="./nips-2005-Nested_sampling_for_Potts_models.html">133 nips-2005-Nested sampling for Potts models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.074), (10, 0.026), (11, 0.012), (27, 0.024), (31, 0.035), (34, 0.151), (39, 0.333), (41, 0.01), (55, 0.029), (69, 0.05), (73, 0.023), (88, 0.098), (91, 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9639557 <a title="14-lda-1" href="./nips-2005-A_Computational_Model_of_Eye_Movements_during_Object_Class_Detection.html">5 nips-2005-A Computational Model of Eye Movements during Object Class Detection</a></p>
<p>Author: Wei Zhang, Hyejin Yang, Dimitris Samaras, Gregory J. Zelinsky</p><p>Abstract: We present a computational model of human eye movements in an object class detection task. The model combines state-of-the-art computer vision object class detection methods (SIFT features trained using AdaBoost) with a biologically plausible model of human eye movement to produce a sequence of simulated ﬁxations, culminating with the acquisition of a target. We validated the model by comparing its behavior to the behavior of human observers performing the identical object class detection task (looking for a teddy bear among visually complex nontarget objects). We found considerable agreement between the model and human data in multiple eye movement measures, including number of ﬁxations, cumulative probability of ﬁxating the target, and scanpath distance.</p><p>2 0.88430929 <a title="14-lda-2" href="./nips-2005-A_Criterion_for_the_Convergence_of_Learning_with_Spike_Timing_Dependent_Plasticity.html">8 nips-2005-A Criterion for the Convergence of Learning with Spike Timing Dependent Plasticity</a></p>
<p>Author: Robert A. Legenstein, Wolfgang Maass</p><p>Abstract: We investigate under what conditions a neuron can learn by experimentally supported rules for spike timing dependent plasticity (STDP) to predict the arrival times of strong “teacher inputs” to the same neuron. It turns out that in contrast to the famous Perceptron Convergence Theorem, which predicts convergence of the perceptron learning rule for a simpliﬁed neuron model whenever a stable solution exists, no equally strong convergence guarantee can be given for spiking neurons with STDP. But we derive a criterion on the statistical dependency structure of input spike trains which characterizes exactly when learning with STDP will converge on average for a simple model of a spiking neuron. This criterion is reminiscent of the linear separability criterion of the Perceptron Convergence Theorem, but it applies here to the rows of a correlation matrix related to the spike inputs. In addition we show through computer simulations for more realistic neuron models that the resulting analytically predicted positive learning results not only hold for the common interpretation of STDP where STDP changes the weights of synapses, but also for a more realistic interpretation suggested by experimental data where STDP modulates the initial release probability of dynamic synapses. 1</p><p>same-paper 3 0.87204337 <a title="14-lda-3" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>Author: Yves Grandvalet, Johnny Mariethoz, Samy Bengio</p><p>Abstract: In this paper, we show that the hinge loss can be interpreted as the neg-log-likelihood of a semi-parametric model of posterior probabilities. From this point of view, SVMs represent the parametric component of a semi-parametric model ﬁtted by a maximum a posteriori estimation procedure. This connection enables to derive a mapping from SVM scores to estimated posterior probabilities. Unlike previous proposals, the suggested mapping is interval-valued, providing a set of posterior probabilities compatible with each SVM score. This framework offers a new way to adapt the SVM optimization problem to unbalanced classiﬁcation, when decisions result in unequal (asymmetric) losses. Experiments show improvements over state-of-the-art procedures. 1</p><p>4 0.83787608 <a title="14-lda-4" href="./nips-2005-The_Role_of_Top-down_and_Bottom-up_Processes_in_Guiding_Eye_Movements_during_Visual_Search.html">193 nips-2005-The Role of Top-down and Bottom-up Processes in Guiding Eye Movements during Visual Search</a></p>
<p>Author: Gregory Zelinsky, Wei Zhang, Bing Yu, Xin Chen, Dimitris Samaras</p><p>Abstract: To investigate how top-down (TD) and bottom-up (BU) information is weighted in the guidance of human search behavior, we manipulated the proportions of BU and TD components in a saliency-based model. The model is biologically plausible and implements an artiﬁcial retina and a neuronal population code. The BU component is based on featurecontrast. The TD component is deﬁned by a feature-template match to a stored target representation. We compared the model’s behavior at different mixtures of TD and BU components to the eye movement behavior of human observers performing the identical search task. We found that a purely TD model provides a much closer match to human behavior than any mixture model using BU information. Only when biological constraints are removed (e.g., eliminating the retina) did a BU/TD mixture model begin to approximate human behavior.</p><p>5 0.67575026 <a title="14-lda-5" href="./nips-2005-Optimal_cue_selection_strategy.html">149 nips-2005-Optimal cue selection strategy</a></p>
<p>Author: Vidhya Navalpakkam, Laurent Itti</p><p>Abstract: Survival in the natural world demands the selection of relevant visual cues to rapidly and reliably guide attention towards prey and predators in cluttered environments. We investigate whether our visual system selects cues that guide search in an optimal manner. We formally obtain the optimal cue selection strategy by maximizing the signal to noise ratio (SN R) between a search target and surrounding distractors. This optimal strategy successfully accounts for several phenomena in visual search behavior, including the effect of target-distractor discriminability, uncertainty in target’s features, distractor heterogeneity, and linear separability. Furthermore, the theory generates a new prediction, which we verify through psychophysical experiments with human subjects. Our results provide direct experimental evidence that humans select visual cues so as to maximize SN R between the targets and surrounding clutter.</p><p>6 0.63661402 <a title="14-lda-6" href="./nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">151 nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<p>7 0.61934394 <a title="14-lda-7" href="./nips-2005-Infinite_latent_feature_models_and_the_Indian_buffet_process.html">98 nips-2005-Infinite latent feature models and the Indian buffet process</a></p>
<p>8 0.60092485 <a title="14-lda-8" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>9 0.59928483 <a title="14-lda-9" href="./nips-2005-Bayesian_Surprise_Attracts_Human_Attention.html">34 nips-2005-Bayesian Surprise Attracts Human Attention</a></p>
<p>10 0.59854233 <a title="14-lda-10" href="./nips-2005-Visual_Encoding_with_Jittering_Eyes.html">203 nips-2005-Visual Encoding with Jittering Eyes</a></p>
<p>11 0.59657496 <a title="14-lda-11" href="./nips-2005-Efficient_Unsupervised_Learning_for_Localization_and_Detection_in_Object_Categories.html">63 nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</a></p>
<p>12 0.59397405 <a title="14-lda-12" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<p>13 0.5885976 <a title="14-lda-13" href="./nips-2005-Identifying_Distributed_Object_Representations_in_Human_Extrastriate_Visual_Cortex.html">94 nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</a></p>
<p>14 0.58030105 <a title="14-lda-14" href="./nips-2005-Nonparametric_inference_of_prior_probabilities_from_Bayes-optimal_behavior.html">140 nips-2005-Nonparametric inference of prior probabilities from Bayes-optimal behavior</a></p>
<p>15 0.58016258 <a title="14-lda-15" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<p>16 0.57958055 <a title="14-lda-16" href="./nips-2005-Top-Down_Control_of_Visual_Attention%3A_A_Rational_Account.html">194 nips-2005-Top-Down Control of Visual Attention: A Rational Account</a></p>
<p>17 0.57861215 <a title="14-lda-17" href="./nips-2005-Ideal_Observers_for_Detecting_Motion%3A_Correspondence_Noise.html">93 nips-2005-Ideal Observers for Detecting Motion: Correspondence Noise</a></p>
<p>18 0.57643503 <a title="14-lda-18" href="./nips-2005-A_Bayesian_Framework_for_Tilt_Perception_and_Confidence.html">3 nips-2005-A Bayesian Framework for Tilt Perception and Confidence</a></p>
<p>19 0.573147 <a title="14-lda-19" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>20 0.56987965 <a title="14-lda-20" href="./nips-2005-A_Cortically-Plausible_Inverse_Problem_Solving_Method_Applied_to_Recognizing_Static_and_Kinematic_3D_Objects.html">7 nips-2005-A Cortically-Plausible Inverse Problem Solving Method Applied to Recognizing Static and Kinematic 3D Objects</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
