<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>8 nips-2005-A Criterion for the Convergence of Learning with Spike Timing Dependent Plasticity</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-8" href="#">nips2005-8</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>8 nips-2005-A Criterion for the Convergence of Learning with Spike Timing Dependent Plasticity</h1>
<br/><p>Source: <a title="nips-2005-8-pdf" href="http://papers.nips.cc/paper/2824-a-criterion-for-the-convergence-of-learning-with-spike-timing-dependent-plasticity.pdf">pdf</a></p><p>Author: Robert A. Legenstein, Wolfgang Maass</p><p>Abstract: We investigate under what conditions a neuron can learn by experimentally supported rules for spike timing dependent plasticity (STDP) to predict the arrival times of strong “teacher inputs” to the same neuron. It turns out that in contrast to the famous Perceptron Convergence Theorem, which predicts convergence of the perceptron learning rule for a simpliﬁed neuron model whenever a stable solution exists, no equally strong convergence guarantee can be given for spiking neurons with STDP. But we derive a criterion on the statistical dependency structure of input spike trains which characterizes exactly when learning with STDP will converge on average for a simple model of a spiking neuron. This criterion is reminiscent of the linear separability criterion of the Perceptron Convergence Theorem, but it applies here to the rows of a correlation matrix related to the spike inputs. In addition we show through computer simulations for more realistic neuron models that the resulting analytically predicted positive learning results not only hold for the common interpretation of STDP where STDP changes the weights of synapses, but also for a more realistic interpretation suggested by experimental data where STDP modulates the initial release probability of dynamic synapses. 1</p><p>Reference: <a title="nips-2005-8-reference" href="../nips2005_reference/nips-2005-A_Criterion_for_the_Convergence_of_Learning_with_Spike_Timing_Dependent_Plasticity_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 at  Abstract We investigate under what conditions a neuron can learn by experimentally supported rules for spike timing dependent plasticity (STDP) to predict the arrival times of strong “teacher inputs” to the same neuron. [sent-3, score-0.815]
</p><p>2 It turns out that in contrast to the famous Perceptron Convergence Theorem, which predicts convergence of the perceptron learning rule for a simpliﬁed neuron model whenever a stable solution exists, no equally strong convergence guarantee can be given for spiking neurons with STDP. [sent-4, score-0.575]
</p><p>3 But we derive a criterion on the statistical dependency structure of input spike trains which characterizes exactly when learning with STDP will converge on average for a simple model of a spiking neuron. [sent-5, score-0.958]
</p><p>4 This criterion is reminiscent of the linear separability criterion of the Perceptron Convergence Theorem, but it applies here to the rows of a correlation matrix related to the spike inputs. [sent-6, score-0.861]
</p><p>5 If during training a teacher induces ﬁring of the postsynaptic neuron, this rule becomes somewhat analogous to the well-known perceptron learning rule for McCulloch-Pitts neurons (= “perceptrons”). [sent-9, score-0.404]
</p><p>6 However, we have constructed examples of input spike trains and teacher spike trains wnew =  (omitted in this abstract) such that although a weight vector exists which produces the desired ﬁring and which is stable under STDP, learning with STDP does not converge to a stable solution. [sent-11, score-1.948]
</p><p>7 On the other hand experiments in vivo have shown that neurons can be taught by suitable teacher input to adopt a given ﬁring response [2, 3] (although the spiketiming dependence is not exploited there). [sent-12, score-0.296]
</p><p>8 We show in section 2 that such convergence of learning can be explained by STDP in the average case, provided that a certain criterion is met for the statistical dependence among Poisson spike inputs. [sent-13, score-0.637]
</p><p>9 2  An analytical criterion for the convergence of STDP  The average case analysis in this section is based on the linear Poisson neuron model (see [4, 5]). [sent-15, score-0.316]
</p><p>10 This neuron model outputs a spike train S post (t) which is a realization of a Poisson process with the underlying instantaneous ﬁring rate R post (t). [sent-16, score-0.8]
</p><p>11 We represent a spike train S(t) as a sum of Dirac-δ functions S(t) = k δ(t − tk ), where tk is the k th spike time of the spike train. [sent-17, score-1.609]
</p><p>12 The effect of an input spike at input i at time t is modeled by an increase in the instantaneous ﬁring rate of an amount wi (t ) (t − t ), where is a response kernel and wi (t ) is the synaptic efﬁcacy of synapse i at time t . [sent-18, score-1.141]
</p><p>13 We assume (s) = 0 for s < 0 ∞ (causality), 0 ds (s) = 1 (normalization of the response kernel), and (s) ≥ 0 for all s as well as wi ≥ 0 for all i (excitatory inputs). [sent-19, score-0.211]
</p><p>14 In the linear model, the contributions of all inputs are summed up linearly: n  ∞  Rpost (t) =  ds wj (t − s) (s) Sj (t − s) , j=1  (2)  0  where S1 , . [sent-20, score-0.192]
</p><p>15 Note that in this spike generation process, the generation of an output spike is independent of previous output spikes. [sent-24, score-1.122]
</p><p>16 Alternatively one can make the weight update dependent on the actual weight value. [sent-26, score-0.324]
</p><p>17 In [5] a general rule is suggested where the weight dependence has the form of a power law with a non-negative exponent µ. [sent-27, score-0.2]
</p><p>18 This weight update rule is deﬁned by ∆w =  W+ · (1 − w)µ · e−∆t/τ+ −W− · wµ · e∆t/τ−  , ,  if ∆t > 0 if ∆t ≤ 0 ,  (3)  where we assumed for simplicity that wmax = 1. [sent-28, score-0.313]
</p><p>19 Instead of looking at speciﬁc input spike trains, we consider the average behavior of the weight vector for (possibly correlated) homogeneous Poisson input spike trains. [sent-29, score-1.441]
</p><p>20 We will in the following focus on the drift by assuming that individual weight changes are very small and only averaged quantities enter the learning dynamics, see [6]. [sent-31, score-0.264]
</p><p>21 Let Si be the spike train of input i and let S ∗ be the output spike train of the neuron. [sent-32, score-1.291]
</p><p>22 The mean drift of synapse i at time t can be approximated as 0  ∞  wi (t) = W+ (1 − wi )µ ˙ 0  µ ds e−s/τ Ci (s; t) − W− wi  ds es/τ Ci (s; t)  ,  (4)  −∞  where Ci (s; t) = Si (t)S ∗ (t + s) E is the ensemble averaged correlation function between input i and the output of the neuron (see [5, 6]). [sent-33, score-1.072]
</p><p>23 For the linear Poisson neuron model, inputoutput correlations can be described by means of correlations in the inputs. [sent-34, score-0.493]
</p><p>24 We deﬁne the normalized cross correlation between input spike trains Si and Sj with a common rate r > 0 as Si (t) Sj (t + s) E 0 Cij (s) = −1, (5) r2 which assumes value 0 for uncorrelated Poisson spike trains. [sent-35, score-1.632]
</p><p>25 In our setup, the output of the neuron during learning is clamped  to the teacher spike train S ∗ which is the output of a neuron with the target weight vector w∗ . [sent-37, score-1.554]
</p><p>26 In our neuron model, correlations are shaped by the response kernel (s) and they enter the learning equation (4) with respect to the learning window. [sent-39, score-0.492]
</p><p>27 This motivates the deﬁnition of window correlations c+ and c− for the positive ij ij and negative learning window respectively: c± = 1 + ij  1 τ  ∞  ∞ 0 ds (s )Cij (±s − s )  ds e−s/τ 0  0  . [sent-40, score-0.631]
</p><p>28 Note that ij window correlations are non-negative and that for homogeneous Poisson input spike trains and for a non-negative response kernel, they are positive. [sent-45, score-1.194]
</p><p>29 For soft weight bounds and µ > 0, a synaptic weight can converge to a value arbitrarily close to 0 or 1, but not to one of these values directly. [sent-46, score-0.409]
</p><p>30 Proof idea: The correlation between an input and the teacher induced output is (by Eq. [sent-55, score-0.426]
</p><p>31 4 yields the synaptic drift  n  wi = τ r2 W+ (1 − wi )µ ˙  E  . [sent-57, score-0.349]
</p><p>32 0    n  µ ∗ wj c + − W − wi ij  j=1  j=1  ∗ wj c −  . [sent-58, score-0.27]
</p><p>33 ij  (7)  We ﬁnd the equilibrium points wµi of synapse i by setting wi = 0 in Eq. [sent-59, score-0.244]
</p><p>34 This ˙ −1  yields wµi =  1+  1 1/µ Λi  , where Λi denotes  Pn ∗ + W+ j=1 wj cij Pn ∗ − . [sent-61, score-0.217]
</p><p>35 1 Because of synaptic delays, the response of a spiking neuron to an input spike is delayed by some time t0 . [sent-72, score-1.039]
</p><p>36 In the following Corollary we consider the case where input 0 correlations Cij (s) appear only in a time window smaller than the delay: 0 Corollary 2. [sent-74, score-0.282]
</p><p>37 , n}, then the following holds for the case of homogeneous Poisson input spike trains to a linear Poisson neuron with positive response kernel :  A weight vector w∗ can be learned if and only if w∗ = 0 and w∗ linearly separates the list ∗ ∗ L = c+ , w1 , . [sent-78, score-1.464]
</p><p>38 Its formulation is tight in the sense that linear separability of the list L alone (as opposed to linear separability by the target vector w ∗ ) is not sufﬁcient to imply learnability. [sent-93, score-0.319]
</p><p>39 For uncorrelated input spike trains of rate r > 0, the normalized δij 0 cross correlation functions are given by Cij (s) = r δ(s), where δij is the Kronecker delta function. [sent-94, score-1.118]
</p><p>40 The positive window correlation matrix C + is therefore essentially a scaled version of the identity matrix. [sent-95, score-0.237]
</p><p>41 2 A target weight vector w ∗ ∈ {0, 1}n can be learned in the case of uncorrelated Poisson input spike trains to a linear Poisson neuron with positive response kernel such that (s) = 0 for all s ≤ 0 if and only if w ∗ = 0. [sent-98, score-1.517]
</p><p>42 3  Computer simulations of supervised learning with STDP  In order to make a theoretical analysis feasible, we needed to make in section 2 a number of simplifying assumptions on the neuron model and the synapse model. [sent-99, score-0.331]
</p><p>43 We consider in this section the more realistic integrate-and-ﬁre model2 for neurons and a model for synapses which are subject to paired-pulse depression and paired-pulse facilitation, in addition to the long term plasticity induced by STDP [7]. [sent-101, score-0.261]
</p><p>44 This model describes synapses with parameters U (initial release probability), D (depression time constant), and F (facilitation time constant) in addition to the synaptic weight w. [sent-102, score-0.467]
</p><p>45 V thresh was set such that each neuron spiked at a rate of about 25 Hz. [sent-124, score-0.219]
</p><p>46 We also show that in some cases a less restrictive teacher forcing sufﬁces, that tolerates undesired ﬁring of the neuron during training. [sent-128, score-0.374]
</p><p>47 We used input spike trains with cross correlations that decay exponentially with a correlation decay constant τcc . [sent-130, score-1.251]
</p><p>48 Since such “broader” correlations are not problematic for STDP, sharper correlations (τcc =6ms) are considered in experiment 2. [sent-132, score-0.324]
</p><p>49 Experiment 1 (correlated input with τcc =10ms): In this experiment, a leaky integrateand-ﬁre neuron received inputs from 100 dynamic synapses. [sent-133, score-0.405]
</p><p>50 90% of these synapses were excitatory and 10% were inhibitory. [sent-134, score-0.198]
</p><p>51 For each excitatory synapse, the maximal efﬁcacy wmax was chosen from a Gaussian distribution with mean 54 and SD 10. [sent-135, score-0.208]
</p><p>52 The 90 excitatory inputs were divided into 9 groups of 10 synapses per group. [sent-137, score-0.309]
</p><p>53 Spike trains were correlated within groups with correlation coefﬁcients between 0 and 0. [sent-138, score-0.503]
</p><p>54 8, whereas there were virtually no correlations between spike trains of different groups. [sent-139, score-0.878]
</p><p>55 4 Target weight vectors w∗ were chosen in the most adverse way: half of the weights of w ∗ within each group was set to 0, the other half to its maximal value wmax (see Fig. [sent-140, score-0.471]
</p><p>56 6  0 angular error [rad] spike correlation [σ=5ms]  0. [sent-152, score-0.804]
</p><p>57 2  20 0  0  500  1000 1500 time [sec]  2000  2500  0  0  20  40 60 Synapse  80  Figure 1: Learning a target weight vector w ∗ on correlated Poisson inputs. [sent-154, score-0.35]
</p><p>58 A) Output spike train on test data after one hour of training (trained) compared to the target output (target). [sent-155, score-0.748]
</p><p>59 B) Evolution of the angle between weight vector w(t) and the vector w ∗ that implements F in radiant (angular error, solid line), and spike correlation (dashed line). [sent-156, score-0.936]
</p><p>60 C) Target weight vector w ∗ consisting of elements with value 0 or the value wmax assigned to that synapse. [sent-157, score-0.311]
</p><p>61 (All time data refer to simulated biological time)  Before training, the weights of all excitatory synapses were initialized by randomly chosen small values. [sent-159, score-0.255]
</p><p>62 Information about the target weight vector w ∗ was given to the neuron only in the form of short current injections (1 µA for 0. [sent-161, score-0.511]
</p><p>63 2 ms) at those times when the neuron with the weight vector w∗ would have produced a spike. [sent-162, score-0.417]
</p><p>64 4 The correlation coefﬁcient cij for spike trains within group k consisting of 10 spike trains was set to cij = cck = 0. [sent-167, score-2.02]
</p><p>65 teacher-induced ﬁring of the neuron (see text below). [sent-172, score-0.219]
</p><p>66 The angular error measures the angle between the current weight vector w(t) and the target weight vector w ∗ . [sent-175, score-0.606]
</p><p>67 One can see that the deviation of the learned weight vector shown in panel D from the target weight vector w∗ (panel C) is very small, even for highly correlated groups of synapses with heterogeneous target weights. [sent-178, score-0.883]
</p><p>68 On 20 trials (each with a new random distribution of maximal weights wmax , different initializations w(0) of the weight vector before learning, and new Poisson spike trains), a spike correlation of 0. [sent-180, score-1.603]
</p><p>69 , not teacher-induced) ﬁring of the neuron which results in weight increases for all weights within the corresponding group of synapses according to well-known results for STDP [8, 5]. [sent-188, score-0.602]
</p><p>70 Obviously this effect makes it quite hard to learn a target weight vector w ∗ where half of the weights for each correlated group have value 0. [sent-189, score-0.488]
</p><p>71 However, without this input a spike correlation of 0. [sent-191, score-0.77]
</p><p>72 9 input correlation cc  predicted to be learnable predicted to be not learnable  0. [sent-210, score-0.615]
</p><p>73 05 0  0  5  10  15  weight error [°]  Figure 2: A) Spike correlation achieved for correlated inputs (solid line). [sent-217, score-0.447]
</p><p>74 Some inputs were correlated with cc plotted on the x-axis. [sent-218, score-0.33]
</p><p>75 Also, as a control the spike correlation achieved by randomly drawn weight vectors is shown (dashed line, where half of the weights were set to w max and the other weights were set to 0). [sent-219, score-0.989]
</p><p>76 B) Comparison between theory and simulation results for a leaky integrateand-ﬁre neuron and input correlations between 0. [sent-220, score-0.475]
</p><p>77 Each cross (open circle) marks a trial where the target vector was learnable (not learnable) according to Theorem 2. [sent-223, score-0.312]
</p><p>78 The actual learning performance of STDP is plotted for each trial in terms of the weight error (x-axis) and 1 minus the spike correlation (y-axis). [sent-225, score-0.902]
</p><p>79 Experiment 2 (testing the theoretical predictions for τcc =6ms): In order to evaluate the dependence of correlation among inputs we proceeded in a setup similar to experiment 1. [sent-226, score-0.291]
</p><p>80 4 input groups consisting each of 10 input spike trains were constructed for which the correlations within each group had the same value cc while the input spike train to the other 50 excitatory synapses were uncorrelated. [sent-227, score-2.194]
</p><p>81 Again, half of the weights of w ∗ within 5 We added 30 inhibitory synapses with weights drawn from a gamma distribution with mean 25 and standard deviation 7. [sent-228, score-0.366]
</p><p>82 5, that received additional 30 uncorrelated Poisson spike trains at 20 Hz. [sent-229, score-0.811]
</p><p>83 6 For that purpose each spike in these two output spike trains was replaced by a Gaussian function with an SD of 5 ms. [sent-230, score-1.302]
</p><p>84 The spike correlation between both output spike trains was deﬁned as the correlation between the resulting smooth functions of time (for segments of length 100 s). [sent-231, score-1.65]
</p><p>85 each correlated group (and within the uncorrelated group) was set to 0, the other half to a randomly chosen maximal value. [sent-232, score-0.242]
</p><p>86 2A for 7 different values of the correlation cc (τcc = 6ms) that is applied in 4 of the input groups (solid line). [sent-234, score-0.505]
</p><p>87 1 for leaky integrate-and-ﬁre neurons and dynamic synapses, we repeated the above experiment for input correlations cc = 0. [sent-236, score-0.55]
</p><p>88 For each correlation value, 20 learning trials (with different target vectors) were simulated. [sent-242, score-0.294]
</p><p>89 For each trial we ﬁrst checked whether the (randomly chosen) target vector w∗ was learnable according to the condition given in Theorem 2. [sent-243, score-0.261]
</p><p>90 It shows that the theoretical prediction of learnability or non-learnability for the case of simpler neuron models and synapses from Theorem 2. [sent-248, score-0.414]
</p><p>91 1  angular error [rad] weight deviation spike correlation  0. [sent-252, score-0.984]
</p><p>92 Experiment 3 (Modulation of initial release probabilities U by STDP): Experimental data from [9] suggest that synaptic plasticity does not change the uniform scaling of the amplitudes of EPSPs resulting from a presynaptic spike train (i. [sent-262, score-0.881]
</p><p>93 3 shows results of an experiment where U was modulated with rule (8) (similar to experiment 1, but with uncorrelated inputs). [sent-267, score-0.222]
</p><p>94 20 repetitions of this experiment yielded after 42 minutes of training the following results: spike correlation 0. [sent-268, score-0.805]
</p><p>95 Apparently the output spike train is less sensitive to changes in the values of U than to changes in w. [sent-275, score-0.696]
</p><p>96 ij ij 8 To guarantee the best possible performance for each learning trial, training was performed on 27 different values for W− /W+ between 1. [sent-277, score-0.188]
</p><p>97 20 repetitions with different target weights and different initial conditions yielded after 35 minutes of training: spike correlation 0. [sent-282, score-0.908]
</p><p>98 4  Discussion  The main conclusion of this article is that for many common distributions of input spikes a spiking neuron can learn with STDP and teacher-induced input currents any map from input spike trains to output spike trains that it could possibly implement in a stable manner. [sent-288, score-2.092]
</p><p>99 This theoretical analysis produces the ﬁrst criterion that allows us to predict whether supervised learning with STDP will succeed in spite of correlations among Poisson input spike trains. [sent-290, score-0.809]
</p><p>100 Learning input correlations through u non-linear temporally asymmetric hebbian plasticity. [sent-332, score-0.261]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spike', 0.514), ('stdp', 0.342), ('trains', 0.227), ('neuron', 0.219), ('cc', 0.205), ('poisson', 0.177), ('correlation', 0.174), ('cij', 0.161), ('weight', 0.148), ('correlations', 0.137), ('synapses', 0.136), ('teacher', 0.123), ('angular', 0.116), ('synaptic', 0.113), ('wmax', 0.113), ('target', 0.094), ('wi', 0.09), ('synapse', 0.086), ('perceptron', 0.086), ('input', 0.082), ('learnable', 0.077), ('wold', 0.074), ('separability', 0.073), ('ring', 0.07), ('release', 0.07), ('uncorrelated', 0.07), ('ds', 0.069), ('ij', 0.068), ('train', 0.067), ('inputs', 0.067), ('window', 0.063), ('excitatory', 0.062), ('corollary', 0.06), ('spiking', 0.059), ('ci', 0.059), ('learnability', 0.059), ('correlated', 0.058), ('weights', 0.057), ('drift', 0.056), ('wj', 0.056), ('ibackground', 0.055), ('plasticity', 0.054), ('rule', 0.052), ('response', 0.052), ('homogeneous', 0.051), ('cross', 0.051), ('vector', 0.05), ('experiment', 0.05), ('criterion', 0.05), ('output', 0.047), ('convergence', 0.047), ('inhibitory', 0.045), ('theorem', 0.045), ('groups', 0.044), ('hebbian', 0.042), ('group', 0.042), ('minutes', 0.041), ('trial', 0.04), ('wk', 0.039), ('currents', 0.039), ('si', 0.039), ('half', 0.039), ('neurons', 0.039), ('analogon', 0.037), ('fregnac', 0.037), ('iinject', 0.037), ('shulz', 0.037), ('umax', 0.037), ('uold', 0.037), ('wnew', 0.037), ('gerstner', 0.037), ('leaky', 0.037), ('markram', 0.037), ('presynaptic', 0.035), ('changes', 0.034), ('decay', 0.033), ('maximal', 0.033), ('sec', 0.033), ('cacy', 0.033), ('deviation', 0.032), ('realistic', 0.032), ('modulates', 0.032), ('isyn', 0.032), ('kempter', 0.032), ('tpre', 0.032), ('undesired', 0.032), ('kernel', 0.032), ('separates', 0.031), ('sj', 0.031), ('cm', 0.03), ('learned', 0.029), ('rad', 0.029), ('list', 0.029), ('initial', 0.028), ('delay', 0.028), ('degrees', 0.028), ('dependent', 0.028), ('membrane', 0.027), ('training', 0.026), ('learning', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="8-tfidf-1" href="./nips-2005-A_Criterion_for_the_Convergence_of_Learning_with_Spike_Timing_Dependent_Plasticity.html">8 nips-2005-A Criterion for the Convergence of Learning with Spike Timing Dependent Plasticity</a></p>
<p>Author: Robert A. Legenstein, Wolfgang Maass</p><p>Abstract: We investigate under what conditions a neuron can learn by experimentally supported rules for spike timing dependent plasticity (STDP) to predict the arrival times of strong “teacher inputs” to the same neuron. It turns out that in contrast to the famous Perceptron Convergence Theorem, which predicts convergence of the perceptron learning rule for a simpliﬁed neuron model whenever a stable solution exists, no equally strong convergence guarantee can be given for spiking neurons with STDP. But we derive a criterion on the statistical dependency structure of input spike trains which characterizes exactly when learning with STDP will converge on average for a simple model of a spiking neuron. This criterion is reminiscent of the linear separability criterion of the Perceptron Convergence Theorem, but it applies here to the rows of a correlation matrix related to the spike inputs. In addition we show through computer simulations for more realistic neuron models that the resulting analytically predicted positive learning results not only hold for the common interpretation of STDP where STDP changes the weights of synapses, but also for a more realistic interpretation suggested by experimental data where STDP modulates the initial release probability of dynamic synapses. 1</p><p>2 0.45266697 <a title="8-tfidf-2" href="./nips-2005-Learning_in_Silicon%3A_Timing_is_Everything.html">118 nips-2005-Learning in Silicon: Timing is Everything</a></p>
<p>Author: John V. Arthur, Kwabena Boahen</p><p>Abstract: We describe a neuromorphic chip that uses binary synapses with spike timing-dependent plasticity (STDP) to learn stimulated patterns of activity and to compensate for variability in excitability. Speciﬁcally, STDP preferentially potentiates (turns on) synapses that project from excitable neurons, which spike early, to lethargic neurons, which spike late. The additional excitatory synaptic current makes lethargic neurons spike earlier, thereby causing neurons that belong to the same pattern to spike in synchrony. Once learned, an entire pattern can be recalled by stimulating a subset. 1 Variability in Neural Systems Evidence suggests precise spike timing is important in neural coding, speciﬁcally, in the hippocampus. The hippocampus uses timing in the spike activity of place cells (in addition to rate) to encode location in space [1]. Place cells employ a phase code: the timing at which a neuron spikes relative to the phase of the inhibitory theta rhythm (5-12Hz) conveys information. As an animal approaches a place cell’s preferred location, the place cell not only increases its spike rate, but also spikes at earlier phases in the theta cycle. To implement a phase code, the theta rhythm is thought to prevent spiking until the input synaptic current exceeds the sum of the neuron threshold and the decreasing inhibition on the downward phase of the cycle [2]. However, even with identical inputs and common theta inhibition, neurons do not spike in synchrony. Variability in excitability spreads the activity in phase. Lethargic neurons (such as those with high thresholds) spike late in the theta cycle, since their input exceeds the sum of the neuron threshold and theta inhibition only after the theta inhibition has had time to decrease. Conversely, excitable neurons (such as those with low thresholds) spike early in the theta cycle. Consequently, variability in excitability translates into variability in timing. We hypothesize that the hippocampus achieves its precise spike timing (about 10ms) through plasticity enhanced phase-coding (PEP). The source of hippocampal timing precision in the presence of variability (and noise) remains unexplained. Synaptic plasticity can compensate for variability in excitability if it increases excitatory synaptic input to neurons in inverse proportion to their excitabilities. Recasting this in a phase-coding framework, we desire a learning rule that increases excitatory synaptic input to neurons directly related to their phases. Neurons that lag require additional synaptic input, whereas neurons that lead 120µm 190µm A B Figure 1: STDP Chip. A The chip has a 16-by-16 array of microcircuits; one microcircuit includes four principal neurons, each with 21 STDP circuits. B The STDP Chip is embedded in a circuit board including DACs, a CPLD, a RAM chip, and a USB chip, which communicates with a PC. require none. The spike timing-dependent plasticity (STDP) observed in the hippocampus satisﬁes this requirement [3]. It requires repeated pre-before-post spike pairings (within a time window) to potentiate and repeated post-before-pre pairings to depress a synapse. Here we validate our hypothesis with a model implemented in silicon, where variability is as ubiquitous as it is in biology [4]. Section 2 presents our silicon system, including the STDP Chip. Section 3 describes and characterizes the STDP circuit. Section 4 demonstrates that PEP compensates for variability and provides evidence that STDP is the compensation mechanism. Section 5 explores a desirable consequence of PEP: unconventional associative pattern recall. Section 6 discusses the implications of the PEP model, including its beneﬁts and applications in the engineering of neuromorphic systems and in the study of neurobiology. 2 Silicon System We have designed, submitted, and tested a silicon implementation of PEP. The STDP Chip was fabricated through MOSIS in a 1P5M 0.25µm CMOS process, with just under 750,000 transistors in just over 10mm2 of area. It has a 32 by 32 array of excitatory principal neurons commingled with a 16 by 16 array of inhibitory interneurons that are not used here (Figure 1A). Each principal neuron has 21 STDP synapses. The address-event representation (AER) [5] is used to transmit spikes off chip and to receive afferent and recurrent spike input. To conﬁgure the STDP Chip as a recurrent network, we embedded it in a circuit board (Figure 1B). The board has ﬁve primary components: a CPLD (complex programmable logic device), the STDP Chip, a RAM chip, a USB interface chip, and DACs (digital-to-analog converters). The central component in the system is the CPLD. The CPLD handles AER trafﬁc, mediates communication between devices, and implements recurrent connections by accessing a lookup table, stored in the RAM chip. The USB interface chip provides a bidirectional link with a PC. The DACs control the analog biases in the system, including the leak current, which the PC varies in real-time to create the global inhibitory theta rhythm. The principal neuron consists of a refractory period and calcium-dependent potassium circuit (RCK), a synapse circuit, and a soma circuit (Figure 2A). RCK and the synapse are ISOMA Soma Synapse STDP Presyn. Spike PE LPF A Presyn. Spike Raster AH 0 0.1 Spike probability RCK Postsyn. Spike B 0.05 0.1 0.05 0.1 0.08 0.06 0.04 0.02 0 0 Time(s) Figure 2: Principal neuron. A A simpliﬁed schematic is shown, including: the synapse, refractory and calcium-dependent potassium channel (RCK), soma, and axon-hillock (AH) circuits, plus their constituent elements, the pulse extender (PE) and the low-pass ﬁlter (LPF). B Spikes (dots) from 81 principal neurons are temporally dispersed, when excited by poisson-like inputs (58Hz) and inhibited by the common 8.3Hz theta rhythm (solid line). The histogram includes spikes from ﬁve theta cycles. composed of two reusable blocks: the low-pass ﬁlter (LPF) and the pulse extender (PE). The soma is a modiﬁed version of the LPF, which receives additional input from an axonhillock circuit (AH). RCK is inhibitory to the neuron. It consists of a PE, which models calcium inﬂux during a spike, and a LPF, which models calcium buffering. When AH ﬁres a spike, a packet of charge is dumped onto a capacitor in the PE. The PE’s output activates until the charge decays away, which takes a few milliseconds. Also, while the PE is active, charge accumulates on the LPF’s capacitor, lowering the LPF’s output voltage. Once the PE deactivates, this charge leaks away as well, but this takes tens of milliseconds because the leak is smaller. The PE’s and the LPF’s inhibitory effects on the soma are both described below in terms of the sum (ISHUNT ) of the currents their output voltages produce in pMOS transistors whose sources are at Vdd (see Figure 2A). Note that, in the absence of spikes, these currents decay exponentially, with a time-constant determined by their respective leaks. The synapse circuit is excitatory to the neuron. It is composed of a PE, which represents the neurotransmitter released into the synaptic cleft, and a LPF, which represents the bound neurotransmitter. The synapse circuit is similar to RCK in structure but differs in function: It is activated not by the principal neuron itself but by the STDP circuits (or directly by afferent spikes that bypass these circuits, i.e., ﬁxed synapses). The synapse’s effect on the soma is also described below in terms of the current (ISYN ) its output voltage produces in a pMOS transistor whose source is at Vdd. The soma circuit is a leaky integrator. It receives excitation from the synapse circuit and shunting inhibition from RCK and has a leak current as well. Its temporal behavior is described by: τ dISOMA ISYN I0 + ISOMA = dt ISHUNT where ISOMA is the current the capacitor’s voltage produces in a pMOS transistor whose source is at Vdd (see Figure 2A). ISHUNT is the sum of the leak, refractory, and calciumdependent potassium currents. These currents also determine the time constant: τ = C Ut κISHUNT , where I0 and κ are transistor parameters and Ut is the thermal voltage. STDP circuit ~LTP SRAM Presynaptic spike A ~LTD Inverse number of pairings Integrator Decay Postsynaptic spike Potentiation 0.1 0.05 0 0.05 0.1 Depression -80 -40 0 Presynaptic spike Postsynaptic spike 40 Spike timing: t pre - t post (ms) 80 B Figure 3: STDP circuit design and characterization. A The circuit is composed of three subcircuits: decay, integrator, and SRAM. B The circuit potentiates when the presynaptic spike precedes the postsynaptic spike and depresses when the postsynaptic spike precedes the presynaptic spike. The soma circuit is connected to an AH, the locus of spike generation. The AH consists of model voltage-dependent sodium and potassium channel populations (modiﬁed from [6] by Kai Hynna). It initiates the AER signaling process required to send a spike off chip. To characterize principal neuron variability, we excited 81 neurons with poisson-like 58Hz spike trains (Figure 2B). We made these spike trains poisson-like by starting with a regular 200Hz spike train and dropping spikes randomly, with probability of 0.71. Thus spikes were delivered to neurons that won the coin toss in synchrony every 5ms. However, neurons did not lock onto the input synchrony due to ﬁltering by the synaptic time constant (see Figure 2B). They also received a common inhibitory input at the theta frequency (8.3Hz), via their leak current. Each neuron was prevented from ﬁring more than one spike in a theta cycle by its model calcium-dependent potassium channel population. The principal neurons’ spike times were variable. To quantify the spike variability, we used timing precision, which we deﬁne as twice the standard deviation of spike times accumulated from ﬁve theta cycles. With an input rate of 58Hz the timing precision was 34ms. 3 STDP Circuit The STDP circuit (related to [7]-[8]), for which the STDP Chip is named, is the most abundant, with 21,504 copies on the chip. This circuit is built from three subcircuits: decay, integrator, and SRAM (Figure 3A). The decay and integrator are used to implement potentiation, and depression, in a symmetric fashion. The SRAM holds the current binary state of the synapse, either potentiated or depressed. For potentiation, the decay remembers the last presynaptic spike. Its capacitor is charged when that spike occurs and discharges linearly thereafter. A postsynaptic spike samples the charge remaining on the capacitor, passes it through an exponential function, and dumps the resultant charge into the integrator. This charge decays linearly thereafter. At the time of the postsynaptic spike, the SRAM, a cross-coupled inverter pair, reads the voltage on the integrator’s capacitor. If it exceeds a threshold, the SRAM switches state from depressed to potentiated (∼LTD goes high and ∼LTP goes low). The depression side of the STDP circuit is exactly symmetric, except that it responds to postsynaptic activation followed by presynaptic activation and switches the SRAM’s state from potentiated to depressed (∼LTP goes high and ∼LTD goes low). When the SRAM is in the potentiated state, the presynaptic 50 After STDP 83 92 100 Timing precision(ms) Before STDP 75 B Before STDP After STDP 40 30 20 10 0 50 60 70 80 90 Input rate(Hz) 100 50 58 67 text A 0.2 0.4 Time(s) 0.6 0.2 0.4 Time(s) 0.6 C Figure 4: Plasticity enhanced phase-coding. A Spike rasters of 81 neurons (9 by 9 cluster) display synchrony over a two-fold range of input rates after STDP. B The degree of enhancement is quantiﬁed by timing precision. C Each neuron (center box) sends synapses to (dark gray) and receives synapses from (light gray) twenty-one randomly chosen neighbors up to ﬁve nodes away (black indicates both connections). spike activates the principal neuron’s synapse; otherwise the spike has no effect. We characterized the STDP circuit by activating a plastic synapse and a ﬁxed synapse– which elicits a spike at different relative times. We repeated this pairing at 16Hz. We counted the number of pairings required to potentiate (or depress) the synapse. Based on this count, we calculated the efﬁcacy of each pairing as the inverse number of pairings required (Figure 3B). For example, if twenty pairings were required to potentiate the synapse, the efﬁcacy of that pre-before-post time-interval was one twentieth. The efﬁcacy of both potentiation and depression are ﬁt by exponentials with time constants of 11.4ms and 94.9ms, respectively. This behavior is similar to that observed in the hippocampus: potentiation has a shorter time constant and higher maximum efﬁcacy than depression [3]. 4 Recurrent Network We carried out an experiment designed to test the STDP circuit’s ability to compensate for variability in spike timing through PEP. Each neuron received recurrent connections from 21 randomly selected neurons within an 11 by 11 neighborhood centered on itself (see Figure 4C). Conversely, it made recurrent connections to randomly chosen neurons within the same neighborhood. These connections were mediated by STDP circuits, initialized to the depressed state. We chose a 9 by 9 cluster of neurons and delivered spikes at a mean rate of 50 to 100Hz to each one (dropping spikes with a probability of 0.75 to 0.5 from a regular 200Hz train) and provided common theta inhibition as before. We compared the variability in spike timing after ﬁve seconds of learning with the initial distribution. Phase coding was enhanced after STDP (Figure 4A). Before STDP, spike timing among neurons was highly variable (except for the very highest input rate). After STDP, variability was virtually eliminated (except for the very lowest input rate). Initially, the variability, characterized by timing precision, was inversely related to the input rate, decreasing from 34 to 13ms. After ﬁve seconds of STDP, variability decreased and was largely independent of input rate, remaining below 11ms. Potentiated synapses 25 A Synaptic state after STDP 20 15 10 5 0 B 50 100 150 200 Spiking order 250 Figure 5: Compensating for variability. A Some synapses (dots) become potentiated (light) while others remain depressed (dark) after STDP. B The number of potentiated synapses neurons make (pluses) and receive (circles) is negatively (r = -0.71) and positively (r = 0.76) correlated to their rank in the spiking order, respectively. Comparing the number of potentiated synapses each neuron made or received with its excitability conﬁrmed the PEP hypothesis (i.e., leading neurons provide additional synaptic current to lagging neurons via potentiated recurrent synapses). In this experiment, to eliminate variability due to noise (as opposed to excitability), we provided a 17 by 17 cluster of neurons with a regular 200Hz excitatory input. Theta inhibition was present as before and all synapses were initialized to the depressed state. After 10 seconds of STDP, a large fraction of the synapses were potentiated (Figure 5A). When the number of potentiated synapses each neuron made or received was plotted versus its rank in spiking order (Figure 5B), a clear correlation emerged (r = -0.71 or 0.76, respectively). As expected, neurons that spiked early made more and received fewer potentiated synapses. In contrast, neurons that spiked late made fewer and received more potentiated synapses. 5 Pattern Completion After STDP, we found that the network could recall an entire pattern given a subset, thus the same mechanisms that compensated for variability and noise could also compensate for lack of information. We chose a 9 by 9 cluster of neurons as our pattern and delivered a poisson-like spike train with mean rate of 67Hz to each one as in the ﬁrst experiment. Theta inhibition was present as before and all synapses were initialized to the depressed state. Before STDP, we stimulated a subset of the pattern and only neurons in that subset spiked (Figure 6A). After ﬁve seconds of STDP, we stimulated the same subset again. This time they recruited spikes from other neurons in the pattern, completing it (Figure 6B). Upon varying the fraction of the pattern presented, we found that the fraction recalled increased faster than the fraction presented. We selected subsets of the original pattern randomly, varying the fraction of neurons chosen from 0.1 to 1.0 (ten trials for each). We classiﬁed neurons as active if they spiked in the two second period over which we recorded. Thus, we characterized PEP’s pattern-recall performance as a function of the probability that the pattern in question’s neurons are activated (Figure 6C). At a fraction of 0.50 presented, nearly all of the neurons in the pattern are consistently activated (0.91±0.06), showing robust pattern completion. We ﬁtted the recall performance with a sigmoid that reached 0.50 recall fraction with an input fraction of 0.30. No spurious neurons were activated during any trials. Rate(Hz) Rate(Hz) 8 7 7 6 6 5 5 0.6 0.4 2 0.2 0 0 3 3 2 1 1 A 0.8 4 4 Network activity before STDP 1 Fraction of pattern actived 8 0 B Network activity after STDP C 0 0.2 0.4 0.6 0.8 Fraction of pattern stimulated 1 Figure 6: Associative recall. A Before STDP, half of the neurons in a pattern are stimulated; only they are activated. B After STDP, half of the neurons in a pattern are stimulated, and all are activated. C The fraction of the pattern activated grows faster than the fraction stimulated. 6 Discussion Our results demonstrate that PEP successfully compensates for graded variations in our silicon recurrent network using binary (on–off) synapses (in contrast with [8], where weights are graded). While our chip results are encouraging, variability was not eliminated in every case. In the case of the lowest input (50Hz), we see virtually no change (Figure 4A). We suspect the timing remains imprecise because, with such low input, neurons do not spike every theta cycle and, consequently, provide fewer opportunities for the STDP synapses to potentiate. This shortfall illustrates the system’s limits; it can only compensate for variability within certain bounds, and only for activity appropriate to the PEP model. As expected, STDP is the mechanism responsible for PEP. STDP potentiated recurrent synapses from leading neurons to lagging neurons, reducing the disparity among the diverse population of neurons. Even though the STDP circuits are themselves variable, with different efﬁcacies and time constants, when using timing the sign of the weight-change is always correct (data not shown). For this reason, we chose STDP over other more physiological implementations of plasticity, such as membrane-voltage-dependent plasticity (MVDP), which has the capability to learn with graded voltage signals [9], such as those found in active dendrites, providing more computational power [10]. Previously, we investigated a MVDP circuit, which modeled a voltage-dependent NMDAreceptor-gated synapse [11]. It potentiated when the calcium current analog exceeded a threshold, which was designed to occur only during a dendritic action potential. This circuit produced behavior similar to STDP, implying it could be used in PEP. However, it was sensitive to variability in the NMDA and potentiation thresholds, causing a fraction of the population to potentiate anytime the synapse received an input and another fraction to never potentiate, rendering both subpopulations useless. Therefore, the simpler, less biophysical STDP circuit won out over the MVDP circuit: In our system timing is everything. Associative storage and recall naturally emerge in the PEP network when synapses between neurons coactivated by a pattern are potentiated. These synapses allow neurons to recruit their peers when a subset of the pattern is presented, thereby completing the pattern. However, this form of pattern storage and completion differs from Hopﬁeld’s attractor model [12] . Rather than forming symmetric, recurrent neuronal circuits, our recurrent network forms asymmetric circuits in which neurons make connections exclusively to less excitable neurons in the pattern. In both the poisson-like and regular cases (Figures 4 & 5), only about six percent of potentiated connections were reciprocated, as expected by chance. We plan to investigate the storage capacity of this asymmetric form of associative memory. Our system lends itself to modeling brain regions that use precise spike timing, such as the hippocampus. We plan to extend the work presented to store and recall sequences of patterns, as the hippocampus is hypothesized to do. Place cells that represent different locations spike at different phases of the theta cycle, in relation to the distance to their preferred locations. This sequential spiking will allow us to link patterns representing different locations in the order those locations are visited, thereby realizing episodic memory. We propose PEP as a candidate neural mechanism for information coding and storage in the hippocampal system. Observations from the CA1 region of the hippocampus suggest that basal dendrites (which primarily receive excitation from recurrent connections) support submillisecond timing precision, consistent with PEP [13]. We have shown, in a silicon model, PEP’s ability to exploit such fast recurrent connections to sharpen timing precision as well as to associatively store and recall patterns. Acknowledgments We thank Joe Lin for assistance with chip generation. The Ofﬁce of Naval Research funded this work (Award No. N000140210468). References [1] O’Keefe J. & Recce M.L. (1993). Phase relationship between hippocampal place units and the EEG theta rhythm. Hippocampus 3(3):317-330. [2] Mehta M.R., Lee A.K. & Wilson M.A. (2002) Role of experience and oscillations in transforming a rate code into a temporal code. Nature 417(6890):741-746. [3] Bi G.Q. & Wang H.X. (2002) Temporal asymmetry in spike timing-dependent synaptic plasticity. Physiology & Behavior 77:551-555. [4] Rodriguez-Vazquez, A., Linan, G., Espejo S. & Dominguez-Castro R. (2003) Mismatch-induced trade-offs and scalability of analog preprocessing visual microprocessor chips. Analog Integrated Circuits and Signal Processing 37:73-83. [5] Boahen K.A. (2000) Point-to-point connectivity between neuromorphic chips using address events. IEEE Transactions on Circuits and Systems II 47:416-434. [6] Culurciello E.R., Etienne-Cummings R. & Boahen K.A. (2003) A biomorphic digital image sensor. IEEE Journal of Solid State Circuits 38:281-294. [7] Boﬁll A., Murray A.F & Thompson D.P. (2005) Citcuits for VLSI Implementation of Temporally Asymmetric Hebbian Learning. In: Advances in Neural Information Processing Systems 14, MIT Press, 2002. [8] Cameron K., Boonsobhak V., Murray A. & Renshaw D. (2005) Spike timing dependent plasticity (STDP) can ameliorate process variations in neuromorphic VLSI. IEEE Transactions on Neural Networks 16(6):1626-1627. [9] Chicca E., Badoni D., Dante V., D’Andreagiovanni M., Salina G., Carota L., Fusi S. & Del Giudice P. (2003) A VLSI recurrent network of integrate-and-ﬁre neurons connected by plastic synapses with long-term memory. IEEE Transaction on Neural Networks 14(5):1297-1307. [10] Poirazi P., & Mel B.W. (2001) Impact of active dendrites and structural plasticity on the memory capacity of neural tissue. Neuron 29(3)779-796. [11] Arthur J.V. & Boahen K. (2004) Recurrently connected silicon neurons with active dendrites for one-shot learning. In: IEEE International Joint Conference on Neural Networks 3, pp.1699-1704. [12] Hopﬁeld J.J. (1984) Neurons with graded response have collective computational properties like those of two-state neurons. Proceedings of the National Academy of Science 81(10):3088-3092. [13] Ariav G., Polsky A. & Schiller J. (2003) Submillisecond precision of the input-output transformation function mediated by fast sodium dendritic spikes in basal dendrites of CA1 pyramidal neurons. Journal of Neuroscience 23(21):7750-7758.</p><p>3 0.4279415 <a title="8-tfidf-3" href="./nips-2005-Spiking_Inputs_to_a_Winner-take-all_Network.html">181 nips-2005-Spiking Inputs to a Winner-take-all Network</a></p>
<p>Author: Matthias Oster, Shih-Chii Liu</p><p>Abstract: Recurrent networks that perform a winner-take-all computation have been studied extensively. Although some of these studies include spiking networks, they consider only analog input rates. We present results of this winner-take-all computation on a network of integrate-and-ﬁre neurons which receives spike trains as inputs. We show how we can conﬁgure the connectivity in the network so that the winner is selected after a pre-determined number of input spikes. We discuss spiking inputs with both regular frequencies and Poisson-distributed rates. The robustness of the computation was tested by implementing the winner-take-all network on an analog VLSI array of 64 integrate-and-ﬁre neurons which have an innate variance in their operating parameters. 1</p><p>4 0.33763179 <a title="8-tfidf-4" href="./nips-2005-Integrate-and-Fire_models_with_adaptation_are_good_enough.html">99 nips-2005-Integrate-and-Fire models with adaptation are good enough</a></p>
<p>Author: Renaud Jolivet, Alexander Rauch, Hans-rudolf Lüscher, Wulfram Gerstner</p><p>Abstract: Integrate-and-Fire-type models are usually criticized because of their simplicity. On the other hand, the Integrate-and-Fire model is the basis of most of the theoretical studies on spiking neuron models. Here, we develop a sequential procedure to quantitatively evaluate an equivalent Integrate-and-Fire-type model based on intracellular recordings of cortical pyramidal neurons. We ﬁnd that the resulting effective model is sufﬁcient to predict the spike train of the real pyramidal neuron with high accuracy. In in vivo-like regimes, predicted and recorded traces are almost indistinguishable and a signiﬁcant part of the spikes can be predicted at the correct timing. Slow processes like spike-frequency adaptation are shown to be a key feature in this context since they are necessary for the model to connect between different driving regimes. 1</p><p>5 0.33047536 <a title="8-tfidf-5" href="./nips-2005-Beyond_Pair-Based_STDP%3A_a_Phenomenological_Rule_for_Spike_Triplet_and_Frequency_Effects.html">39 nips-2005-Beyond Pair-Based STDP: a Phenomenological Rule for Spike Triplet and Frequency Effects</a></p>
<p>Author: Jean-pascal Pfister, Wulfram Gerstner</p><p>Abstract: While classical experiments on spike-timing dependent plasticity analyzed synaptic changes as a function of the timing of pairs of pre- and postsynaptic spikes, more recent experiments also point to the effect of spike triplets. Here we develop a mathematical framework that allows us to characterize timing based learning rules. Moreover, we identify a candidate learning rule with ﬁve variables (and 5 free parameters) that captures a variety of experimental data, including the dependence of potentiation and depression upon pre- and postsynaptic ﬁring frequencies. The relation to the Bienenstock-Cooper-Munro rule as well as to some timing-based rules is discussed. 1</p><p>6 0.29654071 <a title="8-tfidf-6" href="./nips-2005-Temporally_changing_synaptic_plasticity.html">188 nips-2005-Temporally changing synaptic plasticity</a></p>
<p>7 0.25792933 <a title="8-tfidf-7" href="./nips-2005-Efficient_estimation_of_hidden_state_dynamics_from_spike_trains.html">64 nips-2005-Efficient estimation of hidden state dynamics from spike trains</a></p>
<p>8 0.16495249 <a title="8-tfidf-8" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>9 0.1368341 <a title="8-tfidf-9" href="./nips-2005-Principles_of_real-time_computing_with_feedback_applied_to_cortical_microcircuit_models.html">157 nips-2005-Principles of real-time computing with feedback applied to cortical microcircuit models</a></p>
<p>10 0.11604302 <a title="8-tfidf-10" href="./nips-2005-Dynamical_Synapses_Give_Rise_to_a_Power-Law_Distribution_of_Neuronal_Avalanches.html">61 nips-2005-Dynamical Synapses Give Rise to a Power-Law Distribution of Neuronal Avalanches</a></p>
<p>11 0.11426666 <a title="8-tfidf-11" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>12 0.11066648 <a title="8-tfidf-12" href="./nips-2005-Neural_mechanisms_of_contrast_dependent_receptive_field_size_in_V1.html">134 nips-2005-Neural mechanisms of contrast dependent receptive field size in V1</a></p>
<p>13 0.11031116 <a title="8-tfidf-13" href="./nips-2005-Large-scale_biophysical_parameter_estimation_in_single_neurons_via_constrained_linear_regression.html">106 nips-2005-Large-scale biophysical parameter estimation in single neurons via constrained linear regression</a></p>
<p>14 0.097607955 <a title="8-tfidf-14" href="./nips-2005-Measuring_Shared_Information_and_Coordinated_Activity_in_Neuronal_Networks.html">124 nips-2005-Measuring Shared Information and Coordinated Activity in Neuronal Networks</a></p>
<p>15 0.09137699 <a title="8-tfidf-15" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<p>16 0.08712475 <a title="8-tfidf-16" href="./nips-2005-Silicon_growth_cones_map_silicon_retina.html">176 nips-2005-Silicon growth cones map silicon retina</a></p>
<p>17 0.080907337 <a title="8-tfidf-17" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>18 0.070198596 <a title="8-tfidf-18" href="./nips-2005-Optimal_cue_selection_strategy.html">149 nips-2005-Optimal cue selection strategy</a></p>
<p>19 0.069432087 <a title="8-tfidf-19" href="./nips-2005-Representing_Part-Whole_Relationships_in_Recurrent_Neural_Networks.html">164 nips-2005-Representing Part-Whole Relationships in Recurrent Neural Networks</a></p>
<p>20 0.066408932 <a title="8-tfidf-20" href="./nips-2005-Unbiased_Estimator_of_Shape_Parameter_for_Spiking_Irregularities_under_Changing_Environments.html">197 nips-2005-Unbiased Estimator of Shape Parameter for Spiking Irregularities under Changing Environments</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.25), (1, -0.537), (2, -0.163), (3, -0.288), (4, -0.009), (5, -0.054), (6, 0.102), (7, 0.141), (8, -0.037), (9, -0.09), (10, 0.121), (11, 0.021), (12, 0.033), (13, 0.058), (14, 0.051), (15, -0.018), (16, -0.05), (17, 0.018), (18, -0.005), (19, -0.083), (20, -0.047), (21, -0.009), (22, 0.037), (23, 0.004), (24, 0.048), (25, -0.055), (26, -0.026), (27, -0.014), (28, -0.036), (29, -0.019), (30, -0.009), (31, 0.037), (32, 0.066), (33, -0.016), (34, 0.006), (35, 0.058), (36, -0.035), (37, 0.036), (38, 0.047), (39, -0.027), (40, -0.0), (41, 0.0), (42, 0.036), (43, 0.018), (44, -0.047), (45, -0.045), (46, 0.006), (47, -0.074), (48, -0.031), (49, 0.097)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97867799 <a title="8-lsi-1" href="./nips-2005-A_Criterion_for_the_Convergence_of_Learning_with_Spike_Timing_Dependent_Plasticity.html">8 nips-2005-A Criterion for the Convergence of Learning with Spike Timing Dependent Plasticity</a></p>
<p>Author: Robert A. Legenstein, Wolfgang Maass</p><p>Abstract: We investigate under what conditions a neuron can learn by experimentally supported rules for spike timing dependent plasticity (STDP) to predict the arrival times of strong “teacher inputs” to the same neuron. It turns out that in contrast to the famous Perceptron Convergence Theorem, which predicts convergence of the perceptron learning rule for a simpliﬁed neuron model whenever a stable solution exists, no equally strong convergence guarantee can be given for spiking neurons with STDP. But we derive a criterion on the statistical dependency structure of input spike trains which characterizes exactly when learning with STDP will converge on average for a simple model of a spiking neuron. This criterion is reminiscent of the linear separability criterion of the Perceptron Convergence Theorem, but it applies here to the rows of a correlation matrix related to the spike inputs. In addition we show through computer simulations for more realistic neuron models that the resulting analytically predicted positive learning results not only hold for the common interpretation of STDP where STDP changes the weights of synapses, but also for a more realistic interpretation suggested by experimental data where STDP modulates the initial release probability of dynamic synapses. 1</p><p>2 0.9081887 <a title="8-lsi-2" href="./nips-2005-Beyond_Pair-Based_STDP%3A_a_Phenomenological_Rule_for_Spike_Triplet_and_Frequency_Effects.html">39 nips-2005-Beyond Pair-Based STDP: a Phenomenological Rule for Spike Triplet and Frequency Effects</a></p>
<p>Author: Jean-pascal Pfister, Wulfram Gerstner</p><p>Abstract: While classical experiments on spike-timing dependent plasticity analyzed synaptic changes as a function of the timing of pairs of pre- and postsynaptic spikes, more recent experiments also point to the effect of spike triplets. Here we develop a mathematical framework that allows us to characterize timing based learning rules. Moreover, we identify a candidate learning rule with ﬁve variables (and 5 free parameters) that captures a variety of experimental data, including the dependence of potentiation and depression upon pre- and postsynaptic ﬁring frequencies. The relation to the Bienenstock-Cooper-Munro rule as well as to some timing-based rules is discussed. 1</p><p>3 0.90199381 <a title="8-lsi-3" href="./nips-2005-Learning_in_Silicon%3A_Timing_is_Everything.html">118 nips-2005-Learning in Silicon: Timing is Everything</a></p>
<p>Author: John V. Arthur, Kwabena Boahen</p><p>Abstract: We describe a neuromorphic chip that uses binary synapses with spike timing-dependent plasticity (STDP) to learn stimulated patterns of activity and to compensate for variability in excitability. Speciﬁcally, STDP preferentially potentiates (turns on) synapses that project from excitable neurons, which spike early, to lethargic neurons, which spike late. The additional excitatory synaptic current makes lethargic neurons spike earlier, thereby causing neurons that belong to the same pattern to spike in synchrony. Once learned, an entire pattern can be recalled by stimulating a subset. 1 Variability in Neural Systems Evidence suggests precise spike timing is important in neural coding, speciﬁcally, in the hippocampus. The hippocampus uses timing in the spike activity of place cells (in addition to rate) to encode location in space [1]. Place cells employ a phase code: the timing at which a neuron spikes relative to the phase of the inhibitory theta rhythm (5-12Hz) conveys information. As an animal approaches a place cell’s preferred location, the place cell not only increases its spike rate, but also spikes at earlier phases in the theta cycle. To implement a phase code, the theta rhythm is thought to prevent spiking until the input synaptic current exceeds the sum of the neuron threshold and the decreasing inhibition on the downward phase of the cycle [2]. However, even with identical inputs and common theta inhibition, neurons do not spike in synchrony. Variability in excitability spreads the activity in phase. Lethargic neurons (such as those with high thresholds) spike late in the theta cycle, since their input exceeds the sum of the neuron threshold and theta inhibition only after the theta inhibition has had time to decrease. Conversely, excitable neurons (such as those with low thresholds) spike early in the theta cycle. Consequently, variability in excitability translates into variability in timing. We hypothesize that the hippocampus achieves its precise spike timing (about 10ms) through plasticity enhanced phase-coding (PEP). The source of hippocampal timing precision in the presence of variability (and noise) remains unexplained. Synaptic plasticity can compensate for variability in excitability if it increases excitatory synaptic input to neurons in inverse proportion to their excitabilities. Recasting this in a phase-coding framework, we desire a learning rule that increases excitatory synaptic input to neurons directly related to their phases. Neurons that lag require additional synaptic input, whereas neurons that lead 120µm 190µm A B Figure 1: STDP Chip. A The chip has a 16-by-16 array of microcircuits; one microcircuit includes four principal neurons, each with 21 STDP circuits. B The STDP Chip is embedded in a circuit board including DACs, a CPLD, a RAM chip, and a USB chip, which communicates with a PC. require none. The spike timing-dependent plasticity (STDP) observed in the hippocampus satisﬁes this requirement [3]. It requires repeated pre-before-post spike pairings (within a time window) to potentiate and repeated post-before-pre pairings to depress a synapse. Here we validate our hypothesis with a model implemented in silicon, where variability is as ubiquitous as it is in biology [4]. Section 2 presents our silicon system, including the STDP Chip. Section 3 describes and characterizes the STDP circuit. Section 4 demonstrates that PEP compensates for variability and provides evidence that STDP is the compensation mechanism. Section 5 explores a desirable consequence of PEP: unconventional associative pattern recall. Section 6 discusses the implications of the PEP model, including its beneﬁts and applications in the engineering of neuromorphic systems and in the study of neurobiology. 2 Silicon System We have designed, submitted, and tested a silicon implementation of PEP. The STDP Chip was fabricated through MOSIS in a 1P5M 0.25µm CMOS process, with just under 750,000 transistors in just over 10mm2 of area. It has a 32 by 32 array of excitatory principal neurons commingled with a 16 by 16 array of inhibitory interneurons that are not used here (Figure 1A). Each principal neuron has 21 STDP synapses. The address-event representation (AER) [5] is used to transmit spikes off chip and to receive afferent and recurrent spike input. To conﬁgure the STDP Chip as a recurrent network, we embedded it in a circuit board (Figure 1B). The board has ﬁve primary components: a CPLD (complex programmable logic device), the STDP Chip, a RAM chip, a USB interface chip, and DACs (digital-to-analog converters). The central component in the system is the CPLD. The CPLD handles AER trafﬁc, mediates communication between devices, and implements recurrent connections by accessing a lookup table, stored in the RAM chip. The USB interface chip provides a bidirectional link with a PC. The DACs control the analog biases in the system, including the leak current, which the PC varies in real-time to create the global inhibitory theta rhythm. The principal neuron consists of a refractory period and calcium-dependent potassium circuit (RCK), a synapse circuit, and a soma circuit (Figure 2A). RCK and the synapse are ISOMA Soma Synapse STDP Presyn. Spike PE LPF A Presyn. Spike Raster AH 0 0.1 Spike probability RCK Postsyn. Spike B 0.05 0.1 0.05 0.1 0.08 0.06 0.04 0.02 0 0 Time(s) Figure 2: Principal neuron. A A simpliﬁed schematic is shown, including: the synapse, refractory and calcium-dependent potassium channel (RCK), soma, and axon-hillock (AH) circuits, plus their constituent elements, the pulse extender (PE) and the low-pass ﬁlter (LPF). B Spikes (dots) from 81 principal neurons are temporally dispersed, when excited by poisson-like inputs (58Hz) and inhibited by the common 8.3Hz theta rhythm (solid line). The histogram includes spikes from ﬁve theta cycles. composed of two reusable blocks: the low-pass ﬁlter (LPF) and the pulse extender (PE). The soma is a modiﬁed version of the LPF, which receives additional input from an axonhillock circuit (AH). RCK is inhibitory to the neuron. It consists of a PE, which models calcium inﬂux during a spike, and a LPF, which models calcium buffering. When AH ﬁres a spike, a packet of charge is dumped onto a capacitor in the PE. The PE’s output activates until the charge decays away, which takes a few milliseconds. Also, while the PE is active, charge accumulates on the LPF’s capacitor, lowering the LPF’s output voltage. Once the PE deactivates, this charge leaks away as well, but this takes tens of milliseconds because the leak is smaller. The PE’s and the LPF’s inhibitory effects on the soma are both described below in terms of the sum (ISHUNT ) of the currents their output voltages produce in pMOS transistors whose sources are at Vdd (see Figure 2A). Note that, in the absence of spikes, these currents decay exponentially, with a time-constant determined by their respective leaks. The synapse circuit is excitatory to the neuron. It is composed of a PE, which represents the neurotransmitter released into the synaptic cleft, and a LPF, which represents the bound neurotransmitter. The synapse circuit is similar to RCK in structure but differs in function: It is activated not by the principal neuron itself but by the STDP circuits (or directly by afferent spikes that bypass these circuits, i.e., ﬁxed synapses). The synapse’s effect on the soma is also described below in terms of the current (ISYN ) its output voltage produces in a pMOS transistor whose source is at Vdd. The soma circuit is a leaky integrator. It receives excitation from the synapse circuit and shunting inhibition from RCK and has a leak current as well. Its temporal behavior is described by: τ dISOMA ISYN I0 + ISOMA = dt ISHUNT where ISOMA is the current the capacitor’s voltage produces in a pMOS transistor whose source is at Vdd (see Figure 2A). ISHUNT is the sum of the leak, refractory, and calciumdependent potassium currents. These currents also determine the time constant: τ = C Ut κISHUNT , where I0 and κ are transistor parameters and Ut is the thermal voltage. STDP circuit ~LTP SRAM Presynaptic spike A ~LTD Inverse number of pairings Integrator Decay Postsynaptic spike Potentiation 0.1 0.05 0 0.05 0.1 Depression -80 -40 0 Presynaptic spike Postsynaptic spike 40 Spike timing: t pre - t post (ms) 80 B Figure 3: STDP circuit design and characterization. A The circuit is composed of three subcircuits: decay, integrator, and SRAM. B The circuit potentiates when the presynaptic spike precedes the postsynaptic spike and depresses when the postsynaptic spike precedes the presynaptic spike. The soma circuit is connected to an AH, the locus of spike generation. The AH consists of model voltage-dependent sodium and potassium channel populations (modiﬁed from [6] by Kai Hynna). It initiates the AER signaling process required to send a spike off chip. To characterize principal neuron variability, we excited 81 neurons with poisson-like 58Hz spike trains (Figure 2B). We made these spike trains poisson-like by starting with a regular 200Hz spike train and dropping spikes randomly, with probability of 0.71. Thus spikes were delivered to neurons that won the coin toss in synchrony every 5ms. However, neurons did not lock onto the input synchrony due to ﬁltering by the synaptic time constant (see Figure 2B). They also received a common inhibitory input at the theta frequency (8.3Hz), via their leak current. Each neuron was prevented from ﬁring more than one spike in a theta cycle by its model calcium-dependent potassium channel population. The principal neurons’ spike times were variable. To quantify the spike variability, we used timing precision, which we deﬁne as twice the standard deviation of spike times accumulated from ﬁve theta cycles. With an input rate of 58Hz the timing precision was 34ms. 3 STDP Circuit The STDP circuit (related to [7]-[8]), for which the STDP Chip is named, is the most abundant, with 21,504 copies on the chip. This circuit is built from three subcircuits: decay, integrator, and SRAM (Figure 3A). The decay and integrator are used to implement potentiation, and depression, in a symmetric fashion. The SRAM holds the current binary state of the synapse, either potentiated or depressed. For potentiation, the decay remembers the last presynaptic spike. Its capacitor is charged when that spike occurs and discharges linearly thereafter. A postsynaptic spike samples the charge remaining on the capacitor, passes it through an exponential function, and dumps the resultant charge into the integrator. This charge decays linearly thereafter. At the time of the postsynaptic spike, the SRAM, a cross-coupled inverter pair, reads the voltage on the integrator’s capacitor. If it exceeds a threshold, the SRAM switches state from depressed to potentiated (∼LTD goes high and ∼LTP goes low). The depression side of the STDP circuit is exactly symmetric, except that it responds to postsynaptic activation followed by presynaptic activation and switches the SRAM’s state from potentiated to depressed (∼LTP goes high and ∼LTD goes low). When the SRAM is in the potentiated state, the presynaptic 50 After STDP 83 92 100 Timing precision(ms) Before STDP 75 B Before STDP After STDP 40 30 20 10 0 50 60 70 80 90 Input rate(Hz) 100 50 58 67 text A 0.2 0.4 Time(s) 0.6 0.2 0.4 Time(s) 0.6 C Figure 4: Plasticity enhanced phase-coding. A Spike rasters of 81 neurons (9 by 9 cluster) display synchrony over a two-fold range of input rates after STDP. B The degree of enhancement is quantiﬁed by timing precision. C Each neuron (center box) sends synapses to (dark gray) and receives synapses from (light gray) twenty-one randomly chosen neighbors up to ﬁve nodes away (black indicates both connections). spike activates the principal neuron’s synapse; otherwise the spike has no effect. We characterized the STDP circuit by activating a plastic synapse and a ﬁxed synapse– which elicits a spike at different relative times. We repeated this pairing at 16Hz. We counted the number of pairings required to potentiate (or depress) the synapse. Based on this count, we calculated the efﬁcacy of each pairing as the inverse number of pairings required (Figure 3B). For example, if twenty pairings were required to potentiate the synapse, the efﬁcacy of that pre-before-post time-interval was one twentieth. The efﬁcacy of both potentiation and depression are ﬁt by exponentials with time constants of 11.4ms and 94.9ms, respectively. This behavior is similar to that observed in the hippocampus: potentiation has a shorter time constant and higher maximum efﬁcacy than depression [3]. 4 Recurrent Network We carried out an experiment designed to test the STDP circuit’s ability to compensate for variability in spike timing through PEP. Each neuron received recurrent connections from 21 randomly selected neurons within an 11 by 11 neighborhood centered on itself (see Figure 4C). Conversely, it made recurrent connections to randomly chosen neurons within the same neighborhood. These connections were mediated by STDP circuits, initialized to the depressed state. We chose a 9 by 9 cluster of neurons and delivered spikes at a mean rate of 50 to 100Hz to each one (dropping spikes with a probability of 0.75 to 0.5 from a regular 200Hz train) and provided common theta inhibition as before. We compared the variability in spike timing after ﬁve seconds of learning with the initial distribution. Phase coding was enhanced after STDP (Figure 4A). Before STDP, spike timing among neurons was highly variable (except for the very highest input rate). After STDP, variability was virtually eliminated (except for the very lowest input rate). Initially, the variability, characterized by timing precision, was inversely related to the input rate, decreasing from 34 to 13ms. After ﬁve seconds of STDP, variability decreased and was largely independent of input rate, remaining below 11ms. Potentiated synapses 25 A Synaptic state after STDP 20 15 10 5 0 B 50 100 150 200 Spiking order 250 Figure 5: Compensating for variability. A Some synapses (dots) become potentiated (light) while others remain depressed (dark) after STDP. B The number of potentiated synapses neurons make (pluses) and receive (circles) is negatively (r = -0.71) and positively (r = 0.76) correlated to their rank in the spiking order, respectively. Comparing the number of potentiated synapses each neuron made or received with its excitability conﬁrmed the PEP hypothesis (i.e., leading neurons provide additional synaptic current to lagging neurons via potentiated recurrent synapses). In this experiment, to eliminate variability due to noise (as opposed to excitability), we provided a 17 by 17 cluster of neurons with a regular 200Hz excitatory input. Theta inhibition was present as before and all synapses were initialized to the depressed state. After 10 seconds of STDP, a large fraction of the synapses were potentiated (Figure 5A). When the number of potentiated synapses each neuron made or received was plotted versus its rank in spiking order (Figure 5B), a clear correlation emerged (r = -0.71 or 0.76, respectively). As expected, neurons that spiked early made more and received fewer potentiated synapses. In contrast, neurons that spiked late made fewer and received more potentiated synapses. 5 Pattern Completion After STDP, we found that the network could recall an entire pattern given a subset, thus the same mechanisms that compensated for variability and noise could also compensate for lack of information. We chose a 9 by 9 cluster of neurons as our pattern and delivered a poisson-like spike train with mean rate of 67Hz to each one as in the ﬁrst experiment. Theta inhibition was present as before and all synapses were initialized to the depressed state. Before STDP, we stimulated a subset of the pattern and only neurons in that subset spiked (Figure 6A). After ﬁve seconds of STDP, we stimulated the same subset again. This time they recruited spikes from other neurons in the pattern, completing it (Figure 6B). Upon varying the fraction of the pattern presented, we found that the fraction recalled increased faster than the fraction presented. We selected subsets of the original pattern randomly, varying the fraction of neurons chosen from 0.1 to 1.0 (ten trials for each). We classiﬁed neurons as active if they spiked in the two second period over which we recorded. Thus, we characterized PEP’s pattern-recall performance as a function of the probability that the pattern in question’s neurons are activated (Figure 6C). At a fraction of 0.50 presented, nearly all of the neurons in the pattern are consistently activated (0.91±0.06), showing robust pattern completion. We ﬁtted the recall performance with a sigmoid that reached 0.50 recall fraction with an input fraction of 0.30. No spurious neurons were activated during any trials. Rate(Hz) Rate(Hz) 8 7 7 6 6 5 5 0.6 0.4 2 0.2 0 0 3 3 2 1 1 A 0.8 4 4 Network activity before STDP 1 Fraction of pattern actived 8 0 B Network activity after STDP C 0 0.2 0.4 0.6 0.8 Fraction of pattern stimulated 1 Figure 6: Associative recall. A Before STDP, half of the neurons in a pattern are stimulated; only they are activated. B After STDP, half of the neurons in a pattern are stimulated, and all are activated. C The fraction of the pattern activated grows faster than the fraction stimulated. 6 Discussion Our results demonstrate that PEP successfully compensates for graded variations in our silicon recurrent network using binary (on–off) synapses (in contrast with [8], where weights are graded). While our chip results are encouraging, variability was not eliminated in every case. In the case of the lowest input (50Hz), we see virtually no change (Figure 4A). We suspect the timing remains imprecise because, with such low input, neurons do not spike every theta cycle and, consequently, provide fewer opportunities for the STDP synapses to potentiate. This shortfall illustrates the system’s limits; it can only compensate for variability within certain bounds, and only for activity appropriate to the PEP model. As expected, STDP is the mechanism responsible for PEP. STDP potentiated recurrent synapses from leading neurons to lagging neurons, reducing the disparity among the diverse population of neurons. Even though the STDP circuits are themselves variable, with different efﬁcacies and time constants, when using timing the sign of the weight-change is always correct (data not shown). For this reason, we chose STDP over other more physiological implementations of plasticity, such as membrane-voltage-dependent plasticity (MVDP), which has the capability to learn with graded voltage signals [9], such as those found in active dendrites, providing more computational power [10]. Previously, we investigated a MVDP circuit, which modeled a voltage-dependent NMDAreceptor-gated synapse [11]. It potentiated when the calcium current analog exceeded a threshold, which was designed to occur only during a dendritic action potential. This circuit produced behavior similar to STDP, implying it could be used in PEP. However, it was sensitive to variability in the NMDA and potentiation thresholds, causing a fraction of the population to potentiate anytime the synapse received an input and another fraction to never potentiate, rendering both subpopulations useless. Therefore, the simpler, less biophysical STDP circuit won out over the MVDP circuit: In our system timing is everything. Associative storage and recall naturally emerge in the PEP network when synapses between neurons coactivated by a pattern are potentiated. These synapses allow neurons to recruit their peers when a subset of the pattern is presented, thereby completing the pattern. However, this form of pattern storage and completion differs from Hopﬁeld’s attractor model [12] . Rather than forming symmetric, recurrent neuronal circuits, our recurrent network forms asymmetric circuits in which neurons make connections exclusively to less excitable neurons in the pattern. In both the poisson-like and regular cases (Figures 4 & 5), only about six percent of potentiated connections were reciprocated, as expected by chance. We plan to investigate the storage capacity of this asymmetric form of associative memory. Our system lends itself to modeling brain regions that use precise spike timing, such as the hippocampus. We plan to extend the work presented to store and recall sequences of patterns, as the hippocampus is hypothesized to do. Place cells that represent different locations spike at different phases of the theta cycle, in relation to the distance to their preferred locations. This sequential spiking will allow us to link patterns representing different locations in the order those locations are visited, thereby realizing episodic memory. We propose PEP as a candidate neural mechanism for information coding and storage in the hippocampal system. Observations from the CA1 region of the hippocampus suggest that basal dendrites (which primarily receive excitation from recurrent connections) support submillisecond timing precision, consistent with PEP [13]. We have shown, in a silicon model, PEP’s ability to exploit such fast recurrent connections to sharpen timing precision as well as to associatively store and recall patterns. Acknowledgments We thank Joe Lin for assistance with chip generation. The Ofﬁce of Naval Research funded this work (Award No. N000140210468). References [1] O’Keefe J. & Recce M.L. (1993). Phase relationship between hippocampal place units and the EEG theta rhythm. Hippocampus 3(3):317-330. [2] Mehta M.R., Lee A.K. & Wilson M.A. (2002) Role of experience and oscillations in transforming a rate code into a temporal code. Nature 417(6890):741-746. [3] Bi G.Q. & Wang H.X. (2002) Temporal asymmetry in spike timing-dependent synaptic plasticity. Physiology & Behavior 77:551-555. [4] Rodriguez-Vazquez, A., Linan, G., Espejo S. & Dominguez-Castro R. (2003) Mismatch-induced trade-offs and scalability of analog preprocessing visual microprocessor chips. Analog Integrated Circuits and Signal Processing 37:73-83. [5] Boahen K.A. (2000) Point-to-point connectivity between neuromorphic chips using address events. IEEE Transactions on Circuits and Systems II 47:416-434. [6] Culurciello E.R., Etienne-Cummings R. & Boahen K.A. (2003) A biomorphic digital image sensor. IEEE Journal of Solid State Circuits 38:281-294. [7] Boﬁll A., Murray A.F & Thompson D.P. (2005) Citcuits for VLSI Implementation of Temporally Asymmetric Hebbian Learning. In: Advances in Neural Information Processing Systems 14, MIT Press, 2002. [8] Cameron K., Boonsobhak V., Murray A. & Renshaw D. (2005) Spike timing dependent plasticity (STDP) can ameliorate process variations in neuromorphic VLSI. IEEE Transactions on Neural Networks 16(6):1626-1627. [9] Chicca E., Badoni D., Dante V., D’Andreagiovanni M., Salina G., Carota L., Fusi S. & Del Giudice P. (2003) A VLSI recurrent network of integrate-and-ﬁre neurons connected by plastic synapses with long-term memory. IEEE Transaction on Neural Networks 14(5):1297-1307. [10] Poirazi P., & Mel B.W. (2001) Impact of active dendrites and structural plasticity on the memory capacity of neural tissue. Neuron 29(3)779-796. [11] Arthur J.V. & Boahen K. (2004) Recurrently connected silicon neurons with active dendrites for one-shot learning. In: IEEE International Joint Conference on Neural Networks 3, pp.1699-1704. [12] Hopﬁeld J.J. (1984) Neurons with graded response have collective computational properties like those of two-state neurons. Proceedings of the National Academy of Science 81(10):3088-3092. [13] Ariav G., Polsky A. & Schiller J. (2003) Submillisecond precision of the input-output transformation function mediated by fast sodium dendritic spikes in basal dendrites of CA1 pyramidal neurons. Journal of Neuroscience 23(21):7750-7758.</p><p>4 0.84889364 <a title="8-lsi-4" href="./nips-2005-Integrate-and-Fire_models_with_adaptation_are_good_enough.html">99 nips-2005-Integrate-and-Fire models with adaptation are good enough</a></p>
<p>Author: Renaud Jolivet, Alexander Rauch, Hans-rudolf Lüscher, Wulfram Gerstner</p><p>Abstract: Integrate-and-Fire-type models are usually criticized because of their simplicity. On the other hand, the Integrate-and-Fire model is the basis of most of the theoretical studies on spiking neuron models. Here, we develop a sequential procedure to quantitatively evaluate an equivalent Integrate-and-Fire-type model based on intracellular recordings of cortical pyramidal neurons. We ﬁnd that the resulting effective model is sufﬁcient to predict the spike train of the real pyramidal neuron with high accuracy. In in vivo-like regimes, predicted and recorded traces are almost indistinguishable and a signiﬁcant part of the spikes can be predicted at the correct timing. Slow processes like spike-frequency adaptation are shown to be a key feature in this context since they are necessary for the model to connect between different driving regimes. 1</p><p>5 0.83560026 <a title="8-lsi-5" href="./nips-2005-Spiking_Inputs_to_a_Winner-take-all_Network.html">181 nips-2005-Spiking Inputs to a Winner-take-all Network</a></p>
<p>Author: Matthias Oster, Shih-Chii Liu</p><p>Abstract: Recurrent networks that perform a winner-take-all computation have been studied extensively. Although some of these studies include spiking networks, they consider only analog input rates. We present results of this winner-take-all computation on a network of integrate-and-ﬁre neurons which receives spike trains as inputs. We show how we can conﬁgure the connectivity in the network so that the winner is selected after a pre-determined number of input spikes. We discuss spiking inputs with both regular frequencies and Poisson-distributed rates. The robustness of the computation was tested by implementing the winner-take-all network on an analog VLSI array of 64 integrate-and-ﬁre neurons which have an innate variance in their operating parameters. 1</p><p>6 0.78647792 <a title="8-lsi-6" href="./nips-2005-Temporally_changing_synaptic_plasticity.html">188 nips-2005-Temporally changing synaptic plasticity</a></p>
<p>7 0.6647656 <a title="8-lsi-7" href="./nips-2005-Efficient_estimation_of_hidden_state_dynamics_from_spike_trains.html">64 nips-2005-Efficient estimation of hidden state dynamics from spike trains</a></p>
<p>8 0.48659688 <a title="8-lsi-8" href="./nips-2005-Dynamical_Synapses_Give_Rise_to_a_Power-Law_Distribution_of_Neuronal_Avalanches.html">61 nips-2005-Dynamical Synapses Give Rise to a Power-Law Distribution of Neuronal Avalanches</a></p>
<p>9 0.41617981 <a title="8-lsi-9" href="./nips-2005-Principles_of_real-time_computing_with_feedback_applied_to_cortical_microcircuit_models.html">157 nips-2005-Principles of real-time computing with feedback applied to cortical microcircuit models</a></p>
<p>10 0.41507176 <a title="8-lsi-10" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>11 0.4061344 <a title="8-lsi-11" href="./nips-2005-Silicon_growth_cones_map_silicon_retina.html">176 nips-2005-Silicon growth cones map silicon retina</a></p>
<p>12 0.34401834 <a title="8-lsi-12" href="./nips-2005-Measuring_Shared_Information_and_Coordinated_Activity_in_Neuronal_Networks.html">124 nips-2005-Measuring Shared Information and Coordinated Activity in Neuronal Networks</a></p>
<p>13 0.31939027 <a title="8-lsi-13" href="./nips-2005-Modeling_Memory_Transfer_and_Saving_in_Cerebellar_Motor_Learning.html">128 nips-2005-Modeling Memory Transfer and Saving in Cerebellar Motor Learning</a></p>
<p>14 0.31885439 <a title="8-lsi-14" href="./nips-2005-A_Connectionist_Model_for_Constructive_Modal_Reasoning.html">6 nips-2005-A Connectionist Model for Constructive Modal Reasoning</a></p>
<p>15 0.31157365 <a title="8-lsi-15" href="./nips-2005-Large-scale_biophysical_parameter_estimation_in_single_neurons_via_constrained_linear_regression.html">106 nips-2005-Large-scale biophysical parameter estimation in single neurons via constrained linear regression</a></p>
<p>16 0.31140196 <a title="8-lsi-16" href="./nips-2005-Response_Analysis_of_Neuronal_Population_with_Synaptic_Depression.html">165 nips-2005-Response Analysis of Neuronal Population with Synaptic Depression</a></p>
<p>17 0.30799934 <a title="8-lsi-17" href="./nips-2005-Neural_mechanisms_of_contrast_dependent_receptive_field_size_in_V1.html">134 nips-2005-Neural mechanisms of contrast dependent receptive field size in V1</a></p>
<p>18 0.2870627 <a title="8-lsi-18" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>19 0.26357347 <a title="8-lsi-19" href="./nips-2005-AER_Building_Blocks_for_Multi-Layer_Multi-Chip_Neuromorphic_Vision_Systems.html">1 nips-2005-AER Building Blocks for Multi-Layer Multi-Chip Neuromorphic Vision Systems</a></p>
<p>20 0.26223987 <a title="8-lsi-20" href="./nips-2005-CMOL_CrossNets%3A_Possible_Neuromorphic_Nanoelectronic_Circuits.html">40 nips-2005-CMOL CrossNets: Possible Neuromorphic Nanoelectronic Circuits</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.047), (10, 0.053), (27, 0.028), (31, 0.036), (33, 0.011), (34, 0.061), (39, 0.282), (41, 0.011), (55, 0.017), (57, 0.068), (60, 0.019), (69, 0.06), (71, 0.035), (73, 0.015), (88, 0.062), (91, 0.092)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89699006 <a title="8-lda-1" href="./nips-2005-A_Computational_Model_of_Eye_Movements_during_Object_Class_Detection.html">5 nips-2005-A Computational Model of Eye Movements during Object Class Detection</a></p>
<p>Author: Wei Zhang, Hyejin Yang, Dimitris Samaras, Gregory J. Zelinsky</p><p>Abstract: We present a computational model of human eye movements in an object class detection task. The model combines state-of-the-art computer vision object class detection methods (SIFT features trained using AdaBoost) with a biologically plausible model of human eye movement to produce a sequence of simulated ﬁxations, culminating with the acquisition of a target. We validated the model by comparing its behavior to the behavior of human observers performing the identical object class detection task (looking for a teddy bear among visually complex nontarget objects). We found considerable agreement between the model and human data in multiple eye movement measures, including number of ﬁxations, cumulative probability of ﬁxating the target, and scanpath distance.</p><p>same-paper 2 0.89107066 <a title="8-lda-2" href="./nips-2005-A_Criterion_for_the_Convergence_of_Learning_with_Spike_Timing_Dependent_Plasticity.html">8 nips-2005-A Criterion for the Convergence of Learning with Spike Timing Dependent Plasticity</a></p>
<p>Author: Robert A. Legenstein, Wolfgang Maass</p><p>Abstract: We investigate under what conditions a neuron can learn by experimentally supported rules for spike timing dependent plasticity (STDP) to predict the arrival times of strong “teacher inputs” to the same neuron. It turns out that in contrast to the famous Perceptron Convergence Theorem, which predicts convergence of the perceptron learning rule for a simpliﬁed neuron model whenever a stable solution exists, no equally strong convergence guarantee can be given for spiking neurons with STDP. But we derive a criterion on the statistical dependency structure of input spike trains which characterizes exactly when learning with STDP will converge on average for a simple model of a spiking neuron. This criterion is reminiscent of the linear separability criterion of the Perceptron Convergence Theorem, but it applies here to the rows of a correlation matrix related to the spike inputs. In addition we show through computer simulations for more realistic neuron models that the resulting analytically predicted positive learning results not only hold for the common interpretation of STDP where STDP changes the weights of synapses, but also for a more realistic interpretation suggested by experimental data where STDP modulates the initial release probability of dynamic synapses. 1</p><p>3 0.80074382 <a title="8-lda-3" href="./nips-2005-The_Role_of_Top-down_and_Bottom-up_Processes_in_Guiding_Eye_Movements_during_Visual_Search.html">193 nips-2005-The Role of Top-down and Bottom-up Processes in Guiding Eye Movements during Visual Search</a></p>
<p>Author: Gregory Zelinsky, Wei Zhang, Bing Yu, Xin Chen, Dimitris Samaras</p><p>Abstract: To investigate how top-down (TD) and bottom-up (BU) information is weighted in the guidance of human search behavior, we manipulated the proportions of BU and TD components in a saliency-based model. The model is biologically plausible and implements an artiﬁcial retina and a neuronal population code. The BU component is based on featurecontrast. The TD component is deﬁned by a feature-template match to a stored target representation. We compared the model’s behavior at different mixtures of TD and BU components to the eye movement behavior of human observers performing the identical search task. We found that a purely TD model provides a much closer match to human behavior than any mixture model using BU information. Only when biological constraints are removed (e.g., eliminating the retina) did a BU/TD mixture model begin to approximate human behavior.</p><p>4 0.74893767 <a title="8-lda-4" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>Author: Yves Grandvalet, Johnny Mariethoz, Samy Bengio</p><p>Abstract: In this paper, we show that the hinge loss can be interpreted as the neg-log-likelihood of a semi-parametric model of posterior probabilities. From this point of view, SVMs represent the parametric component of a semi-parametric model ﬁtted by a maximum a posteriori estimation procedure. This connection enables to derive a mapping from SVM scores to estimated posterior probabilities. Unlike previous proposals, the suggested mapping is interval-valued, providing a set of posterior probabilities compatible with each SVM score. This framework offers a new way to adapt the SVM optimization problem to unbalanced classiﬁcation, when decisions result in unequal (asymmetric) losses. Experiments show improvements over state-of-the-art procedures. 1</p><p>5 0.6518864 <a title="8-lda-5" href="./nips-2005-Optimal_cue_selection_strategy.html">149 nips-2005-Optimal cue selection strategy</a></p>
<p>Author: Vidhya Navalpakkam, Laurent Itti</p><p>Abstract: Survival in the natural world demands the selection of relevant visual cues to rapidly and reliably guide attention towards prey and predators in cluttered environments. We investigate whether our visual system selects cues that guide search in an optimal manner. We formally obtain the optimal cue selection strategy by maximizing the signal to noise ratio (SN R) between a search target and surrounding distractors. This optimal strategy successfully accounts for several phenomena in visual search behavior, including the effect of target-distractor discriminability, uncertainty in target’s features, distractor heterogeneity, and linear separability. Furthermore, the theory generates a new prediction, which we verify through psychophysical experiments with human subjects. Our results provide direct experimental evidence that humans select visual cues so as to maximize SN R between the targets and surrounding clutter.</p><p>6 0.60597163 <a title="8-lda-6" href="./nips-2005-Visual_Encoding_with_Jittering_Eyes.html">203 nips-2005-Visual Encoding with Jittering Eyes</a></p>
<p>7 0.5988543 <a title="8-lda-7" href="./nips-2005-Spiking_Inputs_to_a_Winner-take-all_Network.html">181 nips-2005-Spiking Inputs to a Winner-take-all Network</a></p>
<p>8 0.56255841 <a title="8-lda-8" href="./nips-2005-Principles_of_real-time_computing_with_feedback_applied_to_cortical_microcircuit_models.html">157 nips-2005-Principles of real-time computing with feedback applied to cortical microcircuit models</a></p>
<p>9 0.56218046 <a title="8-lda-9" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>10 0.55612111 <a title="8-lda-10" href="./nips-2005-Bayesian_Surprise_Attracts_Human_Attention.html">34 nips-2005-Bayesian Surprise Attracts Human Attention</a></p>
<p>11 0.55117577 <a title="8-lda-11" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>12 0.54537076 <a title="8-lda-12" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>13 0.54463905 <a title="8-lda-13" href="./nips-2005-Efficient_Unsupervised_Learning_for_Localization_and_Detection_in_Object_Categories.html">63 nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</a></p>
<p>14 0.53938597 <a title="8-lda-14" href="./nips-2005-Top-Down_Control_of_Visual_Attention%3A_A_Rational_Account.html">194 nips-2005-Top-Down Control of Visual Attention: A Rational Account</a></p>
<p>15 0.53513932 <a title="8-lda-15" href="./nips-2005-Beyond_Pair-Based_STDP%3A_a_Phenomenological_Rule_for_Spike_Triplet_and_Frequency_Effects.html">39 nips-2005-Beyond Pair-Based STDP: a Phenomenological Rule for Spike Triplet and Frequency Effects</a></p>
<p>16 0.53223753 <a title="8-lda-16" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<p>17 0.52881151 <a title="8-lda-17" href="./nips-2005-Infinite_latent_feature_models_and_the_Indian_buffet_process.html">98 nips-2005-Infinite latent feature models and the Indian buffet process</a></p>
<p>18 0.52645439 <a title="8-lda-18" href="./nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">151 nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<p>19 0.52089041 <a title="8-lda-19" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<p>20 0.51410681 <a title="8-lda-20" href="./nips-2005-Learning_in_Silicon%3A_Timing_is_Everything.html">118 nips-2005-Learning in Silicon: Timing is Everything</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
