<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>101 nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-101" href="#">nips2005-101</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>101 nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</h1>
<br/><p>Source: <a title="nips-2005-101-pdf" href="http://papers.nips.cc/paper/2901-is-early-vision-optimized-for-extracting-higher-order-dependencies.pdf">pdf</a></p><p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: Linear implementations of the efﬁcient coding hypothesis, such as independent component analysis (ICA) and sparse coding models, have provided functional explanations for properties of simple cells in V1 [1, 2]. These models, however, ignore the non-linear behavior of neurons and fail to match individual and population properties of neural receptive ﬁelds in subtle but important ways. Hierarchical models, including Gaussian Scale Mixtures [3, 4] and other generative statistical models [5, 6], can capture higher-order regularities in natural images and explain nonlinear aspects of neural processing such as normalization and context effects [6,7]. Previously, it had been assumed that the lower level representation is independent of the hierarchy, and had been ﬁxed when training these models. Here we examine the optimal lower-level representations derived in the context of a hierarchical model and ﬁnd that the resulting representations are strikingly different from those based on linear models. Unlike the the basis functions and ﬁlters learned by ICA or sparse coding, these functions individually more closely resemble simple cell receptive ﬁelds and collectively span a broad range of spatial scales. Our work uniﬁes several related approaches and observations about natural image structure and suggests that hierarchical models might yield better representations of image structure throughout the hierarchy.</p><p>Reference: <a title="nips-2005-101-reference" href="../nips2005_reference/nips-2005-Is_Early_Vision_Optimized_for_Extracting_Higher-order_Dependencies%3F_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 These models, however, ignore the non-linear behavior of neurons and fail to match individual and population properties of neural receptive ﬁelds in subtle but important ways. [sent-8, score-0.318]
</p><p>2 Hierarchical models, including Gaussian Scale Mixtures [3, 4] and other generative statistical models [5, 6], can capture higher-order regularities in natural images and explain nonlinear aspects of neural processing such as normalization and context effects [6,7]. [sent-9, score-0.452]
</p><p>3 Previously, it had been assumed that the lower level representation is independent of the hierarchy, and had been ﬁxed when training these models. [sent-10, score-0.149]
</p><p>4 Here we examine the optimal lower-level representations derived in the context of a hierarchical model and ﬁnd that the resulting representations are strikingly different from those based on linear models. [sent-11, score-0.563]
</p><p>5 Unlike the the basis functions and ﬁlters learned by ICA or sparse coding, these functions individually more closely resemble simple cell receptive ﬁelds and collectively span a broad range of spatial scales. [sent-12, score-0.99]
</p><p>6 Our work uniﬁes several related approaches and observations about natural image structure and suggests that hierarchical models might yield better representations of image structure throughout the hierarchy. [sent-13, score-0.735]
</p><p>7 1 Introduction Efﬁcient coding hypothesis has been proposed as a guiding computational principle for the analysis of early visual system and motivates the search for good statistical models of natural images. [sent-14, score-0.511]
</p><p>8 Early work revealed that image statistics are highly non-Gaussian [8, 9], and models such as independent component analysis (ICA) and sparse coding have been developed to capture these statistics to form efﬁcient representations of natural images. [sent-15, score-0.913]
</p><p>9 It has been suggested that these models explain the basic computational goal of early visual cortex, as evidenced by the similarity between the learned parameters and the measured receptive ﬁelds of simple cells in V1. [sent-16, score-0.463]
</p><p>10 ∗  To whom correspondence should be addressed  In fact, it is not clear exactly how well these methods predict the shapes of neural receptive ﬁelds. [sent-17, score-0.318]
</p><p>11 There has been no thorough characterization of ICA and sparse coding results for different datasets, pre-processing methods, and speciﬁc learning algorithms employed, although some of these factors clearly affect the resulting representation [10]. [sent-18, score-0.508]
</p><p>12 When ICA or sparse coding is applied to natural images, the resulting basis functions resemble Gabor functions [1, 2] — 2D sine waves modulated by Gaussian envelopes — which also accurately model the shapes of simple cell receptive ﬁelds [11]. [sent-19, score-1.17]
</p><p>13 Often, these results are visualized in a transformed space, by taking the logarithm of the pixel intensities, sphering (whitening) the image space, or ﬁltering the images to ﬂatten their spectrum. [sent-20, score-0.192]
</p><p>14 When analyzed in the original image space, the learned ﬁlters (the models’ analogues of neural receptive ﬁelds) do not exhibit the multi-scale properties of the visual system, as they tend to cluster at high spatial frequencies [10, 12]. [sent-21, score-0.778]
</p><p>15 Neural receptive ﬁelds, on the other hand, span a broad range of spatial scales, and exhibit distributions of spatial phase and other parameters unmatched by ICA and SC results [13,14]. [sent-22, score-0.625]
</p><p>16 Therefore, as models of early visual processing, these models fail to predict accurately either the individual or the population properties of cortical visual neurons. [sent-23, score-0.331]
</p><p>17 Linear efﬁcient coding methods are also limited in the type of statistical structure they can capture. [sent-24, score-0.313]
</p><p>18 Applied to natural images, their coefﬁcients contain signiﬁcant residual dependencies that cannot be accounted for by the linear form of the models. [sent-25, score-0.184]
</p><p>19 Several solutions have been proposed, including multiplicative Gaussian Scale Mixtures [4] and generative hierarchical models [5, 6]. [sent-26, score-0.31]
</p><p>20 These models capture some of the observed dependencies; but their analysis so far has been focused on the higher-order structure learned by the model. [sent-27, score-0.26]
</p><p>21 Meanwhile, the lower-level representation is either chosen a priori [4] or adapted separately, in the absence of the hierarchy [6] or with a ﬁxed hierarchical structure speciﬁed in advance [5]. [sent-28, score-0.485]
</p><p>22 Here we examine whether the optimal lower-level representation of natural images is different when trained in the context of such non-linear hierarchical models. [sent-29, score-0.574]
</p><p>23 We also illustrate how the model not only describes sparse marginal densities and magnitude dependencies, but captures a variety of joint density functions that are consistent with previous observations and theoretical conjectures. [sent-30, score-0.496]
</p><p>24 We show that learned lower-level representations are strikingly different from those learned by the linear models: they are more multi-scale, spanning a wide range of spatial scales and phases of the Gabor sinusoid relative to the Gaussian envelope. [sent-31, score-0.574]
</p><p>25 2 Fully adaptable scale mixture model A simple and scalable model for natural image patches is a linear factor model, in which the data x are assumed to be generated as a linear combination of basis functions with additive noise x = Au + . [sent-33, score-0.648]
</p><p>26 (2)  The coefﬁcients u are assumed to be mutually independent, and often modeled with sparse distributions (e. [sent-35, score-0.281]
</p><p>27 Laplacian) that reﬂect the non-Gaussian statistics of natural scenes [8,9], P (ui ) ∝ exp(−  P (u) = i  |ui |) . [sent-37, score-0.175]
</p><p>28 i  (3)  We can then adapt the basis functions A to maximize the expected log-likelihood of the data L = log P (x|A) over the data ensemble, thereby learning a compact, efﬁcient representation of structure in natural images. [sent-38, score-0.409]
</p><p>29 This is the model underlying the sparse coding algorithm [2] and closely related to independent component analysis (ICA) [1]. [sent-39, score-0.528]
</p><p>30 An alternative to ﬁxed sparse priors for u (3) is to use a Gaussian Scale Mixture (GSM) model [3]. [sent-40, score-0.259]
</p><p>31 This type of model can also account for the observed dependencies among coefﬁcients u, for example, by expressing them as pair-wise dependencies among the multiplier variables λ [4, 15]. [sent-42, score-0.29]
</p><p>32 A more general model, proposed in [6, 16], employs a hierarchical prior for P (u) with adapted parameters tuned to the global patterns in higher-order dependencies. [sent-43, score-0.327]
</p><p>33 In fact, if the conditional density P (u|v) is Gaussian, this Hi2 erarchical Scale Mixture (HSM) is equivalent to a GSM model, with λ = σu and P (u|λ) = P (u|v) = N (0, exp(Bv)), with the added advantage of a more ﬂexible representation of higher-order statistical regularities in B. [sent-46, score-0.218]
</p><p>34 Whereas previous GSM models of natural images focused on modeling local relationships between coefﬁcients of ﬁxed linear transforms, this general hierarchical formulation is fully adaptable, allowing us to recover the optimal lower-level representation A, as well as the higher-order components B. [sent-47, score-0.584]
</p><p>35 The optimal lower-level basis A is computed similarly to the sparse coding algorithm — the goal is to minimize reconstruction ˆ ˆ error of the inferred MAP estimate u. [sent-50, score-0.573]
</p><p>36 However, u is estimated not with a ﬁxed sparsifying prior, but with a concurrently adapted hierarchical prior. [sent-51, score-0.327]
</p><p>37 (9) 2  Marginalizing over the latent higher-order variables in the hierarchical models leads to sparse distributions similar to the Laplacian and other density functions assumed in ICA. [sent-53, score-0.705]
</p><p>38 Gaussian, qu = 2  Laplacian, qu = 1  Gen Gauss, qu = 0. [sent-54, score-0.243]
</p><p>39 7  HSM, B = [0;0]  HSM, B = [1;1]  HSM, B = [2;2]  HSM, B = [1;−1]  HSM, B = [2;−2]  HSM, B = [1;−2]  Figure 1: This model can describe a variety of joint density functions for coefﬁcients u. [sent-55, score-0.206]
</p><p>40 For illustration, in the hierarchical models the dimensionality of v is 1, and the matrix B is simply a column vector. [sent-59, score-0.31]
</p><p>41 Even with this simple hierarchy, the model can generate sparse star-shaped (bottom row) or radially symmetric (middle row) densities, as well as more complex non-symmetric densities (bottom right). [sent-61, score-0.353]
</p><p>42 Bi-variate joint distributions of GSM coefﬁcients can capture non-linear dependencies in wavelet coefﬁcients [4]. [sent-68, score-0.326]
</p><p>43 In the fully adaptable HSM, however, the joint density can take a variety of shapes that depend on the learned parameters B (ﬁgure 1). [sent-69, score-0.384]
</p><p>44 Note that this model can produce sparse, star-shaped distributions as in the linear models, or radially symmetric distributions that cannot be described by the linear models. [sent-70, score-0.193]
</p><p>45 Such joint density proﬁles have been observed empirically in the responses of phase-offset wavelet coefﬁcients to natural images and have inspired polar transformation and quadrature pair models [17] (as well as connections to phaseinvariant neural responses). [sent-71, score-0.494]
</p><p>46 The model described here can capture these joint densities and others, but rather than assume this structure a priori, it learns it automatically from the data. [sent-72, score-0.247]
</p><p>47 3 Methods To examine how the lower-level representation is affected by the hierarchical model structure, we compared A learned by the sparse coding algorithm [2] and the HSM described above. [sent-73, score-0.908]
</p><p>48 The models were trained on 20 × 20 image patches sampled from 40 images of out-  door scenes in the Kyoto dataset [12]. [sent-74, score-0.283]
</p><p>49 We applied a low-pass radially symmetric ﬁlter to the full images to eliminate high corner frequencies (artifacts of the square sampling lattice), and removed the DC component from each image patch, but did no further pre-processing. [sent-75, score-0.384]
</p><p>50 1, and the basis functions were initialized to small random values and adapted on stochastically sampled batches of 300 patches. [sent-78, score-0.262]
</p><p>51 The parameters of the hierarchical model were estimated in a similar fashion. [sent-81, score-0.302]
</p><p>52 01, because emergence of the variance patterns requires some stabilization in the basis functions in A. [sent-85, score-0.235]
</p><p>53 Because encoding in the sparse coding and in the hierarchical model is a non-linear process, it is not possible to compare the inverse of A to physiological data. [sent-86, score-0.766]
</p><p>54 4 Results The shapes of basis functions and ﬁlters obtained with sparse coding have been previously analyzed and compared to neural receptive ﬁelds [10, 14]. [sent-89, score-1.02]
</p><p>55 In the original space, sparse coding basis functions have very particular shapes: except for a few large, low frequency functions, all are localized, odd-symmetric, and span only a single period of the sinusoid (ﬁgure 2, top left). [sent-91, score-0.785]
</p><p>56 The estimated ﬁlters are similar but smaller (ﬁgure 2, bottom left), with peak spatial frequencies clustered at higher frequencies (ﬁgure 3). [sent-92, score-0.35]
</p><p>57 In the hierarchical model, the learned representation is strikingly different (ﬁgure 2, right panels). [sent-93, score-0.509]
</p><p>58 Both the basis and the ﬁlters span a wider range of spatial scales, a result previously unobserved for models trained on non-preprocessed images, and one that is more consistent with physiological data [13, 14]. [sent-94, score-0.399]
</p><p>59 Also, the shapes of the basis functions are different — they more closely resemble Gabor functions, although they tend to be less smooth than the sparse coding basis functions. [sent-95, score-0.925]
</p><p>60 We also compared the distributions of spatial phases for ﬁlters obtained with sparse coding and the hierarchical model (ﬁgure 4). [sent-97, score-0.942]
</p><p>61 While sparse coding ﬁlters exhibit a strong tendency for odd-symmetric phase proﬁles, the hierarchical model results in a much more uniform distribution of spatial phases. [sent-98, score-0.968]
</p><p>62 Although some phase asymmetry has been observed in simple cell receptive ﬁelds, their phase properties tend to be much more uniform than sparse coding ﬁlters [14]. [sent-99, score-0.854]
</p><p>63 In the hierarchical model, the higher-order representation B is also adapted to the statistical structure of natural images. [sent-100, score-0.564]
</p><p>64 sparse or Gaussian) can determine the type of structure captured in B, we discovered that it does not affect the nature of the lower-level representation. [sent-103, score-0.225]
</p><p>65 Shown are subsets of the learned basis functions and the estimates for the ﬁlters obtained with reverse correlation. [sent-108, score-0.332]
</p><p>66 These functions are displayed in the original image space. [sent-109, score-0.151]
</p><p>67 25  0°  180°  0  0°  Figure 3: Scatter plots of peak frequencies and orientations of the Gabor functions ﬁtted to the estimated ﬁlters. [sent-114, score-0.203]
</p><p>68 Although both SC and HSM ﬁlters exhibit predominantly high spatial frequencies, the hierarchical model yields a representation that tiles the spatial frequency space much more evenly. [sent-116, score-0.673]
</p><p>69 SC phase  HSM phase  SC freq  HSM freq  40  40  50  50  20  20  25  25  0  0  π/4  π/2  0  0  π/4  π/2  0 0. [sent-117, score-0.272]
</p><p>70 50  Figure 4: The distributions of phases and frequencies for Gabor functions ﬁtted to sparse coding (SC) and hierarchical scale model (HSM) ﬁlters. [sent-125, score-1.056]
</p><p>71 The phase units specify the phase of the sinusoid in relation to the peak of the Gaussian envelope of the Gabor function; 0 is even-symmetric, π/2 is odd-symmetric. [sent-126, score-0.276]
</p><p>72 group co-localized lower-level basis functions and separately represent spatial contrast and oriented image structure. [sent-128, score-0.402]
</p><p>73 As reported previously [6,16], with a sparse prior on v, the model learns higher-order components that individually capture complex spatial, orientation, and scale regularities in image data. [sent-129, score-0.553]
</p><p>74 5 Discussion We have demonstrated that adapting a general hierarchical model yields lower-level representations that are signiﬁcantly different than those obtained using ﬁxed priors and linear generative models. [sent-130, score-0.463]
</p><p>75 The resulting basis functions and ﬁlters are multi-scale and more consistent with several observed characteristics of neural receptive ﬁelds. [sent-131, score-0.453]
</p><p>76 It is interesting that the learned representations are similar to the results obtained when ICA or sparse coding is applied to whitened images (i. [sent-132, score-0.755]
</p><p>77 The hierarchical model is performing a similar scaling operation through the inference of higher-order variables v that scale the priors on basis function coefﬁcients u. [sent-136, score-0.542]
</p><p>78 Thus the model can rely on a generic “white” lower level representation, while employing an adaptive mechanism for normalizing the space, which accounts for non-stationary statistics on an image-by-image basis [6]. [sent-137, score-0.205]
</p><p>79 The ﬂexibility of the hierarchical model allows us to learn a lower-level representation that is optimal in the context of the hierarchy. [sent-139, score-0.409]
</p><p>80 Thus, we expect the learned parameters to deﬁne a better statistical model for natural images than other approaches in which the lower-level representation or the higher-order dependencies are ﬁxed in advance. [sent-140, score-0.525]
</p><p>81 For example, the ﬂexible marginal distributions, illustrated in ﬁgure 1, should be able to capture a wider range of statistical structure in natural images. [sent-141, score-0.251]
</p><p>82 One way to quantify the beneﬁt of an adapted lowerlevel representation is to apply the model to problems like image de-noising and ﬁlling-in missing pixels. [sent-142, score-0.257]
</p><p>83 Finally, although the results presented here are more consistent with the observed properties of neural receptive ﬁelds, several discrepancies remain. [sent-144, score-0.287]
</p><p>84 For example, our results, as well as those of other statistical models, fail to account for the prevalence of low spatial frequency receptive ﬁelds observed in V1. [sent-145, score-0.428]
</p><p>85 This could be a result of the speciﬁc choice of the distribution assumed by the model, although the described hierarchical framework makes few assumptions about the joint distribution of basis function coefﬁcients. [sent-146, score-0.509]
</p><p>86 More likely, the non-stationary statistics of the natural scenes play a role in determining the properties of the learned representation. [sent-147, score-0.309]
</p><p>87 This provides a strong motivation for training models with an “over-complete” basis, in which the number of basis functions is greater than the dimensionality of the input data [19]. [sent-149, score-0.245]
</p><p>88 In this case, different subsets of the basis functions can adapt to optimally represent different image contexts, and the population properties of such over-complete representations could be signiﬁcantly different. [sent-150, score-0.435]
</p><p>89 It would be particularly interesting to investigate representations learned in these models in the context of a hierarchical model. [sent-151, score-0.52]
</p><p>90 Emergence of simple-cell receptive-ﬁeld properties by learning a sparse code for natural images. [sent-164, score-0.32]
</p><p>91 Random cascades on wavelet trees and their use in analyzing and modeling natural images. [sent-180, score-0.178]
</p><p>92 A hierarchical bayesian model for learning non-linear statistical regularities in non-stationary natural signals. [sent-193, score-0.491]
</p><p>93 Independent component ﬁlters of natural images compared with simple cells in primary visual cortex. [sent-217, score-0.318]
</p><p>94 An evaluation of the two-dimensional gabor ﬁlter model of simple receptive ﬁelds in cat striate cortex. [sent-224, score-0.394]
</p><p>95 Sparse coding of natural images using an overcomplete set of limited capacity units. [sent-230, score-0.483]
</p><p>96 Spatial frequency selectivity of cells in macaque visual cortex. [sent-239, score-0.16]
</p><p>97 Spatial structure and symmetry of simple-cell receptive ﬁelds in macaque primary visual cortex. [sent-244, score-0.316]
</p><p>98 Image denoising using Gaussian scale mixtures in the wavelet domain. [sent-253, score-0.199]
</p><p>99 Nonlinear neurons and highorder statistics: New approaches to human vision and electronic image processing. [sent-264, score-0.167]
</p><p>100 Sparse coding with an overcomplete basis set: A strategy employed by V1? [sent-282, score-0.422]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hsm', 0.484), ('hierarchical', 0.267), ('coding', 0.246), ('sc', 0.21), ('receptive', 0.19), ('sparse', 0.188), ('lters', 0.184), ('gabor', 0.169), ('ica', 0.148), ('basis', 0.139), ('cients', 0.114), ('spatial', 0.112), ('coef', 0.112), ('bv', 0.107), ('images', 0.104), ('elds', 0.101), ('frequencies', 0.098), ('learned', 0.098), ('shapes', 0.097), ('natural', 0.096), ('dependencies', 0.088), ('image', 0.088), ('gsm', 0.085), ('wavelet', 0.082), ('phase', 0.082), ('adaptable', 0.081), ('qu', 0.081), ('representations', 0.079), ('representation', 0.074), ('ui', 0.072), ('strikingly', 0.07), ('sinusoid', 0.07), ('karklin', 0.07), ('densities', 0.066), ('scale', 0.065), ('lewicki', 0.064), ('radially', 0.064), ('functions', 0.063), ('regularities', 0.063), ('adapted', 0.06), ('whitening', 0.06), ('gaussian', 0.057), ('joint', 0.057), ('au', 0.056), ('freq', 0.054), ('funcs', 0.054), ('resemble', 0.053), ('capture', 0.052), ('mixtures', 0.052), ('visual', 0.052), ('density', 0.051), ('multiplier', 0.049), ('scenes', 0.048), ('phases', 0.047), ('hierarchy', 0.047), ('distributions', 0.047), ('assumed', 0.046), ('adapting', 0.046), ('laplacian', 0.045), ('gure', 0.045), ('span', 0.044), ('early', 0.044), ('models', 0.043), ('vision', 0.043), ('yan', 0.043), ('peak', 0.042), ('whitened', 0.04), ('collectively', 0.04), ('exhibit', 0.038), ('overcomplete', 0.037), ('macaque', 0.037), ('structure', 0.037), ('cells', 0.036), ('properties', 0.036), ('priors', 0.036), ('marginal', 0.036), ('electronic', 0.036), ('filters', 0.036), ('scatter', 0.036), ('model', 0.035), ('frequency', 0.035), ('analyzed', 0.035), ('wainwright', 0.034), ('olshausen', 0.034), ('context', 0.033), ('neurophysiology', 0.033), ('emergence', 0.033), ('reverse', 0.032), ('fail', 0.031), ('reported', 0.031), ('statistics', 0.031), ('previously', 0.031), ('neural', 0.031), ('component', 0.03), ('statistical', 0.03), ('population', 0.03), ('observed', 0.03), ('physiological', 0.03), ('les', 0.03), ('independent', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="101-tfidf-1" href="./nips-2005-Is_Early_Vision_Optimized_for_Extracting_Higher-order_Dependencies%3F.html">101 nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: Linear implementations of the efﬁcient coding hypothesis, such as independent component analysis (ICA) and sparse coding models, have provided functional explanations for properties of simple cells in V1 [1, 2]. These models, however, ignore the non-linear behavior of neurons and fail to match individual and population properties of neural receptive ﬁelds in subtle but important ways. Hierarchical models, including Gaussian Scale Mixtures [3, 4] and other generative statistical models [5, 6], can capture higher-order regularities in natural images and explain nonlinear aspects of neural processing such as normalization and context effects [6,7]. Previously, it had been assumed that the lower level representation is independent of the hierarchy, and had been ﬁxed when training these models. Here we examine the optimal lower-level representations derived in the context of a hierarchical model and ﬁnd that the resulting representations are strikingly different from those based on linear models. Unlike the the basis functions and ﬁlters learned by ICA or sparse coding, these functions individually more closely resemble simple cell receptive ﬁelds and collectively span a broad range of spatial scales. Our work uniﬁes several related approaches and observations about natural image structure and suggests that hierarchical models might yield better representations of image structure throughout the hierarchy.</p><p>2 0.18004265 <a title="101-tfidf-2" href="./nips-2005-Products_of_%60%60Edge-perts.html">158 nips-2005-Products of ``Edge-perts</a></p>
<p>Author: Max Welling, Peter V. Gehler</p><p>Abstract: Images represent an important and abundant source of data. Understanding their statistical structure has important applications such as image compression and restoration. In this paper we propose a particular kind of probabilistic model, dubbed the “products of edge-perts model” to describe the structure of wavelet transformed images. We develop a practical denoising algorithm based on a single edge-pert and show state-ofthe-art denoising performance on benchmark images. 1</p><p>3 0.16980532 <a title="101-tfidf-3" href="./nips-2005-Learning_Cue-Invariant_Visual_Responses.html">109 nips-2005-Learning Cue-Invariant Visual Responses</a></p>
<p>Author: Jarmo Hurri</p><p>Abstract: Multiple visual cues are used by the visual system to analyze a scene; achromatic cues include luminance, texture, contrast and motion. Singlecell recordings have shown that the mammalian visual cortex contains neurons that respond similarly to scene structure (e.g., orientation of a boundary), regardless of the cue type conveying this information. This paper shows that cue-invariant response properties of simple- and complex-type cells can be learned from natural image data in an unsupervised manner. In order to do this, we also extend a previous conceptual model of cue invariance so that it can be applied to model simple- and complex-cell responses. Our results relate cue-invariant response properties to natural image statistics, thereby showing how the statistical modeling approach can be used to model processing beyond the elemental response properties visual neurons. This work also demonstrates how to learn, from natural image data, more sophisticated feature detectors than those based on changes in mean luminance, thereby paving the way for new data-driven approaches to image processing and computer vision. 1</p><p>4 0.14913349 <a title="101-tfidf-4" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>Author: Eizaburo Doi, Doru C. Balcan, Michael S. Lewicki</p><p>Abstract: Biological sensory systems are faced with the problem of encoding a high-ﬁdelity sensory signal with a population of noisy, low-ﬁdelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and twodimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of highdimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets. 1</p><p>5 0.11862484 <a title="101-tfidf-5" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<p>Author: Neil Bruce, John Tsotsos</p><p>Abstract: A model of bottom-up overt attention is proposed based on the principle of maximizing information sampled from a scene. The proposed operation is based on Shannon's self-information measure and is achieved in a neural circuit, which is demonstrated as having close ties with the circuitry existent in the primate visual cortex. It is further shown that the proposed saliency measure may be extended to address issues that currently elude explanation in the domain of saliency based models. Resu lts on natural images are compared with experimental eye tracking data revealing the efficacy of the model in predicting the deployment of overt attention as compared with existing efforts.</p><p>6 0.10692152 <a title="101-tfidf-6" href="./nips-2005-Gradient_Flow_Independent_Component_Analysis_in_Micropower_VLSI.html">88 nips-2005-Gradient Flow Independent Component Analysis in Micropower VLSI</a></p>
<p>7 0.097500212 <a title="101-tfidf-7" href="./nips-2005-Variational_EM_Algorithms_for_Non-Gaussian_Latent_Variable_Models.html">202 nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</a></p>
<p>8 0.095911607 <a title="101-tfidf-8" href="./nips-2005-Value_Function_Approximation_with_Diffusion_Wavelets_and_Laplacian_Eigenfunctions.html">199 nips-2005-Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions</a></p>
<p>9 0.095163085 <a title="101-tfidf-9" href="./nips-2005-Scaling_Laws_in_Natural_Scenes_and_the_Inference_of_3D_Shape.html">170 nips-2005-Scaling Laws in Natural Scenes and the Inference of 3D Shape</a></p>
<p>10 0.089585148 <a title="101-tfidf-10" href="./nips-2005-Recovery_of_Jointly_Sparse_Signals_from_Few_Random_Projections.html">163 nips-2005-Recovery of Jointly Sparse Signals from Few Random Projections</a></p>
<p>11 0.082845517 <a title="101-tfidf-11" href="./nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">113 nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<p>12 0.080745429 <a title="101-tfidf-12" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>13 0.0797318 <a title="101-tfidf-13" href="./nips-2005-Learning_Depth_from_Single_Monocular_Images.html">110 nips-2005-Learning Depth from Single Monocular Images</a></p>
<p>14 0.079535201 <a title="101-tfidf-14" href="./nips-2005-Neural_mechanisms_of_contrast_dependent_receptive_field_size_in_V1.html">134 nips-2005-Neural mechanisms of contrast dependent receptive field size in V1</a></p>
<p>15 0.076730572 <a title="101-tfidf-15" href="./nips-2005-Describing_Visual_Scenes_using_Transformed_Dirichlet_Processes.html">55 nips-2005-Describing Visual Scenes using Transformed Dirichlet Processes</a></p>
<p>16 0.072636113 <a title="101-tfidf-16" href="./nips-2005-Bayesian_model_learning_in_human_visual_perception.html">35 nips-2005-Bayesian model learning in human visual perception</a></p>
<p>17 0.072520725 <a title="101-tfidf-17" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>18 0.069651261 <a title="101-tfidf-18" href="./nips-2005-Sensory_Adaptation_within_a_Bayesian_Framework_for_Perception.html">173 nips-2005-Sensory Adaptation within a Bayesian Framework for Perception</a></p>
<p>19 0.068608776 <a title="101-tfidf-19" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<p>20 0.067459308 <a title="101-tfidf-20" href="./nips-2005-Visual_Encoding_with_Jittering_Eyes.html">203 nips-2005-Visual Encoding with Jittering Eyes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.227), (1, -0.035), (2, -0.026), (3, 0.199), (4, -0.054), (5, -0.037), (6, -0.12), (7, -0.146), (8, 0.102), (9, -0.108), (10, -0.011), (11, 0.021), (12, 0.071), (13, -0.057), (14, 0.008), (15, 0.002), (16, 0.124), (17, -0.048), (18, -0.027), (19, 0.057), (20, -0.225), (21, -0.004), (22, -0.064), (23, -0.085), (24, 0.175), (25, -0.147), (26, -0.027), (27, -0.126), (28, 0.163), (29, -0.1), (30, -0.193), (31, 0.129), (32, 0.031), (33, 0.089), (34, -0.033), (35, -0.06), (36, -0.076), (37, 0.011), (38, 0.014), (39, -0.089), (40, 0.02), (41, -0.025), (42, 0.009), (43, 0.058), (44, -0.068), (45, -0.018), (46, 0.005), (47, -0.024), (48, -0.073), (49, -0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96740657 <a title="101-lsi-1" href="./nips-2005-Is_Early_Vision_Optimized_for_Extracting_Higher-order_Dependencies%3F.html">101 nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: Linear implementations of the efﬁcient coding hypothesis, such as independent component analysis (ICA) and sparse coding models, have provided functional explanations for properties of simple cells in V1 [1, 2]. These models, however, ignore the non-linear behavior of neurons and fail to match individual and population properties of neural receptive ﬁelds in subtle but important ways. Hierarchical models, including Gaussian Scale Mixtures [3, 4] and other generative statistical models [5, 6], can capture higher-order regularities in natural images and explain nonlinear aspects of neural processing such as normalization and context effects [6,7]. Previously, it had been assumed that the lower level representation is independent of the hierarchy, and had been ﬁxed when training these models. Here we examine the optimal lower-level representations derived in the context of a hierarchical model and ﬁnd that the resulting representations are strikingly different from those based on linear models. Unlike the the basis functions and ﬁlters learned by ICA or sparse coding, these functions individually more closely resemble simple cell receptive ﬁelds and collectively span a broad range of spatial scales. Our work uniﬁes several related approaches and observations about natural image structure and suggests that hierarchical models might yield better representations of image structure throughout the hierarchy.</p><p>2 0.84711474 <a title="101-lsi-2" href="./nips-2005-Products_of_%60%60Edge-perts.html">158 nips-2005-Products of ``Edge-perts</a></p>
<p>Author: Max Welling, Peter V. Gehler</p><p>Abstract: Images represent an important and abundant source of data. Understanding their statistical structure has important applications such as image compression and restoration. In this paper we propose a particular kind of probabilistic model, dubbed the “products of edge-perts model” to describe the structure of wavelet transformed images. We develop a practical denoising algorithm based on a single edge-pert and show state-ofthe-art denoising performance on benchmark images. 1</p><p>3 0.56103998 <a title="101-lsi-3" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>Author: Eizaburo Doi, Doru C. Balcan, Michael S. Lewicki</p><p>Abstract: Biological sensory systems are faced with the problem of encoding a high-ﬁdelity sensory signal with a population of noisy, low-ﬁdelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and twodimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of highdimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets. 1</p><p>4 0.55441254 <a title="101-lsi-4" href="./nips-2005-Learning_Cue-Invariant_Visual_Responses.html">109 nips-2005-Learning Cue-Invariant Visual Responses</a></p>
<p>Author: Jarmo Hurri</p><p>Abstract: Multiple visual cues are used by the visual system to analyze a scene; achromatic cues include luminance, texture, contrast and motion. Singlecell recordings have shown that the mammalian visual cortex contains neurons that respond similarly to scene structure (e.g., orientation of a boundary), regardless of the cue type conveying this information. This paper shows that cue-invariant response properties of simple- and complex-type cells can be learned from natural image data in an unsupervised manner. In order to do this, we also extend a previous conceptual model of cue invariance so that it can be applied to model simple- and complex-cell responses. Our results relate cue-invariant response properties to natural image statistics, thereby showing how the statistical modeling approach can be used to model processing beyond the elemental response properties visual neurons. This work also demonstrates how to learn, from natural image data, more sophisticated feature detectors than those based on changes in mean luminance, thereby paving the way for new data-driven approaches to image processing and computer vision. 1</p><p>5 0.52930284 <a title="101-lsi-5" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<p>Author: Neil Bruce, John Tsotsos</p><p>Abstract: A model of bottom-up overt attention is proposed based on the principle of maximizing information sampled from a scene. The proposed operation is based on Shannon's self-information measure and is achieved in a neural circuit, which is demonstrated as having close ties with the circuitry existent in the primate visual cortex. It is further shown that the proposed saliency measure may be extended to address issues that currently elude explanation in the domain of saliency based models. Resu lts on natural images are compared with experimental eye tracking data revealing the efficacy of the model in predicting the deployment of overt attention as compared with existing efforts.</p><p>6 0.51672345 <a title="101-lsi-6" href="./nips-2005-Recovery_of_Jointly_Sparse_Signals_from_Few_Random_Projections.html">163 nips-2005-Recovery of Jointly Sparse Signals from Few Random Projections</a></p>
<p>7 0.49391142 <a title="101-lsi-7" href="./nips-2005-Scaling_Laws_in_Natural_Scenes_and_the_Inference_of_3D_Shape.html">170 nips-2005-Scaling Laws in Natural Scenes and the Inference of 3D Shape</a></p>
<p>8 0.48353049 <a title="101-lsi-8" href="./nips-2005-Visual_Encoding_with_Jittering_Eyes.html">203 nips-2005-Visual Encoding with Jittering Eyes</a></p>
<p>9 0.47094828 <a title="101-lsi-9" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>10 0.44303384 <a title="101-lsi-10" href="./nips-2005-Separation_of_Music_Signals_by_Harmonic_Structure_Modeling.html">174 nips-2005-Separation of Music Signals by Harmonic Structure Modeling</a></p>
<p>11 0.41982594 <a title="101-lsi-11" href="./nips-2005-Bayesian_model_learning_in_human_visual_perception.html">35 nips-2005-Bayesian model learning in human visual perception</a></p>
<p>12 0.39278421 <a title="101-lsi-12" href="./nips-2005-Value_Function_Approximation_with_Diffusion_Wavelets_and_Laplacian_Eigenfunctions.html">199 nips-2005-Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions</a></p>
<p>13 0.37666163 <a title="101-lsi-13" href="./nips-2005-Large_scale_networks_fingerprinting_and_visualization_using_the_k-core_decomposition.html">107 nips-2005-Large scale networks fingerprinting and visualization using the k-core decomposition</a></p>
<p>14 0.36770591 <a title="101-lsi-14" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>15 0.35405228 <a title="101-lsi-15" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<p>16 0.34176731 <a title="101-lsi-16" href="./nips-2005-Describing_Visual_Scenes_using_Transformed_Dirichlet_Processes.html">55 nips-2005-Describing Visual Scenes using Transformed Dirichlet Processes</a></p>
<p>17 0.34084922 <a title="101-lsi-17" href="./nips-2005-Non-iterative_Estimation_with_Perturbed_Gaussian_Markov_Processes.html">139 nips-2005-Non-iterative Estimation with Perturbed Gaussian Markov Processes</a></p>
<p>18 0.33468652 <a title="101-lsi-18" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>19 0.3304894 <a title="101-lsi-19" href="./nips-2005-Gradient_Flow_Independent_Component_Analysis_in_Micropower_VLSI.html">88 nips-2005-Gradient Flow Independent Component Analysis in Micropower VLSI</a></p>
<p>20 0.32587302 <a title="101-lsi-20" href="./nips-2005-A_matching_pursuit_approach_to_sparse_Gaussian_process_regression.html">16 nips-2005-A matching pursuit approach to sparse Gaussian process regression</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.034), (10, 0.021), (27, 0.479), (31, 0.043), (34, 0.047), (39, 0.01), (55, 0.023), (57, 0.012), (60, 0.012), (69, 0.054), (73, 0.021), (77, 0.011), (78, 0.013), (88, 0.081), (91, 0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93922776 <a title="101-lda-1" href="./nips-2005-Is_Early_Vision_Optimized_for_Extracting_Higher-order_Dependencies%3F.html">101 nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: Linear implementations of the efﬁcient coding hypothesis, such as independent component analysis (ICA) and sparse coding models, have provided functional explanations for properties of simple cells in V1 [1, 2]. These models, however, ignore the non-linear behavior of neurons and fail to match individual and population properties of neural receptive ﬁelds in subtle but important ways. Hierarchical models, including Gaussian Scale Mixtures [3, 4] and other generative statistical models [5, 6], can capture higher-order regularities in natural images and explain nonlinear aspects of neural processing such as normalization and context effects [6,7]. Previously, it had been assumed that the lower level representation is independent of the hierarchy, and had been ﬁxed when training these models. Here we examine the optimal lower-level representations derived in the context of a hierarchical model and ﬁnd that the resulting representations are strikingly different from those based on linear models. Unlike the the basis functions and ﬁlters learned by ICA or sparse coding, these functions individually more closely resemble simple cell receptive ﬁelds and collectively span a broad range of spatial scales. Our work uniﬁes several related approaches and observations about natural image structure and suggests that hierarchical models might yield better representations of image structure throughout the hierarchy.</p><p>2 0.93238795 <a title="101-lda-2" href="./nips-2005-Goal-Based_Imitation_as_Probabilistic_Inference_over_Graphical_Models.html">87 nips-2005-Goal-Based Imitation as Probabilistic Inference over Graphical Models</a></p>
<p>Author: Deepak Verma, Rajesh P. Rao</p><p>Abstract: Humans are extremely adept at learning new skills by imitating the actions of others. A progression of imitative abilities has been observed in children, ranging from imitation of simple body movements to goalbased imitation based on inferring intent. In this paper, we show that the problem of goal-based imitation can be formulated as one of inferring goals and selecting actions using a learned probabilistic graphical model of the environment. We ﬁrst describe algorithms for planning actions to achieve a goal state using probabilistic inference. We then describe how planning can be used to bootstrap the learning of goal-dependent policies by utilizing feedback from the environment. The resulting graphical model is then shown to be powerful enough to allow goal-based imitation. Using a simple maze navigation task, we illustrate how an agent can infer the goals of an observed teacher and imitate the teacher even when the goals are uncertain and the demonstration is incomplete.</p><p>3 0.92682654 <a title="101-lda-3" href="./nips-2005-Subsequence_Kernels_for_Relation_Extraction.html">185 nips-2005-Subsequence Kernels for Relation Extraction</a></p>
<p>Author: Raymond J. Mooney, Razvan C. Bunescu</p><p>Abstract: We present a new kernel method for extracting semantic relations between entities in natural language text, based on a generalization of subsequence kernels. This kernel uses three types of subsequence patterns that are typically employed in natural language to assert relationships between two entities. Experiments on extracting protein interactions from biomedical corpora and top-level relations from newspaper corpora demonstrate the advantages of this approach. 1</p><p>4 0.80413449 <a title="101-lda-4" href="./nips-2005-Predicting_EMG_Data_from_M1_Neurons_with_Variational_Bayesian_Least_Squares.html">155 nips-2005-Predicting EMG Data from M1 Neurons with Variational Bayesian Least Squares</a></p>
<p>Author: Jo-anne Ting, Aaron D'souza, Kenji Yamamoto, Toshinori Yoshioka, Donna Hoffman, Shinji Kakei, Lauren Sergio, John Kalaska, Mitsuo Kawato</p><p>Abstract: An increasing number of projects in neuroscience requires the statistical analysis of high dimensional data sets, as, for instance, in predicting behavior from neural ﬁring or in operating artiﬁcial devices from brain recordings in brain-machine interfaces. Linear analysis techniques remain prevalent in such cases, but classical linear regression approaches are often numerically too fragile in high dimensions. In this paper, we address the question of whether EMG data collected from arm movements of monkeys can be faithfully reconstructed with linear approaches from neural activity in primary motor cortex (M1). To achieve robust data analysis, we develop a full Bayesian approach to linear regression that automatically detects and excludes irrelevant features in the data, regularizing against overﬁtting. In comparison with ordinary least squares, stepwise regression, partial least squares, LASSO regression and a brute force combinatorial search for the most predictive input features in the data, we demonstrate that the new Bayesian method oﬀers a superior mixture of characteristics in terms of regularization against overﬁtting, computational eﬃciency and ease of use, demonstrating its potential as a drop-in replacement for other linear regression techniques. As neuroscientiﬁc results, our analyses demonstrate that EMG data can be well predicted from M1 neurons, further opening the path for possible real-time interfaces between brains and machines. 1</p><p>5 0.60325849 <a title="101-lda-5" href="./nips-2005-Learning_Cue-Invariant_Visual_Responses.html">109 nips-2005-Learning Cue-Invariant Visual Responses</a></p>
<p>Author: Jarmo Hurri</p><p>Abstract: Multiple visual cues are used by the visual system to analyze a scene; achromatic cues include luminance, texture, contrast and motion. Singlecell recordings have shown that the mammalian visual cortex contains neurons that respond similarly to scene structure (e.g., orientation of a boundary), regardless of the cue type conveying this information. This paper shows that cue-invariant response properties of simple- and complex-type cells can be learned from natural image data in an unsupervised manner. In order to do this, we also extend a previous conceptual model of cue invariance so that it can be applied to model simple- and complex-cell responses. Our results relate cue-invariant response properties to natural image statistics, thereby showing how the statistical modeling approach can be used to model processing beyond the elemental response properties visual neurons. This work also demonstrates how to learn, from natural image data, more sophisticated feature detectors than those based on changes in mean luminance, thereby paving the way for new data-driven approaches to image processing and computer vision. 1</p><p>6 0.55534476 <a title="101-lda-6" href="./nips-2005-Bayesian_models_of_human_action_understanding.html">36 nips-2005-Bayesian models of human action understanding</a></p>
<p>7 0.54628032 <a title="101-lda-7" href="./nips-2005-Interpolating_between_types_and_tokens_by_estimating_power-law_generators.html">100 nips-2005-Interpolating between types and tokens by estimating power-law generators</a></p>
<p>8 0.52610219 <a title="101-lda-8" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>9 0.50928015 <a title="101-lda-9" href="./nips-2005-Learning_to_Control_an_Octopus_Arm_with_Gaussian_Process_Temporal_Difference_Methods.html">119 nips-2005-Learning to Control an Octopus Arm with Gaussian Process Temporal Difference Methods</a></p>
<p>10 0.50560981 <a title="101-lda-10" href="./nips-2005-Visual_Encoding_with_Jittering_Eyes.html">203 nips-2005-Visual Encoding with Jittering Eyes</a></p>
<p>11 0.49086747 <a title="101-lda-11" href="./nips-2005-Products_of_%60%60Edge-perts.html">158 nips-2005-Products of ``Edge-perts</a></p>
<p>12 0.48326069 <a title="101-lda-12" href="./nips-2005-Context_as_Filtering.html">48 nips-2005-Context as Filtering</a></p>
<p>13 0.47705758 <a title="101-lda-13" href="./nips-2005-Policy-Gradient_Methods_for_Planning.html">153 nips-2005-Policy-Gradient Methods for Planning</a></p>
<p>14 0.46807057 <a title="101-lda-14" href="./nips-2005-Scaling_Laws_in_Natural_Scenes_and_the_Inference_of_3D_Shape.html">170 nips-2005-Scaling Laws in Natural Scenes and the Inference of 3D Shape</a></p>
<p>15 0.4659546 <a title="101-lda-15" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<p>16 0.46424857 <a title="101-lda-16" href="./nips-2005-Sensory_Adaptation_within_a_Bayesian_Framework_for_Perception.html">173 nips-2005-Sensory Adaptation within a Bayesian Framework for Perception</a></p>
<p>17 0.45973143 <a title="101-lda-17" href="./nips-2005-Phase_Synchrony_Rate_for_the_Recognition_of_Motor_Imagery_in_Brain-Computer_Interface.html">152 nips-2005-Phase Synchrony Rate for the Recognition of Motor Imagery in Brain-Computer Interface</a></p>
<p>18 0.44916457 <a title="101-lda-18" href="./nips-2005-Value_Function_Approximation_with_Diffusion_Wavelets_and_Laplacian_Eigenfunctions.html">199 nips-2005-Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions</a></p>
<p>19 0.44657704 <a title="101-lda-19" href="./nips-2005-Bayesian_model_learning_in_human_visual_perception.html">35 nips-2005-Bayesian model learning in human visual perception</a></p>
<p>20 0.4411895 <a title="101-lda-20" href="./nips-2005-Correlated_Topic_Models.html">52 nips-2005-Correlated Topic Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
