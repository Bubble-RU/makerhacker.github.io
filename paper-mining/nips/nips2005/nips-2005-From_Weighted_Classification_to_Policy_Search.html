<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>78 nips-2005-From Weighted Classification to Policy Search</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-78" href="#">nips2005-78</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>78 nips-2005-From Weighted Classification to Policy Search</h1>
<br/><p>Source: <a title="nips-2005-78-pdf" href="http://papers.nips.cc/paper/2778-from-weighted-classification-to-policy-search.pdf">pdf</a></p><p>Author: Doron Blatt, Alfred O. Hero</p><p>Abstract: This paper proposes an algorithm to convert a T -stage stochastic decision problem with a continuous state space to a sequence of supervised learning problems. The optimization problem associated with the trajectory tree and random trajectory methods of Kearns, Mansour, and Ng, 2000, is solved using the Gauss-Seidel method. The algorithm breaks a multistage reinforcement learning problem into a sequence of single-stage reinforcement learning subproblems, each of which is solved via an exact reduction to a weighted-classiﬁcation problem that can be solved using off-the-self methods. Thus the algorithm converts a reinforcement learning problem into simpler supervised learning subproblems. It is shown that the method converges in a ﬁnite number of steps to a solution that cannot be further improved by componentwise optimization. The implication of the proposed algorithm is that a plethora of classiﬁcation methods can be applied to ﬁnd policies in the reinforcement learning problem. 1</p><p>Reference: <a title="nips-2005-78-reference" href="../nips2005_reference/nips-2005-From_Weighted_Classification_to_Policy_Search_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('policy', 0.712), ('si', 0.296), ('reward', 0.286), ('vn', 0.258), ('reinforc', 0.224), ('traject', 0.123), ('arg', 0.113), ('maxk', 0.11), ('en', 0.092), ('max', 0.091), ('tre', 0.084), ('mdp', 0.079), ('root', 0.073), ('cat', 0.072), ('stag', 0.07), ('st', 0.07), ('convert', 0.07), ('class', 0.067), ('weight', 0.065), ('prun', 0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="78-tfidf-1" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>2 0.44250664 <a title="78-tfidf-2" href="./nips-2005-On_Local_Rewards_and_Scaling_Distributed_Reinforcement_Learning.html">145 nips-2005-On Local Rewards and Scaling Distributed Reinforcement Learning</a></p>
<p>3 0.42245355 <a title="78-tfidf-3" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>4 0.40689477 <a title="78-tfidf-4" href="./nips-2005-Policy-Gradient_Methods_for_Planning.html">153 nips-2005-Policy-Gradient Methods for Planning</a></p>
<p>5 0.34264374 <a title="78-tfidf-5" href="./nips-2005-TD%280%29_Leads_to_Better_Policies_than_Approximate_Value_Iteration.html">186 nips-2005-TD(0) Leads to Better Policies than Approximate Value Iteration</a></p>
<p>6 0.265194 <a title="78-tfidf-6" href="./nips-2005-Goal-Based_Imitation_as_Probabilistic_Inference_over_Graphical_Models.html">87 nips-2005-Goal-Based Imitation as Probabilistic Inference over Graphical Models</a></p>
<p>7 0.18897873 <a title="78-tfidf-7" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>8 0.18620332 <a title="78-tfidf-8" href="./nips-2005-Cyclic_Equilibria_in_Markov_Games.html">53 nips-2005-Cyclic Equilibria in Markov Games</a></p>
<p>9 0.18460183 <a title="78-tfidf-9" href="./nips-2005-Value_Function_Approximation_with_Diffusion_Wavelets_and_Laplacian_Eigenfunctions.html">199 nips-2005-Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions</a></p>
<p>10 0.17429902 <a title="78-tfidf-10" href="./nips-2005-How_fast_to_work%3A_Response_vigor%2C_motivation_and_tonic_dopamine.html">91 nips-2005-How fast to work: Response vigor, motivation and tonic dopamine</a></p>
<p>11 0.12415569 <a title="78-tfidf-11" href="./nips-2005-Learning_to_Control_an_Octopus_Arm_with_Gaussian_Process_Temporal_Difference_Methods.html">119 nips-2005-Learning to Control an Octopus Arm with Gaussian Process Temporal Difference Methods</a></p>
<p>12 0.11487857 <a title="78-tfidf-12" href="./nips-2005-Bayesian_models_of_human_action_understanding.html">36 nips-2005-Bayesian models of human action understanding</a></p>
<p>13 0.113198 <a title="78-tfidf-13" href="./nips-2005-An_exploration-exploitation_model_based_on_norepinepherine_and_dopamine_activity.html">26 nips-2005-An exploration-exploitation model based on norepinepherine and dopamine activity</a></p>
<p>14 0.1062487 <a title="78-tfidf-14" href="./nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">151 nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<p>15 0.09616185 <a title="78-tfidf-15" href="./nips-2005-Temporal_Abstraction_in_Temporal-difference_Networks.html">187 nips-2005-Temporal Abstraction in Temporal-difference Networks</a></p>
<p>16 0.06944681 <a title="78-tfidf-16" href="./nips-2005-Fast_biped_walking_with_a_reflexive_controller_and_real-time_policy_searching.html">73 nips-2005-Fast biped walking with a reflexive controller and real-time policy searching</a></p>
<p>17 0.067043521 <a title="78-tfidf-17" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>18 0.065670498 <a title="78-tfidf-18" href="./nips-2005-Active_Learning_For_Identifying_Function_Threshold_Boundaries.html">18 nips-2005-Active Learning For Identifying Function Threshold Boundaries</a></p>
<p>19 0.063501701 <a title="78-tfidf-19" href="./nips-2005-Noise_and_the_two-thirds_power_Law.html">136 nips-2005-Noise and the two-thirds power Law</a></p>
<p>20 0.060287014 <a title="78-tfidf-20" href="./nips-2005-Kernels_for_gene_regulatory_regions.html">103 nips-2005-Kernels for gene regulatory regions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.214), (1, -0.074), (2, 0.601), (3, 0.255), (4, 0.109), (5, -0.106), (6, 0.039), (7, 0.075), (8, -0.034), (9, 0.004), (10, -0.099), (11, 0.096), (12, -0.016), (13, 0.09), (14, 0.047), (15, -0.01), (16, -0.032), (17, -0.036), (18, 0.118), (19, -0.105), (20, -0.136), (21, 0.01), (22, 0.082), (23, -0.037), (24, -0.072), (25, -0.117), (26, 0.012), (27, -0.049), (28, -0.054), (29, -0.028), (30, -0.032), (31, 0.021), (32, -0.033), (33, 0.045), (34, 0.015), (35, -0.049), (36, 0.017), (37, 0.044), (38, -0.023), (39, -0.105), (40, -0.019), (41, -0.055), (42, -0.022), (43, -0.036), (44, 0.02), (45, -0.01), (46, 0.006), (47, 0.021), (48, -0.065), (49, -0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93971109 <a title="78-lsi-1" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>2 0.80462229 <a title="78-lsi-2" href="./nips-2005-On_Local_Rewards_and_Scaling_Distributed_Reinforcement_Learning.html">145 nips-2005-On Local Rewards and Scaling Distributed Reinforcement Learning</a></p>
<p>3 0.78612179 <a title="78-lsi-3" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>4 0.74130136 <a title="78-lsi-4" href="./nips-2005-Policy-Gradient_Methods_for_Planning.html">153 nips-2005-Policy-Gradient Methods for Planning</a></p>
<p>5 0.7159692 <a title="78-lsi-5" href="./nips-2005-TD%280%29_Leads_to_Better_Policies_than_Approximate_Value_Iteration.html">186 nips-2005-TD(0) Leads to Better Policies than Approximate Value Iteration</a></p>
<p>6 0.66194987 <a title="78-lsi-6" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>7 0.56982219 <a title="78-lsi-7" href="./nips-2005-Learning_to_Control_an_Octopus_Arm_with_Gaussian_Process_Temporal_Difference_Methods.html">119 nips-2005-Learning to Control an Octopus Arm with Gaussian Process Temporal Difference Methods</a></p>
<p>8 0.5425843 <a title="78-lsi-8" href="./nips-2005-Goal-Based_Imitation_as_Probabilistic_Inference_over_Graphical_Models.html">87 nips-2005-Goal-Based Imitation as Probabilistic Inference over Graphical Models</a></p>
<p>9 0.4806149 <a title="78-lsi-9" href="./nips-2005-How_fast_to_work%3A_Response_vigor%2C_motivation_and_tonic_dopamine.html">91 nips-2005-How fast to work: Response vigor, motivation and tonic dopamine</a></p>
<p>10 0.43649825 <a title="78-lsi-10" href="./nips-2005-Cyclic_Equilibria_in_Markov_Games.html">53 nips-2005-Cyclic Equilibria in Markov Games</a></p>
<p>11 0.4065603 <a title="78-lsi-11" href="./nips-2005-Value_Function_Approximation_with_Diffusion_Wavelets_and_Laplacian_Eigenfunctions.html">199 nips-2005-Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions</a></p>
<p>12 0.3372367 <a title="78-lsi-12" href="./nips-2005-Temporal_Abstraction_in_Temporal-difference_Networks.html">187 nips-2005-Temporal Abstraction in Temporal-difference Networks</a></p>
<p>13 0.32169574 <a title="78-lsi-13" href="./nips-2005-An_exploration-exploitation_model_based_on_norepinepherine_and_dopamine_activity.html">26 nips-2005-An exploration-exploitation model based on norepinepherine and dopamine activity</a></p>
<p>14 0.27926135 <a title="78-lsi-14" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>15 0.2637116 <a title="78-lsi-15" href="./nips-2005-Active_Learning_For_Identifying_Function_Threshold_Boundaries.html">18 nips-2005-Active Learning For Identifying Function Threshold Boundaries</a></p>
<p>16 0.24204351 <a title="78-lsi-16" href="./nips-2005-Bayesian_models_of_human_action_understanding.html">36 nips-2005-Bayesian models of human action understanding</a></p>
<p>17 0.23082647 <a title="78-lsi-17" href="./nips-2005-Efficient_Estimation_of_OOMs.html">62 nips-2005-Efficient Estimation of OOMs</a></p>
<p>18 0.22911415 <a title="78-lsi-18" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>19 0.22652216 <a title="78-lsi-19" href="./nips-2005-Learning_Minimum_Volume_Sets.html">112 nips-2005-Learning Minimum Volume Sets</a></p>
<p>20 0.2246089 <a title="78-lsi-20" href="./nips-2005-Modeling_Neuronal_Interactivity_using_Dynamic_Bayesian_Networks.html">130 nips-2005-Modeling Neuronal Interactivity using Dynamic Bayesian Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.072), (7, 0.164), (12, 0.169), (17, 0.021), (54, 0.052), (71, 0.103), (88, 0.076), (89, 0.06), (92, 0.161)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8275665 <a title="78-lda-1" href="./nips-2005-Temporal_Abstraction_in_Temporal-difference_Networks.html">187 nips-2005-Temporal Abstraction in Temporal-difference Networks</a></p>
<p>2 0.82747507 <a title="78-lda-2" href="./nips-2005-Policy-Gradient_Methods_for_Planning.html">153 nips-2005-Policy-Gradient Methods for Planning</a></p>
<p>same-paper 3 0.79807293 <a title="78-lda-3" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>4 0.79020959 <a title="78-lda-4" href="./nips-2005-Dual-Tree_Fast_Gauss_Transforms.html">59 nips-2005-Dual-Tree Fast Gauss Transforms</a></p>
<p>5 0.77620041 <a title="78-lda-5" href="./nips-2005-Learning_Topology_with_the_Generative_Gaussian_Graph_and_the_EM_Algorithm.html">116 nips-2005-Learning Topology with the Generative Gaussian Graph and the EM Algorithm</a></p>
<p>6 0.77020609 <a title="78-lda-6" href="./nips-2005-Silicon_growth_cones_map_silicon_retina.html">176 nips-2005-Silicon growth cones map silicon retina</a></p>
<p>7 0.76634669 <a title="78-lda-7" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>8 0.76090622 <a title="78-lda-8" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>9 0.75119305 <a title="78-lda-9" href="./nips-2005-Efficient_Unsupervised_Learning_for_Localization_and_Detection_in_Object_Categories.html">63 nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</a></p>
<p>10 0.74387676 <a title="78-lda-10" href="./nips-2005-On_Local_Rewards_and_Scaling_Distributed_Reinforcement_Learning.html">145 nips-2005-On Local Rewards and Scaling Distributed Reinforcement Learning</a></p>
<p>11 0.72853798 <a title="78-lda-11" href="./nips-2005-Bayesian_models_of_human_action_understanding.html">36 nips-2005-Bayesian models of human action understanding</a></p>
<p>12 0.72459567 <a title="78-lda-12" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>13 0.69663119 <a title="78-lda-13" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>14 0.69549793 <a title="78-lda-14" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>15 0.69445002 <a title="78-lda-15" href="./nips-2005-Online_Discovery_and_Learning_of_Predictive_State_Representations.html">148 nips-2005-Online Discovery and Learning of Predictive State Representations</a></p>
<p>16 0.69340348 <a title="78-lda-16" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>17 0.69001383 <a title="78-lda-17" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>18 0.68493551 <a title="78-lda-18" href="./nips-2005-How_fast_to_work%3A_Response_vigor%2C_motivation_and_tonic_dopamine.html">91 nips-2005-How fast to work: Response vigor, motivation and tonic dopamine</a></p>
<p>19 0.68449581 <a title="78-lda-19" href="./nips-2005-Fast_Krylov_Methods_for_N-Body_Learning.html">71 nips-2005-Fast Krylov Methods for N-Body Learning</a></p>
<p>20 0.68390876 <a title="78-lda-20" href="./nips-2005-Logic_and_MRF_Circuitry_for_Labeling_Occluding_and_Thinline_Visual_Contours.html">122 nips-2005-Logic and MRF Circuitry for Labeling Occluding and Thinline Visual Contours</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
