<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-11" href="#">nips2005-11</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</h1>
<br/><p>Source: <a title="nips-2005-11-pdf" href="http://papers.nips.cc/paper/2758-a-hierarchical-compositional-system-for-rapid-object-detection.pdf">pdf</a></p><p>Author: Long Zhu, Alan L. Yuille</p><p>Abstract: We describe a hierarchical compositional system for detecting deformable objects in images. Objects are represented by graphical models. The algorithm uses a hierarchical tree where the root of the tree corresponds to the full object and lower-level elements of the tree correspond to simpler features. The algorithm proceeds by passing simple messages up and down the tree. The method works rapidly, in under a second, on 320 × 240 images. We demonstrate the approach on detecting cats, horses, and hands. The method works in the presence of background clutter and occlusions. Our approach is contrasted with more traditional methods such as dynamic programming and belief propagation. 1</p><p>Reference: <a title="nips-2005-11-reference" href="../nips2005_reference/nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We describe a hierarchical compositional system for detecting deformable objects in images. [sent-3, score-0.888]
</p><p>2 The algorithm uses a hierarchical tree where the root of the tree corresponds to the full object and lower-level elements of the tree correspond to simpler features. [sent-5, score-0.415]
</p><p>3 The algorithm proceeds by passing simple messages up and down the tree. [sent-6, score-0.104]
</p><p>4 We demonstrate the approach on detecting cats, horses, and hands. [sent-8, score-0.146]
</p><p>5 The method works in the presence of background clutter and occlusions. [sent-9, score-0.123]
</p><p>6 Our approach is contrasted with more traditional methods such as dynamic programming and belief propagation. [sent-10, score-0.118]
</p><p>7 1  Introduction  Detecting objects rapidly in images is very important. [sent-11, score-0.274]
</p><p>8 There has recently been great progress in detecting objects with limited appearance variability, such as faces and text [1,2,3]. [sent-12, score-0.379]
</p><p>9 The use of the SIFT operator also enables rapid detection of rigid objects [4]. [sent-13, score-0.332]
</p><p>10 The detection of such objects can be performed in under a second even in very large images which makes real time applications practical, see [3]. [sent-14, score-0.326]
</p><p>11 There has been less progress for the rapid detection of deformable objects, such as hands, horses, and cats. [sent-15, score-0.386]
</p><p>12 Such objects can be represented compactly by graphical models, see [5,6,7,8], but their variations in shape and appearance makes searching for them considerably harder. [sent-16, score-0.343]
</p><p>13 Recent work has included the use of dynamic programming [5,6] and belief propagation [7,8] to perform inference on these graphical models by searching over different spatial conﬁgurations. [sent-17, score-0.283]
</p><p>14 These algorithms are successful at detecting objects but pruning was required to obtain reasonable convergence rates [5,7,8]. [sent-18, score-0.33]
</p><p>15 Even so, algorithms can take minutes to converge on images of size 320 × 240. [sent-19, score-0.08]
</p><p>16 In this paper, we propose an alternative methods for performing inference on graphical models of deformable objects. [sent-20, score-0.354]
</p><p>17 Our approach is based on representing objects in a probabilistic compositional hierarchical tree structure. [sent-21, score-0.562]
</p><p>18 This structure enables rapid detection of objects by passing messages up and down the tree structure. [sent-22, score-0.503]
</p><p>19 6 seconds on a 320 × 240 image (without optimized code). [sent-24, score-0.097]
</p><p>20 Our approach can be applied to detect any object that can be represented by a graphical model. [sent-25, score-0.18]
</p><p>21 This includes the models mentioned above [5,6,7,8], compositional models [9], constellation models [10], models using chamfer matching [11] and models using deformable blur ﬁlters [12]. [sent-26, score-0.632]
</p><p>22 2  Background  Graphical models give an attractive framework for modeling object detection problems in computer vision. [sent-27, score-0.196]
</p><p>23 The positions of feature points on the object are represented by {xi : i ∈ Λ}. [sent-29, score-0.152]
</p><p>24 We augment this representation to include attributes of the points and obtain a representation {qi : i ∈ Λ}. [sent-30, score-0.03]
</p><p>25 These attributes can be used to model the appearance of the features in the image. [sent-31, score-0.113]
</p><p>26 For example, a feature point can be associated with an oriented intensity edge and qi can represent the orientation [8]. [sent-32, score-0.448]
</p><p>27 Alternatively, the attribute could represent the output of a blurred edge ﬁlter [12], or the appearance properties of a constellation model part [10]. [sent-33, score-0.171]
</p><p>28 There is a prior probability distribution on the conﬁguration of the model P ({qi }) and a likelihood function for generating the image data P (D|{qi }). [sent-34, score-0.046]
</p><p>29 Our priors are similar to [5,8,12], being based on deformations away from a prototype template. [sent-36, score-0.216]
</p><p>30 As described in [8], this corresponds to a maximizing a posterior of form: 1 ψi (qi ) ψij (qi , qj ), (1) P ({qi }|D) = Z i i,j where {ψi (qi )} and {ψij (qi , qj )} are the unary and pairwise potentials of the graph. [sent-38, score-0.183]
</p><p>31 The unary potentials model how well the individual features match to positions in the image. [sent-39, score-0.166]
</p><p>32 The binary potentials impose (probabilistic) constraints about the spatial relationships between feature points. [sent-40, score-0.038]
</p><p>33 Algorithms such as dynamic programming [5,6] and belief propagation [7,8] have been used to search for optima of P ({qi }|D). [sent-41, score-0.149]
</p><p>34 But the algorithms are time consuming because each state variable qi can take a large number of values (each feature point on the template can, in principle, match any point in the 240 × 320 image). [sent-42, score-0.508]
</p><p>35 But performance remains at speeds of seconds to minutes. [sent-44, score-0.051]
</p><p>36 3  The Hierarchical Compositional System  We deﬁne a compositional hierarchy by breaking down the representation {qi : i ∈ Λ} into substructures which have their own probability models. [sent-45, score-0.298]
</p><p>37 1 At the ﬁrst level, we group elements into K1 subsets {qi : i ∈ Sa } where Λ = K1 1 1 1 ∪a=1 Sa , Sa ∩ Sb = ∅, a = b. [sent-46, score-0.07]
</p><p>38 These subsets correspond to meaningful parts of the object, such as ears and other features. [sent-47, score-0.325]
</p><p>39 Speciﬁc examples for cats and horses will be given later. [sent-49, score-0.181]
</p><p>40 1 For each of these subsets we deﬁne a generative model Pa (D|{qi : i ∈ Sa }) and a prior 1 Pa ({qi : i ∈ Sa }). [sent-50, score-0.07]
</p><p>41 These generative and prior models are inherited from the full model, 1 1 see equation (1), by simply cutting the connections between the subset Sa and the Λ/Sa (the remaining features on the object). [sent-51, score-0.07]
</p><p>42 This is decomposed into subsets S1 , S2 , S3 corresponding to sub-features. [sent-54, score-0.098]
</p><p>43 These, in turn, can be decomposed into subsets corresponding to more elementary features. [sent-55, score-0.098]
</p><p>44 1 Pa1 ({qi : i ∈ Sa })  =  1 ˆ Za1  ψij (qi , qj ). [sent-56, score-0.048]
</p><p>45 , K1 } are composed to form a smaller selection of subsets {Sb : b = 1, . [sent-61, score-0.07]
</p><p>46 by 2 cutting them off the links to the remaining nodes Λ/Sb . [sent-67, score-0.067]
</p><p>47 This 1 gives a (possibly large) set of positions for the {qi : i ∈ Sa }. [sent-71, score-0.049]
</p><p>48 We apply non-maximum suppression to reduce many similar conﬁgurations in same local area to the one with maximum evidence (measured locally). [sent-72, score-0.074]
</p><p>49 Typically, non-maximum suppression keeps around 30 ∼ 500 candidate conﬁgurations for each node. [sent-74, score-0.045]
</p><p>50 These remaining conﬁgurations can be considered as proposals [13] and are passed up the tree to the sub2 1 2 set Sb which contains Sa . [sent-75, score-0.229]
</p><p>51 Node Sb evaluates the proposals to determine which ones are consistent, thus detecting composites of the subfeatures. [sent-76, score-0.308]
</p><p>52 2 There is also top-down message passing which occurs when one part of a node Sb contains µ µ 1 1 high evidence – e. [sent-77, score-0.081]
</p><p>53 In this case, we allow the matching to proceed if the combined matching strength is above threshold T1 . [sent-80, score-0.114]
</p><p>54 This mechanism enables the high-level models and, in particular, the priors for the relative positions of the sub-nodes to overcome weak local evidence. [sent-81, score-0.089]
</p><p>55 This performs a similar function to Coughlan and Shen’s dynamic quantization scheme [8]. [sent-82, score-0.068]
</p><p>56 For example, we could use the proposals to activate a data driven Monte Carlo Markov Chain (DDMCMC) algorithm [13]. [sent-84, score-0.162]
</p><p>57 To our knowledge, the use of hierarchical proposals of this type is unknown in the Monte Carlo sampling literature. [sent-85, score-0.273]
</p><p>58 4  Experimental Results  We illustrate our hierarchical compositional system on examples of cats, horses, and hands. [sent-86, score-0.368]
</p><p>59 The images include background clutter and the objects can be partially occluded. [sent-87, score-0.356]
</p><p>60 Figure 2: The prototype cat (top left panel), edges after grouping (top right panel), prototype template for ears and top of head (bottom left panel), and prototype for ears and eyes (bottom right panel). [sent-88, score-1.587]
</p><p>61 15 points are used for the ears and 24 for the head. [sent-89, score-0.227]
</p><p>62 First we preprocess the image using a Canny edge detector followed by simple edge grouping which eliminates isolated edges. [sent-90, score-0.309]
</p><p>63 Edge detection and edge grouping is illustrated in the top panels of ﬁgure (2). [sent-91, score-0.379]
</p><p>64 This ﬁgure is used to construct a prototype template for the ears, eyes, and head – see bottom panels of ﬁgure (2). [sent-92, score-0.487]
</p><p>65 We construct a graphical model for the cat as described in section (2). [sent-93, score-0.26]
</p><p>66 Then we deﬁne a hierarchical structure, see ﬁgure (3). [sent-94, score-0.111]
</p><p>67 Next we illustrate the results on several cat images, see ﬁgure (4). [sent-96, score-0.209]
</p><p>68 Several of these images were used in [8] and we thank Coughlan and Shen for supplying them. [sent-97, score-0.129]
</p><p>69 In all examples, our  algorithm detects the cat correctly despite the deformations of the cat from the prototype, see ﬁgure (2). [sent-98, score-0.46]
</p><p>70 The images are 320 × 240 and the preprocessing time is included. [sent-101, score-0.08]
</p><p>71 The algorithm is efﬁcient since the subfeatures give bottom-up proposals which constraint the positions of the full model. [sent-102, score-0.211]
</p><p>72 For example, ﬁgure (5) shows the proposals for ears for the cluttered cat image (center panel of ﬁgure (4). [sent-103, score-0.84]
</p><p>73 We next illustrate our approach on the tasks of detecting horses. [sent-107, score-0.172]
</p><p>74 The algorithm succeeds in detecting the horse, see right panels of ﬁgure (7), using the prototype template shown in the left panel of ﬁgure (7). [sent-109, score-0.793]
</p><p>75 Finally, we illustrate this approach for the much studied task of detecting hands, see [5,11]. [sent-110, score-0.172]
</p><p>76 Our approach detects hand from the Cambridge dataset in under a second, see ﬁgure (8). [sent-111, score-0.045]
</p><p>77 (We are grateful to Thayananthan, Stenger, Torr, and Cipolla for supplying these images). [sent-112, score-0.049]
</p><p>78 Figure 5: Cat Proposals: Left ears (left three panels). [sent-113, score-0.227]
</p><p>79 Figure 7: The left panels show the prototype horse (top left panel) and its feature points (bottom left panel). [sent-117, score-0.471]
</p><p>80 The right panel shows the input image (top right panel) and the position of the horse as detected by the algorithm (bottom right panel). [sent-118, score-0.43]
</p><p>81 Figure 8: Prototype hand (top left panel), edge map of prototype hand (bottom left panel), Test hand (top right panel), Test hand edges (bottom right panel). [sent-119, score-0.365]
</p><p>82 5  Comparison with alternative methods  We ran the algorithm on image of typical size 320×240. [sent-121, score-0.046]
</p><p>83 6 G Intel Pentium CPU (including all processing: edge detector, edge grouping, and object detection. [sent-126, score-0.259]
</p><p>84 Other papers report times of seconds to minutes for detecting deformable objects from similar images [5,6,7,8]. [sent-127, score-0.677]
</p><p>85 The Soft-Assign method in [15] has the ability to deal with objects with around 200 key points, but requires the initialization of the template to be close to the target object. [sent-129, score-0.291]
</p><p>86 In our proposed method, there is no need to initialize the template near to the target. [sent-131, score-0.138]
</p><p>87 Our hierarchical compositional tree structure is similar to the standard divide and conquer strategy used in some computer science algorithms. [sent-132, score-0.409]
</p><p>88 This may roughly be expected to scale as log N where N is the number of points on the deformable template. [sent-133, score-0.247]
</p><p>89 But precise complexity convergence results are difﬁcult to obtain because they depend on the topology of the template, the amount of clutter in the background, and other factors. [sent-134, score-0.09]
</p><p>90 This approach can be applied to any graphical model such as [10,12]. [sent-135, score-0.077]
</p><p>91 It is straightforward to design hierarchial compositional structures for objects based on their natural decompositions into parts. [sent-136, score-0.384]
</p><p>92 There are alternative, and more sophisticated ways, to perform inference on graphical models by decomposing them into sub-graphs, see for example [14]. [sent-137, score-0.107]
</p><p>93 6  Conclusion  We have presented a hierarchical compositional system for rapidly detecting deformable objects in images by performing inference on graphical models. [sent-139, score-1.116]
</p><p>94 Computation is performed  by passing messages up and down the tree. [sent-140, score-0.104]
</p><p>95 The systems detects objects in under a second on images of size 320 × 240. [sent-141, score-0.278]
</p><p>96 Our approach is similar in spirit to DDMCMC [13] in that we use proposals to guide the search for objects. [sent-143, score-0.162]
</p><p>97 In this paper, the proposals are based on a hierarchy of features which enables efﬁcient computation. [sent-144, score-0.299]
</p><p>98 The low-level features propose more complex features which are validated by the probability models of the complex features. [sent-145, score-0.06]
</p><p>99 ”A Statistical method for 3D object detection applied to faces and cars”. [sent-156, score-0.223]
</p><p>100 ”Shape context and chamfer matching in cluttered scenes,” In Proc. [sent-218, score-0.152]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sa', 0.396), ('qi', 0.37), ('deformable', 0.247), ('compositional', 0.231), ('ears', 0.227), ('panel', 0.184), ('cat', 0.183), ('prototype', 0.167), ('proposals', 0.162), ('objects', 0.153), ('sb', 0.151), ('detecting', 0.146), ('template', 0.138), ('coughlan', 0.123), ('horse', 0.113), ('horses', 0.113), ('hierarchical', 0.111), ('object', 0.103), ('panels', 0.098), ('detection', 0.093), ('clutter', 0.09), ('gure', 0.087), ('images', 0.08), ('edge', 0.078), ('graphical', 0.077), ('yuille', 0.075), ('shen', 0.074), ('subsets', 0.07), ('pp', 0.069), ('cats', 0.068), ('tree', 0.067), ('hierarchy', 0.067), ('grouping', 0.06), ('zi', 0.057), ('chamfer', 0.057), ('cipolla', 0.057), ('ddmcmc', 0.057), ('stenger', 0.057), ('thayananthan', 0.057), ('torr', 0.057), ('matching', 0.057), ('belief', 0.054), ('bottom', 0.053), ('appearance', 0.053), ('messages', 0.052), ('gurations', 0.052), ('passing', 0.052), ('seconds', 0.051), ('top', 0.05), ('eyes', 0.05), ('positions', 0.049), ('berg', 0.049), ('deformations', 0.049), ('supplying', 0.049), ('unary', 0.049), ('qj', 0.048), ('detector', 0.047), ('image', 0.046), ('rapid', 0.046), ('detects', 0.045), ('suppression', 0.045), ('rapidly', 0.041), ('enables', 0.04), ('constellation', 0.04), ('cutting', 0.04), ('potentials', 0.038), ('adaboost', 0.038), ('cluttered', 0.038), ('hands', 0.038), ('dynamic', 0.036), ('con', 0.035), ('occlusion', 0.035), ('angeles', 0.035), ('proceedings', 0.033), ('background', 0.033), ('shape', 0.033), ('quantization', 0.032), ('propagation', 0.031), ('pruning', 0.031), ('head', 0.031), ('zhu', 0.031), ('left', 0.031), ('attributes', 0.03), ('los', 0.03), ('inference', 0.03), ('features', 0.03), ('vision', 0.03), ('right', 0.029), ('shapes', 0.029), ('evidence', 0.029), ('decomposed', 0.028), ('chen', 0.028), ('meaningful', 0.028), ('programming', 0.028), ('searching', 0.027), ('recognition', 0.027), ('faces', 0.027), ('nodes', 0.027), ('ij', 0.026), ('illustrate', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="11-tfidf-1" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>Author: Long Zhu, Alan L. Yuille</p><p>Abstract: We describe a hierarchical compositional system for detecting deformable objects in images. Objects are represented by graphical models. The algorithm uses a hierarchical tree where the root of the tree corresponds to the full object and lower-level elements of the tree correspond to simpler features. The algorithm proceeds by passing simple messages up and down the tree. The method works rapidly, in under a second, on 320 × 240 images. We demonstrate the approach on detecting cats, horses, and hands. The method works in the presence of background clutter and occlusions. Our approach is contrasted with more traditional methods such as dynamic programming and belief propagation. 1</p><p>2 0.12775563 <a title="11-tfidf-2" href="./nips-2005-Efficient_Unsupervised_Learning_for_Localization_and_Detection_in_Object_Categories.html">63 nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</a></p>
<p>Author: Nicolas Loeff, Himanshu Arora, Alexander Sorokin, David Forsyth</p><p>Abstract: We describe a novel method for learning templates for recognition and localization of objects drawn from categories. A generative model represents the conﬁguration of multiple object parts with respect to an object coordinate system; these parts in turn generate image features. The complexity of the model in the number of features is low, meaning our model is much more efﬁcient to train than comparative methods. Moreover, a variational approximation is introduced that allows learning to be orders of magnitude faster than previous approaches while incorporating many more features. This results in both accuracy and localization improvements. Our model has been carefully tested on standard datasets; we compare with a number of recent template models. In particular, we demonstrate state-of-the-art results for detection and localization. 1</p><p>3 0.12111335 <a title="11-tfidf-3" href="./nips-2005-A_Computational_Model_of_Eye_Movements_during_Object_Class_Detection.html">5 nips-2005-A Computational Model of Eye Movements during Object Class Detection</a></p>
<p>Author: Wei Zhang, Hyejin Yang, Dimitris Samaras, Gregory J. Zelinsky</p><p>Abstract: We present a computational model of human eye movements in an object class detection task. The model combines state-of-the-art computer vision object class detection methods (SIFT features trained using AdaBoost) with a biologically plausible model of human eye movement to produce a sequence of simulated ﬁxations, culminating with the acquisition of a target. We validated the model by comparing its behavior to the behavior of human observers performing the identical object class detection task (looking for a teddy bear among visually complex nontarget objects). We found considerable agreement between the model and human data in multiple eye movement measures, including number of ﬁxations, cumulative probability of ﬁxating the target, and scanpath distance.</p><p>4 0.099669695 <a title="11-tfidf-4" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>Author: Cha Zhang, John C. Platt, Paul A. Viola</p><p>Abstract: A good image object detection algorithm is accurate, fast, and does not require exact locations of objects in a training set. We can create such an object detector by taking the architecture of the Viola-Jones detector cascade and training it with a new variant of boosting that we call MILBoost. MILBoost uses cost functions from the Multiple Instance Learning literature combined with the AnyBoost framework. We adapt the feature selection criterion of MILBoost to optimize the performance of the Viola-Jones cascade. Experiments show that the detection rate is up to 1.6 times better using MILBoost. This increased detection rate shows the advantage of simultaneously learning the locations and scales of the objects in the training set along with the parameters of the classiﬁer. 1</p><p>5 0.090811402 <a title="11-tfidf-5" href="./nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">151 nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<p>Author: Francois Fleuret, Gilles Blanchard</p><p>Abstract: We investigate the learning of the appearance of an object from a single image of it. Instead of using a large number of pictures of the object to recognize, we use a labeled reference database of pictures of other objects to learn invariance to noise and variations in pose and illumination. This acquired knowledge is then used to predict if two pictures of new objects, which do not appear on the training pictures, actually display the same object. We propose a generic scheme called chopping to address this task. It relies on hundreds of random binary splits of the training set chosen to keep together the images of any given object. Those splits are extended to the complete image space with a simple learning algorithm. Given two images, the responses of the split predictors are combined with a Bayesian rule into a posterior probability of similarity. Experiments with the COIL-100 database and with a database of 150 deA graded LTEX symbols compare our method to a classical learning with several examples of the positive class and to a direct learning of the similarity. 1</p><p>6 0.08833731 <a title="11-tfidf-6" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>7 0.085480951 <a title="11-tfidf-7" href="./nips-2005-Describing_Visual_Scenes_using_Transformed_Dirichlet_Processes.html">55 nips-2005-Describing Visual Scenes using Transformed Dirichlet Processes</a></p>
<p>8 0.082037203 <a title="11-tfidf-8" href="./nips-2005-Infinite_latent_feature_models_and_the_Indian_buffet_process.html">98 nips-2005-Infinite latent feature models and the Indian buffet process</a></p>
<p>9 0.080745429 <a title="11-tfidf-9" href="./nips-2005-Is_Early_Vision_Optimized_for_Extracting_Higher-order_Dependencies%3F.html">101 nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</a></p>
<p>10 0.065133706 <a title="11-tfidf-10" href="./nips-2005-Scaling_Laws_in_Natural_Scenes_and_the_Inference_of_3D_Shape.html">170 nips-2005-Scaling Laws in Natural Scenes and the Inference of 3D Shape</a></p>
<p>11 0.063131668 <a title="11-tfidf-11" href="./nips-2005-Goal-Based_Imitation_as_Probabilistic_Inference_over_Graphical_Models.html">87 nips-2005-Goal-Based Imitation as Probabilistic Inference over Graphical Models</a></p>
<p>12 0.061184585 <a title="11-tfidf-12" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>13 0.060215913 <a title="11-tfidf-13" href="./nips-2005-A_Cortically-Plausible_Inverse_Problem_Solving_Method_Applied_to_Recognizing_Static_and_Kinematic_3D_Objects.html">7 nips-2005-A Cortically-Plausible Inverse Problem Solving Method Applied to Recognizing Static and Kinematic 3D Objects</a></p>
<p>14 0.060082644 <a title="11-tfidf-14" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>15 0.059679892 <a title="11-tfidf-15" href="./nips-2005-Fusion_of_Similarity_Data_in_Clustering.html">79 nips-2005-Fusion of Similarity Data in Clustering</a></p>
<p>16 0.058785986 <a title="11-tfidf-16" href="./nips-2005-Searching_for_Character_Models.html">171 nips-2005-Searching for Character Models</a></p>
<p>17 0.058757301 <a title="11-tfidf-17" href="./nips-2005-Learning_Shared_Latent_Structure_for_Image_Synthesis_and_Robotic_Imitation.html">115 nips-2005-Learning Shared Latent Structure for Image Synthesis and Robotic Imitation</a></p>
<p>18 0.057196438 <a title="11-tfidf-18" href="./nips-2005-Location-based_activity_recognition.html">121 nips-2005-Location-based activity recognition</a></p>
<p>19 0.056723505 <a title="11-tfidf-19" href="./nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs.html">90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</a></p>
<p>20 0.056686234 <a title="11-tfidf-20" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.169), (1, 0.011), (2, 0.016), (3, 0.185), (4, -0.074), (5, 0.005), (6, -0.036), (7, 0.231), (8, -0.009), (9, -0.036), (10, 0.049), (11, -0.031), (12, 0.027), (13, 0.075), (14, 0.033), (15, 0.069), (16, -0.06), (17, -0.079), (18, 0.058), (19, 0.093), (20, -0.024), (21, 0.123), (22, -0.017), (23, 0.026), (24, -0.0), (25, -0.002), (26, -0.041), (27, -0.052), (28, 0.064), (29, -0.03), (30, -0.128), (31, 0.012), (32, -0.07), (33, 0.085), (34, 0.028), (35, 0.021), (36, 0.004), (37, 0.055), (38, -0.028), (39, -0.016), (40, -0.042), (41, 0.024), (42, 0.148), (43, 0.026), (44, 0.049), (45, 0.053), (46, 0.072), (47, -0.007), (48, 0.009), (49, -0.134)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96586156 <a title="11-lsi-1" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>Author: Long Zhu, Alan L. Yuille</p><p>Abstract: We describe a hierarchical compositional system for detecting deformable objects in images. Objects are represented by graphical models. The algorithm uses a hierarchical tree where the root of the tree corresponds to the full object and lower-level elements of the tree correspond to simpler features. The algorithm proceeds by passing simple messages up and down the tree. The method works rapidly, in under a second, on 320 × 240 images. We demonstrate the approach on detecting cats, horses, and hands. The method works in the presence of background clutter and occlusions. Our approach is contrasted with more traditional methods such as dynamic programming and belief propagation. 1</p><p>2 0.75743532 <a title="11-lsi-2" href="./nips-2005-Efficient_Unsupervised_Learning_for_Localization_and_Detection_in_Object_Categories.html">63 nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</a></p>
<p>Author: Nicolas Loeff, Himanshu Arora, Alexander Sorokin, David Forsyth</p><p>Abstract: We describe a novel method for learning templates for recognition and localization of objects drawn from categories. A generative model represents the conﬁguration of multiple object parts with respect to an object coordinate system; these parts in turn generate image features. The complexity of the model in the number of features is low, meaning our model is much more efﬁcient to train than comparative methods. Moreover, a variational approximation is introduced that allows learning to be orders of magnitude faster than previous approaches while incorporating many more features. This results in both accuracy and localization improvements. Our model has been carefully tested on standard datasets; we compare with a number of recent template models. In particular, we demonstrate state-of-the-art results for detection and localization. 1</p><p>3 0.63834929 <a title="11-lsi-3" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>Author: Cha Zhang, John C. Platt, Paul A. Viola</p><p>Abstract: A good image object detection algorithm is accurate, fast, and does not require exact locations of objects in a training set. We can create such an object detector by taking the architecture of the Viola-Jones detector cascade and training it with a new variant of boosting that we call MILBoost. MILBoost uses cost functions from the Multiple Instance Learning literature combined with the AnyBoost framework. We adapt the feature selection criterion of MILBoost to optimize the performance of the Viola-Jones cascade. Experiments show that the detection rate is up to 1.6 times better using MILBoost. This increased detection rate shows the advantage of simultaneously learning the locations and scales of the objects in the training set along with the parameters of the classiﬁer. 1</p><p>4 0.62350327 <a title="11-lsi-4" href="./nips-2005-Describing_Visual_Scenes_using_Transformed_Dirichlet_Processes.html">55 nips-2005-Describing Visual Scenes using Transformed Dirichlet Processes</a></p>
<p>Author: Antonio Torralba, Alan S. Willsky, Erik B. Sudderth, William T. Freeman</p><p>Abstract: Motivated by the problem of learning to detect and recognize objects with minimal supervision, we develop a hierarchical probabilistic model for the spatial structure of visual scenes. In contrast with most existing models, our approach explicitly captures uncertainty in the number of object instances depicted in a given image. Our scene model is based on the transformed Dirichlet process (TDP), a novel extension of the hierarchical DP in which a set of stochastically transformed mixture components are shared between multiple groups of data. For visual scenes, mixture components describe the spatial structure of visual features in an object–centered coordinate frame, while transformations model the object positions in a particular image. Learning and inference in the TDP, which has many potential applications beyond computer vision, is based on an empirically effective Gibbs sampler. Applied to a dataset of partially labeled street scenes, we show that the TDP’s inclusion of spatial structure improves detection performance, ﬂexibly exploiting partially labeled training images. 1</p><p>5 0.60874218 <a title="11-lsi-5" href="./nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">151 nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<p>Author: Francois Fleuret, Gilles Blanchard</p><p>Abstract: We investigate the learning of the appearance of an object from a single image of it. Instead of using a large number of pictures of the object to recognize, we use a labeled reference database of pictures of other objects to learn invariance to noise and variations in pose and illumination. This acquired knowledge is then used to predict if two pictures of new objects, which do not appear on the training pictures, actually display the same object. We propose a generic scheme called chopping to address this task. It relies on hundreds of random binary splits of the training set chosen to keep together the images of any given object. Those splits are extended to the complete image space with a simple learning algorithm. Given two images, the responses of the split predictors are combined with a Bayesian rule into a posterior probability of similarity. Experiments with the COIL-100 database and with a database of 150 deA graded LTEX symbols compare our method to a classical learning with several examples of the positive class and to a direct learning of the similarity. 1</p><p>6 0.53226626 <a title="11-lsi-6" href="./nips-2005-A_Computational_Model_of_Eye_Movements_during_Object_Class_Detection.html">5 nips-2005-A Computational Model of Eye Movements during Object Class Detection</a></p>
<p>7 0.47816125 <a title="11-lsi-7" href="./nips-2005-Infinite_latent_feature_models_and_the_Indian_buffet_process.html">98 nips-2005-Infinite latent feature models and the Indian buffet process</a></p>
<p>8 0.44987974 <a title="11-lsi-8" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>9 0.43149912 <a title="11-lsi-9" href="./nips-2005-Location-based_activity_recognition.html">121 nips-2005-Location-based activity recognition</a></p>
<p>10 0.40614942 <a title="11-lsi-10" href="./nips-2005-Large_scale_networks_fingerprinting_and_visualization_using_the_k-core_decomposition.html">107 nips-2005-Large scale networks fingerprinting and visualization using the k-core decomposition</a></p>
<p>11 0.39372981 <a title="11-lsi-11" href="./nips-2005-Searching_for_Character_Models.html">171 nips-2005-Searching for Character Models</a></p>
<p>12 0.38994381 <a title="11-lsi-12" href="./nips-2005-Identifying_Distributed_Object_Representations_in_Human_Extrastriate_Visual_Cortex.html">94 nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</a></p>
<p>13 0.38451537 <a title="11-lsi-13" href="./nips-2005-Message_passing_for_task_redistribution_on_sparse_graphs.html">125 nips-2005-Message passing for task redistribution on sparse graphs</a></p>
<p>14 0.34974968 <a title="11-lsi-14" href="./nips-2005-Is_Early_Vision_Optimized_for_Extracting_Higher-order_Dependencies%3F.html">101 nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</a></p>
<p>15 0.3469795 <a title="11-lsi-15" href="./nips-2005-Products_of_%60%60Edge-perts.html">158 nips-2005-Products of ``Edge-perts</a></p>
<p>16 0.34221616 <a title="11-lsi-16" href="./nips-2005-A_Cortically-Plausible_Inverse_Problem_Solving_Method_Applied_to_Recognizing_Static_and_Kinematic_3D_Objects.html">7 nips-2005-A Cortically-Plausible Inverse Problem Solving Method Applied to Recognizing Static and Kinematic 3D Objects</a></p>
<p>17 0.3196485 <a title="11-lsi-17" href="./nips-2005-Sequence_and_Tree_Kernels_with_Statistical_Feature_Mining.html">175 nips-2005-Sequence and Tree Kernels with Statistical Feature Mining</a></p>
<p>18 0.31434628 <a title="11-lsi-18" href="./nips-2005-Learning_Shared_Latent_Structure_for_Image_Synthesis_and_Robotic_Imitation.html">115 nips-2005-Learning Shared Latent Structure for Image Synthesis and Robotic Imitation</a></p>
<p>19 0.30923668 <a title="11-lsi-19" href="./nips-2005-Scaling_Laws_in_Natural_Scenes_and_the_Inference_of_3D_Shape.html">170 nips-2005-Scaling Laws in Natural Scenes and the Inference of 3D Shape</a></p>
<p>20 0.30682737 <a title="11-lsi-20" href="./nips-2005-A_Bayesian_Spatial_Scan_Statistic.html">4 nips-2005-A Bayesian Spatial Scan Statistic</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.056), (10, 0.03), (27, 0.034), (31, 0.074), (34, 0.047), (39, 0.042), (41, 0.011), (55, 0.03), (57, 0.429), (69, 0.045), (73, 0.032), (88, 0.06), (91, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91224819 <a title="11-lda-1" href="./nips-2005-Temporally_changing_synaptic_plasticity.html">188 nips-2005-Temporally changing synaptic plasticity</a></p>
<p>Author: Minija Tamosiunaite, Bernd Porr, Florentin Wörgötter</p><p>Abstract: Recent experimental results suggest that dendritic and back-propagating spikes can inﬂuence synaptic plasticity in different ways [1]. In this study we investigate how these signals could temporally interact at dendrites leading to changing plasticity properties at local synapse clusters. Similar to a previous study [2], we employ a differential Hebbian plasticity rule to emulate spike-timing dependent plasticity. We use dendritic (D-) and back-propagating (BP-) spikes as post-synaptic signals in the learning rule and investigate how their interaction will inﬂuence plasticity. We will analyze a situation where synapse plasticity characteristics change in the course of time, depending on the type of post-synaptic activity momentarily elicited. Starting with weak synapses, which only elicit local D-spikes, a slow, unspeciﬁc growth process is induced. As soon as the soma begins to spike this process is replaced by fast synaptic changes as the consequence of the much stronger and sharper BP-spike, which now dominates the plasticity rule. This way a winner-take-all-mechanism emerges in a two-stage process, enhancing the best-correlated inputs. These results suggest that synaptic plasticity is a temporal changing process by which the computational properties of dendrites or complete neurons can be substantially augmented. 1</p><p>same-paper 2 0.83675617 <a title="11-lda-2" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>Author: Long Zhu, Alan L. Yuille</p><p>Abstract: We describe a hierarchical compositional system for detecting deformable objects in images. Objects are represented by graphical models. The algorithm uses a hierarchical tree where the root of the tree corresponds to the full object and lower-level elements of the tree correspond to simpler features. The algorithm proceeds by passing simple messages up and down the tree. The method works rapidly, in under a second, on 320 × 240 images. We demonstrate the approach on detecting cats, horses, and hands. The method works in the presence of background clutter and occlusions. Our approach is contrasted with more traditional methods such as dynamic programming and belief propagation. 1</p><p>3 0.8158623 <a title="11-lda-3" href="./nips-2005-Large-scale_biophysical_parameter_estimation_in_single_neurons_via_constrained_linear_regression.html">106 nips-2005-Large-scale biophysical parameter estimation in single neurons via constrained linear regression</a></p>
<p>Author: Misha Ahrens, Liam Paninski, Quentin J. Huys</p><p>Abstract: Our understanding of the input-output function of single cells has been substantially advanced by biophysically accurate multi-compartmental models. The large number of parameters needing hand tuning in these models has, however, somewhat hampered their applicability and interpretability. Here we propose a simple and well-founded method for automatic estimation of many of these key parameters: 1) the spatial distribution of channel densities on the cell’s membrane; 2) the spatiotemporal pattern of synaptic input; 3) the channels’ reversal potentials; 4) the intercompartmental conductances; and 5) the noise level in each compartment. We assume experimental access to: a) the spatiotemporal voltage signal in the dendrite (or some contiguous subpart thereof, e.g. via voltage sensitive imaging techniques), b) an approximate kinetic description of the channels and synapses present in each compartment, and c) the morphology of the part of the neuron under investigation. The key observation is that, given data a)-c), all of the parameters 1)-4) may be simultaneously inferred by a version of constrained linear regression; this regression, in turn, is efﬁciently solved using standard algorithms, without any “local minima” problems despite the large number of parameters and complex dynamics. The noise level 5) may also be estimated by standard techniques. We demonstrate the method’s accuracy on several model datasets, and describe techniques for quantifying the uncertainty in our estimates. 1</p><p>4 0.51451766 <a title="11-lda-4" href="./nips-2005-Spiking_Inputs_to_a_Winner-take-all_Network.html">181 nips-2005-Spiking Inputs to a Winner-take-all Network</a></p>
<p>Author: Matthias Oster, Shih-Chii Liu</p><p>Abstract: Recurrent networks that perform a winner-take-all computation have been studied extensively. Although some of these studies include spiking networks, they consider only analog input rates. We present results of this winner-take-all computation on a network of integrate-and-ﬁre neurons which receives spike trains as inputs. We show how we can conﬁgure the connectivity in the network so that the winner is selected after a pre-determined number of input spikes. We discuss spiking inputs with both regular frequencies and Poisson-distributed rates. The robustness of the computation was tested by implementing the winner-take-all network on an analog VLSI array of 64 integrate-and-ﬁre neurons which have an innate variance in their operating parameters. 1</p><p>5 0.42564961 <a title="11-lda-5" href="./nips-2005-Dynamical_Synapses_Give_Rise_to_a_Power-Law_Distribution_of_Neuronal_Avalanches.html">61 nips-2005-Dynamical Synapses Give Rise to a Power-Law Distribution of Neuronal Avalanches</a></p>
<p>Author: Anna Levina, Michael Herrmann</p><p>Abstract: There is experimental evidence that cortical neurons show avalanche activity with the intensity of ﬁring events being distributed as a power-law. We present a biologically plausible extension of a neural network which exhibits a power-law avalanche distribution for a wide range of connectivity parameters. 1</p><p>6 0.41737652 <a title="11-lda-6" href="./nips-2005-A_Criterion_for_the_Convergence_of_Learning_with_Spike_Timing_Dependent_Plasticity.html">8 nips-2005-A Criterion for the Convergence of Learning with Spike Timing Dependent Plasticity</a></p>
<p>7 0.39669549 <a title="11-lda-7" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>8 0.39239362 <a title="11-lda-8" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>9 0.39104673 <a title="11-lda-9" href="./nips-2005-Principles_of_real-time_computing_with_feedback_applied_to_cortical_microcircuit_models.html">157 nips-2005-Principles of real-time computing with feedback applied to cortical microcircuit models</a></p>
<p>10 0.38448524 <a title="11-lda-10" href="./nips-2005-Silicon_growth_cones_map_silicon_retina.html">176 nips-2005-Silicon growth cones map silicon retina</a></p>
<p>11 0.38418025 <a title="11-lda-11" href="./nips-2005-Integrate-and-Fire_models_with_adaptation_are_good_enough.html">99 nips-2005-Integrate-and-Fire models with adaptation are good enough</a></p>
<p>12 0.38178739 <a title="11-lda-12" href="./nips-2005-Visual_Encoding_with_Jittering_Eyes.html">203 nips-2005-Visual Encoding with Jittering Eyes</a></p>
<p>13 0.36001578 <a title="11-lda-13" href="./nips-2005-Beyond_Pair-Based_STDP%3A_a_Phenomenological_Rule_for_Spike_Triplet_and_Frequency_Effects.html">39 nips-2005-Beyond Pair-Based STDP: a Phenomenological Rule for Spike Triplet and Frequency Effects</a></p>
<p>14 0.35449424 <a title="11-lda-14" href="./nips-2005-Location-based_activity_recognition.html">121 nips-2005-Location-based activity recognition</a></p>
<p>15 0.35104406 <a title="11-lda-15" href="./nips-2005-Learning_in_Silicon%3A_Timing_is_Everything.html">118 nips-2005-Learning in Silicon: Timing is Everything</a></p>
<p>16 0.34867465 <a title="11-lda-16" href="./nips-2005-Measuring_Shared_Information_and_Coordinated_Activity_in_Neuronal_Networks.html">124 nips-2005-Measuring Shared Information and Coordinated Activity in Neuronal Networks</a></p>
<p>17 0.34246707 <a title="11-lda-17" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>18 0.33728719 <a title="11-lda-18" href="./nips-2005-Neural_mechanisms_of_contrast_dependent_receptive_field_size_in_V1.html">134 nips-2005-Neural mechanisms of contrast dependent receptive field size in V1</a></p>
<p>19 0.33473584 <a title="11-lda-19" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<p>20 0.32930186 <a title="11-lda-20" href="./nips-2005-Stimulus_Evoked_Independent_Factor_Analysis_of_MEG_Data_with_Large_Background_Activity.html">183 nips-2005-Stimulus Evoked Independent Factor Analysis of MEG Data with Large Background Activity</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
