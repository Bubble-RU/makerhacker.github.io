<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-42" href="#">nips2005-42</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</h1>
<br/><p>Source: <a title="nips-2005-42-pdf" href="http://papers.nips.cc/paper/2938-combining-graph-laplacians-for-semi-supervised-learning.pdf">pdf</a></p><p>Author: Andreas Argyriou, Mark Herbster, Massimiliano Pontil</p><p>Abstract: A foundational problem in semi-supervised learning is the construction of a graph underlying the data. We propose to use a method which optimally combines a number of differently constructed graphs. For each of these graphs we associate a basic graph kernel. We then compute an optimal combined kernel. This kernel solves an extended regularization problem which requires a joint minimization over both the data and the set of graph kernels. We present encouraging results on different OCR tasks where the optimal combined kernel is computed from graphs constructed with a variety of distances functions and the ‘k’ in nearest neighbors. 1</p><p>Reference: <a title="nips-2005-42-reference" href="../nips2005_reference/nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract A foundational problem in semi-supervised learning is the construction of a graph underlying the data. [sent-7, score-0.396]
</p><p>2 For each of these graphs we associate a basic graph kernel. [sent-9, score-0.372]
</p><p>3 This kernel solves an extended regularization problem which requires a joint minimization over both the data and the set of graph kernels. [sent-11, score-0.713]
</p><p>4 We present encouraging results on different OCR tasks where the optimal combined kernel is computed from graphs constructed with a variety of distances functions and the ‘k’ in nearest neighbors. [sent-12, score-0.484]
</p><p>5 1  Introduction  Semi-supervised learning has received signiﬁcant attention in machine learning in recent years, see, for example, [2, 3, 4, 8, 9, 16, 17, 18] and references therein. [sent-13, score-0.094]
</p><p>6 The deﬁning insight of semi-supervised methods is that unlabeled data may be used to improve the performance of learners in a supervised task. [sent-14, score-0.102]
</p><p>7 Graph construction consists of two stages, ﬁrst selection of a distance function and then application of it to determine the graph’s edges (or weights thereof). [sent-16, score-0.268]
</p><p>8 For example, in this paper we consider distances between images based on the Euclidean distance, Euclidean distance combined with image transformations, and the related tangent distance [6]; we determine the edge set of the graph with k-nearest neighbors. [sent-17, score-1.032]
</p><p>9 Another common choice is 2 to weight edges by a decreasing function of the distance d such as e−βd . [sent-18, score-0.196]
</p><p>10 Although a surplus of unlabeled data may improve the quality of the empirical approximation of the manifold (via the graph) leading to improved performances, practical experience with these methods indicates that their performance signiﬁcantly depends on how the graph is constructed. [sent-19, score-0.458]
</p><p>11 Hence, the model selection problem must consider both the selection of the distance function and the parameters k or β used in the graph building process described above. [sent-20, score-0.508]
</p><p>12 A diversity of methods have been proposed for graph construction; in this paper  we do not advocate selecting a single graph but, rather we propose combining a number of graphs. [sent-21, score-0.599]
</p><p>13 Our solution implements a method based on regularization which builds upon the work in [1]. [sent-22, score-0.235]
</p><p>14 For a given dataset each combination of distance functions and edge set speciﬁcations from the distance will lead to a speciﬁc graph. [sent-23, score-0.501]
</p><p>15 Each of these graphs may then be associated with a kernel. [sent-24, score-0.09]
</p><p>16 We then apply regularization to select the best convex combination of these kernels; the minimizing function will trade off its ﬁt to the data against its norm. [sent-25, score-0.509]
</p><p>17 What is unique about this regularization is that the minimization is not over a single kernel space but rather over a space corresponding to all convex combinations of kernels. [sent-26, score-0.632]
</p><p>18 Thus all data (labeled vertices) may be conserved for training rather than reduced by cross-validation which is not an appealing option when the number of labeled vertices per class is very small. [sent-27, score-0.307]
</p><p>19 There, three different distances for 400 images of the digits ‘six’ and ‘nine’ are depicted, namely, the Euclidean distance, a distance invariant under small centered image rotations from [−10◦ , 10◦ ] and a distance invariant under rotations from [−180◦ , 180◦ ]. [sent-29, score-0.777]
</p><p>20 Clearly, the last distance is problematic as sixes become similar to nines. [sent-30, score-0.196]
</p><p>21 The performance of our graph regularization learning algorithm discussed in Section 2. [sent-31, score-0.534]
</p><p>22 2 with these distances is reported below each plot; as expected, this performance is much lower in the case that the third distance is used. [sent-32, score-0.28]
</p><p>23 In Section 2 we discuss how regularization may be applied to single graphs. [sent-34, score-0.205]
</p><p>24 First, we review regularization in the context of reproducing kernel Hilbert spaces (Section 2. [sent-35, score-0.53]
</p><p>25 Here we review the (normalized) Laplacian of the graph and a kernel which is the pseudoinverse of the graph Laplacian. [sent-38, score-0.763]
</p><p>26 In Section 3 we detail our algorithm for learning an optimal convex combination of Laplacian kernels. [sent-39, score-0.386]
</p><p>27 2  Background on graph regularization  In this section we review graph regularization [2, 9, 14] from the perspective of reproducing kernel Hilbert spaces, see e. [sent-41, score-1.249]
</p><p>28 1  Reproducing kernel Hilbert spaces  Let X be a set and K : X × X → IR a kernel function. [sent-45, score-0.348]
</p><p>29 We say that HK is a reproducing kernel Hilbert space (RKHS) of functions f : X → IR if (i): for every x ∈ X, K(x, ·) ∈ HK and (ii): the reproducing kernel property f (x) = f, K(x, ·) K holds for every f ∈ HK and x ∈ X, where ·, · K is the inner product on HK . [sent-46, score-0.55]
</p><p>30 Regularization in an RKHS learns a function f ∈ HK on the basis of available input/output examples {(xi , yi ) : i ∈ INℓ } by solving the variational problem ℓ  Eγ (K) := min  V (yi , f (xi )) + γ f  2 K  : f ∈ HK  (2. [sent-51, score-0.175]
</p><p>31 1)  i=1  where V : IR × IR → [0, ∞) is a loss function and γ a positive parameter. [sent-52, score-0.077]
</p><p>32 1) then it has the form ℓ  f (x) =  ci K(xi , x), x ∈ X i=1  (2. [sent-54, score-0.093]
</p><p>33 However, in many practical situations it is more convenient to compute c by solving the dual problem to (2. [sent-59, score-0.088]
</p><p>34 1), namely ℓ  −Eγ (K) := min  1 ⊤ c Kc + V ∗ (yi , ci ) : c ∈ IRℓ 4γ i=1  (2. [sent-60, score-0.138]
</p><p>35 3)  ∗ where K = (K(xi , xj ))ℓ i,j=1 and the function V : IR × IR → IR ∪ {+∞} is the conjugate of the loss function V which is deﬁned, for every z, α ∈ IR, as V ∗ (z, α) := sup{λα − V (z, λ) : λ ∈ IR}, see, for example, [1] for a discussion. [sent-61, score-0.077]
</p><p>36 The choice of the loss function V leads to different learning methods among which the most prominent are square loss regularization and support vector machines, see, for example [15]. [sent-62, score-0.461]
</p><p>37 2  Graph regularization  Let G be an undirected graph with m vertices and an m × m adjacency matrix A such that Aij = 1 if there is an edge connecting vertices i and j and zero otherwise1 . [sent-64, score-0.741]
</p><p>38 The graph Laplacian L is the m × m matrix deﬁned as L := D − A, where D = diag(di : i ∈ INm ) m and di is the degree of vertex i, that is di = j=1 Aij . [sent-65, score-0.358]
</p><p>39 We identify the linear space of real-valued functions deﬁned on the graph with IRm and introduce on it the semi-inner product u, v := u⊤ Lv, u, v ∈ IRm . [sent-66, score-0.282]
</p><p>40 We let {σi , ui }m be a system of eigeni=1 values/vectors of L where the eigenvalues are non-decreasing in order, σi = 0, i ∈ INr , and deﬁne the linear subspace H(G) of IRm which is orthogonal to the eigenvectors with zero eigenvalue, that is, H(G) := {v : v⊤ ui = 0, i ∈ INr }. [sent-72, score-0.099]
</p><p>41 Within this framework, we wish to learn a function v ∈ H(G) on the basis of a set of labeled vertices. [sent-73, score-0.149]
</p><p>42 Without loss of generality we assume that the ﬁrst ℓ ≤ m vertices are labeled and let y1 , . [sent-74, score-0.353]
</p><p>43 Following [2] we prescribe a loss function V and compute the function v by solving the optimization problem ℓ  min  V (yi , vi ) + γ v  2  : v ∈ H(G) . [sent-78, score-0.297]
</p><p>44 4)  i=1  We note that a similar approach is presented in [17] where v is (essentially) obtained as the minimal norm interpolant in H(G) to the labeled vertices. [sent-80, score-0.187]
</p><p>45 4) balances the error on the labeled points with a smoothness term measuring the complexity of v on the graph. [sent-82, score-0.211]
</p><p>46 Note that this last term contains the information of both the labeled and unlabeled vertices via the graph Laplacian. [sent-83, score-0.66]
</p><p>47 Moreover, the pseudoinverse of the Laplacian, L+ , is the reproducing kernel of H(G), see, for example, [7] for a proof. [sent-89, score-0.325]
</p><p>48 This means that for every v ∈ H(G) and i ∈ INm there holds the reproducing kernel property vi = L+ , v , where L+ is the i-th i i column of L+ . [sent-90, score-0.362]
</p><p>49 Hence, by setting X ≡ INm , f (i) = vi and K(i, j) = L+ , i, j ∈ INm , we ij see that HK ≡ H(G). [sent-91, score-0.118]
</p><p>50 In particular, for square loss regularization [2] and minimal norm interpolation [17] this requires solving a squared linear system of m and m − ℓ equations respectively. [sent-96, score-0.433]
</p><p>51 The coefﬁcients ci are obtained by solving problem (2. [sent-100, score-0.181]
</p><p>52 For example, for square loss regularization the computation of the parameter vector c = (ci : i ∈ INℓ ) involves solving a linear system of ℓ equations, namely (K + γI)c = y. [sent-102, score-0.395]
</p><p>53 5)  Learning a convex combination of Laplacian kernels  We now describe our framework for learning with multiple graph Laplacians. [sent-104, score-0.893]
</p><p>54 We assume that we are given n graphs G(q) , q ∈ INn , all having m vertices, with corresponding Laplacians L(q) , kernels K (q) = (L(q) )+ , Hilbert spaces H(q) := H(G(q) ) and norms v 2 := v⊤ L(q) v, v ∈ H(q) . [sent-105, score-0.4]
</p><p>55 We propose to learn an optimal convex combination of q graph kernels, that is, we solve the optimization problem ℓ  ρ = min  V (yi , vi ) + γ v  2 K(λ)  : λ ∈ Λ, v ∈ HK(λ)  (3. [sent-106, score-0.783]
</p><p>56 1)  i=1 n  where we have deﬁned the set Λ := {λ ∈ IRn : λq ≥ 0, q=1 λq = 1} and, for each n λ ∈ Λ, the kernel K(λ) := q=1 λq K (q) . [sent-107, score-0.149]
</p><p>57 Hence an optimal convex combination of kernels has a smaller right hand side than that of any individual kernel, motivating the expectation of improved performance. [sent-109, score-0.599]
</p><p>58 1) over v ∈ HK and K ∈ co(K), the convex hull of kernels in a prescribed set K. [sent-113, score-0.484]
</p><p>59 1) it is important to require that the kernels K (q) satisfy a normalization condition such as that they all have the same trace or the same Frobenius norm, see [10] for a discussion. [sent-117, score-0.26]
</p><p>60 ˆ ˆ  Figure 1: Algorithm to compute an optimal convex combination of kernels in the set co{K (q) : q ∈ INn }. [sent-125, score-0.599]
</p><p>61 1) we can rewrite this problem as ℓ  −ρ = max min  1 ⊤ c K(λ)c + V ∗ (yi , ci ) : c ∈ IRℓ : λ ∈ Λ . [sent-128, score-0.168]
</p><p>62 2) expresses the optimal convex combination of the kernels as the solution to a saddle point problem. [sent-131, score-0.64]
</p><p>63 In particular, for square loss regularization this requires ˆ solving the equation (2. [sent-139, score-0.395]
</p><p>64 4  Experiments  In this section we present our experiments on optical character recognition. [sent-141, score-0.105]
</p><p>65 First, the optimal convex combination of kernels computed by our algorithm is competitive with the best base kernels. [sent-143, score-0.632]
</p><p>66 Second, by observing the ‘weights’ of the convex combination we can distinguish the strong from the weak candidate kernels. [sent-144, score-0.304]
</p><p>67 We used the USPS dataset2 of 16×16 images of handwritten digits with pixel values ranging between -1 and 1. [sent-146, score-0.158]
</p><p>68 We present the results for 5 pairwise classiﬁcation tasks of varying difﬁculty and for odd vs. [sent-147, score-0.213]
</p><p>69 For pairwise classiﬁcation, the training set consisted of the ﬁrst 200 images for each digit in the USPS training set and the number of labeled points was chosen to be 4, 8 or 12 (with equal numbers for each digit). [sent-149, score-0.464]
</p><p>70 even digit classiﬁcation, the training set consisted of the ﬁrst 80 images per digit in the USPS training set and the number of labeled points was 10, 20 or 30, with equal numbers for each digit. [sent-151, score-0.541]
</p><p>71 Performance was averaged over 30 random selections, each with the same number of labeled points. [sent-152, score-0.149]
</p><p>72 In each experiment, we constructed n = 30 graphs G(q) (q ∈ INn ) by combining k-nearest neighbors (k ∈ IN10 ) with three different distances. [sent-153, score-0.161]
</p><p>73 Since kernels obtained from different types of graphs can vary widely, it was necessary to renormalize them. [sent-156, score-0.35]
</p><p>74 Hence, we chose to normalize each kernel during the 2  Available at: http://www-stat-class. [sent-157, score-0.149]
</p><p>75 7  Table 1: Misclassiﬁcation error percentage (top) and standard deviation (bottom) for the best convex combination of kernels on different handwritten digit recognition tasks, using different distances. [sent-312, score-0.766]
</p><p>76 training process by the Frobenius norm of its submatrix corresponding to the labeled data. [sent-314, score-0.218]
</p><p>77 The regularization parameter was set to 10−5 in all algorithms. [sent-316, score-0.205]
</p><p>78 For convex minimization, as the starting kernel in the algorithm in Figure 1 we always used the average of the n kernels and as the maximum number of iterations T = 100. [sent-317, score-0.604]
</p><p>79 Table 1 shows the results obtained using three distances as combined with k-NN (k ∈ IN10 ). [sent-318, score-0.128]
</p><p>80 The ﬁrst distance is the Euclidean distance between images. [sent-319, score-0.392]
</p><p>81 The second method is transformation, where the distance between two images is given by the smallest Euclidean distance between any pair of transformed images as determined by applying a number of afﬁne transformations and a thickness transformation3 , see [6] for more information. [sent-320, score-0.566]
</p><p>82 The third distance is tangent distance, as described in [6], which is a ﬁrst-order approximation to the above transformations. [sent-321, score-0.319]
</p><p>83 For the ﬁrst three columns in the table the Euclidean distance was used, for columns 4–6 the image transformation distance was used, for columns 7–9 the tangent distance was used. [sent-322, score-0.98]
</p><p>84 We also noted that within each of the methods the performance of the convex combination is comparable to that of the best kernels. [sent-325, score-0.304]
</p><p>85 Figure 2 reports the weight of each individual kernel learned by our algorithm when 2% labels are used in the pairwise tasks and 20 labels are used for odd vs. [sent-326, score-0.479]
</p><p>86 7 task, the large weights are associated with the graphs/kernels built with the tangent distance. [sent-329, score-0.158]
</p><p>87 The effectiveness of our algorithm in selecting the good graphs/kernels is better demonstrated in Figure 3, where the Euclidean and the transformation kernels are combined with a “low-quality” kernel. [sent-330, score-0.41]
</p><p>88 3  This distance was approximated using Matlab’s constrained minimization function. [sent-332, score-0.243]
</p><p>89 The ﬁgure shows the distance matrix on the set of labeled and unlabeled data for the Euclidean, transformation and “low-quality distance” respectively. [sent-333, score-0.553]
</p><p>90 The best error among 15 different values of k within each method, the error of the learned convex combination and the total learned weights for each method are shown below each plot. [sent-334, score-0.467]
</p><p>91 It is clear that the solution of the algorithm is dominated by the good kernels and is not inﬂuenced by the ones with low performance. [sent-335, score-0.26]
</p><p>92 As a result, the error of the convex combination is comparable to that of the Euclidean and transformation methods. [sent-336, score-0.443]
</p><p>93 The ﬁnal experiment (see Figure 4) demonstrates that unlabeled data improves the performance of our method. [sent-337, score-0.102]
</p><p>94 Euclidean  Transformation  Low−quality distance  0  0  0  50  50  50  100  100  100  150  150  150  200  200  200  250  250  250  300  300  300  350  350  400 0  100  200  300  400  error = 0. [sent-384, score-0.229]
</p><p>95 26% Figure 3: Similarity matrices and corresponding learned coefﬁcients of the convex combination for the 6 vs. [sent-391, score-0.335]
</p><p>96 5  Conclusion  We have presented a method for computing an optimal kernel within the framework of regularization over graphs. [sent-394, score-0.389]
</p><p>97 When tested on optical character recognition tasks, the method exhibits competitive performance and is able to select good graph structures. [sent-396, score-0.42]
</p><p>98 In particular, we may consider a continuous family of graphs each corresponding to a different weight matrix and study graph kernel combinations over this class. [sent-398, score-0.557]
</p><p>99 The number of labeled points is 10 on the left and 20 on the right. [sent-424, score-0.149]
</p><p>100 Diffusion kernels on graphs and other discrete input spaces. [sent-486, score-0.35]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ir', 0.301), ('graph', 0.282), ('kernels', 0.26), ('euclidean', 0.222), ('regularization', 0.205), ('inm', 0.203), ('distance', 0.196), ('convex', 0.195), ('inn', 0.177), ('hk', 0.158), ('kernel', 0.149), ('labeled', 0.149), ('irm', 0.136), ('odd', 0.129), ('vertices', 0.127), ('reproducing', 0.126), ('tangent', 0.123), ('digit', 0.115), ('laplacian', 0.114), ('combination', 0.109), ('transformation', 0.106), ('inp', 0.102), ('unlabeled', 0.102), ('hilbert', 0.097), ('ci', 0.093), ('graphs', 0.09), ('vi', 0.087), ('usps', 0.086), ('distances', 0.084), ('loss', 0.077), ('laplacians', 0.075), ('images', 0.07), ('argyriou', 0.068), ('herbster', 0.068), ('inr', 0.068), ('co', 0.067), ('character', 0.062), ('solving', 0.058), ('aij', 0.056), ('square', 0.055), ('handwritten', 0.054), ('spaces', 0.05), ('micchelli', 0.05), ('pseudoinverse', 0.05), ('th', 0.049), ('minimization', 0.047), ('pontil', 0.047), ('learning', 0.047), ('tasks', 0.046), ('rotations', 0.045), ('min', 0.045), ('manifold', 0.044), ('combined', 0.044), ('optical', 0.043), ('rkhs', 0.043), ('labels', 0.043), ('columns', 0.042), ('yi', 0.042), ('pk', 0.041), ('saddle', 0.041), ('frobenius', 0.041), ('eigenvectors', 0.039), ('belkin', 0.038), ('norm', 0.038), ('di', 0.038), ('pairwise', 0.038), ('construction', 0.037), ('zhu', 0.037), ('misclassi', 0.037), ('image', 0.037), ('constructed', 0.036), ('combinations', 0.036), ('combining', 0.035), ('optimal', 0.035), ('invariant', 0.035), ('weights', 0.035), ('transformations', 0.034), ('colt', 0.034), ('digits', 0.034), ('competitive', 0.033), ('error', 0.033), ('semide', 0.032), ('ij', 0.031), ('learned', 0.031), ('cients', 0.031), ('text', 0.031), ('training', 0.031), ('consisted', 0.03), ('ui', 0.03), ('builds', 0.03), ('smola', 0.03), ('smoothing', 0.03), ('experience', 0.03), ('problem', 0.03), ('balances', 0.029), ('irn', 0.029), ('kondor', 0.029), ('lesser', 0.029), ('matveeva', 0.029), ('prescribed', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="42-tfidf-1" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>Author: Andreas Argyriou, Mark Herbster, Massimiliano Pontil</p><p>Abstract: A foundational problem in semi-supervised learning is the construction of a graph underlying the data. We propose to use a method which optimally combines a number of differently constructed graphs. For each of these graphs we associate a basic graph kernel. We then compute an optimal combined kernel. This kernel solves an extended regularization problem which requires a joint minimization over both the data and the set of graph kernels. We present encouraging results on different OCR tasks where the optimal combined kernel is computed from graphs constructed with a variety of distances functions and the ‘k’ in nearest neighbors. 1</p><p>2 0.23455104 <a title="42-tfidf-2" href="./nips-2005-Maximum_Margin_Semi-Supervised_Learning_for_Structured_Variables.html">123 nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</a></p>
<p>Author: Y. Altun, D. McAllester, M. Belkin</p><p>Abstract: Many real-world classiﬁcation problems involve the prediction of multiple inter-dependent variables forming some structural dependency. Recent progress in machine learning has mainly focused on supervised classiﬁcation of such structured variables. In this paper, we investigate structured classiﬁcation in a semi-supervised setting. We present a discriminative approach that utilizes the intrinsic geometry of input patterns revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables. Unlike transductive algorithms, our formulation naturally extends to new test points. 1</p><p>3 0.23417634 <a title="42-tfidf-3" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>Author: Tong Zhang, Rie Kubota Ando</p><p>Abstract: We consider a framework for semi-supervised learning using spectral decomposition based un-supervised kernel design. This approach subsumes a class of previously proposed semi-supervised learning methods on data graphs. We examine various theoretical properties of such methods. In particular, we derive a generalization performance bound, and obtain the optimal kernel design by minimizing the bound. Based on the theoretical analysis, we are able to demonstrate why spectral kernel design based methods can often improve the predictive performance. Experiments are used to illustrate the main consequences of our analysis.</p><p>4 0.23401718 <a title="42-tfidf-4" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>Author: Ashish Kapoor, Hyungil Ahn, Yuan Qi, Rosalind W. Picard</p><p>Abstract: There have been many graph-based approaches for semi-supervised classiﬁcation. One problem is that of hyperparameter learning: performance depends greatly on the hyperparameters of the similarity graph, transformation of the graph Laplacian and the noise model. We present a Bayesian framework for learning hyperparameters for graph-based semisupervised classiﬁcation. Given some labeled data, which can contain inaccurate labels, we pose the semi-supervised classiﬁcation as an inference problem over the unknown labels. Expectation Propagation is used for approximate inference and the mean of the posterior is used for classiﬁcation. The hyperparameters are learned using EM for evidence maximization. We also show that the posterior mean can be written in terms of the kernel matrix, providing a Bayesian classiﬁer to classify new points. Tests on synthetic and real datasets show cases where there are signiﬁcant improvements in performance over the existing approaches. 1</p><p>5 0.19544083 <a title="42-tfidf-5" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>Author: Kilian Q. Weinberger, John Blitzer, Lawrence K. Saul</p><p>Abstract: We show how to learn a Mahanalobis distance metric for k-nearest neighbor (kNN) classiﬁcation by semideﬁnite programming. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. On seven data sets of varying size and difﬁculty, we ﬁnd that metrics trained in this way lead to signiﬁcant improvements in kNN classiﬁcation—for example, achieving a test error rate of 1.3% on the MNIST handwritten digits. As in support vector machines (SVMs), the learning problem reduces to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our framework requires no modiﬁcation or extension for problems in multiway (as opposed to binary) classiﬁcation. 1</p><p>6 0.16379881 <a title="42-tfidf-6" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>7 0.15478024 <a title="42-tfidf-7" href="./nips-2005-Soft_Clustering_on_Graphs.html">178 nips-2005-Soft Clustering on Graphs</a></p>
<p>8 0.15057719 <a title="42-tfidf-8" href="./nips-2005-Metric_Learning_by_Collapsing_Classes.html">126 nips-2005-Metric Learning by Collapsing Classes</a></p>
<p>9 0.13923551 <a title="42-tfidf-9" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>10 0.12801148 <a title="42-tfidf-10" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>11 0.12526633 <a title="42-tfidf-11" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>12 0.12524198 <a title="42-tfidf-12" href="./nips-2005-Diffusion_Maps%2C_Spectral_Clustering_and_Eigenfunctions_of_Fokker-Planck_Operators.html">56 nips-2005-Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators</a></p>
<p>13 0.11976518 <a title="42-tfidf-13" href="./nips-2005-Sequence_and_Tree_Kernels_with_Statistical_Feature_Mining.html">175 nips-2005-Sequence and Tree Kernels with Statistical Feature Mining</a></p>
<p>14 0.11811413 <a title="42-tfidf-14" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>15 0.1173673 <a title="42-tfidf-15" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>16 0.1109932 <a title="42-tfidf-16" href="./nips-2005-Value_Function_Approximation_with_Diffusion_Wavelets_and_Laplacian_Eigenfunctions.html">199 nips-2005-Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions</a></p>
<p>17 0.10598096 <a title="42-tfidf-17" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>18 0.10507421 <a title="42-tfidf-18" href="./nips-2005-Variational_EM_Algorithms_for_Non-Gaussian_Latent_Variable_Models.html">202 nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</a></p>
<p>19 0.10335619 <a title="42-tfidf-19" href="./nips-2005-Non-Local_Manifold_Parzen_Windows.html">138 nips-2005-Non-Local Manifold Parzen Windows</a></p>
<p>20 0.098105162 <a title="42-tfidf-20" href="./nips-2005-Fixing_two_weaknesses_of_the_Spectral_Method.html">75 nips-2005-Fixing two weaknesses of the Spectral Method</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.321), (1, 0.204), (2, -0.149), (3, -0.141), (4, -0.11), (5, 0.113), (6, 0.157), (7, -0.025), (8, 0.105), (9, -0.036), (10, 0.09), (11, -0.099), (12, -0.19), (13, -0.01), (14, -0.085), (15, -0.245), (16, 0.059), (17, -0.011), (18, -0.018), (19, 0.093), (20, -0.045), (21, 0.005), (22, -0.03), (23, -0.019), (24, 0.062), (25, -0.029), (26, 0.031), (27, -0.041), (28, -0.126), (29, -0.072), (30, 0.022), (31, -0.013), (32, -0.044), (33, 0.03), (34, 0.07), (35, 0.089), (36, -0.072), (37, 0.053), (38, -0.019), (39, -0.007), (40, -0.075), (41, 0.025), (42, 0.087), (43, 0.057), (44, -0.012), (45, -0.045), (46, -0.09), (47, 0.002), (48, 0.015), (49, -0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97389805 <a title="42-lsi-1" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>Author: Andreas Argyriou, Mark Herbster, Massimiliano Pontil</p><p>Abstract: A foundational problem in semi-supervised learning is the construction of a graph underlying the data. We propose to use a method which optimally combines a number of differently constructed graphs. For each of these graphs we associate a basic graph kernel. We then compute an optimal combined kernel. This kernel solves an extended regularization problem which requires a joint minimization over both the data and the set of graph kernels. We present encouraging results on different OCR tasks where the optimal combined kernel is computed from graphs constructed with a variety of distances functions and the ‘k’ in nearest neighbors. 1</p><p>2 0.79151893 <a title="42-lsi-2" href="./nips-2005-Maximum_Margin_Semi-Supervised_Learning_for_Structured_Variables.html">123 nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</a></p>
<p>Author: Y. Altun, D. McAllester, M. Belkin</p><p>Abstract: Many real-world classiﬁcation problems involve the prediction of multiple inter-dependent variables forming some structural dependency. Recent progress in machine learning has mainly focused on supervised classiﬁcation of such structured variables. In this paper, we investigate structured classiﬁcation in a semi-supervised setting. We present a discriminative approach that utilizes the intrinsic geometry of input patterns revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables. Unlike transductive algorithms, our formulation naturally extends to new test points. 1</p><p>3 0.71322095 <a title="42-lsi-3" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>Author: Thomas Gärtner, Quoc V. Le, Simon Burton, Alex J. Smola, Vishy Vishwanathan</p><p>Abstract: We present a method for performing transductive inference on very large datasets. Our algorithm is based on multiclass Gaussian processes and is effective whenever the multiplication of the kernel matrix or its inverse with a vector can be computed sufﬁciently fast. This holds, for instance, for certain graph and string kernels. Transduction is achieved by variational inference over the unlabeled data subject to a balancing constraint. 1</p><p>4 0.70503819 <a title="42-lsi-4" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>Author: Ashish Kapoor, Hyungil Ahn, Yuan Qi, Rosalind W. Picard</p><p>Abstract: There have been many graph-based approaches for semi-supervised classiﬁcation. One problem is that of hyperparameter learning: performance depends greatly on the hyperparameters of the similarity graph, transformation of the graph Laplacian and the noise model. We present a Bayesian framework for learning hyperparameters for graph-based semisupervised classiﬁcation. Given some labeled data, which can contain inaccurate labels, we pose the semi-supervised classiﬁcation as an inference problem over the unknown labels. Expectation Propagation is used for approximate inference and the mean of the posterior is used for classiﬁcation. The hyperparameters are learned using EM for evidence maximization. We also show that the posterior mean can be written in terms of the kernel matrix, providing a Bayesian classiﬁer to classify new points. Tests on synthetic and real datasets show cases where there are signiﬁcant improvements in performance over the existing approaches. 1</p><p>5 0.70500475 <a title="42-lsi-5" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>Author: Tong Zhang, Rie Kubota Ando</p><p>Abstract: We consider a framework for semi-supervised learning using spectral decomposition based un-supervised kernel design. This approach subsumes a class of previously proposed semi-supervised learning methods on data graphs. We examine various theoretical properties of such methods. In particular, we derive a generalization performance bound, and obtain the optimal kernel design by minimizing the bound. Based on the theoretical analysis, we are able to demonstrate why spectral kernel design based methods can often improve the predictive performance. Experiments are used to illustrate the main consequences of our analysis.</p><p>6 0.64142305 <a title="42-lsi-6" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>7 0.59680748 <a title="42-lsi-7" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>8 0.56859267 <a title="42-lsi-8" href="./nips-2005-Metric_Learning_by_Collapsing_Classes.html">126 nips-2005-Metric Learning by Collapsing Classes</a></p>
<p>9 0.52063876 <a title="42-lsi-9" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>10 0.48898911 <a title="42-lsi-10" href="./nips-2005-Computing_the_Solution_Path_for_the_Regularized_Support_Vector_Regression.html">44 nips-2005-Computing the Solution Path for the Regularized Support Vector Regression</a></p>
<p>11 0.48580769 <a title="42-lsi-11" href="./nips-2005-Large_scale_networks_fingerprinting_and_visualization_using_the_k-core_decomposition.html">107 nips-2005-Large scale networks fingerprinting and visualization using the k-core decomposition</a></p>
<p>12 0.4776006 <a title="42-lsi-12" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>13 0.46580634 <a title="42-lsi-13" href="./nips-2005-Fixing_two_weaknesses_of_the_Spectral_Method.html">75 nips-2005-Fixing two weaknesses of the Spectral Method</a></p>
<p>14 0.45632693 <a title="42-lsi-14" href="./nips-2005-Sequence_and_Tree_Kernels_with_Statistical_Feature_Mining.html">175 nips-2005-Sequence and Tree Kernels with Statistical Feature Mining</a></p>
<p>15 0.45607811 <a title="42-lsi-15" href="./nips-2005-Learning_Topology_with_the_Generative_Gaussian_Graph_and_the_EM_Algorithm.html">116 nips-2005-Learning Topology with the Generative Gaussian Graph and the EM Algorithm</a></p>
<p>16 0.41662639 <a title="42-lsi-16" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>17 0.41502389 <a title="42-lsi-17" href="./nips-2005-Soft_Clustering_on_Graphs.html">178 nips-2005-Soft Clustering on Graphs</a></p>
<p>18 0.41354191 <a title="42-lsi-18" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>19 0.41096094 <a title="42-lsi-19" href="./nips-2005-Fast_Krylov_Methods_for_N-Body_Learning.html">71 nips-2005-Fast Krylov Methods for N-Body Learning</a></p>
<p>20 0.40786904 <a title="42-lsi-20" href="./nips-2005-Diffusion_Maps%2C_Spectral_Clustering_and_Eigenfunctions_of_Fokker-Planck_Operators.html">56 nips-2005-Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.04), (10, 0.035), (31, 0.018), (34, 0.573), (41, 0.012), (50, 0.012), (55, 0.023), (65, 0.013), (69, 0.036), (73, 0.041), (88, 0.09), (91, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99452198 <a title="42-lda-1" href="./nips-2005-A_Bayes_Rule_for_Density_Matrices.html">2 nips-2005-A Bayes Rule for Density Matrices</a></p>
<p>Author: Manfred K. Warmuth</p><p>Abstract: The classical Bayes rule computes the posterior model probability from the prior probability and the data likelihood. We generalize this rule to the case when the prior is a density matrix (symmetric positive deﬁnite and trace one) and the data likelihood a covariance matrix. The classical Bayes rule is retained as the special case when the matrices are diagonal. In the classical setting, the calculation of the probability of the data is an expected likelihood, where the expectation is over the prior distribution. In the generalized setting, this is replaced by an expected variance calculation where the variance is computed along the eigenvectors of the prior density matrix and the expectation is over the eigenvalues of the density matrix (which form a probability vector). The variances along any direction is determined by the covariance matrix. Curiously enough this expected variance calculation is a quantum measurement where the co-variance matrix speciﬁes the instrument and the prior density matrix the mixture state of the particle. We motivate both the classical and the generalized Bayes rule with a minimum relative entropy principle, where the Kullbach-Leibler version gives the classical Bayes rule and Umegaki’s quantum relative entropy the new Bayes rule for density matrices. 1</p><p>2 0.98641884 <a title="42-lda-2" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>Author: François Laviolette, Mario Marchand, Mohak Shah</p><p>Abstract: We design a new learning algorithm for the Set Covering Machine from a PAC-Bayes perspective and propose a PAC-Bayes risk bound which is minimized for classiﬁers achieving a non trivial margin-sparsity trade-oﬀ. 1</p><p>3 0.98597836 <a title="42-lda-3" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>Author: Sören Sonnenburg, Gunnar Rätsch, Christin Schäfer</p><p>Abstract: While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lankriet et al. (2004) considered conic combinations of kernel matrices for classiﬁcation, leading to a convex quadratically constraint quadratic program. We show that it can be rewritten as a semi-inﬁnite linear program that can be efﬁciently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classiﬁcation. Experimental results show that the proposed algorithm helps for automatic model selection, improving the interpretability of the learning result and works for hundred thousands of examples or hundreds of kernels to be combined. 1</p><p>same-paper 4 0.98533446 <a title="42-lda-4" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>Author: Andreas Argyriou, Mark Herbster, Massimiliano Pontil</p><p>Abstract: A foundational problem in semi-supervised learning is the construction of a graph underlying the data. We propose to use a method which optimally combines a number of differently constructed graphs. For each of these graphs we associate a basic graph kernel. We then compute an optimal combined kernel. This kernel solves an extended regularization problem which requires a joint minimization over both the data and the set of graph kernels. We present encouraging results on different OCR tasks where the optimal combined kernel is computed from graphs constructed with a variety of distances functions and the ‘k’ in nearest neighbors. 1</p><p>5 0.96565157 <a title="42-lda-5" href="./nips-2005-Off-Road_Obstacle_Avoidance_through_End-to-End_Learning.html">143 nips-2005-Off-Road Obstacle Avoidance through End-to-End Learning</a></p>
<p>Author: Urs Muller, Jan Ben, Eric Cosatto, Beat Flepp, Yann L. Cun</p><p>Abstract: We describe a vision-based obstacle avoidance system for off-road mobile robots. The system is trained from end to end to map raw input images to steering angles. It is trained in supervised mode to predict the steering angles provided by a human driver during training runs collected in a wide variety of terrains, weather conditions, lighting conditions, and obstacle types. The robot is a 50cm off-road truck, with two forwardpointing wireless color cameras. A remote computer processes the video and controls the robot via radio. The learning system is a large 6-layer convolutional network whose input is a single left/right pair of unprocessed low-resolution images. The robot exhibits an excellent ability to detect obstacles and navigate around them in real time at speeds of 2 m/s.</p><p>6 0.83656406 <a title="42-lda-6" href="./nips-2005-Computing_the_Solution_Path_for_the_Regularized_Support_Vector_Regression.html">44 nips-2005-Computing the Solution Path for the Regularized Support Vector Regression</a></p>
<p>7 0.82521278 <a title="42-lda-7" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>8 0.8092525 <a title="42-lda-8" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>9 0.79627877 <a title="42-lda-9" href="./nips-2005-Maximum_Margin_Semi-Supervised_Learning_for_Structured_Variables.html">123 nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</a></p>
<p>10 0.7939831 <a title="42-lda-10" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>11 0.79114282 <a title="42-lda-11" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>12 0.78623897 <a title="42-lda-12" href="./nips-2005-From_Lasso_regression_to_Feature_vector_machine.html">77 nips-2005-From Lasso regression to Feature vector machine</a></p>
<p>13 0.78563625 <a title="42-lda-13" href="./nips-2005-Asymptotics_of_Gaussian_Regularized_Least_Squares.html">31 nips-2005-Asymptotics of Gaussian Regularized Least Squares</a></p>
<p>14 0.77075446 <a title="42-lda-14" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>15 0.76105952 <a title="42-lda-15" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>16 0.7542007 <a title="42-lda-16" href="./nips-2005-Metric_Learning_by_Collapsing_Classes.html">126 nips-2005-Metric Learning by Collapsing Classes</a></p>
<p>17 0.75282496 <a title="42-lda-17" href="./nips-2005-Transfer_learning_for_text_classification.html">195 nips-2005-Transfer learning for text classification</a></p>
<p>18 0.74933219 <a title="42-lda-18" href="./nips-2005-Non-Local_Manifold_Parzen_Windows.html">138 nips-2005-Non-Local Manifold Parzen Windows</a></p>
<p>19 0.74369216 <a title="42-lda-19" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>20 0.74338186 <a title="42-lda-20" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
