<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>109 nips-2005-Learning Cue-Invariant Visual Responses</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-109" href="#">nips2005-109</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>109 nips-2005-Learning Cue-Invariant Visual Responses</h1>
<br/><p>Source: <a title="nips-2005-109-pdf" href="http://papers.nips.cc/paper/2939-learning-cue-invariant-visual-responses.pdf">pdf</a></p><p>Author: Jarmo Hurri</p><p>Abstract: Multiple visual cues are used by the visual system to analyze a scene; achromatic cues include luminance, texture, contrast and motion. Singlecell recordings have shown that the mammalian visual cortex contains neurons that respond similarly to scene structure (e.g., orientation of a boundary), regardless of the cue type conveying this information. This paper shows that cue-invariant response properties of simple- and complex-type cells can be learned from natural image data in an unsupervised manner. In order to do this, we also extend a previous conceptual model of cue invariance so that it can be applied to model simple- and complex-cell responses. Our results relate cue-invariant response properties to natural image statistics, thereby showing how the statistical modeling approach can be used to model processing beyond the elemental response properties visual neurons. This work also demonstrates how to learn, from natural image data, more sophisticated feature detectors than those based on changes in mean luminance, thereby paving the way for new data-driven approaches to image processing and computer vision. 1</p><p>Reference: <a title="nips-2005-109-reference" href="../nips2005_reference/nips-2005-Learning_Cue-Invariant_Visual_Responses_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Box 68, FIN-00014 University of Helsinki, Finland  Abstract Multiple visual cues are used by the visual system to analyze a scene; achromatic cues include luminance, texture, contrast and motion. [sent-3, score-0.479]
</p><p>2 Singlecell recordings have shown that the mammalian visual cortex contains neurons that respond similarly to scene structure (e. [sent-4, score-0.264]
</p><p>3 , orientation of a boundary), regardless of the cue type conveying this information. [sent-6, score-0.392]
</p><p>4 This paper shows that cue-invariant response properties of simple- and complex-type cells can be learned from natural image data in an unsupervised manner. [sent-7, score-0.787]
</p><p>5 In order to do this, we also extend a previous conceptual model of cue invariance so that it can be applied to model simple- and complex-cell responses. [sent-8, score-0.293]
</p><p>6 Our results relate cue-invariant response properties to natural image statistics, thereby showing how the statistical modeling approach can be used to model processing beyond the elemental response properties visual neurons. [sent-9, score-0.871]
</p><p>7 This work also demonstrates how to learn, from natural image data, more sophisticated feature detectors than those based on changes in mean luminance, thereby paving the way for new data-driven approaches to image processing and computer vision. [sent-10, score-0.411]
</p><p>8 Spatiotemporal variations in the mean luminance level – which are also called ﬁrst-order cues – are computationally the simplest of these; the name ’ﬁrst-order’ comes from the idea that a single linear ﬁltering operation can detect these cues. [sent-12, score-0.324]
</p><p>9 Other types of visual cues include contrast, texture and motion; in general, cues related to variations in other characteristics than mean luminance are called higher-order (also called non-Fourier) cues; the analysis of these is thought to involve more than one level of processing/ﬁltering. [sent-13, score-0.612]
</p><p>10 Single-cell recordings have shown that the mammalian visual cortex contains neurons that are selective to both ﬁrst- and higher-order cues. [sent-14, score-0.271]
</p><p>11 For example, a neuron may exhibit similar selectivity to the orientation of a boundary, regardless of whether the boundary is a result of spatial changes in mean luminance or contrast [1]. [sent-15, score-0.518]
</p><p>12 Monkey cortical areas V1 and V2, and cat cortical areas 17 and 18, contain both simple- (orientation-, frequency- and phase-selective) and complex-type (orientation- and frequency-selective, phase-invariant) cells that exhibit such cue-invariant response properties [2, 1, 3, 4, 5]. [sent-16, score-0.532]
</p><p>13 Recent computational modeling of the visual system has produced fundamental results relating stimulus statistics to ﬁrst-order response properties of simple and complex cells (see, e. [sent-18, score-0.689]
</p><p>14 The linear stream responds to ﬁrst-order cues, while the nonlinear stream responds to higher-order cues. [sent-22, score-0.262]
</p><p>15 The model consists of simple cells, complex cells and a feedback path leading from a population of high-frequency ﬁrst-order complex cells to low-frequency cue-invariant simple cells. [sent-26, score-1.114]
</p><p>16 In a cue-invariant simple cell, the feedback is ﬁltered with a ﬁlter that has similar spatial characteristics as the feedforward ﬁlter of the cell. [sent-27, score-0.88]
</p><p>17 Note that while our model results in cue-invariant response properties, it is not a model of cue integration, because in the sum the two paths can cancel out. [sent-29, score-0.49]
</p><p>18 In this instance of the model, the high-frequency cells prefer horizontal stimuli, while the low-frequency cue-invariant cells prefer vertical stimuli; in other instances, this relationship can be different. [sent-31, score-0.427]
</p><p>19 statistics -based framework for cue-invariant responses of both simple and complex cells. [sent-34, score-0.284]
</p><p>20 In order to achieve this, we also extend the two-stream model of cue-invariant responses (Figure 1A) to account for cue-invariant responses at both simple- and complex-cell levels. [sent-35, score-0.398]
</p><p>21 In Section 2 we describe our version of the two-stream model of cue-invariant responses, which is based on feedback from complex cells to simple cells. [sent-37, score-0.792]
</p><p>22 In Section 3 we formulate an unsupervised learning rule for learning these feedback connections. [sent-38, score-0.568]
</p><p>23 We apply our learning rule to natural image data, and show that this results in the emergence of connections that give rise to cue-invariant responses at both simple- and complex-cell levels. [sent-39, score-0.482]
</p><p>24 2  A model of cue-invariant responses  The most prominent model of cue-invariant responses introduced in previous research is the two-stream model (see, e. [sent-41, score-0.45]
</p><p>25 In this research we have extended this model so that it can be applied directly to model the cue-invariant responses of simple and complex cells. [sent-44, score-0.336]
</p><p>26 (a) The feedforward ﬁlter (Gabor function [10]) of a high-frequency ﬁrst-order simple cell; the ﬁlter has size 19 × 19 pixels, which is the size of the image data in our experiments. [sent-47, score-0.482]
</p><p>27 (b) The feedforward ﬁlter of another ﬁrst-order simple cell. [sent-48, score-0.348]
</p><p>28 This feedforward ﬁlter is otherwise similar to the one in (a), except that there is a phase difference of π/2 between the two; together, the feedforward ﬁlters in (a) and (b) are used to implement an energy model of a complex cell. [sent-49, score-0.834]
</p><p>29 (c) A lattice of size 7 × 7 of high-frequency ﬁlters of the type shown in (a); these ﬁlters are otherwise identical, except that their spatial locations vary. [sent-50, score-0.209]
</p><p>30 Together, the lattices shown in (c) and (d) are used to implement a 7 × 7 lattice of energy-model complex cells with different spatial positions; the output of this lattice is the feedback relayed to the low-frequency cueinvariant cells. [sent-52, score-1.176]
</p><p>31 (g) A feedback ﬁlter of size 7 × 7 for the simple cell whose feedforward ﬁlter is shown in (e); in order to avoid confusion between feedforward ﬁlters and feedback ﬁlters, the latter are visualized as lattices of slightly rounded rectangles. [sent-54, score-1.847]
</p><p>32 (h) A feedback ﬁlter for the simple cell whose feedforward ﬁlter is shown in (f). [sent-55, score-0.948]
</p><p>33 The feedback ﬁlters in (g) and (h) have been obtained by applying the learning algorithm introduced in this paper (see Section 3 for details). [sent-56, score-0.492]
</p><p>34 models of simple cells and energy models of complex cells [10], and a feedback path from the complex-cell level to the simple-cell level. [sent-57, score-1.04]
</p><p>35 This feedback path introduces a second, nonlinear input stream to cue-invariant cells, and gives rise to cue-invariant responses in these cells. [sent-58, score-0.87]
</p><p>36 To avoid confusion between the two types of ﬁlters – one type operating on the input image and the other on the feedback – we will use the term ’feedforward ﬁlter’ for the former and the term ’feedback ﬁlter’ for the latter. [sent-59, score-0.681]
</p><p>37 Figure 2 shows the feedforward and feedback ﬁlters of a concrete instance (implementation) of our model. [sent-60, score-0.811]
</p><p>38 Gabor functions [10] are used to model simple-cell feedforward ﬁlters. [sent-61, score-0.345]
</p><p>39 Figure 3 illustrates the design of higher-order gratings, and shows how the complex-cell lattice of the model transforms higher-order cues into feedback activity patterns that resemble corresponding ﬁrst-order cues. [sent-62, score-0.895]
</p><p>40 These measurements show that our model possesses the fundamental cueinvariant response properties: in our model, a cue-invariant neuron has similar selectivity to the orientation, frequency and phase of a grating stimulus, regardless of cue type (see ﬁgure caption for details). [sent-64, score-0.923]
</p><p>41 We now proceed to show how the feedback ﬁlters of our model (Figures 2g and h) can be learned from natural image data. [sent-65, score-0.799]
</p><p>42 1  Learning feedback connections in an unsupervised manner The objective function and the learning algorithm  In this section we introduce an unsupervised algorithm for learning feedback connection weights from complex cells to simple cells. [sent-67, score-1.457]
</p><p>43 Design of grating stimuli: Each row illustrates how, for a particular cue, a grating stimulus is composed of sinusoidal constituents; the equation of each stimulus (B, G, K) as a function of the constituents is shown under the stimulus. [sent-69, score-0.48]
</p><p>44 Note that the orientation, frequency and phase of each grating is determined by the ﬁrst sinusoidal constituent (A, D, I); here these parameters are the same for all stimuli. [sent-70, score-0.31]
</p><p>45 Feedback activity: The rightmost column shows the feedback activity – that is, response of the complex-cell lattice (see Figures 2c and d) – for the three types of stimuli. [sent-72, score-0.927]
</p><p>46 (C) There is no response to the luminance stimuli, since the orientation and frequency of the stimulus are different from those of the high-frequency feedforward ﬁlters. [sent-73, score-0.953]
</p><p>47 (H, L) For other cue types, the lattice detects the locations of energy of the vertical high-frequency constituent (E, J), thereby resulting in feedback activity that has a spatial pattern similar to a corresponding luminance pattern (A). [sent-74, score-1.27]
</p><p>48 Thus, the complex-cell lattice transforms higherorder cues into activity patterns that resemble ﬁrst-order cues, and these can subsequently produce a strong response in a feedback ﬁlter (compare (H) and (L) with the feedback ﬁlter in Figure 2g). [sent-75, score-1.649]
</p><p>49 was shown in Figure 4, these feedback ﬁlters give rise to cue-invariant response properties. [sent-77, score-0.734]
</p><p>50 The intuitive idea behind the learning algorithm is the following: in natural images, higherorder cues tend to coincide with ﬁrst-order cues. [sent-78, score-0.345]
</p><p>51 For example, when two different textures are adjacent, there is often also a luminance border between them; two examples of this phenomenon are shown in Figure 5. [sent-79, score-0.214]
</p><p>52 Therefore, cue-invariant response properties could be a result of learning in which large responses in the feedforward channel (ﬁrst-order responses) have become associated with large responses in the feedback channel (higherorder responses). [sent-80, score-1.437]
</p><p>53 Let vector c(n) = T [c1 (n) c2 (n) · · · cK (n)] denote the responses of a set of K ﬁrst-order high-frequency complex cells for the input image with index n. [sent-85, score-0.565]
</p><p>54 In our case the number of these complex cells is K = 7 × 7 = 49 (see Figures 2c and d), so the dimension of this vector is 49. [sent-86, score-0.245]
</p><p>55 This vectorization can be done in a standard manner [15] by scanning values from the 2D lattice column-wise into a vector; when the learned feedback ﬁlter is visualized, the ﬁlter is “unvectorized” with a reverse procedure. [sent-87, score-0.71]
</p><p>56 Let s(n) denote the response of a single low-  π/2  cue orientation  response  0 0 0. [sent-88, score-0.721]
</p><p>57 3 cue frequency  π  0 cue phase  2π  π/2  cue orientation  standard simple cell (without feedback) 1 0 0  1 0 0 0. [sent-91, score-1.009]
</p><p>58 3 cue frequency  π/2  π  cue orientation  F 1 0  0 0. [sent-94, score-0.606]
</p><p>59 3 cue frequency  I 1 0 π  0  2π  cue phase  J response  0  π  H 1 0 −1  response  response  G  1 0 −1 π 0 cue phase  2π  K 0. [sent-97, score-1.412]
</p><p>60 3 carrier frequency  response  L response  0  E  1  response  0  π  C  1  response  response  0  D response  B  1  response  response  A  cue-invariant complex cell (with feedback)  response  cue-invariant simple cell (with feedback)  0. [sent-102, score-2.303]
</p><p>61 3 carrier frequency  Figure 4: Our model fulﬁlls the fundamental properties of cue-invariant responses. [sent-107, score-0.279]
</p><p>62 The plots show tuning curves for a cue-invariant simple cell – corresponding to the ﬁlters of Figures 2e and g – and complex cell of our new model (two leftmost columns), and a standard simple-cell model without feedback processing (rightmost column). [sent-108, score-0.858]
</p><p>63 Solid lines show responses to luminance-deﬁned gratings (Figure 3B), dotted lines show responses to texture-deﬁned gratings (Figure 3G), and dashed lines show responses to contrast-deﬁned gratings (Figure 3K). [sent-109, score-0.897]
</p><p>64 (A–I) In our model, a neuron has similar selectivity to the orientation, frequency and phase of a grating stimulus, regardless of cue type; in contrast, a standard simple-cell model, without the feedback path, is only selective to the parameters of a luminance-deﬁned grating. [sent-110, score-1.064]
</p><p>65 The preferred frequency is lower for higher-order gratings than for ﬁrst-order gratings; similar observations have been made in single-cell recordings [4]. [sent-111, score-0.225]
</p><p>66 (J–M) In our model, the neurons are also selective to the orientation and frequency of the carrier (Figure 3J) of a contrast-deﬁned grating (Figure 3K), thus conforming with single-cell recordings [1]. [sent-112, score-0.528]
</p><p>67 Note that these measurements were made with the feedback ﬁlters learned by our unsupervised algorithm (see Section 3); thus, these measurements conﬁrm that learning results in cue-invariant response properties. [sent-113, score-0.909]
</p><p>68 Image in (A) contains a near-vertical luminance boundary across the image; the boundary in (B) is nearhorizontal. [sent-115, score-0.32]
</p><p>69 In both (A) and (B), texture is different on different sides of the luminance border. [sent-116, score-0.244]
</p><p>70 )  B  C  D  E  F  A  G  H  I  J  Figure 6: (A-D, F-I) Feedback ﬁlters (top row) learned from natural image data by using our unsupervised learning algorithm; the bottom row shows the corresponding feedforward ﬁlters. [sent-118, score-0.676]
</p><p>71 For a quantitative evaluation of the cue-invariant response properties resulting from the learned ﬁlters (A) and (B), see Figure 4. [sent-119, score-0.336]
</p><p>72 (E, J) The result of a control experiment, in which Gaussian white noise was used as input data; (J) shows the feedforward ﬁlter used in this control experiment. [sent-120, score-0.319]
</p><p>73 frequency simple cell for the input image with index n. [sent-121, score-0.339]
</p><p>74 In our learning algorithm all the feedforward ﬁlters are ﬁxed and only a feedback ﬁlter is learned; this means that c(n) and s(n) can be computed for all n (all images) prior to applying the learning algorithm. [sent-122, score-0.811]
</p><p>75 Let us denote the K-dimensional feedback ﬁlter with w; this ﬁlter is learned by our algorithm. [sent-123, score-0.546]
</p><p>76 Let b(n) = w T c(n), that is, b(n) is the signal obtained when the feedback activity from the complex-cell lattice is ﬁltered with the feedback ﬁlter; the overall activity of a cueinvariant simple cell is then s(n) + b(n). [sent-124, score-1.48]
</p><p>77 To keep the output of the feedback ﬁlter b(n) bounded, we enforce a unit energy constraint on b(n), leading into constraint T  2  h(w) = E b2 (n) = wT E c(n)c(n)T w = wT Cw = 1,  (2)  where C = E c(n)c(n) is also positive-semideﬁnite and can be computed prior to learning. [sent-126, score-0.59]
</p><p>78 2  Experiments  The algorithm described above was applied to natural image data, which was sampled from a set of over 4,000 natural images [8]. [sent-144, score-0.372]
</p><p>79 Simple-cell feedforward responses s(n) were computed using the ﬁlter shown in Figure 2e, and the set of high-frequency complex-cell lattice activities c(n) was computed using the ﬁlters shown in Figures 2c and d. [sent-147, score-0.645]
</p><p>80 This preprocessing tends to weaken contrast borders, implying that in our experiments, learning higher-order responses is mostly based on texture boundaries that coincide with luminance boundaries. [sent-149, score-0.548]
</p><p>81 It should be noted, however, that in spite of this preprocessing step, the resulting feedback ﬁlters produce cue-invariant responses to both texture- and contrast-deﬁned cues (see Figure 4). [sent-150, score-0.866]
</p><p>82 In order to make the components of c(n) have zero mean, and focus on the structure of feedback activity patterns instead of overall constant activation, the local mean (DC component) was removed from each c(n). [sent-151, score-0.558]
</p><p>83 The resulting feedback ﬁlter is shown in Figure 6A (see also Figure 2g). [sent-156, score-0.492]
</p><p>84 Data sampling, preprocessing and the learning algorithm were then repeated, but this time using the feedforward ﬁlter shown in Figure 2f; the feedback ﬁlter obtained from this run is shown in Figure 6B (see also Figure 2h). [sent-157, score-0.857]
</p><p>85 The measurements in Figure 4 show that these feedback ﬁlters result in cueinvariant response properties at both simple- and complex-cell levels. [sent-158, score-0.877]
</p><p>86 Thus, our unsupervised algorithm learns cue-invariant response properties from natural image data. [sent-159, score-0.557]
</p><p>87 This veriﬁes that our original results do reﬂect the statistics of natural image data. [sent-164, score-0.227]
</p><p>88 4  Conclusions  This paper has shown that cue-invariant response properties can be learned from natural image data in an unsupervised manner. [sent-165, score-0.611]
</p><p>89 The results were based on a model in which there is a feedback path from complex cells to simple cells, and an unsupervised algorithm which maximizes the correlation of the energies of the feedforward and ﬁltered feedback signals. [sent-166, score-1.753]
</p><p>90 The intuitive idea behind the algorithm is that in natural visual stimuli, higher-order cues tend to coincide with ﬁrst-order cues. [sent-167, score-0.364]
</p><p>91 Simulations were performed to validate that the learned feedback ﬁlters give rise to in cue-invariant response properties. [sent-168, score-0.788]
</p><p>92 First, for the ﬁrst time it has been shown that cue-invariant response properties of simple and complex cells emerge from the statistical properties of natural images. [sent-170, score-0.676]
</p><p>93 Second, our results suggest that cue invariance can result from feedback from complex cells to simple cells; no feedback from higher cortical areas would thus be needed. [sent-171, score-1.531]
</p><p>94 Third, our research demonstrates how higher-order feature detectors can be learned from natural data in an unsupervised manner; this is an important step towards general-purpose data-driven approaches to image processing and computer vision. [sent-172, score-0.381]
</p><p>95 A processsing stream in mammalian visual cortex neurons for non-Fourier responses. [sent-184, score-0.294]
</p><p>96 Temporal and spatial response to second-order stimuli in cat area 18. [sent-198, score-0.348]
</p><p>97 Physiological responses of New World monkey V1 neurons to stimuli deﬁned by coherent motion. [sent-207, score-0.289]
</p><p>98 Independent component ﬁlters of natural images compared with simple cells in primary visual cortex. [sent-225, score-0.41]
</p><p>99 A two-layer sparse coding model learns simple and complex cell receptive ﬁelds and topography from natural images. [sent-231, score-0.358]
</p><p>100 Temporal and spatiotemporal coherence in simple-cell responses: a generative model of natural image sequences. [sent-253, score-0.305]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('feedback', 0.492), ('feedforward', 0.319), ('lters', 0.245), ('lter', 0.22), ('cue', 0.215), ('response', 0.199), ('responses', 0.186), ('luminance', 0.182), ('cells', 0.176), ('cues', 0.142), ('lattice', 0.14), ('image', 0.134), ('carrier', 0.13), ('gratings', 0.113), ('grating', 0.112), ('cell', 0.108), ('orientation', 0.108), ('stream', 0.101), ('figures', 0.096), ('natural', 0.093), ('cueinvariant', 0.087), ('hyv', 0.086), ('visual', 0.084), ('stimulus', 0.077), ('unsupervised', 0.076), ('stimuli', 0.071), ('boundary', 0.069), ('complex', 0.069), ('frequency', 0.068), ('ltered', 0.067), ('activity', 0.066), ('higherorder', 0.065), ('hurri', 0.065), ('texture', 0.062), ('constituents', 0.057), ('properties', 0.055), ('learned', 0.054), ('rinen', 0.052), ('selectivity', 0.052), ('phase', 0.051), ('energy', 0.05), ('mammalian', 0.048), ('path', 0.048), ('wt', 0.048), ('preprocessing', 0.046), ('sinusoidal', 0.045), ('gabor', 0.045), ('coincide', 0.045), ('recordings', 0.044), ('measurements', 0.044), ('attenuate', 0.043), ('rise', 0.043), ('baker', 0.041), ('spatial', 0.04), ('regardless', 0.04), ('cat', 0.038), ('mareschal', 0.038), ('helsinki', 0.038), ('opt', 0.038), ('constituent', 0.034), ('finland', 0.034), ('selective', 0.034), ('receptive', 0.033), ('neurons', 0.032), ('pseudoinverse', 0.032), ('textures', 0.032), ('lattices', 0.032), ('cortical', 0.032), ('responds', 0.03), ('visualized', 0.03), ('rightmost', 0.03), ('sub', 0.03), ('type', 0.029), ('cortex', 0.029), ('simple', 0.029), ('filters', 0.029), ('resemble', 0.029), ('images', 0.028), ('quantitative', 0.028), ('contrast', 0.027), ('scene', 0.027), ('model', 0.026), ('spatiotemporal', 0.026), ('energies', 0.026), ('emergence', 0.026), ('coherence', 0.026), ('invariance', 0.026), ('confusion', 0.026), ('thereby', 0.026), ('dc', 0.025), ('prefer', 0.025), ('vertical', 0.025), ('manner', 0.024), ('sampled', 0.024), ('detectors', 0.024), ('paths', 0.024), ('subsequently', 0.024), ('constraint', 0.024), ('neuroscience', 0.023), ('objective', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="109-tfidf-1" href="./nips-2005-Learning_Cue-Invariant_Visual_Responses.html">109 nips-2005-Learning Cue-Invariant Visual Responses</a></p>
<p>Author: Jarmo Hurri</p><p>Abstract: Multiple visual cues are used by the visual system to analyze a scene; achromatic cues include luminance, texture, contrast and motion. Singlecell recordings have shown that the mammalian visual cortex contains neurons that respond similarly to scene structure (e.g., orientation of a boundary), regardless of the cue type conveying this information. This paper shows that cue-invariant response properties of simple- and complex-type cells can be learned from natural image data in an unsupervised manner. In order to do this, we also extend a previous conceptual model of cue invariance so that it can be applied to model simple- and complex-cell responses. Our results relate cue-invariant response properties to natural image statistics, thereby showing how the statistical modeling approach can be used to model processing beyond the elemental response properties visual neurons. This work also demonstrates how to learn, from natural image data, more sophisticated feature detectors than those based on changes in mean luminance, thereby paving the way for new data-driven approaches to image processing and computer vision. 1</p><p>2 0.26302105 <a title="109-tfidf-2" href="./nips-2005-Principles_of_real-time_computing_with_feedback_applied_to_cortical_microcircuit_models.html">157 nips-2005-Principles of real-time computing with feedback applied to cortical microcircuit models</a></p>
<p>Author: Wolfgang Maass, Prashant Joshi, Eduardo D. Sontag</p><p>Abstract: The network topology of neurons in the brain exhibits an abundance of feedback connections, but the computational function of these feedback connections is largely unknown. We present a computational theory that characterizes the gain in computational power achieved through feedback in dynamical systems with fading memory. It implies that many such systems acquire through feedback universal computational capabilities for analog computing with a non-fading memory. In particular, we show that feedback enables such systems to process time-varying input streams in diverse ways according to rules that are implemented through internal states of the dynamical system. In contrast to previous attractor-based computational models for neural networks, these ﬂexible internal states are high-dimensional attractors of the circuit dynamics, that still allow the circuit state to absorb new information from online input streams. In this way one arrives at novel models for working memory, integration of evidence, and reward expectation in cortical circuits. We show that they are applicable to circuits of conductance-based Hodgkin-Huxley (HH) neurons with high levels of noise that reﬂect experimental data on invivo conditions. 1</p><p>3 0.21197939 <a title="109-tfidf-3" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<p>Author: Inna Weiner, Tomer Hertz, Israel Nelken, Daphna Weinshall</p><p>Abstract: We present a novel approach to the characterization of complex sensory neurons. One of the main goals of characterizing sensory neurons is to characterize dimensions in stimulus space to which the neurons are highly sensitive (causing large gradients in the neural responses) or alternatively dimensions in stimulus space to which the neuronal response are invariant (deﬁning iso-response manifolds). We formulate this problem as that of learning a geometry on stimulus space that is compatible with the neural responses: the distance between stimuli should be large when the responses they evoke are very different, and small when the responses they evoke are similar. Here we show how to successfully train such distance functions using rather limited amount of information. The data consisted of the responses of neurons in primary auditory cortex (A1) of anesthetized cats to 32 stimuli derived from natural sounds. For each neuron, a subset of all pairs of stimuli was selected such that the responses of the two stimuli in a pair were either very similar or very dissimilar. The distance function was trained to ﬁt these constraints. The resulting distance functions generalized to predict the distances between the responses of a test stimulus and the trained stimuli. 1</p><p>4 0.16980532 <a title="109-tfidf-4" href="./nips-2005-Is_Early_Vision_Optimized_for_Extracting_Higher-order_Dependencies%3F.html">101 nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: Linear implementations of the efﬁcient coding hypothesis, such as independent component analysis (ICA) and sparse coding models, have provided functional explanations for properties of simple cells in V1 [1, 2]. These models, however, ignore the non-linear behavior of neurons and fail to match individual and population properties of neural receptive ﬁelds in subtle but important ways. Hierarchical models, including Gaussian Scale Mixtures [3, 4] and other generative statistical models [5, 6], can capture higher-order regularities in natural images and explain nonlinear aspects of neural processing such as normalization and context effects [6,7]. Previously, it had been assumed that the lower level representation is independent of the hierarchy, and had been ﬁxed when training these models. Here we examine the optimal lower-level representations derived in the context of a hierarchical model and ﬁnd that the resulting representations are strikingly different from those based on linear models. Unlike the the basis functions and ﬁlters learned by ICA or sparse coding, these functions individually more closely resemble simple cell receptive ﬁelds and collectively span a broad range of spatial scales. Our work uniﬁes several related approaches and observations about natural image structure and suggests that hierarchical models might yield better representations of image structure throughout the hierarchy.</p><p>5 0.12469267 <a title="109-tfidf-5" href="./nips-2005-On_the_Accuracy_of_Bounded_Rationality%3A_How_Far_from_Optimal_Is_Fast_and_Frugal%3F.html">146 nips-2005-On the Accuracy of Bounded Rationality: How Far from Optimal Is Fast and Frugal?</a></p>
<p>Author: Michael Schmitt, Laura Martignon</p><p>Abstract: Fast and frugal heuristics are well studied models of bounded rationality. Psychological research has proposed the take-the-best heuristic as a successful strategy in decision making with limited resources. Take-thebest searches for a sufﬁciently good ordering of cues (features) in a task where objects are to be compared lexicographically. We investigate the complexity of the problem of approximating optimal cue permutations for lexicographic strategies. We show that no efﬁcient algorithm can approximate the optimum to within any constant factor, if P = NP. We further consider a greedy approach for building lexicographic strategies and derive tight bounds for the performance ratio of a new and simple algorithm. This algorithm is proven to perform better than take-the-best. 1</p><p>6 0.10861152 <a title="109-tfidf-6" href="./nips-2005-Optimal_cue_selection_strategy.html">149 nips-2005-Optimal cue selection strategy</a></p>
<p>7 0.10434391 <a title="109-tfidf-7" href="./nips-2005-Optimizing_spatio-temporal_filters_for_improving_Brain-Computer_Interfacing.html">150 nips-2005-Optimizing spatio-temporal filters for improving Brain-Computer Interfacing</a></p>
<p>8 0.10288386 <a title="109-tfidf-8" href="./nips-2005-Neural_mechanisms_of_contrast_dependent_receptive_field_size_in_V1.html">134 nips-2005-Neural mechanisms of contrast dependent receptive field size in V1</a></p>
<p>9 0.095770977 <a title="109-tfidf-9" href="./nips-2005-Scaling_Laws_in_Natural_Scenes_and_the_Inference_of_3D_Shape.html">170 nips-2005-Scaling Laws in Natural Scenes and the Inference of 3D Shape</a></p>
<p>10 0.091979221 <a title="109-tfidf-10" href="./nips-2005-Norepinephrine_and_Neural_Interrupts.html">141 nips-2005-Norepinephrine and Neural Interrupts</a></p>
<p>11 0.091161594 <a title="109-tfidf-11" href="./nips-2005-Identifying_Distributed_Object_Representations_in_Human_Extrastriate_Visual_Cortex.html">94 nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</a></p>
<p>12 0.088282302 <a title="109-tfidf-12" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>13 0.082717218 <a title="109-tfidf-13" href="./nips-2005-Visual_Encoding_with_Jittering_Eyes.html">203 nips-2005-Visual Encoding with Jittering Eyes</a></p>
<p>14 0.077560559 <a title="109-tfidf-14" href="./nips-2005-An_exploration-exploitation_model_based_on_norepinepherine_and_dopamine_activity.html">26 nips-2005-An exploration-exploitation model based on norepinepherine and dopamine activity</a></p>
<p>15 0.077159368 <a title="109-tfidf-15" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>16 0.071338654 <a title="109-tfidf-16" href="./nips-2005-Learning_Depth_from_Single_Monocular_Images.html">110 nips-2005-Learning Depth from Single Monocular Images</a></p>
<p>17 0.070539616 <a title="109-tfidf-17" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<p>18 0.065606259 <a title="109-tfidf-18" href="./nips-2005-Sensory_Adaptation_within_a_Bayesian_Framework_for_Perception.html">173 nips-2005-Sensory Adaptation within a Bayesian Framework for Perception</a></p>
<p>19 0.063453265 <a title="109-tfidf-19" href="./nips-2005-An_aVLSI_Cricket_Ear_Model.html">25 nips-2005-An aVLSI Cricket Ear Model</a></p>
<p>20 0.058542505 <a title="109-tfidf-20" href="./nips-2005-Representing_Part-Whole_Relationships_in_Recurrent_Neural_Networks.html">164 nips-2005-Representing Part-Whole Relationships in Recurrent Neural Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.182), (1, -0.183), (2, -0.017), (3, 0.197), (4, -0.059), (5, 0.141), (6, -0.094), (7, -0.178), (8, -0.001), (9, 0.04), (10, -0.035), (11, -0.022), (12, -0.017), (13, -0.18), (14, -0.087), (15, 0.042), (16, 0.275), (17, -0.036), (18, -0.117), (19, 0.102), (20, 0.084), (21, 0.148), (22, -0.11), (23, -0.103), (24, 0.054), (25, -0.121), (26, 0.085), (27, -0.001), (28, 0.041), (29, 0.079), (30, -0.162), (31, 0.071), (32, 0.232), (33, -0.036), (34, -0.019), (35, 0.061), (36, -0.006), (37, 0.038), (38, -0.042), (39, -0.108), (40, -0.076), (41, -0.026), (42, 0.015), (43, -0.045), (44, -0.011), (45, 0.104), (46, 0.058), (47, -0.038), (48, 0.062), (49, -0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97939998 <a title="109-lsi-1" href="./nips-2005-Learning_Cue-Invariant_Visual_Responses.html">109 nips-2005-Learning Cue-Invariant Visual Responses</a></p>
<p>Author: Jarmo Hurri</p><p>Abstract: Multiple visual cues are used by the visual system to analyze a scene; achromatic cues include luminance, texture, contrast and motion. Singlecell recordings have shown that the mammalian visual cortex contains neurons that respond similarly to scene structure (e.g., orientation of a boundary), regardless of the cue type conveying this information. This paper shows that cue-invariant response properties of simple- and complex-type cells can be learned from natural image data in an unsupervised manner. In order to do this, we also extend a previous conceptual model of cue invariance so that it can be applied to model simple- and complex-cell responses. Our results relate cue-invariant response properties to natural image statistics, thereby showing how the statistical modeling approach can be used to model processing beyond the elemental response properties visual neurons. This work also demonstrates how to learn, from natural image data, more sophisticated feature detectors than those based on changes in mean luminance, thereby paving the way for new data-driven approaches to image processing and computer vision. 1</p><p>2 0.72052473 <a title="109-lsi-2" href="./nips-2005-Principles_of_real-time_computing_with_feedback_applied_to_cortical_microcircuit_models.html">157 nips-2005-Principles of real-time computing with feedback applied to cortical microcircuit models</a></p>
<p>Author: Wolfgang Maass, Prashant Joshi, Eduardo D. Sontag</p><p>Abstract: The network topology of neurons in the brain exhibits an abundance of feedback connections, but the computational function of these feedback connections is largely unknown. We present a computational theory that characterizes the gain in computational power achieved through feedback in dynamical systems with fading memory. It implies that many such systems acquire through feedback universal computational capabilities for analog computing with a non-fading memory. In particular, we show that feedback enables such systems to process time-varying input streams in diverse ways according to rules that are implemented through internal states of the dynamical system. In contrast to previous attractor-based computational models for neural networks, these ﬂexible internal states are high-dimensional attractors of the circuit dynamics, that still allow the circuit state to absorb new information from online input streams. In this way one arrives at novel models for working memory, integration of evidence, and reward expectation in cortical circuits. We show that they are applicable to circuits of conductance-based Hodgkin-Huxley (HH) neurons with high levels of noise that reﬂect experimental data on invivo conditions. 1</p><p>3 0.67186522 <a title="109-lsi-3" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<p>Author: Inna Weiner, Tomer Hertz, Israel Nelken, Daphna Weinshall</p><p>Abstract: We present a novel approach to the characterization of complex sensory neurons. One of the main goals of characterizing sensory neurons is to characterize dimensions in stimulus space to which the neurons are highly sensitive (causing large gradients in the neural responses) or alternatively dimensions in stimulus space to which the neuronal response are invariant (deﬁning iso-response manifolds). We formulate this problem as that of learning a geometry on stimulus space that is compatible with the neural responses: the distance between stimuli should be large when the responses they evoke are very different, and small when the responses they evoke are similar. Here we show how to successfully train such distance functions using rather limited amount of information. The data consisted of the responses of neurons in primary auditory cortex (A1) of anesthetized cats to 32 stimuli derived from natural sounds. For each neuron, a subset of all pairs of stimuli was selected such that the responses of the two stimuli in a pair were either very similar or very dissimilar. The distance function was trained to ﬁt these constraints. The resulting distance functions generalized to predict the distances between the responses of a test stimulus and the trained stimuli. 1</p><p>4 0.5777998 <a title="109-lsi-4" href="./nips-2005-Visual_Encoding_with_Jittering_Eyes.html">203 nips-2005-Visual Encoding with Jittering Eyes</a></p>
<p>Author: Michele Rucci</p><p>Abstract: Under natural viewing conditions, small movements of the eye and body prevent the maintenance of a steady direction of gaze. It is known that stimuli tend to fade when they are stabilized on the retina for several seconds. However, it is unclear whether the physiological self-motion of the retinal image serves a visual purpose during the brief periods of natural visual ﬁxation. This study examines the impact of ﬁxational instability on the statistics of visual input to the retina and on the structure of neural activity in the early visual system. Fixational instability introduces ﬂuctuations in the retinal input signals that, in the presence of natural images, lack spatial correlations. These input ﬂuctuations strongly inﬂuence neural activity in a model of the LGN. They decorrelate cell responses, even if the contrast sensitivity functions of simulated cells are not perfectly tuned to counter-balance the power-law spectrum of natural images. A decorrelation of neural activity has been proposed to be beneﬁcial for discarding statistical redundancies in the input signals. Fixational instability might, therefore, contribute to establishing efﬁcient representations of natural stimuli. 1</p><p>5 0.57402438 <a title="109-lsi-5" href="./nips-2005-On_the_Accuracy_of_Bounded_Rationality%3A_How_Far_from_Optimal_Is_Fast_and_Frugal%3F.html">146 nips-2005-On the Accuracy of Bounded Rationality: How Far from Optimal Is Fast and Frugal?</a></p>
<p>Author: Michael Schmitt, Laura Martignon</p><p>Abstract: Fast and frugal heuristics are well studied models of bounded rationality. Psychological research has proposed the take-the-best heuristic as a successful strategy in decision making with limited resources. Take-thebest searches for a sufﬁciently good ordering of cues (features) in a task where objects are to be compared lexicographically. We investigate the complexity of the problem of approximating optimal cue permutations for lexicographic strategies. We show that no efﬁcient algorithm can approximate the optimum to within any constant factor, if P = NP. We further consider a greedy approach for building lexicographic strategies and derive tight bounds for the performance ratio of a new and simple algorithm. This algorithm is proven to perform better than take-the-best. 1</p><p>6 0.49180835 <a title="109-lsi-6" href="./nips-2005-Is_Early_Vision_Optimized_for_Extracting_Higher-order_Dependencies%3F.html">101 nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</a></p>
<p>7 0.47187057 <a title="109-lsi-7" href="./nips-2005-Identifying_Distributed_Object_Representations_in_Human_Extrastriate_Visual_Cortex.html">94 nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</a></p>
<p>8 0.45689023 <a title="109-lsi-8" href="./nips-2005-An_aVLSI_Cricket_Ear_Model.html">25 nips-2005-An aVLSI Cricket Ear Model</a></p>
<p>9 0.37764302 <a title="109-lsi-9" href="./nips-2005-Scaling_Laws_in_Natural_Scenes_and_the_Inference_of_3D_Shape.html">170 nips-2005-Scaling Laws in Natural Scenes and the Inference of 3D Shape</a></p>
<p>10 0.35948697 <a title="109-lsi-10" href="./nips-2005-Neural_mechanisms_of_contrast_dependent_receptive_field_size_in_V1.html">134 nips-2005-Neural mechanisms of contrast dependent receptive field size in V1</a></p>
<p>11 0.33468479 <a title="109-lsi-11" href="./nips-2005-CMOL_CrossNets%3A_Possible_Neuromorphic_Nanoelectronic_Circuits.html">40 nips-2005-CMOL CrossNets: Possible Neuromorphic Nanoelectronic Circuits</a></p>
<p>12 0.33152851 <a title="109-lsi-12" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<p>13 0.32234237 <a title="109-lsi-13" href="./nips-2005-Active_Bidirectional_Coupling_in_a_Cochlear_Chip.html">17 nips-2005-Active Bidirectional Coupling in a Cochlear Chip</a></p>
<p>14 0.32099774 <a title="109-lsi-14" href="./nips-2005-A_Cortically-Plausible_Inverse_Problem_Solving_Method_Applied_to_Recognizing_Static_and_Kinematic_3D_Objects.html">7 nips-2005-A Cortically-Plausible Inverse Problem Solving Method Applied to Recognizing Static and Kinematic 3D Objects</a></p>
<p>15 0.30283016 <a title="109-lsi-15" href="./nips-2005-Norepinephrine_and_Neural_Interrupts.html">141 nips-2005-Norepinephrine and Neural Interrupts</a></p>
<p>16 0.29714069 <a title="109-lsi-16" href="./nips-2005-Optimal_cue_selection_strategy.html">149 nips-2005-Optimal cue selection strategy</a></p>
<p>17 0.29189399 <a title="109-lsi-17" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>18 0.28126952 <a title="109-lsi-18" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>19 0.28079128 <a title="109-lsi-19" href="./nips-2005-Optimizing_spatio-temporal_filters_for_improving_Brain-Computer_Interfacing.html">150 nips-2005-Optimizing spatio-temporal filters for improving Brain-Computer Interfacing</a></p>
<p>20 0.27334329 <a title="109-lsi-20" href="./nips-2005-An_exploration-exploitation_model_based_on_norepinepherine_and_dopamine_activity.html">26 nips-2005-An exploration-exploitation model based on norepinepherine and dopamine activity</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.021), (10, 0.038), (12, 0.01), (27, 0.113), (31, 0.049), (34, 0.057), (39, 0.018), (55, 0.04), (57, 0.025), (60, 0.05), (69, 0.078), (73, 0.035), (75, 0.236), (88, 0.082), (91, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80538338 <a title="109-lda-1" href="./nips-2005-Learning_Cue-Invariant_Visual_Responses.html">109 nips-2005-Learning Cue-Invariant Visual Responses</a></p>
<p>Author: Jarmo Hurri</p><p>Abstract: Multiple visual cues are used by the visual system to analyze a scene; achromatic cues include luminance, texture, contrast and motion. Singlecell recordings have shown that the mammalian visual cortex contains neurons that respond similarly to scene structure (e.g., orientation of a boundary), regardless of the cue type conveying this information. This paper shows that cue-invariant response properties of simple- and complex-type cells can be learned from natural image data in an unsupervised manner. In order to do this, we also extend a previous conceptual model of cue invariance so that it can be applied to model simple- and complex-cell responses. Our results relate cue-invariant response properties to natural image statistics, thereby showing how the statistical modeling approach can be used to model processing beyond the elemental response properties visual neurons. This work also demonstrates how to learn, from natural image data, more sophisticated feature detectors than those based on changes in mean luminance, thereby paving the way for new data-driven approaches to image processing and computer vision. 1</p><p>2 0.63461405 <a title="109-lda-2" href="./nips-2005-Augmented_Rescorla-Wagner_and_Maximum_Likelihood_Estimation.html">32 nips-2005-Augmented Rescorla-Wagner and Maximum Likelihood Estimation</a></p>
<p>Author: Alan L. Yuille</p><p>Abstract: We show that linear generalizations of Rescorla-Wagner can perform Maximum Likelihood estimation of the parameters of all generative models for causal reasoning. Our approach involves augmenting variables to deal with conjunctions of causes, similar to the agumented model of Rescorla. Our results involve genericity assumptions on the distributions of causes. If these assumptions are violated, for example for the Cheng causal power theory, then we show that a linear Rescorla-Wagner can estimate the parameters of the model up to a nonlinear transformtion. Moreover, a nonlinear Rescorla-Wagner is able to estimate the parameters directly to within arbitrary accuracy. Previous results can be used to determine convergence and to estimate convergence rates. 1</p><p>3 0.58745754 <a title="109-lda-3" href="./nips-2005-Predicting_EMG_Data_from_M1_Neurons_with_Variational_Bayesian_Least_Squares.html">155 nips-2005-Predicting EMG Data from M1 Neurons with Variational Bayesian Least Squares</a></p>
<p>Author: Jo-anne Ting, Aaron D'souza, Kenji Yamamoto, Toshinori Yoshioka, Donna Hoffman, Shinji Kakei, Lauren Sergio, John Kalaska, Mitsuo Kawato</p><p>Abstract: An increasing number of projects in neuroscience requires the statistical analysis of high dimensional data sets, as, for instance, in predicting behavior from neural ﬁring or in operating artiﬁcial devices from brain recordings in brain-machine interfaces. Linear analysis techniques remain prevalent in such cases, but classical linear regression approaches are often numerically too fragile in high dimensions. In this paper, we address the question of whether EMG data collected from arm movements of monkeys can be faithfully reconstructed with linear approaches from neural activity in primary motor cortex (M1). To achieve robust data analysis, we develop a full Bayesian approach to linear regression that automatically detects and excludes irrelevant features in the data, regularizing against overﬁtting. In comparison with ordinary least squares, stepwise regression, partial least squares, LASSO regression and a brute force combinatorial search for the most predictive input features in the data, we demonstrate that the new Bayesian method oﬀers a superior mixture of characteristics in terms of regularization against overﬁtting, computational eﬃciency and ease of use, demonstrating its potential as a drop-in replacement for other linear regression techniques. As neuroscientiﬁc results, our analyses demonstrate that EMG data can be well predicted from M1 neurons, further opening the path for possible real-time interfaces between brains and machines. 1</p><p>4 0.57308811 <a title="109-lda-4" href="./nips-2005-Goal-Based_Imitation_as_Probabilistic_Inference_over_Graphical_Models.html">87 nips-2005-Goal-Based Imitation as Probabilistic Inference over Graphical Models</a></p>
<p>Author: Deepak Verma, Rajesh P. Rao</p><p>Abstract: Humans are extremely adept at learning new skills by imitating the actions of others. A progression of imitative abilities has been observed in children, ranging from imitation of simple body movements to goalbased imitation based on inferring intent. In this paper, we show that the problem of goal-based imitation can be formulated as one of inferring goals and selecting actions using a learned probabilistic graphical model of the environment. We ﬁrst describe algorithms for planning actions to achieve a goal state using probabilistic inference. We then describe how planning can be used to bootstrap the learning of goal-dependent policies by utilizing feedback from the environment. The resulting graphical model is then shown to be powerful enough to allow goal-based imitation. Using a simple maze navigation task, we illustrate how an agent can infer the goals of an observed teacher and imitate the teacher even when the goals are uncertain and the demonstration is incomplete.</p><p>5 0.56745982 <a title="109-lda-5" href="./nips-2005-Is_Early_Vision_Optimized_for_Extracting_Higher-order_Dependencies%3F.html">101 nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: Linear implementations of the efﬁcient coding hypothesis, such as independent component analysis (ICA) and sparse coding models, have provided functional explanations for properties of simple cells in V1 [1, 2]. These models, however, ignore the non-linear behavior of neurons and fail to match individual and population properties of neural receptive ﬁelds in subtle but important ways. Hierarchical models, including Gaussian Scale Mixtures [3, 4] and other generative statistical models [5, 6], can capture higher-order regularities in natural images and explain nonlinear aspects of neural processing such as normalization and context effects [6,7]. Previously, it had been assumed that the lower level representation is independent of the hierarchy, and had been ﬁxed when training these models. Here we examine the optimal lower-level representations derived in the context of a hierarchical model and ﬁnd that the resulting representations are strikingly different from those based on linear models. Unlike the the basis functions and ﬁlters learned by ICA or sparse coding, these functions individually more closely resemble simple cell receptive ﬁelds and collectively span a broad range of spatial scales. Our work uniﬁes several related approaches and observations about natural image structure and suggests that hierarchical models might yield better representations of image structure throughout the hierarchy.</p><p>6 0.55619806 <a title="109-lda-6" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>7 0.54769337 <a title="109-lda-7" href="./nips-2005-Subsequence_Kernels_for_Relation_Extraction.html">185 nips-2005-Subsequence Kernels for Relation Extraction</a></p>
<p>8 0.53978544 <a title="109-lda-8" href="./nips-2005-Bayesian_models_of_human_action_understanding.html">36 nips-2005-Bayesian models of human action understanding</a></p>
<p>9 0.53061765 <a title="109-lda-9" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>10 0.52876854 <a title="109-lda-10" href="./nips-2005-Analyzing_Coupled_Brain_Sources%3A_Distinguishing_True_from_Spurious_Interaction.html">29 nips-2005-Analyzing Coupled Brain Sources: Distinguishing True from Spurious Interaction</a></p>
<p>11 0.52686906 <a title="109-lda-11" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<p>12 0.52560335 <a title="109-lda-12" href="./nips-2005-Context_as_Filtering.html">48 nips-2005-Context as Filtering</a></p>
<p>13 0.52549994 <a title="109-lda-13" href="./nips-2005-Interpolating_between_types_and_tokens_by_estimating_power-law_generators.html">100 nips-2005-Interpolating between types and tokens by estimating power-law generators</a></p>
<p>14 0.52463377 <a title="109-lda-14" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<p>15 0.5235126 <a title="109-lda-15" href="./nips-2005-Policy-Gradient_Methods_for_Planning.html">153 nips-2005-Policy-Gradient Methods for Planning</a></p>
<p>16 0.5206663 <a title="109-lda-16" href="./nips-2005-Visual_Encoding_with_Jittering_Eyes.html">203 nips-2005-Visual Encoding with Jittering Eyes</a></p>
<p>17 0.52050936 <a title="109-lda-17" href="./nips-2005-Conditional_Visual_Tracking_in_Kernel_Space.html">45 nips-2005-Conditional Visual Tracking in Kernel Space</a></p>
<p>18 0.52004677 <a title="109-lda-18" href="./nips-2005-Sensory_Adaptation_within_a_Bayesian_Framework_for_Perception.html">173 nips-2005-Sensory Adaptation within a Bayesian Framework for Perception</a></p>
<p>19 0.5192185 <a title="109-lda-19" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>20 0.51899695 <a title="109-lda-20" href="./nips-2005-Spiking_Inputs_to_a_Winner-take-all_Network.html">181 nips-2005-Spiking Inputs to a Winner-take-all Network</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
