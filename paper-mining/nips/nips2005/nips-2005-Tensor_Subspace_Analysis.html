<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>189 nips-2005-Tensor Subspace Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-189" href="#">nips2005-189</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>189 nips-2005-Tensor Subspace Analysis</h1>
<br/><p>Source: <a title="nips-2005-189-pdf" href="http://papers.nips.cc/paper/2899-tensor-subspace-analysis.pdf">pdf</a></p><p>Author: Xiaofei He, Deng Cai, Partha Niyogi</p><p>Abstract: Previous work has demonstrated that the image variations of many objects (human faces in particular) under variable lighting can be effectively modeled by low dimensional linear spaces. The typical linear subspace learning algorithms include Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and Locality Preserving Projection (LPP). All of these methods consider an n1 × n2 image as a high dimensional vector in Rn1 ×n2 , while an image represented in the plane is intrinsically a matrix. In this paper, we propose a new algorithm called Tensor Subspace Analysis (TSA). TSA considers an image as the second order tensor in Rn1 ⊗ Rn2 , where Rn1 and Rn2 are two vector spaces. The relationship between the column vectors of the image matrix and that between the row vectors can be naturally characterized by TSA. TSA detects the intrinsic local geometrical structure of the tensor space by learning a lower dimensional tensor subspace. We compare our proposed approach with PCA, LDA and LPP methods on two standard databases. Experimental results demonstrate that TSA achieves better recognition rate, while being much more efﬁcient. 1</p><p>Reference: <a title="nips-2005-189-reference" href="../nips2005_reference/nips-2005-Tensor_Subspace_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu 1  Abstract Previous work has demonstrated that the image variations of many objects (human faces in particular) under variable lighting can be effectively modeled by low dimensional linear spaces. [sent-4, score-0.117]
</p><p>2 The typical linear subspace learning algorithms include Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and Locality Preserving Projection (LPP). [sent-5, score-0.146]
</p><p>3 All of these methods consider an n1 × n2 image as a high dimensional vector in Rn1 ×n2 , while an image represented in the plane is intrinsically a matrix. [sent-6, score-0.162]
</p><p>4 TSA considers an image as the second order tensor in Rn1 ⊗ Rn2 , where Rn1 and Rn2 are two vector spaces. [sent-8, score-0.423]
</p><p>5 The relationship between the column vectors of the image matrix and that between the row vectors can be naturally characterized by TSA. [sent-9, score-0.077]
</p><p>6 TSA detects the intrinsic local geometrical structure of the tensor space by learning a lower dimensional tensor subspace. [sent-10, score-0.843]
</p><p>7 Experimental results demonstrate that TSA achieves better recognition rate, while being much more efﬁcient. [sent-12, score-0.048]
</p><p>8 1  Introduction  There is currently a great deal of interest in appearance-based approaches to face recognition [1], [5], [8]. [sent-13, score-0.251]
</p><p>9 When using appearance-based approaches, we usually represent an image of size n1 × n2 pixels by a vector in Rn1 ×n2 . [sent-14, score-0.06]
</p><p>10 Throughout this paper, we denote by face space the set of all the face images. [sent-15, score-0.406]
</p><p>11 The face space is generally a low dimensional manifold embedded in the ambient space [6], [7], [10]. [sent-16, score-0.317]
</p><p>12 The typical linear algorithms for learning such a face manifold for recognition include Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA) and Locality Preserving Projection (LPP) [4]. [sent-17, score-0.338]
</p><p>13 Most of previous works on statistical image analysis represent an image by a vector in high-dimensional space. [sent-18, score-0.137]
</p><p>14 However, an image is intrinsically a matrix, or the second order tensor. [sent-19, score-0.083]
</p><p>15 Recently, multilinear algebra, the algebra of higher-order tensors, was applied for analyzing the multifactor structure of image ensembles [9], [11], [12]. [sent-21, score-0.167]
</p><p>16 Vasilescu and Terzopoulos have proposed a novel face representation algorithm called Tensorface [9]. [sent-22, score-0.219]
</p><p>17 Tensorface represents the set of face images by a higher-order tensor and  extends Singular Value Decomposition (SVD) to higher-order tensor data. [sent-23, score-0.985]
</p><p>18 In this way, the multiple factors related to expression, illumination and pose can be separated from different dimensions of the tensor. [sent-24, score-0.048]
</p><p>19 In this paper, we propose a new algorithm for image (human faces in particular) representation based on the considerations of multilinear algebra and differential geometry. [sent-25, score-0.162]
</p><p>20 For an image of size n1 × n2 , it is represented as the second order tensor (or, matrix) in the tensor space Rn1 ⊗ Rn2 . [sent-27, score-0.786]
</p><p>21 On the other hand, the face space is generally a submanifold embedded in Rn1 ⊗ Rn2 . [sent-28, score-0.304]
</p><p>22 Given some images sampled from the face manifold, we can build an adjacency graph to model the local geometrical structure of the manifold. [sent-29, score-0.452]
</p><p>23 TSA ﬁnds a projection that respects this graph structure. [sent-30, score-0.103]
</p><p>24 The obtained tensor subspace provides an optimal linear approximation to the face manifold in the sense of local isometry. [sent-31, score-0.801]
</p><p>25 Vasilescu shows how to extend SVD(PCA) to higher order tensor data. [sent-32, score-0.363]
</p><p>26 While traditional linear dimensionality reduction algorithms like PCA, LDA and LPP ﬁnd a map from Rn to Rl (l < n), TSA ﬁnds a map from Rn1 ⊗ Rn2 to Rl1 ⊗ Rl2 (l1 < n1 , l2 < n2 ). [sent-35, score-0.11]
</p><p>27 When label information is available, it can be easily incorporated into the graph structure. [sent-39, score-0.062]
</p><p>28 The matrices in the eigen-problems are of size n1 ×n1 or n2 ×n2 , which are much smaller than the matrices of size n × n (n = n1 × n2 ) in PCA, LDA and LPP. [sent-44, score-0.044]
</p><p>29 TSA explicitly takes into account the manifold structure of the image space. [sent-48, score-0.152]
</p><p>30 The local geometrical structure is modeled by an adjacency graph. [sent-49, score-0.131]
</p><p>31 2  Tensor Subspace Analysis  In this section, we introduce a new algorithm called Tensor Subspace Analysis for learning a tensor subspace which respects the geometrical and discriminative structures of the original data space. [sent-53, score-0.584]
</p><p>32 1  Laplacian based Dimensionality Reduction  Problems of dimensionality reduction has been considered. [sent-55, score-0.092]
</p><p>33 One general approach is based on graph Laplacian [2]. [sent-56, score-0.062]
</p><p>34 The objective function of Laplacian eigenmap is as follows: 2  min f  (f (xi ) − f (xj )) Sij ij  where S is a similarity matrix. [sent-57, score-0.066]
</p><p>35 These optimal functions are nonlinear but may be expensive to compute. [sent-58, score-0.044]
</p><p>36 In this  paper we will consider a more structured subset of linear functions that arise out of tensor analysis. [sent-61, score-0.381]
</p><p>37 2  The Linear Dimensionality Reduction Problem in Tensor Space  The generic problem of linear dimensionality reduction in the second order tensor space is the following. [sent-64, score-0.473]
</p><p>38 Our method is of particular applicability in the special case where X1 , · · · , Xm ∈ M and M is a nonlinear submanifold embedded in Rn1 ⊗ Rn2 . [sent-66, score-0.125]
</p><p>39 3  Optimal Linear Embeddings  As we described previously, the face space is probably a nonlinear submanifold embedded in the tensor space. [sent-68, score-0.691]
</p><p>40 One hopes then to estimate geometrical and topological properties of the submanifold from random points (“scattered data”) lying on this unknown submanifold. [sent-69, score-0.15]
</p><p>41 In this section, we consider the particular question of ﬁnding a linear subspace approximation to the submanifold in the sense of local isometry. [sent-70, score-0.221]
</p><p>42 Given m data points X = {X1 , · · · , Xm } sampled from the face submanifold M ∈ Rn1 ⊗ Rn1 , one can build a nearest neighbor graph G to model the local geometrical structure of M. [sent-72, score-0.484]
</p><p>43 A possible deﬁnition of S is as follows:  2  e− Xi −Xj , if X is among the k nearest t  i  neighbors of Xj , or Xj is among (1) Sij =  the k nearest neighbors of Xi ;   0, otherwise. [sent-74, score-0.056]
</p><p>44 The function exp(− Xi − Xj 2 /t) is the so called heat kernel which is intimately related to the manifold structure. [sent-76, score-0.069]
</p><p>45 When the label information is available, it can be easily incorporated into the graph as follows:  Sij =  e− 0,  Xi −Xj 2 t  , if Xi and Xj share the same label; otherwise. [sent-80, score-0.062]
</p><p>46 A reasonable transformation respecting the graph structure can be obtained by solving the following objective functions: U T Xi V − U T Xj V  min U,V  2  Sij  (3)  ij  The objective function incurs a heavy penalty if neighboring points Xi and Xj are mapped far apart. [sent-82, score-0.192]
</p><p>47 = tr U T (DV − SV ) U T T where DV = i Dii Xi V V Xi and SV = T tr(A A), so we also have  ij  1 2  2  U T Xi V − U T Xj V  =  Sij  tr (Yi − Yj )T (Yi − Yj ) Sij ij  1 2  =  2  ij  1 2  =  T Sij Xi V V T Xj . [sent-87, score-0.524]
</p><p>48 Similarly, A  tr YiT Yi + YjT Yj − YiT Yj − YjT Yi Sij ij  Dii YiT Yi −  = tr i  = tr V  Sij YiT Yj ij  T  T Dii Xi U U T Xi  T Xi U U T Xj V  −  i  ij  . [sent-88, score-0.687]
</p><p>49 = tr V T (DU − SU ) V T T where DU = i Dii Xi U U T Xi and SU = ij Sij Xi U U T Xj . [sent-89, score-0.229]
</p><p>50 Therefore, we should T simultaneously minimize tr U (DV − SV ) U and tr V T (DU − SU ) V . [sent-90, score-0.326]
</p><p>51 In addition to preserving the graph structure, we also aim at maximizing the global variance on the manifold. [sent-91, score-0.099]
</p><p>52 By spectral graph theory [3], dP can be discretely estimated by T the diagonal matrix D(Dii = j Sij ) on the sample points. [sent-93, score-0.079]
</p><p>53 Let Y = U XV denote the random variable in the tensor subspace and suppose the data points have a zero mean. [sent-94, score-0.491]
</p><p>54 It is easy to see that the optimal U should be the generalized eigenvectors of (DV − SV , DV ) and the optimal V should be the generalized eigenvectors of (DU − SU , DU ). [sent-99, score-0.124]
</p><p>55 3  Experimental Results  In this section, several experiments are carried out to show the efﬁciency and effectiveness of our proposed algorithm for face recognition. [sent-105, score-0.203]
</p><p>56 We compare our algorithm with the Eigenface (PCA) [8], Fisherface (LDA) [1], and Laplacianface (LPP) [5] methods, three of the most popular linear methods for face recognition. [sent-106, score-0.221]
</p><p>57 The ﬁrst one is the PIE (Pose, Illumination, and Experience) database from CMU, and the second one is the ORL database. [sent-108, score-0.057]
</p><p>58 Original images were normalized (in scale and orientation) such that the two eyes were aligned at the same position. [sent-110, score-0.073]
</p><p>59 Then, the facial areas were cropped into the ﬁnal images for matching. [sent-111, score-0.095]
</p><p>60 The size of each cropped image in all the experiments is 32×32 pixels, with 256 gray levels per pixel. [sent-112, score-0.08]
</p><p>61 For the Eigenface, Fisherface, and Laplacianface methods, the image is represented as a 1024-dimensional vector, while in our algorithm the image is represented as a (32 × 32)-dimensional matrix, or the second order tensor. [sent-114, score-0.12]
</p><p>62 The nearest neighbor classiﬁer is used for classiﬁcation for its simplicity. [sent-115, score-0.046]
</p><p>63 First, we calculate the face subspace from the training set of face images; then the new face image to be identiﬁed is projected into d-dimensional subspace (PCA, LDA, and LPP) or (d × d)-dimensional tensor subspace (TSA); ﬁnally, the new face image is identiﬁed by nearest neighbor classiﬁer. [sent-117, score-1.725]
</p><p>64 1  Experiments on PIE Database  The CMU PIE face database contains 68 subjects with 41,368 face images as a whole. [sent-120, score-0.519]
</p><p>65 The face images were captured by 13 synchronized cameras and 21 ﬂashes, under varying pose, illumination and expression. [sent-121, score-0.288]
</p><p>66 dimensionality reduction on PIE database Table 1: Performance comparison on PIE database Method Baseline Eigenfaces Fisherfaces Laplacianfaces TSA  error 69. [sent-123, score-0.234]
</p><p>67 64%  5 Train dim 1024 338 67 67 112 20 Train dim 1024 889 67 146 132  time(s) 0. [sent-133, score-0.182]
</p><p>68 88%  10 Train dim 1024 654 67 134 132 30 Train dim 1024 990 67 131 122  time(s) 5. [sent-151, score-0.182]
</p><p>69 688  C29) and use all the images under different illuminations and expressions, thus we get 170 images for each individual. [sent-159, score-0.112]
</p><p>70 For each individual, l(= 5, 10, 20, 30) images are randomly selected for training and the rest are used for testing. [sent-160, score-0.056]
</p><p>71 The training set is utilized to learn the subspace representation of the face manifold by using Eigenface, Fisherface, Laplacianface and our algorithm. [sent-161, score-0.416]
</p><p>72 The testing images are projected into the face subspace in which recognition is then performed. [sent-162, score-0.435]
</p><p>73 It would be important to note that the Laplacianface algorithm and our algorithm share the same graph structure as deﬁned in Eqn. [sent-164, score-0.085]
</p><p>74 Figure 1 shows the plots of error rate versus dimensionality reduction for the Eigenface, Fisherface, Laplacianface, TSA and baseline methods. [sent-166, score-0.233]
</p><p>75 For the baseline method, the recognition is simply performed in the original 1024-dimensional image space without any dimensionality reduction. [sent-167, score-0.247]
</p><p>76 Note that, the upper bound of the dimensionality of Fisherface is c − 1 where c is the number of individuals. [sent-168, score-0.051]
</p><p>77 For our TSA algorithm, we only show its performance in the (d × d)-dimensional tensor subspace, say, 1, 4, 9, etc. [sent-169, score-0.363]
</p><p>78 We show the best results obtained by them in Table 1 and the corresponding face subspaces are called optimal face subspace for each method. [sent-171, score-0.57]
</p><p>79 It does not obtain any improvement over the baseline method. [sent-174, score-0.088]
</p><p>80 dimensionality reduction on ORL database Table 2: Performance comparison on ORL database Method Baseline Eigenfaces Fisherfaces Laplacianfaces TSA  error 30. [sent-180, score-0.234]
</p><p>81 12%  2 Train dim 1024 79 23 39 102 4 Train dim 1024 122 39 39 102  time 38. [sent-190, score-0.182]
</p><p>82 75%  3 Train dim 1024 113 39 39 112 5 Train dim 1024 182 39 40 102  time 85. [sent-208, score-0.182]
</p><p>83 2  Experiments on ORL Database  The ORL (Olivetti Research Laboratory) face database is used in this test. [sent-221, score-0.26]
</p><p>84 It consists of a total of 400 face images, of a total of 40 people (10 samples per person). [sent-222, score-0.203]
</p><p>85 The images were captured at different times and have different variations including expressions (open or closed eyes, smiling or non-smiling) and facial details (glasses or no glasses). [sent-223, score-0.075]
</p><p>86 The images were taken with a tolerance for some tilting and rotation of the face up to 20 degrees. [sent-224, score-0.259]
</p><p>87 For each individual, l(= 2, 3, 4, 5) images are randomly selected for training and the rest are used for testing. [sent-225, score-0.056]
</p><p>88 2 shows the plots of error rate versus dimensionality reduction for the Eigenface, Fisherface, Laplacianface, TSA and baseline methods. [sent-229, score-0.233]
</p><p>89 Here, for a given d, we show its performance in the (d×d)dimensional tensor subspace. [sent-231, score-0.363]
</p><p>90 The best result obtained in the optimal subspace and the running time (millisecond) of computing the eigenvectors for each method are shown in Table 2. [sent-233, score-0.17]
</p><p>91 4  Conclusions and Future Work  Tensor based face analysis (representation and recognition) is introduced in this paper in order to detect the underlying nonlinear face manifold structure in the manner of tensor subspace learning. [sent-236, score-1.03]
</p><p>92 The manifold structure is approximated by the adjacency graph computed from the data points. [sent-237, score-0.187]
</p><p>93 The optimal tensor subspace respecting the graph structure is then obtained by solving an optimization problem. [sent-238, score-0.637]
</p><p>94 Most of traditional appearance based face recognition methods (i. [sent-240, score-0.251]
</p><p>95 Eigenface, Fisherface, and Laplacianface) consider an image as a vector in high dimensional space. [sent-242, score-0.079]
</p><p>96 In our work, an image is naturally represented as a matrix, or the second order tensor. [sent-244, score-0.06]
</p><p>97 Therefore, if the face manifold is highly nonlinear, it may fail to discover the intrinsic geometrical structure. [sent-248, score-0.347]
</p><p>98 Also, in our algorithm, the adjacency graph is induced from the local geometry and class information. [sent-250, score-0.095]
</p><p>99 It remains unclear how to deﬁne the optimal graph structure in the sense of discrimination. [sent-252, score-0.124]
</p><p>100 Yang, “Two-dimensional PCA: a new approach to appearance-based face representation and recognition,”IEEE. [sent-313, score-0.219]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tsa', 0.489), ('tensor', 0.363), ('lpp', 0.231), ('face', 0.203), ('pca', 0.19), ('laplacianfaces', 0.187), ('laplacianface', 0.173), ('sij', 0.172), ('tr', 0.163), ('eigenface', 0.158), ('fisherface', 0.158), ('dii', 0.151), ('fisherfaces', 0.15), ('eigenfaces', 0.149), ('yit', 0.144), ('dv', 0.139), ('lda', 0.135), ('subspace', 0.128), ('dims', 0.115), ('du', 0.114), ('dim', 0.091), ('baseline', 0.088), ('orl', 0.086), ('sv', 0.076), ('submanifold', 0.075), ('geometrical', 0.075), ('xi', 0.074), ('yjt', 0.072), ('xj', 0.071), ('train', 0.07), ('manifold', 0.069), ('pie', 0.069), ('ij', 0.066), ('su', 0.065), ('graph', 0.062), ('image', 0.06), ('database', 0.057), ('yi', 0.056), ('images', 0.056), ('dimensionality', 0.051), ('yj', 0.049), ('recognition', 0.048), ('multilinear', 0.043), ('vasilescu', 0.043), ('reduction', 0.041), ('preserving', 0.037), ('niyogi', 0.037), ('adjacency', 0.033), ('laplacian', 0.03), ('comparatively', 0.029), ('tensorface', 0.029), ('terzopoulos', 0.029), ('illumination', 0.029), ('locality', 0.029), ('nearest', 0.028), ('error', 0.028), ('december', 0.026), ('embedded', 0.026), ('respecting', 0.025), ('xiaofei', 0.025), ('glasses', 0.025), ('rate', 0.025), ('dp', 0.025), ('nonlinear', 0.024), ('discriminant', 0.024), ('structure', 0.023), ('projection', 0.023), ('algebra', 0.023), ('var', 0.023), ('tensors', 0.023), ('intrinsically', 0.023), ('databases', 0.023), ('eigenvectors', 0.022), ('matrices', 0.022), ('svd', 0.021), ('faces', 0.02), ('xm', 0.02), ('cropped', 0.02), ('generalized', 0.02), ('optimal', 0.02), ('pose', 0.019), ('dimensional', 0.019), ('vol', 0.019), ('unclear', 0.019), ('facial', 0.019), ('eigenvector', 0.019), ('cmu', 0.018), ('linear', 0.018), ('neighbor', 0.018), ('yang', 0.018), ('ensembles', 0.018), ('respects', 0.018), ('matrix', 0.017), ('eyes', 0.017), ('analysis', 0.017), ('representation', 0.016), ('subspaces', 0.016), ('subsection', 0.016), ('solving', 0.016), ('table', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="189-tfidf-1" href="./nips-2005-Tensor_Subspace_Analysis.html">189 nips-2005-Tensor Subspace Analysis</a></p>
<p>Author: Xiaofei He, Deng Cai, Partha Niyogi</p><p>Abstract: Previous work has demonstrated that the image variations of many objects (human faces in particular) under variable lighting can be effectively modeled by low dimensional linear spaces. The typical linear subspace learning algorithms include Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and Locality Preserving Projection (LPP). All of these methods consider an n1 × n2 image as a high dimensional vector in Rn1 ×n2 , while an image represented in the plane is intrinsically a matrix. In this paper, we propose a new algorithm called Tensor Subspace Analysis (TSA). TSA considers an image as the second order tensor in Rn1 ⊗ Rn2 , where Rn1 and Rn2 are two vector spaces. The relationship between the column vectors of the image matrix and that between the row vectors can be naturally characterized by TSA. TSA detects the intrinsic local geometrical structure of the tensor space by learning a lower dimensional tensor subspace. We compare our proposed approach with PCA, LDA and LPP methods on two standard databases. Experimental results demonstrate that TSA achieves better recognition rate, while being much more efﬁcient. 1</p><p>2 0.14545535 <a title="189-tfidf-2" href="./nips-2005-Neuronal_Fiber_Delineation_in_Area_of_Edema_from_Diffusion_Weighted_MRI.html">135 nips-2005-Neuronal Fiber Delineation in Area of Edema from Diffusion Weighted MRI</a></p>
<p>Author: Ofer Pasternak, Nathan Intrator, Nir Sochen, Yaniv Assaf</p><p>Abstract: Diffusion Tensor Magnetic Resonance Imaging (DT-MRI) is a non invasive method for brain neuronal ﬁbers delineation. Here we show a modiﬁcation for DT-MRI that allows delineation of neuronal ﬁbers which are inﬁltrated by edema. We use the Muliple Tensor Variational (MTV) framework which replaces the diffusion model of DT-MRI with a multiple component model and ﬁts it to the signal attenuation with a variational regularization mechanism. In order to reduce free water contamination we estimate the free water compartment volume fraction in each voxel, remove it, and then calculate the anisotropy of the remaining compartment. The variational framework was applied on data collected with conventional clinical parameters, containing only six diffusion directions. By using the variational framework we were able to overcome the highly ill posed ﬁtting. The results show that we were able to ﬁnd ﬁbers that were not found by DT-MRI.</p><p>3 0.094244108 <a title="189-tfidf-3" href="./nips-2005-Laplacian_Score_for_Feature_Selection.html">104 nips-2005-Laplacian Score for Feature Selection</a></p>
<p>Author: Xiaofei He, Deng Cai, Partha Niyogi</p><p>Abstract: In supervised learning scenarios, feature selection has been studied widely in the literature. Selecting features in unsupervised learning scenarios is a much harder problem, due to the absence of class labels that would guide the search for relevant information. And, almost all of previous unsupervised feature selection methods are “wrapper” techniques that require a learning algorithm to evaluate the candidate feature subsets. In this paper, we propose a “ﬁlter” method for feature selection which is independent of any learning algorithm. Our method can be performed in either supervised or unsupervised fashion. The proposed method is based on the observation that, in many real world classiﬁcation problems, data from the same class are often close to each other. The importance of a feature is evaluated by its power of locality preserving, or, Laplacian Score. We compare our method with data variance (unsupervised) and Fisher score (supervised) on two data sets. Experimental results demonstrate the effectiveness and efﬁciency of our algorithm. 1</p><p>4 0.087739028 <a title="189-tfidf-4" href="./nips-2005-Robust_Fisher_Discriminant_Analysis.html">166 nips-2005-Robust Fisher Discriminant Analysis</a></p>
<p>Author: Seung-jean Kim, Alessandro Magnani, Stephen Boyd</p><p>Abstract: Fisher linear discriminant analysis (LDA) can be sensitive to the problem data. Robust Fisher LDA can systematically alleviate the sensitivity problem by explicitly incorporating a model of data uncertainty in a classiﬁcation problem and optimizing for the worst-case scenario under this model. The main contribution of this paper is show that with general convex uncertainty models on the problem data, robust Fisher LDA can be carried out using convex optimization. For a certain type of product form uncertainty model, robust Fisher LDA can be carried out at a cost comparable to standard Fisher LDA. The method is demonstrated with some numerical examples. Finally, we show how to extend these results to robust kernel Fisher discriminant analysis, i.e., robust Fisher LDA in a high dimensional feature space. 1</p><p>5 0.081906289 <a title="189-tfidf-5" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>Author: Kilian Q. Weinberger, John Blitzer, Lawrence K. Saul</p><p>Abstract: We show how to learn a Mahanalobis distance metric for k-nearest neighbor (kNN) classiﬁcation by semideﬁnite programming. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. On seven data sets of varying size and difﬁculty, we ﬁnd that metrics trained in this way lead to signiﬁcant improvements in kNN classiﬁcation—for example, achieving a test error rate of 1.3% on the MNIST handwritten digits. As in support vector machines (SVMs), the learning problem reduces to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our framework requires no modiﬁcation or extension for problems in multiway (as opposed to binary) classiﬁcation. 1</p><p>6 0.078920417 <a title="189-tfidf-6" href="./nips-2005-An_Approximate_Inference_Approach_for_the_PCA_Reconstruction_Error.html">24 nips-2005-An Approximate Inference Approach for the PCA Reconstruction Error</a></p>
<p>7 0.078536406 <a title="189-tfidf-7" href="./nips-2005-Non-Local_Manifold_Parzen_Windows.html">138 nips-2005-Non-Local Manifold Parzen Windows</a></p>
<p>8 0.069545113 <a title="189-tfidf-8" href="./nips-2005-A_Bayes_Rule_for_Density_Matrices.html">2 nips-2005-A Bayes Rule for Density Matrices</a></p>
<p>9 0.066839941 <a title="189-tfidf-9" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>10 0.064425446 <a title="189-tfidf-10" href="./nips-2005-Selecting_Landmark_Points_for_Sparse_Manifold_Learning.html">172 nips-2005-Selecting Landmark Points for Sparse Manifold Learning</a></p>
<p>11 0.060019244 <a title="189-tfidf-11" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>12 0.058910988 <a title="189-tfidf-12" href="./nips-2005-Metric_Learning_by_Collapsing_Classes.html">126 nips-2005-Metric Learning by Collapsing Classes</a></p>
<p>13 0.058761038 <a title="189-tfidf-13" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>14 0.058272876 <a title="189-tfidf-14" href="./nips-2005-Location-based_activity_recognition.html">121 nips-2005-Location-based activity recognition</a></p>
<p>15 0.056202773 <a title="189-tfidf-15" href="./nips-2005-Inferring_Motor_Programs_from_Images_of_Handwritten_Digits.html">97 nips-2005-Inferring Motor Programs from Images of Handwritten Digits</a></p>
<p>16 0.054394666 <a title="189-tfidf-16" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>17 0.053059492 <a title="189-tfidf-17" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>18 0.05065478 <a title="189-tfidf-18" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>19 0.048975747 <a title="189-tfidf-19" href="./nips-2005-Correlated_Topic_Models.html">52 nips-2005-Correlated Topic Models</a></p>
<p>20 0.048872661 <a title="189-tfidf-20" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.14), (1, 0.071), (2, -0.067), (3, 0.023), (4, -0.087), (5, -0.004), (6, 0.054), (7, -0.045), (8, 0.122), (9, -0.101), (10, 0.095), (11, -0.059), (12, -0.003), (13, -0.049), (14, -0.087), (15, -0.006), (16, -0.197), (17, -0.02), (18, -0.134), (19, -0.002), (20, 0.027), (21, 0.111), (22, 0.098), (23, 0.151), (24, -0.001), (25, 0.038), (26, 0.062), (27, 0.113), (28, -0.004), (29, 0.016), (30, -0.047), (31, -0.046), (32, 0.011), (33, -0.037), (34, -0.057), (35, 0.083), (36, -0.014), (37, -0.075), (38, 0.116), (39, -0.036), (40, -0.01), (41, -0.098), (42, -0.094), (43, -0.048), (44, 0.005), (45, 0.177), (46, 0.079), (47, -0.036), (48, -0.04), (49, -0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93101752 <a title="189-lsi-1" href="./nips-2005-Tensor_Subspace_Analysis.html">189 nips-2005-Tensor Subspace Analysis</a></p>
<p>Author: Xiaofei He, Deng Cai, Partha Niyogi</p><p>Abstract: Previous work has demonstrated that the image variations of many objects (human faces in particular) under variable lighting can be effectively modeled by low dimensional linear spaces. The typical linear subspace learning algorithms include Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and Locality Preserving Projection (LPP). All of these methods consider an n1 × n2 image as a high dimensional vector in Rn1 ×n2 , while an image represented in the plane is intrinsically a matrix. In this paper, we propose a new algorithm called Tensor Subspace Analysis (TSA). TSA considers an image as the second order tensor in Rn1 ⊗ Rn2 , where Rn1 and Rn2 are two vector spaces. The relationship between the column vectors of the image matrix and that between the row vectors can be naturally characterized by TSA. TSA detects the intrinsic local geometrical structure of the tensor space by learning a lower dimensional tensor subspace. We compare our proposed approach with PCA, LDA and LPP methods on two standard databases. Experimental results demonstrate that TSA achieves better recognition rate, while being much more efﬁcient. 1</p><p>2 0.48899689 <a title="189-lsi-2" href="./nips-2005-An_Approximate_Inference_Approach_for_the_PCA_Reconstruction_Error.html">24 nips-2005-An Approximate Inference Approach for the PCA Reconstruction Error</a></p>
<p>Author: Manfred Opper</p><p>Abstract: The problem of computing a resample estimate for the reconstruction error in PCA is reformulated as an inference problem with the help of the replica method. Using the expectation consistent (EC) approximation, the intractable inference problem can be solved efﬁciently using only two variational parameters. A perturbative correction to the result is computed and an alternative simpliﬁed derivation is also presented. 1</p><p>3 0.4813883 <a title="189-lsi-3" href="./nips-2005-Laplacian_Score_for_Feature_Selection.html">104 nips-2005-Laplacian Score for Feature Selection</a></p>
<p>Author: Xiaofei He, Deng Cai, Partha Niyogi</p><p>Abstract: In supervised learning scenarios, feature selection has been studied widely in the literature. Selecting features in unsupervised learning scenarios is a much harder problem, due to the absence of class labels that would guide the search for relevant information. And, almost all of previous unsupervised feature selection methods are “wrapper” techniques that require a learning algorithm to evaluate the candidate feature subsets. In this paper, we propose a “ﬁlter” method for feature selection which is independent of any learning algorithm. Our method can be performed in either supervised or unsupervised fashion. The proposed method is based on the observation that, in many real world classiﬁcation problems, data from the same class are often close to each other. The importance of a feature is evaluated by its power of locality preserving, or, Laplacian Score. We compare our method with data variance (unsupervised) and Fisher score (supervised) on two data sets. Experimental results demonstrate the effectiveness and efﬁciency of our algorithm. 1</p><p>4 0.48076773 <a title="189-lsi-4" href="./nips-2005-Robust_Fisher_Discriminant_Analysis.html">166 nips-2005-Robust Fisher Discriminant Analysis</a></p>
<p>Author: Seung-jean Kim, Alessandro Magnani, Stephen Boyd</p><p>Abstract: Fisher linear discriminant analysis (LDA) can be sensitive to the problem data. Robust Fisher LDA can systematically alleviate the sensitivity problem by explicitly incorporating a model of data uncertainty in a classiﬁcation problem and optimizing for the worst-case scenario under this model. The main contribution of this paper is show that with general convex uncertainty models on the problem data, robust Fisher LDA can be carried out using convex optimization. For a certain type of product form uncertainty model, robust Fisher LDA can be carried out at a cost comparable to standard Fisher LDA. The method is demonstrated with some numerical examples. Finally, we show how to extend these results to robust kernel Fisher discriminant analysis, i.e., robust Fisher LDA in a high dimensional feature space. 1</p><p>5 0.44107509 <a title="189-lsi-5" href="./nips-2005-Neuronal_Fiber_Delineation_in_Area_of_Edema_from_Diffusion_Weighted_MRI.html">135 nips-2005-Neuronal Fiber Delineation in Area of Edema from Diffusion Weighted MRI</a></p>
<p>Author: Ofer Pasternak, Nathan Intrator, Nir Sochen, Yaniv Assaf</p><p>Abstract: Diffusion Tensor Magnetic Resonance Imaging (DT-MRI) is a non invasive method for brain neuronal ﬁbers delineation. Here we show a modiﬁcation for DT-MRI that allows delineation of neuronal ﬁbers which are inﬁltrated by edema. We use the Muliple Tensor Variational (MTV) framework which replaces the diffusion model of DT-MRI with a multiple component model and ﬁts it to the signal attenuation with a variational regularization mechanism. In order to reduce free water contamination we estimate the free water compartment volume fraction in each voxel, remove it, and then calculate the anisotropy of the remaining compartment. The variational framework was applied on data collected with conventional clinical parameters, containing only six diffusion directions. By using the variational framework we were able to overcome the highly ill posed ﬁtting. The results show that we were able to ﬁnd ﬁbers that were not found by DT-MRI.</p><p>6 0.42275199 <a title="189-lsi-6" href="./nips-2005-A_Bayes_Rule_for_Density_Matrices.html">2 nips-2005-A Bayes Rule for Density Matrices</a></p>
<p>7 0.41887677 <a title="189-lsi-7" href="./nips-2005-Metric_Learning_by_Collapsing_Classes.html">126 nips-2005-Metric Learning by Collapsing Classes</a></p>
<p>8 0.38335437 <a title="189-lsi-8" href="./nips-2005-Non-Local_Manifold_Parzen_Windows.html">138 nips-2005-Non-Local Manifold Parzen Windows</a></p>
<p>9 0.36759928 <a title="189-lsi-9" href="./nips-2005-Diffusion_Maps%2C_Spectral_Clustering_and_Eigenfunctions_of_Fokker-Planck_Operators.html">56 nips-2005-Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators</a></p>
<p>10 0.35089004 <a title="189-lsi-10" href="./nips-2005-Fast_Krylov_Methods_for_N-Body_Learning.html">71 nips-2005-Fast Krylov Methods for N-Body Learning</a></p>
<p>11 0.32816303 <a title="189-lsi-11" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>12 0.31948689 <a title="189-lsi-12" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>13 0.3188107 <a title="189-lsi-13" href="./nips-2005-The_Information-Form_Data_Association_Filter.html">192 nips-2005-The Information-Form Data Association Filter</a></p>
<p>14 0.31644571 <a title="189-lsi-14" href="./nips-2005-Two_view_learning%3A_SVM-2K%2C_Theory_and_Practice.html">196 nips-2005-Two view learning: SVM-2K, Theory and Practice</a></p>
<p>15 0.3061237 <a title="189-lsi-15" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>16 0.29785284 <a title="189-lsi-16" href="./nips-2005-Location-based_activity_recognition.html">121 nips-2005-Location-based activity recognition</a></p>
<p>17 0.29206404 <a title="189-lsi-17" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>18 0.28429389 <a title="189-lsi-18" href="./nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<p>19 0.2731356 <a title="189-lsi-19" href="./nips-2005-Estimation_of_Intrinsic_Dimensionality_Using_High-Rate_Vector_Quantization.html">66 nips-2005-Estimation of Intrinsic Dimensionality Using High-Rate Vector Quantization</a></p>
<p>20 0.27267098 <a title="189-lsi-20" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.035), (10, 0.024), (27, 0.025), (31, 0.017), (34, 0.104), (39, 0.019), (41, 0.016), (50, 0.021), (55, 0.013), (65, 0.017), (69, 0.038), (73, 0.13), (85, 0.352), (88, 0.074), (91, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76858628 <a title="189-lda-1" href="./nips-2005-Tensor_Subspace_Analysis.html">189 nips-2005-Tensor Subspace Analysis</a></p>
<p>Author: Xiaofei He, Deng Cai, Partha Niyogi</p><p>Abstract: Previous work has demonstrated that the image variations of many objects (human faces in particular) under variable lighting can be effectively modeled by low dimensional linear spaces. The typical linear subspace learning algorithms include Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and Locality Preserving Projection (LPP). All of these methods consider an n1 × n2 image as a high dimensional vector in Rn1 ×n2 , while an image represented in the plane is intrinsically a matrix. In this paper, we propose a new algorithm called Tensor Subspace Analysis (TSA). TSA considers an image as the second order tensor in Rn1 ⊗ Rn2 , where Rn1 and Rn2 are two vector spaces. The relationship between the column vectors of the image matrix and that between the row vectors can be naturally characterized by TSA. TSA detects the intrinsic local geometrical structure of the tensor space by learning a lower dimensional tensor subspace. We compare our proposed approach with PCA, LDA and LPP methods on two standard databases. Experimental results demonstrate that TSA achieves better recognition rate, while being much more efﬁcient. 1</p><p>2 0.55308348 <a title="189-lda-2" href="./nips-2005-Noise_and_the_two-thirds_power_Law.html">136 nips-2005-Noise and the two-thirds power Law</a></p>
<p>Author: Uri Maoz, Elon Portugaly, Tamar Flash, Yair Weiss</p><p>Abstract: The two-thirds power law, an empirical law stating an inverse non-linear relationship between the tangential hand speed and the curvature of its trajectory during curved motion, is widely acknowledged to be an invariant of upper-limb movement. It has also been shown to exist in eyemotion, locomotion and was even demonstrated in motion perception and prediction. This ubiquity has fostered various attempts to uncover the origins of this empirical relationship. In these it was generally attributed either to smoothness in hand- or joint-space or to the result of mechanisms that damp noise inherent in the motor system to produce the smooth trajectories evident in healthy human motion. We show here that white Gaussian noise also obeys this power-law. Analysis of signal and noise combinations shows that trajectories that were synthetically created not to comply with the power-law are transformed to power-law compliant ones after combination with low levels of noise. Furthermore, there exist colored noise types that drive non-power-law trajectories to power-law compliance and are not affected by smoothing. These results suggest caution when running experiments aimed at verifying the power-law or assuming its underlying existence without proper analysis of the noise. Our results could also suggest that the power-law might be derived not from smoothness or smoothness-inducing mechanisms operating on the noise inherent in our motor system but rather from the correlated noise which is inherent in this motor system. 1</p><p>3 0.47555485 <a title="189-lda-3" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>Author: Tong Zhang, Rie Kubota Ando</p><p>Abstract: We consider a framework for semi-supervised learning using spectral decomposition based un-supervised kernel design. This approach subsumes a class of previously proposed semi-supervised learning methods on data graphs. We examine various theoretical properties of such methods. In particular, we derive a generalization performance bound, and obtain the optimal kernel design by minimizing the bound. Based on the theoretical analysis, we are able to demonstrate why spectral kernel design based methods can often improve the predictive performance. Experiments are used to illustrate the main consequences of our analysis.</p><p>4 0.47325262 <a title="189-lda-4" href="./nips-2005-Fast_Krylov_Methods_for_N-Body_Learning.html">71 nips-2005-Fast Krylov Methods for N-Body Learning</a></p>
<p>Author: Nando D. Freitas, Yang Wang, Maryam Mahdaviani, Dustin Lang</p><p>Abstract: This paper addresses the issue of numerical computation in machine learning domains based on similarity metrics, such as kernel methods, spectral techniques and Gaussian processes. It presents a general solution strategy based on Krylov subspace iteration and fast N-body learning methods. The experiments show signiﬁcant gains in computation and storage on datasets arising in image segmentation, object detection and dimensionality reduction. The paper also presents theoretical bounds on the stability of these methods.</p><p>5 0.46708706 <a title="189-lda-5" href="./nips-2005-Using_%60%60epitomes%27%27_to_model_genetic_diversity%3A_Rational_design_of_HIV_vaccine_cocktails.html">198 nips-2005-Using ``epitomes'' to model genetic diversity: Rational design of HIV vaccine cocktails</a></p>
<p>Author: Nebojsa Jojic, Vladimir Jojic, Christopher Meek, David Heckerman, Brendan J. Frey</p><p>Abstract: We introduce a new model of genetic diversity which summarizes a large input dataset into an epitome, a short sequence or a small set of short sequences of probability distributions capturing many overlapping subsequences from the dataset. The epitome as a representation has already been used in modeling real-valued signals, such as images and audio. The discrete sequence model we introduce in this paper targets applications in genetics, from multiple alignment to recombination and mutation inference. In our experiments, we concentrate on modeling the diversity of HIV where the epitome emerges as a natural model for producing relatively small vaccines covering a large number of immune system targets known as epitopes. Our experiments show that the epitome includes more epitopes than other vaccine designs of similar length, including cocktails of consensus strains, phylogenetic tree centers, and observed strains. We also discuss epitome designs that take into account uncertainty about Tcell cross reactivity and epitope presentation. In our experiments, we ﬁnd that vaccine optimization is fairly robust to these uncertainties. 1</p><p>6 0.46334574 <a title="189-lda-6" href="./nips-2005-Describing_Visual_Scenes_using_Transformed_Dirichlet_Processes.html">55 nips-2005-Describing Visual Scenes using Transformed Dirichlet Processes</a></p>
<p>7 0.4631677 <a title="189-lda-7" href="./nips-2005-Laplacian_Score_for_Feature_Selection.html">104 nips-2005-Laplacian Score for Feature Selection</a></p>
<p>8 0.43521765 <a title="189-lda-8" href="./nips-2005-Kernelized_Infomax_Clustering.html">102 nips-2005-Kernelized Infomax Clustering</a></p>
<p>9 0.4272283 <a title="189-lda-9" href="./nips-2005-A_Probabilistic_Approach_for_Optimizing_Spectral_Clustering.html">13 nips-2005-A Probabilistic Approach for Optimizing Spectral Clustering</a></p>
<p>10 0.42496836 <a title="189-lda-10" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>11 0.41868162 <a title="189-lda-11" href="./nips-2005-From_Lasso_regression_to_Feature_vector_machine.html">77 nips-2005-From Lasso regression to Feature vector machine</a></p>
<p>12 0.40824008 <a title="189-lda-12" href="./nips-2005-Infinite_latent_feature_models_and_the_Indian_buffet_process.html">98 nips-2005-Infinite latent feature models and the Indian buffet process</a></p>
<p>13 0.4070752 <a title="189-lda-13" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>14 0.40635586 <a title="189-lda-14" href="./nips-2005-Diffusion_Maps%2C_Spectral_Clustering_and_Eigenfunctions_of_Fokker-Planck_Operators.html">56 nips-2005-Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators</a></p>
<p>15 0.40184075 <a title="189-lda-15" href="./nips-2005-Soft_Clustering_on_Graphs.html">178 nips-2005-Soft Clustering on Graphs</a></p>
<p>16 0.40029037 <a title="189-lda-16" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>17 0.39897692 <a title="189-lda-17" href="./nips-2005-Metric_Learning_by_Collapsing_Classes.html">126 nips-2005-Metric Learning by Collapsing Classes</a></p>
<p>18 0.39754212 <a title="189-lda-18" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>19 0.39586028 <a title="189-lda-19" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>20 0.39542091 <a title="189-lda-20" href="./nips-2005-Sequence_and_Tree_Kernels_with_Statistical_Feature_Mining.html">175 nips-2005-Sequence and Tree Kernels with Statistical Feature Mining</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
