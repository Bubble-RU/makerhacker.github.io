<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>94 nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-94" href="#">nips2005-94</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>94 nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</h1>
<br/><p>Source: <a title="nips-2005-94-pdf" href="http://papers.nips.cc/paper/2799-identifying-distributed-object-representations-in-human-extrastriate-visual-cortex.pdf">pdf</a></p><p>Author: Rory Sayres, David Ress, Kalanit Grill-spector</p><p>Abstract: The category of visual stimuli has been reliably decoded from patterns of neural activity in extrastriate visual cortex [1]. It has yet to be seen whether object identity can be inferred from this activity. We present fMRI data measuring responses in human extrastriate cortex to a set of 12 distinct object images. We use a simple winner-take-all classifier, using half the data from each recording session as a training set, to evaluate encoding of object identity across fMRI voxels. Since this approach is sensitive to the inclusion of noisy voxels, we describe two methods for identifying subsets of voxels in the data which optimally distinguish object identity. One method characterizes the reliability of each voxel within subsets of the data, while another estimates the mutual information of each voxel with the stimulus set. We find that both metrics can identify subsets of the data which reliably encode object identity, even when noisy measurements are artificially added to the data. The mutual information metric is less efficient at this task, likely due to constraints in fMRI data. 1</p><p>Reference: <a title="nips-2005-94-reference" href="../nips2005_reference/nips-2005-Identifying_Distributed_Object_Representations_in_Human_Extrastriate_Visual_Cortex_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract The category of visual stimuli has been reliably decoded from patterns of neural activity in extrastriate visual cortex [1]. [sent-5, score-0.492]
</p><p>2 It has yet to be seen whether object identity can be inferred from this activity. [sent-6, score-0.178]
</p><p>3 We present fMRI data measuring responses in human extrastriate cortex to a set of 12 distinct object images. [sent-7, score-0.401]
</p><p>4 We use a simple winner-take-all classifier, using half the data from each recording session as a training set, to evaluate encoding of object identity across fMRI voxels. [sent-8, score-0.496]
</p><p>5 Since this approach is sensitive to the inclusion of noisy voxels, we describe two methods for identifying subsets of voxels in the data which optimally distinguish object identity. [sent-9, score-0.829]
</p><p>6 One method characterizes the reliability of each voxel within subsets of the data, while another estimates the mutual information of each voxel with the stimulus set. [sent-10, score-1.086]
</p><p>7 We find that both metrics can identify subsets of the data which reliably encode object identity, even when noisy measurements are artificially added to the data. [sent-11, score-0.326]
</p><p>8 The mutual information metric is less efficient at this task, likely due to constraints in fMRI data. [sent-12, score-0.198]
</p><p>9 This cortex has been examined using electrophysiological recordings, optical imaging techniques, and a variety of neuroimaging techniques including functional magnetic resonance imaging (fMRI) [refs]. [sent-15, score-0.246]
</p><p>10 With fMRI, these regions can be reliably identified by their strong preferential response to intact objects over other visual stimuli [9,10]. [sent-16, score-0.444]
</p><p>11 A number of regions have been identified within this cortex, which preferentially respond to particular categories of images [refs]; it has been proposed that these regions are specialized for processing visual information about those categories [refs]. [sent-18, score-0.257]
</p><p>12 A recent study by Haxby and  colleagues [1] found that the category identity of different stimuli could be decoded from fMRI response patterns, using a simple classifier in which half of each data set was used as a training set and half as a test set. [sent-19, score-0.812]
</p><p>13 These results were interpreted as evidence for a distributed representation of objects across ventral cortex, in which both positive and negative responses contribute information about object identity. [sent-20, score-0.351]
</p><p>14 It is not clear, however, to what extent information about objects is processed at the category level, and to what extent it reflects individual object identity, or features within objects [1,8]. [sent-21, score-0.318]
</p><p>15 The study in [1] is one of a growing number of recent attempts to decode stimulus identity by examining fMRI response patterns across cortex [1-4]. [sent-22, score-0.546]
</p><p>16 Among its advantages are the ability to make many measurements across a large extent of cortex in awake, behaving humans. [sent-24, score-0.215]
</p><p>17 Because it is not clear which parts of this cortex are involved in representing different objects and which aren’t, analyses may include fMRI image locations (voxels) which are not involved in object representation. [sent-27, score-0.341]
</p><p>18 The present study addresses a number of these questions by examining the response patterns across object-selective cortex to a set of 12 individual object images, using highresolution fMRI. [sent-28, score-0.451]
</p><p>19 We sought to address the following experimental questions: (1) Can individual object identity be decoded from fMRI responses in object-selective cortex? [sent-29, score-0.311]
</p><p>20 (2) How can one identify those subsets of fMRI voxels which reliably encode identity about a stimulus, among a large set of potentially unrelated voxels? [sent-30, score-0.802]
</p><p>21 We adopt a similar approach to that described in [1], subdividing each data set into training and test subsets, and evaluate the efficiency of a set of voxels in discriminating object identity among the 12 possible images with a simple winner-take-all classifier. [sent-31, score-0.914]
</p><p>22 We then describe two metrics from which to identify sets of voxels which reliably discriminate different objects. [sent-32, score-0.787]
</p><p>23 The first metric estimates the replicability of voxels to each stimulus between the training and the test data. [sent-33, score-0.876]
</p><p>24 The second metric estimates the mutual information each voxel has with the stimulus set. [sent-34, score-0.623]
</p><p>25 We chose a stimulus set of 12 line drawings of different object stimuli, shown in Figure 1a. [sent-36, score-0.272]
</p><p>26 These objects can be readily categorized as faces, animals, or vehicles; these categories have been previously identified as producing distinct patterns of blood-oxygenation-level-dependent (BOLD) response in object-selective cortex [10]. [sent-37, score-0.389]
</p><p>27 This allows us to compare category and object identity as potential explanatory factors for BOLD response patterns. [sent-38, score-0.337]
</p><p>28 Further, the use of black-and-white line drawings reduces the number of stimulus features which differentiate the stimuli, such as spatial frequency bands. [sent-39, score-0.162]
</p><p>29 We presented one of the 12 object images to the subject within the foveal 5 degrees of visual field for 2 sec, then masked the image with a scrambled version of a random image for 10 sec. [sent-41, score-0.392]
</p><p>30 These scrambled images are known to produce minimal response in our regions of interest [11], and serve as a baseline condition for these experiments. [sent-42, score-0.28]
</p><p>31 This allowed us to collect full hemodynamic responses to each image, which in BOLD signal lags several seconds after stimulus onset. [sent-45, score-0.261]
</p><p>32 In this way we were able to analyze trial-bytrial variations in response to different images, without the analytic and design restrictions involved in analyzing fMRI data with more closely-spaced trials [5]. [sent-46, score-0.163]
</p><p>33 This feature was essential for computing the mutual information of a voxel with the stimulus set. [sent-47, score-0.563]
</p><p>34 a)  b) face1  face2  face3  face4  2 sec donkey  buffalo  ferret  dragster  truck  bus  boxster  c)  Left ↔ Right  Posterior ↔ Anterior  bull  10 sec  Figure 1: Experimental Design. [sent-48, score-0.672]
</p><p>35 The image is an axial slice from a T1-weighted anatomical image for one subject. [sent-52, score-0.188]
</p><p>36 Because of the restricted field of view of this coil, we imaged only right hemisphere cortex for these experiments. [sent-59, score-0.227]
</p><p>37 We imaged 4 subjects (1 female), each of whom participated in multiple recording sessions. [sent-60, score-0.162]
</p><p>38 For each recording session, we imaged 12 oblique slices, with voxel dimensions of 1 x 1 x 1 mm and a frame period of 2 seconds. [sent-61, score-0.519]
</p><p>39 Within each session, we identified object-selective voxels by applying a general linear model to the time series data, estimating the amplitude of BOLD response to different images [5]. [sent-65, score-0.866]
</p><p>40 We then computed contrast maps representing T tests of response of different images against the baseline scrambled condition. [sent-66, score-0.25]
</p><p>41 An example of voxels localized in this way is illustrated in Figure 2a, superimposed over mean T1-weighted anatomical images for two slices. [sent-67, score-0.718]
</p><p>42 Our criterion for defining object-selective voxels was that a voxel needed to respond to at least one of the 12 stimulus images relative to baseline with a significance level of p ≤ 0. [sent-68, score-1.18]
</p><p>43 The design of our surface coil, combined with its proximity to the imaged cortex, allowed us to observe significant event-related responses within single voxels. [sent-71, score-0.202]
</p><p>44 These responses are summarized by subtracting the mean BOLD response after stimulus onset with the response during the baseline period, as illustrated in Figure 2c. [sent-73, score-0.537]
</p><p>45 In this way we can summarize a data set as a matrix A of response amplitudes to different voxels, where Ai,j represents the response to the ith image of the jth voxel. [sent-74, score-0.375]
</p><p>46 001) for many stimuli, yet the voxels are heterogeneous in their responses—different voxels respond to different stimuli. [sent-76, score-1.191]
</p><p>47 This response diversity prompts the questions of deciding which sets of responses, if any, are informative of image identity. [sent-77, score-0.221]
</p><p>48 (a) T1-weighted anatomical images from a sample session, with object-selective voxels indicated in orange. [sent-79, score-0.69]
</p><p>49 (b) Mean peristimulus time courses from 4 object-selective voxels in the lower slice of (a) (locations indicated by arrow), for each image. [sent-80, score-0.608]
</p><p>50 Dotted lines indicate trial onset; dark bars at bottom indicate stimulus presentation duration. [sent-81, score-0.164]
</p><p>51 (c) Mean response amplitudes from the voxels depicted in (b), represented as a set of column vectors for each voxel. [sent-83, score-0.761]
</p><p>52 3  Winner-take-all classifier  Given a set of response amplitudes across object-selective voxels, how can we characterize the discriminabilty of responses to different stimuli? [sent-85, score-0.699]
</p><p>53 This question can be answered by constructing a classifier, which takes a set of responses to an unknown stimulus, and compares it to a training set of responses to known stimuli. [sent-86, score-0.237]
</p><p>54 This general approach has been successfully applied to fMRI responses in early visual cortex [3-4], object-selective cortex [1], and across multiple cortical regions [2]. [sent-87, score-0.468]
</p><p>55 As in the previous study, we subdivide each data set into a training set and a test set, with the training set representing odd-numbered runs and the test set representing even-numbered runs. [sent-89, score-0.192]
</p><p>56 We construct a training matrix, Atraining, in which each row represents the response across voxels to a different image in the training data set. [sent-91, score-0.977]
</p><p>57 We construct a second matrix, Atest, which contains the responses to different images during the test set. [sent-92, score-0.192]
</p><p>58 The overall performance of the classifier is evaluated by its success rate at classifying test responses based on the correlation to training responses. [sent-95, score-0.631]
</p><p>59 1  1 2 3 4 5 6 7 8 91011 12  Training Image  c)  400  1 2 3 4 5 6 7 8 9 10 11 12  1 2 3 4 5 6 7 8 91011 12  face1 face2 face3 face4 bull donkey buffalo ferret dragster truck bus boxster 200  Percent Correct: 100 %  1 2 3 4 5 6 7 8 9 10 11 12  Test Image  Training Data  Test Image  a)  0  0. [sent-97, score-0.58]
</p><p>60 2  Correlation Coefficient  Figure 3: Illustration of winner-take-all classifier for two sample sessions. [sent-99, score-0.363]
</p><p>61 (a) Response amplitudes for all object-selective voxels for the training (top) and test (bottom) data sets, for one recording session. [sent-100, score-0.781]
</p><p>62 The red square in each row represents the image from the test set that produced the highest correlation with the training set, and is the “guess” of the classifier. [sent-104, score-0.242]
</p><p>63 The percent correct is evaluated as the number of guesses that lie along the diagonal (the same image in the training and test sets produces the highest correlation). [sent-105, score-0.304]
</p><p>64 We evaluate classifier performance with a winner-take-all criterion, which is more conservative than the criterion in [1]. [sent-107, score-0.416]
</p><p>65 Then, for each row in the correlation matrix, the classifier “guesses” the identity of the test stimulus by selecting the element with the highest coefficient (shown on the right in Figure 3b and 3c). [sent-110, score-0.743]
</p><p>66 The previously-used method evaluated classifier performance by successively pairing off the correct stimulus with incorrect stimuli from the training set [1]. [sent-112, score-0.703]
</p><p>67 With this criterion, responses from the test set which do not correlate maximally with the same stimulus in the training set might still lead to high classifier performance. [sent-113, score-0.688]
</p><p>68 For instance, if an element Ri,i is larger than all but one coefficient in row i, pairwise comparisons would reveal correct guesses for 10 out of 11 comparisons, or 91% correct, while the winner-take-all criterion would consider this 0%. [sent-114, score-0.189]
</p><p>69 This conservative criterion reduces chance performance from 1/2 to 1/12, and ensures that high classifier performance reflects a high level of discriminability between different stimuli, providing a stringent test for decoding. [sent-115, score-0.551]
</p><p>70 4  Identifying voxels which distinguish objects  When we examined response patterns across all object-selective voxels, we observed high levels of classifier performance from some recording sessions, as shown in Session A in Figure 3. [sent-116, score-1.295]
</p><p>71 Many sessions, however, were more similar to Session B: limited success at decoding object identity when using all voxels. [sent-117, score-0.178]
</p><p>72 The distributed representation implied in Session A may be driven by only a few informative voxels; conversely, excessively noisy or unrelated activity from other voxels may be affected classifier performance on Session B. [sent-119, score-0.974]
</p><p>73 This is of particular concern given that the functional organization of this cortex is not well understood. [sent-120, score-0.187]
</p><p>74 In addition to using such classifiers to test a hypothesis that a pre-defined region of interest can discriminate stimuli, it would be highly useful to use the classifier to identify cortical regions which represent a stimulus. [sent-121, score-0.548]
</p><p>75 1  Voxel reliability metric  The voxel reliability metric is computed for each voxel by taking the vectors of 12 response amplitudes to each stimulus in the training and test sets, and calculating their correlation coefficient. [sent-124, score-1.461]
</p><p>76 Voxels with high reliability will have high values for the diagonal elements in the R correlation matrix, but this does not place constraints on correlations for the off-diagonal comparisons. [sent-125, score-0.162]
</p><p>77 through co-registering data across many sessions; and reduce the number of possible response bins for the data. [sent-133, score-0.227]
</p><p>78 Given the low number of trials per image, we reduce the number of possible response levels to only two bins, 0 and 1. [sent-135, score-0.163]
</p><p>79 Given these two bins, the next question is deciding how to threshold responses to decide if a given voxel responded significantly (r=1) or not (r=0) on a given trial. [sent-137, score-0.452]
</p><p>80 5  1  Subset Size [Proportion of all voxels]  0  Chance Performance  0  500 1000 1500  Subset Size [Voxels]  Figure 4: Comparison of metrics for identifying reliable subsets of voxels in data sets. [sent-141, score-0.784]
</p><p>81 (a) Performance on winner-take-all classifier of different-sized subsets of one data set (“Session B” in Figure 3), sorted by voxel reliability (gray, solid) and mutual information (red, dashed) metrics. [sent-142, score-1.008]
</p><p>82 (c) Performance on data set from (a) when reverse-sorting voxels by each metric. [sent-145, score-0.583]
</p><p>83 After ranking each voxel with the two metrics, we evaluated how well these voxels found reliable object representations. [sent-147, score-1.022]
</p><p>84 To do this, we sorted the voxels in descending order according to each metric; selected progressively larger subsets of voxels, starting with the 10 highest-ranked voxels and proceeding to the full set of voxels; and evaluated performance on the classifier for each subset. [sent-148, score-1.666]
</p><p>85 As can be seen, while performance using all voxels is at 42% correct, by removing voxels, performance quickly reaches 100% using the reliability criterion. [sent-151, score-0.749]
</p><p>86 Figure 4c shows the mean performance across sessions for the two metrics. [sent-155, score-0.16]
</p><p>87 These curves are normalized by the proportion of total available voxels for each data set. [sent-156, score-0.583]
</p><p>88 Note that simply removing voxels does not guarantee the better performance on the classifier. [sent-158, score-0.611]
</p><p>89 If the voxels are sorted in reverse order, starting with e. [sent-159, score-0.608]
</p><p>90 the lowest values of voxel reliability or mutual information, subsets containing half the voxels are consistently at or below chance performance (Figure 4c). [sent-161, score-1.272]
</p><p>91 One drawback to these methods, however, is that they often require prior knowledge of which voxels are involved in specifying a cognitive state, and which aren’t. [sent-163, score-0.583]
</p><p>92 Given the poorly-understood functional organization of the majority of cortex, an important goal is to develop methods to search across cortex for regions which represent such states. [sent-164, score-0.283]
</p><p>93 Our voxel-ranking metrics successfully identified subsets of object-selective voxels  which discriminate object identity. [sent-166, score-0.959]
</p><p>94 This demonstrates the feasibility of adapting classifier methods to search across cortical regions. [sent-167, score-0.429]
</p><p>95 The most important improvement is providing a larger set of trials from which to compute response probabilities. [sent-169, score-0.163]
</p><p>96 Given more extensive data, the set of possible response bins can be increased from the current binary set, which should improve performance of our mutual information metric. [sent-171, score-0.286]
</p><p>97 Moreover, this discrimination could be performed with sets of voxels of widely varying sizes. [sent-174, score-0.635]
</p><p>98 This has implications for the distributed nature of object representation in extrastriate cortex. [sent-176, score-0.165]
</p><p>99 In this way, we can use intuitions generated by ideal observers of the data, such as the classifier described here,and apply them to understanding how the brain processes this information. [sent-180, score-0.363]
</p><p>100 (2001) Complex objects are represented in macaque inferotemporal cortex by the combination of feature columns. [sent-211, score-0.162]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('voxels', 0.583), ('classifier', 0.363), ('voxel', 0.329), ('fmri', 0.222), ('session', 0.147), ('stimulus', 0.137), ('response', 0.128), ('cortex', 0.117), ('imaged', 0.11), ('reliability', 0.11), ('object', 0.11), ('mutual', 0.097), ('responses', 0.092), ('stimuli', 0.085), ('subsets', 0.084), ('boxster', 0.079), ('buffalo', 0.079), ('bull', 0.079), ('donkey', 0.079), ('dragster', 0.079), ('image', 0.069), ('ferret', 0.069), ('identified', 0.069), ('identity', 0.068), ('across', 0.066), ('sessions', 0.066), ('metrics', 0.065), ('bold', 0.065), ('atest', 0.063), ('nat', 0.063), ('metric', 0.06), ('bus', 0.058), ('truck', 0.058), ('images', 0.057), ('coefficient', 0.055), ('extrastriate', 0.055), ('training', 0.053), ('correlation', 0.052), ('recording', 0.052), ('identifying', 0.052), ('anatomical', 0.05), ('amplitudes', 0.05), ('discriminate', 0.048), ('coil', 0.047), ('refs', 0.047), ('guesses', 0.047), ('sec', 0.046), ('visual', 0.046), ('objects', 0.045), ('test', 0.043), ('decoded', 0.041), ('efficient', 0.041), ('scrambled', 0.041), ('chance', 0.041), ('reliably', 0.041), ('classifiers', 0.038), ('ventral', 0.038), ('correct', 0.037), ('functional', 0.036), ('trials', 0.035), ('imaging', 0.035), ('organization', 0.034), ('bins', 0.033), ('extent', 0.032), ('aren', 0.032), ('atraining', 0.032), ('haxby', 0.032), ('hemodynamic', 0.032), ('kalanit', 0.032), ('malach', 0.032), ('ress', 0.032), ('sayres', 0.032), ('significantly', 0.031), ('category', 0.031), ('percent', 0.031), ('regions', 0.03), ('patterns', 0.03), ('amplitude', 0.029), ('discrimination', 0.028), ('mm', 0.028), ('illustrated', 0.028), ('performance', 0.028), ('trial', 0.027), ('stanford', 0.027), ('human', 0.027), ('identify', 0.026), ('criterion', 0.025), ('respond', 0.025), ('chechik', 0.025), ('occipital', 0.025), ('courses', 0.025), ('drawings', 0.025), ('row', 0.025), ('sorted', 0.025), ('sets', 0.024), ('baseline', 0.024), ('contained', 0.024), ('disadvantages', 0.023), ('resonance', 0.023), ('reflects', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="94-tfidf-1" href="./nips-2005-Identifying_Distributed_Object_Representations_in_Human_Extrastriate_Visual_Cortex.html">94 nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</a></p>
<p>Author: Rory Sayres, David Ress, Kalanit Grill-spector</p><p>Abstract: The category of visual stimuli has been reliably decoded from patterns of neural activity in extrastriate visual cortex [1]. It has yet to be seen whether object identity can be inferred from this activity. We present fMRI data measuring responses in human extrastriate cortex to a set of 12 distinct object images. We use a simple winner-take-all classifier, using half the data from each recording session as a training set, to evaluate encoding of object identity across fMRI voxels. Since this approach is sensitive to the inclusion of noisy voxels, we describe two methods for identifying subsets of voxels in the data which optimally distinguish object identity. One method characterizes the reliability of each voxel within subsets of the data, while another estimates the mutual information of each voxel with the stimulus set. We find that both metrics can identify subsets of the data which reliably encode object identity, even when noisy measurements are artificially added to the data. The mutual information metric is less efficient at this task, likely due to constraints in fMRI data. 1</p><p>2 0.16013867 <a title="94-tfidf-2" href="./nips-2005-Modeling_Neuronal_Interactivity_using_Dynamic_Bayesian_Networks.html">130 nips-2005-Modeling Neuronal Interactivity using Dynamic Bayesian Networks</a></p>
<p>Author: Lei Zhang, Dimitris Samaras, Nelly Alia-klein, Nora Volkow, Rita Goldstein</p><p>Abstract: Functional Magnetic Resonance Imaging (fMRI) has enabled scientists to look into the active brain. However, interactivity between functional brain regions, is still little studied. In this paper, we contribute a novel framework for modeling the interactions between multiple active brain regions, using Dynamic Bayesian Networks (DBNs) as generative models for brain activation patterns. This framework is applied to modeling of neuronal circuits associated with reward. The novelty of our framework from a Machine Learning perspective lies in the use of DBNs to reveal the brain connectivity and interactivity. Such interactivity models which are derived from fMRI data are then validated through a group classiﬁcation task. We employ and compare four different types of DBNs: Parallel Hidden Markov Models, Coupled Hidden Markov Models, Fully-linked Hidden Markov Models and Dynamically MultiLinked HMMs (DML-HMM). Moreover, we propose and compare two schemes of learning DML-HMMs. Experimental results show that by using DBNs, group classiﬁcation can be performed even if the DBNs are constructed from as few as 5 brain regions. We also demonstrate that, by using the proposed learning algorithms, different DBN structures characterize drug addicted subjects vs. control subjects. This ﬁnding provides an independent test for the effect of psychopathology on brain function. In general, we demonstrate that incorporation of computer science principles into functional neuroimaging clinical studies provides a novel approach for probing human brain function.</p><p>3 0.14449938 <a title="94-tfidf-3" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<p>Author: Inna Weiner, Tomer Hertz, Israel Nelken, Daphna Weinshall</p><p>Abstract: We present a novel approach to the characterization of complex sensory neurons. One of the main goals of characterizing sensory neurons is to characterize dimensions in stimulus space to which the neurons are highly sensitive (causing large gradients in the neural responses) or alternatively dimensions in stimulus space to which the neuronal response are invariant (deﬁning iso-response manifolds). We formulate this problem as that of learning a geometry on stimulus space that is compatible with the neural responses: the distance between stimuli should be large when the responses they evoke are very different, and small when the responses they evoke are similar. Here we show how to successfully train such distance functions using rather limited amount of information. The data consisted of the responses of neurons in primary auditory cortex (A1) of anesthetized cats to 32 stimuli derived from natural sounds. For each neuron, a subset of all pairs of stimuli was selected such that the responses of the two stimuli in a pair were either very similar or very dissimilar. The distance function was trained to ﬁt these constraints. The resulting distance functions generalized to predict the distances between the responses of a test stimulus and the trained stimuli. 1</p><p>4 0.091161594 <a title="94-tfidf-4" href="./nips-2005-Learning_Cue-Invariant_Visual_Responses.html">109 nips-2005-Learning Cue-Invariant Visual Responses</a></p>
<p>Author: Jarmo Hurri</p><p>Abstract: Multiple visual cues are used by the visual system to analyze a scene; achromatic cues include luminance, texture, contrast and motion. Singlecell recordings have shown that the mammalian visual cortex contains neurons that respond similarly to scene structure (e.g., orientation of a boundary), regardless of the cue type conveying this information. This paper shows that cue-invariant response properties of simple- and complex-type cells can be learned from natural image data in an unsupervised manner. In order to do this, we also extend a previous conceptual model of cue invariance so that it can be applied to model simple- and complex-cell responses. Our results relate cue-invariant response properties to natural image statistics, thereby showing how the statistical modeling approach can be used to model processing beyond the elemental response properties visual neurons. This work also demonstrates how to learn, from natural image data, more sophisticated feature detectors than those based on changes in mean luminance, thereby paving the way for new data-driven approaches to image processing and computer vision. 1</p><p>5 0.083159469 <a title="94-tfidf-5" href="./nips-2005-A_Computational_Model_of_Eye_Movements_during_Object_Class_Detection.html">5 nips-2005-A Computational Model of Eye Movements during Object Class Detection</a></p>
<p>Author: Wei Zhang, Hyejin Yang, Dimitris Samaras, Gregory J. Zelinsky</p><p>Abstract: We present a computational model of human eye movements in an object class detection task. The model combines state-of-the-art computer vision object class detection methods (SIFT features trained using AdaBoost) with a biologically plausible model of human eye movement to produce a sequence of simulated ﬁxations, culminating with the acquisition of a target. We validated the model by comparing its behavior to the behavior of human observers performing the identical object class detection task (looking for a teddy bear among visually complex nontarget objects). We found considerable agreement between the model and human data in multiple eye movement measures, including number of ﬁxations, cumulative probability of ﬁxating the target, and scanpath distance.</p><p>6 0.069420047 <a title="94-tfidf-6" href="./nips-2005-Norepinephrine_and_Neural_Interrupts.html">141 nips-2005-Norepinephrine and Neural Interrupts</a></p>
<p>7 0.068944864 <a title="94-tfidf-7" href="./nips-2005-Bayesian_Surprise_Attracts_Human_Attention.html">34 nips-2005-Bayesian Surprise Attracts Human Attention</a></p>
<p>8 0.068758361 <a title="94-tfidf-8" href="./nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">151 nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<p>9 0.068046361 <a title="94-tfidf-9" href="./nips-2005-Efficient_Unsupervised_Learning_for_Localization_and_Detection_in_Object_Categories.html">63 nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</a></p>
<p>10 0.062209927 <a title="94-tfidf-10" href="./nips-2005-Sensory_Adaptation_within_a_Bayesian_Framework_for_Perception.html">173 nips-2005-Sensory Adaptation within a Bayesian Framework for Perception</a></p>
<p>11 0.058543701 <a title="94-tfidf-11" href="./nips-2005-Top-Down_Control_of_Visual_Attention%3A_A_Rational_Account.html">194 nips-2005-Top-Down Control of Visual Attention: A Rational Account</a></p>
<p>12 0.054686073 <a title="94-tfidf-12" href="./nips-2005-An_exploration-exploitation_model_based_on_norepinepherine_and_dopamine_activity.html">26 nips-2005-An exploration-exploitation model based on norepinepherine and dopamine activity</a></p>
<p>13 0.053992316 <a title="94-tfidf-13" href="./nips-2005-Neuronal_Fiber_Delineation_in_Area_of_Edema_from_Diffusion_Weighted_MRI.html">135 nips-2005-Neuronal Fiber Delineation in Area of Edema from Diffusion Weighted MRI</a></p>
<p>14 0.05384896 <a title="94-tfidf-14" href="./nips-2005-A_Cortically-Plausible_Inverse_Problem_Solving_Method_Applied_to_Recognizing_Static_and_Kinematic_3D_Objects.html">7 nips-2005-A Cortically-Plausible Inverse Problem Solving Method Applied to Recognizing Static and Kinematic 3D Objects</a></p>
<p>15 0.050786223 <a title="94-tfidf-15" href="./nips-2005-Stimulus_Evoked_Independent_Factor_Analysis_of_MEG_Data_with_Large_Background_Activity.html">183 nips-2005-Stimulus Evoked Independent Factor Analysis of MEG Data with Large Background Activity</a></p>
<p>16 0.050740816 <a title="94-tfidf-16" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<p>17 0.05067404 <a title="94-tfidf-17" href="./nips-2005-Integrate-and-Fire_models_with_adaptation_are_good_enough.html">99 nips-2005-Integrate-and-Fire models with adaptation are good enough</a></p>
<p>18 0.049229141 <a title="94-tfidf-18" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>19 0.047252476 <a title="94-tfidf-19" href="./nips-2005-Infinite_latent_feature_models_and_the_Indian_buffet_process.html">98 nips-2005-Infinite latent feature models and the Indian buffet process</a></p>
<p>20 0.046002422 <a title="94-tfidf-20" href="./nips-2005-Describing_Visual_Scenes_using_Transformed_Dirichlet_Processes.html">55 nips-2005-Describing Visual Scenes using Transformed Dirichlet Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.143), (1, -0.078), (2, 0.001), (3, 0.175), (4, -0.038), (5, 0.097), (6, 0.027), (7, -0.049), (8, -0.076), (9, 0.016), (10, 0.024), (11, -0.101), (12, 0.015), (13, -0.064), (14, -0.021), (15, 0.038), (16, 0.01), (17, -0.054), (18, -0.067), (19, 0.103), (20, -0.055), (21, 0.176), (22, 0.104), (23, -0.008), (24, -0.11), (25, -0.083), (26, -0.057), (27, -0.042), (28, -0.004), (29, -0.015), (30, -0.073), (31, -0.031), (32, 0.01), (33, -0.054), (34, 0.01), (35, 0.123), (36, 0.135), (37, 0.065), (38, -0.052), (39, 0.08), (40, -0.022), (41, 0.087), (42, -0.08), (43, 0.042), (44, 0.077), (45, 0.015), (46, -0.088), (47, -0.057), (48, -0.096), (49, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94924116 <a title="94-lsi-1" href="./nips-2005-Identifying_Distributed_Object_Representations_in_Human_Extrastriate_Visual_Cortex.html">94 nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</a></p>
<p>Author: Rory Sayres, David Ress, Kalanit Grill-spector</p><p>Abstract: The category of visual stimuli has been reliably decoded from patterns of neural activity in extrastriate visual cortex [1]. It has yet to be seen whether object identity can be inferred from this activity. We present fMRI data measuring responses in human extrastriate cortex to a set of 12 distinct object images. We use a simple winner-take-all classifier, using half the data from each recording session as a training set, to evaluate encoding of object identity across fMRI voxels. Since this approach is sensitive to the inclusion of noisy voxels, we describe two methods for identifying subsets of voxels in the data which optimally distinguish object identity. One method characterizes the reliability of each voxel within subsets of the data, while another estimates the mutual information of each voxel with the stimulus set. We find that both metrics can identify subsets of the data which reliably encode object identity, even when noisy measurements are artificially added to the data. The mutual information metric is less efficient at this task, likely due to constraints in fMRI data. 1</p><p>2 0.67756379 <a title="94-lsi-2" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<p>Author: Inna Weiner, Tomer Hertz, Israel Nelken, Daphna Weinshall</p><p>Abstract: We present a novel approach to the characterization of complex sensory neurons. One of the main goals of characterizing sensory neurons is to characterize dimensions in stimulus space to which the neurons are highly sensitive (causing large gradients in the neural responses) or alternatively dimensions in stimulus space to which the neuronal response are invariant (deﬁning iso-response manifolds). We formulate this problem as that of learning a geometry on stimulus space that is compatible with the neural responses: the distance between stimuli should be large when the responses they evoke are very different, and small when the responses they evoke are similar. Here we show how to successfully train such distance functions using rather limited amount of information. The data consisted of the responses of neurons in primary auditory cortex (A1) of anesthetized cats to 32 stimuli derived from natural sounds. For each neuron, a subset of all pairs of stimuli was selected such that the responses of the two stimuli in a pair were either very similar or very dissimilar. The distance function was trained to ﬁt these constraints. The resulting distance functions generalized to predict the distances between the responses of a test stimulus and the trained stimuli. 1</p><p>3 0.60381639 <a title="94-lsi-3" href="./nips-2005-Modeling_Neuronal_Interactivity_using_Dynamic_Bayesian_Networks.html">130 nips-2005-Modeling Neuronal Interactivity using Dynamic Bayesian Networks</a></p>
<p>Author: Lei Zhang, Dimitris Samaras, Nelly Alia-klein, Nora Volkow, Rita Goldstein</p><p>Abstract: Functional Magnetic Resonance Imaging (fMRI) has enabled scientists to look into the active brain. However, interactivity between functional brain regions, is still little studied. In this paper, we contribute a novel framework for modeling the interactions between multiple active brain regions, using Dynamic Bayesian Networks (DBNs) as generative models for brain activation patterns. This framework is applied to modeling of neuronal circuits associated with reward. The novelty of our framework from a Machine Learning perspective lies in the use of DBNs to reveal the brain connectivity and interactivity. Such interactivity models which are derived from fMRI data are then validated through a group classiﬁcation task. We employ and compare four different types of DBNs: Parallel Hidden Markov Models, Coupled Hidden Markov Models, Fully-linked Hidden Markov Models and Dynamically MultiLinked HMMs (DML-HMM). Moreover, we propose and compare two schemes of learning DML-HMMs. Experimental results show that by using DBNs, group classiﬁcation can be performed even if the DBNs are constructed from as few as 5 brain regions. We also demonstrate that, by using the proposed learning algorithms, different DBN structures characterize drug addicted subjects vs. control subjects. This ﬁnding provides an independent test for the effect of psychopathology on brain function. In general, we demonstrate that incorporation of computer science principles into functional neuroimaging clinical studies provides a novel approach for probing human brain function.</p><p>4 0.51385236 <a title="94-lsi-4" href="./nips-2005-Bayesian_Surprise_Attracts_Human_Attention.html">34 nips-2005-Bayesian Surprise Attracts Human Attention</a></p>
<p>Author: Laurent Itti, Pierre F. Baldi</p><p>Abstract: The concept of surprise is central to sensory processing, adaptation, learning, and attention. Yet, no widely-accepted mathematical theory currently exists to quantitatively characterize surprise elicited by a stimulus or event, for observers that range from single neurons to complex natural or engineered systems. We describe a formal Bayesian deﬁnition of surprise that is the only consistent formulation under minimal axiomatic assumptions. Surprise quantiﬁes how data affects a natural or artiﬁcial observer, by measuring the difference between posterior and prior beliefs of the observer. Using this framework we measure the extent to which humans direct their gaze towards surprising items while watching television and video games. We ﬁnd that subjects are strongly attracted towards surprising locations, with 72% of all human gaze shifts directed towards locations more surprising than the average, a ﬁgure which rises to 84% when considering only gaze targets simultaneously selected by all subjects. The resulting theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction. Life is full of surprises, ranging from a great christmas gift or a new magic trick, to wardrobe malfunctions, reckless drivers, terrorist attacks, and tsunami waves. Key to survival is our ability to rapidly attend to, identify, and learn from surprising events, to decide on present and future courses of action [1]. Yet, little theoretical and computational understanding exists of the very essence of surprise, as evidenced by the absence from our everyday vocabulary of a quantitative unit of surprise: Qualities such as the “wow factor” have remained vague and elusive to mathematical analysis. Informal correlates of surprise exist at nearly all stages of neural processing. In sensory neuroscience, it has been suggested that only the unexpected at one stage is transmitted to the next stage [2]. Hence, sensory cortex may have evolved to adapt to, to predict, and to quiet down the expected statistical regularities of the world [3, 4, 5, 6], focusing instead on events that are unpredictable or surprising. Electrophysiological evidence for this early sensory emphasis onto surprising stimuli exists from studies of adaptation in visual [7, 8, 4, 9], olfactory [10, 11], and auditory cortices [12], subcortical structures like the LGN [13], and even retinal ganglion cells [14, 15] and cochlear hair cells [16]: neural response greatly attenuates with repeated or prolonged exposure to an initially novel stimulus. Surprise and novelty are also central to learning and memory formation [1], to the point that surprise is believed to be a necessary trigger for associative learning [17, 18], as supported by mounting evidence for a role of the hippocampus as a novelty detector [19, 20, 21]. Finally, seeking novelty is a well-identiﬁed human character trait, with possible association with the dopamine D4 receptor gene [22, 23, 24]. In the Bayesian framework, we develop the only consistent theory of surprise, in terms of the difference between the posterior and prior distributions of beliefs of an observer over the available class of models or hypotheses about the world. We show that this deﬁnition derived from ﬁrst principles presents key advantages over more ad-hoc formulations, typically relying on detecting outlier stimuli. Armed with this new framework, we provide direct experimental evidence that surprise best characterizes what attracts human gaze in large amounts of natural video stimuli. We here extend a recent pilot study [25], adding more comprehensive theory, large-scale human data collection, and additional analysis. 1 Theory Bayesian Deﬁnition of Surprise. We propose that surprise is a general concept, which can be derived from ﬁrst principles and formalized across spatio-temporal scales, sensory modalities, and, more generally, data types and data sources. Two elements are essential for a principled deﬁnition of surprise. First, surprise can exist only in the presence of uncertainty, which can arise from intrinsic stochasticity, missing information, or limited computing resources. A world that is purely deterministic and predictable in real-time for a given observer contains no surprises. Second, surprise can only be deﬁned in a relative, subjective, manner and is related to the expectations of the observer, be it a single synapse, neuronal circuit, organism, or computer device. The same data may carry different amount of surprise for different observers, or even for the same observer taken at different times. In probability and decision theory it can be shown that the only consistent and optimal way for modeling and reasoning about uncertainty is provided by the Bayesian theory of probability [26, 27, 28]. Furthermore, in the Bayesian framework, probabilities correspond to subjective degrees of beliefs in hypotheses or models which are updated, as data is acquired, using Bayes’ theorem as the fundamental tool for transforming prior belief distributions into posterior belief distributions. Therefore, within the same optimal framework, the only consistent deﬁnition of surprise must involve: (1) probabilistic concepts to cope with uncertainty; and (2) prior and posterior distributions to capture subjective expectations. Consistently with this Bayesian approach, the background information of an observer is captured by his/her/its prior probability distribution {P (M )}M ∈M over the hypotheses or models M in a model space M. Given this prior distribution of beliefs, the fundamental effect of a new data observation D on the observer is to change the prior distribution {P (M )}M ∈M into the posterior distribution {P (M |D)}M ∈M via Bayes theorem, whereby P (D|M ) ∀M ∈ M, P (M |D) = P (M ). (1) P (D) In this framework, the new data observation D carries no surprise if it leaves the observer beliefs unaffected, that is, if the posterior is identical to the prior; conversely, D is surprising if the posterior distribution resulting from observing D signiﬁcantly differs from the prior distribution. Therefore we formally measure surprise elicited by data as some distance measure between the posterior and prior distributions. This is best done using the relative entropy or Kullback-Leibler (KL) divergence [29]. Thus, surprise is deﬁned by the average of the log-odd ratio: P (M |D) S(D, M) = KL(P (M |D), P (M )) = P (M |D) log dM (2) P (M ) M taken with respect to the posterior distribution over the model class M. Note that KL is not symmetric but has well-known theoretical advantages, including invariance with respect to Figure 1: Computing surprise in early sensory neurons. (a) Prior data observations, tuning preferences, and top-down inﬂuences contribute to shaping a set of “prior beliefs” a neuron may have over a class of internal models or hypotheses about the world. For instance, M may be a set of Poisson processes parameterized by the rate λ, with {P (M )}M ∈M = {P (λ)}λ∈I +∗ the prior distribution R of beliefs about which Poisson models well describe the world as sensed by the neuron. New data D updates the prior into the posterior using Bayes’ theorem. Surprise quantiﬁes the difference between the posterior and prior distributions over the model class M. The remaining panels detail how surprise differs from conventional model ﬁtting and outlier-based novelty. (b) In standard iterative Bayesian model ﬁtting, at every iteration N , incoming data DN is used to update the prior {P (M |D1 , D2 , ..., DN −1 )}M ∈M into the posterior {P (M |D1 , D2 , ..., DN )}M ∈M . Freezing this learning at a given iteration, one then picks the currently best model, usually using either a maximum likelihood criterion, or a maximum a posteriori one (yielding MM AP shown). (c) This best model is used for a number of tasks at the current iteration, including outlier-based novelty detection. New data is then considered novel at that instant if it has low likelihood for the best model b a (e.g., DN is more novel than DN ). This focus onto the single best model presents obvious limitations, especially in situations where other models are nearly as good (e.g., M∗ in panel (b) is entirely ignored during standard novelty computation). One palliative solution is to consider mixture models, or simply P (D), but this just amounts to shifting the problem into a different model class. (d) Surprise directly addresses this problem by simultaneously considering all models and by measuring how data changes the observer’s distribution of beliefs from {P (M |D1 , D2 , ..., DN −1 )}M ∈M to {P (M |D1 , D2 , ..., DN )}M ∈M over the entire model class M (orange shaded area). reparameterizations. A unit of surprise — a “wow” — may then be deﬁned for a single model M as the amount of surprise corresponding to a two-fold variation between P (M |D) and P (M ), i.e., as log P (M |D)/P (M ) (with log taken in base 2), with the total number of wows experienced for all models obtained through the integration in eq. 2. Surprise and outlier detection. Outlier detection based on the likelihood P (D|M best ) of D given a single best model Mbest is at best an approximation to surprise and, in some cases, is misleading. Consider, for instance, a case where D has very small probability both for a model or hypothesis M and for a single alternative hypothesis M. Although D is a strong outlier, it carries very little information regarding whether M or M is the better model, and therefore very little surprise. Thus an outlier detection method would strongly focus attentional resources onto D, although D is a false positive, in the sense that it carries no useful information for discriminating between the two alternative hypotheses M and M. Figure 1 further illustrates this disconnect between outlier detection and surprise. 2 Human experiments To test the surprise hypothesis — that surprise attracts human attention and gaze in natural scenes — we recorded eye movements from eight na¨ve observers (three females and ı ﬁve males, ages 23-32, normal or corrected-to-normal vision). Each watched a subset from 50 videoclips totaling over 25 minutes of playtime (46,489 video frames, 640 × 480, 60.27 Hz, mean screen luminance 30 cd/m2 , room 4 cd/m2 , viewing distance 80cm, ﬁeld of view 28◦ × 21◦ ). Clips comprised outdoors daytime and nighttime scenes of crowded environments, video games, and television broadcast including news, sports, and commercials. Right-eye position was tracked with a 240 Hz video-based device (ISCAN RK-464), with methods as previously [30]. Two hundred calibrated eye movement traces (10,192 saccades) were analyzed, corresponding to four distinct observers for each of the 50 clips. Figure 2 shows sample scanpaths for one videoclip. To characterize image regions selected by participants, we process videoclips through computational metrics that output a topographic dynamic master response map, assigning in real-time a response value to every input location. A good master map would highlight, more than expected by chance, locations gazed to by observers. To score each metric we hence sample, at onset of every human saccade, master map activity around the saccade’s future endpoint, and around a uniformly random endpoint (random sampling was repeated 100 times to evaluate variability). We quantify differences between histograms of master Figure 2: (a) Sample eye movement traces from four observers (squares denote saccade endpoints). (b) Our data exhibits high inter-individual overlap, shown here with the locations where one human saccade endpoint was nearby (≈ 5◦ ) one (white squares), two (cyan squares), or all three (black squares) other humans. (c) A metric where the master map was created from the three eye movement traces other than that being tested yields an upper-bound KL score, computed by comparing the histograms of metric values at human (narrow blue bars) and random (wider green bars) saccade targets. Indeed, this metric’s map was very sparse (many random saccades landing on locations with nearzero response), yet humans preferentially saccaded towards the three active hotspots corresponding to the eye positions of three other humans (many human saccades landing on locations with near-unity responses). map samples collected from human and random saccades using again the Kullback-Leibler (KL) distance: metrics which better predict human scanpaths exhibit higher distances from random as, typically, observers non-uniformly gaze towards a minority of regions with highest metric responses while avoiding a majority of regions with low metric responses. This approach presents several advantages over simpler scoring schemes [31, 32], including agnosticity to putative mechanisms for generating saccades and the fact that applying any continuous nonlinearity to master map values would not affect scoring. Experimental results. We test six computational metrics, encompassing and extending the state-of-the-art found in previous studies. The ﬁrst three quantify static image properties (local intensity variance in 16 × 16 image patches [31]; local oriented edge density as measured with Gabor ﬁlters [33]; and local Shannon entropy in 16 × 16 image patches [34]). The remaining three metrics are more sensitive to dynamic events (local motion [33]; outlier-based saliency [33]; and surprise [25]). For all metrics, we ﬁnd that humans are signiﬁcantly attracted by image regions with higher metric responses. However, the static metrics typically respond vigorously at numerous visual locations (Figure 3), hence they are poorly speciﬁc and yield relatively low KL scores between humans and random. The metrics sensitive to motion, outliers, and surprising events, in comparison, yield sparser maps and higher KL scores. The surprise metric of interest here quantiﬁes low-level surprise in image patches over space and time, and at this point does not account for high-level or cognitive beliefs of our human observers. Rather, it assumes a family of simple models for image patches, each processed through 72 early feature detectors sensitive to color, orientation, motion, etc., and computes surprise from shifts in the distribution of beliefs about which models better describe the patches (see [25] and [35] for details). We ﬁnd that the surprise metric signiﬁcantly outperforms all other computational metrics (p < 10−100 or better on t-tests for equality of KL scores), scoring nearly 20% better than the second-best metric (saliency) and 60% better than the best static metric (entropy). Surprising stimuli often substantially differ from simple feature outliers; for example, a continually blinking light on a static background elicits sustained ﬂicker due to its locally outlier temporal dynamics but is only surprising for a moment. Similarly, a shower of randomly-colored pixels continually excites all low-level feature detectors but rapidly becomes unsurprising. Strongest attractors of human attention. Clearly, in our and previous eye-tracking experiments, in some situations potentially interesting targets were more numerous than in others. With many possible targets, different observers may orient towards different locations, making it more difﬁcult for a single metric to accurately predict all observers. Hence we consider (Figure 4) subsets of human saccades where at least two, three, or all four observers simultaneously agreed on a gaze target. Observers could have agreed based on bottom-up factors (e.g., only one location had interesting visual appearance at that time), top-down factors (e.g., only one object was of current cognitive interest), or both (e.g., a single cognitively interesting object was present which also had distinctive appearance). Irrespectively of the cause for agreement, it indicates consolidated belief that a location was attractive. While the KL scores of all metrics improved when progressively focusing onto only those locations, dynamic metrics improved more steeply, indicating that stimuli which more reliably attracted all observers carried more motion, saliency, and surprise. Surprise remained signiﬁcantly the best metric to characterize these agreed-upon attractors of human gaze (p < 10−100 or better on t-tests for equality of KL scores). Overall, surprise explained the greatest fraction of human saccades, indicating that humans are signiﬁcantly attracted towards surprising locations in video displays. Over 72% of all human saccades were targeted to locations predicted to be more surprising than on average. When only considering saccades where two, three, or four observers agreed on a common gaze target, this ﬁgure rose to 76%, 80%, and 84%, respectively. Figure 3: (a) Sample video frames, with corresponding human saccades and predictions from the entropy, surprise, and human-derived metrics. Entropy maps, like intensity variance and orientation maps, exhibited many locations with high responses, hence had low speciﬁcity and were poorly discriminative. In contrast, motion, saliency, and surprise maps were much sparser and more speciﬁc, with surprise signiﬁcantly more often on target. For three example frames (ﬁrst column), saccades from one subject are shown (arrows) with corresponding apertures over which master map activity at the saccade endpoint was sampled (circles). (b) KL scores for these metrics indicate signiﬁcantly different performance levels, and a strict ranking of variance < orientation < entropy < motion < saliency < surprise < human-derived. KL scores were computed by comparing the number of human saccades landing onto each given range of master map values (narrow blue bars) to the number of random saccades hitting the same range (wider green bars). A score of zero would indicate equality between the human and random histograms, i.e., humans did not tend to hit various master map values any differently from expected by chance, or, the master map could not predict human saccades better than random saccades. Among the six computational metrics tested in total, surprise performed best, in that surprising locations were relatively few yet reliably gazed to by humans. Figure 4: KL scores when considering only saccades where at least one (all 10,192 saccades), two (7,948 saccades), three (5,565 saccades), or all four (2,951 saccades) humans agreed on a common gaze location, for the static (a) and dynamic metrics (b). Static metrics improved substantially when progressively focusing onto saccades with stronger inter-observer agreement (average slope 0.56 ± 0.37 percent KL score units per 1,000 pruned saccades). Hence, when humans agreed on a location, they also tended to be more reliably predicted by the metrics. Furthermore, dynamic metrics improved 4.5 times more steeply (slope 2.44 ± 0.37), suggesting a stronger role of dynamic events in attracting human attention. Surprising events were signiﬁcantly the strongest (t-tests for equality of KL scores between surprise and other metrics, p < 10−100 ). 3 Discussion While previous research has shown with either static scenes or dynamic synthetic stimuli that humans preferentially ﬁxate regions of high entropy [34], contrast [31], saliency [32], ﬂicker [36], or motion [37], our data provides direct experimental evidence that humans ﬁxate surprising locations even more reliably. These conclusions were made possible by developing new tools to quantify what attracts human gaze over space and time in dynamic natural scenes. Surprise explained best where humans look when considering all saccades, and even more so when restricting the analysis to only those saccades for which human observers tended to agree. Surprise hence represents an inexpensive, easily computable approximation to human attentional allocation. In the absence of quantitative tools to measure surprise, most experimental and modeling work to date has adopted the approximation that novel events are surprising, and has focused on experimental scenarios which are simple enough to ensure an overlap between informal notions of novelty and surprise: for example, a stimulus is novel during testing if it has not been seen during training [9]. Our deﬁnition opens new avenues for more sophisticated experiments, where surprise elicited by different stimuli can be precisely compared and calibrated, yielding predictions at the single-unit as well as behavioral levels. The deﬁnition of surprise — as the distance between the posterior and prior distributions of beliefs over models — is entirely general and readily applicable to the analysis of auditory, olfactory, gustatory, or somatosensory data. While here we have focused on behavior rather than detailed biophysical implementation, it is worth noting that detecting surprise in neural spike trains does not require semantic understanding of the data carried by the spike trains, and thus could provide guiding signals during self-organization and development of sensory areas. At higher processing levels, top-down cues and task demands are known to combine with stimulus novelty in capturing attention and triggering learning [1, 38], ideas which may now be formalized and quantiﬁed in terms of priors, posteriors, and surprise. Surprise, indeed, inherently depends on uncertainty and on prior beliefs. Hence surprise theory can further be tested and utilized in experiments where the prior is biased, for ex- ample by top-down instructions or prior exposures to stimuli [38]. In addition, simple surprise-based behavioral measures such as the eye-tracking one used here may prove useful for early diagnostic of human conditions including autism and attention-deﬁcit hyperactive disorder, as well as for quantitative comparison between humans and animals which may have lower or different priors, including monkeys, frogs, and ﬂies. Beyond sensory biology, computable surprise could guide the development of data mining and compression systems (giving more bits to surprising regions of interest), to ﬁnd surprising agents in crowds, surprising sentences in books or speeches, surprising sequences in genomes, surprising medical symptoms, surprising odors in airport luggage racks, surprising documents on the world-wide-web, or to design surprising advertisements. Acknowledgments: Supported by HFSP, NSF and NGA (L.I.), NIH and NSF (P.B.). We thank UCI’s Institute for Genomics and Bioinformatics and USC’s Center High Performance Computing and Communications (www.usc.edu/hpcc) for access to their computing clusters. References [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] Ranganath, C. & Rainer, G. Nat Rev Neurosci 4, 193–202 (2003). Rao, R. P. & Ballard, D. H. Nat Neurosci 2, 79–87 (1999). Olshausen, B. A. & Field, D. J. Nature 381, 607–609 (1996). M¨ ller, J. R., Metha, A. B., Krauskopf, J. & Lennie, P. Science 285, 1405–1408 (1999). u Dragoi, V., Sharma, J., Miller, E. K. & Sur, M. Nat Neurosci 5, 883–891 (2002). David, S. V., Vinje, W. E. & Gallant, J. L. J Neurosci 24, 6991–7006 (2004). Maffei, L., Fiorentini, A. & Bisti, S. Science 182, 1036–1038 (1973). Movshon, J. A. & Lennie, P. Nature 278, 850–852 (1979). Fecteau, J. H. & Munoz, D. P. Nat Rev Neurosci 4, 435–443 (2003). Kurahashi, T. & Menini, A. Nature 385, 725–729 (1997). Bradley, J., Bonigk, W., Yau, K. W. & Frings, S. Nat Neurosci 7, 705–710 (2004). Ulanovsky, N., Las, L. & Nelken, I. Nat Neurosci 6, 391–398 (2003). Solomon, S. G., Peirce, J. W., Dhruv, N. T. & Lennie, P. Neuron 42, 155–162 (2004). Smirnakis, S. M., Berry, M. J. & et al. Nature 386, 69–73 (1997). Brown, S. P. & Masland, R. H. Nat Neurosci 4, 44–51 (2001). Kennedy, H. J., Evans, M. G. & et al. Nat Neurosci 6, 832–836 (2003). Schultz, W. & Dickinson, A. Annu Rev Neurosci 23, 473–500 (2000). Fletcher, P. C., Anderson, J. M., Shanks, D. R. et al. Nat Neurosci 4, 1043–1048 (2001). Knight, R. Nature 383, 256–259 (1996). Stern, C. E., Corkin, S., Gonzalez, R. G. et al. Proc Natl Acad Sci U S A 93, 8660–8665 (1996). Li, S., Cullen, W. K., Anwyl, R. & Rowan, M. J. Nat Neurosci 6, 526–531 (2003). Ebstein, R. P., Novick, O., Umansky, R. et al. Nat Genet 12, 78–80 (1996). Benjamin, J., Li, L. & et al. Nat Genet 12, 81–84 (1996). Lusher, J. M., Chandler, C. & Ball, D. Mol Psychiatry 6, 497–499 (2001). Itti, L. & Baldi, P. In Proc. IEEE CVPR. San Siego, CA (2005 in press). Cox, R. T. Am. J. Phys. 14, 1–13 (1964). Savage, L. J. The foundations of statistics (Dover, New York, 1972). (First Edition in 1954). Jaynes, E. T. Probability Theory. The Logic of Science (Cambridge University Press, 2003). Kullback, S. Information Theory and Statistics (Wiley, New York:New York, 1959). Itti, L. Visual Cognition (2005 in press). Reinagel, P. & Zador, A. M. Network 10, 341–350 (1999). Parkhurst, D., Law, K. & Niebur, E. Vision Res 42, 107–123 (2002). Itti, L. & Koch, C. Nat Rev Neurosci 2, 194–203 (2001). Privitera, C. M. & Stark, L. W. IEEE Trans Patt Anal Mach Intell 22, 970–982 (2000). All source code for all metrics is freely available at http://iLab.usc.edu/toolkit/. Theeuwes, J. Percept Psychophys 57, 637–644 (1995). Abrams, R. A. & Christ, S. E. Psychol Sci 14, 427–432 (2003). Wolfe, J. M. & Horowitz, T. S. Nat Rev Neurosci 5, 495–501 (2004).</p><p>5 0.50337178 <a title="94-lsi-5" href="./nips-2005-Visual_Encoding_with_Jittering_Eyes.html">203 nips-2005-Visual Encoding with Jittering Eyes</a></p>
<p>Author: Michele Rucci</p><p>Abstract: Under natural viewing conditions, small movements of the eye and body prevent the maintenance of a steady direction of gaze. It is known that stimuli tend to fade when they are stabilized on the retina for several seconds. However, it is unclear whether the physiological self-motion of the retinal image serves a visual purpose during the brief periods of natural visual ﬁxation. This study examines the impact of ﬁxational instability on the statistics of visual input to the retina and on the structure of neural activity in the early visual system. Fixational instability introduces ﬂuctuations in the retinal input signals that, in the presence of natural images, lack spatial correlations. These input ﬂuctuations strongly inﬂuence neural activity in a model of the LGN. They decorrelate cell responses, even if the contrast sensitivity functions of simulated cells are not perfectly tuned to counter-balance the power-law spectrum of natural images. A decorrelation of neural activity has been proposed to be beneﬁcial for discarding statistical redundancies in the input signals. Fixational instability might, therefore, contribute to establishing efﬁcient representations of natural stimuli. 1</p><p>6 0.45256981 <a title="94-lsi-6" href="./nips-2005-Learning_Cue-Invariant_Visual_Responses.html">109 nips-2005-Learning Cue-Invariant Visual Responses</a></p>
<p>7 0.42325518 <a title="94-lsi-7" href="./nips-2005-An_exploration-exploitation_model_based_on_norepinepherine_and_dopamine_activity.html">26 nips-2005-An exploration-exploitation model based on norepinepherine and dopamine activity</a></p>
<p>8 0.41591159 <a title="94-lsi-8" href="./nips-2005-Bayesian_model_learning_in_human_visual_perception.html">35 nips-2005-Bayesian model learning in human visual perception</a></p>
<p>9 0.41384667 <a title="94-lsi-9" href="./nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">151 nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<p>10 0.39053068 <a title="94-lsi-10" href="./nips-2005-Describing_Visual_Scenes_using_Transformed_Dirichlet_Processes.html">55 nips-2005-Describing Visual Scenes using Transformed Dirichlet Processes</a></p>
<p>11 0.3792142 <a title="94-lsi-11" href="./nips-2005-Efficient_Unsupervised_Learning_for_Localization_and_Detection_in_Object_Categories.html">63 nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</a></p>
<p>12 0.37649068 <a title="94-lsi-12" href="./nips-2005-A_Computational_Model_of_Eye_Movements_during_Object_Class_Detection.html">5 nips-2005-A Computational Model of Eye Movements during Object Class Detection</a></p>
<p>13 0.35630387 <a title="94-lsi-13" href="./nips-2005-Ideal_Observers_for_Detecting_Motion%3A_Correspondence_Noise.html">93 nips-2005-Ideal Observers for Detecting Motion: Correspondence Noise</a></p>
<p>14 0.35606724 <a title="94-lsi-14" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>15 0.35523844 <a title="94-lsi-15" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>16 0.34420064 <a title="94-lsi-16" href="./nips-2005-Norepinephrine_and_Neural_Interrupts.html">141 nips-2005-Norepinephrine and Neural Interrupts</a></p>
<p>17 0.34062159 <a title="94-lsi-17" href="./nips-2005-Stimulus_Evoked_Independent_Factor_Analysis_of_MEG_Data_with_Large_Background_Activity.html">183 nips-2005-Stimulus Evoked Independent Factor Analysis of MEG Data with Large Background Activity</a></p>
<p>18 0.33964616 <a title="94-lsi-18" href="./nips-2005-Prediction_and_Change_Detection.html">156 nips-2005-Prediction and Change Detection</a></p>
<p>19 0.32622403 <a title="94-lsi-19" href="./nips-2005-Off-Road_Obstacle_Avoidance_through_End-to-End_Learning.html">143 nips-2005-Off-Road Obstacle Avoidance through End-to-End Learning</a></p>
<p>20 0.31630191 <a title="94-lsi-20" href="./nips-2005-Sensory_Adaptation_within_a_Bayesian_Framework_for_Perception.html">173 nips-2005-Sensory Adaptation within a Bayesian Framework for Perception</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.035), (10, 0.039), (11, 0.013), (12, 0.028), (22, 0.085), (25, 0.016), (27, 0.032), (31, 0.031), (34, 0.055), (39, 0.039), (41, 0.01), (50, 0.01), (55, 0.02), (60, 0.041), (65, 0.019), (69, 0.051), (70, 0.014), (73, 0.033), (87, 0.037), (88, 0.08), (91, 0.02), (92, 0.181), (94, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.7899648 <a title="94-lda-1" href="./nips-2005-Optimizing_spatio-temporal_filters_for_improving_Brain-Computer_Interfacing.html">150 nips-2005-Optimizing spatio-temporal filters for improving Brain-Computer Interfacing</a></p>
<p>Author: Guido Dornhege, Benjamin Blankertz, Matthias Krauledat, Florian Losch, Gabriel Curio, Klaus-Robert Müller</p><p>Abstract: Brain-Computer Interface (BCI) systems create a novel communication channel from the brain to an output device by bypassing conventional motor output pathways of nerves and muscles. Therefore they could provide a new communication and control option for paralyzed patients. Modern BCI technology is essentially based on techniques for the classiﬁcation of single-trial brain signals. Here we present a novel technique that allows the simultaneous optimization of a spatial and a spectral ﬁlter enhancing discriminability of multi-channel EEG single-trials. The evaluation of 60 experiments involving 22 different subjects demonstrates the superiority of the proposed algorithm. Apart from the enhanced classiﬁcation, the spatial and/or the spectral ﬁlter that are determined by the algorithm can also be used for further analysis of the data, e.g., for source localization of the respective brain rhythms.</p><p>same-paper 2 0.78602254 <a title="94-lda-2" href="./nips-2005-Identifying_Distributed_Object_Representations_in_Human_Extrastriate_Visual_Cortex.html">94 nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</a></p>
<p>Author: Rory Sayres, David Ress, Kalanit Grill-spector</p><p>Abstract: The category of visual stimuli has been reliably decoded from patterns of neural activity in extrastriate visual cortex [1]. It has yet to be seen whether object identity can be inferred from this activity. We present fMRI data measuring responses in human extrastriate cortex to a set of 12 distinct object images. We use a simple winner-take-all classifier, using half the data from each recording session as a training set, to evaluate encoding of object identity across fMRI voxels. Since this approach is sensitive to the inclusion of noisy voxels, we describe two methods for identifying subsets of voxels in the data which optimally distinguish object identity. One method characterizes the reliability of each voxel within subsets of the data, while another estimates the mutual information of each voxel with the stimulus set. We find that both metrics can identify subsets of the data which reliably encode object identity, even when noisy measurements are artificially added to the data. The mutual information metric is less efficient at this task, likely due to constraints in fMRI data. 1</p><p>3 0.76892531 <a title="94-lda-3" href="./nips-2005-Bayesian_models_of_human_action_understanding.html">36 nips-2005-Bayesian models of human action understanding</a></p>
<p>Author: Chris Baker, Rebecca Saxe, Joshua B. Tenenbaum</p><p>Abstract: We present a Bayesian framework for explaining how people reason about and predict the actions of an intentional agent, based on observing its behavior. Action-understanding is cast as a problem of inverting a probabilistic generative model, which assumes that agents tend to act rationally in order to achieve their goals given the constraints of their environment. Working in a simple sprite-world domain, we show how this model can be used to infer the goal of an agent and predict how the agent will act in novel situations or when environmental constraints change. The model provides a qualitative account of several kinds of inferences that preverbal infants have been shown to perform, and also ﬁts quantitative predictions that adult observers make in a new experiment.</p><p>4 0.71915358 <a title="94-lda-4" href="./nips-2005-Nested_sampling_for_Potts_models.html">133 nips-2005-Nested sampling for Potts models</a></p>
<p>Author: Iain Murray, David MacKay, Zoubin Ghahramani, John Skilling</p><p>Abstract: Nested sampling is a new Monte Carlo method by Skilling [1] intended for general Bayesian computation. Nested sampling provides a robust alternative to annealing-based methods for computing normalizing constants. It can also generate estimates of other quantities such as posterior expectations. The key technical requirement is an ability to draw samples uniformly from the prior subject to a constraint on the likelihood. We provide a demonstration with the Potts model, an undirected graphical model. 1</p><p>5 0.63823396 <a title="94-lda-5" href="./nips-2005-Phase_Synchrony_Rate_for_the_Recognition_of_Motor_Imagery_in_Brain-Computer_Interface.html">152 nips-2005-Phase Synchrony Rate for the Recognition of Motor Imagery in Brain-Computer Interface</a></p>
<p>Author: Le Song, Evian Gordon, Elly Gysels</p><p>Abstract: Motor imagery attenuates EEG µ and β rhythms over sensorimotor cortices. These amplitude changes are most successfully captured by the method of Common Spatial Patterns (CSP) and widely used in braincomputer interfaces (BCI). BCI methods based on amplitude information, however, have not incoporated the rich phase dynamics in the EEG rhythm. This study reports on a BCI method based on phase synchrony rate (SR). SR, computed from binarized phase locking value, describes the number of discrete synchronization events within a window. Statistical nonparametric tests show that SRs contain signiﬁcant differences between 2 types of motor imageries. Classiﬁers trained on SRs consistently demonstrate satisfactory results for all 5 subjects. It is further observed that, for 3 subjects, phase is more discriminative than amplitude in the ﬁrst 1.5-2.0 s, which suggests that phase has the potential to boost the information transfer rate in BCIs. 1</p><p>6 0.61518675 <a title="94-lda-6" href="./nips-2005-Gaussian_Processes_for_Multiuser_Detection_in_CDMA_receivers.html">81 nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</a></p>
<p>7 0.57454771 <a title="94-lda-7" href="./nips-2005-Improved_risk_tail_bounds_for_on-line_algorithms.html">95 nips-2005-Improved risk tail bounds for on-line algorithms</a></p>
<p>8 0.55690885 <a title="94-lda-8" href="./nips-2005-Query_by_Committee_Made_Real.html">160 nips-2005-Query by Committee Made Real</a></p>
<p>9 0.55595821 <a title="94-lda-9" href="./nips-2005-Bayesian_Surprise_Attracts_Human_Attention.html">34 nips-2005-Bayesian Surprise Attracts Human Attention</a></p>
<p>10 0.55375153 <a title="94-lda-10" href="./nips-2005-Modeling_Neuronal_Interactivity_using_Dynamic_Bayesian_Networks.html">130 nips-2005-Modeling Neuronal Interactivity using Dynamic Bayesian Networks</a></p>
<p>11 0.55371106 <a title="94-lda-11" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<p>12 0.52948678 <a title="94-lda-12" href="./nips-2005-An_exploration-exploitation_model_based_on_norepinepherine_and_dopamine_activity.html">26 nips-2005-An exploration-exploitation model based on norepinepherine and dopamine activity</a></p>
<p>13 0.52605969 <a title="94-lda-13" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<p>14 0.52405918 <a title="94-lda-14" href="./nips-2005-Non-Local_Manifold_Parzen_Windows.html">138 nips-2005-Non-Local Manifold Parzen Windows</a></p>
<p>15 0.51972753 <a title="94-lda-15" href="./nips-2005-Analyzing_Coupled_Brain_Sources%3A_Distinguishing_True_from_Spurious_Interaction.html">29 nips-2005-Analyzing Coupled Brain Sources: Distinguishing True from Spurious Interaction</a></p>
<p>16 0.51858425 <a title="94-lda-16" href="./nips-2005-Conditional_Visual_Tracking_in_Kernel_Space.html">45 nips-2005-Conditional Visual Tracking in Kernel Space</a></p>
<p>17 0.51813602 <a title="94-lda-17" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>18 0.51752532 <a title="94-lda-18" href="./nips-2005-Integrate-and-Fire_models_with_adaptation_are_good_enough.html">99 nips-2005-Integrate-and-Fire models with adaptation are good enough</a></p>
<p>19 0.51715928 <a title="94-lda-19" href="./nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">151 nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<p>20 0.51685286 <a title="94-lda-20" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
