<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>183 nips-2005-Stimulus Evoked Independent Factor Analysis of MEG Data with Large Background Activity</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-183" href="#">nips2005-183</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>183 nips-2005-Stimulus Evoked Independent Factor Analysis of MEG Data with Large Background Activity</h1>
<br/><p>Source: <a title="nips-2005-183-pdf" href="http://papers.nips.cc/paper/2777-stimulus-evoked-independent-factor-analysis-of-meg-data-with-large-background-activity.pdf">pdf</a></p><p>Author: Kenneth Hild, Kensuke Sekihara, Hagai T. Attias, Srikantan S. Nagarajan</p><p>Abstract: This paper presents a novel technique for analyzing electromagnetic imaging data obtained using the stimulus evoked experimental paradigm. The technique is based on a probabilistic graphical model, which describes the data in terms of underlying evoked and interference sources, and explicitly models the stimulus evoked paradigm. A variational Bayesian EM algorithm infers the model from data, suppresses interference sources, and reconstructs the activity of separated individual brain sources. The new algorithm outperforms existing techniques on two real datasets, as well as on simulated data. 1</p><p>Reference: <a title="nips-2005-183-reference" href="../nips2005_reference/nips-2005-Stimulus_Evoked_Independent_Factor_Analysis_of_MEG_Data_with_Large_Background_Activity_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp  Abstract This paper presents a novel technique for analyzing electromagnetic imaging data obtained using the stimulus evoked experimental paradigm. [sent-20, score-0.992]
</p><p>2 The technique is based on a probabilistic graphical model, which describes the data in terms of underlying evoked and interference sources, and explicitly models the stimulus evoked paradigm. [sent-21, score-1.904]
</p><p>3 A variational Bayesian EM algorithm infers the model from data, suppresses interference sources, and reconstructs the activity of separated individual brain sources. [sent-22, score-0.452]
</p><p>4 1  Introduction  Electromagnetic source imaging, the reconstruction of the spatiotemporal activation of brain sources from MEG and EEG data, is currently being used in numerous studies of human cognition, both in normal and in various clinical populations [1]. [sent-24, score-0.306]
</p><p>5 An experimental paradigm that is very popular in imaging studies is the stimulus evoked paradigm. [sent-26, score-0.929]
</p><p>6 Each presentation (or trial) produces activity in a set of brain sources, which generates an electromagnetic ﬁeld captured by the sensor array. [sent-30, score-0.308]
</p><p>7 These data constitute the stimulus evoked response, and analyzing them can help to gain insights into the mechanism used by the brain to process the stimulus and similar sensory inputs. [sent-31, score-1.101]
</p><p>8 This paper presents a new technique for analyzing stimulus evoked electromagnetic imaging data. [sent-32, score-0.992]
</p><p>9 Interference signals overlap spatially and temporally with the stimulus evoked signals, making it difﬁcult to obtain accurate reconstructions of evoked brain sources. [sent-34, score-1.773]
</p><p>10 A related problem is that signals from different evoked sources themselves overlap with each other, making it difﬁcult to localize individual sources and reconstruct their separate responses. [sent-35, score-1.339]
</p><p>11 Many approaches have been taken to the problem of suppressing interference signals. [sent-36, score-0.332]
</p><p>12 One method is averaging over multiple trials, which reduces the contributions from interference sources, assuming that they are uncorrelated with the stimulus and that their autocorrelation time scale is shorter than the trial length. [sent-37, score-0.471]
</p><p>13 A set of methods termed subspace techniques computes a projection of the sensor data onto the signal Figure 1: Simulation example (see text) subspace, which corresponds to brain sources of interest. [sent-40, score-0.529]
</p><p>14 Consequently, those methods perform well only when the interference level is low. [sent-42, score-0.312]
</p><p>15 Independent component analysis (ICA) techniques [4-8], introduced more recently, attempt to decompose the sensor data into a set of signals that are mutually statistically independent. [sent-43, score-0.247]
</p><p>16 Artifacts such as eye blinks are independent of brain source activity and ICA has been able in many cases to successfully separate the two types of signals into distinct groups of output variables. [sent-44, score-0.309]
</p><p>17 First, they require pre-processing the sensor data to reduce dimensionality from, which causes loss of information on brain sources with relatively low amplitude. [sent-46, score-0.429]
</p><p>18 Second, ICA assumes L + M = K , where L, M are the number of evoked and interference sources and K < K is the reduced input dimensionality. [sent-48, score-1.253]
</p><p>19 Third, ICA requires post-processing of its output signals, usually via manual examination by experts (though sometime by thresholding), to determine which signals correspond evoked brain sources of interest. [sent-50, score-1.129]
</p><p>20 The fourth drawback of ICA techniques is that, by design, they cannot exploit the advantage offered by the evoked stimulus paradigm. [sent-51, score-0.859]
</p><p>21 Whereas interference sources are continuously active, evoked sources become active at each trial only near the time of stimulus presentation, termed stimulus onset time. [sent-52, score-1.871]
</p><p>22 Hence, knowledge of the onset times can help separate the evoked sources. [sent-53, score-0.825]
</p><p>23 In this paper we present a novel technique for suppressing interference signals and separating signals from individual evoked sources. [sent-55, score-1.278]
</p><p>24 The technique is based on a new probabilistic graphical model termed stimulus evoked independent factor analysis (SEIFA). [sent-56, score-0.99]
</p><p>25 This model,  an extension of [2], describes the observed sensor data in terms of two sets of independent variables, termed factors, which are not directly observable. [sent-57, score-0.234]
</p><p>26 The factors in the ﬁrst set represent evoked sources, and the factors in the second set represent interference sources. [sent-58, score-1.253]
</p><p>27 The sensor data are generated by linearly combining the factors in the two sets using two mixing matrices, followed by adding sensor noise. [sent-59, score-0.474]
</p><p>28 The mixing matrices and the precision matrix of the sensor noise constitute the SEIFA model parameters, and are inferred from data using a variational Bayesian EM algorithm [3], which computes their posterior distribution. [sent-60, score-0.496]
</p><p>29 Separation of the evoked sources is achieved in the course of processing by the algorithm. [sent-61, score-0.941]
</p><p>30 It can be applied directly to the sensor data without dimensionality reduction, therefore no information is lost. [sent-63, score-0.153]
</p><p>31 Rather than learning a square K × K unmixing matrix, it learns a K × (L + M ) mixing matrix, where the number of interference factors M is minimized using automatic Bayesian model selection which is part of the algorithm. [sent-64, score-0.506]
</p><p>32 In addition, SEIFA is designed to explicitly model the stimulus evoked paradigm, hence it optimally exploits the knowledge of stimulus onset times. [sent-65, score-1.05]
</p><p>33 Consequently, evoked sources are automatically identiﬁed and no post-processing is required. [sent-66, score-0.941]
</p><p>34 The SEIFA model describes observed MEG sensor data in terms of three types of underlying, unobserved signals: (1) signals arising from stimulus evoked sources, (2) signals arising from interference sources, and (2) sensor noise signals. [sent-68, score-1.694]
</p><p>35 Following inference, Figure 2: Performance on simulated the model is used to separate the evoked source data (see text) signals from those of the interference sources and from sensor noise, thus providing a clean version of the evoked response. [sent-70, score-2.322]
</p><p>36 The model further separates the evoked response into statistically independent factors. [sent-71, score-0.783]
</p><p>37 In addition, it produces a regularized correlation matrix of the clean evoked response and of each independent factors, which facilitates localization. [sent-72, score-0.887]
</p><p>38 Let yin denote the signal recorded by sensor i = 1 : K at time n = 1 : N . [sent-73, score-0.174]
</p><p>39 We assume that these signals arise from L evoked factors and M interference factors that are combined linearly. [sent-74, score-1.347]
</p><p>40 Let xjn denote the signal of evoked factor j = 1 : L, and let ujn denote the signal of interference factor j = 1 : M , both at time n. [sent-75, score-1.298]
</p><p>41 Let Aij denote the evoked mixing matrix, and let Bij denote the interference mixing matrix. [sent-77, score-1.173]
</p><p>42 We use an evoked stimulus paradigm, where a stimulus is presented at a speciﬁc time, termed the stimulus onset time, and is absent beforehand. [sent-81, score-1.236]
</p><p>43 The stimulus onset time is de-  ﬁned as n = N0 + 1. [sent-82, score-0.191]
</p><p>44 The period preceding the onset n = 1 : N0 is termed pre-stimulus period, and the period following the onset n = N0 + 1 : N is termed post-stimulus period. [sent-83, score-0.308]
</p><p>45 We assume the evoked factors are active only post stimulus and satisfy xjn = 0 before its onset. [sent-84, score-1.052]
</p><p>46 Hence yn =  Bun + vn , Axn + Bun + vn ,  n = 1 : N0 n = N0 + 1 : N  (1)  To turn (1) into a probabilistic model, each signal must be modelled by a probability distribution. [sent-85, score-0.211]
</p><p>47 Here, each evoked factor is modelled by a mixture of Gaussian (MOG) distributions. [sent-86, score-0.808]
</p><p>48 For factor j we have a MOG model with Sj components, also termed states, Sj  L  p(xn ) =  p(xjn ) ,  N (xjn | µj,sj , νj,sj )πj,sj  p(xjn ) =  (2)  sj =1  j=1  State sj is a Gaussian with mean µj,sj and precision νj,sj , and its probability is πj,sj . [sent-87, score-0.224]
</p><p>49 There are three reasons for using MOG distributions, rather than Gaussians, to describe the evoked factors. [sent-89, score-0.733]
</p><p>50 First, evoked brain sources are often characterized by spikes or by modulated harmonic functions, leading to non-Gaussian distributions. [sent-90, score-1.009]
</p><p>51 Second, previous work on ICA has shown that independent Gaussian sources that are linearly mixed cannot be separated. [sent-91, score-0.229]
</p><p>52 Since we aim to separate the evoked response into contributions from individual factors, we must therefore use independent non-Gaussian factor distributions. [sent-92, score-0.885]
</p><p>53 For interference signals and sensor noise we employ a Gaussian model. [sent-94, score-0.588]
</p><p>54 Each interference factor is modelled by an independent, zero-mean Gaussian distribution with unit precision, M  N (ujn | 0, 1) = N (un | 0, I)  p(un ) =  (3)  j=1  The Gaussian model implies that we exploit only second order statistics of the interference signals. [sent-95, score-0.699]
</p><p>55 This contrasts with the evoked signals, whose MOG model facilitates exploiting higher order statistics, leading to more accurate reconstruction and to separation. [sent-96, score-0.755]
</p><p>56 The sensor noise is modelled by a zero-mean Gaussian distribution with a diagonal precision matrix λ, p(vn ) = N (vn | 0, λ). [sent-97, score-0.286]
</p><p>57 From (1) we obtain p(yn | xn , un ) = p(vn ) where we substitute vn = yn − Axn − Bun with xn = 0 for n = 1 : N0 . [sent-98, score-0.312]
</p><p>58 Hence, we obtain the distribution of the sensor signals conditioned on the evoked and interference factors, p(yn | xn , un , A, B) =  N (yn | Bun , λ), n = 1 : N0 N (yn | Axn + Bun , λ), n = N0 + 1 : N  (4)  SEIFA also makes an i. [sent-99, score-1.46]
</p><p>59 Hence p(y, x, u | A, B) = n p(yn | xn , un , A, B)p(xn )p(un ). [sent-103, score-0.146]
</p><p>60 where y, x, u denote collectively the signals yn , xn , un at all time points. [sent-104, score-0.303]
</p><p>61 For the mixing matrices A, B we choose to use a conjugate prior N (Aij | 0, λi αj ) ,  p(A) = ij  p(B)  N (Bij | 0, λi βj )  = ij  (5)  where all matrix elements are independent zero-mean Gaussians and the precision of the ijth matrix element is proportional to the noise precision λi on sensor i. [sent-111, score-0.461]
</p><p>62 SEIFA is a probabilistic model with hidden variables, since the evoked and interference factors are not directly observable, hence it must be treated in the EM framework. [sent-120, score-1.149]
</p><p>63 Second, VB-EM produces automatically regularized estimators for the evoked response correlation matrices (required for source localization), where standard EM produces poorly conditioned ones. [sent-126, score-0.871]
</p><p>64 For the pre-stimulus period n = 1 : N0 we compute the posterior over the interference factors un only. [sent-130, score-0.584]
</p><p>65 For the post-stimulus period n = N0 + 1 : N we compute the posterior over the evoked and interference factors xn , un , and the collective state sn of the evoked factors. [sent-132, score-2.151]
</p><p>66 , sLn ), where sjn = 1 : Sj is the state of evoked factor j at time n. [sent-136, score-0.783]
</p><p>67 To simplify the notation, we combine the evoked and interference factors into a single L × 1 vector xn = (xn , un ), where L = L + M , and their mixing matrices into a single K × L matrix A = (A, B). [sent-138, score-1.424]
</p><p>68 For each r, the posterior over the factors conditioned on sn = r is Gaussian, with posterior mean xrn , urn and covariance Γr given by ¯ ¯ ¯ xrn = Γr A T λyn + νr µr , ¯  ¯ ¯ Γr = A T λA + νr + KΨ  −1  (7)  ¯ ¯ ¯ x ¯ We have deﬁned xrn = (¯rn , urn ) and A = (A, B). [sent-140, score-0.572]
</p><p>69 Next, compute the posterior probability that sn = r by πrn = ¯  1 πr zn  1 T 1 1 | νr || Γr | exp − yn λyn + µT νr µr − xrn Γ−1 xrn ¯ r r ¯ 2 2 2  where zn is a normalization constant and µr , νr , πr are the MOG parameters of (2). [sent-143, score-0.376]
</p><p>70 The ﬁrst set includes the mixing matrices A, B, for which we compute full posterior distributions. [sent-146, score-0.145]
</p><p>71 Ryx , Ryu , Rxx , Rxu , Ruu are posterior correlations between the factors and the data and among the factors themselves, e. [sent-150, score-0.253]
</p><p>72 , Ryx = n yn xn , Rxx = n xn xn , where · denotes posterior averaging. [sent-152, score-0.264]
</p><p>73 They are easily computed in terms of the E-step quantities un , xrn , Φ, Γr , πrn and are omitted. [sent-153, score-0.183]
</p><p>74 ¯ ¯ ¯ Next, the hyperparameter matrices α, β are updated by ¯ ¯ α−1 = diag AT λA/K + ΨAA ,  ¯ ¯ β −1 = diag B T λB/K + ΨBB  (10)  ¯ T ¯ T and the noise precision matrix by λ−1 = diag(Ryy − ARyx − BRyu )/N . [sent-154, score-0.144]
</p><p>75 The interference mixing matrix and the noise precision are initialized from pre-stimulus data. [sent-156, score-0.484]
</p><p>76 Let zin = Aij xjn denote the inferred individual contribution from evoked factor j to sensor signal i. [sent-159, score-1.116]
</p><p>77 It is given via posterior averaging by  ¯ ¯ zin = Aij xjn ¯j  (11)  where xn = ¯ ¯ ¯ r πr xrn . [sent-160, score-0.301]
</p><p>78 Computing this estimate amounts to obtaining a clean version of the individual contribution from each factor and of their combined contribution, and removing contributions from interference factors and sensor noise. [sent-161, score-0.676]
</p><p>79 j The localization of individual evoked factors using sensor signals zn can be achieved by many algorithms. [sent-162, score-1.2]
</p><p>80 Let C j = n zn (zn )T denote the inferred sensor data correlation matrix corresponding to the individual contribution from evoked factor j. [sent-164, score-1.063]
</p><p>81 1 shows a simulation with two evoked sources and three interference sources with N = 10000, signal-to-interference (SIR) of 0 dB and signal-to-sensor-noise (SNR) of 5dB. [sent-170, score-1.461]
</p><p>82 The true locations of the evoked sources, each of which is denoted by •, and the true locations of the background sources, each of which is denoted by × are shown in the top left panel. [sent-171, score-0.773]
</p><p>83 The right column in the top row shows the time courses of the evoked sources as they appear at the sensors. [sent-172, score-0.961]
</p><p>84 The time courses of the actual sensor signals, which also include the effects of background sources and sensor noise, are shown in the middle row (right column). [sent-173, score-0.534]
</p><p>85 The bottom row shows the localization and time-course of cleaned  Figure 3: Estimating auditory-evoked responses from small trial averages (see text)  evoked sources estimated using SEIFA, which agrees with the true location and timecourse. [sent-174, score-1.032]
</p><p>86 2 shows the mean performance as a function of SIR, across 50 Monte Carlo trials for N = 1000 and SNR of 10 dB, for different locations of evoked and interference sources. [sent-176, score-1.089]
</p><p>87 Separation performance of individual evoked factors is quantiﬁed by (separated-signal)-to-(noise+interference) ratio (SSNIR) (deﬁnition omitted) and is shown in the middle panel. [sent-179, score-0.862]
</p><p>88 JADE is able to separate the background sources from the evoked sources (hence gives good denoising performance), but it is not always able to separate the evoked sources from each other. [sent-181, score-2.166]
</p><p>89 Finally, localization performance is quantiﬁed by the mean distance in cm between the true evoked source locations and the estimated locations, as shown in the bottom panel. [sent-183, score-0.841]
</p><p>90 assumption of the evoked and background sources, because in these simulations evoked sources were assumed to be damped sinusoids and interference sources were sinusoids. [sent-188, score-2.194]
</p><p>91 Auditory evoked responses from a particular subject obtained by averaging different number of trials are shown in ﬁgure 3 (left panel). [sent-191, score-0.757]
</p><p>92 To highlight SEIFA’s ability to separately localize evoked sources, we conducted an experiment involving simultaneous presentation of auditory and somatosensory stimuli. [sent-197, score-0.852]
</p><p>93 In these ﬁgures, one panel shows a contour map that shows the polarity and magnitude of the denoised and raw sensor signals in sensor space. [sent-202, score-0.4]
</p><p>94 The contour plot of the magnetic ﬁeld on the sensor array, corresponding to the mapping of three-dimensional sensor surface array to points within a  circle, shows the magnetic ﬁeld proﬁle at a particular instant of time relative to the stimulus presentation. [sent-203, score-0.432]
</p><p>95 Other panels show localization of a particular evoked factor overlaid on the subjects’ MRI. [sent-204, score-0.841]
</p><p>96 The ﬁrst two factors localize to primary somatosensory cortex (SI), however with differential latencies. [sent-208, score-0.18]
</p><p>97 The ﬁrst factor shows a peak response at a latency of 50 ms, whereas the second factor shows the response at a later latency. [sent-209, score-0.158]
</p><p>98 Interestingly, the third factor localizes to auditory cortex and the extracted timecourse corresponds well to an auditory evoked response that is well-separated from the somatosensory response (ﬁgure 3 right panels). [sent-210, score-0.978]
</p><p>99 We also plan to infer the distribution of evoked sources (MOG parameters) from data rather than using a ﬁxed distribution. [sent-212, score-0.941]
</p><p>100 Reconstructing spatiotemporal activities of neural sources using a MEG vector beamformer technique. [sent-269, score-0.208]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('evoked', 0.733), ('interference', 0.312), ('seifa', 0.294), ('sources', 0.208), ('sensor', 0.153), ('stimulus', 0.126), ('factors', 0.104), ('un', 0.094), ('signals', 0.094), ('xjn', 0.089), ('xrn', 0.089), ('mog', 0.089), ('brain', 0.068), ('onset', 0.065), ('mixing', 0.064), ('bun', 0.064), ('electromagnetic', 0.064), ('yn', 0.063), ('ica', 0.063), ('termed', 0.06), ('localization', 0.058), ('meg', 0.056), ('xn', 0.052), ('vn', 0.051), ('bb', 0.051), ('somatosensory', 0.051), ('factor', 0.05), ('precision', 0.05), ('imaging', 0.049), ('posterior', 0.045), ('rxx', 0.044), ('auditory', 0.043), ('francisco', 0.042), ('attias', 0.038), ('axn', 0.038), ('jade', 0.038), ('navg', 0.038), ('rxu', 0.038), ('ryx', 0.038), ('matrices', 0.036), ('aa', 0.036), ('zn', 0.033), ('trial', 0.033), ('clean', 0.032), ('sj', 0.032), ('source', 0.03), ('matrix', 0.029), ('response', 0.029), ('period', 0.029), ('noise', 0.029), ('constitute', 0.028), ('aij', 0.028), ('san', 0.028), ('separate', 0.027), ('biomagnetic', 0.026), ('blinks', 0.026), ('hild', 0.026), ('makeig', 0.026), ('nagarajan', 0.026), ('ruu', 0.026), ('sekihara', 0.026), ('sir', 0.026), ('snir', 0.026), ('sometime', 0.026), ('unmixing', 0.026), ('zin', 0.026), ('localize', 0.025), ('collective', 0.025), ('modelled', 0.025), ('individual', 0.025), ('sn', 0.024), ('trials', 0.024), ('variational', 0.024), ('activity', 0.023), ('conditioned', 0.022), ('urn', 0.022), ('contralateral', 0.022), ('jj', 0.022), ('ryu', 0.022), ('ujn', 0.022), ('separation', 0.022), ('denoising', 0.022), ('facilitates', 0.022), ('independent', 0.021), ('correlation', 0.021), ('paradigm', 0.021), ('signal', 0.021), ('covariance', 0.021), ('suppressing', 0.02), ('courses', 0.02), ('radiology', 0.02), ('locations', 0.02), ('eye', 0.02), ('db', 0.02), ('analyzing', 0.02), ('em', 0.02), ('inferred', 0.019), ('overlap', 0.019), ('computes', 0.019), ('tone', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="183-tfidf-1" href="./nips-2005-Stimulus_Evoked_Independent_Factor_Analysis_of_MEG_Data_with_Large_Background_Activity.html">183 nips-2005-Stimulus Evoked Independent Factor Analysis of MEG Data with Large Background Activity</a></p>
<p>Author: Kenneth Hild, Kensuke Sekihara, Hagai T. Attias, Srikantan S. Nagarajan</p><p>Abstract: This paper presents a novel technique for analyzing electromagnetic imaging data obtained using the stimulus evoked experimental paradigm. The technique is based on a probabilistic graphical model, which describes the data in terms of underlying evoked and interference sources, and explicitly models the stimulus evoked paradigm. A variational Bayesian EM algorithm infers the model from data, suppresses interference sources, and reconstructs the activity of separated individual brain sources. The new algorithm outperforms existing techniques on two real datasets, as well as on simulated data. 1</p><p>2 0.12353242 <a title="183-tfidf-2" href="./nips-2005-Analyzing_Coupled_Brain_Sources%3A_Distinguishing_True_from_Spurious_Interaction.html">29 nips-2005-Analyzing Coupled Brain Sources: Distinguishing True from Spurious Interaction</a></p>
<p>Author: Guido Nolte, Andreas Ziehe, Frank Meinecke, Klaus-Robert Müller</p><p>Abstract: When trying to understand the brain, it is of fundamental importance to analyse (e.g. from EEG/MEG measurements) what parts of the cortex interact with each other in order to infer more accurate models of brain activity. Common techniques like Blind Source Separation (BSS) can estimate brain sources and single out artifacts by using the underlying assumption of source signal independence. However, physiologically interesting brain sources typically interact, so BSS will—by construction— fail to characterize them properly. Noting that there are truly interacting sources and signals that only seemingly interact due to effects of volume conduction, this work aims to contribute by distinguishing these effects. For this a new BSS technique is proposed that uses anti-symmetrized cross-correlation matrices and subsequent diagonalization. The resulting decomposition consists of the truly interacting brain sources and suppresses any spurious interaction stemming from volume conduction. Our new concept of interacting source analysis (ISA) is successfully demonstrated on MEG data. 1</p><p>3 0.11864015 <a title="183-tfidf-3" href="./nips-2005-Gradient_Flow_Independent_Component_Analysis_in_Micropower_VLSI.html">88 nips-2005-Gradient Flow Independent Component Analysis in Micropower VLSI</a></p>
<p>Author: Abdullah Celik, Milutin Stanacevic, Gert Cauwenberghs</p><p>Abstract: We present micropower mixed-signal VLSI hardware for real-time blind separation and localization of acoustic sources. Gradient ﬂow representation of the traveling wave signals acquired over a miniature (1cm diameter) array of four microphones yields linearly mixed instantaneous observations of the time-differentiated sources, separated and localized by independent component analysis (ICA). The gradient ﬂow and ICA processors each measure 3mm × 3mm in 0.5 µm CMOS, and consume 54 µW and 180 µW power, respectively, from a 3 V supply at 16 ks/s sampling rate. Experiments demonstrate perceptually clear (12dB) separation and precise localization of two speech sources presented through speakers positioned at 1.5m from the array on a conference room table. Analysis of the multipath residuals shows that they are spectrally diffuse, and void of the direct path.</p><p>4 0.088388398 <a title="183-tfidf-4" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>Author: O. P. Kreidl, Alan S. Willsky</p><p>Abstract: Given a directed graphical model with binary-valued hidden nodes and real-valued noisy observations, consider deciding upon the maximum a-posteriori (MAP) or the maximum posterior-marginal (MPM) assignment under the restriction that each node broadcasts only to its children exactly one single-bit message. We present a variational formulation, viewing the processing rules local to all nodes as degrees-of-freedom, that minimizes the loss in expected (MAP or MPM) performance subject to such online communication constraints. The approach leads to a novel message-passing algorithm to be executed ofﬂine, or before observations are realized, which mitigates the performance loss by iteratively coupling all rules in a manner implicitly driven by global statistics. We also provide (i) illustrative examples, (ii) assumptions that guarantee convergence and efﬁciency and (iii) connections to active research areas. 1</p><p>5 0.086562596 <a title="183-tfidf-5" href="./nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">113 nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<p>Author: Jian Zhang, Zoubin Ghahramani, Yiming Yang</p><p>Abstract: We propose a probabilistic model based on Independent Component Analysis for learning multiple related tasks. In our model the task parameters are assumed to be generated from independent sources which account for the relatedness of the tasks. We use Laplace distributions to model hidden sources which makes it possible to identify the hidden, independent components instead of just modeling correlations. Furthermore, our model enjoys a sparsity property which makes it both parsimonious and robust. We also propose efﬁcient algorithms for both empirical Bayes method and point estimation. Our experimental results on two multi-label text classiﬁcation data sets show that the proposed approach is promising.</p><p>6 0.068287179 <a title="183-tfidf-6" href="./nips-2005-Affine_Structure_From_Sound.html">20 nips-2005-Affine Structure From Sound</a></p>
<p>7 0.061202608 <a title="183-tfidf-7" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<p>8 0.061097629 <a title="183-tfidf-8" href="./nips-2005-Recovery_of_Jointly_Sparse_Signals_from_Few_Random_Projections.html">163 nips-2005-Recovery of Jointly Sparse Signals from Few Random Projections</a></p>
<p>9 0.052088972 <a title="183-tfidf-9" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>10 0.050786223 <a title="183-tfidf-10" href="./nips-2005-Identifying_Distributed_Object_Representations_in_Human_Extrastriate_Visual_Cortex.html">94 nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</a></p>
<p>11 0.048516288 <a title="183-tfidf-11" href="./nips-2005-Gaussian_Processes_for_Multiuser_Detection_in_CDMA_receivers.html">81 nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</a></p>
<p>12 0.048048954 <a title="183-tfidf-12" href="./nips-2005-Norepinephrine_and_Neural_Interrupts.html">141 nips-2005-Norepinephrine and Neural Interrupts</a></p>
<p>13 0.042208187 <a title="183-tfidf-13" href="./nips-2005-Rate_Distortion_Codes_in_Sensor_Networks%3A_A_System-level_Analysis.html">162 nips-2005-Rate Distortion Codes in Sensor Networks: A System-level Analysis</a></p>
<p>14 0.04001632 <a title="183-tfidf-14" href="./nips-2005-Fusion_of_Similarity_Data_in_Clustering.html">79 nips-2005-Fusion of Similarity Data in Clustering</a></p>
<p>15 0.039870437 <a title="183-tfidf-15" href="./nips-2005-Modeling_Neuronal_Interactivity_using_Dynamic_Bayesian_Networks.html">130 nips-2005-Modeling Neuronal Interactivity using Dynamic Bayesian Networks</a></p>
<p>16 0.039623901 <a title="183-tfidf-16" href="./nips-2005-Optimizing_spatio-temporal_filters_for_improving_Brain-Computer_Interfacing.html">150 nips-2005-Optimizing spatio-temporal filters for improving Brain-Computer Interfacing</a></p>
<p>17 0.037083786 <a title="183-tfidf-17" href="./nips-2005-Variational_EM_Algorithms_for_Non-Gaussian_Latent_Variable_Models.html">202 nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</a></p>
<p>18 0.036303192 <a title="183-tfidf-18" href="./nips-2005-The_Information-Form_Data_Association_Filter.html">192 nips-2005-The Information-Form Data Association Filter</a></p>
<p>19 0.034029059 <a title="183-tfidf-19" href="./nips-2005-Nonparametric_inference_of_prior_probabilities_from_Bayes-optimal_behavior.html">140 nips-2005-Nonparametric inference of prior probabilities from Bayes-optimal behavior</a></p>
<p>20 0.033631593 <a title="183-tfidf-20" href="./nips-2005-Separation_of_Music_Signals_by_Harmonic_Structure_Modeling.html">174 nips-2005-Separation of Music Signals by Harmonic Structure Modeling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.121), (1, -0.04), (2, -0.005), (3, 0.074), (4, 0.02), (5, -0.031), (6, -0.117), (7, -0.117), (8, 0.029), (9, 0.042), (10, -0.148), (11, -0.112), (12, 0.02), (13, -0.021), (14, 0.02), (15, 0.019), (16, -0.073), (17, -0.052), (18, 0.097), (19, 0.004), (20, -0.102), (21, -0.025), (22, 0.182), (23, 0.045), (24, -0.012), (25, -0.147), (26, -0.054), (27, -0.104), (28, -0.089), (29, -0.045), (30, 0.174), (31, 0.007), (32, 0.041), (33, -0.05), (34, -0.045), (35, 0.077), (36, 0.061), (37, 0.084), (38, 0.116), (39, -0.057), (40, -0.089), (41, 0.02), (42, 0.055), (43, -0.041), (44, 0.071), (45, 0.07), (46, -0.059), (47, 0.149), (48, 0.011), (49, 0.062)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96086395 <a title="183-lsi-1" href="./nips-2005-Stimulus_Evoked_Independent_Factor_Analysis_of_MEG_Data_with_Large_Background_Activity.html">183 nips-2005-Stimulus Evoked Independent Factor Analysis of MEG Data with Large Background Activity</a></p>
<p>Author: Kenneth Hild, Kensuke Sekihara, Hagai T. Attias, Srikantan S. Nagarajan</p><p>Abstract: This paper presents a novel technique for analyzing electromagnetic imaging data obtained using the stimulus evoked experimental paradigm. The technique is based on a probabilistic graphical model, which describes the data in terms of underlying evoked and interference sources, and explicitly models the stimulus evoked paradigm. A variational Bayesian EM algorithm infers the model from data, suppresses interference sources, and reconstructs the activity of separated individual brain sources. The new algorithm outperforms existing techniques on two real datasets, as well as on simulated data. 1</p><p>2 0.72687167 <a title="183-lsi-2" href="./nips-2005-Analyzing_Coupled_Brain_Sources%3A_Distinguishing_True_from_Spurious_Interaction.html">29 nips-2005-Analyzing Coupled Brain Sources: Distinguishing True from Spurious Interaction</a></p>
<p>Author: Guido Nolte, Andreas Ziehe, Frank Meinecke, Klaus-Robert Müller</p><p>Abstract: When trying to understand the brain, it is of fundamental importance to analyse (e.g. from EEG/MEG measurements) what parts of the cortex interact with each other in order to infer more accurate models of brain activity. Common techniques like Blind Source Separation (BSS) can estimate brain sources and single out artifacts by using the underlying assumption of source signal independence. However, physiologically interesting brain sources typically interact, so BSS will—by construction— fail to characterize them properly. Noting that there are truly interacting sources and signals that only seemingly interact due to effects of volume conduction, this work aims to contribute by distinguishing these effects. For this a new BSS technique is proposed that uses anti-symmetrized cross-correlation matrices and subsequent diagonalization. The resulting decomposition consists of the truly interacting brain sources and suppresses any spurious interaction stemming from volume conduction. Our new concept of interacting source analysis (ISA) is successfully demonstrated on MEG data. 1</p><p>3 0.67014033 <a title="183-lsi-3" href="./nips-2005-Gradient_Flow_Independent_Component_Analysis_in_Micropower_VLSI.html">88 nips-2005-Gradient Flow Independent Component Analysis in Micropower VLSI</a></p>
<p>Author: Abdullah Celik, Milutin Stanacevic, Gert Cauwenberghs</p><p>Abstract: We present micropower mixed-signal VLSI hardware for real-time blind separation and localization of acoustic sources. Gradient ﬂow representation of the traveling wave signals acquired over a miniature (1cm diameter) array of four microphones yields linearly mixed instantaneous observations of the time-differentiated sources, separated and localized by independent component analysis (ICA). The gradient ﬂow and ICA processors each measure 3mm × 3mm in 0.5 µm CMOS, and consume 54 µW and 180 µW power, respectively, from a 3 V supply at 16 ks/s sampling rate. Experiments demonstrate perceptually clear (12dB) separation and precise localization of two speech sources presented through speakers positioned at 1.5m from the array on a conference room table. Analysis of the multipath residuals shows that they are spectrally diffuse, and void of the direct path.</p><p>4 0.54739881 <a title="183-lsi-4" href="./nips-2005-Separation_of_Music_Signals_by_Harmonic_Structure_Modeling.html">174 nips-2005-Separation of Music Signals by Harmonic Structure Modeling</a></p>
<p>Author: Yun-gang Zhang, Chang-shui Zhang</p><p>Abstract: Separation of music signals is an interesting but difﬁcult problem. It is helpful for many other music researches such as audio content analysis. In this paper, a new music signal separation method is proposed, which is based on harmonic structure modeling. The main idea of harmonic structure modeling is that the harmonic structure of a music signal is stable, so a music signal can be represented by a harmonic structure model. Accordingly, a corresponding separation algorithm is proposed. The main idea is to learn a harmonic structure model for each music signal in the mixture, and then separate signals by using these models to distinguish harmonic structures of different signals. Experimental results show that the algorithm can separate signals and obtain not only a very high Signalto-Noise Ratio (SNR) but also a rather good subjective audio quality. 1</p><p>5 0.47866794 <a title="183-lsi-5" href="./nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">113 nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<p>Author: Jian Zhang, Zoubin Ghahramani, Yiming Yang</p><p>Abstract: We propose a probabilistic model based on Independent Component Analysis for learning multiple related tasks. In our model the task parameters are assumed to be generated from independent sources which account for the relatedness of the tasks. We use Laplace distributions to model hidden sources which makes it possible to identify the hidden, independent components instead of just modeling correlations. Furthermore, our model enjoys a sparsity property which makes it both parsimonious and robust. We also propose efﬁcient algorithms for both empirical Bayes method and point estimation. Our experimental results on two multi-label text classiﬁcation data sets show that the proposed approach is promising.</p><p>6 0.44520089 <a title="183-lsi-6" href="./nips-2005-Affine_Structure_From_Sound.html">20 nips-2005-Affine Structure From Sound</a></p>
<p>7 0.40346831 <a title="183-lsi-7" href="./nips-2005-Recovery_of_Jointly_Sparse_Signals_from_Few_Random_Projections.html">163 nips-2005-Recovery of Jointly Sparse Signals from Few Random Projections</a></p>
<p>8 0.33152014 <a title="183-lsi-8" href="./nips-2005-Gaussian_Processes_for_Multiuser_Detection_in_CDMA_receivers.html">81 nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</a></p>
<p>9 0.30374658 <a title="183-lsi-9" href="./nips-2005-Factorial_Switching_Kalman_Filters_for_Condition_Monitoring_in_Neonatal_Intensive_Care.html">68 nips-2005-Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care</a></p>
<p>10 0.30033872 <a title="183-lsi-10" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>11 0.28819412 <a title="183-lsi-11" href="./nips-2005-Modeling_Neuronal_Interactivity_using_Dynamic_Bayesian_Networks.html">130 nips-2005-Modeling Neuronal Interactivity using Dynamic Bayesian Networks</a></p>
<p>12 0.27662209 <a title="183-lsi-12" href="./nips-2005-Visual_Encoding_with_Jittering_Eyes.html">203 nips-2005-Visual Encoding with Jittering Eyes</a></p>
<p>13 0.26613918 <a title="183-lsi-13" href="./nips-2005-Rate_Distortion_Codes_in_Sensor_Networks%3A_A_System-level_Analysis.html">162 nips-2005-Rate Distortion Codes in Sensor Networks: A System-level Analysis</a></p>
<p>14 0.26495788 <a title="183-lsi-14" href="./nips-2005-Large-scale_biophysical_parameter_estimation_in_single_neurons_via_constrained_linear_regression.html">106 nips-2005-Large-scale biophysical parameter estimation in single neurons via constrained linear regression</a></p>
<p>15 0.26251245 <a title="183-lsi-15" href="./nips-2005-Identifying_Distributed_Object_Representations_in_Human_Extrastriate_Visual_Cortex.html">94 nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</a></p>
<p>16 0.2559689 <a title="183-lsi-16" href="./nips-2005-Optimizing_spatio-temporal_filters_for_improving_Brain-Computer_Interfacing.html">150 nips-2005-Optimizing spatio-temporal filters for improving Brain-Computer Interfacing</a></p>
<p>17 0.25522313 <a title="183-lsi-17" href="./nips-2005-An_aVLSI_Cricket_Ear_Model.html">25 nips-2005-An aVLSI Cricket Ear Model</a></p>
<p>18 0.2506704 <a title="183-lsi-18" href="./nips-2005-Norepinephrine_and_Neural_Interrupts.html">141 nips-2005-Norepinephrine and Neural Interrupts</a></p>
<p>19 0.22995991 <a title="183-lsi-19" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>20 0.21549916 <a title="183-lsi-20" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.029), (10, 0.03), (11, 0.347), (12, 0.01), (27, 0.042), (31, 0.031), (34, 0.053), (39, 0.013), (55, 0.031), (57, 0.024), (60, 0.042), (69, 0.068), (73, 0.023), (88, 0.09), (91, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78991008 <a title="183-lda-1" href="./nips-2005-Stimulus_Evoked_Independent_Factor_Analysis_of_MEG_Data_with_Large_Background_Activity.html">183 nips-2005-Stimulus Evoked Independent Factor Analysis of MEG Data with Large Background Activity</a></p>
<p>Author: Kenneth Hild, Kensuke Sekihara, Hagai T. Attias, Srikantan S. Nagarajan</p><p>Abstract: This paper presents a novel technique for analyzing electromagnetic imaging data obtained using the stimulus evoked experimental paradigm. The technique is based on a probabilistic graphical model, which describes the data in terms of underlying evoked and interference sources, and explicitly models the stimulus evoked paradigm. A variational Bayesian EM algorithm infers the model from data, suppresses interference sources, and reconstructs the activity of separated individual brain sources. The new algorithm outperforms existing techniques on two real datasets, as well as on simulated data. 1</p><p>2 0.68877226 <a title="183-lda-2" href="./nips-2005-Nonparametric_inference_of_prior_probabilities_from_Bayes-optimal_behavior.html">140 nips-2005-Nonparametric inference of prior probabilities from Bayes-optimal behavior</a></p>
<p>Author: Liam Paninski</p><p>Abstract: We discuss a method for obtaining a subject’s a priori beliefs from his/her behavior in a psychophysics context, under the assumption that the behavior is (nearly) optimal from a Bayesian perspective. The method is nonparametric in the sense that we do not assume that the prior belongs to any ﬁxed class of distributions (e.g., Gaussian). Despite this increased generality, the method is relatively simple to implement, being based in the simplest case on a linear programming algorithm, and more generally on a straightforward maximum likelihood or maximum a posteriori formulation, which turns out to be a convex optimization problem (with no non-global local maxima) in many important cases. In addition, we develop methods for analyzing the uncertainty of these estimates. We demonstrate the accuracy of the method in a simple simulated coin-ﬂipping setting; in particular, the method is able to precisely track the evolution of the subject’s posterior distribution as more and more data are observed. We close by brieﬂy discussing an interesting connection to recent models of neural population coding.</p><p>3 0.66074049 <a title="183-lda-3" href="./nips-2005-Diffusion_Maps%2C_Spectral_Clustering_and_Eigenfunctions_of_Fokker-Planck_Operators.html">56 nips-2005-Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators</a></p>
<p>Author: Boaz Nadler, Stephane Lafon, Ioannis Kevrekidis, Ronald R. Coifman</p><p>Abstract: This paper presents a diffusion based probabilistic interpretation of spectral clustering and dimensionality reduction algorithms that use the eigenvectors of the normalized graph Laplacian. Given the pairwise adjacency matrix of all points, we deﬁne a diffusion distance between any two data points and show that the low dimensional representation of the data by the ﬁrst few eigenvectors of the corresponding Markov matrix is optimal under a certain mean squared error criterion. Furthermore, assuming that data points are random samples from a density p(x) = e−U (x) we identify these eigenvectors as discrete approximations of eigenfunctions of a Fokker-Planck operator in a potential 2U (x) with reﬂecting boundary conditions. Finally, applying known results regarding the eigenvalues and eigenfunctions of the continuous Fokker-Planck operator, we provide a mathematical justiﬁcation for the success of spectral clustering and dimensional reduction algorithms based on these ﬁrst few eigenvectors. This analysis elucidates, in terms of the characteristics of diffusion processes, many empirical ﬁndings regarding spectral clustering algorithms. Keywords: Algorithms and architectures, learning theory. 1</p><p>4 0.42446312 <a title="183-lda-4" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>Author: Afsheen Afshar, Gopal Santhanam, Stephen I. Ryu, Maneesh Sahani, Byron M. Yu, Krishna V. Shenoy</p><p>Abstract: Spiking activity from neurophysiological experiments often exhibits dynamics beyond that driven by external stimulation, presumably reﬂecting the extensive recurrence of neural circuitry. Characterizing these dynamics may reveal important features of neural computation, particularly during internally-driven cognitive operations. For example, the activity of premotor cortex (PMd) neurons during an instructed delay period separating movement-target speciﬁcation and a movementinitiation cue is believed to be involved in motor planning. We show that the dynamics underlying this activity can be captured by a lowdimensional non-linear dynamical systems model, with underlying recurrent structure and stochastic point-process output. We present and validate latent variable methods that simultaneously estimate the system parameters and the trial-by-trial dynamical trajectories. These methods are applied to characterize the dynamics in PMd data recorded from a chronically-implanted 96-electrode array while monkeys perform delayed-reach tasks. 1</p><p>5 0.41322193 <a title="183-lda-5" href="./nips-2005-Noise_and_the_two-thirds_power_Law.html">136 nips-2005-Noise and the two-thirds power Law</a></p>
<p>Author: Uri Maoz, Elon Portugaly, Tamar Flash, Yair Weiss</p><p>Abstract: The two-thirds power law, an empirical law stating an inverse non-linear relationship between the tangential hand speed and the curvature of its trajectory during curved motion, is widely acknowledged to be an invariant of upper-limb movement. It has also been shown to exist in eyemotion, locomotion and was even demonstrated in motion perception and prediction. This ubiquity has fostered various attempts to uncover the origins of this empirical relationship. In these it was generally attributed either to smoothness in hand- or joint-space or to the result of mechanisms that damp noise inherent in the motor system to produce the smooth trajectories evident in healthy human motion. We show here that white Gaussian noise also obeys this power-law. Analysis of signal and noise combinations shows that trajectories that were synthetically created not to comply with the power-law are transformed to power-law compliant ones after combination with low levels of noise. Furthermore, there exist colored noise types that drive non-power-law trajectories to power-law compliance and are not affected by smoothing. These results suggest caution when running experiments aimed at verifying the power-law or assuming its underlying existence without proper analysis of the noise. Our results could also suggest that the power-law might be derived not from smoothness or smoothness-inducing mechanisms operating on the noise inherent in our motor system but rather from the correlated noise which is inherent in this motor system. 1</p><p>6 0.40646398 <a title="183-lda-6" href="./nips-2005-Identifying_Distributed_Object_Representations_in_Human_Extrastriate_Visual_Cortex.html">94 nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</a></p>
<p>7 0.4014641 <a title="183-lda-7" href="./nips-2005-A_Bayesian_Framework_for_Tilt_Perception_and_Confidence.html">3 nips-2005-A Bayesian Framework for Tilt Perception and Confidence</a></p>
<p>8 0.39720696 <a title="183-lda-8" href="./nips-2005-Integrate-and-Fire_models_with_adaptation_are_good_enough.html">99 nips-2005-Integrate-and-Fire models with adaptation are good enough</a></p>
<p>9 0.39437148 <a title="183-lda-9" href="./nips-2005-Sensory_Adaptation_within_a_Bayesian_Framework_for_Perception.html">173 nips-2005-Sensory Adaptation within a Bayesian Framework for Perception</a></p>
<p>10 0.39330158 <a title="183-lda-10" href="./nips-2005-Bayesian_models_of_human_action_understanding.html">36 nips-2005-Bayesian models of human action understanding</a></p>
<p>11 0.39288977 <a title="183-lda-11" href="./nips-2005-Bayesian_model_learning_in_human_visual_perception.html">35 nips-2005-Bayesian model learning in human visual perception</a></p>
<p>12 0.39233947 <a title="183-lda-12" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>13 0.39087075 <a title="183-lda-13" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>14 0.38620177 <a title="183-lda-14" href="./nips-2005-An_Application_of_Markov_Random_Fields_to_Range_Sensing.html">23 nips-2005-An Application of Markov Random Fields to Range Sensing</a></p>
<p>15 0.38457799 <a title="183-lda-15" href="./nips-2005-Learning_to_Control_an_Octopus_Arm_with_Gaussian_Process_Temporal_Difference_Methods.html">119 nips-2005-Learning to Control an Octopus Arm with Gaussian Process Temporal Difference Methods</a></p>
<p>16 0.38453391 <a title="183-lda-16" href="./nips-2005-Ideal_Observers_for_Detecting_Motion%3A_Correspondence_Noise.html">93 nips-2005-Ideal Observers for Detecting Motion: Correspondence Noise</a></p>
<p>17 0.38282928 <a title="183-lda-17" href="./nips-2005-Dynamical_Synapses_Give_Rise_to_a_Power-Law_Distribution_of_Neuronal_Avalanches.html">61 nips-2005-Dynamical Synapses Give Rise to a Power-Law Distribution of Neuronal Avalanches</a></p>
<p>18 0.38197839 <a title="183-lda-18" href="./nips-2005-Learning_Cue-Invariant_Visual_Responses.html">109 nips-2005-Learning Cue-Invariant Visual Responses</a></p>
<p>19 0.38010854 <a title="183-lda-19" href="./nips-2005-Rodeo%3A_Sparse_Nonparametric_Regression_in_High_Dimensions.html">168 nips-2005-Rodeo: Sparse Nonparametric Regression in High Dimensions</a></p>
<p>20 0.37806457 <a title="183-lda-20" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
