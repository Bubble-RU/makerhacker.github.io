<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>158 nips-2005-Products of ``Edge-perts</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-158" href="#">nips2005-158</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>158 nips-2005-Products of ``Edge-perts</h1>
<br/><p>Source: <a title="nips-2005-158-pdf" href="http://papers.nips.cc/paper/2797-products-of-edge-perts.pdf">pdf</a></p><p>Author: Max Welling, Peter V. Gehler</p><p>Abstract: Images represent an important and abundant source of data. Understanding their statistical structure has important applications such as image compression and restoration. In this paper we propose a particular kind of probabilistic model, dubbed the “products of edge-perts model” to describe the structure of wavelet transformed images. We develop a practical denoising algorithm based on a single edge-pert and show state-ofthe-art denoising performance on benchmark images. 1</p><p>Reference: <a title="nips-2005-158-reference" href="../nips2005_reference/nips-2005-Products_of_%60%60Edge-perts_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Understanding their statistical structure has important applications such as image compression and restoration. [sent-6, score-0.099]
</p><p>2 In this paper we propose a particular kind of probabilistic model, dubbed the “products of edge-perts model” to describe the structure of wavelet transformed images. [sent-7, score-0.594]
</p><p>3 We develop a practical denoising algorithm based on a single edge-pert and show state-ofthe-art denoising performance on benchmark images. [sent-8, score-0.412]
</p><p>4 Wavelet transforms, which capture most of the second order dependencies, form the basis of many successful image processing applications such as image compression (e. [sent-10, score-0.206]
</p><p>5 However, the higher order dependencies can not be ﬁltered out by these linear transforms. [sent-15, score-0.114]
</p><p>6 In particular, the absolute values of neighboring wavelet coefﬁcients (but not their signs) are mutually dependent. [sent-16, score-0.594]
</p><p>7 This kind of dependency is caused by the presence of edges that induce clustering of wavelet activity. [sent-17, score-0.637]
</p><p>8 Our philosophy is that by modelling this clustering effect we can potentially improve the performance of some important image processing tasks. [sent-18, score-0.131]
</p><p>9 Our model builds on earlier work in the image processing literature. [sent-19, score-0.113]
</p><p>10 The state-of-art in this area is the joint model discussed in [3] based on the “Gaussian scale mixture” model (GSM). [sent-21, score-0.103]
</p><p>11 While the GSM falls in the category of directed graphical models and has a top-down structure, the PoEdges model is best classiﬁed as an (undirected) Markov random ﬁeld model and follows bottom-up semantics. [sent-22, score-0.064]
</p><p>12 1) and 3) a new “iterated Wiener denoising algorithm” (section 3. [sent-24, score-0.206]
</p><p>13 The statistics were collected from the vertical subband at the lowest level of a Haar ﬁlter wavelet decomposition of the ”Lena” image. [sent-31, score-0.68]
</p><p>14 Note that the “bow-tie” dependencies are captured by the PoEdges model. [sent-32, score-0.095]
</p><p>15 2  “Product of Edge-perts”  It has long been recognized in the image processing community that wavelet transforms form an excellent basis for representation of images. [sent-35, score-0.722]
</p><p>16 Within the class of linear transforms, it represents a compromise between many conﬂicting but desirable properties of image representation such as multi-scale and multi-orientation representation, locality both in space and frequency, and orthogonality resulting in decorrelation. [sent-36, score-0.081]
</p><p>17 A particularly suitable wavelet transform which forms the basis of the best denoising algorithms today is the over-complete steerable wavelet pyramid [4] freely downloadable from http://www. [sent-37, score-1.535]
</p><p>18 In our experiments we have conﬁrmed that the best results were obtained using this wavelet pyramid. [sent-42, score-0.594]
</p><p>19 In the following we will describe a model for the statistical dependencies between wavelet coefﬁcients. [sent-43, score-0.721]
</p><p>20 This model was inspired by recent studies of these dependencies (see e. [sent-44, score-0.127]
</p><p>21 It also represents a generalization of the bivariate Laplacian model proposed in [2]. [sent-47, score-0.081]
</p><p>22 The parameters {βj } control the shape of the contours: for β = 2 we have elliptical contours, for β = 1 the contours are straight lines while for β < 1 the contours curve inwards. [sent-53, score-0.084]
</p><p>23 reﬂections of any subset of the {zi } in the origin, which implies that the wavelet coefﬁcients are necessarily decorrelated (although higher order dependencies may still remain). [sent-61, score-0.837]
</p><p>24 Finally, the weights {Wij } model the scale (inverse variance) of the wavelet coefﬁcients. [sent-62, score-0.646]
</p><p>25 We mention that it is possible to entertain a larger number of bases vectors than wavelet coefﬁcients (a so-called “over-complete basis”), which seems appropriate for some of the empirical joint histograms shown in [1]. [sent-63, score-0.613]
</p><p>26 the wavelet coefﬁcients) undergo a nonlinear transformation zi → |zi |βi → u = W |z|β → uα . [sent-77, score-0.667]
</p><p>27 What is the reason that the PoEdges model captures the clustering of wavelet activities? [sent-80, score-0.647]
</p><p>28 Consider a local model describing the statistical structure of a patch of wavelet coefﬁcients and recall that the weighted sum of these activities is penalized. [sent-81, score-0.653]
</p><p>29 This “sparse” pattern of activity incurs less penalty than for instance the same amount1 of activity distributed equally over all images because of the concave shape of the penalty function, i. [sent-84, score-0.143]
</p><p>30 1  Related Work  Early wavelet denoising techniques were based on the observation that the marginal distribution of a wavelet coefﬁcient is highly kurtotic (peaked and heavy tails). [sent-88, score-1.418]
</p><p>31 (1) p(z) = 1 exp [−(w|z|) ] , 2Γ( α ) This has lead to the successful wavelet coring and shrinkage methods. [sent-90, score-0.656]
</p><p>32 A bivariate generalization of that model describing a wavelet coefﬁcient zc and its “parent” zp at a higher level in the pyramid jointly, was proposed in [2]. [sent-91, score-0.843]
</p><p>33 The probability density, p(zc , zp ) =  w 2 2 exp − w(zc + zp ) 2π  (2)  is easily seen to be a special case of the PoEdges model proposed here. [sent-92, score-0.11]
</p><p>34 This model, unlike the univariate model, captures the bow-tie dependencies described above resulting a signiﬁcant gain in denoising performance. [sent-93, score-0.301]
</p><p>35 “Gaussian scale mixtures” (GSM) have been proposed to model even larger neighborhoods of wavelet coefﬁcients. [sent-94, score-0.669]
</p><p>36 In particular, very good denoising results have been obtained by including within subband neighborhoods of size 3 × 3 in addition to the parent of a wavelet coefﬁcient [3]. [sent-95, score-0.956]
</p><p>37 A GSM is deﬁned in terms of a precision √ variable u, the squareroot of which multiplies a multivariate Gaussian variable: z = u y, y ∼ N [0, Σ], resulting in the following expression for the distribution over the wavelet coefﬁcients: p(z) = du Nz [0, uΣ] p(u). [sent-96, score-0.594]
</p><p>38 1  We assume the total amount of variance in wavelet activity is ﬁxed in this comparison. [sent-99, score-0.614]
</p><p>39 3  Edge-pert Denoising  Based on the PoEdges model discussed in the previous sections we now introduce a simpliﬁed model that forms the basis for a practical denoising algorithm. [sent-102, score-0.296]
</p><p>40 Recent progress in the ﬁeld has indicated that it is important to model the higher order dependencies which exist between wavelet coefﬁcients [2, 3]. [sent-103, score-0.74]
</p><p>41 This can be realized through the estimation of a joint model on a small cluster of wavelet coefﬁcients around each coefﬁcient. [sent-104, score-0.645]
</p><p>42 Therefore, in order to keep computations tractable, we proceed with a simpliﬁed model, ˆ wj aj T z  p(z) ∝ exp −  2 α  . [sent-106, score-0.09]
</p><p>43 1  Model Estimation  Our next task is to estimate the parameters of this model efﬁciently. [sent-109, score-0.051]
</p><p>44 We will learn separate models for each wavelet coefﬁcient jointly with a small neighborhood of dependent coefﬁcients. [sent-110, score-0.649]
</p><p>45 Each such model is estimated in three steps: I) determine the coefﬁcients that participate in each model, II) transform each model into a decorrelated domain (this implicitly estimates the {ˆj }) and III) estimate the remaining parameters w, α in the decorrelated a domain using moment matching. [sent-111, score-0.519]
</p><p>46 By zi , zi we will denote the clean and noisy wavelet coefﬁcients respectively. [sent-113, score-0.789]
</p><p>47 With yi , yi ˜ ˜ we denote the decorrelated clean and noisy wavelet coefﬁcients while ni denotes the Gaussian noise random variable in the wavelet domain, i. [sent-114, score-1.452]
</p><p>48 Both due to ˜ the details of the wavelet decomposition and due to the properties of the noise itself we assume the noise to be correlated and zero mean: E[ni ] = 0, E[ni nj ] = Σij . [sent-117, score-0.694]
</p><p>49 In this paper we further assume that we know the noise covariance in the image domain from which one can easily compute the noise covariance in the wavelet domain, however only minor changes are needed to estimate it from the noisy image itself. [sent-118, score-0.995]
</p><p>50 Step I: We start with a 7 × 7 neighborhood from which we will adaptively select the best candidates to include in the model. [sent-119, score-0.055]
</p><p>51 In addition, we will always include the parent coefﬁcient in the subband of a coarser scale if it exists (this is done by ﬁrst up-sampling this band, see [3]). [sent-120, score-0.153]
</p><p>52 The coefﬁcients that participate in a model are selected by estimating their dependencies relative to the center coefﬁcient. [sent-121, score-0.158]
</p><p>53 Anticipating that (second order) correlations will be removed by sphering we are only interested in higher order dependencies, in particular dependencies between the variances. [sent-122, score-0.114]
</p><p>54 The following cumulant is used to obtain these estimates, Hcj = E[˜c zj ] − 2E[˜c zj ]2 − E[˜c ]E[˜j ] z 2 ˜2 z ˜ z2 z2 (4) where c is the center coefﬁcient which will be denoised. [sent-123, score-0.16]
</p><p>55 Effectively, we meaz sure the (higher order) dependencies between squared wavelet coefﬁcients after subtraction of all correlations. [sent-128, score-0.689]
</p><p>56 Finally, we select the participants of a model centered at coefﬁcient zc by ˜ ranking the positive Hcj and picking all the ones which satisfy: Hci > 0. [sent-129, score-0.103]
</p><p>57 Step II: For each model (with varying number of participants) we estimate the covariance, Cij = E[zi , zj ] = E[˜i zj ] − Σij z˜  (5)  and correct it by setting to zero all negative eigenvalues in such a way that the sum of the eigenvalues is invariant (see [3]). [sent-131, score-0.187]
</p><p>58 (6)  In this new space (which is different for every wavelet coefﬁcient) we can now assume ˆ aj = ej , the axis aligned basis vector. [sent-135, score-0.66]
</p><p>59 Step III: In the decorrelated space we estimate the single edge-pert model by moment matching. [sent-136, score-0.22]
</p><p>60 The moments of the edge-pert model in this space are easily computed using Np 2 wj yj )  E (  =Γ  j=1  Np + 2 2α  / Γ  Np 2α  (7)  where Np is the number of participating coefﬁcients in the model. [sent-137, score-0.13]
</p><p>61 permutations of the variables uj = wj yj we ﬁnd wj = Γ  Np +2 2α  /  Np (E[˜i ] − 1) Γ y2  Np 2α  . [sent-145, score-0.125]
</p><p>62 (9)  A common strategy in the wavelet literature is to estimate the averages E[·] by collecting samples in a local neighborhood around the coefﬁcient under consideration. [sent-146, score-0.689]
</p><p>63 We have adopted this strategy and used a 11 × 11 box around each coefﬁcient to collect 121 samples in the decorrelated wavelet domain. [sent-148, score-0.723]
</p><p>64 The estimation of α depends on the fourth moment and is thus very sensitive to outliers, which is a commonly known problem with the moment matching method. [sent-150, score-0.08]
</p><p>65 2  The Iterated Wiener Filter  To infer a wavelet coefﬁcient given its noisy observation in the decorrelated wavelet domain, we maximize the a posteriori probability of our joint model. [sent-155, score-1.364]
</p><p>66 z  (10)  z  When we assume Gaussian pixel noise, this translates into, z∗ = argmin z  1 2 (z  ˜ ˜ − z)T K(z − z) +  2 wj zj  α  (11)  j  ˜ where J is the (linear) wavelet transform z = Jx, K = J #T Σ−1 J # with J # = n (J T J)−1 J T the pseudo-inverse of J (i. [sent-157, score-0.77]
</p><p>67 In the decorrelated wavelet domain we simply set K = I. [sent-160, score-0.761]
</p><p>68 Determine coefﬁcients participating in joint model by using Eqn. [sent-212, score-0.074]
</p><p>69 Estimate parameters {α, wi } on a local neighborhood using Eqn. [sent-224, score-0.055]
</p><p>70 Denoise all wavelet coefﬁcients in the neighborhood using IWF from section 3. [sent-228, score-0.649]
</p><p>71 Transform denoised cluster back to the wavelet domain and retain the “center coefﬁcient” only. [sent-231, score-0.671]
</p><p>72 4  Experiments  Denoising experiments were run on the steerable wavelet pyramid with oriented highpass residual bands (FSpyr) using 8 orientations as described in [3]. [sent-234, score-0.678]
</p><p>73 In each experiment an image was artiﬁcially contaminated with independent Gaussian pixel noise of some predetermined variance and denoised using 20 iterations of the proposed algorithm. [sent-236, score-0.197]
</p><p>74 In table 1 we compare performance between the PoEdges and GSM based denoising algorithms on six test images and ten different noise levels. [sent-242, score-0.303]
</p><p>75 66  Table 1: Comparison of image denoising results between PoEdges (EP above) and its closest competitor (GSM). [sent-363, score-0.322]
</p><p>76 Note that PoEdges outperforms GSM for low noise levels while the GSM performs better at high noise levels. [sent-367, score-0.119]
</p><p>77 Also, PoEdges performs best at all noise levels on the Barbara image, while GSM is superior on the boat image. [sent-368, score-0.156]
</p><p>78 FSpyr against various methods published in the literature [3, 2, 9] on the images “Lena” and “Barbara”. [sent-369, score-0.065]
</p><p>79 In comparing PoEdges with GSM the general trend seems to be that PoEdges performs superior at lower noise levels while the reverse is true for higher noise levels. [sent-371, score-0.178]
</p><p>80 We observe that the PoEdges give signiﬁcantly better results on the ”Barbara” image than any other published method (by a large magin). [sent-372, score-0.099]
</p><p>81 Increasing the estimation window in step 3ii of the algorithm let the denoising results drop down to the GSM solution (not reported here). [sent-374, score-0.206]
</p><p>82 Comparing the quality of restored images in detail (as in ﬁgure 3) we conclude that the GSM produces slightly sharper edges at the expense of more artifacts. [sent-375, score-0.066]
</p><p>83 Denoising a 512 × 512 pixel sized image on a pentium 4 2. [sent-376, score-0.108]
</p><p>84 8GHz PC for our adaptive neighborhood selection model took 26 seconds for the QMF9 and 440 seconds for the FSpyr. [sent-377, score-0.087]
</p><p>85 We also compared GSM and EP using a separable orthonormal pyramid (QMF9). [sent-378, score-0.058]
</p><p>86 However the results are signiﬁcantly inferior because the wavelet representation plays a prominent role for denoising performance. [sent-380, score-0.8]
</p><p>87 5  Discussion  We have proposed a general “product of edge-perts” model to capture the dependency structure in wavelet coefﬁcients. [sent-382, score-0.648]
</p><p>88 This was turned into a practical denoising algorithm by simplifying to a single edge-pert and choosing βj = 2 ∀j. [sent-383, score-0.206]
</p><p>89 The parameters of this model can be adapted based on the noisy observation of the image. [sent-384, score-0.06]
</p><p>90 In comparison with the closest competitor (GSM [3]) we found superior performance at low noise levels while the reverse is true for high noise levels. [sent-385, score-0.194]
</p><p>91 Also, the PoEdges model performs better than any competitor on the Barbara image, but consistency less well than GSM on the boat image. [sent-386, score-0.133]
</p><p>92 The GSM model aims at capturing the same statistical regularities as the PoEdges but using a very different modelling paradigm: where PoEdges is best interpreted as a bottom-up constraint satisfaction model, the GSM is a causal generative model with top-down semantics. [sent-387, score-0.093]
</p><p>93 We have found that these two modelling paradigms exhibit different denoising accuracies 2 3  Personal communication http://www. [sent-388, score-0.235]
</p><p>94 69) on Barbara image (cropped to 150 × 150 to enhance artifacts). [sent-393, score-0.081]
</p><p>95 For example, we can lift the ˆ restriction on βj = 2, allow more basis-vectors aj than coefﬁcients or extend the neighborhood selection to subbands of different scales and/or orientations. [sent-400, score-0.095]
</p><p>96 However, approximations in the estimation of these models will become necessary to keep the denoising algorithm practical. [sent-402, score-0.206]
</p><p>97 Further performance gains may still be expected through the development of new wavelet pyramids and through modelling of new dependency structures such as the phenomenon of phase alignment at the edges. [sent-405, score-0.645]
</p><p>98 Image denoising using scale mixtures of Gaussians in the wavelet domain. [sent-427, score-0.82]
</p><p>99 Modeling the joint statistics of images in the wavelet domain. [sent-439, score-0.66]
</p><p>100 Low-complexity image denoising based on statistical modeling of wavelet coefﬁcients. [sent-470, score-0.881]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wavelet', 0.594), ('gsm', 0.407), ('poedges', 0.347), ('coef', 0.207), ('denoising', 0.206), ('psnr', 0.149), ('np', 0.146), ('decorrelated', 0.129), ('cients', 0.125), ('barbara', 0.118), ('ep', 0.109), ('dependencies', 0.095), ('lena', 0.086), ('subband', 0.086), ('image', 0.081), ('zi', 0.073), ('zj', 0.068), ('boat', 0.066), ('pyramid', 0.058), ('neighborhood', 0.055), ('wiener', 0.052), ('zc', 0.052), ('wj', 0.05), ('noise', 0.05), ('biv', 0.05), ('hcj', 0.05), ('lior', 0.05), ('bivariate', 0.049), ('parent', 0.047), ('images', 0.047), ('contours', 0.042), ('moment', 0.04), ('aj', 0.04), ('denoised', 0.039), ('zp', 0.039), ('lm', 0.039), ('domain', 0.038), ('ni', 0.036), ('iterated', 0.035), ('competitor', 0.035), ('db', 0.034), ('wij', 0.034), ('copied', 0.033), ('coring', 0.033), ('fingerprint', 0.033), ('fspyr', 0.033), ('iia', 0.033), ('iib', 0.033), ('iwf', 0.033), ('peppers', 0.033), ('pgehler', 0.033), ('model', 0.032), ('participate', 0.031), ('transform', 0.031), ('act', 0.03), ('shrinkage', 0.029), ('modelling', 0.029), ('conf', 0.029), ('noisy', 0.028), ('penalty', 0.028), ('gure', 0.027), ('int', 0.027), ('activities', 0.027), ('pixel', 0.027), ('covariance', 0.027), ('basis', 0.026), ('welling', 0.026), ('ib', 0.026), ('steerable', 0.026), ('yj', 0.025), ('heavy', 0.024), ('cumulant', 0.024), ('peaked', 0.024), ('neighborhoods', 0.023), ('house', 0.023), ('participating', 0.023), ('dependency', 0.022), ('products', 0.022), ('simoncelli', 0.022), ('clustering', 0.021), ('transforms', 0.021), ('clean', 0.021), ('tails', 0.021), ('collecting', 0.021), ('superior', 0.021), ('activity', 0.02), ('scale', 0.02), ('gaussian', 0.02), ('ia', 0.019), ('artifacts', 0.019), ('reverse', 0.019), ('participants', 0.019), ('levels', 0.019), ('higher', 0.019), ('joint', 0.019), ('estimate', 0.019), ('expense', 0.019), ('published', 0.018), ('compression', 0.018), ('cient', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="158-tfidf-1" href="./nips-2005-Products_of_%60%60Edge-perts.html">158 nips-2005-Products of ``Edge-perts</a></p>
<p>Author: Max Welling, Peter V. Gehler</p><p>Abstract: Images represent an important and abundant source of data. Understanding their statistical structure has important applications such as image compression and restoration. In this paper we propose a particular kind of probabilistic model, dubbed the “products of edge-perts model” to describe the structure of wavelet transformed images. We develop a practical denoising algorithm based on a single edge-pert and show state-ofthe-art denoising performance on benchmark images. 1</p><p>2 0.18004265 <a title="158-tfidf-2" href="./nips-2005-Is_Early_Vision_Optimized_for_Extracting_Higher-order_Dependencies%3F.html">101 nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: Linear implementations of the efﬁcient coding hypothesis, such as independent component analysis (ICA) and sparse coding models, have provided functional explanations for properties of simple cells in V1 [1, 2]. These models, however, ignore the non-linear behavior of neurons and fail to match individual and population properties of neural receptive ﬁelds in subtle but important ways. Hierarchical models, including Gaussian Scale Mixtures [3, 4] and other generative statistical models [5, 6], can capture higher-order regularities in natural images and explain nonlinear aspects of neural processing such as normalization and context effects [6,7]. Previously, it had been assumed that the lower level representation is independent of the hierarchy, and had been ﬁxed when training these models. Here we examine the optimal lower-level representations derived in the context of a hierarchical model and ﬁnd that the resulting representations are strikingly different from those based on linear models. Unlike the the basis functions and ﬁlters learned by ICA or sparse coding, these functions individually more closely resemble simple cell receptive ﬁelds and collectively span a broad range of spatial scales. Our work uniﬁes several related approaches and observations about natural image structure and suggests that hierarchical models might yield better representations of image structure throughout the hierarchy.</p><p>3 0.15534535 <a title="158-tfidf-3" href="./nips-2005-Value_Function_Approximation_with_Diffusion_Wavelets_and_Laplacian_Eigenfunctions.html">199 nips-2005-Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions</a></p>
<p>Author: Sridhar Mahadevan, Mauro Maggioni</p><p>Abstract: We investigate the problem of automatically constructing efﬁcient representations or basis functions for approximating value functions based on analyzing the structure and topology of the state space. In particular, two novel approaches to value function approximation are explored based on automatically constructing basis functions on state spaces that can be represented as graphs or manifolds: one approach uses the eigenfunctions of the Laplacian, in effect performing a global Fourier analysis on the graph; the second approach is based on diffusion wavelets, which generalize classical wavelets to graphs using multiscale dilations induced by powers of a diffusion operator or random walk on the graph. Together, these approaches form the foundation of a new generation of methods for solving large Markov decision processes, in which the underlying representation and policies are simultaneously learned.</p><p>4 0.079997659 <a title="158-tfidf-4" href="./nips-2005-Scaling_Laws_in_Natural_Scenes_and_the_Inference_of_3D_Shape.html">170 nips-2005-Scaling Laws in Natural Scenes and the Inference of 3D Shape</a></p>
<p>Author: Tai-sing Lee, Brian R. Potetz</p><p>Abstract: This paper explores the statistical relationship between natural images and their underlying range (depth) images. We look at how this relationship changes over scale, and how this information can be used to enhance low resolution range data using a full resolution intensity image. Based on our ﬁndings, we propose an extension to an existing technique known as shape recipes [3], and the success of the two methods are compared using images and laser scans of real scenes. Our extension is shown to provide a two-fold improvement over the current method. Furthermore, we demonstrate that ideal linear shape-from-shading ﬁlters, when learned from natural scenes, may derive even more strength from shadow cues than from the traditional linear-Lambertian shading cues. 1</p><p>5 0.075525247 <a title="158-tfidf-5" href="./nips-2005-Recovery_of_Jointly_Sparse_Signals_from_Few_Random_Projections.html">163 nips-2005-Recovery of Jointly Sparse Signals from Few Random Projections</a></p>
<p>Author: Michael B. Wakin, Marco F. Duarte, Shriram Sarvotham, Dror Baron, Richard G. Baraniuk</p><p>Abstract: Compressed sensing is an emerging ﬁeld based on the revelation that a small group of linear projections of a sparse signal contains enough information for reconstruction. In this paper we introduce a new theory for distributed compressed sensing (DCS) that enables new distributed coding algorithms for multi-signal ensembles that exploit both intra- and inter-signal correlation structures. The DCS theory rests on a new concept that we term the joint sparsity of a signal ensemble. We study three simple models for jointly sparse signals, propose algorithms for joint recovery of multiple signals from incoherent projections, and characterize theoretically and empirically the number of measurements per sensor required for accurate reconstruction. In some sense DCS is a framework for distributed compression of sources with memory, which has remained a challenging problem in information theory for some time. DCS is immediately applicable to a range of problems in sensor networks and arrays. 1</p><p>6 0.070534132 <a title="158-tfidf-6" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>7 0.064372107 <a title="158-tfidf-7" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>8 0.059874594 <a title="158-tfidf-8" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>9 0.053414583 <a title="158-tfidf-9" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>10 0.04873652 <a title="158-tfidf-10" href="./nips-2005-Noise_and_the_two-thirds_power_Law.html">136 nips-2005-Noise and the two-thirds power Law</a></p>
<p>11 0.047656752 <a title="158-tfidf-11" href="./nips-2005-An_Application_of_Markov_Random_Fields_to_Range_Sensing.html">23 nips-2005-An Application of Markov Random Fields to Range Sensing</a></p>
<p>12 0.046487335 <a title="158-tfidf-12" href="./nips-2005-Kernelized_Infomax_Clustering.html">102 nips-2005-Kernelized Infomax Clustering</a></p>
<p>13 0.045759413 <a title="158-tfidf-13" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>14 0.045166332 <a title="158-tfidf-14" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>15 0.042097699 <a title="158-tfidf-15" href="./nips-2005-Fusion_of_Similarity_Data_in_Clustering.html">79 nips-2005-Fusion of Similarity Data in Clustering</a></p>
<p>16 0.041013729 <a title="158-tfidf-16" href="./nips-2005-Learning_Depth_from_Single_Monocular_Images.html">110 nips-2005-Learning Depth from Single Monocular Images</a></p>
<p>17 0.038997903 <a title="158-tfidf-17" href="./nips-2005-Inferring_Motor_Programs_from_Images_of_Handwritten_Digits.html">97 nips-2005-Inferring Motor Programs from Images of Handwritten Digits</a></p>
<p>18 0.038930897 <a title="158-tfidf-18" href="./nips-2005-Conditional_Visual_Tracking_in_Kernel_Space.html">45 nips-2005-Conditional Visual Tracking in Kernel Space</a></p>
<p>19 0.038642641 <a title="158-tfidf-19" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<p>20 0.037653565 <a title="158-tfidf-20" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.142), (1, 0.017), (2, -0.015), (3, 0.071), (4, -0.07), (5, -0.007), (6, -0.081), (7, -0.067), (8, 0.109), (9, -0.102), (10, 0.032), (11, 0.084), (12, 0.098), (13, -0.081), (14, 0.0), (15, -0.007), (16, 0.083), (17, 0.002), (18, 0.093), (19, -0.046), (20, -0.179), (21, -0.001), (22, 0.003), (23, -0.054), (24, 0.173), (25, -0.08), (26, -0.023), (27, 0.017), (28, 0.147), (29, -0.174), (30, -0.158), (31, 0.019), (32, -0.029), (33, 0.131), (34, -0.019), (35, -0.119), (36, -0.067), (37, 0.042), (38, 0.011), (39, -0.038), (40, 0.107), (41, 0.051), (42, -0.028), (43, 0.046), (44, -0.06), (45, 0.015), (46, 0.073), (47, -0.033), (48, -0.067), (49, -0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95230407 <a title="158-lsi-1" href="./nips-2005-Products_of_%60%60Edge-perts.html">158 nips-2005-Products of ``Edge-perts</a></p>
<p>Author: Max Welling, Peter V. Gehler</p><p>Abstract: Images represent an important and abundant source of data. Understanding their statistical structure has important applications such as image compression and restoration. In this paper we propose a particular kind of probabilistic model, dubbed the “products of edge-perts model” to describe the structure of wavelet transformed images. We develop a practical denoising algorithm based on a single edge-pert and show state-ofthe-art denoising performance on benchmark images. 1</p><p>2 0.76325589 <a title="158-lsi-2" href="./nips-2005-Is_Early_Vision_Optimized_for_Extracting_Higher-order_Dependencies%3F.html">101 nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: Linear implementations of the efﬁcient coding hypothesis, such as independent component analysis (ICA) and sparse coding models, have provided functional explanations for properties of simple cells in V1 [1, 2]. These models, however, ignore the non-linear behavior of neurons and fail to match individual and population properties of neural receptive ﬁelds in subtle but important ways. Hierarchical models, including Gaussian Scale Mixtures [3, 4] and other generative statistical models [5, 6], can capture higher-order regularities in natural images and explain nonlinear aspects of neural processing such as normalization and context effects [6,7]. Previously, it had been assumed that the lower level representation is independent of the hierarchy, and had been ﬁxed when training these models. Here we examine the optimal lower-level representations derived in the context of a hierarchical model and ﬁnd that the resulting representations are strikingly different from those based on linear models. Unlike the the basis functions and ﬁlters learned by ICA or sparse coding, these functions individually more closely resemble simple cell receptive ﬁelds and collectively span a broad range of spatial scales. Our work uniﬁes several related approaches and observations about natural image structure and suggests that hierarchical models might yield better representations of image structure throughout the hierarchy.</p><p>3 0.51135647 <a title="158-lsi-3" href="./nips-2005-Value_Function_Approximation_with_Diffusion_Wavelets_and_Laplacian_Eigenfunctions.html">199 nips-2005-Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions</a></p>
<p>Author: Sridhar Mahadevan, Mauro Maggioni</p><p>Abstract: We investigate the problem of automatically constructing efﬁcient representations or basis functions for approximating value functions based on analyzing the structure and topology of the state space. In particular, two novel approaches to value function approximation are explored based on automatically constructing basis functions on state spaces that can be represented as graphs or manifolds: one approach uses the eigenfunctions of the Laplacian, in effect performing a global Fourier analysis on the graph; the second approach is based on diffusion wavelets, which generalize classical wavelets to graphs using multiscale dilations induced by powers of a diffusion operator or random walk on the graph. Together, these approaches form the foundation of a new generation of methods for solving large Markov decision processes, in which the underlying representation and policies are simultaneously learned.</p><p>4 0.49322748 <a title="158-lsi-4" href="./nips-2005-Recovery_of_Jointly_Sparse_Signals_from_Few_Random_Projections.html">163 nips-2005-Recovery of Jointly Sparse Signals from Few Random Projections</a></p>
<p>Author: Michael B. Wakin, Marco F. Duarte, Shriram Sarvotham, Dror Baron, Richard G. Baraniuk</p><p>Abstract: Compressed sensing is an emerging ﬁeld based on the revelation that a small group of linear projections of a sparse signal contains enough information for reconstruction. In this paper we introduce a new theory for distributed compressed sensing (DCS) that enables new distributed coding algorithms for multi-signal ensembles that exploit both intra- and inter-signal correlation structures. The DCS theory rests on a new concept that we term the joint sparsity of a signal ensemble. We study three simple models for jointly sparse signals, propose algorithms for joint recovery of multiple signals from incoherent projections, and characterize theoretically and empirically the number of measurements per sensor required for accurate reconstruction. In some sense DCS is a framework for distributed compression of sources with memory, which has remained a challenging problem in information theory for some time. DCS is immediately applicable to a range of problems in sensor networks and arrays. 1</p><p>5 0.45409152 <a title="158-lsi-5" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>Author: Eizaburo Doi, Doru C. Balcan, Michael S. Lewicki</p><p>Abstract: Biological sensory systems are faced with the problem of encoding a high-ﬁdelity sensory signal with a population of noisy, low-ﬁdelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and twodimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of highdimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets. 1</p><p>6 0.42358693 <a title="158-lsi-6" href="./nips-2005-Scaling_Laws_in_Natural_Scenes_and_the_Inference_of_3D_Shape.html">170 nips-2005-Scaling Laws in Natural Scenes and the Inference of 3D Shape</a></p>
<p>7 0.38554657 <a title="158-lsi-7" href="./nips-2005-Separation_of_Music_Signals_by_Harmonic_Structure_Modeling.html">174 nips-2005-Separation of Music Signals by Harmonic Structure Modeling</a></p>
<p>8 0.37561786 <a title="158-lsi-8" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>9 0.35046995 <a title="158-lsi-9" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<p>10 0.28558287 <a title="158-lsi-10" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>11 0.2848596 <a title="158-lsi-11" href="./nips-2005-Laplacian_Score_for_Feature_Selection.html">104 nips-2005-Laplacian Score for Feature Selection</a></p>
<p>12 0.283636 <a title="158-lsi-12" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>13 0.27974671 <a title="158-lsi-13" href="./nips-2005-Visual_Encoding_with_Jittering_Eyes.html">203 nips-2005-Visual Encoding with Jittering Eyes</a></p>
<p>14 0.27830231 <a title="158-lsi-14" href="./nips-2005-Non-iterative_Estimation_with_Perturbed_Gaussian_Markov_Processes.html">139 nips-2005-Non-iterative Estimation with Perturbed Gaussian Markov Processes</a></p>
<p>15 0.27411589 <a title="158-lsi-15" href="./nips-2005-Searching_for_Character_Models.html">171 nips-2005-Searching for Character Models</a></p>
<p>16 0.2666975 <a title="158-lsi-16" href="./nips-2005-Large_scale_networks_fingerprinting_and_visualization_using_the_k-core_decomposition.html">107 nips-2005-Large scale networks fingerprinting and visualization using the k-core decomposition</a></p>
<p>17 0.26336315 <a title="158-lsi-17" href="./nips-2005-Bayesian_model_learning_in_human_visual_perception.html">35 nips-2005-Bayesian model learning in human visual perception</a></p>
<p>18 0.25264984 <a title="158-lsi-18" href="./nips-2005-Learning_Cue-Invariant_Visual_Responses.html">109 nips-2005-Learning Cue-Invariant Visual Responses</a></p>
<p>19 0.24477986 <a title="158-lsi-19" href="./nips-2005-Rodeo%3A_Sparse_Nonparametric_Regression_in_High_Dimensions.html">168 nips-2005-Rodeo: Sparse Nonparametric Regression in High Dimensions</a></p>
<p>20 0.23945042 <a title="158-lsi-20" href="./nips-2005-A_matching_pursuit_approach_to_sparse_Gaussian_process_regression.html">16 nips-2005-A matching pursuit approach to sparse Gaussian process regression</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.056), (10, 0.028), (11, 0.011), (27, 0.06), (31, 0.05), (34, 0.068), (39, 0.01), (50, 0.017), (55, 0.025), (60, 0.01), (65, 0.016), (69, 0.059), (73, 0.052), (78, 0.3), (88, 0.083), (91, 0.034), (93, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77713901 <a title="158-lda-1" href="./nips-2005-Products_of_%60%60Edge-perts.html">158 nips-2005-Products of ``Edge-perts</a></p>
<p>Author: Max Welling, Peter V. Gehler</p><p>Abstract: Images represent an important and abundant source of data. Understanding their statistical structure has important applications such as image compression and restoration. In this paper we propose a particular kind of probabilistic model, dubbed the “products of edge-perts model” to describe the structure of wavelet transformed images. We develop a practical denoising algorithm based on a single edge-pert and show state-ofthe-art denoising performance on benchmark images. 1</p><p>2 0.69624215 <a title="158-lda-2" href="./nips-2005-Benchmarking_Non-Parametric_Statistical_Tests.html">37 nips-2005-Benchmarking Non-Parametric Statistical Tests</a></p>
<p>Author: Mikaela Keller, Samy Bengio, Siew Y. Wong</p><p>Abstract: Although non-parametric tests have already been proposed for that purpose, statistical signiﬁcance tests for non-standard measures (different from the classiﬁcation error) are less often used in the literature. This paper is an attempt at empirically verifying how these tests compare with more classical tests, on various conditions. More precisely, using a very large dataset to estimate the whole “population”, we analyzed the behavior of several statistical test, varying the class unbalance, the compared models, the performance measure, and the sample size. The main result is that providing big enough evaluation sets non-parametric tests are relatively reliable in all conditions. 1</p><p>3 0.46256784 <a title="158-lda-3" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>Author: Amir Navot, Lavi Shpigelman, Naftali Tishby, Eilon Vaadia</p><p>Abstract: We present a non-linear, simple, yet effective, feature subset selection method for regression and use it in analyzing cortical neural activity. Our algorithm involves a feature-weighted version of the k-nearest-neighbor algorithm. It is able to capture complex dependency of the target function on its input and makes use of the leave-one-out error as a natural regularization. We explain the characteristics of our algorithm on synthetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey. By applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data.</p><p>4 0.46005121 <a title="158-lda-4" href="./nips-2005-Scaling_Laws_in_Natural_Scenes_and_the_Inference_of_3D_Shape.html">170 nips-2005-Scaling Laws in Natural Scenes and the Inference of 3D Shape</a></p>
<p>Author: Tai-sing Lee, Brian R. Potetz</p><p>Abstract: This paper explores the statistical relationship between natural images and their underlying range (depth) images. We look at how this relationship changes over scale, and how this information can be used to enhance low resolution range data using a full resolution intensity image. Based on our ﬁndings, we propose an extension to an existing technique known as shape recipes [3], and the success of the two methods are compared using images and laser scans of real scenes. Our extension is shown to provide a two-fold improvement over the current method. Furthermore, we demonstrate that ideal linear shape-from-shading ﬁlters, when learned from natural scenes, may derive even more strength from shadow cues than from the traditional linear-Lambertian shading cues. 1</p><p>5 0.4595153 <a title="158-lda-5" href="./nips-2005-Kernelized_Infomax_Clustering.html">102 nips-2005-Kernelized Infomax Clustering</a></p>
<p>Author: David Barber, Felix V. Agakov</p><p>Abstract: We propose a simple information-theoretic approach to soft clustering based on maximizing the mutual information I(x, y) between the unknown cluster labels y and the training patterns x with respect to parameters of speciﬁcally constrained encoding distributions. The constraints are chosen such that patterns are likely to be clustered similarly if they lie close to speciﬁc unknown vectors in the feature space. The method may be conveniently applied to learning the optimal aﬃnity matrix, which corresponds to learning parameters of the kernelized encoder. The procedure does not require computations of eigenvalues of the Gram matrices, which makes it potentially attractive for clustering large data sets. 1</p><p>6 0.45821515 <a title="158-lda-6" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>7 0.4573144 <a title="158-lda-7" href="./nips-2005-Conditional_Visual_Tracking_in_Kernel_Space.html">45 nips-2005-Conditional Visual Tracking in Kernel Space</a></p>
<p>8 0.45574889 <a title="158-lda-8" href="./nips-2005-Interpolating_between_types_and_tokens_by_estimating_power-law_generators.html">100 nips-2005-Interpolating between types and tokens by estimating power-law generators</a></p>
<p>9 0.45394444 <a title="158-lda-9" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>10 0.4528794 <a title="158-lda-10" href="./nips-2005-Recovery_of_Jointly_Sparse_Signals_from_Few_Random_Projections.html">163 nips-2005-Recovery of Jointly Sparse Signals from Few Random Projections</a></p>
<p>11 0.45211866 <a title="158-lda-11" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>12 0.45144948 <a title="158-lda-12" href="./nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<p>13 0.45118618 <a title="158-lda-13" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>14 0.45040184 <a title="158-lda-14" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>15 0.45037088 <a title="158-lda-15" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>16 0.4492155 <a title="158-lda-16" href="./nips-2005-Diffusion_Maps%2C_Spectral_Clustering_and_Eigenfunctions_of_Fokker-Planck_Operators.html">56 nips-2005-Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators</a></p>
<p>17 0.44785562 <a title="158-lda-17" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>18 0.44644317 <a title="158-lda-18" href="./nips-2005-Context_as_Filtering.html">48 nips-2005-Context as Filtering</a></p>
<p>19 0.44529477 <a title="158-lda-19" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>20 0.44412586 <a title="158-lda-20" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
