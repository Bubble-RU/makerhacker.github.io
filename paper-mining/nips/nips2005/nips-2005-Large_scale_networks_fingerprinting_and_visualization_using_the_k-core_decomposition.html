<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>107 nips-2005-Large scale networks fingerprinting and visualization using the k-core decomposition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-107" href="#">nips2005-107</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>107 nips-2005-Large scale networks fingerprinting and visualization using the k-core decomposition</h1>
<br/><p>Source: <a title="nips-2005-107-pdf" href="http://papers.nips.cc/paper/2789-large-scale-networks-fingerprinting-and-visualization-using-the-k-core-decomposition.pdf">pdf</a></p><p>Author: J. I. Alvarez-hamelin, Luca Dall'asta, Alain Barrat, Alessandro Vespignani</p><p>Abstract: We use the k-core decomposition to develop algorithms for the analysis of large scale complex networks. This decomposition, based on a recursive pruning of the least connected vertices, allows to disentangle the hierarchical structure of networks by progressively focusing on their central cores. By using this strategy we develop a general visualization algorithm that can be used to compare the structural properties of various networks and highlight their hierarchical structure. The low computational complexity of the algorithm, O(n + e), where n is the size of the network, and e is the number of edges, makes it suitable for the visualization of very large sparse networks. We show how the proposed visualization tool allows to ﬁnd speciﬁc structural ﬁngerprints of networks. 1</p><p>Reference: <a title="nips-2005-107-reference" href="../nips2005_reference/nips-2005-Large_scale_networks_fingerprinting_and_visualization_using_the_k-core_decomposition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Large scale networks ﬁngerprinting and visualization using the k-core decomposition  J. [sent-1, score-0.602]
</p><p>2 Ignacio Alvarez-Hamelin∗ LPT (UMR du CNRS 8627), Universit´ de Paris-Sud, e 91405 ORSAY Cedex France Ignacio. [sent-2, score-0.074]
</p><p>3 fr  Luca Dall’Asta LPT (UMR du CNRS 8627), Universit´ de Paris-Sud, e 91405 ORSAY Cedex France Luca. [sent-4, score-0.074]
</p><p>4 fr  Alain Barrat LPT (UMR du CNRS 8627), Universit´ de Paris-Sud, e 91405 ORSAY Cedex France Alain. [sent-7, score-0.074]
</p><p>5 edu  Abstract We use the k-core decomposition to develop algorithms for the analysis of large scale complex networks. [sent-11, score-0.154]
</p><p>6 This decomposition, based on a recursive pruning of the least connected vertices, allows to disentangle the hierarchical structure of networks by progressively focusing on their central cores. [sent-12, score-0.327]
</p><p>7 By using this strategy we develop a general visualization algorithm that can be used to compare the structural properties of various networks and highlight their hierarchical structure. [sent-13, score-0.608]
</p><p>8 The low computational complexity of the algorithm, O(n + e), where n is the size of the network, and e is the number of edges, makes it suitable for the visualization of very large sparse networks. [sent-14, score-0.348]
</p><p>9 We show how the proposed visualization tool allows to ﬁnd speciﬁc structural ﬁngerprints of networks. [sent-15, score-0.5]
</p><p>10 1  Introduction  In recent times, the possibility of accessing, handling and mining large-scale networks datasets has revamped the interest in their investigation and theoretical characterization along with the deﬁnition of new modeling frameworks. [sent-16, score-0.145]
</p><p>11 In particular, mapping projects of the World Wide Web and the physical Internet offered the ﬁrst chance to study topology and trafﬁc of large-scale networks. [sent-17, score-0.056]
</p><p>12 Other studies followed describing population networks of practical interest in social science, critical infrastructures and epidemiology [1, 2, 3]. [sent-18, score-0.176]
</p><p>13 The study of large scale networks, however, faces us with an array of new challenges. [sent-19, score-0.077]
</p><p>14 The deﬁnitions of centrality, hierarchies and structural organizations are hindered by the large size of these networks and the complex interplay of connectivity patterns, trafﬁc ﬂows and geographical, social and economical attributes characterizing their basic elements. [sent-20, score-0.375]
</p><p>15 o  context, a large research effort is devoted to provide effective visualization and analysis tools able to cope with graphs whose size may easily reach millions of vertices. [sent-25, score-0.457]
</p><p>16 In this paper, we propose a visualization algorithm based on the k-core decomposition able to uncover in a two-dimensional layout several topological and hierarchical properties of large scale networks. [sent-26, score-0.755]
</p><p>17 The k-core decomposition [4] consists in identifying particular subsets of the graph, called k-cores, each one obtained by recursively removing all the vertices of degree smaller than k, until the degree of all remaining vertices is larger than or equal to k. [sent-27, score-0.974]
</p><p>18 Larger values of the index k clearly correspond to vertices with larger degree and more central position in the network’s structure. [sent-28, score-0.566]
</p><p>19 This visualization tool allows the identiﬁcation of real or computer-generated networks’ ﬁngerprints, according to properties such as hierarchical arrangement, degree correlations and centrality. [sent-29, score-0.619]
</p><p>20 The distinction between networks with seemingly similar properties is achieved by inspecting the different layouts generated by the visualization algorithm. [sent-30, score-0.536]
</p><p>21 In addition, the running time of the algorithm grows only linearly with the size of the network, granting the scalability needed for the visualization of very large sparse networks. [sent-31, score-0.402]
</p><p>22 The proposed (publicly available [5]) algorithm appears therefore as a convenient method for the general analysis of large scale complex networks and the study of their architecture. [sent-32, score-0.205]
</p><p>23 The paper is organized as follows: after a brief survey on k-core studies (section 2), we present the basic deﬁnitions and the graphical algorithms in section 3 along with the basic features of the visualization layout. [sent-33, score-0.444]
</p><p>24 Section 4 shows how the visualizations obtained with the present algorithm may be used for network ﬁngerprinting, and presents two examples of visualization of real networks. [sent-34, score-0.45]
</p><p>25 2  Related work  While a large number of algorithms aimed at the visualization of large scale networks have been developed (e. [sent-35, score-0.497]
</p><p>26 [7] studied the k-core decomposition applied to visualization problems, introducing some graphical tools to analyse the cores, mainly based on the visualization of the adjacency matrix of certain k-cores. [sent-39, score-0.906]
</p><p>27 To the best of our knowledge, the algorithm presented by Baur et al. [sent-40, score-0.028]
</p><p>28 in [8] is the only one completely based on a k-core analysis and directly targeted at the study of large information networks. [sent-41, score-0.06]
</p><p>29 This algorithm uses a spectral layout to place vertices having the largest shell index. [sent-42, score-1.036]
</p><p>30 A combination of barycentric and iteratively directed-forces allows to place the vertices of each k-shell, in decreasing order. [sent-43, score-0.349]
</p><p>31 Finally, the network is drawn in three dimensions, using the z axis to place each shell in a distinct horizontal layer. [sent-44, score-0.584]
</p><p>32 Note that the spectral layout is not able to distinguish two or more disconnected components. [sent-45, score-0.207]
</p><p>33 is also tuned for representing AS graphs and its total complexity depends on the size of the highest k-core (see [9] for more details on spectral layout), making the computation time of this proposal largely variable. [sent-47, score-0.067]
</p><p>34 In this respect, the algorithm presented here is different in that it can represent networks in which k-cores are composed by several connected components. [sent-48, score-0.235]
</p><p>35 Another difference is that representations in 2D are more suited for information visualization than other representations (see [10] and references therein). [sent-49, score-0.348]
</p><p>36 Finally, the algorithm parameters can be universally deﬁned, yielding a fast and general tool for analyzing all types of networks. [sent-50, score-0.105]
</p><p>37 It is interesting to note that the notion of k-cores has been recently used in biologically related contexts, where it was applied to the analysis of protein interaction networks [11] or in the prediction of protein functions [12, 13]. [sent-51, score-0.188]
</p><p>38 Further applications in Internet-related areas can be found in [14], where the k-core decomposition is used for ﬁltering out peripheral Autonomous Systems (ASes), and in [15] where the scale invariant structure of degree correlations and mapping biases in AS maps is shown. [sent-52, score-0.321]
</p><p>39 Finally in [16, 17], an interesting approach based on the k-core decomposition has been used to provide a conceptual and  structural model of the Internet; the so-called medusa model for the Internet. [sent-53, score-0.18]
</p><p>40 A k-core of G can therefore be obtained by recursively removing all the vertices of degree less than k, until all vertices in the remaining graph have at least degree k. [sent-55, score-0.902]
</p><p>41 Furthermore, we will use the following deﬁnitions: -A vertex i has shell index c if it belongs to the c-core but not to (c + 1)-core. [sent-56, score-0.914]
</p><p>42 -A shell Cc is composed by all the vertices whose shell index is c. [sent-58, score-1.471]
</p><p>43 The maximum value c such that Cc is not empty is denoted cmax . [sent-59, score-0.202]
</p><p>44 The k-core is thus the union of all shells Cc with c ≥ k. [sent-60, score-0.121]
</p><p>45 -Each connected set of vertices having the same shell index c is a cluster Qc . [sent-61, score-1.033]
</p><p>46 Each shell c c Cc is thus composed by clusters Qc , such that Cc = ∪1≤m≤qmax Qc , where qmax is the m m number of clusters in Cc . [sent-62, score-0.718]
</p><p>47 The visualization algorithm we propose places vertices in 2 dimensions, the position of each vertex depending on its shell index and on the index of its neighbors. [sent-63, score-1.649]
</p><p>48 A color code allows for the identiﬁcation of shell indices, while the vertex’s original degree is provided by its size that depends logarithmically on the degree. [sent-64, score-0.673]
</p><p>49 For the sake of clarity, our algorithm represents a small percentage of the edges, chosen uniformly at random. [sent-65, score-0.028]
</p><p>50 As mentioned, a central role in our visualization method is played by multi-components representation of kcores. [sent-66, score-0.422]
</p><p>51 In the most general situation, indeed, the recursive removal of vertices having degree less than a given k can break the original network into various connected components, each of which might even be once again broken by the subsequent decomposition. [sent-67, score-0.561]
</p><p>52 Our method takes into account this possibility, however we will ﬁrst present the algorithm in the simpliﬁed case, in which none of the k-cores is fragmented. [sent-68, score-0.028]
</p><p>53 Then, this algorithm will be used as a subroutine for treating the general case (Table 1). [sent-69, score-0.063]
</p><p>54 1  Drawing algorithm for k-cores with single connected component  k-core decomposition. [sent-71, score-0.095]
</p><p>55 The shell index of each vertex is computed and stored in a vector C, along with the shells Cc and the maximum index cmax . [sent-72, score-1.313]
</p><p>56 Each shell is then decomposed into clusters Qc of connected vertices, and each vertex i is labeled by its shell index ci and m by a number qi representing the cluster it belongs to. [sent-73, score-1.768]
</p><p>57 The visualization is obtained assigning to each vertex i a couple of polar coordinates (ρi , αi ): the radius ρi is a function of the shell index of the vertex i and of its neighbors; the angle αi depends on the cluster number qi . [sent-75, score-1.575]
</p><p>58 In this way, k-shells are displayed as layers with the form of circular shells, the innermost one corresponding to the set of vertices with highest shell index. [sent-76, score-0.825]
</p><p>59 A vertex i belongs to the cmax − ci layer from the center. [sent-77, score-0.622]
</p><p>60 More precisely, ρi is computed according to the following formula: ρi = (1 − )(cmax − ci ) +  |Vcj ≥ci (i)|  (cmax − cj ) , j∈Vcj ≥ci (i)  (1)  Vcj ≥ci (i) is the set of neighbors of i having shell index cj larger or equal to ci . [sent-78, score-1.054]
</p><p>61 The parameter controls the possibility of rings overlapping, and is one of the only three external parameters required to tune image’s rendering. [sent-79, score-0.08]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('shell', 0.507), ('visualization', 0.348), ('vertices', 0.283), ('vertex', 0.215), ('cc', 0.209), ('cmax', 0.202), ('ci', 0.147), ('layout', 0.14), ('index', 0.134), ('cnrs', 0.121), ('lpt', 0.121), ('shells', 0.121), ('umr', 0.121), ('vcj', 0.121), ('degree', 0.11), ('qc', 0.107), ('decomposition', 0.105), ('networks', 0.1), ('orsay', 0.096), ('cedex', 0.09), ('aires', 0.081), ('baur', 0.081), ('buenos', 0.081), ('ngerprinting', 0.081), ('ngerprints', 0.081), ('qmax', 0.081), ('structural', 0.075), ('du', 0.074), ('universit', 0.071), ('france', 0.069), ('connected', 0.067), ('belongs', 0.058), ('hierarchical', 0.057), ('subgraph', 0.056), ('internet', 0.056), ('traf', 0.051), ('nitions', 0.051), ('tool', 0.049), ('scale', 0.049), ('qi', 0.046), ('clusters', 0.045), ('possibility', 0.045), ('cj', 0.044), ('protein', 0.044), ('social', 0.044), ('graphical', 0.044), ('removing', 0.042), ('cluster', 0.042), ('recursively', 0.041), ('edges', 0.04), ('composed', 0.04), ('spectral', 0.04), ('network', 0.039), ('central', 0.039), ('place', 0.038), ('angle', 0.036), ('recursive', 0.036), ('layouts', 0.035), ('played', 0.035), ('geographical', 0.035), ('vespignani', 0.035), ('rings', 0.035), ('alessandro', 0.035), ('economical', 0.035), ('hierarchies', 0.035), ('innermost', 0.035), ('publicly', 0.035), ('subroutine', 0.035), ('visualizations', 0.035), ('vladimir', 0.035), ('graph', 0.033), ('epidemiology', 0.032), ('interplay', 0.032), ('analyse', 0.032), ('targeted', 0.032), ('ows', 0.032), ('accessing', 0.032), ('polar', 0.032), ('neighbors', 0.031), ('peripheral', 0.03), ('tools', 0.029), ('study', 0.028), ('uncover', 0.028), ('offered', 0.028), ('universally', 0.028), ('logarithmically', 0.028), ('organizations', 0.028), ('allows', 0.028), ('algorithm', 0.028), ('correlations', 0.027), ('graphs', 0.027), ('arrangement', 0.027), ('seemingly', 0.027), ('devoted', 0.027), ('disconnected', 0.027), ('basic', 0.026), ('millions', 0.026), ('distinction', 0.026), ('broken', 0.026), ('scalability', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="107-tfidf-1" href="./nips-2005-Large_scale_networks_fingerprinting_and_visualization_using_the_k-core_decomposition.html">107 nips-2005-Large scale networks fingerprinting and visualization using the k-core decomposition</a></p>
<p>Author: J. I. Alvarez-hamelin, Luca Dall'asta, Alain Barrat, Alessandro Vespignani</p><p>Abstract: We use the k-core decomposition to develop algorithms for the analysis of large scale complex networks. This decomposition, based on a recursive pruning of the least connected vertices, allows to disentangle the hierarchical structure of networks by progressively focusing on their central cores. By using this strategy we develop a general visualization algorithm that can be used to compare the structural properties of various networks and highlight their hierarchical structure. The low computational complexity of the algorithm, O(n + e), where n is the size of the network, and e is the number of edges, makes it suitable for the visualization of very large sparse networks. We show how the proposed visualization tool allows to ﬁnd speciﬁc structural ﬁngerprints of networks. 1</p><p>2 0.090003915 <a title="107-tfidf-2" href="./nips-2005-Soft_Clustering_on_Graphs.html">178 nips-2005-Soft Clustering on Graphs</a></p>
<p>Author: Kai Yu, Shipeng Yu, Volker Tresp</p><p>Abstract: We propose a simple clustering framework on graphs encoding pairwise data similarities. Unlike usual similarity-based methods, the approach softly assigns data to clusters in a probabilistic way. More importantly, a hierarchical clustering is naturally derived in this framework to gradually merge lower-level clusters into higher-level ones. A random walk analysis indicates that the algorithm exposes clustering structures in various resolutions, i.e., a higher level statistically models a longer-term diffusion on graphs and thus discovers a more global clustering structure. Finally we provide very encouraging experimental results. 1</p><p>3 0.089878067 <a title="107-tfidf-3" href="./nips-2005-Message_passing_for_task_redistribution_on_sparse_graphs.html">125 nips-2005-Message passing for task redistribution on sparse graphs</a></p>
<p>Author: K. Wong, Zhuo Gao, David Tax</p><p>Abstract: The problem of resource allocation in sparse graphs with real variables is studied using methods of statistical physics. An efﬁcient distributed algorithm is devised on the basis of insight gained from the analysis and is examined using numerical simulations, showing excellent performance and full agreement with the theoretical results.</p><p>4 0.084632404 <a title="107-tfidf-4" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>Author: Andreas Argyriou, Mark Herbster, Massimiliano Pontil</p><p>Abstract: A foundational problem in semi-supervised learning is the construction of a graph underlying the data. We propose to use a method which optimally combines a number of differently constructed graphs. For each of these graphs we associate a basic graph kernel. We then compute an optimal combined kernel. This kernel solves an extended regularization problem which requires a joint minimization over both the data and the set of graph kernels. We present encouraging results on different OCR tasks where the optimal combined kernel is computed from graphs constructed with a variety of distances functions and the ‘k’ in nearest neighbors. 1</p><p>5 0.05988884 <a title="107-tfidf-5" href="./nips-2005-A_Criterion_for_the_Convergence_of_Learning_with_Spike_Timing_Dependent_Plasticity.html">8 nips-2005-A Criterion for the Convergence of Learning with Spike Timing Dependent Plasticity</a></p>
<p>Author: Robert A. Legenstein, Wolfgang Maass</p><p>Abstract: We investigate under what conditions a neuron can learn by experimentally supported rules for spike timing dependent plasticity (STDP) to predict the arrival times of strong “teacher inputs” to the same neuron. It turns out that in contrast to the famous Perceptron Convergence Theorem, which predicts convergence of the perceptron learning rule for a simpliﬁed neuron model whenever a stable solution exists, no equally strong convergence guarantee can be given for spiking neurons with STDP. But we derive a criterion on the statistical dependency structure of input spike trains which characterizes exactly when learning with STDP will converge on average for a simple model of a spiking neuron. This criterion is reminiscent of the linear separability criterion of the Perceptron Convergence Theorem, but it applies here to the rows of a correlation matrix related to the spike inputs. In addition we show through computer simulations for more realistic neuron models that the resulting analytically predicted positive learning results not only hold for the common interpretation of STDP where STDP changes the weights of synapses, but also for a more realistic interpretation suggested by experimental data where STDP modulates the initial release probability of dynamic synapses. 1</p><p>6 0.059572656 <a title="107-tfidf-6" href="./nips-2005-Learning_Topology_with_the_Generative_Gaussian_Graph_and_the_EM_Algorithm.html">116 nips-2005-Learning Topology with the Generative Gaussian Graph and the EM Algorithm</a></p>
<p>7 0.051171266 <a title="107-tfidf-7" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>8 0.049594149 <a title="107-tfidf-8" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>9 0.04189124 <a title="107-tfidf-9" href="./nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<p>10 0.039924711 <a title="107-tfidf-10" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>11 0.039875545 <a title="107-tfidf-11" href="./nips-2005-Kernelized_Infomax_Clustering.html">102 nips-2005-Kernelized Infomax Clustering</a></p>
<p>12 0.039222918 <a title="107-tfidf-12" href="./nips-2005-Fixing_two_weaknesses_of_the_Spectral_Method.html">75 nips-2005-Fixing two weaknesses of the Spectral Method</a></p>
<p>13 0.03805954 <a title="107-tfidf-13" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>14 0.037993647 <a title="107-tfidf-14" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>15 0.036454853 <a title="107-tfidf-15" href="./nips-2005-Is_Early_Vision_Optimized_for_Extracting_Higher-order_Dependencies%3F.html">101 nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</a></p>
<p>16 0.035168577 <a title="107-tfidf-16" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>17 0.034185421 <a title="107-tfidf-17" href="./nips-2005-Q-Clustering.html">159 nips-2005-Q-Clustering</a></p>
<p>18 0.033323687 <a title="107-tfidf-18" href="./nips-2005-Temporal_Abstraction_in_Temporal-difference_Networks.html">187 nips-2005-Temporal Abstraction in Temporal-difference Networks</a></p>
<p>19 0.032870639 <a title="107-tfidf-19" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>20 0.032350011 <a title="107-tfidf-20" href="./nips-2005-Fusion_of_Similarity_Data_in_Clustering.html">79 nips-2005-Fusion of Similarity Data in Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.115), (1, 0.02), (2, -0.035), (3, -0.029), (4, -0.099), (5, -0.021), (6, -0.03), (7, 0.035), (8, -0.002), (9, 0.016), (10, 0.021), (11, 0.015), (12, -0.051), (13, -0.013), (14, 0.047), (15, -0.031), (16, 0.034), (17, 0.02), (18, 0.013), (19, 0.084), (20, -0.022), (21, 0.017), (22, -0.035), (23, 0.035), (24, 0.028), (25, -0.011), (26, 0.008), (27, 0.036), (28, 0.029), (29, -0.012), (30, -0.063), (31, 0.127), (32, -0.057), (33, 0.017), (34, 0.015), (35, 0.055), (36, -0.029), (37, -0.016), (38, 0.098), (39, 0.051), (40, -0.062), (41, 0.059), (42, 0.076), (43, 0.068), (44, 0.008), (45, -0.043), (46, -0.078), (47, -0.12), (48, -0.095), (49, -0.124)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95799708 <a title="107-lsi-1" href="./nips-2005-Large_scale_networks_fingerprinting_and_visualization_using_the_k-core_decomposition.html">107 nips-2005-Large scale networks fingerprinting and visualization using the k-core decomposition</a></p>
<p>Author: J. I. Alvarez-hamelin, Luca Dall'asta, Alain Barrat, Alessandro Vespignani</p><p>Abstract: We use the k-core decomposition to develop algorithms for the analysis of large scale complex networks. This decomposition, based on a recursive pruning of the least connected vertices, allows to disentangle the hierarchical structure of networks by progressively focusing on their central cores. By using this strategy we develop a general visualization algorithm that can be used to compare the structural properties of various networks and highlight their hierarchical structure. The low computational complexity of the algorithm, O(n + e), where n is the size of the network, and e is the number of edges, makes it suitable for the visualization of very large sparse networks. We show how the proposed visualization tool allows to ﬁnd speciﬁc structural ﬁngerprints of networks. 1</p><p>2 0.66410905 <a title="107-lsi-2" href="./nips-2005-Learning_Topology_with_the_Generative_Gaussian_Graph_and_the_EM_Algorithm.html">116 nips-2005-Learning Topology with the Generative Gaussian Graph and the EM Algorithm</a></p>
<p>Author: Michaël Aupetit</p><p>Abstract: Given a set of points and a set of prototypes representing them, how to create a graph of the prototypes whose topology accounts for that of the points? This problem had not yet been explored in the framework of statistical learning theory. In this work, we propose a generative model based on the Delaunay graph of the prototypes and the ExpectationMaximization algorithm to learn the parameters. This work is a ﬁrst step towards the construction of a topological model of a set of points grounded on statistics. 1 1.1</p><p>3 0.49514982 <a title="107-lsi-3" href="./nips-2005-Soft_Clustering_on_Graphs.html">178 nips-2005-Soft Clustering on Graphs</a></p>
<p>Author: Kai Yu, Shipeng Yu, Volker Tresp</p><p>Abstract: We propose a simple clustering framework on graphs encoding pairwise data similarities. Unlike usual similarity-based methods, the approach softly assigns data to clusters in a probabilistic way. More importantly, a hierarchical clustering is naturally derived in this framework to gradually merge lower-level clusters into higher-level ones. A random walk analysis indicates that the algorithm exposes clustering structures in various resolutions, i.e., a higher level statistically models a longer-term diffusion on graphs and thus discovers a more global clustering structure. Finally we provide very encouraging experimental results. 1</p><p>4 0.45779395 <a title="107-lsi-4" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>Author: Glenn Fung, Rómer Rosales, Balaji Krishnapuram</p><p>Abstract: We propose efﬁcient algorithms for learning ranking functions from order constraints between sets—i.e. classes—of training samples. Our algorithms may be used for maximizing the generalized Wilcoxon Mann Whitney statistic that accounts for the partial ordering of the classes: special cases include maximizing the area under the ROC curve for binary classiﬁcation and its generalization for ordinal regression. Experiments on public benchmarks indicate that: (a) the proposed algorithm is at least as accurate as the current state-of-the-art; (b) computationally, it is several orders of magnitude faster and—unlike current methods—it is easily able to handle even large datasets with over 20,000 samples. 1</p><p>5 0.4261122 <a title="107-lsi-5" href="./nips-2005-Message_passing_for_task_redistribution_on_sparse_graphs.html">125 nips-2005-Message passing for task redistribution on sparse graphs</a></p>
<p>Author: K. Wong, Zhuo Gao, David Tax</p><p>Abstract: The problem of resource allocation in sparse graphs with real variables is studied using methods of statistical physics. An efﬁcient distributed algorithm is devised on the basis of insight gained from the analysis and is examined using numerical simulations, showing excellent performance and full agreement with the theoretical results.</p><p>6 0.38816601 <a title="107-lsi-6" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>7 0.37738404 <a title="107-lsi-7" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>8 0.35618618 <a title="107-lsi-8" href="./nips-2005-Fixing_two_weaknesses_of_the_Spectral_Method.html">75 nips-2005-Fixing two weaknesses of the Spectral Method</a></p>
<p>9 0.35044676 <a title="107-lsi-9" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>10 0.3495523 <a title="107-lsi-10" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>11 0.34871951 <a title="107-lsi-11" href="./nips-2005-Fast_Information_Value_for_Graphical_Models.html">70 nips-2005-Fast Information Value for Graphical Models</a></p>
<p>12 0.34363672 <a title="107-lsi-12" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>13 0.31748134 <a title="107-lsi-13" href="./nips-2005-Is_Early_Vision_Optimized_for_Extracting_Higher-order_Dependencies%3F.html">101 nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</a></p>
<p>14 0.31561905 <a title="107-lsi-14" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>15 0.30513552 <a title="107-lsi-15" href="./nips-2005-Group_and_Topic_Discovery_from_Relations_and_Their_Attributes.html">89 nips-2005-Group and Topic Discovery from Relations and Their Attributes</a></p>
<p>16 0.2830312 <a title="107-lsi-16" href="./nips-2005-Walk-Sum_Interpretation_and_Analysis_of_Gaussian_Belief_Propagation.html">204 nips-2005-Walk-Sum Interpretation and Analysis of Gaussian Belief Propagation</a></p>
<p>17 0.27496266 <a title="107-lsi-17" href="./nips-2005-Q-Clustering.html">159 nips-2005-Q-Clustering</a></p>
<p>18 0.27149352 <a title="107-lsi-18" href="./nips-2005-Products_of_%60%60Edge-perts.html">158 nips-2005-Products of ``Edge-perts</a></p>
<p>19 0.27017587 <a title="107-lsi-19" href="./nips-2005-Estimation_of_Intrinsic_Dimensionality_Using_High-Rate_Vector_Quantization.html">66 nips-2005-Estimation of Intrinsic Dimensionality Using High-Rate Vector Quantization</a></p>
<p>20 0.26641464 <a title="107-lsi-20" href="./nips-2005-Active_Learning_for_Misspecified_Models.html">19 nips-2005-Active Learning for Misspecified Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.036), (10, 0.033), (27, 0.028), (31, 0.033), (34, 0.093), (41, 0.502), (55, 0.039), (69, 0.036), (73, 0.028), (88, 0.045), (91, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8557933 <a title="107-lda-1" href="./nips-2005-Large_scale_networks_fingerprinting_and_visualization_using_the_k-core_decomposition.html">107 nips-2005-Large scale networks fingerprinting and visualization using the k-core decomposition</a></p>
<p>Author: J. I. Alvarez-hamelin, Luca Dall'asta, Alain Barrat, Alessandro Vespignani</p><p>Abstract: We use the k-core decomposition to develop algorithms for the analysis of large scale complex networks. This decomposition, based on a recursive pruning of the least connected vertices, allows to disentangle the hierarchical structure of networks by progressively focusing on their central cores. By using this strategy we develop a general visualization algorithm that can be used to compare the structural properties of various networks and highlight their hierarchical structure. The low computational complexity of the algorithm, O(n + e), where n is the size of the network, and e is the number of edges, makes it suitable for the visualization of very large sparse networks. We show how the proposed visualization tool allows to ﬁnd speciﬁc structural ﬁngerprints of networks. 1</p><p>2 0.82374966 <a title="107-lda-2" href="./nips-2005-Kernels_for_gene_regulatory_regions.html">103 nips-2005-Kernels for gene regulatory regions</a></p>
<p>Author: Jean-philippe Vert, Robert Thurman, William S. Noble</p><p>Abstract: We describe a hierarchy of motif-based kernels for multiple alignments of biological sequences, particularly suitable to process regulatory regions of genes. The kernels incorporate progressively more information, with the most complex kernel accounting for a multiple alignment of orthologous regions, the phylogenetic tree relating the species, and the prior knowledge that relevant sequence patterns occur in conserved motif blocks. These kernels can be used in the presence of a library of known transcription factor binding sites, or de novo by iterating over all k-mers of a given length. In the latter mode, a discriminative classiﬁer built from such a kernel not only recognizes a given class of promoter regions, but as a side effect simultaneously identiﬁes a collection of relevant, discriminative sequence motifs. We demonstrate the utility of the motif-based multiple alignment kernels by using a collection of aligned promoter regions from ﬁve yeast species to recognize classes of cell-cycle regulated genes. Supplementary data is available at http://noble.gs.washington.edu/proj/pkernel. 1</p><p>3 0.6914643 <a title="107-lda-3" href="./nips-2005-Fusion_of_Similarity_Data_in_Clustering.html">79 nips-2005-Fusion of Similarity Data in Clustering</a></p>
<p>Author: Tilman Lange, Joachim M. Buhmann</p><p>Abstract: Fusing multiple information sources can yield signiﬁcant beneﬁts to successfully accomplish learning tasks. Many studies have focussed on fusing information in supervised learning contexts. We present an approach to utilize multiple information sources in the form of similarity data for unsupervised learning. Based on similarity information, the clustering task is phrased as a non-negative matrix factorization problem of a mixture of similarity measurements. The tradeoff between the informativeness of data sources and the sparseness of their mixture is controlled by an entropy-based weighting mechanism. For the purpose of model selection, a stability-based approach is employed to ensure the selection of the most self-consistent hypothesis. The experiments demonstrate the performance of the method on toy as well as real world data sets. 1</p><p>4 0.35480091 <a title="107-lda-4" href="./nips-2005-Soft_Clustering_on_Graphs.html">178 nips-2005-Soft Clustering on Graphs</a></p>
<p>Author: Kai Yu, Shipeng Yu, Volker Tresp</p><p>Abstract: We propose a simple clustering framework on graphs encoding pairwise data similarities. Unlike usual similarity-based methods, the approach softly assigns data to clusters in a probabilistic way. More importantly, a hierarchical clustering is naturally derived in this framework to gradually merge lower-level clusters into higher-level ones. A random walk analysis indicates that the algorithm exposes clustering structures in various resolutions, i.e., a higher level statistically models a longer-term diffusion on graphs and thus discovers a more global clustering structure. Finally we provide very encouraging experimental results. 1</p><p>5 0.32851344 <a title="107-lda-5" href="./nips-2005-Scaling_Laws_in_Natural_Scenes_and_the_Inference_of_3D_Shape.html">170 nips-2005-Scaling Laws in Natural Scenes and the Inference of 3D Shape</a></p>
<p>Author: Tai-sing Lee, Brian R. Potetz</p><p>Abstract: This paper explores the statistical relationship between natural images and their underlying range (depth) images. We look at how this relationship changes over scale, and how this information can be used to enhance low resolution range data using a full resolution intensity image. Based on our ﬁndings, we propose an extension to an existing technique known as shape recipes [3], and the success of the two methods are compared using images and laser scans of real scenes. Our extension is shown to provide a two-fold improvement over the current method. Furthermore, we demonstrate that ideal linear shape-from-shading ﬁlters, when learned from natural scenes, may derive even more strength from shadow cues than from the traditional linear-Lambertian shading cues. 1</p><p>6 0.32565171 <a title="107-lda-6" href="./nips-2005-Using_%60%60epitomes%27%27_to_model_genetic_diversity%3A_Rational_design_of_HIV_vaccine_cocktails.html">198 nips-2005-Using ``epitomes'' to model genetic diversity: Rational design of HIV vaccine cocktails</a></p>
<p>7 0.31159025 <a title="107-lda-7" href="./nips-2005-Generalized_Nonnegative_Matrix_Approximations_with_Bregman_Divergences.html">86 nips-2005-Generalized Nonnegative Matrix Approximations with Bregman Divergences</a></p>
<p>8 0.3102589 <a title="107-lda-8" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>9 0.30782095 <a title="107-lda-9" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>10 0.29452729 <a title="107-lda-10" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>11 0.2944715 <a title="107-lda-11" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>12 0.29443899 <a title="107-lda-12" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<p>13 0.2918672 <a title="107-lda-13" href="./nips-2005-An_Application_of_Markov_Random_Fields_to_Range_Sensing.html">23 nips-2005-An Application of Markov Random Fields to Range Sensing</a></p>
<p>14 0.28858998 <a title="107-lda-14" href="./nips-2005-Mixture_Modeling_by_Affinity_Propagation.html">127 nips-2005-Mixture Modeling by Affinity Propagation</a></p>
<p>15 0.28823328 <a title="107-lda-15" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>16 0.28685367 <a title="107-lda-16" href="./nips-2005-Dynamic_Social_Network_Analysis_using_Latent_Space_Models.html">60 nips-2005-Dynamic Social Network Analysis using Latent Space Models</a></p>
<p>17 0.28131565 <a title="107-lda-17" href="./nips-2005-Sequence_and_Tree_Kernels_with_Statistical_Feature_Mining.html">175 nips-2005-Sequence and Tree Kernels with Statistical Feature Mining</a></p>
<p>18 0.28094119 <a title="107-lda-18" href="./nips-2005-Correcting_sample_selection_bias_in_maximum_entropy_density_estimation.html">51 nips-2005-Correcting sample selection bias in maximum entropy density estimation</a></p>
<p>19 0.27979037 <a title="107-lda-19" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>20 0.27925903 <a title="107-lda-20" href="./nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
