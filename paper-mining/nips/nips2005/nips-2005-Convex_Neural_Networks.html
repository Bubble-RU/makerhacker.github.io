<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>50 nips-2005-Convex Neural Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-50" href="#">nips2005-50</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>50 nips-2005-Convex Neural Networks</h1>
<br/><p>Source: <a title="nips-2005-50-pdf" href="http://papers.nips.cc/paper/2800-convex-neural-networks.pdf">pdf</a></p><p>Author: Yoshua Bengio, Nicolas L. Roux, Pascal Vincent, Olivier Delalleau, Patrice Marcotte</p><p>Abstract: Convexity has recently received a lot of attention in the machine learning community, and the lack of convexity has been seen as a major disadvantage of many learning algorithms, such as multi-layer artiﬁcial neural networks. We show that training multi-layer neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem. This problem involves an inﬁnite number of variables, but can be solved by incrementally inserting a hidden unit at a time, each time ﬁnding a linear classiﬁer that minimizes a weighted sum of errors. 1</p><p>Reference: <a title="nips-2005-50-reference" href="../nips2005_reference/nips-2005-Convex_Neural_Networks_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('convex', 0.301), ('hid', 0.294), ('xt', 0.286), ('qt', 0.264), ('nn', 0.216), ('hj', 0.204), ('hi', 0.203), ('iz', 0.182), ('yt', 0.17), ('qtj', 0.164), ('boost', 0.147), ('weight', 0.145), ('output', 0.14), ('vj', 0.138), ('hing', 0.124), ('wj', 0.124), ('unit', 0.124), ('loss', 0.123), ('nns', 0.109), ('grady', 0.106)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="50-tfidf-1" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>2 0.21380658 <a title="50-tfidf-2" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>3 0.18002364 <a title="50-tfidf-3" href="./nips-2005-Variational_EM_Algorithms_for_Non-Gaussian_Latent_Variable_Models.html">202 nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</a></p>
<p>4 0.17794365 <a title="50-tfidf-4" href="./nips-2005-Beyond_Gaussian_Processes%3A_On_the_Distributions_of_Infinite_Networks.html">38 nips-2005-Beyond Gaussian Processes: On the Distributions of Infinite Networks</a></p>
<p>5 0.17671877 <a title="50-tfidf-5" href="./nips-2005-Dynamic_Social_Network_Analysis_using_Latent_Space_Models.html">60 nips-2005-Dynamic Social Network Analysis using Latent Space Models</a></p>
<p>6 0.15273933 <a title="50-tfidf-6" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>7 0.14067613 <a title="50-tfidf-7" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>8 0.13676603 <a title="50-tfidf-8" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>9 0.13106023 <a title="50-tfidf-9" href="./nips-2005-Online_Discovery_and_Learning_of_Predictive_State_Representations.html">148 nips-2005-Online Discovery and Learning of Predictive State Representations</a></p>
<p>10 0.1279735 <a title="50-tfidf-10" href="./nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">113 nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<p>11 0.12696703 <a title="50-tfidf-11" href="./nips-2005-Conditional_Visual_Tracking_in_Kernel_Space.html">45 nips-2005-Conditional Visual Tracking in Kernel Space</a></p>
<p>12 0.12471863 <a title="50-tfidf-12" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>13 0.12247289 <a title="50-tfidf-13" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>14 0.12225861 <a title="50-tfidf-14" href="./nips-2005-Improved_risk_tail_bounds_for_on-line_algorithms.html">95 nips-2005-Improved risk tail bounds for on-line algorithms</a></p>
<p>15 0.12088999 <a title="50-tfidf-15" href="./nips-2005-Gaussian_Process_Dynamical_Models.html">80 nips-2005-Gaussian Process Dynamical Models</a></p>
<p>16 0.12072588 <a title="50-tfidf-16" href="./nips-2005-Rodeo%3A_Sparse_Nonparametric_Regression_in_High_Dimensions.html">168 nips-2005-Rodeo: Sparse Nonparametric Regression in High Dimensions</a></p>
<p>17 0.1194564 <a title="50-tfidf-17" href="./nips-2005-Augmented_Rescorla-Wagner_and_Maximum_Likelihood_Estimation.html">32 nips-2005-Augmented Rescorla-Wagner and Maximum Likelihood Estimation</a></p>
<p>18 0.11862153 <a title="50-tfidf-18" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>19 0.11671779 <a title="50-tfidf-19" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>20 0.11597455 <a title="50-tfidf-20" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.327), (1, -0.083), (2, -0.057), (3, 0.142), (4, -0.184), (5, -0.06), (6, -0.096), (7, -0.004), (8, 0.077), (9, -0.126), (10, -0.043), (11, -0.189), (12, 0.141), (13, -0.227), (14, -0.072), (15, -0.157), (16, -0.053), (17, 0.102), (18, 0.017), (19, 0.054), (20, -0.231), (21, -0.01), (22, 0.045), (23, 0.136), (24, 0.002), (25, -0.06), (26, 0.0), (27, 0.092), (28, -0.076), (29, 0.057), (30, -0.016), (31, -0.023), (32, 0.022), (33, -0.062), (34, -0.084), (35, -0.118), (36, 0.042), (37, 0.042), (38, 0.081), (39, 0.044), (40, -0.011), (41, 0.131), (42, 0.017), (43, 0.018), (44, -0.075), (45, -0.029), (46, 0.03), (47, -0.013), (48, -0.016), (49, -0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94358194 <a title="50-lsi-1" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>2 0.63335735 <a title="50-lsi-2" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>3 0.58985233 <a title="50-lsi-3" href="./nips-2005-Augmented_Rescorla-Wagner_and_Maximum_Likelihood_Estimation.html">32 nips-2005-Augmented Rescorla-Wagner and Maximum Likelihood Estimation</a></p>
<p>4 0.5625428 <a title="50-lsi-4" href="./nips-2005-Dynamic_Social_Network_Analysis_using_Latent_Space_Models.html">60 nips-2005-Dynamic Social Network Analysis using Latent Space Models</a></p>
<p>5 0.4934575 <a title="50-lsi-5" href="./nips-2005-Rodeo%3A_Sparse_Nonparametric_Regression_in_High_Dimensions.html">168 nips-2005-Rodeo: Sparse Nonparametric Regression in High Dimensions</a></p>
<p>6 0.49159265 <a title="50-lsi-6" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>7 0.48251113 <a title="50-lsi-7" href="./nips-2005-Beyond_Gaussian_Processes%3A_On_the_Distributions_of_Infinite_Networks.html">38 nips-2005-Beyond Gaussian Processes: On the Distributions of Infinite Networks</a></p>
<p>8 0.46938175 <a title="50-lsi-8" href="./nips-2005-Gaussian_Process_Dynamical_Models.html">80 nips-2005-Gaussian Process Dynamical Models</a></p>
<p>9 0.46335924 <a title="50-lsi-9" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>10 0.46292004 <a title="50-lsi-10" href="./nips-2005-Online_Discovery_and_Learning_of_Predictive_State_Representations.html">148 nips-2005-Online Discovery and Learning of Predictive State Representations</a></p>
<p>11 0.45697257 <a title="50-lsi-11" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>12 0.4438709 <a title="50-lsi-12" href="./nips-2005-Variational_EM_Algorithms_for_Non-Gaussian_Latent_Variable_Models.html">202 nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</a></p>
<p>13 0.42705539 <a title="50-lsi-13" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>14 0.42507166 <a title="50-lsi-14" href="./nips-2005-Fusion_of_Similarity_Data_in_Clustering.html">79 nips-2005-Fusion of Similarity Data in Clustering</a></p>
<p>15 0.42372981 <a title="50-lsi-15" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>16 0.42271426 <a title="50-lsi-16" href="./nips-2005-Improved_risk_tail_bounds_for_on-line_algorithms.html">95 nips-2005-Improved risk tail bounds for on-line algorithms</a></p>
<p>17 0.42177936 <a title="50-lsi-17" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>18 0.41229734 <a title="50-lsi-18" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>19 0.40554732 <a title="50-lsi-19" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>20 0.4054575 <a title="50-lsi-20" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.044), (12, 0.013), (17, 0.011), (53, 0.01), (54, 0.445), (71, 0.12), (88, 0.071), (89, 0.044), (92, 0.149)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85923505 <a title="50-lda-1" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>2 0.84656441 <a title="50-lda-2" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>3 0.64808244 <a title="50-lda-3" href="./nips-2005-Improved_risk_tail_bounds_for_on-line_algorithms.html">95 nips-2005-Improved risk tail bounds for on-line algorithms</a></p>
<p>4 0.63771462 <a title="50-lda-4" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>5 0.63685149 <a title="50-lda-5" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>6 0.62374091 <a title="50-lda-6" href="./nips-2005-Sensory_Adaptation_within_a_Bayesian_Framework_for_Perception.html">173 nips-2005-Sensory Adaptation within a Bayesian Framework for Perception</a></p>
<p>7 0.57451057 <a title="50-lda-7" href="./nips-2005-Worst-Case_Bounds_for_Gaussian_Process_Models.html">205 nips-2005-Worst-Case Bounds for Gaussian Process Models</a></p>
<p>8 0.57009506 <a title="50-lda-8" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>9 0.56947058 <a title="50-lda-9" href="./nips-2005-Learning_Minimum_Volume_Sets.html">112 nips-2005-Learning Minimum Volume Sets</a></p>
<p>10 0.56436133 <a title="50-lda-10" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>11 0.56252748 <a title="50-lda-11" href="./nips-2005-Beyond_Gaussian_Processes%3A_On_the_Distributions_of_Infinite_Networks.html">38 nips-2005-Beyond Gaussian Processes: On the Distributions of Infinite Networks</a></p>
<p>12 0.5621255 <a title="50-lda-12" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>13 0.56038177 <a title="50-lda-13" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>14 0.56006974 <a title="50-lda-14" href="./nips-2005-Rodeo%3A_Sparse_Nonparametric_Regression_in_High_Dimensions.html">168 nips-2005-Rodeo: Sparse Nonparametric Regression in High Dimensions</a></p>
<p>15 0.55422592 <a title="50-lda-15" href="./nips-2005-Asymptotics_of_Gaussian_Regularized_Least_Squares.html">31 nips-2005-Asymptotics of Gaussian Regularized Least Squares</a></p>
<p>16 0.55414689 <a title="50-lda-16" href="./nips-2005-Computing_the_Solution_Path_for_the_Regularized_Support_Vector_Regression.html">44 nips-2005-Computing the Solution Path for the Regularized Support Vector Regression</a></p>
<p>17 0.55395305 <a title="50-lda-17" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>18 0.550017 <a title="50-lda-18" href="./nips-2005-TD%280%29_Leads_to_Better_Policies_than_Approximate_Value_Iteration.html">186 nips-2005-TD(0) Leads to Better Policies than Approximate Value Iteration</a></p>
<p>19 0.5483309 <a title="50-lda-19" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>20 0.54759467 <a title="50-lda-20" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
