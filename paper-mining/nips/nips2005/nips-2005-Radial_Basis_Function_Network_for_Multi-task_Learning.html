<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>161 nips-2005-Radial Basis Function Network for Multi-task Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-161" href="#">nips2005-161</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>161 nips-2005-Radial Basis Function Network for Multi-task Learning</h1>
<br/><p>Source: <a title="nips-2005-161-pdf" href="http://papers.nips.cc/paper/2907-radial-basis-function-network-for-multi-task-learning.pdf">pdf</a></p><p>Author: Xuejun Liao, Lawrence Carin</p><p>Abstract: We extend radial basis function (RBF) networks to the scenario in which multiple correlated tasks are learned simultaneously, and present the corresponding learning algorithms. We develop the algorithms for learning the network structure, in either a supervised or unsupervised manner. Training data may also be actively selected to improve the network’s generalization to test data. Experimental results based on real data demonstrate the advantage of the proposed algorithms and support our conclusions. 1</p><p>Reference: <a title="nips-2005-161-reference" href="../nips2005_reference/nips-2005-Radial_Basis_Function_Network_for_Multi-task_Learning_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('yik', 0.449), ('jk', 0.346), ('ik', 0.297), ('xjk', 0.293), ('fk', 0.284), ('xik', 0.271), ('rbf', 0.263), ('dk', 0.22), ('qk', 0.198), ('wk', 0.173), ('det', 0.152), ('yk', 0.135), ('ak', 0.126), ('yjk', 0.119), ('anew', 0.1), ('network', 0.076), ('ck', 0.075), ('nm', 0.073), ('task', 0.07), ('nod', 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="161-tfidf-1" href="./nips-2005-Radial_Basis_Function_Network_for_Multi-task_Learning.html">161 nips-2005-Radial Basis Function Network for Multi-task Learning</a></p>
<p>Author: Xuejun Liao, Lawrence Carin</p><p>Abstract: We extend radial basis function (RBF) networks to the scenario in which multiple correlated tasks are learned simultaneously, and present the corresponding learning algorithms. We develop the algorithms for learning the network structure, in either a supervised or unsupervised manner. Training data may also be actively selected to improve the network’s generalization to test data. Experimental results based on real data demonstrate the advantage of the proposed algorithms and support our conclusions. 1</p><p>2 0.16966936 <a title="161-tfidf-2" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>Author: Ben Taskar, Simon Lacoste-Julian, Michael I. Jordan</p><p>Abstract: We present a simple and scalable algorithm for large-margin estimation of structured models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem and apply the extragradient method, yielding an algorithm with linear convergence using simple gradient and projection calculations. The projection step can be solved using combinatorial algorithms for min-cost quadratic ﬂow. This makes the approach an efﬁcient alternative to formulations based on reductions to a quadratic program (QP). We present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm. 1</p><p>3 0.14652987 <a title="161-tfidf-3" href="./nips-2005-Estimation_of_Intrinsic_Dimensionality_Using_High-Rate_Vector_Quantization.html">66 nips-2005-Estimation of Intrinsic Dimensionality Using High-Rate Vector Quantization</a></p>
<p>Author: Maxim Raginsky, Svetlana Lazebnik</p><p>Abstract: We introduce a technique for dimensionality estimation based on the notion of quantization dimension, which connects the asymptotic optimal quantization error for a probability distribution on a manifold to its intrinsic dimension. The deﬁnition of quantization dimension yields a family of estimation algorithms, whose limiting case is equivalent to a recent method based on packing numbers. Using the formalism of high-rate vector quantization, we address issues of statistical consistency and analyze the behavior of our scheme in the presence of noise.</p><p>4 0.090351701 <a title="161-tfidf-4" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>Author: Frank Wood, Stefan Roth, Michael J. Black</p><p>Abstract: Probabilistic modeling of correlated neural population ﬁring activity is central to understanding the neural code and building practical decoding algorithms. No parametric models currently exist for modeling multivariate correlated neural data and the high dimensional nature of the data makes fully non-parametric methods impractical. To address these problems we propose an energy-based model in which the joint probability of neural activity is represented using learned functions of the 1D marginal histograms of the data. The parameters of the model are learned using contrastive divergence and an optimization procedure for ﬁnding appropriate marginal directions. We evaluate the method using real data recorded from a population of motor cortical neurons. In particular, we model the joint probability of population spiking times and 2D hand position and show that the likelihood of test data under our model is signiﬁcantly higher than under other models. These results suggest that our model captures correlations in the ﬁring activity. Our rich probabilistic model of neural population activity is a step towards both measurement of the importance of correlations in neural coding and improved decoding of population activity. 1</p><p>5 0.089517459 <a title="161-tfidf-5" href="./nips-2005-Transfer_learning_for_text_classification.html">195 nips-2005-Transfer learning for text classification</a></p>
<p>Author: Chuong B. Do, Andrew Y. Ng</p><p>Abstract: Linear text classiﬁcation algorithms work by computing an inner product between a test document vector and a parameter vector. In many such algorithms, including naive Bayes and most TFIDF variants, the parameters are determined by some simple, closed-form, function of training set statistics; we call this mapping mapping from statistics to parameters, the parameter function. Much research in text classiﬁcation over the last few decades has consisted of manual efforts to identify better parameter functions. In this paper, we propose an algorithm for automatically learning this function from related classiﬁcation problems. The parameter function found by our algorithm then deﬁnes a new learning algorithm for text classiﬁcation, which we can apply to novel classiﬁcation tasks. We ﬁnd that our learned classiﬁer outperforms existing methods on a variety of multiclass text classiﬁcation tasks. 1</p><p>6 0.080827393 <a title="161-tfidf-6" href="./nips-2005-Response_Analysis_of_Neuronal_Population_with_Synaptic_Depression.html">165 nips-2005-Response Analysis of Neuronal Population with Synaptic Depression</a></p>
<p>7 0.057163656 <a title="161-tfidf-7" href="./nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">113 nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<p>8 0.055188403 <a title="161-tfidf-8" href="./nips-2005-Variational_Bayesian_Stochastic_Complexity_of_Mixture_Models.html">201 nips-2005-Variational Bayesian Stochastic Complexity of Mixture Models</a></p>
<p>9 0.054709654 <a title="161-tfidf-9" href="./nips-2005-Gaussian_Process_Dynamical_Models.html">80 nips-2005-Gaussian Process Dynamical Models</a></p>
<p>10 0.049627133 <a title="161-tfidf-10" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>11 0.047937743 <a title="161-tfidf-11" href="./nips-2005-Interpolating_between_types_and_tokens_by_estimating_power-law_generators.html">100 nips-2005-Interpolating between types and tokens by estimating power-law generators</a></p>
<p>12 0.045528494 <a title="161-tfidf-12" href="./nips-2005-A_Criterion_for_the_Convergence_of_Learning_with_Spike_Timing_Dependent_Plasticity.html">8 nips-2005-A Criterion for the Convergence of Learning with Spike Timing Dependent Plasticity</a></p>
<p>13 0.042334132 <a title="161-tfidf-13" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>14 0.040609375 <a title="161-tfidf-14" href="./nips-2005-Fast_Gaussian_Process_Regression_using_KD-Trees.html">69 nips-2005-Fast Gaussian Process Regression using KD-Trees</a></p>
<p>15 0.040158007 <a title="161-tfidf-15" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>16 0.040018026 <a title="161-tfidf-16" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>17 0.039737482 <a title="161-tfidf-17" href="./nips-2005-A_Connectionist_Model_for_Constructive_Modal_Reasoning.html">6 nips-2005-A Connectionist Model for Constructive Modal Reasoning</a></p>
<p>18 0.036914147 <a title="161-tfidf-18" href="./nips-2005-An_Approximate_Inference_Approach_for_the_PCA_Reconstruction_Error.html">24 nips-2005-An Approximate Inference Approach for the PCA Reconstruction Error</a></p>
<p>19 0.036035851 <a title="161-tfidf-19" href="./nips-2005-Message_passing_for_task_redistribution_on_sparse_graphs.html">125 nips-2005-Message passing for task redistribution on sparse graphs</a></p>
<p>20 0.035622351 <a title="161-tfidf-20" href="./nips-2005-Representing_Part-Whole_Relationships_in_Recurrent_Neural_Networks.html">164 nips-2005-Representing Part-Whole Relationships in Recurrent Neural Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.115), (1, -0.016), (2, -0.013), (3, 0.017), (4, -0.028), (5, 0.053), (6, 0.035), (7, -0.018), (8, -0.038), (9, 0.014), (10, -0.018), (11, 0.051), (12, -0.029), (13, -0.072), (14, -0.021), (15, 0.052), (16, -0.005), (17, -0.034), (18, 0.027), (19, 0.08), (20, 0.004), (21, 0.075), (22, 0.1), (23, 0.135), (24, 0.166), (25, -0.083), (26, 0.068), (27, 0.023), (28, 0.168), (29, -0.025), (30, 0.156), (31, -0.223), (32, 0.156), (33, 0.273), (34, 0.065), (35, -0.048), (36, 0.146), (37, 0.069), (38, 0.121), (39, 0.023), (40, -0.222), (41, 0.011), (42, 0.011), (43, 0.193), (44, -0.151), (45, -0.025), (46, 0.04), (47, 0.057), (48, -0.104), (49, -0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94765019 <a title="161-lsi-1" href="./nips-2005-Radial_Basis_Function_Network_for_Multi-task_Learning.html">161 nips-2005-Radial Basis Function Network for Multi-task Learning</a></p>
<p>Author: Xuejun Liao, Lawrence Carin</p><p>Abstract: We extend radial basis function (RBF) networks to the scenario in which multiple correlated tasks are learned simultaneously, and present the corresponding learning algorithms. We develop the algorithms for learning the network structure, in either a supervised or unsupervised manner. Training data may also be actively selected to improve the network’s generalization to test data. Experimental results based on real data demonstrate the advantage of the proposed algorithms and support our conclusions. 1</p><p>2 0.58193642 <a title="161-lsi-2" href="./nips-2005-Response_Analysis_of_Neuronal_Population_with_Synaptic_Depression.html">165 nips-2005-Response Analysis of Neuronal Population with Synaptic Depression</a></p>
<p>Author: Wentao Huang, Licheng Jiao, Shan Tan, Maoguo Gong</p><p>Abstract: In this paper, we aim at analyzing the characteristic of neuronal population responses to instantaneous or time-dependent inputs and the role of synapses in neural information processing. We have derived an evolution equation of the membrane potential density function with synaptic depression, and obtain the formulas for analytic computing the response of instantaneous re rate. Through a technical analysis, we arrive at several signi cant conclusions: The background inputs play an important role in information processing and act as a switch betwee temporal integration and coincidence detection. the role of synapses can be regarded as a spatio-temporal lter; it is important in neural information processing for the spatial distribution of synapses and the spatial and temporal relation of inputs. The instantaneous input frequency can affect the response amplitude and phase delay. 1</p><p>3 0.53551531 <a title="161-lsi-3" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>Author: Ben Taskar, Simon Lacoste-Julian, Michael I. Jordan</p><p>Abstract: We present a simple and scalable algorithm for large-margin estimation of structured models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem and apply the extragradient method, yielding an algorithm with linear convergence using simple gradient and projection calculations. The projection step can be solved using combinatorial algorithms for min-cost quadratic ﬂow. This makes the approach an efﬁcient alternative to formulations based on reductions to a quadratic program (QP). We present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm. 1</p><p>4 0.48124155 <a title="161-lsi-4" href="./nips-2005-Estimation_of_Intrinsic_Dimensionality_Using_High-Rate_Vector_Quantization.html">66 nips-2005-Estimation of Intrinsic Dimensionality Using High-Rate Vector Quantization</a></p>
<p>Author: Maxim Raginsky, Svetlana Lazebnik</p><p>Abstract: We introduce a technique for dimensionality estimation based on the notion of quantization dimension, which connects the asymptotic optimal quantization error for a probability distribution on a manifold to its intrinsic dimension. The deﬁnition of quantization dimension yields a family of estimation algorithms, whose limiting case is equivalent to a recent method based on packing numbers. Using the formalism of high-rate vector quantization, we address issues of statistical consistency and analyze the behavior of our scheme in the presence of noise.</p><p>5 0.3340483 <a title="161-lsi-5" href="./nips-2005-Transfer_learning_for_text_classification.html">195 nips-2005-Transfer learning for text classification</a></p>
<p>Author: Chuong B. Do, Andrew Y. Ng</p><p>Abstract: Linear text classiﬁcation algorithms work by computing an inner product between a test document vector and a parameter vector. In many such algorithms, including naive Bayes and most TFIDF variants, the parameters are determined by some simple, closed-form, function of training set statistics; we call this mapping mapping from statistics to parameters, the parameter function. Much research in text classiﬁcation over the last few decades has consisted of manual efforts to identify better parameter functions. In this paper, we propose an algorithm for automatically learning this function from related classiﬁcation problems. The parameter function found by our algorithm then deﬁnes a new learning algorithm for text classiﬁcation, which we can apply to novel classiﬁcation tasks. We ﬁnd that our learned classiﬁer outperforms existing methods on a variety of multiclass text classiﬁcation tasks. 1</p><p>6 0.30128288 <a title="161-lsi-6" href="./nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">113 nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<p>7 0.28393728 <a title="161-lsi-7" href="./nips-2005-Active_Learning_for_Misspecified_Models.html">19 nips-2005-Active Learning for Misspecified Models</a></p>
<p>8 0.28261337 <a title="161-lsi-8" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>9 0.27934733 <a title="161-lsi-9" href="./nips-2005-Benchmarking_Non-Parametric_Statistical_Tests.html">37 nips-2005-Benchmarking Non-Parametric Statistical Tests</a></p>
<p>10 0.27560306 <a title="161-lsi-10" href="./nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<p>11 0.27441064 <a title="161-lsi-11" href="./nips-2005-Layered_Dynamic_Textures.html">108 nips-2005-Layered Dynamic Textures</a></p>
<p>12 0.26219314 <a title="161-lsi-12" href="./nips-2005-Learning_from_Data_of_Variable_Quality.html">117 nips-2005-Learning from Data of Variable Quality</a></p>
<p>13 0.24259804 <a title="161-lsi-13" href="./nips-2005-An_Approximate_Inference_Approach_for_the_PCA_Reconstruction_Error.html">24 nips-2005-An Approximate Inference Approach for the PCA Reconstruction Error</a></p>
<p>14 0.22913857 <a title="161-lsi-14" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>15 0.22773662 <a title="161-lsi-15" href="./nips-2005-Interpolating_between_types_and_tokens_by_estimating_power-law_generators.html">100 nips-2005-Interpolating between types and tokens by estimating power-law generators</a></p>
<p>16 0.20198397 <a title="161-lsi-16" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>17 0.19455649 <a title="161-lsi-17" href="./nips-2005-Online_Discovery_and_Learning_of_Predictive_State_Representations.html">148 nips-2005-Online Discovery and Learning of Predictive State Representations</a></p>
<p>18 0.19441584 <a title="161-lsi-18" href="./nips-2005-Location-based_activity_recognition.html">121 nips-2005-Location-based activity recognition</a></p>
<p>19 0.19437538 <a title="161-lsi-19" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>20 0.19146454 <a title="161-lsi-20" href="./nips-2005-Maximum_Margin_Semi-Supervised_Learning_for_Structured_Variables.html">123 nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.053), (9, 0.012), (12, 0.013), (18, 0.018), (29, 0.477), (54, 0.023), (71, 0.066), (88, 0.077), (89, 0.034), (92, 0.086)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69787645 <a title="161-lda-1" href="./nips-2005-Radial_Basis_Function_Network_for_Multi-task_Learning.html">161 nips-2005-Radial Basis Function Network for Multi-task Learning</a></p>
<p>Author: Xuejun Liao, Lawrence Carin</p><p>Abstract: We extend radial basis function (RBF) networks to the scenario in which multiple correlated tasks are learned simultaneously, and present the corresponding learning algorithms. We develop the algorithms for learning the network structure, in either a supervised or unsupervised manner. Training data may also be actively selected to improve the network’s generalization to test data. Experimental results based on real data demonstrate the advantage of the proposed algorithms and support our conclusions. 1</p><p>2 0.56094074 <a title="161-lda-2" href="./nips-2005-Spiking_Inputs_to_a_Winner-take-all_Network.html">181 nips-2005-Spiking Inputs to a Winner-take-all Network</a></p>
<p>Author: Matthias Oster, Shih-Chii Liu</p><p>Abstract: Recurrent networks that perform a winner-take-all computation have been studied extensively. Although some of these studies include spiking networks, they consider only analog input rates. We present results of this winner-take-all computation on a network of integrate-and-ﬁre neurons which receives spike trains as inputs. We show how we can conﬁgure the connectivity in the network so that the winner is selected after a pre-determined number of input spikes. We discuss spiking inputs with both regular frequencies and Poisson-distributed rates. The robustness of the computation was tested by implementing the winner-take-all network on an analog VLSI array of 64 integrate-and-ﬁre neurons which have an innate variance in their operating parameters. 1</p><p>3 0.2967566 <a title="161-lda-3" href="./nips-2005-AER_Building_Blocks_for_Multi-Layer_Multi-Chip_Neuromorphic_Vision_Systems.html">1 nips-2005-AER Building Blocks for Multi-Layer Multi-Chip Neuromorphic Vision Systems</a></p>
<p>Author: R. Serrano-Gotarredona, M. Oster, P. Lichtsteiner, A. Linares-Barranco, R. Paz-Vicente, F. Gomez-Rodriguez, H. Kolle Riis, T. Delbruck, S. C. Liu, S. Zahnd, A. M. Whatley, R. Douglas, P. Hafliger, G. Jimenez-Moreno, A. Civit, T. Serrano-Gotarredona, A. Acosta-Jimenez, B. Linares-Barranco</p><p>Abstract: A 5-layer neuromorphic vision processor whose components communicate spike events asychronously using the address-eventrepresentation (AER) is demonstrated. The system includes a retina chip, two convolution chips, a 2D winner-take-all chip, a delay line chip, a learning classiﬁer chip, and a set of PCBs for computer interfacing and address space remappings. The components use a mixture of analog and digital computation and will learn to classify trajectories of a moving object. A complete experimental setup and measurements results are shown.</p><p>4 0.28145257 <a title="161-lda-4" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>Author: Glenn Fung, Rómer Rosales, Balaji Krishnapuram</p><p>Abstract: We propose efﬁcient algorithms for learning ranking functions from order constraints between sets—i.e. classes—of training samples. Our algorithms may be used for maximizing the generalized Wilcoxon Mann Whitney statistic that accounts for the partial ordering of the classes: special cases include maximizing the area under the ROC curve for binary classiﬁcation and its generalization for ordinal regression. Experiments on public benchmarks indicate that: (a) the proposed algorithm is at least as accurate as the current state-of-the-art; (b) computationally, it is several orders of magnitude faster and—unlike current methods—it is easily able to handle even large datasets with over 20,000 samples. 1</p><p>5 0.28072312 <a title="161-lda-5" href="./nips-2005-Walk-Sum_Interpretation_and_Analysis_of_Gaussian_Belief_Propagation.html">204 nips-2005-Walk-Sum Interpretation and Analysis of Gaussian Belief Propagation</a></p>
<p>Author: Dmitry Malioutov, Alan S. Willsky, Jason K. Johnson</p><p>Abstract: This paper presents a new framework based on walks in a graph for analysis and inference in Gaussian graphical models. The key idea is to decompose correlations between variables as a sum over all walks between those variables in the graph. The weight of each walk is given by a product of edgewise partial correlations. We provide a walk-sum interpretation of Gaussian belief propagation in trees and of the approximate method of loopy belief propagation in graphs with cycles. This perspective leads to a better understanding of Gaussian belief propagation and of its convergence in loopy graphs. 1</p><p>6 0.28061321 <a title="161-lda-6" href="./nips-2005-Principles_of_real-time_computing_with_feedback_applied_to_cortical_microcircuit_models.html">157 nips-2005-Principles of real-time computing with feedback applied to cortical microcircuit models</a></p>
<p>7 0.28047884 <a title="161-lda-7" href="./nips-2005-Fast_Krylov_Methods_for_N-Body_Learning.html">71 nips-2005-Fast Krylov Methods for N-Body Learning</a></p>
<p>8 0.28002128 <a title="161-lda-8" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>9 0.27991831 <a title="161-lda-9" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>10 0.27961963 <a title="161-lda-10" href="./nips-2005-Message_passing_for_task_redistribution_on_sparse_graphs.html">125 nips-2005-Message passing for task redistribution on sparse graphs</a></p>
<p>11 0.27942097 <a title="161-lda-11" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>12 0.27853608 <a title="161-lda-12" href="./nips-2005-Dynamic_Social_Network_Analysis_using_Latent_Space_Models.html">60 nips-2005-Dynamic Social Network Analysis using Latent Space Models</a></p>
<p>13 0.27811354 <a title="161-lda-13" href="./nips-2005-Noise_and_the_two-thirds_power_Law.html">136 nips-2005-Noise and the two-thirds power Law</a></p>
<p>14 0.27774525 <a title="161-lda-14" href="./nips-2005-A_Connectionist_Model_for_Constructive_Modal_Reasoning.html">6 nips-2005-A Connectionist Model for Constructive Modal Reasoning</a></p>
<p>15 0.27755898 <a title="161-lda-15" href="./nips-2005-Fast_Information_Value_for_Graphical_Models.html">70 nips-2005-Fast Information Value for Graphical Models</a></p>
<p>16 0.2774888 <a title="161-lda-16" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>17 0.27733958 <a title="161-lda-17" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>18 0.27649933 <a title="161-lda-18" href="./nips-2005-Efficient_Estimation_of_OOMs.html">62 nips-2005-Efficient Estimation of OOMs</a></p>
<p>19 0.27618578 <a title="161-lda-19" href="./nips-2005-Query_by_Committee_Made_Real.html">160 nips-2005-Query by Committee Made Real</a></p>
<p>20 0.27573535 <a title="161-lda-20" href="./nips-2005-Predicting_EMG_Data_from_M1_Neurons_with_Variational_Bayesian_Least_Squares.html">155 nips-2005-Predicting EMG Data from M1 Neurons with Variational Bayesian Least Squares</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
