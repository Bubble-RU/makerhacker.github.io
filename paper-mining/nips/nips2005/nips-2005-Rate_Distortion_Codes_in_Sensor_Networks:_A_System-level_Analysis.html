<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>162 nips-2005-Rate Distortion Codes in Sensor Networks: A System-level Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-162" href="#">nips2005-162</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>162 nips-2005-Rate Distortion Codes in Sensor Networks: A System-level Analysis</h1>
<br/><p>Source: <a title="nips-2005-162-pdf" href="http://papers.nips.cc/paper/2801-rate-distortion-codes-in-sensor-networks-a-system-level-analysis.pdf">pdf</a></p><p>Author: Tatsuto Murayama, Peter Davis</p><p>Abstract: This paper provides a system-level analysis of a scalable distributed sensing model for networked sensors. In our system model, a data center acquires data from a bunch of L sensors which each independently encode their noisy observations of an original binary sequence, and transmit their encoded data sequences to the data center at a combined rate R, which is limited. Supposing that the sensors use independent LDGM rate distortion codes, we show that the system performance can be evaluated for any given ﬁnite R when the number of sensors L goes to inﬁnity. The analysis shows how the optimal strategy for the distributed sensing problem changes at critical values of the data rate R or the noise level. 1</p><p>Reference: <a title="nips-2005-162-reference" href="../nips2005_reference/nips-2005-Rate_Distortion_Codes_in_Sensor_Networks%3A_A_System-level_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp  Abstract This paper provides a system-level analysis of a scalable distributed sensing model for networked sensors. [sent-5, score-0.219]
</p><p>2 In our system model, a data center acquires data from a bunch of L sensors which each independently encode their noisy observations of an original binary sequence, and transmit their encoded data sequences to the data center at a combined rate R, which is limited. [sent-6, score-0.975]
</p><p>3 Supposing that the sensors use independent LDGM rate distortion codes, we show that the system performance can be evaluated for any given ﬁnite R when the number of sensors L goes to inﬁnity. [sent-7, score-1.109]
</p><p>4 The analysis shows how the optimal strategy for the distributed sensing problem changes at critical values of the data rate R or the noise level. [sent-8, score-0.267]
</p><p>5 1  Introduction  Device and sensor networks are shaping many activities in our society. [sent-9, score-0.181]
</p><p>6 These networks are being deployed in a growing number of applications as diverse as agricultural management, industrial controls, crime watch, and military applications. [sent-10, score-0.169]
</p><p>7 Indeed, sensor networks can be considered as a promising technology with a wide range of potential future markets [1]. [sent-11, score-0.181]
</p><p>8 Still, for all the promise, it is often difﬁcult to integrate the individual components of a sensor network in a smart way. [sent-12, score-0.216]
</p><p>9 We demonstrate the usefulness of adopting new approaches by considering the following scenario. [sent-17, score-0.034]
</p><p>10 Consider that a data center is interested in the data sequence, {X(t)} ∞ , which cannot be t=1 observed directly. [sent-18, score-0.112]
</p><p>11 Therefore, the data center deploys a bunch of L sensors which each independently encodes its noisy observation of the sequence, {Y i (t)}∞ , without sharing any t=1 information, i. [sent-19, score-0.764]
</p><p>12 , the sensors are not permitted to communicate and decide what to send to the data center beforehand. [sent-21, score-0.634]
</p><p>13 The data center collects separate samples from all the L sensors and uses them to recover the original sequence. [sent-22, score-0.532]
</p><p>14 However, since {X(t)}∞ is not the only t=1 pressing matter which the data center must consider, the combined data rate R at which the sensors can communicate with it is strictly limited. [sent-23, score-0.73]
</p><p>15 A formulation of decentralized communication with estimation task, the “CEO problem”, was ﬁrst proposed by Berger  and Zhang [2], providing a new theoretical framework for large scale sensing systems. [sent-24, score-0.213]
</p><p>16 If the sensors were permitted to communicate on the basis of their pooled observations, then they would be able to smooth out their independent observation noises entirely as L goes to inﬁnity. [sent-26, score-0.69]
</p><p>17 Therefore, the data center can achieve an arbitrary ﬁdelity D(R), where D(·) denotes the distortion rate function of {X(t)}. [sent-27, score-0.435]
</p><p>18 In particular, the data center recovers almost complete information if R exceeds the entropy rate of {X(t)}. [sent-28, score-0.253]
</p><p>19 However, if the sensors are not allowed to communicate with each other, there does not exist a ﬁnite value of R for which even inﬁnitely many sensors can make D arbitrarily small [2]. [sent-29, score-0.81]
</p><p>20 In this paper, we introduce a new analytical model for a massive sensing system with a ﬁnite data rate R. [sent-30, score-0.354]
</p><p>21 More speciﬁcally, we assume that the sensors use LDGM codes for rate distortion coding, while the data center recovers the original sequence by using optimal “majority vote” estimation [3]. [sent-31, score-0.948]
</p><p>22 We consider the distributed sensing problem of deciding the optimal number of sensors L given the combined data rate R. [sent-32, score-0.679]
</p><p>23 Our asymptotic analysis successfully provides the performance of the whole sensing system when L goes to inﬁnity, where the data rate for an individual sensor information vanishes. [sent-33, score-0.581]
</p><p>24 Here, we exploit statistical methods which have recently been developed in the ﬁeld of disordered statistical systems, in particular, the spin glass theory. [sent-34, score-0.034]
</p><p>25 In Section 2, we introduce a system model for the sensor network. [sent-36, score-0.191]
</p><p>26 Section 3 summarizes the results of our approach, where the following section provides the outline of our analysis. [sent-37, score-0.046]
</p><p>27 2  System Model  Let P (x) be a probability distribution common to {X(t)} ∈ X , and W (y|x) be a stochastic matrix deﬁned on X × Y, with Y denotes the common alphabet of {Y i (t)}, where i = 1, · · · , L and t ≥ 1. [sent-39, score-0.1]
</p><p>28 In the general setup, we assume that the instantaneous joint probability distribution in the form L  Pr[x, y1 , · · · , yL ] = P (x)  W (yi |x) i=1  for the temporally memoryless source {X(t)} ∞ . [sent-40, score-0.071]
</p><p>29 In this paper, we impose the binary assumptions to the problem, i. [sent-42, score-0.038]
</p><p>30 , the data sequence {X(t)} and its noisy observations {Y i (t)} are all assumed to be binary sequences. [sent-44, score-0.132]
</p><p>31 Therefore, the stochastic matrix can be parameterized as W (y|x) =  1 − p, if y = x , p, otherwise  where p ∈ [0, 1] represents the observation noise. [sent-45, score-0.083]
</p><p>32 Note also that the alphabets have been selected as X = Y. [sent-46, score-0.086]
</p><p>33 At the encoding stage, a sensor i encodes a block y i = [yi (1), · · · , yi (n)]T of length n from the noisy observation {y i (t)}∞ , into a block z i = [zi (1), · · · , zi (m)]T of length t=1 m deﬁned on Z. [sent-48, score-0.528]
</p><p>34 Hereafter, we take the Boolean representation of the binary alphabet ˆ X = {0, 1}, therefore Y = Z = {0, 1} as well. [sent-49, score-0.102]
</p><p>35 Let y i be a reproduction sequence for the block, and we have a known integer m < n. [sent-50, score-0.332]
</p><p>36 Then, making use of a Boolean matrix Ai of dimensionality n×m, we are to ﬁnd an m bit codeword sequence z i = [zi (1), · · · , zi (m)]T which satisﬁes ˆ y i = Ai z i  (mod 2) ,  (1)  where the ﬁdelity criterion 1 ˆ dH (y i , yi ) (2) n holds [4]. [sent-51, score-0.415]
</p><p>37 Here the Hamming distance d H (·, ·) is used for the distortion measure. [sent-52, score-0.201]
</p><p>38 Note that we have applied modulo-2 arithmetic for the additive operation in (1). [sent-53, score-0.032]
</p><p>39 The data center then collects the L codeword sequences, z1 , · · · , zL . [sent-56, score-0.262]
</p><p>40 Since all the L codewords are of the same length m, the combined data rate will be R = L × m/n. [sent-57, score-0.191]
</p><p>41 Therefore, in our scenario, the data center ˆ ˆ deploys exchangeable sensors with ﬁxed quality reproductions, y 1 , · · · , yL . [sent-58, score-0.628]
</p><p>42 Lastly, the tth ˆ symbol of the estimate, x = [ˆ(1), · · · , x(n)]T , is to be calculated by majority vote [3], x ˆ D=  x(t) = ˆ  ˆ 0, if y1 (t) + · · · + yL (t) ≤ L/2 ˆ . [sent-59, score-0.183]
</p><p>43 1, otherwise  (3)  Therefore, overall performance of the system can be measured by the expected bit error ˆ frequency for decisions by the majority vote (3), P e = Pr[x = x]. [sent-60, score-0.482]
</p><p>44 In this paper, we consider two limit cases of decentralization levels; (1) The extreme situation of L → ∞, and (2) the case of L = R. [sent-61, score-0.038]
</p><p>45 The former case means that the data rate for an individual sensor information vanishes, while the latter case results in the transmission without coding techniques. [sent-62, score-0.301]
</p><p>46 In general, it is difﬁcult to determine which level is optimal for the estimation, i. [sent-63, score-0.05]
</p><p>47 , which scenario results in the smaller value of P e. [sent-65, score-0.067]
</p><p>48 Indeed, by using the rate distortion codes, the data center could use as many sensors as possible for a given R. [sent-66, score-0.744]
</p><p>49 However, the quality of the individual reproduction would be less informative. [sent-67, score-0.335]
</p><p>50 Let p be a given observation noise level, and R the ﬁnite real value of a given combined data rate. [sent-70, score-0.108]
</p><p>51 Here N(X, Y ) denotes the normal distribution with the mean X and the variance Y . [sent-72, score-0.036]
</p><p>52 The rescaled variance σ2 and the scale invariant parameter α is determined numerically, where we use the following notations. [sent-73, score-0.051]
</p><p>53 (a) Narrow band (b) Broadband  Therefore, it is straightforward to evaluate (4) with (5) for given parameters, p and R. [sent-86, score-0.137]
</p><p>54 For a given ﬁnite value of R, we see what happens to the quality of the estimate when the noise level p varies. [sent-87, score-0.135]
</p><p>55 2 shows the typical behavior of the bit error frequency, Pe(p, R), in decibel (dB), where the reference level is chosen as (0) Pe (p, R) =  (R−1)/2 R (1 − p)l pR−l , l=0 l R/2−1 R R (1 − p)l pR−l + 1 R/2 l=0 2 l  (1 − p)  R/2 R/2  p  (R is odd) (6) (R is even)  for a given integer R. [sent-90, score-0.543]
</p><p>56 The reference (6) denotes Pe for the case of L = R, i. [sent-91, score-0.108]
</p><p>57 , the case when the sensors are not allowed to compress their observations. [sent-93, score-0.379]
</p><p>58 Note that the zero level in decibel occurs when the measured error frequency Pe (p, R) is equal to the reference level. [sent-95, score-0.396]
</p><p>59 Therefore, it is also possible to have negative levels, which would mean an expected bit error frequency much smaller than the reference level. [sent-96, score-0.318]
</p><p>60 In the case of small combined data rate R, the narrow band case, the numerical results in Fig. [sent-97, score-0.417]
</p><p>61 2 (a) show that the quality of the estimate is sensitive to the parity of the integer R. [sent-99, score-0.155]
</p><p>62 In particular, the R = 2 case has the lowest threshold level, pc = 0. [sent-100, score-0.042]
</p><p>63 2 (a) respectively, beyond which the L → ∞ scenario outperforms the L = R scenario, while the R = 1 case does not have such a threshold. [sent-104, score-0.067]
</p><p>64 In contrast, if the bandwidth is wide enough, the difference of the (dB) expected bit error probabilities in decibel, P e (p, R), is proved to have similar qualitative characteristics as shown in Fig. [sent-105, score-0.178]
</p><p>65 146 respectively, as L goes to inﬁnity; we are currently working on the theoretical derivation. [sent-110, score-0.089]
</p><p>66 4  Outline of Derivation  Since the predetermined matrices A1 , · · · , AL are selected randomly, it is quite natural to ˆ say that the instantaneous series, deﬁned by y (t) = [ˆ1 (t), · · · , yL(t)]T , can be modeled y ˆ  (a) Narrow Band  Pe  (dB)  (p, R)  2 R=1  1 0  R=2  −1  R = 10  −2 0  0. [sent-111, score-0.037]
</p><p>67 (a) Narrow band (b) Broadband  using the Bernoulli trials. [sent-122, score-0.137]
</p><p>68 Here, the reproduction problem reduces to a channel model, where the stochastic matrix is deﬁned as q, if y = x ˆ W (ˆ|x) = y , (7) 1 − q, otherwise where q denotes the quality of the reproductions, i. [sent-123, score-0.416]
</p><p>69 Note that the reproduction quality q can be easily obtained by the simple algebra q = pD + (1 − p)(1 − D), where D is the distortion with respect to coding. [sent-127, score-0.502]
</p><p>70 Since the error probability (8) is given by a function of q, we ﬁrstly derive an analytical solution for the quality q in the limit L → ∞, keeping R ﬁnite. [sent-128, score-0.162]
</p><p>71 In this approach, we apply the method of statistical mechanics to evaluate the typical performance of the codes [4]. [sent-129, score-0.103]
</p><p>72 As a ﬁrst step, we translate the Boolean alphabets Z = {0, 1} to the “Ising” ones, S = {+1, −1}. [sent-130, score-0.151]
</p><p>73 Consequently, we need to translate the additive operations, such as, z i (s) + zi (s ) (mod 2) into their multiplicative representations, σ i (s) × σi (s ) ∈ S for s, s = 1, · · · , m. [sent-131, score-0.152]
</p><p>74 Similarly, we translate the Boolean yi (t)s into the Ising J i (t)s. [sent-132, score-0.127]
</p><p>75 Following the prescription of Sourlas [5], we examine the Gibbs-Boltzmann distribution exp [−βH(σ|J)] with Z(J ) = Pr[σ] = e−βH(σ|J) , (9) Z(J ) σ  where the Hamiltonian of the Ising system is deﬁned as As1 . [sent-134, score-0.075]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pe', 0.415), ('sensors', 0.345), ('reproduction', 0.216), ('db', 0.201), ('distortion', 0.201), ('sensing', 0.181), ('decibel', 0.173), ('sensor', 0.148), ('bit', 0.145), ('yl', 0.137), ('band', 0.137), ('pr', 0.132), ('ldgm', 0.13), ('broadband', 0.128), ('narrow', 0.127), ('communicate', 0.12), ('center', 0.112), ('boolean', 0.11), ('codes', 0.103), ('ising', 0.096), ('goes', 0.089), ('zi', 0.087), ('alphabets', 0.086), ('bunch', 0.086), ('deploys', 0.086), ('reproductions', 0.086), ('rate', 0.086), ('vote', 0.086), ('quality', 0.085), ('collects', 0.075), ('codeword', 0.075), ('reference', 0.072), ('integer', 0.07), ('delity', 0.069), ('frequency', 0.068), ('scenario', 0.067), ('combined', 0.067), ('translate', 0.065), ('majority', 0.065), ('alphabet', 0.064), ('yi', 0.062), ('permitted', 0.057), ('recovers', 0.055), ('mod', 0.055), ('cg', 0.055), ('odd', 0.055), ('rescaled', 0.051), ('level', 0.05), ('noisy', 0.048), ('ln', 0.048), ('block', 0.048), ('sequence', 0.046), ('encodes', 0.046), ('outline', 0.046), ('analytical', 0.044), ('system', 0.043), ('sk', 0.042), ('nity', 0.042), ('pc', 0.042), ('otherwise', 0.042), ('observation', 0.041), ('letting', 0.04), ('binary', 0.038), ('noises', 0.038), ('acquires', 0.038), ('keihanna', 0.038), ('decentralization', 0.038), ('crime', 0.038), ('watch', 0.038), ('zl', 0.038), ('networked', 0.038), ('codewords', 0.038), ('nite', 0.037), ('instantaneous', 0.037), ('channel', 0.037), ('denotes', 0.036), ('deployed', 0.034), ('davis', 0.034), ('guessing', 0.034), ('supposing', 0.034), ('adopting', 0.034), ('enforcement', 0.034), ('telephone', 0.034), ('disordered', 0.034), ('hamming', 0.034), ('smart', 0.034), ('ntt', 0.034), ('memoryless', 0.034), ('compress', 0.034), ('individual', 0.034), ('error', 0.033), ('networks', 0.033), ('coding', 0.033), ('communication', 0.032), ('military', 0.032), ('arithmetic', 0.032), ('tth', 0.032), ('industrial', 0.032), ('prescription', 0.032), ('rstly', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="162-tfidf-1" href="./nips-2005-Rate_Distortion_Codes_in_Sensor_Networks%3A_A_System-level_Analysis.html">162 nips-2005-Rate Distortion Codes in Sensor Networks: A System-level Analysis</a></p>
<p>Author: Tatsuto Murayama, Peter Davis</p><p>Abstract: This paper provides a system-level analysis of a scalable distributed sensing model for networked sensors. In our system model, a data center acquires data from a bunch of L sensors which each independently encode their noisy observations of an original binary sequence, and transmit their encoded data sequences to the data center at a combined rate R, which is limited. Supposing that the sensors use independent LDGM rate distortion codes, we show that the system performance can be evaluated for any given ﬁnite R when the number of sensors L goes to inﬁnity. The analysis shows how the optimal strategy for the distributed sensing problem changes at critical values of the data rate R or the noise level. 1</p><p>2 0.18355356 <a title="162-tfidf-2" href="./nips-2005-An_Analog_Visual_Pre-Processing_Processor_Employing_Cyclic_Line_Access_in_Only-Nearest-Neighbor-Interconnects_Architecture.html">22 nips-2005-An Analog Visual Pre-Processing Processor Employing Cyclic Line Access in Only-Nearest-Neighbor-Interconnects Architecture</a></p>
<p>Author: Yusuke Nakashita, Yoshio Mita, Tadashi Shibata</p><p>Abstract: An analog focal-plane processor having a 128¢128 photodiode array has been developed for directional edge ﬁltering. It can perform 4¢4-pixel kernel convolution for entire pixels only with 256 steps of simple analog processing. Newly developed cyclic line access and row-parallel processing scheme in conjunction with the “only-nearest-neighbor interconnects” architecture has enabled a very simple implementation. A proof-of-concept chip was fabricated in a 0.35- m 2-poly 3-metal CMOS technology and the edge ﬁltering at a rate of 200 frames/sec. has been experimentally demonstrated.</p><p>3 0.11033072 <a title="162-tfidf-3" href="./nips-2005-Affine_Structure_From_Sound.html">20 nips-2005-Affine Structure From Sound</a></p>
<p>Author: Sebastian Thrun</p><p>Abstract: We consider the problem of localizing a set of microphones together with a set of external acoustic events (e.g., hand claps), emitted at unknown times and unknown locations. We propose a solution that approximates this problem under a far ﬁeld approximation deﬁned in the calculus of afﬁne geometry, and that relies on singular value decomposition (SVD) to recover the afﬁne structure of the problem. We then deﬁne low-dimensional optimization techniques for embedding the solution into Euclidean geometry, and further techniques for recovering the locations and emission times of the acoustic events. The approach is useful for the calibration of ad-hoc microphone arrays and sensor networks. 1</p><p>4 0.095812649 <a title="162-tfidf-4" href="./nips-2005-Recovery_of_Jointly_Sparse_Signals_from_Few_Random_Projections.html">163 nips-2005-Recovery of Jointly Sparse Signals from Few Random Projections</a></p>
<p>Author: Michael B. Wakin, Marco F. Duarte, Shriram Sarvotham, Dror Baron, Richard G. Baraniuk</p><p>Abstract: Compressed sensing is an emerging ﬁeld based on the revelation that a small group of linear projections of a sparse signal contains enough information for reconstruction. In this paper we introduce a new theory for distributed compressed sensing (DCS) that enables new distributed coding algorithms for multi-signal ensembles that exploit both intra- and inter-signal correlation structures. The DCS theory rests on a new concept that we term the joint sparsity of a signal ensemble. We study three simple models for jointly sparse signals, propose algorithms for joint recovery of multiple signals from incoherent projections, and characterize theoretically and empirically the number of measurements per sensor required for accurate reconstruction. In some sense DCS is a framework for distributed compression of sources with memory, which has remained a challenging problem in information theory for some time. DCS is immediately applicable to a range of problems in sensor networks and arrays. 1</p><p>5 0.065828592 <a title="162-tfidf-5" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>Author: Eizaburo Doi, Doru C. Balcan, Michael S. Lewicki</p><p>Abstract: Biological sensory systems are faced with the problem of encoding a high-ﬁdelity sensory signal with a population of noisy, low-ﬁdelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and twodimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of highdimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets. 1</p><p>6 0.060966611 <a title="162-tfidf-6" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>7 0.059585314 <a title="162-tfidf-7" href="./nips-2005-Gaussian_Processes_for_Multiuser_Detection_in_CDMA_receivers.html">81 nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</a></p>
<p>8 0.05624396 <a title="162-tfidf-8" href="./nips-2005-Learning_in_Silicon%3A_Timing_is_Everything.html">118 nips-2005-Learning in Silicon: Timing is Everything</a></p>
<p>9 0.05418507 <a title="162-tfidf-9" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>10 0.052231465 <a title="162-tfidf-10" href="./nips-2005-Policy-Gradient_Methods_for_Planning.html">153 nips-2005-Policy-Gradient Methods for Planning</a></p>
<p>11 0.049840078 <a title="162-tfidf-11" href="./nips-2005-Scaling_Laws_in_Natural_Scenes_and_the_Inference_of_3D_Shape.html">170 nips-2005-Scaling Laws in Natural Scenes and the Inference of 3D Shape</a></p>
<p>12 0.04967108 <a title="162-tfidf-12" href="./nips-2005-An_Application_of_Markov_Random_Fields_to_Range_Sensing.html">23 nips-2005-An Application of Markov Random Fields to Range Sensing</a></p>
<p>13 0.04728492 <a title="162-tfidf-13" href="./nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">113 nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<p>14 0.046996929 <a title="162-tfidf-14" href="./nips-2005-Estimation_of_Intrinsic_Dimensionality_Using_High-Rate_Vector_Quantization.html">66 nips-2005-Estimation of Intrinsic Dimensionality Using High-Rate Vector Quantization</a></p>
<p>15 0.045504071 <a title="162-tfidf-15" href="./nips-2005-Optimizing_spatio-temporal_filters_for_improving_Brain-Computer_Interfacing.html">150 nips-2005-Optimizing spatio-temporal filters for improving Brain-Computer Interfacing</a></p>
<p>16 0.045260318 <a title="162-tfidf-16" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>17 0.045195904 <a title="162-tfidf-17" href="./nips-2005-Generalization_to_Unseen_Cases.html">85 nips-2005-Generalization to Unseen Cases</a></p>
<p>18 0.045178674 <a title="162-tfidf-18" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>19 0.044304177 <a title="162-tfidf-19" href="./nips-2005-Gradient_Flow_Independent_Component_Analysis_in_Micropower_VLSI.html">88 nips-2005-Gradient Flow Independent Component Analysis in Micropower VLSI</a></p>
<p>20 0.044214323 <a title="162-tfidf-20" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.15), (1, -0.02), (2, -0.003), (3, 0.024), (4, -0.004), (5, 0.042), (6, -0.133), (7, 0.001), (8, 0.141), (9, -0.031), (10, -0.221), (11, -0.008), (12, 0.025), (13, 0.053), (14, -0.001), (15, 0.02), (16, -0.045), (17, 0.029), (18, 0.054), (19, -0.038), (20, 0.005), (21, -0.082), (22, 0.101), (23, 0.145), (24, 0.046), (25, 0.053), (26, -0.044), (27, 0.04), (28, 0.018), (29, -0.003), (30, -0.009), (31, -0.172), (32, 0.029), (33, 0.114), (34, -0.04), (35, 0.093), (36, 0.038), (37, -0.009), (38, -0.076), (39, -0.119), (40, 0.104), (41, -0.044), (42, -0.059), (43, -0.08), (44, 0.099), (45, 0.082), (46, -0.202), (47, -0.149), (48, 0.112), (49, -0.17)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95231235 <a title="162-lsi-1" href="./nips-2005-Rate_Distortion_Codes_in_Sensor_Networks%3A_A_System-level_Analysis.html">162 nips-2005-Rate Distortion Codes in Sensor Networks: A System-level Analysis</a></p>
<p>Author: Tatsuto Murayama, Peter Davis</p><p>Abstract: This paper provides a system-level analysis of a scalable distributed sensing model for networked sensors. In our system model, a data center acquires data from a bunch of L sensors which each independently encode their noisy observations of an original binary sequence, and transmit their encoded data sequences to the data center at a combined rate R, which is limited. Supposing that the sensors use independent LDGM rate distortion codes, we show that the system performance can be evaluated for any given ﬁnite R when the number of sensors L goes to inﬁnity. The analysis shows how the optimal strategy for the distributed sensing problem changes at critical values of the data rate R or the noise level. 1</p><p>2 0.55517 <a title="162-lsi-2" href="./nips-2005-Recovery_of_Jointly_Sparse_Signals_from_Few_Random_Projections.html">163 nips-2005-Recovery of Jointly Sparse Signals from Few Random Projections</a></p>
<p>Author: Michael B. Wakin, Marco F. Duarte, Shriram Sarvotham, Dror Baron, Richard G. Baraniuk</p><p>Abstract: Compressed sensing is an emerging ﬁeld based on the revelation that a small group of linear projections of a sparse signal contains enough information for reconstruction. In this paper we introduce a new theory for distributed compressed sensing (DCS) that enables new distributed coding algorithms for multi-signal ensembles that exploit both intra- and inter-signal correlation structures. The DCS theory rests on a new concept that we term the joint sparsity of a signal ensemble. We study three simple models for jointly sparse signals, propose algorithms for joint recovery of multiple signals from incoherent projections, and characterize theoretically and empirically the number of measurements per sensor required for accurate reconstruction. In some sense DCS is a framework for distributed compression of sources with memory, which has remained a challenging problem in information theory for some time. DCS is immediately applicable to a range of problems in sensor networks and arrays. 1</p><p>3 0.53844196 <a title="162-lsi-3" href="./nips-2005-An_Analog_Visual_Pre-Processing_Processor_Employing_Cyclic_Line_Access_in_Only-Nearest-Neighbor-Interconnects_Architecture.html">22 nips-2005-An Analog Visual Pre-Processing Processor Employing Cyclic Line Access in Only-Nearest-Neighbor-Interconnects Architecture</a></p>
<p>Author: Yusuke Nakashita, Yoshio Mita, Tadashi Shibata</p><p>Abstract: An analog focal-plane processor having a 128¢128 photodiode array has been developed for directional edge ﬁltering. It can perform 4¢4-pixel kernel convolution for entire pixels only with 256 steps of simple analog processing. Newly developed cyclic line access and row-parallel processing scheme in conjunction with the “only-nearest-neighbor interconnects” architecture has enabled a very simple implementation. A proof-of-concept chip was fabricated in a 0.35- m 2-poly 3-metal CMOS technology and the edge ﬁltering at a rate of 200 frames/sec. has been experimentally demonstrated.</p><p>4 0.47063756 <a title="162-lsi-4" href="./nips-2005-Gaussian_Processes_for_Multiuser_Detection_in_CDMA_receivers.html">81 nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</a></p>
<p>Author: Juan J. Murillo-fuentes, Sebastian Caro, Fernando Pérez-Cruz</p><p>Abstract: In this paper we propose a new receiver for digital communications. We focus on the application of Gaussian Processes (GPs) to the multiuser detection (MUD) in code division multiple access (CDMA) systems to solve the near-far problem. Hence, we aim to reduce the interference from other users sharing the same frequency band. While usual approaches minimize the mean square error (MMSE) to linearly retrieve the user of interest, we exploit the same criteria but in the design of a nonlinear MUD. Since the optimal solution is known to be nonlinear, the performance of this novel method clearly improves that of the MMSE detectors. Furthermore, the GP based MUD achieves excellent interference suppression even for short training sequences. We also include some experiments to illustrate that other nonlinear detectors such as those based on Support Vector Machines (SVMs) exhibit a worse performance. 1</p><p>5 0.44885153 <a title="162-lsi-5" href="./nips-2005-Affine_Structure_From_Sound.html">20 nips-2005-Affine Structure From Sound</a></p>
<p>Author: Sebastian Thrun</p><p>Abstract: We consider the problem of localizing a set of microphones together with a set of external acoustic events (e.g., hand claps), emitted at unknown times and unknown locations. We propose a solution that approximates this problem under a far ﬁeld approximation deﬁned in the calculus of afﬁne geometry, and that relies on singular value decomposition (SVD) to recover the afﬁne structure of the problem. We then deﬁne low-dimensional optimization techniques for embedding the solution into Euclidean geometry, and further techniques for recovering the locations and emission times of the acoustic events. The approach is useful for the calibration of ad-hoc microphone arrays and sensor networks. 1</p><p>6 0.39336199 <a title="162-lsi-6" href="./nips-2005-Efficient_Estimation_of_OOMs.html">62 nips-2005-Efficient Estimation of OOMs</a></p>
<p>7 0.39250612 <a title="162-lsi-7" href="./nips-2005-An_Approximate_Inference_Approach_for_the_PCA_Reconstruction_Error.html">24 nips-2005-An Approximate Inference Approach for the PCA Reconstruction Error</a></p>
<p>8 0.35109812 <a title="162-lsi-8" href="./nips-2005-Fast_biped_walking_with_a_reflexive_controller_and_real-time_policy_searching.html">73 nips-2005-Fast biped walking with a reflexive controller and real-time policy searching</a></p>
<p>9 0.34297118 <a title="162-lsi-9" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>10 0.31710121 <a title="162-lsi-10" href="./nips-2005-Gradient_Flow_Independent_Component_Analysis_in_Micropower_VLSI.html">88 nips-2005-Gradient Flow Independent Component Analysis in Micropower VLSI</a></p>
<p>11 0.31457305 <a title="162-lsi-11" href="./nips-2005-Off-Road_Obstacle_Avoidance_through_End-to-End_Learning.html">143 nips-2005-Off-Road Obstacle Avoidance through End-to-End Learning</a></p>
<p>12 0.30592346 <a title="162-lsi-12" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>13 0.30509803 <a title="162-lsi-13" href="./nips-2005-AER_Building_Blocks_for_Multi-Layer_Multi-Chip_Neuromorphic_Vision_Systems.html">1 nips-2005-AER Building Blocks for Multi-Layer Multi-Chip Neuromorphic Vision Systems</a></p>
<p>14 0.30298087 <a title="162-lsi-14" href="./nips-2005-Estimation_of_Intrinsic_Dimensionality_Using_High-Rate_Vector_Quantization.html">66 nips-2005-Estimation of Intrinsic Dimensionality Using High-Rate Vector Quantization</a></p>
<p>15 0.28218371 <a title="162-lsi-15" href="./nips-2005-Prediction_and_Change_Detection.html">156 nips-2005-Prediction and Change Detection</a></p>
<p>16 0.27599069 <a title="162-lsi-16" href="./nips-2005-Location-based_activity_recognition.html">121 nips-2005-Location-based activity recognition</a></p>
<p>17 0.27274621 <a title="162-lsi-17" href="./nips-2005-Stimulus_Evoked_Independent_Factor_Analysis_of_MEG_Data_with_Large_Background_Activity.html">183 nips-2005-Stimulus Evoked Independent Factor Analysis of MEG Data with Large Background Activity</a></p>
<p>18 0.26066229 <a title="162-lsi-18" href="./nips-2005-The_Information-Form_Data_Association_Filter.html">192 nips-2005-The Information-Form Data Association Filter</a></p>
<p>19 0.2584143 <a title="162-lsi-19" href="./nips-2005-Sequence_and_Tree_Kernels_with_Statistical_Feature_Mining.html">175 nips-2005-Sequence and Tree Kernels with Statistical Feature Mining</a></p>
<p>20 0.2578615 <a title="162-lsi-20" href="./nips-2005-Q-Clustering.html">159 nips-2005-Q-Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.372), (3, 0.059), (10, 0.035), (27, 0.024), (31, 0.025), (34, 0.1), (55, 0.052), (69, 0.057), (73, 0.035), (77, 0.039), (88, 0.07), (91, 0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77567065 <a title="162-lda-1" href="./nips-2005-Rate_Distortion_Codes_in_Sensor_Networks%3A_A_System-level_Analysis.html">162 nips-2005-Rate Distortion Codes in Sensor Networks: A System-level Analysis</a></p>
<p>Author: Tatsuto Murayama, Peter Davis</p><p>Abstract: This paper provides a system-level analysis of a scalable distributed sensing model for networked sensors. In our system model, a data center acquires data from a bunch of L sensors which each independently encode their noisy observations of an original binary sequence, and transmit their encoded data sequences to the data center at a combined rate R, which is limited. Supposing that the sensors use independent LDGM rate distortion codes, we show that the system performance can be evaluated for any given ﬁnite R when the number of sensors L goes to inﬁnity. The analysis shows how the optimal strategy for the distributed sensing problem changes at critical values of the data rate R or the noise level. 1</p><p>2 0.44972011 <a title="162-lda-2" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>Author: Doron Blatt, Alfred O. Hero</p><p>Abstract: This paper proposes an algorithm to convert a T -stage stochastic decision problem with a continuous state space to a sequence of supervised learning problems. The optimization problem associated with the trajectory tree and random trajectory methods of Kearns, Mansour, and Ng, 2000, is solved using the Gauss-Seidel method. The algorithm breaks a multistage reinforcement learning problem into a sequence of single-stage reinforcement learning subproblems, each of which is solved via an exact reduction to a weighted-classiﬁcation problem that can be solved using off-the-self methods. Thus the algorithm converts a reinforcement learning problem into simpler supervised learning subproblems. It is shown that the method converges in a ﬁnite number of steps to a solution that cannot be further improved by componentwise optimization. The implication of the proposed algorithm is that a plethora of classiﬁcation methods can be applied to ﬁnd policies in the reinforcement learning problem. 1</p><p>3 0.40296113 <a title="162-lda-3" href="./nips-2005-Efficient_Estimation_of_OOMs.html">62 nips-2005-Efficient Estimation of OOMs</a></p>
<p>Author: Herbert Jaeger, Mingjie Zhao, Andreas Kolling</p><p>Abstract: A standard method to obtain stochastic models for symbolic time series is to train state-emitting hidden Markov models (SE-HMMs) with the Baum-Welch algorithm. Based on observable operator models (OOMs), in the last few months a number of novel learning algorithms for similar purposes have been developed: (1,2) two versions of an ”efﬁciency sharpening” (ES) algorithm, which iteratively improves the statistical efﬁciency of a sequence of OOM estimators, (3) a constrained gradient descent ML estimator for transition-emitting HMMs (TE-HMMs). We give an overview on these algorithms and compare them with SE-HMM/EM learning on synthetic and real-life data. 1</p><p>4 0.39898691 <a title="162-lda-4" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>Author: Yoshua Bengio, Nicolas L. Roux, Pascal Vincent, Olivier Delalleau, Patrice Marcotte</p><p>Abstract: Convexity has recently received a lot of attention in the machine learning community, and the lack of convexity has been seen as a major disadvantage of many learning algorithms, such as multi-layer artiﬁcial neural networks. We show that training multi-layer neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem. This problem involves an inﬁnite number of variables, but can be solved by incrementally inserting a hidden unit at a time, each time ﬁnding a linear classiﬁer that minimizes a weighted sum of errors. 1</p><p>5 0.39704859 <a title="162-lda-5" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>Author: Yixin Chen, Ya Zhang, Xiang Ji</p><p>Abstract: We present a novel spectral clustering method that enables users to incorporate prior knowledge of the size of clusters into the clustering process. The cost function, which is named size regularized cut (SRcut), is deﬁned as the sum of the inter-cluster similarity and a regularization term measuring the relative size of two clusters. Finding a partition of the data set to minimize SRcut is proved to be NP-complete. An approximation algorithm is proposed to solve a relaxed version of the optimization problem as an eigenvalue problem. Evaluations over different data sets demonstrate that the method is not sensitive to outliers and performs better than normalized cut. 1</p><p>6 0.39569378 <a title="162-lda-6" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>7 0.39550924 <a title="162-lda-7" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>8 0.39541468 <a title="162-lda-8" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>9 0.39536881 <a title="162-lda-9" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>10 0.39479643 <a title="162-lda-10" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>11 0.39334768 <a title="162-lda-11" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>12 0.39304048 <a title="162-lda-12" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>13 0.39303285 <a title="162-lda-13" href="./nips-2005-Query_by_Committee_Made_Real.html">160 nips-2005-Query by Committee Made Real</a></p>
<p>14 0.39145005 <a title="162-lda-14" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>15 0.39100739 <a title="162-lda-15" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>16 0.39072305 <a title="162-lda-16" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>17 0.38985574 <a title="162-lda-17" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>18 0.38947421 <a title="162-lda-18" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>19 0.38920543 <a title="162-lda-19" href="./nips-2005-Kernelized_Infomax_Clustering.html">102 nips-2005-Kernelized Infomax Clustering</a></p>
<p>20 0.38908851 <a title="162-lda-20" href="./nips-2005-Computing_the_Solution_Path_for_the_Regularized_Support_Vector_Regression.html">44 nips-2005-Computing the Solution Path for the Regularized Support Vector Regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
