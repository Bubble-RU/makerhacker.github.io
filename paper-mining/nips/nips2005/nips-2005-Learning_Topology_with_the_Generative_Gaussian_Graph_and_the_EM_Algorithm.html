<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>116 nips-2005-Learning Topology with the Generative Gaussian Graph and the EM Algorithm</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-116" href="#">nips2005-116</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>116 nips-2005-Learning Topology with the Generative Gaussian Graph and the EM Algorithm</h1>
<br/><p>Source: <a title="nips-2005-116-pdf" href="http://papers.nips.cc/paper/2922-learning-topology-with-the-generative-gaussian-graph-and-the-em-algorithm.pdf">pdf</a></p><p>Author: Michaël Aupetit</p><p>Abstract: Given a set of points and a set of prototypes representing them, how to create a graph of the prototypes whose topology accounts for that of the points? This problem had not yet been explored in the framework of statistical learning theory. In this work, we propose a generative model based on the Delaunay graph of the prototypes and the ExpectationMaximization algorithm to learn the parameters. This work is a ﬁrst step towards the construction of a topological model of a set of points grounded on statistics. 1 1.1</p><p>Reference: <a title="nips-2005-116-reference" href="../nips2005_reference/nips-2005-Learning_Topology_with_the_Generative_Gaussian_Graph_and_the_EM_Algorithm_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 fr  Abstract Given a set of points and a set of prototypes representing them, how to create a graph of the prototypes whose topology accounts for that of the points? [sent-4, score-1.404]
</p><p>2 In this work, we propose a generative model based on the Delaunay graph of the prototypes and the ExpectationMaximization algorithm to learn the parameters. [sent-6, score-0.667]
</p><p>3 This work is a ﬁrst step towards the construction of a topological model of a set of points grounded on statistics. [sent-7, score-0.245]
</p><p>4 Given a set of points in a high-dimensional euclidean space, we intend to extract the topology of the manifolds from which they are drawn. [sent-10, score-0.596]
</p><p>5 There are several reasons for this among which: increasing our knowledge about this set of points by measuring its topological features (connectedness, intrinsic dimension, Betti numbers (number of voids, holes, tunnels. [sent-11, score-0.246]
</p><p>6 )) in the context of exploratory data analysis [1], allowing to compare two sets of points wrt their topological characteristics or to ﬁnd clusters as connected components in the context of pattern recognition [2], or ﬁnding shortest path along manifolds in the context of robotics [3]. [sent-14, score-0.422]
</p><p>7 In all these approaches, the intrinsic dimension of the model is ﬁxed a priori which eases the visualization but arbitrarily forces the topology of the model. [sent-17, score-0.601]
</p><p>8 And when the dimension is not ﬁxed as in the mixture of Principal Component Analyzers [8], the connectedness is lost. [sent-18, score-0.151]
</p><p>9 Its aim is not to project and visualize a high-dimensional set of points, but to extract the topological information from it directly in the high-dimensional space, so that the model must be freed as much as possible from any a priori topological constraint. [sent-20, score-0.391]
</p><p>10 A simplicial complex2 is such a model based on the combination of simplices, each with its own dimension (a 1-simplex is a line segment, a 2-simplex is a triangle. [sent-23, score-0.262]
</p><p>11 In a simplicial complex, the simplices are exclusively connected by their vertices or their faces. [sent-27, score-0.278]
</p><p>12 Such a structure is appealing because it is possible to extract from it topological information like Betti numbers, connectedness and intrinsic dimension [10]. [sent-28, score-0.339]
</p><p>13 A particular simplicial complex is the Delaunay complex deﬁned as the set of simplices whose Vorono¨ ı cells3 of the vertices are adjacent assuming general position for the vertices. [sent-29, score-0.311]
</p><p>14 The Delaunay graph is made of vertices and edges of the Delaunay complex [12]. [sent-30, score-0.329]
</p><p>15 In the present work, the manifold M is not known but through a ﬁnite set of M data points v ∈ MM . [sent-32, score-0.099]
</p><p>16 Martinetz and Schulten proposed to build a graph of the prototypes with an algorithm called ”Competitive Hebbian Learning” (CHL)[11] to tackle this problem. [sent-33, score-0.518]
</p><p>17 Their approach has been extended to simplicial complexes by De Silva and Carlsson with the deﬁnition of ”weak witnesses” [10]. [sent-34, score-0.157]
</p><p>18 In both cases, the ES-conditions about M are weakened so they can be veriﬁed by a ﬁnite sample v of M, so that the graph or the simplicial complex built over w is proved to have the same topology as M if v is a sufﬁciently dense sampling of M. [sent-35, score-0.775]
</p><p>19 The CHL consists in connecting two prototypes in w if they are the ﬁrst and the second closest neighbors to a point of v (closeness wrt the Euclidean norm). [sent-36, score-0.469]
</p><p>20 Each point of v leads to an edge, and is called a ”weak witness” of the connected prototypes [10]. [sent-37, score-0.387]
</p><p>21 The topology representing graph obtained is a subgraph of the Delaunay graph. [sent-38, score-0.674]
</p><p>22 The region of R D in which any data point would connect the same prototypes, is the ”region of inﬂuence” (ROI) of this edge (see Figure 2 d-f). [sent-39, score-0.068]
</p><p>23 Therefore, the model obtained is based on regions of inﬂuence: a simplex exists in the model if there is at least one datum in its ROI. [sent-41, score-0.109]
</p><p>24 Hence, the capacity of this model to correctly represent the topology of a set of points, strongly depends on the shape and location of the ROI wrt the points, and on the presence of noise in the data. [sent-42, score-0.614]
</p><p>25 Moreover, as far as N 0 > 2, it cannot exist an isolated prototype allowing to represent an isolated bump in the data distribution, because any datum of this bump will have two closest prototypes to connect to each other. [sent-43, score-0.679]
</p><p>26 An aging process has been proposed by Martinetz and Schulten to ﬁlter out the noise, which works roughly such that edges with fewer data than a threshold in there ROI are pruned from the graph. [sent-44, score-0.123]
</p><p>27 This looks like a ﬁlter based on the probability density of the data distribution, but no statistical criterion is proposed to tune the parameters. [sent-45, score-0.075]
</p><p>28 Moreover the area of the ROI may be intractable in high dimension and is not trivially related to the 1 For simplicity, we call ”manifold” what can be actually a set of manifolds connected or not to each other with possibly various intrinsic dimensions. [sent-46, score-0.225]
</p><p>29 3 Given a set of points w in RD , Vi = {v ∈ RD |(v − wi )2 ≤ (v − wj )2 , ∀j} deﬁnes the Vorono¨ ı cell associated to wi ∈ w. [sent-48, score-0.189]
</p><p>30 corresponding line segment, so measuring the frequency over such a region is not relevant to deﬁne a useful probability density. [sent-49, score-0.047]
</p><p>31 At last, the line segment associated to an edge of the graph is not part of the model: data are not projected on it, data drawn from such a line segment may not give rise to the corresponding edge, and the line segment may not intersect at all its associated ROI. [sent-50, score-0.648]
</p><p>32 In other words, the model is not self-consistent, that is the geometrical realization of the graph is not always a good model of its own topology whatever the density of the sampling. [sent-51, score-0.69]
</p><p>33 We proposed to deﬁne Vorono¨ cells of line segments as ROI for the edges and deﬁned a ı criterion to cut edges with a lower density of data projecting on their middle than on their borders [9]. [sent-52, score-0.397]
</p><p>34 no criterion allows to assess the quality of the model especially in dimension greater than 3. [sent-55, score-0.097]
</p><p>35 3  Emerging topology from a statistical generative model  For all the above reasons, we propose another way for modelling topology. [sent-57, score-0.644]
</p><p>36 The idea is to construct a ”good” statistical generative model of the data taking the noise into account, and to assume that its topology is therefore a ”good” model of the topology of the manifold which generated the data. [sent-58, score-1.166]
</p><p>37 The only constraint we impose on this generative model is that its topology must be as ”ﬂexible” as possible and must be ”extractible”. [sent-59, score-0.582]
</p><p>38 ”Flexible” to avoid at best any a priori constraint on the topology so as to allow the modelling of any one. [sent-60, score-0.509]
</p><p>39 ”Extractible” to get a ”white box” model from which the topological characteristics are tractable in terms of computation. [sent-61, score-0.174]
</p><p>40 So we propose to deﬁne a ”generative simplicial complex”. [sent-62, score-0.157]
</p><p>41 However, this work being preliminary, we expose here the simpler case of deﬁning a ”generative graph” (a simplicial complex made only of vertices and edges) and tuning its parameters. [sent-63, score-0.228]
</p><p>42 This allows to demonstrate the feasibility of this approach and to foresee future difﬁculties when it is extended to simplicial complexes. [sent-64, score-0.157]
</p><p>43 Given a set of prototypes located over the data distribution using e. [sent-66, score-0.391]
</p><p>44 Vector Quantization [14], the Delaunay graph (DG) of the prototypes is constructed [15]. [sent-68, score-0.518]
</p><p>45 Then, each edge and each vertex of the graph is the basis of a generative model so that the graph generates a mixture of gaussian density functions. [sent-69, score-0.626]
</p><p>46 The maximization of the likelihood of the data wrt the model, using Expectation-Maximization, allows to tune the weights of this mixture and leads to the emergence of the expected topology representing graph through the edges with non-negligible weights that remain after the optimization process. [sent-70, score-0.93]
</p><p>47 1  A Generative Gaussian Graph to learn topology The Generative Gaussian Graph  In this work, M is the support of the probability density function (pdf) p from which are drawn the data v. [sent-74, score-0.492]
</p><p>48 In fact, this is not the topology of M which is of interest, but the topology of manifolds Mprin called ”principal manifolds” of the distribution p (in reference to the deﬁnition of Tibshirani [7]) which can be viewed as the manifold M without the noise. [sent-75, score-1.013]
</p><p>49 We assume the data have been generated by some set of points and segments constituting the set of manifolds Mprin which have been corrupted with additive spherical gaussian noise 2 with mean 0 and unknown variance σnoise . [sent-76, score-0.347]
</p><p>50 Then, we deﬁne a gaussian mixture model to account for the observed data, which is based on both gaussian kernels that we call ”gaussian-points”, and what we call ”gaussian-segments”, forming a ”Generative Gaussian Graph” (GGG). [sent-77, score-0.195]
</p><p>51 Thus, this is the integral of a gaussian-point along a line segment. [sent-79, score-0.047]
</p><p>52 In the case where wai = wbi , we set g 1 (vj , {wai , wbi }, σ) = g 0 (vj , wai , σ). [sent-81, score-0.604]
</p><p>53 The left part of the dot product accounts for the gaussian noise orthogonal to the line segment, and the right part for the gaussian noise integrated along the line segment. [sent-82, score-0.292]
</p><p>54 The functions g 0 and g 1 are positive and we can prove that: RD g 0 (v, wi , σ)dv = 1 and g 1 (v, {wa , wb }, σ)dv = 1, so they are both probability density functions. [sent-83, score-0.091]
</p><p>55 A gaussianRD point is associated to each prototype in w and a gaussian-segment to each edge in DG. [sent-84, score-0.108]
</p><p>56 πi ) is the probability that a datum v was drawn from the gaussian-point associated to wi (resp. [sent-87, score-0.159]
</p><p>57 the gaussian-segment associated to the ith edge of DG). [sent-88, score-0.076]
</p><p>58 2  Measure of quality  The function p(vj |π, w, σ, DG) is the probability density at vj given the parameters of the model. [sent-90, score-0.264]
</p><p>59 We measure the likelihood P of the data v wrt the parameters of the GGG model: M  P = P (π, w, σ, DG) =  p(vj |π, w, σ, DG)  (3)  j=1  2. [sent-91, score-0.151]
</p><p>60 3  The Expectation-Maximization algorithm  In order to maximize the likelihood P or equivalently to minimize the negative loglikelihood L = −log(P ) wrt π and σ, we use the Expectation-Maximization algorithm. [sent-92, score-0.151]
</p><p>61 The minimization of the negative log-likelihood consists in tmax iterative steps updating π and σ which ensure the decrease of L. [sent-94, score-0.071]
</p><p>62 4  Emerging topology by maximizing the likelihood  Finally, to get the topology representing graph from the generative model, the core idea is to prune from the initial DG the edges for which there is probability they generated the data. [sent-97, score-1.367]
</p><p>63 Initialize the location of the prototypes w using vector quantization [14]. [sent-99, score-0.413]
</p><p>64 Given w and DG, use updating rules (4) to ﬁnd σ 2∗ and π ∗ maximizing the likelihood P . [sent-105, score-0.068]
</p><p>65 Prune the edges {ai bi } of DG associated to the gaussian segments with 1 1 probability πi ≤ where πi ∈ π ∗ . [sent-107, score-0.363]
</p><p>66 The topology representing graph emerges from the edges with probabilities π ∗ > . [sent-108, score-0.763]
</p><p>67 It is the graph which best models the topology of the data in the sense of the maximum likelihood wrt π, σ, and the set of prototypes w and their Delaunay graph. [sent-109, score-1.102]
</p><p>68 3  Experiments  In these experiments, given a set of points and a set of prototypes located thanks to vector quantization [14], we want to verify the relevance of the GGG to learn the topology in various noise conditions. [sent-110, score-0.973]
</p><p>69 In the Figure 2, we show the comparison of the GGG to a CHL for which we ﬁlter out edges which have a number of hits lower than a threshold T . [sent-112, score-0.123]
</p><p>70 The data and prototypes are the same for both algorithms. [sent-113, score-0.362]
</p><p>71 We set T ∗ such that the graph obtained matches visually as close as possible the expected solution. [sent-114, score-0.156]
</p><p>72 8  (d)  Figure 1: Principle of the Generative Gaussian Graph: (a) Data drawn from an oblique segment,  an horizontal one and an isolated point with respective density {0. [sent-148, score-0.12]
</p><p>73 The prototypes are located at the extreme points of the segments, and at the isolated point. [sent-152, score-0.496]
</p><p>74 They are connected with edges from the Delaunay graph. [sent-153, score-0.127]
</p><p>75 (d) The edges of the optimal GGG associated to non-negligible probabilities model the topology of the data. [sent-156, score-0.586]
</p><p>76 4  Discussion  We propose that the problem of learning the topology of a set of points can be posed as a statistical learning problem: we assume that the topology of a statistical generative model of a set of points is an estimator of the topology of the principal manifold of this set. [sent-157, score-1.69]
</p><p>77 From this assumption, we deﬁne a topologically ﬂexible statistical generative mixture model that we call Generative Gaussian Graph from which we can extract the topology. [sent-158, score-0.261]
</p><p>78 The ﬁnal topology representing graph emerges from the edges with non-negligible probability. [sent-159, score-0.763]
</p><p>79 We propose to use the Delaunay graph as an initial graph assuming it is rich enough to contain as a subgraph a good topological model of the data. [sent-160, score-0.524]
</p><p>80 The use of the likelihood criterion makes possible cross-validation to select the best generative model hence the best topological model in terms of generalization capacities. [sent-161, score-0.386]
</p><p>81 In particular, it allows to take into account the noise and to model isolated bumps. [sent-163, score-0.135]
</p><p>82 Moreover, the likelihood of the data wrt the GGG is maximized during the learning, allowing to measure the quality of the model even when no visualization is possible. [sent-164, score-0.217]
</p><p>83 For some particular data distributions where all the data lie on the Delaunay line segments, no maximum of the likelihood exists. [sent-165, score-0.091]
</p><p>84 This case is not a problem because σ = 0 effectively deﬁnes a good solution (no noise in a data set drawn from a graph). [sent-166, score-0.079]
</p><p>85 If only some of the data lie exactly on the line segments, a maximum of the likelihood still exists because σ 2 deﬁnes the variance for all the generative gaussian points and segments at the same time so it cannot vanish to 0. [sent-167, score-0.4]
</p><p>86 The 3 computing time complexity of the GGG is o(D(N0 + N1 )M tmax ) plus the time O(DN0 ) [15] needed to build the Delaunay graph which dominates the overall worst time complexity. [sent-168, score-0.203]
</p><p>87 As in general, the CHL builds too much edges than needed to model the topology, it would be interesting to use the Delaunay subgraph obtained with the CHL as a starting point for the GGG model. [sent-170, score-0.16]
</p><p>88 The Generative Gaussian Graph can be viewed as a generalization of gaussian mixtures to points and segments: a gaussian mixture is a GGG with no edge. [sent-171, score-0.175]
</p><p>89 In contrast, other generative models do not provide any insight about the topology of the data, except the Generative Topographic Map (GTM) [4], the revisited Principal Manifolds [7] or the mixture of Probabilistics Principal Component Analysers (PPCA) [8]. [sent-173, score-0.603]
</p><p>90 However, in the two former cases, the intrinsic dimension of the model is ﬁxed a priori and  σnoise = 0. [sent-174, score-0.142]
</p><p>91 5  1  (i) CHL: T = 58 ∗  Figure 2: Learning the topology of a data set: 600 data drawn from a spirale and an isolated point  2 corrupted with additive gaussian noise with mean 0 and variance σ noise . [sent-236, score-0.715]
</p><p>92 (a-c) The edges of the GGG with weights greater than allow to recover the topology of the principal manifolds except for large noise variance (c) where a triangle was created at the center of the spirale. [sent-238, score-0.778]
</p><p>93 σ ∗ over-estimates σnoise because the model is piecewise linear while the true manifolds are non-linear. [sent-239, score-0.112]
</p><p>94 (d-f) The CHL without threshold (T=0) is not able to recover the true topology of the data for even small σnoise . [sent-240, score-0.474]
</p><p>95 The grey cells correspond to ROI of the edges (darker cells contain more data). [sent-242, score-0.15]
</p><p>96 It shows these cells are not intuitively related to the edges they are associated to (e. [sent-243, score-0.157]
</p><p>97 (g-h) The CHL with a threshold T allows to recover the topology of the data only for small noise variance (g) (Notice T1 < T2 ⇒ DGCHL (T2 ) ⊆ DGCHL (T1 )). [sent-246, score-0.55]
</p><p>98 not learned from the data, while in the latter the local intrinsic dimension is learned but the connectedness between the local models is not. [sent-248, score-0.158]
</p><p>99 One obvious way to follow to extend this work is considering a simplicial complex in place of the graph to get the full topological information extractible. [sent-249, score-0.496]
</p><p>100 “neural-gas” network for vector quantization and its application to time-series prediction. [sent-351, score-0.051]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('topology', 0.433), ('prototypes', 0.362), ('ggg', 0.307), ('chl', 0.271), ('delaunay', 0.271), ('vj', 0.21), ('dg', 0.173), ('simplicial', 0.157), ('wai', 0.157), ('graph', 0.156), ('topological', 0.154), ('wbi', 0.145), ('generative', 0.129), ('bi', 0.116), ('wrt', 0.107), ('edges', 0.102), ('roi', 0.094), ('manifolds', 0.092), ('segment', 0.073), ('connectedness', 0.072), ('martinetz', 0.072), ('vorono', 0.072), ('qj', 0.072), ('segments', 0.069), ('lai', 0.067), ('isolated', 0.061), ('ai', 0.058), ('erf', 0.057), ('wi', 0.057), ('manifold', 0.055), ('principal', 0.055), ('noise', 0.054), ('aupetit', 0.054), ('edelsbrunner', 0.054), ('simplices', 0.054), ('quantization', 0.051), ('intrinsic', 0.048), ('bump', 0.047), ('tmax', 0.047), ('representing', 0.047), ('line', 0.047), ('datum', 0.046), ('gaussian', 0.045), ('edge', 0.045), ('points', 0.044), ('likelihood', 0.044), ('vertices', 0.042), ('mixture', 0.041), ('rd', 0.04), ('modelling', 0.04), ('dimension', 0.038), ('subgraph', 0.038), ('priori', 0.036), ('belgium', 0.036), ('betti', 0.036), ('bruges', 0.036), ('dgchl', 0.036), ('extractible', 0.036), ('mprin', 0.036), ('schulten', 0.036), ('witness', 0.036), ('density', 0.034), ('topographic', 0.033), ('prototype', 0.032), ('gtm', 0.031), ('associated', 0.031), ('silva', 0.031), ('located', 0.029), ('complex', 0.029), ('elsevier', 0.029), ('extract', 0.027), ('dv', 0.027), ('geometrical', 0.027), ('grounded', 0.027), ('visualization', 0.026), ('drawn', 0.025), ('eds', 0.025), ('emerges', 0.025), ('connected', 0.025), ('symposium', 0.025), ('cells', 0.024), ('updating', 0.024), ('connect', 0.023), ('simplex', 0.023), ('prune', 0.023), ('call', 0.022), ('la', 0.022), ('emerging', 0.022), ('statistical', 0.022), ('variance', 0.022), ('threshold', 0.021), ('hebbian', 0.021), ('qi', 0.021), ('corrupted', 0.021), ('quality', 0.02), ('recover', 0.02), ('model', 0.02), ('criterion', 0.019), ('deal', 0.019), ('exp', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="116-tfidf-1" href="./nips-2005-Learning_Topology_with_the_Generative_Gaussian_Graph_and_the_EM_Algorithm.html">116 nips-2005-Learning Topology with the Generative Gaussian Graph and the EM Algorithm</a></p>
<p>Author: Michaël Aupetit</p><p>Abstract: Given a set of points and a set of prototypes representing them, how to create a graph of the prototypes whose topology accounts for that of the points? This problem had not yet been explored in the framework of statistical learning theory. In this work, we propose a generative model based on the Delaunay graph of the prototypes and the ExpectationMaximization algorithm to learn the parameters. This work is a ﬁrst step towards the construction of a topological model of a set of points grounded on statistics. 1 1.1</p><p>2 0.13599508 <a title="116-tfidf-2" href="./nips-2005-Beyond_Gaussian_Processes%3A_On_the_Distributions_of_Infinite_Networks.html">38 nips-2005-Beyond Gaussian Processes: On the Distributions of Infinite Networks</a></p>
<p>Author: Ricky Der, Daniel D. Lee</p><p>Abstract: A general analysis of the limiting distribution of neural network functions is performed, with emphasis on non-Gaussian limits. We show that with i.i.d. symmetric stable output weights, and more generally with weights distributed from the normal domain of attraction of a stable variable, that the neural functions converge in distribution to stable processes. Conditions are also investigated under which Gaussian limits do occur when the weights are independent but not identically distributed. Some particularly tractable classes of stable distributions are examined, and the possibility of learning with such processes.</p><p>3 0.10473528 <a title="116-tfidf-3" href="./nips-2005-Soft_Clustering_on_Graphs.html">178 nips-2005-Soft Clustering on Graphs</a></p>
<p>Author: Kai Yu, Shipeng Yu, Volker Tresp</p><p>Abstract: We propose a simple clustering framework on graphs encoding pairwise data similarities. Unlike usual similarity-based methods, the approach softly assigns data to clusters in a probabilistic way. More importantly, a hierarchical clustering is naturally derived in this framework to gradually merge lower-level clusters into higher-level ones. A random walk analysis indicates that the algorithm exposes clustering structures in various resolutions, i.e., a higher level statistically models a longer-term diffusion on graphs and thus discovers a more global clustering structure. Finally we provide very encouraging experimental results. 1</p><p>4 0.098195009 <a title="116-tfidf-4" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>Author: Tong Zhang, Rie Kubota Ando</p><p>Abstract: We consider a framework for semi-supervised learning using spectral decomposition based un-supervised kernel design. This approach subsumes a class of previously proposed semi-supervised learning methods on data graphs. We examine various theoretical properties of such methods. In particular, we derive a generalization performance bound, and obtain the optimal kernel design by minimizing the bound. Based on the theoretical analysis, we are able to demonstrate why spectral kernel design based methods can often improve the predictive performance. Experiments are used to illustrate the main consequences of our analysis.</p><p>5 0.08491908 <a title="116-tfidf-5" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>Author: Andreas Argyriou, Mark Herbster, Massimiliano Pontil</p><p>Abstract: A foundational problem in semi-supervised learning is the construction of a graph underlying the data. We propose to use a method which optimally combines a number of differently constructed graphs. For each of these graphs we associate a basic graph kernel. We then compute an optimal combined kernel. This kernel solves an extended regularization problem which requires a joint minimization over both the data and the set of graph kernels. We present encouraging results on different OCR tasks where the optimal combined kernel is computed from graphs constructed with a variety of distances functions and the ‘k’ in nearest neighbors. 1</p><p>6 0.080835819 <a title="116-tfidf-6" href="./nips-2005-Non-Local_Manifold_Parzen_Windows.html">138 nips-2005-Non-Local Manifold Parzen Windows</a></p>
<p>7 0.067085132 <a title="116-tfidf-7" href="./nips-2005-Value_Function_Approximation_with_Diffusion_Wavelets_and_Laplacian_Eigenfunctions.html">199 nips-2005-Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions</a></p>
<p>8 0.059572656 <a title="116-tfidf-8" href="./nips-2005-Large_scale_networks_fingerprinting_and_visualization_using_the_k-core_decomposition.html">107 nips-2005-Large scale networks fingerprinting and visualization using the k-core decomposition</a></p>
<p>9 0.057555672 <a title="116-tfidf-9" href="./nips-2005-Estimation_of_Intrinsic_Dimensionality_Using_High-Rate_Vector_Quantization.html">66 nips-2005-Estimation of Intrinsic Dimensionality Using High-Rate Vector Quantization</a></p>
<p>10 0.057012103 <a title="116-tfidf-10" href="./nips-2005-Selecting_Landmark_Points_for_Sparse_Manifold_Learning.html">172 nips-2005-Selecting Landmark Points for Sparse Manifold Learning</a></p>
<p>11 0.054008007 <a title="116-tfidf-11" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>12 0.053981073 <a title="116-tfidf-12" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>13 0.052721951 <a title="116-tfidf-13" href="./nips-2005-Coarse_sample_complexity_bounds_for_active_learning.html">41 nips-2005-Coarse sample complexity bounds for active learning</a></p>
<p>14 0.049372479 <a title="116-tfidf-14" href="./nips-2005-Augmented_Rescorla-Wagner_and_Maximum_Likelihood_Estimation.html">32 nips-2005-Augmented Rescorla-Wagner and Maximum Likelihood Estimation</a></p>
<p>15 0.048319101 <a title="116-tfidf-15" href="./nips-2005-Fixing_two_weaknesses_of_the_Spectral_Method.html">75 nips-2005-Fixing two weaknesses of the Spectral Method</a></p>
<p>16 0.047083255 <a title="116-tfidf-16" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>17 0.046563078 <a title="116-tfidf-17" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>18 0.045829892 <a title="116-tfidf-18" href="./nips-2005-Maximum_Margin_Semi-Supervised_Learning_for_Structured_Variables.html">123 nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</a></p>
<p>19 0.045521617 <a title="116-tfidf-19" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>20 0.04452249 <a title="116-tfidf-20" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.152), (1, 0.056), (2, -0.038), (3, -0.034), (4, -0.101), (5, -0.014), (6, -0.001), (7, -0.039), (8, 0.037), (9, 0.005), (10, 0.051), (11, 0.04), (12, 0.018), (13, 0.03), (14, -0.054), (15, -0.023), (16, -0.015), (17, 0.163), (18, 0.108), (19, 0.122), (20, -0.036), (21, 0.138), (22, -0.237), (23, 0.055), (24, -0.026), (25, -0.035), (26, -0.052), (27, -0.019), (28, -0.022), (29, -0.002), (30, -0.011), (31, 0.135), (32, -0.071), (33, -0.024), (34, 0.084), (35, 0.047), (36, -0.057), (37, -0.056), (38, 0.12), (39, -0.026), (40, -0.006), (41, 0.084), (42, -0.04), (43, 0.033), (44, 0.049), (45, -0.072), (46, -0.142), (47, -0.15), (48, -0.089), (49, -0.111)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94846058 <a title="116-lsi-1" href="./nips-2005-Learning_Topology_with_the_Generative_Gaussian_Graph_and_the_EM_Algorithm.html">116 nips-2005-Learning Topology with the Generative Gaussian Graph and the EM Algorithm</a></p>
<p>Author: Michaël Aupetit</p><p>Abstract: Given a set of points and a set of prototypes representing them, how to create a graph of the prototypes whose topology accounts for that of the points? This problem had not yet been explored in the framework of statistical learning theory. In this work, we propose a generative model based on the Delaunay graph of the prototypes and the ExpectationMaximization algorithm to learn the parameters. This work is a ﬁrst step towards the construction of a topological model of a set of points grounded on statistics. 1 1.1</p><p>2 0.67374885 <a title="116-lsi-2" href="./nips-2005-Large_scale_networks_fingerprinting_and_visualization_using_the_k-core_decomposition.html">107 nips-2005-Large scale networks fingerprinting and visualization using the k-core decomposition</a></p>
<p>Author: J. I. Alvarez-hamelin, Luca Dall'asta, Alain Barrat, Alessandro Vespignani</p><p>Abstract: We use the k-core decomposition to develop algorithms for the analysis of large scale complex networks. This decomposition, based on a recursive pruning of the least connected vertices, allows to disentangle the hierarchical structure of networks by progressively focusing on their central cores. By using this strategy we develop a general visualization algorithm that can be used to compare the structural properties of various networks and highlight their hierarchical structure. The low computational complexity of the algorithm, O(n + e), where n is the size of the network, and e is the number of edges, makes it suitable for the visualization of very large sparse networks. We show how the proposed visualization tool allows to ﬁnd speciﬁc structural ﬁngerprints of networks. 1</p><p>3 0.55783433 <a title="116-lsi-3" href="./nips-2005-Beyond_Gaussian_Processes%3A_On_the_Distributions_of_Infinite_Networks.html">38 nips-2005-Beyond Gaussian Processes: On the Distributions of Infinite Networks</a></p>
<p>Author: Ricky Der, Daniel D. Lee</p><p>Abstract: A general analysis of the limiting distribution of neural network functions is performed, with emphasis on non-Gaussian limits. We show that with i.i.d. symmetric stable output weights, and more generally with weights distributed from the normal domain of attraction of a stable variable, that the neural functions converge in distribution to stable processes. Conditions are also investigated under which Gaussian limits do occur when the weights are independent but not identically distributed. Some particularly tractable classes of stable distributions are examined, and the possibility of learning with such processes.</p><p>4 0.45487306 <a title="116-lsi-4" href="./nips-2005-Non-Local_Manifold_Parzen_Windows.html">138 nips-2005-Non-Local Manifold Parzen Windows</a></p>
<p>Author: Yoshua Bengio, Hugo Larochelle, Pascal Vincent</p><p>Abstract: To escape from the curse of dimensionality, we claim that one can learn non-local functions, in the sense that the value and shape of the learned function at x must be inferred using examples that may be far from x. With this objective, we present a non-local non-parametric density estimator. It builds upon previously proposed Gaussian mixture models with regularized covariance matrices to take into account the local shape of the manifold. It also builds upon recent work on non-local estimators of the tangent plane of a manifold, which are able to generalize in places with little training data, unlike traditional, local, non-parametric models.</p><p>5 0.45261115 <a title="116-lsi-5" href="./nips-2005-Soft_Clustering_on_Graphs.html">178 nips-2005-Soft Clustering on Graphs</a></p>
<p>Author: Kai Yu, Shipeng Yu, Volker Tresp</p><p>Abstract: We propose a simple clustering framework on graphs encoding pairwise data similarities. Unlike usual similarity-based methods, the approach softly assigns data to clusters in a probabilistic way. More importantly, a hierarchical clustering is naturally derived in this framework to gradually merge lower-level clusters into higher-level ones. A random walk analysis indicates that the algorithm exposes clustering structures in various resolutions, i.e., a higher level statistically models a longer-term diffusion on graphs and thus discovers a more global clustering structure. Finally we provide very encouraging experimental results. 1</p><p>6 0.4492065 <a title="116-lsi-6" href="./nips-2005-Augmented_Rescorla-Wagner_and_Maximum_Likelihood_Estimation.html">32 nips-2005-Augmented Rescorla-Wagner and Maximum Likelihood Estimation</a></p>
<p>7 0.38381243 <a title="116-lsi-7" href="./nips-2005-Active_Learning_for_Misspecified_Models.html">19 nips-2005-Active Learning for Misspecified Models</a></p>
<p>8 0.38056153 <a title="116-lsi-8" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>9 0.37146559 <a title="116-lsi-9" href="./nips-2005-Estimation_of_Intrinsic_Dimensionality_Using_High-Rate_Vector_Quantization.html">66 nips-2005-Estimation of Intrinsic Dimensionality Using High-Rate Vector Quantization</a></p>
<p>10 0.36076921 <a title="116-lsi-10" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>11 0.35060066 <a title="116-lsi-11" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>12 0.34110975 <a title="116-lsi-12" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>13 0.32253563 <a title="116-lsi-13" href="./nips-2005-Selecting_Landmark_Points_for_Sparse_Manifold_Learning.html">172 nips-2005-Selecting Landmark Points for Sparse Manifold Learning</a></p>
<p>14 0.32033309 <a title="116-lsi-14" href="./nips-2005-Bayesian_model_learning_in_human_visual_perception.html">35 nips-2005-Bayesian model learning in human visual perception</a></p>
<p>15 0.30287299 <a title="116-lsi-15" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>16 0.29937431 <a title="116-lsi-16" href="./nips-2005-Modeling_Neuronal_Interactivity_using_Dynamic_Bayesian_Networks.html">130 nips-2005-Modeling Neuronal Interactivity using Dynamic Bayesian Networks</a></p>
<p>17 0.29734871 <a title="116-lsi-17" href="./nips-2005-Efficient_Estimation_of_OOMs.html">62 nips-2005-Efficient Estimation of OOMs</a></p>
<p>18 0.29713866 <a title="116-lsi-18" href="./nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<p>19 0.29184869 <a title="116-lsi-19" href="./nips-2005-Walk-Sum_Interpretation_and_Analysis_of_Gaussian_Belief_Propagation.html">204 nips-2005-Walk-Sum Interpretation and Analysis of Gaussian Belief Propagation</a></p>
<p>20 0.28228167 <a title="116-lsi-20" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.041), (10, 0.036), (12, 0.353), (27, 0.03), (31, 0.066), (34, 0.09), (41, 0.017), (50, 0.012), (55, 0.011), (57, 0.016), (65, 0.032), (69, 0.052), (73, 0.016), (88, 0.078), (91, 0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94282454 <a title="116-lda-1" href="./nips-2005-Neuronal_Fiber_Delineation_in_Area_of_Edema_from_Diffusion_Weighted_MRI.html">135 nips-2005-Neuronal Fiber Delineation in Area of Edema from Diffusion Weighted MRI</a></p>
<p>Author: Ofer Pasternak, Nathan Intrator, Nir Sochen, Yaniv Assaf</p><p>Abstract: Diffusion Tensor Magnetic Resonance Imaging (DT-MRI) is a non invasive method for brain neuronal ﬁbers delineation. Here we show a modiﬁcation for DT-MRI that allows delineation of neuronal ﬁbers which are inﬁltrated by edema. We use the Muliple Tensor Variational (MTV) framework which replaces the diffusion model of DT-MRI with a multiple component model and ﬁts it to the signal attenuation with a variational regularization mechanism. In order to reduce free water contamination we estimate the free water compartment volume fraction in each voxel, remove it, and then calculate the anisotropy of the remaining compartment. The variational framework was applied on data collected with conventional clinical parameters, containing only six diffusion directions. By using the variational framework we were able to overcome the highly ill posed ﬁtting. The results show that we were able to ﬁnd ﬁbers that were not found by DT-MRI.</p><p>2 0.77108997 <a title="116-lda-2" href="./nips-2005-Generalized_Nonnegative_Matrix_Approximations_with_Bregman_Divergences.html">86 nips-2005-Generalized Nonnegative Matrix Approximations with Bregman Divergences</a></p>
<p>Author: Suvrit Sra, Inderjit S. Dhillon</p><p>Abstract: Nonnegative matrix approximation (NNMA) is a recent technique for dimensionality reduction and data analysis that yields a parts based, sparse nonnegative representation for nonnegative input data. NNMA has found a wide variety of applications, including text analysis, document clustering, face/image recognition, language modeling, speech processing and many others. Despite these numerous applications, the algorithmic development for computing the NNMA factors has been relatively deﬁcient. This paper makes algorithmic progress by modeling and solving (using multiplicative updates) new generalized NNMA problems that minimize Bregman divergences between the input matrix and its lowrank approximation. The multiplicative update formulae in the pioneering work by Lee and Seung [11] arise as a special case of our algorithms. In addition, the paper shows how to use penalty functions for incorporating constraints other than nonnegativity into the problem. Further, some interesting extensions to the use of “link” functions for modeling nonlinear relationships are also discussed. 1</p><p>same-paper 3 0.77085787 <a title="116-lda-3" href="./nips-2005-Learning_Topology_with_the_Generative_Gaussian_Graph_and_the_EM_Algorithm.html">116 nips-2005-Learning Topology with the Generative Gaussian Graph and the EM Algorithm</a></p>
<p>Author: Michaël Aupetit</p><p>Abstract: Given a set of points and a set of prototypes representing them, how to create a graph of the prototypes whose topology accounts for that of the points? This problem had not yet been explored in the framework of statistical learning theory. In this work, we propose a generative model based on the Delaunay graph of the prototypes and the ExpectationMaximization algorithm to learn the parameters. This work is a ﬁrst step towards the construction of a topological model of a set of points grounded on statistics. 1 1.1</p><p>4 0.68365484 <a title="116-lda-4" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<p>Author: Inna Weiner, Tomer Hertz, Israel Nelken, Daphna Weinshall</p><p>Abstract: We present a novel approach to the characterization of complex sensory neurons. One of the main goals of characterizing sensory neurons is to characterize dimensions in stimulus space to which the neurons are highly sensitive (causing large gradients in the neural responses) or alternatively dimensions in stimulus space to which the neuronal response are invariant (deﬁning iso-response manifolds). We formulate this problem as that of learning a geometry on stimulus space that is compatible with the neural responses: the distance between stimuli should be large when the responses they evoke are very different, and small when the responses they evoke are similar. Here we show how to successfully train such distance functions using rather limited amount of information. The data consisted of the responses of neurons in primary auditory cortex (A1) of anesthetized cats to 32 stimuli derived from natural sounds. For each neuron, a subset of all pairs of stimuli was selected such that the responses of the two stimuli in a pair were either very similar or very dissimilar. The distance function was trained to ﬁt these constraints. The resulting distance functions generalized to predict the distances between the responses of a test stimulus and the trained stimuli. 1</p><p>5 0.42098507 <a title="116-lda-5" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>Author: Doron Blatt, Alfred O. Hero</p><p>Abstract: This paper proposes an algorithm to convert a T -stage stochastic decision problem with a continuous state space to a sequence of supervised learning problems. The optimization problem associated with the trajectory tree and random trajectory methods of Kearns, Mansour, and Ng, 2000, is solved using the Gauss-Seidel method. The algorithm breaks a multistage reinforcement learning problem into a sequence of single-stage reinforcement learning subproblems, each of which is solved via an exact reduction to a weighted-classiﬁcation problem that can be solved using off-the-self methods. Thus the algorithm converts a reinforcement learning problem into simpler supervised learning subproblems. It is shown that the method converges in a ﬁnite number of steps to a solution that cannot be further improved by componentwise optimization. The implication of the proposed algorithm is that a plethora of classiﬁcation methods can be applied to ﬁnd policies in the reinforcement learning problem. 1</p><p>6 0.41007075 <a title="116-lda-6" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>7 0.40752396 <a title="116-lda-7" href="./nips-2005-Augmented_Rescorla-Wagner_and_Maximum_Likelihood_Estimation.html">32 nips-2005-Augmented Rescorla-Wagner and Maximum Likelihood Estimation</a></p>
<p>8 0.40712342 <a title="116-lda-8" href="./nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs.html">90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</a></p>
<p>9 0.40694982 <a title="116-lda-9" href="./nips-2005-Identifying_Distributed_Object_Representations_in_Human_Extrastriate_Visual_Cortex.html">94 nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</a></p>
<p>10 0.40693393 <a title="116-lda-10" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>11 0.40547591 <a title="116-lda-11" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>12 0.40371311 <a title="116-lda-12" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>13 0.40137568 <a title="116-lda-13" href="./nips-2005-Value_Function_Approximation_with_Diffusion_Wavelets_and_Laplacian_Eigenfunctions.html">199 nips-2005-Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions</a></p>
<p>14 0.40131396 <a title="116-lda-14" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>15 0.40068391 <a title="116-lda-15" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>16 0.40054286 <a title="116-lda-16" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>17 0.39906412 <a title="116-lda-17" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>18 0.39823928 <a title="116-lda-18" href="./nips-2005-Selecting_Landmark_Points_for_Sparse_Manifold_Learning.html">172 nips-2005-Selecting Landmark Points for Sparse Manifold Learning</a></p>
<p>19 0.39633754 <a title="116-lda-19" href="./nips-2005-An_Application_of_Markov_Random_Fields_to_Range_Sensing.html">23 nips-2005-An Application of Markov Random Fields to Range Sensing</a></p>
<p>20 0.39453378 <a title="116-lda-20" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
