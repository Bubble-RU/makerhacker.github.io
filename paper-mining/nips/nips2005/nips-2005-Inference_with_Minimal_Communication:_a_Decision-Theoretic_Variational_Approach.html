<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-96" href="#">nips2005-96</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</h1>
<br/><p>Source: <a title="nips-2005-96-pdf" href="http://papers.nips.cc/paper/2811-inference-with-minimal-communication-a-decision-theoretic-variational-approach.pdf">pdf</a></p><p>Author: O. P. Kreidl, Alan S. Willsky</p><p>Abstract: Given a directed graphical model with binary-valued hidden nodes and real-valued noisy observations, consider deciding upon the maximum a-posteriori (MAP) or the maximum posterior-marginal (MPM) assignment under the restriction that each node broadcasts only to its children exactly one single-bit message. We present a variational formulation, viewing the processing rules local to all nodes as degrees-of-freedom, that minimizes the loss in expected (MAP or MPM) performance subject to such online communication constraints. The approach leads to a novel message-passing algorithm to be executed ofﬂine, or before observations are realized, which mitigates the performance loss by iteratively coupling all rules in a manner implicitly driven by global statistics. We also provide (i) illustrative examples, (ii) assumptions that guarantee convergence and efﬁciency and (iii) connections to active research areas. 1</p><p>Reference: <a title="nips-2005-96-reference" href="../nips2005_reference/nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We present a variational formulation, viewing the processing rules local to all nodes as degrees-of-freedom, that minimizes the loss in expected (MAP or MPM) performance subject to such online communication constraints. [sent-5, score-0.565]
</p><p>2 The approach leads to a novel message-passing algorithm to be executed ofﬂine, or before observations are realized, which mitigates the performance loss by iteratively coupling all rules in a manner implicitly driven by global statistics. [sent-6, score-0.122]
</p><p>3 We also provide (i) illustrative examples, (ii) assumptions that guarantee convergence and efﬁciency and (iii) connections to active research areas. [sent-7, score-0.096]
</p><p>4 1  Introduction  Given a probabilistic model with discrete-valued hidden variables, Belief Propagation (BP) and related graph-based algorithms are commonly employed to solve for the Maximum APosteriori (MAP) assignment (i. [sent-8, score-0.164]
</p><p>5 , the mode of the joint distribution of all hidden variables) and Maximum-Posterior-Marginal (MPM) assignment (i. [sent-10, score-0.164]
</p><p>6 , the modes of the marginal distributions of every hidden variable) [1]. [sent-12, score-0.097]
</p><p>7 Speciﬁcally, each computation event corresponds to a node evaluating its local processing rule, or a function by which all messages received in the preceding communication event map to messages sent in the next communication event. [sent-14, score-1.057]
</p><p>8 Practically, the viability of BP appears to rest upon an implicit assumption that network communication resources are abundant. [sent-15, score-0.326]
</p><p>9 In a general network, because termination of the algorithm is in question, the required communication resources are a-priori unbounded. [sent-16, score-0.21]
</p><p>10 Even when termination can be guaranteed, transmission of exact messages presumes communication channels with inﬁnite capacity (in bits per observation), or at least of sufﬁciently high bandwidth such that the resulting ﬁnite message precision is essentially error-free. [sent-17, score-0.507]
</p><p>11 , energy-limited wireless sensor networks), it may be prohibitively costly to justify such idealized online communications. [sent-20, score-0.054]
</p><p>12 While recent evidence suggests substantial but “small-enough” message errors will not alter the behavior of BP [2], [3], it also suggests BP may perform poorly when communication is very constrained. [sent-21, score-0.226]
</p><p>13 Assuming communication constraints are severe, we examine the extent to which alternative processing rules can avoid a loss in (MAP or MPM) performance. [sent-22, score-0.325]
</p><p>14 Speciﬁcally, given a directed graphical model with binary-valued hidden variables and real-valued noisy observations, we assume each node may broadcast only to its children a single binary-valued message. [sent-23, score-0.573]
</p><p>15 We cast the problem within a variational formulation [4], seeking to minimize a decision-theoretic penalty function subject to such online communication constraints. [sent-24, score-0.423]
</p><p>16 To our knowledge, that this relaxation permits analytical progress given any directed acyclic network is new. [sent-28, score-0.354]
</p><p>17 Moreover, for MPM assignment in a tree-structured network, we discover an added convenience with respect to the envisioned distributed processor setting: the ofﬂine computation itself admits an efﬁcient message-passing interpretation. [sent-29, score-0.107]
</p><p>18 Section 2 details the decision-theoretic variational formulation for discrete-variable assignment. [sent-31, score-0.113]
</p><p>19 Section 3 summarizes the main results derived from its connection to decentralized detection, culminating in the ofﬂine message-passing algorithm and the assumptions that guarantee convergence and maximal efﬁciency. [sent-32, score-0.156]
</p><p>20 The ensuing optimization problem is expressed by J(γ ∗ ) = min J(γ) subject to γ ∈ ΓG , (1) γ∈Γ  ∗  where γ then represents an optimal network-constrained strategy for discrete-variable assignment. [sent-36, score-0.132]
</p><p>21 1  Decision-Theoretic Penalty Function  Let U = γ(Y ) denote the decision process induced from the observation process Y by any candidate assignment strategy γ ∈ Γ. [sent-39, score-0.35]
</p><p>22 If we associate a numeric “cost” c(u, x) to every possible joint realization of (U, X), then the expected cost is a well-posed penalty function: J(γ) = E [c (γ(Y ), X)] = E [E [c(γ(Y ), X) | Y ]] . [sent-40, score-0.191]
</p><p>23 (2) Expanding the inner expectation and recognizing p(x|y) to be proportional to p(x)p(y|x) for every y such that p(y) > 0, it follows that γ ∗ minimizes (2) over Γ if and only if ¯ γ ∗ (Y ) = arg ¯  min  u∈{0,1}N  x∈{0,1}N  p(x)c(u, x)p(Y |x)  with probability one. [sent-41, score-0.113]
</p><p>24 (3)  Of note are (i) the likelihood function p(Y |x) is a ﬁnite-dimensional sufﬁcient statistic of Y , (ii) real-valued coefﬁcients ¯ x) provide a ﬁnite parameterization of the function b(u, space Γ and (iii) optimal coefﬁcient values ¯∗ (u, x) = p(x)c(u, x) are computable ofﬂine. [sent-42, score-0.092]
</p><p>25 b  Before introducing communication constraints, we illustrate by examples how the decisiontheoretic penalty function relates to familiar discrete-variable assignment problems. [sent-43, score-0.363]
</p><p>26 Then (2) and (3) specialize to, respectively, the word error rate (viewing each x as an N -bit word) and the MAP strategy: γ ∗ (Y ) = arg max p(x|Y ) with probability one. [sent-45, score-0.082]
</p><p>27 ¯ x∈{0,1}N  N  Example 2: Let c(u, x) = n=1 cn (un , xn ), where each cn indicates whether un = xn . [sent-46, score-1.114]
</p><p>28 Then (2) and (3) specialize to, respectively, the bit error rate and the MPM strategy: γ ∗ (Y ) = ¯ 2. [sent-47, score-0.079]
</p><p>29 , arg max p(xN |Y ) x1 ∈{0,1}  xN ∈{0,1}  with probability one. [sent-51, score-0.037]
</p><p>30 Network Communication Constraints  Let G(V, E) be any directed acyclic graph with vertex set V = {1, . [sent-52, score-0.185]
</p><p>31 , N } and edge set E = {(i, j) ∈ V × V | i ∈ π(j) ⇔ j ∈ χ(i)}, where index sets π(n) ⊂ V and χ(n) ⊂ V indicate, respectively, the parents and children of each node n ∈ V. [sent-55, score-0.359]
</p><p>32 Without loss-of-generality, we assume the node labels respect the natural partial-order implied by the graph G; speciﬁcally, we assume every node n has parent nodes π(n) ⊂ {1, . [sent-56, score-0.625]
</p><p>33 Local to each node n ∈ V are the respective components Xn and Yn of the joint process (X, Y ). [sent-63, score-0.224]
</p><p>34 , max-product in Example 1, sum-product in Example 2) require at least 2|E| real-valued messages per observation Y = y, one per direction along each edge in G. [sent-66, score-0.241]
</p><p>35 In contrast, we insist upon a single forward-pass through G where each node n broadcasts to its children (if any) a single binary-valued message. [sent-67, score-0.413]
</p><p>36 This yields communication overhead of only |E| bits per observation Y = y, but also renders the minimizing strategy of (3) infeasible. [sent-68, score-0.376]
</p><p>37 γ Speciﬁcally, we now translate the stipulated restrictions on communication into explicit constraints on the function space Γ over which to minimize (2). [sent-70, score-0.25]
</p><p>38 The simplest such translation assumes the binary-valued message produced by node n also determines the respective component un in decision vector u = γ(y). [sent-71, score-0.946]
</p><p>39 Recognizing that every node n receives the messages uπ(n) from its parents (if any) as side information to yn , any function of the form γn : R × {0, 1}|π(n)| → {0, 1} is a feasible processing rule; we denote the set of all such rules by Γn . [sent-72, score-0.849]
</p><p>40 Then, every strategy in the set ΓG = Γ1 × · · · × ΓN respects the constraints. [sent-73, score-0.172]
</p><p>41 3  Summary of Main Results  As stated in Section 1, the variational formulation presented in Section 2 can be viewed as an extension of the optimization problem underlying decentralized Bayesian detection [5], [6]. [sent-74, score-0.242]
</p><p>42 , the N -node chain), it is known that exact solution to (1) is NP-hard, stemming from the absence of a guarantee that γ ∗ ∈ ΓG possesses a ﬁnite parameterization. [sent-77, score-0.036]
</p><p>43 Also known is that analytical progress can be made for a ∗ ∗ relaxation of (1), which is based on the following intuition: if strategy γ ∗ = (γ1 , . [sent-78, score-0.256]
</p><p>44 , γN ) G is optimal over Γ , then for each n and assuming all components i ∈ V\n are ﬁxed at ∗ ∗ rules γi , the component rule γn must be optimal over Γn . [sent-81, score-0.23]
</p><p>45 Decentralized detection has roots in team decision theory [7], a subset of game theory, in which the relaxation is named person-by-person (pbp) optimality. [sent-82, score-0.169]
</p><p>46 Nonetheless, pbp-optimality (along with a specialized observation process) justiﬁes a particular ﬁnite parameterization for the function space ΓG , leading to a nonlinear ﬁxed-point equation and an iterative algorithm with favorable convergence properties. [sent-84, score-0.174]
</p><p>47 Example 3: Consider the MPM assignment problem in Example 2, assuming N = 2 and distribution p(x, y) is deﬁned by positive-valued parameters α, β1 and β2 as follows: p(x) ∝  1 α  , x1 = x2 , x1 = x2  N  and  p(y|x) =  (y − βn xn )2 1 √ exp − n . [sent-86, score-0.272]
</p><p>48 2 2π n=1  Note that X1 and X2 are marginally uniform and α captures their correlation (positive, zero, or negative when α is less than, equal to, or greater than unity, respectively), while Y captures the presence of additive white Gaussian noise with signal-to-noise ratio at node n equal to βn . [sent-87, score-0.224]
</p><p>49 , as if the myopic decision by node 1 is always correct). [sent-91, score-0.344]
</p><p>50 Figure 1 compares these strategies and a pbp-optimal strategy γ k —only γ k is both feasible and consistently “hedging” against all uncertainty i. [sent-92, score-0.207]
</p><p>51 γ  Example 4: Extend Example 3 to N > 2 nodes, but assuming X is equally-likely to be all zeros or all ones (i. [sent-106, score-0.074]
</p><p>52 The MPM strategy employs thresholds ηn = i∈V\n 1/Li (yi ) for all n, leading to U = γ ∗ (Y ) also being all zeros or all ones; ¯∗ ¯ thus, its cost distribution, or the probability mass function for c(¯ ∗ (Y ), X), has mass only γ 0 on the values 0 and N . [sent-109, score-0.39]
</p><p>53 The myopic strategy employs thresholds ηn = 1 for all n, leading to 0 independent and identically-distributed (binary-valued) random variables cn (γn (Yn ), Xn ); thus, its cost distribution, approaching a normal shape as N gets large, has mass on all values 0, 1, . [sent-110, score-0.529]
</p><p>54 8  0  probability mass function  k γ1  0  0  0  0. [sent-124, score-0.036]
</p><p>55 Illustration of the iterative ofﬂine computation given p(x, y) as described in Example 4 and the directed network shown (N = 12). [sent-128, score-0.188]
</p><p>56 05)—thus, with a total of just |E| = 11 γ bits of communication, the pbp-optimal strategy γ 3 recovers roughly 28% of the loss J(γ 0 ) − J(¯ ∗ ). [sent-131, score-0.201]
</p><p>57 Lemma 1 The minimum penalty J(γ ∗ ) deﬁned in (1) is, ﬁrstly, achievable by a deterministic1 strategy and, secondly, equivalently deﬁned by J(γ ∗ )  =  min  p(u|y)  p(x) x∈{0,1}N  c(u, x) u∈{0,1}N  subject to p(u|y) = n∈V  p(u|y)p(y|x) dy y∈RN  p(un |yn , uπ(n) ). [sent-134, score-0.221]
</p><p>58 Lemma 1 is primarily of conceptual value, establishing a correspondence between ﬁxing a component rule γn ∈ Γn and inducing a decision process Un from the information (Yn , Uπ(n) ) local to node n. [sent-135, score-0.543]
</p><p>59 The following assumption permits analytical progress towards a ﬁnite parameterization for each function space Γn and the basis of an ofﬂine algorithm. [sent-136, score-0.199]
</p><p>60 Assumption 1 The observation process Y satisﬁes p(y|x) =  n∈V  p(yn |x). [sent-137, score-0.04]
</p><p>61 Upon ﬁxing a deterministic rule γn ∈ Γn local to node n (in correspondence with p(un |yn , uπ(n) ) by virtue of Lemma 1), we have the identity p(un |x, uπ(n) ) =  yn ∈R  p(un |yn , uπ(n) )p(yn |x) dyn . [sent-139, score-0.702]
</p><p>62 (4)  Moreover, upon ﬁxing a deterministic strategy γ ∈ ΓG , we have the identity p(u|x) = n∈V  p(un |x, uπ(n) ). [sent-140, score-0.253]
</p><p>63 (5)  Lemma 2 implies ﬁxing component rule γn ∈ Γn is in correspondence with inducing the conditional distribution p(un |x, uπ(n) ), now a probabilistic description that persists local to node n no matter the rule γi at any other node i ∈ V\n. [sent-141, score-0.751]
</p><p>64 That deterministic strategies sufﬁce, however, justiﬁes “post-hoc” our initial abuse of notation for elements in the set Γ. [sent-143, score-0.119]
</p><p>65 We now argue that, despite these simpliﬁcations, the component rules of γ ∗ continue to be globally coupled. [sent-145, score-0.134]
</p><p>66 Starting with any deterministic strategy γ ∈ ΓG , consider optimizing the nth component rule γn over Γn assuming all other components stay ﬁxed. [sent-146, score-0.316]
</p><p>67 With γn a degree-of-freedom, decision process Un is no longer well-deﬁned so each un ∈ {0, 1} merely represents a candidate decision local to node n. [sent-147, score-0.972]
</p><p>68 Online, each local decision will be made only upon receiving both the local observation Yn = yn and all parents’ local decisions Uπ(n) = uπ(n) . [sent-148, score-0.631]
</p><p>69 It follows that node n, upon deciding a particular un , may assert that random vector U is re′ stricted to values in the subset U[uπ(n) , un ] = {u′ ∈ {0, 1}N | u′ π(n) = uπ(n) , un = un }. [sent-149, score-2.523]
</p><p>70 Then, viewing (Yn , Uπ(n) ) as a composite local observation and proceeding in the manner by which (3) is derived, the pbp-optimal relaxation of (1) reduces to the following form. [sent-150, score-0.199]
</p><p>71 2  Ofﬂine Message-Passing Algorithm  Let fn map from coefﬁcients {bi ; i ∈ V\n} to coefﬁcients bn by the following operations: 1. [sent-155, score-0.179]
</p><p>72 compute bn via (7) given p(x), c(u, x) and {p(ui |x, uπ(i) ); i ∈ V\n}. [sent-157, score-0.094]
</p><p>73 Then, the simultaneous satisfaction of Proposition 1 at all N nodes can be viewed as a system of 2N +1 n∈V 2|π(n)| nonlinear equations in as many unknowns, bn = fn (b1 , . [sent-158, score-0.191]
</p><p>74 , b0 ) and generate the sequence 1 N {bk } using a component-wise iterative application of f in (8) i. [sent-172, score-0.045]
</p><p>75 γ  Direct implementation of (9) is clearly imprudent from a computational perspective, because the transformation from ﬁxed coefﬁcients bk to the corresponding distribution n pk (un |x, uπ(n) ) need not be repeated within every component evaluation of f . [sent-188, score-0.27]
</p><p>76 In fact, assuming every node n stores in memory its own likelihood function p(yn |x), this transformation can be accomplished locally (cf. [sent-189, score-0.336]
</p><p>77 (4) and (6)) and, also assuming the resulting distribution is broadcast to all other nodes before they proceed with their subsequent component evaluation of f , the termination guarantee of Proposition 2 is retained. [sent-190, score-0.29]
</p><p>78 Requiring every node to perform a network-wide broadcast within every iteration k makes (9) a decidedly global algorithm, not to mention that each node n must also store in memory p(x, yn ) and c(u, x) to carry forth the supporting local computations. [sent-191, score-1.009]
</p><p>79 Assumption 2 The cost function satisﬁes c(u, x) = n∈V cn (un , x) for some collection of functions {cn : {0, 1}N +1 → R} and the directed graph G is tree-structured. [sent-192, score-0.354]
</p><p>80 An intuitive interpretation of Proposition 3, from the perspective of node n, is as follows. [sent-195, score-0.224]
</p><p>81 From (10) in the forward pass, the messages received from each parent deﬁne what, during subsequent online operation, that parent’s local decision means (in a likelihood sense) about its ancestors’ outputs and the hidden process. [sent-196, score-0.495]
</p><p>82 From (12) in the reverse pass, the messages received from each child deﬁne what the local decision will mean (in an expected cost sense) to that child and its descendants. [sent-197, score-0.5]
</p><p>83 From (11), both types of incoming messages impact the local rule update and, in turn, the outgoing messages to both types of neighbors. [sent-198, score-0.515]
</p><p>84 While Proposition 3 alleviates the need for the iterative global broadcast of distributions pk (un |x, uπ(n) ), the explicit dependence of (10)-(12) on the full vector x implies the memory and computation requirements local to each node can still be exponential in N . [sent-199, score-0.49]
</p><p>85 Assumption 3 The hidden process X is Markov on G, or p(x) = n∈V p(xn |xπ(n) ), and all component likelihoods/costs satisfy p(yn |x) = p(yn |xn ) and cn (un , x) = cn (un , xn ). [sent-200, score-0.543]
</p><p>86 Proposition 4 Under Assumption 3, the iterates in Proposition 3 specialize to the form of bk (un , xn ; uπ(n) ), n  k Pn→j (un |xn )  k and Cn→i (ui , xi ),  k = 0, 1, . [sent-201, score-0.298]
</p><p>87 and each node n need only store in memory p(xπ(n) , xn , yn ) and cn (un , xn ) to carry forth the supporting local computations. [sent-204, score-1.037]
</p><p>88 )  Proposition 4 implies the convergence properties of Proposition 2 are upheld with maximal efﬁciency (linear in N ) when G is tree-structured and the global distribution and costs satisfy p(x, y) = n∈V p(xn |xπ(n) )p(yn |xn ) and c(u, x) = n∈V cn (un , xn ), respectively. [sent-206, score-0.313]
</p><p>89 Note that these conditions hold for the MPM assignment problems in Examples 3 & 4. [sent-207, score-0.107]
</p><p>90 4  Discussion  Our decision-theoretic variational approach reﬂects several departures from existing methods for communication-constrained inference. [sent-208, score-0.071]
</p><p>91 Firstly, instead of imposing the constraints on an algorithm derived from an ideal model, we explicitly model the constraints and derive a different algorithm. [sent-209, score-0.072]
</p><p>92 Secondly, our penalty function drives the approximation by the desired application of inference (e. [sent-210, score-0.089]
</p><p>93 Thirdly, the necessary ofﬂine computation gives rise to a downside, namely less ﬂexibility against time-varying statistical environments, decision objectives or network conditions. [sent-215, score-0.113]
</p><p>94 Similar to the sum-product version of Belief Propagation (BP), our message-passing algorithm originates assuming a tree structure, an additive cost and a synchronous message schedule. [sent-217, score-0.162]
</p><p>95 That we solve for correlated equilibria and depend on probabilistic structure commensurate with cost structure for efﬁciency is in common with graphical games [10], which distinctly are formulated on undirected graphs and absent of hidden variables. [sent-223, score-0.205]
</p><p>96 Finally, our ofﬂine computation resembles learning a conditional random ﬁeld [11], in the sense that factors of p(u|x) are iteratively modiﬁed to reduce penalty J(γ); online computation via strategy u = γ(y), repeated per realization Y = y, is then viewed as sampling from this distribution. [sent-224, score-0.275]
</p><p>97 Along the learning thread, a special case of our formulation appears in [12], but assuming p(x, y) is unknown. [sent-225, score-0.083]
</p><p>98 Acknowledgments This work supported by the Air Force Ofﬁce of Scientiﬁc Research under contract FA955004-1 and by the Army Research Ofﬁce under contract DAAD19-00-1-0466. [sent-226, score-0.06]
</p><p>99 Data association based on optimization in graphical models with application to sensor networks. [sent-234, score-0.051]
</p><p>100 Posterior assignment in directed graphical models with minimal online communication. [sent-273, score-0.313]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('un', 0.548), ('mpm', 0.259), ('yn', 0.224), ('node', 0.224), ('messages', 0.201), ('communication', 0.167), ('cn', 0.159), ('ine', 0.142), ('strategy', 0.132), ('bk', 0.129), ('ui', 0.126), ('proposition', 0.126), ('xn', 0.124), ('assignment', 0.107), ('directed', 0.101), ('bn', 0.094), ('rules', 0.09), ('decentralized', 0.09), ('penalty', 0.089), ('bp', 0.088), ('upon', 0.077), ('strategies', 0.075), ('broadcast', 0.075), ('decision', 0.071), ('variational', 0.071), ('parents', 0.07), ('lemma', 0.066), ('children', 0.065), ('cost', 0.062), ('message', 0.059), ('relaxation', 0.059), ('parameterization', 0.059), ('local', 0.058), ('pk', 0.057), ('hidden', 0.057), ('rule', 0.055), ('online', 0.054), ('xing', 0.054), ('child', 0.054), ('parent', 0.054), ('acyclic', 0.052), ('store', 0.052), ('nodes', 0.051), ('graphical', 0.051), ('correspondence', 0.05), ('employs', 0.05), ('myopic', 0.049), ('coef', 0.049), ('broadcasts', 0.047), ('kreidl', 0.047), ('stipulated', 0.047), ('virtue', 0.047), ('fn', 0.046), ('specialize', 0.045), ('receiving', 0.045), ('iterative', 0.045), ('deterministic', 0.044), ('component', 0.044), ('termination', 0.043), ('cients', 0.043), ('formulation', 0.042), ('network', 0.042), ('viewing', 0.042), ('thresholds', 0.041), ('inducing', 0.041), ('forth', 0.041), ('assuming', 0.041), ('observation', 0.04), ('every', 0.04), ('assumption', 0.04), ('map', 0.039), ('detection', 0.039), ('iii', 0.037), ('arg', 0.037), ('bits', 0.037), ('constraints', 0.036), ('mass', 0.036), ('guarantee', 0.036), ('belief', 0.036), ('pass', 0.036), ('recognizing', 0.036), ('analytical', 0.036), ('permits', 0.035), ('equilibria', 0.035), ('ce', 0.035), ('bit', 0.034), ('computable', 0.033), ('zeros', 0.033), ('loss', 0.032), ('graph', 0.032), ('memory', 0.031), ('rn', 0.031), ('nite', 0.031), ('illustrative', 0.03), ('deciding', 0.03), ('contract', 0.03), ('respectively', 0.03), ('convergence', 0.03), ('progress', 0.029), ('realized', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="96-tfidf-1" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>Author: O. P. Kreidl, Alan S. Willsky</p><p>Abstract: Given a directed graphical model with binary-valued hidden nodes and real-valued noisy observations, consider deciding upon the maximum a-posteriori (MAP) or the maximum posterior-marginal (MPM) assignment under the restriction that each node broadcasts only to its children exactly one single-bit message. We present a variational formulation, viewing the processing rules local to all nodes as degrees-of-freedom, that minimizes the loss in expected (MAP or MPM) performance subject to such online communication constraints. The approach leads to a novel message-passing algorithm to be executed ofﬂine, or before observations are realized, which mitigates the performance loss by iteratively coupling all rules in a manner implicitly driven by global statistics. We also provide (i) illustrative examples, (ii) assumptions that guarantee convergence and efﬁciency and (iii) connections to active research areas. 1</p><p>2 0.16118552 <a title="96-tfidf-2" href="./nips-2005-Fast_Information_Value_for_Graphical_Models.html">70 nips-2005-Fast Information Value for Graphical Models</a></p>
<p>Author: Brigham S. Anderson, Andrew W. Moore</p><p>Abstract: Calculations that quantify the dependencies between variables are vital to many operations with graphical models, e.g., active learning and sensitivity analysis. Previously, pairwise information gain calculation has involved a cost quadratic in network size. In this work, we show how to perform a similar computation with cost linear in network size. The loss function that allows this is of a form amenable to computation by dynamic programming. The message-passing algorithm that results is described and empirical results demonstrate large speedups without decrease in accuracy. In the cost-sensitive domains examined, superior accuracy is achieved.</p><p>3 0.13833986 <a title="96-tfidf-3" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>Author: Benjamin V. Roy, Ciamac C. Moallemi</p><p>Abstract: We propose consensus propagation, an asynchronous distributed protocol for averaging numbers across a network. We establish convergence, characterize the convergence rate for regular graphs, and demonstrate that the protocol exhibits better scaling properties than pairwise averaging, an alternative that has received much recent attention. Consensus propagation can be viewed as a special case of belief propagation, and our results contribute to the belief propagation literature. In particular, beyond singly-connected graphs, there are very few classes of relevant problems for which belief propagation is known to converge. 1</p><p>4 0.13222867 <a title="96-tfidf-4" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>Author: Aurelie C. Lozano, Sanjeev R. Kulkarni, Robert E. Schapire</p><p>Abstract: We study the statistical convergence and consistency of regularized Boosting methods, where the samples are not independent and identically distributed (i.i.d.) but come from empirical processes of stationary β-mixing sequences. Utilizing a technique that constructs a sequence of independent blocks close in distribution to the original samples, we prove the consistency of the composite classiﬁers resulting from a regularization achieved by restricting the 1-norm of the base classiﬁers’ weights. When compared to the i.i.d. case, the nature of sampling manifests in the consistency result only through generalization of the original condition on the growth of the regularization parameter.</p><p>5 0.12835592 <a title="96-tfidf-5" href="./nips-2005-Message_passing_for_task_redistribution_on_sparse_graphs.html">125 nips-2005-Message passing for task redistribution on sparse graphs</a></p>
<p>Author: K. Wong, Zhuo Gao, David Tax</p><p>Abstract: The problem of resource allocation in sparse graphs with real variables is studied using methods of statistical physics. An efﬁcient distributed algorithm is devised on the basis of insight gained from the analysis and is examined using numerical simulations, showing excellent performance and full agreement with the theoretical results.</p><p>6 0.099007986 <a title="96-tfidf-6" href="./nips-2005-Location-based_activity_recognition.html">121 nips-2005-Location-based activity recognition</a></p>
<p>7 0.095859841 <a title="96-tfidf-7" href="./nips-2005-Variational_Bayesian_Stochastic_Complexity_of_Mixture_Models.html">201 nips-2005-Variational Bayesian Stochastic Complexity of Mixture Models</a></p>
<p>8 0.091235429 <a title="96-tfidf-8" href="./nips-2005-Walk-Sum_Interpretation_and_Analysis_of_Gaussian_Belief_Propagation.html">204 nips-2005-Walk-Sum Interpretation and Analysis of Gaussian Belief Propagation</a></p>
<p>9 0.090779923 <a title="96-tfidf-9" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>10 0.088388398 <a title="96-tfidf-10" href="./nips-2005-Stimulus_Evoked_Independent_Factor_Analysis_of_MEG_Data_with_Large_Background_Activity.html">183 nips-2005-Stimulus Evoked Independent Factor Analysis of MEG Data with Large Background Activity</a></p>
<p>11 0.083246432 <a title="96-tfidf-11" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>12 0.076502711 <a title="96-tfidf-12" href="./nips-2005-Fast_Gaussian_Process_Regression_using_KD-Trees.html">69 nips-2005-Fast Gaussian Process Regression using KD-Trees</a></p>
<p>13 0.07579869 <a title="96-tfidf-13" href="./nips-2005-Variational_EM_Algorithms_for_Non-Gaussian_Latent_Variable_Models.html">202 nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</a></p>
<p>14 0.075514503 <a title="96-tfidf-14" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>15 0.072729871 <a title="96-tfidf-15" href="./nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs.html">90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</a></p>
<p>16 0.071776688 <a title="96-tfidf-16" href="./nips-2005-Temporal_Abstraction_in_Temporal-difference_Networks.html">187 nips-2005-Temporal Abstraction in Temporal-difference Networks</a></p>
<p>17 0.066247158 <a title="96-tfidf-17" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>18 0.064750381 <a title="96-tfidf-18" href="./nips-2005-Beyond_Gaussian_Processes%3A_On_the_Distributions_of_Infinite_Networks.html">38 nips-2005-Beyond Gaussian Processes: On the Distributions of Infinite Networks</a></p>
<p>19 0.063850708 <a title="96-tfidf-19" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>20 0.063815899 <a title="96-tfidf-20" href="./nips-2005-Generalization_to_Unseen_Cases.html">85 nips-2005-Generalization to Unseen Cases</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.222), (1, 0.054), (2, 0.037), (3, -0.058), (4, 0.059), (5, -0.055), (6, -0.289), (7, 0.141), (8, 0.008), (9, 0.114), (10, 0.039), (11, -0.077), (12, -0.077), (13, -0.01), (14, 0.046), (15, 0.0), (16, -0.058), (17, 0.033), (18, 0.058), (19, 0.078), (20, -0.043), (21, 0.012), (22, -0.017), (23, -0.015), (24, -0.059), (25, -0.088), (26, -0.073), (27, 0.121), (28, -0.027), (29, -0.118), (30, -0.024), (31, -0.087), (32, 0.054), (33, -0.031), (34, -0.134), (35, -0.083), (36, 0.013), (37, 0.097), (38, 0.076), (39, -0.056), (40, -0.009), (41, -0.033), (42, -0.039), (43, 0.026), (44, 0.059), (45, 0.074), (46, 0.031), (47, 0.033), (48, 0.05), (49, -0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97214919 <a title="96-lsi-1" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>Author: O. P. Kreidl, Alan S. Willsky</p><p>Abstract: Given a directed graphical model with binary-valued hidden nodes and real-valued noisy observations, consider deciding upon the maximum a-posteriori (MAP) or the maximum posterior-marginal (MPM) assignment under the restriction that each node broadcasts only to its children exactly one single-bit message. We present a variational formulation, viewing the processing rules local to all nodes as degrees-of-freedom, that minimizes the loss in expected (MAP or MPM) performance subject to such online communication constraints. The approach leads to a novel message-passing algorithm to be executed ofﬂine, or before observations are realized, which mitigates the performance loss by iteratively coupling all rules in a manner implicitly driven by global statistics. We also provide (i) illustrative examples, (ii) assumptions that guarantee convergence and efﬁciency and (iii) connections to active research areas. 1</p><p>2 0.71200281 <a title="96-lsi-2" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>Author: Benjamin V. Roy, Ciamac C. Moallemi</p><p>Abstract: We propose consensus propagation, an asynchronous distributed protocol for averaging numbers across a network. We establish convergence, characterize the convergence rate for regular graphs, and demonstrate that the protocol exhibits better scaling properties than pairwise averaging, an alternative that has received much recent attention. Consensus propagation can be viewed as a special case of belief propagation, and our results contribute to the belief propagation literature. In particular, beyond singly-connected graphs, there are very few classes of relevant problems for which belief propagation is known to converge. 1</p><p>3 0.69948536 <a title="96-lsi-3" href="./nips-2005-Message_passing_for_task_redistribution_on_sparse_graphs.html">125 nips-2005-Message passing for task redistribution on sparse graphs</a></p>
<p>Author: K. Wong, Zhuo Gao, David Tax</p><p>Abstract: The problem of resource allocation in sparse graphs with real variables is studied using methods of statistical physics. An efﬁcient distributed algorithm is devised on the basis of insight gained from the analysis and is examined using numerical simulations, showing excellent performance and full agreement with the theoretical results.</p><p>4 0.68826437 <a title="96-lsi-4" href="./nips-2005-Fast_Information_Value_for_Graphical_Models.html">70 nips-2005-Fast Information Value for Graphical Models</a></p>
<p>Author: Brigham S. Anderson, Andrew W. Moore</p><p>Abstract: Calculations that quantify the dependencies between variables are vital to many operations with graphical models, e.g., active learning and sensitivity analysis. Previously, pairwise information gain calculation has involved a cost quadratic in network size. In this work, we show how to perform a similar computation with cost linear in network size. The loss function that allows this is of a form amenable to computation by dynamic programming. The message-passing algorithm that results is described and empirical results demonstrate large speedups without decrease in accuracy. In the cost-sensitive domains examined, superior accuracy is achieved.</p><p>5 0.67840016 <a title="96-lsi-5" href="./nips-2005-Walk-Sum_Interpretation_and_Analysis_of_Gaussian_Belief_Propagation.html">204 nips-2005-Walk-Sum Interpretation and Analysis of Gaussian Belief Propagation</a></p>
<p>Author: Dmitry Malioutov, Alan S. Willsky, Jason K. Johnson</p><p>Abstract: This paper presents a new framework based on walks in a graph for analysis and inference in Gaussian graphical models. The key idea is to decompose correlations between variables as a sum over all walks between those variables in the graph. The weight of each walk is given by a product of edgewise partial correlations. We provide a walk-sum interpretation of Gaussian belief propagation in trees and of the approximate method of loopy belief propagation in graphs with cycles. This perspective leads to a better understanding of Gaussian belief propagation and of its convergence in loopy graphs. 1</p><p>6 0.66849816 <a title="96-lsi-6" href="./nips-2005-Location-based_activity_recognition.html">121 nips-2005-Location-based activity recognition</a></p>
<p>7 0.54425496 <a title="96-lsi-7" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>8 0.40156579 <a title="96-lsi-8" href="./nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs.html">90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</a></p>
<p>9 0.39023381 <a title="96-lsi-9" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>10 0.38537464 <a title="96-lsi-10" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>11 0.3525559 <a title="96-lsi-11" href="./nips-2005-Temporal_Abstraction_in_Temporal-difference_Networks.html">187 nips-2005-Temporal Abstraction in Temporal-difference Networks</a></p>
<p>12 0.35001513 <a title="96-lsi-12" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>13 0.34733903 <a title="96-lsi-13" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>14 0.34606856 <a title="96-lsi-14" href="./nips-2005-Stimulus_Evoked_Independent_Factor_Analysis_of_MEG_Data_with_Large_Background_Activity.html">183 nips-2005-Stimulus Evoked Independent Factor Analysis of MEG Data with Large Background Activity</a></p>
<p>15 0.33815393 <a title="96-lsi-15" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>16 0.33343607 <a title="96-lsi-16" href="./nips-2005-Rodeo%3A_Sparse_Nonparametric_Regression_in_High_Dimensions.html">168 nips-2005-Rodeo: Sparse Nonparametric Regression in High Dimensions</a></p>
<p>17 0.31319165 <a title="96-lsi-17" href="./nips-2005-Variational_Bayesian_Stochastic_Complexity_of_Mixture_Models.html">201 nips-2005-Variational Bayesian Stochastic Complexity of Mixture Models</a></p>
<p>18 0.31229919 <a title="96-lsi-18" href="./nips-2005-Recovery_of_Jointly_Sparse_Signals_from_Few_Random_Projections.html">163 nips-2005-Recovery of Jointly Sparse Signals from Few Random Projections</a></p>
<p>19 0.31154859 <a title="96-lsi-19" href="./nips-2005-Fast_Gaussian_Process_Regression_using_KD-Trees.html">69 nips-2005-Fast Gaussian Process Regression using KD-Trees</a></p>
<p>20 0.30439028 <a title="96-lsi-20" href="./nips-2005-Policy-Gradient_Methods_for_Planning.html">153 nips-2005-Policy-Gradient Methods for Planning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.047), (10, 0.041), (27, 0.035), (29, 0.171), (31, 0.09), (34, 0.075), (39, 0.024), (41, 0.01), (50, 0.013), (55, 0.103), (69, 0.091), (73, 0.018), (77, 0.016), (88, 0.073), (91, 0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89983898 <a title="96-lda-1" href="./nips-2005-Cyclic_Equilibria_in_Markov_Games.html">53 nips-2005-Cyclic Equilibria in Markov Games</a></p>
<p>Author: Martin Zinkevich, Amy Greenwald, Michael L. Littman</p><p>Abstract: Although variants of value iteration have been proposed for ﬁnding Nash or correlated equilibria in general-sum Markov games, these variants have not been shown to be effective in general. In this paper, we demonstrate by construction that existing variants of value iteration cannot ﬁnd stationary equilibrium policies in arbitrary general-sum Markov games. Instead, we propose an alternative interpretation of the output of value iteration based on a new (non-stationary) equilibrium concept that we call “cyclic equilibria.” We prove that value iteration identiﬁes cyclic equilibria in a class of games in which it fails to ﬁnd stationary equilibria. We also demonstrate empirically that value iteration ﬁnds cyclic equilibria in nearly all examples drawn from a random distribution of Markov games. 1</p><p>same-paper 2 0.88986754 <a title="96-lda-2" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>Author: O. P. Kreidl, Alan S. Willsky</p><p>Abstract: Given a directed graphical model with binary-valued hidden nodes and real-valued noisy observations, consider deciding upon the maximum a-posteriori (MAP) or the maximum posterior-marginal (MPM) assignment under the restriction that each node broadcasts only to its children exactly one single-bit message. We present a variational formulation, viewing the processing rules local to all nodes as degrees-of-freedom, that minimizes the loss in expected (MAP or MPM) performance subject to such online communication constraints. The approach leads to a novel message-passing algorithm to be executed ofﬂine, or before observations are realized, which mitigates the performance loss by iteratively coupling all rules in a manner implicitly driven by global statistics. We also provide (i) illustrative examples, (ii) assumptions that guarantee convergence and efﬁciency and (iii) connections to active research areas. 1</p><p>3 0.70485479 <a title="96-lda-3" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>Author: Jin Yu, Douglas Aberdeen, Nicol N. Schraudolph</p><p>Abstract: Reinforcement learning by direct policy gradient estimation is attractive in theory but in practice leads to notoriously ill-behaved optimization problems. We improve its robustness and speed of convergence with stochastic meta-descent, a gain vector adaptation method that employs fast Hessian-vector products. In our experiments the resulting algorithms outperform previously employed online stochastic, ofﬂine conjugate, and natural policy gradient methods. 1</p><p>4 0.69680971 <a title="96-lda-4" href="./nips-2005-Spiking_Inputs_to_a_Winner-take-all_Network.html">181 nips-2005-Spiking Inputs to a Winner-take-all Network</a></p>
<p>Author: Matthias Oster, Shih-Chii Liu</p><p>Abstract: Recurrent networks that perform a winner-take-all computation have been studied extensively. Although some of these studies include spiking networks, they consider only analog input rates. We present results of this winner-take-all computation on a network of integrate-and-ﬁre neurons which receives spike trains as inputs. We show how we can conﬁgure the connectivity in the network so that the winner is selected after a pre-determined number of input spikes. We discuss spiking inputs with both regular frequencies and Poisson-distributed rates. The robustness of the computation was tested by implementing the winner-take-all network on an analog VLSI array of 64 integrate-and-ﬁre neurons which have an innate variance in their operating parameters. 1</p><p>5 0.69393659 <a title="96-lda-5" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>Author: Benjamin V. Roy, Ciamac C. Moallemi</p><p>Abstract: We propose consensus propagation, an asynchronous distributed protocol for averaging numbers across a network. We establish convergence, characterize the convergence rate for regular graphs, and demonstrate that the protocol exhibits better scaling properties than pairwise averaging, an alternative that has received much recent attention. Consensus propagation can be viewed as a special case of belief propagation, and our results contribute to the belief propagation literature. In particular, beyond singly-connected graphs, there are very few classes of relevant problems for which belief propagation is known to converge. 1</p><p>6 0.69373614 <a title="96-lda-6" href="./nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs.html">90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</a></p>
<p>7 0.68529785 <a title="96-lda-7" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>8 0.68523395 <a title="96-lda-8" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>9 0.6832298 <a title="96-lda-9" href="./nips-2005-Measuring_Shared_Information_and_Coordinated_Activity_in_Neuronal_Networks.html">124 nips-2005-Measuring Shared Information and Coordinated Activity in Neuronal Networks</a></p>
<p>10 0.68316311 <a title="96-lda-10" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>11 0.6827811 <a title="96-lda-11" href="./nips-2005-Variational_Bayesian_Stochastic_Complexity_of_Mixture_Models.html">201 nips-2005-Variational Bayesian Stochastic Complexity of Mixture Models</a></p>
<p>12 0.67973495 <a title="96-lda-12" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>13 0.67265147 <a title="96-lda-13" href="./nips-2005-Unbiased_Estimator_of_Shape_Parameter_for_Spiking_Irregularities_under_Changing_Environments.html">197 nips-2005-Unbiased Estimator of Shape Parameter for Spiking Irregularities under Changing Environments</a></p>
<p>14 0.67212152 <a title="96-lda-14" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>15 0.6719324 <a title="96-lda-15" href="./nips-2005-The_Information-Form_Data_Association_Filter.html">192 nips-2005-The Information-Form Data Association Filter</a></p>
<p>16 0.67015028 <a title="96-lda-16" href="./nips-2005-Augmented_Rescorla-Wagner_and_Maximum_Likelihood_Estimation.html">32 nips-2005-Augmented Rescorla-Wagner and Maximum Likelihood Estimation</a></p>
<p>17 0.66256529 <a title="96-lda-17" href="./nips-2005-Policy-Gradient_Methods_for_Planning.html">153 nips-2005-Policy-Gradient Methods for Planning</a></p>
<p>18 0.66034269 <a title="96-lda-18" href="./nips-2005-Sensory_Adaptation_within_a_Bayesian_Framework_for_Perception.html">173 nips-2005-Sensory Adaptation within a Bayesian Framework for Perception</a></p>
<p>19 0.66029704 <a title="96-lda-19" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>20 0.65737188 <a title="96-lda-20" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
