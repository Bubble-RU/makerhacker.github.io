<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>47 nips-2005-Consistency of one-class SVM and related algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-47" href="#">nips2005-47</a> knowledge-graph by maker-knowledge-mining</p><h1>47 nips-2005-Consistency of one-class SVM and related algorithms</h1>
<br/><p>Source: <a title="nips-2005-47-pdf" href="http://papers.nips.cc/paper/2756-consistency-of-one-class-svm-and-related-algorithms.pdf">pdf</a></p><p>Author: Régis Vert, Jean-philippe Vert</p><p>Abstract: We determine the asymptotic limit of the function computed by support vector machines (SVM) and related algorithms that minimize a regularized empirical convex loss function in the reproducing kernel Hilbert space of the Gaussian RBF kernel, in the situation where the number of examples tends to inﬁnity, the bandwidth of the Gaussian kernel tends to 0, and the regularization parameter is held ﬁxed. Non-asymptotic convergence bounds to this limit in the L2 sense are provided, together with upper bounds on the classiﬁcation error that is shown to converge to the Bayes risk, therefore proving the Bayes-consistency of a variety of methods although the regularization term does not vanish. These results are particularly relevant to the one-class SVM, for which the regularization can not vanish by construction, and which is shown for the ﬁrst time to be a consistent density level set estimator. 1</p><p>Reference: <a title="nips-2005-47-reference" href="../nips2005_reference/nips-2005-Consistency_of_one-class_SVM_and_related_algorithms_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('svm', 0.369), ('rd', 0.354), ('rkhs', 0.272), ('risk', 0.245), ('theorem', 0.208), ('excess', 0.184), ('dens', 0.164), ('asymptot', 0.142), ('lebesgu', 0.141), ('hing', 0.135), ('modul', 0.133), ('regul', 0.132), ('convex', 0.128), ('quantil', 0.124), ('loss', 0.118), ('level', 0.115), ('converg', 0.112), ('kernel', 0.111), ('rademach', 0.1), ('bandwid', 0.096)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="47-tfidf-1" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>2 0.27547261 <a title="47-tfidf-2" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>3 0.19571301 <a title="47-tfidf-3" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>4 0.18190788 <a title="47-tfidf-4" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>5 0.17802057 <a title="47-tfidf-5" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>6 0.1769319 <a title="47-tfidf-6" href="./nips-2005-Improved_risk_tail_bounds_for_on-line_algorithms.html">95 nips-2005-Improved risk tail bounds for on-line algorithms</a></p>
<p>7 0.1652583 <a title="47-tfidf-7" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>8 0.15886714 <a title="47-tfidf-8" href="./nips-2005-Two_view_learning%3A_SVM-2K%2C_Theory_and_Practice.html">196 nips-2005-Two view learning: SVM-2K, Theory and Practice</a></p>
<p>9 0.14579315 <a title="47-tfidf-9" href="./nips-2005-Variational_EM_Algorithms_for_Non-Gaussian_Latent_Variable_Models.html">202 nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</a></p>
<p>10 0.14067613 <a title="47-tfidf-10" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>11 0.14027219 <a title="47-tfidf-11" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>12 0.13338652 <a title="47-tfidf-12" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>13 0.13228786 <a title="47-tfidf-13" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>14 0.12695773 <a title="47-tfidf-14" href="./nips-2005-Maximum_Margin_Semi-Supervised_Learning_for_Structured_Variables.html">123 nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</a></p>
<p>15 0.1238743 <a title="47-tfidf-15" href="./nips-2005-TD%280%29_Leads_to_Better_Policies_than_Approximate_Value_Iteration.html">186 nips-2005-TD(0) Leads to Better Policies than Approximate Value Iteration</a></p>
<p>16 0.11686727 <a title="47-tfidf-16" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>17 0.10846569 <a title="47-tfidf-17" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>18 0.10578448 <a title="47-tfidf-18" href="./nips-2005-Learning_Minimum_Volume_Sets.html">112 nips-2005-Learning Minimum Volume Sets</a></p>
<p>19 0.10257253 <a title="47-tfidf-19" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>20 0.10037167 <a title="47-tfidf-20" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.288), (1, -0.177), (2, -0.106), (3, 0.184), (4, -0.248), (5, -0.076), (6, -0.144), (7, 0.072), (8, 0.064), (9, 0.002), (10, -0.203), (11, 0.112), (12, 0.068), (13, -0.003), (14, -0.061), (15, 0.079), (16, 0.096), (17, -0.053), (18, -0.132), (19, -0.102), (20, 0.001), (21, 0.039), (22, 0.018), (23, 0.033), (24, -0.094), (25, -0.037), (26, 0.028), (27, -0.001), (28, 0.116), (29, 0.015), (30, 0.059), (31, 0.016), (32, 0.039), (33, -0.11), (34, 0.061), (35, 0.045), (36, -0.013), (37, 0.032), (38, -0.011), (39, 0.039), (40, 0.108), (41, 0.031), (42, -0.033), (43, 0.046), (44, 0.058), (45, 0.092), (46, -0.039), (47, 0.062), (48, 0.058), (49, -0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95076394 <a title="47-lsi-1" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>2 0.76359653 <a title="47-lsi-2" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>3 0.75768614 <a title="47-lsi-3" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>4 0.65262991 <a title="47-lsi-4" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>5 0.6508311 <a title="47-lsi-5" href="./nips-2005-Learning_Minimum_Volume_Sets.html">112 nips-2005-Learning Minimum Volume Sets</a></p>
<p>6 0.63214684 <a title="47-lsi-6" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>7 0.62718153 <a title="47-lsi-7" href="./nips-2005-Two_view_learning%3A_SVM-2K%2C_Theory_and_Practice.html">196 nips-2005-Two view learning: SVM-2K, Theory and Practice</a></p>
<p>8 0.59899282 <a title="47-lsi-8" href="./nips-2005-Improved_risk_tail_bounds_for_on-line_algorithms.html">95 nips-2005-Improved risk tail bounds for on-line algorithms</a></p>
<p>9 0.59818369 <a title="47-lsi-9" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>10 0.59635991 <a title="47-lsi-10" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>11 0.55830973 <a title="47-lsi-11" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>12 0.5191887 <a title="47-lsi-12" href="./nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<p>13 0.51438463 <a title="47-lsi-13" href="./nips-2005-Computing_the_Solution_Path_for_the_Regularized_Support_Vector_Regression.html">44 nips-2005-Computing the Solution Path for the Regularized Support Vector Regression</a></p>
<p>14 0.50584894 <a title="47-lsi-14" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>15 0.4837088 <a title="47-lsi-15" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>16 0.47794464 <a title="47-lsi-16" href="./nips-2005-TD%280%29_Leads_to_Better_Policies_than_Approximate_Value_Iteration.html">186 nips-2005-TD(0) Leads to Better Policies than Approximate Value Iteration</a></p>
<p>17 0.46649355 <a title="47-lsi-17" href="./nips-2005-Statistical_Convergence_of_Kernel_CCA.html">182 nips-2005-Statistical Convergence of Kernel CCA</a></p>
<p>18 0.46157035 <a title="47-lsi-18" href="./nips-2005-Worst-Case_Bounds_for_Gaussian_Process_Models.html">205 nips-2005-Worst-Case Bounds for Gaussian Process Models</a></p>
<p>19 0.46028087 <a title="47-lsi-19" href="./nips-2005-Variational_EM_Algorithms_for_Non-Gaussian_Latent_Variable_Models.html">202 nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</a></p>
<p>20 0.45888257 <a title="47-lsi-20" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.046), (17, 0.019), (54, 0.514), (71, 0.148), (75, 0.011), (88, 0.063), (89, 0.029), (92, 0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83026659 <a title="47-lda-1" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>2 0.81241846 <a title="47-lda-2" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>3 0.58304274 <a title="47-lda-3" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>4 0.5721122 <a title="47-lda-4" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>5 0.57001078 <a title="47-lda-5" href="./nips-2005-Sensory_Adaptation_within_a_Bayesian_Framework_for_Perception.html">173 nips-2005-Sensory Adaptation within a Bayesian Framework for Perception</a></p>
<p>6 0.56325775 <a title="47-lda-6" href="./nips-2005-Improved_risk_tail_bounds_for_on-line_algorithms.html">95 nips-2005-Improved risk tail bounds for on-line algorithms</a></p>
<p>7 0.52792501 <a title="47-lda-7" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>8 0.5261181 <a title="47-lda-8" href="./nips-2005-Learning_Minimum_Volume_Sets.html">112 nips-2005-Learning Minimum Volume Sets</a></p>
<p>9 0.51874954 <a title="47-lda-9" href="./nips-2005-Beyond_Gaussian_Processes%3A_On_the_Distributions_of_Infinite_Networks.html">38 nips-2005-Beyond Gaussian Processes: On the Distributions of Infinite Networks</a></p>
<p>10 0.51752144 <a title="47-lda-10" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>11 0.50252289 <a title="47-lda-11" href="./nips-2005-Asymptotics_of_Gaussian_Regularized_Least_Squares.html">31 nips-2005-Asymptotics of Gaussian Regularized Least Squares</a></p>
<p>12 0.49908018 <a title="47-lda-12" href="./nips-2005-TD%280%29_Leads_to_Better_Policies_than_Approximate_Value_Iteration.html">186 nips-2005-TD(0) Leads to Better Policies than Approximate Value Iteration</a></p>
<p>13 0.49865183 <a title="47-lda-13" href="./nips-2005-Rodeo%3A_Sparse_Nonparametric_Regression_in_High_Dimensions.html">168 nips-2005-Rodeo: Sparse Nonparametric Regression in High Dimensions</a></p>
<p>14 0.49499297 <a title="47-lda-14" href="./nips-2005-On_the_Convergence_of_Eigenspaces_in_Kernel_Principal_Component_Analysis.html">147 nips-2005-On the Convergence of Eigenspaces in Kernel Principal Component Analysis</a></p>
<p>15 0.49299785 <a title="47-lda-15" href="./nips-2005-Worst-Case_Bounds_for_Gaussian_Process_Models.html">205 nips-2005-Worst-Case Bounds for Gaussian Process Models</a></p>
<p>16 0.48723418 <a title="47-lda-16" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>17 0.48521695 <a title="47-lda-17" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>18 0.48197544 <a title="47-lda-18" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>19 0.48135132 <a title="47-lda-19" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>20 0.48095921 <a title="47-lda-20" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
