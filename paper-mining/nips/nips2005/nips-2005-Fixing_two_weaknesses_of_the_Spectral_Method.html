<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>75 nips-2005-Fixing two weaknesses of the Spectral Method</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-75" href="#">nips2005-75</a> knowledge-graph by maker-knowledge-mining</p><h1>75 nips-2005-Fixing two weaknesses of the Spectral Method</h1>
<br/><p>Source: <a title="nips-2005-75-pdf" href="http://papers.nips.cc/paper/2766-fixing-two-weaknesses-of-the-spectral-method.pdf">pdf</a></p><p>Author: Kevin Lang</p><p>Abstract: We discuss two intrinsic weaknesses of the spectral graph partitioning method, both of which have practical consequences. The ﬁrst is that spectral embeddings tend to hide the best cuts from the commonly used hyperplane rounding method. Rather than cleaning up the resulting suboptimal cuts with local search, we recommend the adoption of ﬂow-based rounding. The second weakness is that for many “power law” graphs, the spectral method produces cuts that are highly unbalanced, thus decreasing the usefulness of the method for visualization (see ﬁgure 4(b)) or as a basis for divide-and-conquer algorithms. These balance problems, which occur even though the spectral method’s quotient-style objective function does encourage balance, can be ﬁxed with a stricter balance constraint that turns the spectral mathematical program into an SDP that can be solved for million-node graphs by a method of Burer and Monteiro. 1 Background Graph partitioning is the NP-hard problem of ﬁnding a small graph cut subject to the constraint that neither side of the resulting partitioning of the nodes is “too small”. We will be dealing with several versions: the graph bisection problem, which requires perfect 1 : 1 2 2 balance; the β-balanced cut problem (with β a fraction such as 1 ), which requires at least 3 β : (1 − β) balance; and the quotient cut problem, which requires the small side to be large enough to “pay for” the edges in the cut. The quotient cut metric is c/ min(a, b), where c is the cutsize and a and b are the sizes of the two sides of the cut. All of the well-known variants of the quotient cut metric (e.g. normalized cut [15]) have similar behavior with respect to the issues discussed in this paper. The spectral method for graph partitioning was introduced in 1973 by Fiedler and Donath & Hoffman [6]. In the mid-1980’s Alon & Milman [1] proved that spectral cuts can be at worst quadratically bad; in the mid 1990’s Guattery & Miller [10] proved that this analysis is tight by exhibiting a family of n-node graphs whose spectral bisections cut O(n 2/3 ) edges versus the optimal O(n1/3 ) edges. On the other hand, Spielman & Teng [16] have proved stronger performance guarantees for the special case of spacelike graphs. The spectral method can be derived by relaxing a quadratic integer program which encodes the graph bisection problem (see section 3.1). The solution to this relaxation is the “Fiedler vector”, or second smallest eigenvector of the graph’s discrete Laplacian matrix, whose elements xi can be interpreted as an embedding of the graph on the line. To obtain a (A) Graph with nearly balanced 8-cut (B) Spectral Embedding (C) Notional Flow-based Embedding Figure 1: The spectral embedding hides the best solution from hyperplane rounding. speciﬁc cut, one must apply a “rounding method” to this embedding. The hyperplane rounding method chooses one of the n − 1 cuts which separate the nodes whose x i values lie above and below some split value x. ˆ 2 Using ﬂow to ﬁnd cuts that are hidden from hyperplane rounding Theorists have long known that the spectral method cannot distinguish between deep cuts and long paths, and that this confusion can cause it to cut a graph in the wrong direction thereby producing the spectral method’s worst-case behavior [10]. In this section we will show by example that even when the spectral method is not fooled into cutting in the wrong direction, the resulting embedding can hide the best cuts from the hyperplane rounding method. This is a possible explanation for the frequently made empirical observation (see e.g. [12]) that hyperplane roundings of spectral embeddings are noisy and therefore beneﬁt from cleanup with a local search method such as Fiduccia-Matheyses [8]. Consider the graph in ﬁgure 1(a), which has a near-bisection cutting 8 edges. For this graph the spectral method produces the embedding shown in ﬁgure 1(b), and recommends that we make a vertical cut (across the horizontal dimension which is based on the Fiedler vector). This is correct in a generalized sense, but it is obvious that no hyperplane (or vertical line in this picture) can possibly extract the optimal 8-edge cut. Some insight into why spectral embeddings tend to have this problem can be obtained from the spectral method’s electrical interpretation. In this view the graph is represented by a resistor network [7]. Current ﬂowing in this network causes voltage drops across the resistors, thus determining the nodes’ voltages and hence their positions. When current ﬂows through a long series of resistors, it induces a progressive voltage drop. This is what causes the excessive length of the embeddings of the horizontal girder-like structures which are blocking all vertical hyperplane cuts in ﬁgure 1(b). If the embedding method were somehow not based on current, but rather on ﬂow, which does not distinguish between a pipe and a series of pipes, then the long girders could retract into the two sides of the embedding, as suggested by ﬁgure 1(c), and the best cut would be revealed. Because theoretical ﬂow-like embedding methods such as [14] are currently not practical, we point out that in cases like ﬁgure 1(b), where the spectral method has not chosen an incorrect direction for the cut, one can use an S-T max ﬂow problem with the ﬂow running in the recommended direction (horizontally for this embedding) to extract the good cut even though it is hidden from all hyperplanes. We currently use two different ﬂow-based rounding methods. A method called MQI looks for quotient cuts, and is already described in [13]. Another method, that we shall call Midﬂow, looks for β-balanced cuts. The input to Midﬂow is a graph and an ordering of its nodes (obtained e.g. from a spectral embedding or from the projection of any embedding onto a line). We divide the graph’s nodes into 3 sets F, L, and U. The sets F and L respectively contain the ﬁrst βn and last βn nodes in the ordering, and U contains the remaining 50-50 balance ng s ro un di Hy pe r pl an e neg-pos split quotient cut score (cutsize / size of small side) 0.01 ctor r ve iedle of F 0.004 0.003 0.00268 0.00232 Best hyperplane rounding of Fiedler Vector Best improvement with local search 0.002 0.00138 0.001 60000 80000 Midflow rounding beta = 1/4 100000 120000 0.00145 140000 Midflow rounding of Fiedler Vector beta = 1/3 160000 180000 200000 220000 240000 number of nodes on ’left’ side of cut (out of 324800) Figure 2: A typical example (see section 2.1) where ﬂow-based rounding beats hyperplane rounding, even when the hyperplane cuts are improved with Fiduccia-Matheyses search. Note that for this spacelike graph, the best quotient cuts have reasonably good balance. U = n − 2βn nodes, which are “up for grabs”. We set up an S-T max ﬂow problem with one node for every graph node plus 2 new nodes for the source and sink. For each graph edge there are two arcs, one in each direction, with unit capacity. Finally, the nodes in F are pinned to the source and the nodes in L are pinned to sink by inﬁnite capacity arcs. This max-ﬂow problem can be solved by a good implementation of the push-relabel algorithm (such as Goldberg and Cherkassky’s hi pr [4]) in time that empirically is nearly linear with a very good constant factor. Figure 6 shows that solving a MidFlow problem with hi pr can be 1000 times cheaper than ﬁnding a spectral embedding with ARPACK. When the goal is ﬁnding good β-balanced cuts, MidFlow rounding is strictly more powerful than hyperplane rounding; from a given node ordering hyperplane rounding chooses the best of U + 1 candidate cuts, while MidFlow rounding chooses the best of 2U candidates, including all of those considered by hyperplane rounding. [Similarly, MQI rounding is strictly more powerful than hyperplane rounding for the task of ﬁnding good quotient cuts.] 2.1 A concrete example The plot in ﬁgure 2 shows a number of cuts in a 324,800 node nearly planar graph derived from a 700x464 pixel downward-looking view of some clouds over some mountains.1 The y-axis of the plot is quotient cut score; smaller values are better. We note in passing that the commonly used split point x = 0 does not yield the best hyperplane cut. Our main ˆ point is that the two cuts generated by MidFlow rounding of the Fiedler vector (with β = 1 3 and β = 1 ) are nearly twice as good as the best hyperplane cut. Even after the best 4 hyperplane cut has been improved by taking the best result of 100 runs of a version of Fiduccia-Matheyses local search, it is still much worse than the cuts obtained by ﬂowbased rounding. 1 The graph’s edges are unweighted but are chosen by a randomized rule which is more likely to include an edge between two neighboring pixels if they have a similar grey value. Good cuts in the graph tend to run along discontinuities in the image, as one would expect. quotient cut score 1 SDP-LB (smaller is better) 0.1 Scatter plot showing cuts in a</p><p>Reference: <a title="nips-2005-75-reference" href="../nips2005_reference/nips-2005-Fixing_two_weaknesses_of_the_Spectral_Method_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cut', 0.55), ('sdp', 0.33), ('quoty', 0.253), ('hyperpl', 0.249), ('round', 0.222), ('spect', 0.218), ('graph', 0.194), ('embed', 0.186), ('bal', 0.183), ('yahoo', 0.158), ('ow', 0.137), ('nod', 0.13), ('midflow', 0.127), ('fiedl', 0.109), ('bur', 0.094), ('mqi', 0.091), ('bisect', 0.08), ('spacelik', 0.072), ('mid', 0.072), ('soc', 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="75-tfidf-1" href="./nips-2005-Fixing_two_weaknesses_of_the_Spectral_Method.html">75 nips-2005-Fixing two weaknesses of the Spectral Method</a></p>
<p>2 0.33988982 <a title="75-tfidf-2" href="./nips-2005-A_Probabilistic_Approach_for_Optimizing_Spectral_Clustering.html">13 nips-2005-A Probabilistic Approach for Optimizing Spectral Clustering</a></p>
<p>3 0.32695886 <a title="75-tfidf-3" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>4 0.14008696 <a title="75-tfidf-4" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>5 0.097448707 <a title="75-tfidf-5" href="./nips-2005-Coarse_sample_complexity_bounds_for_active_learning.html">41 nips-2005-Coarse sample complexity bounds for active learning</a></p>
<p>6 0.092963114 <a title="75-tfidf-6" href="./nips-2005-Diffusion_Maps%2C_Spectral_Clustering_and_Eigenfunctions_of_Fokker-Planck_Operators.html">56 nips-2005-Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators</a></p>
<p>7 0.091732368 <a title="75-tfidf-7" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>8 0.08685232 <a title="75-tfidf-8" href="./nips-2005-Message_passing_for_task_redistribution_on_sparse_graphs.html">125 nips-2005-Message passing for task redistribution on sparse graphs</a></p>
<p>9 0.077959567 <a title="75-tfidf-9" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>10 0.077435926 <a title="75-tfidf-10" href="./nips-2005-Soft_Clustering_on_Graphs.html">178 nips-2005-Soft Clustering on Graphs</a></p>
<p>11 0.074968778 <a title="75-tfidf-11" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>12 0.071793772 <a title="75-tfidf-12" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>13 0.071245782 <a title="75-tfidf-13" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>14 0.069085643 <a title="75-tfidf-14" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>15 0.067373335 <a title="75-tfidf-15" href="./nips-2005-Searching_for_Character_Models.html">171 nips-2005-Searching for Character Models</a></p>
<p>16 0.066788606 <a title="75-tfidf-16" href="./nips-2005-The_Forgetron%3A_A_Kernel-Based_Perceptron_on_a_Fixed_Budget.html">191 nips-2005-The Forgetron: A Kernel-Based Perceptron on a Fixed Budget</a></p>
<p>17 0.06464655 <a title="75-tfidf-17" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>18 0.06294734 <a title="75-tfidf-18" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>19 0.062598206 <a title="75-tfidf-19" href="./nips-2005-Kernelized_Infomax_Clustering.html">102 nips-2005-Kernelized Infomax Clustering</a></p>
<p>20 0.05969381 <a title="75-tfidf-20" href="./nips-2005-Inferring_Motor_Programs_from_Images_of_Handwritten_Digits.html">97 nips-2005-Inferring Motor Programs from Images of Handwritten Digits</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.181), (1, -0.106), (2, -0.081), (3, 0.067), (4, 0.287), (5, 0.108), (6, -0.098), (7, 0.033), (8, -0.055), (9, -0.028), (10, 0.133), (11, -0.025), (12, -0.003), (13, 0.099), (14, -0.115), (15, -0.073), (16, 0.006), (17, 0.038), (18, -0.011), (19, 0.094), (20, 0.071), (21, 0.218), (22, 0.309), (23, 0.061), (24, 0.008), (25, -0.042), (26, 0.027), (27, 0.245), (28, 0.168), (29, 0.038), (30, -0.27), (31, 0.132), (32, -0.092), (33, -0.014), (34, 0.012), (35, 0.003), (36, -0.108), (37, 0.003), (38, 0.045), (39, 0.165), (40, 0.034), (41, 0.012), (42, -0.023), (43, -0.039), (44, -0.006), (45, 0.025), (46, -0.035), (47, 0.029), (48, 0.034), (49, -0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97132343 <a title="75-lsi-1" href="./nips-2005-Fixing_two_weaknesses_of_the_Spectral_Method.html">75 nips-2005-Fixing two weaknesses of the Spectral Method</a></p>
<p>2 0.79660088 <a title="75-lsi-2" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>3 0.5964663 <a title="75-lsi-3" href="./nips-2005-A_Probabilistic_Approach_for_Optimizing_Spectral_Clustering.html">13 nips-2005-A Probabilistic Approach for Optimizing Spectral Clustering</a></p>
<p>4 0.28261095 <a title="75-lsi-4" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>5 0.28233865 <a title="75-lsi-5" href="./nips-2005-Diffusion_Maps%2C_Spectral_Clustering_and_Eigenfunctions_of_Fokker-Planck_Operators.html">56 nips-2005-Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators</a></p>
<p>6 0.27238017 <a title="75-lsi-6" href="./nips-2005-Large_scale_networks_fingerprinting_and_visualization_using_the_k-core_decomposition.html">107 nips-2005-Large scale networks fingerprinting and visualization using the k-core decomposition</a></p>
<p>7 0.27136436 <a title="75-lsi-7" href="./nips-2005-Spectral_Bounds_for_Sparse_PCA%3A_Exact_and_Greedy_Algorithms.html">180 nips-2005-Spectral Bounds for Sparse PCA: Exact and Greedy Algorithms</a></p>
<p>8 0.26984209 <a title="75-lsi-8" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>9 0.24709639 <a title="75-lsi-9" href="./nips-2005-Fast_Krylov_Methods_for_N-Body_Learning.html">71 nips-2005-Fast Krylov Methods for N-Body Learning</a></p>
<p>10 0.24680853 <a title="75-lsi-10" href="./nips-2005-Message_passing_for_task_redistribution_on_sparse_graphs.html">125 nips-2005-Message passing for task redistribution on sparse graphs</a></p>
<p>11 0.24205533 <a title="75-lsi-11" href="./nips-2005-Searching_for_Character_Models.html">171 nips-2005-Searching for Character Models</a></p>
<p>12 0.22679356 <a title="75-lsi-12" href="./nips-2005-Soft_Clustering_on_Graphs.html">178 nips-2005-Soft Clustering on Graphs</a></p>
<p>13 0.22478189 <a title="75-lsi-13" href="./nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs.html">90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</a></p>
<p>14 0.22347607 <a title="75-lsi-14" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>15 0.21984267 <a title="75-lsi-15" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>16 0.21543257 <a title="75-lsi-16" href="./nips-2005-Benchmarking_Non-Parametric_Statistical_Tests.html">37 nips-2005-Benchmarking Non-Parametric Statistical Tests</a></p>
<p>17 0.21531951 <a title="75-lsi-17" href="./nips-2005-Coarse_sample_complexity_bounds_for_active_learning.html">41 nips-2005-Coarse sample complexity bounds for active learning</a></p>
<p>18 0.20837705 <a title="75-lsi-18" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>19 0.20665303 <a title="75-lsi-19" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>20 0.2030578 <a title="75-lsi-20" href="./nips-2005-Walk-Sum_Interpretation_and_Analysis_of_Gaussian_Belief_Propagation.html">204 nips-2005-Walk-Sum Interpretation and Analysis of Gaussian Belief Propagation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.056), (9, 0.04), (12, 0.027), (17, 0.015), (18, 0.014), (33, 0.388), (54, 0.023), (58, 0.015), (71, 0.094), (86, 0.013), (88, 0.071), (89, 0.024), (92, 0.117)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7309202 <a title="75-lda-1" href="./nips-2005-Fixing_two_weaknesses_of_the_Spectral_Method.html">75 nips-2005-Fixing two weaknesses of the Spectral Method</a></p>
<p>2 0.63119996 <a title="75-lda-2" href="./nips-2005-Asymptotics_of_Gaussian_Regularized_Least_Squares.html">31 nips-2005-Asymptotics of Gaussian Regularized Least Squares</a></p>
<p>3 0.60701078 <a title="75-lda-3" href="./nips-2005-Kernels_for_gene_regulatory_regions.html">103 nips-2005-Kernels for gene regulatory regions</a></p>
<p>4 0.41327447 <a title="75-lda-4" href="./nips-2005-Query_by_Committee_Made_Real.html">160 nips-2005-Query by Committee Made Real</a></p>
<p>5 0.41059899 <a title="75-lda-5" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>6 0.41019395 <a title="75-lda-6" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>7 0.40949339 <a title="75-lda-7" href="./nips-2005-Fast_Krylov_Methods_for_N-Body_Learning.html">71 nips-2005-Fast Krylov Methods for N-Body Learning</a></p>
<p>8 0.40845025 <a title="75-lda-8" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>9 0.40812787 <a title="75-lda-9" href="./nips-2005-Noise_and_the_two-thirds_power_Law.html">136 nips-2005-Noise and the two-thirds power Law</a></p>
<p>10 0.40784669 <a title="75-lda-10" href="./nips-2005-Dynamic_Social_Network_Analysis_using_Latent_Space_Models.html">60 nips-2005-Dynamic Social Network Analysis using Latent Space Models</a></p>
<p>11 0.40764338 <a title="75-lda-11" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>12 0.40676135 <a title="75-lda-12" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>13 0.4066866 <a title="75-lda-13" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>14 0.40664324 <a title="75-lda-14" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>15 0.40554968 <a title="75-lda-15" href="./nips-2005-Principles_of_real-time_computing_with_feedback_applied_to_cortical_microcircuit_models.html">157 nips-2005-Principles of real-time computing with feedback applied to cortical microcircuit models</a></p>
<p>16 0.40367213 <a title="75-lda-16" href="./nips-2005-Logic_and_MRF_Circuitry_for_Labeling_Occluding_and_Thinline_Visual_Contours.html">122 nips-2005-Logic and MRF Circuitry for Labeling Occluding and Thinline Visual Contours</a></p>
<p>17 0.40253055 <a title="75-lda-17" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>18 0.40212524 <a title="75-lda-18" href="./nips-2005-Online_Discovery_and_Learning_of_Predictive_State_Representations.html">148 nips-2005-Online Discovery and Learning of Predictive State Representations</a></p>
<p>19 0.40196824 <a title="75-lda-19" href="./nips-2005-Coarse_sample_complexity_bounds_for_active_learning.html">41 nips-2005-Coarse sample complexity bounds for active learning</a></p>
<p>20 0.40181741 <a title="75-lda-20" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
