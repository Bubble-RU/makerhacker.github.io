<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-9" href="#">nips2005-9</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</h1>
<br/><p>Source: <a title="nips-2005-9-pdf" href="http://papers.nips.cc/paper/2818-a-domain-decomposition-method-for-fast-manifold-learning.pdf">pdf</a></p><p>Author: Zhenyue Zhang, Hongyuan Zha</p><p>Abstract: We propose a fast manifold learning algorithm based on the methodology of domain decomposition. Starting with the set of sample points partitioned into two subdomains, we develop the solution of the interface problem that can glue the embeddings on the two subdomains into an embedding on the whole domain. We provide a detailed analysis to assess the errors produced by the gluing process using matrix perturbation theory. Numerical examples are given to illustrate the efﬁciency and effectiveness of the proposed methods. 1</p><p>Reference: <a title="nips-2005-9-reference" href="../nips2005_reference/nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('subdomain', 0.596), ('glu', 0.405), ('embed', 0.236), ('tj', 0.228), ('ltsa', 0.191), ('manifold', 0.17), ('cet', 0.143), ('af', 0.141), ('domain', 0.13), ('nely', 0.124), ('span', 0.114), ('whol', 0.106), ('partit', 0.103), ('ti', 0.102), ('overlap', 0.1), ('gt', 0.095), ('kh', 0.095), ('tk', 0.091), ('decomposit', 0.088), ('cpu', 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="9-tfidf-1" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>Author: Zhenyue Zhang, Hongyuan Zha</p><p>Abstract: We propose a fast manifold learning algorithm based on the methodology of domain decomposition. Starting with the set of sample points partitioned into two subdomains, we develop the solution of the interface problem that can glue the embeddings on the two subdomains into an embedding on the whole domain. We provide a detailed analysis to assess the errors produced by the gluing process using matrix perturbation theory. Numerical examples are given to illustrate the efﬁciency and effectiveness of the proposed methods. 1</p><p>2 0.10394397 <a title="9-tfidf-2" href="./nips-2005-Non-Local_Manifold_Parzen_Windows.html">138 nips-2005-Non-Local Manifold Parzen Windows</a></p>
<p>Author: Yoshua Bengio, Hugo Larochelle, Pascal Vincent</p><p>Abstract: To escape from the curse of dimensionality, we claim that one can learn non-local functions, in the sense that the value and shape of the learned function at x must be inferred using examples that may be far from x. With this objective, we present a non-local non-parametric density estimator. It builds upon previously proposed Gaussian mixture models with regularized covariance matrices to take into account the local shape of the manifold. It also builds upon recent work on non-local estimators of the tangent plane of a manifold, which are able to generalize in places with little training data, unlike traditional, local, non-parametric models.</p><p>3 0.091559708 <a title="9-tfidf-3" href="./nips-2005-Mixture_Modeling_by_Affinity_Propagation.html">127 nips-2005-Mixture Modeling by Affinity Propagation</a></p>
<p>Author: Brendan J. Frey, Delbert Dueck</p><p>Abstract: Clustering is a fundamental problem in machine learning and has been approached in many ways. Two general and quite different approaches include iteratively ﬁtting a mixture model (e.g., using EM) and linking together pairs of training cases that have high afﬁnity (e.g., using spectral methods). Pair-wise clustering algorithms need not compute sufﬁcient statistics and avoid poor solutions by directly placing similar examples in the same cluster. However, many applications require that each cluster of data be accurately described by a prototype or model, so afﬁnity-based clustering – and its beneﬁts – cannot be directly realized. We describe a technique called “afﬁnity propagation”, which combines the advantages of both approaches. The method learns a mixture model of the data by recursively propagating afﬁnity messages. We demonstrate afﬁnity propagation on the problems of clustering image patches for image segmentation and learning mixtures of gene expression models from microarray data. We ﬁnd that afﬁnity propagation obtains better solutions than mixtures of Gaussians, the K-medoids algorithm, spectral clustering and hierarchical clustering, and is both able to ﬁnd a pre-speciﬁed number of clusters and is able to automatically determine the number of clusters. Interestingly, afﬁnity propagation can be viewed as belief propagation in a graphical model that accounts for pairwise training case likelihood functions and the identiﬁcation of cluster centers. 1</p><p>4 0.091031022 <a title="9-tfidf-4" href="./nips-2005-Affine_Structure_From_Sound.html">20 nips-2005-Affine Structure From Sound</a></p>
<p>Author: Sebastian Thrun</p><p>Abstract: We consider the problem of localizing a set of microphones together with a set of external acoustic events (e.g., hand claps), emitted at unknown times and unknown locations. We propose a solution that approximates this problem under a far ﬁeld approximation deﬁned in the calculus of afﬁne geometry, and that relies on singular value decomposition (SVD) to recover the afﬁne structure of the problem. We then deﬁne low-dimensional optimization techniques for embedding the solution into Euclidean geometry, and further techniques for recovering the locations and emission times of the acoustic events. The approach is useful for the calibration of ad-hoc microphone arrays and sensor networks. 1</p><p>5 0.090085559 <a title="9-tfidf-5" href="./nips-2005-Selecting_Landmark_Points_for_Sparse_Manifold_Learning.html">172 nips-2005-Selecting Landmark Points for Sparse Manifold Learning</a></p>
<p>Author: Jorge Silva, Jorge Marques, João Lemos</p><p>Abstract: There has been a surge of interest in learning non-linear manifold models to approximate high-dimensional data. Both for computational complexity reasons and for generalization capability, sparsity is a desired feature in such models. This usually means dimensionality reduction, which naturally implies estimating the intrinsic dimension, but it can also mean selecting a subset of the data to use as landmarks, which is especially important because many existing algorithms have quadratic complexity in the number of observations. This paper presents an algorithm for selecting landmarks, based on LASSO regression, which is well known to favor sparse approximations because it uses regularization with an l1 norm. As an added beneﬁt, a continuous manifold parameterization, based on the landmarks, is also found. Experimental results with synthetic and real data illustrate the algorithm. 1</p><p>6 0.074968778 <a title="9-tfidf-6" href="./nips-2005-Fixing_two_weaknesses_of_the_Spectral_Method.html">75 nips-2005-Fixing two weaknesses of the Spectral Method</a></p>
<p>7 0.073570281 <a title="9-tfidf-7" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>8 0.072533548 <a title="9-tfidf-8" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>9 0.064456187 <a title="9-tfidf-9" href="./nips-2005-Diffusion_Maps%2C_Spectral_Clustering_and_Eigenfunctions_of_Fokker-Planck_Operators.html">56 nips-2005-Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators</a></p>
<p>10 0.062801942 <a title="9-tfidf-10" href="./nips-2005-Estimation_of_Intrinsic_Dimensionality_Using_High-Rate_Vector_Quantization.html">66 nips-2005-Estimation of Intrinsic Dimensionality Using High-Rate Vector Quantization</a></p>
<p>11 0.062167145 <a title="9-tfidf-11" href="./nips-2005-Fast_Krylov_Methods_for_N-Body_Learning.html">71 nips-2005-Fast Krylov Methods for N-Body Learning</a></p>
<p>12 0.061910089 <a title="9-tfidf-12" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>13 0.061270051 <a title="9-tfidf-13" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>14 0.059419334 <a title="9-tfidf-14" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>15 0.058044173 <a title="9-tfidf-15" href="./nips-2005-Metric_Learning_by_Collapsing_Classes.html">126 nips-2005-Metric Learning by Collapsing Classes</a></p>
<p>16 0.05747642 <a title="9-tfidf-16" href="./nips-2005-Q-Clustering.html">159 nips-2005-Q-Clustering</a></p>
<p>17 0.052874725 <a title="9-tfidf-17" href="./nips-2005-Dynamic_Social_Network_Analysis_using_Latent_Space_Models.html">60 nips-2005-Dynamic Social Network Analysis using Latent Space Models</a></p>
<p>18 0.052524932 <a title="9-tfidf-18" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>19 0.051781282 <a title="9-tfidf-19" href="./nips-2005-Kernelized_Infomax_Clustering.html">102 nips-2005-Kernelized Infomax Clustering</a></p>
<p>20 0.051756095 <a title="9-tfidf-20" href="./nips-2005-On_the_Convergence_of_Eigenspaces_in_Kernel_Principal_Component_Analysis.html">147 nips-2005-On the Convergence of Eigenspaces in Kernel Principal Component Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.152), (1, -0.069), (2, -0.05), (3, 0.043), (4, 0.046), (5, 0.011), (6, 0.017), (7, 0.076), (8, 0.049), (9, 0.033), (10, -0.024), (11, -0.007), (12, -0.033), (13, -0.055), (14, 0.041), (15, -0.013), (16, 0.078), (17, 0.054), (18, 0.128), (19, 0.135), (20, 0.012), (21, 0.118), (22, 0.048), (23, -0.024), (24, 0.087), (25, 0.004), (26, 0.093), (27, -0.032), (28, -0.091), (29, -0.062), (30, 0.011), (31, 0.1), (32, 0.063), (33, -0.069), (34, 0.026), (35, 0.204), (36, -0.076), (37, -0.034), (38, 0.008), (39, -0.021), (40, 0.059), (41, 0.052), (42, -0.1), (43, 0.051), (44, 0.009), (45, 0.061), (46, 0.046), (47, 0.097), (48, -0.156), (49, -0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91795957 <a title="9-lsi-1" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>Author: Zhenyue Zhang, Hongyuan Zha</p><p>Abstract: We propose a fast manifold learning algorithm based on the methodology of domain decomposition. Starting with the set of sample points partitioned into two subdomains, we develop the solution of the interface problem that can glue the embeddings on the two subdomains into an embedding on the whole domain. We provide a detailed analysis to assess the errors produced by the gluing process using matrix perturbation theory. Numerical examples are given to illustrate the efﬁciency and effectiveness of the proposed methods. 1</p><p>2 0.61506176 <a title="9-lsi-2" href="./nips-2005-Affine_Structure_From_Sound.html">20 nips-2005-Affine Structure From Sound</a></p>
<p>Author: Sebastian Thrun</p><p>Abstract: We consider the problem of localizing a set of microphones together with a set of external acoustic events (e.g., hand claps), emitted at unknown times and unknown locations. We propose a solution that approximates this problem under a far ﬁeld approximation deﬁned in the calculus of afﬁne geometry, and that relies on singular value decomposition (SVD) to recover the afﬁne structure of the problem. We then deﬁne low-dimensional optimization techniques for embedding the solution into Euclidean geometry, and further techniques for recovering the locations and emission times of the acoustic events. The approach is useful for the calibration of ad-hoc microphone arrays and sensor networks. 1</p><p>3 0.55539119 <a title="9-lsi-3" href="./nips-2005-Selecting_Landmark_Points_for_Sparse_Manifold_Learning.html">172 nips-2005-Selecting Landmark Points for Sparse Manifold Learning</a></p>
<p>Author: Jorge Silva, Jorge Marques, João Lemos</p><p>Abstract: There has been a surge of interest in learning non-linear manifold models to approximate high-dimensional data. Both for computational complexity reasons and for generalization capability, sparsity is a desired feature in such models. This usually means dimensionality reduction, which naturally implies estimating the intrinsic dimension, but it can also mean selecting a subset of the data to use as landmarks, which is especially important because many existing algorithms have quadratic complexity in the number of observations. This paper presents an algorithm for selecting landmarks, based on LASSO regression, which is well known to favor sparse approximations because it uses regularization with an l1 norm. As an added beneﬁt, a continuous manifold parameterization, based on the landmarks, is also found. Experimental results with synthetic and real data illustrate the algorithm. 1</p><p>4 0.5511235 <a title="9-lsi-4" href="./nips-2005-Fast_Krylov_Methods_for_N-Body_Learning.html">71 nips-2005-Fast Krylov Methods for N-Body Learning</a></p>
<p>Author: Nando D. Freitas, Yang Wang, Maryam Mahdaviani, Dustin Lang</p><p>Abstract: This paper addresses the issue of numerical computation in machine learning domains based on similarity metrics, such as kernel methods, spectral techniques and Gaussian processes. It presents a general solution strategy based on Krylov subspace iteration and fast N-body learning methods. The experiments show signiﬁcant gains in computation and storage on datasets arising in image segmentation, object detection and dimensionality reduction. The paper also presents theoretical bounds on the stability of these methods.</p><p>5 0.52396512 <a title="9-lsi-5" href="./nips-2005-Non-Local_Manifold_Parzen_Windows.html">138 nips-2005-Non-Local Manifold Parzen Windows</a></p>
<p>Author: Yoshua Bengio, Hugo Larochelle, Pascal Vincent</p><p>Abstract: To escape from the curse of dimensionality, we claim that one can learn non-local functions, in the sense that the value and shape of the learned function at x must be inferred using examples that may be far from x. With this objective, we present a non-local non-parametric density estimator. It builds upon previously proposed Gaussian mixture models with regularized covariance matrices to take into account the local shape of the manifold. It also builds upon recent work on non-local estimators of the tangent plane of a manifold, which are able to generalize in places with little training data, unlike traditional, local, non-parametric models.</p><p>6 0.49243981 <a title="9-lsi-6" href="./nips-2005-Spectral_Bounds_for_Sparse_PCA%3A_Exact_and_Greedy_Algorithms.html">180 nips-2005-Spectral Bounds for Sparse PCA: Exact and Greedy Algorithms</a></p>
<p>7 0.44320253 <a title="9-lsi-7" href="./nips-2005-Tensor_Subspace_Analysis.html">189 nips-2005-Tensor Subspace Analysis</a></p>
<p>8 0.41640028 <a title="9-lsi-8" href="./nips-2005-Estimation_of_Intrinsic_Dimensionality_Using_High-Rate_Vector_Quantization.html">66 nips-2005-Estimation of Intrinsic Dimensionality Using High-Rate Vector Quantization</a></p>
<p>9 0.41292098 <a title="9-lsi-9" href="./nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<p>10 0.39242449 <a title="9-lsi-10" href="./nips-2005-Efficient_Estimation_of_OOMs.html">62 nips-2005-Efficient Estimation of OOMs</a></p>
<p>11 0.38020474 <a title="9-lsi-11" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>12 0.3763769 <a title="9-lsi-12" href="./nips-2005-Mixture_Modeling_by_Affinity_Propagation.html">127 nips-2005-Mixture Modeling by Affinity Propagation</a></p>
<p>13 0.37251711 <a title="9-lsi-13" href="./nips-2005-An_Approximate_Inference_Approach_for_the_PCA_Reconstruction_Error.html">24 nips-2005-An Approximate Inference Approach for the PCA Reconstruction Error</a></p>
<p>14 0.36856312 <a title="9-lsi-14" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>15 0.36449918 <a title="9-lsi-15" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>16 0.36027002 <a title="9-lsi-16" href="./nips-2005-Asymptotics_of_Gaussian_Regularized_Least_Squares.html">31 nips-2005-Asymptotics of Gaussian Regularized Least Squares</a></p>
<p>17 0.34808335 <a title="9-lsi-17" href="./nips-2005-Diffusion_Maps%2C_Spectral_Clustering_and_Eigenfunctions_of_Fokker-Planck_Operators.html">56 nips-2005-Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators</a></p>
<p>18 0.33859909 <a title="9-lsi-18" href="./nips-2005-Nested_sampling_for_Potts_models.html">133 nips-2005-Nested sampling for Potts models</a></p>
<p>19 0.32812881 <a title="9-lsi-19" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>20 0.327499 <a title="9-lsi-20" href="./nips-2005-Generalized_Nonnegative_Matrix_Approximations_with_Bregman_Divergences.html">86 nips-2005-Generalized Nonnegative Matrix Approximations with Bregman Divergences</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.079), (9, 0.424), (12, 0.019), (17, 0.029), (54, 0.028), (71, 0.109), (74, 0.014), (88, 0.066), (89, 0.024), (92, 0.097)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81313819 <a title="9-lda-1" href="./nips-2005-Fast_biped_walking_with_a_reflexive_controller_and_real-time_policy_searching.html">73 nips-2005-Fast biped walking with a reflexive controller and real-time policy searching</a></p>
<p>Author: Tao Geng, Bernd Porr, Florentin Wörgötter</p><p>Abstract: In this paper, we present our design and experiments of a planar biped robot (“RunBot”) under pure reﬂexive neuronal control. The goal of this study is to combine neuronal mechanisms with biomechanics to obtain very fast speed and the on-line learning of circuit parameters. Our controller is built with biologically inspired sensor- and motor-neuron models, including local reﬂexes and not employing any kind of position or trajectory-tracking control algorithm. Instead, this reﬂexive controller allows RunBot to exploit its own natural dynamics during critical stages of its walking gait cycle. To our knowledge, this is the ﬁrst time that dynamic biped walking is achieved using only a pure reﬂexive controller. In addition, this structure allows using a policy gradient reinforcement learning algorithm to tune the parameters of the reﬂexive controller in real-time during walking. This way RunBot can reach a relative speed of 3.5 leg-lengths per second after a few minutes of online learning, which is faster than that of any other biped robot, and is also comparable to the fastest relative speed of human walking. In addition, the stability domain of stable walking is quite large supporting this design strategy. 1</p><p>same-paper 2 0.75270057 <a title="9-lda-2" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>Author: Zhenyue Zhang, Hongyuan Zha</p><p>Abstract: We propose a fast manifold learning algorithm based on the methodology of domain decomposition. Starting with the set of sample points partitioned into two subdomains, we develop the solution of the interface problem that can glue the embeddings on the two subdomains into an embedding on the whole domain. We provide a detailed analysis to assess the errors produced by the gluing process using matrix perturbation theory. Numerical examples are given to illustrate the efﬁciency and effectiveness of the proposed methods. 1</p><p>3 0.71274483 <a title="9-lda-3" href="./nips-2005-Using_%60%60epitomes%27%27_to_model_genetic_diversity%3A_Rational_design_of_HIV_vaccine_cocktails.html">198 nips-2005-Using ``epitomes'' to model genetic diversity: Rational design of HIV vaccine cocktails</a></p>
<p>Author: Nebojsa Jojic, Vladimir Jojic, Christopher Meek, David Heckerman, Brendan J. Frey</p><p>Abstract: We introduce a new model of genetic diversity which summarizes a large input dataset into an epitome, a short sequence or a small set of short sequences of probability distributions capturing many overlapping subsequences from the dataset. The epitome as a representation has already been used in modeling real-valued signals, such as images and audio. The discrete sequence model we introduce in this paper targets applications in genetics, from multiple alignment to recombination and mutation inference. In our experiments, we concentrate on modeling the diversity of HIV where the epitome emerges as a natural model for producing relatively small vaccines covering a large number of immune system targets known as epitopes. Our experiments show that the epitome includes more epitopes than other vaccine designs of similar length, including cocktails of consensus strains, phylogenetic tree centers, and observed strains. We also discuss epitome designs that take into account uncertainty about Tcell cross reactivity and epitope presentation. In our experiments, we ﬁnd that vaccine optimization is fairly robust to these uncertainties. 1</p><p>4 0.68798888 <a title="9-lda-4" href="./nips-2005-Fast_Gaussian_Process_Regression_using_KD-Trees.html">69 nips-2005-Fast Gaussian Process Regression using KD-Trees</a></p>
<p>Author: Yirong Shen, Matthias Seeger, Andrew Y. Ng</p><p>Abstract: The computation required for Gaussian process regression with n training examples is about O(n3 ) during training and O(n) for each prediction. This makes Gaussian process regression too slow for large datasets. In this paper, we present a fast approximation method, based on kd-trees, that signiﬁcantly reduces both the prediction and the training times of Gaussian process regression.</p><p>5 0.6362074 <a title="9-lda-5" href="./nips-2005-Bayesian_model_learning_in_human_visual_perception.html">35 nips-2005-Bayesian model learning in human visual perception</a></p>
<p>Author: Gergő Orbán, Jozsef Fiser, Richard N. Aslin, Máté Lengyel</p><p>Abstract: Humans make optimal perceptual decisions in noisy and ambiguous conditions. Computations underlying such optimal behavior have been shown to rely on probabilistic inference according to generative models whose structure is usually taken to be known a priori. We argue that Bayesian model selection is ideal for inferring similar and even more complex model structures from experience. We ﬁnd in experiments that humans learn subtle statistical properties of visual scenes in a completely unsupervised manner. We show that these ﬁndings are well captured by Bayesian model learning within a class of models that seek to explain observed variables by independent hidden causes. 1</p><p>6 0.61763614 <a title="9-lda-6" href="./nips-2005-Query_by_Committee_Made_Real.html">160 nips-2005-Query by Committee Made Real</a></p>
<p>7 0.476035 <a title="9-lda-7" href="./nips-2005-Walk-Sum_Interpretation_and_Analysis_of_Gaussian_Belief_Propagation.html">204 nips-2005-Walk-Sum Interpretation and Analysis of Gaussian Belief Propagation</a></p>
<p>8 0.47408801 <a title="9-lda-8" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>9 0.46070763 <a title="9-lda-9" href="./nips-2005-Fast_Krylov_Methods_for_N-Body_Learning.html">71 nips-2005-Fast Krylov Methods for N-Body Learning</a></p>
<p>10 0.44113311 <a title="9-lda-10" href="./nips-2005-Fixing_two_weaknesses_of_the_Spectral_Method.html">75 nips-2005-Fixing two weaknesses of the Spectral Method</a></p>
<p>11 0.43896884 <a title="9-lda-11" href="./nips-2005-Selecting_Landmark_Points_for_Sparse_Manifold_Learning.html">172 nips-2005-Selecting Landmark Points for Sparse Manifold Learning</a></p>
<p>12 0.42953241 <a title="9-lda-12" href="./nips-2005-Dynamic_Social_Network_Analysis_using_Latent_Space_Models.html">60 nips-2005-Dynamic Social Network Analysis using Latent Space Models</a></p>
<p>13 0.4248426 <a title="9-lda-13" href="./nips-2005-Non-Local_Manifold_Parzen_Windows.html">138 nips-2005-Non-Local Manifold Parzen Windows</a></p>
<p>14 0.42424738 <a title="9-lda-14" href="./nips-2005-Gaussian_Process_Dynamical_Models.html">80 nips-2005-Gaussian Process Dynamical Models</a></p>
<p>15 0.42424008 <a title="9-lda-15" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>16 0.4224481 <a title="9-lda-16" href="./nips-2005-Diffusion_Maps%2C_Spectral_Clustering_and_Eigenfunctions_of_Fokker-Planck_Operators.html">56 nips-2005-Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators</a></p>
<p>17 0.41870591 <a title="9-lda-17" href="./nips-2005-Bayesian_Sets.html">33 nips-2005-Bayesian Sets</a></p>
<p>18 0.41725788 <a title="9-lda-18" href="./nips-2005-Kernels_for_gene_regulatory_regions.html">103 nips-2005-Kernels for gene regulatory regions</a></p>
<p>19 0.41656518 <a title="9-lda-19" href="./nips-2005-Fast_Information_Value_for_Graphical_Models.html">70 nips-2005-Fast Information Value for Graphical Models</a></p>
<p>20 0.41597748 <a title="9-lda-20" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
