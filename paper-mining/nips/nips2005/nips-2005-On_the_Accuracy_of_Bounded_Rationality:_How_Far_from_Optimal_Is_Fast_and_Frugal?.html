<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>146 nips-2005-On the Accuracy of Bounded Rationality: How Far from Optimal Is Fast and Frugal?</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-146" href="#">nips2005-146</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>146 nips-2005-On the Accuracy of Bounded Rationality: How Far from Optimal Is Fast and Frugal?</h1>
<br/><p>Source: <a title="nips-2005-146-pdf" href="http://papers.nips.cc/paper/2813-on-the-accuracy-of-bounded-rationality-how-far-from-optimal-is-fast-and-frugal.pdf">pdf</a></p><p>Author: Michael Schmitt, Laura Martignon</p><p>Abstract: Fast and frugal heuristics are well studied models of bounded rationality. Psychological research has proposed the take-the-best heuristic as a successful strategy in decision making with limited resources. Take-thebest searches for a sufﬁciently good ordering of cues (features) in a task where objects are to be compared lexicographically. We investigate the complexity of the problem of approximating optimal cue permutations for lexicographic strategies. We show that no efﬁcient algorithm can approximate the optimum to within any constant factor, if P = NP. We further consider a greedy approach for building lexicographic strategies and derive tight bounds for the performance ratio of a new and simple algorithm. This algorithm is proven to perform better than take-the-best. 1</p><p>Reference: <a title="nips-2005-146-reference" href="../nips2005_reference/nips-2005-On_the_Accuracy_of_Bounded_Rationality%3A_How_Far_from_Optimal_Is_Fast_and_Frugal%3F_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract Fast and frugal heuristics are well studied models of bounded rationality. [sent-4, score-0.218]
</p><p>2 Psychological research has proposed the take-the-best heuristic as a successful strategy in decision making with limited resources. [sent-5, score-0.117]
</p><p>3 Take-thebest searches for a sufﬁciently good ordering of cues (features) in a task where objects are to be compared lexicographically. [sent-6, score-0.165]
</p><p>4 We investigate the complexity of the problem of approximating optimal cue permutations for lexicographic strategies. [sent-7, score-0.753]
</p><p>5 We further consider a greedy approach for building lexicographic strategies and derive tight bounds for the performance ratio of a new and simple algorithm. [sent-9, score-0.364]
</p><p>6 Cognitive psychology categorizes human judgments made under such constraints as being boundedly rational if they are “satisﬁcing” (Simon, 1982) or, more generally, if they do not fall too far behind the rational standards. [sent-12, score-0.121]
</p><p>7 A class of models for human reasoning studied in the context of bounded rationality consists of simple algorithms termed “fast and frugal heuristics”. [sent-13, score-0.212]
</p><p>8 Great efforts have been put into testing these heuristics by empirical means in experiments with human subjects (Br¨ der, 2000; o Br¨ der and Schiffer, 2003; Lee and Cummins, 2004; Newell and Shanks, 2003; Newell o et al. [sent-16, score-0.21]
</p><p>9 ) Among the fast and frugal heuristics there is an algorithm called “take-the-best” (TTB) that is considered a process model for human judgments based on one-reason decision making. [sent-20, score-0.27]
</p><p>10 Which of the two cities has a larger population: (a) D¨ sseldorf (b) Hamburg? [sent-21, score-0.16]
</p><p>11 The available information on each city consists of the values of nine binary cues, or attributes, indicating  Hamburg Essen D¨ sseldorf u Validity  Soccer Team 1 0 0 1  State Capital 1 0 1 1/2  License Plate 0 1 1 0  Table 1: Part of the German cities task of Gigerenzer and Goldstein (1996). [sent-23, score-0.197]
</p><p>12 Shown are proﬁles and validities of three cues for three cities. [sent-24, score-0.218]
</p><p>13 The original data has different validities but the same cue ranking. [sent-26, score-0.55]
</p><p>14 The cues being used are, for instance, whether the city is a state capital, whether it is indicated on car license plates by a single letter, or whether it has a soccer team in the national league. [sent-28, score-0.211]
</p><p>15 The judgment which city is larger is made on the basis of the two binary vectors, or cue proﬁles, representing the two cities. [sent-29, score-0.485]
</p><p>16 TTB performs a lexicographic strategy, comparing the cues one after the other and using the ﬁrst cue that discriminates as the one reason to yield the ﬁnal decision. [sent-30, score-0.884]
</p><p>17 If the cue values of both cities are equal, the algorithm passes on to the next cue. [sent-32, score-0.521]
</p><p>18 The validity of a cue is a real number in the interval [0, 1] that is computed in terms of the known outcomes of paired comparisons. [sent-35, score-0.539]
</p><p>19 It is deﬁned as the number of pairs the cue discriminates correctly (i. [sent-36, score-0.592]
</p><p>20 , where it makes a correct inference) divided by the number of pairs it discriminates (i. [sent-38, score-0.12]
</p><p>21 TTB always chooses a cue with the highest validity, that is, it “takes the best” among those cues not yet considered. [sent-41, score-0.564]
</p><p>22 Table 1 shows cue proﬁles and validities for three cities. [sent-42, score-0.55]
</p><p>23 The ordering deﬁned by the size of their population is given by { D¨ sseldorf , Essen , D¨ sseldorf , Hamburg , Essen , Hamburg }, u u where a pair a, b indicates that a has less inhabitants than b. [sent-43, score-0.305]
</p><p>24 As an example for calculating the validity, the state-capital cue distinguishes the ﬁrst and the third pair but is correct only on the latter. [sent-44, score-0.501]
</p><p>25 The order in which the cues are ranked is crucial for success or failure of TTB. [sent-46, score-0.116]
</p><p>26 In the example of D¨ sseldorf and Hamburg, the car-license-plate cue would yield that D¨ sseldorf (D) u u is larger than Hamburg (HH), whereas the soccer-team cue would correctly favor Hamburg. [sent-47, score-1.094]
</p><p>27 Thus, how successful a lexicographic strategy is in a comparison task consisting of a partial ordering of cue proﬁles depends on how well the cue ranking minimizes the number of incorrect comparisons. [sent-48, score-1.502]
</p><p>28 Speciﬁcally, the accuracy of TTB relies on the degree of optimality achieved by the ranking according to decreasing cue validities. [sent-49, score-0.473]
</p><p>29 For TTB and the German cities task, computer simulations have shown that TTB discriminates at least as accurate as other models (Gigerenzer and Goldstein, 1996; Gigerenzer et al. [sent-50, score-0.2]
</p><p>30 TTB made as many correct inferences as standard algorithms proposed by cognitive psychology and even outperformed some of them. [sent-52, score-0.171]
</p><p>31 Here we subject the problem of ﬁnding optimal cue orderings to a rigorous theoretical analysis employing methods from the theory of computational complexity (Ausiello et al. [sent-54, score-0.531]
</p><p>32 Given a list of ordered pairs, it computes all cue validities in polynomially many computing steps in terms of the size of the list. [sent-57, score-0.55]
</p><p>33 We deﬁne the optimization problem M INIMUM I NCORRECT L EXICOGRAPHIC S TRATEGY as the task of minimizing the number of incorrect inferences for the lexicographic strategy on a given list of pairs. [sent-58, score-0.664]
</p><p>34 As an extension of TTB we consider an algorithm for ﬁnding cue orderings that was called “TTB by Conditional Validity” in the context of bounded rationality. [sent-61, score-0.509]
</p><p>35 This greedy algorithm runs in polynomial time and we derive tight bounds for it, showing that it approximates the optimum with a performance ratio proportional to the number of cues. [sent-63, score-0.209]
</p><p>36 An important consequence of this result is a guarantee that for those instances that have a solution that discriminates all pairs correctly, the greedy algorithm always ﬁnds a permutation attaining this minimum. [sent-64, score-0.359]
</p><p>37 We are not aware that this quality has been established for any of the previously studied heuristics for paired comparison. [sent-65, score-0.11]
</p><p>38 In addition, we show that TTB does not have this property, concluding that the greedy method of constructing cue permutations performs provably better than TTB. [sent-66, score-0.557]
</p><p>39 2  Lexicographic Strategies  A lexicographic strategy is a method for comparing elements of a set B ⊆ {0, 1}n . [sent-68, score-0.295]
</p><p>40 , bn ), the lexicographic strategy searches for the smallest cue index i ∈ {1, . [sent-79, score-0.743]
</p><p>41 The strategy then outputs one of “ < ” or “ > ” according to whether ai < bi or ai > bi assuming the usual order 0 < 1 of the truth values. [sent-83, score-0.156]
</p><p>42 If no such cue exists, the strategy returns “ = ”. [sent-84, score-0.496]
</p><p>43 , n + 1} be the function where diff(a, b) is the smallest cue index on which a and b are different, or n + 1 if they are equal, that is, diff(a, b)  =  min{{i : ai = bi } ∪ {n + 1}}. [sent-88, score-0.502]
</p><p>44 Then, the function S : B × B → {“ < ”, “ = ”, “ > ”} computed by the lexicographic strategy is   “ < ” if diff(a, b) ≤ n and adiff(a,b) < bdiff(a,b) , “ > ” if diff(a, b) ≤ n and adiff(a,b) > bdiff(a,b) , S(a, b) =  “ = ” otherwise. [sent-89, score-0.295]
</p><p>45 Lexicographic strategies may take into account that the cues come in an order that is different from 1, . [sent-90, score-0.156]
</p><p>46 The lexicographic strategy under cue permutation π passes through the cues in the order π(1), . [sent-109, score-1.047]
</p><p>47 The problem we study is that of ﬁnding a cue permutation that minimizes the number of incorrect comparisons in a given list of element pairs using the lexicographic strategy. [sent-113, score-1.255]
</p><p>48 Given a cue permutation π, we say that the lexicographic strategy under π infers the pair a, b correctly if Sπ (a, b) ∈ {“ < ”, “ = ”}, otherwise the inference is incorrect. [sent-116, score-1.008]
</p><p>49 The task is to ﬁnd a permutation π such that the number of incorrect inferences in L using Sπ is minimal, that is, a permutation π that minimizes INCORRECT(π, L)  = |{ a, b ∈ L : Sπ (a, b) = “ > ”}|. [sent-117, score-0.745]
</p><p>50 Here, we prove that, if P = NP, there is no polynomial-time algorithm whose solutions yield a number of incorrect comparisons that is by at most a constant factor larger than the minimal number possible. [sent-122, score-0.327]
</p><p>51 It follows that the problem of approximating the optimal cue permutation is even harder than any problem in APX. [sent-123, score-0.636]
</p><p>52 Measure: The number of incorrect inferences in L for the lexicographic strategy under cue permutation π, that is, INCORRECT(π, L). [sent-127, score-1.3]
</p><p>53 Solution: A hitting set for C, that is, a subset U ′ ⊆ U such that U ′ contains at least one element from each subset in C. [sent-132, score-0.137]
</p><p>54 Measure: The cardinality of the hitting set, that is, |U ′ |. [sent-133, score-0.137]
</p><p>55 For every r, there is no polynomial-time algorithm that approximates M INI MUM I NCORRECT L EXICOGRAPHIC S TRATEGY to within a factor of r, unless P = NP. [sent-139, score-0.113]
</p><p>56 We show that the existence of a polynomial-time algorithm that approximates M IN IMUM I NCORRECT L EXICOGRAPHIC S TRATEGY to within some constant factor implies the existence of a polynomial-time algorithm that approximates M INIMUM H ITTING S ET to within the same factor. [sent-141, score-0.134]
</p><p>57 We ﬁrst deﬁne a function f that is computable in polynomial time and maps each instance of M INIMUM H ITTING S ET to an instance of M INIMUM I NCORRECT L EXICOGRAPHIC S TRATEGY. [sent-148, score-0.147]
</p><p>58 (1) In the following, a pair from the ﬁrst and second set on the right-hand side of equation (1) is referred to as an element pair and a subset pair, respectively. [sent-184, score-0.146]
</p><p>59 If C has a hitting set of cardinality k or less then f (C) has a cue permutation π where INCORRECT(π, L) ≤ k. [sent-189, score-0.773]
</p><p>60 To prove this, assume without loss of generality that C has a hitting set U ′ of cardinality exactly k, say U ′ = {uj1 , . [sent-190, score-0.137]
</p><p>61 results in no more than k incorrect inferences in L. [sent-204, score-0.369]
</p><p>62 Hence, the ﬁrst cue that distinguishes this pair has value 0 in (1i1 ,. [sent-216, score-0.501]
</p><p>63 Further, let (1, 0), (1i , 1) be an element pair with ui ∈ U ′ . [sent-220, score-0.171]
</p><p>64 This pair is distinguished correctly by cue n + 1. [sent-221, score-0.562]
</p><p>65 Finally, each element pair (1, 0), (1i , 1) with ui ∈ U ′ is distinguished by cue i with a result that disagrees with the ordering given by L. [sent-222, score-0.705]
</p><p>66 Thus, only element pairs with ui ∈ U ′ yield incorrect comparisons and no subset pair. [sent-223, score-0.45]
</p><p>67 Hence, the number of incorrect inferences is not larger than |U ′ |. [sent-224, score-0.369]
</p><p>68 Next, we deﬁne a polynomial-time computable function g that maps each collection C of subsets of a ﬁnite set U and each cue permutation π for f (C) to a subset of U . [sent-225, score-0.677]
</p><p>69 For every element pair (1, 0), (1i , 1) ∈ L that is compared incorrectly by π, let ui ∈ g(C, π). [sent-227, score-0.171]
</p><p>70 If INCORRECT(π, L) ≤ k then g(C, π) is a hitting set of cardinality k or less for C. [sent-240, score-0.137]
</p><p>71 Hence, we have correct comparisons for the element pairs corresponding to ui1 , . [sent-250, score-0.135]
</p><p>72 As the subset pair is distinguished correctly, one of the cues i1 , . [sent-257, score-0.206]
</p><p>73 This contradicts the assertion that the comparisons for these element pairs are all correct. [sent-265, score-0.135]
</p><p>74 Thus, g(C, π) is a hitting set and the claim is established. [sent-266, score-0.137]
</p><p>75 We show that it approximates M INIMUM  Algorithm 1 G REEDY C UE P ERMUTATION Input: a set B ⊆ {0, 1}n and a set L ⊆ B × B Output: a cue permutation π for n cues I := {1, . [sent-270, score-0.798]
</p><p>76 , n do let j ∈ I be a cue where INCORRECT(j, L) = minj ′ ∈I INCORRECT(j ′ , L); π(i) := j; I := I \ {j}; L := L \ { a, b : aj = bj } end for. [sent-276, score-0.448]
</p><p>77 The idea is to select the ﬁrst cue according to which single cue makes a minimum number of incorrect inferences (choosing one arbitrarily if there are two or more). [sent-286, score-1.265]
</p><p>78 After that the algorithm removes those pairs that are distinguished by the selected cue, which is reasonable as the distinctions drawn by this cue cannot be undone by later cues. [sent-287, score-0.532]
</p><p>79 It employs an extension of the function INCORRECT applicable to single cues, such that for a cue i we have INCORRECT(i, L)  = |{ a, b ∈ L : ai > bi }|. [sent-290, score-0.502]
</p><p>80 In particular, it always ﬁnds a cue permutation with no incorrect inferences if one exists. [sent-297, score-1.005]
</p><p>81 We show by induction on n that the permutation returned by the algorithm makes a number of incorrect inferences no larger than n · opt(L). [sent-299, score-0.557]
</p><p>82 If n = 1, the optimal cue  001 , 010 010 , 100 010 , 101 100 , 111 Figure 1: A set of lexicographically ordered pairs with nondecreasing cue validities (1, 1/2, and 2/3). [sent-300, score-1.074]
</p><p>83 The cue ordering of TTB (1, 3, 2) causes an incorrect inference on the ﬁrst pair. [sent-301, score-0.734]
</p><p>84 By Theorem 2, G REEDY C UE P ERMUTATION ﬁnds the lexicographic ordering. [sent-302, score-0.247]
</p><p>85 Clearly, as the incorrect inferences of a cue cannot be reversed by other cues, there is a cue j with INCORRECT(j, L) ≤ opt(L). [sent-305, score-1.265]
</p><p>86 The algorithm selects such a cue in the ﬁrst round of the loop. [sent-306, score-0.448]
</p><p>87 During the rest of the rounds, a permutation of n − 1 cues is constructed for the set of remaining pairs. [sent-307, score-0.304]
</p><p>88 Let j be the cue that is chosen in the ﬁrst round, I ′ = {1, . [sent-308, score-0.448]
</p><p>89 Further, let optI ′ (L′ ) denote the minimum number of incorrect inferences taken over the permutations of I ′ on the set L′ . [sent-315, score-0.427]
</p><p>90 ) The equality holds as cue j does not distinguish any pair in L′ . [sent-319, score-0.501]
</p><p>91 By the induction hypothesis, rounds 2 to n of the loop determine a cue permutation π ′ with INCORRECT(π ′ , L′ ) ≤ (n − 1) · optI ′ (L′ ). [sent-320, score-0.636]
</p><p>92 Thus, the number of incorrect inferences made by the permutation π ﬁnally returned by the algorithm satisﬁes INCORRECT(π, L) ≤ INCORRECT(j, L) + (n − 1) · optI ′ (L′ ), which is, by the inequalities derived above, not larger than opt(L) + (n − 1) · opt(L) as stated. [sent-321, score-0.557]
</p><p>93 On inputs that have a cue ordering without incorrect comparisons under the lexicographic strategy, G REEDY C UE P ERMUTATION can be better than TTB. [sent-323, score-1.029]
</p><p>94 According to Theorem 2, G REEDY C UE P ERMUTATION comes up with the given permutation of the cues. [sent-326, score-0.188]
</p><p>95 Thus, TTB ranks the cues as 1, 3, 2 whereupon the ﬁrst pair is inferred incorrectly. [sent-328, score-0.169]
</p><p>96 A further question is concerned with a speciﬁc fast and frugal heuristic: How accurate is TTB? [sent-335, score-0.127]
</p><p>97 Assessing the empirical validity of the “take-the-best” heuristic as a model of o human probabilistic inference. [sent-360, score-0.133]
</p><p>98 Take the best versus simultaneous feature matching: Probabilistic o inferences from memory and effects of representation format. [sent-369, score-0.132]
</p><p>99 Reasoning the fast and frugal way: Models of bounded rationality. [sent-375, score-0.159]
</p><p>100 Fast, frugal, and ﬁt: Simple heuristics for paired comparison. [sent-400, score-0.11]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cue', 0.448), ('inimum', 0.335), ('lexicographic', 0.247), ('ttb', 0.247), ('incorrect', 0.237), ('permutation', 0.188), ('opt', 0.177), ('exicographic', 0.175), ('reedy', 0.16), ('ue', 0.16), ('itting', 0.145), ('ncorrect', 0.139), ('gigerenzer', 0.139), ('inferences', 0.132), ('ermutation', 0.131), ('trategy', 0.131), ('cues', 0.116), ('validities', 0.102), ('frugal', 0.101), ('hitting', 0.097), ('hamburg', 0.087), ('martignon', 0.087), ('sseldorf', 0.087), ('todd', 0.087), ('heuristics', 0.085), ('ui', 0.078), ('cities', 0.073), ('diff', 0.073), ('discriminates', 0.073), ('goldstein', 0.069), ('validity', 0.066), ('br', 0.061), ('ausiello', 0.058), ('newell', 0.058), ('opti', 0.058), ('permutations', 0.058), ('polynomial', 0.056), ('et', 0.054), ('pair', 0.053), ('greedy', 0.051), ('rationality', 0.051), ('ordering', 0.049), ('strategy', 0.048), ('comparisons', 0.048), ('pairs', 0.047), ('approximates', 0.046), ('approximability', 0.044), ('bellare', 0.044), ('dieckmann', 0.044), ('der', 0.043), ('factor', 0.042), ('computable', 0.041), ('element', 0.04), ('cardinality', 0.04), ('claim', 0.04), ('strategies', 0.04), ('psychology', 0.039), ('heuristic', 0.039), ('german', 0.038), ('essen', 0.038), ('schmitt', 0.038), ('distinguished', 0.037), ('city', 0.037), ('shanks', 0.035), ('psychological', 0.033), ('bounded', 0.032), ('les', 0.032), ('bi', 0.031), ('optimum', 0.03), ('decision', 0.03), ('adiff', 0.029), ('bdiff', 0.029), ('cummins', 0.029), ('hoffrage', 0.029), ('inhabitants', 0.029), ('karelaia', 0.029), ('lexicographically', 0.029), ('license', 0.029), ('ludwigsburg', 0.029), ('nellen', 0.029), ('orderings', 0.029), ('organizational', 0.029), ('schiffer', 0.029), ('slegers', 0.029), ('soccer', 0.029), ('ujk', 0.029), ('nds', 0.028), ('human', 0.028), ('pro', 0.027), ('rational', 0.027), ('ratio', 0.026), ('fast', 0.026), ('np', 0.026), ('hogarth', 0.025), ('instance', 0.025), ('unless', 0.025), ('paired', 0.025), ('ranking', 0.025), ('correctly', 0.024), ('ai', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="146-tfidf-1" href="./nips-2005-On_the_Accuracy_of_Bounded_Rationality%3A_How_Far_from_Optimal_Is_Fast_and_Frugal%3F.html">146 nips-2005-On the Accuracy of Bounded Rationality: How Far from Optimal Is Fast and Frugal?</a></p>
<p>Author: Michael Schmitt, Laura Martignon</p><p>Abstract: Fast and frugal heuristics are well studied models of bounded rationality. Psychological research has proposed the take-the-best heuristic as a successful strategy in decision making with limited resources. Take-thebest searches for a sufﬁciently good ordering of cues (features) in a task where objects are to be compared lexicographically. We investigate the complexity of the problem of approximating optimal cue permutations for lexicographic strategies. We show that no efﬁcient algorithm can approximate the optimum to within any constant factor, if P = NP. We further consider a greedy approach for building lexicographic strategies and derive tight bounds for the performance ratio of a new and simple algorithm. This algorithm is proven to perform better than take-the-best. 1</p><p>2 0.12469267 <a title="146-tfidf-2" href="./nips-2005-Learning_Cue-Invariant_Visual_Responses.html">109 nips-2005-Learning Cue-Invariant Visual Responses</a></p>
<p>Author: Jarmo Hurri</p><p>Abstract: Multiple visual cues are used by the visual system to analyze a scene; achromatic cues include luminance, texture, contrast and motion. Singlecell recordings have shown that the mammalian visual cortex contains neurons that respond similarly to scene structure (e.g., orientation of a boundary), regardless of the cue type conveying this information. This paper shows that cue-invariant response properties of simple- and complex-type cells can be learned from natural image data in an unsupervised manner. In order to do this, we also extend a previous conceptual model of cue invariance so that it can be applied to model simple- and complex-cell responses. Our results relate cue-invariant response properties to natural image statistics, thereby showing how the statistical modeling approach can be used to model processing beyond the elemental response properties visual neurons. This work also demonstrates how to learn, from natural image data, more sophisticated feature detectors than those based on changes in mean luminance, thereby paving the way for new data-driven approaches to image processing and computer vision. 1</p><p>3 0.11426675 <a title="146-tfidf-3" href="./nips-2005-Optimal_cue_selection_strategy.html">149 nips-2005-Optimal cue selection strategy</a></p>
<p>Author: Vidhya Navalpakkam, Laurent Itti</p><p>Abstract: Survival in the natural world demands the selection of relevant visual cues to rapidly and reliably guide attention towards prey and predators in cluttered environments. We investigate whether our visual system selects cues that guide search in an optimal manner. We formally obtain the optimal cue selection strategy by maximizing the signal to noise ratio (SN R) between a search target and surrounding distractors. This optimal strategy successfully accounts for several phenomena in visual search behavior, including the effect of target-distractor discriminability, uncertainty in target’s features, distractor heterogeneity, and linear separability. Furthermore, the theory generates a new prediction, which we verify through psychophysical experiments with human subjects. Our results provide direct experimental evidence that humans select visual cues so as to maximize SN R between the targets and surrounding clutter.</p><p>4 0.079525881 <a title="146-tfidf-4" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>Author: Afsheen Afshar, Gopal Santhanam, Stephen I. Ryu, Maneesh Sahani, Byron M. Yu, Krishna V. Shenoy</p><p>Abstract: Spiking activity from neurophysiological experiments often exhibits dynamics beyond that driven by external stimulation, presumably reﬂecting the extensive recurrence of neural circuitry. Characterizing these dynamics may reveal important features of neural computation, particularly during internally-driven cognitive operations. For example, the activity of premotor cortex (PMd) neurons during an instructed delay period separating movement-target speciﬁcation and a movementinitiation cue is believed to be involved in motor planning. We show that the dynamics underlying this activity can be captured by a lowdimensional non-linear dynamical systems model, with underlying recurrent structure and stochastic point-process output. We present and validate latent variable methods that simultaneously estimate the system parameters and the trial-by-trial dynamical trajectories. These methods are applied to characterize the dynamics in PMd data recorded from a chronically-implanted 96-electrode array while monkeys perform delayed-reach tasks. 1</p><p>5 0.076517805 <a title="146-tfidf-5" href="./nips-2005-The_Information-Form_Data_Association_Filter.html">192 nips-2005-The Information-Form Data Association Filter</a></p>
<p>Author: Brad Schumitsch, Sebastian Thrun, Gary Bradski, Kunle Olukotun</p><p>Abstract: This paper presents a new ﬁlter for online data association problems in high-dimensional spaces. The key innovation is a representation of the data association posterior in information form, in which the “proximity” of objects and tracks are expressed by numerical links. Updating these links requires linear time, compared to exponential time required for computing the exact posterior probabilities. The paper derives the algorithm formally and provides comparative results using data obtained by a real-world camera array and by a large-scale sensor network simulation.</p><p>6 0.044366721 <a title="146-tfidf-6" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>7 0.042173073 <a title="146-tfidf-7" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>8 0.037544768 <a title="146-tfidf-8" href="./nips-2005-Active_Learning_For_Identifying_Function_Threshold_Boundaries.html">18 nips-2005-Active Learning For Identifying Function Threshold Boundaries</a></p>
<p>9 0.035435159 <a title="146-tfidf-9" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>10 0.031939365 <a title="146-tfidf-10" href="./nips-2005-Bayesian_models_of_human_action_understanding.html">36 nips-2005-Bayesian models of human action understanding</a></p>
<p>11 0.029439278 <a title="146-tfidf-11" href="./nips-2005-Prediction_and_Change_Detection.html">156 nips-2005-Prediction and Change Detection</a></p>
<p>12 0.029107453 <a title="146-tfidf-12" href="./nips-2005-Spectral_Bounds_for_Sparse_PCA%3A_Exact_and_Greedy_Algorithms.html">180 nips-2005-Spectral Bounds for Sparse PCA: Exact and Greedy Algorithms</a></p>
<p>13 0.02829138 <a title="146-tfidf-13" href="./nips-2005-Fusion_of_Similarity_Data_in_Clustering.html">79 nips-2005-Fusion of Similarity Data in Clustering</a></p>
<p>14 0.028148618 <a title="146-tfidf-14" href="./nips-2005-Top-Down_Control_of_Visual_Attention%3A_A_Rational_Account.html">194 nips-2005-Top-Down Control of Visual Attention: A Rational Account</a></p>
<p>15 0.027947597 <a title="146-tfidf-15" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>16 0.02782305 <a title="146-tfidf-16" href="./nips-2005-Scaling_Laws_in_Natural_Scenes_and_the_Inference_of_3D_Shape.html">170 nips-2005-Scaling Laws in Natural Scenes and the Inference of 3D Shape</a></p>
<p>17 0.027764224 <a title="146-tfidf-17" href="./nips-2005-Asymptotics_of_Gaussian_Regularized_Least_Squares.html">31 nips-2005-Asymptotics of Gaussian Regularized Least Squares</a></p>
<p>18 0.025693759 <a title="146-tfidf-18" href="./nips-2005-Unbiased_Estimator_of_Shape_Parameter_for_Spiking_Irregularities_under_Changing_Environments.html">197 nips-2005-Unbiased Estimator of Shape Parameter for Spiking Irregularities under Changing Environments</a></p>
<p>19 0.024163969 <a title="146-tfidf-19" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>20 0.024113569 <a title="146-tfidf-20" href="./nips-2005-TD%280%29_Leads_to_Better_Policies_than_Approximate_Value_Iteration.html">186 nips-2005-TD(0) Leads to Better Policies than Approximate Value Iteration</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.096), (1, -0.014), (2, 0.018), (3, 0.041), (4, 0.015), (5, 0.058), (6, -0.043), (7, -0.03), (8, -0.057), (9, 0.045), (10, 0.004), (11, 0.014), (12, -0.026), (13, -0.05), (14, -0.009), (15, 0.05), (16, 0.068), (17, 0.049), (18, -0.009), (19, -0.018), (20, 0.092), (21, 0.047), (22, -0.023), (23, -0.045), (24, 0.053), (25, -0.071), (26, 0.117), (27, 0.093), (28, 0.071), (29, 0.076), (30, -0.075), (31, 0.002), (32, 0.251), (33, 0.051), (34, -0.012), (35, 0.061), (36, -0.087), (37, 0.008), (38, -0.077), (39, -0.152), (40, -0.013), (41, 0.004), (42, 0.105), (43, 0.048), (44, 0.261), (45, 0.034), (46, 0.059), (47, -0.038), (48, 0.08), (49, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96036941 <a title="146-lsi-1" href="./nips-2005-On_the_Accuracy_of_Bounded_Rationality%3A_How_Far_from_Optimal_Is_Fast_and_Frugal%3F.html">146 nips-2005-On the Accuracy of Bounded Rationality: How Far from Optimal Is Fast and Frugal?</a></p>
<p>Author: Michael Schmitt, Laura Martignon</p><p>Abstract: Fast and frugal heuristics are well studied models of bounded rationality. Psychological research has proposed the take-the-best heuristic as a successful strategy in decision making with limited resources. Take-thebest searches for a sufﬁciently good ordering of cues (features) in a task where objects are to be compared lexicographically. We investigate the complexity of the problem of approximating optimal cue permutations for lexicographic strategies. We show that no efﬁcient algorithm can approximate the optimum to within any constant factor, if P = NP. We further consider a greedy approach for building lexicographic strategies and derive tight bounds for the performance ratio of a new and simple algorithm. This algorithm is proven to perform better than take-the-best. 1</p><p>2 0.51646686 <a title="146-lsi-2" href="./nips-2005-Learning_Cue-Invariant_Visual_Responses.html">109 nips-2005-Learning Cue-Invariant Visual Responses</a></p>
<p>Author: Jarmo Hurri</p><p>Abstract: Multiple visual cues are used by the visual system to analyze a scene; achromatic cues include luminance, texture, contrast and motion. Singlecell recordings have shown that the mammalian visual cortex contains neurons that respond similarly to scene structure (e.g., orientation of a boundary), regardless of the cue type conveying this information. This paper shows that cue-invariant response properties of simple- and complex-type cells can be learned from natural image data in an unsupervised manner. In order to do this, we also extend a previous conceptual model of cue invariance so that it can be applied to model simple- and complex-cell responses. Our results relate cue-invariant response properties to natural image statistics, thereby showing how the statistical modeling approach can be used to model processing beyond the elemental response properties visual neurons. This work also demonstrates how to learn, from natural image data, more sophisticated feature detectors than those based on changes in mean luminance, thereby paving the way for new data-driven approaches to image processing and computer vision. 1</p><p>3 0.49085793 <a title="146-lsi-3" href="./nips-2005-Optimal_cue_selection_strategy.html">149 nips-2005-Optimal cue selection strategy</a></p>
<p>Author: Vidhya Navalpakkam, Laurent Itti</p><p>Abstract: Survival in the natural world demands the selection of relevant visual cues to rapidly and reliably guide attention towards prey and predators in cluttered environments. We investigate whether our visual system selects cues that guide search in an optimal manner. We formally obtain the optimal cue selection strategy by maximizing the signal to noise ratio (SN R) between a search target and surrounding distractors. This optimal strategy successfully accounts for several phenomena in visual search behavior, including the effect of target-distractor discriminability, uncertainty in target’s features, distractor heterogeneity, and linear separability. Furthermore, the theory generates a new prediction, which we verify through psychophysical experiments with human subjects. Our results provide direct experimental evidence that humans select visual cues so as to maximize SN R between the targets and surrounding clutter.</p><p>4 0.43328428 <a title="146-lsi-4" href="./nips-2005-Principles_of_real-time_computing_with_feedback_applied_to_cortical_microcircuit_models.html">157 nips-2005-Principles of real-time computing with feedback applied to cortical microcircuit models</a></p>
<p>Author: Wolfgang Maass, Prashant Joshi, Eduardo D. Sontag</p><p>Abstract: The network topology of neurons in the brain exhibits an abundance of feedback connections, but the computational function of these feedback connections is largely unknown. We present a computational theory that characterizes the gain in computational power achieved through feedback in dynamical systems with fading memory. It implies that many such systems acquire through feedback universal computational capabilities for analog computing with a non-fading memory. In particular, we show that feedback enables such systems to process time-varying input streams in diverse ways according to rules that are implemented through internal states of the dynamical system. In contrast to previous attractor-based computational models for neural networks, these ﬂexible internal states are high-dimensional attractors of the circuit dynamics, that still allow the circuit state to absorb new information from online input streams. In this way one arrives at novel models for working memory, integration of evidence, and reward expectation in cortical circuits. We show that they are applicable to circuits of conductance-based Hodgkin-Huxley (HH) neurons with high levels of noise that reﬂect experimental data on invivo conditions. 1</p><p>5 0.32089019 <a title="146-lsi-5" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>Author: Afsheen Afshar, Gopal Santhanam, Stephen I. Ryu, Maneesh Sahani, Byron M. Yu, Krishna V. Shenoy</p><p>Abstract: Spiking activity from neurophysiological experiments often exhibits dynamics beyond that driven by external stimulation, presumably reﬂecting the extensive recurrence of neural circuitry. Characterizing these dynamics may reveal important features of neural computation, particularly during internally-driven cognitive operations. For example, the activity of premotor cortex (PMd) neurons during an instructed delay period separating movement-target speciﬁcation and a movementinitiation cue is believed to be involved in motor planning. We show that the dynamics underlying this activity can be captured by a lowdimensional non-linear dynamical systems model, with underlying recurrent structure and stochastic point-process output. We present and validate latent variable methods that simultaneously estimate the system parameters and the trial-by-trial dynamical trajectories. These methods are applied to characterize the dynamics in PMd data recorded from a chronically-implanted 96-electrode array while monkeys perform delayed-reach tasks. 1</p><p>6 0.31490132 <a title="146-lsi-6" href="./nips-2005-Top-Down_Control_of_Visual_Attention%3A_A_Rational_Account.html">194 nips-2005-Top-Down Control of Visual Attention: A Rational Account</a></p>
<p>7 0.30296338 <a title="146-lsi-7" href="./nips-2005-Spectral_Bounds_for_Sparse_PCA%3A_Exact_and_Greedy_Algorithms.html">180 nips-2005-Spectral Bounds for Sparse PCA: Exact and Greedy Algorithms</a></p>
<p>8 0.27174601 <a title="146-lsi-8" href="./nips-2005-A_Bayesian_Spatial_Scan_Statistic.html">4 nips-2005-A Bayesian Spatial Scan Statistic</a></p>
<p>9 0.25632143 <a title="146-lsi-9" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>10 0.25246063 <a title="146-lsi-10" href="./nips-2005-The_Information-Form_Data_Association_Filter.html">192 nips-2005-The Information-Form Data Association Filter</a></p>
<p>11 0.24768724 <a title="146-lsi-11" href="./nips-2005-Norepinephrine_and_Neural_Interrupts.html">141 nips-2005-Norepinephrine and Neural Interrupts</a></p>
<p>12 0.21248436 <a title="146-lsi-12" href="./nips-2005-Asymptotics_of_Gaussian_Regularized_Least_Squares.html">31 nips-2005-Asymptotics of Gaussian Regularized Least Squares</a></p>
<p>13 0.2093316 <a title="146-lsi-13" href="./nips-2005-Factorial_Switching_Kalman_Filters_for_Condition_Monitoring_in_Neonatal_Intensive_Care.html">68 nips-2005-Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care</a></p>
<p>14 0.20679966 <a title="146-lsi-14" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>15 0.20190288 <a title="146-lsi-15" href="./nips-2005-Oblivious_Equilibrium%3A_A_Mean_Field_Approximation_for_Large-Scale_Dynamic_Games.html">142 nips-2005-Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games</a></p>
<p>16 0.19590849 <a title="146-lsi-16" href="./nips-2005-A_Bayesian_Framework_for_Tilt_Perception_and_Confidence.html">3 nips-2005-A Bayesian Framework for Tilt Perception and Confidence</a></p>
<p>17 0.19561975 <a title="146-lsi-17" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>18 0.19199014 <a title="146-lsi-18" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>19 0.18672934 <a title="146-lsi-19" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<p>20 0.18362488 <a title="146-lsi-20" href="./nips-2005-The_Forgetron%3A_A_Kernel-Based_Perceptron_on_a_Fixed_Budget.html">191 nips-2005-The Forgetron: A Kernel-Based Perceptron on a Fixed Budget</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.064), (10, 0.038), (27, 0.014), (31, 0.033), (33, 0.467), (34, 0.056), (39, 0.011), (41, 0.011), (55, 0.054), (69, 0.063), (73, 0.026), (88, 0.03), (91, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81286329 <a title="146-lda-1" href="./nips-2005-On_the_Accuracy_of_Bounded_Rationality%3A_How_Far_from_Optimal_Is_Fast_and_Frugal%3F.html">146 nips-2005-On the Accuracy of Bounded Rationality: How Far from Optimal Is Fast and Frugal?</a></p>
<p>Author: Michael Schmitt, Laura Martignon</p><p>Abstract: Fast and frugal heuristics are well studied models of bounded rationality. Psychological research has proposed the take-the-best heuristic as a successful strategy in decision making with limited resources. Take-thebest searches for a sufﬁciently good ordering of cues (features) in a task where objects are to be compared lexicographically. We investigate the complexity of the problem of approximating optimal cue permutations for lexicographic strategies. We show that no efﬁcient algorithm can approximate the optimum to within any constant factor, if P = NP. We further consider a greedy approach for building lexicographic strategies and derive tight bounds for the performance ratio of a new and simple algorithm. This algorithm is proven to perform better than take-the-best. 1</p><p>2 0.56907839 <a title="146-lda-2" href="./nips-2005-Principles_of_real-time_computing_with_feedback_applied_to_cortical_microcircuit_models.html">157 nips-2005-Principles of real-time computing with feedback applied to cortical microcircuit models</a></p>
<p>Author: Wolfgang Maass, Prashant Joshi, Eduardo D. Sontag</p><p>Abstract: The network topology of neurons in the brain exhibits an abundance of feedback connections, but the computational function of these feedback connections is largely unknown. We present a computational theory that characterizes the gain in computational power achieved through feedback in dynamical systems with fading memory. It implies that many such systems acquire through feedback universal computational capabilities for analog computing with a non-fading memory. In particular, we show that feedback enables such systems to process time-varying input streams in diverse ways according to rules that are implemented through internal states of the dynamical system. In contrast to previous attractor-based computational models for neural networks, these ﬂexible internal states are high-dimensional attractors of the circuit dynamics, that still allow the circuit state to absorb new information from online input streams. In this way one arrives at novel models for working memory, integration of evidence, and reward expectation in cortical circuits. We show that they are applicable to circuits of conductance-based Hodgkin-Huxley (HH) neurons with high levels of noise that reﬂect experimental data on invivo conditions. 1</p><p>3 0.25424606 <a title="146-lda-3" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>Author: Yixin Chen, Ya Zhang, Xiang Ji</p><p>Abstract: We present a novel spectral clustering method that enables users to incorporate prior knowledge of the size of clusters into the clustering process. The cost function, which is named size regularized cut (SRcut), is deﬁned as the sum of the inter-cluster similarity and a regularization term measuring the relative size of two clusters. Finding a partition of the data set to minimize SRcut is proved to be NP-complete. An approximation algorithm is proposed to solve a relaxed version of the optimization problem as an eigenvalue problem. Evaluations over different data sets demonstrate that the method is not sensitive to outliers and performs better than normalized cut. 1</p><p>4 0.25396448 <a title="146-lda-4" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>Author: O. P. Kreidl, Alan S. Willsky</p><p>Abstract: Given a directed graphical model with binary-valued hidden nodes and real-valued noisy observations, consider deciding upon the maximum a-posteriori (MAP) or the maximum posterior-marginal (MPM) assignment under the restriction that each node broadcasts only to its children exactly one single-bit message. We present a variational formulation, viewing the processing rules local to all nodes as degrees-of-freedom, that minimizes the loss in expected (MAP or MPM) performance subject to such online communication constraints. The approach leads to a novel message-passing algorithm to be executed ofﬂine, or before observations are realized, which mitigates the performance loss by iteratively coupling all rules in a manner implicitly driven by global statistics. We also provide (i) illustrative examples, (ii) assumptions that guarantee convergence and efﬁciency and (iii) connections to active research areas. 1</p><p>5 0.25139421 <a title="146-lda-5" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>Author: Xuanlong Nguyen, Martin J. Wainwright, Michael I. Jordan</p><p>Abstract: In this paper, we provide a general theorem that establishes a correspondence between surrogate loss functions in classiﬁcation and the family of f -divergences. Moreover, we provide constructive procedures for determining the f -divergence induced by a given surrogate loss, and conversely for ﬁnding all surrogate loss functions that realize a given f -divergence. Next we introduce the notion of universal equivalence among loss functions and corresponding f -divergences, and provide necessary and sufﬁcient conditions for universal equivalence to hold. These ideas have applications to classiﬁcation problems that also involve a component of experiment design; in particular, we leverage our results to prove consistency of a procedure for learning a classiﬁer under decentralization requirements. 1</p><p>6 0.2512638 <a title="146-lda-6" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>7 0.24996464 <a title="146-lda-7" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>8 0.24686022 <a title="146-lda-8" href="./nips-2005-Learning_Minimum_Volume_Sets.html">112 nips-2005-Learning Minimum Volume Sets</a></p>
<p>9 0.24461201 <a title="146-lda-9" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>10 0.24353263 <a title="146-lda-10" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>11 0.24315022 <a title="146-lda-11" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>12 0.2410759 <a title="146-lda-12" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>13 0.240738 <a title="146-lda-13" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>14 0.24057081 <a title="146-lda-14" href="./nips-2005-Affine_Structure_From_Sound.html">20 nips-2005-Affine Structure From Sound</a></p>
<p>15 0.24049498 <a title="146-lda-15" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>16 0.2385204 <a title="146-lda-16" href="./nips-2005-Oblivious_Equilibrium%3A_A_Mean_Field_Approximation_for_Large-Scale_Dynamic_Games.html">142 nips-2005-Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games</a></p>
<p>17 0.23767945 <a title="146-lda-17" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>18 0.23765373 <a title="146-lda-18" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>19 0.23747586 <a title="146-lda-19" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>20 0.23734979 <a title="146-lda-20" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
