<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>139 nips-2005-Non-iterative Estimation with Perturbed Gaussian Markov Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-139" href="#">nips2005-139</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>139 nips-2005-Non-iterative Estimation with Perturbed Gaussian Markov Processes</h1>
<br/><p>Source: <a title="nips-2005-139-pdf" href="http://papers.nips.cc/paper/2908-non-iterative-estimation-with-perturbed-gaussian-markov-processes.pdf">pdf</a></p><p>Author: Yunsong Huang, B. Keith Jenkins</p><p>Abstract: We develop an approach for estimation with Gaussian Markov processes that imposes a smoothness prior while allowing for discontinuities. Instead of propagating information laterally between neighboring nodes in a graph, we study the posterior distribution of the hidden nodes as a whole—how it is perturbed by invoking discontinuities, or weakening the edges, in the graph. We show that the resulting computation amounts to feed-forward fan-in operations reminiscent of V1 neurons. Moreover, using suitable matrix preconditioners, the incurred matrix inverse and determinant can be approximated, without iteration, in the same computational style. Simulation results illustrate the merits of this approach.</p><p>Reference: <a title="nips-2005-139-reference" href="../nips2005_reference/nips-2005-Non-iterative_Estimation_with_Perturbed_Gaussian_Markov_Processes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We develop an approach for estimation with Gaussian Markov processes that imposes a smoothness prior while allowing for discontinuities. [sent-4, score-0.081]
</p><p>2 Instead of propagating information laterally between neighboring nodes in a graph, we study the posterior distribution of the hidden nodes as a whole—how it is perturbed by invoking discontinuities, or weakening the edges, in the graph. [sent-5, score-0.454]
</p><p>3 We show that the resulting computation amounts to feed-forward fan-in operations reminiscent of V1 neurons. [sent-6, score-0.077]
</p><p>4 Moreover, using suitable matrix preconditioners, the incurred matrix inverse and determinant can be approximated, without iteration, in the same computational style. [sent-7, score-0.216]
</p><p>5 For generative models, often the ease of generation and the ease of inference are two conﬂicting features. [sent-10, score-0.038]
</p><p>6 While the generation, or synthesis, of the input is immediate, the inference part is usually not. [sent-12, score-0.038]
</p><p>7 In vision applications, it’s suitable to employ smoothness priors admitting discontinuities [4]. [sent-18, score-0.073]
</p><p>8 Examples include weak membranes and plates [5], formulated in the context of variational energy minimization. [sent-19, score-0.171]
</p><p>9 Typically, the inference for MRF or graphical models would incur lateral propagation of information between neighboring units [6]. [sent-20, score-0.038]
</p><p>10 This is appealing in the sense that it consists of only simple, local operations carried out in parallel. [sent-21, score-0.043]
</p><p>11 However, the resulting latency could undermine the plausibility that such algorithms are employed in human early vision inference tasks [7]. [sent-22, score-0.079]
</p><p>12 In this paper we take the weak membrane and plate as instances of Gaussian processes (GP). [sent-23, score-0.389]
</p><p>13 We show that the effect of marking each discontinuity (hereafter termed as “bond-  breaking”) is to perturb the inverse of covariance matrix of the hidden nodes x by a matrix of rank 1. [sent-24, score-0.473]
</p><p>14 When multiple bonds are broken, the computation of the posterior mean and covariance of x would involve the inversion of a matrix, which typically has large condition number, implying very slow convergence in straight-forward iterative approaches. [sent-25, score-0.344]
</p><p>15 We show that there exists a family of preconditioners that can bring the condition number close to 1, thereby greatly speeding up the iteration—to the extent that a single step would sufﬁce in practice. [sent-26, score-0.191]
</p><p>16 In what follows, we perturb the potential matrix Q−1 by reducing the coupling 0 energy of certain bonds2 . [sent-31, score-0.242]
</p><p>17 This relieves the smoothness constraint on the nodes connected via those bonds. [sent-32, score-0.12]
</p><p>18 Suppose the energy reduction of a bond connecting node i and j (whose state vectors are xi and xj , respectively) can be expressed as (xT fi + xT fj )2 , where fi and fj are coefﬁcient i j vectors. [sent-33, score-1.005]
</p><p>19 This becomes (xT f )2 , if f is constructed to be a vector of same size as x, with the only non-zero entries fi and fj corresponding to node i and j. [sent-34, score-0.279]
</p><p>20 This manipulation can be identiﬁed with a rank-1 perturbation of Q−1 , as Q−1 ← Q−1 − f f T , which is equivalent 0 1 0 to xT Q−1 x ← xT Q−1 x − (xT f )2 , ∀x. [sent-35, score-0.205]
</p><p>21 We call this an elementary perturbation of Q−1 , 1 0 0 and f an elementary perturbation vector associated with the particular bond. [sent-36, score-0.488]
</p><p>22 1), we form the L perturbation vectors into a matrix F1 = [f 1 , . [sent-39, score-0.269]
</p><p>23 , f L ], and then the collective perturbations yield Q−1 1 and thus  Q1  T = Q−1 − F1 F1 0  (1)  = Q0 + Q0 F1 (I −  T T F1 Q0 F1 )−1 F1 Q0 ,  (2)  which follows from the Sherman-Morrison-Woodbury Formula (SMWF). [sent-42, score-0.036]
</p><p>24 1 Perturbing a membrane and a plate In a membrane model [5], xi is scalar and the energy of the bond connecting xi and xj is (xi − xj )2 /q, where q is a parameter denoting the variance of state noise. [sent-44, score-1.14]
</p><p>25 Upon perturba1 ensures positivity of the tion, this energy is reduced to η 2 (xi − xj )2 /q, where 0 < η energy. [sent-45, score-0.203]
</p><p>26 Then, the energy reduction is (1 − η 2 )(xi − xj )2 /q, from which we can identify fi = (1 − η 2 )/q and fj = −fi . [sent-46, score-0.371]
</p><p>27 In the case of a plate [5], xi = [ui , uh i , uv i ]T , in which ui represents the intensity, while uhi and uvi represent its gradient in the horizontal and vertical direction, respectively. [sent-47, score-0.761]
</p><p>28 (−,i) We deﬁne the energy of a horizontal bond connecting node j and i as E0 = (uv i − 2 (−,i) T −1 (−,i) O d , where uv j ) /q + d d(−,i) = 1 2  ui uh i  −  1 1 0 1  uj uhj  and O = q  1/3 1/2 1/2 1  ,  Henceforth called bonds, as edge will refer to intensity discontinuity in an image. [sent-48, score-1.099]
</p><p>29 the superscript (−, i) representing horizontal bond to the left of node i. [sent-51, score-0.42]
</p><p>30 If E0 is re(−,i) duced to E1 = [(uvi − uv j )2 + (uhi − uhj )2 ]/q, i. [sent-54, score-0.141]
</p><p>31 , coupling between node i and j exists only through their gradient values, one can show that the energy reduction (−,i) (−,i) E0 − E1 = [ui − uj − (uh i + uh j )/2]2 · 12/q. [sent-56, score-0.361]
</p><p>32 Taking the actual energy reduction to (−,i) (−,i) − E1 ), we can identify fi (−,i) = 12(1 − η 2 )/q[1, −1/2, 0]T be (1 − η 2 )(E0 (−,i) T and fj = 12(1 − η 2 )/q[−1, −1/2, 0] , where 0 < η 1 ensures the positive deﬁniteness of the resulting potential matrix. [sent-57, score-0.367]
</p><p>33 A similar procedure can be applied to a vertical bond in the plate, producing a perturbation vector f (|,i) , whose components are zero everywhere except for fi (|,i) = 12(1 − η 2 )/q[1, 0, −1/2]T and fj (|,i) = 12(1 − η 2 )/q[−1, 0, −1/2]T , for which node j is the lower neighbor of node i. [sent-58, score-0.896]
</p><p>34 One can verify that xT f = 0 when the plate assumes the shape of a linear slope, meaning that this perturbation produces no energy difference in such a case. [sent-59, score-0.566]
</p><p>35 (xT f )2 becomes signiﬁcant when the perturbed, or broken, bond associated with f straddles across a step discontinuity of the image. [sent-60, score-0.357]
</p><p>36 2 Hidden state estimation Standard formulae exist for the posterior covariance K and mean x of x, given a noisy ˆ observation3 y = Cx + n, where n ∼ N (0, rI). [sent-63, score-0.044]
</p><p>37 xα = Kα C T y/r, and Kα = [Q−1 + C T C/r]−1 , ˆ α for either the unperturbed (α = 0) or perturbed (α = 1) process. [sent-64, score-0.299]
</p><p>38 For example, K1 equals K0 —a circulant matrix—plus a rank-L perturbation (cf. [sent-67, score-0.264]
</p><p>39 Since each column of W1 is a spatially shifted copy of a prototypical vector, arising from breaking either a horizontal or a vertical −1 T bond, convolution can be utilized in computing W1 C T y. [sent-70, score-0.276]
</p><p>40 For instance, z 1 r is the result of inner-products between the input y and the feed-forward fan-in weights CW , coded by the dendrites of identical −1 neurons, each situated at a broken bond. [sent-73, score-0.299]
</p><p>41 , apply F1 and then F2 , both consisting of a set of perturbation vectors. [sent-78, score-0.205]
</p><p>42 Quantities resulting from the α’th perturba3 4  The observation matrix C = I for a membrane, and C = I ⊗ [1, 0, 0] for a plate. [sent-79, score-0.064]
</p><p>43 Solid and broken lines denote intact and broken bonds, respectively. [sent-86, score-0.7]
</p><p>44 Open circles denote hidden nodes xi and ﬁlled circles denote observed nodes yi . [sent-87, score-0.248]
</p><p>45 15  20  25  (c)  Figure 2: The resulting receptive ﬁeld of the edge detector produced by breaking the shaded bond shown in Fig. [sent-88, score-0.544]
</p><p>46 The central vertical dashed line in (a) and (b) marks the location of the vertical streak of bonds shown as broken in Fig. [sent-90, score-0.838]
</p><p>47 In (a), those bonds are not actually broken; in (b), they are. [sent-92, score-0.307]
</p><p>48 In (c), a central horizontal slice of (a) is plotted as a solid curve and the counterpart of (b) as a dashed curve. [sent-93, score-0.065]
</p><p>49 ,  y x1 ˆ xc ˆ x0 ˆ  Figure 3: Estimation of x given input y. [sent-94, score-0.093]
</p><p>50 x0 : by unperturbed rod; x1 : coinciding perˆ ˆ fectly with y, is obtained by a rod whose two bonds at the step edges of y are broken; xc : ˆ correction term, engendered by the perturbed rod. [sent-95, score-0.799]
</p><p>51 In particular, −1 T W2 = K1 F2 = K0 F2 + W1 H1 W1 F2 , g W2  (8)  δW2  where W2 refers to the weights due to F2 in the absence of perturbation F1 , which, when indeed existent, would exert a contextual effect on F2 , thereby contributing to the term δW2 . [sent-98, score-0.242]
</p><p>52 Figure 2 illustrates this effect on one perturbation vector (termed ‘edge detector’) in a membrane model, wherein ‘receptive ﬁeld’ refers to W2 and W2 in the case of panel (a) and (b), respectively. [sent-99, score-0.36]
</p><p>53 Evidently, the receptive ﬁeld of W2 across the contextual boundary is pinched off. [sent-100, score-0.136]
</p><p>54 We stress that once the relevant edges are detected, xc is computed almost instantly, ˆ without the need of iterative reﬁnement via lateral propagation. [sent-106, score-0.142]
</p><p>55 3 Parameter estimation As edge inference/detection is outside the scope of this paper, we limit our attention to ﬁnding optimal values for the parameters r and q. [sent-109, score-0.125]
</p><p>56 Given a possibly perturbed model Mα , in which x ∼ N (0, Qα ), we have y ∼ N (0, Sα ), where Sα = rI + CQα C T . [sent-112, score-0.24]
</p><p>57 10 needs two identities, which are included here without proof (the second can be proven by using SMWF and its associated determinant identity): Eα = y T (y − C xα ) (cf. [sent-120, score-0.054]
</p><p>58 3 Matrix Preconditioning Some of the foregoing computation necessitates matrix determinant and matrix inverse, e. [sent-123, score-0.182]
</p><p>59 Methods exist in the literature for ﬁnding a matrix P ([9] and references therein) satisfying the following two criteria: (1) inverting P is easy; (2) the condition number κ(P −1 H) approaches 1. [sent-129, score-0.101]
</p><p>60 Here we summarize our ﬁndings regarding the best class of preconditioners when H arises from some prototypical conﬁgurations of bond breaking. [sent-131, score-0.496]
</p><p>61 When a streak of broken bonds forms a closed contour, with a consistent polarity convention (e. [sent-134, score-0.724]
</p><p>62 , the excitatory region of the receptive ﬁeld of the edge detector associated with each bond lies inside the enclosed region), H and B (cf. [sent-136, score-0.563]
</p><p>63 Let X be the unitary Fourier matrix of same size as H, then H e = X † HX would be approximately diagonal. [sent-139, score-0.129]
</p><p>64 Let ΛH be diagonal: ΛH ij = δij H e ii , then H = XΛH X † is XΛH −1 X † approxa circulant matrix approximating H; i ΛH ii approximates |H|; −1 −1 1 imates H . [sent-140, score-0.123]
</p><p>65 The quality of this preconditioner H can be evaluated by both the condition number κ(H −1 H) and the relative error between the inverse matrices: H −1 − H −1  F/  H −1  F,  (12)  where F denotes Frobenius norm. [sent-142, score-0.071]
</p><p>66 The same X can approximately diagonalize B, and the product of the diagonal elements of the resulting matrix approximates |B|. [sent-143, score-0.064]
</p><p>67 One end of the streak of broken bonds (target contour) abuts another contour, and the other end is open (i. [sent-145, score-0.796]
</p><p>68 Imagine a vibrational mode of the membrane/plate given the conﬁguration of broken bonds. [sent-148, score-0.387]
</p><p>69 The vibrational contrast of the nodes across the broken bond at a line-end has to be small, since in the immediate vicinity there exist paths of intact bonds linking the two nodes. [sent-149, score-1.159]
</p><p>70 This suggests a Dirichlet boundary condition at the line-end. [sent-150, score-0.077]
</p><p>71 , a T-junction), however, the vibrational contrast can be large, since the nodes on different sides of the contour are practically decoupled. [sent-153, score-0.272]
</p><p>72 This analysis leads to using a transform (termed ‘HSWA’ in [10]) which we call ‘DCST’, denoting sine phase at the open end and cosine phase at the abutting end. [sent-155, score-0.281]
</p><p>73 The unitary transform matrix X is given by: Xi,j = √ 2 2L + 1 cos(π(i − 1/2)(j − 1/2)/(L + 1/2)), 1 ≤ i, j ≤ L, where L is the number of broken bonds in the target contour. [sent-156, score-0.79]
</p><p>74 When the streak of broken bonds form an open-ended contour, H can be approximately diagonalized by Sine Transform (cf. [sent-158, score-0.724]
</p><p>75 the intuitive rationale stated in case (2)), of which the unitary transform matrix X is given by: Xi,j = 2/(L + 1) sin(πij/(L + 1)), 1 ≤ i, j ≤ L. [sent-159, score-0.184]
</p><p>76 For a ‘clean’ prototypical contour, the performance of such preconditioners is remarkable, typically producing 1 ≤ κ < 1. [sent-160, score-0.216]
</p><p>77 When contours in the image are interconnected in a complex way, we ﬁrst parse the image domain into non-overlapping enclosed regions, and then treat each region independently. [sent-163, score-0.25]
</p><p>78 A contour segment dividing two regions is shared between them, and thus would contribute two copies, each belonging to one region[11]. [sent-164, score-0.101]
</p><p>79 4 Experiment We test our approach on a real image (Fig. [sent-165, score-0.048]
</p><p>80 We used both membrane and plate models, and in each case we used both the ‘direct’ method, which directly computes H −1 in Eq. [sent-172, score-0.389]
</p><p>81 We ﬁrst apply a Canny detector to generate an edge map (Fig. [sent-175, score-0.146]
</p><p>82 4g) for each noisy image, which is then converted to broken bonds. [sent-176, score-0.299]
</p><p>83 The large number (over 104 ) of broken bonds makes the direct method impractical. [sent-177, score-0.649]
</p><p>84 In order to attain a ‘direct’ result, we partition the image domain into a 5 × 5 array of blocks (one such block is delineated by the inner square in Fig. [sent-178, score-0.161]
</p><p>85 4g), and focus on each of them in turn by retaining edges not more than 10 pixels from the target block (this block’s outer scope is delineated with the outer square in Fig. [sent-179, score-0.228]
</p><p>86 When x is inferred given this partial edge map, only its pixels within the block ˆ are considered valid and are retained. [sent-181, score-0.135]
</p><p>87 In ‘AD’, we parse the contours in each block and apply different diagonalizers accordingly, as summarized in Section 3. [sent-183, score-0.13]
</p><p>88 4e and f illustrate the procedure to ﬁnd optimal q/r for a membrane and a plate, respectively, as explained in Section 2. [sent-188, score-0.155]
</p><p>89 Note how good the cubic polynomial ﬁt is, and that the results of AD do not deviate much from those of the direct (rigorous) method. [sent-190, score-0.13]
</p><p>90 4c and 4d show x by a perturbed and intact membrane model, respectively. [sent-192, score-0.497]
</p><p>91 Estimation by (c) a perturbed membrane, and (d) an intact membrane. [sent-211, score-0.342]
</p><p>92 The criterion function of varying q/r for (e) perturbed membrane, and (f) perturbed plate, which shares the same legend as in (e). [sent-212, score-0.48]
</p><p>93 05 0  0  10  20 (a) DFT  30  0  0  100 (b) DST  200  0  Figure 5: Histograms of condition number κ after preconditioning, and relative error as deﬁned in Eq. [sent-234, score-0.037]
</p><p>94 membrane model plate model direct AD direct AD q/r MSE q/r MSE q/r MSE q/r MSE 0. [sent-241, score-0.475]
</p><p>95 031 121  Improved Entropic [12] MSE  121 138 166  5 Conclusions We have shown how the estimation with perturbed Gaussian Markov processes—hidden state and parameter estimation—can be carried out in non-iterative way. [sent-253, score-0.284]
</p><p>96 Instead of focusing on each individual hidden node, we have taken each process as an entity under scrutiny. [sent-255, score-0.048]
</p><p>97 This paradigm shift changes the way information is stored and represented—from the scenario where the global pattern of the process is embodied entirely by local couplings to the scenario where fan-in and fan-out weigths, in addition to local couplings, reﬂect the patterns of larger scales. [sent-256, score-0.044]
</p><p>98 Although edge detection has not been treated in this paper, our formulation is capable of doing so, and our preliminary results are encouraging. [sent-257, score-0.081]
</p><p>99 It may be premature at this stage to translate the operations of our model to neural substrate; we speculate nevertheless that our approach may have relevance to understanding biological visual systems. [sent-258, score-0.078]
</p><p>100 Symmetric convolution and the discrete sine and cosine transforms. [sent-325, score-0.164]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bonds', 0.307), ('broken', 0.299), ('bond', 0.28), ('perturbed', 0.24), ('plate', 0.234), ('perturbation', 0.205), ('membrane', 0.155), ('preconditioners', 0.154), ('ad', 0.144), ('energy', 0.127), ('mse', 0.123), ('dcst', 0.118), ('streak', 0.118), ('uh', 0.118), ('fi', 0.11), ('intact', 0.102), ('contour', 0.101), ('fj', 0.094), ('xc', 0.093), ('dft', 0.088), ('dst', 0.088), ('smwf', 0.088), ('vibrational', 0.088), ('sine', 0.087), ('cubic', 0.087), ('xt', 0.087), ('nodes', 0.083), ('uv', 0.082), ('edge', 0.081), ('discontinuity', 0.077), ('node', 0.075), ('unitary', 0.065), ('horizontal', 0.065), ('detector', 0.065), ('matrix', 0.064), ('prototypical', 0.062), ('receptive', 0.059), ('abutting', 0.059), ('circulant', 0.059), ('delineated', 0.059), ('gmp', 0.059), ('perturbing', 0.059), ('preconditioning', 0.059), ('uhi', 0.059), ('uhj', 0.059), ('unperturbed', 0.059), ('uvi', 0.059), ('breaking', 0.059), ('vertical', 0.057), ('transform', 0.055), ('ln', 0.055), ('block', 0.054), ('determinant', 0.054), ('ui', 0.053), ('termed', 0.052), ('canny', 0.051), ('rod', 0.051), ('perturb', 0.051), ('niteness', 0.051), ('edges', 0.049), ('image', 0.048), ('hidden', 0.048), ('deferred', 0.047), ('estimation', 0.044), ('variational', 0.044), ('cosine', 0.044), ('enclosed', 0.044), ('couplings', 0.044), ('direct', 0.043), ('operations', 0.043), ('connecting', 0.041), ('cw', 0.041), ('substrate', 0.041), ('fit', 0.041), ('uj', 0.041), ('latency', 0.041), ('boundary', 0.04), ('xj', 0.04), ('elementary', 0.039), ('parse', 0.039), ('von', 0.039), ('inference', 0.038), ('contextual', 0.037), ('contours', 0.037), ('condition', 0.037), ('smoothness', 0.037), ('markov', 0.036), ('end', 0.036), ('discontinuities', 0.036), ('mrf', 0.036), ('perturbations', 0.036), ('ensures', 0.036), ('visual', 0.035), ('snr', 0.035), ('amounts', 0.034), ('inverse', 0.034), ('region', 0.034), ('xi', 0.034), ('convolution', 0.033), ('outer', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="139-tfidf-1" href="./nips-2005-Non-iterative_Estimation_with_Perturbed_Gaussian_Markov_Processes.html">139 nips-2005-Non-iterative Estimation with Perturbed Gaussian Markov Processes</a></p>
<p>Author: Yunsong Huang, B. Keith Jenkins</p><p>Abstract: We develop an approach for estimation with Gaussian Markov processes that imposes a smoothness prior while allowing for discontinuities. Instead of propagating information laterally between neighboring nodes in a graph, we study the posterior distribution of the hidden nodes as a whole—how it is perturbed by invoking discontinuities, or weakening the edges, in the graph. We show that the resulting computation amounts to feed-forward fan-in operations reminiscent of V1 neurons. Moreover, using suitable matrix preconditioners, the incurred matrix inverse and determinant can be approximated, without iteration, in the same computational style. Simulation results illustrate the merits of this approach.</p><p>2 0.12990962 <a title="139-tfidf-2" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>Author: John D. Lafferty, Pradeep K. Ravikumar</p><p>Abstract: We present a family of approximation techniques for probabilistic graphical models, based on the use of graphical preconditioners developed in the scientiﬁc computing literature. Our framework yields rigorous upper and lower bounds on event probabilities and the log partition function of undirected graphical models, using non-iterative procedures that have low time complexity. As in mean ﬁeld approaches, the approximations are built upon tractable subgraphs; however, we recast the problem of optimizing the tractable distribution parameters and approximate inference in terms of the well-studied linear systems problem of obtaining a good matrix preconditioner. Experiments are presented that compare the new approximation schemes to variational methods. 1</p><p>3 0.12648652 <a title="139-tfidf-3" href="./nips-2005-Logic_and_MRF_Circuitry_for_Labeling_Occluding_and_Thinline_Visual_Contours.html">122 nips-2005-Logic and MRF Circuitry for Labeling Occluding and Thinline Visual Contours</a></p>
<p>Author: Eric Saund</p><p>Abstract: This paper presents representation and logic for labeling contrast edges and ridges in visual scenes in terms of both surface occlusion (border ownership) and thinline objects. In natural scenes, thinline objects include sticks and wires, while in human graphical communication thinlines include connectors, dividers, and other abstract devices. Our analysis is directed at both natural and graphical domains. The basic problem is to formulate the logic of the interactions among local image events, speciﬁcally contrast edges, ridges, junctions, and alignment relations, such as to encode the natural constraints among these events in visual scenes. In a sparse heterogeneous Markov Random Field framework, we deﬁne a set of interpretation nodes and energy/potential functions among them. The minimum energy conﬁguration found by Loopy Belief Propagation is shown to correspond to preferred human interpretation across a wide range of prototypical examples including important illusory contour ﬁgures such as the Kanizsa Triangle, as well as more difﬁcult examples. In practical terms, the approach delivers correct interpretations of inherently ambiguous hand-drawn box-and-connector diagrams at low computational cost.</p><p>4 0.10651117 <a title="139-tfidf-4" href="./nips-2005-Nested_sampling_for_Potts_models.html">133 nips-2005-Nested sampling for Potts models</a></p>
<p>Author: Iain Murray, David MacKay, Zoubin Ghahramani, John Skilling</p><p>Abstract: Nested sampling is a new Monte Carlo method by Skilling [1] intended for general Bayesian computation. Nested sampling provides a robust alternative to annealing-based methods for computing normalizing constants. It can also generate estimates of other quantities such as posterior expectations. The key technical requirement is an ability to draw samples uniformly from the prior subject to a constraint on the likelihood. We provide a demonstration with the Potts model, an undirected graphical model. 1</p><p>5 0.10577202 <a title="139-tfidf-5" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>Author: Eizaburo Doi, Doru C. Balcan, Michael S. Lewicki</p><p>Abstract: Biological sensory systems are faced with the problem of encoding a high-ﬁdelity sensory signal with a population of noisy, low-ﬁdelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and twodimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of highdimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets. 1</p><p>6 0.090956159 <a title="139-tfidf-6" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>7 0.087086745 <a title="139-tfidf-7" href="./nips-2005-Message_passing_for_task_redistribution_on_sparse_graphs.html">125 nips-2005-Message passing for task redistribution on sparse graphs</a></p>
<p>8 0.078989178 <a title="139-tfidf-8" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>9 0.073736534 <a title="139-tfidf-9" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>10 0.072434559 <a title="139-tfidf-10" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>11 0.068946213 <a title="139-tfidf-11" href="./nips-2005-An_Approximate_Inference_Approach_for_the_PCA_Reconstruction_Error.html">24 nips-2005-An Approximate Inference Approach for the PCA Reconstruction Error</a></p>
<p>12 0.067198612 <a title="139-tfidf-12" href="./nips-2005-Dynamic_Social_Network_Analysis_using_Latent_Space_Models.html">60 nips-2005-Dynamic Social Network Analysis using Latent Space Models</a></p>
<p>13 0.067165017 <a title="139-tfidf-13" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>14 0.065768205 <a title="139-tfidf-14" href="./nips-2005-Variational_EM_Algorithms_for_Non-Gaussian_Latent_Variable_Models.html">202 nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</a></p>
<p>15 0.063178487 <a title="139-tfidf-15" href="./nips-2005-A_Cortically-Plausible_Inverse_Problem_Solving_Method_Applied_to_Recognizing_Static_and_Kinematic_3D_Objects.html">7 nips-2005-A Cortically-Plausible Inverse Problem Solving Method Applied to Recognizing Static and Kinematic 3D Objects</a></p>
<p>16 0.062775582 <a title="139-tfidf-16" href="./nips-2005-Is_Early_Vision_Optimized_for_Extracting_Higher-order_Dependencies%3F.html">101 nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</a></p>
<p>17 0.060705885 <a title="139-tfidf-17" href="./nips-2005-Correcting_sample_selection_bias_in_maximum_entropy_density_estimation.html">51 nips-2005-Correcting sample selection bias in maximum entropy density estimation</a></p>
<p>18 0.059579697 <a title="139-tfidf-18" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>19 0.058169864 <a title="139-tfidf-19" href="./nips-2005-Fast_Information_Value_for_Graphical_Models.html">70 nips-2005-Fast Information Value for Graphical Models</a></p>
<p>20 0.055844821 <a title="139-tfidf-20" href="./nips-2005-An_Application_of_Markov_Random_Fields_to_Range_Sensing.html">23 nips-2005-An Application of Markov Random Fields to Range Sensing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.213), (1, 0.004), (2, -0.01), (3, 0.049), (4, -0.011), (5, -0.051), (6, -0.213), (7, 0.047), (8, 0.082), (9, 0.064), (10, 0.11), (11, 0.024), (12, -0.074), (13, -0.005), (14, -0.007), (15, 0.005), (16, 0.018), (17, 0.014), (18, -0.001), (19, -0.036), (20, 0.062), (21, -0.024), (22, -0.028), (23, 0.129), (24, 0.074), (25, 0.003), (26, 0.086), (27, -0.139), (28, 0.08), (29, 0.061), (30, -0.019), (31, 0.13), (32, -0.101), (33, -0.03), (34, 0.073), (35, -0.073), (36, 0.135), (37, -0.003), (38, -0.116), (39, -0.031), (40, 0.086), (41, 0.124), (42, -0.012), (43, 0.008), (44, -0.128), (45, 0.08), (46, 0.111), (47, -0.003), (48, -0.011), (49, 0.131)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94039422 <a title="139-lsi-1" href="./nips-2005-Non-iterative_Estimation_with_Perturbed_Gaussian_Markov_Processes.html">139 nips-2005-Non-iterative Estimation with Perturbed Gaussian Markov Processes</a></p>
<p>Author: Yunsong Huang, B. Keith Jenkins</p><p>Abstract: We develop an approach for estimation with Gaussian Markov processes that imposes a smoothness prior while allowing for discontinuities. Instead of propagating information laterally between neighboring nodes in a graph, we study the posterior distribution of the hidden nodes as a whole—how it is perturbed by invoking discontinuities, or weakening the edges, in the graph. We show that the resulting computation amounts to feed-forward fan-in operations reminiscent of V1 neurons. Moreover, using suitable matrix preconditioners, the incurred matrix inverse and determinant can be approximated, without iteration, in the same computational style. Simulation results illustrate the merits of this approach.</p><p>2 0.71738106 <a title="139-lsi-2" href="./nips-2005-Logic_and_MRF_Circuitry_for_Labeling_Occluding_and_Thinline_Visual_Contours.html">122 nips-2005-Logic and MRF Circuitry for Labeling Occluding and Thinline Visual Contours</a></p>
<p>Author: Eric Saund</p><p>Abstract: This paper presents representation and logic for labeling contrast edges and ridges in visual scenes in terms of both surface occlusion (border ownership) and thinline objects. In natural scenes, thinline objects include sticks and wires, while in human graphical communication thinlines include connectors, dividers, and other abstract devices. Our analysis is directed at both natural and graphical domains. The basic problem is to formulate the logic of the interactions among local image events, speciﬁcally contrast edges, ridges, junctions, and alignment relations, such as to encode the natural constraints among these events in visual scenes. In a sparse heterogeneous Markov Random Field framework, we deﬁne a set of interpretation nodes and energy/potential functions among them. The minimum energy conﬁguration found by Loopy Belief Propagation is shown to correspond to preferred human interpretation across a wide range of prototypical examples including important illusory contour ﬁgures such as the Kanizsa Triangle, as well as more difﬁcult examples. In practical terms, the approach delivers correct interpretations of inherently ambiguous hand-drawn box-and-connector diagrams at low computational cost.</p><p>3 0.52927154 <a title="139-lsi-3" href="./nips-2005-Nested_sampling_for_Potts_models.html">133 nips-2005-Nested sampling for Potts models</a></p>
<p>Author: Iain Murray, David MacKay, Zoubin Ghahramani, John Skilling</p><p>Abstract: Nested sampling is a new Monte Carlo method by Skilling [1] intended for general Bayesian computation. Nested sampling provides a robust alternative to annealing-based methods for computing normalizing constants. It can also generate estimates of other quantities such as posterior expectations. The key technical requirement is an ability to draw samples uniformly from the prior subject to a constraint on the likelihood. We provide a demonstration with the Potts model, an undirected graphical model. 1</p><p>4 0.5021888 <a title="139-lsi-4" href="./nips-2005-An_Approximate_Inference_Approach_for_the_PCA_Reconstruction_Error.html">24 nips-2005-An Approximate Inference Approach for the PCA Reconstruction Error</a></p>
<p>Author: Manfred Opper</p><p>Abstract: The problem of computing a resample estimate for the reconstruction error in PCA is reformulated as an inference problem with the help of the replica method. Using the expectation consistent (EC) approximation, the intractable inference problem can be solved efﬁciently using only two variational parameters. A perturbative correction to the result is computed and an alternative simpliﬁed derivation is also presented. 1</p><p>5 0.46977928 <a title="139-lsi-5" href="./nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs.html">90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</a></p>
<p>Author: Firas Hamze, Nando de Freitas</p><p>Abstract: This paper presents a new sampling algorithm for approximating functions of variables representable as undirected graphical models of arbitrary connectivity with pairwise potentials, as well as for estimating the notoriously difﬁcult partition function of the graph. The algorithm ﬁts into the framework of sequential Monte Carlo methods rather than the more widely used MCMC, and relies on constructing a sequence of intermediate distributions which get closer to the desired one. While the idea of using “tempered” proposals is known, we construct a novel sequence of target distributions where, rather than dropping a global temperature parameter, we sequentially couple individual pairs of variables that are, initially, sampled exactly from a spanning tree of the variables. We present experimental results on inference and estimation of the partition function for sparse and densely-connected graphs.</p><p>6 0.46251407 <a title="139-lsi-6" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>7 0.45389506 <a title="139-lsi-7" href="./nips-2005-Message_passing_for_task_redistribution_on_sparse_graphs.html">125 nips-2005-Message passing for task redistribution on sparse graphs</a></p>
<p>8 0.42897362 <a title="139-lsi-8" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>9 0.41047534 <a title="139-lsi-9" href="./nips-2005-A_Bayesian_Framework_for_Tilt_Perception_and_Confidence.html">3 nips-2005-A Bayesian Framework for Tilt Perception and Confidence</a></p>
<p>10 0.41014236 <a title="139-lsi-10" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>11 0.40794173 <a title="139-lsi-11" href="./nips-2005-Correcting_sample_selection_bias_in_maximum_entropy_density_estimation.html">51 nips-2005-Correcting sample selection bias in maximum entropy density estimation</a></p>
<p>12 0.38988334 <a title="139-lsi-12" href="./nips-2005-Spectral_Bounds_for_Sparse_PCA%3A_Exact_and_Greedy_Algorithms.html">180 nips-2005-Spectral Bounds for Sparse PCA: Exact and Greedy Algorithms</a></p>
<p>13 0.38191923 <a title="139-lsi-13" href="./nips-2005-Dynamic_Social_Network_Analysis_using_Latent_Space_Models.html">60 nips-2005-Dynamic Social Network Analysis using Latent Space Models</a></p>
<p>14 0.36667755 <a title="139-lsi-14" href="./nips-2005-Factorial_Switching_Kalman_Filters_for_Condition_Monitoring_in_Neonatal_Intensive_Care.html">68 nips-2005-Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care</a></p>
<p>15 0.35247579 <a title="139-lsi-15" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>16 0.34312528 <a title="139-lsi-16" href="./nips-2005-Walk-Sum_Interpretation_and_Analysis_of_Gaussian_Belief_Propagation.html">204 nips-2005-Walk-Sum Interpretation and Analysis of Gaussian Belief Propagation</a></p>
<p>17 0.31798261 <a title="139-lsi-17" href="./nips-2005-Large-scale_biophysical_parameter_estimation_in_single_neurons_via_constrained_linear_regression.html">106 nips-2005-Large-scale biophysical parameter estimation in single neurons via constrained linear regression</a></p>
<p>18 0.31617281 <a title="139-lsi-18" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>19 0.315514 <a title="139-lsi-19" href="./nips-2005-A_Cortically-Plausible_Inverse_Problem_Solving_Method_Applied_to_Recognizing_Static_and_Kinematic_3D_Objects.html">7 nips-2005-A Cortically-Plausible Inverse Problem Solving Method Applied to Recognizing Static and Kinematic 3D Objects</a></p>
<p>20 0.309313 <a title="139-lsi-20" href="./nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.037), (6, 0.338), (10, 0.041), (18, 0.017), (27, 0.034), (31, 0.08), (34, 0.071), (39, 0.014), (55, 0.036), (60, 0.015), (65, 0.01), (69, 0.048), (73, 0.055), (88, 0.062), (91, 0.047), (94, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79086953 <a title="139-lda-1" href="./nips-2005-Non-iterative_Estimation_with_Perturbed_Gaussian_Markov_Processes.html">139 nips-2005-Non-iterative Estimation with Perturbed Gaussian Markov Processes</a></p>
<p>Author: Yunsong Huang, B. Keith Jenkins</p><p>Abstract: We develop an approach for estimation with Gaussian Markov processes that imposes a smoothness prior while allowing for discontinuities. Instead of propagating information laterally between neighboring nodes in a graph, we study the posterior distribution of the hidden nodes as a whole—how it is perturbed by invoking discontinuities, or weakening the edges, in the graph. We show that the resulting computation amounts to feed-forward fan-in operations reminiscent of V1 neurons. Moreover, using suitable matrix preconditioners, the incurred matrix inverse and determinant can be approximated, without iteration, in the same computational style. Simulation results illustrate the merits of this approach.</p><p>2 0.71969795 <a title="139-lda-2" href="./nips-2005-Affine_Structure_From_Sound.html">20 nips-2005-Affine Structure From Sound</a></p>
<p>Author: Sebastian Thrun</p><p>Abstract: We consider the problem of localizing a set of microphones together with a set of external acoustic events (e.g., hand claps), emitted at unknown times and unknown locations. We propose a solution that approximates this problem under a far ﬁeld approximation deﬁned in the calculus of afﬁne geometry, and that relies on singular value decomposition (SVD) to recover the afﬁne structure of the problem. We then deﬁne low-dimensional optimization techniques for embedding the solution into Euclidean geometry, and further techniques for recovering the locations and emission times of the acoustic events. The approach is useful for the calibration of ad-hoc microphone arrays and sensor networks. 1</p><p>3 0.6865027 <a title="139-lda-3" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>Author: Jin Yu, Douglas Aberdeen, Nicol N. Schraudolph</p><p>Abstract: Reinforcement learning by direct policy gradient estimation is attractive in theory but in practice leads to notoriously ill-behaved optimization problems. We improve its robustness and speed of convergence with stochastic meta-descent, a gain vector adaptation method that employs fast Hessian-vector products. In our experiments the resulting algorithms outperform previously employed online stochastic, ofﬂine conjugate, and natural policy gradient methods. 1</p><p>4 0.41260463 <a title="139-lda-4" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>Author: Doron Blatt, Alfred O. Hero</p><p>Abstract: This paper proposes an algorithm to convert a T -stage stochastic decision problem with a continuous state space to a sequence of supervised learning problems. The optimization problem associated with the trajectory tree and random trajectory methods of Kearns, Mansour, and Ng, 2000, is solved using the Gauss-Seidel method. The algorithm breaks a multistage reinforcement learning problem into a sequence of single-stage reinforcement learning subproblems, each of which is solved via an exact reduction to a weighted-classiﬁcation problem that can be solved using off-the-self methods. Thus the algorithm converts a reinforcement learning problem into simpler supervised learning subproblems. It is shown that the method converges in a ﬁnite number of steps to a solution that cannot be further improved by componentwise optimization. The implication of the proposed algorithm is that a plethora of classiﬁcation methods can be applied to ﬁnd policies in the reinforcement learning problem. 1</p><p>5 0.40839878 <a title="139-lda-5" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>Author: John D. Lafferty, Pradeep K. Ravikumar</p><p>Abstract: We present a family of approximation techniques for probabilistic graphical models, based on the use of graphical preconditioners developed in the scientiﬁc computing literature. Our framework yields rigorous upper and lower bounds on event probabilities and the log partition function of undirected graphical models, using non-iterative procedures that have low time complexity. As in mean ﬁeld approaches, the approximations are built upon tractable subgraphs; however, we recast the problem of optimizing the tractable distribution parameters and approximate inference in terms of the well-studied linear systems problem of obtaining a good matrix preconditioner. Experiments are presented that compare the new approximation schemes to variational methods. 1</p><p>6 0.4067561 <a title="139-lda-6" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>7 0.40429464 <a title="139-lda-7" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>8 0.40426576 <a title="139-lda-8" href="./nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs.html">90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</a></p>
<p>9 0.39821708 <a title="139-lda-9" href="./nips-2005-Kernelized_Infomax_Clustering.html">102 nips-2005-Kernelized Infomax Clustering</a></p>
<p>10 0.39759532 <a title="139-lda-10" href="./nips-2005-Policy-Gradient_Methods_for_Planning.html">153 nips-2005-Policy-Gradient Methods for Planning</a></p>
<p>11 0.39720684 <a title="139-lda-11" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>12 0.39573544 <a title="139-lda-12" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>13 0.39509037 <a title="139-lda-13" href="./nips-2005-Oblivious_Equilibrium%3A_A_Mean_Field_Approximation_for_Large-Scale_Dynamic_Games.html">142 nips-2005-Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games</a></p>
<p>14 0.39399454 <a title="139-lda-14" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>15 0.39337522 <a title="139-lda-15" href="./nips-2005-Measuring_Shared_Information_and_Coordinated_Activity_in_Neuronal_Networks.html">124 nips-2005-Measuring Shared Information and Coordinated Activity in Neuronal Networks</a></p>
<p>16 0.39184842 <a title="139-lda-16" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>17 0.39163429 <a title="139-lda-17" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>18 0.39127308 <a title="139-lda-18" href="./nips-2005-Soft_Clustering_on_Graphs.html">178 nips-2005-Soft Clustering on Graphs</a></p>
<p>19 0.39089254 <a title="139-lda-19" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>20 0.38963187 <a title="139-lda-20" href="./nips-2005-Conditional_Visual_Tracking_in_Kernel_Space.html">45 nips-2005-Conditional Visual Tracking in Kernel Space</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
