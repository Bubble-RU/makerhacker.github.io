<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-90" href="#">nips2005-90</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</h1>
<br/><p>Source: <a title="nips-2005-90-pdf" href="http://papers.nips.cc/paper/2788-hot-coupling-a-particle-approach-to-inference-and-normalization-on-pairwise-undirected-graphs.pdf">pdf</a></p><p>Author: Firas Hamze, Nando de Freitas</p><p>Abstract: This paper presents a new sampling algorithm for approximating functions of variables representable as undirected graphical models of arbitrary connectivity with pairwise potentials, as well as for estimating the notoriously difﬁcult partition function of the graph. The algorithm ﬁts into the framework of sequential Monte Carlo methods rather than the more widely used MCMC, and relies on constructing a sequence of intermediate distributions which get closer to the desired one. While the idea of using “tempered” proposals is known, we construct a novel sequence of target distributions where, rather than dropping a global temperature parameter, we sequentially couple individual pairs of variables that are, initially, sampled exactly from a spanning tree of the variables. We present experimental results on inference and estimation of the partition function for sparse and densely-connected graphs.</p><p>Reference: <a title="nips-2005-90-reference" href="../nips2005_reference/nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The algorithm ﬁts into the framework of sequential Monte Carlo methods rather than the more widely used MCMC, and relies on constructing a sequence of intermediate distributions which get closer to the desired one. [sent-2, score-0.164]
</p><p>2 While the idea of using “tempered” proposals is known, we construct a novel sequence of target distributions where, rather than dropping a global temperature parameter, we sequentially couple individual pairs of variables that are, initially, sampled exactly from a spanning tree of the variables. [sent-3, score-0.352]
</p><p>3 We present experimental results on inference and estimation of the partition function for sparse and densely-connected graphs. [sent-4, score-0.098]
</p><p>4 The methods used to approximate these intractable quantities generally fall into the categories of Markov Chain Monte Carlo (MCMC) [6] and variational methods [7]. [sent-7, score-0.101]
</p><p>5 The former, involving running a Markov chain whose invariant distribution is the distribution of interest, can suffer from slow convergence to stationarity and high correlation between samples at stationarity, while the latter is not guaranteed to give the right answer or always converge. [sent-8, score-0.17]
</p><p>6 When performing learning in such models however, a more serious problem arises: the parameter update equations involve the normalization constant of the joint model at the current value of parameters, from here on called the partition function. [sent-9, score-0.128]
</p><p>7 Although there exists a polynomial time MCMC algorithm for simple graphs with binary nodes, ferromagnetic potentials and uniform observations [9], this algorithm is hardly applicable to the complex models encountered in practice. [sent-11, score-0.149]
</p><p>8 Of more interest, perhaps, are the theoretical results that show that Gibbs sampling and even Swendsen-Wang[10] can mix exponentially slowly in many situations [11]. [sent-12, score-0.104]
</p><p>9 This paper introduces a new sequential Monte Carlo method for approximating expectations of a pairwise graph’s variables (of which beliefs are a special case) and of reasonably estimating the partition function. [sent-13, score-0.234]
</p><p>10 Intuitively, the new method uses interacting parallel chains to handle multimodal distributions,  xi xj y  ψ(xi ,x j )  φ (xj ,y)  Figure 1: A small example of the type of graphical model treated in this paper. [sent-14, score-0.137]
</p><p>11 In addition, there is no requirement that the chains converge to equilibrium as the bias due to incomplete convergence is corrected for by importance sampling. [sent-17, score-0.12]
</p><p>12 Formally, given hidden variables x and observations y, the model is speciﬁed on a graph G(V, E), with edges E and M nodes V by: 1 ψ(xi , xj ) φ(xi , yi ) π(x, y) = Z i∈V  (i,j)∈E  where x = {x1 , . [sent-18, score-0.161]
</p><p>13 , xM }, Z is the partition function, φ(·) denotes the observation potentials and ψ(·) denotes the pair-wise interaction potentials, which are strictly positive but otherwise arbitrary. [sent-21, score-0.158]
</p><p>14 The partition function is: Z = x i∈V φ(xi , yi ) (i,j)∈E ψ(xi , xj ),where the sum is over all possible system states. [sent-22, score-0.124]
</p><p>15 We present experimental results on both fully-connected graphs (cases where each node neighbors every other node) and sparse graphs. [sent-24, score-0.113]
</p><p>16 We introduce artiﬁcial dynamics simply as a constructive strategy for obtaining samples of a sequence of distributions converging to the distribution of interest. [sent-29, score-0.131]
</p><p>17 This distribution is then used as a proposal mechanism to obtain samples from a slightly more complex distribution that is closer to the target distribution. [sent-31, score-0.341]
</p><p>18 The process is repeated until the sequence of distributions of increasing complexity reaches the target distribution. [sent-32, score-0.115]
</p><p>19 Our algorithm has connections to a general annealing strategy proposed in the physics [14] and statistics [15] literature, known as Annealed Importance Sampling (AIS). [sent-33, score-0.078]
</p><p>20 The term annealing refers to the lowering of a “temperature parameter,” the process of which makes the joint distribution more concentrated on its modes, whose number can be massive for difﬁcult problems. [sent-35, score-0.103]
</p><p>21 The celebrated simulated annealing (SA) [17] algorithm is an optimization method relying on this phenomenon; presently, however we are interested in integration and so SA does not apply here. [sent-36, score-0.081]
</p><p>22 For our initial distribution we choose a spanning tree of the variables, on which analytic marginalization, exact sampling, and computation of the partition function are easily done. [sent-39, score-0.249]
</p><p>23 After drawing a population of samples (particles) from this distribution, the sequential phase begins: an edge of the desired graph is chosen and gradually added to the current one as shown in Figure 2. [sent-40, score-0.302]
</p><p>24 The particles then follow a trajectory according to some proposal  mechanism. [sent-41, score-0.293]
</p><p>25 The “ﬁtness” of the particles is measured via their importance weights. [sent-42, score-0.221]
</p><p>26 When the set of samples has become skewed, that is with some containing high weights and many containing low ones, the particles are resampled according to their weights. [sent-43, score-0.22]
</p><p>27 The sequential structure is thus imposed by the propose-and-resample mechanism rather than by any property of the original system. [sent-44, score-0.111]
</p><p>28 First we construct a spanning tree, of which a  population of iid samples can be easily drawn using the forward ﬁltering/backward sampling algorithm for trees. [sent-47, score-0.234]
</p><p>29 The tree then becomes the proposal mechanism for generating samples for a graph with an extra potential. [sent-48, score-0.343]
</p><p>30 The process is repeated until we obtain samples from the target distribution (deﬁned on a fully connected graph in this case). [sent-49, score-0.206]
</p><p>31 , πn (x1:n ), where π1 (x1 ) is the distribution on the weighted spanning tree. [sent-54, score-0.106]
</p><p>32 Marginalizing over x1:n−1 gives us the target distribution of interest πn (xn ) (the distribution of the graphical model that we want to sample from as illustrated in Figure 2 for n = 4). [sent-56, score-0.151]
</p><p>33 So we ﬁrst focus on sampling from the sequence of auxiliary distributions. [sent-57, score-0.103]
</p><p>34 The joint distribution is only known up to a −1 normalization constant: πn (x1:n ) = Zn fn (x1:n ), where Zn fn (x1:n )dx1:n is the partition function. [sent-58, score-0.512]
</p><p>35 We are often interested in computing this partition function and other expectations, such as I(g(xn )) = g(xn )πn (xn )dxn , where g is a function of interest (e. [sent-59, score-0.098]
</p><p>36 (i)  If we had a set of samples {x1:n }N from π, we could approximate this integral with the i=1 1 following Monte Carlo estimator: π n (dx1:n ) = N N δx(i) (dx1:n ), where δx(i) (dx1:n ) i=1 1:n 1:n denotes the delta Dirac function, and consequently approximate any expectations of interest. [sent-62, score-0.188]
</p><p>37 Instead, we sample from a proposal distribution q and weight the samples according to the following importance ratio fn (x1:n ) fn (x1:n ) qn−1 (x1:n−1 ) wn = = wn−1 qn (x1:n ) qn (x1:n ) fn−1 (x1:n−1 ) The proposal is constructed sequentially: q(x1:n ) = qn−1 (x1:n−1 )qn (xn |x1:n−1 ). [sent-65, score-1.239]
</p><p>38 Hence, the importance weights can be updated recursively fn (x1:n ) wn = wn−1 (1) qn (xn |x1:n−1 )fn−1 (x1:n−1 ) (i)  (i)  Given a set of N particles x1:n−1 , we obtain a set of particles xn by sampling from (i) qn (xn |x1:n−1 ) and applying the weights of equation (1). [sent-66, score-1.297]
</p><p>39 To overcome slow drift in the particle population, a resampling (selection) step chooses the ﬁttest particles (see the introductory chapter in [13] for a more detailed explanation). [sent-67, score-0.255]
</p><p>40 We use a state-of-the-art minimum variance resampling algorithm [18]. [sent-68, score-0.082]
</p><p>41 We can choose a (non-homogeneous) Markov chain with transition kernel K n (xn−1 , xn ) as the proposal distribution qn (xn |x1:n−1 ). [sent-70, score-0.54]
</p><p>42 Hence, given an initial proposal distribution q1 (·), we have joint proposal distribution at step n: qn (x1:n ) = q1 (x1 ) n Kk (xk−1 , xk ). [sent-71, score-0.532]
</p><p>43 Under these choices, the (unnormalized) incremental importance weight becomes: fn (xn )Ln−1 (xn , xn−1 ) wn ∝ (2) fn−1 (xn−1 )Kn (xn−1 , xn ) Different choices of the backward Kernel L result in different algorithms [16]. [sent-73, score-0.643]
</p><p>44 For example, )Kn the choice: Ln−1 (xn , xn−1 ) = fn (xn−1fn (x(xn−1 ,xn ) results in the AIS algorithm, with n) fn (xn−1 ) weights wn ∝ fn−1 (xn−1 ) . [sent-74, score-0.53]
</p><p>45 Note that in this case, the importance weights do not depend on xn and, hence, it is possible to do resampling before the importance sampling step. [sent-76, score-0.489]
</p><p>46 Also, note that if there are big discrepancies between fn (·) and fn−1 (·) the method might perform poorly. [sent-78, score-0.178]
</p><p>47 3 The new algorithm We could try to perform traditional importance sampling by seeking some proposal distribution for the entire graph. [sent-80, score-0.324]
</p><p>48 This is very difﬁcult and performance degrades exponentially in dimension if the proposal is mismatched [20]. [sent-81, score-0.171]
</p><p>49 We propose, however, to use the samples from the tree distribution (which we call π0 ) as candidates to an intermediate target distribution, consisting of the tree along with a “weak” version of a potential corresponding to some edge of the original graph. [sent-82, score-0.322]
</p><p>50 Given a set of edges G0 which form a spanning tree of the target graph, we can can use the belief propagation equations [21] and bottom-up propagation, top-down sampling [22], to draw a set of N independent samples from the tree. [sent-83, score-0.366]
</p><p>51 From then on, however, the normalization constants of subsequent target distributions cannot be analytically computed. [sent-85, score-0.116]
</p><p>52 We then choose a new edge e1 from the set of “unused” edges E − G0 and add it to G0 to form the new edge set G1 = e1 ∪ G0 . [sent-86, score-0.201]
</p><p>53 Then, the intermediate target distribution π1 is proportional to π0 (x1 )ψe1 (xu1 , xv1 ). [sent-88, score-0.116]
</p><p>54 In doing straightforward importance sampling, using π0 as a proposal for π1 , the importance weight is proportional to ψe1 (xu1 , xv1 ). [sent-89, score-0.297]
</p><p>55 We adopt a slow proposal process to move the population of particles towards π1 . [sent-90, score-0.328]
</p><p>56 We gradually introduce the potential between Xu1 and Xv1 via a coupling parameter α which increases from 0 to 1 in order to “softly” bring the edge’s potential in and allow the particles to adjust to the new environment. [sent-91, score-0.2]
</p><p>57 At each time step, we want a proposal mechanism that is close to the target distribution. [sent-93, score-0.238]
</p><p>58 We can, however, employ a single-site Gibbs sampler with random scan whose invariant distribution at each step is the the next target density in the sequence; this kernel is applied to each particle. [sent-96, score-0.157]
</p><p>59 When an edge has been fully added a new one is chosen and the process is repeated until the ﬁnal target density is the full graph. [sent-97, score-0.128]
</p><p>60 To alleviate potential confusion with MCMC, while any one particle obviously forms a correlated path, we are using a population and are making no assumption or requirement that the chains have converged as is done in MCMC as we are correcting for incomplete convergence with the weights. [sent-99, score-0.136]
</p><p>61 4 Experiments and discussion Four approximate inference methods were compared: our SMC method with sequential edge addition (Hot Coupling (HC)), a more typical annealing strategy with a global temperature parameter(SMCG), single-site Gibbs sampling with random scan and loopy belief propagation. [sent-100, score-0.626]
</p><p>62 SMCG can be thought of as related to HC but where all the edges and local evidence are annealed at the same time. [sent-101, score-0.103]
</p><p>63 The majority of our experiments were performed on graphs that were small enough for exact marginals and partition functions to be exhaustively calculated. [sent-102, score-0.219]
</p><p>64 However, even in toy cases MCMC and loopy can give unsatisfactory and sometimes disastrous results. [sent-103, score-0.261]
</p><p>65 Our pairwise potentials corresponded to the well-known Potts model: ψ i,j (xi , xj ) = 1 1 e T Jij δxi ,xj , φi (xi ) = e T Jδxi (yi ) . [sent-107, score-0.115]
</p><p>66 The output potentials were randomly selected in both the uniform and random interaction cases. [sent-111, score-0.085]
</p><p>67 The HC method used a linear coupling schedule for each edge, increasing from α = 0 to α = 1 over 100 iterations; our SMCG implementation used a linear global cooling schedule, whose number of steps depended on the graph in order to match those taken by SMCG. [sent-112, score-0.153]
</p><p>68 Our SMC simulations used 1000 particles for each run, while each Gibbs run performed 20000 single-site updates. [sent-114, score-0.146]
</p><p>69 For these models, this was more than enough steps to settle into local minima; runs of up to 1 million iterations did not yield a difference, which is characteristic of the exponential mixing time of the sampler on these graphs. [sent-115, score-0.122]
</p><p>70 For our HC method, spanning trees and edges in the sequential construction were randomly chosen from the full graph; the rationale for doing so is to allay any criticism that “tweaking” the ordering may have had a crucial effect on the algorithm. [sent-116, score-0.22]
</p><p>71 First, we used HC, SMCG and Gibbs to approximate the expected sum of our graphs’ variables, the so-called magnetization: m = E[ M xi ]. [sent-119, score-0.074]
</p><p>72 We then approximated the partition i=1 functions of the graphs using HC, SMCG, and loopy. [sent-120, score-0.187]
</p><p>73 08  Figure 3: Approximate magnetization for the nodes of the graphs, as deﬁned in the text, calculated  using HC, SMCG, and Gibbs sampling and compared to the true value obtained by brute force. [sent-149, score-0.169]
</p><p>74 Observe the massive variance of Gibbs sampling in some cases. [sent-150, score-0.127]
</p><p>75 075 -  Figure 4: Approximate partition function of the graphs discussed in the text calculated using HC, SMCG, and Loopy Belief Propagation (loopy. [sent-168, score-0.187]
</p><p>76 ) For HC and SMCG are shown the error of the sample average of results over 50 independent runs and the variance across those runs. [sent-169, score-0.114]
</p><p>77 loopy is of course a deterministic algorithm and has no variance. [sent-170, score-0.235]
</p><p>78 Figure 3 shows the results of the magnetization experiments. [sent-172, score-0.095]
</p><p>79 On both positive-potential graphs, Gibbs sampling gives high error and huge variance; SMCG gives lower variance but is still quite skewed. [sent-174, score-0.14]
</p><p>80 Our method experiences its worst performance on the homogeneous MRF but it is only 2. [sent-176, score-0.105]
</p><p>81 For the homogeneous MRF, SMCG degrades rapidly; loopy is still satisfactory at 15% error, but HC is at 2. [sent-181, score-0.364]
</p><p>82 On the uniform fully-connected graph, loopy actually gives a reasonable estimate of Z at 7. [sent-186, score-0.235]
</p><p>83 Figure 5 shows the variational (L1 ) distance between the exact marginal for a randomly chosen node in each graph and the approximate marginals of the 4 algorithms, a common measure of the “distance” between 2 distributions. [sent-188, score-0.281]
</p><p>84 For the Monte Carlo methods (HC, SMCG and Gibbs) the average over 50 independent runs was used to approximate the expected L1 error of the estimate. [sent-189, score-0.129]
</p><p>85 On the MRF with homogeneous Ψ, both loopy and SMCG degrade, but HC maintains a low error. [sent-191, score-0.34]
</p><p>86 Among the FC graphs, HC performs extremely well on the homogeneous Ψ and surprisingly loopy does well too. [sent-192, score-0.34]
</p><p>87 We chose this problem because it is a large model on which loopy is known to do well on, and can hence provide us with a measure of quality of the HC and SMCG results as larger numbers of edges are involved. [sent-196, score-0.298]
</p><p>88 From the toy examples we infer that the mechanism of HC is quite different from that of loopy as we have seen that it can work when loopy does not. [sent-197, score-0.528]
</p><p>89 Hence good performance on this problem would suggest that HC would scale well, which is a crucial question as in the large graph the ﬁnal distribution has many more edges than the initial spanning tree. [sent-198, score-0.241]
</p><p>90 The results were promising: the mean-squared reconstruction error using loopy and using HC were virtually identical at 9. [sent-199, score-0.272]
</p><p>91 5  HC SMCG Gibbs Loopy  0  Loopy  HC  Figure 5: Variational(L1 ) distance between estimated and true marginals for a randomly chosen node in each of the 4 graphs using the four approximate methods (smaller values mean less error. [sent-209, score-0.216]
</p><p>92 robust to the addition of around 9000 edges and many resampling stages. [sent-214, score-0.116]
</p><p>93 It is crucial to realize that MCMC is completely unsuited to some problems; see for example the “convergence” plots of the estimated magnetization of 3 independent Gibbs sampler runs on one of our “toy” graphs shown in Figure 6. [sent-216, score-0.277]
</p><p>94 Such behavior has been studied by Gore and Jerrum [11] and others, who discuss pessimistic theoretical results on the mixing properties of both Gibbs sampling and the celebrated Swendsen-Wang algorithm in several cases. [sent-217, score-0.133]
</p><p>95 To obtain a good estimate, MCMC requires that the process “visit” each of the target distribution’s basins of energy with a frequency representative of their probability. [sent-218, score-0.092]
</p><p>96 Unfortunately, some basins take an exponential amount of time to exit, and so different ﬁnite runs of MCMC will give quite different answers, leading to tremendous variance. [sent-219, score-0.081]
</p><p>97 The methodology presented here is an attempt to sidestep the whole issue of mixing by permitting the independent particles to be stuck in modes, but then considering them jointly when estimating. [sent-220, score-0.23]
</p><p>98 The object of the sequential phase is to address the difﬁcult problem of constructing a suitable proposal for high-dimensional problems; to this the resamplingbased methodology of particle ﬁlters was thought to be particularly suited. [sent-222, score-0.313]
</p><p>99 For the graphs we have considered, the single-edge algorithm we propose seems to be preferable to global annealing. [sent-223, score-0.089]
</p><p>100 The Markov chain Monte Carlo method: an approach to approximate counting and integration. [sent-257, score-0.078]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hc', 0.515), ('smcg', 0.401), ('loopy', 0.235), ('xn', 0.185), ('fn', 0.178), ('gibbs', 0.17), ('proposal', 0.147), ('wn', 0.147), ('qn', 0.146), ('particles', 0.146), ('mrf', 0.14), ('smc', 0.134), ('carlo', 0.128), ('monte', 0.128), ('homogeneous', 0.105), ('mcmc', 0.103), ('fc', 0.1), ('partition', 0.098), ('magnetization', 0.095), ('graphs', 0.089), ('var', 0.081), ('sequential', 0.079), ('spanning', 0.078), ('importance', 0.075), ('sampling', 0.074), ('graph', 0.072), ('edge', 0.069), ('edges', 0.063), ('potentials', 0.06), ('target', 0.059), ('variational', 0.057), ('particle', 0.056), ('coupling', 0.054), ('resampling', 0.053), ('annealing', 0.051), ('temperature', 0.049), ('zn', 0.049), ('runs', 0.048), ('samples', 0.047), ('jij', 0.045), ('tree', 0.045), ('chains', 0.045), ('sampler', 0.045), ('approximate', 0.044), ('ais', 0.042), ('annealed', 0.04), ('dxn', 0.038), ('freitas', 0.038), ('error', 0.037), ('graphical', 0.036), ('proposals', 0.036), ('xk', 0.036), ('population', 0.035), ('chain', 0.034), ('basins', 0.033), ('stationarity', 0.033), ('gore', 0.033), ('hot', 0.033), ('jerrum', 0.033), ('marginals', 0.032), ('kn', 0.032), ('mechanism', 0.032), ('markov', 0.031), ('methodology', 0.031), ('incremental', 0.031), ('celebrated', 0.03), ('mix', 0.03), ('normalization', 0.03), ('xi', 0.03), ('variance', 0.029), ('sequence', 0.029), ('mixing', 0.029), ('sequentially', 0.029), ('intermediate', 0.029), ('pairwise', 0.029), ('notoriously', 0.028), ('expectations', 0.028), ('distribution', 0.028), ('weights', 0.027), ('distributions', 0.027), ('physics', 0.027), ('backward', 0.027), ('undirected', 0.027), ('rumelhart', 0.027), ('doucet', 0.027), ('schedule', 0.027), ('distance', 0.027), ('toy', 0.026), ('xj', 0.026), ('minima', 0.026), ('marginal', 0.025), ('answers', 0.025), ('integral', 0.025), ('random', 0.025), ('sa', 0.024), ('stuck', 0.024), ('massive', 0.024), ('degrades', 0.024), ('node', 0.024), ('grid', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="90-tfidf-1" href="./nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs.html">90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</a></p>
<p>Author: Firas Hamze, Nando de Freitas</p><p>Abstract: This paper presents a new sampling algorithm for approximating functions of variables representable as undirected graphical models of arbitrary connectivity with pairwise potentials, as well as for estimating the notoriously difﬁcult partition function of the graph. The algorithm ﬁts into the framework of sequential Monte Carlo methods rather than the more widely used MCMC, and relies on constructing a sequence of intermediate distributions which get closer to the desired one. While the idea of using “tempered” proposals is known, we construct a novel sequence of target distributions where, rather than dropping a global temperature parameter, we sequentially couple individual pairs of variables that are, initially, sampled exactly from a spanning tree of the variables. We present experimental results on inference and estimation of the partition function for sparse and densely-connected graphs.</p><p>2 0.156929 <a title="90-tfidf-2" href="./nips-2005-Nested_sampling_for_Potts_models.html">133 nips-2005-Nested sampling for Potts models</a></p>
<p>Author: Iain Murray, David MacKay, Zoubin Ghahramani, John Skilling</p><p>Abstract: Nested sampling is a new Monte Carlo method by Skilling [1] intended for general Bayesian computation. Nested sampling provides a robust alternative to annealing-based methods for computing normalizing constants. It can also generate estimates of other quantities such as posterior expectations. The key technical requirement is an ability to draw samples uniformly from the prior subject to a constraint on the likelihood. We provide a demonstration with the Potts model, an undirected graphical model. 1</p><p>3 0.095308416 <a title="90-tfidf-3" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>Author: John D. Lafferty, Pradeep K. Ravikumar</p><p>Abstract: We present a family of approximation techniques for probabilistic graphical models, based on the use of graphical preconditioners developed in the scientiﬁc computing literature. Our framework yields rigorous upper and lower bounds on event probabilities and the log partition function of undirected graphical models, using non-iterative procedures that have low time complexity. As in mean ﬁeld approaches, the approximations are built upon tractable subgraphs; however, we recast the problem of optimizing the tractable distribution parameters and approximate inference in terms of the well-studied linear systems problem of obtaining a good matrix preconditioner. Experiments are presented that compare the new approximation schemes to variational methods. 1</p><p>4 0.093809091 <a title="90-tfidf-4" href="./nips-2005-An_Application_of_Markov_Random_Fields_to_Range_Sensing.html">23 nips-2005-An Application of Markov Random Fields to Range Sensing</a></p>
<p>Author: James Diebel, Sebastian Thrun</p><p>Abstract: This paper describes a highly successful application of MRFs to the problem of generating high-resolution range images. A new generation of range sensors combines the capture of low-resolution range images with the acquisition of registered high-resolution camera images. The MRF in this paper exploits the fact that discontinuities in range and coloring tend to co-align. This enables it to generate high-resolution, low-noise range images by integrating regular camera images into the range data. We show that by using such an MRF, we can substantially improve over existing range imaging technology. 1</p><p>5 0.082402259 <a title="90-tfidf-5" href="./nips-2005-An_Approximate_Inference_Approach_for_the_PCA_Reconstruction_Error.html">24 nips-2005-An Approximate Inference Approach for the PCA Reconstruction Error</a></p>
<p>Author: Manfred Opper</p><p>Abstract: The problem of computing a resample estimate for the reconstruction error in PCA is reformulated as an inference problem with the help of the replica method. Using the expectation consistent (EC) approximation, the intractable inference problem can be solved efﬁciently using only two variational parameters. A perturbative correction to the result is computed and an alternative simpliﬁed derivation is also presented. 1</p><p>6 0.076708838 <a title="90-tfidf-6" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>7 0.074868195 <a title="90-tfidf-7" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>8 0.072851777 <a title="90-tfidf-8" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>9 0.072729871 <a title="90-tfidf-9" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>10 0.072263941 <a title="90-tfidf-10" href="./nips-2005-Beyond_Gaussian_Processes%3A_On_the_Distributions_of_Infinite_Networks.html">38 nips-2005-Beyond Gaussian Processes: On the Distributions of Infinite Networks</a></p>
<p>11 0.070986137 <a title="90-tfidf-11" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>12 0.07004559 <a title="90-tfidf-12" href="./nips-2005-Context_as_Filtering.html">48 nips-2005-Context as Filtering</a></p>
<p>13 0.069267526 <a title="90-tfidf-13" href="./nips-2005-Variational_EM_Algorithms_for_Non-Gaussian_Latent_Variable_Models.html">202 nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</a></p>
<p>14 0.068807617 <a title="90-tfidf-14" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>15 0.06659022 <a title="90-tfidf-15" href="./nips-2005-Walk-Sum_Interpretation_and_Analysis_of_Gaussian_Belief_Propagation.html">204 nips-2005-Walk-Sum Interpretation and Analysis of Gaussian Belief Propagation</a></p>
<p>16 0.064180128 <a title="90-tfidf-16" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>17 0.063091256 <a title="90-tfidf-17" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>18 0.061294358 <a title="90-tfidf-18" href="./nips-2005-Interpolating_between_types_and_tokens_by_estimating_power-law_generators.html">100 nips-2005-Interpolating between types and tokens by estimating power-law generators</a></p>
<p>19 0.060929786 <a title="90-tfidf-19" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>20 0.060542144 <a title="90-tfidf-20" href="./nips-2005-An_Alternative_Infinite_Mixture_Of_Gaussian_Process_Experts.html">21 nips-2005-An Alternative Infinite Mixture Of Gaussian Process Experts</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.198), (1, 0.052), (2, 0.0), (3, 0.023), (4, 0.014), (5, -0.093), (6, -0.154), (7, 0.114), (8, 0.012), (9, 0.04), (10, 0.107), (11, 0.032), (12, 0.061), (13, 0.012), (14, -0.054), (15, -0.054), (16, 0.058), (17, 0.08), (18, 0.126), (19, -0.071), (20, 0.072), (21, 0.039), (22, -0.09), (23, 0.078), (24, 0.019), (25, 0.045), (26, 0.127), (27, -0.068), (28, -0.119), (29, 0.026), (30, 0.012), (31, -0.103), (32, 0.094), (33, -0.039), (34, 0.009), (35, -0.001), (36, 0.202), (37, 0.09), (38, 0.026), (39, -0.047), (40, -0.061), (41, 0.045), (42, 0.107), (43, 0.041), (44, -0.113), (45, 0.08), (46, -0.177), (47, -0.01), (48, 0.06), (49, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94779694 <a title="90-lsi-1" href="./nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs.html">90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</a></p>
<p>Author: Firas Hamze, Nando de Freitas</p><p>Abstract: This paper presents a new sampling algorithm for approximating functions of variables representable as undirected graphical models of arbitrary connectivity with pairwise potentials, as well as for estimating the notoriously difﬁcult partition function of the graph. The algorithm ﬁts into the framework of sequential Monte Carlo methods rather than the more widely used MCMC, and relies on constructing a sequence of intermediate distributions which get closer to the desired one. While the idea of using “tempered” proposals is known, we construct a novel sequence of target distributions where, rather than dropping a global temperature parameter, we sequentially couple individual pairs of variables that are, initially, sampled exactly from a spanning tree of the variables. We present experimental results on inference and estimation of the partition function for sparse and densely-connected graphs.</p><p>2 0.75176126 <a title="90-lsi-2" href="./nips-2005-Nested_sampling_for_Potts_models.html">133 nips-2005-Nested sampling for Potts models</a></p>
<p>Author: Iain Murray, David MacKay, Zoubin Ghahramani, John Skilling</p><p>Abstract: Nested sampling is a new Monte Carlo method by Skilling [1] intended for general Bayesian computation. Nested sampling provides a robust alternative to annealing-based methods for computing normalizing constants. It can also generate estimates of other quantities such as posterior expectations. The key technical requirement is an ability to draw samples uniformly from the prior subject to a constraint on the likelihood. We provide a demonstration with the Potts model, an undirected graphical model. 1</p><p>3 0.47102064 <a title="90-lsi-3" href="./nips-2005-Correcting_sample_selection_bias_in_maximum_entropy_density_estimation.html">51 nips-2005-Correcting sample selection bias in maximum entropy density estimation</a></p>
<p>Author: Miroslav Dudík, Steven J. Phillips, Robert E. Schapire</p><p>Abstract: We study the problem of maximum entropy density estimation in the presence of known sample selection bias. We propose three bias correction approaches. The ﬁrst one takes advantage of unbiased sufﬁcient statistics which can be obtained from biased samples. The second one estimates the biased distribution and then factors the bias out. The third one approximates the second by only using samples from the sampling distribution. We provide guarantees for the ﬁrst two approaches and evaluate the performance of all three approaches in synthetic experiments and on real data from species habitat modeling, where maxent has been successfully applied and where sample selection bias is a signiﬁcant problem. 1</p><p>4 0.45007753 <a title="90-lsi-4" href="./nips-2005-Non-iterative_Estimation_with_Perturbed_Gaussian_Markov_Processes.html">139 nips-2005-Non-iterative Estimation with Perturbed Gaussian Markov Processes</a></p>
<p>Author: Yunsong Huang, B. Keith Jenkins</p><p>Abstract: We develop an approach for estimation with Gaussian Markov processes that imposes a smoothness prior while allowing for discontinuities. Instead of propagating information laterally between neighboring nodes in a graph, we study the posterior distribution of the hidden nodes as a whole—how it is perturbed by invoking discontinuities, or weakening the edges, in the graph. We show that the resulting computation amounts to feed-forward fan-in operations reminiscent of V1 neurons. Moreover, using suitable matrix preconditioners, the incurred matrix inverse and determinant can be approximated, without iteration, in the same computational style. Simulation results illustrate the merits of this approach.</p><p>5 0.44383785 <a title="90-lsi-5" href="./nips-2005-Walk-Sum_Interpretation_and_Analysis_of_Gaussian_Belief_Propagation.html">204 nips-2005-Walk-Sum Interpretation and Analysis of Gaussian Belief Propagation</a></p>
<p>Author: Dmitry Malioutov, Alan S. Willsky, Jason K. Johnson</p><p>Abstract: This paper presents a new framework based on walks in a graph for analysis and inference in Gaussian graphical models. The key idea is to decompose correlations between variables as a sum over all walks between those variables in the graph. The weight of each walk is given by a product of edgewise partial correlations. We provide a walk-sum interpretation of Gaussian belief propagation in trees and of the approximate method of loopy belief propagation in graphs with cycles. This perspective leads to a better understanding of Gaussian belief propagation and of its convergence in loopy graphs. 1</p><p>6 0.43891332 <a title="90-lsi-6" href="./nips-2005-Prediction_and_Change_Detection.html">156 nips-2005-Prediction and Change Detection</a></p>
<p>7 0.41484147 <a title="90-lsi-7" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>8 0.39338726 <a title="90-lsi-8" href="./nips-2005-Logic_and_MRF_Circuitry_for_Labeling_Occluding_and_Thinline_Visual_Contours.html">122 nips-2005-Logic and MRF Circuitry for Labeling Occluding and Thinline Visual Contours</a></p>
<p>9 0.3830761 <a title="90-lsi-9" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>10 0.37426889 <a title="90-lsi-10" href="./nips-2005-Message_passing_for_task_redistribution_on_sparse_graphs.html">125 nips-2005-Message passing for task redistribution on sparse graphs</a></p>
<p>11 0.37307331 <a title="90-lsi-11" href="./nips-2005-An_Approximate_Inference_Approach_for_the_PCA_Reconstruction_Error.html">24 nips-2005-An Approximate Inference Approach for the PCA Reconstruction Error</a></p>
<p>12 0.37036175 <a title="90-lsi-12" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>13 0.35912269 <a title="90-lsi-13" href="./nips-2005-Context_as_Filtering.html">48 nips-2005-Context as Filtering</a></p>
<p>14 0.35456368 <a title="90-lsi-14" href="./nips-2005-Interpolating_between_types_and_tokens_by_estimating_power-law_generators.html">100 nips-2005-Interpolating between types and tokens by estimating power-law generators</a></p>
<p>15 0.35092053 <a title="90-lsi-15" href="./nips-2005-Beyond_Gaussian_Processes%3A_On_the_Distributions_of_Infinite_Networks.html">38 nips-2005-Beyond Gaussian Processes: On the Distributions of Infinite Networks</a></p>
<p>16 0.32294703 <a title="90-lsi-16" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>17 0.31762689 <a title="90-lsi-17" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>18 0.3115072 <a title="90-lsi-18" href="./nips-2005-Infinite_latent_feature_models_and_the_Indian_buffet_process.html">98 nips-2005-Infinite latent feature models and the Indian buffet process</a></p>
<p>19 0.31008926 <a title="90-lsi-19" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>20 0.30234176 <a title="90-lsi-20" href="./nips-2005-Efficient_Estimation_of_OOMs.html">62 nips-2005-Efficient Estimation of OOMs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.036), (10, 0.046), (27, 0.031), (31, 0.083), (34, 0.076), (35, 0.245), (39, 0.021), (41, 0.014), (55, 0.032), (57, 0.018), (69, 0.072), (73, 0.047), (88, 0.065), (91, 0.089), (92, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83969396 <a title="90-lda-1" href="./nips-2005-Location-based_activity_recognition.html">121 nips-2005-Location-based activity recognition</a></p>
<p>Author: Lin Liao, Dieter Fox, Henry Kautz</p><p>Abstract: Learning patterns of human behavior from sensor data is extremely important for high-level activity inference. We show how to extract and label a person’s activities and signiﬁcant places from traces of GPS data. In contrast to existing techniques, our approach simultaneously detects and classiﬁes the signiﬁcant locations of a person and takes the highlevel context into account. Our system uses relational Markov networks to represent the hierarchical activity model that encodes the complex relations among GPS readings, activities and signiﬁcant places. We apply FFT-based message passing to perform efﬁcient summation over large numbers of nodes in the networks. We present experiments that show signiﬁcant improvements over existing techniques. 1</p><p>same-paper 2 0.81394446 <a title="90-lda-2" href="./nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs.html">90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</a></p>
<p>Author: Firas Hamze, Nando de Freitas</p><p>Abstract: This paper presents a new sampling algorithm for approximating functions of variables representable as undirected graphical models of arbitrary connectivity with pairwise potentials, as well as for estimating the notoriously difﬁcult partition function of the graph. The algorithm ﬁts into the framework of sequential Monte Carlo methods rather than the more widely used MCMC, and relies on constructing a sequence of intermediate distributions which get closer to the desired one. While the idea of using “tempered” proposals is known, we construct a novel sequence of target distributions where, rather than dropping a global temperature parameter, we sequentially couple individual pairs of variables that are, initially, sampled exactly from a spanning tree of the variables. We present experimental results on inference and estimation of the partition function for sparse and densely-connected graphs.</p><p>3 0.79937965 <a title="90-lda-3" href="./nips-2005-Oblivious_Equilibrium%3A_A_Mean_Field_Approximation_for_Large-Scale_Dynamic_Games.html">142 nips-2005-Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games</a></p>
<p>Author: Gabriel Y. Weintraub, Lanier Benkard, Benjamin Van Roy</p><p>Abstract: We propose a mean-ﬁeld approximation that dramatically reduces the computational complexity of solving stochastic dynamic games. We provide conditions that guarantee our method approximates an equilibrium as the number of agents grow. We then derive a performance bound to assess how well the approximation performs for any given number of agents. We apply our method to an important class of problems in applied microeconomics. We show with numerical experiments that we are able to greatly expand the set of economic problems that can be analyzed computationally. 1</p><p>4 0.58512139 <a title="90-lda-4" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>Author: O. P. Kreidl, Alan S. Willsky</p><p>Abstract: Given a directed graphical model with binary-valued hidden nodes and real-valued noisy observations, consider deciding upon the maximum a-posteriori (MAP) or the maximum posterior-marginal (MPM) assignment under the restriction that each node broadcasts only to its children exactly one single-bit message. We present a variational formulation, viewing the processing rules local to all nodes as degrees-of-freedom, that minimizes the loss in expected (MAP or MPM) performance subject to such online communication constraints. The approach leads to a novel message-passing algorithm to be executed ofﬂine, or before observations are realized, which mitigates the performance loss by iteratively coupling all rules in a manner implicitly driven by global statistics. We also provide (i) illustrative examples, (ii) assumptions that guarantee convergence and efﬁciency and (iii) connections to active research areas. 1</p><p>5 0.58171391 <a title="90-lda-5" href="./nips-2005-Nested_sampling_for_Potts_models.html">133 nips-2005-Nested sampling for Potts models</a></p>
<p>Author: Iain Murray, David MacKay, Zoubin Ghahramani, John Skilling</p><p>Abstract: Nested sampling is a new Monte Carlo method by Skilling [1] intended for general Bayesian computation. Nested sampling provides a robust alternative to annealing-based methods for computing normalizing constants. It can also generate estimates of other quantities such as posterior expectations. The key technical requirement is an ability to draw samples uniformly from the prior subject to a constraint on the likelihood. We provide a demonstration with the Potts model, an undirected graphical model. 1</p><p>6 0.56055248 <a title="90-lda-6" href="./nips-2005-Spiking_Inputs_to_a_Winner-take-all_Network.html">181 nips-2005-Spiking Inputs to a Winner-take-all Network</a></p>
<p>7 0.55989176 <a title="90-lda-7" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>8 0.55823016 <a title="90-lda-8" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>9 0.55510318 <a title="90-lda-9" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>10 0.55264604 <a title="90-lda-10" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>11 0.55042017 <a title="90-lda-11" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>12 0.54910165 <a title="90-lda-12" href="./nips-2005-Measuring_Shared_Information_and_Coordinated_Activity_in_Neuronal_Networks.html">124 nips-2005-Measuring Shared Information and Coordinated Activity in Neuronal Networks</a></p>
<p>13 0.54678047 <a title="90-lda-13" href="./nips-2005-Augmented_Rescorla-Wagner_and_Maximum_Likelihood_Estimation.html">32 nips-2005-Augmented Rescorla-Wagner and Maximum Likelihood Estimation</a></p>
<p>14 0.54511279 <a title="90-lda-14" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>15 0.54274601 <a title="90-lda-15" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>16 0.54233289 <a title="90-lda-16" href="./nips-2005-Principles_of_real-time_computing_with_feedback_applied_to_cortical_microcircuit_models.html">157 nips-2005-Principles of real-time computing with feedback applied to cortical microcircuit models</a></p>
<p>17 0.54138386 <a title="90-lda-17" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>18 0.54061359 <a title="90-lda-18" href="./nips-2005-Variational_Bayesian_Stochastic_Complexity_of_Mixture_Models.html">201 nips-2005-Variational Bayesian Stochastic Complexity of Mixture Models</a></p>
<p>19 0.53949732 <a title="90-lda-19" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>20 0.53645831 <a title="90-lda-20" href="./nips-2005-Dynamical_Synapses_Give_Rise_to_a_Power-Law_Distribution_of_Neuronal_Avalanches.html">61 nips-2005-Dynamical Synapses Give Rise to a Power-Law Distribution of Neuronal Avalanches</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
