<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>91 nips-2005-How fast to work: Response vigor, motivation and tonic dopamine</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-91" href="#">nips2005-91</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>91 nips-2005-How fast to work: Response vigor, motivation and tonic dopamine</h1>
<br/><p>Source: <a title="nips-2005-91-pdf" href="http://papers.nips.cc/paper/2842-how-fast-to-work-response-vigor-motivation-and-tonic-dopamine.pdf">pdf</a></p><p>Author: Yael Niv, Nathaniel D. Daw, Peter Dayan</p><p>Abstract: Reinforcement learning models have long promised to unify computational, psychological and neural accounts of appetitively conditioned behavior. However, the bulk of data on animal conditioning comes from free-operant experiments measuring how fast animals will work for reinforcement. Existing reinforcement learning (RL) models are silent about these tasks, because they lack any notion of vigor. They thus fail to address the simple observation that hungrier animals will work harder for food, as well as stranger facts such as their sometimes greater productivity even when working for irrelevant outcomes such as water. Here, we develop an RL framework for free-operant behavior, suggesting that subjects choose how vigorously to perform selected actions by optimally balancing the costs and beneﬁts of quick responding. Motivational states such as hunger shift these factors, skewing the tradeoff. This accounts normatively for the effects of motivation on response rates, as well as many other classic ﬁndings. Finally, we suggest that tonic levels of dopamine may be involved in the computation linking motivational state to optimal responding, thereby explaining the complex vigor-related effects of pharmacological manipulation of dopamine. 1</p><p>Reference: <a title="nips-2005-91-reference" href="../nips2005_reference/nips-2005-How_fast_to_work%3A_Response_vigor%2C_motivation_and_tonic_dopamine_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 How fast to work: Response vigor, motivation and tonic dopamine 1  Yael Niv1,2 Nathaniel D. [sent-1, score-0.364]
</p><p>2 This accounts normatively for the effects of motivation on response rates, as well as many other classic ﬁndings. [sent-15, score-0.273]
</p><p>3 Finally, we suggest that tonic levels of dopamine may be involved in the computation linking motivational state to optimal responding, thereby explaining the complex vigor-related effects of pharmacological manipulation of dopamine. [sent-16, score-0.637]
</p><p>4 1  Introduction  A banal, but nonetheless valid, behaviorist observation is that hungry animals work harder to get food [1]. [sent-17, score-0.49]
</p><p>5 The ﬁrst weird fact is that hungry animals will in some circumstances work more vigorously even for motivationally irrelevant outcomes such as water [2, 3], which seems highly counterintuitive. [sent-19, score-0.392]
</p><p>6 Finally, computational theories fail to deliver on the close link they trumpet between DA, behavior, and reinforcement learning (RL; eg [6]), as they do not address the whole experimental paradigm of free-operant tasks [7], whence hail those and many other results. [sent-21, score-0.225]
</p><p>7 Hungry rats (open circles) clearly press the lever at a higher rate than sated rats (ﬁlled circles). [sent-26, score-0.459]
</p><p>8 (b) The relationship between rate of responding and rate of reinforcement (reciprocal of the interval) on an RI schedule, is hyperbolic (of the form y = B · x/(x + x0 )). [sent-28, score-0.46]
</p><p>9 (c) Total number of leverpresses per session averaged over ﬁve 30 minute sessions by rats pressing for food on different FR schedules. [sent-30, score-0.675]
</p><p>10 Rats with nucleus accumbens 6-OHDA dopamine lesions (gray) press signiﬁcantly less than control rats (black), with the difference larger for higher ratio requirements. [sent-31, score-0.543]
</p><p>11 We consider optimal control in a continuous-time Markov Decision Process (MDP), in which agents must choose both an action and the latency with which to emit it (ie how vigorously, or at what instantaneous rate to perform it). [sent-34, score-0.319]
</p><p>12 Our model treats response vigor as being determined normatively, as the outcome of a battle between the cost of behaving more expeditiously and the beneﬁt of achieving desirable outcomes more quickly. [sent-35, score-0.357]
</p><p>13 A key feature of this model is that response rates are strongly dependent on the expected average reward rate, because this determines the opportunity cost of sloth. [sent-38, score-0.458]
</p><p>14 By inﬂuencing the value of reinforcers — and through this, the average reward rate — motivational states such as hunger inﬂuence the output response latencies (and not only response choice). [sent-39, score-0.896]
</p><p>15 Thus, in our model, hungry animals should optimally also work harder for water, since in typical circumstances, this should allow them to return more quickly to working for food. [sent-40, score-0.222]
</p><p>16 2  Free-operant behavior  We consider the free-operant scenario common in experimental psychology, in which an animal is placed in an experimental chamber, and can choose freely which actions to emit and when. [sent-42, score-0.215]
</p><p>17 Most actions have no programmed consequences; however, one action (eg leverpressing; LP) is rewarded with food (which falls into a food magazine) according to an experimenter-determined schedule of reinforcement. [sent-43, score-0.922]
</p><p>18 The schedule of reinforcement deﬁnes the (possibly stochastic) relationship between the delivery of a reward and one or both of (a) the number of LPs, and (b) the time since the last reward was delivered. [sent-45, score-0.906]
</p><p>19 Schedules are often labelled by their type and a parameter, so RI30 is a random interval schedule with the exponential waiting time having a mean of 30 seconds [7]. [sent-47, score-0.232]
</p><p>20 Fig 1a shows response metrics from rats leverpressing on an RI30 schedule. [sent-49, score-0.411]
</p><p>21 Leverpressing builds up to a relatively constant rate following a rather long pause after gaining each reward, during which the food is consumed. [sent-50, score-0.334]
</p><p>22 Figure 1b shows the total number of LP responses in a 30 minute session for different interval schedules. [sent-53, score-0.223]
</p><p>23 The hyperbolic relationship between the reward rate (the inverse of the interval) and the response rate is a classic hallmark of free operant behavior [9]. [sent-54, score-0.724]
</p><p>24 Based on its state, the agent chooses both an action (a), and a latency (τ ) at which to emit it. [sent-56, score-0.318]
</p><p>25 After time τ has elapsed, the action is completed, the agent receives rewards and incurs costs associated with its choice, and then selects a new (a, τ ) pair based on its new state. [sent-57, score-0.279]
</p><p>26 We deﬁne three possible actions a ∈ {LP, NP, other}, where we take a = other to include the various miscellaneous behaviors such as grooming, rearing, and snifﬁng which animals typically perform during the experiment. [sent-58, score-0.209]
</p><p>27 For simplicity we consider unit actions, with the latency τ related to the vigor with which this unit is performed. [sent-59, score-0.377]
</p><p>28 To account for consumption time (which is nonnegligible [11, 13]), if the agent nose-pokes and food is available, a predeﬁned time t eat passes before the next decision point (and the next state) is reached. [sent-60, score-0.333]
</p><p>29 Following Staddon [14], we assume one part of the cost of an action to be proportional to the vigor of its execution, ie inversely proportional to τ . [sent-62, score-0.344]
</p><p>30 Each action also incurs a ﬁxed ‘internal’ reward or cost of ρ(a) per unit, typically with other being rewarding. [sent-64, score-0.417]
</p><p>31 The reinforcement schedule deﬁnes the probability of reward delivery for each state-action-latency triplet. [sent-65, score-0.639]
</p><p>32 An available reward can be harvested by a = NP into the magazine, and we assume that the thereby obtained subjective utility U (r) of the food reward is motivation-dependent, such that food is worth more to a hungry animal than to a sated one. [sent-66, score-1.332]
</p><p>33 Speciﬁcally, the state space includes the identity of the previous action, an indicator as to whether a reward is available in the food magazine, and, as necessary, the number of LPs since the previous reinforcement (for FR) or the elapsed time since the previous LP (for RI). [sent-68, score-0.758]
</p><p>34 The transitions between the states P (S |S, a, τ ) and the reward function Pr (S, a, τ ) are deﬁned by the dynamics of the schedule of reinforcement, and all rewards and costs are harvested at state transitions and considered as point events. [sent-69, score-0.575]
</p><p>35 In the following we treat the problem of optimising a policy (which action to take and with what latency, given the state) in order to maximize the average rate of return (rewards minus costs per time). [sent-70, score-0.248]
</p><p>36 (c) The relationship between the total number of responses (circles) and rate of reinforcement is hyperbolic (solid line: hyperbolic curve ﬁt). [sent-73, score-0.366]
</p><p>37 The mean latency to leverpress (dashed line) decreases as the rate of reinforcement increases. [sent-74, score-0.472]
</p><p>38 average-adjusted) value of state S is:  ﬀ Z Kv (aprev , a) V ∗ (S) = max ρ(a)− +U (r)Pr (S, a, τ )−τ · r+ dS P (S |S, a, τ )V ∗ (S ) a,τ τ  (1)  where r is the long term average reward rate (whose subtraction from the value quantiﬁes the opportunity cost of delay). [sent-75, score-0.396]
</p><p>39 Our overriding aim is to replicate basic aspects of free operant behavior qualitatively, in order to understand the normative foundations of response vigor. [sent-81, score-0.306]
</p><p>40 In rough accordance with the behavior displayed by animals (which is similar to that shown in Fig 1a), the LP rate is constant over time, bar a pause for consumption. [sent-84, score-0.241]
</p><p>41 Fig 2b depicts the model’s behavior in a yoked random interval schedule, in which the intervals between rewards were set to match exactly the intervals obtained by the agent trained on the ratio schedule in Fig 2a. [sent-85, score-0.482]
</p><p>42 The response rate is again constant over time, but it is also considerably lower than that in the corresponding RR schedule, although the external reward density is similar. [sent-86, score-0.456]
</p><p>43 In ratio schedules, since P (Sr |τ ) is independent of τ , the optimizing latency ∗ is τLP = Kv (LP, LP)/r, its inverse deﬁning the optimal rate of leverpressing. [sent-90, score-0.254]
</p><p>44 In interval schedules, however, P (Sr |τ ) = 1 − exp{−τ /T } where T is the schedule interval. [sent-91, score-0.232]
</p><p>45 (2) we ﬁnd that the optimal latency to leverpress τ LP satisﬁes ∗ ∗2 Kv (LP, LP)/τLP − r + (1/T )[V ∗ (Sr ) − V ∗ (Snr )] · exp{−τLP /T } = 0. [sent-93, score-0.246]
</p><p>46 Intuitively, since longer inter-response intervals increase the probability of reward per press in interval schedules but not in ratio schedules, the optimal leverpressing rate is lower in the former than in the latter. [sent-95, score-0.858]
</p><p>47 Fig 2c shows the average number of LPs in a 5 minute session for different interval schedules. [sent-96, score-0.223]
</p><p>48 On the ‘molecular’ level of single action choices, the mean latency τ LP between consecutive LPs decreases as the probability of reinforcement increases. [sent-98, score-0.413]
</p><p>49 This measure of response vigor is actually more accurate than the overall response measure, as it is not contaminated by competition with other actions, or confounded with the number of reinforcers per session for different schedules (and the time forgone when consuming them). [sent-99, score-0.819]
</p><p>50 For this reason, although we (correctly; see [13]) predict that inter-response latency should slow for higher ratio requirements, raw LP counts can actually increase, as in Fig. [sent-100, score-0.224]
</p><p>51 5  Drive and dopamine  Having provided a qualitative account of the basic patterns of free operant rates of behavior, we turn to the main theoretical conundrum — the effects of drive and DA manipulations on response vigor. [sent-102, score-0.608]
</p><p>52 The key to understanding these is the role that the average reward r plays in the tradeoffs determining optimal response vigor. [sent-103, score-0.39]
</p><p>53 In effect, the average expected reward per unit time quantiﬁes the opportunity cost for doing nothing (and receiving no reward) for that time; its increase thus produces general pressure for faster work. [sent-104, score-0.34]
</p><p>54 A direct consequence of making the agent hungrier is that the subjective utility of food is enhanced. [sent-105, score-0.405]
</p><p>55 This will have interrelated effects on the optimal average reward r, the optimal values V ∗ , and the resultant optimal action choices and vigors. [sent-106, score-0.45]
</p><p>56 Notably, so long as the policy obtains food, its average reward rate will increase. [sent-107, score-0.333]
</p><p>57 The increase in r will increase the optimal ∗ LP rate 1/τLP = r/Kv (LP, LP), as the higher reward utility offsets higher procurement costs. [sent-109, score-0.448]
</p><p>58 The higher r increases the cost of sloth, since every τ time without reward forgoes an expected (τ · r) mean reward. [sent-112, score-0.303]
</p><p>59 Higher average rewards penalize late actions more than they do early ones, thus tilting action selection toward faster behavior, for all pre-potent actions. [sent-113, score-0.284]
</p><p>60 Essentially, hunger encourages the agent to complete irrelevant actions faster, in order to be able to resume leverpressing more quickly. [sent-114, score-0.453]
</p><p>61 For other schedules, the same effects generally hold (although the analytical reasoning is complicated by the fact that the optimal latencies may in these cases depend not only on the new average reward but also on the new values V ∗ ). [sent-115, score-0.413]
</p><p>62 Fig 3a shows simulated responding on an RI25 schedule in which the internal reward for the food-irrelevant action other has been set high enough to warrant non-negligible base responding. [sent-116, score-0.649]
</p><p>63 (c) The effect of an irrelevant drive (hungry animals leverpressing for water rewards): r was increased by 4% compared to (a). [sent-121, score-0.454]
</p><p>64 (d) Mean latencies to responding τ for LP and other in baseline (a; black), increased hunger (b; white) and irrelevant drive (c; gray). [sent-122, score-0.382]
</p><p>65 (e) Q values for leverpressing at different latencies τ . [sent-123, score-0.234]
</p><p>66 In red (middle, solid) and green (bottom, solid) are the values adjusted for two different average reward rates. [sent-125, score-0.267]
</p><p>67 The higher reward rate penalizes late actions more, thereby causing faster responding, as shown by the corresponding softmaxed action probability curves (dashed). [sent-126, score-0.581]
</p><p>68 (f) Simulation of DA depletion: overall leverpress count over 30 minute sessions (each bar averaging 15 sessions), for different FR requirements (bottom). [sent-127, score-0.25]
</p><p>69 The effects of the depletion seem more pronounced in higher schedules (compare to Fig 1c), but this actually results from the interaction with the number of rewards attained (see text). [sent-129, score-0.475]
</p><p>70 the utility of food is increased by 50%, the agent chooses to leverpress more, at the expense of other actions. [sent-130, score-0.479]
</p><p>71 This illustrates the ‘directing’ effect of motivation, by which the agent is directed more forcefully toward the motivationally relevant action [17]. [sent-131, score-0.245]
</p><p>72 Furthermore, the second, ‘driving’ effect, by which motivation increases vigor globally [17], is illustrated in Fig 3d which shows that, in fact, the latency to both actions has decreased. [sent-132, score-0.527]
</p><p>73 This general drive effect can be better isolated if we examine hungry agents leverpressing for water (rather than food), without competition from actions for food. [sent-134, score-0.527]
</p><p>74 We can view our leverpressing MDP as a portion of a larger one, which also includes (for instance) occasional opportunities for visits to a home cage where food is available. [sent-135, score-0.429]
</p><p>75 Without explicitly specifying all this extra structure, a good approximation is to take hunger as again causing an increase in the global rate of reinforcement r, reﬂecting the increase in the utility of food received elsewhere. [sent-136, score-0.625]
</p><p>76 Fig 3c shows the effects on responding on an interval schedule, of estimating the average reward rate to be 4% higher than in Fig 3a, and deriving new Q values from the previous V ∗ with this new r as illustrated in Fig 3e. [sent-137, score-0.598]
</p><p>77 1c is representative of a host of results from the lab of Salamone [4, 12] which show that lower levels of DA in the nucleus accumbens (a structure in the basal ganglia implicated in action selection) result in lower response rates. [sent-142, score-0.458]
</p><p>78 As a result of this apparent dependence on the response requirement, Salamone and his colleagues have hypothesized that DA enables animals to overcome higher work demands. [sent-144, score-0.266]
</p><p>79 We suggest that tonic levels of DA represent the average reward rate (a role tentatively proposed for serotonin in [16]). [sent-145, score-0.554]
</p><p>80 Thus a higher tonic level of DA represents a situation akin to higher drive, in which behavior is more vigorous, and lower tonic levels of DA cause a general slowing of behavior. [sent-146, score-0.491]
</p><p>81 3f shows the simulated response counts for different FR schedules in two conditions. [sent-148, score-0.344]
</p><p>82 The control condition is the standard model described above; DA depletion was modeled by decreasing tonic DA levels (and therefore r) to 40% of their original levels. [sent-149, score-0.25]
</p><p>83 Here, the apparently small effect on the number of LPs for low ratio schedules actually arises because of the large amount of time spent eating. [sent-152, score-0.333]
</p><p>84 In addition to the normative starting point it offers for investigations of response vigor, our theory provides a relatively ﬁne scalpel for dissecting the temporal details of behavior, such as the distributions of interresponse intervals at particular state transitions. [sent-157, score-0.204]
</p><p>85 Response vigor and dopamine’s role in controlling it have appeared in previous RL models of behavior [20, 21], but only as fairly ad-hoc bolt-ons — for instance, using repeated choices between doing nothing versus something to capture response latency. [sent-161, score-0.425]
</p><p>86 Here, these aspects are wholly integrated into the explanatory framework: optimizing response vigor is treated as itself an RL problem, with a natural dopaminergic substrate. [sent-162, score-0.401]
</p><p>87 By contrast, due to the way they cache outcome values, the action choices of such RL systems are characteristically insensitive to the ‘directing’ effects of motivational manipulations [22]. [sent-165, score-0.351]
</p><p>88 In animal behavior, ‘habitual actions’ (the ones associated with the DA system) are indeed motivationally insensitive for action choice, but show a direct effect of drive on vigor [23]. [sent-166, score-0.545]
</p><p>89 We further elaborate this link by suggesting an additional role for tonic levels of DA in online vigor selection. [sent-169, score-0.411]
</p><p>90 A major question remains as to whether phasic responses (which are known to correlate with response latency [25]) play an additional role in determining response vigor. [sent-170, score-0.432]
</p><p>91 The representation of state is more challenging — the assumption of a fully observable state space automatically appropriate for the schedule of reinforcement is not realistic. [sent-174, score-0.394]
</p><p>92 Indeed, apparently sub-optimal actions emitted by animals, eg engaging in excessive nose-poking even when a reward has not audibly dropped into the food magazine [11], may provide clues to this issue. [sent-175, score-0.772]
</p><p>93 Finally, it will be crucial to consider the fact that animals’ decisions about vigor may translate only noisily into response times, due, for instance, to the variability of internal timing [27]. [sent-176, score-0.357]
</p><p>94 The role of nucleus accumbens dopamine in motivated behavior: a unifying interpretation with special reference to reward-seeking. [sent-203, score-0.335]
</p><p>95 Performance on ratio and interval schedules with matched reinforcement rates. [sent-233, score-0.484]
</p><p>96 Motivational effects on behavior: Towards a reinforcement learning model of rates of responding. [sent-244, score-0.268]
</p><p>97 Nucleus accumbens dopamine depletions make rats more sensitive to high ratio requirements but do not impair primary food reinforcement. [sent-250, score-0.714]
</p><p>98 Average reward reinforcement learning: Foundations, algorithms and empirical results. [sent-270, score-0.427]
</p><p>99 A functional effect of dopamine in the nucleus accumbens and in some other dopamine-rich parts of the rat brain. [sent-295, score-0.367]
</p><p>100 Correlated coding of motivation and outcome of decision by dopamine neurons. [sent-341, score-0.234]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lp', 0.286), ('food', 0.268), ('reward', 0.267), ('da', 0.262), ('vigor', 0.234), ('schedules', 0.221), ('dopamine', 0.186), ('schedule', 0.174), ('leverpressing', 0.161), ('reinforcement', 0.16), ('lps', 0.146), ('latency', 0.143), ('fig', 0.141), ('tonic', 0.13), ('motivational', 0.127), ('rats', 0.127), ('response', 0.123), ('minute', 0.116), ('hungry', 0.115), ('action', 0.11), ('animals', 0.107), ('leverpress', 0.103), ('actions', 0.102), ('responding', 0.098), ('rl', 0.094), ('sr', 0.089), ('accumbens', 0.088), ('hunger', 0.088), ('drive', 0.086), ('depletion', 0.073), ('latencies', 0.073), ('effects', 0.073), ('rewards', 0.072), ('hyperbolic', 0.07), ('magazine', 0.07), ('behavior', 0.068), ('rate', 0.066), ('eg', 0.065), ('fr', 0.065), ('agent', 0.065), ('vigorously', 0.064), ('kv', 0.064), ('operant', 0.064), ('nucleus', 0.061), ('salamone', 0.059), ('sated', 0.059), ('interval', 0.058), ('daw', 0.054), ('np', 0.052), ('normative', 0.051), ('session', 0.049), ('motivation', 0.048), ('levels', 0.047), ('ratio', 0.045), ('animal', 0.045), ('balleine', 0.044), ('dopaminergic', 0.044), ('herrnstein', 0.044), ('lever', 0.044), ('leverpresses', 0.044), ('pharmacological', 0.044), ('poke', 0.044), ('reinforcements', 0.044), ('serotonin', 0.044), ('slowing', 0.044), ('phasic', 0.043), ('sec', 0.043), ('utility', 0.043), ('manipulations', 0.041), ('per', 0.04), ('dayan', 0.039), ('delivery', 0.038), ('motivationally', 0.038), ('irrelevant', 0.037), ('higher', 0.036), ('rates', 0.035), ('dickinson', 0.035), ('nose', 0.035), ('spent', 0.035), ('snr', 0.034), ('behavioral', 0.034), ('rr', 0.033), ('elapsed', 0.033), ('opportunity', 0.033), ('effect', 0.032), ('costs', 0.032), ('water', 0.031), ('circles', 0.031), ('bellman', 0.031), ('sessions', 0.031), ('state', 0.03), ('amphetamine', 0.029), ('directing', 0.029), ('ganglia', 0.029), ('hungrier', 0.029), ('molar', 0.029), ('normatively', 0.029), ('reinforcers', 0.029), ('staddon', 0.029), ('stranger', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="91-tfidf-1" href="./nips-2005-How_fast_to_work%3A_Response_vigor%2C_motivation_and_tonic_dopamine.html">91 nips-2005-How fast to work: Response vigor, motivation and tonic dopamine</a></p>
<p>Author: Yael Niv, Nathaniel D. Daw, Peter Dayan</p><p>Abstract: Reinforcement learning models have long promised to unify computational, psychological and neural accounts of appetitively conditioned behavior. However, the bulk of data on animal conditioning comes from free-operant experiments measuring how fast animals will work for reinforcement. Existing reinforcement learning (RL) models are silent about these tasks, because they lack any notion of vigor. They thus fail to address the simple observation that hungrier animals will work harder for food, as well as stranger facts such as their sometimes greater productivity even when working for irrelevant outcomes such as water. Here, we develop an RL framework for free-operant behavior, suggesting that subjects choose how vigorously to perform selected actions by optimally balancing the costs and beneﬁts of quick responding. Motivational states such as hunger shift these factors, skewing the tradeoff. This accounts normatively for the effects of motivation on response rates, as well as many other classic ﬁndings. Finally, we suggest that tonic levels of dopamine may be involved in the computation linking motivational state to optimal responding, thereby explaining the complex vigor-related effects of pharmacological manipulation of dopamine. 1</p><p>2 0.23692279 <a title="91-tfidf-2" href="./nips-2005-An_exploration-exploitation_model_based_on_norepinepherine_and_dopamine_activity.html">26 nips-2005-An exploration-exploitation model based on norepinepherine and dopamine activity</a></p>
<p>Author: Samuel M. McClure, Mark S. Gilzenrat, Jonathan D. Cohen</p><p>Abstract: We propose a model by which dopamine (DA) and norepinepherine (NE) combine to alternate behavior between relatively exploratory and exploitative modes. The model is developed for a target detection task for which there is extant single neuron recording data available from locus coeruleus (LC) NE neurons. An exploration-exploitation trade-off is elicited by regularly switching which of the two stimuli are rewarded. DA functions within the model to change synaptic weights according to a reinforcement learning algorithm. Exploration is mediated by the state of LC firing, with higher tonic and lower phasic activity producing greater response variability. The opposite state of LC function, with lower baseline firing rate and greater phasic responses, favors exploitative behavior. Changes in LC firing mode result from combined measures of response conflict and reward rate, where response conflict is monitored using models of anterior cingulate cortex (ACC). Increased long-term response conflict and decreased reward rate, which occurs following reward contingency switch, favors the higher tonic state of LC function and NE release. This increases exploration, and facilitates discovery of the new target. 1 In t rod u ct i on A central problem in reinforcement learning is determining how to adaptively move between exploitative and exploratory behaviors in changing environments. We propose a set of neurophysiologic mechanisms whose interaction may mediate this behavioral shift. Empirical work on the midbrain dopamine (DA) system has suggested that this system is particularly well suited for guiding exploitative behaviors. This hypothesis has been reified by a number of studies showing that a temporal difference (TD) learning algorithm accounts for activity in these neurons in a wide variety of behavioral tasks [1,2]. DA release is believed to encode a reward prediction error signal that acts to change synaptic weights relevant for producing behaviors [3]. Through learning, this allows neural pathways to predict future expected reward through the relative strength of their synaptic connections [1]. Decision-making procedures based on these value estimates are necessarily greedy. Including reward bonuses for exploratory choices supports non-greedy actions [4] and accounts for additional data derived from DA neurons [5]. We show that combining a DA learning algorithm with models of response conflict detection [6] and NE function [7] produces an effective annealing procedure for alternating between exploration and exploitation. NE neurons within the LC alternate between two firing modes [8]. In the first mode, known as the phasic mode, NE neurons fire at a low baseline rate but have relatively robust phasic responses to behaviorally salient stimuli. The second mode, called the tonic mode, is associated with a higher baseline firing and absent or attenuated phasic responses. The effects of NE on efferent areas are modulatory in nature, and are well captured as a change in the gain of efferent inputs so that neuronal responses are potentiated in the presence of NE [9]. Thus, in phasic mode, the LC provides transient facilitation in processing, time-locked to the presence of behaviorally salient information in motor or decision areas. Conversely, in tonic mode, higher overall LC discharge rate increases gain generally and hence increases the probability of arbitrary responding. Consistent with this account, for periods when NE neurons are in the phasic mode, monkey performance is nearly perfect. However, when NE neurons are in the tonic mode, performance is more erratic, with increased response times and error rate [8]. These findings have led to a recent characterization of the LC as a dynamic temporal filter, adjusting the system's relative responsivity to salient and irrelevant information [8]. In this way, the LC is ideally positioned to mediate the shift between exploitative and exploratory behavior. The parameters that underlie changes in LC firing mode remain largely unexplored. Based on data from a target detection task by Aston-Jones and colleagues [10], we propose that LC firing mode is determined in part by measures of response conflict and reward rate as calculated by the ACC and OFC, respectively [8]. Together, the ACC and OFC are the principle sources of cortical input to the LC [8]. Activity in the ACC is known, largely through human neuroimaging experiments, to change in accord with response conflict [6]. In brief, relatively equal activity in competing behavioral responses (reflecting uncertainty) produces high conflict. Low conflict results when one behavioral response predominates. We propose that increased long-term response conflict biases the LC towards a tonic firing mode. Increased conflict necessarily follows changes in reward contingency. As the previously rewarded target no longer produces reward, there will be a relative increase in response ambiguity and hence conflict. This relationship between conflict and LC firing is analogous to other modeling work [11], which proposes that increased tonic firing reflects increased environmental uncertainty. As a final component to our model, we hypothesize that the OFC maintains an ongoing estimate in reward rate, and that this estimate of reward rate also influences LC firing mode. As reward rate increases, we assume that the OFC tends to bias the LC in favor of phasic firing to target stimuli. We have aimed to fix model parameters based on previous work using simpler networks. We use parameters derived primarily from a previous model of the LC by Gilzenrat and colleagues [7]. Integration of response conflict by the ACC and its influence on LC firing was borrowed from unpublished work by Gilzenrat and colleagues in which they fit human behavioral data in a diminishing utilities task. Given this approach, we interpret our observed improvement in model performance with combined NE and DA function as validation of a mechanism for automatically switching between exploitative and exploratory action selection. 2 G o- No- G o Task and Core Mod el We have modeled an experiment in which monkeys performed a target detection task [10]. In the task, monkeys were shown either a vertical bar or a horizontal bar and were required to make or omit a motor response appropriately. Initially, the vertical bar was the target stimulus and correctly responding was rewarded with a squirt of fruit juice (r=1 in the model). Responding to the non-target horizontal stimulus resulted in time out punishment (r=-.1; Figure 1A). No responses to either the target or non-target gave zero reward. After the monkeys had fully acquired the task, the experimenters periodically switched the reward contingency such that the previously rewarded stimulus (target) became the distractor, and vice versa. Following such reversals, LC neurons were observed to change from emitting phasic bursts of firing to the target, to tonic firing following the switch, and slowly back to phasic firing for the new target as the new response criteria was obtained [10]. Figure 1: Task and model design. (A) Responses were required for targets in order to obtain reward. Responses to distractors resulted in a minor punishment. No responses gave zero reward. (B) In the model, vertical and horizontal bar inputs (I1 and I 2 ) fed to integrator neurons (X1 and X2 ) which then drove response units (Y1 and Y2 ). Responses were made if Y 1 or Y2 crossed a threshold while input units were active. We have previously modeled this task [7,12] with a three-layer connectionist network in which two input units, I1 and I 2 , corresponding to the vertical and horizontal bars, drive two mutually inhibitory integrator units, X1 and X2 . The integrator units subsequently feed two response units, Y1 and Y2 (Figure 1B). Responses are made whenever output from Y1 or Y2 crosses a threshold level of activity, θ. Relatively weak cross connections from each input unit to the opposite integrator unit (I1 to X2 and I 2 to X1 ) are intended to model stimulus similarity. Both the integrator and response units were modeled as noisy, leaky accumulators: ˙ X i =</p><p>3 0.18302837 <a title="91-tfidf-3" href="./nips-2005-On_Local_Rewards_and_Scaling_Distributed_Reinforcement_Learning.html">145 nips-2005-On Local Rewards and Scaling Distributed Reinforcement Learning</a></p>
<p>Author: Drew Bagnell, Andrew Y. Ng</p><p>Abstract: We consider the scaling of the number of examples necessary to achieve good performance in distributed, cooperative, multi-agent reinforcement learning, as a function of the the number of agents n. We prove a worstcase lower bound showing that algorithms that rely solely on a global reward signal to learn policies confront a fundamental limit: They require a number of real-world examples that scales roughly linearly in the number of agents. For settings of interest with a very large number of agents, this is impractical. We demonstrate, however, that there is a class of algorithms that, by taking advantage of local reward signals in large distributed Markov Decision Processes, are able to ensure good performance with a number of samples that scales as O(log n). This makes them applicable even in settings with a very large number of agents n. 1</p><p>4 0.14394183 <a title="91-tfidf-4" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>Author: Doron Blatt, Alfred O. Hero</p><p>Abstract: This paper proposes an algorithm to convert a T -stage stochastic decision problem with a continuous state space to a sequence of supervised learning problems. The optimization problem associated with the trajectory tree and random trajectory methods of Kearns, Mansour, and Ng, 2000, is solved using the Gauss-Seidel method. The algorithm breaks a multistage reinforcement learning problem into a sequence of single-stage reinforcement learning subproblems, each of which is solved via an exact reduction to a weighted-classiﬁcation problem that can be solved using off-the-self methods. Thus the algorithm converts a reinforcement learning problem into simpler supervised learning subproblems. It is shown that the method converges in a ﬁnite number of steps to a solution that cannot be further improved by componentwise optimization. The implication of the proposed algorithm is that a plethora of classiﬁcation methods can be applied to ﬁnd policies in the reinforcement learning problem. 1</p><p>5 0.11973672 <a title="91-tfidf-5" href="./nips-2005-Policy-Gradient_Methods_for_Planning.html">153 nips-2005-Policy-Gradient Methods for Planning</a></p>
<p>Author: Douglas Aberdeen</p><p>Abstract: Probabilistic temporal planning attempts to ﬁnd good policies for acting in domains with concurrent durative tasks, multiple uncertain outcomes, and limited resources. These domains are typically modelled as Markov decision problems and solved using dynamic programming methods. This paper demonstrates the application of reinforcement learning — in the form of a policy-gradient method — to these domains. Our emphasis is large domains that are infeasible for dynamic programming. Our approach is to construct simple policies, or agents, for each planning task. The result is a general probabilistic temporal planner, named the Factored Policy-Gradient Planner (FPG-Planner), which can handle hundreds of tasks, optimising for probability of success, duration, and resource use. 1</p><p>6 0.1153698 <a title="91-tfidf-6" href="./nips-2005-Norepinephrine_and_Neural_Interrupts.html">141 nips-2005-Norepinephrine and Neural Interrupts</a></p>
<p>7 0.081895486 <a title="91-tfidf-7" href="./nips-2005-Goal-Based_Imitation_as_Probabilistic_Inference_over_Graphical_Models.html">87 nips-2005-Goal-Based Imitation as Probabilistic Inference over Graphical Models</a></p>
<p>8 0.075270042 <a title="91-tfidf-8" href="./nips-2005-Temporal_Abstraction_in_Temporal-difference_Networks.html">187 nips-2005-Temporal Abstraction in Temporal-difference Networks</a></p>
<p>9 0.074576698 <a title="91-tfidf-9" href="./nips-2005-Bayesian_models_of_human_action_understanding.html">36 nips-2005-Bayesian models of human action understanding</a></p>
<p>10 0.068675645 <a title="91-tfidf-10" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>11 0.067330927 <a title="91-tfidf-11" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>12 0.065946668 <a title="91-tfidf-12" href="./nips-2005-Learning_to_Control_an_Octopus_Arm_with_Gaussian_Process_Temporal_Difference_Methods.html">119 nips-2005-Learning to Control an Octopus Arm with Gaussian Process Temporal Difference Methods</a></p>
<p>13 0.060175985 <a title="91-tfidf-13" href="./nips-2005-Response_Analysis_of_Neuronal_Population_with_Synaptic_Depression.html">165 nips-2005-Response Analysis of Neuronal Population with Synaptic Depression</a></p>
<p>14 0.054615673 <a title="91-tfidf-14" href="./nips-2005-Modeling_Neuronal_Interactivity_using_Dynamic_Bayesian_Networks.html">130 nips-2005-Modeling Neuronal Interactivity using Dynamic Bayesian Networks</a></p>
<p>15 0.052718148 <a title="91-tfidf-15" href="./nips-2005-TD%280%29_Leads_to_Better_Policies_than_Approximate_Value_Iteration.html">186 nips-2005-TD(0) Leads to Better Policies than Approximate Value Iteration</a></p>
<p>16 0.045620881 <a title="91-tfidf-16" href="./nips-2005-Phase_Synchrony_Rate_for_the_Recognition_of_Motor_Imagery_in_Brain-Computer_Interface.html">152 nips-2005-Phase Synchrony Rate for the Recognition of Motor Imagery in Brain-Computer Interface</a></p>
<p>17 0.042907082 <a title="91-tfidf-17" href="./nips-2005-Integrate-and-Fire_models_with_adaptation_are_good_enough.html">99 nips-2005-Integrate-and-Fire models with adaptation are good enough</a></p>
<p>18 0.042498004 <a title="91-tfidf-18" href="./nips-2005-Laplacian_Score_for_Feature_Selection.html">104 nips-2005-Laplacian Score for Feature Selection</a></p>
<p>19 0.041316275 <a title="91-tfidf-19" href="./nips-2005-Identifying_Distributed_Object_Representations_in_Human_Extrastriate_Visual_Cortex.html">94 nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</a></p>
<p>20 0.040552836 <a title="91-tfidf-20" href="./nips-2005-Oblivious_Equilibrium%3A_A_Mean_Field_Approximation_for_Large-Scale_Dynamic_Games.html">142 nips-2005-Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.139), (1, -0.084), (2, 0.259), (3, -0.022), (4, -0.03), (5, 0.072), (6, 0.01), (7, -0.114), (8, -0.04), (9, 0.045), (10, 0.004), (11, -0.134), (12, 0.028), (13, -0.076), (14, -0.028), (15, 0.02), (16, 0.071), (17, -0.019), (18, 0.029), (19, -0.224), (20, 0.059), (21, 0.164), (22, 0.064), (23, 0.027), (24, -0.038), (25, 0.061), (26, -0.169), (27, 0.0), (28, -0.07), (29, -0.035), (30, -0.044), (31, -0.128), (32, -0.202), (33, -0.125), (34, -0.101), (35, 0.065), (36, -0.068), (37, 0.015), (38, -0.047), (39, 0.026), (40, 0.042), (41, -0.032), (42, -0.091), (43, 0.051), (44, -0.162), (45, -0.038), (46, 0.049), (47, -0.107), (48, -0.037), (49, -0.121)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9714365 <a title="91-lsi-1" href="./nips-2005-How_fast_to_work%3A_Response_vigor%2C_motivation_and_tonic_dopamine.html">91 nips-2005-How fast to work: Response vigor, motivation and tonic dopamine</a></p>
<p>Author: Yael Niv, Nathaniel D. Daw, Peter Dayan</p><p>Abstract: Reinforcement learning models have long promised to unify computational, psychological and neural accounts of appetitively conditioned behavior. However, the bulk of data on animal conditioning comes from free-operant experiments measuring how fast animals will work for reinforcement. Existing reinforcement learning (RL) models are silent about these tasks, because they lack any notion of vigor. They thus fail to address the simple observation that hungrier animals will work harder for food, as well as stranger facts such as their sometimes greater productivity even when working for irrelevant outcomes such as water. Here, we develop an RL framework for free-operant behavior, suggesting that subjects choose how vigorously to perform selected actions by optimally balancing the costs and beneﬁts of quick responding. Motivational states such as hunger shift these factors, skewing the tradeoff. This accounts normatively for the effects of motivation on response rates, as well as many other classic ﬁndings. Finally, we suggest that tonic levels of dopamine may be involved in the computation linking motivational state to optimal responding, thereby explaining the complex vigor-related effects of pharmacological manipulation of dopamine. 1</p><p>2 0.85143995 <a title="91-lsi-2" href="./nips-2005-An_exploration-exploitation_model_based_on_norepinepherine_and_dopamine_activity.html">26 nips-2005-An exploration-exploitation model based on norepinepherine and dopamine activity</a></p>
<p>Author: Samuel M. McClure, Mark S. Gilzenrat, Jonathan D. Cohen</p><p>Abstract: We propose a model by which dopamine (DA) and norepinepherine (NE) combine to alternate behavior between relatively exploratory and exploitative modes. The model is developed for a target detection task for which there is extant single neuron recording data available from locus coeruleus (LC) NE neurons. An exploration-exploitation trade-off is elicited by regularly switching which of the two stimuli are rewarded. DA functions within the model to change synaptic weights according to a reinforcement learning algorithm. Exploration is mediated by the state of LC firing, with higher tonic and lower phasic activity producing greater response variability. The opposite state of LC function, with lower baseline firing rate and greater phasic responses, favors exploitative behavior. Changes in LC firing mode result from combined measures of response conflict and reward rate, where response conflict is monitored using models of anterior cingulate cortex (ACC). Increased long-term response conflict and decreased reward rate, which occurs following reward contingency switch, favors the higher tonic state of LC function and NE release. This increases exploration, and facilitates discovery of the new target. 1 In t rod u ct i on A central problem in reinforcement learning is determining how to adaptively move between exploitative and exploratory behaviors in changing environments. We propose a set of neurophysiologic mechanisms whose interaction may mediate this behavioral shift. Empirical work on the midbrain dopamine (DA) system has suggested that this system is particularly well suited for guiding exploitative behaviors. This hypothesis has been reified by a number of studies showing that a temporal difference (TD) learning algorithm accounts for activity in these neurons in a wide variety of behavioral tasks [1,2]. DA release is believed to encode a reward prediction error signal that acts to change synaptic weights relevant for producing behaviors [3]. Through learning, this allows neural pathways to predict future expected reward through the relative strength of their synaptic connections [1]. Decision-making procedures based on these value estimates are necessarily greedy. Including reward bonuses for exploratory choices supports non-greedy actions [4] and accounts for additional data derived from DA neurons [5]. We show that combining a DA learning algorithm with models of response conflict detection [6] and NE function [7] produces an effective annealing procedure for alternating between exploration and exploitation. NE neurons within the LC alternate between two firing modes [8]. In the first mode, known as the phasic mode, NE neurons fire at a low baseline rate but have relatively robust phasic responses to behaviorally salient stimuli. The second mode, called the tonic mode, is associated with a higher baseline firing and absent or attenuated phasic responses. The effects of NE on efferent areas are modulatory in nature, and are well captured as a change in the gain of efferent inputs so that neuronal responses are potentiated in the presence of NE [9]. Thus, in phasic mode, the LC provides transient facilitation in processing, time-locked to the presence of behaviorally salient information in motor or decision areas. Conversely, in tonic mode, higher overall LC discharge rate increases gain generally and hence increases the probability of arbitrary responding. Consistent with this account, for periods when NE neurons are in the phasic mode, monkey performance is nearly perfect. However, when NE neurons are in the tonic mode, performance is more erratic, with increased response times and error rate [8]. These findings have led to a recent characterization of the LC as a dynamic temporal filter, adjusting the system's relative responsivity to salient and irrelevant information [8]. In this way, the LC is ideally positioned to mediate the shift between exploitative and exploratory behavior. The parameters that underlie changes in LC firing mode remain largely unexplored. Based on data from a target detection task by Aston-Jones and colleagues [10], we propose that LC firing mode is determined in part by measures of response conflict and reward rate as calculated by the ACC and OFC, respectively [8]. Together, the ACC and OFC are the principle sources of cortical input to the LC [8]. Activity in the ACC is known, largely through human neuroimaging experiments, to change in accord with response conflict [6]. In brief, relatively equal activity in competing behavioral responses (reflecting uncertainty) produces high conflict. Low conflict results when one behavioral response predominates. We propose that increased long-term response conflict biases the LC towards a tonic firing mode. Increased conflict necessarily follows changes in reward contingency. As the previously rewarded target no longer produces reward, there will be a relative increase in response ambiguity and hence conflict. This relationship between conflict and LC firing is analogous to other modeling work [11], which proposes that increased tonic firing reflects increased environmental uncertainty. As a final component to our model, we hypothesize that the OFC maintains an ongoing estimate in reward rate, and that this estimate of reward rate also influences LC firing mode. As reward rate increases, we assume that the OFC tends to bias the LC in favor of phasic firing to target stimuli. We have aimed to fix model parameters based on previous work using simpler networks. We use parameters derived primarily from a previous model of the LC by Gilzenrat and colleagues [7]. Integration of response conflict by the ACC and its influence on LC firing was borrowed from unpublished work by Gilzenrat and colleagues in which they fit human behavioral data in a diminishing utilities task. Given this approach, we interpret our observed improvement in model performance with combined NE and DA function as validation of a mechanism for automatically switching between exploitative and exploratory action selection. 2 G o- No- G o Task and Core Mod el We have modeled an experiment in which monkeys performed a target detection task [10]. In the task, monkeys were shown either a vertical bar or a horizontal bar and were required to make or omit a motor response appropriately. Initially, the vertical bar was the target stimulus and correctly responding was rewarded with a squirt of fruit juice (r=1 in the model). Responding to the non-target horizontal stimulus resulted in time out punishment (r=-.1; Figure 1A). No responses to either the target or non-target gave zero reward. After the monkeys had fully acquired the task, the experimenters periodically switched the reward contingency such that the previously rewarded stimulus (target) became the distractor, and vice versa. Following such reversals, LC neurons were observed to change from emitting phasic bursts of firing to the target, to tonic firing following the switch, and slowly back to phasic firing for the new target as the new response criteria was obtained [10]. Figure 1: Task and model design. (A) Responses were required for targets in order to obtain reward. Responses to distractors resulted in a minor punishment. No responses gave zero reward. (B) In the model, vertical and horizontal bar inputs (I1 and I 2 ) fed to integrator neurons (X1 and X2 ) which then drove response units (Y1 and Y2 ). Responses were made if Y 1 or Y2 crossed a threshold while input units were active. We have previously modeled this task [7,12] with a three-layer connectionist network in which two input units, I1 and I 2 , corresponding to the vertical and horizontal bars, drive two mutually inhibitory integrator units, X1 and X2 . The integrator units subsequently feed two response units, Y1 and Y2 (Figure 1B). Responses are made whenever output from Y1 or Y2 crosses a threshold level of activity, θ. Relatively weak cross connections from each input unit to the opposite integrator unit (I1 to X2 and I 2 to X1 ) are intended to model stimulus similarity. Both the integrator and response units were modeled as noisy, leaky accumulators: ˙ X i =</p><p>3 0.54942602 <a title="91-lsi-3" href="./nips-2005-On_Local_Rewards_and_Scaling_Distributed_Reinforcement_Learning.html">145 nips-2005-On Local Rewards and Scaling Distributed Reinforcement Learning</a></p>
<p>Author: Drew Bagnell, Andrew Y. Ng</p><p>Abstract: We consider the scaling of the number of examples necessary to achieve good performance in distributed, cooperative, multi-agent reinforcement learning, as a function of the the number of agents n. We prove a worstcase lower bound showing that algorithms that rely solely on a global reward signal to learn policies confront a fundamental limit: They require a number of real-world examples that scales roughly linearly in the number of agents. For settings of interest with a very large number of agents, this is impractical. We demonstrate, however, that there is a class of algorithms that, by taking advantage of local reward signals in large distributed Markov Decision Processes, are able to ensure good performance with a number of samples that scales as O(log n). This makes them applicable even in settings with a very large number of agents n. 1</p><p>4 0.5441643 <a title="91-lsi-4" href="./nips-2005-Norepinephrine_and_Neural_Interrupts.html">141 nips-2005-Norepinephrine and Neural Interrupts</a></p>
<p>Author: Peter Dayan, Angela J. Yu</p><p>Abstract: Experimental data indicate that norepinephrine is critically involved in aspects of vigilance and attention. Previously, we considered the function of this neuromodulatory system on a time scale of minutes and longer, and suggested that it signals global uncertainty arising from gross changes in environmental contingencies. However, norepinephrine is also known to be activated phasically by familiar stimuli in welllearned tasks. Here, we extend our uncertainty-based treatment of norepinephrine to this phasic mode, proposing that it is involved in the detection and reaction to state uncertainty within a task. This role of norepinephrine can be understood through the metaphor of neural interrupts. 1</p><p>5 0.40935975 <a title="91-lsi-5" href="./nips-2005-Policy-Gradient_Methods_for_Planning.html">153 nips-2005-Policy-Gradient Methods for Planning</a></p>
<p>Author: Douglas Aberdeen</p><p>Abstract: Probabilistic temporal planning attempts to ﬁnd good policies for acting in domains with concurrent durative tasks, multiple uncertain outcomes, and limited resources. These domains are typically modelled as Markov decision problems and solved using dynamic programming methods. This paper demonstrates the application of reinforcement learning — in the form of a policy-gradient method — to these domains. Our emphasis is large domains that are infeasible for dynamic programming. Our approach is to construct simple policies, or agents, for each planning task. The result is a general probabilistic temporal planner, named the Factored Policy-Gradient Planner (FPG-Planner), which can handle hundreds of tasks, optimising for probability of success, duration, and resource use. 1</p><p>6 0.3861509 <a title="91-lsi-6" href="./nips-2005-Response_Analysis_of_Neuronal_Population_with_Synaptic_Depression.html">165 nips-2005-Response Analysis of Neuronal Population with Synaptic Depression</a></p>
<p>7 0.37071186 <a title="91-lsi-7" href="./nips-2005-Learning_to_Control_an_Octopus_Arm_with_Gaussian_Process_Temporal_Difference_Methods.html">119 nips-2005-Learning to Control an Octopus Arm with Gaussian Process Temporal Difference Methods</a></p>
<p>8 0.33507386 <a title="91-lsi-8" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>9 0.32511792 <a title="91-lsi-9" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>10 0.29963827 <a title="91-lsi-10" href="./nips-2005-Goal-Based_Imitation_as_Probabilistic_Inference_over_Graphical_Models.html">87 nips-2005-Goal-Based Imitation as Probabilistic Inference over Graphical Models</a></p>
<p>11 0.27687514 <a title="91-lsi-11" href="./nips-2005-Identifying_Distributed_Object_Representations_in_Human_Extrastriate_Visual_Cortex.html">94 nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</a></p>
<p>12 0.22576632 <a title="91-lsi-12" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>13 0.21851137 <a title="91-lsi-13" href="./nips-2005-TD%280%29_Leads_to_Better_Policies_than_Approximate_Value_Iteration.html">186 nips-2005-TD(0) Leads to Better Policies than Approximate Value Iteration</a></p>
<p>14 0.21481797 <a title="91-lsi-14" href="./nips-2005-Modeling_Neuronal_Interactivity_using_Dynamic_Bayesian_Networks.html">130 nips-2005-Modeling Neuronal Interactivity using Dynamic Bayesian Networks</a></p>
<p>15 0.21391845 <a title="91-lsi-15" href="./nips-2005-Sensory_Adaptation_within_a_Bayesian_Framework_for_Perception.html">173 nips-2005-Sensory Adaptation within a Bayesian Framework for Perception</a></p>
<p>16 0.20914608 <a title="91-lsi-16" href="./nips-2005-Bayesian_models_of_human_action_understanding.html">36 nips-2005-Bayesian models of human action understanding</a></p>
<p>17 0.19669427 <a title="91-lsi-17" href="./nips-2005-Oblivious_Equilibrium%3A_A_Mean_Field_Approximation_for_Large-Scale_Dynamic_Games.html">142 nips-2005-Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games</a></p>
<p>18 0.19303462 <a title="91-lsi-18" href="./nips-2005-Measuring_Shared_Information_and_Coordinated_Activity_in_Neuronal_Networks.html">124 nips-2005-Measuring Shared Information and Coordinated Activity in Neuronal Networks</a></p>
<p>19 0.19247243 <a title="91-lsi-19" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>20 0.19221584 <a title="91-lsi-20" href="./nips-2005-Dynamical_Synapses_Give_Rise_to_a_Power-Law_Distribution_of_Neuronal_Avalanches.html">61 nips-2005-Dynamical Synapses Give Rise to a Power-Law Distribution of Neuronal Avalanches</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.03), (10, 0.052), (11, 0.01), (27, 0.046), (31, 0.07), (34, 0.032), (36, 0.361), (39, 0.03), (55, 0.039), (57, 0.02), (60, 0.025), (65, 0.042), (69, 0.045), (73, 0.025), (88, 0.043), (91, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84197396 <a title="91-lda-1" href="./nips-2005-How_fast_to_work%3A_Response_vigor%2C_motivation_and_tonic_dopamine.html">91 nips-2005-How fast to work: Response vigor, motivation and tonic dopamine</a></p>
<p>Author: Yael Niv, Nathaniel D. Daw, Peter Dayan</p><p>Abstract: Reinforcement learning models have long promised to unify computational, psychological and neural accounts of appetitively conditioned behavior. However, the bulk of data on animal conditioning comes from free-operant experiments measuring how fast animals will work for reinforcement. Existing reinforcement learning (RL) models are silent about these tasks, because they lack any notion of vigor. They thus fail to address the simple observation that hungrier animals will work harder for food, as well as stranger facts such as their sometimes greater productivity even when working for irrelevant outcomes such as water. Here, we develop an RL framework for free-operant behavior, suggesting that subjects choose how vigorously to perform selected actions by optimally balancing the costs and beneﬁts of quick responding. Motivational states such as hunger shift these factors, skewing the tradeoff. This accounts normatively for the effects of motivation on response rates, as well as many other classic ﬁndings. Finally, we suggest that tonic levels of dopamine may be involved in the computation linking motivational state to optimal responding, thereby explaining the complex vigor-related effects of pharmacological manipulation of dopamine. 1</p><p>2 0.4493807 <a title="91-lda-2" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>Author: Amir Navot, Lavi Shpigelman, Naftali Tishby, Eilon Vaadia</p><p>Abstract: We present a non-linear, simple, yet effective, feature subset selection method for regression and use it in analyzing cortical neural activity. Our algorithm involves a feature-weighted version of the k-nearest-neighbor algorithm. It is able to capture complex dependency of the target function on its input and makes use of the leave-one-out error as a natural regularization. We explain the characteristics of our algorithm on synthetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey. By applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data.</p><p>3 0.3350862 <a title="91-lda-3" href="./nips-2005-Value_Function_Approximation_with_Diffusion_Wavelets_and_Laplacian_Eigenfunctions.html">199 nips-2005-Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions</a></p>
<p>Author: Sridhar Mahadevan, Mauro Maggioni</p><p>Abstract: We investigate the problem of automatically constructing efﬁcient representations or basis functions for approximating value functions based on analyzing the structure and topology of the state space. In particular, two novel approaches to value function approximation are explored based on automatically constructing basis functions on state spaces that can be represented as graphs or manifolds: one approach uses the eigenfunctions of the Laplacian, in effect performing a global Fourier analysis on the graph; the second approach is based on diffusion wavelets, which generalize classical wavelets to graphs using multiscale dilations induced by powers of a diffusion operator or random walk on the graph. Together, these approaches form the foundation of a new generation of methods for solving large Markov decision processes, in which the underlying representation and policies are simultaneously learned.</p><p>4 0.32572269 <a title="91-lda-4" href="./nips-2005-Norepinephrine_and_Neural_Interrupts.html">141 nips-2005-Norepinephrine and Neural Interrupts</a></p>
<p>Author: Peter Dayan, Angela J. Yu</p><p>Abstract: Experimental data indicate that norepinephrine is critically involved in aspects of vigilance and attention. Previously, we considered the function of this neuromodulatory system on a time scale of minutes and longer, and suggested that it signals global uncertainty arising from gross changes in environmental contingencies. However, norepinephrine is also known to be activated phasically by familiar stimuli in welllearned tasks. Here, we extend our uncertainty-based treatment of norepinephrine to this phasic mode, proposing that it is involved in the detection and reaction to state uncertainty within a task. This role of norepinephrine can be understood through the metaphor of neural interrupts. 1</p><p>5 0.31762648 <a title="91-lda-5" href="./nips-2005-Policy-Gradient_Methods_for_Planning.html">153 nips-2005-Policy-Gradient Methods for Planning</a></p>
<p>Author: Douglas Aberdeen</p><p>Abstract: Probabilistic temporal planning attempts to ﬁnd good policies for acting in domains with concurrent durative tasks, multiple uncertain outcomes, and limited resources. These domains are typically modelled as Markov decision problems and solved using dynamic programming methods. This paper demonstrates the application of reinforcement learning — in the form of a policy-gradient method — to these domains. Our emphasis is large domains that are infeasible for dynamic programming. Our approach is to construct simple policies, or agents, for each planning task. The result is a general probabilistic temporal planner, named the Factored Policy-Gradient Planner (FPG-Planner), which can handle hundreds of tasks, optimising for probability of success, duration, and resource use. 1</p><p>6 0.31612036 <a title="91-lda-6" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>7 0.31272227 <a title="91-lda-7" href="./nips-2005-Spiking_Inputs_to_a_Winner-take-all_Network.html">181 nips-2005-Spiking Inputs to a Winner-take-all Network</a></p>
<p>8 0.30986205 <a title="91-lda-8" href="./nips-2005-Phase_Synchrony_Rate_for_the_Recognition_of_Motor_Imagery_in_Brain-Computer_Interface.html">152 nips-2005-Phase Synchrony Rate for the Recognition of Motor Imagery in Brain-Computer Interface</a></p>
<p>9 0.3091605 <a title="91-lda-9" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>10 0.30233857 <a title="91-lda-10" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>11 0.30221674 <a title="91-lda-11" href="./nips-2005-Measuring_Shared_Information_and_Coordinated_Activity_in_Neuronal_Networks.html">124 nips-2005-Measuring Shared Information and Coordinated Activity in Neuronal Networks</a></p>
<p>12 0.30107176 <a title="91-lda-12" href="./nips-2005-Oblivious_Equilibrium%3A_A_Mean_Field_Approximation_for_Large-Scale_Dynamic_Games.html">142 nips-2005-Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games</a></p>
<p>13 0.30081725 <a title="91-lda-13" href="./nips-2005-Bayesian_models_of_human_action_understanding.html">36 nips-2005-Bayesian models of human action understanding</a></p>
<p>14 0.29868776 <a title="91-lda-14" href="./nips-2005-On_Local_Rewards_and_Scaling_Distributed_Reinforcement_Learning.html">145 nips-2005-On Local Rewards and Scaling Distributed Reinforcement Learning</a></p>
<p>15 0.29772806 <a title="91-lda-15" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>16 0.29715937 <a title="91-lda-16" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>17 0.29699862 <a title="91-lda-17" href="./nips-2005-Sensory_Adaptation_within_a_Bayesian_Framework_for_Perception.html">173 nips-2005-Sensory Adaptation within a Bayesian Framework for Perception</a></p>
<p>18 0.29699653 <a title="91-lda-18" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>19 0.29669693 <a title="91-lda-19" href="./nips-2005-An_exploration-exploitation_model_based_on_norepinepherine_and_dopamine_activity.html">26 nips-2005-An exploration-exploitation model based on norepinepherine and dopamine activity</a></p>
<p>20 0.2962079 <a title="91-lda-20" href="./nips-2005-Temporal_Abstraction_in_Temporal-difference_Networks.html">187 nips-2005-Temporal Abstraction in Temporal-difference Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
