<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>44 nips-2005-Computing the Solution Path for the Regularized Support Vector Regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-44" href="#">nips2005-44</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>44 nips-2005-Computing the Solution Path for the Regularized Support Vector Regression</h1>
<br/><p>Source: <a title="nips-2005-44-pdf" href="http://papers.nips.cc/paper/2856-computing-the-solution-path-for-the-regularized-support-vector-regression.pdf">pdf</a></p><p>Author: Lacey Gunter, Ji Zhu</p><p>Abstract: In this paper we derive an algorithm that computes the entire solution path of the support vector regression, with essentially the same computational cost as ﬁtting one SVR model. We also propose an unbiased estimate for the degrees of freedom of the SVR model, which allows convenient selection of the regularization parameter. 1</p><p>Reference: <a title="nips-2005-44-reference" href="../nips2005_reference/nips-2005-Computing_the_Solution_Path_for_the_Regularized_Support_Vector_Regression_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract In this paper we derive an algorithm that computes the entire solution path of the support vector regression, with essentially the same computational cost as ﬁtting one SVR model. [sent-3, score-0.496]
</p><p>2 We also propose an unbiased estimate for the degrees of freedom of the SVR model, which allows convenient selection of the regularization parameter. [sent-4, score-0.606]
</p><p>3 1  Introduction  The support vector regression (SVR) is a popular tool for function estimation problems, and it has been widely used on many real applications in the past decade, for example, time series prediction [1], signal processing [2] and neural decoding [3]. [sent-5, score-0.161]
</p><p>4 In this paper, we focus on the regularization parameter of the SVR, and propose an eﬃcient algorithm that computes the entire regularized solution path; we also propose an unbiased estimate for the degrees of freedom of the SVR, which allows convenient selection of the regularization parameter. [sent-6, score-0.958]
</p><p>5 , (xn , yn ), where the input xi ∈ Rp and the output yi ∈ R. [sent-10, score-0.323]
</p><p>6 Notice that it has two non-diﬀerentiable points at ± . [sent-13, score-0.051]
</p><p>7 The regularization parameter λ controls the trade-oﬀ between the -insensitive loss and the complexity of the ﬁtted model. [sent-14, score-0.148]
</p><p>8 0  Elbow L  −3  −2  −1  0  1  2  3  y−f  Figure 1: The -insensitive loss function. [sent-22, score-0.048]
</p><p>9 Notice that we write f (x) in a way that involves λ explicitly, and we will see later that θi ∈ [−1, 1]. [sent-24, score-0.055]
</p><p>10 Both (1) and (2) can be transformed into a quadratic programming problem, hence most commercially available packages can be used to solve the SVR. [sent-25, score-0.08]
</p><p>11 In the past years, many speciﬁc algorithms for the SVR have also been developed, for example, interior point algorithms [4-5], subset selection algorithms [6–7], and sequential minimal optimization [4, 8–9]. [sent-26, score-0.109]
</p><p>12 All these algorithms solve the SVR for a pre-ﬁxed regularization parameter λ, and it is well known that an appropriate value of λ is crucial for achieving small prediction error of the SVR. [sent-27, score-0.125]
</p><p>13 In this paper, we show that the solution θ(λ) is piecewise linear as a function of λ, which allows us to derive an eﬃcient algorithm that computes the exact entire solution path {θ(λ), 0 ≤ λ ≤ ∞}. [sent-28, score-0.503]
</p><p>14 We acknowledge that this work was inspired by one of the authors’ earlier work on the SVM setting [10]. [sent-29, score-0.029]
</p><p>15 Before delving into the technical details, we illustrate the concept of piecewise linearity of the solution path with a simple example. [sent-30, score-0.305]
</p><p>16 192 ) πx We use the SVR with a 1-dimensional spline kernel y=  K(x, x ) = 1 + k1 (x)k1 (x ) + k2 (x)k2 (x ) − k4 (|x − x |)  (3)  where k1 (·) = · − 1/2, k2 = − 1/12)/2, k4 = − + 7/240)/24. [sent-32, score-0.15]
</p><p>17 Figure 2 shows a subset of the piecewise linear solution path θ(λ) as a function of λ. [sent-33, score-0.305]
</p><p>18 2 (k1  4 (k1  2 k1 /2  In section 2, we describe the algorithm that computes the entire solution path of the SVR. [sent-34, score-0.375]
</p><p>19 In section 3, we propose an unbiased estimate for the degrees of freedom of the SVR, which can be used to select the regularization parameter λ. [sent-35, score-0.528]
</p><p>20 In section 4, we present numerical results on simulation data. [sent-36, score-0.062]
</p><p>21 0 1  2  3  4  λ  5  Figure 2: A subset of the solution path θ(λ) as a function of λ. [sent-43, score-0.232]
</p><p>22 2  Algorithm  For simplicity in notation, we describe the problem setup using the linear SVR, and the algorithm using the kernel SVR. [sent-44, score-0.12]
</p><p>23 1  Problem Setup  The linear -SVR (1) can be re-written in an equivalent way: n  min β0 ,β  subject to  (ξi + δi ) + i=1  λ T β β 2  −(δi + ) ≤ yi − f (xi ) ≤ (ξi + ),  ξi , δi ≥ 0;  T  f (xi ) = β0 + β xi , i = 1, . [sent-46, score-0.323]
</p><p>24 For points in R, L and C, the values of αi and γi are known; therefore, the algorithm will focus on points resting at the two elbows ER and EL . [sent-54, score-0.28]
</p><p>25 For lack of space, we focus on the case that all the values of yi are distinct, and furthermore, the initial sets ER and EL have at most one point combined (which is the usual situation). [sent-58, score-0.266]
</p><p>26 Since β0 is not unique, we can focus on one particular solution path, for example, by always setting β0 equal to one of its boundary values (thus keeping one point at an elbow). [sent-60, score-0.084]
</p><p>27 As λ decreases, the range of β0 shrinks toward zero and reaches zero when we have two points at the elbows, and the algorithm proceeds from there. [sent-61, score-0.131]
</p><p>28 3  The Path  The formalized setup above can be easily modiﬁed to accommodate non-linear kernels; in fact, θi in (2) is equal to αi − γi . [sent-63, score-0.035]
</p><p>29 For the remaining portion of the algorithm we will use the kernel notation. [sent-64, score-0.085]
</p><p>30 The algorithm focuses on the sets of points ER and EL . [sent-65, score-0.119]
</p><p>31 These points have either f (xi ) = yi − with αi ∈ [0, 1], or f (xi ) = yi + with γi ∈ [0, 1]. [sent-66, score-0.397]
</p><p>32 As we follow the path we will examine these sets until one or both of them change, at which point we will say an event has occurred. [sent-67, score-0.32]
</p><p>33 The initial event, for which two points must enter the elbow(s) A point from R has just entered ER , with αi initially 1 A point from L has just entered EL , with γi initially 1 A point from C has just entered ER , with αi initially 0  5. [sent-72, score-0.606]
</p><p>34 A point from C has just entered EL , with γi initially 0 6. [sent-73, score-0.185]
</p><p>35 One or more points in ER and/or EL have just left the elbow(s) to join either R, L, or C, with αi and γi initially 0 or 1 Until another event has occurred, all sets will remain the same. [sent-74, score-0.251]
</p><p>36 As a point passes through ER or EL , its respective αi or γi must change from 0 → 1 or 1 → 0. [sent-75, score-0.029]
</p><p>37 Relying on the fact that f (xi ) = yi − or f (xi ) = yi + for all points in ER or EL respectively, we can calculate αi and γi for these points. [sent-76, score-0.397]
</p><p>38 We use the subscript to index the sets above immediately after the th event has occurred, and let αi , γi , β0 and λ be the parameter values immediately after the th event. [sent-77, score-0.114]
</p><p>39 So we can write: αi  =  αi + (λ − λ )bi  ∀i ∈ ER  (12)  γj  =  γj + (λ − λ )bj  ∀j ∈ EL  (13)  β0,λ  =  β0,λ + (λ − λ )b0  (14)  λ f (x) − h (x) + h (x) λ  (15)  f (x) =  where (bi , bj , b0 ) is the solution when λ − λ is equal to 1, and bi K(x, xi ) −  h (x) = i∈ER  bj K(x, xj ) + b0 . [sent-83, score-0.404]
</p><p>40 j∈EL  Given λ , equations (12), (13) and (15) allow us to compute the λ at which the next event will occur, λ +1 . [sent-84, score-0.073]
</p><p>41 This will be the largest λ less than λ , such that either αi for i ∈ ER reaches 0 or 1, or γj for j ∈ EL reaches 0 or 1, or one of the points in R, L or C reaches an elbow. [sent-85, score-0.21]
</p><p>42 We terminate the algorithm either when the sets R and L become empty, or when λ has become suﬃciently close to zero. [sent-86, score-0.068]
</p><p>43 In the later case we must have f − h suﬃciently small as well. [sent-87, score-0.026]
</p><p>44 4  Computational cost  The major computational cost for updating the solutions at any event involves two things: solving the system of (nR +nL ) linear equations, and computing h (x). [sent-89, score-0.157]
</p><p>45 The former takes O((nR + nL )2 ) calculations by using inverse updating and downdating since the elbow sets usually diﬀer by only one point between consecutive events, and the latter requires O(n(nR + nL )) computations. [sent-90, score-0.306]
</p><p>46 According to our experience, the total number of steps taken by the algorithm is on average some small multiple of n. [sent-91, score-0.027]
</p><p>47 Letting m be the average size of ER ∪ EL , then the approximate computational cost of the algorithm is O cn2 m + nm2 , which is comparable to a single SVR ﬁtting algorithm that uses quadratic programming. [sent-92, score-0.113]
</p><p>48 3  The Degrees of Freedom  The degrees of freedom is an informative measure of the complexity of a ﬁtted model. [sent-93, score-0.246]
</p><p>49 In this section, we propose an unbiased estimate for the degrees of freedom of the SVR, which allows convenient selection of the regularization parameter λ. [sent-94, score-0.606]
</p><p>50 Since the usual goal of regression analysis is to minimize the predicted squared-error loss, we study the degrees of freedom using Stein’s unbiased risk estimation (SURE) theory [11]. [sent-95, score-0.442]
</p><p>51 Then the degrees of freedom of a ﬁtted model f (x) can be deﬁned as n  cov(f (xi ), yi )/σ 2  df(f ) = i=1  n  Stein showed that under mild conditions, i=1 ∂fi /∂yi is an unbiased estimate of n df(f ). [sent-97, score-0.548]
</p><p>52 It turns out that for the SVR model, for every ﬁxed λ, i=1 ∂fi /∂yi has an extremely simple formula: n  df ≡ i=1  ∂fi = |ER | + |EL | ∂yi  (16)  Therefore, |ER | + |EL | is a convenient unbiased estimate for the degrees of freedom of f (x). [sent-98, score-0.51]
</p><p>53 In applying (16) to select the regularization parameter λ, we plug it into the GCV criterion [12] for model selection: n i=1 (yi  − f (xi ))2  (n − df)2 The advantages of this criterion are that it does not assume a known σ 2 , and it avoids cross-validation, which is computationally intensive. [sent-100, score-0.261]
</p><p>54 In practice, we can ﬁrst use our eﬃcient algorithm to compute the entire solution path, then identify the appropriate value of λ that minimizes the GCV criterion. [sent-101, score-0.142]
</p><p>55 4  Numerical Results  To demonstrate our algorithm and the selection of λ using the GCV criterion, we show numerical results on simulated data. [sent-102, score-0.104]
</p><p>56 We consider both additive and multiplicative kernels using the 1-dimensional spline kernel (3), which are respectively p  K(x, x ) =  p  K(xj , xj )  and K(x, x ) =  j=1  K(xj , xj ) j=1  Simulations were based on the following four functions [13]: 1. [sent-103, score-0.311]
</p><p>57 We generated 300 training observations from each function along with 10,000 validation observations and 10,000 test observations. [sent-113, score-0.026]
</p><p>58 For the ﬁrst two simulations we used the additive 1-dimensional spline kernel and for the second two simulations the multiplicative 1-dimensional spline kernel. [sent-114, score-0.337]
</p><p>59 We then found the λ that minimized the GCV criterion. [sent-115, score-0.026]
</p><p>60 The validation set was used to select the gold standard λ which minimized the prediction MSE. [sent-116, score-0.1]
</p><p>61 Using these λ’s we calculated the prediction MSE with the test data for each criterion. [sent-117, score-0.025]
</p><p>62 After repeating this for 20 times, the average MSE and standard deviation for the MSE can be seen in Table 1, which indicates the GCV criterion performs closely to optimal. [sent-118, score-0.057]
</p><p>63 Table 1: Simulation results of λ selection for SVR f (x) MSE-Gold Standard MSE-GCV 1 0. [sent-119, score-0.042]
</p><p>64 0028)  5  Discussion  In this paper, we have proposed an eﬃcient algorithm that computes the entire regularization path of the SVR. [sent-131, score-0.42]
</p><p>65 We have also proposed the GCV criterion for selecting the best λ given the entire path. [sent-132, score-0.117]
</p><p>66 The GCV criterion seems to work suﬃciently well on the simulation data. [sent-133, score-0.084]
</p><p>67 However, we acknowledge that according to our experience on real data sets (not shown here due to lack of the space), the GCV criterion sometimes tends to over-ﬁt the model. [sent-134, score-0.153]
</p><p>68 Due to the diﬃculty of also selecting the best for the SVR, an alternate algorithm exists that automatically adjusts the value of , called the ν-SVR [4]. [sent-136, score-0.027]
</p><p>69 Using arguments similar to those for β0 in our above algorithm, one can show that is piecewise linear in 1/λ and its path can be calculated similarly. [sent-138, score-0.25]
</p><p>70 References [1] M¨ler K, Smola A, R¨tsch G, Sch¨lkopf B, Kohlmorgen J & Vapnik V (1997) Predicting u a o time series with support vector machines. [sent-141, score-0.092]
</p><p>71 [2] Vapnik V, Golowich S & Smola A (1997) Support vector method for function approximation, regression estimation, and signal processing. [sent-143, score-0.073]
</p><p>72 [4] Smola A & Sch¨lkopf B (2004) A tutorial on support vector regression. [sent-147, score-0.092]
</p><p>73 (1994) LOQO: An interior point code for quadratic programming. [sent-150, score-0.097]
</p><p>74 [6] Osuna E, Freund R & Girosi F (1997) An improved training algorithm for support vector machines. [sent-152, score-0.119]
</p><p>75 [8] Platt J (1999) Fast training of support vector machines using sequential minimal optimization. [sent-156, score-0.092]
</p><p>76 [9] Keerthi S, Shevade S, Bhattacharyya C & Murthy K (1999) Improvements to Platt’s SMO algorithm for SVM classiﬁer design. [sent-158, score-0.027]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('svr', 0.542), ('el', 0.386), ('gcv', 0.241), ('elbow', 0.21), ('er', 0.195), ('path', 0.177), ('yi', 0.173), ('elbows', 0.151), ('xi', 0.15), ('nl', 0.147), ('freedom', 0.145), ('nr', 0.14), ('unbiased', 0.129), ('degrees', 0.101), ('regularization', 0.1), ('entered', 0.096), ('spline', 0.092), ('stein', 0.079), ('df', 0.076), ('piecewise', 0.073), ('event', 0.073), ('xj', 0.066), ('mse', 0.063), ('support', 0.063), ('gunter', 0.06), ('rosset', 0.06), ('initially', 0.06), ('entire', 0.06), ('convenient', 0.059), ('kernel', 0.058), ('criterion', 0.057), ('computes', 0.056), ('solution', 0.055), ('reaches', 0.053), ('su', 0.051), ('points', 0.051), ('zhu', 0.05), ('fi', 0.048), ('loss', 0.048), ('bj', 0.047), ('regression', 0.044), ('tted', 0.043), ('michigan', 0.042), ('platt', 0.042), ('ann', 0.042), ('arbor', 0.042), ('selection', 0.042), ('sets', 0.041), ('smola', 0.04), ('notice', 0.039), ('bi', 0.039), ('interior', 0.038), ('vapnik', 0.035), ('setup', 0.035), ('svm', 0.035), ('numerical', 0.035), ('occurred', 0.034), ('simulations', 0.033), ('reproducing', 0.032), ('mi', 0.031), ('quadratic', 0.03), ('propose', 0.03), ('write', 0.029), ('cost', 0.029), ('acknowledge', 0.029), ('multiplicative', 0.029), ('vector', 0.029), ('point', 0.029), ('xm', 0.028), ('xk', 0.028), ('simulation', 0.027), ('algorithm', 0.027), ('sin', 0.027), ('lkopf', 0.026), ('updating', 0.026), ('experience', 0.026), ('commercially', 0.026), ('sinc', 0.026), ('categorized', 0.026), ('craven', 0.026), ('join', 0.026), ('kohlmorgen', 0.026), ('paz', 0.026), ('tan', 0.026), ('wahba', 0.026), ('later', 0.026), ('minimized', 0.026), ('validation', 0.026), ('prediction', 0.025), ('sch', 0.025), ('annals', 0.025), ('events', 0.024), ('bhattacharyya', 0.024), ('decade', 0.024), ('joachims', 0.024), ('packages', 0.024), ('plug', 0.024), ('regularized', 0.024), ('usual', 0.023), ('select', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="44-tfidf-1" href="./nips-2005-Computing_the_Solution_Path_for_the_Regularized_Support_Vector_Regression.html">44 nips-2005-Computing the Solution Path for the Regularized Support Vector Regression</a></p>
<p>Author: Lacey Gunter, Ji Zhu</p><p>Abstract: In this paper we derive an algorithm that computes the entire solution path of the support vector regression, with essentially the same computational cost as ﬁtting one SVR model. We also propose an unbiased estimate for the degrees of freedom of the SVR model, which allows convenient selection of the regularization parameter. 1</p><p>2 0.10582764 <a title="44-tfidf-2" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>Author: Sören Sonnenburg, Gunnar Rätsch, Christin Schäfer</p><p>Abstract: While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lankriet et al. (2004) considered conic combinations of kernel matrices for classiﬁcation, leading to a convex quadratically constraint quadratic program. We show that it can be rewritten as a semi-inﬁnite linear program that can be efﬁciently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classiﬁcation. Experimental results show that the proposed algorithm helps for automatic model selection, improving the interpretability of the learning result and works for hundred thousands of examples or hundreds of kernels to be combined. 1</p><p>3 0.092894636 <a title="44-tfidf-3" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>Author: Andreas Argyriou, Mark Herbster, Massimiliano Pontil</p><p>Abstract: A foundational problem in semi-supervised learning is the construction of a graph underlying the data. We propose to use a method which optimally combines a number of differently constructed graphs. For each of these graphs we associate a basic graph kernel. We then compute an optimal combined kernel. This kernel solves an extended regularization problem which requires a joint minimization over both the data and the set of graph kernels. We present encouraging results on different OCR tasks where the optimal combined kernel is computed from graphs constructed with a variety of distances functions and the ‘k’ in nearest neighbors. 1</p><p>4 0.090657398 <a title="44-tfidf-4" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>Author: Tong Zhang, Rie Kubota Ando</p><p>Abstract: We consider a framework for semi-supervised learning using spectral decomposition based un-supervised kernel design. This approach subsumes a class of previously proposed semi-supervised learning methods on data graphs. We examine various theoretical properties of such methods. In particular, we derive a generalization performance bound, and obtain the optimal kernel design by minimizing the bound. Based on the theoretical analysis, we are able to demonstrate why spectral kernel design based methods can often improve the predictive performance. Experiments are used to illustrate the main consequences of our analysis.</p><p>5 0.088260077 <a title="44-tfidf-5" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>Author: Yoshua Bengio, Nicolas L. Roux, Pascal Vincent, Olivier Delalleau, Patrice Marcotte</p><p>Abstract: Convexity has recently received a lot of attention in the machine learning community, and the lack of convexity has been seen as a major disadvantage of many learning algorithms, such as multi-layer artiﬁcial neural networks. We show that training multi-layer neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem. This problem involves an inﬁnite number of variables, but can be solved by incrementally inserting a hidden unit at a time, each time ﬁnding a linear classiﬁer that minimizes a weighted sum of errors. 1</p><p>6 0.079513401 <a title="44-tfidf-6" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>7 0.079492792 <a title="44-tfidf-7" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>8 0.07905519 <a title="44-tfidf-8" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>9 0.074349828 <a title="44-tfidf-9" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>10 0.073264502 <a title="44-tfidf-10" href="./nips-2005-Correcting_sample_selection_bias_in_maximum_entropy_density_estimation.html">51 nips-2005-Correcting sample selection bias in maximum entropy density estimation</a></p>
<p>11 0.062842429 <a title="44-tfidf-11" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>12 0.062182322 <a title="44-tfidf-12" href="./nips-2005-Maximum_Margin_Semi-Supervised_Learning_for_Structured_Variables.html">123 nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</a></p>
<p>13 0.05995623 <a title="44-tfidf-13" href="./nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">113 nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<p>14 0.059295427 <a title="44-tfidf-14" href="./nips-2005-Non-Local_Manifold_Parzen_Windows.html">138 nips-2005-Non-Local Manifold Parzen Windows</a></p>
<p>15 0.058721848 <a title="44-tfidf-15" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>16 0.056682326 <a title="44-tfidf-16" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>17 0.055283524 <a title="44-tfidf-17" href="./nips-2005-Rodeo%3A_Sparse_Nonparametric_Regression_in_High_Dimensions.html">168 nips-2005-Rodeo: Sparse Nonparametric Regression in High Dimensions</a></p>
<p>18 0.052297045 <a title="44-tfidf-18" href="./nips-2005-Asymptotics_of_Gaussian_Regularized_Least_Squares.html">31 nips-2005-Asymptotics of Gaussian Regularized Least Squares</a></p>
<p>19 0.050466415 <a title="44-tfidf-19" href="./nips-2005-Nonparametric_inference_of_prior_probabilities_from_Bayes-optimal_behavior.html">140 nips-2005-Nonparametric inference of prior probabilities from Bayes-optimal behavior</a></p>
<p>20 0.050348472 <a title="44-tfidf-20" href="./nips-2005-From_Lasso_regression_to_Feature_vector_machine.html">77 nips-2005-From Lasso regression to Feature vector machine</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.175), (1, 0.082), (2, -0.043), (3, -0.061), (4, 0.064), (5, 0.077), (6, 0.048), (7, -0.009), (8, 0.081), (9, 0.027), (10, 0.01), (11, -0.074), (12, -0.043), (13, 0.061), (14, -0.012), (15, 0.057), (16, 0.012), (17, 0.121), (18, 0.006), (19, -0.015), (20, 0.033), (21, -0.009), (22, 0.032), (23, -0.032), (24, -0.027), (25, 0.049), (26, 0.025), (27, -0.054), (28, -0.008), (29, -0.005), (30, 0.018), (31, 0.019), (32, -0.016), (33, -0.007), (34, -0.019), (35, -0.027), (36, 0.077), (37, -0.042), (38, -0.095), (39, 0.074), (40, -0.022), (41, -0.016), (42, 0.15), (43, -0.052), (44, 0.025), (45, 0.09), (46, -0.005), (47, -0.028), (48, -0.038), (49, -0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9417907 <a title="44-lsi-1" href="./nips-2005-Computing_the_Solution_Path_for_the_Regularized_Support_Vector_Regression.html">44 nips-2005-Computing the Solution Path for the Regularized Support Vector Regression</a></p>
<p>Author: Lacey Gunter, Ji Zhu</p><p>Abstract: In this paper we derive an algorithm that computes the entire solution path of the support vector regression, with essentially the same computational cost as ﬁtting one SVR model. We also propose an unbiased estimate for the degrees of freedom of the SVR model, which allows convenient selection of the regularization parameter. 1</p><p>2 0.73467052 <a title="44-lsi-2" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>Author: Sören Sonnenburg, Gunnar Rätsch, Christin Schäfer</p><p>Abstract: While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lankriet et al. (2004) considered conic combinations of kernel matrices for classiﬁcation, leading to a convex quadratically constraint quadratic program. We show that it can be rewritten as a semi-inﬁnite linear program that can be efﬁciently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classiﬁcation. Experimental results show that the proposed algorithm helps for automatic model selection, improving the interpretability of the learning result and works for hundred thousands of examples or hundreds of kernels to be combined. 1</p><p>3 0.57495695 <a title="44-lsi-3" href="./nips-2005-Rodeo%3A_Sparse_Nonparametric_Regression_in_High_Dimensions.html">168 nips-2005-Rodeo: Sparse Nonparametric Regression in High Dimensions</a></p>
<p>Author: Larry Wasserman, John D. Lafferty</p><p>Abstract: We present a method for nonparametric regression that performs bandwidth selection and variable selection simultaneously. The approach is based on the technique of incrementally decreasing the bandwidth in directions where the gradient of the estimator with respect to bandwidth is large. When the unknown function satisﬁes a sparsity condition, our approach avoids the curse of dimensionality, achieving the optimal minimax rate of convergence, up to logarithmic factors, as if the relevant variables were known in advance. The method—called rodeo (regularization of derivative expectation operator)—conducts a sequence of hypothesis tests, and is easy to implement. A modiﬁed version that replaces hard with soft thresholding effectively solves a sequence of lasso problems. 1</p><p>4 0.55283487 <a title="44-lsi-4" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>Author: Yves Grandvalet, Johnny Mariethoz, Samy Bengio</p><p>Abstract: In this paper, we show that the hinge loss can be interpreted as the neg-log-likelihood of a semi-parametric model of posterior probabilities. From this point of view, SVMs represent the parametric component of a semi-parametric model ﬁtted by a maximum a posteriori estimation procedure. This connection enables to derive a mapping from SVM scores to estimated posterior probabilities. Unlike previous proposals, the suggested mapping is interval-valued, providing a set of posterior probabilities compatible with each SVM score. This framework offers a new way to adapt the SVM optimization problem to unbalanced classiﬁcation, when decisions result in unequal (asymmetric) losses. Experiments show improvements over state-of-the-art procedures. 1</p><p>5 0.54057294 <a title="44-lsi-5" href="./nips-2005-Maximum_Margin_Semi-Supervised_Learning_for_Structured_Variables.html">123 nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</a></p>
<p>Author: Y. Altun, D. McAllester, M. Belkin</p><p>Abstract: Many real-world classiﬁcation problems involve the prediction of multiple inter-dependent variables forming some structural dependency. Recent progress in machine learning has mainly focused on supervised classiﬁcation of such structured variables. In this paper, we investigate structured classiﬁcation in a semi-supervised setting. We present a discriminative approach that utilizes the intrinsic geometry of input patterns revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables. Unlike transductive algorithms, our formulation naturally extends to new test points. 1</p><p>6 0.51683599 <a title="44-lsi-6" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>7 0.50387144 <a title="44-lsi-7" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>8 0.50174427 <a title="44-lsi-8" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>9 0.50079978 <a title="44-lsi-9" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>10 0.49459091 <a title="44-lsi-10" href="./nips-2005-From_Lasso_regression_to_Feature_vector_machine.html">77 nips-2005-From Lasso regression to Feature vector machine</a></p>
<p>11 0.49349591 <a title="44-lsi-11" href="./nips-2005-Kernels_for_gene_regulatory_regions.html">103 nips-2005-Kernels for gene regulatory regions</a></p>
<p>12 0.48634684 <a title="44-lsi-12" href="./nips-2005-Worst-Case_Bounds_for_Gaussian_Process_Models.html">205 nips-2005-Worst-Case Bounds for Gaussian Process Models</a></p>
<p>13 0.48488116 <a title="44-lsi-13" href="./nips-2005-Asymptotics_of_Gaussian_Regularized_Least_Squares.html">31 nips-2005-Asymptotics of Gaussian Regularized Least Squares</a></p>
<p>14 0.46770573 <a title="44-lsi-14" href="./nips-2005-Nonparametric_inference_of_prior_probabilities_from_Bayes-optimal_behavior.html">140 nips-2005-Nonparametric inference of prior probabilities from Bayes-optimal behavior</a></p>
<p>15 0.45608425 <a title="44-lsi-15" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>16 0.45448726 <a title="44-lsi-16" href="./nips-2005-Sequence_and_Tree_Kernels_with_Statistical_Feature_Mining.html">175 nips-2005-Sequence and Tree Kernels with Statistical Feature Mining</a></p>
<p>17 0.45143381 <a title="44-lsi-17" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>18 0.45081186 <a title="44-lsi-18" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>19 0.44398579 <a title="44-lsi-19" href="./nips-2005-Subsequence_Kernels_for_Relation_Extraction.html">185 nips-2005-Subsequence Kernels for Relation Extraction</a></p>
<p>20 0.44190592 <a title="44-lsi-20" href="./nips-2005-Non-Local_Manifold_Parzen_Windows.html">138 nips-2005-Non-Local Manifold Parzen Windows</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.049), (10, 0.026), (27, 0.017), (31, 0.023), (34, 0.191), (55, 0.03), (65, 0.012), (69, 0.044), (73, 0.03), (77, 0.36), (88, 0.084), (91, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83955806 <a title="44-lda-1" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>Author: Eizaburo Doi, Doru C. Balcan, Michael S. Lewicki</p><p>Abstract: Biological sensory systems are faced with the problem of encoding a high-ﬁdelity sensory signal with a population of noisy, low-ﬁdelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and twodimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of highdimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets. 1</p><p>same-paper 2 0.82696861 <a title="44-lda-2" href="./nips-2005-Computing_the_Solution_Path_for_the_Regularized_Support_Vector_Regression.html">44 nips-2005-Computing the Solution Path for the Regularized Support Vector Regression</a></p>
<p>Author: Lacey Gunter, Ji Zhu</p><p>Abstract: In this paper we derive an algorithm that computes the entire solution path of the support vector regression, with essentially the same computational cost as ﬁtting one SVR model. We also propose an unbiased estimate for the degrees of freedom of the SVR model, which allows convenient selection of the regularization parameter. 1</p><p>3 0.82210052 <a title="44-lda-3" href="./nips-2005-Efficient_Estimation_of_OOMs.html">62 nips-2005-Efficient Estimation of OOMs</a></p>
<p>Author: Herbert Jaeger, Mingjie Zhao, Andreas Kolling</p><p>Abstract: A standard method to obtain stochastic models for symbolic time series is to train state-emitting hidden Markov models (SE-HMMs) with the Baum-Welch algorithm. Based on observable operator models (OOMs), in the last few months a number of novel learning algorithms for similar purposes have been developed: (1,2) two versions of an ”efﬁciency sharpening” (ES) algorithm, which iteratively improves the statistical efﬁciency of a sequence of OOM estimators, (3) a constrained gradient descent ML estimator for transition-emitting HMMs (TE-HMMs). We give an overview on these algorithms and compare them with SE-HMM/EM learning on synthetic and real-life data. 1</p><p>4 0.52810711 <a title="44-lda-4" href="./nips-2005-Off-Road_Obstacle_Avoidance_through_End-to-End_Learning.html">143 nips-2005-Off-Road Obstacle Avoidance through End-to-End Learning</a></p>
<p>Author: Urs Muller, Jan Ben, Eric Cosatto, Beat Flepp, Yann L. Cun</p><p>Abstract: We describe a vision-based obstacle avoidance system for off-road mobile robots. The system is trained from end to end to map raw input images to steering angles. It is trained in supervised mode to predict the steering angles provided by a human driver during training runs collected in a wide variety of terrains, weather conditions, lighting conditions, and obstacle types. The robot is a 50cm off-road truck, with two forwardpointing wireless color cameras. A remote computer processes the video and controls the robot via radio. The learning system is a large 6-layer convolutional network whose input is a single left/right pair of unprocessed low-resolution images. The robot exhibits an excellent ability to detect obstacles and navigate around them in real time at speeds of 2 m/s.</p><p>5 0.52105051 <a title="44-lda-5" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>Author: Andreas Argyriou, Mark Herbster, Massimiliano Pontil</p><p>Abstract: A foundational problem in semi-supervised learning is the construction of a graph underlying the data. We propose to use a method which optimally combines a number of differently constructed graphs. For each of these graphs we associate a basic graph kernel. We then compute an optimal combined kernel. This kernel solves an extended regularization problem which requires a joint minimization over both the data and the set of graph kernels. We present encouraging results on different OCR tasks where the optimal combined kernel is computed from graphs constructed with a variety of distances functions and the ‘k’ in nearest neighbors. 1</p><p>6 0.51978946 <a title="44-lda-6" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>7 0.51218486 <a title="44-lda-7" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>8 0.51041859 <a title="44-lda-8" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>9 0.50894672 <a title="44-lda-9" href="./nips-2005-A_Bayes_Rule_for_Density_Matrices.html">2 nips-2005-A Bayes Rule for Density Matrices</a></p>
<p>10 0.50869232 <a title="44-lda-10" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>11 0.50784743 <a title="44-lda-11" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>12 0.50713825 <a title="44-lda-12" href="./nips-2005-Rate_Distortion_Codes_in_Sensor_Networks%3A_A_System-level_Analysis.html">162 nips-2005-Rate Distortion Codes in Sensor Networks: A System-level Analysis</a></p>
<p>13 0.50683689 <a title="44-lda-13" href="./nips-2005-Asymptotics_of_Gaussian_Regularized_Least_Squares.html">31 nips-2005-Asymptotics of Gaussian Regularized Least Squares</a></p>
<p>14 0.50614244 <a title="44-lda-14" href="./nips-2005-Sequence_and_Tree_Kernels_with_Statistical_Feature_Mining.html">175 nips-2005-Sequence and Tree Kernels with Statistical Feature Mining</a></p>
<p>15 0.50097215 <a title="44-lda-15" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>16 0.50062752 <a title="44-lda-16" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>17 0.49779025 <a title="44-lda-17" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>18 0.49591362 <a title="44-lda-18" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>19 0.49306345 <a title="44-lda-19" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>20 0.4925392 <a title="44-lda-20" href="./nips-2005-From_Lasso_regression_to_Feature_vector_machine.html">77 nips-2005-From Lasso regression to Feature vector machine</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
