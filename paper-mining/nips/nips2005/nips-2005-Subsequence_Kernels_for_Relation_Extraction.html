<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>185 nips-2005-Subsequence Kernels for Relation Extraction</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-185" href="#">nips2005-185</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>185 nips-2005-Subsequence Kernels for Relation Extraction</h1>
<br/><p>Source: <a title="nips-2005-185-pdf" href="http://papers.nips.cc/paper/2787-subsequence-kernels-for-relation-extraction.pdf">pdf</a></p><p>Author: Raymond J. Mooney, Razvan C. Bunescu</p><p>Abstract: We present a new kernel method for extracting semantic relations between entities in natural language text, based on a generalization of subsequence kernels. This kernel uses three types of subsequence patterns that are typically employed in natural language to assert relationships between two entities. Experiments on extracting protein interactions from biomedical corpora and top-level relations from newspaper corpora demonstrate the advantages of this approach. 1</p><p>Reference: <a title="nips-2005-185-reference" href="../nips2005_reference/nips-2005-Subsequence_Kernels_for_Relation_Extraction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We present a new kernel method for extracting semantic relations between entities in natural language text, based on a generalization of subsequence kernels. [sent-7, score-0.957]
</p><p>2 This kernel uses three types of subsequence patterns that are typically employed in natural language to assert relationships between two entities. [sent-8, score-0.617]
</p><p>3 Experiments on extracting protein interactions from biomedical corpora and top-level relations from newspaper corpora demonstrate the advantages of this approach. [sent-9, score-0.782]
</p><p>4 It involves the analysis of text documents, with the aim of identifying particular types of entities and relations among them. [sent-11, score-0.428]
</p><p>5 Reliably extracting relations between entities in natural-language documents is still a difﬁcult, unsolved problem. [sent-12, score-0.554]
</p><p>6 For example, in the sentence “protesters seized several pumping stations”, the task is to identify a L OCATED AT relationship between protesters (a P ERSON entity) and stations (a L OCATION entity). [sent-15, score-0.339]
</p><p>7 For example, the sentence “TR6 speciﬁcally binds Fas ligand”, asserts an interaction relationship between the two proteins TR6 and Fas ligand. [sent-17, score-0.429]
</p><p>8 As in the case of the more traditional applications of IE, systems based on manually developed extraction rules [3, 4] were soon superseded by information extractors learned through training on supervised corpora [5, 6]. [sent-18, score-0.394]
</p><p>9 One challenge posed by the biological domain is that current systems for doing part-of-speech (POS) tagging or parsing do not perform as well on the biomedical narrative as on the newspaper corpora on which they were originally trained. [sent-19, score-0.327]
</p><p>10 Motivated by the task of extracting protein-protein interactions from biomedical corpora, we present a generalization of the subsequence kernel from [7] that works with sequences containing combinations of words and word classes. [sent-21, score-0.929]
</p><p>11 This generalized kernel is further tailored for the task of relation extraction. [sent-22, score-0.369]
</p><p>12 Experimental results show that the new relation  kernel outperforms two previous rule-based methods for interaction extraction. [sent-23, score-0.457]
</p><p>13 With a small modiﬁcation, the same kernel is used for extracting top-level relations from ACE corpora, providing better results than a recent approach based on dependency tree kernels. [sent-24, score-0.585]
</p><p>14 2  Background  One of the ﬁrst approaches to extracting protein interactions is that of Blaschke et al. [sent-25, score-0.315]
</p><p>15 Between every two adjacent words is a number indicating the maximum number of intervening words allowed when matching the rule to a sentence. [sent-28, score-0.278]
</p><p>16 A sentence matches the rule if and only if it satisﬁes the word constraints in the given order and respects the respective word gaps. [sent-30, score-0.516]
</p><p>17 one of the given words must match the sentence at that position. [sent-37, score-0.341]
</p><p>18 3  Extraction using a Relation Kernel  Both Blaschke and ELCS do interaction extraction based on a limited set of matching rules, where a rule is simply a sparse (gappy) subsequence of words or POS tags anchored on the two protein-name tokens. [sent-39, score-1.02]
</p><p>19 Therefore, the two methods share a common limitation: either through manual selection (Blaschke), or as a result of the greedy learning procedure (ELCS), they end up using only a subset of all possible anchored sparse subsequences. [sent-40, score-0.221]
</p><p>20 Ideally, we would want to use all such anchored sparse subsequences as features, with weights reﬂecting their relative accuracy. [sent-41, score-0.31]
</p><p>21 However explicitly creating for each sentence a vector with a position for each such feature is infeasible, due to the high dimensionality of the feature space. [sent-42, score-0.218]
</p><p>22 Computing the dot-product between two such vectors amounts to calculating the number of common anchored subsequences between the two sentences. [sent-44, score-0.306]
</p><p>23 This can be done very efﬁciently by modifying the dynamic programming algorithm used in the string kernel from [7] to account only for common sparse subsequences constrained to contain the two protein-name tokens. [sent-45, score-0.385]
</p><p>24 • [B] Between: only words between the two entities are essential for asserting the relationship. [sent-48, score-0.367]
</p><p>25 • [BA] Between–After: words between and after the two entity mentions are simultaneously used to express the relationship. [sent-50, score-0.418]
</p><p>26 Another observation is that all these patterns use at most 4 words to express the relationship (not counting the two entity names). [sent-52, score-0.558]
</p><p>27 Consequently, when computing the relation kernel, we restrict the counting of common anchored subsequences only to those having one of the three types described above, with a maximum word-length of 4. [sent-53, score-0.609]
</p><p>28 This type of feature  selection leads not only to a faster kernel computation, but also to less overﬁtting, which results in increased accuracy (see Section 5 for comparative experiments). [sent-54, score-0.2]
</p><p>29 This can be alleviated by categorizing words into classes with varying degrees of generality, and then allowing patterns to use both words and their classes. [sent-56, score-0.362]
</p><p>30 Examples of word classes are POS tags and generalizations over POS tags such as Noun, Active Verb or Passive Verb. [sent-57, score-0.419]
</p><p>31 The entity type can also be used, if the word is part of a known named entity, as well as the type of the chunk containing the word, when chunking information is available. [sent-58, score-0.527]
</p><p>32 Patterns then will consist of sparse subsequences of words, POS tags, general POS (GPOS) tags, entity and chunk types, or WordNet synsets. [sent-60, score-0.476]
</p><p>33 4  Subsequence Kernels for Relation Extraction  We are going to show how to compute the relation kernel described in the previous section in two steps. [sent-62, score-0.369]
</p><p>34 1 we present a generalization of the subsequence kernel from [7]. [sent-64, score-0.403]
</p><p>35 This new kernel works with patterns construed as mixtures of words and word classes. [sent-65, score-0.531]
</p><p>36 Based on this generalized subsequence kernel, in Section 4. [sent-66, score-0.244]
</p><p>37 2 we formally deﬁne and show the efﬁcient computation of the relation kernel used in our experiments. [sent-67, score-0.369]
</p><p>38 The sequence s[i : j] is the contiguous subsequence si . [sent-84, score-0.244]
</p><p>39 We say that the sequence u ∈ Σ∗ is a (sparse) subsequence of s if there is a sequence of |u| indices i such that ∪ uk ∈ sik , for all k = 1, . [sent-98, score-0.302]
</p><p>40 Finally, let Kn (s, t, λ) (Equation 1) be the number of weighted sparse subsequences u of length n common to s and t (i. [sent-103, score-0.266]
</p><p>41 Through them, we try to capture head-modiﬁer dependencies that are important for relation extraction; for lack of reliable dependency information, the larger the word gap is between two words, the less conﬁdent we are in the existence of a headmodiﬁer relationship between them. [sent-108, score-0.481]
</p><p>42 ′  =  1, f or all s, t  =  λKi (sx, t) + λ2 Ki−1 (s, t) · c(x, y)  Ki (sx, t)  =  λKi (s, t) + Ki (sx, t)  Kn (sx, t)  =  Kn (s, t) +  K0 (s, t) ′′  Ki (sx, ty) ′  ′′  ′  ′  ′′  ′  λ2 Kn−1 (s, t[1 : j − 1]) · c(x, t[j]) j  Figure 1: Computation of subsequence kernel. [sent-113, score-0.244]
</p><p>43 2  Computing the Relation Kernel  As described in Section 2, the input consists of a set of sentences, where each sentence contains exactly two entities (protein names in the case of interaction extraction). [sent-115, score-0.59]
</p><p>44 In Figure 2 we show the segments that will be used for computing the relation kernel between two example sentences s and t. [sent-116, score-0.463]
</p><p>45 In sentence s for instance, x1 and x2 are the two entities, sf is the sentence segment before x1 , sb is the segment between x1 and x2 , and sa is the sentence ′ segment after x2 . [sent-117, score-0.982]
</p><p>46 For convenience, we also include the auxiliary segment sb = x1 sb x2 , ′ whose span is computed as l(sb ) = l(sb ) + 2 (in all length computations, we consider x1 and x2 as contributing one unit only). [sent-118, score-0.335]
</p><p>47 sb  sf x1  s =  sa x2  s’ b tf t =  tb y1  ta y2  t’ b  Figure 2: Sentence segments. [sent-119, score-0.322]
</p><p>48 The relation kernel computes the number of common patterns between two sentences s and t, where the set of patterns is restricted to the three types introduced in Section 3. [sent-120, score-0.783]
</p><p>49 All three sub-kernels include in their computation the counting of common subsequences ′ ′ between sb and tb . [sent-123, score-0.432]
</p><p>50 In order to speed up the computation, all these common counts can be calculated separately in bKi , which is deﬁned as the number of common subsequences of ′ ′ length i between sb and tb , anchored at x1 /x2 and y1 /y2 respectively (i. [sent-124, score-0.626]
</p><p>51 Then f bK simply counts the number of subsequences that match j positions before the ﬁrst entity and i positions between the entities, constrained to have length less than a constant f bmax . [sent-127, score-0.502]
</p><p>52 In Section 3 a a we observed that all three subsequence patterns use at most 4 words to express a relation, ′ therefore we set constants f bmax , bmax and bamax to 4. [sent-131, score-0.693]
</p><p>53 5  Experimental Results  The relation kernel (ERK) is evaluated on the task of extracting relations from two corpora with different types of narrative, which are described in more detail in the following sections. [sent-134, score-0.81]
</p><p>54 In both cases, we assume that the entities and their labels are known. [sent-135, score-0.244]
</p><p>55 All preprocessing steps – sentence segmentation, tokenization, POS tagging and chunking – were performed using the OpenNLP1 package. [sent-136, score-0.319]
</p><p>56 If a sentence contains n entities (n ≥ 2), it is replicated into n sentences, each containing only two entities. [sent-137, score-0.5]
</p><p>57 If the two entities are 2 known to be in a relationship, then the replicated sentence is added to the set of corresponding positive sentences, otherwise it is added to the set of negative sentences. [sent-138, score-0.5]
</p><p>58 During testing, a sentence having n entities (n ≥ 2) is again replicated into n sentences in a similar way. [sent-139, score-0.594]
</p><p>59 2 The relation kernel is used in conjunction with SVM learning in order to ﬁnd a decision hyperplane that best separates the positive examples from negative examples. [sent-140, score-0.369]
</p><p>60 The performance is measured using precision (percentage of correctly extracted relations out of total extracted) and recall (percentage of correctly extracted relations out of total number of relations annotated in the corpus). [sent-144, score-0.482]
</p><p>61 The graph points are obtained by varying a threshold on the minimum acceptable extraction conﬁdence, based on the probability estimates from LibSVM. [sent-146, score-0.234]
</p><p>62 1  Interaction Extraction from AImed  We did comparative experiments on the AImed corpus, which has been previously used for training the protein interaction extraction systems in [6]. [sent-154, score-0.469]
</p><p>63 The results, summarized in Figure 4(a), show that the relation kernel outperforms both ELCS and the manually written rules. [sent-161, score-0.369]
</p><p>64 To evaluate the impact that the three types of patterns have on performance, we compare ERK with an ablated system (ERK-A) that uses all possible patterns, constrained only to be anchored on the two entity names. [sent-165, score-0.488]
</p><p>65 2  Relation Extraction from ACE  To evaluate how well this relation kernel ports to other types of narrative, we applied it to the problem of extracting top-level relations from the ACE corpus [2], the version used for the September 2002 evaluation. [sent-168, score-0.746]
</p><p>66 This version of the ACE corpus contains three types of annotations: coreference, named entities and relations. [sent-170, score-0.398]
</p><p>67 There are ﬁve types of entities – P ERSON, O RGANIZATION, FACILITY, L OCATION, and G EO -P OLITICAL E NTITY – which can participate in ﬁve general, top-level relations: ROLE, PART, L OCATED, N EAR, and S OCIAL. [sent-171, score-0.298]
</p><p>68 A recent approach to extracting relations is described in [9]. [sent-172, score-0.266]
</p><p>69 The authors use a generalized version of the tree kernel from [10] to compute a kernel over relation examples, where a relation example consists of the smallest dependency tree containing the two entities of the relation. [sent-173, score-1.207]
</p><p>70 – [S2] One binary SVM is trained for relation detection, meaning that all positive relation instances are combined into one class. [sent-175, score-0.42]
</p><p>71 The thresholded output of this binary classiﬁer is used as training data for a second multi-class SVM, trained for relation classiﬁcation. [sent-176, score-0.21]
</p><p>72 We trained our relation kernel, under the ﬁrst scenario, to recognize the same 5 top-level relation types. [sent-177, score-0.42]
</p><p>73 While for interaction extraction we used only the lexicalized version of the kernel, here we utilize more features, corresponding to the following feature spaces: Σ1 is the word vocabulary, Σ2 is the set of POS tags, Σ3 is the set of generic POS tags, and Σ4 contains the 5 entity types. [sent-178, score-0.694]
</p><p>74 We also used chunking information as follows: all (sparse) subsequences were created exclusively from the chunk heads, where a head is deﬁned as the last word in a chunk. [sent-179, score-0.481]
</p><p>75 The same criterion was used for computing the length of a subsequence – all words other than head words were ignored. [sent-180, score-0.573]
</p><p>76 This is based on the observation that in general words other than the chunk head do not contribute to establishing a relationship between two entities outside of that chunk. [sent-181, score-0.537]
</p><p>77 One exception is when both entities in the example sentence are contained in the same chunk. [sent-182, score-0.462]
</p><p>78 In these cases, we let one chunk contribute both entity heads. [sent-186, score-0.284]
</p><p>79 Also, an important difference from the interaction extraction case is that often the two entities in a relation do not have any words separating them, as for example in noun-noun compounds. [sent-187, score-0.899]
</p><p>80 This pattern consists of a sequence of length two formed from the head words (or their word classes) of the two entities. [sent-189, score-0.339]
</p><p>81 Correspondingly, we updated the relation kernel from Figure 3 with a new kernel term mK, as illustrated in Equation 4. [sent-190, score-0.528]
</p><p>82 mK(s, t) = c(x1 , y1 ) · c(x2 , y2 ) · λ2+2  (5)  We present in Table 1 the results of using our updated relation kernel to extract relations from ACE, under the ﬁrst scenario. [sent-192, score-0.499]
</p><p>83 We also show the results presented in [9] for their best performing kernel K4 (a sum between a bag-of-words kernel and the dependency kernel) under both scenarios. [sent-193, score-0.413]
</p><p>84 Therefore we expect to get an even more signiﬁcant increase in performance by training our relation kernel in the same cascaded fashion. [sent-206, score-0.403]
</p><p>85 6  Related Work  In [10], a tree kernel is deﬁned over shallow parse representations of text, together with an efﬁcient algorithm for computing it. [sent-207, score-0.224]
</p><p>86 Experiments on extracting P ERSON -A FFILIATION and O RGANIZATION -L OCATION relations from 200 news articles show the advantage of using this new type of tree kernels over three feature-based algorithms. [sent-208, score-0.378]
</p><p>87 The same kernel was slightly generalized in [9] and applied on dependency tree representations of sentences, with dependency trees being created from head-modiﬁer relationships extracted from syntactic parse trees. [sent-209, score-0.453]
</p><p>88 Experimental results show a clear win of the dependency tree kernel over a bag-of-words kernel. [sent-210, score-0.319]
</p><p>89 For relation extraction, word order is important, and our experimental results support this claim – all subsequence patterns used in our approach retain the order between words. [sent-212, score-0.703]
</p><p>90 For subsequence kernels, the semantics is known by deﬁnition: each subsequence pattern corresponds to a dimension in the Hilbert space. [sent-214, score-0.488]
</p><p>91 This enabled us to easily restrict the types of patterns counted by the kernel to the three types that we deemed relevant for relation extraction. [sent-215, score-0.593]
</p><p>92 7  Conclusion and Future Work  We have presented a new relation extraction method based on a generalization of subsequence kernels. [sent-216, score-0.688]
</p><p>93 After a small modiﬁcation, the same kernel was evaluated on the task of extracting top-level relations from the ACE corpus, showing better performance when compared with a recent dependency tree kernel. [sent-218, score-0.585]
</p><p>94 2 – using the relation kernel in a cascaded fashion, in order to improve the low recall caused by the highly unbalanced data distribution. [sent-220, score-0.45]
</p><p>95 Currently, the method assumes the named entities are known. [sent-222, score-0.287]
</p><p>96 A natural extension is to integrate named entity recognition with relation extraction. [sent-223, score-0.453]
</p><p>97 Valencia, The frame-based module of the Suiseki information extraction system, IEEE Intelligent Systems 17 (2002) 14–20. [sent-242, score-0.234]
</p><p>98 Craven, Representing sentence structure in hidden Markov models for information extraction, in: Proceedings of the Seventeenth International Joint Conference on Artiﬁcial Intelligence (IJCAI-2001), Seattle, WA, 2001, pp. [sent-245, score-0.218]
</p><p>99 Sorensen, Dependency tree kernels for relation extraction, in: Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), Barcelona, Spain, 2004, pp. [sent-271, score-0.322]
</p><p>100 Richardella, Kernel methods for relation extraction, Journal of Machine Learning Research 3 (2003) 1083–1106. [sent-276, score-0.21]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pos', 0.271), ('entities', 0.244), ('subsequence', 0.244), ('extraction', 0.234), ('sentence', 0.218), ('relation', 0.21), ('entity', 0.2), ('erk', 0.193), ('elcs', 0.174), ('kernel', 0.159), ('subsequences', 0.154), ('kn', 0.154), ('ace', 0.151), ('tags', 0.143), ('extracting', 0.136), ('word', 0.133), ('relations', 0.13), ('sb', 0.128), ('words', 0.123), ('corpora', 0.121), ('anchored', 0.118), ('patterns', 0.116), ('protein', 0.106), ('bk', 0.106), ('bki', 0.097), ('blaschke', 0.097), ('dependency', 0.095), ('sentences', 0.094), ('sx', 0.092), ('interaction', 0.088), ('chunk', 0.084), ('narrative', 0.077), ('ki', 0.077), ('bak', 0.077), ('tb', 0.077), ('interactions', 0.073), ('bmax', 0.067), ('chunking', 0.067), ('tree', 0.065), ('biomedical', 0.061), ('austin', 0.061), ('erson', 0.058), ('mentions', 0.058), ('mooney', 0.058), ('ocation', 0.058), ('sik', 0.058), ('corpus', 0.057), ('types', 0.054), ('ie', 0.049), ('proteins', 0.049), ('kernels', 0.047), ('recall', 0.047), ('mk', 0.047), ('sf', 0.046), ('precision', 0.045), ('documents', 0.044), ('language', 0.044), ('relationship', 0.043), ('head', 0.043), ('named', 0.043), ('counts', 0.041), ('comparative', 0.041), ('names', 0.04), ('fb', 0.04), ('length', 0.04), ('segment', 0.039), ('counting', 0.039), ('bamax', 0.039), ('bunescu', 0.039), ('extractors', 0.039), ('fas', 0.039), ('fore', 0.039), ('kate', 0.039), ('lexicalized', 0.039), ('noun', 0.039), ('ocated', 0.039), ('protesters', 0.039), ('razvan', 0.039), ('rganization', 0.039), ('stations', 0.039), ('syntactic', 0.039), ('tjk', 0.039), ('replicated', 0.038), ('sparse', 0.038), ('express', 0.037), ('sa', 0.037), ('cascaded', 0.034), ('url', 0.034), ('newspaper', 0.034), ('lexical', 0.034), ('ta', 0.034), ('tagging', 0.034), ('ty', 0.034), ('valencia', 0.034), ('common', 0.034), ('rk', 0.033), ('rule', 0.032), ('manual', 0.031), ('aimed', 0.031), ('asserts', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="185-tfidf-1" href="./nips-2005-Subsequence_Kernels_for_Relation_Extraction.html">185 nips-2005-Subsequence Kernels for Relation Extraction</a></p>
<p>Author: Raymond J. Mooney, Razvan C. Bunescu</p><p>Abstract: We present a new kernel method for extracting semantic relations between entities in natural language text, based on a generalization of subsequence kernels. This kernel uses three types of subsequence patterns that are typically employed in natural language to assert relationships between two entities. Experiments on extracting protein interactions from biomedical corpora and top-level relations from newspaper corpora demonstrate the advantages of this approach. 1</p><p>2 0.14431806 <a title="185-tfidf-2" href="./nips-2005-Dynamic_Social_Network_Analysis_using_Latent_Space_Models.html">60 nips-2005-Dynamic Social Network Analysis using Latent Space Models</a></p>
<p>Author: Purnamrita Sarkar, Andrew W. Moore</p><p>Abstract: This paper explores two aspects of social network modeling. First, we generalize a successful static model of relationships into a dynamic model that accounts for friendships drifting over time. Second, we show how to make it tractable to learn such models from data, even as the number of entities n gets large. The generalized model associates each entity with a point in p-dimensional Euclidian latent space. The points can move as time progresses but large moves in latent space are improbable. Observed links between entities are more likely if the entities are close in latent space. We show how to make such a model tractable (subquadratic in the number of entities) by the use of appropriate kernel functions for similarity in latent space; the use of low dimensional kd-trees; a new efﬁcient dynamic adaptation of multidimensional scaling for a ﬁrst pass of approximate projection of entities into latent space; and an efﬁcient conjugate gradient update rule for non-linear local optimization in which amortized time per entity during an update is O(log n). We use both synthetic and real-world data on upto 11,000 entities which indicate linear scaling in computation time and improved performance over four alternative approaches. We also illustrate the system operating on twelve years of NIPS co-publication data. We present a detailed version of this work in [1]. 1</p><p>3 0.12604928 <a title="185-tfidf-3" href="./nips-2005-Group_and_Topic_Discovery_from_Relations_and_Their_Attributes.html">89 nips-2005-Group and Topic Discovery from Relations and Their Attributes</a></p>
<p>Author: Xuerui Wang, Natasha Mohanty, Andrew McCallum</p><p>Abstract: We present a probabilistic generative model of entity relationships and their attributes that simultaneously discovers groups among the entities and topics among the corresponding textual attributes. Block-models of relationship data have been studied in social network analysis for some time. Here we simultaneously cluster in several modalities at once, incorporating the attributes (here, words) associated with certain relationships. Signiﬁcantly, joint inference allows the discovery of topics to be guided by the emerging groups, and vice-versa. We present experimental results on two large data sets: sixteen years of bills put before the U.S. Senate, comprising their corresponding text and voting records, and thirteen years of similar data from the United Nations. We show that in comparison with traditional, separate latent-variable models for words, or Blockstructures for votes, the Group-Topic model’s joint inference discovers more cohesive groups and improved topics. 1</p><p>4 0.095348239 <a title="185-tfidf-4" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>Author: Tong Zhang, Rie Kubota Ando</p><p>Abstract: We consider a framework for semi-supervised learning using spectral decomposition based un-supervised kernel design. This approach subsumes a class of previously proposed semi-supervised learning methods on data graphs. We examine various theoretical properties of such methods. In particular, we derive a generalization performance bound, and obtain the optimal kernel design by minimizing the bound. Based on the theoretical analysis, we are able to demonstrate why spectral kernel design based methods can often improve the predictive performance. Experiments are used to illustrate the main consequences of our analysis.</p><p>5 0.093912035 <a title="185-tfidf-5" href="./nips-2005-Interpolating_between_types_and_tokens_by_estimating_power-law_generators.html">100 nips-2005-Interpolating between types and tokens by estimating power-law generators</a></p>
<p>Author: Sharon Goldwater, Mark Johnson, Thomas L. Griffiths</p><p>Abstract: Standard statistical models of language fail to capture one of the most striking properties of natural languages: the power-law distribution in the frequencies of word tokens. We present a framework for developing statistical models that generically produce power-laws, augmenting standard generative models with an adaptor that produces the appropriate pattern of token frequencies. We show that taking a particular stochastic process – the Pitman-Yor process – as an adaptor justiﬁes the appearance of type frequencies in formal analyses of natural language, and improves the performance of a model for unsupervised learning of morphology.</p><p>6 0.089446411 <a title="185-tfidf-6" href="./nips-2005-Sequence_and_Tree_Kernels_with_Statistical_Feature_Mining.html">175 nips-2005-Sequence and Tree Kernels with Statistical Feature Mining</a></p>
<p>7 0.087044194 <a title="185-tfidf-7" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>8 0.080729395 <a title="185-tfidf-8" href="./nips-2005-Maximum_Margin_Semi-Supervised_Learning_for_Structured_Variables.html">123 nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</a></p>
<p>9 0.073394865 <a title="185-tfidf-9" href="./nips-2005-A_matching_pursuit_approach_to_sparse_Gaussian_process_regression.html">16 nips-2005-A matching pursuit approach to sparse Gaussian process regression</a></p>
<p>10 0.073191382 <a title="185-tfidf-10" href="./nips-2005-Transfer_learning_for_text_classification.html">195 nips-2005-Transfer learning for text classification</a></p>
<p>11 0.072611034 <a title="185-tfidf-11" href="./nips-2005-Searching_for_Character_Models.html">171 nips-2005-Searching for Character Models</a></p>
<p>12 0.071116179 <a title="185-tfidf-12" href="./nips-2005-Context_as_Filtering.html">48 nips-2005-Context as Filtering</a></p>
<p>13 0.059942778 <a title="185-tfidf-13" href="./nips-2005-Kernels_for_gene_regulatory_regions.html">103 nips-2005-Kernels for gene regulatory regions</a></p>
<p>14 0.057914339 <a title="185-tfidf-14" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>15 0.056760695 <a title="185-tfidf-15" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>16 0.054948747 <a title="185-tfidf-16" href="./nips-2005-Improved_risk_tail_bounds_for_on-line_algorithms.html">95 nips-2005-Improved risk tail bounds for on-line algorithms</a></p>
<p>17 0.053930741 <a title="185-tfidf-17" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>18 0.052736614 <a title="185-tfidf-18" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>19 0.052245971 <a title="185-tfidf-19" href="./nips-2005-Representing_Part-Whole_Relationships_in_Recurrent_Neural_Networks.html">164 nips-2005-Representing Part-Whole Relationships in Recurrent Neural Networks</a></p>
<p>20 0.051103845 <a title="185-tfidf-20" href="./nips-2005-Metric_Learning_by_Collapsing_Classes.html">126 nips-2005-Metric Learning by Collapsing Classes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.164), (1, 0.049), (2, -0.025), (3, 0.018), (4, 0.017), (5, -0.053), (6, 0.092), (7, 0.156), (8, 0.051), (9, 0.075), (10, -0.058), (11, -0.072), (12, -0.082), (13, -0.185), (14, -0.174), (15, 0.082), (16, -0.024), (17, 0.01), (18, -0.002), (19, -0.053), (20, -0.05), (21, -0.186), (22, 0.026), (23, -0.129), (24, -0.01), (25, 0.026), (26, -0.051), (27, -0.032), (28, 0.004), (29, 0.122), (30, -0.022), (31, 0.131), (32, -0.031), (33, 0.028), (34, 0.063), (35, -0.055), (36, -0.06), (37, 0.061), (38, -0.142), (39, 0.1), (40, 0.147), (41, -0.148), (42, 0.043), (43, -0.046), (44, -0.03), (45, 0.076), (46, -0.04), (47, -0.051), (48, -0.035), (49, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95088798 <a title="185-lsi-1" href="./nips-2005-Subsequence_Kernels_for_Relation_Extraction.html">185 nips-2005-Subsequence Kernels for Relation Extraction</a></p>
<p>Author: Raymond J. Mooney, Razvan C. Bunescu</p><p>Abstract: We present a new kernel method for extracting semantic relations between entities in natural language text, based on a generalization of subsequence kernels. This kernel uses three types of subsequence patterns that are typically employed in natural language to assert relationships between two entities. Experiments on extracting protein interactions from biomedical corpora and top-level relations from newspaper corpora demonstrate the advantages of this approach. 1</p><p>2 0.70291215 <a title="185-lsi-2" href="./nips-2005-Group_and_Topic_Discovery_from_Relations_and_Their_Attributes.html">89 nips-2005-Group and Topic Discovery from Relations and Their Attributes</a></p>
<p>Author: Xuerui Wang, Natasha Mohanty, Andrew McCallum</p><p>Abstract: We present a probabilistic generative model of entity relationships and their attributes that simultaneously discovers groups among the entities and topics among the corresponding textual attributes. Block-models of relationship data have been studied in social network analysis for some time. Here we simultaneously cluster in several modalities at once, incorporating the attributes (here, words) associated with certain relationships. Signiﬁcantly, joint inference allows the discovery of topics to be guided by the emerging groups, and vice-versa. We present experimental results on two large data sets: sixteen years of bills put before the U.S. Senate, comprising their corresponding text and voting records, and thirteen years of similar data from the United Nations. We show that in comparison with traditional, separate latent-variable models for words, or Blockstructures for votes, the Group-Topic model’s joint inference discovers more cohesive groups and improved topics. 1</p><p>3 0.50529534 <a title="185-lsi-3" href="./nips-2005-Searching_for_Character_Models.html">171 nips-2005-Searching for Character Models</a></p>
<p>Author: Jaety Edwards, David Forsyth</p><p>Abstract: We introduce a method to automatically improve character models for a handwritten script without the use of transcriptions and using a minimum of document speciﬁc training data. We show that we can use searches for the words in a dictionary to identify portions of the document whose transcriptions are unambiguous. Using templates extracted from those regions, we retrain our character prediction model to drastically improve our search retrieval performance for words in the document.</p><p>4 0.48657015 <a title="185-lsi-4" href="./nips-2005-Sequence_and_Tree_Kernels_with_Statistical_Feature_Mining.html">175 nips-2005-Sequence and Tree Kernels with Statistical Feature Mining</a></p>
<p>Author: Jun Suzuki, Hideki Isozaki</p><p>Abstract: This paper proposes a new approach to feature selection based on a statistical feature mining technique for sequence and tree kernels. Since natural language data take discrete structures, convolution kernels, such as sequence and tree kernels, are advantageous for both the concept and accuracy of many natural language processing tasks. However, experiments have shown that the best results can only be achieved when limited small sub-structures are dealt with by these kernels. This paper discusses this issue of convolution kernels and then proposes a statistical feature selection that enable us to use larger sub-structures effectively. The proposed method, in order to execute efﬁciently, can be embedded into an original kernel calculation process by using sub-structure mining algorithms. Experiments on real NLP tasks conﬁrm the problem in the conventional method and compare the performance of a conventional method to that of the proposed method.</p><p>5 0.44721022 <a title="185-lsi-5" href="./nips-2005-Kernels_for_gene_regulatory_regions.html">103 nips-2005-Kernels for gene regulatory regions</a></p>
<p>Author: Jean-philippe Vert, Robert Thurman, William S. Noble</p><p>Abstract: We describe a hierarchy of motif-based kernels for multiple alignments of biological sequences, particularly suitable to process regulatory regions of genes. The kernels incorporate progressively more information, with the most complex kernel accounting for a multiple alignment of orthologous regions, the phylogenetic tree relating the species, and the prior knowledge that relevant sequence patterns occur in conserved motif blocks. These kernels can be used in the presence of a library of known transcription factor binding sites, or de novo by iterating over all k-mers of a given length. In the latter mode, a discriminative classiﬁer built from such a kernel not only recognizes a given class of promoter regions, but as a side effect simultaneously identiﬁes a collection of relevant, discriminative sequence motifs. We demonstrate the utility of the motif-based multiple alignment kernels by using a collection of aligned promoter regions from ﬁve yeast species to recognize classes of cell-cycle regulated genes. Supplementary data is available at http://noble.gs.washington.edu/proj/pkernel. 1</p><p>6 0.41637662 <a title="185-lsi-6" href="./nips-2005-Interpolating_between_types_and_tokens_by_estimating_power-law_generators.html">100 nips-2005-Interpolating between types and tokens by estimating power-law generators</a></p>
<p>7 0.41609916 <a title="185-lsi-7" href="./nips-2005-Dynamic_Social_Network_Analysis_using_Latent_Space_Models.html">60 nips-2005-Dynamic Social Network Analysis using Latent Space Models</a></p>
<p>8 0.3736369 <a title="185-lsi-8" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>9 0.35315636 <a title="185-lsi-9" href="./nips-2005-Computing_the_Solution_Path_for_the_Regularized_Support_Vector_Regression.html">44 nips-2005-Computing the Solution Path for the Regularized Support Vector Regression</a></p>
<p>10 0.35182816 <a title="185-lsi-10" href="./nips-2005-A_matching_pursuit_approach_to_sparse_Gaussian_process_regression.html">16 nips-2005-A matching pursuit approach to sparse Gaussian process regression</a></p>
<p>11 0.32598066 <a title="185-lsi-11" href="./nips-2005-Maximum_Margin_Semi-Supervised_Learning_for_Structured_Variables.html">123 nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</a></p>
<p>12 0.32363889 <a title="185-lsi-12" href="./nips-2005-Transfer_learning_for_text_classification.html">195 nips-2005-Transfer learning for text classification</a></p>
<p>13 0.31892332 <a title="185-lsi-13" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>14 0.31549963 <a title="185-lsi-14" href="./nips-2005-Context_as_Filtering.html">48 nips-2005-Context as Filtering</a></p>
<p>15 0.31464112 <a title="185-lsi-15" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>16 0.29775926 <a title="185-lsi-16" href="./nips-2005-Correlated_Topic_Models.html">52 nips-2005-Correlated Topic Models</a></p>
<p>17 0.28636655 <a title="185-lsi-17" href="./nips-2005-Fast_Krylov_Methods_for_N-Body_Learning.html">71 nips-2005-Fast Krylov Methods for N-Body Learning</a></p>
<p>18 0.27654952 <a title="185-lsi-18" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>19 0.26026469 <a title="185-lsi-19" href="./nips-2005-Logic_and_MRF_Circuitry_for_Labeling_Occluding_and_Thinline_Visual_Contours.html">122 nips-2005-Logic and MRF Circuitry for Labeling Occluding and Thinline Visual Contours</a></p>
<p>20 0.25784218 <a title="185-lsi-20" href="./nips-2005-Fast_Gaussian_Process_Regression_using_KD-Trees.html">69 nips-2005-Fast Gaussian Process Regression using KD-Trees</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.042), (10, 0.027), (11, 0.013), (27, 0.565), (31, 0.033), (34, 0.08), (41, 0.012), (55, 0.014), (69, 0.037), (73, 0.023), (88, 0.052), (91, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92117196 <a title="185-lda-1" href="./nips-2005-Subsequence_Kernels_for_Relation_Extraction.html">185 nips-2005-Subsequence Kernels for Relation Extraction</a></p>
<p>Author: Raymond J. Mooney, Razvan C. Bunescu</p><p>Abstract: We present a new kernel method for extracting semantic relations between entities in natural language text, based on a generalization of subsequence kernels. This kernel uses three types of subsequence patterns that are typically employed in natural language to assert relationships between two entities. Experiments on extracting protein interactions from biomedical corpora and top-level relations from newspaper corpora demonstrate the advantages of this approach. 1</p><p>2 0.90787768 <a title="185-lda-2" href="./nips-2005-Goal-Based_Imitation_as_Probabilistic_Inference_over_Graphical_Models.html">87 nips-2005-Goal-Based Imitation as Probabilistic Inference over Graphical Models</a></p>
<p>Author: Deepak Verma, Rajesh P. Rao</p><p>Abstract: Humans are extremely adept at learning new skills by imitating the actions of others. A progression of imitative abilities has been observed in children, ranging from imitation of simple body movements to goalbased imitation based on inferring intent. In this paper, we show that the problem of goal-based imitation can be formulated as one of inferring goals and selecting actions using a learned probabilistic graphical model of the environment. We ﬁrst describe algorithms for planning actions to achieve a goal state using probabilistic inference. We then describe how planning can be used to bootstrap the learning of goal-dependent policies by utilizing feedback from the environment. The resulting graphical model is then shown to be powerful enough to allow goal-based imitation. Using a simple maze navigation task, we illustrate how an agent can infer the goals of an observed teacher and imitate the teacher even when the goals are uncertain and the demonstration is incomplete.</p><p>3 0.90192199 <a title="185-lda-3" href="./nips-2005-Is_Early_Vision_Optimized_for_Extracting_Higher-order_Dependencies%3F.html">101 nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</a></p>
<p>Author: Yan Karklin, Michael S. Lewicki</p><p>Abstract: Linear implementations of the efﬁcient coding hypothesis, such as independent component analysis (ICA) and sparse coding models, have provided functional explanations for properties of simple cells in V1 [1, 2]. These models, however, ignore the non-linear behavior of neurons and fail to match individual and population properties of neural receptive ﬁelds in subtle but important ways. Hierarchical models, including Gaussian Scale Mixtures [3, 4] and other generative statistical models [5, 6], can capture higher-order regularities in natural images and explain nonlinear aspects of neural processing such as normalization and context effects [6,7]. Previously, it had been assumed that the lower level representation is independent of the hierarchy, and had been ﬁxed when training these models. Here we examine the optimal lower-level representations derived in the context of a hierarchical model and ﬁnd that the resulting representations are strikingly different from those based on linear models. Unlike the the basis functions and ﬁlters learned by ICA or sparse coding, these functions individually more closely resemble simple cell receptive ﬁelds and collectively span a broad range of spatial scales. Our work uniﬁes several related approaches and observations about natural image structure and suggests that hierarchical models might yield better representations of image structure throughout the hierarchy.</p><p>4 0.75364619 <a title="185-lda-4" href="./nips-2005-Predicting_EMG_Data_from_M1_Neurons_with_Variational_Bayesian_Least_Squares.html">155 nips-2005-Predicting EMG Data from M1 Neurons with Variational Bayesian Least Squares</a></p>
<p>Author: Jo-anne Ting, Aaron D'souza, Kenji Yamamoto, Toshinori Yoshioka, Donna Hoffman, Shinji Kakei, Lauren Sergio, John Kalaska, Mitsuo Kawato</p><p>Abstract: An increasing number of projects in neuroscience requires the statistical analysis of high dimensional data sets, as, for instance, in predicting behavior from neural ﬁring or in operating artiﬁcial devices from brain recordings in brain-machine interfaces. Linear analysis techniques remain prevalent in such cases, but classical linear regression approaches are often numerically too fragile in high dimensions. In this paper, we address the question of whether EMG data collected from arm movements of monkeys can be faithfully reconstructed with linear approaches from neural activity in primary motor cortex (M1). To achieve robust data analysis, we develop a full Bayesian approach to linear regression that automatically detects and excludes irrelevant features in the data, regularizing against overﬁtting. In comparison with ordinary least squares, stepwise regression, partial least squares, LASSO regression and a brute force combinatorial search for the most predictive input features in the data, we demonstrate that the new Bayesian method oﬀers a superior mixture of characteristics in terms of regularization against overﬁtting, computational eﬃciency and ease of use, demonstrating its potential as a drop-in replacement for other linear regression techniques. As neuroscientiﬁc results, our analyses demonstrate that EMG data can be well predicted from M1 neurons, further opening the path for possible real-time interfaces between brains and machines. 1</p><p>5 0.54493397 <a title="185-lda-5" href="./nips-2005-Learning_Cue-Invariant_Visual_Responses.html">109 nips-2005-Learning Cue-Invariant Visual Responses</a></p>
<p>Author: Jarmo Hurri</p><p>Abstract: Multiple visual cues are used by the visual system to analyze a scene; achromatic cues include luminance, texture, contrast and motion. Singlecell recordings have shown that the mammalian visual cortex contains neurons that respond similarly to scene structure (e.g., orientation of a boundary), regardless of the cue type conveying this information. This paper shows that cue-invariant response properties of simple- and complex-type cells can be learned from natural image data in an unsupervised manner. In order to do this, we also extend a previous conceptual model of cue invariance so that it can be applied to model simple- and complex-cell responses. Our results relate cue-invariant response properties to natural image statistics, thereby showing how the statistical modeling approach can be used to model processing beyond the elemental response properties visual neurons. This work also demonstrates how to learn, from natural image data, more sophisticated feature detectors than those based on changes in mean luminance, thereby paving the way for new data-driven approaches to image processing and computer vision. 1</p><p>6 0.50544173 <a title="185-lda-6" href="./nips-2005-Bayesian_models_of_human_action_understanding.html">36 nips-2005-Bayesian models of human action understanding</a></p>
<p>7 0.48813257 <a title="185-lda-7" href="./nips-2005-Interpolating_between_types_and_tokens_by_estimating_power-law_generators.html">100 nips-2005-Interpolating between types and tokens by estimating power-law generators</a></p>
<p>8 0.47495487 <a title="185-lda-8" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>9 0.46888831 <a title="185-lda-9" href="./nips-2005-Learning_to_Control_an_Octopus_Arm_with_Gaussian_Process_Temporal_Difference_Methods.html">119 nips-2005-Learning to Control an Octopus Arm with Gaussian Process Temporal Difference Methods</a></p>
<p>10 0.43821999 <a title="185-lda-10" href="./nips-2005-Visual_Encoding_with_Jittering_Eyes.html">203 nips-2005-Visual Encoding with Jittering Eyes</a></p>
<p>11 0.42672828 <a title="185-lda-11" href="./nips-2005-Scaling_Laws_in_Natural_Scenes_and_the_Inference_of_3D_Shape.html">170 nips-2005-Scaling Laws in Natural Scenes and the Inference of 3D Shape</a></p>
<p>12 0.42612457 <a title="185-lda-12" href="./nips-2005-Policy-Gradient_Methods_for_Planning.html">153 nips-2005-Policy-Gradient Methods for Planning</a></p>
<p>13 0.42552146 <a title="185-lda-13" href="./nips-2005-Products_of_%60%60Edge-perts.html">158 nips-2005-Products of ``Edge-perts</a></p>
<p>14 0.42121935 <a title="185-lda-14" href="./nips-2005-Phase_Synchrony_Rate_for_the_Recognition_of_Motor_Imagery_in_Brain-Computer_Interface.html">152 nips-2005-Phase Synchrony Rate for the Recognition of Motor Imagery in Brain-Computer Interface</a></p>
<p>15 0.41624719 <a title="185-lda-15" href="./nips-2005-Context_as_Filtering.html">48 nips-2005-Context as Filtering</a></p>
<p>16 0.41384178 <a title="185-lda-16" href="./nips-2005-Value_Function_Approximation_with_Diffusion_Wavelets_and_Laplacian_Eigenfunctions.html">199 nips-2005-Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions</a></p>
<p>17 0.40848228 <a title="185-lda-17" href="./nips-2005-Sensory_Adaptation_within_a_Bayesian_Framework_for_Perception.html">173 nips-2005-Sensory Adaptation within a Bayesian Framework for Perception</a></p>
<p>18 0.40623415 <a title="185-lda-18" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<p>19 0.38215226 <a title="185-lda-19" href="./nips-2005-Bayesian_model_learning_in_human_visual_perception.html">35 nips-2005-Bayesian model learning in human visual perception</a></p>
<p>20 0.37585673 <a title="185-lda-20" href="./nips-2005-Searching_for_Character_Models.html">171 nips-2005-Searching for Character Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
