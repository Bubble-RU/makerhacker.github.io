<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>81 nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-81" href="#">nips2005-81</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>81 nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</h1>
<br/><p>Source: <a title="nips-2005-81-pdf" href="http://papers.nips.cc/paper/2791-gaussian-processes-for-multiuser-detection-in-cdma-receivers.pdf">pdf</a></p><p>Author: Juan J. Murillo-fuentes, Sebastian Caro, Fernando Pérez-Cruz</p><p>Abstract: In this paper we propose a new receiver for digital communications. We focus on the application of Gaussian Processes (GPs) to the multiuser detection (MUD) in code division multiple access (CDMA) systems to solve the near-far problem. Hence, we aim to reduce the interference from other users sharing the same frequency band. While usual approaches minimize the mean square error (MMSE) to linearly retrieve the user of interest, we exploit the same criteria but in the design of a nonlinear MUD. Since the optimal solution is known to be nonlinear, the performance of this novel method clearly improves that of the MMSE detectors. Furthermore, the GP based MUD achieves excellent interference suppression even for short training sequences. We also include some experiments to illustrate that other nonlinear detectors such as those based on Support Vector Machines (SVMs) exhibit a worse performance. 1</p><p>Reference: <a title="nips-2005-81-reference" href="../nips2005_reference/nips-2005-Gaussian_Processes_for_Multiuser_Detection_in_CDMA_receivers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract In this paper we propose a new receiver for digital communications. [sent-6, score-0.357]
</p><p>2 We focus on the application of Gaussian Processes (GPs) to the multiuser detection (MUD) in code division multiple access (CDMA) systems to solve the near-far problem. [sent-7, score-0.367]
</p><p>3 Hence, we aim to reduce the interference from other users sharing the same frequency band. [sent-8, score-0.488]
</p><p>4 While usual approaches minimize the mean square error (MMSE) to linearly retrieve the user of interest, we exploit the same criteria but in the design of a nonlinear MUD. [sent-9, score-0.372]
</p><p>5 Furthermore, the GP based MUD achieves excellent interference suppression even for short training sequences. [sent-11, score-0.227]
</p><p>6 We also include some experiments to illustrate that other nonlinear detectors such as those based on Support Vector Machines (SVMs) exhibit a worse performance. [sent-12, score-0.193]
</p><p>7 1  Introduction  One of the major issues in present wireless communications is how users share the resources. [sent-13, score-0.461]
</p><p>8 Code division multiple access (CDMA) is one of the techniques exploited in third generation communications systems and is to be employed in the next generation. [sent-15, score-0.152]
</p><p>9 In CDMA each user uses direct sequence spread spectrum (DS-SS) to modulate its bits with an assigned code, spreading them over the entire frequency band. [sent-16, score-0.52]
</p><p>10 While typical receivers deal only with interferences and noise intrinsic to the channel (i. [sent-17, score-0.182]
</p><p>11 Inter-Symbolic Interference, intermodulation products, spurious frequencies, and thermal noise), in CDMA we also have interference produced by other users accessing the channel at the same time. [sent-19, score-0.535]
</p><p>12 Interference limitation due to the simultaneous access of multiple users systems has been the stimulus to the development of a powerful family of Signal Processing techniques, namely Multiuser Detection (MUD). [sent-20, score-0.396]
</p><p>13 In CDMA, we face the retrieval of a given user, the user of interest (UOI), with the knowledge of its associated code or even the whole set of users codes. [sent-24, score-0.569]
</p><p>14 Hence, we face the suppression of interference due to others users. [sent-25, score-0.137]
</p><p>15 If all users transmit with the same power,  bt(1) h1(z)  M  Channel  bt(2) M  h2(z)  . [sent-26, score-0.362]
</p><p>16 M  Noise nt  hK(z)  MUD  C(z)  Chip rate sampler  bt(K)  Code filters  Figure 1: Synchronous CDMA system  but the UOI is far from the receiver, most users reach the receiver with a larger amplitude, making it more difﬁcult to detect the bits of the UOI. [sent-32, score-0.86]
</p><p>17 Simple detectors can be designed by minimizing the mean square error (MMSE) to linearly retrieve the user of interest [5]. [sent-34, score-0.336]
</p><p>18 However, these detectors need large sequences of training data. [sent-35, score-0.213]
</p><p>19 This solution need very long training sequences (a few hundreds bits) and they are only tested in toy examples with very few users and short spreading sequences (the code for each user). [sent-40, score-0.934]
</p><p>20 In this paper, we will present a multiuser detector based on Gaussian Processes [7]. [sent-41, score-0.327]
</p><p>21 The MUD detector is inspired by the linear MMSE criteria, which can be interpreted as a Bayesian linear regressor. [sent-42, score-0.166]
</p><p>22 In this sense, we can extend the linear MMSE criteria to nonlinear decision functions using the same ideas developed in [6] to present Gaussian Processes for regression. [sent-43, score-0.176]
</p><p>23 In Section 2, we present the multiuser detection problem in CDMA communication systems and the widely used minimum mean square error receiver. [sent-45, score-0.368]
</p><p>24 We propose a nonlinear receiver based on Gaussian Processes in Section 3. [sent-46, score-0.409]
</p><p>25 Section 4 is devoted to show, through computer experiments, the advantages of the GP-MUD receiver with short training sequences. [sent-47, score-0.41]
</p><p>26 We compare it to the linear MMSE and the nonlinear SVM MUD. [sent-48, score-0.114]
</p><p>27 2  CDMA Communication System Model and MUD  Consider a synchronous CDMA digital communication system [5] as depicted in Figure 1. [sent-50, score-0.144]
</p><p>28 Each transmitted bit is upsampled and multiplied by the users’ spreading codes and then the chips for each bit are transmitted into the channel (each element of the spreading code is either +1 or −1 and they are known as chips). [sent-52, score-1.075]
</p><p>29 The channel is assumed to be linear and noisy, therefore the chips from different users are added together, plus Gaussian noise. [sent-53, score-0.558]
</p><p>30 Hence, the MUD has to recover from these chips the bits corresponding to each user. [sent-54, score-0.204]
</p><p>31 At each time step t, the signal in the receiver can be represented  in matrix notation as: xt = HAbt + nt  (1)  where bt is a column vector that contains the bits (+1 or −1) for the K users at time k. [sent-55, score-1.062]
</p><p>32 The K × K diagonal matrix A contains the amplitude of each user, which represents the attenuation that each user’s transmission suffers through the channel (this attenuation depends on the distance between the user and the receiver). [sent-56, score-0.3]
</p><p>33 H is an L × K matrix which contains in each column the L-dimensional spreading code for each of the K users. [sent-57, score-0.306]
</p><p>34 The spreading codes are designed to present a low cross-correlation between them and between any shifted version of the codes, to guarantee that the bits from each user can be readily recovered. [sent-58, score-0.599]
</p><p>35 The codes are known as spreading sequences, because they augment the occupied bandwidth of the transmitted signal by L. [sent-59, score-0.451]
</p><p>36 Finally, xt represents the L received chips to which Gaussian noise has been added, which is denoted by nt . [sent-60, score-0.364]
</p><p>37 At reception, we aim to estimate the original transmitted symbols of any user i, bt (i), hereafter the user of interest. [sent-61, score-0.478]
</p><p>38 Linear MUDs estimate these bits as ˆ bt (i) = sgn{w⊤ xt } i  (2)  The matched ﬁlter (MF) wi = hi , a simple correlation between xt and the ith spreading code, is the optimal receiver if there were no additional users in the system, i. [sent-62, score-1.465]
</p><p>39 the received signal is only corrupted by Gaussian noise. [sent-64, score-0.164]
</p><p>40 While the optimal solution is known to be nonlinear [5], some linear receivers such as the minimum mean square error (MMSE) present good performances and are used in practice. [sent-66, score-0.281]
</p><p>41 The MMSE receiver for the ith user solves: ∗ ⊤ ⊤ wi = arg min E (bt (i) − wi xt )2 = arg min E (bt (i) − wi (HAbt + ν k ))2 wi  (3)  wi  where wi represents the decision function of the linear classiﬁer. [sent-67, score-1.232]
</p><p>42 We can derive the MMSE receiver by taking derivatives with respect to wi and equating to zero, obtaining: M wi M SEde = R−1 hi xx  (4)  where Rxx = E[xt x⊤ ] is the correlation between the received vectors and hi represents the t spreading sequence of the UOI. [sent-68, score-0.938]
</p><p>43 This receiver is known as the decentralized MMSE receiver as it can be implemented without knowing the spreading sequences of the remaining users. [sent-69, score-1.087]
</p><p>44 Its main limitation is its performance, which is very low even for high signal to noise ratio, and it needs many examples (thousands) before it can recover the received symbols. [sent-70, score-0.204]
</p><p>45 If the spreading codes of all the users are available, as in the base station, this information can be used to improve the performance of the MMSE detector. [sent-71, score-0.715]
</p><p>46 The vector z k is the matched-ﬁlter output for each user and it reduces the dimensionality of our problem from the number of chips L to the number of users K, which is signiﬁcantly lower in most applications. [sent-73, score-0.612]
</p><p>47 In this case the receiver is known as the centralized detector and it is deﬁned as: M wi M SEcent = HR−1 H ⊤ hi (5) zz where Rzz = E[z t z ⊤ ] is the correlation matrix of the received chips after the MFs. [sent-74, score-0.964]
</p><p>48 t These MUDs have good convergence properties and do not need a training sequence to decode the received bits, but they need large training sequences before their probability of error is low. [sent-75, score-0.336]
</p><p>49 Therefore the initially received bits will present a very high probability of error that will make impossible to send any information on them. [sent-76, score-0.234]
</p><p>50 Some improvements can be achieved by using higher order statistics [2], but still the training sequences are not short enough for most applications. [sent-77, score-0.147]
</p><p>51 3  Gaussian Processes for Multiuser Detection  The MMSE detector minimizes the functional in (3), which gives the best linear classiﬁer. [sent-78, score-0.141]
</p><p>52 As we know, the optimal classiﬁer is nonlinear [5], and the MMSE criteria can be readily extended to provide nonlinear models by mapping the received chips to a higher dimensional space. [sent-79, score-0.466]
</p><p>53 In this case we will need to solve: N ∗ wi = arg min  ⊤ bt (i) − wi φ(xt )  wi  2  + λ wi  2  (6)  k=1  in which we have changed the expectation by the empirical mean over a training set and we have incorporated a regularizer to avoid overﬁtting. [sent-80, score-0.617]
</p><p>54 φ(·) represents the nonlinear mapping of the received chips. [sent-81, score-0.195]
</p><p>55 The wi that minimizes (6) can be interpreted as the mode of the parameters in a Bayesian linear regressor, as noted in [6], and since the likelihood and the prior are both Gaussians, so it will be the posterior. [sent-82, score-0.125]
</p><p>56 For any received symbol x∗ , we know that it will be distributed as a Gaussian with mean: 1 µ(x∗ ) = φ⊤ (x∗ )A−1 Φ⊤ b (7) λ and variance σ 2 (x∗ ) = φ⊤ (x∗ )A−1 φ(x∗ ) (8) where Φ = [φ(x1 ), φ(x2 ), . [sent-83, score-0.133]
</p><p>57 The kernel that we will use in our experiments are: k(xt , xℓ ) = eθ[1] exp(−eθ[4] xt − xℓ 2 ) + eθ[3] x⊤ xℓ + eθ[2] δr,ℓ t  (12)  The covariance function in (12) is a good kernel for solving the GP-MUD, because it contains a linear and a nonlinear part. [sent-94, score-0.233]
</p><p>58 The optimal decision surface for MUD is nonlinear, unless the spreading codes are orthogonal to each other, and its deviation from the linear solution depends on how strong the correlations between codes are. [sent-95, score-0.551]
</p><p>59 In most cases, a linear detector is very close to the optimal decision surface, as spreading codes are almost orthogonal, and only a minor correction is needed to achieve the optimal decision boundary. [sent-96, score-0.61]
</p><p>60 The linear part can mimic the best linear decision boundary and the nonlinear part modiﬁes it, where the linear explanation is not optimal. [sent-98, score-0.195]
</p><p>61 Also using a radial basis kernel for the nonlinear part is a good choice to achieve nonlinear decisions. [sent-99, score-0.236]
</p><p>62 Because, the received chips form a constellation of 2K clouds of points with Gaussian spread around its centres. [sent-100, score-0.243]
</p><p>63 Picturing the receiver as a Gaussian Process for regression, instead of a Regularised Least Square functional, allows us to either obtain the hyperparameters by maximizing the likelihood or marginalised them out using Monte Carlo techniques, as explained in [6]. [sent-101, score-0.32]
</p><p>64 The powers of the interfering users is distributed homogeneously between 0 and 30 dB above that of the UOI. [sent-103, score-0.51]
</p><p>65 We have just shown above how we can make predictions in the nonlinear case (9) using the received symbols from the channel. [sent-105, score-0.216]
</p><p>66 In an analogy with the MMSE receiver, this will correspond to the decentralized GP-MUD detector as we will not need to know the other users’ codes to detect the bits sent to us. [sent-106, score-0.52]
</p><p>67 It is also relevant to notice that we do not need our spreading code for detection, as the decentralized MMSE detector did. [sent-107, score-0.588]
</p><p>68 We can also obtain a centralized GP-MUD detector using as input vectors z t = H ⊤ xt . [sent-108, score-0.374]
</p><p>69 4  Experiments  In this section we include the typical evaluation of the performance in a digital communications system, i. [sent-109, score-0.13]
</p><p>70 The test environment is a synchronous CDMA system in which the users are spread using Gold sequences with spreading factor L = 31 and K = 8 users, which are typical values in CDMA based mobile communication systems. [sent-112, score-0.84]
</p><p>71 These amplitudes are random values to achieve an interferer to signal ratio of 30 dB. [sent-114, score-0.11]
</p><p>72 We study the worse scenario and hence we will detect the user which arrives to the receiver with the lowest amplitude. [sent-116, score-0.505]
</p><p>73 We compare the performance of the GP centralized and decentralized MUDs to the performance of the MMSE detectors, the Matched Filter detector and the (centralized) SVMMUD in [4]. [sent-117, score-0.479]
</p><p>74 The SVM-MUD detector uses a Gaussian kernel and its width is adapted incorporating knowledge of the noise variance in the channel. [sent-118, score-0.178]
</p><p>75 The powers of the interfering users is distributed homogeneously between 0 and 30 dB above that of the UOI. [sent-120, score-0.51]
</p><p>76 We believe this might be due to either the reduced number of users in their experiments (2 or 3) or because they used the same amplitude for all the users, so they did not encounter the near-far problem. [sent-122, score-0.397]
</p><p>77 The results in Figure 2 show that the detectors based on GPs are able to reduce the probability of error as the signal to noise ratio in the channel decreases with only 30 samples in the training sequence. [sent-126, score-0.401]
</p><p>78 5-2dB worse than the best achievable probability of error, which is obtained in absence of interference (indicated by the dashed line). [sent-128, score-0.127]
</p><p>79 The GP decentralized MUD reduces the probability of error as the signal to noise increases, but it remains between 3-4dB from the optimal performance. [sent-129, score-0.3]
</p><p>80 The other detectors are not able to decrease the BER even for a very high signal to noise ratio in the channel. [sent-130, score-0.235]
</p><p>81 These ﬁgures show that the GP based MUD can outperform the other MUD when very short training sequences are available. [sent-131, score-0.147]
</p><p>82 Figure 3 highlights that the SVM-MUD (centralized) and the MSSE centralized detectors are able to reduce the BER as the SNR increases, but they are still far from the performance of the GP-MUD. [sent-132, score-0.321]
</p><p>83 The centralized GP-MUD basically provides optimal performance as it is less than 0. [sent-133, score-0.227]
</p><p>84 The decentralized GP-MUD outperforms the other two centralized detectors (SVM and MMSE) since it is able to provide lower BER without needing to know the code of the remaining users. [sent-135, score-0.52]
</p><p>85 The powers of the interfering users is distributed homogeneously between 0 and 30 dB above that of the UOI. [sent-137, score-0.51]
</p><p>86 In this case, the centralized GP-MUD lies above the optimal BER curve and the decentralized GP-MUD performs as the SVM-MUD detector. [sent-139, score-0.356]
</p><p>87 The centralized MMSE detector still presents very high probability of error for high signal to noise ratios and we need over 500 samples to obtain a performance similar to the centralized GP with 80 samples. [sent-140, score-0.646]
</p><p>88 For 160 samples the MMSE decentralized is already able to slightly reduce the bit error rate for very high signal to noise ratios. [sent-141, score-0.384]
</p><p>89 But to achieve the performance showed by the decentralized GP-MUD it needs several thousands samples. [sent-142, score-0.198]
</p><p>90 Since the optimal solution is known to be nonlinear the Gaussian Processes are able to obtain this nonlinear decision surface with very few training examples. [sent-144, score-0.346]
</p><p>91 This is the main advantage of this method as it only requires a few tens training examples instead of the few hundreds needed by other nonlinear techniques as SVMs. [sent-145, score-0.182]
</p><p>92 This will allow its application in real communication systems, as training sequence of 26 samples are typically used in the GSM standard for mobile Telecommunications. [sent-146, score-0.133]
</p><p>93 The most relevant result of this paper is the performance shown by the decentralized GPMUD receiver, since it can be directly used over any CDMA system. [sent-147, score-0.163]
</p><p>94 The decentralized GP-MUD receiver does not need to know the codes from the other users and does not require the users to be aligned, as the other methods do. [sent-148, score-1.329]
</p><p>95 While the other receiver will degrade its performance if the users are not aligned, the decentralized GP-MUD receiver will not, providing a more robust solution to the near far problem. [sent-149, score-1.185]
</p><p>96 We have left for further work a more extensive set of experiments changing other parameters of the system such as: the number of users, the length of the spreading code, and the interferences with other users. [sent-151, score-0.298]
</p><p>97 But still, we believe the reported results are signiﬁcant since we obtain low bit error rates for training sequences as short as 30 bits. [sent-152, score-0.25]
</p><p>98 Neural networks for multiuser detection in codedivision multiple-access communications. [sent-161, score-0.248]
</p><p>99 Support vector machine multiuser receiver for DS-CDMA signals in multipath channels. [sent-180, score-0.531]
</p><p>100 Prediction with gaussian processes: From linear regression to linear prediction and beyond. [sent-188, score-0.108]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mmse', 0.364), ('users', 0.362), ('receiver', 0.32), ('cdma', 0.307), ('mud', 0.249), ('spreading', 0.244), ('multiuser', 0.211), ('ber', 0.184), ('centralized', 0.183), ('decentralized', 0.146), ('user', 0.145), ('detector', 0.116), ('bt', 0.11), ('interference', 0.107), ('received', 0.106), ('chips', 0.105), ('wi', 0.1), ('bits', 0.099), ('codes', 0.092), ('nonlinear', 0.089), ('detectors', 0.084), ('snr', 0.079), ('communications', 0.076), ('xt', 0.075), ('bit', 0.074), ('db', 0.07), ('mf', 0.067), ('channel', 0.066), ('gp', 0.064), ('code', 0.062), ('signal', 0.058), ('fernando', 0.058), ('homogeneously', 0.058), ('mmsecentralized', 0.058), ('muds', 0.058), ('uoi', 0.058), ('sequences', 0.057), ('transmitted', 0.057), ('training', 0.052), ('interfering', 0.05), ('gps', 0.044), ('communication', 0.043), ('receivers', 0.043), ('synchronous', 0.043), ('processes', 0.041), ('noise', 0.04), ('powers', 0.04), ('gaussian', 0.039), ('habt', 0.038), ('juan', 0.038), ('nt', 0.038), ('mobile', 0.038), ('short', 0.038), ('digital', 0.037), ('detection', 0.037), ('amplitude', 0.035), ('ratio', 0.035), ('access', 0.034), ('hi', 0.034), ('spanish', 0.033), ('interferences', 0.033), ('spread', 0.032), ('square', 0.031), ('decision', 0.031), ('criteria', 0.031), ('suppression', 0.03), ('retrieve', 0.03), ('error', 0.029), ('vs', 0.028), ('know', 0.027), ('attenuation', 0.027), ('optimal', 0.027), ('linear', 0.025), ('ministry', 0.024), ('division', 0.023), ('wireless', 0.023), ('svm', 0.022), ('kernel', 0.022), ('hundreds', 0.022), ('system', 0.021), ('symbols', 0.021), ('solution', 0.02), ('surface', 0.02), ('detect', 0.02), ('worse', 0.02), ('need', 0.02), ('reduce', 0.019), ('matched', 0.019), ('readily', 0.019), ('radial', 0.019), ('techniques', 0.019), ('regression', 0.019), ('thousands', 0.018), ('able', 0.018), ('arg', 0.018), ('aligned', 0.018), ('versus', 0.017), ('achieve', 0.017), ('mean', 0.017), ('performance', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="81-tfidf-1" href="./nips-2005-Gaussian_Processes_for_Multiuser_Detection_in_CDMA_receivers.html">81 nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</a></p>
<p>Author: Juan J. Murillo-fuentes, Sebastian Caro, Fernando Pérez-Cruz</p><p>Abstract: In this paper we propose a new receiver for digital communications. We focus on the application of Gaussian Processes (GPs) to the multiuser detection (MUD) in code division multiple access (CDMA) systems to solve the near-far problem. Hence, we aim to reduce the interference from other users sharing the same frequency band. While usual approaches minimize the mean square error (MMSE) to linearly retrieve the user of interest, we exploit the same criteria but in the design of a nonlinear MUD. Since the optimal solution is known to be nonlinear, the performance of this novel method clearly improves that of the MMSE detectors. Furthermore, the GP based MUD achieves excellent interference suppression even for short training sequences. We also include some experiments to illustrate that other nonlinear detectors such as those based on Support Vector Machines (SVMs) exhibit a worse performance. 1</p><p>2 0.069983192 <a title="81-tfidf-2" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>Author: Eizaburo Doi, Doru C. Balcan, Michael S. Lewicki</p><p>Abstract: Biological sensory systems are faced with the problem of encoding a high-ﬁdelity sensory signal with a population of noisy, low-ﬁdelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and twodimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of highdimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets. 1</p><p>3 0.059585314 <a title="81-tfidf-3" href="./nips-2005-Rate_Distortion_Codes_in_Sensor_Networks%3A_A_System-level_Analysis.html">162 nips-2005-Rate Distortion Codes in Sensor Networks: A System-level Analysis</a></p>
<p>Author: Tatsuto Murayama, Peter Davis</p><p>Abstract: This paper provides a system-level analysis of a scalable distributed sensing model for networked sensors. In our system model, a data center acquires data from a bunch of L sensors which each independently encode their noisy observations of an original binary sequence, and transmit their encoded data sequences to the data center at a combined rate R, which is limited. Supposing that the sensors use independent LDGM rate distortion codes, we show that the system performance can be evaluated for any given ﬁnite R when the number of sensors L goes to inﬁnity. The analysis shows how the optimal strategy for the distributed sensing problem changes at critical values of the data rate R or the noise level. 1</p><p>4 0.055732213 <a title="81-tfidf-4" href="./nips-2005-Sparse_Gaussian_Processes_using_Pseudo-inputs.html">179 nips-2005-Sparse Gaussian Processes using Pseudo-inputs</a></p>
<p>Author: Edward Snelson, Zoubin Ghahramani</p><p>Abstract: We present a new Gaussian process (GP) regression model whose covariance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M 2 N ) training cost and O(M 2 ) prediction cost per test case. We also ﬁnd hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We ﬁnally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M , i.e. very sparse solutions, and it signiﬁcantly outperforms other approaches in this regime. 1</p><p>5 0.055460334 <a title="81-tfidf-5" href="./nips-2005-Gaussian_Process_Dynamical_Models.html">80 nips-2005-Gaussian Process Dynamical Models</a></p>
<p>Author: Jack Wang, Aaron Hertzmann, David M. Blei</p><p>Abstract: This paper introduces Gaussian Process Dynamical Models (GPDM) for nonlinear time series analysis. A GPDM comprises a low-dimensional latent space with associated dynamics, and a map from the latent space to an observation space. We marginalize out the model parameters in closed-form, using Gaussian Process (GP) priors for both the dynamics and the observation mappings. This results in a nonparametric model for dynamical systems that accounts for uncertainty in the model. We demonstrate the approach on human motion capture data in which each pose is 62-dimensional. Despite the use of small data sets, the GPDM learns an effective representation of the nonlinear dynamics in these spaces. Webpage: http://www.dgp.toronto.edu/∼ jmwang/gpdm/ 1</p><p>6 0.055032641 <a title="81-tfidf-6" href="./nips-2005-An_Alternative_Infinite_Mixture_Of_Gaussian_Process_Experts.html">21 nips-2005-An Alternative Infinite Mixture Of Gaussian Process Experts</a></p>
<p>7 0.051940896 <a title="81-tfidf-7" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>8 0.048516288 <a title="81-tfidf-8" href="./nips-2005-Stimulus_Evoked_Independent_Factor_Analysis_of_MEG_Data_with_Large_Background_Activity.html">183 nips-2005-Stimulus Evoked Independent Factor Analysis of MEG Data with Large Background Activity</a></p>
<p>9 0.048306514 <a title="81-tfidf-9" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>10 0.047243662 <a title="81-tfidf-10" href="./nips-2005-Neuronal_Fiber_Delineation_in_Area_of_Edema_from_Diffusion_Weighted_MRI.html">135 nips-2005-Neuronal Fiber Delineation in Area of Edema from Diffusion Weighted MRI</a></p>
<p>11 0.046772711 <a title="81-tfidf-11" href="./nips-2005-Dynamic_Social_Network_Analysis_using_Latent_Space_Models.html">60 nips-2005-Dynamic Social Network Analysis using Latent Space Models</a></p>
<p>12 0.041995455 <a title="81-tfidf-12" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>13 0.041452508 <a title="81-tfidf-13" href="./nips-2005-Noise_and_the_two-thirds_power_Law.html">136 nips-2005-Noise and the two-thirds power Law</a></p>
<p>14 0.041387409 <a title="81-tfidf-14" href="./nips-2005-Large-scale_biophysical_parameter_estimation_in_single_neurons_via_constrained_linear_regression.html">106 nips-2005-Large-scale biophysical parameter estimation in single neurons via constrained linear regression</a></p>
<p>15 0.038896166 <a title="81-tfidf-15" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>16 0.038557936 <a title="81-tfidf-16" href="./nips-2005-Robust_design_of_biological_experiments.html">167 nips-2005-Robust design of biological experiments</a></p>
<p>17 0.036659315 <a title="81-tfidf-17" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>18 0.034135677 <a title="81-tfidf-18" href="./nips-2005-A_matching_pursuit_approach_to_sparse_Gaussian_process_regression.html">16 nips-2005-A matching pursuit approach to sparse Gaussian process regression</a></p>
<p>19 0.03353313 <a title="81-tfidf-19" href="./nips-2005-Non-iterative_Estimation_with_Perturbed_Gaussian_Markov_Processes.html">139 nips-2005-Non-iterative Estimation with Perturbed Gaussian Markov Processes</a></p>
<p>20 0.033051025 <a title="81-tfidf-20" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.127), (1, 0.006), (2, -0.018), (3, 0.02), (4, 0.036), (5, -0.024), (6, -0.004), (7, -0.05), (8, 0.065), (9, 0.056), (10, -0.056), (11, 0.013), (12, -0.024), (13, 0.019), (14, 0.049), (15, -0.001), (16, -0.063), (17, -0.032), (18, 0.046), (19, -0.078), (20, -0.045), (21, 0.012), (22, 0.048), (23, 0.029), (24, -0.084), (25, -0.004), (26, -0.03), (27, -0.04), (28, 0.068), (29, -0.011), (30, -0.053), (31, -0.067), (32, -0.076), (33, -0.035), (34, -0.026), (35, 0.013), (36, 0.031), (37, 0.078), (38, -0.033), (39, -0.053), (40, 0.077), (41, 0.062), (42, -0.021), (43, -0.17), (44, 0.114), (45, 0.122), (46, -0.054), (47, 0.003), (48, -0.072), (49, -0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92521161 <a title="81-lsi-1" href="./nips-2005-Gaussian_Processes_for_Multiuser_Detection_in_CDMA_receivers.html">81 nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</a></p>
<p>Author: Juan J. Murillo-fuentes, Sebastian Caro, Fernando Pérez-Cruz</p><p>Abstract: In this paper we propose a new receiver for digital communications. We focus on the application of Gaussian Processes (GPs) to the multiuser detection (MUD) in code division multiple access (CDMA) systems to solve the near-far problem. Hence, we aim to reduce the interference from other users sharing the same frequency band. While usual approaches minimize the mean square error (MMSE) to linearly retrieve the user of interest, we exploit the same criteria but in the design of a nonlinear MUD. Since the optimal solution is known to be nonlinear, the performance of this novel method clearly improves that of the MMSE detectors. Furthermore, the GP based MUD achieves excellent interference suppression even for short training sequences. We also include some experiments to illustrate that other nonlinear detectors such as those based on Support Vector Machines (SVMs) exhibit a worse performance. 1</p><p>2 0.52894658 <a title="81-lsi-2" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>Author: Eizaburo Doi, Doru C. Balcan, Michael S. Lewicki</p><p>Abstract: Biological sensory systems are faced with the problem of encoding a high-ﬁdelity sensory signal with a population of noisy, low-ﬁdelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and twodimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of highdimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets. 1</p><p>3 0.50141633 <a title="81-lsi-3" href="./nips-2005-Rate_Distortion_Codes_in_Sensor_Networks%3A_A_System-level_Analysis.html">162 nips-2005-Rate Distortion Codes in Sensor Networks: A System-level Analysis</a></p>
<p>Author: Tatsuto Murayama, Peter Davis</p><p>Abstract: This paper provides a system-level analysis of a scalable distributed sensing model for networked sensors. In our system model, a data center acquires data from a bunch of L sensors which each independently encode their noisy observations of an original binary sequence, and transmit their encoded data sequences to the data center at a combined rate R, which is limited. Supposing that the sensors use independent LDGM rate distortion codes, we show that the system performance can be evaluated for any given ﬁnite R when the number of sensors L goes to inﬁnity. The analysis shows how the optimal strategy for the distributed sensing problem changes at critical values of the data rate R or the noise level. 1</p><p>4 0.47317153 <a title="81-lsi-4" href="./nips-2005-Large-scale_biophysical_parameter_estimation_in_single_neurons_via_constrained_linear_regression.html">106 nips-2005-Large-scale biophysical parameter estimation in single neurons via constrained linear regression</a></p>
<p>Author: Misha Ahrens, Liam Paninski, Quentin J. Huys</p><p>Abstract: Our understanding of the input-output function of single cells has been substantially advanced by biophysically accurate multi-compartmental models. The large number of parameters needing hand tuning in these models has, however, somewhat hampered their applicability and interpretability. Here we propose a simple and well-founded method for automatic estimation of many of these key parameters: 1) the spatial distribution of channel densities on the cell’s membrane; 2) the spatiotemporal pattern of synaptic input; 3) the channels’ reversal potentials; 4) the intercompartmental conductances; and 5) the noise level in each compartment. We assume experimental access to: a) the spatiotemporal voltage signal in the dendrite (or some contiguous subpart thereof, e.g. via voltage sensitive imaging techniques), b) an approximate kinetic description of the channels and synapses present in each compartment, and c) the morphology of the part of the neuron under investigation. The key observation is that, given data a)-c), all of the parameters 1)-4) may be simultaneously inferred by a version of constrained linear regression; this regression, in turn, is efﬁciently solved using standard algorithms, without any “local minima” problems despite the large number of parameters and complex dynamics. The noise level 5) may also be estimated by standard techniques. We demonstrate the method’s accuracy on several model datasets, and describe techniques for quantifying the uncertainty in our estimates. 1</p><p>5 0.40628701 <a title="81-lsi-5" href="./nips-2005-Factorial_Switching_Kalman_Filters_for_Condition_Monitoring_in_Neonatal_Intensive_Care.html">68 nips-2005-Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care</a></p>
<p>Author: Christopher Williams, John Quinn, Neil Mcintosh</p><p>Abstract: The observed physiological dynamics of an infant receiving intensive care are affected by many possible factors, including interventions to the baby, the operation of the monitoring equipment and the state of health. The Factorial Switching Kalman Filter can be used to infer the presence of such factors from a sequence of observations, and to estimate the true values where these observations have been corrupted. We apply this model to clinical time series data and show it to be effective in identifying a number of artifactual and physiological patterns. 1</p><p>6 0.37553531 <a title="81-lsi-6" href="./nips-2005-Stimulus_Evoked_Independent_Factor_Analysis_of_MEG_Data_with_Large_Background_Activity.html">183 nips-2005-Stimulus Evoked Independent Factor Analysis of MEG Data with Large Background Activity</a></p>
<p>7 0.37343067 <a title="81-lsi-7" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>8 0.36245707 <a title="81-lsi-8" href="./nips-2005-Sparse_Gaussian_Processes_using_Pseudo-inputs.html">179 nips-2005-Sparse Gaussian Processes using Pseudo-inputs</a></p>
<p>9 0.34978351 <a title="81-lsi-9" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>10 0.34382811 <a title="81-lsi-10" href="./nips-2005-Gaussian_Process_Dynamical_Models.html">80 nips-2005-Gaussian Process Dynamical Models</a></p>
<p>11 0.34082627 <a title="81-lsi-11" href="./nips-2005-A_matching_pursuit_approach_to_sparse_Gaussian_process_regression.html">16 nips-2005-A matching pursuit approach to sparse Gaussian process regression</a></p>
<p>12 0.34000796 <a title="81-lsi-12" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>13 0.33242655 <a title="81-lsi-13" href="./nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">113 nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<p>14 0.31845289 <a title="81-lsi-14" href="./nips-2005-Dynamic_Social_Network_Analysis_using_Latent_Space_Models.html">60 nips-2005-Dynamic Social Network Analysis using Latent Space Models</a></p>
<p>15 0.31644568 <a title="81-lsi-15" href="./nips-2005-The_Forgetron%3A_A_Kernel-Based_Perceptron_on_a_Fixed_Budget.html">191 nips-2005-The Forgetron: A Kernel-Based Perceptron on a Fixed Budget</a></p>
<p>16 0.31215924 <a title="81-lsi-16" href="./nips-2005-An_Analog_Visual_Pre-Processing_Processor_Employing_Cyclic_Line_Access_in_Only-Nearest-Neighbor-Interconnects_Architecture.html">22 nips-2005-An Analog Visual Pre-Processing Processor Employing Cyclic Line Access in Only-Nearest-Neighbor-Interconnects Architecture</a></p>
<p>17 0.31028715 <a title="81-lsi-17" href="./nips-2005-Worst-Case_Bounds_for_Gaussian_Process_Models.html">205 nips-2005-Worst-Case Bounds for Gaussian Process Models</a></p>
<p>18 0.31014729 <a title="81-lsi-18" href="./nips-2005-Robust_design_of_biological_experiments.html">167 nips-2005-Robust design of biological experiments</a></p>
<p>19 0.30210435 <a title="81-lsi-19" href="./nips-2005-Computing_the_Solution_Path_for_the_Regularized_Support_Vector_Regression.html">44 nips-2005-Computing the Solution Path for the Regularized Support Vector Regression</a></p>
<p>20 0.29920071 <a title="81-lsi-20" href="./nips-2005-An_Approximate_Inference_Approach_for_the_PCA_Reconstruction_Error.html">24 nips-2005-An Approximate Inference Approach for the PCA Reconstruction Error</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.059), (10, 0.026), (22, 0.394), (27, 0.018), (31, 0.044), (34, 0.067), (39, 0.015), (41, 0.017), (55, 0.015), (65, 0.013), (69, 0.054), (73, 0.019), (77, 0.025), (88, 0.087), (91, 0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77059352 <a title="81-lda-1" href="./nips-2005-Gaussian_Processes_for_Multiuser_Detection_in_CDMA_receivers.html">81 nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</a></p>
<p>Author: Juan J. Murillo-fuentes, Sebastian Caro, Fernando Pérez-Cruz</p><p>Abstract: In this paper we propose a new receiver for digital communications. We focus on the application of Gaussian Processes (GPs) to the multiuser detection (MUD) in code division multiple access (CDMA) systems to solve the near-far problem. Hence, we aim to reduce the interference from other users sharing the same frequency band. While usual approaches minimize the mean square error (MMSE) to linearly retrieve the user of interest, we exploit the same criteria but in the design of a nonlinear MUD. Since the optimal solution is known to be nonlinear, the performance of this novel method clearly improves that of the MMSE detectors. Furthermore, the GP based MUD achieves excellent interference suppression even for short training sequences. We also include some experiments to illustrate that other nonlinear detectors such as those based on Support Vector Machines (SVMs) exhibit a worse performance. 1</p><p>2 0.71071541 <a title="81-lda-2" href="./nips-2005-Improved_risk_tail_bounds_for_on-line_algorithms.html">95 nips-2005-Improved risk tail bounds for on-line algorithms</a></p>
<p>Author: NicolĂ˛ Cesa-bianchi, Claudio Gentile</p><p>Abstract: We prove the strongest known bound for the risk of hypotheses selected from the ensemble generated by running a learning algorithm incrementally on the training data. Our result is based on proof techniques that are remarkably different from the standard risk analysis based on uniform convergence arguments.</p><p>3 0.5517869 <a title="81-lda-3" href="./nips-2005-Query_by_Committee_Made_Real.html">160 nips-2005-Query by Committee Made Real</a></p>
<p>Author: Ran Gilad-bachrach, Amir Navot, Naftali Tishby</p><p>Abstract: Training a learning algorithm is a costly task. A major goal of active learning is to reduce this cost. In this paper we introduce a new algorithm, KQBC, which is capable of actively learning large scale problems by using selective sampling. The algorithm overcomes the costly sampling step of the well known Query By Committee (QBC) algorithm by projecting onto a low dimensional space. KQBC also enables the use of kernels, providing a simple way of extending QBC to the non-linear scenario. Sampling the low dimension space is done using the hit and run random walk. We demonstrate the success of this novel algorithm by applying it to both artiﬁcial and a real world problems.</p><p>4 0.433763 <a title="81-lda-4" href="./nips-2005-Identifying_Distributed_Object_Representations_in_Human_Extrastriate_Visual_Cortex.html">94 nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</a></p>
<p>Author: Rory Sayres, David Ress, Kalanit Grill-spector</p><p>Abstract: The category of visual stimuli has been reliably decoded from patterns of neural activity in extrastriate visual cortex [1]. It has yet to be seen whether object identity can be inferred from this activity. We present fMRI data measuring responses in human extrastriate cortex to a set of 12 distinct object images. We use a simple winner-take-all classifier, using half the data from each recording session as a training set, to evaluate encoding of object identity across fMRI voxels. Since this approach is sensitive to the inclusion of noisy voxels, we describe two methods for identifying subsets of voxels in the data which optimally distinguish object identity. One method characterizes the reliability of each voxel within subsets of the data, while another estimates the mutual information of each voxel with the stimulus set. We find that both metrics can identify subsets of the data which reliably encode object identity, even when noisy measurements are artificially added to the data. The mutual information metric is less efficient at this task, likely due to constraints in fMRI data. 1</p><p>5 0.34720385 <a title="81-lda-5" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>Author: Rebecca Willett, Robert Nowak, Rui M. Castro</p><p>Abstract: This paper presents a rigorous statistical analysis characterizing regimes in which active learning signiﬁcantly outperforms classical passive learning. Active learning algorithms are able to make queries or select sample locations in an online fashion, depending on the results of the previous queries. In some regimes, this extra ﬂexibility leads to signiﬁcantly faster rates of error decay than those possible in classical passive learning settings. The nature of these regimes is explored by studying fundamental performance limits of active and passive learning in two illustrative nonparametric function classes. In addition to examining the theoretical potential of active learning, this paper describes a practical algorithm capable of exploiting the extra ﬂexibility of the active setting and provably improving upon the classical passive techniques. Our active learning theory and methods show promise in a number of applications, including ﬁeld estimation using wireless sensor networks and fault line detection. 1</p><p>6 0.34612903 <a title="81-lda-6" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>7 0.3458873 <a title="81-lda-7" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>8 0.34566548 <a title="81-lda-8" href="./nips-2005-Coarse_sample_complexity_bounds_for_active_learning.html">41 nips-2005-Coarse sample complexity bounds for active learning</a></p>
<p>9 0.34543255 <a title="81-lda-9" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>10 0.34470624 <a title="81-lda-10" href="./nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<p>11 0.34459245 <a title="81-lda-11" href="./nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">151 nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<p>12 0.34334382 <a title="81-lda-12" href="./nips-2005-Estimation_of_Intrinsic_Dimensionality_Using_High-Rate_Vector_Quantization.html">66 nips-2005-Estimation of Intrinsic Dimensionality Using High-Rate Vector Quantization</a></p>
<p>13 0.34274906 <a title="81-lda-13" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>14 0.3425872 <a title="81-lda-14" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>15 0.3419795 <a title="81-lda-15" href="./nips-2005-Efficient_Estimation_of_OOMs.html">62 nips-2005-Efficient Estimation of OOMs</a></p>
<p>16 0.34162018 <a title="81-lda-16" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>17 0.34106296 <a title="81-lda-17" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>18 0.34094056 <a title="81-lda-18" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>19 0.34047779 <a title="81-lda-19" href="./nips-2005-Augmented_Rescorla-Wagner_and_Maximum_Likelihood_Estimation.html">32 nips-2005-Augmented Rescorla-Wagner and Maximum Likelihood Estimation</a></p>
<p>20 0.33978516 <a title="81-lda-20" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
