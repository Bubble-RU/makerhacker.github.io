<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>51 nips-2005-Correcting sample selection bias in maximum entropy density estimation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-51" href="#">nips2005-51</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>51 nips-2005-Correcting sample selection bias in maximum entropy density estimation</h1>
<br/><p>Source: <a title="nips-2005-51-pdf" href="http://papers.nips.cc/paper/2929-correcting-sample-selection-bias-in-maximum-entropy-density-estimation.pdf">pdf</a></p><p>Author: Miroslav Dudík, Steven J. Phillips, Robert E. Schapire</p><p>Abstract: We study the problem of maximum entropy density estimation in the presence of known sample selection bias. We propose three bias correction approaches. The ﬁrst one takes advantage of unbiased sufﬁcient statistics which can be obtained from biased samples. The second one estimates the biased distribution and then factors the bias out. The third one approximates the second by only using samples from the sampling distribution. We provide guarantees for the ﬁrst two approaches and evaluate the performance of all three approaches in synthetic experiments and on real data from species habitat modeling, where maxent has been successfully applied and where sample selection bias is a signiﬁcant problem. 1</p><p>Reference: <a title="nips-2005-51-reference" href="../nips2005_reference/nips-2005-Correcting_sample_selection_bias_in_maximum_entropy_density_estimation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Correcting sample selection bias in maximum entropy density estimation  Miroslav Dud´k, Robert E. [sent-1, score-0.45]
</p><p>2 edu  Abstract We study the problem of maximum entropy density estimation in the presence of known sample selection bias. [sent-6, score-0.292]
</p><p>3 The ﬁrst one takes advantage of unbiased sufﬁcient statistics which can be obtained from biased samples. [sent-8, score-0.396]
</p><p>4 The second one estimates the biased distribution and then factors the bias out. [sent-9, score-0.365]
</p><p>5 The third one approximates the second by only using samples from the sampling distribution. [sent-10, score-0.228]
</p><p>6 We provide guarantees for the ﬁrst two approaches and evaluate the performance of all three approaches in synthetic experiments and on real data from species habitat modeling, where maxent has been successfully applied and where sample selection bias is a signiﬁcant problem. [sent-11, score-1.32]
</p><p>7 1  Introduction  We study the problem of estimating a probability distribution, particularly in the context of species habitat modeling. [sent-12, score-0.419]
</p><p>8 It is very common in distribution modeling to assume access to independent samples from the distribution being estimated. [sent-13, score-0.353]
</p><p>9 For example, habitat modeling is typically based on known occurrence locations derived from collections in natural history museums and herbariums as well as biological surveys [1, 2, 3]. [sent-15, score-0.311]
</p><p>10 To achieve this in a statistically sound manner using current methods, it is necessary to assume that the sampling distribution and species distributions are not correlated. [sent-17, score-0.487]
</p><p>11 Furthermore, the independence assumption may not hold since roads and waterways are often correlated with topography and vegetation which inﬂuence species distributions. [sent-19, score-0.36]
</p><p>12 New unbiased sampling may be expensive, so much can be gained by using the extensive existing biased data, especially since it is becoming freely available online [5]. [sent-20, score-0.51]
</p><p>13 For instance, in the case of habitat modeling, some factors inﬂuencing the sampling distribution are well known, such as distance from roads, towns, etc. [sent-22, score-0.357]
</p><p>14 In addition, a list of visited sites may be available and viewed as a sample of the sampling distribution itself. [sent-23, score-0.263]
</p><p>15 If such a list is not available, the set of sites where any species from a large group has been observed may be a reasonable approximation of all visited locations. [sent-24, score-0.251]
</p><p>16 We  assume that the sampling distribution (or an approximation) is known during training, but we require that unbiased models not use any knowledge of sample selection bias during testing. [sent-26, score-0.775]
</p><p>17 This requirement is vital for habitat modeling where models are often applied to a different region or under different climatic conditions. [sent-27, score-0.252]
</p><p>18 To our knowledge this is the ﬁrst work addressing sample selection bias in a statistically sound manner and in a setup suitable for species habitat modeling from presence-only data. [sent-28, score-0.793]
</p><p>19 We propose three approaches that incorporate sample selection bias in a common density estimation technique based on the principle of maximum entropy (maxent). [sent-29, score-0.492]
</p><p>20 Maxent with ℓ1 -regularization has been successfully used to model geographic distributions of species under the assumption that samples are unbiased [3]. [sent-30, score-0.772]
</p><p>21 We review ℓ1 -regularized maxent with unbiased data in Section 2, and give details of the new approaches in Section 3. [sent-31, score-0.672]
</p><p>22 Our three approaches make simple modiﬁcations to unbiased maxent and achieve analogous provable performance guarantees. [sent-32, score-0.702]
</p><p>23 The ﬁrst approach uses a bias correction technique similar to that of Zadrozny et al. [sent-33, score-0.245]
</p><p>24 [6, 7] to obtain unbiased conﬁdence intervals from biased samples as required by our version of maxent. [sent-34, score-0.562]
</p><p>25 We prove that, as in the unbiased case, this produces models whose log loss approaches that of the best possible Gibbs distribution (with increasing sample size). [sent-35, score-0.572]
</p><p>26 In contrast, the second approach we propose ﬁrst estimates the biased distribution and then factors the bias out. [sent-36, score-0.365]
</p><p>27 When the target distribution is a Gibbs distribution, the solution again approaches the log loss of the target distribution. [sent-37, score-0.351]
</p><p>28 When the target distribution is not Gibbs, we demonstrate that the second approach need not produce the optimal Gibbs distribution (with respect to log loss) even in the limit of inﬁnitely many samples. [sent-38, score-0.268]
</p><p>29 The third approach is an approximation of the second approach which uses samples from the sampling distribution instead of the distribution itself. [sent-41, score-0.378]
</p><p>30 One of the challenges in studying methods for correcting sample selection bias is that unbiased data sets, though not required during training, are needed as test sets to evaluate performance. [sent-42, score-0.614]
</p><p>31 We use both fully synthetic data, as well as a biological dataset consisting of a biased training set and an independently collected reasonably unbiased test set. [sent-46, score-0.497]
</p><p>32 Sample selection bias also arises in econometrics where it stems from factors such as attrition, nonresponse and self selection [8, 9, 10]. [sent-48, score-0.276]
</p><p>33 In the machine learning community, sample selection bias has been recently considered for classiﬁcation problems by Zadrozny [6]. [sent-50, score-0.291]
</p><p>34 In addition, in the case of modeling species habitats, we face the challenge of sample sizes that are very small (2–100) by machine learning standards. [sent-56, score-0.402]
</p><p>35 2  Maxent setup  In this section, we describe the setup for unbiased maximum entropy density estimation and review performance guarantees. [sent-57, score-0.556]
</p><p>36 The goal is to estimate an unknown target distribution π over a known sample space X based on samples x1 , . [sent-59, score-0.364]
</p><p>37 We assume that samples are independently distributed according to π and denote the empirical distribution by π (x) = |{1 ≤ i ≤ m : xi = ˜ x}|/m. [sent-63, score-0.22]
</p><p>38 The structure of the problem is speciﬁed by real valued functions fj : X → R, j = 1, . [sent-64, score-0.296]
</p><p>39 , n, called features and by a distribution q0 representing a default estimate. [sent-67, score-0.233]
</p><p>40 We would like to ﬁnd a distribution p which satisﬁes |p[fj ] − π [fj ]| ≤ βj for all 1 ≤ j ≤ n, ˜ (1) for some estimates βj of deviations of empirical averages from their expectations. [sent-73, score-0.208]
</p><p>41 For the case when the default distribution q0 is uniform, the maximum entropy principle tells us to choose the distribution of maximum entropy satisfying these constraints. [sent-75, score-0.457]
</p><p>42 Instead of solving (2) directly, we solve its dual: min log Zλ −  λ∈Rn  1 2  j  (bj + aj )λj +  1 2  j  (bj − aj )|λj | . [sent-84, score-0.207]
</p><p>43 When all the primal constraints are satisﬁed by the target distribution π then the solution q of the dual is guaranteed to be not much worse an approximation of π than the best Gibbs ˆ distribution q ∗ . [sent-90, score-0.305]
</p><p>44 , fn where fj : X → [0, 1] default estimate q0 regularization parameter β > 0 sampling distribution s samples x1 , . [sent-98, score-0.697]
</p><p>45 , xm ∈ X Output: qλ approximating the target distribution `ˆ √ ´ Let β0 = β/ ˆ m · min {f σ s[1/s], (max 1/s − min 1/s)/2} ˜ ˆ [c0 , d0 ] = π s[1/s] − β0 , π s[1/s] + β0 ∩ min 1/s, max 1/s] f f For j = 1, . [sent-101, score-0.301]
</p><p>46 √n: ´ ` , βj = β/ ˆ m · min {f j /s], (max fj /s − ˆ σ s[f min fj /s)/2} ˜ [cj , dj ] = π s[fj /s] − βj , π s[fj /s] + βj ∩ min fj /s, max fj /s] f f ˆ ˜ ˆ [aj , bj ] = cj /d0 , dj /c0 ∩ min fj , max fj ] Solve the dual (4) Algorithm 1: D EBIAS AVERAGES. [sent-104, score-2.122]
</p><p>47 Then the relative entropy from q to π will not be worse than the relative ˆ entropy from any Gibbs distribution q ∗ to π by more than O( λ∗ 1 (log n)/m). [sent-132, score-0.328]
</p><p>48 In practice, we set √ βj = β/ m · min {˜ [fj ], σmax [fj ]} σ (7) where β is a tuned constant, σ [fj ] is the sample deviation of fj , and σmax [fj ] is an upper ˜ bound on the standard deviation, such as (maxx fj (x) − minx fj (x))/2. [sent-133, score-0.917]
</p><p>49 We refer to this algorithm for unbiased data as U NBIASED M AXENT. [sent-134, score-0.295]
</p><p>50 3  Maxent with sample selection bias  In the biased case, the goal is to estimate the target distribution π, but samples do not come directly from π. [sent-135, score-0.682]
</p><p>51 , xm come from the biased distribution πs where s is the sampling distribution. [sent-140, score-0.325]
</p><p>52 The empirical distribution of m samples drawn from πs will be denoted by πs. [sent-143, score-0.22]
</p><p>53 In our ﬁrst approach, we use the same algorithm as for the unbiased case but employ a different method to obtain conﬁdence intervals [aj , bj ]. [sent-146, score-0.415]
</p><p>54 Since we do not have direct access to samples from π, we use a version of the Bias Correction Theorem of Zadrozny [6] to convert expectations with respect to πs to expectations with respect to π. [sent-147, score-0.21]
</p><p>55 Assume that for some sample-derived bounds cj , dj , 0 ≤ j ≤ n, with high probability 0 < c0 ≤ πs[1/s] ≤ d0 and 0 ≤ cj ≤ πs[fj /s] ≤ dj for all 1 ≤ j ≤ n. [sent-152, score-0.326]
</p><p>56 In practice, conﬁdence intervals [cj , dj ] may be determined using expressions analogous to (5) and (7) for random variables fj /s, 1/s and the empirical distribution πs. [sent-156, score-0.517]
</p><p>57 The second algorithm does not approximate π directly, but uses maxent to estimate the distribution πs and then converts this estimate into an approximation of π. [sent-160, score-0.464]
</p><p>58 If the default estimate of π is q0 , then the default estimate of πs is q0 s. [sent-161, score-0.252]
</p><p>59 Applying unbiased maxent to the empirical distribution πs with the default q0 s, we ˆ ˆ obtain a q0 s-Gibbs distribution q0 seλ·f approximating πs. [sent-162, score-0.91]
</p><p>60 Performance guarantees for unbiased maxent imply that estimates of πs converge to πs as the number of samples increases. [sent-167, score-0.833]
</p><p>61 Features f1 , f2 , target distribution π, sampling distribution s and the biased distribution πs are given in Table 1. [sent-174, score-0.514]
</p><p>62 We use the algorithm FACTOR B IAS O UT with the sampling distribution s replaced by the corresponding empirical distribution s. [sent-196, score-0.295]
</p><p>63 ˜ To simplify the algorithm, we note that instead of using q0 s as a default estimate for ˜ πs, it sufﬁces to replace the sample space X by X ′ = x(1) , x(2) , . [sent-197, score-0.2]
</p><p>64 5 10  100  number of training samples (m)  1000  10  100  unbiased maxent debias averages factor bias out  1000  10  100  1000  approximate factor bias out 1,000 samples 10,000 samples  Figure 1: Learning curves for synthetic experiments. [sent-208, score-1.685]
</p><p>65 Performance is measured in terms of relative entropy to the target distribution as a function of an increasing number of training samples. [sent-212, score-0.279]
</p><p>66 We therefore do “clamping”, restricting values fj (x) to their ranges over X ′ and ˆ capping values of the exponent λ · f (x) at its maximum over X ′ . [sent-218, score-0.268]
</p><p>67 4  Experiments  Conducting real data experiments to evaluate bias correction techniques is difﬁcult, because bias is typically unknown and samples from unbiased distributions are not available. [sent-220, score-0.887]
</p><p>68 Nevertheless, in addition to synthetic experiments, we were also able to conduct experiments with real-world data for habitat modeling. [sent-222, score-0.243]
</p><p>69 These distributions were derived from 65 features indexed as fi , 0 ≤ i ≤ 9 and fij , 0 ≤ i ≤ j ≤ 9. [sent-225, score-0.265]
</p><p>70 Values fi (x) were chosen independently and uniformly in [0, 1], and we set fij (x) = fi (x)fj (x). [sent-226, score-0.288]
</p><p>71 We used features fi′ , 0 ≤ i ≤ 9 and their squares fii , where fi′ (x) = fi (x) for ′ 0 ≤ i ≤ 5 (relevant features) and fi (x) = U[0,1] for 6 ≤ i ≤ 9 (irrelevant features). [sent-240, score-0.354]
</p><p>72 We generated a sampling distribution s correlated with target distributions. [sent-242, score-0.263]
</p><p>73 More speciﬁcally, s was a Gibbs distrib(s) (s) (s) ution generated from features fi , 0 ≤ i ≤ 5 and their squares fii , where fi (x) = U[0,1] (s) (s) (s) for 0 ≤ i ≤ 1 and fi = fi+2 for 2 ≤ i ≤ 5. [sent-243, score-0.483]
</p><p>74 For every target distribution, we evaluated the performance of U NBIASED M AXENT, D EBIAS AVERAGES, FACTOR B IAS O UT and A PPROX FACTOR B IAS O UT with 1,000 and 10,000 samples from the sampling distribution. [sent-245, score-0.332]
</p><p>75 The performance was evaluated in terms of relative entropy to the target distribution. [sent-246, score-0.208]
</p><p>76 Average performance of unbiased maxent and three bias correction approaches over all species in six regions. [sent-253, score-1.198]
</p><p>77 Results of bias correction approaches are italicized if they are signiﬁcantly worse and set in boldface if they are signiﬁcantly better than those of the unbiased maxent according to a paired t-test at the level of signiﬁcance 5%. [sent-257, score-0.962]
</p><p>78 D EBIAS AVERAGES is worse than U NBIASED M AXENT for small sample sizes, but as the number of training samples increases, it soon outperforms U NBIASED M AXENT and eventually also outperforms FACTOR B IAS O UT. [sent-310, score-0.259]
</p><p>79 A PPROX FACTOR B IAS O UT improves as the number of samples from the sampling distribution increases from 1,000 to 10,000, but both versions of A PPROX FACTOR B IAS O UT perform worse than U NBIASED M AXENT for the distribution π2 . [sent-311, score-0.423]
</p><p>80 In this set of experiments, we evaluated maxent in the task of estimating species habitats. [sent-313, score-0.586]
</p><p>81 The sample space is a geographic region divided into a grid of cells and samples are known occurrence localities — cells where a given species was observed. [sent-314, score-0.535]
</p><p>82 Species sample locations and environmental variables were all produced and used as part of the “Testing alternative methodologies for modeling species’ ecological niches and predicting geographic distributions” Working Group at the National Center for Ecological Analysis and Synthesis (NCEAS). [sent-319, score-0.374]
</p><p>83 The working group compared modeling methods across a variety of species and regions. [sent-320, score-0.327]
</p><p>84 We compared performance of our bias correction approaches with that of the unbiased maxent which was among the top methods in the NCEAS comparison [13]. [sent-323, score-0.947]
</p><p>85 We used the full dataset consisting of 226 species in 6 regions with 2–5822 training presences per species (233 on average) and 102–19120 test presences/absences. [sent-324, score-0.602]
</p><p>86 We treated training occurrence locations for all species in each region as sampling distribution samples and used them directly in A PPROX FACTOR B IAS O UT. [sent-326, score-0.611]
</p><p>87 In order to apply D EBIAS AVERAGES and FACTOR B IAS O UT, we estimated the sampling distribution using unbiased maxent. [sent-327, score-0.484]
</p><p>88 In contrast with that work, however, our experiments do not use the sampling distribution estimate during evaluation and hence do not depend on its quality. [sent-329, score-0.216]
</p><p>89 The resulting distributions were evaluated on test presences according to the log loss and on test presences and absences according to the area under an ROC curve (AUC) [14]. [sent-330, score-0.281]
</p><p>90 In Table 2 we show performance of our three approaches compared with the unbiased maxent. [sent-336, score-0.367]
</p><p>91 All three algorithms yield on average a worse log loss than the unbiased maxent. [sent-337, score-0.426]
</p><p>92 This can perhaps be attributed to the imperfect estimate of the sampling distribution or to  the sampling distribution being zero over large portions of the sample space. [sent-338, score-0.479]
</p><p>93 5  Conclusions  We have proposed three approaches that incorporate information about sample selection bias in maxent and demonstrated their utility in synthetic and real data experiments. [sent-341, score-0.771]
</p><p>94 Experiments also raise several questions that merit further research: D EBIAS AVERAGES has the strongest performance guarantees, but it performs the worst in real data experiments and catches up with other methods only for large sample sizes in synthetic experiments. [sent-342, score-0.237]
</p><p>95 This may be due to poor estimates of unbiased conﬁdence intervals and could be possibly improved using a different estimation method. [sent-343, score-0.406]
</p><p>96 This disagreement suggests that methods which aim to optimize AUC directly could be more successful in species modeling, possibly incorporating some concepts from FACTOR B IAS O UT and A PPROX FACTOR B IAS O UT. [sent-345, score-0.251]
</p><p>97 A PPROX FACTOR B IAS O UT performs the best on real world data, possibly due to the direct use of samples from the sampling distribution rather than a sampling distribution estimate. [sent-346, score-0.52]
</p><p>98 Quantitative methods for modeling species habitat: Comparative performance and an application to Australian plants. [sent-353, score-0.328]
</p><p>99 A maximum entropy approach to species distriı bution modeling. [sent-365, score-0.355]
</p><p>100 Geographical sampling bias and its implications for conservation priorities in a Africa. [sent-371, score-0.3]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ias', 0.391), ('maxent', 0.335), ('unbiased', 0.295), ('fj', 0.268), ('species', 0.251), ('pprox', 0.186), ('habitat', 0.168), ('bias', 0.158), ('nbiased', 0.149), ('ut', 0.145), ('auc', 0.13), ('fi', 0.129), ('sampling', 0.114), ('samples', 0.114), ('axent', 0.112), ('entropy', 0.104), ('biased', 0.101), ('default', 0.099), ('gibbs', 0.096), ('factor', 0.094), ('ebias', 0.093), ('dj', 0.091), ('correction', 0.087), ('distribution', 0.075), ('synthetic', 0.075), ('nceas', 0.074), ('phillips', 0.074), ('presences', 0.074), ('sample', 0.074), ('target', 0.074), ('cj', 0.072), ('averages', 0.071), ('ecological', 0.069), ('bj', 0.068), ('bregman', 0.065), ('geographic', 0.065), ('zadrozny', 0.065), ('aj', 0.062), ('selection', 0.059), ('features', 0.059), ('guarantees', 0.058), ('awt', 0.056), ('dud', 0.056), ('miroslav', 0.056), ('swi', 0.056), ('dence', 0.056), ('intervals', 0.052), ('factoring', 0.049), ('steven', 0.049), ('distributions', 0.047), ('modeling', 0.047), ('re', 0.045), ('worse', 0.045), ('roads', 0.044), ('log', 0.044), ('loss', 0.042), ('approaches', 0.042), ('access', 0.042), ('methodologies', 0.041), ('environmental', 0.041), ('min', 0.039), ('robert', 0.039), ('bianca', 0.037), ('climatic', 0.037), ('debias', 0.037), ('fii', 0.037), ('museums', 0.037), ('niches', 0.037), ('towns', 0.037), ('waterways', 0.037), ('setup', 0.036), ('dual', 0.036), ('xm', 0.035), ('ii', 0.035), ('park', 0.032), ('divergence', 0.032), ('estimates', 0.031), ('empirical', 0.031), ('occurrence', 0.031), ('sizes', 0.03), ('performance', 0.03), ('barbara', 0.03), ('nsw', 0.03), ('fij', 0.03), ('working', 0.029), ('convex', 0.028), ('estimation', 0.028), ('real', 0.028), ('nz', 0.028), ('vegetation', 0.028), ('surveys', 0.028), ('categorical', 0.028), ('conservation', 0.028), ('correcting', 0.028), ('estimate', 0.027), ('expectations', 0.027), ('density', 0.027), ('training', 0.026), ('con', 0.026), ('theorem', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="51-tfidf-1" href="./nips-2005-Correcting_sample_selection_bias_in_maximum_entropy_density_estimation.html">51 nips-2005-Correcting sample selection bias in maximum entropy density estimation</a></p>
<p>Author: Miroslav Dudík, Steven J. Phillips, Robert E. Schapire</p><p>Abstract: We study the problem of maximum entropy density estimation in the presence of known sample selection bias. We propose three bias correction approaches. The ﬁrst one takes advantage of unbiased sufﬁcient statistics which can be obtained from biased samples. The second one estimates the biased distribution and then factors the bias out. The third one approximates the second by only using samples from the sampling distribution. We provide guarantees for the ﬁrst two approaches and evaluate the performance of all three approaches in synthetic experiments and on real data from species habitat modeling, where maxent has been successfully applied and where sample selection bias is a signiﬁcant problem. 1</p><p>2 0.096229479 <a title="51-tfidf-2" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>Author: Tong Zhang, Rie Kubota Ando</p><p>Abstract: We consider a framework for semi-supervised learning using spectral decomposition based un-supervised kernel design. This approach subsumes a class of previously proposed semi-supervised learning methods on data graphs. We examine various theoretical properties of such methods. In particular, we derive a generalization performance bound, and obtain the optimal kernel design by minimizing the bound. Based on the theoretical analysis, we are able to demonstrate why spectral kernel design based methods can often improve the predictive performance. Experiments are used to illustrate the main consequences of our analysis.</p><p>3 0.090166949 <a title="51-tfidf-3" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>Author: Nicolas Usunier, Massih-reza Amini, Patrick Gallinari</p><p>Abstract: In this paper we propose a general framework to study the generalization properties of binary classiﬁers trained with data which may be dependent, but are deterministically generated upon a sample of independent examples. It provides generalization bounds for binary classiﬁcation and some cases of ranking problems, and clariﬁes the relationship between these learning tasks. 1</p><p>4 0.073264502 <a title="51-tfidf-4" href="./nips-2005-Computing_the_Solution_Path_for_the_Regularized_Support_Vector_Regression.html">44 nips-2005-Computing the Solution Path for the Regularized Support Vector Regression</a></p>
<p>Author: Lacey Gunter, Ji Zhu</p><p>Abstract: In this paper we derive an algorithm that computes the entire solution path of the support vector regression, with essentially the same computational cost as ﬁtting one SVR model. We also propose an unbiased estimate for the degrees of freedom of the SVR model, which allows convenient selection of the regularization parameter. 1</p><p>5 0.068696588 <a title="51-tfidf-5" href="./nips-2005-Kernels_for_gene_regulatory_regions.html">103 nips-2005-Kernels for gene regulatory regions</a></p>
<p>Author: Jean-philippe Vert, Robert Thurman, William S. Noble</p><p>Abstract: We describe a hierarchy of motif-based kernels for multiple alignments of biological sequences, particularly suitable to process regulatory regions of genes. The kernels incorporate progressively more information, with the most complex kernel accounting for a multiple alignment of orthologous regions, the phylogenetic tree relating the species, and the prior knowledge that relevant sequence patterns occur in conserved motif blocks. These kernels can be used in the presence of a library of known transcription factor binding sites, or de novo by iterating over all k-mers of a given length. In the latter mode, a discriminative classiﬁer built from such a kernel not only recognizes a given class of promoter regions, but as a side effect simultaneously identiﬁes a collection of relevant, discriminative sequence motifs. We demonstrate the utility of the motif-based multiple alignment kernels by using a collection of aligned promoter regions from ﬁve yeast species to recognize classes of cell-cycle regulated genes. Supplementary data is available at http://noble.gs.washington.edu/proj/pkernel. 1</p><p>6 0.066207886 <a title="51-tfidf-6" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>7 0.063543104 <a title="51-tfidf-7" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>8 0.063031435 <a title="51-tfidf-8" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>9 0.0617106 <a title="51-tfidf-9" href="./nips-2005-Efficient_Unsupervised_Learning_for_Localization_and_Detection_in_Object_Categories.html">63 nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</a></p>
<p>10 0.061639819 <a title="51-tfidf-10" href="./nips-2005-Nested_sampling_for_Potts_models.html">133 nips-2005-Nested sampling for Potts models</a></p>
<p>11 0.061092924 <a title="51-tfidf-11" href="./nips-2005-Active_Learning_For_Identifying_Function_Threshold_Boundaries.html">18 nips-2005-Active Learning For Identifying Function Threshold Boundaries</a></p>
<p>12 0.060705885 <a title="51-tfidf-12" href="./nips-2005-Non-iterative_Estimation_with_Perturbed_Gaussian_Markov_Processes.html">139 nips-2005-Non-iterative Estimation with Perturbed Gaussian Markov Processes</a></p>
<p>13 0.059143256 <a title="51-tfidf-13" href="./nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs.html">90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</a></p>
<p>14 0.058296356 <a title="51-tfidf-14" href="./nips-2005-Optimal_cue_selection_strategy.html">149 nips-2005-Optimal cue selection strategy</a></p>
<p>15 0.057716113 <a title="51-tfidf-15" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>16 0.057636965 <a title="51-tfidf-16" href="./nips-2005-Active_Learning_for_Misspecified_Models.html">19 nips-2005-Active Learning for Misspecified Models</a></p>
<p>17 0.056957092 <a title="51-tfidf-17" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>18 0.054335643 <a title="51-tfidf-18" href="./nips-2005-An_Approximate_Inference_Approach_for_the_PCA_Reconstruction_Error.html">24 nips-2005-An Approximate Inference Approach for the PCA Reconstruction Error</a></p>
<p>19 0.054030947 <a title="51-tfidf-19" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>20 0.053055342 <a title="51-tfidf-20" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.182), (1, 0.043), (2, -0.02), (3, 0.002), (4, 0.058), (5, 0.054), (6, -0.025), (7, 0.018), (8, -0.048), (9, 0.016), (10, -0.016), (11, -0.009), (12, 0.095), (13, 0.053), (14, -0.062), (15, 0.06), (16, 0.027), (17, 0.07), (18, 0.043), (19, -0.133), (20, 0.043), (21, -0.038), (22, 0.042), (23, 0.136), (24, 0.064), (25, -0.009), (26, 0.165), (27, -0.088), (28, 0.035), (29, 0.07), (30, -0.01), (31, -0.005), (32, 0.0), (33, 0.113), (34, 0.008), (35, 0.017), (36, 0.129), (37, -0.012), (38, -0.015), (39, 0.173), (40, -0.114), (41, 0.111), (42, 0.08), (43, -0.006), (44, -0.054), (45, -0.08), (46, 0.061), (47, -0.048), (48, -0.04), (49, 0.069)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9329862 <a title="51-lsi-1" href="./nips-2005-Correcting_sample_selection_bias_in_maximum_entropy_density_estimation.html">51 nips-2005-Correcting sample selection bias in maximum entropy density estimation</a></p>
<p>Author: Miroslav Dudík, Steven J. Phillips, Robert E. Schapire</p><p>Abstract: We study the problem of maximum entropy density estimation in the presence of known sample selection bias. We propose three bias correction approaches. The ﬁrst one takes advantage of unbiased sufﬁcient statistics which can be obtained from biased samples. The second one estimates the biased distribution and then factors the bias out. The third one approximates the second by only using samples from the sampling distribution. We provide guarantees for the ﬁrst two approaches and evaluate the performance of all three approaches in synthetic experiments and on real data from species habitat modeling, where maxent has been successfully applied and where sample selection bias is a signiﬁcant problem. 1</p><p>2 0.55525672 <a title="51-lsi-2" href="./nips-2005-Nested_sampling_for_Potts_models.html">133 nips-2005-Nested sampling for Potts models</a></p>
<p>Author: Iain Murray, David MacKay, Zoubin Ghahramani, John Skilling</p><p>Abstract: Nested sampling is a new Monte Carlo method by Skilling [1] intended for general Bayesian computation. Nested sampling provides a robust alternative to annealing-based methods for computing normalizing constants. It can also generate estimates of other quantities such as posterior expectations. The key technical requirement is an ability to draw samples uniformly from the prior subject to a constraint on the likelihood. We provide a demonstration with the Potts model, an undirected graphical model. 1</p><p>3 0.48776239 <a title="51-lsi-3" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>Author: Glenn Fung, Rómer Rosales, Balaji Krishnapuram</p><p>Abstract: We propose efﬁcient algorithms for learning ranking functions from order constraints between sets—i.e. classes—of training samples. Our algorithms may be used for maximizing the generalized Wilcoxon Mann Whitney statistic that accounts for the partial ordering of the classes: special cases include maximizing the area under the ROC curve for binary classiﬁcation and its generalization for ordinal regression. Experiments on public benchmarks indicate that: (a) the proposed algorithm is at least as accurate as the current state-of-the-art; (b) computationally, it is several orders of magnitude faster and—unlike current methods—it is easily able to handle even large datasets with over 20,000 samples. 1</p><p>4 0.47985256 <a title="51-lsi-4" href="./nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs.html">90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</a></p>
<p>Author: Firas Hamze, Nando de Freitas</p><p>Abstract: This paper presents a new sampling algorithm for approximating functions of variables representable as undirected graphical models of arbitrary connectivity with pairwise potentials, as well as for estimating the notoriously difﬁcult partition function of the graph. The algorithm ﬁts into the framework of sequential Monte Carlo methods rather than the more widely used MCMC, and relies on constructing a sequence of intermediate distributions which get closer to the desired one. While the idea of using “tempered” proposals is known, we construct a novel sequence of target distributions where, rather than dropping a global temperature parameter, we sequentially couple individual pairs of variables that are, initially, sampled exactly from a spanning tree of the variables. We present experimental results on inference and estimation of the partition function for sparse and densely-connected graphs.</p><p>5 0.44770581 <a title="51-lsi-5" href="./nips-2005-Active_Learning_For_Identifying_Function_Threshold_Boundaries.html">18 nips-2005-Active Learning For Identifying Function Threshold Boundaries</a></p>
<p>Author: Brent Bryan, Robert C. Nichol, Christopher R. Genovese, Jeff Schneider, Christopher J. Miller, Larry Wasserman</p><p>Abstract: We present an efﬁcient algorithm to actively select queries for learning the boundaries separating a function domain into regions where the function is above and below a given threshold. We develop experiment selection methods based on entropy, misclassiﬁcation rate, variance, and their combinations, and show how they perform on a number of data sets. We then show how these algorithms are used to determine simultaneously valid 1 − α conﬁdence intervals for seven cosmological parameters. Experimentation shows that the algorithm reduces the computation necessary for the parameter estimation problem by an order of magnitude.</p><p>6 0.4428252 <a title="51-lsi-6" href="./nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<p>7 0.4287537 <a title="51-lsi-7" href="./nips-2005-Affine_Structure_From_Sound.html">20 nips-2005-Affine Structure From Sound</a></p>
<p>8 0.42608148 <a title="51-lsi-8" href="./nips-2005-Non-iterative_Estimation_with_Perturbed_Gaussian_Markov_Processes.html">139 nips-2005-Non-iterative Estimation with Perturbed Gaussian Markov Processes</a></p>
<p>9 0.42348355 <a title="51-lsi-9" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>10 0.41109362 <a title="51-lsi-10" href="./nips-2005-Learning_from_Data_of_Variable_Quality.html">117 nips-2005-Learning from Data of Variable Quality</a></p>
<p>11 0.4058747 <a title="51-lsi-11" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>12 0.40261304 <a title="51-lsi-12" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>13 0.39936683 <a title="51-lsi-13" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>14 0.3880007 <a title="51-lsi-14" href="./nips-2005-Active_Learning_for_Misspecified_Models.html">19 nips-2005-Active Learning for Misspecified Models</a></p>
<p>15 0.38286927 <a title="51-lsi-15" href="./nips-2005-Computing_the_Solution_Path_for_the_Regularized_Support_Vector_Regression.html">44 nips-2005-Computing the Solution Path for the Regularized Support Vector Regression</a></p>
<p>16 0.38196024 <a title="51-lsi-16" href="./nips-2005-Learning_Minimum_Volume_Sets.html">112 nips-2005-Learning Minimum Volume Sets</a></p>
<p>17 0.3744925 <a title="51-lsi-17" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>18 0.37067342 <a title="51-lsi-18" href="./nips-2005-An_Approximate_Inference_Approach_for_the_PCA_Reconstruction_Error.html">24 nips-2005-An Approximate Inference Approach for the PCA Reconstruction Error</a></p>
<p>19 0.36756822 <a title="51-lsi-19" href="./nips-2005-Kernels_for_gene_regulatory_regions.html">103 nips-2005-Kernels for gene regulatory regions</a></p>
<p>20 0.35761842 <a title="51-lsi-20" href="./nips-2005-Factorial_Switching_Kalman_Filters_for_Condition_Monitoring_in_Neonatal_Intensive_Care.html">68 nips-2005-Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.068), (8, 0.304), (10, 0.049), (27, 0.016), (31, 0.033), (34, 0.077), (41, 0.022), (55, 0.032), (69, 0.075), (73, 0.075), (77, 0.015), (88, 0.088), (91, 0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77501643 <a title="51-lda-1" href="./nips-2005-Correcting_sample_selection_bias_in_maximum_entropy_density_estimation.html">51 nips-2005-Correcting sample selection bias in maximum entropy density estimation</a></p>
<p>Author: Miroslav Dudík, Steven J. Phillips, Robert E. Schapire</p><p>Abstract: We study the problem of maximum entropy density estimation in the presence of known sample selection bias. We propose three bias correction approaches. The ﬁrst one takes advantage of unbiased sufﬁcient statistics which can be obtained from biased samples. The second one estimates the biased distribution and then factors the bias out. The third one approximates the second by only using samples from the sampling distribution. We provide guarantees for the ﬁrst two approaches and evaluate the performance of all three approaches in synthetic experiments and on real data from species habitat modeling, where maxent has been successfully applied and where sample selection bias is a signiﬁcant problem. 1</p><p>2 0.69325554 <a title="51-lda-2" href="./nips-2005-An_Approximate_Inference_Approach_for_the_PCA_Reconstruction_Error.html">24 nips-2005-An Approximate Inference Approach for the PCA Reconstruction Error</a></p>
<p>Author: Manfred Opper</p><p>Abstract: The problem of computing a resample estimate for the reconstruction error in PCA is reformulated as an inference problem with the help of the replica method. Using the expectation consistent (EC) approximation, the intractable inference problem can be solved efﬁciently using only two variational parameters. A perturbative correction to the result is computed and an alternative simpliﬁed derivation is also presented. 1</p><p>3 0.5125047 <a title="51-lda-3" href="./nips-2005-Kernelized_Infomax_Clustering.html">102 nips-2005-Kernelized Infomax Clustering</a></p>
<p>Author: David Barber, Felix V. Agakov</p><p>Abstract: We propose a simple information-theoretic approach to soft clustering based on maximizing the mutual information I(x, y) between the unknown cluster labels y and the training patterns x with respect to parameters of speciﬁcally constrained encoding distributions. The constraints are chosen such that patterns are likely to be clustered similarly if they lie close to speciﬁc unknown vectors in the feature space. The method may be conveniently applied to learning the optimal aﬃnity matrix, which corresponds to learning parameters of the kernelized encoder. The procedure does not require computations of eigenvalues of the Gram matrices, which makes it potentially attractive for clustering large data sets. 1</p><p>4 0.50689721 <a title="51-lda-4" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>Author: Amir Navot, Lavi Shpigelman, Naftali Tishby, Eilon Vaadia</p><p>Abstract: We present a non-linear, simple, yet effective, feature subset selection method for regression and use it in analyzing cortical neural activity. Our algorithm involves a feature-weighted version of the k-nearest-neighbor algorithm. It is able to capture complex dependency of the target function on its input and makes use of the leave-one-out error as a natural regularization. We explain the characteristics of our algorithm on synthetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey. By applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data.</p><p>5 0.50667393 <a title="51-lda-5" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>Author: Yixin Chen, Ya Zhang, Xiang Ji</p><p>Abstract: We present a novel spectral clustering method that enables users to incorporate prior knowledge of the size of clusters into the clustering process. The cost function, which is named size regularized cut (SRcut), is deﬁned as the sum of the inter-cluster similarity and a regularization term measuring the relative size of two clusters. Finding a partition of the data set to minimize SRcut is proved to be NP-complete. An approximation algorithm is proposed to solve a relaxed version of the optimization problem as an eigenvalue problem. Evaluations over different data sets demonstrate that the method is not sensitive to outliers and performs better than normalized cut. 1</p><p>6 0.50254369 <a title="51-lda-6" href="./nips-2005-Using_%60%60epitomes%27%27_to_model_genetic_diversity%3A_Rational_design_of_HIV_vaccine_cocktails.html">198 nips-2005-Using ``epitomes'' to model genetic diversity: Rational design of HIV vaccine cocktails</a></p>
<p>7 0.49891141 <a title="51-lda-7" href="./nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<p>8 0.49353176 <a title="51-lda-8" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>9 0.49053612 <a title="51-lda-9" href="./nips-2005-Diffusion_Maps%2C_Spectral_Clustering_and_Eigenfunctions_of_Fokker-Planck_Operators.html">56 nips-2005-Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators</a></p>
<p>10 0.49003759 <a title="51-lda-10" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>11 0.48804107 <a title="51-lda-11" href="./nips-2005-Nested_sampling_for_Potts_models.html">133 nips-2005-Nested sampling for Potts models</a></p>
<p>12 0.48784432 <a title="51-lda-12" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>13 0.48708665 <a title="51-lda-13" href="./nips-2005-Learning_Minimum_Volume_Sets.html">112 nips-2005-Learning Minimum Volume Sets</a></p>
<p>14 0.48705542 <a title="51-lda-14" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>15 0.48618338 <a title="51-lda-15" href="./nips-2005-Soft_Clustering_on_Graphs.html">178 nips-2005-Soft Clustering on Graphs</a></p>
<p>16 0.48611322 <a title="51-lda-16" href="./nips-2005-Estimation_of_Intrinsic_Dimensionality_Using_High-Rate_Vector_Quantization.html">66 nips-2005-Estimation of Intrinsic Dimensionality Using High-Rate Vector Quantization</a></p>
<p>17 0.48602846 <a title="51-lda-17" href="./nips-2005-Infinite_latent_feature_models_and_the_Indian_buffet_process.html">98 nips-2005-Infinite latent feature models and the Indian buffet process</a></p>
<p>18 0.48476642 <a title="51-lda-18" href="./nips-2005-Coarse_sample_complexity_bounds_for_active_learning.html">41 nips-2005-Coarse sample complexity bounds for active learning</a></p>
<p>19 0.4846049 <a title="51-lda-19" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>20 0.48378724 <a title="51-lda-20" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
