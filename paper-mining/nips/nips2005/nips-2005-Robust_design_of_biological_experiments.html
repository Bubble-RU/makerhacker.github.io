<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>167 nips-2005-Robust design of biological experiments</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-167" href="#">nips2005-167</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>167 nips-2005-Robust design of biological experiments</h1>
<br/><p>Source: <a title="nips-2005-167-pdf" href="http://papers.nips.cc/paper/2924-robust-design-of-biological-experiments.pdf">pdf</a></p><p>Author: Patrick Flaherty, Adam Arkin, Michael I. Jordan</p><p>Abstract: We address the problem of robust, computationally-efﬁcient design of biological experiments. Classical optimal experiment design methods have not been widely adopted in biological practice, in part because the resulting designs can be very brittle if the nominal parameter estimates for the model are poor, and in part because of computational constraints. We present a method for robust experiment design based on a semideﬁnite programming relaxation. We present an application of this method to the design of experiments for a complex calcium signal transduction pathway, where we have found that the parameter estimates obtained from the robust design are better than those obtained from an “optimal” design. 1</p><p>Reference: <a title="nips-2005-167-reference" href="../nips2005_reference/nips-2005-Robust_design_of_biological_experiments_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Robust design of biological experiments  Patrick Flaherty EECS Department University of California Berkeley, CA 94720 ﬂaherty@berkeley. [sent-1, score-0.651]
</p><p>2 gov  Abstract We address the problem of robust, computationally-efﬁcient design of biological experiments. [sent-7, score-0.621]
</p><p>3 Classical optimal experiment design methods have not been widely adopted in biological practice, in part because the resulting designs can be very brittle if the nominal parameter estimates for the model are poor, and in part because of computational constraints. [sent-8, score-1.268]
</p><p>4 We present a method for robust experiment design based on a semideﬁnite programming relaxation. [sent-9, score-0.955]
</p><p>5 We present an application of this method to the design of experiments for a complex calcium signal transduction pathway, where we have found that the parameter estimates obtained from the robust design are better than those obtained from an “optimal” design. [sent-10, score-1.849]
</p><p>6 1  Introduction  Statistical machine learning methods are making increasing inroads in the area of biological data analysis, particularly in the context of genome-scale data, where computational efﬁciency is paramount. [sent-11, score-0.124]
</p><p>7 Learning methods are particularly valuable for their ability to fuse multiple sources of information, aiding the biologist to interpret a phenomenon in its appropriate cellular, genetic and evolutionary context. [sent-12, score-0.134]
</p><p>8 At least as important to the biologist, however, is to use the results of data analysis to aid in the design of further experiments. [sent-13, score-0.523]
</p><p>9 In this paper we take up this challenge—we show how recent developments in computationally-efﬁcient optimization can be brought to bear on the problem of the design of experiments for complex biological data. [sent-14, score-0.722]
</p><p>10 We present results for a speciﬁc model of calcium signal transduction in which choices must be made among 17 kinds of RNAi knockdown experiments. [sent-15, score-0.548]
</p><p>11 There are three main objectives for experiment design: parameter estimation, hypothesis testing and prediction. [sent-16, score-0.282]
</p><p>12 Suppose in particular that we have a nonlinear  model y = f (x, θ) + ε, ε ∼ N (0, σ 2 ), where x ∈ X represents the controllable conditions of the experiment (such as dose or temperature), y is the experimental measurement and θ ∈ Rp is the set of parameters to be estimated. [sent-18, score-0.43]
</p><p>13 We consider a ﬁnite menu of available experiments X = {x1 , . [sent-19, score-0.181]
</p><p>14 Relaxing the problem to a continuous representation, we solve for a distribution over the design points and then multiply the weights by N at the end [2]. [sent-24, score-0.523]
</p><p>15 , w m  m  , i=1  wi = 1, wi ≥ 0, ∀i,  (1)  and it is our goal to select values of wi that satisfy an experimental design criterion. [sent-31, score-1.079]
</p><p>16 The covariance matrix for the parameter estimate is cov(θ|ξ) = σ 2 V T W V , which is the inverse of the observed Fisher information matrix. [sent-35, score-0.226]
</p><p>17 The aim of optimal experiment design methods is to minimize the covariance matrix of the parameter estimate [4, 5, 6]. [sent-36, score-1.028]
</p><p>18 There are two well-known difﬁculties that must be surmounted in the case of nonlinear models [6]: • The optimal design depends on an evaluation of the derivative of the model with respect to the parameters at a particular parameter estimate. [sent-37, score-0.743]
</p><p>19 • Simple optimal design procedures tend to concentrate experimental weight on only a few design points [7]. [sent-39, score-1.156]
</p><p>20 Such designs are overly optimistic about the appropriateness of the model, and provide little information about possible lack of ﬁt over a wider experimental range. [sent-40, score-0.178]
</p><p>21 There have been three main responses to these problems: sequential experiment design [7], Bayesian methods [8], and maximin approaches [9]. [sent-41, score-0.833]
</p><p>22 In the sequential approach, a working parameter estimate is ﬁrst used to construct a tentative experiment design. [sent-42, score-0.331]
</p><p>23 Data are collected under that design and the parameter estimate is updated. [sent-43, score-0.645]
</p><p>24 While heuristically reasonable, this approach is often inapplicable in practice because of costs associated with experiment set-up time. [sent-45, score-0.209]
</p><p>25 The objective function is the KL divergence between the prior distribution and the expected posterior distribution; this KL divergence is maximized (thereby maximizing the amount of expected information in the experiment design). [sent-47, score-0.209]
</p><p>26 Sensitivity to priors is a serious concern, however, particularly in the biological setting in which it can be quite difﬁcult to choose priors for quantities such as bulk rates for a complex process. [sent-48, score-0.16]
</p><p>27 The maximin approach considers a bounded range for each parameter and ﬁnds the optimal design for the worst case parameters in that range. [sent-49, score-0.767]
</p><p>28 We view both of the problems discussed above as arguments for a robust design, one which is insensitive to the linearization point and to model error. [sent-52, score-0.251]
</p><p>29 We work within the framework of E-optimal design (see below) and consider perturbations to the rank-one Fisher information matrix for each design point. [sent-53, score-1.219]
</p><p>30 An optimization with respect to such perturbations yields a robust semideﬁnite program [10, 11, 12]. [sent-54, score-0.42]
</p><p>31 3  Optimal Experiment Design  The three most common scalar measures of the size of the parameter covariance matrix in optimal experiment design are: • D-optimal design: determinant of the covariance matrix. [sent-55, score-1.069]
</p><p>32 We adopt the E-optimal design criterion, and formulate the design problem as follows:   m  P0 :  p∗ 0  = min λmax  w  T w i vi vi  −1  m   s. [sent-58, score-1.471]
</p><p>33 i=1  wi = 1  (3)  i=1  wi ≥ 0, ∀i,  where λmax [M ] is the maximum eigenvalue of a matrix M . [sent-60, score-0.393]
</p><p>34 w,s  i=1  T wi vi vi ≥ sIp  (4)  m  i=1  wi = 1, wi ≥ 0, ∀i,  which forms the basis of the robust extension that we develop in the following section. [sent-63, score-1.137]
</p><p>35 4  Robust Experiment Design  The uncertain parameters appear in the experiment design optimization problem through the Jacobian matrix, V . [sent-64, score-0.833]
</p><p>36 We consider additive unstructured perturbations on the Jacobian or “data” in this problem. [sent-65, score-0.124]
</p><p>37 The uncertain observed Fisher information matrix is F (w, ∆) = m T i=1 wi (vi vi − ∆i ), where ∆i is a p × p matrix for i = 1, . [sent-66, score-0.535]
</p><p>38 Incorporating the perturbations, the E-optimal experiment design problem with uncertainty based on (4) can be cast as the following minimax problem: Pρ : p∗ = ρ subject to  minw,s max m i=1  ∆ ≤ρ  −s  T wi (vi vi − ∆i ) ≥ sIp  ∆ = blkdiag(∆1 , . [sent-74, score-1.18]
</p><p>39 We will call equation (5) an E-robust experiment design. [sent-78, score-0.209]
</p><p>40 Taking ∆1 = · · · = ∆m , a special case of the S-procedure [11] yields the following semideﬁnite program: Pρ : p∗ = ρ  minw,s,τ −s m i=1  subject to  m i=1  T wi vi vi − sIp − ρ w ⊗ √2 Ip  m 2 τ Ip  ρ wT ⊗ √2 Ip τ Imp  ≥0  (7)  wi = 1, wi ≥ 0, ∀i. [sent-83, score-0.914]
</p><p>41 Using the Schur complement the ﬁrst constraint in (7) can be further simpliﬁed to m √ T (8) wi vi vi − ρ m w 2 ≥ sIp , i=1  which makes the regularization of the optimization problem (4) explicit. [sent-85, score-0.605]
</p><p>42 5  Results  We demonstrate the robust experiment design on two models of biological systems. [sent-87, score-1.053]
</p><p>43 The ﬁrst model is the Michaelis-Menten model of a simple enzyme reaction system. [sent-88, score-0.253]
</p><p>44 The second example is a model of a complex calcium signal transduction pathway in macrophage immune cells. [sent-90, score-0.552]
</p><p>45 In this example we consider RNAi knockdowns at a variety of ligand doses for the estimation of receptor level parameters. [sent-91, score-0.236]
</p><p>46 The basic chemical reaction that leads to this model is E +S −− C −2 E +P , → k−1  where E is the enzyme concentration, S is the substrate concentration and P is the product concentration. [sent-94, score-0.492]
</p><p>47 We employ mass action kinetics to develop a differential equation model for this reaction system [13]. [sent-95, score-0.239]
</p><p>48 The velocity of the reaction is deﬁned to be the rate of product formation, V0 = ∂P t . [sent-96, score-0.155]
</p><p>49 The initial velocity of the reaction is ∂t 0  V0 ≈  θ1 x , θ2 + x  where θ1 = k+2 E0 , θ2 =  k−1 + k+2 . [sent-97, score-0.185]
</p><p>50 k+1  (9)  (10)  We have taken the controllable factor, x, in this system to be the initial substrate concentration S0 . [sent-98, score-0.305]
</p><p>51 The parameter θ1 is the saturating velocity and θ2 is the initial substrate concentration at which product is formed at one-half the maximal velocity. [sent-99, score-0.408]
</p><p>52 In this example θ1 = 2 and θ2 = 2 are the total enzyme and initial substrate concentrations. [sent-100, score-0.218]
</p><p>53 We consider six initial substrate concentrations as the menu of experiments, X = 1 , 1, 2, 4, 8, 16 . [sent-101, score-0.282]
</p><p>54 8  Figure 1 shows the robust experiment design weights as a function of the uncertainty parameter with the Jacobian computed at the true parameter values. [sent-102, score-1.134]
</p><p>55 When ρ is small, the experimental weight is concentrated on only two design points. [sent-103, score-0.563]
</p><p>56 As ρ → ρmax the design converges to a uniform distribution over the entire menu of design points. [sent-104, score-1.243]
</p><p>57 In a sense, this uniform allocation of experimental energy is most robust to parameter uncertainty. [sent-105, score-0.414]
</p><p>58 Intermediate values of ρ yield an allocation of design points that reﬂects a tradeoff between robustness and nominal optimality. [sent-106, score-0.647]
</p><p>59 1 0 −4 10  −3  −2  10  10 ρ  −1  0  10  10  Figure 1: Michaelis-Menten model experiment design weights as a function of ρ. [sent-115, score-0.76]
</p><p>60 T For moderate values of ρ we gain signiﬁcantly in terms of robustness to errors in v i vi , at a moderate cost to maximal value of the minimum eigenvalues of the parameter estimate covariance matrix. [sent-116, score-0.477]
</p><p>61 Figure 2 shows the efﬁciency of the experiment design as a function of ρ and the prior estimate θ02 used to compute the Jacobian matrix. [sent-117, score-0.781]
</p><p>62 The E-efﬁciency of a design is deﬁned to be  ˆ λmax cov θ|θ, ξ0 efﬁciency  ˆ λmax cov θ|θ0 , ξρ  . [sent-118, score-0.625]
</p><p>63 (11)  If the Jacobian is computed at the correct point in parameter space the optimal design achieves maximal efﬁciency. [sent-119, score-0.697]
</p><p>64 As the distance between θ0 and θ grows the efﬁciency of the optimal design decreases rapidly. [sent-120, score-0.593]
</p><p>65 If the estimate, θ02 , is eight instead of the true value, two, the efﬁciency of the optimal design at θ0 is 36% of the optimal design at θ. [sent-121, score-1.186]
</p><p>66 However, at the cost of a decrease in efﬁciency for parameter estimates close to the true parameter value we guarantee the efﬁciency is better for points further from the true parameters with a robust design. [sent-122, score-0.406]
</p><p>67 001 the robust design is less efﬁcient for the range 0 < θ02 < 7, but is more efﬁcient for 7 < θ02 < 16. [sent-124, score-0.746]
</p><p>68 2  Calcium Signal Transduction Model  When certain small molecule ligands such as the anaphylatoxin C5a are introduced into the environment of an immune cell a complex chain of chemical reactions leads to the  1 ρ=0 ρ=1e−3 ρ=1e−2 ρ=1e−1 ρ=10  0. [sent-126, score-0.255]
</p><p>69 1 0 0  2  4  6  8 θ02  10  12  14  16  Figure 2: Efﬁciency of robust designs as a function of ρ and perturbations in the prior parameter estimate θ02 . [sent-135, score-0.607]
</p><p>70 transduction of the extracellular ligand concentration information and a transient increase in the intracellular calcium concentration. [sent-136, score-0.651]
</p><p>71 This chain of reactions can be mathematically modeled using the principles of mass-action kinetics and nonlinear ordinary differential equations. [sent-137, score-0.208]
</p><p>72 The menu of available experiments is indexed by one of two different cell lines in combination with different ligand doses. [sent-139, score-0.384]
</p><p>73 The cell lines are: wild-type and a GRK2 knockdown line. [sent-140, score-0.193]
</p><p>74 GRK2 is a protein that represses signaling in the G-protein receptor complex. [sent-141, score-0.147]
</p><p>75 When its concentration is decreased with interfering RNA the repression of the signal due to GRK2 is reduced. [sent-142, score-0.194]
</p><p>76 There are 17 experiments on the menu and we choose to do 100 experiments allocated according the experiment design. [sent-143, score-0.42]
</p><p>77 For each experiment we are able measure the transient calcium spike peak height using a ﬂuorescent calcium dye. [sent-144, score-0.843]
</p><p>78 We have selected the initial parameter estimates based on a least-squares ﬁt to a separate data set of 67 experiments on a wild-type cell line with a ligand concentration of 250nM. [sent-146, score-0.501]
</p><p>79 Observations are simulated from these data to obtain the least-squares parameter estimate for the optimal, robust (ρ = 1. [sent-148, score-0.345]
</p><p>80 Figure 3 shows the model ﬁts with associated 95% conﬁdence bands for the wild-type and knockdown cell lines for the parameter estimates from the three experiment designs. [sent-150, score-0.54]
</p><p>81 A separate validation data set is generated uniformly across the design menu. [sent-151, score-0.523]
</p><p>82 Compared to the optimal design, the parameter estimates based on the robust design provide a better ﬁt across the whole dose range for both cell types as measured by mean-squared residual error. [sent-152, score-1.061]
</p><p>83 Note also that the measured response at high ligand concentration is better ﬁt with parameters estimated from the robust design. [sent-153, score-0.477]
</p><p>84 Near 1µM of C5a concentration the peak height is predicted to decrease slightly in the wild-type cell line, but plateaus for the GRK2 knockdown cell line. [sent-154, score-0.594]
</p><p>85 05  0 −4 10  −2  10  0  10  [C5A] (uM)  Figure 3: Model predictions based on the least squares parameter estimate using data observed from the optimal, robust and uniform design. [sent-168, score-0.391]
</p><p>86 The predicted peak height curve (black line) based on the robust design data is shifted to the left compared to the peak height curve based on the optimal design data and matches the validation sample (shown as blue dots) more accurately. [sent-169, score-1.757]
</p><p>87 6  Discussion  The methodology of optimal experiment design leads to efﬁcient algorithms for the construction of designs in general nonlinear situations [15]. [sent-170, score-1.02]
</p><p>88 However, these varianceminimizing designs fail to account for uncertainty in the nominal parameter estimate and the model. [sent-171, score-0.385]
</p><p>89 We demonstrated this robust experiment design method on two example systems. [sent-173, score-0.955]
</p><p>90 In the Michaelis-Menten model, we showed that the E-optimal design is recovered for ρ = 0 and the uniform design is recovered as ρ → ρmax . [sent-174, score-1.15]
</p><p>91 It was also shown that the robust design is more efﬁcient than the optimal for large perturbations of the nominal parameter estimate away from the true parameter. [sent-175, score-1.154]
</p><p>92 The second example, of a calcium signal transduction model, is a more realistic case of the need for experiment design in high-throughput biological research. [sent-176, score-1.234]
</p><p>93 The model captures some of the important kinetics of the system, but is far from complete. [sent-177, score-0.129]
</p><p>94 We require a reasonably accurate model to make further predictions about the system and drive a set of experiments to estimate critical parameters of the model more accurately. [sent-178, score-0.135]
</p><p>95 The resulting robust design spreads some experiments across the menu, but also concentrates on experiments that will help minimize the variance of the parameter estimates. [sent-179, score-0.879]
</p><p>96 These robust experiment designs were obtained using SeDuMi 1. [sent-180, score-0.57]
</p><p>97 The design for the calcium signal transduction model takes approximately one second on a 2GHz processor, which is less time than required to compute the Jacobian matrix for the model. [sent-182, score-1.004]
</p><p>98 Research in machine learning has led to signiﬁcant advances in computationally-efﬁcient  data analysis methods, allowing increasingly complex models to be ﬁt to biological data. [sent-183, score-0.134]
</p><p>99 Challenges in experimental design are the ﬂip side of this coin—for complex models to be useful in closing the loop in biological research it is essential to begin to focus on the development of computationally-efﬁcient experimental design methods. [sent-184, score-1.285]
</p><p>100 Metabotropic receptor activation, desensitization and sequestrationi: modelling calcium and inositol 1,4,5-trisphosphate dynamics following receptor activation. [sent-280, score-0.422]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('design', 0.523), ('robust', 0.223), ('experiment', 0.209), ('calcium', 0.202), ('vi', 0.199), ('wi', 0.172), ('jacobian', 0.161), ('transduction', 0.161), ('menu', 0.151), ('sip', 0.145), ('designs', 0.138), ('concentration', 0.128), ('ligand', 0.126), ('perturbations', 0.124), ('knockdown', 0.116), ('receptor', 0.11), ('reaction', 0.11), ('height', 0.106), ('substrate', 0.101), ('kinetics', 0.101), ('maximin', 0.101), ('biological', 0.098), ('semide', 0.097), ('ciency', 0.093), ('nominal', 0.092), ('peak', 0.09), ('blkdiag', 0.087), ('enzyme', 0.087), ('grk', 0.087), ('ip', 0.085), ('cell', 0.077), ('um', 0.074), ('parameter', 0.073), ('optimal', 0.07), ('uncertain', 0.066), ('wt', 0.064), ('siam', 0.062), ('biologist', 0.058), ('dose', 0.058), ('reactions', 0.058), ('rnai', 0.058), ('sedumi', 0.058), ('covariance', 0.055), ('cov', 0.051), ('vandenberghe', 0.05), ('estimate', 0.049), ('matrix', 0.049), ('nonlinear', 0.049), ('uniform', 0.046), ('controllable', 0.046), ('immune', 0.046), ('velocity', 0.045), ('fisher', 0.045), ('max', 0.044), ('hughes', 0.043), ('signal', 0.041), ('ghaoui', 0.04), ('boyd', 0.04), ('experimental', 0.04), ('berkeley', 0.04), ('ef', 0.04), ('pathway', 0.038), ('nih', 0.038), ('chemical', 0.038), ('howard', 0.038), ('program', 0.038), ('california', 0.037), ('cellular', 0.037), ('signaling', 0.037), ('estimates', 0.037), ('complex', 0.036), ('optimization', 0.035), ('moderate', 0.035), ('el', 0.035), ('determinant', 0.035), ('recast', 0.035), ('transient', 0.034), ('nite', 0.034), ('uncertainty', 0.033), ('allocation', 0.032), ('methodology', 0.031), ('maximal', 0.031), ('experiments', 0.03), ('initial', 0.03), ('recovered', 0.029), ('culties', 0.028), ('model', 0.028), ('adopt', 0.027), ('medical', 0.026), ('kl', 0.026), ('matches', 0.026), ('particularly', 0.026), ('aiding', 0.025), ('fuse', 0.025), ('experimenters', 0.025), ('rna', 0.025), ('biosciences', 0.025), ('andy', 0.025), ('interfering', 0.025), ('closing', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="167-tfidf-1" href="./nips-2005-Robust_design_of_biological_experiments.html">167 nips-2005-Robust design of biological experiments</a></p>
<p>Author: Patrick Flaherty, Adam Arkin, Michael I. Jordan</p><p>Abstract: We address the problem of robust, computationally-efﬁcient design of biological experiments. Classical optimal experiment design methods have not been widely adopted in biological practice, in part because the resulting designs can be very brittle if the nominal parameter estimates for the model are poor, and in part because of computational constraints. We present a method for robust experiment design based on a semideﬁnite programming relaxation. We present an application of this method to the design of experiments for a complex calcium signal transduction pathway, where we have found that the parameter estimates obtained from the robust design are better than those obtained from an “optimal” design. 1</p><p>2 0.12642372 <a title="167-tfidf-2" href="./nips-2005-Robust_Fisher_Discriminant_Analysis.html">166 nips-2005-Robust Fisher Discriminant Analysis</a></p>
<p>Author: Seung-jean Kim, Alessandro Magnani, Stephen Boyd</p><p>Abstract: Fisher linear discriminant analysis (LDA) can be sensitive to the problem data. Robust Fisher LDA can systematically alleviate the sensitivity problem by explicitly incorporating a model of data uncertainty in a classiﬁcation problem and optimizing for the worst-case scenario under this model. The main contribution of this paper is show that with general convex uncertainty models on the problem data, robust Fisher LDA can be carried out using convex optimization. For a certain type of product form uncertainty model, robust Fisher LDA can be carried out at a cost comparable to standard Fisher LDA. The method is demonstrated with some numerical examples. Finally, we show how to extend these results to robust kernel Fisher discriminant analysis, i.e., robust Fisher LDA in a high dimensional feature space. 1</p><p>3 0.10629778 <a title="167-tfidf-3" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>Author: Tong Zhang, Rie Kubota Ando</p><p>Abstract: We consider a framework for semi-supervised learning using spectral decomposition based un-supervised kernel design. This approach subsumes a class of previously proposed semi-supervised learning methods on data graphs. We examine various theoretical properties of such methods. In particular, we derive a generalization performance bound, and obtain the optimal kernel design by minimizing the bound. Based on the theoretical analysis, we are able to demonstrate why spectral kernel design based methods can often improve the predictive performance. Experiments are used to illustrate the main consequences of our analysis.</p><p>4 0.079745196 <a title="167-tfidf-4" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>Author: Anatoli Juditsky, Alexander Nazin, Alexandre Tsybakov, Nicolas Vayatis</p><p>Abstract: We consider the problem of constructing an aggregated estimator from a ﬁnite class of base functions which approximately minimizes a convex risk functional under the ℓ1 constraint. For this purpose, we propose a stochastic procedure, the mirror descent, which performs gradient descent in the dual space. The generated estimates are additionally averaged in a recursive fashion with speciﬁc weights. Mirror descent algorithms have been developed in different contexts and they are known to be particularly efﬁcient in high dimensional problems. Moreover their implementation is adapted to the online setting. The main result of the paper is the upper bound on the convergence rate for the generalization error. 1</p><p>5 0.078840546 <a title="167-tfidf-5" href="./nips-2005-Soft_Clustering_on_Graphs.html">178 nips-2005-Soft Clustering on Graphs</a></p>
<p>Author: Kai Yu, Shipeng Yu, Volker Tresp</p><p>Abstract: We propose a simple clustering framework on graphs encoding pairwise data similarities. Unlike usual similarity-based methods, the approach softly assigns data to clusters in a probabilistic way. More importantly, a hierarchical clustering is naturally derived in this framework to gradually merge lower-level clusters into higher-level ones. A random walk analysis indicates that the algorithm exposes clustering structures in various resolutions, i.e., a higher level statistically models a longer-term diffusion on graphs and thus discovers a more global clustering structure. Finally we provide very encouraging experimental results. 1</p><p>6 0.074081533 <a title="167-tfidf-6" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>7 0.073655032 <a title="167-tfidf-7" href="./nips-2005-Large-scale_biophysical_parameter_estimation_in_single_neurons_via_constrained_linear_regression.html">106 nips-2005-Large-scale biophysical parameter estimation in single neurons via constrained linear regression</a></p>
<p>8 0.063594781 <a title="167-tfidf-8" href="./nips-2005-Nonparametric_inference_of_prior_probabilities_from_Bayes-optimal_behavior.html">140 nips-2005-Nonparametric inference of prior probabilities from Bayes-optimal behavior</a></p>
<p>9 0.062493138 <a title="167-tfidf-9" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>10 0.061028212 <a title="167-tfidf-10" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>11 0.058005761 <a title="167-tfidf-11" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>12 0.057274409 <a title="167-tfidf-12" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>13 0.055134263 <a title="167-tfidf-13" href="./nips-2005-Fusion_of_Similarity_Data_in_Clustering.html">79 nips-2005-Fusion of Similarity Data in Clustering</a></p>
<p>14 0.051838223 <a title="167-tfidf-14" href="./nips-2005-Active_Learning_For_Identifying_Function_Threshold_Boundaries.html">18 nips-2005-Active Learning For Identifying Function Threshold Boundaries</a></p>
<p>15 0.049376495 <a title="167-tfidf-15" href="./nips-2005-A_Criterion_for_the_Convergence_of_Learning_with_Spike_Timing_Dependent_Plasticity.html">8 nips-2005-A Criterion for the Convergence of Learning with Spike Timing Dependent Plasticity</a></p>
<p>16 0.048607666 <a title="167-tfidf-16" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>17 0.046868391 <a title="167-tfidf-17" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<p>18 0.044910584 <a title="167-tfidf-18" href="./nips-2005-Augmented_Rescorla-Wagner_and_Maximum_Likelihood_Estimation.html">32 nips-2005-Augmented Rescorla-Wagner and Maximum Likelihood Estimation</a></p>
<p>19 0.044259883 <a title="167-tfidf-19" href="./nips-2005-Top-Down_Control_of_Visual_Attention%3A_A_Rational_Account.html">194 nips-2005-Top-Down Control of Visual Attention: A Rational Account</a></p>
<p>20 0.044018567 <a title="167-tfidf-20" href="./nips-2005-Improved_risk_tail_bounds_for_on-line_algorithms.html">95 nips-2005-Improved risk tail bounds for on-line algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.17), (1, 0.003), (2, -0.026), (3, -0.028), (4, -0.012), (5, 0.013), (6, -0.055), (7, -0.075), (8, -0.069), (9, -0.03), (10, -0.011), (11, -0.012), (12, -0.084), (13, 0.009), (14, -0.01), (15, -0.04), (16, -0.022), (17, 0.056), (18, -0.131), (19, -0.037), (20, -0.061), (21, -0.001), (22, -0.125), (23, 0.031), (24, -0.041), (25, 0.013), (26, 0.104), (27, -0.18), (28, 0.028), (29, -0.014), (30, -0.027), (31, -0.097), (32, -0.079), (33, -0.07), (34, 0.03), (35, 0.248), (36, -0.183), (37, 0.033), (38, 0.113), (39, 0.097), (40, 0.065), (41, 0.043), (42, 0.048), (43, -0.125), (44, 0.06), (45, -0.054), (46, -0.034), (47, 0.021), (48, -0.048), (49, 0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97780472 <a title="167-lsi-1" href="./nips-2005-Robust_design_of_biological_experiments.html">167 nips-2005-Robust design of biological experiments</a></p>
<p>Author: Patrick Flaherty, Adam Arkin, Michael I. Jordan</p><p>Abstract: We address the problem of robust, computationally-efﬁcient design of biological experiments. Classical optimal experiment design methods have not been widely adopted in biological practice, in part because the resulting designs can be very brittle if the nominal parameter estimates for the model are poor, and in part because of computational constraints. We present a method for robust experiment design based on a semideﬁnite programming relaxation. We present an application of this method to the design of experiments for a complex calcium signal transduction pathway, where we have found that the parameter estimates obtained from the robust design are better than those obtained from an “optimal” design. 1</p><p>2 0.67543858 <a title="167-lsi-2" href="./nips-2005-Robust_Fisher_Discriminant_Analysis.html">166 nips-2005-Robust Fisher Discriminant Analysis</a></p>
<p>Author: Seung-jean Kim, Alessandro Magnani, Stephen Boyd</p><p>Abstract: Fisher linear discriminant analysis (LDA) can be sensitive to the problem data. Robust Fisher LDA can systematically alleviate the sensitivity problem by explicitly incorporating a model of data uncertainty in a classiﬁcation problem and optimizing for the worst-case scenario under this model. The main contribution of this paper is show that with general convex uncertainty models on the problem data, robust Fisher LDA can be carried out using convex optimization. For a certain type of product form uncertainty model, robust Fisher LDA can be carried out at a cost comparable to standard Fisher LDA. The method is demonstrated with some numerical examples. Finally, we show how to extend these results to robust kernel Fisher discriminant analysis, i.e., robust Fisher LDA in a high dimensional feature space. 1</p><p>3 0.44060475 <a title="167-lsi-3" href="./nips-2005-Large-scale_biophysical_parameter_estimation_in_single_neurons_via_constrained_linear_regression.html">106 nips-2005-Large-scale biophysical parameter estimation in single neurons via constrained linear regression</a></p>
<p>Author: Misha Ahrens, Liam Paninski, Quentin J. Huys</p><p>Abstract: Our understanding of the input-output function of single cells has been substantially advanced by biophysically accurate multi-compartmental models. The large number of parameters needing hand tuning in these models has, however, somewhat hampered their applicability and interpretability. Here we propose a simple and well-founded method for automatic estimation of many of these key parameters: 1) the spatial distribution of channel densities on the cell’s membrane; 2) the spatiotemporal pattern of synaptic input; 3) the channels’ reversal potentials; 4) the intercompartmental conductances; and 5) the noise level in each compartment. We assume experimental access to: a) the spatiotemporal voltage signal in the dendrite (or some contiguous subpart thereof, e.g. via voltage sensitive imaging techniques), b) an approximate kinetic description of the channels and synapses present in each compartment, and c) the morphology of the part of the neuron under investigation. The key observation is that, given data a)-c), all of the parameters 1)-4) may be simultaneously inferred by a version of constrained linear regression; this regression, in turn, is efﬁciently solved using standard algorithms, without any “local minima” problems despite the large number of parameters and complex dynamics. The noise level 5) may also be estimated by standard techniques. We demonstrate the method’s accuracy on several model datasets, and describe techniques for quantifying the uncertainty in our estimates. 1</p><p>4 0.41107163 <a title="167-lsi-4" href="./nips-2005-Augmented_Rescorla-Wagner_and_Maximum_Likelihood_Estimation.html">32 nips-2005-Augmented Rescorla-Wagner and Maximum Likelihood Estimation</a></p>
<p>Author: Alan L. Yuille</p><p>Abstract: We show that linear generalizations of Rescorla-Wagner can perform Maximum Likelihood estimation of the parameters of all generative models for causal reasoning. Our approach involves augmenting variables to deal with conjunctions of causes, similar to the agumented model of Rescorla. Our results involve genericity assumptions on the distributions of causes. If these assumptions are violated, for example for the Cheng causal power theory, then we show that a linear Rescorla-Wagner can estimate the parameters of the model up to a nonlinear transformtion. Moreover, a nonlinear Rescorla-Wagner is able to estimate the parameters directly to within arbitrary accuracy. Previous results can be used to determine convergence and to estimate convergence rates. 1</p><p>5 0.39854652 <a title="167-lsi-5" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>Author: Anatoli Juditsky, Alexander Nazin, Alexandre Tsybakov, Nicolas Vayatis</p><p>Abstract: We consider the problem of constructing an aggregated estimator from a ﬁnite class of base functions which approximately minimizes a convex risk functional under the ℓ1 constraint. For this purpose, we propose a stochastic procedure, the mirror descent, which performs gradient descent in the dual space. The generated estimates are additionally averaged in a recursive fashion with speciﬁc weights. Mirror descent algorithms have been developed in different contexts and they are known to be particularly efﬁcient in high dimensional problems. Moreover their implementation is adapted to the online setting. The main result of the paper is the upper bound on the convergence rate for the generalization error. 1</p><p>6 0.39810297 <a title="167-lsi-6" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>7 0.35047105 <a title="167-lsi-7" href="./nips-2005-Gaussian_Processes_for_Multiuser_Detection_in_CDMA_receivers.html">81 nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</a></p>
<p>8 0.34968928 <a title="167-lsi-8" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>9 0.34946516 <a title="167-lsi-9" href="./nips-2005-Beyond_Gaussian_Processes%3A_On_the_Distributions_of_Infinite_Networks.html">38 nips-2005-Beyond Gaussian Processes: On the Distributions of Infinite Networks</a></p>
<p>10 0.34409624 <a title="167-lsi-10" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>11 0.34346589 <a title="167-lsi-11" href="./nips-2005-Metric_Learning_by_Collapsing_Classes.html">126 nips-2005-Metric Learning by Collapsing Classes</a></p>
<p>12 0.34208027 <a title="167-lsi-12" href="./nips-2005-Modeling_Memory_Transfer_and_Saving_in_Cerebellar_Motor_Learning.html">128 nips-2005-Modeling Memory Transfer and Saving in Cerebellar Motor Learning</a></p>
<p>13 0.33019033 <a title="167-lsi-13" href="./nips-2005-Learning_Topology_with_the_Generative_Gaussian_Graph_and_the_EM_Algorithm.html">116 nips-2005-Learning Topology with the Generative Gaussian Graph and the EM Algorithm</a></p>
<p>14 0.32976675 <a title="167-lsi-14" href="./nips-2005-Soft_Clustering_on_Graphs.html">178 nips-2005-Soft Clustering on Graphs</a></p>
<p>15 0.32910013 <a title="167-lsi-15" href="./nips-2005-Nonparametric_inference_of_prior_probabilities_from_Bayes-optimal_behavior.html">140 nips-2005-Nonparametric inference of prior probabilities from Bayes-optimal behavior</a></p>
<p>16 0.31331897 <a title="167-lsi-16" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>17 0.30738398 <a title="167-lsi-17" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>18 0.3052845 <a title="167-lsi-18" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>19 0.30429333 <a title="167-lsi-19" href="./nips-2005-A_Bayes_Rule_for_Density_Matrices.html">2 nips-2005-A Bayes Rule for Density Matrices</a></p>
<p>20 0.29047528 <a title="167-lsi-20" href="./nips-2005-Q-Clustering.html">159 nips-2005-Q-Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.047), (10, 0.04), (18, 0.011), (27, 0.026), (31, 0.056), (34, 0.071), (38, 0.32), (39, 0.01), (50, 0.013), (55, 0.041), (57, 0.013), (69, 0.078), (73, 0.041), (77, 0.017), (88, 0.076), (91, 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79729432 <a title="167-lda-1" href="./nips-2005-Robust_design_of_biological_experiments.html">167 nips-2005-Robust design of biological experiments</a></p>
<p>Author: Patrick Flaherty, Adam Arkin, Michael I. Jordan</p><p>Abstract: We address the problem of robust, computationally-efﬁcient design of biological experiments. Classical optimal experiment design methods have not been widely adopted in biological practice, in part because the resulting designs can be very brittle if the nominal parameter estimates for the model are poor, and in part because of computational constraints. We present a method for robust experiment design based on a semideﬁnite programming relaxation. We present an application of this method to the design of experiments for a complex calcium signal transduction pathway, where we have found that the parameter estimates obtained from the robust design are better than those obtained from an “optimal” design. 1</p><p>2 0.45496881 <a title="167-lda-2" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>Author: O. P. Kreidl, Alan S. Willsky</p><p>Abstract: Given a directed graphical model with binary-valued hidden nodes and real-valued noisy observations, consider deciding upon the maximum a-posteriori (MAP) or the maximum posterior-marginal (MPM) assignment under the restriction that each node broadcasts only to its children exactly one single-bit message. We present a variational formulation, viewing the processing rules local to all nodes as degrees-of-freedom, that minimizes the loss in expected (MAP or MPM) performance subject to such online communication constraints. The approach leads to a novel message-passing algorithm to be executed ofﬂine, or before observations are realized, which mitigates the performance loss by iteratively coupling all rules in a manner implicitly driven by global statistics. We also provide (i) illustrative examples, (ii) assumptions that guarantee convergence and efﬁciency and (iii) connections to active research areas. 1</p><p>3 0.44946304 <a title="167-lda-3" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>Author: Jeremy Kubica, Joseph Masiero, Robert Jedicke, Andrew Connolly, Andrew W. Moore</p><p>Abstract: In this paper we consider the problem of ﬁnding sets of points that conform to a given underlying model from within a dense, noisy set of observations. This problem is motivated by the task of efﬁciently linking faint asteroid detections, but is applicable to a range of spatial queries. We survey current tree-based approaches, showing a trade-off exists between single tree and multiple tree algorithms. To this end, we present a new type of multiple tree algorithm that uses a variable number of trees to exploit the advantages of both approaches. We empirically show that this algorithm performs well using both simulated and astronomical data.</p><p>4 0.44255644 <a title="167-lda-4" href="./nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs.html">90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</a></p>
<p>Author: Firas Hamze, Nando de Freitas</p><p>Abstract: This paper presents a new sampling algorithm for approximating functions of variables representable as undirected graphical models of arbitrary connectivity with pairwise potentials, as well as for estimating the notoriously difﬁcult partition function of the graph. The algorithm ﬁts into the framework of sequential Monte Carlo methods rather than the more widely used MCMC, and relies on constructing a sequence of intermediate distributions which get closer to the desired one. While the idea of using “tempered” proposals is known, we construct a novel sequence of target distributions where, rather than dropping a global temperature parameter, we sequentially couple individual pairs of variables that are, initially, sampled exactly from a spanning tree of the variables. We present experimental results on inference and estimation of the partition function for sparse and densely-connected graphs.</p><p>5 0.44178331 <a title="167-lda-5" href="./nips-2005-Kernelized_Infomax_Clustering.html">102 nips-2005-Kernelized Infomax Clustering</a></p>
<p>Author: David Barber, Felix V. Agakov</p><p>Abstract: We propose a simple information-theoretic approach to soft clustering based on maximizing the mutual information I(x, y) between the unknown cluster labels y and the training patterns x with respect to parameters of speciﬁcally constrained encoding distributions. The constraints are chosen such that patterns are likely to be clustered similarly if they lie close to speciﬁc unknown vectors in the feature space. The method may be conveniently applied to learning the optimal aﬃnity matrix, which corresponds to learning parameters of the kernelized encoder. The procedure does not require computations of eigenvalues of the Gram matrices, which makes it potentially attractive for clustering large data sets. 1</p><p>6 0.44112688 <a title="167-lda-6" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>7 0.44093788 <a title="167-lda-7" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>8 0.44027746 <a title="167-lda-8" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>9 0.43980283 <a title="167-lda-9" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>10 0.43930578 <a title="167-lda-10" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>11 0.43880537 <a title="167-lda-11" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>12 0.4380483 <a title="167-lda-12" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>13 0.43479988 <a title="167-lda-13" href="./nips-2005-Augmented_Rescorla-Wagner_and_Maximum_Likelihood_Estimation.html">32 nips-2005-Augmented Rescorla-Wagner and Maximum Likelihood Estimation</a></p>
<p>14 0.43423539 <a title="167-lda-14" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>15 0.43369058 <a title="167-lda-15" href="./nips-2005-Conditional_Visual_Tracking_in_Kernel_Space.html">45 nips-2005-Conditional Visual Tracking in Kernel Space</a></p>
<p>16 0.43295982 <a title="167-lda-16" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>17 0.43280426 <a title="167-lda-17" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>18 0.43242961 <a title="167-lda-18" href="./nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<p>19 0.43190008 <a title="167-lda-19" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>20 0.4315336 <a title="167-lda-20" href="./nips-2005-Sparse_Gaussian_Processes_using_Pseudo-inputs.html">179 nips-2005-Sparse Gaussian Processes using Pseudo-inputs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
