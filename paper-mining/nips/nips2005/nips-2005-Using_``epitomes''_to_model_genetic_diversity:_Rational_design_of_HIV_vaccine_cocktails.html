<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>198 nips-2005-Using ``epitomes'' to model genetic diversity: Rational design of HIV vaccine cocktails</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-198" href="#">nips2005-198</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>198 nips-2005-Using ``epitomes'' to model genetic diversity: Rational design of HIV vaccine cocktails</h1>
<br/><p>Source: <a title="nips-2005-198-pdf" href="http://papers.nips.cc/paper/2895-using-epitomes-to-model-genetic-diversity-rational-design-of-hiv-vaccine-cocktails.pdf">pdf</a></p><p>Author: Nebojsa Jojic, Vladimir Jojic, Christopher Meek, David Heckerman, Brendan J. Frey</p><p>Abstract: We introduce a new model of genetic diversity which summarizes a large input dataset into an epitome, a short sequence or a small set of short sequences of probability distributions capturing many overlapping subsequences from the dataset. The epitome as a representation has already been used in modeling real-valued signals, such as images and audio. The discrete sequence model we introduce in this paper targets applications in genetics, from multiple alignment to recombination and mutation inference. In our experiments, we concentrate on modeling the diversity of HIV where the epitome emerges as a natural model for producing relatively small vaccines covering a large number of immune system targets known as epitopes. Our experiments show that the epitome includes more epitopes than other vaccine designs of similar length, including cocktails of consensus strains, phylogenetic tree centers, and observed strains. We also discuss epitome designs that take into account uncertainty about Tcell cross reactivity and epitope presentation. In our experiments, we ﬁnd that vaccine optimization is fairly robust to these uncertainties. 1</p><p>Reference: <a title="nips-2005-198-reference" href="../nips2005_reference/nips-2005-Using_%60%60epitomes%27%27_to_model_genetic_diversity%3A_Rational_design_of_HIV_vaccine_cocktails_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The epitome as a representation has already been used in modeling real-valued signals, such as images and audio. [sent-2, score-0.79]
</p><p>2 In our experiments, we concentrate on modeling the diversity of HIV where the epitome emerges as a natural model for producing relatively small vaccines covering a large number of immune system targets known as epitopes. [sent-4, score-1.061]
</p><p>3 Our experiments show that the epitome includes more epitopes than other vaccine designs of similar length, including cocktails of consensus strains, phylogenetic tree centers, and observed strains. [sent-5, score-1.246]
</p><p>4 We also discuss epitome designs that take into account uncertainty about Tcell cross reactivity and epitope presentation. [sent-6, score-0.99]
</p><p>5 In our experiments, we ﬁnd that vaccine optimization is fairly robust to these uncertainties. [sent-7, score-0.222]
</p><p>6 Instead of deﬁning size, variability and typical relative locations of repeating fragments manually, in an application-driven way, the ‘epitomic analysis’ [5] is an unsupervised approach to estimating repeating fragment models, and simultaneously aligning the data to them. [sent-13, score-0.189]
</p><p>7 This paper introduces a new form of the epitome as a sequence of multinomial distributions (Fig. [sent-16, score-0.845]
</p><p>8 1), and describe its applications to HIV diversity modeling and rational vaccine design. [sent-17, score-0.325]
</p><p>9 We show that the vaccines optimized using our algorithms are likely to have broader predicted coverage of immune targets in HIV than the previous rational designs. [sent-18, score-0.384]
</p><p>10 The generating proﬁle sequence  Data (colorcoded according to the posterior epitome mapping Q(T)) p(T) e  Figure 1: The epitome (e) learned from data synthesized from the generating proﬁle sequence (Section 5). [sent-19, score-1.759]
</p><p>11 A color coding in the epitome and data sequences is used to show the mapping between epitome and data positions. [sent-20, score-1.587]
</p><p>12 The distribution p(T ) shows which 9mers from the epitome were more likely to generate patches of the data. [sent-22, score-0.861]
</p><p>13 The sequences are generated synthetically by combining the pieces of the proﬁle sequence given in the ﬁrst line of the ﬁgure, with occasional insertions of random sequence fragments, as discussed in Section 5. [sent-26, score-0.185]
</p><p>14 Sequence variability in this synthetic example is slightly higher than that found in the NEF protein of the human immunodeﬁciency virus (HIV) [7], while the envelope proteins of the same virus exhibit more variability. [sent-27, score-0.236]
</p><p>15 Examples of high genetic diversity can also be found in higher-level organisms, for example in the regions coding for immune system’s pattern recognition molecules. [sent-28, score-0.191]
</p><p>16 The last row in the ﬁgure illustrates an epitome optimized to represent the variability in the sequences above. [sent-29, score-0.865]
</p><p>17 In general, the epitome is a smaller array E = {emn } of size Me × Ne , where Me Ne M N . [sent-30, score-0.769]
</p><p>18 An epitome can be parameterized in different ways, but in the ﬁgure, each epitome element emn is a multinomial distribution with the probability of each letter represented by its height. [sent-32, score-1.591]
</p><p>19 For instance, the set S = {(4, 8), (4, 9), (4, 10), (4, 11)} points to a contiguous patch of letters in the fourth sequence XS = RQKK. [sent-35, score-0.181]
</p><p>20 (This reduces local minima problems in the EM algorithm for epitome learning as discussed in Sections 4 and 5). [sent-39, score-0.769]
</p><p>21 |T |  • Generate a patch XS from ET according to p(XS |ET ) = k=1 eT (k) (XS(k) ), with T (k) and S(k) denoting the k-th element in the epitome and data patches. [sent-48, score-0.838]
</p><p>22 In Section 4, we discuss algorithms for estimating the epitome distributions. [sent-53, score-0.769]
</p><p>23 Our illustration points to possible applications of epitomes to multiple sequence alignment, and therefore requires a short discussion on similarity to other biological sequence models [3]. [sent-54, score-0.234]
</p><p>24 The epitome’s robustness to the length, position and variability of repeating sequence fragments allows us to bypass both the task of optimal global alignment, and the problem of deﬁning the notion of global alignment. [sent-58, score-0.21]
</p><p>25 In addition, consideration of overlapping patches in a biological sequence can be viewed as modeling independent binding processes, making the patch independence assumption of our generative model biologically relevant. [sent-59, score-0.377]
</p><p>26 We illustrate these properties of the epitome on the problem of HIV diversity modeling and rational vaccine design. [sent-60, score-1.094]
</p><p>27 3  HIV evolution and rational vaccine design  Recent work on the rational design of HIV vaccines has turned to cocktail approaches with the intention of protecting a person against many possible variants of the HIV virus. [sent-61, score-0.447]
</p><p>28 One of the potential difﬁculties with cocktail design is vaccine size. [sent-62, score-0.268]
</p><p>29 In this section, we will show that epitome modeling can overcome this limitation by providing a means for generating smaller vaccines representing a wide diversity of HIV in an immunologically relevant way. [sent-64, score-1.014]
</p><p>30 We focus on the problem of constructing an optimal cellular vaccine in terms of its coverage of MHC-I epitopes, short contiguous patterns of 8-11 aminoacids in HIV proteins [8]. [sent-65, score-0.464]
</p><p>31 The killer cells and their offspring have the opportunity to bind to multiple infected cells, and so their ﬁrst binding to a particular foreign epitope is used to accelerate an immune reaction to other infected cells exposing the same epitope. [sent-68, score-0.52]
</p><p>32 The goal of vaccine design is to create artiﬁcal means to produce such immunological memory of a particular virus without the danger of developing the disease. [sent-70, score-0.286]
</p><p>33 In the case of a less variable virus, the vaccination may be possible by delivering a foreign protein similar to the viral protein into a patient’s cells, triggering the immune response. [sent-71, score-0.243]
</p><p>34 In fact, without appropriate optimization, the number of different proteins needed to cover the viral diversity would be too large for the known vaccine delivery mechanisms. [sent-73, score-0.362]
</p><p>35 It is well known that epitopes within and across the strains in a population overlap [7]. [sent-74, score-0.276]
</p><p>36 The epitome model naturally exploits this overlap to construct a vaccine that can prime the immune system to attack as many potential epitopes as possible. [sent-75, score-1.277]
</p><p>37 For instance, if the sequences in Fig 1 were HIV fragments from different strains of the virus, then the epitome would contain many potential epitopes of lengths 8-11 from these sequences. [sent-76, score-1.177]
</p><p>38 Furthermore, the context of the captured epitopes in the epitome is similar to the context in the epitomized sequences, which increases the chances of equivalent presentation of the epitome and data epitopes. [sent-77, score-1.775]
</p><p>39 Due to the limitation in MHC binding, each person’s cells are capable of presenting only a small number of epitopes from the invading virus, but an entire human population attacks a diverse set of epitopes. [sent-81, score-0.232]
</p><p>40 The MHC molecule selects the protein fragments for presentation through a binding process which is loosely motifspeciﬁc. [sent-82, score-0.223]
</p><p>41 Some of these processes can be inﬂuenced by a context of the epitope (short amino acid fragments in the regions on either side of the epitope). [sent-84, score-0.341]
</p><p>42 Another issue to be considered in HIV evolution and vaccine design is the T-cell cross reactivity: The killer cells primed with one epitope may be capable of binding to other related epitopes, and therefore a small set of priming epitopes may induce a broader immunity. [sent-85, score-0.755]
</p><p>43 The epitome model maps directly to these immunity variables. [sent-87, score-0.787]
</p><p>44 If the epitome content is to be delivered to a cell in the vaccination phase, then each patch ET indexed by data index set T corresponds either to an epitope or to a longer contiguous patch (e. [sent-88, score-1.156]
</p><p>45 12 amino acids or more) containing both an epitope and its context that inﬂuences presentation. [sent-90, score-0.252]
</p><p>46 The prior p(T ) reﬂects the probability of presentation of the epitome fragments, and should reﬂect processes invloved in presentation, including MHC binding. [sent-91, score-0.796]
</p><p>47 The presented epitome fragments ET in different patients’cells may prime T-cells capable of cross-reacting with some of the epitopes XS presented by the infected cells infected by one of the known strains in the dataset X. [sent-92, score-1.225]
</p><p>48 The cross-reaction distribution corresponds to the epitome distribution p(XS |ET ). [sent-93, score-0.769]
</p><p>49 Vaccination is successful if the vaccine primes the immune system to attack targets found in the known circulating strains. [sent-94, score-0.337]
</p><p>50 A natural criterion to optimize is the similarity between the distribution over the epitopes learned by the immune systems of patients vaccinated with the epitome (taking into account the cross-reactivity) and the distribution over the epitopes from circulating strains. [sent-95, score-1.282]
</p><p>51 Therefore, the vaccine quality directly depends on the likelihood of the designated epitopes p(XS ) under the epitome. [sent-96, score-0.393]
</p><p>52 To see this, consider directly optimizing the KL divergence between the distribution pd (Xs ) over epitopes found in the data and the distribution over the targets for which the T-cells are primed according to p(Xs ). [sent-97, score-0.303]
</p><p>53 This KL distance differs from the log likelihood of all the data patches weighted by pd (Xs ), log p({XS }d ) =  pd (XS ) log S  p(XS |ET )p(T ),  (2)  T  only by a constant (the entropy of pd (Xs )). [sent-98, score-0.311]
</p><p>54 The distribution pd (Xs ) can serve as the indicator of epitopes and be equal to either zero or a constant for all patches, and then the above weighted likelihood is equivalent to the total likelihood of selected patches. [sent-99, score-0.265]
</p><p>55 distribution can also reﬂect the probability of presentation of epitopes XS , or the uncertainty of the experiment or the prediction algorithm used to predict which parts of the circulating strains correspond to MHC epitopes. [sent-101, score-0.302]
</p><p>56 While the epitome can serve as a diversity model and be used to construct evolutionary models and peptides for experimental epitope discovery, it can also serve as as an actual immungen (the pattern containing the immunologically important message to the cell) in vaccine. [sent-102, score-1.063]
</p><p>57 The most general version of epitome as a sequence of mutlinomial distributions could be relevant for sequence classiﬁcation, recombination modeling, and design of peptides for binding essays. [sent-103, score-1.05]
</p><p>58 On the other hand, when the epitome is used for immunogen design, then cross-reactivity p(XS |ET ) can be conveniently captured by constraining each distribution emn to have probabilities for the twenty aminoacids from the set { 19 , 1 − }. [sent-105, score-0.823]
</p><p>59 The mode of the epitome can then be used as a deterministic vaccine immunogen3 , and the probability of cross-reaction will then directly depend on the number of letters in XS that are different from the mode of ET . [sent-106, score-0.991]
</p><p>60 While the epitome model components are mapped here to the elements of the interaction between HIV and the immune system of the host, other applications in biology would probably be based on a different semantics for the epitome components. [sent-107, score-1.625]
</p><p>61 We would expect that the epitome would map to biological sequence analysis problems more naturally than to image and audio modeling tasks, where the issue of the partition function arises. [sent-108, score-0.893]
</p><p>62 However, the strains of a virus are observed by the immune system through overlapping patches, independently sampled from the viral proteins by biological processes. [sent-111, score-0.357]
</p><p>63 More generally, epitome is compatible with the evolutionary forces that act independently on overlapping patches of a biological sequence. [sent-113, score-0.92]
</p><p>64 When modeling biological sequences, the free energy may be associated with real physical events, such as molecular binding processes, where log probabilities correspond to molecular binding energies. [sent-117, score-0.255]
</p><p>65 (7) p(T ) ← S S pd (XS ) The E step assigns a responsibility for S to each possible epitome patch. [sent-121, score-0.842]
</p><p>66 The M step reestimates the epitome multinomials using these responsibilities. [sent-122, score-0.769]
</p><p>67 For example, if we allowed the epitome to be longer, then some of the sites with two equally likely letters could be split into two separate regions of the epitome (and in some applications, such as vaccine optimization, this is preferred, as the epitomes need to become deterministic). [sent-128, score-1.833]
</p><p>68 In some situations, such as vaccine design, it is desirable to produce deterministic epitomes (containing pointmass probability distributions). [sent-131, score-0.274]
</p><p>69 →0  E  q  (8)  Finally, in cases when the probability mass is uniformly spread over the letters other than the modes of the epitome distributions, i. [sent-133, score-0.79]
</p><p>70 ,emn ( ) ∈ { 19 , 1 − }, the myopic optimization is a faster way of creating epitomes of high fragment (epitope) coverage than the EM with multiple initializations. [sent-135, score-0.235]
</p><p>71 The myopic optimization consists of iteratively increasing the length of the epitome by appending a patch (possibly with overlap) from the data which maximally reduces the free energy. [sent-136, score-0.903]
</p><p>72 5  Experiments  To illustrate the EM algorithm for epitome learning, we created the synthetic data shown (in part) in Figure 1. [sent-138, score-0.769]
</p><p>73 The data, eighty sequences in all, were synthesized from the generating proﬁle sequence of length ﬁfty shown on the top line of the ﬁgure. [sent-139, score-0.186]
</p><p>74 The resulting data sequences ranged in length from 38 to 43; and on average 80% of aminoacids in each sequence come from the generating sequence. [sent-142, score-0.195]
</p><p>75 We learned an epitome model using the EM algorithm applied to all 9mer patches from the data, equally weighted. [sent-144, score-0.876]
</p><p>76 We used a two-component epitome mixture, where the ﬁrst component is an (initially unknown) sequence of probability distributions, and the second component is a garbage component, useful for representing the random insertions and  mutations. [sent-145, score-0.907]
</p><p>77 Also, we used an epitome with a circular topology. [sent-149, score-0.769]
</p><p>78 The ﬁrst (non-garbage) component of the epitome learned after sixty iterations, shown in Figure 1, closely resembels the generating sequence even though it never saw this generating sequence during learning. [sent-150, score-1.007]
</p><p>79 (Roughly, the generating sequence starts near the end of the epitome with the patch “LIC” coded in red, and wraps around to the patch “EHQ” coded in yellow. [sent-151, score-1.001]
</p><p>80 The portion of the epitome between yellow and red is not responsible for many patches, as reﬂected in the distribution p(T ). [sent-152, score-0.769]
</p><p>81 For each iteration, we show the ﬁrst (non-garbage) component of the epitome E, the distribution p(T ), and the ﬁrst ten sequences in the dataset, color-coded according to the mode of q(T |S), as in Figure 1. [sent-158, score-0.835]
</p><p>82 The video illustrates how the EM algorithm simultaneously learns the epitome model and aligns the data sequences. [sent-159, score-0.791]
</p><p>83 When used for vaccine optimization, some epitome parameters can be preset based on biological knowledge. [sent-160, score-0.995]
</p><p>84 The advantage of the uniform data distribution is that we only need sequence data for vaccine optimization, and not the epitope identities. [sent-165, score-0.425]
</p><p>85 The free energy criterion can be easily shown to be proportional (with a negative constant) to the coverage - the percentage of all 10mers from the data covered by the epitome, where the 10mer is considered covered if it can be found as a contiguous patch in the epitome’s mode. [sent-166, score-0.238]
</p><p>86 Another advantage of this approach is that it can not miss epitopes due to errors in prediction algorithms or experimental epitope discovery, as long as sufﬁcient coverage can be guaranteed for the given vaccine length. [sent-167, score-0.68]
</p><p>87 The advantage of this approach is that we can potentially focus our modeling power only to immunologically important variability, as long as the known epitope dataset is sufﬁcient to capture properly the epitope distribution for at least the most frequent MHC-I molecules. [sent-170, score-0.378]
</p><p>88 Thus, for a given epitome length, we may obtain more potent vaccines than using the ﬁrst parameter setting. [sent-171, score-0.861]
</p><p>89 Since = 0, the resulting optimization reduces to optimizing expected epitope coverage, i. [sent-172, score-0.186]
</p><p>90 Both epitome models yield better coverage and the expected epitope coverage than other designs for any ﬁxed length. [sent-179, score-1.207]
</p><p>91 An interesting ﬁnding to note is that the epitome optimized for coverage (using uniform distribution pd (XS )) provides essentially equally good expected coverage as the epitome directly optimized for the expected coverage. [sent-181, score-1.911]
</p><p>92 This is less surprising than it may seem - both true and predicted epitopes overlap in the sequence data, and so epitomizing all 10mers leads to similar epitomes as optimizing for coverage of the select few, but frequently overlapping epitopes. [sent-182, score-0.508]
</p><p>93 This is a direct consequence of the epitome representation, which was found appealing in previous applications for the same robustness to the number and sizes of the overlapping patches. [sent-183, score-0.803]
</p><p>94 It also indicates the possibility that an effective vaccine can be optimized without precise knowledge of all HIV epitopes. [sent-184, score-0.229]
</p><p>95 For comparison, we show expected coverage for the epitome optimized to cover all 10mers. [sent-186, score-0.919]
</p><p>96 6  Conclusions  We have introduced the epitome as a new model of genetic diversity, especially well suited to highly variable biological sequences. [sent-187, score-0.828]
</p><p>97 We show that our model can be used to optimize HIV vaccines with larger predicted coverage of MHC-I epitopes than other constructs of similar lengths and so epitome can be used to create vaccines that cover a large fraction of HIV diversity. [sent-188, score-1.284]
</p><p>98 We also show that epitome optimization leads to good vaccines even when all subsequence of length 10 are considered epitopes. [sent-189, score-0.907]
</p><p>99 This suggests that the vaccines could be optimized directly from sequence data, which are technologically much easier to obtain than epitope data. [sent-190, score-0.344]
</p><p>100 Our analysis of cross-reactivity which provided similar empirical evidence of epitome robustness to cross-reactivity assumptions (see www. [sent-191, score-0.769]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('epitome', 0.769), ('xs', 0.311), ('vaccine', 0.201), ('epitopes', 0.192), ('hiv', 0.183), ('epitope', 0.165), ('coverage', 0.122), ('fragments', 0.094), ('patches', 0.092), ('mhc', 0.092), ('vaccines', 0.092), ('immune', 0.087), ('binding', 0.077), ('epitomes', 0.073), ('pd', 0.073), ('diversity', 0.07), ('patch', 0.069), ('virus', 0.064), ('proteins', 0.064), ('sequence', 0.059), ('amino', 0.058), ('strains', 0.056), ('sequences', 0.049), ('cocktail', 0.046), ('em', 0.042), ('repeating', 0.038), ('cocktails', 0.037), ('infected', 0.037), ('vaccination', 0.037), ('generating', 0.035), ('overlapping', 0.034), ('genetic', 0.034), ('rational', 0.033), ('contiguous', 0.032), ('peptides', 0.032), ('pro', 0.03), ('acids', 0.029), ('designs', 0.029), ('le', 0.028), ('optimized', 0.028), ('overlap', 0.028), ('aminoacids', 0.027), ('circulating', 0.027), ('emn', 0.027), ('gag', 0.027), ('garbage', 0.027), ('immunologically', 0.027), ('killer', 0.027), ('reactivity', 0.027), ('viral', 0.027), ('presentation', 0.027), ('et', 0.026), ('letter', 0.026), ('biological', 0.025), ('protein', 0.025), ('cells', 0.025), ('length', 0.025), ('acid', 0.024), ('foreign', 0.024), ('molecules', 0.024), ('targets', 0.022), ('video', 0.022), ('alignment', 0.022), ('mutation', 0.022), ('jojic', 0.022), ('modeling', 0.021), ('design', 0.021), ('letters', 0.021), ('optimization', 0.021), ('molecular', 0.02), ('myopic', 0.019), ('audio', 0.019), ('variability', 0.019), ('delivering', 0.018), ('epitomic', 0.018), ('epitomized', 0.018), ('immunity', 0.018), ('insertions', 0.018), ('nef', 0.018), ('phylip', 0.018), ('sixty', 0.018), ('strain', 0.018), ('syfpeithi', 0.018), ('synthesized', 0.018), ('consensus', 0.018), ('facial', 0.018), ('short', 0.018), ('distributions', 0.017), ('component', 0.017), ('lengths', 0.017), ('frey', 0.017), ('primed', 0.016), ('recombination', 0.016), ('settling', 0.016), ('bind', 0.016), ('priming', 0.016), ('cell', 0.015), ('energy', 0.015), ('capable', 0.015), ('learned', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="198-tfidf-1" href="./nips-2005-Using_%60%60epitomes%27%27_to_model_genetic_diversity%3A_Rational_design_of_HIV_vaccine_cocktails.html">198 nips-2005-Using ``epitomes'' to model genetic diversity: Rational design of HIV vaccine cocktails</a></p>
<p>Author: Nebojsa Jojic, Vladimir Jojic, Christopher Meek, David Heckerman, Brendan J. Frey</p><p>Abstract: We introduce a new model of genetic diversity which summarizes a large input dataset into an epitome, a short sequence or a small set of short sequences of probability distributions capturing many overlapping subsequences from the dataset. The epitome as a representation has already been used in modeling real-valued signals, such as images and audio. The discrete sequence model we introduce in this paper targets applications in genetics, from multiple alignment to recombination and mutation inference. In our experiments, we concentrate on modeling the diversity of HIV where the epitome emerges as a natural model for producing relatively small vaccines covering a large number of immune system targets known as epitopes. Our experiments show that the epitome includes more epitopes than other vaccine designs of similar length, including cocktails of consensus strains, phylogenetic tree centers, and observed strains. We also discuss epitome designs that take into account uncertainty about Tcell cross reactivity and epitope presentation. In our experiments, we ﬁnd that vaccine optimization is fairly robust to these uncertainties. 1</p><p>2 0.1575397 <a title="198-tfidf-2" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>Author: Martin J. Wainwright</p><p>Abstract: Consider the problem of joint parameter estimation and prediction in a Markov random ﬁeld: i.e., the model parameters are estimated on the basis of an initial set of data, and then the ﬁtted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working in the computation-limited setting, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for ﬁtting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the “wrong” model even in the inﬁnite data limit) is provably beneﬁcial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of variational methods. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product. 1 Keywords: Markov random ﬁelds; variational method; message-passing algorithms; sum-product; belief propagation; parameter estimation; learning. 1</p><p>3 0.046681855 <a title="198-tfidf-3" href="./nips-2005-Learning_Depth_from_Single_Monocular_Images.html">110 nips-2005-Learning Depth from Single Monocular Images</a></p>
<p>Author: Ashutosh Saxena, Sung H. Chung, Andrew Y. Ng</p><p>Abstract: We consider the task of depth estimation from a single monocular image. We take a supervised learning approach to this problem, in which we begin by collecting a training set of monocular images (of unstructured outdoor environments which include forests, trees, buildings, etc.) and their corresponding ground-truth depthmaps. Then, we apply supervised learning to predict the depthmap as a function of the image. Depth estimation is a challenging problem, since local features alone are insufﬁcient to estimate depth at a point, and one needs to consider the global context of the image. Our model uses a discriminatively-trained Markov Random Field (MRF) that incorporates multiscale local- and global-image features, and models both depths at individual points as well as the relation between depths at different points. We show that, even on unstructured scenes, our algorithm is frequently able to recover fairly accurate depthmaps. 1</p><p>4 0.046527725 <a title="198-tfidf-4" href="./nips-2005-Kernels_for_gene_regulatory_regions.html">103 nips-2005-Kernels for gene regulatory regions</a></p>
<p>Author: Jean-philippe Vert, Robert Thurman, William S. Noble</p><p>Abstract: We describe a hierarchy of motif-based kernels for multiple alignments of biological sequences, particularly suitable to process regulatory regions of genes. The kernels incorporate progressively more information, with the most complex kernel accounting for a multiple alignment of orthologous regions, the phylogenetic tree relating the species, and the prior knowledge that relevant sequence patterns occur in conserved motif blocks. These kernels can be used in the presence of a library of known transcription factor binding sites, or de novo by iterating over all k-mers of a given length. In the latter mode, a discriminative classiﬁer built from such a kernel not only recognizes a given class of promoter regions, but as a side effect simultaneously identiﬁes a collection of relevant, discriminative sequence motifs. We demonstrate the utility of the motif-based multiple alignment kernels by using a collection of aligned promoter regions from ﬁve yeast species to recognize classes of cell-cycle regulated genes. Supplementary data is available at http://noble.gs.washington.edu/proj/pkernel. 1</p><p>5 0.034711048 <a title="198-tfidf-5" href="./nips-2005-Robust_design_of_biological_experiments.html">167 nips-2005-Robust design of biological experiments</a></p>
<p>Author: Patrick Flaherty, Adam Arkin, Michael I. Jordan</p><p>Abstract: We address the problem of robust, computationally-efﬁcient design of biological experiments. Classical optimal experiment design methods have not been widely adopted in biological practice, in part because the resulting designs can be very brittle if the nominal parameter estimates for the model are poor, and in part because of computational constraints. We present a method for robust experiment design based on a semideﬁnite programming relaxation. We present an application of this method to the design of experiments for a complex calcium signal transduction pathway, where we have found that the parameter estimates obtained from the robust design are better than those obtained from an “optimal” design. 1</p><p>6 0.030944614 <a title="198-tfidf-6" href="./nips-2005-Mixture_Modeling_by_Affinity_Propagation.html">127 nips-2005-Mixture Modeling by Affinity Propagation</a></p>
<p>7 0.029238781 <a title="198-tfidf-7" href="./nips-2005-An_Analog_Visual_Pre-Processing_Processor_Employing_Cyclic_Line_Access_in_Only-Nearest-Neighbor-Interconnects_Architecture.html">22 nips-2005-An Analog Visual Pre-Processing Processor Employing Cyclic Line Access in Only-Nearest-Neighbor-Interconnects Architecture</a></p>
<p>8 0.025907118 <a title="198-tfidf-8" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>9 0.023753546 <a title="198-tfidf-9" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<p>10 0.023515735 <a title="198-tfidf-10" href="./nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">113 nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<p>11 0.023498863 <a title="198-tfidf-11" href="./nips-2005-Subsequence_Kernels_for_Relation_Extraction.html">185 nips-2005-Subsequence Kernels for Relation Extraction</a></p>
<p>12 0.023123369 <a title="198-tfidf-12" href="./nips-2005-Correlated_Topic_Models.html">52 nips-2005-Correlated Topic Models</a></p>
<p>13 0.022521807 <a title="198-tfidf-13" href="./nips-2005-Layered_Dynamic_Textures.html">108 nips-2005-Layered Dynamic Textures</a></p>
<p>14 0.022465309 <a title="198-tfidf-14" href="./nips-2005-Is_Early_Vision_Optimized_for_Extracting_Higher-order_Dependencies%3F.html">101 nips-2005-Is Early Vision Optimized for Extracting Higher-order Dependencies?</a></p>
<p>15 0.022202002 <a title="198-tfidf-15" href="./nips-2005-Generalization_to_Unseen_Cases.html">85 nips-2005-Generalization to Unseen Cases</a></p>
<p>16 0.021534001 <a title="198-tfidf-16" href="./nips-2005-Variational_EM_Algorithms_for_Non-Gaussian_Latent_Variable_Models.html">202 nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</a></p>
<p>17 0.021272812 <a title="198-tfidf-17" href="./nips-2005-Analyzing_Auditory_Neurons_by_Learning_Distance_Functions.html">28 nips-2005-Analyzing Auditory Neurons by Learning Distance Functions</a></p>
<p>18 0.020531157 <a title="198-tfidf-18" href="./nips-2005-Top-Down_Control_of_Visual_Attention%3A_A_Rational_Account.html">194 nips-2005-Top-Down Control of Visual Attention: A Rational Account</a></p>
<p>19 0.020512611 <a title="198-tfidf-19" href="./nips-2005-Fusion_of_Similarity_Data_in_Clustering.html">79 nips-2005-Fusion of Similarity Data in Clustering</a></p>
<p>20 0.020484326 <a title="198-tfidf-20" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.08), (1, 0.002), (2, -0.004), (3, 0.027), (4, 0.013), (5, -0.01), (6, -0.072), (7, 0.021), (8, -0.015), (9, -0.058), (10, 0.007), (11, -0.001), (12, -0.049), (13, -0.001), (14, 0.001), (15, -0.064), (16, 0.056), (17, 0.036), (18, -0.009), (19, -0.009), (20, 0.043), (21, -0.085), (22, 0.003), (23, 0.019), (24, -0.059), (25, -0.059), (26, -0.085), (27, -0.074), (28, 0.011), (29, 0.05), (30, -0.105), (31, -0.027), (32, 0.003), (33, 0.049), (34, 0.007), (35, -0.047), (36, 0.109), (37, 0.026), (38, -0.065), (39, -0.011), (40, -0.072), (41, -0.149), (42, -0.137), (43, -0.161), (44, 0.154), (45, -0.245), (46, -0.046), (47, 0.099), (48, -0.112), (49, 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93441904 <a title="198-lsi-1" href="./nips-2005-Using_%60%60epitomes%27%27_to_model_genetic_diversity%3A_Rational_design_of_HIV_vaccine_cocktails.html">198 nips-2005-Using ``epitomes'' to model genetic diversity: Rational design of HIV vaccine cocktails</a></p>
<p>Author: Nebojsa Jojic, Vladimir Jojic, Christopher Meek, David Heckerman, Brendan J. Frey</p><p>Abstract: We introduce a new model of genetic diversity which summarizes a large input dataset into an epitome, a short sequence or a small set of short sequences of probability distributions capturing many overlapping subsequences from the dataset. The epitome as a representation has already been used in modeling real-valued signals, such as images and audio. The discrete sequence model we introduce in this paper targets applications in genetics, from multiple alignment to recombination and mutation inference. In our experiments, we concentrate on modeling the diversity of HIV where the epitome emerges as a natural model for producing relatively small vaccines covering a large number of immune system targets known as epitopes. Our experiments show that the epitome includes more epitopes than other vaccine designs of similar length, including cocktails of consensus strains, phylogenetic tree centers, and observed strains. We also discuss epitome designs that take into account uncertainty about Tcell cross reactivity and epitope presentation. In our experiments, we ﬁnd that vaccine optimization is fairly robust to these uncertainties. 1</p><p>2 0.59363925 <a title="198-lsi-2" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>Author: Martin J. Wainwright</p><p>Abstract: Consider the problem of joint parameter estimation and prediction in a Markov random ﬁeld: i.e., the model parameters are estimated on the basis of an initial set of data, and then the ﬁtted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working in the computation-limited setting, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for ﬁtting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the “wrong” model even in the inﬁnite data limit) is provably beneﬁcial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of variational methods. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product. 1 Keywords: Markov random ﬁelds; variational method; message-passing algorithms; sum-product; belief propagation; parameter estimation; learning. 1</p><p>3 0.42140675 <a title="198-lsi-3" href="./nips-2005-Mixture_Modeling_by_Affinity_Propagation.html">127 nips-2005-Mixture Modeling by Affinity Propagation</a></p>
<p>Author: Brendan J. Frey, Delbert Dueck</p><p>Abstract: Clustering is a fundamental problem in machine learning and has been approached in many ways. Two general and quite different approaches include iteratively ﬁtting a mixture model (e.g., using EM) and linking together pairs of training cases that have high afﬁnity (e.g., using spectral methods). Pair-wise clustering algorithms need not compute sufﬁcient statistics and avoid poor solutions by directly placing similar examples in the same cluster. However, many applications require that each cluster of data be accurately described by a prototype or model, so afﬁnity-based clustering – and its beneﬁts – cannot be directly realized. We describe a technique called “afﬁnity propagation”, which combines the advantages of both approaches. The method learns a mixture model of the data by recursively propagating afﬁnity messages. We demonstrate afﬁnity propagation on the problems of clustering image patches for image segmentation and learning mixtures of gene expression models from microarray data. We ﬁnd that afﬁnity propagation obtains better solutions than mixtures of Gaussians, the K-medoids algorithm, spectral clustering and hierarchical clustering, and is both able to ﬁnd a pre-speciﬁed number of clusters and is able to automatically determine the number of clusters. Interestingly, afﬁnity propagation can be viewed as belief propagation in a graphical model that accounts for pairwise training case likelihood functions and the identiﬁcation of cluster centers. 1</p><p>4 0.3521162 <a title="198-lsi-4" href="./nips-2005-Kernels_for_gene_regulatory_regions.html">103 nips-2005-Kernels for gene regulatory regions</a></p>
<p>Author: Jean-philippe Vert, Robert Thurman, William S. Noble</p><p>Abstract: We describe a hierarchy of motif-based kernels for multiple alignments of biological sequences, particularly suitable to process regulatory regions of genes. The kernels incorporate progressively more information, with the most complex kernel accounting for a multiple alignment of orthologous regions, the phylogenetic tree relating the species, and the prior knowledge that relevant sequence patterns occur in conserved motif blocks. These kernels can be used in the presence of a library of known transcription factor binding sites, or de novo by iterating over all k-mers of a given length. In the latter mode, a discriminative classiﬁer built from such a kernel not only recognizes a given class of promoter regions, but as a side effect simultaneously identiﬁes a collection of relevant, discriminative sequence motifs. We demonstrate the utility of the motif-based multiple alignment kernels by using a collection of aligned promoter regions from ﬁve yeast species to recognize classes of cell-cycle regulated genes. Supplementary data is available at http://noble.gs.washington.edu/proj/pkernel. 1</p><p>5 0.33276561 <a title="198-lsi-5" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>Author: Xuanlong Nguyen, Martin J. Wainwright, Michael I. Jordan</p><p>Abstract: In this paper, we provide a general theorem that establishes a correspondence between surrogate loss functions in classiﬁcation and the family of f -divergences. Moreover, we provide constructive procedures for determining the f -divergence induced by a given surrogate loss, and conversely for ﬁnding all surrogate loss functions that realize a given f -divergence. Next we introduce the notion of universal equivalence among loss functions and corresponding f -divergences, and provide necessary and sufﬁcient conditions for universal equivalence to hold. These ideas have applications to classiﬁcation problems that also involve a component of experiment design; in particular, we leverage our results to prove consistency of a procedure for learning a classiﬁer under decentralization requirements. 1</p><p>6 0.26900497 <a title="198-lsi-6" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>7 0.26734003 <a title="198-lsi-7" href="./nips-2005-Searching_for_Character_Models.html">171 nips-2005-Searching for Character Models</a></p>
<p>8 0.26103371 <a title="198-lsi-8" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>9 0.21983351 <a title="198-lsi-9" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>10 0.21333268 <a title="198-lsi-10" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<p>11 0.20625257 <a title="198-lsi-11" href="./nips-2005-Factorial_Switching_Kalman_Filters_for_Condition_Monitoring_in_Neonatal_Intensive_Care.html">68 nips-2005-Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care</a></p>
<p>12 0.19977936 <a title="198-lsi-12" href="./nips-2005-Measuring_Shared_Information_and_Coordinated_Activity_in_Neuronal_Networks.html">124 nips-2005-Measuring Shared Information and Coordinated Activity in Neuronal Networks</a></p>
<p>13 0.19908692 <a title="198-lsi-13" href="./nips-2005-Location-based_activity_recognition.html">121 nips-2005-Location-based activity recognition</a></p>
<p>14 0.19375433 <a title="198-lsi-14" href="./nips-2005-Learning_Influence_among_Interacting_Markov_Chains.html">111 nips-2005-Learning Influence among Interacting Markov Chains</a></p>
<p>15 0.18312344 <a title="198-lsi-15" href="./nips-2005-Gaussian_Processes_for_Multiuser_Detection_in_CDMA_receivers.html">81 nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</a></p>
<p>16 0.18146312 <a title="198-lsi-16" href="./nips-2005-Learning_Depth_from_Single_Monocular_Images.html">110 nips-2005-Learning Depth from Single Monocular Images</a></p>
<p>17 0.18039626 <a title="198-lsi-17" href="./nips-2005-Off-Road_Obstacle_Avoidance_through_End-to-End_Learning.html">143 nips-2005-Off-Road Obstacle Avoidance through End-to-End Learning</a></p>
<p>18 0.17532572 <a title="198-lsi-18" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>19 0.17311212 <a title="198-lsi-19" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>20 0.17156498 <a title="198-lsi-20" href="./nips-2005-Layered_Dynamic_Textures.html">108 nips-2005-Layered Dynamic Textures</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.038), (10, 0.05), (27, 0.036), (31, 0.033), (34, 0.041), (41, 0.049), (55, 0.016), (69, 0.049), (73, 0.441), (80, 0.013), (88, 0.061), (91, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91497004 <a title="198-lda-1" href="./nips-2005-Laplacian_Score_for_Feature_Selection.html">104 nips-2005-Laplacian Score for Feature Selection</a></p>
<p>Author: Xiaofei He, Deng Cai, Partha Niyogi</p><p>Abstract: In supervised learning scenarios, feature selection has been studied widely in the literature. Selecting features in unsupervised learning scenarios is a much harder problem, due to the absence of class labels that would guide the search for relevant information. And, almost all of previous unsupervised feature selection methods are “wrapper” techniques that require a learning algorithm to evaluate the candidate feature subsets. In this paper, we propose a “ﬁlter” method for feature selection which is independent of any learning algorithm. Our method can be performed in either supervised or unsupervised fashion. The proposed method is based on the observation that, in many real world classiﬁcation problems, data from the same class are often close to each other. The importance of a feature is evaluated by its power of locality preserving, or, Laplacian Score. We compare our method with data variance (unsupervised) and Fisher score (supervised) on two data sets. Experimental results demonstrate the effectiveness and efﬁciency of our algorithm. 1</p><p>2 0.89959663 <a title="198-lda-2" href="./nips-2005-Fast_Krylov_Methods_for_N-Body_Learning.html">71 nips-2005-Fast Krylov Methods for N-Body Learning</a></p>
<p>Author: Nando D. Freitas, Yang Wang, Maryam Mahdaviani, Dustin Lang</p><p>Abstract: This paper addresses the issue of numerical computation in machine learning domains based on similarity metrics, such as kernel methods, spectral techniques and Gaussian processes. It presents a general solution strategy based on Krylov subspace iteration and fast N-body learning methods. The experiments show signiﬁcant gains in computation and storage on datasets arising in image segmentation, object detection and dimensionality reduction. The paper also presents theoretical bounds on the stability of these methods.</p><p>3 0.87261695 <a title="198-lda-3" href="./nips-2005-Describing_Visual_Scenes_using_Transformed_Dirichlet_Processes.html">55 nips-2005-Describing Visual Scenes using Transformed Dirichlet Processes</a></p>
<p>Author: Antonio Torralba, Alan S. Willsky, Erik B. Sudderth, William T. Freeman</p><p>Abstract: Motivated by the problem of learning to detect and recognize objects with minimal supervision, we develop a hierarchical probabilistic model for the spatial structure of visual scenes. In contrast with most existing models, our approach explicitly captures uncertainty in the number of object instances depicted in a given image. Our scene model is based on the transformed Dirichlet process (TDP), a novel extension of the hierarchical DP in which a set of stochastically transformed mixture components are shared between multiple groups of data. For visual scenes, mixture components describe the spatial structure of visual features in an object–centered coordinate frame, while transformations model the object positions in a particular image. Learning and inference in the TDP, which has many potential applications beyond computer vision, is based on an empirically effective Gibbs sampler. Applied to a dataset of partially labeled street scenes, we show that the TDP’s inclusion of spatial structure improves detection performance, ﬂexibly exploiting partially labeled training images. 1</p><p>same-paper 4 0.83231926 <a title="198-lda-4" href="./nips-2005-Using_%60%60epitomes%27%27_to_model_genetic_diversity%3A_Rational_design_of_HIV_vaccine_cocktails.html">198 nips-2005-Using ``epitomes'' to model genetic diversity: Rational design of HIV vaccine cocktails</a></p>
<p>Author: Nebojsa Jojic, Vladimir Jojic, Christopher Meek, David Heckerman, Brendan J. Frey</p><p>Abstract: We introduce a new model of genetic diversity which summarizes a large input dataset into an epitome, a short sequence or a small set of short sequences of probability distributions capturing many overlapping subsequences from the dataset. The epitome as a representation has already been used in modeling real-valued signals, such as images and audio. The discrete sequence model we introduce in this paper targets applications in genetics, from multiple alignment to recombination and mutation inference. In our experiments, we concentrate on modeling the diversity of HIV where the epitome emerges as a natural model for producing relatively small vaccines covering a large number of immune system targets known as epitopes. Our experiments show that the epitome includes more epitopes than other vaccine designs of similar length, including cocktails of consensus strains, phylogenetic tree centers, and observed strains. We also discuss epitome designs that take into account uncertainty about Tcell cross reactivity and epitope presentation. In our experiments, we ﬁnd that vaccine optimization is fairly robust to these uncertainties. 1</p><p>5 0.80257344 <a title="198-lda-5" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>Author: Tong Zhang, Rie Kubota Ando</p><p>Abstract: We consider a framework for semi-supervised learning using spectral decomposition based un-supervised kernel design. This approach subsumes a class of previously proposed semi-supervised learning methods on data graphs. We examine various theoretical properties of such methods. In particular, we derive a generalization performance bound, and obtain the optimal kernel design by minimizing the bound. Based on the theoretical analysis, we are able to demonstrate why spectral kernel design based methods can often improve the predictive performance. Experiments are used to illustrate the main consequences of our analysis.</p><p>6 0.60987884 <a title="198-lda-6" href="./nips-2005-Kernelized_Infomax_Clustering.html">102 nips-2005-Kernelized Infomax Clustering</a></p>
<p>7 0.55702919 <a title="198-lda-7" href="./nips-2005-A_Probabilistic_Approach_for_Optimizing_Spectral_Clustering.html">13 nips-2005-A Probabilistic Approach for Optimizing Spectral Clustering</a></p>
<p>8 0.54197639 <a title="198-lda-8" href="./nips-2005-Tensor_Subspace_Analysis.html">189 nips-2005-Tensor Subspace Analysis</a></p>
<p>9 0.52922302 <a title="198-lda-9" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>10 0.52130145 <a title="198-lda-10" href="./nips-2005-Generalization_in_Clustering_with_Unobserved_Features.html">84 nips-2005-Generalization in Clustering with Unobserved Features</a></p>
<p>11 0.49514741 <a title="198-lda-11" href="./nips-2005-Soft_Clustering_on_Graphs.html">178 nips-2005-Soft Clustering on Graphs</a></p>
<p>12 0.49508342 <a title="198-lda-12" href="./nips-2005-Diffusion_Maps%2C_Spectral_Clustering_and_Eigenfunctions_of_Fokker-Planck_Operators.html">56 nips-2005-Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators</a></p>
<p>13 0.4793677 <a title="198-lda-13" href="./nips-2005-From_Lasso_regression_to_Feature_vector_machine.html">77 nips-2005-From Lasso regression to Feature vector machine</a></p>
<p>14 0.47915208 <a title="198-lda-14" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>15 0.47428951 <a title="198-lda-15" href="./nips-2005-Infinite_latent_feature_models_and_the_Indian_buffet_process.html">98 nips-2005-Infinite latent feature models and the Indian buffet process</a></p>
<p>16 0.47181424 <a title="198-lda-16" href="./nips-2005-Fusion_of_Similarity_Data_in_Clustering.html">79 nips-2005-Fusion of Similarity Data in Clustering</a></p>
<p>17 0.46581614 <a title="198-lda-17" href="./nips-2005-Correcting_sample_selection_bias_in_maximum_entropy_density_estimation.html">51 nips-2005-Correcting sample selection bias in maximum entropy density estimation</a></p>
<p>18 0.45926273 <a title="198-lda-18" href="./nips-2005-Efficient_Unsupervised_Learning_for_Localization_and_Detection_in_Object_Categories.html">63 nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</a></p>
<p>19 0.45543301 <a title="198-lda-19" href="./nips-2005-Context_as_Filtering.html">48 nips-2005-Context as Filtering</a></p>
<p>20 0.45299762 <a title="198-lda-20" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
