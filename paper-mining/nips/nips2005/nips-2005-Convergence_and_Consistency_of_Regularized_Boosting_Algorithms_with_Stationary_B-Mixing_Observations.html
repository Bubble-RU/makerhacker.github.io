<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-49" href="#">nips2005-49</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</h1>
<br/><p>Source: <a title="nips-2005-49-pdf" href="http://papers.nips.cc/paper/2790-convergence-and-consistency-of-regularized-boosting-algorithms-with-stationary-b-mixing-observations.pdf">pdf</a></p><p>Author: Aurelie C. Lozano, Sanjeev R. Kulkarni, Robert E. Schapire</p><p>Abstract: We study the statistical convergence and consistency of regularized Boosting methods, where the samples are not independent and identically distributed (i.i.d.) but come from empirical processes of stationary β-mixing sequences. Utilizing a technique that constructs a sequence of independent blocks close in distribution to the original samples, we prove the consistency of the composite classiﬁers resulting from a regularization achieved by restricting the 1-norm of the base classiﬁers’ weights. When compared to the i.i.d. case, the nature of sampling manifests in the consistency result only through generalization of the original condition on the growth of the regularization parameter.</p><p>Reference: <a title="nips-2005-49-reference" href="../nips2005_reference/nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We study the statistical convergence and consistency of regularized Boosting methods, where the samples are not independent and identically distributed (i. [sent-8, score-0.3]
</p><p>2 ) but come from empirical processes of stationary β-mixing sequences. [sent-11, score-0.255]
</p><p>3 Utilizing a technique that constructs a sequence of independent blocks close in distribution to the original samples, we prove the consistency of the composite classiﬁers resulting from a regularization achieved by restricting the 1-norm of the base classiﬁers’ weights. [sent-12, score-0.475]
</p><p>4 case, the nature of sampling manifests in the consistency result only through generalization of the original condition on the growth of the regularization parameter. [sent-16, score-0.303]
</p><p>5 1 Introduction A signiﬁcant development in machine learning for classiﬁcation has been the emergence of boosting algorithms [1]. [sent-17, score-0.185]
</p><p>6 Simply put, a boosting algorithm is an iterative procedure that combines weak prediction rules to produce a composite classiﬁer, the idea being that one can obtain very precise prediction rules by combining rough ones. [sent-18, score-0.378]
</p><p>7 It was shown in [2] that AdaBoost, the most popular Boosting algorithm, can be seen as stage-wise ﬁtting of additive models under the exponential loss function and it effectively minimizes an empirical loss function that differs from the probability of incorrect prediction. [sent-19, score-0.117]
</p><p>8 From this perspective, boosting can be seen as performing a greedy stage-wise minimization of various loss functions empirically. [sent-20, score-0.244]
</p><p>9 The question of whether boosting achieves Bayes-consistency then arises, since minimizing an empirical loss function does not necessarily imply minimizing the generalization error. [sent-21, score-0.308]
</p><p>10 Consequently, one approach for the study of consistency is to modify the original Adaboost algorithm by imposing some constraints on the weights of the composite classiﬁer to avoid overﬁtting. [sent-24, score-0.182]
</p><p>11 In this regularized version of Adaboost, the 1-norm of the weights of the base classiﬁers is restricted to a ﬁxed value. [sent-25, score-0.101]
</p><p>12 The minimization of the loss function is performed over the restricted class [4, 5]. [sent-26, score-0.085]
</p><p>13 In this paper, we examine the convergence and consistency of regularized boosting algorithms with samples that are no longer i. [sent-27, score-0.454]
</p><p>14 but come from empirical processes of stationary weakly dependent sequences. [sent-30, score-0.298]
</p><p>15 sampling is that in many learning applications observations are intrinsically temporal and hence often weakly dependent. [sent-34, score-0.119]
</p><p>16 To cope with weak dependence we apply mixing theory which, through its deﬁnition of mixing coefﬁcients, offers a powerful approach to extend results for the traditional i. [sent-40, score-0.453]
</p><p>17 observations to the case of weakly dependent or mixing sequences. [sent-43, score-0.264]
</p><p>18 Intuitively, they provide a “measure” of how fast the dependence between the observations diminishes as the distance between them increases. [sent-47, score-0.089]
</p><p>19 If certain conditions on the mixing coefﬁcients are satisﬁed to reﬂect a sufﬁciently fast decline in the dependence between observations as their distance grows, counterparts to results for i. [sent-48, score-0.273]
</p><p>20 A comprehensive review of mixing theory results is provided in [13]. [sent-52, score-0.184]
</p><p>21 Our principal ﬁnding is that consistency of regularized Boosting methods can be established in the case of non-i. [sent-53, score-0.202]
</p><p>22 samples coming from empirical sequences of stationary β-mixing sequences. [sent-56, score-0.254]
</p><p>23 Among the conditions that guarantee consistency, the mixing nature of sampling appears only through a generalization of the one on the growth of the regularization parameter originally stated for the i. [sent-57, score-0.319]
</p><p>24 1  Background and Setup Mixing Sequences  Let W = (Wi )i≥1 be a strictly stationary sequence of random variables, each having the l same distribution P on D ⊂ Rd . [sent-62, score-0.231]
</p><p>25 The following mixing coefﬁcients characterize how close to independent a sequence W is. [sent-74, score-0.274]
</p><p>26 For any sequence W , the β-mixing1 coefﬁcient is deﬁned by k ∞ βW (n) = supk E sup |P A|σ1 − P (A) | : A ∈ σk+n , k where the expectation is taken w. [sent-76, score-0.28]
</p><p>27 Hence βW (n) quantiﬁes the degree of dependence between ’future’ observations and ’past’ ones separated by a distance of at least n. [sent-80, score-0.089]
</p><p>28 This property implies that the dependence between observations decreases fast enough as the distance between them increases. [sent-94, score-0.089]
</p><p>29 Further, it is algebraically β-mixing if there is a positive constant rβ such that βW (n) = O (n−rβ ) . [sent-97, score-0.115]
</p><p>30 The choice of β-mixing appears appropriate given previous results that showed “uniform convergence of empirical means uniformly in probability” and “probably approximately correct” properties to be preserved for β-mixing inputs [11]. [sent-98, score-0.117]
</p><p>31 Some examples of β-mixing sequences that ﬁt naturally in a learning scenario are certain Markov processes and Hidden Markov Models [11]. [sent-99, score-0.08]
</p><p>32 In practice, if the mixing properties are unknown, they need to be estimated. [sent-100, score-0.206]
</p><p>33 Although it is difﬁcult to ﬁnd them in general, there exist simple methods to determine the mixing rates for various classes of random processes (e. [sent-101, score-0.226]
</p><p>34 Hence the assumption of a known mixing rate is reasonable and has been adopted by many studies [6, 7, 8, 9, 10, 12]. [sent-104, score-0.231]
</p><p>35 Using Sn , a classiﬁer hn : X → {−1, 1} is built to predict the label Y of an unlabeled observation X. [sent-110, score-0.073]
</p><p>36 , and to our knowledge, this assumption is made by all the studies on boosting consistency. [sent-114, score-0.232]
</p><p>37 but corresponds to an empirical process of stationary β-mixing sequences. [sent-118, score-0.185]
</p><p>38 We suppose that W = (Wi )i≥1 is a strictly stationary sequence of random variables, each having the same distribution P on D and that W is β-mixing (see Deﬁnition 2). [sent-121, score-0.231]
</p><p>39 Let H denote the class of base classiﬁers h : X → {−1, 1}, which usually consists of simple rules (for instance decision stumps). [sent-126, score-0.081]
</p><p>40 Call F, the class of functions f : X → [−1, 1] obtained as convex combinations of the classiﬁers in H: t  F = f (X) =  t  αj hj (X) : t ∈ N, α1 , . [sent-128, score-0.071]
</p><p>41 j=1  (1) Each fn ∈ F deﬁnes a classiﬁer hfn = sign (fn ) and for simplicity the generalization error L (hfn ) is denoted by L (fn ). [sent-135, score-0.321]
</p><p>42 Instead of minimizing the indicator of misclassiﬁcation (I[−f (X)Y >0] ), boosting methods are shown to effectively minimize a smooth convex cost function of Z(f ). [sent-138, score-0.297]
</p><p>43 Consider a positive, differentiable, strictly increasing, and strictly convex function φ : R → R+ and assume that φ (0) = 1 and that limx→−∞ φ (x) = 0. [sent-140, score-0.137]
</p><p>44 The corresponding cost function and empirical cost funcn tion are respectively C (f ) = Eφ (Z (f )) and Cn (f ) = 1/n i=1 φ (Zi (f )) . [sent-141, score-0.193]
</p><p>45 The iterative aspect of boosting methods is ignored to consider only their performing an (approximate) minimization of the empirical cost function or, as we shall see, a series of cost functions. [sent-143, score-0.433]
</p><p>46 To avoid overﬁtting, the following regularization procedure is developed for the choice of the cost functions. [sent-144, score-0.093]
</p><p>47 The corn 1 λ responding empirical and expected cost functions become Cn (f ) = n i=1 φλ (Zi (f )) λ and C (f ) = Eφλ (Z (f )) . [sent-146, score-0.126]
</p><p>48 The minimization of a series of cost functions C λ over the convex hull of H is then analyzed. [sent-147, score-0.167]
</p><p>49 2 Statistical Convergence The nature of the sampling intervenes in the following two lemmas that relate the empirical λ cost Cn (f ) and true cost C λ (f ). [sent-149, score-0.258]
</p><p>50 (Xn , Yn ) comes from a stationary algebraically β-mixing sequence with β-mixing coefﬁcients β (m) satisfying β (m) = O (m−rβ ), m ∈ N and rβ a positive constant. [sent-154, score-0.324]
</p><p>51 Then for any λ > 0 and b ∈ [0, 1), λ E sup |C λ (f ) − Cn (f ) | ≤ 4λφ (λ) f ∈F  2 c1 1 + 2φ (λ) b(1+r )−1 + 1−b . [sent-155, score-0.221]
</p><p>52 Then for any λ > 0 λ P sup |C λ (f ) − Cn (f ) | > f ∈F  n  ≤ exp(−4c2 nα ) + O(n1−b(rβ +1) ). [sent-159, score-0.221]
</p><p>53 , (Xn , Yn ) are assumed to come from a stationary algebraically β-mixing sequence with β-mixing coefﬁcients βX,Y (n) = O (n−rβ ), rβ being a positive constant. [sent-169, score-0.328]
</p><p>54 II- Properties of the cost function φ: φ is assumed to be a differentiable, strictly convex, strictly increasing cost function such that φ (0) = 1 and limx→−∞ φ (x) = 0. [sent-170, score-0.226]
</p><p>55 The distribution of (X, Y ) and the class H are such that limλ→∞ inf f ∈λF C (f ) = C ∗ , where λF = {λf : f ∈ F} and C ∗ = inf C (f ) over all measurable functions f : X → R. [sent-172, score-0.193]
</p><p>56 is a sequence of positive numbers satisfying λn → ∞ as n → ∞, and that there exists a constant 1 c ∈ 1+rβ , 1 such that λn φ (λn ) /n(1−c)/2 → 0 as n → ∞. [sent-176, score-0.105]
</p><p>57 λ ˆλ ˆλ Call fn the function in F which approximatively minimizes Cn (f ), i. [sent-177, score-0.213]
</p><p>58 fn is such that n 1 λ ˆλ λ Cn (fn ) ≤ inf f ∈F Cn (f ) + n = inf f ∈F n i=1 φλ (Zi (f )) + n , with n → 0 as n → ∞. [sent-179, score-0.285]
</p><p>59 Consistency of regularized boosting methods for stationary β-mixing seλ ˆλ ˆλ quences. [sent-182, score-0.379]
</p><p>60 Let fn = fn n ∈ F, where fn n (approximatively) minimizes Cn n (f ) . [sent-183, score-0.537]
</p><p>61 Under Assumption 1, limn→∞ L (hfn = sign (fn )) = L∗ almost surely and hfn is strongly Bayes-risk consistent. [sent-184, score-0.107]
</p><p>62 IV, notice that the nature of sampling leads to a generalization of the condition on the growth of λn φ (λn ) already present in the i. [sent-190, score-0.109]
</p><p>63 More precisely, the nature of sampling manifests through parameter c, which is limited by rβ . [sent-194, score-0.073]
</p><p>64 The assumption that rβ is known is quite strict but cannot be avoided (for instance this assumption is widely made in the ﬁeld of time series analysis). [sent-195, score-0.119]
</p><p>65 1 Preparation to the Proofs: the Blocking Technique The key issue resides in upper bounding n  sup f ∈F  λ Cn  λ  (f ) − C (f ) = sup 1/n f ∈F  φ (−λf (Xi ) Yi ) − Eφ (−λf (X1 ) Y1 ) ,  (4)  i=1  where F is given by (1). [sent-199, score-0.442]
</p><p>66 Then (4) can be rewritten as n λ supf ∈F Cn (f ) − C λ (f ) = supgλ ∈Gλ n−1 i=1 gλ (Wi ) − Egλ (W1 ) . [sent-202, score-0.085]
</p><p>67 Besides, if H is a class of measurable functions, then Gλ is also a class of measurable functions, by measurability of F. [sent-204, score-0.174]
</p><p>68 d blocks of observations which are close in distribution to the original sequence W1 , . [sent-209, score-0.24]
</p><p>69 This enables us to work on the sequence of independent blocks instead of the original sequence. [sent-213, score-0.234]
</p><p>70 , Wn = (Xn , Yn ) of the mixing sequence into 2µn blocks of size bn , followed by a remaining block (of size at most 2bn ). [sent-220, score-0.972]
</p><p>71 If their size bn is large enough, the dependence between them is weak, since two odd blocks are separated by an even block of the same size bn . [sent-222, score-1.437]
</p><p>72 Therefore, the odd blocks can be approximated by a sequence of independent blocks with the same within-block structure. [sent-223, score-0.473]
</p><p>73 , ξ2µn bn be independent blocks such that ξjbn +1 , . [sent-237, score-0.736]
</p><p>74 , 2µn , and any g ∈ Gλ , deﬁne jbn jbn ˜ Zj,g := i=(j−1)bn +1 g (ξi ) − bn Eg (ξ1 ) , Zj,g := i=(j−1)bn +1 g (Wi ) − bn Eg (W1 ) . [sent-250, score-1.25]
</p><p>75 These correspond to the Zk (f ) = −f (Xk ) Yk for k in the odd blocks 1, . [sent-259, score-0.239]
</p><p>76 We show that E sup g∈Gλ  1 n  n  g (Wi )−Eg (W1 ) ≤ 2E sup i=1  g∈Gλ  1 n  Zj,g +φ (λ) µn βW (bn )+ j∈Oµn  2bn . [sent-272, score-0.442]
</p><p>77 n 1 1 ˜ ˜ Then, E supg n i=1 g (Wi ) = E supg n , where R Oµn Zj,g + Eµn Zj,g + R is the remainder term consisting of a sum of at most 2bn terms. [sent-275, score-0.902]
</p><p>78 Noting that ∀g ∈ n 1 1 ˜ Gλ , |g| ≤ φ (λ), it follows that E supg | n i=1 g (Wi ) | ≤ E(supg | n Oµn Zj,g |) + φ(λ)(2bn ) 1 ˜ Zj,g |) + . [sent-276, score-0.451]
</p><p>79 ) = supg | n Eµn Zj,g | n 1 respectively, and noting that H = φ (λ) /2, we have E supg | n i=1 g (Wi ) | ≤ 1 E supg | n  Oµn  1 Zj,g | + φ(λ) µn βW (bn ) + E supg | n 2  Eµn  Zj,g | + φ(λ) µn βW (bn ) + φ(λ)(2bn ) . [sent-322, score-1.846]
</p><p>80 2 n  As the Zj,g ’s from odd and even blocks have the same distribution, we obtain (6). [sent-323, score-0.239]
</p><p>81 The odd blocks Zj,g ’s being independent, we can use the standard symmetrization techniques. [sent-326, score-0.239]
</p><p>82 a sequence of independent random variables taking the values ±1 with probability 1/2. [sent-334, score-0.09]
</p><p>83 3 (Proof is omitted due to space constraints), we have 1 1 Zj,g ≤ E sup σj Zj,g − Zj,g . [sent-336, score-0.221]
</p><p>84 We now show that E sup g∈Gλ  1 n  Zj,g ≤ 2 · bn λφ (λ) E sup  f ∈F  j∈Oµn  1 n  µn  σj Z1,j (f ) . [sent-339, score-1.003]
</p><p>85 , with (7) µn bn 1 1 ≤ E supg n j∈Oµn Zj,g ≤ E supg n j=1 σj i=1 φλ (Zi,j (f )) − φλ Zi,j (f ) µn 1 2bn E supg n j=1 σj (φλ (Z1,j (f ))−1) . [sent-344, score-1.914]
</p><p>86 One can write E supf ∈F | n 1 µn j=1  1 n E supN ≥1  N k=1  µn j=1  (9)  σj Z1,j (f )| =  suphN ∈HN supα1 ,. [sent-354, score-0.085]
</p><p>87 for all j = j (they come from different blocks), and (σj ) is a Rademacher sequence, then σj ξ(2j−2)bn +1,2 hk ξ(2j−2)bn +1,1 j=1,. [sent-361, score-0.073]
</p><p>88 Hence n  E sup f ∈F  1 n  µn  σj Z1,j (f ) = j=1  1 E sup sup sup n N ≥1 hN ∈HN α1 ,. [sent-368, score-0.884]
</p><p>89 Hence we get  E supf ∈F  µn j=1  1 n  σj Z1,j (f ) =  1 n E suph∈H  µn j=1  σj h ξ(1,j),1  . [sent-374, score-0.085]
</p><p>90 8 1 E sup n h∈H  µn  µn  σj h ξ(2j−2)bn +1,1  ≤  j=1  ≤  1 E sup σj h ξ(2j−2)bn +1,1 n h∈H∪{0} j=1 √ c µn ∞ (log sup N ( , ρ2,Pn , H ∪ {0}))1/2 d , n P 0  where c is a constant and N ( , ρ2,Pn , H ∪ {0}) is the empirical L2 covering number. [sent-380, score-0.722]
</p><p>91 Besides, as we assumed that the sequence W is algebraically β-mixing (see Deﬁnition 2), βW (n) = O (n−rβ ). [sent-392, score-0.152]
</p><p>92 (10) n = 3(2c1 + n We show n 1 1 g (Wi )−Eg (W1 ) > n ≤ 2P sup Zj,g > n /3 +O(n1−b(1+rβ ) ). [sent-398, score-0.221]
</p><p>93 The Zj,g ’s of the odd block being independent, we can apply McDiarmid’s bounded 1 difference inequality ([19], Theorem 9. [sent-412, score-0.119]
</p><p>94 Noting that changing the value of one variable does not change the value of the function by more that bn φ (λ) /n,we obtain with bn = nb that for all > 0, 2 1−b 1 1 . [sent-418, score-1.167]
</p><p>95 P supg∈Gλ n j∈Oµn Zj,g > E supg∈Gλ n j∈Oµn Zj,g + ≤ exp −4 n 2 φ(λ) Combining (8) and (9) from the proof of Lemma 1, and with bn = nb , we have 1 E supg∈Gλ n j∈Oµn Zj,g ≤ 2λφ (λ) C/n(1−b)/2 . [sent-419, score-0.633]
</p><p>96 With fn = fn n , we have ˆλ ¯ C (λn fn ) − C ∗ = (C λn (fn n ) − C λn (fλn )) + (inf f ∈λn F C(f ) − C ∗ ). [sent-428, score-0.537]
</p><p>97 2, we have C λn (fn n ) − C λn fλn ≤ 2 supf ∈F |C λn (f ) − λn λn λn Cn (f ) |. [sent-432, score-0.085]
</p><p>98 By Lemma 2, supf ∈F |C (f ) − Cn (f ) | → 0 with probability 1 if, as n → ∞, λn φ (λn ) n(α+b−1)/2 → 0 and b > 1/(1 + rβ ). [sent-433, score-0.085]
</p><p>99 : On the consistency in nonparametric estimation under mixing assumptions. [sent-466, score-0.353]
</p><p>100 : Rate of convergence for empirical processes of stationary mixing sequences. [sent-500, score-0.447]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bn', 0.561), ('supg', 0.451), ('sup', 0.221), ('boosting', 0.185), ('mixing', 0.184), ('fn', 0.179), ('cn', 0.145), ('blocks', 0.144), ('consistency', 0.134), ('stationary', 0.126), ('eg', 0.123), ('wi', 0.112), ('lemma', 0.112), ('hfn', 0.107), ('odd', 0.095), ('algebraically', 0.093), ('adaboost', 0.086), ('supf', 0.085), ('hn', 0.073), ('regularized', 0.068), ('cost', 0.067), ('jbn', 0.064), ('wbn', 0.064), ('princeton', 0.062), ('measurable', 0.061), ('empirical', 0.059), ('sequence', 0.059), ('wl', 0.057), ('inf', 0.053), ('dependence', 0.052), ('composite', 0.048), ('assumption', 0.047), ('strictly', 0.046), ('convex', 0.045), ('nb', 0.045), ('rademacher', 0.045), ('coef', 0.045), ('hk', 0.045), ('kulkarni', 0.043), ('limx', 0.043), ('classi', 0.043), ('weakly', 0.043), ('noting', 0.042), ('processes', 0.042), ('sampling', 0.039), ('cients', 0.039), ('sequences', 0.038), ('ers', 0.038), ('mcdiarmid', 0.037), ('qh', 0.037), ('observations', 0.037), ('convergence', 0.036), ('growth', 0.035), ('nonparametric', 0.035), ('generalization', 0.035), ('zi', 0.035), ('meir', 0.034), ('approximatively', 0.034), ('manifests', 0.034), ('yu', 0.033), ('schapire', 0.033), ('weak', 0.033), ('base', 0.033), ('yn', 0.033), ('lugosi', 0.032), ('sketch', 0.032), ('sn', 0.031), ('samples', 0.031), ('independent', 0.031), ('theorem', 0.03), ('nj', 0.03), ('minimization', 0.03), ('contraction', 0.03), ('loss', 0.029), ('let', 0.029), ('blocking', 0.029), ('gy', 0.029), ('come', 0.028), ('proof', 0.027), ('lemmas', 0.026), ('limn', 0.026), ('regularization', 0.026), ('event', 0.026), ('class', 0.026), ('copies', 0.025), ('york', 0.025), ('xn', 0.025), ('proofs', 0.025), ('series', 0.025), ('er', 0.024), ('satisfying', 0.024), ('nition', 0.024), ('block', 0.024), ('wn', 0.024), ('wk', 0.023), ('prediction', 0.023), ('rules', 0.022), ('combining', 0.022), ('positive', 0.022), ('properties', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="49-tfidf-1" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>Author: Aurelie C. Lozano, Sanjeev R. Kulkarni, Robert E. Schapire</p><p>Abstract: We study the statistical convergence and consistency of regularized Boosting methods, where the samples are not independent and identically distributed (i.i.d.) but come from empirical processes of stationary β-mixing sequences. Utilizing a technique that constructs a sequence of independent blocks close in distribution to the original samples, we prove the consistency of the composite classiﬁers resulting from a regularization achieved by restricting the 1-norm of the base classiﬁers’ weights. When compared to the i.i.d. case, the nature of sampling manifests in the consistency result only through generalization of the original condition on the growth of the regularization parameter.</p><p>2 0.13222867 <a title="49-tfidf-2" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>Author: O. P. Kreidl, Alan S. Willsky</p><p>Abstract: Given a directed graphical model with binary-valued hidden nodes and real-valued noisy observations, consider deciding upon the maximum a-posteriori (MAP) or the maximum posterior-marginal (MPM) assignment under the restriction that each node broadcasts only to its children exactly one single-bit message. We present a variational formulation, viewing the processing rules local to all nodes as degrees-of-freedom, that minimizes the loss in expected (MAP or MPM) performance subject to such online communication constraints. The approach leads to a novel message-passing algorithm to be executed ofﬂine, or before observations are realized, which mitigates the performance loss by iteratively coupling all rules in a manner implicitly driven by global statistics. We also provide (i) illustrative examples, (ii) assumptions that guarantee convergence and efﬁciency and (iii) connections to active research areas. 1</p><p>3 0.12658048 <a title="49-tfidf-3" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>Author: Xuanlong Nguyen, Martin J. Wainwright, Michael I. Jordan</p><p>Abstract: In this paper, we provide a general theorem that establishes a correspondence between surrogate loss functions in classiﬁcation and the family of f -divergences. Moreover, we provide constructive procedures for determining the f -divergence induced by a given surrogate loss, and conversely for ﬁnding all surrogate loss functions that realize a given f -divergence. Next we introduce the notion of universal equivalence among loss functions and corresponding f -divergences, and provide necessary and sufﬁcient conditions for universal equivalence to hold. These ideas have applications to classiﬁcation problems that also involve a component of experiment design; in particular, we leverage our results to prove consistency of a procedure for learning a classiﬁer under decentralization requirements. 1</p><p>4 0.12363494 <a title="49-tfidf-4" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>Author: Régis Vert, Jean-philippe Vert</p><p>Abstract: We determine the asymptotic limit of the function computed by support vector machines (SVM) and related algorithms that minimize a regularized empirical convex loss function in the reproducing kernel Hilbert space of the Gaussian RBF kernel, in the situation where the number of examples tends to inﬁnity, the bandwidth of the Gaussian kernel tends to 0, and the regularization parameter is held ﬁxed. Non-asymptotic convergence bounds to this limit in the L2 sense are provided, together with upper bounds on the classiﬁcation error that is shown to converge to the Bayes risk, therefore proving the Bayes-consistency of a variety of methods although the regularization term does not vanish. These results are particularly relevant to the one-class SVM, for which the regularization can not vanish by construction, and which is shown for the ﬁrst time to be a consistent density level set estimator. 1</p><p>5 0.1110585 <a title="49-tfidf-5" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>Author: Nicolas Usunier, Massih-reza Amini, Patrick Gallinari</p><p>Abstract: In this paper we propose a general framework to study the generalization properties of binary classiﬁers trained with data which may be dependent, but are deterministically generated upon a sample of independent examples. It provides generalization bounds for binary classiﬁcation and some cases of ranking problems, and clariﬁes the relationship between these learning tasks. 1</p><p>6 0.10240678 <a title="49-tfidf-6" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>7 0.087774791 <a title="49-tfidf-7" href="./nips-2005-Beyond_Gaussian_Processes%3A_On_the_Distributions_of_Infinite_Networks.html">38 nips-2005-Beyond Gaussian Processes: On the Distributions of Infinite Networks</a></p>
<p>8 0.087557547 <a title="49-tfidf-8" href="./nips-2005-Efficient_estimation_of_hidden_state_dynamics_from_spike_trains.html">64 nips-2005-Efficient estimation of hidden state dynamics from spike trains</a></p>
<p>9 0.087196939 <a title="49-tfidf-9" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>10 0.078357965 <a title="49-tfidf-10" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>11 0.077562816 <a title="49-tfidf-11" href="./nips-2005-Rodeo%3A_Sparse_Nonparametric_Regression_in_High_Dimensions.html">168 nips-2005-Rodeo: Sparse Nonparametric Regression in High Dimensions</a></p>
<p>12 0.076882802 <a title="49-tfidf-12" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>13 0.07101053 <a title="49-tfidf-13" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>14 0.066509105 <a title="49-tfidf-14" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>15 0.064180128 <a title="49-tfidf-15" href="./nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs.html">90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</a></p>
<p>16 0.062355135 <a title="49-tfidf-16" href="./nips-2005-Statistical_Convergence_of_Kernel_CCA.html">182 nips-2005-Statistical Convergence of Kernel CCA</a></p>
<p>17 0.056194339 <a title="49-tfidf-17" href="./nips-2005-Learning_Minimum_Volume_Sets.html">112 nips-2005-Learning Minimum Volume Sets</a></p>
<p>18 0.055688236 <a title="49-tfidf-18" href="./nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">113 nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<p>19 0.055471726 <a title="49-tfidf-19" href="./nips-2005-On_the_Convergence_of_Eigenspaces_in_Kernel_Principal_Component_Analysis.html">147 nips-2005-On the Convergence of Eigenspaces in Kernel Principal Component Analysis</a></p>
<p>20 0.052203983 <a title="49-tfidf-20" href="./nips-2005-Two_view_learning%3A_SVM-2K%2C_Theory_and_Practice.html">196 nips-2005-Two view learning: SVM-2K, Theory and Practice</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.176), (1, 0.071), (2, -0.026), (3, -0.115), (4, 0.134), (5, 0.097), (6, -0.107), (7, 0.036), (8, -0.059), (9, -0.088), (10, -0.021), (11, 0.028), (12, -0.039), (13, -0.013), (14, 0.074), (15, 0.108), (16, 0.05), (17, -0.015), (18, 0.158), (19, 0.028), (20, -0.069), (21, -0.044), (22, -0.036), (23, 0.023), (24, -0.198), (25, -0.029), (26, 0.046), (27, 0.013), (28, -0.07), (29, -0.154), (30, -0.053), (31, -0.111), (32, 0.079), (33, 0.036), (34, -0.09), (35, -0.028), (36, 0.027), (37, 0.122), (38, 0.098), (39, -0.091), (40, 0.018), (41, 0.026), (42, -0.006), (43, -0.037), (44, -0.003), (45, 0.081), (46, 0.071), (47, -0.005), (48, -0.104), (49, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9583624 <a title="49-lsi-1" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>Author: Aurelie C. Lozano, Sanjeev R. Kulkarni, Robert E. Schapire</p><p>Abstract: We study the statistical convergence and consistency of regularized Boosting methods, where the samples are not independent and identically distributed (i.i.d.) but come from empirical processes of stationary β-mixing sequences. Utilizing a technique that constructs a sequence of independent blocks close in distribution to the original samples, we prove the consistency of the composite classiﬁers resulting from a regularization achieved by restricting the 1-norm of the base classiﬁers’ weights. When compared to the i.i.d. case, the nature of sampling manifests in the consistency result only through generalization of the original condition on the growth of the regularization parameter.</p><p>2 0.57543272 <a title="49-lsi-2" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>Author: Régis Vert, Jean-philippe Vert</p><p>Abstract: We determine the asymptotic limit of the function computed by support vector machines (SVM) and related algorithms that minimize a regularized empirical convex loss function in the reproducing kernel Hilbert space of the Gaussian RBF kernel, in the situation where the number of examples tends to inﬁnity, the bandwidth of the Gaussian kernel tends to 0, and the regularization parameter is held ﬁxed. Non-asymptotic convergence bounds to this limit in the L2 sense are provided, together with upper bounds on the classiﬁcation error that is shown to converge to the Bayes risk, therefore proving the Bayes-consistency of a variety of methods although the regularization term does not vanish. These results are particularly relevant to the one-class SVM, for which the regularization can not vanish by construction, and which is shown for the ﬁrst time to be a consistent density level set estimator. 1</p><p>3 0.55342096 <a title="49-lsi-3" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>Author: O. P. Kreidl, Alan S. Willsky</p><p>Abstract: Given a directed graphical model with binary-valued hidden nodes and real-valued noisy observations, consider deciding upon the maximum a-posteriori (MAP) or the maximum posterior-marginal (MPM) assignment under the restriction that each node broadcasts only to its children exactly one single-bit message. We present a variational formulation, viewing the processing rules local to all nodes as degrees-of-freedom, that minimizes the loss in expected (MAP or MPM) performance subject to such online communication constraints. The approach leads to a novel message-passing algorithm to be executed ofﬂine, or before observations are realized, which mitigates the performance loss by iteratively coupling all rules in a manner implicitly driven by global statistics. We also provide (i) illustrative examples, (ii) assumptions that guarantee convergence and efﬁciency and (iii) connections to active research areas. 1</p><p>4 0.5528338 <a title="49-lsi-4" href="./nips-2005-Learning_Minimum_Volume_Sets.html">112 nips-2005-Learning Minimum Volume Sets</a></p>
<p>Author: Clayton Scott, Robert Nowak</p><p>Abstract: Given a probability measure P and a reference measure µ, one is often interested in the minimum µ-measure set with P -measure at least α. Minimum volume sets of this type summarize the regions of greatest probability mass of P , and are useful for detecting anomalies and constructing conﬁdence regions. This paper addresses the problem of estimating minimum volume sets based on independent samples distributed according to P . Other than these samples, no other information is available regarding P , but the reference measure µ is assumed to be known. We introduce rules for estimating minimum volume sets that parallel the empirical risk minimization and structural risk minimization principles in classiﬁcation. As in classiﬁcation, we show that the performances of our estimators are controlled by the rate of uniform convergence of empirical to true probabilities over the class from which the estimator is drawn. Thus we obtain ﬁnite sample size performance bounds in terms of VC dimension and related quantities. We also demonstrate strong universal consistency and an oracle inequality. Estimators based on histograms and dyadic partitions illustrate the proposed rules. 1</p><p>5 0.5391562 <a title="49-lsi-5" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>Author: Xuanlong Nguyen, Martin J. Wainwright, Michael I. Jordan</p><p>Abstract: In this paper, we provide a general theorem that establishes a correspondence between surrogate loss functions in classiﬁcation and the family of f -divergences. Moreover, we provide constructive procedures for determining the f -divergence induced by a given surrogate loss, and conversely for ﬁnding all surrogate loss functions that realize a given f -divergence. Next we introduce the notion of universal equivalence among loss functions and corresponding f -divergences, and provide necessary and sufﬁcient conditions for universal equivalence to hold. These ideas have applications to classiﬁcation problems that also involve a component of experiment design; in particular, we leverage our results to prove consistency of a procedure for learning a classiﬁer under decentralization requirements. 1</p><p>6 0.48025948 <a title="49-lsi-6" href="./nips-2005-Statistical_Convergence_of_Kernel_CCA.html">182 nips-2005-Statistical Convergence of Kernel CCA</a></p>
<p>7 0.45866889 <a title="49-lsi-7" href="./nips-2005-Beyond_Gaussian_Processes%3A_On_the_Distributions_of_Infinite_Networks.html">38 nips-2005-Beyond Gaussian Processes: On the Distributions of Infinite Networks</a></p>
<p>8 0.45433682 <a title="49-lsi-8" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>9 0.45014054 <a title="49-lsi-9" href="./nips-2005-Rodeo%3A_Sparse_Nonparametric_Regression_in_High_Dimensions.html">168 nips-2005-Rodeo: Sparse Nonparametric Regression in High Dimensions</a></p>
<p>10 0.44518107 <a title="49-lsi-10" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>11 0.40192774 <a title="49-lsi-11" href="./nips-2005-Generalization_Error_Bounds_for_Aggregation_by_Mirror_Descent_with_Averaging.html">82 nips-2005-Generalization Error Bounds for Aggregation by Mirror Descent with Averaging</a></p>
<p>12 0.37295607 <a title="49-lsi-12" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>13 0.36988661 <a title="49-lsi-13" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>14 0.34661311 <a title="49-lsi-14" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>15 0.34026369 <a title="49-lsi-15" href="./nips-2005-Gaussian_Processes_for_Multiuser_Detection_in_CDMA_receivers.html">81 nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</a></p>
<p>16 0.33458281 <a title="49-lsi-16" href="./nips-2005-TD%280%29_Leads_to_Better_Policies_than_Approximate_Value_Iteration.html">186 nips-2005-TD(0) Leads to Better Policies than Approximate Value Iteration</a></p>
<p>17 0.33038273 <a title="49-lsi-17" href="./nips-2005-Worst-Case_Bounds_for_Gaussian_Process_Models.html">205 nips-2005-Worst-Case Bounds for Gaussian Process Models</a></p>
<p>18 0.32200986 <a title="49-lsi-18" href="./nips-2005-Efficient_estimation_of_hidden_state_dynamics_from_spike_trains.html">64 nips-2005-Efficient estimation of hidden state dynamics from spike trains</a></p>
<p>19 0.31146455 <a title="49-lsi-19" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>20 0.30994451 <a title="49-lsi-20" href="./nips-2005-Factorial_Switching_Kalman_Filters_for_Condition_Monitoring_in_Neonatal_Intensive_Care.html">68 nips-2005-Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.137), (5, 0.012), (10, 0.049), (27, 0.027), (31, 0.06), (34, 0.097), (44, 0.175), (54, 0.047), (55, 0.053), (69, 0.03), (72, 0.014), (73, 0.03), (88, 0.076), (91, 0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9110679 <a title="49-lda-1" href="./nips-2005-Efficient_estimation_of_hidden_state_dynamics_from_spike_trains.html">64 nips-2005-Efficient estimation of hidden state dynamics from spike trains</a></p>
<p>Author: Marton G. Danoczy, Richard H. R. Hahnloser</p><p>Abstract: Neurons can have rapidly changing spike train statistics dictated by the underlying network excitability or behavioural state of an animal. To estimate the time course of such state dynamics from single- or multiple neuron recordings, we have developed an algorithm that maximizes the likelihood of observed spike trains by optimizing the state lifetimes and the state-conditional interspike-interval (ISI) distributions. Our nonparametric algorithm is free of time-binning and spike-counting problems and has the computational complexity of a Mixed-state Markov Model operating on a state sequence of length equal to the total number of recorded spikes. As an example, we ﬁt a two-state model to paired recordings of premotor neurons in the sleeping songbird. We ﬁnd that the two state-conditional ISI functions are highly similar to the ones measured during waking and singing, respectively. 1</p><p>same-paper 2 0.89518327 <a title="49-lda-2" href="./nips-2005-Convergence_and_Consistency_of_Regularized_Boosting_Algorithms_with_Stationary_B-Mixing_Observations.html">49 nips-2005-Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations</a></p>
<p>Author: Aurelie C. Lozano, Sanjeev R. Kulkarni, Robert E. Schapire</p><p>Abstract: We study the statistical convergence and consistency of regularized Boosting methods, where the samples are not independent and identically distributed (i.i.d.) but come from empirical processes of stationary β-mixing sequences. Utilizing a technique that constructs a sequence of independent blocks close in distribution to the original samples, we prove the consistency of the composite classiﬁers resulting from a regularization achieved by restricting the 1-norm of the base classiﬁers’ weights. When compared to the i.i.d. case, the nature of sampling manifests in the consistency result only through generalization of the original condition on the growth of the regularization parameter.</p><p>3 0.87726092 <a title="49-lda-3" href="./nips-2005-Measuring_Shared_Information_and_Coordinated_Activity_in_Neuronal_Networks.html">124 nips-2005-Measuring Shared Information and Coordinated Activity in Neuronal Networks</a></p>
<p>Author: Kristina Klinkner, Cosma Shalizi, Marcelo Camperi</p><p>Abstract: Most nervous systems encode information about stimuli in the responding activity of large neuronal networks. This activity often manifests itself as dynamically coordinated sequences of action potentials. Since multiple electrode recordings are now a standard tool in neuroscience research, it is important to have a measure of such network-wide behavioral coordination and information sharing, applicable to multiple neural spike train data. We propose a new statistic, informational coherence, which measures how much better one unit can be predicted by knowing the dynamical state of another. We argue informational coherence is a measure of association and shared information which is superior to traditional pairwise measures of synchronization and correlation. To ﬁnd the dynamical states, we use a recently-introduced algorithm which reconstructs effective state spaces from stochastic time series. We then extend the pairwise measure to a multivariate analysis of the network by estimating the network multi-information. We illustrate our method by testing it on a detailed model of the transition from gamma to beta rhythms. Much of the most important information in neural systems is shared over multiple neurons or cortical areas, in such forms as population codes and distributed representations [1]. On behavioral time scales, neural information is stored in temporal patterns of activity as opposed to static markers; therefore, as information is shared between neurons or brain regions, it is physically instantiated as coordination between entire sequences of neural spikes. Furthermore, neural systems and regions of the brain often require coordinated neural activity to perform important functions; acting in concert requires multiple neurons or cortical areas to share information [2]. Thus, if we want to measure the dynamic network-wide behavior of neurons and test hypotheses about them, we need reliable, practical methods to detect and quantify behavioral coordination and the associated information sharing across multiple neural units. These would be especially useful in testing ideas about how particular forms of coordination relate to distributed coding (e.g., that of [3]). Current techniques to analyze relations among spike trains handle only pairs of neurons, so we further need a method which is extendible to analyze the coordination in the network, system, or region as a whole. Here we propose a new measure of behavioral coordination and information sharing, informational coherence, based on the notion of dynamical state. Section 1 argues that coordinated behavior in neural systems is often not captured by exist- ing measures of synchronization or correlation, and that something sensitive to nonlinear, stochastic, predictive relationships is needed. Section 2 deﬁnes informational coherence as the (normalized) mutual information between the dynamical states of two systems and explains how looking at the states, rather than just observables, fulﬁlls the needs laid out in Section 1. Since we rarely know the right states a prori, Section 2.1 brieﬂy describes how we reconstruct effective state spaces from data. Section 2.2 gives some details about how we calculate the informational coherence and approximate the global information stored in the network. Section 3 applies our method to a model system (a biophysically detailed conductance-based model) comparing our results to those of more familiar second-order statistics. In the interest of space, we omit proofs and a full discussion of the existing literature, giving only minimal references here; proofs and references will appear in a longer paper now in preparation. 1 Synchrony or Coherence? Most hypotheses which involve the idea that information sharing is reﬂected in coordinated activity across neural units invoke a very speciﬁc notion of coordinated activity, namely strict synchrony: the units should be doing exactly the same thing (e.g., spiking) at exactly the same time. Investigators then measure coordination by measuring how close the units come to being strictly synchronized (e.g., variance in spike times). From an informational point of view, there is no reason to favor strict synchrony over other kinds of coordination. One neuron consistently spiking 50 ms after another is just as informative a relationship as two simultaneously spiking, but such stable phase relations are missed by strict-synchrony approaches. Indeed, whatever the exact nature of the neural code, it uses temporally extended patterns of activity, and so information sharing should be reﬂected in coordination of those patterns, rather than just the instantaneous activity. There are three common ways of going beyond strict synchrony: cross-correlation and related second-order statistics, mutual information, and topological generalized synchrony. The cross-correlation function (the normalized covariance function; this includes, for present purposes, the joint peristimulus time histogram [2]), is one of the most widespread measures of synchronization. It can be efﬁciently calculated from observable series; it handles statistical as well as deterministic relationships between processes; by incorporating variable lags, it reduces the problem of phase locking. Fourier transformation of the covariance function γXY (h) yields the cross-spectrum FXY (ν), which in turn gives the 2 spectral coherence cXY (ν) = FXY (ν)/FX (ν)FY (ν), a normalized correlation between the Fourier components of X and Y . Integrated over frequencies, the spectral coherence measures, essentially, the degree of linear cross-predictability of the two series. ([4] applies spectral coherence to coordinated neural activity.) However, such second-order statistics only handle linear relationships. Since neural processes are known to be strongly nonlinear, there is little reason to think these statistics adequately measure coordination and synchrony in neural systems. Mutual information is attractive because it handles both nonlinear and stochastic relationships and has a very natural and appealing interpretation. Unfortunately, it often seems to fail in practice, being disappointingly small even between signals which are known to be tightly coupled [5]. The major reason is that the neural codes use distinct patterns of activity over time, rather than many different instantaneous actions, and the usual approach misses these extended patterns. Consider two neurons, one of which drives the other to spike 50 ms after it does, the driving neuron spiking once every 500 ms. These are very tightly coordinated, but whether the ﬁrst neuron spiked at time t conveys little information about what the second neuron is doing at t — it’s not spiking, but it’s not spiking most of the time anyway. Mutual information calculated from the direct observations conﬂates the “no spike” of the second neuron preparing to ﬁre with its just-sitting-around “no spike”. Here, mutual information could ﬁnd the coordination if we used a 50 ms lag, but that won’t work in general. Take two rate-coding neurons with base-line ﬁring rates of 1 Hz, and suppose that a stimulus excites one to 10 Hz and suppresses the other to 0.1 Hz. The spiking rates thus share a lot of information, but whether the one neuron spiked at t is uninformative about what the other neuron did then, and lagging won’t help. Generalized synchrony is based on the idea of establishing relationships between the states of the various units. “State” here is taken in the sense of physics, dynamics and control theory: the state at time t is a variable which ﬁxes the distribution of observables at all times ≥ t, rendering the past of the system irrelevant [6]. Knowing the state allows us to predict, as well as possible, how the system will evolve, and how it will respond to external forces [7]. Two coupled systems are said to exhibit generalized synchrony if the state of one system is given by a mapping from the state of the other. Applications to data employ statespace reconstruction [8]: if the state x ∈ X evolves according to smooth, d-dimensional deterministic dynamics, and we observe a generic function y = f (x), then the space Y of time-delay vectors [y(t), y(t − τ ), ...y(t − (k − 1)τ )] is diffeomorphic to X if k > 2d, for generic choices of lag τ . The various versions of generalized synchrony differ on how, precisely, to quantify the mappings between reconstructed state spaces, but they all appear to be empirically equivalent to one another and to notions of phase synchronization based on Hilbert transforms [5]. Thus all of these measures accommodate nonlinear relationships, and are potentially very ﬂexible. Unfortunately, there is essentially no reason to believe that neural systems have deterministic dynamics at experimentally-accessible levels of detail, much less that there are deterministic relationships among such states for different units. What we want, then, but none of these alternatives provides, is a quantity which measures predictive relationships among states, but allows those relationships to be nonlinear and stochastic. The next section introduces just such a measure, which we call “informational coherence”. 2 States and Informational Coherence There are alternatives to calculating the “surface” mutual information between the sequences of observations themselves (which, as described, fails to capture coordination). If we know that the units are phase oscillators, or rate coders, we can estimate their instantaneous phase or rate and, by calculating the mutual information between those variables, see how coordinated the units’ patterns of activity are. However, phases and rates do not exhaust the repertoire of neural patterns and a more general, common scheme is desirable. The most general notion of “pattern of activity” is simply that of the dynamical state of the system, in the sense mentioned above. We now formalize this. Assuming the usual notation for Shannon information [9], the information content of a state variable X is H[X] and the mutual information between X and Y is I[X; Y ]. As is well-known, I[X; Y ] ≤ min H[X], H[Y ]. We use this to normalize the mutual state information to a 0 − 1 scale, and this is the informational coherence (IC). ψ(X, Y ) = I[X; Y ] , with 0/0 = 0 . min H[X], H[Y ] (1) ψ can be interpreted as follows. I[X; Y ] is the Kullback-Leibler divergence between the joint distribution of X and Y , and the product of their marginal distributions [9], indicating the error involved in ignoring the dependence between X and Y . The mutual information between predictive, dynamical states thus gauges the error involved in assuming the two systems are independent, i.e., how much predictions could improve by taking into account the dependence. Hence it measures the amount of dynamically-relevant information shared between the two systems. ψ simply normalizes this value, and indicates the degree to which two systems have coordinated patterns of behavior (cf. [10], although this only uses directly observable quantities). 2.1 Reconstruction and Estimation of Effective State Spaces As mentioned, the state space of a deterministic dynamical system can be reconstructed from a sequence of observations. This is the main tool of experimental nonlinear dynamics [8]; but the assumption of determinism is crucial and false, for almost any interesting neural system. While classical state-space reconstruction won’t work on stochastic processes, such processes do have state-space representations [11], and, in the special case of discretevalued, discrete-time series, there are ways to reconstruct the state space. Here we use the CSSR algorithm, introduced in [12] (code available at http://bactra.org/CSSR). This produces causal state models, which are stochastic automata capable of statistically-optimal nonlinear prediction; the state of the machine is a minimal sufﬁcient statistic for the future of the observable process[13].1 The basic idea is to form a set of states which should be (1) Markovian, (2) sufﬁcient statistics for the next observable, and (3) have deterministic transitions (in the automata-theory sense). The algorithm begins with a minimal, one-state, IID model, and checks whether these properties hold, by means of hypothesis tests. If they fail, the model is modiﬁed, generally but not always by adding more states, and the new model is checked again. Each state of the model corresponds to a distinct distribution over future events, i.e., to a statistical pattern of behavior. Under mild conditions, which do not involve prior knowledge of the state space, CSSR converges in probability to the unique causal state model of the data-generating process [12]. In practice, CSSR is quite fast (linear in the data size), and generalizes at least as well as training hidden Markov models with the EM algorithm and using cross-validation for selection, the standard heuristic [12]. One advantage of the causal state approach (which it shares with classical state-space reconstruction) is that state estimation is greatly simpliﬁed. In the general case of nonlinear state estimation, it is necessary to know not just the form of the stochastic dynamics in the state space and the observation function, but also their precise parametric values and the distribution of observation and driving noises. Estimating the state from the observable time series then becomes a computationally-intensive application of Bayes’s Rule [17]. Due to the way causal states are built as statistics of the data, with probability 1 there is a ﬁnite time, t, at which the causal state at time t is certain. This is not just with some degree of belief or conﬁdence: because of the way the states are constructed, it is impossible for the process to be in any other state at that time. Once the causal state has been established, it can be updated recursively, i.e., the causal state at time t + 1 is an explicit function of the causal state at time t and the observation at t + 1. The causal state model can be automatically converted, therefore, into a ﬁnite-state transducer which reads in an observation time series and outputs the corresponding series of states [18, 13]. (Our implementation of CSSR ﬁlters its training data automatically.) The result is a new time series of states, from which all non-predictive components have been ﬁltered out. 2.2 Estimating the Coherence Our algorithm for estimating the matrix of informational coherences is as follows. For each unit, we reconstruct the causal state model, and ﬁlter the observable time series to produce a series of causal states. Then, for each pair of neurons, we construct a joint histogram of 1 Causal state models have the same expressive power as observable operator models [14] or predictive state representations [7], and greater power than variable-length Markov models [15, 16]. a b Figure 1: Rastergrams of neuronal spike-times in the network. Excitatory, pyramidal neurons (numbers 1 to 1000) are shown in green, inhibitory interneurons (numbers 1001 to 1300) in red. During the ﬁrst 10 seconds (a), the current connections among the pyramidal cells are suppressed and a gamma rhythm emerges (left). At t = 10s, those connections become active, leading to a beta rhythm (b, right). the state distribution, estimate the mutual information between the states, and normalize by the single-unit state informations. This gives a symmetric matrix of ψ values. Even if two systems are independent, their estimated IC will, on average, be positive, because, while they should have zero mutual information, the empirical estimate of mutual information is non-negative. Thus, the signiﬁcance of IC values must be assessed against the null hypothesis of system independence. The easiest way to do so is to take the reconstructed state models for the two systems and run them forward, independently of one another, to generate a large number of simulated state sequences; from these calculate values of the IC. This procedure will approximate the sampling distribution of the IC under a null model which preserves the dynamics of each system, but not their interaction. We can then ﬁnd p-values as usual. We omit them here to save space. 2.3 Approximating the Network Multi-Information There is broad agreement [2] that analyses of networks should not just be an analysis of pairs of neurons, averaged over pairs. Ideally, an analysis of information sharing in a network would look at the over-all structure of statistical dependence between the various units, reﬂected in the complete joint probability distribution P of the states. This would then allow us, for instance, to calculate the n-fold multi-information, I[X1 , X2 , . . . Xn ] ≡ D(P ||Q), the Kullback-Leibler divergence between the joint distribution P and the product of marginal distributions Q, analogous to the pairwise mutual information [19]. Calculated over the predictive states, the multi-information would give the total amount of shared dynamical information in the system. Just as we normalized the mutual information I[X1 , X2 ] by its maximum possible value, min H[X1 ], H[X2 ], we normalize the multiinformation by its maximum, which is the smallest sum of n − 1 marginal entropies: I[X1 ; X2 ; . . . Xn ] ≤ min k H[Xn ] i=k Unfortunately, P is a distribution over a very high dimensional space and so, hard to estimate well without strong parametric constraints. We thus consider approximations. The lowest-order approximation treats all the units as independent; this is the distribution Q. One step up are tree distributions, where the global distribution is a function of the joint distributions of pairs of units. Not every pair of units needs to enter into such a distribution, though every unit must be part of some pair. Graphically, a tree distribution corresponds to a spanning tree, with edges linking units whose interactions enter into the global probability, and conversely spanning trees determine tree distributions. Writing ET for the set of pairs (i, j) and abbreviating X1 = x1 , X2 = x2 , . . . Xn = xn by X = x, one has n T (X = x) = (i,j)∈ET T (Xi = xi , Xj = xj ) T (Xi = xi ) T (Xi = xi )T (Xj = xj ) i=1 (2) where the marginal distributions T (Xi ) and the pair distributions T (Xi , Xj ) are estimated by the empirical marginal and pair distributions. We must now pick edges ET so that T best approximates the true global distribution P . A natural approach is to minimize D(P ||T ), the divergence between P and its tree approximation. Chow and Liu [20] showed that the maximum-weight spanning tree gives the divergence-minimizing distribution, taking an edge’s weight to be the mutual information between the variables it links. There are three advantages to using the Chow-Liu approximation. (1) Estimating T from empirical probabilities gives a consistent maximum likelihood estimator of the ideal ChowLiu tree [20], with reasonable rates of convergence, so T can be reliably known even if P cannot. (2) There are efﬁcient algorithms for constructing maximum-weight spanning trees, such as Prim’s algorithm [21, sec. 23.2], which runs in time O(n2 + n log n). Thus, the approximation is computationally tractable. (3) The KL divergence of the Chow-Liu distribution from Q gives a lower bound on the network multi-information; that bound is just the sum of the mutual informations along the edges in the tree: I[X1 ; X2 ; . . . Xn ] ≥ D(T ||Q) = I[Xi ; Xj ] (3) (i,j)∈ET Even if we knew P exactly, Eq. 3 would be useful as an alternative to calculating D(P ||Q) directly, evaluating log P (x)/Q(x) for all the exponentially-many conﬁgurations x. It is natural to seek higher-order approximations to P , e.g., using three-way interactions not decomposable into pairwise interactions [22, 19]. But it is hard to do so effectively, because ﬁnding the optimal approximation to P when such interactions are allowed is NP [23], and analytical formulas like Eq. 3 generally do not exist [19]. We therefore conﬁne ourselves to the Chow-Liu approximation here. 3 Example: A Model of Gamma and Beta Rhythms We use simulated data as a test case, instead of empirical multiple electrode recordings, which allows us to try the method on a system of over 1000 neurons and compare the measure against expected results. The model, taken from [24], was originally designed to study episodes of gamma (30–80Hz) and beta (12–30Hz) oscillations in the mammalian nervous system, which often occur successively with a spontaneous transition between them. More concretely, the rhythms studied were those displayed by in vitro hippocampal (CA1) slice preparations and by in vivo neocortical EEGs. The model contains two neuron populations: excitatory (AMPA) pyramidal neurons and inhibitory (GABAA ) interneurons, deﬁned by conductance-based Hodgkin-Huxley-style equations. Simulations were carried out in a network of 1000 pyramidal cells and 300 interneurons. Each cell was modeled as a one-compartment neuron with all-to-all coupling, endowed with the basic sodium and potassium spiking currents, an external applied current, and some Gaussian input noise. The ﬁrst 10 seconds of the simulation correspond to the gamma rhythm, in which only a group of neurons is made to spike via a linearly increasing applied current. The beta rhythm a b c d Figure 2: Heat-maps of coordination for the network, as measured by zero-lag cross-correlation (top row) and informational coherence (bottom), contrasting the gamma rhythm (left column) with the beta (right). Colors run from red (no coordination) through yellow to pale cream (maximum). (subsequent 10 seconds) is obtained by activating pyramidal-pyramidal recurrent connections (potentiated by Hebbian preprocessing as a result of synchrony during the gamma rhythm) and a slow outward after-hyper-polarization (AHP) current (the M-current), suppressed during gamma due to the metabotropic activation used in the generation of the rhythm. During the beta rhythm, pyramidal cells, silent during gamma rhythm, ﬁre on a subset of interneurons cycles (Fig. 1). Fig. 2 compares zero-lag cross-correlation, a second-order method of quantifying coordination, with the informational coherence calculated from the reconstructed states. (In this simulation, we could have calculated the actual states of the model neurons directly, rather than reconstructing them, but for purposes of testing our method we did not.) Crosscorrelation ﬁnds some of the relationships visible in Fig. 1, but is confused by, for instance, the phase shifts between pyramidal cells. (Surface mutual information, not shown, gives similar results.) Informational coherence, however, has no trouble recognizing the two populations as effectively coordinated blocks. The presence of dynamical noise, problematic for ordinary state reconstruction, is not an issue. The average IC is 0.411 (or 0.797 if the inactive, low-numbered neurons are excluded). The tree estimate of the global informational multi-information is 3243.7 bits, with a global coherence of 0.777. The right half of Fig. 2 repeats this analysis for the beta rhythm; in this stage, the average IC is 0.614, and the tree estimate of the global multi-information is 7377.7 bits, though the estimated global coherence falls very slightly to 0.742. This is because low-numbered neurons which were quiescent before are now active, contributing to the global information, but the over-all pattern is somewhat weaker and more noisy (as can be seen from Fig. 1b.) So, as expected, the total information content is higher, but the overall coordination across the network is lower. 4 Conclusion Informational coherence provides a measure of neural information sharing and coordinated activity which accommodates nonlinear, stochastic relationships between extended patterns of spiking. It is robust to dynamical noise and leads to a genuinely multivariate measure of global coordination across networks or regions. Applied to data from multi-electrode recordings, it should be a valuable tool in evaluating hypotheses about distributed neural representation and function. Acknowledgments Thanks to R. Haslinger, E. Ionides and S. Page; and for support to the Santa Fe Institute (under grants from Intel, the NSF and the MacArthur Foundation, and DARPA agreement F30602-00-2-0583), the Clare Booth Luce Foundation (KLK) and the James S. McDonnell Foundation (CRS). References [1] L. F. Abbott and T. J. Sejnowski, eds. Neural Codes and Distributed Representations. MIT Press, 1998. [2] E. N. Brown, R. E. Kass, and P. P. Mitra. Nature Neuroscience, 7:456–461, 2004. [3] D. H. Ballard, Z. Zhang, and R. P. N. Rao. In R. P. N. Rao, B. A. Olshausen, and M. S. Lewicki, eds., Probabilistic Models of the Brain, pp. 273–284, MIT Press, 2002. [4] D. R. Brillinger and A. E. P. Villa. In D. R. Brillinger, L. T. Fernholz, and S. Morgenthaler, eds., The Practice of Data Analysis, pp. 77–92. Princeton U.P., 1997. [5] R. Quian Quiroga et al. Physical Review E, 65:041903, 2002. [6] R. F. Streater. Statistical Dynamics. Imperial College Press, London. [7] M. L. Littman, R. S. Sutton, and S. Singh. In T. G. Dietterich, S. Becker, and Z. Ghahramani, eds., Advances in Neural Information Processing Systems 14, pp. 1555–1561. MIT Press, 2002. [8] H. Kantz and T. Schreiber. Nonlinear Time Series Analysis. Cambridge U.P., 1997. [9] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley, 1991. [10] M. Palus et al. Physical Review E, 63:046211, 2001. [11] F. B. Knight. Annals of Probability, 3:573–596, 1975. [12] C. R. Shalizi and K. L. Shalizi. In M. Chickering and J. Halpern, eds., Uncertainty in Artiﬁcial Intelligence: Proceedings of the Twentieth Conference, pp. 504–511. AUAI Press, 2004. [13] C. R. Shalizi and J. P. Crutchﬁeld. Journal of Statistical Physics, 104:817–819, 2001. [14] H. Jaeger. Neural Computation, 12:1371–1398, 2000. [15] D. Ron, Y. Singer, and N. Tishby. Machine Learning, 25:117–149, 1996. [16] P. B¨ hlmann and A. J. Wyner. Annals of Statistics, 27:480–513, 1999. u [17] N. U. Ahmed. Linear and Nonlinear Filtering for Scientists and Engineers. World Scientiﬁc, 1998. [18] D. R. Upper. PhD thesis, University of California, Berkeley, 1997. [19] E. Schneidman, S. Still, M. J. Berry, and W. Bialek. Physical Review Letters, 91:238701, 2003. [20] C. K. Chow and C. N. Liu. IEEE Transactions on Information Theory, IT-14:462–467, 1968. [21] T. H. Cormen et al. Introduction to Algorithms. 2nd ed. MIT Press, 2001. [22] S. Amari. IEEE Transacttions on Information Theory, 47:1701–1711, 2001. [23] S. Kirshner, P. Smyth, and A. Robertson. Tech. Rep. 04-04, UC Irvine, Information and Computer Science, 2004. [24] M. S. Olufsen et al. Journal of Computational Neuroscience, 14:33–54, 2003.</p><p>4 0.82785755 <a title="49-lda-4" href="./nips-2005-A_Probabilistic_Approach_for_Optimizing_Spectral_Clustering.html">13 nips-2005-A Probabilistic Approach for Optimizing Spectral Clustering</a></p>
<p>Author: Rong Jin, Feng Kang, Chris H. Ding</p><p>Abstract: Spectral clustering enjoys its success in both data clustering and semisupervised learning. But, most spectral clustering algorithms cannot handle multi-class clustering problems directly. Additional strategies are needed to extend spectral clustering algorithms to multi-class clustering problems. Furthermore, most spectral clustering algorithms employ hard cluster membership, which is likely to be trapped by the local optimum. In this paper, we present a new spectral clustering algorithm, named “Soft Cut”. It improves the normalized cut algorithm by introducing soft membership, and can be efﬁciently computed using a bound optimization algorithm. Our experiments with a variety of datasets have shown the promising performance of the proposed clustering algorithm. 1</p><p>5 0.75823194 <a title="49-lda-5" href="./nips-2005-Learning_Minimum_Volume_Sets.html">112 nips-2005-Learning Minimum Volume Sets</a></p>
<p>Author: Clayton Scott, Robert Nowak</p><p>Abstract: Given a probability measure P and a reference measure µ, one is often interested in the minimum µ-measure set with P -measure at least α. Minimum volume sets of this type summarize the regions of greatest probability mass of P , and are useful for detecting anomalies and constructing conﬁdence regions. This paper addresses the problem of estimating minimum volume sets based on independent samples distributed according to P . Other than these samples, no other information is available regarding P , but the reference measure µ is assumed to be known. We introduce rules for estimating minimum volume sets that parallel the empirical risk minimization and structural risk minimization principles in classiﬁcation. As in classiﬁcation, we show that the performances of our estimators are controlled by the rate of uniform convergence of empirical to true probabilities over the class from which the estimator is drawn. Thus we obtain ﬁnite sample size performance bounds in terms of VC dimension and related quantities. We also demonstrate strong universal consistency and an oracle inequality. Estimators based on histograms and dyadic partitions illustrate the proposed rules. 1</p><p>6 0.7309581 <a title="49-lda-6" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>7 0.72908008 <a title="49-lda-7" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>8 0.72043115 <a title="49-lda-8" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>9 0.71506101 <a title="49-lda-9" href="./nips-2005-Coarse_sample_complexity_bounds_for_active_learning.html">41 nips-2005-Coarse sample complexity bounds for active learning</a></p>
<p>10 0.70636433 <a title="49-lda-10" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>11 0.70629019 <a title="49-lda-11" href="./nips-2005-Variational_Bayesian_Stochastic_Complexity_of_Mixture_Models.html">201 nips-2005-Variational Bayesian Stochastic Complexity of Mixture Models</a></p>
<p>12 0.70584518 <a title="49-lda-12" href="./nips-2005-Generalization_to_Unseen_Cases.html">85 nips-2005-Generalization to Unseen Cases</a></p>
<p>13 0.70559746 <a title="49-lda-13" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>14 0.70249099 <a title="49-lda-14" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>15 0.70228022 <a title="49-lda-15" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>16 0.7003845 <a title="49-lda-16" href="./nips-2005-The_Forgetron%3A_A_Kernel-Based_Perceptron_on_a_Fixed_Budget.html">191 nips-2005-The Forgetron: A Kernel-Based Perceptron on a Fixed Budget</a></p>
<p>17 0.70034385 <a title="49-lda-17" href="./nips-2005-Q-Clustering.html">159 nips-2005-Q-Clustering</a></p>
<p>18 0.69726974 <a title="49-lda-18" href="./nips-2005-Unbiased_Estimator_of_Shape_Parameter_for_Spiking_Irregularities_under_Changing_Environments.html">197 nips-2005-Unbiased Estimator of Shape Parameter for Spiking Irregularities under Changing Environments</a></p>
<p>19 0.69237977 <a title="49-lda-19" href="./nips-2005-Kernelized_Infomax_Clustering.html">102 nips-2005-Kernelized Infomax Clustering</a></p>
<p>20 0.69144005 <a title="49-lda-20" href="./nips-2005-Learning_from_Data_of_Variable_Quality.html">117 nips-2005-Learning from Data of Variable Quality</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
