<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>105 nips-2005-Large-Scale Multiclass Transduction</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-105" href="#">nips2005-105</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>105 nips-2005-Large-Scale Multiclass Transduction</h1>
<br/><p>Source: <a title="nips-2005-105-pdf" href="http://papers.nips.cc/paper/2881-large-scale-multiclass-transduction.pdf">pdf</a></p><p>Author: Thomas Gärtner, Quoc V. Le, Simon Burton, Alex J. Smola, Vishy Vishwanathan</p><p>Abstract: We present a method for performing transductive inference on very large datasets. Our algorithm is based on multiclass Gaussian processes and is effective whenever the multiplication of the kernel matrix or its inverse with a vector can be computed sufﬁciently fast. This holds, for instance, for certain graph and string kernels. Transduction is achieved by variational inference over the unlabeled data subject to a balancing constraint. 1</p><p>Reference: <a title="nips-2005-105-reference" href="../nips2005_reference/nips-2005-Large-Scale_Multiclass_Transduction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 au  Abstract We present a method for performing transductive inference on very large datasets. [sent-13, score-0.194]
</p><p>2 Our algorithm is based on multiclass Gaussian processes and is effective whenever the multiplication of the kernel matrix or its inverse with a vector can be computed sufﬁciently fast. [sent-14, score-0.409]
</p><p>3 This holds, for instance, for certain graph and string kernels. [sent-15, score-0.204]
</p><p>4 Transduction is achieved by variational inference over the unlabeled data subject to a balancing constraint. [sent-16, score-0.245]
</p><p>5 1  Introduction  While obtaining labeled data remains a time and labor consuming task, acquisition and storage of unlabelled data is becoming increasingly cheap and easy. [sent-17, score-0.113]
</p><p>6 This development has driven machine learning research into exploring algorithms that make extensive use of unlabelled data at training time in order to obtain better generalization performance. [sent-18, score-0.116]
</p><p>7 A common problem of many transductive approaches is that they scale badly with the amount of unlabeled data, which prohibits the use of massive sets of unlabeled data. [sent-19, score-0.312]
</p><p>8 We perform classiﬁcation on a dataset consisting of a digraph with 75, 888 vertices and 508, 960 edges. [sent-21, score-0.177]
</p><p>9 To the best of our knowledge it has so far not been possible to perform transduction on graphs of this size in reasonable time (with standard hardware). [sent-22, score-0.296]
</p><p>10 Moreover, on graphs various methods for unsupervised learning have been proposed [12, 11], all of which are mainly concerned with computing the kernel matrix on training and test set jointly. [sent-25, score-0.358]
</p><p>11 Other formulations impose that the label assignment on the test set be consistent with the assumption of conﬁdent classiﬁcation [8]. [sent-26, score-0.138]
</p><p>12 Yet others impose that training and test set have similar marginal distributions [4]. [sent-27, score-0.222]
</p><p>13 It is particularly efﬁcient whenever Kα or K −1 α can be computed in linear time, where K ∈ Rm×m is the kernel matrix and α ∈ Rm . [sent-29, score-0.123]
</p><p>14 • We require consistency of training and test marginals. [sent-30, score-0.115]
</p><p>15 • Kernels (or their inverses) are computed on training and test set simultaneously. [sent-32, score-0.115]
</p><p>16 On graphs this can lead to considerable computational savings. [sent-33, score-0.152]
</p><p>17 • Self consistency of the estimates is achieved by a variational approach. [sent-34, score-0.073]
</p><p>18 This allows us to make use of Gaussian Process multiclass formulations. [sent-35, score-0.176]
</p><p>19 2  Multiclass Classiﬁcation  We begin with a brief overview over Gaussian Process multiclass classiﬁcation [10] recast in terms of exponential families. [sent-36, score-0.176]
</p><p>20 It is our goal to estimate y|x via p(y|x, θ) = exp ( φ(x, y), θ − g(θ|x)) where g(θ|x) = log  exp ( φ(x, y), θ ) . [sent-47, score-0.141]
</p><p>21 We impose a normal prior on θ, leading to the following negative joint likelihood in θ and Y : m  P := − log p(θ, Y |X) =  [g(θ|xi ) − φ(xi , yi ), θ ] + i=1  1 θ 2σ 2  2  + const. [sent-49, score-0.185]
</p><p>22 (2)  For transduction purposes p(θ, Y |X) will prove more useful than p(θ|Y, X). [sent-50, score-0.176]
</p><p>23 Note that a normal prior on θ with variance σ 2 1 implies a Gaussian process on the random variable t(x, y) := φ(x, y), θ with covariance kernel Cov [t(x, y), t(x′ , y ′ )] = σ 2 φ(x, y), φ(x′ , y ′ ) =: σ 2 k((x, y), (x′ , y ′ )). [sent-51, score-0.104]
</p><p>24 Here joint log-likelihood (2) in terms of α and K yields m  n  exp ([Kα]iy ) − tr µ⊤ Kα +  log i=1  y=1  1 tr α⊤ Kα + const. [sent-60, score-0.354]
</p><p>25 This is commonly done in Gaussian process literature and we will use both formulations, depending on the problem we need to solve: if Kα can be computed effectively, as is the case with string kernels [9], we use the α-parameterization. [sent-62, score-0.186]
</p><p>26 Conversely, if K −1 α is cheap, as for example with graph kernels [7], we use the t-parameterization. [sent-63, score-0.222]
</p><p>27 Derivatives Second order methods such as Conjugate Gradient require the computation of derivatives of − log p(θ, Y |X) with respect to θ in terms of α or t. [sent-64, score-0.09]
</p><p>28 ∗ (Kβ)) + σ −2 tr γ ⊤ Kβ  (8a)  2 ∂t P[u, v]  (8b)  ⊤  ⊤  = tr u (π. [sent-71, score-0.256]
</p><p>29 Combining this with rates of convergence for Newton-type or nonlinear CG solver strategies yields overall time costs in the order of O(m log m) to O(m2 ) worst case, a signiﬁcant improvement over conventional O(m3 ) methods. [sent-77, score-0.095]
</p><p>30 3  Transductive Inference by Variational Methods  As we are interested in transduction, the labels Y (and analogously the data X) decompose as Y = Ytrain ∪ Ytest . [sent-78, score-0.07]
</p><p>31 Instead, we now aim at estimating the mode of p(θ|X, Ytrain ) by variational means. [sent-80, score-0.073]
</p><p>32 [5]) − log p(θ|X, Ytrain ) ≤ − log p(θ|X, Ytrain ) + D(q(Ytest ) p(Ytest |X, Ytrain , θ)) =−  (log p(Ytest , θ|X, Ytrain ) − log q(Ytest )) q(Ytest )  (9) (10)  Ytest  holds. [sent-83, score-0.165]
</p><p>33 The key trick is that while using a factorizing approximation for q we restrict the latter to distributions which satisfy balancing constraints. [sent-85, score-0.122]
</p><p>34 That is, we require them to yield marginals on the unlabeled data which are comparable with the labeled observations. [sent-86, score-0.106]
</p><p>35 With qij := q(yi = j) we deﬁne µij (q) = qij for all i > mtrain and µij (q) = 1 if yi = 1 and 0 otherwise for all i ≤ mtrain . [sent-89, score-1.258]
</p><p>36 In other words, we are taking the expectation in µ over all unobserved labels Ytest with respect to the distribution q(Ytest ). [sent-90, score-0.07]
</p><p>37 We have q(Ytest ) log p(Ytest , θ|X, Ytrain ) Ytest m  =  n  exp ([Kα]ij ) − tr µ(q)⊤ Kα +  log i=1  j=1  1 tr α⊤ Kα + const. [sent-91, score-0.409]
</p><p>38 Since q facm  q(Ytest ) log q(Ytest ) = Ytest  qij log qij . [sent-95, score-0.834]
</p><p>39 (13)  i=mtrain +1  It is unreasonable to assume that q may be chosen freely from all factorizing distributions (the latter would lead to a straightforward EM algorithm for transductive inference): if we observe a certain distribution of labels on the training set, e. [sent-96, score-0.373]
</p><p>40 If m ≫ mtrain , however, a naive application of the variational bound can lead to cases where q is concentrated on one class — the increase in likelihood for a resulting very simple classiﬁer completely outweighs any balancing constraints implicit in the data. [sent-100, score-0.496]
</p><p>41 It is, incidentally, also the reason why SVM transduction optimization codes [4] impose a balancing constraint on the assignment of test labels. [sent-102, score-0.415]
</p><p>42 We impose the following conditions: m  n  − rj ≤  + qij ≤ rj for all j ∈ Y and i=mtrain +1  qij = 1 for all i ∈ {mtrain . [sent-103, score-0.949]
</p><p>43 j=1  − + Here the constraints rj = pemp (y = j) − ǫ and rj = pemp (y = j) + ǫ are chosen such as to correspond to conﬁdence intervals given by ﬁnite sample size tail bounds. [sent-106, score-0.36]
</p><p>44 In other mtrain words we set pemp (y = j) = m−1 train i=1 {yi = j} and ǫ such as to satisfy mtrain  m−1 train  Pr  mtest  ξi − m−1 test i=1  ′ ξi > ǫ  ≤δ  (14)  i=1  ′ for iid {0, 1} random variables ξi and ξi with mean p. [sent-107, score-0.713]
</p><p>45 7)] after application of a union bound over the class labels that ǫ ≤ log(2n/δ)m/ (2mtrain mtest ). [sent-111, score-0.186]
</p><p>46 4  Graphs, Strings and Vectors  We now discuss the two main applications where computational savings can be achieved: graphs and strings. [sent-112, score-0.12]
</p><p>47 In the case of graphs, the advantage arises from the fact that K −1 is sparse, whereas for texts we can use fast string kernels [9] to compute Kα in linear time. [sent-113, score-0.186]
</p><p>48 Graphs Denote by G(V, E) the graph given by vertices V and edges E where each edge is a set of two vertices. [sent-114, score-0.172]
</p><p>49 Then W ∈ R|V |×|V | denotes the adjacency matrix of the graph, where Wij > 0 only if edge {i, j} ∈ E. [sent-115, score-0.101]
</p><p>50 We assume that the graph G, and thus also the adjacency matrix W , is sparse. [sent-116, score-0.221]
</p><p>51 Now denote by 1 the identity matrix and by D the diagonal matrix of vertex degrees, i. [sent-117, score-0.108]
</p><p>52 Then the graph Laplacian and the normalized graph Laplacian of G are given by L := D − W  1 1 ˜ L := 1 − D− 2 W D− 2 ,  and  (15)  respectively. [sent-120, score-0.24]
</p><p>53 Many kernels K (or their inverse) on G are given by low-degree polynomials of the Laplacian or the adjacency matrix of G, such as the following: l  l  ci W 2i , K =  K= i=1  ˜ ˜ (1 − ci L), or K −1 = L + ǫ1. [sent-121, score-0.279]
</p><p>54 The ﬁrst kernel arises from an l-step random walk, the third case is typically referred to as regularized graph Laplacian. [sent-123, score-0.189]
</p><p>55 This means that if the average degree of the graph does not increase with the number of observations, L = O(m) as m = |V | for inference on graphs. [sent-125, score-0.155]
</p><p>56 From Graphs to Graphical Models Graphs are one of the examples where transduction actually improves computational cost: Assume that we are given the inverse kernel matrix K −1 on training and test set and we wish to perform induction only. [sent-126, score-0.494]
</p><p>57 In this case we need to compute the kernel matrix (or its inverse) restricted to the training set. [sent-127, score-0.191]
</p><p>58 Let K −1 = A B , then the upper left hand corner (representing the training set part only) of B⊤ C  −1  K is given by the Schur complement A − B ⊤ C −1 B . [sent-128, score-0.101]
</p><p>59 Moreover, neither the Schur complement nor its inverse are typically sparse. [sent-130, score-0.113]
</p><p>60 Here we have a nice connection between graphical models and graph kernels. [sent-131, score-0.165]
</p><p>61 In this case the inverse covariance matrix has nonzero entries only for variables with a direct dependency structure. [sent-133, score-0.134]
</p><p>62 In other words, if we are given a graphical model of normal random variables, their conditional independence structure is reﬂected by K −1 . [sent-135, score-0.08]
</p><p>63 In the same way as in graphical models marginalization may induce dependencies, computing the kernel matrix on the training set only, may lead to dense matrices, even when the inverse kernel on training and test data combined is sparse. [sent-136, score-0.587]
</p><p>64 The bottom line is there are cases where it is computationally cheaper to take both training and test set into account and optimize over a larger set of variables rather than dealing with a smaller dense matrix. [sent-137, score-0.17]
</p><p>65 Strings: Efﬁcient computation of string kernels using sufﬁx trees was described in [9]. [sent-138, score-0.186]
</p><p>66 The efﬁcient computation scheme covers all kernels of type k(x, x′ ) =  ws #s (x)#s (x′ )  (17)  s  for arbitrary ws ≥ 0. [sent-141, score-0.184]
</p><p>67 This means that computation time for evaluating Kα is again O( i |xi |) as we need to evaluate the kernel expansion for all x ∈ X. [sent-143, score-0.069]
</p><p>68 Since the average string length is independent of m this yields an O(m) algorithm for Kα. [sent-144, score-0.084]
</p><p>69 We have: minimize tr q ⊤ τ + q  qij log qij  (18)  i,j  − subject to qj ≤  + qij ≤ qj , qij ≥ 0 and i  qli = 1 for all j ∈ Y, l ∈ {1. [sent-158, score-1.695]
</p><p>70 mtest } i  Table 1: Error rates on some benchmark datasets (mostly from UCI). [sent-160, score-0.176]
</p><p>71 The last column is the error rates reported in [1] DATASET cancer cancer (progn. [sent-161, score-0.15]
</p><p>72 Using Lagrange multipliers one can show that q n needs to satisfy qij = exp(−τij )bi cj where bi , cj ≥ 0. [sent-219, score-0.424]
</p><p>73 Solving for j qij = 1 yields exp(−τ )c  ij j qij = Pn exp(−τil )cl . [sent-220, score-0.809]
</p><p>74 This means that instead of an optimization problem in mtest × n l=1 variables we now only need to optimize over n variables subject to 2n constraints. [sent-221, score-0.136]
</p><p>75 + − Note that the exact matching constraint where qi = qi amounts to a maximum likelihood problem for a shifted exponential family model where qij = exp(τij ) exp(γi − gj (γi )). [sent-222, score-0.426]
</p><p>76 It can be shown that the approximate matching problem is equivalent to a maximum a posteriori optimization problem using the norm dual to expectation constraints on qij . [sent-223, score-0.445]
</p><p>77 As initialization we choose γi such that the per class averages match the marginal constraint while ignoring the per sample balance. [sent-226, score-0.079]
</p><p>78 6  Experiments  Unfortunately, we are not aware of other multiclass transductive learning algorithms. [sent-228, score-0.369]
</p><p>79 To still be able to compare our approach to other transductive learning algorithms we performed experiments on some benchmark datasets. [sent-229, score-0.212]
</p><p>80 To investigate the performance of our algorithm in classifying vertices of a graph, we choose the WebKB dataset. [sent-230, score-0.084]
</p><p>81 Benchmark datasets Table 1 reports results on some benchmark datasets. [sent-231, score-0.083]
</p><p>82 To be able to compare the error rates of the transductive multiclass Gaussian Process classiﬁer proposed in this paper, we also report error rates from [2] and an inductive multiclass Gaussian Process classiﬁer. [sent-232, score-0.591]
</p><p>83 Parameters were chosen by crossvalidation inside the training folds. [sent-234, score-0.134]
</p><p>84 Graph Mining To illustrate the effectiveness of our approach on graphs we performed experiments on the well known WebKB dataset. [sent-235, score-0.12]
</p><p>85 This dataset consists of 8275 webpages classiﬁed into 7 classes. [sent-236, score-0.117]
</p><p>86 As we are using this dataset to evaluate our graph mining algorithm, we ignore the text on each webpage and consider the dataset as a labelled directed graph. [sent-238, score-0.335]
</p><p>87 We use the co-linkage graph and report results for ‘inverse’ 10fold stratiﬁed crossvalidations, i. [sent-242, score-0.12]
</p><p>88 , we use 1 fold as training data and 9 folds as test data. [sent-244, score-0.115]
</p><p>89 To overcome this, we predict on the test set as follows: For each class the instances that are most likely to be in this class are picked (if they haven’t been picked for a class with lower index) such that the fraction of instances assigned to this class is the same on the training and test set. [sent-247, score-0.382]
</p><p>90 Although a directed graph approach outperforms there an undirected approach, we resorted to kernels for undirected graphs, as those are computationally more attractive. [sent-250, score-0.258]
</p><p>91 We will investigate computationally attractive digraph kernels in future work and expect similar beneﬁts as reported by [11]. [sent-251, score-0.222]
</p><p>92 To investigate the behaviour of our algorithm with less training data, we performed a 20-fold inverse crossvalidation on the ‘wisconsin’ subset and observed an error rate of 17% there. [sent-253, score-0.246]
</p><p>93 To further strengthen our results and show that the runtime performance of our algorithm is sufﬁcient for classifying the vertices of massive graphs, we also performed initial experiments on the Epinions dataset collected by Mathew Richardson and Pedro Domingos. [sent-254, score-0.156]
</p><p>94 However, the experiments show that the algorithm can be run on very large graph datasets. [sent-259, score-0.12]
</p><p>95 7  Discussion and Extensions  We presented an efﬁcient method for performing transduction on multiclass estimation problems with Gaussian Processes. [sent-260, score-0.352]
</p><p>96 It performs particularly well whenever the kernel matrix has special numerical properties which allow fast matrix vector multiplication. [sent-261, score-0.177]
</p><p>97 That said, also on standard dense problems we observed very good improvements (typically a 10% reduction of the training error) over standard induction. [sent-262, score-0.123]
</p><p>98 Structured Labels and Conditional Random Fields are a clear area where to extend the transductive setting. [sent-263, score-0.159]
</p><p>99 The key obstacle to overcome in this context is to ﬁnd a suitable marginal distribution: with increasing structure of the labels the conﬁdence bounds per subclass decrease dramatically. [sent-264, score-0.116]
</p><p>100 Learning from labeled and unlabeled data on a o directed graph. [sent-358, score-0.095]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ytest', 0.459), ('qij', 0.362), ('ytrain', 0.306), ('mtrain', 0.25), ('transduction', 0.176), ('multiclass', 0.176), ('transductive', 0.159), ('tr', 0.128), ('graphs', 0.12), ('graph', 0.12), ('webkb', 0.111), ('kernels', 0.102), ('ij', 0.085), ('string', 0.084), ('iy', 0.083), ('mtest', 0.083), ('pemp', 0.083), ('rj', 0.082), ('inverse', 0.08), ('balancing', 0.078), ('variational', 0.073), ('labels', 0.07), ('kernel', 0.069), ('dataset', 0.069), ('training', 0.068), ('rm', 0.067), ('crossvalidation', 0.066), ('impose', 0.061), ('unlabeled', 0.059), ('classi', 0.059), ('digraph', 0.056), ('epinions', 0.056), ('mathew', 0.056), ('pedro', 0.056), ('rror', 0.056), ('topreviewers', 0.056), ('vishwanathan', 0.056), ('log', 0.055), ('dense', 0.055), ('matrix', 0.054), ('optimization', 0.053), ('benchmark', 0.053), ('gp', 0.053), ('cg', 0.053), ('vertices', 0.052), ('cornell', 0.048), ('unlabelled', 0.048), ('schur', 0.048), ('webpages', 0.048), ('test', 0.047), ('adjacency', 0.047), ('marginals', 0.047), ('sch', 0.047), ('marginal', 0.046), ('graphical', 0.045), ('picked', 0.044), ('richardson', 0.044), ('factorizing', 0.044), ('xi', 0.044), ('exp', 0.043), ('ws', 0.041), ('australian', 0.041), ('webpage', 0.041), ('rates', 0.04), ('gaussian', 0.04), ('wisconsin', 0.039), ('strings', 0.039), ('cancer', 0.039), ('ci', 0.038), ('lkopf', 0.036), ('smola', 0.036), ('directed', 0.036), ('inference', 0.035), ('australia', 0.035), ('cheap', 0.035), ('usps', 0.035), ('massive', 0.035), ('normal', 0.035), ('laplacian', 0.035), ('derivatives', 0.035), ('yi', 0.034), ('kluwer', 0.034), ('aware', 0.034), ('class', 0.033), ('complement', 0.033), ('suf', 0.032), ('investigate', 0.032), ('reported', 0.032), ('cation', 0.032), ('lead', 0.032), ('expand', 0.032), ('qj', 0.032), ('qi', 0.032), ('conjugate', 0.031), ('cj', 0.031), ('constraints', 0.03), ('storage', 0.03), ('formulations', 0.03), ('reports', 0.03), ('multiplication', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="105-tfidf-1" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>Author: Thomas Gärtner, Quoc V. Le, Simon Burton, Alex J. Smola, Vishy Vishwanathan</p><p>Abstract: We present a method for performing transductive inference on very large datasets. Our algorithm is based on multiclass Gaussian processes and is effective whenever the multiplication of the kernel matrix or its inverse with a vector can be computed sufﬁciently fast. This holds, for instance, for certain graph and string kernels. Transduction is achieved by variational inference over the unlabeled data subject to a balancing constraint. 1</p><p>2 0.16379881 <a title="105-tfidf-2" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>Author: Andreas Argyriou, Mark Herbster, Massimiliano Pontil</p><p>Abstract: A foundational problem in semi-supervised learning is the construction of a graph underlying the data. We propose to use a method which optimally combines a number of differently constructed graphs. For each of these graphs we associate a basic graph kernel. We then compute an optimal combined kernel. This kernel solves an extended regularization problem which requires a joint minimization over both the data and the set of graph kernels. We present encouraging results on different OCR tasks where the optimal combined kernel is computed from graphs constructed with a variety of distances functions and the ‘k’ in nearest neighbors. 1</p><p>3 0.14308925 <a title="105-tfidf-3" href="./nips-2005-Maximum_Margin_Semi-Supervised_Learning_for_Structured_Variables.html">123 nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</a></p>
<p>Author: Y. Altun, D. McAllester, M. Belkin</p><p>Abstract: Many real-world classiﬁcation problems involve the prediction of multiple inter-dependent variables forming some structural dependency. Recent progress in machine learning has mainly focused on supervised classiﬁcation of such structured variables. In this paper, we investigate structured classiﬁcation in a semi-supervised setting. We present a discriminative approach that utilizes the intrinsic geometry of input patterns revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables. Unlike transductive algorithms, our formulation naturally extends to new test points. 1</p><p>4 0.12674733 <a title="105-tfidf-4" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>Author: Benjamin V. Roy, Ciamac C. Moallemi</p><p>Abstract: We propose consensus propagation, an asynchronous distributed protocol for averaging numbers across a network. We establish convergence, characterize the convergence rate for regular graphs, and demonstrate that the protocol exhibits better scaling properties than pairwise averaging, an alternative that has received much recent attention. Consensus propagation can be viewed as a special case of belief propagation, and our results contribute to the belief propagation literature. In particular, beyond singly-connected graphs, there are very few classes of relevant problems for which belief propagation is known to converge. 1</p><p>5 0.12626649 <a title="105-tfidf-5" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>Author: Ashish Kapoor, Hyungil Ahn, Yuan Qi, Rosalind W. Picard</p><p>Abstract: There have been many graph-based approaches for semi-supervised classiﬁcation. One problem is that of hyperparameter learning: performance depends greatly on the hyperparameters of the similarity graph, transformation of the graph Laplacian and the noise model. We present a Bayesian framework for learning hyperparameters for graph-based semisupervised classiﬁcation. Given some labeled data, which can contain inaccurate labels, we pose the semi-supervised classiﬁcation as an inference problem over the unknown labels. Expectation Propagation is used for approximate inference and the mean of the posterior is used for classiﬁcation. The hyperparameters are learned using EM for evidence maximization. We also show that the posterior mean can be written in terms of the kernel matrix, providing a Bayesian classiﬁer to classify new points. Tests on synthetic and real datasets show cases where there are signiﬁcant improvements in performance over the existing approaches. 1</p><p>6 0.1261327 <a title="105-tfidf-6" href="./nips-2005-Transfer_learning_for_text_classification.html">195 nips-2005-Transfer learning for text classification</a></p>
<p>7 0.12348839 <a title="105-tfidf-7" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>8 0.10389281 <a title="105-tfidf-8" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>9 0.098580308 <a title="105-tfidf-9" href="./nips-2005-Variational_EM_Algorithms_for_Non-Gaussian_Latent_Variable_Models.html">202 nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</a></p>
<p>10 0.095311105 <a title="105-tfidf-10" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>11 0.086056724 <a title="105-tfidf-11" href="./nips-2005-Soft_Clustering_on_Graphs.html">178 nips-2005-Soft Clustering on Graphs</a></p>
<p>12 0.083744206 <a title="105-tfidf-12" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>13 0.082099386 <a title="105-tfidf-13" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>14 0.079251081 <a title="105-tfidf-14" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>15 0.076144606 <a title="105-tfidf-15" href="./nips-2005-From_Batch_to_Transductive_Online_Learning.html">76 nips-2005-From Batch to Transductive Online Learning</a></p>
<p>16 0.076079644 <a title="105-tfidf-16" href="./nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">113 nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<p>17 0.075721607 <a title="105-tfidf-17" href="./nips-2005-Sequence_and_Tree_Kernels_with_Statistical_Feature_Mining.html">175 nips-2005-Sequence and Tree Kernels with Statistical Feature Mining</a></p>
<p>18 0.074483678 <a title="105-tfidf-18" href="./nips-2005-An_Alternative_Infinite_Mixture_Of_Gaussian_Process_Experts.html">21 nips-2005-An Alternative Infinite Mixture Of Gaussian Process Experts</a></p>
<p>19 0.070461266 <a title="105-tfidf-19" href="./nips-2005-Correlated_Topic_Models.html">52 nips-2005-Correlated Topic Models</a></p>
<p>20 0.070095919 <a title="105-tfidf-20" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.258), (1, 0.143), (2, -0.075), (3, -0.072), (4, 0.001), (5, -0.049), (6, 0.051), (7, 0.057), (8, 0.075), (9, 0.064), (10, 0.056), (11, -0.119), (12, -0.03), (13, 0.029), (14, -0.078), (15, -0.165), (16, 0.025), (17, -0.069), (18, 0.031), (19, 0.082), (20, -0.034), (21, 0.056), (22, 0.014), (23, -0.003), (24, 0.103), (25, -0.067), (26, 0.021), (27, 0.045), (28, -0.008), (29, 0.057), (30, -0.002), (31, -0.044), (32, -0.031), (33, -0.025), (34, 0.088), (35, 0.111), (36, -0.077), (37, 0.029), (38, 0.047), (39, -0.013), (40, -0.024), (41, -0.001), (42, -0.038), (43, -0.08), (44, 0.017), (45, -0.017), (46, -0.143), (47, -0.032), (48, -0.047), (49, -0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92930567 <a title="105-lsi-1" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>Author: Thomas Gärtner, Quoc V. Le, Simon Burton, Alex J. Smola, Vishy Vishwanathan</p><p>Abstract: We present a method for performing transductive inference on very large datasets. Our algorithm is based on multiclass Gaussian processes and is effective whenever the multiplication of the kernel matrix or its inverse with a vector can be computed sufﬁciently fast. This holds, for instance, for certain graph and string kernels. Transduction is achieved by variational inference over the unlabeled data subject to a balancing constraint. 1</p><p>2 0.7376439 <a title="105-lsi-2" href="./nips-2005-Combining_Graph_Laplacians_for_Semi--Supervised_Learning.html">42 nips-2005-Combining Graph Laplacians for Semi--Supervised Learning</a></p>
<p>Author: Andreas Argyriou, Mark Herbster, Massimiliano Pontil</p><p>Abstract: A foundational problem in semi-supervised learning is the construction of a graph underlying the data. We propose to use a method which optimally combines a number of differently constructed graphs. For each of these graphs we associate a basic graph kernel. We then compute an optimal combined kernel. This kernel solves an extended regularization problem which requires a joint minimization over both the data and the set of graph kernels. We present encouraging results on different OCR tasks where the optimal combined kernel is computed from graphs constructed with a variety of distances functions and the ‘k’ in nearest neighbors. 1</p><p>3 0.72102213 <a title="105-lsi-3" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>Author: Ashish Kapoor, Hyungil Ahn, Yuan Qi, Rosalind W. Picard</p><p>Abstract: There have been many graph-based approaches for semi-supervised classiﬁcation. One problem is that of hyperparameter learning: performance depends greatly on the hyperparameters of the similarity graph, transformation of the graph Laplacian and the noise model. We present a Bayesian framework for learning hyperparameters for graph-based semisupervised classiﬁcation. Given some labeled data, which can contain inaccurate labels, we pose the semi-supervised classiﬁcation as an inference problem over the unknown labels. Expectation Propagation is used for approximate inference and the mean of the posterior is used for classiﬁcation. The hyperparameters are learned using EM for evidence maximization. We also show that the posterior mean can be written in terms of the kernel matrix, providing a Bayesian classiﬁer to classify new points. Tests on synthetic and real datasets show cases where there are signiﬁcant improvements in performance over the existing approaches. 1</p><p>4 0.68863273 <a title="105-lsi-4" href="./nips-2005-Maximum_Margin_Semi-Supervised_Learning_for_Structured_Variables.html">123 nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</a></p>
<p>Author: Y. Altun, D. McAllester, M. Belkin</p><p>Abstract: Many real-world classiﬁcation problems involve the prediction of multiple inter-dependent variables forming some structural dependency. Recent progress in machine learning has mainly focused on supervised classiﬁcation of such structured variables. In this paper, we investigate structured classiﬁcation in a semi-supervised setting. We present a discriminative approach that utilizes the intrinsic geometry of input patterns revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables. Unlike transductive algorithms, our formulation naturally extends to new test points. 1</p><p>5 0.6062026 <a title="105-lsi-5" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>Author: Glenn Fung, Rómer Rosales, Balaji Krishnapuram</p><p>Abstract: We propose efﬁcient algorithms for learning ranking functions from order constraints between sets—i.e. classes—of training samples. Our algorithms may be used for maximizing the generalized Wilcoxon Mann Whitney statistic that accounts for the partial ordering of the classes: special cases include maximizing the area under the ROC curve for binary classiﬁcation and its generalization for ordinal regression. Experiments on public benchmarks indicate that: (a) the proposed algorithm is at least as accurate as the current state-of-the-art; (b) computationally, it is several orders of magnitude faster and—unlike current methods—it is easily able to handle even large datasets with over 20,000 samples. 1</p><p>6 0.55372673 <a title="105-lsi-6" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>7 0.53148818 <a title="105-lsi-7" href="./nips-2005-Transfer_learning_for_text_classification.html">195 nips-2005-Transfer learning for text classification</a></p>
<p>8 0.51904714 <a title="105-lsi-8" href="./nips-2005-Efficient_Estimation_of_OOMs.html">62 nips-2005-Efficient Estimation of OOMs</a></p>
<p>9 0.51876402 <a title="105-lsi-9" href="./nips-2005-Sequence_and_Tree_Kernels_with_Statistical_Feature_Mining.html">175 nips-2005-Sequence and Tree Kernels with Statistical Feature Mining</a></p>
<p>10 0.51594549 <a title="105-lsi-10" href="./nips-2005-Learning_Topology_with_the_Generative_Gaussian_Graph_and_the_EM_Algorithm.html">116 nips-2005-Learning Topology with the Generative Gaussian Graph and the EM Algorithm</a></p>
<p>11 0.50732523 <a title="105-lsi-11" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>12 0.49674147 <a title="105-lsi-12" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>13 0.48568073 <a title="105-lsi-13" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>14 0.47564873 <a title="105-lsi-14" href="./nips-2005-Large_scale_networks_fingerprinting_and_visualization_using_the_k-core_decomposition.html">107 nips-2005-Large scale networks fingerprinting and visualization using the k-core decomposition</a></p>
<p>15 0.45697549 <a title="105-lsi-15" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>16 0.4495343 <a title="105-lsi-16" href="./nips-2005-Fast_Gaussian_Process_Regression_using_KD-Trees.html">69 nips-2005-Fast Gaussian Process Regression using KD-Trees</a></p>
<p>17 0.44937837 <a title="105-lsi-17" href="./nips-2005-Tensor_Subspace_Analysis.html">189 nips-2005-Tensor Subspace Analysis</a></p>
<p>18 0.44658872 <a title="105-lsi-18" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>19 0.44515204 <a title="105-lsi-19" href="./nips-2005-From_Batch_to_Transductive_Online_Learning.html">76 nips-2005-From Batch to Transductive Online Learning</a></p>
<p>20 0.4416644 <a title="105-lsi-20" href="./nips-2005-Walk-Sum_Interpretation_and_Analysis_of_Gaussian_Belief_Propagation.html">204 nips-2005-Walk-Sum Interpretation and Analysis of Gaussian Belief Propagation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.052), (10, 0.035), (11, 0.01), (18, 0.013), (27, 0.025), (31, 0.049), (34, 0.151), (39, 0.011), (41, 0.021), (50, 0.013), (55, 0.03), (65, 0.022), (69, 0.061), (73, 0.046), (88, 0.082), (91, 0.067), (98, 0.238)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82134283 <a title="105-lda-1" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>Author: Thomas Gärtner, Quoc V. Le, Simon Burton, Alex J. Smola, Vishy Vishwanathan</p><p>Abstract: We present a method for performing transductive inference on very large datasets. Our algorithm is based on multiclass Gaussian processes and is effective whenever the multiplication of the kernel matrix or its inverse with a vector can be computed sufﬁciently fast. This holds, for instance, for certain graph and string kernels. Transduction is achieved by variational inference over the unlabeled data subject to a balancing constraint. 1</p><p>2 0.64463753 <a title="105-lda-2" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>Author: Ben Taskar, Simon Lacoste-Julian, Michael I. Jordan</p><p>Abstract: We present a simple and scalable algorithm for large-margin estimation of structured models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem and apply the extragradient method, yielding an algorithm with linear convergence using simple gradient and projection calculations. The projection step can be solved using combinatorial algorithms for min-cost quadratic ﬂow. This makes the approach an efﬁcient alternative to formulations based on reductions to a quadratic program (QP). We present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm. 1</p><p>3 0.6410777 <a title="105-lda-3" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>Author: John D. Lafferty, Pradeep K. Ravikumar</p><p>Abstract: We present a family of approximation techniques for probabilistic graphical models, based on the use of graphical preconditioners developed in the scientiﬁc computing literature. Our framework yields rigorous upper and lower bounds on event probabilities and the log partition function of undirected graphical models, using non-iterative procedures that have low time complexity. As in mean ﬁeld approaches, the approximations are built upon tractable subgraphs; however, we recast the problem of optimizing the tractable distribution parameters and approximate inference in terms of the well-studied linear systems problem of obtaining a good matrix preconditioner. Experiments are presented that compare the new approximation schemes to variational methods. 1</p><p>4 0.64083856 <a title="105-lda-4" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>Author: Glenn Fung, Rómer Rosales, Balaji Krishnapuram</p><p>Abstract: We propose efﬁcient algorithms for learning ranking functions from order constraints between sets—i.e. classes—of training samples. Our algorithms may be used for maximizing the generalized Wilcoxon Mann Whitney statistic that accounts for the partial ordering of the classes: special cases include maximizing the area under the ROC curve for binary classiﬁcation and its generalization for ordinal regression. Experiments on public benchmarks indicate that: (a) the proposed algorithm is at least as accurate as the current state-of-the-art; (b) computationally, it is several orders of magnitude faster and—unlike current methods—it is easily able to handle even large datasets with over 20,000 samples. 1</p><p>5 0.63859224 <a title="105-lda-5" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>Author: Yoshua Bengio, Olivier Delalleau, Nicolas L. Roux</p><p>Abstract: We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior – with similarity between examples expressed with a local kernel – are sensitive to the curse of dimensionality, or more precisely to the variability of the target. Our discussion covers supervised, semisupervised and unsupervised learning algorithms. These algorithms are found to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set. This makes them sensitive to the curse of dimensionality, well studied for classical non-parametric statistical learning. We show in the case of the Gaussian kernel that when the function to be learned has many variations, these algorithms require a number of training examples proportional to the number of variations, which could be large even though there may exist short descriptions of the target function, i.e. their Kolmogorov complexity may be low. This suggests that there exist non-local learning algorithms that at least have the potential to learn about such structured but apparently complex functions (because locally they have many variations), while not using very speciﬁc prior domain knowledge. 1</p><p>6 0.63766217 <a title="105-lda-6" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>7 0.6350463 <a title="105-lda-7" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>8 0.62900883 <a title="105-lda-8" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>9 0.6282962 <a title="105-lda-9" href="./nips-2005-From_Lasso_regression_to_Feature_vector_machine.html">77 nips-2005-From Lasso regression to Feature vector machine</a></p>
<p>10 0.62823635 <a title="105-lda-10" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>11 0.62749755 <a title="105-lda-11" href="./nips-2005-Divergences%2C_surrogate_loss_functions_and_experimental_design.html">58 nips-2005-Divergences, surrogate loss functions and experimental design</a></p>
<p>12 0.62713784 <a title="105-lda-12" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>13 0.62674308 <a title="105-lda-13" href="./nips-2005-Infinite_latent_feature_models_and_the_Indian_buffet_process.html">98 nips-2005-Infinite latent feature models and the Indian buffet process</a></p>
<p>14 0.62525553 <a title="105-lda-14" href="./nips-2005-Diffusion_Maps%2C_Spectral_Clustering_and_Eigenfunctions_of_Fokker-Planck_Operators.html">56 nips-2005-Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators</a></p>
<p>15 0.6249305 <a title="105-lda-15" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>16 0.62481803 <a title="105-lda-16" href="./nips-2005-Augmented_Rescorla-Wagner_and_Maximum_Likelihood_Estimation.html">32 nips-2005-Augmented Rescorla-Wagner and Maximum Likelihood Estimation</a></p>
<p>17 0.6208393 <a title="105-lda-17" href="./nips-2005-Off-Road_Obstacle_Avoidance_through_End-to-End_Learning.html">143 nips-2005-Off-Road Obstacle Avoidance through End-to-End Learning</a></p>
<p>18 0.6205343 <a title="105-lda-18" href="./nips-2005-An_Application_of_Markov_Random_Fields_to_Range_Sensing.html">23 nips-2005-An Application of Markov Random Fields to Range Sensing</a></p>
<p>19 0.61927974 <a title="105-lda-19" href="./nips-2005-Soft_Clustering_on_Graphs.html">178 nips-2005-Soft Clustering on Graphs</a></p>
<p>20 0.61845326 <a title="105-lda-20" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
