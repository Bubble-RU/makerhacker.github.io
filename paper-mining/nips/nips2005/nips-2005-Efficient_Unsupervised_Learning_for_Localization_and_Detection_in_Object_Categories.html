<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>63 nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-63" href="#">nips2005-63</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>63 nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</h1>
<br/><p>Source: <a title="nips-2005-63-pdf" href="http://papers.nips.cc/paper/2796-efficient-unsupervised-learning-for-localization-and-detection-in-object-categories.pdf">pdf</a></p><p>Author: Nicolas Loeff, Himanshu Arora, Alexander Sorokin, David Forsyth</p><p>Abstract: We describe a novel method for learning templates for recognition and localization of objects drawn from categories. A generative model represents the conﬁguration of multiple object parts with respect to an object coordinate system; these parts in turn generate image features. The complexity of the model in the number of features is low, meaning our model is much more efﬁcient to train than comparative methods. Moreover, a variational approximation is introduced that allows learning to be orders of magnitude faster than previous approaches while incorporating many more features. This results in both accuracy and localization improvements. Our model has been carefully tested on standard datasets; we compare with a number of recent template models. In particular, we demonstrate state-of-the-art results for detection and localization. 1</p><p>Reference: <a title="nips-2005-63-reference" href="../nips2005_reference/nips-2005-Efficient_Unsupervised_Learning_for_Localization_and_Detection_in_Object_Categories_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We describe a novel method for learning templates for recognition and localization of objects drawn from categories. [sent-3, score-0.271]
</p><p>2 A generative model represents the conﬁguration of multiple object parts with respect to an object coordinate system; these parts in turn generate image features. [sent-4, score-1.143]
</p><p>3 The complexity of the model in the number of features is low, meaning our model is much more efﬁcient to train than comparative methods. [sent-5, score-0.25]
</p><p>4 1  Introduction  Building appropriate object models is central to object recognition, which is a fundamental problem in computer vision. [sent-10, score-0.742]
</p><p>5 We believe an appropriate representation of an object should allow for both detection of its presence and localization (‘where is it? [sent-12, score-0.67]
</p><p>6 So far the quality of object recognition in the literature has been measured by its detection performance only. [sent-14, score-0.557]
</p><p>7 Viola and Jones [1] present a fast object detection system boosting Haar ﬁlter responses. [sent-15, score-0.486]
</p><p>8 It is based on clustering image patches using appearance only, disregarding geometric information. [sent-17, score-0.278]
</p><p>9 However as no geometry cues are used during training, features that do not belong to the object can be incorporated into the object model. [sent-19, score-0.844]
</p><p>10 This is similar to classic overﬁtting and typically leads to problems in object localization. [sent-20, score-0.371]
</p><p>11 [4] represent an object as a constellation of parts. [sent-23, score-0.411]
</p><p>12 In [7], Leibe and Schiele use a voting scheme to predict object conﬁguration from locations of individual patches. [sent-32, score-0.371]
</p><p>13 Their model however has higher learning complexity and inferior detection performance despite being of discriminative nature. [sent-37, score-0.292]
</p><p>14 In this paper, we present a generative probabilistic model for detection and localization of objects that can be efﬁciently learnt with minimal supervision. [sent-38, score-0.461]
</p><p>15 The ﬁrst crucial property of the model is that it represents the conﬁguration of multiple object parts with respect to an unobserved, abstract object root (unlike [9, 10], where an “object root” is chosen as one of the visible parts of the object). [sent-39, score-1.084]
</p><p>16 This simpliﬁes localization and allows our model to overcome occlusion and errors in feature extraction. [sent-40, score-0.335]
</p><p>17 The second crucial assumption of the model is that a single part can generate multiple features in the image (or none). [sent-42, score-0.278]
</p><p>18 This hypothesis also makes an explicit model for part occlusion unnecessary: instead occlusion of a part means implicitly that no feature in the image is produced by it. [sent-44, score-0.401]
</p><p>19 These assumptions allow us to model all features in the image as being emitted independently conditioned on the object center. [sent-45, score-0.64]
</p><p>20 As a result the complexity of inference in our model is linear in the number of parts of the model and the number of features in the image, obviating the exponential complexity of combinatoric assignments in other approaches [4, 5, 11]. [sent-46, score-0.406]
</p><p>21 This means our model is much easier than constellation models to train using Expectation Maximization (EM), which enables the use of more features and more complex models with resulting improvements in both accuracy and localization. [sent-47, score-0.195]
</p><p>22 2  Model  Our model of an object category is a template that generates features in the image. [sent-49, score-0.573]
</p><p>23 Each image is represented as a set {fj } of F features extracted with the scale-saliency point detector [13]. [sent-50, score-0.253]
</p><p>24 As described in the introduction, we hypothesize that given the object center all features are generated independently: pobj (f1 , . [sent-53, score-0.519]
</p><p>25 The abstract object center - which does not generate any features - is represented by a hidden random variable oc . [sent-56, score-1.12]
</p><p>26 For simplicity it takes values in a discrete grid of size Nx × Ny inside the image and oc is assumed to be a priori uniformly distributed in its domain. [sent-57, score-0.688]
</p><p>27 Conditioned on the object center, each feature is generated by a mixture of P parts plus a background part. [sent-58, score-0.593]
</p><p>28 A set of hidden variables {ωij } represents which part (i) produced feature P +1 fj . [sent-59, score-0.252]
</p><p>29 In other words, ωij = 1 means feature j was produced by part i; each part can produce multiple features, each feature is produced by only one part. [sent-61, score-0.218]
</p><p>30 The distribution of a feature conditioned on the object center is then p(fj |oc ) = i p(fj , wij = 1|oc ) = i p(fj |wij = 1, oc )πi , where P +1 πi is the prior emission probability of part i. [sent-62, score-1.161]
</p><p>31 Each part has a location distribution with respect to the object center corresponding to a two dimensional full covariance Gaussian, pi (x|oc ). [sent-64, score-0.604]
</p><p>32 The appearance (see section 3 for details) L of a part does not depend on the conﬁguration of the object; we consider two models :  Gaussian Model (G) Appearance pi is modeled as a k dimensional diagonal covariance A Gaussian distribution. [sent-65, score-0.226]
</p><p>33 Local Topic Model (LT) Appearance pi is modeled as a multinomial distribution on a A previously learnt k-word image patch dictionary. [sent-66, score-0.199]
</p><p>34 The complete data likelihood (joint distribution) for image n in the object model is then,  [oc =o′ ] c   [ωij =1] obj i ′ i ′ Pθ ({ωij }, oc , {fj }) = pL (fj |oc )pA (fj )πi P (oc ) (1)   ′ j,i  oc  where [expr] is one if expr is true and zero otherwise. [sent-69, score-1.809]
</p><p>35 Marginalizing, the probability of the observed image in the object model is then, obj Pθ ({fj }) =  P (oc ) oc  P (fj ′ , ωij ′ = 1|oc ) j′  (2)  i  The background model assumes all features are produced independently, with uniform location on the image. [sent-70, score-1.516]
</p><p>36 In the G model of appearance, the appearance is modeled with a k dimensional full covariance matrix Gaussian distribution. [sent-71, score-0.215]
</p><p>37 In the LT model, we use a multinomial distribution on the k-word image patch dictionary to model the appearance. [sent-72, score-0.17]
</p><p>38 The background model is learnt before the object model is trained. [sent-82, score-0.599]
</p><p>39 As assumed earlier, for Gaussian appearance model the background appearance model is a single gaussian, whose mean and variance are estimated as the sample mean and covariance. [sent-83, score-0.498]
</p><p>40 The model for background feature location is uniform and does not have any parameters. [sent-85, score-0.251]
</p><p>41 2  Detection and localization  For detection of object presence, a natural decision rule is the likelihood ratio test. [sent-103, score-0.67]
</p><p>42 Once the presence of the object is established, the most likely location is given by the MAP estimate of oc . [sent-105, score-1.062]
</p><p>43 We assign parts in the model to the object if they exhibit consistent appearance and location. [sent-106, score-0.7]
</p><p>44 To remove model parts representing background we use a threshold on the entropy of the appearance distribution for the LT model (the determinant of the covariance in location for the G model). [sent-107, score-0.54]
</p><p>45 The MAP estimate of which features in the image are assigned (marginalizing over the object center) to parts in the model determines the support of the object. [sent-108, score-0.772]
</p><p>46 Bounding boxes include all keypoints assigned to the object and means of all model parts belonging to the object even if no keypoint is observed to be produced by such part. [sent-109, score-1.172]
</p><p>47 The F regions with highest saliency over the image provide the features for learning and recognition. [sent-115, score-0.213]
</p><p>48 For model G, due to the high dimensionality of resulting space, PCA is performed choosing k = 15 components to represent the appearance of a feature. [sent-118, score-0.215]
</p><p>49 For model LT, we instead cluster the appearance of features in the original SIFT space with a gaussian mixture model with k = 250 components and use the most likely cluster as feature appearance representation. [sent-119, score-0.572]
</p><p>50 We initialize appearance and location of the parts with P randomly chosen features from the training set. [sent-125, score-0.468]
</p><p>51 Figure 1: Local Topic model for faces, motorbikes and airplanes datasets [5]. [sent-127, score-0.251]
</p><p>52 In (a) the most likely location of the object center is plotted as a black circle. [sent-128, score-0.538]
</p><p>53 With respect to this reference, the spatial distribution (2D gaussian) of each part associated with the object is plotted in green. [sent-129, score-0.501]
</p><p>54 In (b) the centers of all features extracted are depicted. [sent-130, score-0.184]
</p><p>55 Image (c) shows how many features in the image are assigned to the same part (a property of our model, not shared by [5]): six parts are chosen, their spatial distribution is plotted (green), and the features assigned to them are depicted in blue. [sent-133, score-0.592]
</p><p>56 For each these parts, image (d) image shows the best matches in features extracted from the dataset. [sent-135, score-0.308]
</p><p>57 Note that the local topic model can learn parts uniform in appearance (i. [sent-136, score-0.404]
</p><p>58 The G appearance model and [5] do not have this property. [sent-141, score-0.215]
</p><p>59 4  Results  Detection: Although we believe that localization is an essential performance criterion, it is useless if the approach cannot detect objects. [sent-144, score-0.223]
</p><p>60 The results show higher detection performance of all our algorithms compared to the generative model presented in [5]. [sent-148, score-0.207]
</p><p>61 The local topic (LT) model performs better than the model presented in [8]. [sent-149, score-0.181]
</p><p>62 The purely discriminative approach presented in [3] shows higher detection performance with different (“optimal combination”) features, but performs worse for the features we are using. [sent-150, score-0.299]
</p><p>63 The LT model showed consistently higher detection performance than the Gaussian (G) model. [sent-151, score-0.207]
</p><p>64 The result is that most modern methods that infer the template form partially supervised data can tend to model some background parts as lying on the object (see ﬁgure 4). [sent-156, score-0.653]
</p><p>65 One symptom of this phenomenon (as in classical overﬁtting) is that methods that detect very well may be bad at localization, because they cannot separate the object from background. [sent-159, score-0.371]
</p><p>66 We are able to avoid this difﬁculty by predicting object extent conditioned on detection using only a subset of parts known to have relatively low variance in location or appearance, given the object center. [sent-160, score-1.12]
</p><p>67 Localization: Previous work on localization required aligned images (bounding boxes) or segmentation masks [7, 6]. [sent-164, score-0.209]
</p><p>68 A novel property of our model is that it learns to localize the object and determine its spatial extent without supervision. [sent-165, score-0.511]
</p><p>69 There is no standard measure to evaluate localization performance in an unsupervised setting. [sent-167, score-0.263]
</p><p>70 In such a case, the object center can be learnt at any position in the image, provided that this position is consistent across all images. [sent-168, score-0.471]
</p><p>71 We thus use as our performance measure, the standard deviation of estimated object centers and bounding boxes (obtained as in §2. [sent-169, score-0.655]
</p><p>72 2), after normalizing the estimates of each image to a coordinate system in which the ground truth bounding box is a unit square (0, 0) − (1, 1). [sent-170, score-0.246]
</p><p>73 All objects of interest in both airplane and motorbike datasets are centered in the image. [sent-172, score-0.176]
</p><p>74 As a result the baseline is a good predictor of the object center and is hard to beat. [sent-173, score-0.454]
</p><p>75 Figure 3 shows the scatterplot of normalized object centers and bounding boxes. [sent-175, score-0.565]
</p><p>76 The table in ﬁgure 2 shows the localization performance results using the proposed metric. [sent-176, score-0.223]
</p><p>77 On the other hand, for the LT model, the variational bound is loose during learning and localization performance is equivalent, but slightly lower than that of exact LT model. [sent-179, score-0.323]
</p><p>78 This may be explained by the fact that gaussian appearance model is less ﬂexible then the topic model and thus G model can better tolerate decoupling of location and appearance. [sent-180, score-0.524]
</p><p>79 01  DL  Figure 2: Plots on the left show detection performance on Caltech 5 datasets [5]. [sent-212, score-0.192]
</p><p>80 We show performance for our G model (G), LT model (L) and their variational approximations (GV) and (LV) respectively. [sent-217, score-0.245]
</p><p>81 On the right we show localization performance for all models on Faces dataset and performance of the best model (LT) on all datasets. [sent-219, score-0.315]
</p><p>82 Standard deviation is reported in percentage units with respect to the ground truth bounding box. [sent-220, score-0.192]
</p><p>83 For bounding boxes we average the standard deviation in each direction. [sent-221, score-0.195]
</p><p>84 Thus the image center baseline (b), (d) performs well there. [sent-224, score-0.17]
</p><p>85 (e) shows the bounding boxes computed by our approach (LT model). [sent-228, score-0.195]
</p><p>86 Object centers and bounding boxes are rectiﬁed using the ground truth bounding boxes (blue). [sent-229, score-0.493]
</p><p>87 No information about location or spatial extent of the object is given to the algorithm. [sent-230, score-0.523]
</p><p>88 Therefore, correlation between background and object in the dataset is incorporated into the object model. [sent-232, score-0.81]
</p><p>89 In this case the ellipses represent the features that are used by the algorithm in [3] to decide the presence of a face and motorbike (left images taken from [3]). [sent-233, score-0.177]
</p><p>90 Blue circles represent the features assigned by the model to the face, the red points are centers of features assigned to background (plot for Local Topic Model). [sent-235, score-0.465]
</p><p>91 5  Conclusions and future work  We have presented a novel model for object categories. [sent-236, score-0.424]
</p><p>92 Our model allows efﬁcient unsupervised learning, bringing the learning time to a few hours for full models and to minutes for variational approximations. [sent-237, score-0.193]
</p><p>93 The signiﬁcant reduction in complexity allows to handle many more parts and features than comparable algorithms. [sent-238, score-0.258]
</p><p>94 The detection performance of our approach compares favorably to the state of the art even when compared to purely discriminative approaches. [sent-239, score-0.197]
</p><p>95 Also our model is capable of learning the spatial extent of the objects without supervision, with good results. [sent-240, score-0.17]
</p><p>96 Among the most interesting applications we see unsupervised segmentation, learning, detection and localization of multiple object categories, deformable objects and objects with varying aspects. [sent-242, score-0.82]
</p><p>97 Rapid object detection using a boosted cascade of simple features. [sent-246, score-0.486]
</p><p>98 Combined object categorization and segmentation with an implicit shape model. [sent-288, score-0.371]
</p><p>99 A sparse object category model for efﬁcient learning and exhaustive recognition. [sent-304, score-0.424]
</p><p>100 Learning generative visual models from few training examples an incremental bayesian approach tested on 101 object categories. [sent-318, score-0.371]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('oc', 0.601), ('object', 0.371), ('lt', 0.298), ('localization', 0.184), ('appearance', 0.162), ('fj', 0.143), ('gv', 0.117), ('detection', 0.115), ('parts', 0.114), ('bounding', 0.106), ('features', 0.102), ('variational', 0.1), ('lv', 0.099), ('dlc', 0.096), ('location', 0.09), ('boxes', 0.089), ('image', 0.087), ('motorbikes', 0.083), ('airplanes', 0.077), ('fe', 0.076), ('topic', 0.075), ('dl', 0.071), ('background', 0.068), ('nx', 0.067), ('faces', 0.063), ('fergus', 0.061), ('ij', 0.059), ('occlusion', 0.058), ('caltech', 0.058), ('keypoints', 0.058), ('obj', 0.058), ('ny', 0.057), ('objects', 0.055), ('learnt', 0.054), ('model', 0.053), ('bl', 0.051), ('motorbike', 0.05), ('centers', 0.05), ('em', 0.047), ('template', 0.047), ('center', 0.046), ('assigned', 0.045), ('ji', 0.044), ('discriminative', 0.043), ('complexity', 0.042), ('unsupervised', 0.04), ('feature', 0.04), ('constellation', 0.04), ('wij', 0.04), ('performance', 0.039), ('decoupling', 0.038), ('dkl', 0.038), ('expr', 0.038), ('hillel', 0.038), ('horz', 0.038), ('keypoint', 0.038), ('leibe', 0.038), ('rear', 0.038), ('scatterplot', 0.038), ('weber', 0.038), ('eccv', 0.038), ('cvpr', 0.038), ('datasets', 0.038), ('baseline', 0.037), ('cars', 0.037), ('part', 0.036), ('hrs', 0.033), ('airplane', 0.033), ('mouth', 0.033), ('spotted', 0.033), ('minimized', 0.033), ('produced', 0.033), ('respect', 0.033), ('extracted', 0.032), ('pages', 0.032), ('recognition', 0.032), ('detector', 0.032), ('extent', 0.032), ('plotted', 0.031), ('orders', 0.031), ('cats', 0.03), ('spatial', 0.03), ('multinomial', 0.03), ('guration', 0.03), ('patches', 0.029), ('agarwal', 0.028), ('factorizes', 0.028), ('illinois', 0.028), ('pi', 0.028), ('visible', 0.028), ('truth', 0.028), ('conditioned', 0.027), ('ijcv', 0.025), ('localize', 0.025), ('sift', 0.025), ('vert', 0.025), ('ground', 0.025), ('images', 0.025), ('saliency', 0.024), ('perona', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="63-tfidf-1" href="./nips-2005-Efficient_Unsupervised_Learning_for_Localization_and_Detection_in_Object_Categories.html">63 nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</a></p>
<p>Author: Nicolas Loeff, Himanshu Arora, Alexander Sorokin, David Forsyth</p><p>Abstract: We describe a novel method for learning templates for recognition and localization of objects drawn from categories. A generative model represents the conﬁguration of multiple object parts with respect to an object coordinate system; these parts in turn generate image features. The complexity of the model in the number of features is low, meaning our model is much more efﬁcient to train than comparative methods. Moreover, a variational approximation is introduced that allows learning to be orders of magnitude faster than previous approaches while incorporating many more features. This results in both accuracy and localization improvements. Our model has been carefully tested on standard datasets; we compare with a number of recent template models. In particular, we demonstrate state-of-the-art results for detection and localization. 1</p><p>2 0.21024296 <a title="63-tfidf-2" href="./nips-2005-A_Computational_Model_of_Eye_Movements_during_Object_Class_Detection.html">5 nips-2005-A Computational Model of Eye Movements during Object Class Detection</a></p>
<p>Author: Wei Zhang, Hyejin Yang, Dimitris Samaras, Gregory J. Zelinsky</p><p>Abstract: We present a computational model of human eye movements in an object class detection task. The model combines state-of-the-art computer vision object class detection methods (SIFT features trained using AdaBoost) with a biologically plausible model of human eye movement to produce a sequence of simulated ﬁxations, culminating with the acquisition of a target. We validated the model by comparing its behavior to the behavior of human observers performing the identical object class detection task (looking for a teddy bear among visually complex nontarget objects). We found considerable agreement between the model and human data in multiple eye movement measures, including number of ﬁxations, cumulative probability of ﬁxating the target, and scanpath distance.</p><p>3 0.15548293 <a title="63-tfidf-3" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>Author: Cha Zhang, John C. Platt, Paul A. Viola</p><p>Abstract: A good image object detection algorithm is accurate, fast, and does not require exact locations of objects in a training set. We can create such an object detector by taking the architecture of the Viola-Jones detector cascade and training it with a new variant of boosting that we call MILBoost. MILBoost uses cost functions from the Multiple Instance Learning literature combined with the AnyBoost framework. We adapt the feature selection criterion of MILBoost to optimize the performance of the Viola-Jones cascade. Experiments show that the detection rate is up to 1.6 times better using MILBoost. This increased detection rate shows the advantage of simultaneously learning the locations and scales of the objects in the training set along with the parameters of the classiﬁer. 1</p><p>4 0.13104293 <a title="63-tfidf-4" href="./nips-2005-Describing_Visual_Scenes_using_Transformed_Dirichlet_Processes.html">55 nips-2005-Describing Visual Scenes using Transformed Dirichlet Processes</a></p>
<p>Author: Antonio Torralba, Alan S. Willsky, Erik B. Sudderth, William T. Freeman</p><p>Abstract: Motivated by the problem of learning to detect and recognize objects with minimal supervision, we develop a hierarchical probabilistic model for the spatial structure of visual scenes. In contrast with most existing models, our approach explicitly captures uncertainty in the number of object instances depicted in a given image. Our scene model is based on the transformed Dirichlet process (TDP), a novel extension of the hierarchical DP in which a set of stochastically transformed mixture components are shared between multiple groups of data. For visual scenes, mixture components describe the spatial structure of visual features in an object–centered coordinate frame, while transformations model the object positions in a particular image. Learning and inference in the TDP, which has many potential applications beyond computer vision, is based on an empirically effective Gibbs sampler. Applied to a dataset of partially labeled street scenes, we show that the TDP’s inclusion of spatial structure improves detection performance, ﬂexibly exploiting partially labeled training images. 1</p><p>5 0.12775563 <a title="63-tfidf-5" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>Author: Long Zhu, Alan L. Yuille</p><p>Abstract: We describe a hierarchical compositional system for detecting deformable objects in images. Objects are represented by graphical models. The algorithm uses a hierarchical tree where the root of the tree corresponds to the full object and lower-level elements of the tree correspond to simpler features. The algorithm proceeds by passing simple messages up and down the tree. The method works rapidly, in under a second, on 320 × 240 images. We demonstrate the approach on detecting cats, horses, and hands. The method works in the presence of background clutter and occlusions. Our approach is contrasted with more traditional methods such as dynamic programming and belief propagation. 1</p><p>6 0.11293953 <a title="63-tfidf-6" href="./nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">151 nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<p>7 0.1121887 <a title="63-tfidf-7" href="./nips-2005-Infinite_latent_feature_models_and_the_Indian_buffet_process.html">98 nips-2005-Infinite latent feature models and the Indian buffet process</a></p>
<p>8 0.08914154 <a title="63-tfidf-8" href="./nips-2005-Variational_EM_Algorithms_for_Non-Gaussian_Latent_Variable_Models.html">202 nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</a></p>
<p>9 0.084532745 <a title="63-tfidf-9" href="./nips-2005-The_Information-Form_Data_Association_Filter.html">192 nips-2005-The Information-Form Data Association Filter</a></p>
<p>10 0.079203047 <a title="63-tfidf-10" href="./nips-2005-Fusion_of_Similarity_Data_in_Clustering.html">79 nips-2005-Fusion of Similarity Data in Clustering</a></p>
<p>11 0.077847548 <a title="63-tfidf-11" href="./nips-2005-Learning_Depth_from_Single_Monocular_Images.html">110 nips-2005-Learning Depth from Single Monocular Images</a></p>
<p>12 0.075715221 <a title="63-tfidf-12" href="./nips-2005-Correlated_Topic_Models.html">52 nips-2005-Correlated Topic Models</a></p>
<p>13 0.072141513 <a title="63-tfidf-13" href="./nips-2005-Sequence_and_Tree_Kernels_with_Statistical_Feature_Mining.html">175 nips-2005-Sequence and Tree Kernels with Statistical Feature Mining</a></p>
<p>14 0.068046361 <a title="63-tfidf-14" href="./nips-2005-Identifying_Distributed_Object_Representations_in_Human_Extrastriate_Visual_Cortex.html">94 nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</a></p>
<p>15 0.066834353 <a title="63-tfidf-15" href="./nips-2005-Searching_for_Character_Models.html">171 nips-2005-Searching for Character Models</a></p>
<p>16 0.064832076 <a title="63-tfidf-16" href="./nips-2005-Variational_Bayesian_Stochastic_Complexity_of_Mixture_Models.html">201 nips-2005-Variational Bayesian Stochastic Complexity of Mixture Models</a></p>
<p>17 0.062909462 <a title="63-tfidf-17" href="./nips-2005-A_Cortically-Plausible_Inverse_Problem_Solving_Method_Applied_to_Recognizing_Static_and_Kinematic_3D_Objects.html">7 nips-2005-A Cortically-Plausible Inverse Problem Solving Method Applied to Recognizing Static and Kinematic 3D Objects</a></p>
<p>18 0.0617106 <a title="63-tfidf-18" href="./nips-2005-Correcting_sample_selection_bias_in_maximum_entropy_density_estimation.html">51 nips-2005-Correcting sample selection bias in maximum entropy density estimation</a></p>
<p>19 0.060209349 <a title="63-tfidf-19" href="./nips-2005-Two_view_learning%3A_SVM-2K%2C_Theory_and_Practice.html">196 nips-2005-Two view learning: SVM-2K, Theory and Practice</a></p>
<p>20 0.058408685 <a title="63-tfidf-20" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.191), (1, 0.021), (2, -0.023), (3, 0.237), (4, -0.038), (5, 0.011), (6, 0.03), (7, 0.202), (8, -0.098), (9, -0.176), (10, 0.009), (11, -0.031), (12, 0.016), (13, 0.076), (14, -0.036), (15, 0.081), (16, -0.132), (17, -0.156), (18, 0.034), (19, 0.025), (20, 0.059), (21, 0.13), (22, -0.025), (23, 0.023), (24, -0.008), (25, -0.144), (26, -0.03), (27, -0.053), (28, -0.01), (29, -0.024), (30, 0.011), (31, -0.063), (32, -0.137), (33, 0.093), (34, 0.044), (35, -0.003), (36, 0.085), (37, 0.044), (38, -0.058), (39, -0.023), (40, -0.112), (41, 0.007), (42, 0.102), (43, -0.041), (44, -0.062), (45, 0.039), (46, 0.155), (47, -0.02), (48, -0.064), (49, -0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96006912 <a title="63-lsi-1" href="./nips-2005-Efficient_Unsupervised_Learning_for_Localization_and_Detection_in_Object_Categories.html">63 nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</a></p>
<p>Author: Nicolas Loeff, Himanshu Arora, Alexander Sorokin, David Forsyth</p><p>Abstract: We describe a novel method for learning templates for recognition and localization of objects drawn from categories. A generative model represents the conﬁguration of multiple object parts with respect to an object coordinate system; these parts in turn generate image features. The complexity of the model in the number of features is low, meaning our model is much more efﬁcient to train than comparative methods. Moreover, a variational approximation is introduced that allows learning to be orders of magnitude faster than previous approaches while incorporating many more features. This results in both accuracy and localization improvements. Our model has been carefully tested on standard datasets; we compare with a number of recent template models. In particular, we demonstrate state-of-the-art results for detection and localization. 1</p><p>2 0.75797731 <a title="63-lsi-2" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>Author: Long Zhu, Alan L. Yuille</p><p>Abstract: We describe a hierarchical compositional system for detecting deformable objects in images. Objects are represented by graphical models. The algorithm uses a hierarchical tree where the root of the tree corresponds to the full object and lower-level elements of the tree correspond to simpler features. The algorithm proceeds by passing simple messages up and down the tree. The method works rapidly, in under a second, on 320 × 240 images. We demonstrate the approach on detecting cats, horses, and hands. The method works in the presence of background clutter and occlusions. Our approach is contrasted with more traditional methods such as dynamic programming and belief propagation. 1</p><p>3 0.69624144 <a title="63-lsi-3" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>Author: Cha Zhang, John C. Platt, Paul A. Viola</p><p>Abstract: A good image object detection algorithm is accurate, fast, and does not require exact locations of objects in a training set. We can create such an object detector by taking the architecture of the Viola-Jones detector cascade and training it with a new variant of boosting that we call MILBoost. MILBoost uses cost functions from the Multiple Instance Learning literature combined with the AnyBoost framework. We adapt the feature selection criterion of MILBoost to optimize the performance of the Viola-Jones cascade. Experiments show that the detection rate is up to 1.6 times better using MILBoost. This increased detection rate shows the advantage of simultaneously learning the locations and scales of the objects in the training set along with the parameters of the classiﬁer. 1</p><p>4 0.68471307 <a title="63-lsi-4" href="./nips-2005-Describing_Visual_Scenes_using_Transformed_Dirichlet_Processes.html">55 nips-2005-Describing Visual Scenes using Transformed Dirichlet Processes</a></p>
<p>Author: Antonio Torralba, Alan S. Willsky, Erik B. Sudderth, William T. Freeman</p><p>Abstract: Motivated by the problem of learning to detect and recognize objects with minimal supervision, we develop a hierarchical probabilistic model for the spatial structure of visual scenes. In contrast with most existing models, our approach explicitly captures uncertainty in the number of object instances depicted in a given image. Our scene model is based on the transformed Dirichlet process (TDP), a novel extension of the hierarchical DP in which a set of stochastically transformed mixture components are shared between multiple groups of data. For visual scenes, mixture components describe the spatial structure of visual features in an object–centered coordinate frame, while transformations model the object positions in a particular image. Learning and inference in the TDP, which has many potential applications beyond computer vision, is based on an empirically effective Gibbs sampler. Applied to a dataset of partially labeled street scenes, we show that the TDP’s inclusion of spatial structure improves detection performance, ﬂexibly exploiting partially labeled training images. 1</p><p>5 0.66307819 <a title="63-lsi-5" href="./nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">151 nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<p>Author: Francois Fleuret, Gilles Blanchard</p><p>Abstract: We investigate the learning of the appearance of an object from a single image of it. Instead of using a large number of pictures of the object to recognize, we use a labeled reference database of pictures of other objects to learn invariance to noise and variations in pose and illumination. This acquired knowledge is then used to predict if two pictures of new objects, which do not appear on the training pictures, actually display the same object. We propose a generic scheme called chopping to address this task. It relies on hundreds of random binary splits of the training set chosen to keep together the images of any given object. Those splits are extended to the complete image space with a simple learning algorithm. Given two images, the responses of the split predictors are combined with a Bayesian rule into a posterior probability of similarity. Experiments with the COIL-100 database and with a database of 150 deA graded LTEX symbols compare our method to a classical learning with several examples of the positive class and to a direct learning of the similarity. 1</p><p>6 0.62996697 <a title="63-lsi-6" href="./nips-2005-A_Computational_Model_of_Eye_Movements_during_Object_Class_Detection.html">5 nips-2005-A Computational Model of Eye Movements during Object Class Detection</a></p>
<p>7 0.53543597 <a title="63-lsi-7" href="./nips-2005-Infinite_latent_feature_models_and_the_Indian_buffet_process.html">98 nips-2005-Infinite latent feature models and the Indian buffet process</a></p>
<p>8 0.40932131 <a title="63-lsi-8" href="./nips-2005-Identifying_Distributed_Object_Representations_in_Human_Extrastriate_Visual_Cortex.html">94 nips-2005-Identifying Distributed Object Representations in Human Extrastriate Visual Cortex</a></p>
<p>9 0.39663613 <a title="63-lsi-9" href="./nips-2005-Fusion_of_Similarity_Data_in_Clustering.html">79 nips-2005-Fusion of Similarity Data in Clustering</a></p>
<p>10 0.35937229 <a title="63-lsi-10" href="./nips-2005-Searching_for_Character_Models.html">171 nips-2005-Searching for Character Models</a></p>
<p>11 0.34274453 <a title="63-lsi-11" href="./nips-2005-A_Cortically-Plausible_Inverse_Problem_Solving_Method_Applied_to_Recognizing_Static_and_Kinematic_3D_Objects.html">7 nips-2005-A Cortically-Plausible Inverse Problem Solving Method Applied to Recognizing Static and Kinematic 3D Objects</a></p>
<p>12 0.32238433 <a title="63-lsi-12" href="./nips-2005-Learning_Depth_from_Single_Monocular_Images.html">110 nips-2005-Learning Depth from Single Monocular Images</a></p>
<p>13 0.31769428 <a title="63-lsi-13" href="./nips-2005-Correlated_Topic_Models.html">52 nips-2005-Correlated Topic Models</a></p>
<p>14 0.31619179 <a title="63-lsi-14" href="./nips-2005-The_Information-Form_Data_Association_Filter.html">192 nips-2005-The Information-Form Data Association Filter</a></p>
<p>15 0.30424115 <a title="63-lsi-15" href="./nips-2005-The_Role_of_Top-down_and_Bottom-up_Processes_in_Guiding_Eye_Movements_during_Visual_Search.html">193 nips-2005-The Role of Top-down and Bottom-up Processes in Guiding Eye Movements during Visual Search</a></p>
<p>16 0.2917971 <a title="63-lsi-16" href="./nips-2005-Two_view_learning%3A_SVM-2K%2C_Theory_and_Practice.html">196 nips-2005-Two view learning: SVM-2K, Theory and Practice</a></p>
<p>17 0.28940189 <a title="63-lsi-17" href="./nips-2005-Correcting_sample_selection_bias_in_maximum_entropy_density_estimation.html">51 nips-2005-Correcting sample selection bias in maximum entropy density estimation</a></p>
<p>18 0.28505605 <a title="63-lsi-18" href="./nips-2005-Scaling_Laws_in_Natural_Scenes_and_the_Inference_of_3D_Shape.html">170 nips-2005-Scaling Laws in Natural Scenes and the Inference of 3D Shape</a></p>
<p>19 0.28192022 <a title="63-lsi-19" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<p>20 0.2805959 <a title="63-lsi-20" href="./nips-2005-Variational_EM_Algorithms_for_Non-Gaussian_Latent_Variable_Models.html">202 nips-2005-Variational EM Algorithms for Non-Gaussian Latent Variable Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.072), (10, 0.042), (27, 0.032), (31, 0.067), (34, 0.052), (39, 0.066), (41, 0.012), (55, 0.025), (57, 0.016), (67, 0.267), (69, 0.059), (73, 0.073), (88, 0.084), (91, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89843655 <a title="63-lda-1" href="./nips-2005-Separation_of_Music_Signals_by_Harmonic_Structure_Modeling.html">174 nips-2005-Separation of Music Signals by Harmonic Structure Modeling</a></p>
<p>Author: Yun-gang Zhang, Chang-shui Zhang</p><p>Abstract: Separation of music signals is an interesting but difﬁcult problem. It is helpful for many other music researches such as audio content analysis. In this paper, a new music signal separation method is proposed, which is based on harmonic structure modeling. The main idea of harmonic structure modeling is that the harmonic structure of a music signal is stable, so a music signal can be represented by a harmonic structure model. Accordingly, a corresponding separation algorithm is proposed. The main idea is to learn a harmonic structure model for each music signal in the mixture, and then separate signals by using these models to distinguish harmonic structures of different signals. Experimental results show that the algorithm can separate signals and obtain not only a very high Signalto-Noise Ratio (SNR) but also a rather good subjective audio quality. 1</p><p>same-paper 2 0.77254045 <a title="63-lda-2" href="./nips-2005-Efficient_Unsupervised_Learning_for_Localization_and_Detection_in_Object_Categories.html">63 nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</a></p>
<p>Author: Nicolas Loeff, Himanshu Arora, Alexander Sorokin, David Forsyth</p><p>Abstract: We describe a novel method for learning templates for recognition and localization of objects drawn from categories. A generative model represents the conﬁguration of multiple object parts with respect to an object coordinate system; these parts in turn generate image features. The complexity of the model in the number of features is low, meaning our model is much more efﬁcient to train than comparative methods. Moreover, a variational approximation is introduced that allows learning to be orders of magnitude faster than previous approaches while incorporating many more features. This results in both accuracy and localization improvements. Our model has been carefully tested on standard datasets; we compare with a number of recent template models. In particular, we demonstrate state-of-the-art results for detection and localization. 1</p><p>3 0.66313952 <a title="63-lda-3" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>Author: Jeremy Kubica, Joseph Masiero, Robert Jedicke, Andrew Connolly, Andrew W. Moore</p><p>Abstract: In this paper we consider the problem of ﬁnding sets of points that conform to a given underlying model from within a dense, noisy set of observations. This problem is motivated by the task of efﬁciently linking faint asteroid detections, but is applicable to a range of spatial queries. We survey current tree-based approaches, showing a trade-off exists between single tree and multiple tree algorithms. To this end, we present a new type of multiple tree algorithm that uses a variable number of trees to exploit the advantages of both approaches. We empirically show that this algorithm performs well using both simulated and astronomical data.</p><p>4 0.51974744 <a title="63-lda-4" href="./nips-2005-A_Criterion_for_the_Convergence_of_Learning_with_Spike_Timing_Dependent_Plasticity.html">8 nips-2005-A Criterion for the Convergence of Learning with Spike Timing Dependent Plasticity</a></p>
<p>Author: Robert A. Legenstein, Wolfgang Maass</p><p>Abstract: We investigate under what conditions a neuron can learn by experimentally supported rules for spike timing dependent plasticity (STDP) to predict the arrival times of strong “teacher inputs” to the same neuron. It turns out that in contrast to the famous Perceptron Convergence Theorem, which predicts convergence of the perceptron learning rule for a simpliﬁed neuron model whenever a stable solution exists, no equally strong convergence guarantee can be given for spiking neurons with STDP. But we derive a criterion on the statistical dependency structure of input spike trains which characterizes exactly when learning with STDP will converge on average for a simple model of a spiking neuron. This criterion is reminiscent of the linear separability criterion of the Perceptron Convergence Theorem, but it applies here to the rows of a correlation matrix related to the spike inputs. In addition we show through computer simulations for more realistic neuron models that the resulting analytically predicted positive learning results not only hold for the common interpretation of STDP where STDP changes the weights of synapses, but also for a more realistic interpretation suggested by experimental data where STDP modulates the initial release probability of dynamic synapses. 1</p><p>5 0.51763999 <a title="63-lda-5" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>Author: Amir Navot, Lavi Shpigelman, Naftali Tishby, Eilon Vaadia</p><p>Abstract: We present a non-linear, simple, yet effective, feature subset selection method for regression and use it in analyzing cortical neural activity. Our algorithm involves a feature-weighted version of the k-nearest-neighbor algorithm. It is able to capture complex dependency of the target function on its input and makes use of the leave-one-out error as a natural regularization. We explain the characteristics of our algorithm on synthetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey. By applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data.</p><p>6 0.51324934 <a title="63-lda-6" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>7 0.51040012 <a title="63-lda-7" href="./nips-2005-Kernelized_Infomax_Clustering.html">102 nips-2005-Kernelized Infomax Clustering</a></p>
<p>8 0.50991797 <a title="63-lda-8" href="./nips-2005-Describing_Visual_Scenes_using_Transformed_Dirichlet_Processes.html">55 nips-2005-Describing Visual Scenes using Transformed Dirichlet Processes</a></p>
<p>9 0.50891864 <a title="63-lda-9" href="./nips-2005-Using_%60%60epitomes%27%27_to_model_genetic_diversity%3A_Rational_design_of_HIV_vaccine_cocktails.html">198 nips-2005-Using ``epitomes'' to model genetic diversity: Rational design of HIV vaccine cocktails</a></p>
<p>10 0.50549048 <a title="63-lda-10" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>11 0.50493973 <a title="63-lda-11" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>12 0.5041517 <a title="63-lda-12" href="./nips-2005-The_Role_of_Top-down_and_Bottom-up_Processes_in_Guiding_Eye_Movements_during_Visual_Search.html">193 nips-2005-The Role of Top-down and Bottom-up Processes in Guiding Eye Movements during Visual Search</a></p>
<p>13 0.50366938 <a title="63-lda-13" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>14 0.503003 <a title="63-lda-14" href="./nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">151 nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<p>15 0.50052655 <a title="63-lda-15" href="./nips-2005-Conditional_Visual_Tracking_in_Kernel_Space.html">45 nips-2005-Conditional Visual Tracking in Kernel Space</a></p>
<p>16 0.50007457 <a title="63-lda-16" href="./nips-2005-Infinite_latent_feature_models_and_the_Indian_buffet_process.html">98 nips-2005-Infinite latent feature models and the Indian buffet process</a></p>
<p>17 0.49840584 <a title="63-lda-17" href="./nips-2005-Coarse_sample_complexity_bounds_for_active_learning.html">41 nips-2005-Coarse sample complexity bounds for active learning</a></p>
<p>18 0.49764925 <a title="63-lda-18" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>19 0.49697071 <a title="63-lda-19" href="./nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs.html">90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</a></p>
<p>20 0.49631715 <a title="63-lda-20" href="./nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
