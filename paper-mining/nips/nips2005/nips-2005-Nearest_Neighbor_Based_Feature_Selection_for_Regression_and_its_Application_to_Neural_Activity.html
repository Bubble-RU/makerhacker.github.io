<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-132" href="#">nips2005-132</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</h1>
<br/><p>Source: <a title="nips-2005-132-pdf" href="http://papers.nips.cc/paper/2848-nearest-neighbor-based-feature-selection-for-regression-and-its-application-to-neural-activity.pdf">pdf</a></p><p>Author: Amir Navot, Lavi Shpigelman, Naftali Tishby, Eilon Vaadia</p><p>Abstract: We present a non-linear, simple, yet effective, feature subset selection method for regression and use it in analyzing cortical neural activity. Our algorithm involves a feature-weighted version of the k-nearest-neighbor algorithm. It is able to capture complex dependency of the target function on its input and makes use of the leave-one-out error as a natural regularization. We explain the characteristics of our algorithm on synthetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey. By applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data.</p><p>Reference: <a title="nips-2005-132-reference" href="../nips2005_reference/nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 il  Abstract We present a non-linear, simple, yet effective, feature subset selection method for regression and use it in analyzing cortical neural activity. [sent-5, score-0.494]
</p><p>2 It is able to capture complex dependency of the target function on its input and makes use of the leave-one-out error as a natural regularization. [sent-7, score-0.18]
</p><p>3 We explain the characteristics of our algorithm on synthetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey. [sent-8, score-0.244]
</p><p>4 By applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data. [sent-9, score-0.382]
</p><p>5 Feature selection is the task of choosing a small subset of features that is sufﬁcient to predict the target labels well. [sent-11, score-0.46]
</p><p>6 Feature selection reduces the computational complexity of learning and prediction algorithms and saves on the cost of measuring non selected features. [sent-12, score-0.279]
</p><p>7 In many situations, feature selection can also enhance the prediction accuracy by improving the signal to noise ratio. [sent-13, score-0.36]
</p><p>8 Another beneﬁt of feature selection is that the identity of the selected features can provide insights into the nature of the problem at hand. [sent-14, score-0.555]
</p><p>9 Therefore feature selection is an important step in efﬁcient learning of large multi-featured data sets. [sent-15, score-0.36]
</p><p>10 Feature selection (variously known as subset selection, attribute selection or variable selection) has been studied extensively both in statistics and by the machine learning community over the last few decades. [sent-16, score-0.388]
</p><p>11 In the most common selection paradigm an evaluation function is used to assign scores to subsets of features and a search algorithm is used to search for a subset with a high score. [sent-17, score-0.693]
</p><p>12 The evaluation function can be based on the performance of a speciﬁc predictor (wrapper model, [1]) or on some general (typically cheaper to compute) relevance measure of the features to the prediction (ﬁlter model). [sent-18, score-0.284]
</p><p>13 In any case, an exhaustive search over all feature sets is generally intractable due to the exponentially large number of possible sets. [sent-19, score-0.237]
</p><p>14 Other methods simply rank individual features, assigning a score to each feature independently. [sent-21, score-0.186]
</p><p>15 These methods are usually very fast,  but inevitably fail in situations where only a combined set of features is predictive of the target function. [sent-22, score-0.27]
</p><p>16 See [2] for a comprehensive overview of feature selection and [3] which discusses selection methods for linear regression. [sent-23, score-0.516]
</p><p>17 They searched for good subsets using forward selection, backward elimination and an algorithm (called schemata) that races feature sets against each other (eliminating poor sets, keeping the ﬁttest) in order to ﬁnd a subset with a good score. [sent-27, score-0.291]
</p><p>18 All these algorithms perform a local search by ﬂipping one or more features at a time. [sent-28, score-0.257]
</p><p>19 In this paper we develop a novel selection algorithm. [sent-30, score-0.176]
</p><p>20 We extend the LOO-kNN-MSE evaluation function to assign scores to weight vectors over the features, instead of just to feature subsets. [sent-31, score-0.336]
</p><p>21 This results in a smooth (“almost everywhere”) function over a continuous domain, which allows us to compute the gradient analytically and to employ a stochastic gradient ascent to ﬁnd a locally optimal weight vector. [sent-32, score-0.234]
</p><p>22 In this way we can apply an easy-to-compute, gradient directed search, without relearning of a regression model at each step but while employing a strong non-linear function estimate (kNN) that can capture complex dependency of the function on its features1 . [sent-34, score-0.302]
</p><p>23 Our motivation for developing this method is to address a major computational neuroscience question: which features of the neural code are relevant to the observed behavior. [sent-35, score-0.239]
</p><p>24 Feature selection is a promising tool for this task. [sent-37, score-0.176]
</p><p>25 Here, we apply our feature selection method to the task of reconstructing hand movements from neural activity, which is one of the main challenges in implementing brain computer interfaces [8]. [sent-38, score-0.434]
</p><p>26 We look at neural population spike counts, recorded in motor cortex of a monkey while it performed hand movements and locate the most informative subset of neural features. [sent-39, score-0.216]
</p><p>27 Our algorithm, which considers feature subsets, outperforms methods that consider features on an individual basis, suggesting that complex dependency on a set of features exists in the code. [sent-41, score-0.662]
</p><p>28 Next, we demonstrate its ability to cope with a complicated dependency of the target function on groups of features using synthetic data (section 4). [sent-44, score-0.467]
</p><p>29 ([7]) which used a large margin based evaluation function to derive feature selection algorithms for classiﬁcation. [sent-57, score-0.443]
</p><p>30 This regression method is a special form of locally weighted regression (See [5] for an overview of the literature on this subject. [sent-67, score-0.226]
</p><p>31 As will be seen in the next section, we use the MSE of a modiﬁed kNN regressor to guide the search for a set of features F ⊂ {1, . [sent-72, score-0.234]
</p><p>32 2 x′ 2  3 The Feature Selection Algorithm In this section we present our selection algorithm called RGS (Regression, Gradient guided, feature Selection). [sent-77, score-0.34]
</p><p>33 Our goal is to ﬁnd subsets of features that induce a small estimation error. [sent-79, score-0.252]
</p><p>34 As in most supervised learning problems, we wish to ﬁnd subsets that induce a small generalization error, but since it is not known, we use an evaluation function on the training set. [sent-80, score-0.19]
</p><p>35 This evaluation function is deﬁned not only for subsets but for any weight vector over the features. [sent-81, score-0.191]
</p><p>36 This is more general because a feature subset can be represented by a binary weight vector that assigns a value of one to features in the set and zero to the rest of the features. [sent-82, score-0.457]
</p><p>37 For a given weights vector over the features w ∈ Rn , we consider the weighted squared ℓ2 2 2 2 norm induced by w, deﬁned as z w = i zi wi . [sent-83, score-0.283]
</p><p>38 Given a training set S, we denote by ˆ fw (x) the value assigned to x by a weighted kNN estimator, deﬁned in equation 1, using the weighted squared ℓ2 -norm as the distances d(x, x′ ) and the nearest neighbors are found among the points of S excluding x. [sent-84, score-0.363]
</p><p>39 The evaluation function is deﬁned as the negative (halved) square error of the weighted kNN estimator: 2 1 ˆ e(w) = − f (x) − fw (x) . [sent-85, score-0.215]
</p><p>40 Clearly, it is impossible to go over all the weight vectors (or even over all the feature subsets), and therefore some search technique is required. [sent-93, score-0.257]
</p><p>41 Our method ﬁnds a weight vector w that locally maximizes e(w) as deﬁned in (2) and then uses a threshold in order to obtain a feature subset. [sent-104, score-0.263]
</p><p>42 RGS considers the weights of all the features at the same time and thus it can handle dependency on a group of features. [sent-110, score-0.349]
</p><p>43 In this respect, it is superior to selection algorithms that scores each feature independently. [sent-112, score-0.412]
</p><p>44 The computational complexity of RGS is Θ(T N m) where T is the number of iterations, N is the number of features and m is the size of the training set S. [sent-120, score-0.226]
</p><p>45 This is correct for a naive implementation which ﬁnds the nearest neighbors and their distances from scratch at each step by measuring the distances between the current point to all the other points. [sent-121, score-0.202]
</p><p>46 (e),(f): demonstration of the effect of feature selection on estimating the second function using kNN regression (k = 5, β = 0. [sent-144, score-0.437]
</p><p>47 03), (f) using the relevant feature only (mse = 0. [sent-147, score-0.197]
</p><p>48 004)  selection methods: infoGain [10], correlation coefﬁcients (corrcoef ) and forward selection (see [2]) . [sent-148, score-0.373]
</p><p>49 infoGain and corrcoef simply rank features according to the mutual information2 or the correlation coefﬁcient (respectively) between each feature and the labels (i. [sent-149, score-0.545]
</p><p>50 Forward selection (fwdSel) is a greedy method in which features are iteratively added into a growing subset. [sent-152, score-0.36]
</p><p>51 In each step, the feature showing the greatest improvement (given the previously selected subset) is added. [sent-153, score-0.195]
</p><p>52 This is a search method that can be applied to any evaluation function and we use our criterion (equation 2 on feature subsets). [sent-154, score-0.294]
</p><p>53 This well known method has the advantages of considering feature subsets and that it can be used with non linear predictors. [sent-155, score-0.261]
</p><p>54 Another algorithm we compare with scores each feature independently using our evaluation function (2). [sent-156, score-0.293]
</p><p>55 Clearly, only the ﬁrst feature is relevant for the ﬁrst two target functions, and only the ﬁrst two features are relevant for the last two target functions. [sent-163, score-0.542]
</p><p>56 Note also that the last function is a smoothed version of parity function learning and is considered hard for many feature selection algorithms [2]. [sent-164, score-0.407]
</p><p>57 First, to illustrate the importance of feature selection on regression quality we use kNN to estimate the second target function. [sent-165, score-0.479]
</p><p>58 Figure 1(e-f) shows the regression results for target (b), using either only the relevant feature or both the relevant and an irrelevant feature. [sent-166, score-0.369]
</p><p>59 We measure their success by counting the number of times that the relevant features were assigned the highest rank (repeating the experiment 250 times by re-sampling the training set). [sent-169, score-0.334]
</p><p>60 We can see that all the algorithms succeeded on the ﬁrst function which is monotonic and depends on one feature alone. [sent-171, score-0.209]
</p><p>61 infoGain and corrcoef fail on the second, non-monotonic function. [sent-172, score-0.176]
</p><p>62 The third target function depends on two features but the dependency is simple as each of them alone is highly correlated with the function value. [sent-175, score-0.386]
</p><p>63 The fourth, XOR-like function exhibits a complicated dependency that requires consideration of the two relevant features simultaneously. [sent-176, score-0.359]
</p><p>64 SKS which considers features separately sees the effect of all other features as noise and, therefore, has only marginal success on the third 2  Feature and function values were “binarized” by comparing them to the median value. [sent-177, score-0.479]
</p><p>65 fwdSel considers subsets but can evaluate only one additional feature in each step, giving it some advantage over RGS on the third function but causing it to fail on the fourth. [sent-182, score-0.312]
</p><p>66 16 electrodes, inserted daily into novel positions in primary motor cortex were used to detect and sort spikes in up to 64 channels (4 per electrode). [sent-187, score-0.219]
</p><p>67 Most of the channels detected isolated neuronal spikes by template matching. [sent-188, score-0.183]
</p><p>68 The rest of the channels (one per electrode) produced spikes by threshold passing. [sent-191, score-0.185]
</p><p>69 In order to evaluate the different feature selection methods we separate the data into training and test sets. [sent-197, score-0.382]
</p><p>70 Each selection method is used to produce a ranking of the features. [sent-198, score-0.231]
</p><p>71 We then apply kNN (based on the training set) using different size groups of top ranking features to the test set. [sent-199, score-0.327]
</p><p>72 Figure 3 shows the average (over permutations, folds and velocity components) MSE as a function of the number of selected features on four of the different data sets (results on the rest are similar and omitted due to lack of space)4 . [sent-202, score-0.36]
</p><p>73 It is clear that RGS achieves better results than the other methods throughout the range of feature numbers. [sent-203, score-0.196]
</p><p>74 These parameters were manually tuned for good kNN results and were not optimized for any of the feature selection algorithms. [sent-208, score-0.365]
</p><p>75 27 7  200  400  600  7  200  400  600  Figure 3: MSE results for the different feature selection methods on the neural activity data sets. [sent-220, score-0.408]
</p><p>76 MSEs are presented as a function of the number of features used. [sent-222, score-0.206]
</p><p>77 Figure 4 shows the winning percentages of RGS versus the other methods. [sent-226, score-0.228]
</p><p>78 For a very low number of features, while the error is still high, RGS winning scores are only slightly better than chance but once there are enough features for good predictions the winning percentages are higher than 90%. [sent-227, score-0.589]
</p><p>79 In ﬁgure 3 we see that the MSE achieved when using only approximately 100 features selected by RGS is better than when using all the features. [sent-228, score-0.215]
</p><p>80 RGS not only ranks the features but also gives them weights that achieve locally optimal results when using kNN regression. [sent-231, score-0.252]
</p><p>81 It therefore makes sense not only to select the features but to weigh them accordingly. [sent-232, score-0.184]
</p><p>82 Figure 5 shows the winning percentages of RGS using the weighted features versus RGS using uniformly weighted features. [sent-233, score-0.498]
</p><p>83 It is clear that using the weights improves the results in a manner that becomes increasingly signiﬁcant as the number of features grows, especially when the number of features is greater than the optimal number. [sent-235, score-0.403]
</p><p>84 Thus, using weighted features can compensate for choosing too many by diminishing the effect of the surplus features. [sent-236, score-0.25]
</p><p>85 To take a closer look at what features are selected, ﬁgure 6 shows the 100 highest ranking features for all algorithms on one data set. [sent-237, score-0.446]
</p><p>86 Similar selection results were obtained in the rest of the folds. [sent-238, score-0.206]
</p><p>87 Indeed, all the algorithms select isolated cells more frequently within the top 100 features (RGS does so in 95% of the time and the rest in 70%-80%). [sent-240, score-0.265]
</p><p>88 A human selection of channels, based only on looking at raster plots and selecting channels with stable ﬁring rates was also available to us. [sent-241, score-0.259]
</p><p>89 Since RGS achieves best results, we  90  RGS vs SKS RGS vs infoGain RGS vs corrcoef  70  80  winning percentages uniform weights non−uniform weights  70  60  60  50 0  0. [sent-245, score-0.568]
</p><p>90 8  MSE  80  winning percentage  100  90 winning percentage  100  100  200 300 400 number of features  500  600  Figure 4: Winning percentages of RGS over the other algorithms. [sent-246, score-0.54]
</p><p>91 50 0  100  200 300 400 number of features  500  600  0. [sent-248, score-0.184]
</p><p>92 6  Figure 5: Winning percentages of RGS with and without weighting of features (black). [sent-249, score-0.284]
</p><p>93 RGS  SKS  corrCoef  infoGain  Figure 6: 100 highest ranking features (grayed out) selected by the algorithms. [sent-251, score-0.27]
</p><p>94 Apparently RGS found these patterns thanks to its ability to evaluate complex dependency on feature subsets. [sent-256, score-0.258]
</p><p>95 6 Summary In this paper we present a new method of selecting features for function estimation and use it to analyze neural activity during a motor control task . [sent-258, score-0.357]
</p><p>96 This yields a selection method which can handle a complicated dependency of the target function on groups of features yet can be applied to large scale problems. [sent-260, score-0.589]
</p><p>97 This is valuable since many common selection methods lack one of these properties. [sent-261, score-0.176]
</p><p>98 By comparing the result of our method to other selection methods on the motor control task, we show that consideration of complex dependency helps to achieve better performance. [sent-262, score-0.353]
</p><p>99 Our future work is aimed at a better understanding of neural activity through the use of feature selection. [sent-264, score-0.232]
</p><p>100 One possibility is to perform feature selection on other kinds of neural data such as local ﬁeld potentials or retinal activity. [sent-265, score-0.362]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rgs', 0.711), ('knn', 0.256), ('mse', 0.188), ('features', 0.184), ('selection', 0.176), ('feature', 0.164), ('corrcoef', 0.154), ('infogain', 0.154), ('sks', 0.135), ('winning', 0.128), ('percentages', 0.1), ('fwdsel', 0.096), ('dependency', 0.094), ('fw', 0.092), ('channels', 0.083), ('mses', 0.077), ('regression', 0.075), ('subsets', 0.068), ('target', 0.064), ('motor', 0.061), ('evaluation', 0.058), ('ranking', 0.055), ('synthetic', 0.054), ('nearest', 0.053), ('success', 0.053), ('neighbors', 0.051), ('wrapper', 0.05), ('search', 0.05), ('spikes', 0.049), ('scores', 0.049), ('activity', 0.046), ('gradient', 0.046), ('cross', 0.044), ('ascent', 0.044), ('weight', 0.043), ('weighted', 0.043), ('training', 0.042), ('estimator', 0.042), ('validation', 0.041), ('folds', 0.04), ('distances', 0.039), ('permutations', 0.038), ('considers', 0.036), ('subset', 0.036), ('weights', 0.035), ('sin', 0.034), ('epochs', 0.033), ('relevant', 0.033), ('locally', 0.033), ('channel', 0.033), ('rn', 0.032), ('gure', 0.032), ('movement', 0.032), ('achieves', 0.032), ('selected', 0.031), ('velocity', 0.03), ('rest', 0.03), ('non', 0.029), ('lags', 0.028), ('vs', 0.028), ('replaced', 0.028), ('isolated', 0.028), ('succeed', 0.027), ('sub', 0.027), ('cortex', 0.026), ('fourth', 0.026), ('decay', 0.026), ('complicated', 0.026), ('templates', 0.026), ('electrode', 0.026), ('tuned', 0.025), ('movements', 0.025), ('trial', 0.025), ('hand', 0.024), ('repetitions', 0.024), ('decays', 0.024), ('everywhere', 0.024), ('neighbor', 0.024), ('reconstruction', 0.024), ('sets', 0.023), ('optima', 0.023), ('template', 0.023), ('batch', 0.023), ('apply', 0.023), ('algorithms', 0.023), ('threshold', 0.023), ('groups', 0.023), ('compensate', 0.023), ('fail', 0.022), ('control', 0.022), ('neural', 0.022), ('function', 0.022), ('rank', 0.022), ('correlation', 0.021), ('cortical', 0.021), ('induced', 0.021), ('counts', 0.02), ('prediction', 0.02), ('step', 0.02), ('coef', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="132-tfidf-1" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>Author: Amir Navot, Lavi Shpigelman, Naftali Tishby, Eilon Vaadia</p><p>Abstract: We present a non-linear, simple, yet effective, feature subset selection method for regression and use it in analyzing cortical neural activity. Our algorithm involves a feature-weighted version of the k-nearest-neighbor algorithm. It is able to capture complex dependency of the target function on its input and makes use of the leave-one-out error as a natural regularization. We explain the characteristics of our algorithm on synthetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey. By applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data.</p><p>2 0.17652509 <a title="132-tfidf-2" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>Author: Kilian Q. Weinberger, John Blitzer, Lawrence K. Saul</p><p>Abstract: We show how to learn a Mahanalobis distance metric for k-nearest neighbor (kNN) classiﬁcation by semideﬁnite programming. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. On seven data sets of varying size and difﬁculty, we ﬁnd that metrics trained in this way lead to signiﬁcant improvements in kNN classiﬁcation—for example, achieving a test error rate of 1.3% on the MNIST handwritten digits. As in support vector machines (SVMs), the learning problem reduces to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our framework requires no modiﬁcation or extension for problems in multiway (as opposed to binary) classiﬁcation. 1</p><p>3 0.11574968 <a title="132-tfidf-3" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>Author: Eizaburo Doi, Doru C. Balcan, Michael S. Lewicki</p><p>Abstract: Biological sensory systems are faced with the problem of encoding a high-ﬁdelity sensory signal with a population of noisy, low-ﬁdelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and twodimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of highdimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets. 1</p><p>4 0.090201244 <a title="132-tfidf-4" href="./nips-2005-From_Lasso_regression_to_Feature_vector_machine.html">77 nips-2005-From Lasso regression to Feature vector machine</a></p>
<p>Author: Fan Li, Yiming Yang, Eric P. Xing</p><p>Abstract: Lasso regression tends to assign zero weights to most irrelevant or redundant features, and hence is a promising technique for feature selection. Its limitation, however, is that it only offers solutions to linear models. Kernel machines with feature scaling techniques have been studied for feature selection with non-linear models. However, such approaches require to solve hard non-convex optimization problems. This paper proposes a new approach named the Feature Vector Machine (FVM). It reformulates the standard Lasso regression into a form isomorphic to SVM, and this form can be easily extended for feature selection with non-linear models by introducing kernels deﬁned on feature vectors. FVM generates sparse solutions in the nonlinear feature space and it is much more tractable compared to feature scaling kernel machines. Our experiments with FVM on simulated data show encouraging results in identifying the small number of dominating features that are non-linearly correlated to the response, a task the standard Lasso fails to complete.</p><p>5 0.073191196 <a title="132-tfidf-5" href="./nips-2005-Laplacian_Score_for_Feature_Selection.html">104 nips-2005-Laplacian Score for Feature Selection</a></p>
<p>Author: Xiaofei He, Deng Cai, Partha Niyogi</p><p>Abstract: In supervised learning scenarios, feature selection has been studied widely in the literature. Selecting features in unsupervised learning scenarios is a much harder problem, due to the absence of class labels that would guide the search for relevant information. And, almost all of previous unsupervised feature selection methods are “wrapper” techniques that require a learning algorithm to evaluate the candidate feature subsets. In this paper, we propose a “ﬁlter” method for feature selection which is independent of any learning algorithm. Our method can be performed in either supervised or unsupervised fashion. The proposed method is based on the observation that, in many real world classiﬁcation problems, data from the same class are often close to each other. The importance of a feature is evaluated by its power of locality preserving, or, Laplacian Score. We compare our method with data variance (unsupervised) and Fisher score (supervised) on two data sets. Experimental results demonstrate the effectiveness and efﬁciency of our algorithm. 1</p><p>6 0.072739623 <a title="132-tfidf-6" href="./nips-2005-Inferring_Motor_Programs_from_Images_of_Handwritten_Digits.html">97 nips-2005-Inferring Motor Programs from Images of Handwritten Digits</a></p>
<p>7 0.071606353 <a title="132-tfidf-7" href="./nips-2005-A_Computational_Model_of_Eye_Movements_during_Object_Class_Detection.html">5 nips-2005-A Computational Model of Eye Movements during Object Class Detection</a></p>
<p>8 0.069801003 <a title="132-tfidf-8" href="./nips-2005-Noise_and_the_two-thirds_power_Law.html">136 nips-2005-Noise and the two-thirds power Law</a></p>
<p>9 0.068507247 <a title="132-tfidf-9" href="./nips-2005-Infinite_latent_feature_models_and_the_Indian_buffet_process.html">98 nips-2005-Infinite latent feature models and the Indian buffet process</a></p>
<p>10 0.066201873 <a title="132-tfidf-10" href="./nips-2005-Optimal_cue_selection_strategy.html">149 nips-2005-Optimal cue selection strategy</a></p>
<p>11 0.063173622 <a title="132-tfidf-11" href="./nips-2005-Top-Down_Control_of_Visual_Attention%3A_A_Rational_Account.html">194 nips-2005-Top-Down Control of Visual Attention: A Rational Account</a></p>
<p>12 0.06233409 <a title="132-tfidf-12" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>13 0.062032707 <a title="132-tfidf-13" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>14 0.062015865 <a title="132-tfidf-14" href="./nips-2005-Sparse_Gaussian_Processes_using_Pseudo-inputs.html">179 nips-2005-Sparse Gaussian Processes using Pseudo-inputs</a></p>
<p>15 0.060617097 <a title="132-tfidf-15" href="./nips-2005-Large-scale_biophysical_parameter_estimation_in_single_neurons_via_constrained_linear_regression.html">106 nips-2005-Large-scale biophysical parameter estimation in single neurons via constrained linear regression</a></p>
<p>16 0.06029699 <a title="132-tfidf-16" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>17 0.059712198 <a title="132-tfidf-17" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<p>18 0.056523543 <a title="132-tfidf-18" href="./nips-2005-Sequence_and_Tree_Kernels_with_Statistical_Feature_Mining.html">175 nips-2005-Sequence and Tree Kernels with Statistical Feature Mining</a></p>
<p>19 0.05464866 <a title="132-tfidf-19" href="./nips-2005-Learning_Depth_from_Single_Monocular_Images.html">110 nips-2005-Learning Depth from Single Monocular Images</a></p>
<p>20 0.054075085 <a title="132-tfidf-20" href="./nips-2005-A_matching_pursuit_approach_to_sparse_Gaussian_process_regression.html">16 nips-2005-A matching pursuit approach to sparse Gaussian process regression</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.194), (1, -0.022), (2, -0.03), (3, 0.05), (4, 0.011), (5, 0.093), (6, 0.089), (7, -0.018), (8, 0.044), (9, 0.033), (10, 0.034), (11, -0.072), (12, -0.003), (13, -0.006), (14, 0.11), (15, 0.038), (16, -0.121), (17, 0.022), (18, -0.166), (19, -0.175), (20, 0.032), (21, -0.024), (22, -0.048), (23, 0.052), (24, 0.012), (25, -0.131), (26, 0.024), (27, 0.031), (28, 0.108), (29, -0.03), (30, -0.053), (31, -0.0), (32, -0.033), (33, 0.1), (34, -0.044), (35, -0.146), (36, 0.078), (37, 0.067), (38, -0.014), (39, 0.073), (40, -0.041), (41, 0.206), (42, -0.015), (43, 0.042), (44, 0.089), (45, 0.01), (46, -0.103), (47, 0.139), (48, 0.063), (49, 0.109)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93215168 <a title="132-lsi-1" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>Author: Amir Navot, Lavi Shpigelman, Naftali Tishby, Eilon Vaadia</p><p>Abstract: We present a non-linear, simple, yet effective, feature subset selection method for regression and use it in analyzing cortical neural activity. Our algorithm involves a feature-weighted version of the k-nearest-neighbor algorithm. It is able to capture complex dependency of the target function on its input and makes use of the leave-one-out error as a natural regularization. We explain the characteristics of our algorithm on synthetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey. By applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data.</p><p>2 0.57453781 <a title="132-lsi-2" href="./nips-2005-From_Lasso_regression_to_Feature_vector_machine.html">77 nips-2005-From Lasso regression to Feature vector machine</a></p>
<p>Author: Fan Li, Yiming Yang, Eric P. Xing</p><p>Abstract: Lasso regression tends to assign zero weights to most irrelevant or redundant features, and hence is a promising technique for feature selection. Its limitation, however, is that it only offers solutions to linear models. Kernel machines with feature scaling techniques have been studied for feature selection with non-linear models. However, such approaches require to solve hard non-convex optimization problems. This paper proposes a new approach named the Feature Vector Machine (FVM). It reformulates the standard Lasso regression into a form isomorphic to SVM, and this form can be easily extended for feature selection with non-linear models by introducing kernels deﬁned on feature vectors. FVM generates sparse solutions in the nonlinear feature space and it is much more tractable compared to feature scaling kernel machines. Our experiments with FVM on simulated data show encouraging results in identifying the small number of dominating features that are non-linearly correlated to the response, a task the standard Lasso fails to complete.</p><p>3 0.48391694 <a title="132-lsi-3" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>Author: Kilian Q. Weinberger, John Blitzer, Lawrence K. Saul</p><p>Abstract: We show how to learn a Mahanalobis distance metric for k-nearest neighbor (kNN) classiﬁcation by semideﬁnite programming. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. On seven data sets of varying size and difﬁculty, we ﬁnd that metrics trained in this way lead to signiﬁcant improvements in kNN classiﬁcation—for example, achieving a test error rate of 1.3% on the MNIST handwritten digits. As in support vector machines (SVMs), the learning problem reduces to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our framework requires no modiﬁcation or extension for problems in multiway (as opposed to binary) classiﬁcation. 1</p><p>4 0.44553667 <a title="132-lsi-4" href="./nips-2005-Selecting_Landmark_Points_for_Sparse_Manifold_Learning.html">172 nips-2005-Selecting Landmark Points for Sparse Manifold Learning</a></p>
<p>Author: Jorge Silva, Jorge Marques, João Lemos</p><p>Abstract: There has been a surge of interest in learning non-linear manifold models to approximate high-dimensional data. Both for computational complexity reasons and for generalization capability, sparsity is a desired feature in such models. This usually means dimensionality reduction, which naturally implies estimating the intrinsic dimension, but it can also mean selecting a subset of the data to use as landmarks, which is especially important because many existing algorithms have quadratic complexity in the number of observations. This paper presents an algorithm for selecting landmarks, based on LASSO regression, which is well known to favor sparse approximations because it uses regularization with an l1 norm. As an added beneﬁt, a continuous manifold parameterization, based on the landmarks, is also found. Experimental results with synthetic and real data illustrate the algorithm. 1</p><p>5 0.44470498 <a title="132-lsi-5" href="./nips-2005-A_Theoretical_Analysis_of_Robust_Coding_over_Noisy_Overcomplete_Channels.html">15 nips-2005-A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels</a></p>
<p>Author: Eizaburo Doi, Doru C. Balcan, Michael S. Lewicki</p><p>Abstract: Biological sensory systems are faced with the problem of encoding a high-ﬁdelity sensory signal with a population of noisy, low-ﬁdelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and twodimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of highdimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets. 1</p><p>6 0.43036851 <a title="132-lsi-6" href="./nips-2005-Laplacian_Score_for_Feature_Selection.html">104 nips-2005-Laplacian Score for Feature Selection</a></p>
<p>7 0.42717853 <a title="132-lsi-7" href="./nips-2005-Inferring_Motor_Programs_from_Images_of_Handwritten_Digits.html">97 nips-2005-Inferring Motor Programs from Images of Handwritten Digits</a></p>
<p>8 0.40148473 <a title="132-lsi-8" href="./nips-2005-Correcting_sample_selection_bias_in_maximum_entropy_density_estimation.html">51 nips-2005-Correcting sample selection bias in maximum entropy density estimation</a></p>
<p>9 0.39093375 <a title="132-lsi-9" href="./nips-2005-Generalization_in_Clustering_with_Unobserved_Features.html">84 nips-2005-Generalization in Clustering with Unobserved Features</a></p>
<p>10 0.38468319 <a title="132-lsi-10" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>11 0.37448037 <a title="132-lsi-11" href="./nips-2005-Metric_Learning_by_Collapsing_Classes.html">126 nips-2005-Metric Learning by Collapsing Classes</a></p>
<p>12 0.36624461 <a title="132-lsi-12" href="./nips-2005-Top-Down_Control_of_Visual_Attention%3A_A_Rational_Account.html">194 nips-2005-Top-Down Control of Visual Attention: A Rational Account</a></p>
<p>13 0.3618339 <a title="132-lsi-13" href="./nips-2005-Spectral_Bounds_for_Sparse_PCA%3A_Exact_and_Greedy_Algorithms.html">180 nips-2005-Spectral Bounds for Sparse PCA: Exact and Greedy Algorithms</a></p>
<p>14 0.35949087 <a title="132-lsi-14" href="./nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">151 nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<p>15 0.35937417 <a title="132-lsi-15" href="./nips-2005-Optimal_cue_selection_strategy.html">149 nips-2005-Optimal cue selection strategy</a></p>
<p>16 0.35439125 <a title="132-lsi-16" href="./nips-2005-Gaussian_Processes_for_Multiuser_Detection_in_CDMA_receivers.html">81 nips-2005-Gaussian Processes for Multiuser Detection in CDMA receivers</a></p>
<p>17 0.34475252 <a title="132-lsi-17" href="./nips-2005-A_matching_pursuit_approach_to_sparse_Gaussian_process_regression.html">16 nips-2005-A matching pursuit approach to sparse Gaussian process regression</a></p>
<p>18 0.34171647 <a title="132-lsi-18" href="./nips-2005-Predicting_EMG_Data_from_M1_Neurons_with_Variational_Bayesian_Least_Squares.html">155 nips-2005-Predicting EMG Data from M1 Neurons with Variational Bayesian Least Squares</a></p>
<p>19 0.33442423 <a title="132-lsi-19" href="./nips-2005-Large-scale_biophysical_parameter_estimation_in_single_neurons_via_constrained_linear_regression.html">106 nips-2005-Large-scale biophysical parameter estimation in single neurons via constrained linear regression</a></p>
<p>20 0.33230448 <a title="132-lsi-20" href="./nips-2005-Infinite_latent_feature_models_and_the_Indian_buffet_process.html">98 nips-2005-Infinite latent feature models and the Indian buffet process</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.068), (10, 0.037), (11, 0.018), (27, 0.037), (31, 0.048), (34, 0.085), (36, 0.192), (39, 0.023), (41, 0.014), (55, 0.045), (57, 0.031), (60, 0.012), (65, 0.01), (69, 0.07), (73, 0.074), (88, 0.104), (91, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95697993 <a title="132-lda-1" href="./nips-2005-How_fast_to_work%3A_Response_vigor%2C_motivation_and_tonic_dopamine.html">91 nips-2005-How fast to work: Response vigor, motivation and tonic dopamine</a></p>
<p>Author: Yael Niv, Nathaniel D. Daw, Peter Dayan</p><p>Abstract: Reinforcement learning models have long promised to unify computational, psychological and neural accounts of appetitively conditioned behavior. However, the bulk of data on animal conditioning comes from free-operant experiments measuring how fast animals will work for reinforcement. Existing reinforcement learning (RL) models are silent about these tasks, because they lack any notion of vigor. They thus fail to address the simple observation that hungrier animals will work harder for food, as well as stranger facts such as their sometimes greater productivity even when working for irrelevant outcomes such as water. Here, we develop an RL framework for free-operant behavior, suggesting that subjects choose how vigorously to perform selected actions by optimally balancing the costs and beneﬁts of quick responding. Motivational states such as hunger shift these factors, skewing the tradeoff. This accounts normatively for the effects of motivation on response rates, as well as many other classic ﬁndings. Finally, we suggest that tonic levels of dopamine may be involved in the computation linking motivational state to optimal responding, thereby explaining the complex vigor-related effects of pharmacological manipulation of dopamine. 1</p><p>same-paper 2 0.82813919 <a title="132-lda-2" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>Author: Amir Navot, Lavi Shpigelman, Naftali Tishby, Eilon Vaadia</p><p>Abstract: We present a non-linear, simple, yet effective, feature subset selection method for regression and use it in analyzing cortical neural activity. Our algorithm involves a feature-weighted version of the k-nearest-neighbor algorithm. It is able to capture complex dependency of the target function on its input and makes use of the leave-one-out error as a natural regularization. We explain the characteristics of our algorithm on synthetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey. By applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data.</p><p>3 0.68681043 <a title="132-lda-3" href="./nips-2005-Kernelized_Infomax_Clustering.html">102 nips-2005-Kernelized Infomax Clustering</a></p>
<p>Author: David Barber, Felix V. Agakov</p><p>Abstract: We propose a simple information-theoretic approach to soft clustering based on maximizing the mutual information I(x, y) between the unknown cluster labels y and the training patterns x with respect to parameters of speciﬁcally constrained encoding distributions. The constraints are chosen such that patterns are likely to be clustered similarly if they lie close to speciﬁc unknown vectors in the feature space. The method may be conveniently applied to learning the optimal aﬃnity matrix, which corresponds to learning parameters of the kernelized encoder. The procedure does not require computations of eigenvalues of the Gram matrices, which makes it potentially attractive for clustering large data sets. 1</p><p>4 0.67661649 <a title="132-lda-4" href="./nips-2005-Diffusion_Maps%2C_Spectral_Clustering_and_Eigenfunctions_of_Fokker-Planck_Operators.html">56 nips-2005-Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators</a></p>
<p>Author: Boaz Nadler, Stephane Lafon, Ioannis Kevrekidis, Ronald R. Coifman</p><p>Abstract: This paper presents a diffusion based probabilistic interpretation of spectral clustering and dimensionality reduction algorithms that use the eigenvectors of the normalized graph Laplacian. Given the pairwise adjacency matrix of all points, we deﬁne a diffusion distance between any two data points and show that the low dimensional representation of the data by the ﬁrst few eigenvectors of the corresponding Markov matrix is optimal under a certain mean squared error criterion. Furthermore, assuming that data points are random samples from a density p(x) = e−U (x) we identify these eigenvectors as discrete approximations of eigenfunctions of a Fokker-Planck operator in a potential 2U (x) with reﬂecting boundary conditions. Finally, applying known results regarding the eigenvalues and eigenfunctions of the continuous Fokker-Planck operator, we provide a mathematical justiﬁcation for the success of spectral clustering and dimensional reduction algorithms based on these ﬁrst few eigenvectors. This analysis elucidates, in terms of the characteristics of diffusion processes, many empirical ﬁndings regarding spectral clustering algorithms. Keywords: Algorithms and architectures, learning theory. 1</p><p>5 0.67328304 <a title="132-lda-5" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>Author: Rebecca Willett, Robert Nowak, Rui M. Castro</p><p>Abstract: This paper presents a rigorous statistical analysis characterizing regimes in which active learning signiﬁcantly outperforms classical passive learning. Active learning algorithms are able to make queries or select sample locations in an online fashion, depending on the results of the previous queries. In some regimes, this extra ﬂexibility leads to signiﬁcantly faster rates of error decay than those possible in classical passive learning settings. The nature of these regimes is explored by studying fundamental performance limits of active and passive learning in two illustrative nonparametric function classes. In addition to examining the theoretical potential of active learning, this paper describes a practical algorithm capable of exploiting the extra ﬂexibility of the active setting and provably improving upon the classical passive techniques. Our active learning theory and methods show promise in a number of applications, including ﬁeld estimation using wireless sensor networks and fault line detection. 1</p><p>6 0.6723603 <a title="132-lda-6" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>7 0.67176312 <a title="132-lda-7" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>8 0.67065161 <a title="132-lda-8" href="./nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<p>9 0.66542375 <a title="132-lda-9" href="./nips-2005-Assessing_Approximations_for_Gaussian_Process_Classification.html">30 nips-2005-Assessing Approximations for Gaussian Process Classification</a></p>
<p>10 0.66486013 <a title="132-lda-10" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>11 0.66470689 <a title="132-lda-11" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>12 0.66457522 <a title="132-lda-12" href="./nips-2005-Conditional_Visual_Tracking_in_Kernel_Space.html">45 nips-2005-Conditional Visual Tracking in Kernel Space</a></p>
<p>13 0.66449314 <a title="132-lda-13" href="./nips-2005-Using_%60%60epitomes%27%27_to_model_genetic_diversity%3A_Rational_design_of_HIV_vaccine_cocktails.html">198 nips-2005-Using ``epitomes'' to model genetic diversity: Rational design of HIV vaccine cocktails</a></p>
<p>14 0.66226792 <a title="132-lda-14" href="./nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">151 nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<p>15 0.66093588 <a title="132-lda-15" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>16 0.65958482 <a title="132-lda-16" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>17 0.65924299 <a title="132-lda-17" href="./nips-2005-An_Application_of_Markov_Random_Fields_to_Range_Sensing.html">23 nips-2005-An Application of Markov Random Fields to Range Sensing</a></p>
<p>18 0.65837389 <a title="132-lda-18" href="./nips-2005-Infinite_latent_feature_models_and_the_Indian_buffet_process.html">98 nips-2005-Infinite latent feature models and the Indian buffet process</a></p>
<p>19 0.65749705 <a title="132-lda-19" href="./nips-2005-Efficient_Unsupervised_Learning_for_Localization_and_Detection_in_Object_Categories.html">63 nips-2005-Efficient Unsupervised Learning for Localization and Detection in Object Categories</a></p>
<p>20 0.6574738 <a title="132-lda-20" href="./nips-2005-Soft_Clustering_on_Graphs.html">178 nips-2005-Soft Clustering on Graphs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
