<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>195 nips-2005-Transfer learning for text classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-195" href="#">nips2005-195</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>195 nips-2005-Transfer learning for text classification</h1>
<br/><p>Source: <a title="nips-2005-195-pdf" href="http://papers.nips.cc/paper/2843-transfer-learning-for-text-classification.pdf">pdf</a></p><p>Author: Chuong B. Do, Andrew Y. Ng</p><p>Abstract: Linear text classiﬁcation algorithms work by computing an inner product between a test document vector and a parameter vector. In many such algorithms, including naive Bayes and most TFIDF variants, the parameters are determined by some simple, closed-form, function of training set statistics; we call this mapping mapping from statistics to parameters, the parameter function. Much research in text classiﬁcation over the last few decades has consisted of manual efforts to identify better parameter functions. In this paper, we propose an algorithm for automatically learning this function from related classiﬁcation problems. The parameter function found by our algorithm then deﬁnes a new learning algorithm for text classiﬁcation, which we can apply to novel classiﬁcation tasks. We ﬁnd that our learned classiﬁer outperforms existing methods on a variety of multiclass text classiﬁcation tasks. 1</p><p>Reference: <a title="nips-2005-195-reference" href="../nips2005_reference/nips-2005-Transfer_learning_for_text_classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Transfer learning for text classiﬁcation  Chuong B. [sent-1, score-0.204]
</p><p>2 Ng Computer Science Department Stanford University Stanford, CA 94305  Abstract Linear text classiﬁcation algorithms work by computing an inner product between a test document vector and a parameter vector. [sent-3, score-0.431]
</p><p>3 In many such algorithms, including naive Bayes and most TFIDF variants, the parameters are determined by some simple, closed-form, function of training set statistics; we call this mapping mapping from statistics to parameters, the parameter function. [sent-4, score-0.438]
</p><p>4 Much research in text classiﬁcation over the last few decades has consisted of manual efforts to identify better parameter functions. [sent-5, score-0.231]
</p><p>5 The parameter function found by our algorithm then deﬁnes a new learning algorithm for text classiﬁcation, which we can apply to novel classiﬁcation tasks. [sent-7, score-0.262]
</p><p>6 We ﬁnd that our learned classiﬁer outperforms existing methods on a variety of multiclass text classiﬁcation tasks. [sent-8, score-0.362]
</p><p>7 1  Introduction  In the multiclass text classiﬁcation task, we are given a training set of documents, each labeled as belonging to one of K disjoint classes, and a new unlabeled test document. [sent-9, score-0.409]
</p><p>8 Using the training set as a guide, we must predict the most likely class for the test document. [sent-10, score-0.173]
</p><p>9 “Bag-of-words” linear text classiﬁers represent a document as a vector x of word counts, and predict the class whose score (a linear function of x) is highest, i. [sent-11, score-0.424]
</p><p>10 Choosing parameters {θki } which give high classiﬁcation accuracy on test data, thus, is the main challenge for linear text classiﬁcation algorithms. [sent-17, score-0.256]
</p><p>11 In this paper, we focus on linear text classiﬁcation algorithms in which the parameters are pre-speciﬁed functions of training set statistics; that is, each θki is a function θki := g(uki ) of some ﬁxed statistics uki of the training set. [sent-18, score-1.046]
</p><p>12 Unlike discriminative learning methods, such as logistic regression [1] or support vector machines (SVMs) [2], which use numerical optimization to pick parameters, the learners we consider perform no optimization. [sent-19, score-0.234]
</p><p>13 Rather, in our technique, parameter learning involves tabulating statistics vectors {u ki } and applying the closed-form function g to obtain parameters. [sent-20, score-0.396]
</p><p>14 We refer to g, this mapping from statistics to parameters, as the parameter function. [sent-21, score-0.14]
</p><p>15 Many common text classiﬁcation methods—including the multinomial and multivariate Bernoulli event models for naive Bayes [3], the vector space-based TFIDF classiﬁer [4], and its probabilistic variant, PrTFIDF [5]—belong to this class of algorithms. [sent-22, score-0.454]
</p><p>16 Here, picking a good text classiﬁer from this class is equivalent to ﬁnding the right parameter function for the available statistics. [sent-23, score-0.315]
</p><p>17 In practice, researchers often develop text classiﬁcation algorithms by trial-and-error, guided by empirical testing on real-world classiﬁcation tasks (cf. [sent-24, score-0.211]
</p><p>18 In this paper, we consider the task of automatically learning a parameter function g for text classiﬁcation. [sent-30, score-0.292]
</p><p>19 Given a set of example text classiﬁcation problems, we wish to “metalearn” a new learning algorithm (as speciﬁed by the parameter function g), which may then be applied new classiﬁcation problems. [sent-31, score-0.262]
</p><p>20 In low training data classiﬁcation tasks, the learning algorithm given by our automatically learned parameter function consistently outperforms human-designed parameter functions based on naive Bayes and TFIDF, as well as existing discriminative learning approaches. [sent-34, score-0.602]
</p><p>21 A labeled document is a pair (x, y) ∈ X × Y, where x is an n-dimensional vector with xi indicating the number of occurrences of word wi in the document, and y is the document’s class label. [sent-42, score-0.392]
</p><p>22 A classiﬁcation problem is a tuple D, S, (xtest , ytest ) , where D is a distribution over X × Y, S = {(xi , yi )}M is a set of M training examples, (xtest , ytest ) is a single test example, and i=1 all M + 1 examples are drawn iid from D. [sent-43, score-0.319]
</p><p>23 Given a training set S and a test input vector xtest , we must predict the value of the test class label ytest . [sent-44, score-0.52]
</p><p>24 In linear classiﬁcation algorithms, we evaluate the score fk (xtest ) := i θki xtest i for assigning xtest to each class k ∈ {1, . [sent-45, score-0.583]
</p><p>25 , K} and pick the class y = arg maxk fk (xtest ) with the highest score. [sent-48, score-0.162]
</p><p>26 In our meta-learning setting, we deﬁne each θki as the component-wise evaluation of the parameter function g on some vector of training set statistics u ki :     θk1 g(uk1 )  θk2   g(uk2 )   . [sent-49, score-0.439]
</p><p>27 , n) is a vector whose components are computed from the training set S (we will provide speciﬁc examples later). [sent-62, score-0.149]
</p><p>28 Furthermore, g : Rq → R is the parameter function mapping from uki to its corresponding parameter θki . [sent-63, score-0.775]
</p><p>29 To illustrate these deﬁnitions, we show that two speciﬁc cases of the naive Bayes and TFIDF classiﬁcation methods belong to the class of algorithms described above. [sent-64, score-0.224]
</p><p>30 Naive Bayes: In the multinomial variant of the naive Bayes classiﬁcation algorithm, 1 the score for assigning a document x to class k is n NB fk (x) := log p(y = k) + i=1 xi log p(wi | y = k). [sent-65, score-0.596]
</p><p>31 This view has proved useful for analysis of naive Bayes even when none of its probabilistic assumptions hold [9]; here, we adopt this view, without attaching any particular probabilistic meaning to the empirical frequencies p(·) that happen to be computed by the algorithm. [sent-67, score-0.212]
</p><p>32 For balanced training sets, the ﬁrst term is NB irrelevant. [sent-69, score-0.104]
</p><p>33 2 As before, we write fk (x) = i θki xi with θki = gTFIDF (uki ). [sent-73, score-0.144]
</p><p>34 The statistics vector is again deﬁned as in (3), but this time, gTFIDF (uki ) :=  uki1 uki4  log  uki5 uki2  2  . [sent-74, score-0.128]
</p><p>35 These include most other variants of TFIDF based on different TF and IDF terms [7], PrTFIDF [5], and various heuristically modiﬁed versions of naive Bayes [6]. [sent-76, score-0.217]
</p><p>36 3  Learning the parameter function  In the last section, we gave two examples of algorithms that obtain their parameters θ ki by applying a function g to a statistics vector uki . [sent-77, score-1.046]
</p><p>37 In each case, the parameter function was hand-designed, either from probabilistic (in the case of naive Bayes [3]) or geometric (in the case of TFIDF [4]) considerations. [sent-78, score-0.246]
</p><p>38 We now consider the problem of automatically learning a parameter function from example classiﬁcation tasks. [sent-79, score-0.119]
</p><p>39 In the sequel, we assume ﬁxed statistics vectors {uki } and focus on ﬁnding an optimal parameter function g. [sent-80, score-0.153]
</p><p>40 In the standard supervised learning setting, we are given a training set of examples sampled from some unknown distribution D, and our goal is to use the training set to make a prediction on a new test example also sampled from D. [sent-81, score-0.326]
</p><p>41 By using the training examples to understand the statistical regularities in D, we hope to predict ytest from xtest with low error. [sent-82, score-0.422]
</p><p>42 Analogously, the problem of meta-learning g is again a supervised learning task; here, however, the training “examples” are now classiﬁcation problems sampled from a distribution D over classiﬁcation problems. [sent-83, score-0.169]
</p><p>43 3 By seeing many instances of text classiﬁcation problems 2  TFIDF Note that (5) implicitly deﬁnes fk (x) as a dot product of two vectors, each of whose components consist of a product of two terms. [sent-84, score-0.317]
</p><p>44 In the normalized TFIDF classiﬁer, both vectors are normalized to unit length before computing the dot product, a modiﬁcation that makes the algorithm more stable for documents of varying length. [sent-85, score-0.158]
</p><p>45 3 Note that in our meta-learning problem, the output of our algorithm is a parameter function g mapping statistics to parameters. [sent-87, score-0.14]
</p><p>46 Our training data, however, do not explicitly indicate the best parameter function g ∗ for each example classiﬁcation problem. [sent-88, score-0.14]
</p><p>47 Effectively then, in the meta-learning task, the central problem is to ﬁt g to some unseen g∗ , based on test examples in each training classiﬁcation problem. [sent-89, score-0.147]
</p><p>48 drawn from D, we hope to learn a parameter function g that exploits the statistical regularities in problems from D. [sent-90, score-0.104]
</p><p>49 For a new, test classiﬁcation problem Dtest , Stest , (xtest , ytest ) sampled independently from D, we desire that our learned g correctly classify xtest with high probability. [sent-92, score-0.41]
</p><p>50 Using the linearity assumption, we pose a convex optimization problem for ﬁnding a parameter function g that achieves small loss on test examples in the training collection. [sent-94, score-0.229]
</p><p>51 1  Softmax learning  Recall that in softmax regression, the class probabilities p(y | x) are modeled as exp( i θki xi ) , i θk i xi ) k exp(  p(y = k | x; {θki }) :=  k = 1, . [sent-97, score-0.381]
</p><p>52 , K,  (7)  where the parameters {θki } are learned from the training data S by maximizing the conditional log likelihood of the data. [sent-100, score-0.21]
</p><p>53 To pose our optimization problem, we start by learning the linear form g(uki ) = β T uki . [sent-104, score-0.686]
</p><p>54 Under this parameterization, the conditional likelihood of an example (x, y) is p(y = k | x; β) =  exp( k  i  exp(  β T uki xi ) i  β T uk i xi )  ,  k = 1, . [sent-105, score-0.8]
</p><p>55 (8)  In this setup, one natural approach for learning a linear function g is to maximize the (regularized) conditional log likelihood (β : S ) for the entire collection S : m j=1  log p(y (j) | x(j) ; β) − C||β||2   (j) (j) m exp β T i uy(j) i xi  − C||β||2 . [sent-109, score-0.253]
</p><p>56 = log  (j) (j) exp β T i uki xi j=1 k  (β : S ) :=  (9)  In (9), the latter term corresponds to a Gaussian prior on the parameters β, which provides a means for controlling the complexity of the learned parameter function g. [sent-110, score-0.953]
</p><p>57 The maximization of (9) is similar to softmax regression training except that here, instead of optimizing over the parameters {θki } directly, we optimize over the choice of β. [sent-111, score-0.288]
</p><p>58 By the Representer Theorem [10], there exists a maximizing solution to (9) for which the optimal parameter vector β ∗ is a linear combination of training set statistics: β∗ =  m j=1  k  ∗ αjk  (j) (j)  i  uki xi . [sent-114, score-0.877]
</p><p>59 (10)  From this, we reparameterize the original optimization over β in (9) as an equivalent optimization over training example weights {αjk }. [sent-115, score-0.13]
</p><p>60 For notational convenience, let K(j, j , k, k ) :=  (j) (j )  i  i  xi xi  (j)  (j )  (uki )T uk i . [sent-116, score-0.169]
</p><p>61 8  1  −30 −35  (b)  −30  −25  −20  −15 uk1  −10  −5  0  Figure 1: Distribution of unnormalized uki vectors in dmoz data (a) with and (b) without applying the log transformation in (15). [sent-125, score-0.88]
</p><p>62 Substituting (10) and (11) into (9), we obtain  m exp ({αjk } : S ) := log  j =1 k exp m  m j=1  k  αjk K(j, j , k, y (j ) )  m j=1  k  αjk K(j, j , k, k )  m  −C  αjk αj j=1 j =1 k  k     K(j, j , k, k ). [sent-129, score-0.123]
</p><p>63 The assumption that g is a linear function of uki , however, places a severe restriction on the class of learnable parameter functions. [sent-131, score-0.749]
</p><p>64 4 In particular, choosing a Gaussian (RBF) kernel, K(u, v) := exp(−γ||u − v||2 ), gives a non-parametric representation for g: g(uki ) = β T Φ(uki ) =  m j=1  (j)  k  i  (j)  αjk xi exp(−γ||uki − uki ||2 ). [sent-133, score-0.704]
</p><p>65 (14)  (j) {αjk xi },  Thus, g(uki ) is a weighted combination of the values where the weights depend (j) exponentially on the squared 2 -distance of uki to each of the statistics vectors {uki }. [sent-134, score-0.799]
</p><p>66 4  Experiments  To validate our method, we evaluated its ability to learn parameter functions on a variety of email and webpage classiﬁcation tasks in which the number of classes, K, was large (K = 10), and the number of number of training examples per class, m/K, was small (m/K = 2). [sent-136, score-0.241]
</p><p>67 We used the dmoz Open Directory Project hierarchy,5 the 20 Newsgroups dataset,6 the Reuters-21578 dataset,7 and the Industry Sector dataset8 . [sent-137, score-0.138]
</p><p>68 gz  Table 1: Test set accuracy on dmoz categories. [sent-155, score-0.166]
</p><p>69 Columns 2-4 give the proportion of correct classiﬁcations using non-discriminative methods: the learned g, Naive Bayes, and TFIDF, respectively. [sent-156, score-0.105]
</p><p>70 Columns 5-7 give the corresponding values for the discriminative methods: softmax regression, 1-vs-all SVMs, and multiclass SVMs. [sent-157, score-0.303]
</p><p>71 397  The dmoz project is a hierarchical collection of webpage links organized by subject matter. [sent-261, score-0.193]
</p><p>72 9 Given a dataset of documents, we sampled 10-class 2-training-examples-per-class classiﬁcation problems by randomly selecting 10 different classes within the dataset, picking 2 training examples within each class, and choosing one test example from a randomly chosen class. [sent-265, score-0.227]
</p><p>73 For simplicity, we reduced the feature vector in (3) to the following two-dimensional representation,10 uki =  log(proportion of wi among words from documents of class k) . [sent-268, score-0.917]
</p><p>74 log(proportion of documents containing wi )  (15)  Note that up to the log transformation, the components of uki correspond to the relative term frequency and document frequency of a word relative to class k (see Figure 1). [sent-269, score-1.126]
</p><p>75 2  Generalization performance  We tested our meta-learning algorithm on classiﬁcation problems taken from each of the 16 top-level dmoz categories. [sent-271, score-0.161]
</p><p>76 Dataset gdmoz gnews greut gindu gNB gTFIDF softmax 1VA-SVM MC-SVM dmoz n/a 0. [sent-277, score-0.438]
</p><p>77 To assess the accuracy of our meta-learning algorithm for a particular test category, we used the g learned from a set of 450 classiﬁcation problems drawn from the other 15 top-level categories. [sent-310, score-0.145]
</p><p>78 In 15 out of 16 categories, the learned parameter function g outperforms naive Bayes and TFIDF in addition to the discriminative methods we tested (softmax regression, 1-vs-all SVMs [12], and multiclass SVMs [13]12 ; see Table 1). [sent-312, score-0.47]
</p><p>79 Here, for each of the four corpora (dmoz, 20 Newsgroups, Reuters-21578, Industry Sector), we constructed independent training and testing datasets of 480 random classiﬁcation problems. [sent-314, score-0.151]
</p><p>80 After training separate classiﬁers (gdmoz , gnews , greut , and gindu ) using data from each of the four corpora, we tested the performance of each learned classiﬁer on the remaining three corpora (see Table 2). [sent-315, score-0.331]
</p><p>81 Again, the learned parameter functions compare favorably to the other methods. [sent-316, score-0.121]
</p><p>82 Moreover, these tests show that a single parameter function may give an accurate classiﬁcation algorithm for many different corpora, demonstrating the effectiveness of our approach for achieving transfer across related learning tasks. [sent-317, score-0.171]
</p><p>83 5  Discussion and Related Work  In this paper, we presented an algorithm based on softmax regression for learning a parameter function g from example classiﬁcation problems. [sent-318, score-0.271]
</p><p>84 Another approach for learning g is to modify the multiclass support vector machine formulation of Crammer and Singer [13] in a manner analagous to the modiﬁcation of softmax regression in Section 3. [sent-320, score-0.372]
</p><p>85 1, giving the following quadratic program: minimize 1 ||β||2 + C j ξj 2 n m β∈R ,ξ∈R  subject to  βT  (j)  i  (j)  (j)  xi (uy(j) i − uki ) ≥ I{k=y(j) } − ξj ,  ∀k, ∀j. [sent-321, score-0.704]
</p><p>86 We implemented this method and found that the learned g, like in the softmax formulation, outperforms naive Bayes, TFIDF, and the other discriminative methods. [sent-323, score-0.456]
</p><p>87 The techniques described in this paper give one approach for achieving inductive transfer in classiﬁer design—using labeled data from related example classiﬁcation problems to solve a particular classiﬁcation problem [16, 17]. [sent-324, score-0.151]
</p><p>88 [18] also consider the issue of knowledge transfer in text classiﬁcation in the context of ensemble classiﬁers, and propose a system for using related classiﬁcation problems to learn the reliability of individual classiﬁers within the ensemble. [sent-326, score-0.303]
</p><p>89 The same holdout set was used to select regularization parameters for the discriminative learning algorithms. [sent-328, score-0.153]
</p><p>90 m/K = 10), softmax and multiclass SVMs consistently outperform naive Bayes and TFIDF; nevertheless, the learned g achieves a performance on par with discriminative methods, despite being constrained to parameters which are explicit functions of training data statistics. [sent-332, score-0.636]
</p><p>91 This result is consistent with a previous study in which a heuristically hand-tuned version of Naive Bayes attained near-SVM text classiﬁcation performance for large datasets [6]. [sent-333, score-0.2]
</p><p>92 Though not directly applied to text classiﬁcation, Teevan and Karger [19] consider the problem of automatically learning term distributions for use in information retrieval. [sent-335, score-0.256]
</p><p>93 A similar concept is implicit in the kernelized parameter function learned by our algorithm, where the Gaussian kernel facilitates transfer between similar statistics vectors. [sent-338, score-0.287]
</p><p>94 A comparison of event models for Naive Bayes text classiﬁcation. [sent-355, score-0.173]
</p><p>95 A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization. [sent-364, score-0.197]
</p><p>96 Tackling the poor assumptions of naive Bayes text classiﬁers. [sent-373, score-0.337]
</p><p>97 generative classiﬁers: a comparison of logistic regression and naive Bayes. [sent-388, score-0.202]
</p><p>98 On the algorithmic implementation of multiclass kernel-based vector machines. [sent-417, score-0.133]
</p><p>99 Inductive transfer for text classiﬁcation using generalized reliability indicators. [sent-451, score-0.28]
</p><p>100 Empirical development of an exponential probabilistic model for text retrieval: Using textual analysis to build a better model. [sent-457, score-0.197]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('uki', 0.631), ('tfidf', 0.308), ('classi', 0.216), ('ki', 0.212), ('xtest', 0.197), ('text', 0.173), ('naive', 0.164), ('softmax', 0.144), ('dmoz', 0.138), ('cation', 0.127), ('documents', 0.117), ('bayes', 0.114), ('multiclass', 0.1), ('document', 0.088), ('ytest', 0.086), ('jk', 0.084), ('transfer', 0.082), ('training', 0.082), ('gnb', 0.079), ('gtfidf', 0.079), ('sector', 0.079), ('wi', 0.076), ('xi', 0.073), ('fk', 0.071), ('corpora', 0.069), ('learned', 0.063), ('industry', 0.063), ('class', 0.06), ('teevan', 0.059), ('discriminative', 0.059), ('newsgroups', 0.058), ('parameter', 0.058), ('statistics', 0.054), ('er', 0.05), ('ers', 0.049), ('svms', 0.042), ('proportion', 0.042), ('vectors', 0.041), ('log', 0.041), ('exp', 0.041), ('gdmoz', 0.039), ('gindu', 0.039), ('gnews', 0.039), ('greut', 0.039), ('holdout', 0.039), ('libsvm', 0.039), ('prtfidf', 0.039), ('uy', 0.039), ('category', 0.039), ('word', 0.039), ('regression', 0.038), ('categories', 0.038), ('tasks', 0.038), ('tc', 0.034), ('rq', 0.034), ('examples', 0.034), ('stanford', 0.034), ('kn', 0.034), ('sampled', 0.033), ('vector', 0.033), ('learning', 0.031), ('test', 0.031), ('maxk', 0.031), ('score', 0.031), ('kernel', 0.03), ('automatically', 0.03), ('bennett', 0.029), ('unnormalized', 0.029), ('webpage', 0.029), ('accuracy', 0.028), ('mapping', 0.028), ('nb', 0.027), ('heuristically', 0.027), ('assigning', 0.027), ('variants', 0.026), ('outperforms', 0.026), ('collection', 0.026), ('discarded', 0.026), ('sigir', 0.026), ('support', 0.026), ('frequency', 0.026), ('reliability', 0.025), ('vocabulary', 0.025), ('product', 0.025), ('nes', 0.025), ('optimization', 0.024), ('parameters', 0.024), ('picking', 0.024), ('crammer', 0.024), ('probabilistic', 0.024), ('uk', 0.023), ('inductive', 0.023), ('regularities', 0.023), ('categorization', 0.023), ('labeled', 0.023), ('numerical', 0.023), ('http', 0.023), ('problems', 0.023), ('inner', 0.023), ('term', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="195-tfidf-1" href="./nips-2005-Transfer_learning_for_text_classification.html">195 nips-2005-Transfer learning for text classification</a></p>
<p>Author: Chuong B. Do, Andrew Y. Ng</p><p>Abstract: Linear text classiﬁcation algorithms work by computing an inner product between a test document vector and a parameter vector. In many such algorithms, including naive Bayes and most TFIDF variants, the parameters are determined by some simple, closed-form, function of training set statistics; we call this mapping mapping from statistics to parameters, the parameter function. Much research in text classiﬁcation over the last few decades has consisted of manual efforts to identify better parameter functions. In this paper, we propose an algorithm for automatically learning this function from related classiﬁcation problems. The parameter function found by our algorithm then deﬁnes a new learning algorithm for text classiﬁcation, which we can apply to novel classiﬁcation tasks. We ﬁnd that our learned classiﬁer outperforms existing methods on a variety of multiclass text classiﬁcation tasks. 1</p><p>2 0.15491892 <a title="195-tfidf-2" href="./nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">113 nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<p>Author: Jian Zhang, Zoubin Ghahramani, Yiming Yang</p><p>Abstract: We propose a probabilistic model based on Independent Component Analysis for learning multiple related tasks. In our model the task parameters are assumed to be generated from independent sources which account for the relatedness of the tasks. We use Laplace distributions to model hidden sources which makes it possible to identify the hidden, independent components instead of just modeling correlations. Furthermore, our model enjoys a sparsity property which makes it both parsimonious and robust. We also propose efﬁcient algorithms for both empirical Bayes method and point estimation. Our experimental results on two multi-label text classiﬁcation data sets show that the proposed approach is promising.</p><p>3 0.1261327 <a title="195-tfidf-3" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>Author: Thomas Gärtner, Quoc V. Le, Simon Burton, Alex J. Smola, Vishy Vishwanathan</p><p>Abstract: We present a method for performing transductive inference on very large datasets. Our algorithm is based on multiclass Gaussian processes and is effective whenever the multiplication of the kernel matrix or its inverse with a vector can be computed sufﬁciently fast. This holds, for instance, for certain graph and string kernels. Transduction is achieved by variational inference over the unlabeled data subject to a balancing constraint. 1</p><p>4 0.1144027 <a title="195-tfidf-4" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>Author: Kilian Q. Weinberger, John Blitzer, Lawrence K. Saul</p><p>Abstract: We show how to learn a Mahanalobis distance metric for k-nearest neighbor (kNN) classiﬁcation by semideﬁnite programming. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. On seven data sets of varying size and difﬁculty, we ﬁnd that metrics trained in this way lead to signiﬁcant improvements in kNN classiﬁcation—for example, achieving a test error rate of 1.3% on the MNIST handwritten digits. As in support vector machines (SVMs), the learning problem reduces to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our framework requires no modiﬁcation or extension for problems in multiway (as opposed to binary) classiﬁcation. 1</p><p>5 0.096336454 <a title="195-tfidf-5" href="./nips-2005-A_matching_pursuit_approach_to_sparse_Gaussian_process_regression.html">16 nips-2005-A matching pursuit approach to sparse Gaussian process regression</a></p>
<p>Author: Sathiya Keerthi, Wei Chu</p><p>Abstract: In this paper we propose a new basis selection criterion for building sparse GP regression models that provides promising gains in accuracy as well as efﬁciency over previous methods. Our algorithm is much faster than that of Smola and Bartlett, while, in generalization it greatly outperforms the information gain approach proposed by Seeger et al, especially on the quality of predictive distributions. 1</p><p>6 0.092525735 <a title="195-tfidf-6" href="./nips-2005-Maximum_Margin_Semi-Supervised_Learning_for_Structured_Variables.html">123 nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</a></p>
<p>7 0.090719022 <a title="195-tfidf-7" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>8 0.088631153 <a title="195-tfidf-8" href="./nips-2005-Benchmarking_Non-Parametric_Statistical_Tests.html">37 nips-2005-Benchmarking Non-Parametric Statistical Tests</a></p>
<p>9 0.075551949 <a title="195-tfidf-9" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>10 0.075276896 <a title="195-tfidf-10" href="./nips-2005-Context_as_Filtering.html">48 nips-2005-Context as Filtering</a></p>
<p>11 0.073191382 <a title="195-tfidf-11" href="./nips-2005-Subsequence_Kernels_for_Relation_Extraction.html">185 nips-2005-Subsequence Kernels for Relation Extraction</a></p>
<p>12 0.071337245 <a title="195-tfidf-12" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>13 0.069073968 <a title="195-tfidf-13" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>14 0.06897369 <a title="195-tfidf-14" href="./nips-2005-Radial_Basis_Function_Network_for_Multi-task_Learning.html">161 nips-2005-Radial Basis Function Network for Multi-task Learning</a></p>
<p>15 0.068497971 <a title="195-tfidf-15" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>16 0.06675265 <a title="195-tfidf-16" href="./nips-2005-Analysis_of_Spectral_Kernel_Design_based_Semi-supervised_Learning.html">27 nips-2005-Analysis of Spectral Kernel Design based Semi-supervised Learning</a></p>
<p>17 0.065044805 <a title="195-tfidf-17" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>18 0.063399024 <a title="195-tfidf-18" href="./nips-2005-Correlated_Topic_Models.html">52 nips-2005-Correlated Topic Models</a></p>
<p>19 0.062455226 <a title="195-tfidf-19" href="./nips-2005-Generalization_error_bounds_for_classifiers_trained_with_interdependent_data.html">83 nips-2005-Generalization error bounds for classifiers trained with interdependent data</a></p>
<p>20 0.061782233 <a title="195-tfidf-20" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.201), (1, 0.092), (2, -0.04), (3, -0.014), (4, 0.086), (5, 0.014), (6, 0.16), (7, 0.055), (8, 0.041), (9, -0.015), (10, -0.06), (11, -0.167), (12, 0.073), (13, -0.056), (14, -0.018), (15, -0.015), (16, 0.022), (17, -0.022), (18, -0.001), (19, 0.04), (20, -0.061), (21, -0.026), (22, -0.038), (23, -0.03), (24, -0.095), (25, 0.014), (26, -0.055), (27, 0.039), (28, 0.081), (29, 0.046), (30, 0.077), (31, 0.071), (32, 0.097), (33, -0.109), (34, 0.028), (35, -0.053), (36, -0.133), (37, -0.052), (38, 0.061), (39, -0.09), (40, 0.075), (41, 0.028), (42, -0.014), (43, -0.154), (44, -0.008), (45, 0.021), (46, -0.096), (47, -0.01), (48, -0.079), (49, 0.153)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93011189 <a title="195-lsi-1" href="./nips-2005-Transfer_learning_for_text_classification.html">195 nips-2005-Transfer learning for text classification</a></p>
<p>Author: Chuong B. Do, Andrew Y. Ng</p><p>Abstract: Linear text classiﬁcation algorithms work by computing an inner product between a test document vector and a parameter vector. In many such algorithms, including naive Bayes and most TFIDF variants, the parameters are determined by some simple, closed-form, function of training set statistics; we call this mapping mapping from statistics to parameters, the parameter function. Much research in text classiﬁcation over the last few decades has consisted of manual efforts to identify better parameter functions. In this paper, we propose an algorithm for automatically learning this function from related classiﬁcation problems. The parameter function found by our algorithm then deﬁnes a new learning algorithm for text classiﬁcation, which we can apply to novel classiﬁcation tasks. We ﬁnd that our learned classiﬁer outperforms existing methods on a variety of multiclass text classiﬁcation tasks. 1</p><p>2 0.65658259 <a title="195-lsi-2" href="./nips-2005-Learning_Multiple_Related_Tasks_using_Latent_Independent_Component_Analysis.html">113 nips-2005-Learning Multiple Related Tasks using Latent Independent Component Analysis</a></p>
<p>Author: Jian Zhang, Zoubin Ghahramani, Yiming Yang</p><p>Abstract: We propose a probabilistic model based on Independent Component Analysis for learning multiple related tasks. In our model the task parameters are assumed to be generated from independent sources which account for the relatedness of the tasks. We use Laplace distributions to model hidden sources which makes it possible to identify the hidden, independent components instead of just modeling correlations. Furthermore, our model enjoys a sparsity property which makes it both parsimonious and robust. We also propose efﬁcient algorithms for both empirical Bayes method and point estimation. Our experimental results on two multi-label text classiﬁcation data sets show that the proposed approach is promising.</p><p>3 0.56446779 <a title="195-lsi-3" href="./nips-2005-Benchmarking_Non-Parametric_Statistical_Tests.html">37 nips-2005-Benchmarking Non-Parametric Statistical Tests</a></p>
<p>Author: Mikaela Keller, Samy Bengio, Siew Y. Wong</p><p>Abstract: Although non-parametric tests have already been proposed for that purpose, statistical signiﬁcance tests for non-standard measures (different from the classiﬁcation error) are less often used in the literature. This paper is an attempt at empirically verifying how these tests compare with more classical tests, on various conditions. More precisely, using a very large dataset to estimate the whole “population”, we analyzed the behavior of several statistical test, varying the class unbalance, the compared models, the performance measure, and the sample size. The main result is that providing big enough evaluation sets non-parametric tests are relatively reliable in all conditions. 1</p><p>4 0.5613513 <a title="195-lsi-4" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>Author: Kilian Q. Weinberger, John Blitzer, Lawrence K. Saul</p><p>Abstract: We show how to learn a Mahanalobis distance metric for k-nearest neighbor (kNN) classiﬁcation by semideﬁnite programming. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. On seven data sets of varying size and difﬁculty, we ﬁnd that metrics trained in this way lead to signiﬁcant improvements in kNN classiﬁcation—for example, achieving a test error rate of 1.3% on the MNIST handwritten digits. As in support vector machines (SVMs), the learning problem reduces to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our framework requires no modiﬁcation or extension for problems in multiway (as opposed to binary) classiﬁcation. 1</p><p>5 0.54867458 <a title="195-lsi-5" href="./nips-2005-A_PAC-Bayes_approach_to_the_Set_Covering_Machine.html">12 nips-2005-A PAC-Bayes approach to the Set Covering Machine</a></p>
<p>Author: François Laviolette, Mario Marchand, Mohak Shah</p><p>Abstract: We design a new learning algorithm for the Set Covering Machine from a PAC-Bayes perspective and propose a PAC-Bayes risk bound which is minimized for classiﬁers achieving a non trivial margin-sparsity trade-oﬀ. 1</p><p>6 0.53814757 <a title="195-lsi-6" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>7 0.49884903 <a title="195-lsi-7" href="./nips-2005-A_General_and_Efficient_Multiple_Kernel_Learning_Algorithm.html">10 nips-2005-A General and Efficient Multiple Kernel Learning Algorithm</a></p>
<p>8 0.46655145 <a title="195-lsi-8" href="./nips-2005-A_Probabilistic_Interpretation_of_SVMs_with_an_Application_to_Unbalanced_Classification.html">14 nips-2005-A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification</a></p>
<p>9 0.45151058 <a title="195-lsi-9" href="./nips-2005-Maximum_Margin_Semi-Supervised_Learning_for_Structured_Variables.html">123 nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</a></p>
<p>10 0.44220105 <a title="195-lsi-10" href="./nips-2005-Subsequence_Kernels_for_Relation_Extraction.html">185 nips-2005-Subsequence Kernels for Relation Extraction</a></p>
<p>11 0.41632575 <a title="195-lsi-11" href="./nips-2005-Modeling_Neuronal_Interactivity_using_Dynamic_Bayesian_Networks.html">130 nips-2005-Modeling Neuronal Interactivity using Dynamic Bayesian Networks</a></p>
<p>12 0.41406074 <a title="195-lsi-12" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>13 0.41194004 <a title="195-lsi-13" href="./nips-2005-A_matching_pursuit_approach_to_sparse_Gaussian_process_regression.html">16 nips-2005-A matching pursuit approach to sparse Gaussian process regression</a></p>
<p>14 0.41100609 <a title="195-lsi-14" href="./nips-2005-Fast_Gaussian_Process_Regression_using_KD-Trees.html">69 nips-2005-Fast Gaussian Process Regression using KD-Trees</a></p>
<p>15 0.4099035 <a title="195-lsi-15" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>16 0.4054769 <a title="195-lsi-16" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>17 0.4045535 <a title="195-lsi-17" href="./nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">151 nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<p>18 0.40360618 <a title="195-lsi-18" href="./nips-2005-Searching_for_Character_Models.html">171 nips-2005-Searching for Character Models</a></p>
<p>19 0.38814232 <a title="195-lsi-19" href="./nips-2005-The_Curse_of_Highly_Variable_Functions_for_Local_Kernel_Machines.html">190 nips-2005-The Curse of Highly Variable Functions for Local Kernel Machines</a></p>
<p>20 0.38083866 <a title="195-lsi-20" href="./nips-2005-Radial_Basis_Function_Network_for_Multi-task_Learning.html">161 nips-2005-Radial Basis Function Network for Multi-task Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.051), (5, 0.01), (10, 0.033), (25, 0.285), (27, 0.035), (31, 0.043), (34, 0.125), (41, 0.011), (50, 0.012), (55, 0.027), (69, 0.051), (73, 0.039), (88, 0.142), (91, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93160808 <a title="195-lda-1" href="./nips-2005-A_Bayesian_Spatial_Scan_Statistic.html">4 nips-2005-A Bayesian Spatial Scan Statistic</a></p>
<p>Author: Daniel B. Neill, Andrew W. Moore, Gregory F. Cooper</p><p>Abstract: We propose a new Bayesian method for spatial cluster detection, the “Bayesian spatial scan statistic,” and compare this method to the standard (frequentist) scan statistic approach. We demonstrate that the Bayesian statistic has several advantages over the frequentist approach, including increased power to detect clusters and (since randomization testing is unnecessary) much faster runtime. We evaluate the Bayesian and frequentist methods on the task of prospective disease surveillance: detecting spatial clusters of disease cases resulting from emerging disease outbreaks. We demonstrate that our Bayesian methods are successful in rapidly detecting outbreaks while keeping number of false positives low. 1</p><p>2 0.84678501 <a title="195-lda-2" href="./nips-2005-Saliency_Based_on_Information_Maximization.html">169 nips-2005-Saliency Based on Information Maximization</a></p>
<p>Author: Neil Bruce, John Tsotsos</p><p>Abstract: A model of bottom-up overt attention is proposed based on the principle of maximizing information sampled from a scene. The proposed operation is based on Shannon's self-information measure and is achieved in a neural circuit, which is demonstrated as having close ties with the circuitry existent in the primate visual cortex. It is further shown that the proposed saliency measure may be extended to address issues that currently elude explanation in the domain of saliency based models. Resu lts on natural images are compared with experimental eye tracking data revealing the efficacy of the model in predicting the deployment of overt attention as compared with existing efforts.</p><p>same-paper 3 0.75783432 <a title="195-lda-3" href="./nips-2005-Transfer_learning_for_text_classification.html">195 nips-2005-Transfer learning for text classification</a></p>
<p>Author: Chuong B. Do, Andrew Y. Ng</p><p>Abstract: Linear text classiﬁcation algorithms work by computing an inner product between a test document vector and a parameter vector. In many such algorithms, including naive Bayes and most TFIDF variants, the parameters are determined by some simple, closed-form, function of training set statistics; we call this mapping mapping from statistics to parameters, the parameter function. Much research in text classiﬁcation over the last few decades has consisted of manual efforts to identify better parameter functions. In this paper, we propose an algorithm for automatically learning this function from related classiﬁcation problems. The parameter function found by our algorithm then deﬁnes a new learning algorithm for text classiﬁcation, which we can apply to novel classiﬁcation tasks. We ﬁnd that our learned classiﬁer outperforms existing methods on a variety of multiclass text classiﬁcation tasks. 1</p><p>4 0.59528875 <a title="195-lda-4" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>Author: Ashish Kapoor, Hyungil Ahn, Yuan Qi, Rosalind W. Picard</p><p>Abstract: There have been many graph-based approaches for semi-supervised classiﬁcation. One problem is that of hyperparameter learning: performance depends greatly on the hyperparameters of the similarity graph, transformation of the graph Laplacian and the noise model. We present a Bayesian framework for learning hyperparameters for graph-based semisupervised classiﬁcation. Given some labeled data, which can contain inaccurate labels, we pose the semi-supervised classiﬁcation as an inference problem over the unknown labels. Expectation Propagation is used for approximate inference and the mean of the posterior is used for classiﬁcation. The hyperparameters are learned using EM for evidence maximization. We also show that the posterior mean can be written in terms of the kernel matrix, providing a Bayesian classiﬁer to classify new points. Tests on synthetic and real datasets show cases where there are signiﬁcant improvements in performance over the existing approaches. 1</p><p>5 0.59128737 <a title="195-lda-5" href="./nips-2005-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">57 nips-2005-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>Author: Kilian Q. Weinberger, John Blitzer, Lawrence K. Saul</p><p>Abstract: We show how to learn a Mahanalobis distance metric for k-nearest neighbor (kNN) classiﬁcation by semideﬁnite programming. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. On seven data sets of varying size and difﬁculty, we ﬁnd that metrics trained in this way lead to signiﬁcant improvements in kNN classiﬁcation—for example, achieving a test error rate of 1.3% on the MNIST handwritten digits. As in support vector machines (SVMs), the learning problem reduces to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our framework requires no modiﬁcation or extension for problems in multiway (as opposed to binary) classiﬁcation. 1</p><p>6 0.58654249 <a title="195-lda-6" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>7 0.58479589 <a title="195-lda-7" href="./nips-2005-Convex_Neural_Networks.html">50 nips-2005-Convex Neural Networks</a></p>
<p>8 0.58443791 <a title="195-lda-8" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>9 0.58311254 <a title="195-lda-9" href="./nips-2005-An_Application_of_Markov_Random_Fields_to_Range_Sensing.html">23 nips-2005-An Application of Markov Random Fields to Range Sensing</a></p>
<p>10 0.58129078 <a title="195-lda-10" href="./nips-2005-Learning_Rankings_via_Convex_Hull_Separation.html">114 nips-2005-Learning Rankings via Convex Hull Separation</a></p>
<p>11 0.5799157 <a title="195-lda-11" href="./nips-2005-Conditional_Visual_Tracking_in_Kernel_Space.html">45 nips-2005-Conditional Visual Tracking in Kernel Space</a></p>
<p>12 0.57991481 <a title="195-lda-12" href="./nips-2005-Sequence_and_Tree_Kernels_with_Statistical_Feature_Mining.html">175 nips-2005-Sequence and Tree Kernels with Statistical Feature Mining</a></p>
<p>13 0.57839203 <a title="195-lda-13" href="./nips-2005-An_Approximate_Inference_Approach_for_the_PCA_Reconstruction_Error.html">24 nips-2005-An Approximate Inference Approach for the PCA Reconstruction Error</a></p>
<p>14 0.57820743 <a title="195-lda-14" href="./nips-2005-Large-Scale_Multiclass_Transduction.html">105 nips-2005-Large-Scale Multiclass Transduction</a></p>
<p>15 0.57739508 <a title="195-lda-15" href="./nips-2005-A_matching_pursuit_approach_to_sparse_Gaussian_process_regression.html">16 nips-2005-A matching pursuit approach to sparse Gaussian process regression</a></p>
<p>16 0.57725137 <a title="195-lda-16" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>17 0.57700527 <a title="195-lda-17" href="./nips-2005-An_Alternative_Infinite_Mixture_Of_Gaussian_Process_Experts.html">21 nips-2005-An Alternative Infinite Mixture Of Gaussian Process Experts</a></p>
<p>18 0.57686585 <a title="195-lda-18" href="./nips-2005-Pattern_Recognition_from_One_Example_by_Chopping.html">151 nips-2005-Pattern Recognition from One Example by Chopping</a></p>
<p>19 0.57607406 <a title="195-lda-19" href="./nips-2005-Non-Gaussian_Component_Analysis%3A_a_Semi-parametric_Framework_for_Linear_Dimension_Reduction.html">137 nips-2005-Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction</a></p>
<p>20 0.57597613 <a title="195-lda-20" href="./nips-2005-Sparse_Gaussian_Processes_using_Pseudo-inputs.html">179 nips-2005-Sparse Gaussian Processes using Pseudo-inputs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
