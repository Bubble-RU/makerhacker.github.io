<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>121 nips-2005-Location-based activity recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2005" href="../home/nips2005_home.html">nips2005</a> <a title="nips-2005-121" href="#">nips2005-121</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>121 nips-2005-Location-based activity recognition</h1>
<br/><p>Source: <a title="nips-2005-121-pdf" href="http://papers.nips.cc/paper/2911-location-based-activity-recognition.pdf">pdf</a></p><p>Author: Lin Liao, Dieter Fox, Henry Kautz</p><p>Abstract: Learning patterns of human behavior from sensor data is extremely important for high-level activity inference. We show how to extract and label a person’s activities and signiﬁcant places from traces of GPS data. In contrast to existing techniques, our approach simultaneously detects and classiﬁes the signiﬁcant locations of a person and takes the highlevel context into account. Our system uses relational Markov networks to represent the hierarchical activity model that encodes the complex relations among GPS readings, activities and signiﬁcant places. We apply FFT-based message passing to perform efﬁcient summation over large numbers of nodes in the networks. We present experiments that show signiﬁcant improvements over existing techniques. 1</p><p>Reference: <a title="nips-2005-121-reference" href="../nips2005_reference/nips-2005-Location-based_activity_recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We show how to extract and label a person’s activities and signiﬁcant places from traces of GPS data. [sent-2, score-0.696]
</p><p>2 In contrast to existing techniques, our approach simultaneously detects and classiﬁes the signiﬁcant locations of a person and takes the highlevel context into account. [sent-3, score-0.204]
</p><p>3 Our system uses relational Markov networks to represent the hierarchical activity model that encodes the complex relations among GPS readings, activities and signiﬁcant places. [sent-4, score-0.672]
</p><p>4 We apply FFT-based message passing to perform efﬁcient summation over large numbers of nodes in the networks. [sent-5, score-0.402]
</p><p>5 Such data is used to recognize the high-level activities in which a person is engaged and to determine the relationship between activities and locations that are important to the user [1, 6, 8, 3]. [sent-9, score-1.022]
</p><p>6 Our goal is to segment the user’s day into everyday activities such as “working,” “visiting,” “travel,” and to recognize and label signiﬁcant locations that are associated with one or more activity, such as “work place,” “friend’s house,” “user’s bus stop. [sent-10, score-0.656]
</p><p>7 Some signiﬁcant locations, for example, the place where the user drops off his children at school, may be visited only brieﬂy, and so would be excluded by a high threshold. [sent-16, score-0.247]
</p><p>8 A lower threshold, however, would include too many insigniﬁcant locations, for example, a place where the user brieﬂy waited at a trafﬁc light. [sent-17, score-0.216]
</p><p>9 Second, concerns for computational efﬁciency prevented previous approaches from tackling the problem of activity and place labeling in full generality. [sent-19, score-0.446]
</p><p>10 [1] does not distinguish between places and activities; although [8] does, the implementation limited places to a single activity. [sent-20, score-0.548]
</p><p>11 Neither approaches model or label the user’s activities when moving between places. [sent-21, score-0.375]
</p><p>12 [6] and [3] learn transportation patterns, but not place labels. [sent-22, score-0.203]
</p><p>13 For a simple example, if a system could learn that a person rarely went to a restaurant more than once a day, then it could correctly give a low probability to an interpretation of a day’s data under which the user went to three restaurants. [sent-25, score-0.316]
</p><p>14 Our previous work [8] used clique templates in relational Markov networks for concisely expressing global features, but the MCMC inference algorithm we used made it costly to reason with aggregate features, such as statistics on the number of times a given activity occurs. [sent-26, score-0.73]
</p><p>15 The ability to efﬁciently leverage global features of the data stream could enhance the scope and accuracy of activity recognition. [sent-27, score-0.286]
</p><p>16 This paper presents a uniﬁed approach to automated activity and place labeling which overcomes these limitations. [sent-28, score-0.446]
</p><p>17 Contributions of this work include the following: • We show how to simultaneously solve the tasks of identifying signiﬁcant locations and labeling both places and activities from raw GPS data, all in a conditionally trained relational Markov network. [sent-29, score-0.948]
</p><p>18 Our approach is notable in that nodes representing signiﬁcant places are dynamically added to the graph during inference. [sent-30, score-0.372]
</p><p>19 No arbitrary thresholds regarding the time spent at a location or the number of signiﬁcant places are employed. [sent-31, score-0.342]
</p><p>20 • Our model creates a complete interpretation of the log of a user’s data, including transportation activities as well as activities performed at particular places. [sent-32, score-0.775]
</p><p>21 It allows different kinds of activities to be performed at the same location. [sent-33, score-0.347]
</p><p>22 • We extend our work on using clique templates for global features to support efﬁcient inference by belief propagation. [sent-34, score-0.43]
</p><p>23 We introduce, in particular, specialized Fast Fourier Transform (FFT) templates for belief propagation over aggregate (counting) features, which reduce computation time by an exponential amount. [sent-35, score-0.237]
</p><p>24 We begin with a discussion of relational Markov networks and a description of an FFT belief propagation algorithm for aggregate statistical features. [sent-38, score-0.274]
</p><p>25 RMNs extend CRFs by providing a relational language for describing clique structures and enforcing parameter sharing at the template level. [sent-44, score-0.392]
</p><p>26 Thereby RMNs provide a very ﬂexible and concise framework for deﬁning the features we use in our activity recognition context. [sent-45, score-0.28]
</p><p>27 A key concept of RMNs are relational clique templates, which specify the structure of a CRF in a concise way. [sent-46, score-0.33]
</p><p>28 In a nutshell, a clique template C ∈ C is similar to a database query (e. [sent-47, score-0.269]
</p><p>29 Each clique template C is additionally associated with a potential function φC (vC ) that maps values of variables to a non-negative real number. [sent-50, score-0.269]
</p><p>30 To compute such a conditional distribution, the RMN generates a CRF with the cliques speciﬁed by the clique templates. [sent-53, score-0.321]
</p><p>31 All cliques that originate from the same template must share the same weight vector wC . [sent-54, score-0.234]
</p><p>32 In the context of place labeling, [8] showed how to use non-zero mean priors in order to transfer weights learned for one person to another person. [sent-60, score-0.244]
</p><p>33 Performing aggregation would require the generation of cliques that contain all nodes over which the aggregation is performed. [sent-68, score-0.379]
</p><p>34 2  Efﬁcient summation templates  In our model, we address the inference of aggregate cliques at the template level within the framework of BP. [sent-71, score-0.643]
</p><p>35 Each type of aggregation function is associated with a computation template that speciﬁes how to propagate messages through the clique. [sent-72, score-0.284]
</p><p>36 To handle summation cliques with potentially large numbers of addends, our summation template dynamically builds a summation tree, which is a pairwise Markov network as shown in Fig. [sent-74, score-0.78]
</p><p>37 In a summation tree, the leaves are the original addends and each  (a)  p1  Place a1  Activity Local evidence  a2  p2  . [sent-76, score-0.219]
</p><p>38 eE 1  e1 N  eE N  Figure 1: (a) Summation tree that represents ysum = yi , where the Si ’s are auxiliary nodes to ensure the summation relation. [sent-92, score-0.503]
</p><p>39 Each activity node ai is connected to E observed local evidence nodes e1 to eE . [sent-94, score-0.353]
</p><p>40 Place nodes pi are generated based on the i i inferred activities and each place is connected to all activity nodes that are within a certain distance. [sent-95, score-0.867]
</p><p>41 8 i=1  internal node yjk represents the sum of its two children yj and yk , and this sum relation is encoded by an auxiliary node Sjk and its potential. [sent-96, score-0.335]
</p><p>42 It is easy to see that the n summation tree guarantees that the root represents ysum = i=1 yi , where y1 to yn are the leaves of the tree. [sent-98, score-0.363]
</p><p>43 To deﬁne the BP protocol for summation trees, we need to specify two types of messages: an upward message from an auxiliary node to its parent (e. [sent-99, score-0.48]
</p><p>44 , mS12 y12 ), and a downward message from an auxiliary node to one of its two children (e. [sent-101, score-0.33]
</p><p>45 Upward message update: Starting with Equation (3), we can update an upward message mSij yij as follows. [sent-104, score-0.461]
</p><p>46 mSij yij (yij ) = φS (yi , yj , yij ) myi Sij (yi ) myj Sij (yj ) yi ,yj  myi Sij (yi ) myj Sij (yij − yi )  =  (4)  yi  = F −1 F(myi Sij (yi )) · F(myj Sij (yj ))  (5)  where φS (yi , yj , yij ) is the local potential of Sij encoding the equality yij = yi + yj . [sent-105, score-1.795]
</p><p>47 Therefore, message mSij yij is the convolution of myi Sij and myj Sij . [sent-107, score-0.523]
</p><p>48 The computational complexity of one summation using FFT is O(k log k), where k is the maximum number of states in yi and yj . [sent-110, score-0.415]
</p><p>49 Downward message update: We also allow messages to pass from sum variables downward to its children. [sent-111, score-0.328]
</p><p>50 From Equation (3) we get the downward message mSij yi as mSij yi (yi ) = φS (yi , yj , yij )myj Sij (yj )myij Sij (yij ) yj ,yij  myj Sij (yj )myij Sij (yi + yj )  (6)  = F −1 F(myj Sij (yj )) · F(myij Sij (yij ))  (7)  = yj  where (6) again follows from the sum relation. [sent-117, score-1.209]
</p><p>51 Note that the downward message mSij yi turns out to be the correlation of messages myj Sij and myij Sij . [sent-118, score-0.657]
</p><p>52 At each level of a summation tree, the number of messages (nodes) is reduced by half and the size of each message is doubled. [sent-121, score-0.428]
</p><p>53 Suppose the tree has n upward messages at the bottom and the maximum size of a message is k . [sent-122, score-0.365]
</p><p>54 Therefore, updating all messages in a summation clique takes O(n log2 n) instead of time exponential in n, as would be the case for a non-specialized implementation of aggregation. [sent-124, score-0.484]
</p><p>55 1  Location-based Activity Model Overview  To recognize activities and places, we ﬁrst segment raw GPS traces by grouping consecutive GPS readings based on their spatial relationship. [sent-126, score-0.491]
</p><p>56 In this section, we focus on inferring activities and types of signiﬁcant places after segmentation. [sent-130, score-0.621]
</p><p>57 To do so, we construct a hierarchical RMN that explicitly encodes the relations between activities and places. [sent-131, score-0.347]
</p><p>58 At the lower level of the hierarchy, each activity node is connected to various features, summarizing information resulting from the GPS segmentation. [sent-134, score-0.255]
</p><p>59 These features measure compatibility between types of activities at neighboring nodes in the trace. [sent-136, score-0.494]
</p><p>60 Our model also aims at determining those places that play a signiﬁcant role in the activities of a person, such as home, work place, friend’s home, grocery stores, restaurants, and bus stops. [sent-137, score-0.703]
</p><p>61 Such signiﬁcant places comprise the upper level of the CRF shown in Fig. [sent-138, score-0.274]
</p><p>62 However, since these places are not known a priori, we must additionally detect a person’s signiﬁcant places. [sent-140, score-0.274]
</p><p>63 To incorporate place detection into our system, we use an iterative algorithm that re-estimates activities and places. [sent-141, score-0.469]
</p><p>64 Before we describe this algorithm, let us ﬁrst look at the features that are used to determine the types of signiﬁcant places under the assumption that the locations and number of these places are known. [sent-142, score-0.679]
</p><p>65 • The activities that occur at a place strongly indicate the type of the place. [sent-143, score-0.469]
</p><p>66 Our features consider the frequencies of the different activities at a place. [sent-145, score-0.396]
</p><p>67 This is done by generating a clique for each place that contains all activity nodes in its vicinity. [sent-146, score-0.6]
</p><p>68 We add two additional summation cliques that count the number of homes and work places. [sent-150, score-0.39]
</p><p>69 a 0 := BP inference( CRF0 ) // infer sequence of activities 5. [sent-181, score-0.347]
</p><p>70 , pK i := generate places(a∗ i−1 ) // Instantiate places 8. [sent-187, score-0.315]
</p><p>71 return a∗ , p∗ i i Table 1: Algorithm for extracting and labeling activities and signiﬁcant places. [sent-202, score-0.497]
</p><p>72 Note that the above two types of aggregation features can generate large cliques in the CRF, which could make standard inference intractable. [sent-203, score-0.369]
</p><p>73 In our inference, we use the optimized summation templates discussed in Section 2. [sent-204, score-0.268]
</p><p>74 2  Place Detection and Labeling Algorithm  Table 1 summarizes our algorithm for efﬁciently constructing a CRF that jointly estimates a person’s activities and the types of his signiﬁcant places. [sent-207, score-0.347]
</p><p>75 In Step 2 and 3, this trace is segmented into activities ai and their local evidence ej , which are then used to generate CRF0 without signiﬁcant places. [sent-209, score-0.419]
</p><p>76 BP inference i is ﬁrst performed in this CRF so as to determine the activity estimate a∗ 0 , which consists of a sequence of locations and the most likely activity performed at that location (Step 4). [sent-210, score-0.589]
</p><p>77 This is done by classifying individual activities in the sequence according to whether or not they belong to a signiﬁcant place. [sent-212, score-0.347]
</p><p>78 All instances at which a signiﬁcant activity occurs generate a place node. [sent-214, score-0.365]
</p><p>79 Because a place can be visited multiple times within a sequence, we perform clustering and merge duplicate places into the same place node. [sent-215, score-0.518]
</p><p>80 These places are added to the model and BP is performed in this complete CRF. [sent-217, score-0.274]
</p><p>81 Since a CRFi can have a different structure than the previous CRFi−1 , it might generate a different activity sequence. [sent-218, score-0.243]
</p><p>82 If this is the case, the algorithm returns to Step 5 and re-generates the set of places using the improved activity sequence. [sent-219, score-0.476]
</p><p>83 Extracting signiﬁcant places We compare our model with a widely-used approach that uses a time threshold to determine whether or not a location is signiﬁcant [1, 6, 8, 3]. [sent-226, score-0.336]
</p><p>84 1 minute to 10 minutes, and we measure the false positive and false negative locations extracted from the GPS traces. [sent-230, score-0.281]
</p><p>85 2(a), any ﬁxed threshold is not satisfactory: low thresholds have many false positives, and high thresholds result in many false negatives. [sent-232, score-0.26]
</p><p>86 In contrast, our model performs much better: it only generates 4 false positives and 3 false negative. [sent-233, score-0.209]
</p><p>87 Labeling places and activities In our system the labels of activities generate instances of places, which then help to better estimate the activities occurring in their spatial area. [sent-235, score-1.386]
</p><p>88 The results are given with and without taking the detected places into account. [sent-237, score-0.274]
</p><p>89 More speciﬁcally, without places are results achieved by CRF0 generated by Step 4 of the algorithm in Table 1, and results with places are those achieved after model convergence. [sent-238, score-0.548]
</p><p>90 Second, performing joint inference over activities and places increases the quality of inference. [sent-242, score-0.688]
</p><p>91 The reason for this is that a place node connects all the activities occurring in its spatial area so that these activities can be labeled in a more consistent way. [sent-243, score-0.925]
</p><p>92 A further evaluation of the detected places showed that our system achieved 90. [sent-244, score-0.274]
</p><p>93 6% accuracy in place detection and labeling (see [7] for more results). [sent-245, score-0.279]
</p><p>94 Efﬁciency of inference We compared our optimized BP algorithm using FFT summation cliques with inference based on MCMC and regular BP, using the model and data from [8]. [sent-246, score-0.459]
</p><p>95 As can be seen, naive  BP becomes extremely slow for only 20 nodes, MCMC only works for up to 500 nodes, while our algorithm can perform summation for 2, 000 variables within a few minutes. [sent-252, score-0.206]
</p><p>96 Furthermore, once these locations are determined, they help to better detect low-level activities occurring in their vicinity. [sent-256, score-0.459]
</p><p>97 Summation cliques are extremely important to introduce long-term, soft constraints into activity recognition. [sent-257, score-0.345]
</p><p>98 We show how to incorporate such cliques into belief propagation using bi-directional FFT computations. [sent-258, score-0.22]
</p><p>99 The clique templates of RMNs are well suited to specify such clique-speciﬁc inference mechanisms and we are developing additional techniques, including clique-speciﬁc MCMC and local dynamic programming. [sent-259, score-0.331]
</p><p>100 We demonstrate that the model can be trained from a group of persons and then applied successfully to a different person, achieving more than 85% accuracy in determining low-level activities and above 90% accuracy in detecting and labeling signiﬁcant places. [sent-261, score-0.587]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('activities', 0.347), ('places', 0.274), ('crf', 0.26), ('sij', 0.209), ('activity', 0.202), ('summation', 0.182), ('clique', 0.178), ('gps', 0.169), ('bp', 0.158), ('myj', 0.149), ('cliques', 0.143), ('yij', 0.136), ('fft', 0.13), ('yj', 0.127), ('messages', 0.124), ('relational', 0.123), ('message', 0.122), ('person', 0.122), ('labeling', 0.122), ('place', 0.122), ('msij', 0.111), ('rmn', 0.111), ('rmns', 0.111), ('cant', 0.108), ('yi', 0.106), ('nodes', 0.098), ('user', 0.094), ('template', 0.091), ('ee', 0.088), ('templates', 0.086), ('false', 0.085), ('bus', 0.082), ('downward', 0.082), ('locations', 0.082), ('fourier', 0.082), ('transportation', 0.081), ('upward', 0.081), ('crfi', 0.074), ('myi', 0.074), ('myij', 0.074), ('aggregate', 0.074), ('aggregation', 0.069), ('inference', 0.067), ('homes', 0.065), ('signi', 0.061), ('fox', 0.059), ('liao', 0.059), ('wc', 0.059), ('day', 0.059), ('mcmc', 0.055), ('vc', 0.055), ('node', 0.053), ('belief', 0.05), ('features', 0.049), ('transform', 0.049), ('fc', 0.048), ('persons', 0.048), ('instantiate', 0.048), ('crfs', 0.048), ('traces', 0.047), ('home', 0.044), ('convolution', 0.042), ('auxiliary', 0.042), ('generate', 0.041), ('positives', 0.039), ('readings', 0.039), ('tree', 0.038), ('addends', 0.037), ('friend', 0.037), ('leisure', 0.037), ('pickup', 0.037), ('went', 0.037), ('ysum', 0.037), ('car', 0.037), ('location', 0.036), ('accuracy', 0.035), ('markov', 0.034), ('pk', 0.034), ('thresholds', 0.032), ('geographic', 0.032), ('sleep', 0.032), ('sjk', 0.032), ('trace', 0.031), ('children', 0.031), ('occurring', 0.03), ('recognize', 0.03), ('week', 0.029), ('minute', 0.029), ('concise', 0.029), ('mb', 0.029), ('taskar', 0.029), ('yjk', 0.029), ('segment', 0.028), ('extracting', 0.028), ('label', 0.028), ('propagation', 0.027), ('threshold', 0.026), ('connects', 0.026), ('restaurant', 0.026), ('naive', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="121-tfidf-1" href="./nips-2005-Location-based_activity_recognition.html">121 nips-2005-Location-based activity recognition</a></p>
<p>Author: Lin Liao, Dieter Fox, Henry Kautz</p><p>Abstract: Learning patterns of human behavior from sensor data is extremely important for high-level activity inference. We show how to extract and label a person’s activities and signiﬁcant places from traces of GPS data. In contrast to existing techniques, our approach simultaneously detects and classiﬁes the signiﬁcant locations of a person and takes the highlevel context into account. Our system uses relational Markov networks to represent the hierarchical activity model that encodes the complex relations among GPS readings, activities and signiﬁcant places. We apply FFT-based message passing to perform efﬁcient summation over large numbers of nodes in the networks. We present experiments that show signiﬁcant improvements over existing techniques. 1</p><p>2 0.10979994 <a title="121-tfidf-2" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>Author: Benjamin V. Roy, Ciamac C. Moallemi</p><p>Abstract: We propose consensus propagation, an asynchronous distributed protocol for averaging numbers across a network. We establish convergence, characterize the convergence rate for regular graphs, and demonstrate that the protocol exhibits better scaling properties than pairwise averaging, an alternative that has received much recent attention. Consensus propagation can be viewed as a special case of belief propagation, and our results contribute to the belief propagation literature. In particular, beyond singly-connected graphs, there are very few classes of relevant problems for which belief propagation is known to converge. 1</p><p>3 0.099007986 <a title="121-tfidf-3" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>Author: O. P. Kreidl, Alan S. Willsky</p><p>Abstract: Given a directed graphical model with binary-valued hidden nodes and real-valued noisy observations, consider deciding upon the maximum a-posteriori (MAP) or the maximum posterior-marginal (MPM) assignment under the restriction that each node broadcasts only to its children exactly one single-bit message. We present a variational formulation, viewing the processing rules local to all nodes as degrees-of-freedom, that minimizes the loss in expected (MAP or MPM) performance subject to such online communication constraints. The approach leads to a novel message-passing algorithm to be executed ofﬂine, or before observations are realized, which mitigates the performance loss by iteratively coupling all rules in a manner implicitly driven by global statistics. We also provide (i) illustrative examples, (ii) assumptions that guarantee convergence and efﬁciency and (iii) connections to active research areas. 1</p><p>4 0.096585982 <a title="121-tfidf-4" href="./nips-2005-Message_passing_for_task_redistribution_on_sparse_graphs.html">125 nips-2005-Message passing for task redistribution on sparse graphs</a></p>
<p>Author: K. Wong, Zhuo Gao, David Tax</p><p>Abstract: The problem of resource allocation in sparse graphs with real variables is studied using methods of statistical physics. An efﬁcient distributed algorithm is devised on the basis of insight gained from the analysis and is examined using numerical simulations, showing excellent performance and full agreement with the theoretical results.</p><p>5 0.086989082 <a title="121-tfidf-5" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>Author: Ben Taskar, Simon Lacoste-Julian, Michael I. Jordan</p><p>Abstract: We present a simple and scalable algorithm for large-margin estimation of structured models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem and apply the extragradient method, yielding an algorithm with linear convergence using simple gradient and projection calculations. The projection step can be solved using combinatorial algorithms for min-cost quadratic ﬂow. This makes the approach an efﬁcient alternative to formulations based on reductions to a quadratic program (QP). We present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm. 1</p><p>6 0.084142514 <a title="121-tfidf-6" href="./nips-2005-Fast_Information_Value_for_Graphical_Models.html">70 nips-2005-Fast Information Value for Graphical Models</a></p>
<p>7 0.081774928 <a title="121-tfidf-7" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>8 0.070906222 <a title="121-tfidf-8" href="./nips-2005-Searching_for_Character_Models.html">171 nips-2005-Searching for Character Models</a></p>
<p>9 0.068612292 <a title="121-tfidf-9" href="./nips-2005-Norepinephrine_and_Neural_Interrupts.html">141 nips-2005-Norepinephrine and Neural Interrupts</a></p>
<p>10 0.061002966 <a title="121-tfidf-10" href="./nips-2005-Modeling_Neural_Population_Spiking_Activity_with_Gibbs_Distributions.html">129 nips-2005-Modeling Neural Population Spiking Activity with Gibbs Distributions</a></p>
<p>11 0.059699398 <a title="121-tfidf-11" href="./nips-2005-Multiple_Instance_Boosting_for_Object_Detection.html">131 nips-2005-Multiple Instance Boosting for Object Detection</a></p>
<p>12 0.058876272 <a title="121-tfidf-12" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>13 0.058272876 <a title="121-tfidf-13" href="./nips-2005-Tensor_Subspace_Analysis.html">189 nips-2005-Tensor Subspace Analysis</a></p>
<p>14 0.057196438 <a title="121-tfidf-14" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>15 0.056072447 <a title="121-tfidf-15" href="./nips-2005-Walk-Sum_Interpretation_and_Analysis_of_Gaussian_Belief_Propagation.html">204 nips-2005-Walk-Sum Interpretation and Analysis of Gaussian Belief Propagation</a></p>
<p>16 0.055171546 <a title="121-tfidf-16" href="./nips-2005-Sequence_and_Tree_Kernels_with_Statistical_Feature_Mining.html">175 nips-2005-Sequence and Tree Kernels with Statistical Feature Mining</a></p>
<p>17 0.055008616 <a title="121-tfidf-17" href="./nips-2005-Extracting_Dynamical_Structure_Embedded_in_Neural_Activity.html">67 nips-2005-Extracting Dynamical Structure Embedded in Neural Activity</a></p>
<p>18 0.054668687 <a title="121-tfidf-18" href="./nips-2005-Maximum_Margin_Semi-Supervised_Learning_for_Structured_Variables.html">123 nips-2005-Maximum Margin Semi-Supervised Learning for Structured Variables</a></p>
<p>19 0.052432682 <a title="121-tfidf-19" href="./nips-2005-Conditional_Visual_Tracking_in_Kernel_Space.html">45 nips-2005-Conditional Visual Tracking in Kernel Space</a></p>
<p>20 0.051370718 <a title="121-tfidf-20" href="./nips-2005-Estimating_the_wrong_Markov_random_field%3A_Benefits_in_the_computation-limited_setting.html">65 nips-2005-Estimating the wrong Markov random field: Benefits in the computation-limited setting</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.186), (1, 0.007), (2, 0.006), (3, 0.05), (4, -0.037), (5, -0.02), (6, -0.102), (7, 0.17), (8, 0.069), (9, 0.143), (10, 0.047), (11, -0.108), (12, -0.04), (13, -0.016), (14, 0.023), (15, 0.013), (16, -0.01), (17, 0.02), (18, 0.038), (19, -0.027), (20, -0.045), (21, 0.05), (22, 0.065), (23, 0.05), (24, 0.04), (25, -0.034), (26, -0.045), (27, 0.086), (28, -0.035), (29, -0.114), (30, -0.067), (31, -0.074), (32, 0.001), (33, 0.043), (34, -0.04), (35, -0.053), (36, 0.034), (37, 0.022), (38, 0.061), (39, 0.074), (40, 0.007), (41, -0.14), (42, -0.073), (43, -0.072), (44, 0.098), (45, 0.021), (46, 0.138), (47, -0.007), (48, 0.099), (49, -0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95359397 <a title="121-lsi-1" href="./nips-2005-Location-based_activity_recognition.html">121 nips-2005-Location-based activity recognition</a></p>
<p>Author: Lin Liao, Dieter Fox, Henry Kautz</p><p>Abstract: Learning patterns of human behavior from sensor data is extremely important for high-level activity inference. We show how to extract and label a person’s activities and signiﬁcant places from traces of GPS data. In contrast to existing techniques, our approach simultaneously detects and classiﬁes the signiﬁcant locations of a person and takes the highlevel context into account. Our system uses relational Markov networks to represent the hierarchical activity model that encodes the complex relations among GPS readings, activities and signiﬁcant places. We apply FFT-based message passing to perform efﬁcient summation over large numbers of nodes in the networks. We present experiments that show signiﬁcant improvements over existing techniques. 1</p><p>2 0.68049961 <a title="121-lsi-2" href="./nips-2005-Consensus_Propagation.html">46 nips-2005-Consensus Propagation</a></p>
<p>Author: Benjamin V. Roy, Ciamac C. Moallemi</p><p>Abstract: We propose consensus propagation, an asynchronous distributed protocol for averaging numbers across a network. We establish convergence, characterize the convergence rate for regular graphs, and demonstrate that the protocol exhibits better scaling properties than pairwise averaging, an alternative that has received much recent attention. Consensus propagation can be viewed as a special case of belief propagation, and our results contribute to the belief propagation literature. In particular, beyond singly-connected graphs, there are very few classes of relevant problems for which belief propagation is known to converge. 1</p><p>3 0.65779823 <a title="121-lsi-3" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>Author: O. P. Kreidl, Alan S. Willsky</p><p>Abstract: Given a directed graphical model with binary-valued hidden nodes and real-valued noisy observations, consider deciding upon the maximum a-posteriori (MAP) or the maximum posterior-marginal (MPM) assignment under the restriction that each node broadcasts only to its children exactly one single-bit message. We present a variational formulation, viewing the processing rules local to all nodes as degrees-of-freedom, that minimizes the loss in expected (MAP or MPM) performance subject to such online communication constraints. The approach leads to a novel message-passing algorithm to be executed ofﬂine, or before observations are realized, which mitigates the performance loss by iteratively coupling all rules in a manner implicitly driven by global statistics. We also provide (i) illustrative examples, (ii) assumptions that guarantee convergence and efﬁciency and (iii) connections to active research areas. 1</p><p>4 0.55300617 <a title="121-lsi-4" href="./nips-2005-Walk-Sum_Interpretation_and_Analysis_of_Gaussian_Belief_Propagation.html">204 nips-2005-Walk-Sum Interpretation and Analysis of Gaussian Belief Propagation</a></p>
<p>Author: Dmitry Malioutov, Alan S. Willsky, Jason K. Johnson</p><p>Abstract: This paper presents a new framework based on walks in a graph for analysis and inference in Gaussian graphical models. The key idea is to decompose correlations between variables as a sum over all walks between those variables in the graph. The weight of each walk is given by a product of edgewise partial correlations. We provide a walk-sum interpretation of Gaussian belief propagation in trees and of the approximate method of loopy belief propagation in graphs with cycles. This perspective leads to a better understanding of Gaussian belief propagation and of its convergence in loopy graphs. 1</p><p>5 0.54129767 <a title="121-lsi-5" href="./nips-2005-Message_passing_for_task_redistribution_on_sparse_graphs.html">125 nips-2005-Message passing for task redistribution on sparse graphs</a></p>
<p>Author: K. Wong, Zhuo Gao, David Tax</p><p>Abstract: The problem of resource allocation in sparse graphs with real variables is studied using methods of statistical physics. An efﬁcient distributed algorithm is devised on the basis of insight gained from the analysis and is examined using numerical simulations, showing excellent performance and full agreement with the theoretical results.</p><p>6 0.48714599 <a title="121-lsi-6" href="./nips-2005-Fast_Information_Value_for_Graphical_Models.html">70 nips-2005-Fast Information Value for Graphical Models</a></p>
<p>7 0.47273114 <a title="121-lsi-7" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>8 0.45861709 <a title="121-lsi-8" href="./nips-2005-Mixture_Modeling_by_Affinity_Propagation.html">127 nips-2005-Mixture Modeling by Affinity Propagation</a></p>
<p>9 0.41914573 <a title="121-lsi-9" href="./nips-2005-Searching_for_Character_Models.html">171 nips-2005-Searching for Character Models</a></p>
<p>10 0.40804365 <a title="121-lsi-10" href="./nips-2005-A_Hierarchical_Compositional_System_for_Rapid_Object_Detection.html">11 nips-2005-A Hierarchical Compositional System for Rapid Object Detection</a></p>
<p>11 0.3928397 <a title="121-lsi-11" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>12 0.35815653 <a title="121-lsi-12" href="./nips-2005-Laplacian_Score_for_Feature_Selection.html">104 nips-2005-Laplacian Score for Feature Selection</a></p>
<p>13 0.35762298 <a title="121-lsi-13" href="./nips-2005-Recovery_of_Jointly_Sparse_Signals_from_Few_Random_Projections.html">163 nips-2005-Recovery of Jointly Sparse Signals from Few Random Projections</a></p>
<p>14 0.33074799 <a title="121-lsi-14" href="./nips-2005-Separation_of_Music_Signals_by_Harmonic_Structure_Modeling.html">174 nips-2005-Separation of Music Signals by Harmonic Structure Modeling</a></p>
<p>15 0.32738554 <a title="121-lsi-15" href="./nips-2005-Tensor_Subspace_Analysis.html">189 nips-2005-Tensor Subspace Analysis</a></p>
<p>16 0.30808014 <a title="121-lsi-16" href="./nips-2005-Hyperparameter_and_Kernel_Learning_for_Graph_Based_Semi-Supervised_Classification.html">92 nips-2005-Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification</a></p>
<p>17 0.30192134 <a title="121-lsi-17" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<p>18 0.29645395 <a title="121-lsi-18" href="./nips-2005-Conditional_Visual_Tracking_in_Kernel_Space.html">45 nips-2005-Conditional Visual Tracking in Kernel Space</a></p>
<p>19 0.29615688 <a title="121-lsi-19" href="./nips-2005-Factorial_Switching_Kalman_Filters_for_Condition_Monitoring_in_Neonatal_Intensive_Care.html">68 nips-2005-Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care</a></p>
<p>20 0.29174453 <a title="121-lsi-20" href="./nips-2005-Rate_Distortion_Codes_in_Sensor_Networks%3A_A_System-level_Analysis.html">162 nips-2005-Rate Distortion Codes in Sensor Networks: A System-level Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.062), (10, 0.04), (27, 0.042), (31, 0.062), (34, 0.076), (35, 0.324), (39, 0.027), (55, 0.06), (57, 0.03), (69, 0.055), (73, 0.05), (88, 0.062), (91, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79730558 <a title="121-lda-1" href="./nips-2005-Location-based_activity_recognition.html">121 nips-2005-Location-based activity recognition</a></p>
<p>Author: Lin Liao, Dieter Fox, Henry Kautz</p><p>Abstract: Learning patterns of human behavior from sensor data is extremely important for high-level activity inference. We show how to extract and label a person’s activities and signiﬁcant places from traces of GPS data. In contrast to existing techniques, our approach simultaneously detects and classiﬁes the signiﬁcant locations of a person and takes the highlevel context into account. Our system uses relational Markov networks to represent the hierarchical activity model that encodes the complex relations among GPS readings, activities and signiﬁcant places. We apply FFT-based message passing to perform efﬁcient summation over large numbers of nodes in the networks. We present experiments that show signiﬁcant improvements over existing techniques. 1</p><p>2 0.72504002 <a title="121-lda-2" href="./nips-2005-Oblivious_Equilibrium%3A_A_Mean_Field_Approximation_for_Large-Scale_Dynamic_Games.html">142 nips-2005-Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games</a></p>
<p>Author: Gabriel Y. Weintraub, Lanier Benkard, Benjamin Van Roy</p><p>Abstract: We propose a mean-ﬁeld approximation that dramatically reduces the computational complexity of solving stochastic dynamic games. We provide conditions that guarantee our method approximates an equilibrium as the number of agents grow. We then derive a performance bound to assess how well the approximation performs for any given number of agents. We apply our method to an important class of problems in applied microeconomics. We show with numerical experiments that we are able to greatly expand the set of economic problems that can be analyzed computationally. 1</p><p>3 0.68302137 <a title="121-lda-3" href="./nips-2005-Hot_Coupling%3A_A_Particle_Approach_to_Inference_and_Normalization_on_Pairwise_Undirected_Graphs.html">90 nips-2005-Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs</a></p>
<p>Author: Firas Hamze, Nando de Freitas</p><p>Abstract: This paper presents a new sampling algorithm for approximating functions of variables representable as undirected graphical models of arbitrary connectivity with pairwise potentials, as well as for estimating the notoriously difﬁcult partition function of the graph. The algorithm ﬁts into the framework of sequential Monte Carlo methods rather than the more widely used MCMC, and relies on constructing a sequence of intermediate distributions which get closer to the desired one. While the idea of using “tempered” proposals is known, we construct a novel sequence of target distributions where, rather than dropping a global temperature parameter, we sequentially couple individual pairs of variables that are, initially, sampled exactly from a spanning tree of the variables. We present experimental results on inference and estimation of the partition function for sparse and densely-connected graphs.</p><p>4 0.44324777 <a title="121-lda-4" href="./nips-2005-Nested_sampling_for_Potts_models.html">133 nips-2005-Nested sampling for Potts models</a></p>
<p>Author: Iain Murray, David MacKay, Zoubin Ghahramani, John Skilling</p><p>Abstract: Nested sampling is a new Monte Carlo method by Skilling [1] intended for general Bayesian computation. Nested sampling provides a robust alternative to annealing-based methods for computing normalizing constants. It can also generate estimates of other quantities such as posterior expectations. The key technical requirement is an ability to draw samples uniformly from the prior subject to a constraint on the likelihood. We provide a demonstration with the Potts model, an undirected graphical model. 1</p><p>5 0.43965703 <a title="121-lda-5" href="./nips-2005-Fast_Online_Policy_Gradient_Learning_with_SMD_Gain_Vector_Adaptation.html">72 nips-2005-Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation</a></p>
<p>Author: Jin Yu, Douglas Aberdeen, Nicol N. Schraudolph</p><p>Abstract: Reinforcement learning by direct policy gradient estimation is attractive in theory but in practice leads to notoriously ill-behaved optimization problems. We improve its robustness and speed of convergence with stochastic meta-descent, a gain vector adaptation method that employs fast Hessian-vector products. In our experiments the resulting algorithms outperform previously employed online stochastic, ofﬂine conjugate, and natural policy gradient methods. 1</p><p>6 0.43850291 <a title="121-lda-6" href="./nips-2005-Nearest_Neighbor_Based_Feature_Selection_for_Regression_and_its_Application_to_Neural_Activity.html">132 nips-2005-Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity</a></p>
<p>7 0.43844721 <a title="121-lda-7" href="./nips-2005-Preconditioner_Approximations_for_Probabilistic_Graphical_Models.html">154 nips-2005-Preconditioner Approximations for Probabilistic Graphical Models</a></p>
<p>8 0.43157351 <a title="121-lda-8" href="./nips-2005-Off-policy_Learning_with_Options_and_Recognizers.html">144 nips-2005-Off-policy Learning with Options and Recognizers</a></p>
<p>9 0.43110844 <a title="121-lda-9" href="./nips-2005-Inference_with_Minimal_Communication%3A_a_Decision-Theoretic_Variational_Approach.html">96 nips-2005-Inference with Minimal Communication: a Decision-Theoretic Variational Approach</a></p>
<p>10 0.42964885 <a title="121-lda-10" href="./nips-2005-Size_Regularized_Cut_for_Data_Clustering.html">177 nips-2005-Size Regularized Cut for Data Clustering</a></p>
<p>11 0.42663217 <a title="121-lda-11" href="./nips-2005-Structured_Prediction_via_the_Extragradient_Method.html">184 nips-2005-Structured Prediction via the Extragradient Method</a></p>
<p>12 0.42524096 <a title="121-lda-12" href="./nips-2005-Variable_KD-Tree_Algorithms_for_Spatial_Pattern_Search_and_Discovery.html">200 nips-2005-Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery</a></p>
<p>13 0.4249405 <a title="121-lda-13" href="./nips-2005-From_Weighted_Classification_to_Policy_Search.html">78 nips-2005-From Weighted Classification to Policy Search</a></p>
<p>14 0.42486286 <a title="121-lda-14" href="./nips-2005-Kernelized_Infomax_Clustering.html">102 nips-2005-Kernelized Infomax Clustering</a></p>
<p>15 0.42098525 <a title="121-lda-15" href="./nips-2005-A_Domain_Decomposition_Method_for_Fast_Manifold_Learning.html">9 nips-2005-A Domain Decomposition Method for Fast Manifold Learning</a></p>
<p>16 0.4198643 <a title="121-lda-16" href="./nips-2005-Policy-Gradient_Methods_for_Planning.html">153 nips-2005-Policy-Gradient Methods for Planning</a></p>
<p>17 0.4197199 <a title="121-lda-17" href="./nips-2005-Soft_Clustering_on_Graphs.html">178 nips-2005-Soft Clustering on Graphs</a></p>
<p>18 0.41904348 <a title="121-lda-18" href="./nips-2005-Faster_Rates_in_Regression_via_Active_Learning.html">74 nips-2005-Faster Rates in Regression via Active Learning</a></p>
<p>19 0.41889054 <a title="121-lda-19" href="./nips-2005-Consistency_of_one-class_SVM_and_related_algorithms.html">47 nips-2005-Consistency of one-class SVM and related algorithms</a></p>
<p>20 0.41876003 <a title="121-lda-20" href="./nips-2005-Comparing_the_Effects_of_Different_Weight_Distributions_on_Finding_Sparse_Representations.html">43 nips-2005-Comparing the Effects of Different Weight Distributions on Finding Sparse Representations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
