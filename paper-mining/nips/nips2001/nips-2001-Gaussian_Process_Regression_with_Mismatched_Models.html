<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>79 nips-2001-Gaussian Process Regression with Mismatched Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-79" href="#">nips2001-79</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>79 nips-2001-Gaussian Process Regression with Mismatched Models</h1>
<br/><p>Source: <a title="nips-2001-79-pdf" href="http://papers.nips.cc/paper/1987-gaussian-process-regression-with-mismatched-models.pdf">pdf</a></p><p>Author: Peter Sollich</p><p>Abstract: Learning curves for Gaussian process regression are well understood when the 'student' model happens to match the 'teacher' (true data generation process). I derive approximations to the learning curves for the more generic case of mismatched models, and find very rich behaviour: For large input space dimensionality, where the results become exact, there are universal (student-independent) plateaux in the learning curve, with transitions in between that can exhibit arbitrarily many over-fitting maxima; over-fitting can occur even if the student estimates the teacher noise level correctly. In lower dimensions, plateaux also appear, and the learning curve remains dependent on the mismatch between student and teacher even in the asymptotic limit of a large number of training examples. Learning with excessively strong smoothness assumptions can be particularly dangerous: For example, a student with a standard radial basis function covariance function will learn a rougher teacher function only logarithmically slowly. All predictions are confirmed by simulations. 1</p><p>Reference: <a title="nips-2001-79-reference" href="../nips2001_reference/nips-2001-Gaussian_Process_Regression_with_Mismatched_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract Learning curves for Gaussian process regression are well understood when the 'student' model happens to match the 'teacher' (true data generation process). [sent-6, score-0.177]
</p><p>2 In lower dimensions, plateaux also appear, and the learning curve remains dependent on the mismatch between student and teacher even in the asymptotic limit of a large number of training examples. [sent-8, score-1.229]
</p><p>3 Learning with excessively strong smoothness assumptions can be particularly dangerous: For example, a student with a standard radial basis function covariance function will learn a rougher teacher function only logarithmically slowly. [sent-9, score-1.24]
</p><p>4 1  Introduction  There has in the last few years been a good deal of excitement about the use of Gaussian processes (GPs) as an alternative to feedforward networks [1]. [sent-11, score-0.088]
</p><p>5 GPs make prior assumptions about the problem to be learned very transparent, and even though they are non-parametric models, inference- at least in the case of regression considered below- is relatively straightforward. [sent-12, score-0.182]
</p><p>6 how many training examples are needed to achieve a certain level of generalization performance. [sent-15, score-0.154]
</p><p>7 The typical (as opposed to worst case) behaviour is captured in the learning curve, which gives the average generalization error t as a function of the number of training examples n. [sent-16, score-0.302]
</p><p>8 Good bounds and approximations for t(n) are now available [1, 2, 3, 4, 5], but these are mostly restricted to the case where the 'student' model exactly matches the true 'teacher' generating the datal. [sent-17, score-0.143]
</p><p>9 In practice, such a match is unlikely, and so it is lThe exception is the elegant work of Malzahn and Opper [2], which uses a statistical physics framework to derive approximate learning curves that also apply for any fixed target function. [sent-18, score-0.186]
</p><p>10 However, this framework has not yet to my knowledge been exploited to  important to understand how GPs learn if there is some model mismatch. [sent-19, score-0.084]
</p><p>11 In its simplest form , the regression problem is this: We are trying to learn a function B* which maps inputs x (real-valued vectors) to (real-valued scalar) outputs B*(x). [sent-21, score-0.322]
</p><p>12 We are given a set of training data D , consisting of n input-output pairs (xl, yl) ; the training outputs yl may differ from the 'clean' teacher outputs B*(x l ) due to corruption by noise. [sent-22, score-1.057]
</p><p>13 Given a test input x, we are then asked to come up with a prediction B(x), plus error bar, for the corresponding output B(x). [sent-23, score-0.136]
</p><p>14 In a Bayesian setting, we do this by specifying a prior P(B) over hypothesis functions , and a likelihood P(DIB) with which each B could have generated the training data; from this we deduce the posterior distribution P(BID) ex P(DIB)P(B). [sent-24, score-0.384]
</p><p>15 For a GP, the prior is defined directly over input-output functions B; this is simpler than for a Bayesian feedforward net since no weights are involved which would have to be integrated out. [sent-25, score-0.209]
</p><p>16 Any B is uniquely determined by its output values B(x) for all x from the input domain, and for a GP, these are assumed to have a joint Gaussian distribution (hence the name). [sent-26, score-0.133]
</p><p>17 If we set the means to zero as is commonly done, this distribution is fully specified by the covariance function (B(x)B(xl))o = C(X,XI). [sent-27, score-0.087]
</p><p>18 The latter transparently encodes prior assumptions about the function to be learned. [sent-28, score-0.171]
</p><p>19 Here I is a lengthscale parameter, corresponding directly to the distance in input space over which we expect significant variation in the function values. [sent-30, score-0.057]
</p><p>20 1  There are good reviews on how inference with GPs works [1 , 6], so I only give a brief summary here. [sent-31, score-0.036]
</p><p>21 The student assumes that outputs y are generated from the 'clean' values of a hypothesis function B(x) by adding Gaussian noise of xindependent variance (J2. [sent-32, score-0.644]
</p><p>22 where in the last expression I have replaced the average over D by one over the training inputs since the outputs no longer appear. [sent-36, score-0.364]
</p><p>23 If the student model matches the true teacher model, E and € coincide and give the Bayes error, i. [sent-37, score-1.022]
</p><p>24 the best achievable (average) generalization performance for the given teacher. [sent-39, score-0.081]
</p><p>25 I assume in what follows that the teacher is also a GP, but with a possibly different covariance function C* (x, x') and noise level (}";. [sent-40, score-0.597]
</p><p>26 (3) for E to be simplified, since by exact analogy with the argument for the student posterior (()*(x )k iD = k* (x) TK :;-1 y ,  ((); (x) )O. [sent-42, score-0.579]
</p><p>27 A more convenient starting point is obtained if (using Mercer's theorem) we decompose the covariance function into its eigenfunctions ¢i(X) and eigenvalues Ai, defined w. [sent-45, score-0.297]
</p><p>28 the input distribution so that (C(x, X') ¢i (X') )x' = Ai¢i(X) with the corresponding normalization (¢i(X)¢j(x))x = bij. [sent-48, score-0.057]
</p><p>29 Then 00  00  i=1  i=1  For simplicity I assume here that the student and teacher covariance functions have the same eigenfunctions (but different eigenvalues). [sent-49, score-1.091]
</p><p>30 This is not as restrictive as it may seem; several examples are given below. [sent-50, score-0.034]
</p><p>31 The averages over the test input x in (5) are now easily carried out: E . [sent-51, score-0.057]
</p><p>32 for the last term we need  ((k( x) k(x)T)lm)x =  L AiAj¢i(Xl)(¢i(X)¢j (x))x¢j (xm) = L A7¢i(X l )¢i(Xm) i  ij  Introducing the diagonal eigenvalue matrix (A)ij = Aibij and the 'design matrix ' ( A2 T . [sent-53, score-0.074]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('teacher', 0.441), ('student', 0.441), ('xid', 0.289), ('gps', 0.229), ('xl', 0.202), ('id', 0.183), ('yl', 0.158), ('outputs', 0.132), ('dib', 0.115), ('plateaux', 0.115), ('behaviour', 0.101), ('gp', 0.101), ('bid', 0.1), ('mismatched', 0.1), ('posterior', 0.091), ('covariance', 0.087), ('eigenfunctions', 0.085), ('curve', 0.084), ('inputs', 0.077), ('mismatch', 0.076), ('smoothness', 0.076), ('curves', 0.076), ('prior', 0.075), ('xm', 0.073), ('lm', 0.073), ('training', 0.072), ('tk', 0.07), ('aa', 0.061), ('regression', 0.061), ('matches', 0.059), ('bar', 0.058), ('input', 0.057), ('feedforward', 0.056), ('gaussian', 0.054), ('tr', 0.054), ('learn', 0.052), ('radial', 0.051), ('eigenvalues', 0.051), ('average', 0.051), ('malzahn', 0.05), ('sollich', 0.05), ('strand', 0.05), ('aiaj', 0.05), ('corruption', 0.05), ('teachers', 0.05), ('transparent', 0.05), ('transparently', 0.05), ('woodbury', 0.05), ('yib', 0.05), ('exact', 0.047), ('dangerous', 0.046), ('king', 0.046), ('logarithmically', 0.046), ('lthe', 0.046), ('prediction', 0.046), ('assumptions', 0.046), ('generalization', 0.045), ('true', 0.045), ('giving', 0.044), ('dropping', 0.043), ('ij', 0.042), ('joint', 0.042), ('defined', 0.041), ('london', 0.04), ('infinitely', 0.04), ('match', 0.04), ('hypothesis', 0.039), ('approximations', 0.039), ('deduce', 0.038), ('email', 0.038), ('level', 0.037), ('ai', 0.037), ('functions', 0.037), ('reviews', 0.036), ('subscript', 0.036), ('achievable', 0.036), ('coincide', 0.036), ('elegant', 0.036), ('confirmed', 0.036), ('squared', 0.035), ('name', 0.035), ('opper', 0.035), ('maxima', 0.035), ('universal', 0.034), ('exception', 0.034), ('uniquely', 0.034), ('restrictive', 0.034), ('college', 0.034), ('simplified', 0.033), ('asked', 0.033), ('decompose', 0.033), ('rough', 0.033), ('worst', 0.033), ('noise', 0.032), ('last', 0.032), ('rich', 0.032), ('specifying', 0.032), ('covariances', 0.032), ('mathematics', 0.032), ('exploited', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="79-tfidf-1" href="./nips-2001-Gaussian_Process_Regression_with_Mismatched_Models.html">79 nips-2001-Gaussian Process Regression with Mismatched Models</a></p>
<p>Author: Peter Sollich</p><p>Abstract: Learning curves for Gaussian process regression are well understood when the 'student' model happens to match the 'teacher' (true data generation process). I derive approximations to the learning curves for the more generic case of mismatched models, and find very rich behaviour: For large input space dimensionality, where the results become exact, there are universal (student-independent) plateaux in the learning curve, with transitions in between that can exhibit arbitrarily many over-fitting maxima; over-fitting can occur even if the student estimates the teacher noise level correctly. In lower dimensions, plateaux also appear, and the learning curve remains dependent on the mismatch between student and teacher even in the asymptotic limit of a large number of training examples. Learning with excessively strong smoothness assumptions can be particularly dangerous: For example, a student with a standard radial basis function covariance function will learn a rougher teacher function only logarithmically slowly. All predictions are confirmed by simulations. 1</p><p>2 0.14092274 <a title="79-tfidf-2" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We combine the replica approach from statistical physics with a variational approach to analyze learning curves analytically. We apply the method to Gaussian process regression. As a main result we derive approximative relations between empirical error measures, the generalization error and the posterior variance.</p><p>3 0.13368493 <a title="79-tfidf-3" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>Author: K. Wong, F. Li</p><p>Abstract: We propose a method for the fast estimation of hyperparameters in large networks, based on the linear response relation in the cavity method, and an empirical measurement of the Green's function. Simulation results show that it is efficient and precise, when compared with cross-validation and other techniques which require matrix inversion. 1</p><p>4 0.1227888 <a title="79-tfidf-4" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>Author: Manfred Opper, Robert Urbanczik</p><p>Abstract: Using methods of Statistical Physics, we investigate the rOle of model complexity in learning with support vector machines (SVMs). We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error. Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel. 1</p><p>5 0.12104672 <a title="79-tfidf-5" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>Author: Carl E. Rasmussen, Zoubin Ghahramani</p><p>Abstract: We present an extension to the Mixture of Experts (ME) model, where the individual experts are Gaussian Process (GP) regression models. Using an input-dependent adaptation of the Dirichlet Process, we implement a gating network for an inﬁnite number of Experts. Inference in this model may be done efﬁciently using a Markov Chain relying on Gibbs sampling. The model allows the effective covariance function to vary with the inputs, and may handle large datasets – thus potentially overcoming two of the biggest hurdles with GP models. Simulations show the viability of this approach.</p><p>6 0.079569861 <a title="79-tfidf-6" href="./nips-2001-On_the_Concentration_of_Spectral_Properties.html">136 nips-2001-On the Concentration of Spectral Properties</a></p>
<p>7 0.078257889 <a title="79-tfidf-7" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>8 0.069090091 <a title="79-tfidf-8" href="./nips-2001-Fast%2C_Large-Scale_Transformation-Invariant_Clustering.html">75 nips-2001-Fast, Large-Scale Transformation-Invariant Clustering</a></p>
<p>9 0.068866268 <a title="79-tfidf-9" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>10 0.062992595 <a title="79-tfidf-10" href="./nips-2001-Analysis_of_Sparse_Bayesian_Learning.html">35 nips-2001-Analysis of Sparse Bayesian Learning</a></p>
<p>11 0.061835401 <a title="79-tfidf-11" href="./nips-2001-Rao-Blackwellised_Particle_Filtering_via_Data_Augmentation.html">156 nips-2001-Rao-Blackwellised Particle Filtering via Data Augmentation</a></p>
<p>12 0.061035015 <a title="79-tfidf-12" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>13 0.059010264 <a title="79-tfidf-13" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>14 0.053074773 <a title="79-tfidf-14" href="./nips-2001-Distribution_of_Mutual_Information.html">61 nips-2001-Distribution of Mutual Information</a></p>
<p>15 0.049037535 <a title="79-tfidf-15" href="./nips-2001-Products_of_Gaussians.html">154 nips-2001-Products of Gaussians</a></p>
<p>16 0.048501447 <a title="79-tfidf-16" href="./nips-2001-%28Not%29_Bounding_the_True_Error.html">1 nips-2001-(Not) Bounding the True Error</a></p>
<p>17 0.047039162 <a title="79-tfidf-17" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>18 0.046213683 <a title="79-tfidf-18" href="./nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes.html">133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</a></p>
<p>19 0.045341 <a title="79-tfidf-19" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>20 0.045337129 <a title="79-tfidf-20" href="./nips-2001-The_Noisy_Euclidean_Traveling_Salesman_Problem_and_Learning.html">186 nips-2001-The Noisy Euclidean Traveling Salesman Problem and Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.168), (1, 0.006), (2, -0.01), (3, -0.063), (4, -0.015), (5, -0.06), (6, 0.104), (7, 0.024), (8, 0.006), (9, -0.001), (10, 0.027), (11, 0.113), (12, 0.024), (13, -0.187), (14, -0.037), (15, 0.116), (16, 0.02), (17, 0.009), (18, 0.066), (19, -0.028), (20, -0.053), (21, -0.141), (22, 0.059), (23, 0.121), (24, 0.056), (25, 0.018), (26, -0.016), (27, -0.124), (28, -0.042), (29, 0.201), (30, 0.086), (31, 0.077), (32, -0.049), (33, 0.008), (34, 0.088), (35, 0.081), (36, -0.097), (37, 0.023), (38, 0.0), (39, -0.061), (40, -0.046), (41, 0.164), (42, 0.017), (43, 0.02), (44, -0.054), (45, -0.048), (46, -0.025), (47, 0.008), (48, -0.042), (49, 0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9459002 <a title="79-lsi-1" href="./nips-2001-Gaussian_Process_Regression_with_Mismatched_Models.html">79 nips-2001-Gaussian Process Regression with Mismatched Models</a></p>
<p>Author: Peter Sollich</p><p>Abstract: Learning curves for Gaussian process regression are well understood when the 'student' model happens to match the 'teacher' (true data generation process). I derive approximations to the learning curves for the more generic case of mismatched models, and find very rich behaviour: For large input space dimensionality, where the results become exact, there are universal (student-independent) plateaux in the learning curve, with transitions in between that can exhibit arbitrarily many over-fitting maxima; over-fitting can occur even if the student estimates the teacher noise level correctly. In lower dimensions, plateaux also appear, and the learning curve remains dependent on the mismatch between student and teacher even in the asymptotic limit of a large number of training examples. Learning with excessively strong smoothness assumptions can be particularly dangerous: For example, a student with a standard radial basis function covariance function will learn a rougher teacher function only logarithmically slowly. All predictions are confirmed by simulations. 1</p><p>2 0.77293193 <a title="79-lsi-2" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>Author: K. Wong, F. Li</p><p>Abstract: We propose a method for the fast estimation of hyperparameters in large networks, based on the linear response relation in the cavity method, and an empirical measurement of the Green's function. Simulation results show that it is efficient and precise, when compared with cross-validation and other techniques which require matrix inversion. 1</p><p>3 0.65558112 <a title="79-lsi-3" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We combine the replica approach from statistical physics with a variational approach to analyze learning curves analytically. We apply the method to Gaussian process regression. As a main result we derive approximative relations between empirical error measures, the generalization error and the posterior variance.</p><p>4 0.55663061 <a title="79-lsi-4" href="./nips-2001-Analysis_of_Sparse_Bayesian_Learning.html">35 nips-2001-Analysis of Sparse Bayesian Learning</a></p>
<p>Author: Anita C. Faul, Michael E. Tipping</p><p>Abstract: The recent introduction of the 'relevance vector machine' has effectively demonstrated how sparsity may be obtained in generalised linear models within a Bayesian framework. Using a particular form of Gaussian parameter prior, 'learning' is the maximisation, with respect to hyperparameters, of the marginal likelihood of the data. This paper studies the properties of that objective function, and demonstrates that conditioned on an individual hyperparameter, the marginal likelihood has a unique maximum which is computable in closed form. It is further shown that if a derived 'sparsity criterion' is satisfied, this maximum is exactly equivalent to 'pruning' the corresponding parameter from the model. 1</p><p>5 0.54542428 <a title="79-lsi-5" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>Author: Lehel Csató, Manfred Opper, Ole Winther</p><p>Abstract: The adaptive TAP Gibbs free energy for a general densely connected probabilistic model with quadratic interactions and arbritary single site constraints is derived. We show how a speciﬁc sequential minimization of the free energy leads to a generalization of Minka’s expectation propagation. Lastly, we derive a sparse representation version of the sequential algorithm. The usefulness of the approach is demonstrated on classiﬁcation and density estimation with Gaussian processes and on an independent component analysis problem.</p><p>6 0.49853706 <a title="79-lsi-6" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>7 0.49147919 <a title="79-lsi-7" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>8 0.45894057 <a title="79-lsi-8" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>9 0.45792103 <a title="79-lsi-9" href="./nips-2001-Geometrical_Singularities_in_the_Neuromanifold_of_Multilayer_Perceptrons.html">83 nips-2001-Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons</a></p>
<p>10 0.42583907 <a title="79-lsi-10" href="./nips-2001-Products_of_Gaussians.html">154 nips-2001-Products of Gaussians</a></p>
<p>11 0.41326866 <a title="79-lsi-11" href="./nips-2001-Distribution_of_Mutual_Information.html">61 nips-2001-Distribution of Mutual Information</a></p>
<p>12 0.37672767 <a title="79-lsi-12" href="./nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes.html">133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</a></p>
<p>13 0.37642318 <a title="79-lsi-13" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>14 0.34682861 <a title="79-lsi-14" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<p>15 0.32955888 <a title="79-lsi-15" href="./nips-2001-Improvisation_and_Learning.html">91 nips-2001-Improvisation and Learning</a></p>
<p>16 0.32697022 <a title="79-lsi-16" href="./nips-2001-Fast%2C_Large-Scale_Transformation-Invariant_Clustering.html">75 nips-2001-Fast, Large-Scale Transformation-Invariant Clustering</a></p>
<p>17 0.32168597 <a title="79-lsi-17" href="./nips-2001-Rao-Blackwellised_Particle_Filtering_via_Data_Augmentation.html">156 nips-2001-Rao-Blackwellised Particle Filtering via Data Augmentation</a></p>
<p>18 0.32150772 <a title="79-lsi-18" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>19 0.31119749 <a title="79-lsi-19" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>20 0.30939189 <a title="79-lsi-20" href="./nips-2001-Learning_Body_Pose_via_Specialized_Maps.html">108 nips-2001-Learning Body Pose via Specialized Maps</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.03), (17, 0.04), (19, 0.022), (27, 0.145), (30, 0.05), (43, 0.256), (59, 0.038), (72, 0.119), (79, 0.08), (83, 0.019), (91, 0.117)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92485774 <a title="79-lda-1" href="./nips-2001-Receptive_field_structure_of_flow_detectors_for_heading_perception.html">158 nips-2001-Receptive field structure of flow detectors for heading perception</a></p>
<p>Author: J. A. Beintema, M. Lappe, Alexander C. Berg</p><p>Abstract: Observer translation relative to the world creates image flow that expands from the observer's direction of translation (heading) from which the observer can recover heading direction. Yet, the image flow is often more complex, depending on rotation of the eye, scene layout and translation velocity. A number of models [1-4] have been proposed on how the human visual system extracts heading from flow in a neurophysiologic ally plausible way. These models represent heading by a set of neurons that respond to large image flow patterns and receive input from motion sensed at different image locations. We analysed these models to determine the exact receptive field of these heading detectors. We find most models predict that, contrary to widespread believe, the contribut ing motion sensors have a preferred motion directed circularly rather than radially around the detector's preferred heading. Moreover, the results suggest to look for more refined structure within the circular flow, such as bi-circularity or local motion-opponency.</p><p>2 0.85724163 <a title="79-lda-2" href="./nips-2001-The_Unified_Propagation_and_Scaling_Algorithm.html">188 nips-2001-The Unified Propagation and Scaling Algorithm</a></p>
<p>Author: Yee W. Teh, Max Welling</p><p>Abstract: In this paper we will show that a restricted class of constrained minimum divergence problems, named generalized inference problems, can be solved by approximating the KL divergence with a Bethe free energy. The algorithm we derive is closely related to both loopy belief propagation and iterative scaling. This uniﬁed propagation and scaling algorithm reduces to a convergent alternative to loopy belief propagation when no constraints are present. Experiments show the viability of our algorithm.</p><p>same-paper 3 0.8536374 <a title="79-lda-3" href="./nips-2001-Gaussian_Process_Regression_with_Mismatched_Models.html">79 nips-2001-Gaussian Process Regression with Mismatched Models</a></p>
<p>Author: Peter Sollich</p><p>Abstract: Learning curves for Gaussian process regression are well understood when the 'student' model happens to match the 'teacher' (true data generation process). I derive approximations to the learning curves for the more generic case of mismatched models, and find very rich behaviour: For large input space dimensionality, where the results become exact, there are universal (student-independent) plateaux in the learning curve, with transitions in between that can exhibit arbitrarily many over-fitting maxima; over-fitting can occur even if the student estimates the teacher noise level correctly. In lower dimensions, plateaux also appear, and the learning curve remains dependent on the mismatch between student and teacher even in the asymptotic limit of a large number of training examples. Learning with excessively strong smoothness assumptions can be particularly dangerous: For example, a student with a standard radial basis function covariance function will learn a rougher teacher function only logarithmically slowly. All predictions are confirmed by simulations. 1</p><p>4 0.66923732 <a title="79-lda-4" href="./nips-2001-Tree-based_reparameterization_for_approximate_inference_on_loopy_graphs.html">192 nips-2001-Tree-based reparameterization for approximate inference on loopy graphs</a></p>
<p>Author: Martin J. Wainwright, Tommi Jaakkola, Alan S. Willsky</p><p>Abstract: We develop a tree-based reparameterization framework that provides a new conceptual view of a large class of iterative algorithms for computing approximate marginals in graphs with cycles. It includes belief propagation (BP), which can be reformulated as a very local form of reparameterization. More generally, we consider algorithms that perform exact computations over spanning trees of the full graph. On the practical side, we find that such tree reparameterization (TRP) algorithms have convergence properties superior to BP. The reparameterization perspective also provides a number of theoretical insights into approximate inference, including a new characterization of fixed points; and an invariance intrinsic to TRP /BP. These two properties enable us to analyze and bound the error between the TRP /BP approximations and the actual marginals. While our results arise naturally from the TRP perspective, most of them apply in an algorithm-independent manner to any local minimum of the Bethe free energy. Our results also have natural extensions to more structured approximations [e.g. , 1, 2]. 1</p><p>5 0.66899252 <a title="79-lda-5" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>Author: Carlotta Domeniconi, Dimitrios Gunopulos</p><p>Abstract: The nearest neighbor technique is a simple and appealing method to address classification problems. It relies on t he assumption of locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with a finite number of examples due to the curse of dimensionality. We propose a technique that computes a locally flexible metric by means of Support Vector Machines (SVMs). The maximum margin boundary found by the SVM is used to determine the most discriminant direction over the query's neighborhood. Such direction provides a local weighting scheme for input features. We present experimental evidence of classification performance improvement over the SVM algorithm alone and over a variety of adaptive learning schemes, by using both simulated and real data sets. 1</p><p>6 0.66540676 <a title="79-lda-6" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>7 0.66189218 <a title="79-lda-7" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>8 0.66124964 <a title="79-lda-8" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>9 0.65886939 <a title="79-lda-9" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>10 0.65837979 <a title="79-lda-10" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>11 0.65716559 <a title="79-lda-11" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>12 0.65711427 <a title="79-lda-12" href="./nips-2001-Distribution_of_Mutual_Information.html">61 nips-2001-Distribution of Mutual Information</a></p>
<p>13 0.65607107 <a title="79-lda-13" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>14 0.65448999 <a title="79-lda-14" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>15 0.65244877 <a title="79-lda-15" href="./nips-2001-Learning_from_Infinite_Data_in_Finite_Time.html">114 nips-2001-Learning from Infinite Data in Finite Time</a></p>
<p>16 0.65177786 <a title="79-lda-16" href="./nips-2001-Quantizing_Density_Estimators.html">155 nips-2001-Quantizing Density Estimators</a></p>
<p>17 0.65085757 <a title="79-lda-17" href="./nips-2001-Multi_Dimensional_ICA_to_Separate_Correlated_Sources.html">127 nips-2001-Multi Dimensional ICA to Separate Correlated Sources</a></p>
<p>18 0.65057719 <a title="79-lda-18" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>19 0.65050459 <a title="79-lda-19" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>20 0.65041149 <a title="79-lda-20" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
