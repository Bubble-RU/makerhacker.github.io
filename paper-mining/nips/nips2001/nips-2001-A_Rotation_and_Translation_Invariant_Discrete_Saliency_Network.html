<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-19" href="#">nips2001-19</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</h1>
<br/><p>Source: <a title="nips-2001-19-pdf" href="http://papers.nips.cc/paper/1991-a-rotation-and-translation-invariant-discrete-saliency-network.pdf">pdf</a></p><p>Author: Lance R. Williams, John W. Zweck</p><p>Abstract: We describe a neural network which enhances and completes salient closed contours. Our work is different from all previous work in three important ways. First, like the input provided to V1 by LGN, the input to our computation is isotropic. That is, the input is composed of spots not edges. Second, our network computes a well deﬁned function of the input based on a distribution of closed contours characterized by a random process. Third, even though our computation is implemented in a discrete network, its output is invariant to continuous rotations and translations of the input pattern.</p><p>Reference: <a title="nips-2001-19-reference" href="../nips2001_reference/nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 of Maryland Baltimore County Baltimore, MD 21250  Abstract We describe a neural network which enhances and completes salient closed contours. [sent-7, score-0.458]
</p><p>2 First, like the input provided to V1 by LGN, the input to our computation is isotropic. [sent-9, score-0.308]
</p><p>3 That is, the input is composed of spots not edges. [sent-10, score-0.38]
</p><p>4 Second, our network computes a well deﬁned function of the input based on a distribution of closed contours characterized by a random process. [sent-11, score-0.434]
</p><p>5 Third, even though our computation is implemented in a discrete network, its output is invariant to continuous rotations and translations of the input pattern. [sent-12, score-0.754]
</p><p>6 1 Introduction There is a long history of research on neural networks inspired by the structure of visual cortex whose functions have been described as contour completion, saliency enhancement, orientation sharpening, or segmentation[6, 7, 8, 9, 12]. [sent-13, score-0.481]
</p><p>7 A similiar network has been proposed as a model of visual hallucinations[1]. [sent-14, score-0.154]
</p><p>8 In this paper, we describe a neural network which enhances and completes salient closed contours. [sent-15, score-0.458]
</p><p>9 First, like the input provided to V1 by LGN, the input to our computation is isotropic. [sent-17, score-0.308]
</p><p>10 That is, the input is composed of spots not edges. [sent-18, score-0.38]
</p><p>11 Second, our network computes a well deﬁned function of the input based on a distribution of closed contours characterized by a random process. [sent-19, score-0.434]
</p><p>12 Third, even though our computation is implemented in a discrete network, its output is invariant to continuous rotations and translations of the input pattern. [sent-20, score-0.754]
</p><p>13 There are two important properties which a computation must possess if it is to be invariant to rotations and translations, i. [sent-21, score-0.324]
</p><p>14 First, the input, the output, and all intermediate representations must be Euclidean invariant. [sent-24, score-0.044]
</p><p>15 Second, all transformations of these representations must also be Euclidean invariant. [sent-25, score-0.074]
</p><p>16 The models described in [6, 7, 8, 9, 12] are not Euclidean invariant, ﬁrst and foremost, because their input representations are not Euclidean invariant. [sent-26, score-0.116]
</p><p>17 That is, not all rotations and translations of the input can be represented equally well. [sent-27, score-0.406]
</p><p>18 This problem is often skirted by researchers by choosing input patterns which match particular choices of sampling rate and phase. [sent-28, score-0.116]
</p><p>19 For example, Li [7] used only six samples in orientation (including ) and Heitger and von der Heydt[5] only twelve (including , and ). [sent-29, score-0.23]
</p><p>20 Li’s ﬁrst test pattern was a dashed line of orientation, , while Heitger and von der Heydt used a Kanizsa Triangle with sides of , , and  ¡ ¢   ¡ £   ¤  ¡ ©   ¡ ¥   § ¨¦  ¡ ¥   ¤  ¡ £   ¡ ¥   ¡ ¥   orientation. [sent-30, score-0.146]
</p><p>21 There is no reason to believe that the experimental results they showed would be similiar if the input patterns were rotated by as little as . [sent-31, score-0.286]
</p><p>22 ¡     § ¨¦  2 A continuum formulation of the saliency problem The following section reviews the continuum formulation of the contour completion and saliency problem as described in Williams and Thornber[11]. [sent-33, score-0.998]
</p><p>23 1 Shape distribution Mumford[3] observed that the probability distribution of object boundary shapes could be modeled by a Fokker-Planck equation of the following form:  3 4¢ ¦  (1)  B &9  E"4D5  2  §  ¨ 0 ¡ 0 ¢ 0 ¡ 1)  7  '(&¥¡ %$" ¥¡ ©§¥¡  #! [sent-35, score-0.078]
</p><p>24 ¨      ¨ ¦ ¤ ¢ £¡ ¢ £¡ ¢ £¡ B C¤  9 7 5 686¢  where is the probability that a particle is located at position, , and is moving in direction, , at time, . [sent-36, score-0.044]
</p><p>25 This partial differential equation can be viewed as a set of independent advection equations in and (the ﬁrst and second terms) coupled in the dimension by the diffusion equation (the third term). [sent-37, score-0.218]
</p><p>26 The advection equations translate probability mass in direction, , with unit speed, while the diffusion term models the Brownian motion in direction, with diffusion parameter, . [sent-38, score-0.323]
</p><p>27 The combined effect of these three terms is that particles tend to travel in straight lines, but over time they drift to the left or right by an amount proportional to . [sent-39, score-0.131]
</p><p>28 Finally, the effect of the fourth term is that particles decay over time, with a half-life given by the decay constant, . [sent-40, score-0.181]
</p><p>29 2 The propagators  B V XW¤  T 7S P H U9 RQI¤  F @ A T 9 7 G85  The Green’s function, , gives the probability that a particle observed at position, , and direction, , at time, , will later be observed at position, , and direction, , at time, . [sent-42, score-0.119]
</p><p>30 1 The short-time propagator:  (2)  (3)   S 7X5 B 685 9 7 g V f¦ e  B T 7S ¥I9 dP 6X5 H c 9 7  gives the probability that and are from the boundary of a single object but are really the same edge. [sent-46, score-0.078]
</p><p>31   The cut-off function is characterized by three parameters, , , and . [sent-48, score-0.046]
</p><p>32 The parameter, , is the scale of the edge detection process. [sent-50, score-0.043]
</p><p>33   d  d        1  We assume that the probability that two edges are the same depends only on the distance between them, and that for particles travelling at unit speed. [sent-51, score-0.154]
</p><p>34 3 Eigenfunctions  B C5 3     The integral linear operator, , combines three sources of information: 1) the probability that two edges belong to the same object; 2) the probability that the two edges are distinct; and 3) the probability that the two edges exist. [sent-53, score-0.225]
</p><p>35 4 Stochastic completion ﬁeld  B T 7S ¥I9 85  The magnitude of the stochastic completion ﬁeld, , equals the probability that a closed contour satisfying a subset of the constraints exists at . [sent-56, score-0.837]
</p><p>36 §¢  in this way is to remove the contribution, , The purpose of writing of closed contours at scales smaller than which would otherwise dominate the completion ﬁeld. [sent-63, score-0.435]
</p><p>37 Given the above expression for the completion ﬁeld, it is clear that the key problem is computing the eigenfunction, , of with largest positive real eigenvalue. [sent-64, score-0.363]
</p><p>38 To accomplish this, we can use the well known power method (see [4]). [sent-65, score-0.08]
</p><p>39 In this case, the power method involves repeated application of the linear operator, , to the function, , followed by normalization:  B3 y5  B3 y5  3  (13)  B C5 3     B T 7S bU9 85  ! [sent-66, score-0.08]
</p><p>40 ¦  B3 y5  ¦  In the limit, as gets very large, converges to the eigenfunction of , with largest positive real eigenvalue. [sent-70, score-0.187]
</p><p>41 We observe that the above computation can be considered a continuous state, discrete time, recurrent neural network. [sent-71, score-0.192]
</p><p>42 3 A discrete implementation of the continuum formulation The continuous functions comprising the state of the computation are represented as weighted sums of a ﬁnite set of shiftable-twistable basis functions. [sent-72, score-0.593]
</p><p>43 The computation we describe is biologically plausible in the sense that all transformations of state are effected by linear transformations (or other vector parallel operations) on the coefﬁcient vectors. [sent-74, score-0.224]
</p><p>44 1 Shiftable-twistable bases The input and output of the above computation are functions deﬁned on the continuous space, , of positions in the plane, , and directions in the circle, . [sent-76, score-0.433]
</p><p>45 For such computations, the important symmetry is determined by those transformations, , of , which perform a shift in by , followed by a twist in through an angle, . [sent-77, score-0.239]
</p><p>46 A twist through an angle, , consists of two parts: (1) a rotation, , of and (2) a translation in , both by . [sent-78, score-0.183]
</p><p>47 Correspondingly, we deﬁne a shiftable-twistable basis 2 of functions on to be a set of functions on with the property that whenever a function, , is in their span, then so is , for every choice of in . [sent-82, score-0.299]
</p><p>48 As such, the notion of a shiftable-twistable basis on generalizes that of a shiftablesteerable basis on [2, 10]. [sent-83, score-0.358]
</p><p>49 H  £ &0   ¡  H   B V 9 V 85 7  H  7  B 685 9 7  ¡ &0    H  ©  © ¦ ¨ ¥ 5 6 £ ¡ 0     B 685 6 9 7 £ ¡ 0     B 9 7 CB G85  £ ¡ 870    0     Shiftable-twistable bases can be constructed as follows. [sent-84, score-0.046]
</p><p>50 Let be a function on which is periodic (with period ) in both spatial variables, . [sent-85, score-0.098]
</p><p>51 In analogy with the , we say that is shiftable-twistable on deﬁnition of a shiftable-steerable function on if there are integers, and , and interpolation functions, , such that for each , the shift-twist of by is a linear combination of a ﬁnite number of basic shift-twists of by amounts , i. [sent-86, score-0.094]
</p><p>52  ¦ F  H  £¡¢0   GB V  9   Q R¦   B 9 7 CB G85  £ A¡  7 V X5  £  0     ©  © ¦¨  PI¥ 5 9  Here is the basic shift amount and is the basic twist amount. [sent-92, score-0.375]
</p><p>53 B  Y `H 9 ¨ H 5  ¦  C XU  7H  7  C c  ¦  &  BU WV@  d  ¦  d  B Y H 9 ¨ H b  a c  &  The Gaussian-Fourier basis is the product of a shiftable-steerable basis of Gaussians in and a Fourier series basis in . [sent-94, score-0.592]
</p><p>54 For the experiments in this paper, the standard deviation of , equals the basic shift amount, . [sent-95, score-0.218]
</p><p>55 We the Gaussian basis function, regard as a periodic function of period, , which is chosen to be much larger than , so that and its derivatives are essentially zero. [sent-96, score-0.227]
</p><p>56 2  We use this terminology even though the basis functions need not be linearly independent. [sent-98, score-0.239]
</p><p>57 2 Power method update formula can be represented in the Gaussian-Fourier basis as (17)    '¦  ¦  B G85 9 7  s ¦ F 9 s ¦'F ¦ s ¦ F Q  (! [sent-100, score-0.302]
</p><p>58 , the basic step in the power method) can be implemented as a discrete linear transform in a Gaussian-Fourier shiftable-twistable basis:  (! [sent-111, score-0.229]
</p><p>59 3 The propagation operator P  (19)  ¢  In practice, we do not explicitly represent the matrix, . [sent-114, score-0.141]
</p><p>60 Instead we compute the necessary matrix-vector product using the advection-diffusion-decay operator in the Gaussian-Fourier shiftable-twistable basis, , described in detail in Zweck and Williams[13]:  D '¦F d s   ¦ F ¦  ! [sent-115, score-0.196]
</p><p>61   ¡ ¤¢ £   ¦  (V  and where:  (22)  , is a discrete convolution:  '    In the shiftable-twistable basis, the advection operator,  (23)  s  f  1) 5 ' %$ 20¨ 4(&©B #"w    f   #! [sent-128, score-0.177]
</p><p>62 ¨  where  (25)  In the continuum, the bias operator effects a multiplication of the function, , by the input bias function, . [sent-137, score-0.425]
</p><p>63 Our aim is to identify an equivalent linear operator in the shiftabletwistable basis. [sent-138, score-0.141]
</p><p>64 Suppose that both and are represented in a Gaussian basis, . [sent-139, score-0.051]
</p><p>65 Their product is:  B 7 bX5  ¦F d  B 7 ¥X5  ¡  ¦  ¦ ¦F ! [sent-140, score-0.055]
</p><p>66 d ¦  ¡ ¦  Q 9b87 5 ¦ F d 1¦ ¦ F Q ¦F ¦ ¦F B 8 B  3 b85 B 7  ¦  ¡  B 7 B 7 185 b85  ¦ ¡  Now, the product of two Gaussian basis functions, and ance which cannot be represented in the Gaussian basis,  (26)  , is a Gaussian of smaller vari. [sent-143, score-0.285]
</p><p>67 Because is a linear  combination of the products of pairs of Gaussian basis functions, it cannot be represented in the Gaussian basis either. [sent-144, score-0.409]
</p><p>68 However, we observe that the convolution of and , where , can be represented in the a Gaussian, Gaussian basis. [sent-145, score-0.101]
</p><p>69 D  where are the interpolation functions, equals , and shift amount, to express in the Gaussian basis. [sent-148, score-0.228]
</p><p>70 The standard deviation of the Gaussian was set equal to the shift amount, . [sent-150, score-0.126]
</p><p>71 For illustration purposes, all functions were rendered at a resolution of . [sent-151, score-0.06]
</p><p>72 The diffusion parameter, , equaled , and the decay constant, , equaled . [sent-152, score-0.352]
</p><p>73 The time step, , used to solve the Fokker-Planck equation in the basis equaled . [sent-153, score-0.277]
</p><p>74 § ¥ §  ¥¦  2    ¦  ¦  3  B   3     ¦ @     ¦  3    )  ¦   ¤  B 7 b85  d  BU WV@  ¦  ¤ R§   d  ¡    Y¦  ¡  §    ¨¦ § 3 ¤ R§    ¦  U d  In the ﬁrst experiment, the input bias function, , consisted of twenty randomly positioned spots and twenty spots on the boundary of an avocado. [sent-155, score-1.084]
</p><p>75 The stochastic completion ﬁeld computed using 32 iterations of the power method is shown in Fig. [sent-161, score-0.384]
</p><p>76 In the second experiment, the input bias function from the ﬁrst experiment was rotated by and translated by half the distance between the centers of adjacent basis functions, . [sent-163, score-0.522]
</p><p>77 The stochastic completion ﬁeld is identical (up to rotation and translation) to the one computed in the ﬁrst experiment. [sent-166, score-0.426]
</p><p>78 The estimate of the largest positive real eigenvalue, , as a function of , the power method iteration is shown in Fig. [sent-170, score-0.201]
</p><p>79    5 £  ¡  ¨  5 Conclusion We described a neural network which enhances and completes salient closed contours. [sent-173, score-0.458]
</p><p>80 Even though the computation is implemented in a discrete network, its output is invariant under continuous rotations and translations of the input pattern. [sent-174, score-0.754]
</p><p>81 B 7 ¥X5  ¡  Figure 1: Left: The input bias function, . [sent-182, score-0.2]
</p><p>82 Twenty randomly positioned spots were added to twenty spots on the boundary of an avocado. [sent-183, score-0.773]
</p><p>83 Right: The stochastic completion ﬁeld, , computed using basis functions. [sent-187, score-0.483]
</p><p>84 § £  ¡  § § ¡ £ ¦ ¡ £ ¦  T i bU9 85 B T 7S  £e  Figure 2: Left: The input bias function from Fig. [sent-188, score-0.2]
</p><p>85 1, rotated by and translated by half the distance between the centers of adjacent basis functions, . [sent-189, score-0.322]
</p><p>86 Right: The stochastic completion ﬁeld, is identical (up to rotation and translation) to the one shown in Fig. [sent-190, score-0.426]
</p><p>87 02  0 5  10  15  20  25  30  35  &  0  ¨  Figure 3: The estimate of the largest positive real eigenvalue, , as a function of , the power method iteration. [sent-201, score-0.201]
</p><p>88 Both the ﬁnal value and all intermediate values are identical in the rotated and non-rotated cases. [sent-202, score-0.183]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spots', 0.264), ('completion', 0.242), ('saliency', 0.197), ('basis', 0.179), ('zweck', 0.151), ('operator', 0.141), ('contour', 0.14), ('williams', 0.129), ('eld', 0.128), ('shift', 0.126), ('rotations', 0.125), ('invariant', 0.123), ('euclidean', 0.12), ('salient', 0.119), ('dp', 0.119), ('input', 0.116), ('translations', 0.114), ('advection', 0.113), ('heitger', 0.113), ('heydt', 0.113), ('thornber', 0.113), ('twist', 0.113), ('twenty', 0.111), ('continuum', 0.111), ('diffusion', 0.105), ('closed', 0.101), ('equaled', 0.098), ('rotated', 0.095), ('contours', 0.092), ('bu', 0.09), ('enhances', 0.09), ('integers', 0.09), ('bias', 0.084), ('orientation', 0.084), ('eigenfunctions', 0.083), ('wv', 0.083), ('cb', 0.08), ('power', 0.08), ('network', 0.079), ('baltimore', 0.079), ('particles', 0.079), ('boundary', 0.078), ('rotation', 0.078), ('computation', 0.076), ('cdvq', 0.075), ('propagator', 0.075), ('propagators', 0.075), ('similiar', 0.075), ('edges', 0.075), ('von', 0.075), ('transformations', 0.074), ('formula', 0.072), ('der', 0.071), ('translation', 0.07), ('completes', 0.069), ('largest', 0.066), ('eigenfunction', 0.066), ('illusory', 0.066), ('discrete', 0.064), ('vision', 0.063), ('stochastic', 0.062), ('functions', 0.06), ('rp', 0.06), ('adelson', 0.06), ('lgn', 0.06), ('yv', 0.06), ('direction', 0.058), ('gaussian', 0.058), ('ee', 0.057), ('positioned', 0.056), ('sd', 0.056), ('real', 0.055), ('product', 0.055), ('mumford', 0.052), ('interpolation', 0.052), ('amount', 0.052), ('continuous', 0.052), ('decay', 0.051), ('represented', 0.051), ('li', 0.05), ('equals', 0.05), ('period', 0.05), ('convolution', 0.05), ('periodic', 0.048), ('translated', 0.048), ('xv', 0.048), ('characterized', 0.046), ('eigenvalue', 0.046), ('valued', 0.046), ('bases', 0.046), ('md', 0.046), ('identical', 0.044), ('fourier', 0.044), ('intermediate', 0.044), ('particle', 0.044), ('implemented', 0.043), ('detection', 0.043), ('basic', 0.042), ('positions', 0.042), ('output', 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="19-tfidf-1" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>Author: Lance R. Williams, John W. Zweck</p><p>Abstract: We describe a neural network which enhances and completes salient closed contours. Our work is different from all previous work in three important ways. First, like the input provided to V1 by LGN, the input to our computation is isotropic. That is, the input is composed of spots not edges. Second, our network computes a well deﬁned function of the input based on a distribution of closed contours characterized by a random process. Third, even though our computation is implemented in a discrete network, its output is invariant to continuous rotations and translations of the input pattern.</p><p>2 0.097595379 <a title="19-tfidf-2" href="./nips-2001-Fast%2C_Large-Scale_Transformation-Invariant_Clustering.html">75 nips-2001-Fast, Large-Scale Transformation-Invariant Clustering</a></p>
<p>Author: Brendan J. Frey, Nebojsa Jojic</p><p>Abstract: In previous work on “transformed mixtures of Gaussians” and “transformed hidden Markov models”, we showed how the EM algorithm in a discrete latent variable model can be used to jointly normalize data (e.g., center images, pitch-normalize spectrograms) and learn a mixture model of the normalized data. The only input to the algorithm is the data, a list of possible transformations, and the number of clusters to ﬁnd. The main criticism of this work was that the exhaustive computation of the posterior probabilities over transformations would make scaling up to large feature vectors and large sets of transformations intractable. Here, we describe how a tremendous speed-up is acheived through the use of a variational technique for decoupling transformations, and a fast Fourier transform method for computing posterior probabilities. For N ×N images, learning C clusters under N rotations, N scales, N x-translations and N y-translations takes only (C + 2 log N )N 2 scalar operations per iteration. In contrast, the original algorithm takes CN 6 operations to account for these transformations. We give results on learning a 4-component mixture model from a video sequence with frames of size 320 ×240. The model accounts for 360 rotations and 76,800 translations. Each iteration of EM takes only 10 seconds per frame in MATLAB, which is over 5 million times faster than the original algorithm. 1</p><p>3 0.095412821 <a title="19-tfidf-3" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>Author: Thomas P. Trappenberg, Edmund T. Rolls, Simon M. Stringer</p><p>Abstract: Inferior temporal cortex (IT) neurons have large receptive ﬁelds when a single effective object stimulus is shown against a blank background, but have much smaller receptive ﬁelds when the object is placed in a natural scene. Thus, translation invariant object recognition is reduced in natural scenes, and this may help object selection. We describe a model which accounts for this by competition within an attractor in which the neurons are tuned to different objects in the scene, and the fovea has a higher cortical magniﬁcation factor than the peripheral visual ﬁeld. Furthermore, we show that top-down object bias can increase the receptive ﬁeld size, facilitating object search in complex visual scenes, and providing a model of object-based attention. The model leads to the prediction that introduction of a second object into a scene with blank background will reduce the receptive ﬁeld size to values that depend on the closeness of the second object to the target stimulus. We suggest that mechanisms of this type enable the output of IT to be primarily about one object, so that the areas that receive from IT can select the object as a potential target for action.</p><p>4 0.092351988 <a title="19-tfidf-4" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>Author: Heiko Wersing</p><p>Abstract: We present a new approach to the supervised learning of lateral interactions for the competitive layer model (CLM) dynamic feature binding architecture. The method is based on consistency conditions, which were recently shown to characterize the attractor states of this linear threshold recurrent network. For a given set of training examples the learning problem is formulated as a convex quadratic optimization problem in the lateral interaction weights. An efﬁcient dimension reduction of the learning problem can be achieved by using a linear superposition of basis interactions. We show the successful application of the method to a medical image segmentation problem of ﬂuorescence microscope cell images.</p><p>5 0.089159146 <a title="19-tfidf-5" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>Author: Antonio Torralba</p><p>Abstract: The most popular algorithms for object detection require the use of exhaustive spatial and scale search procedures. In such approaches, an object is defined by means of local features. fu this paper we show that including contextual information in object detection procedures provides an efficient way of cutting down the need for exhaustive search. We present results with real images showing that the proposed scheme is able to accurately predict likely object classes, locations and sizes. 1</p><p>6 0.08464665 <a title="19-tfidf-6" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>7 0.080349118 <a title="19-tfidf-7" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>8 0.07890448 <a title="19-tfidf-8" href="./nips-2001-Fragment_Completion_in_Humans_and_Machines.html">78 nips-2001-Fragment Completion in Humans and Machines</a></p>
<p>9 0.078608103 <a title="19-tfidf-9" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>10 0.077036366 <a title="19-tfidf-10" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>11 0.073259726 <a title="19-tfidf-11" href="./nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables.html">153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</a></p>
<p>12 0.071555302 <a title="19-tfidf-12" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>13 0.071490034 <a title="19-tfidf-13" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>14 0.069122337 <a title="19-tfidf-14" href="./nips-2001-Linking_Motor_Learning_to_Function_Approximation%3A_Learning_in_an_Unlearnable_Force_Field.html">116 nips-2001-Linking Motor Learning to Function Approximation: Learning in an Unlearnable Force Field</a></p>
<p>15 0.066404827 <a title="19-tfidf-15" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>16 0.066366956 <a title="19-tfidf-16" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>17 0.065743268 <a title="19-tfidf-17" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>18 0.065619498 <a title="19-tfidf-18" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>19 0.064026207 <a title="19-tfidf-19" href="./nips-2001-Generalizable_Relational_Binding_from_Coarse-coded_Distributed_Representations.html">80 nips-2001-Generalizable Relational Binding from Coarse-coded Distributed Representations</a></p>
<p>20 0.060544934 <a title="19-tfidf-20" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.213), (1, -0.076), (2, -0.031), (3, -0.037), (4, -0.053), (5, -0.005), (6, -0.108), (7, 0.055), (8, 0.055), (9, 0.01), (10, -0.024), (11, 0.056), (12, 0.11), (13, -0.046), (14, 0.045), (15, 0.09), (16, 0.02), (17, -0.052), (18, 0.018), (19, -0.004), (20, 0.032), (21, 0.031), (22, 0.104), (23, -0.005), (24, 0.011), (25, 0.046), (26, 0.006), (27, -0.017), (28, -0.057), (29, 0.129), (30, -0.028), (31, -0.011), (32, -0.017), (33, 0.097), (34, -0.093), (35, 0.051), (36, -0.015), (37, 0.0), (38, -0.122), (39, -0.096), (40, -0.078), (41, -0.13), (42, 0.056), (43, -0.099), (44, -0.058), (45, -0.067), (46, 0.065), (47, -0.096), (48, -0.072), (49, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95405942 <a title="19-lsi-1" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>Author: Lance R. Williams, John W. Zweck</p><p>Abstract: We describe a neural network which enhances and completes salient closed contours. Our work is different from all previous work in three important ways. First, like the input provided to V1 by LGN, the input to our computation is isotropic. That is, the input is composed of spots not edges. Second, our network computes a well deﬁned function of the input based on a distribution of closed contours characterized by a random process. Third, even though our computation is implemented in a discrete network, its output is invariant to continuous rotations and translations of the input pattern.</p><p>2 0.48366764 <a title="19-lsi-2" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>Author: Heiko Wersing</p><p>Abstract: We present a new approach to the supervised learning of lateral interactions for the competitive layer model (CLM) dynamic feature binding architecture. The method is based on consistency conditions, which were recently shown to characterize the attractor states of this linear threshold recurrent network. For a given set of training examples the learning problem is formulated as a convex quadratic optimization problem in the lateral interaction weights. An efﬁcient dimension reduction of the learning problem can be achieved by using a linear superposition of basis interactions. We show the successful application of the method to a medical image segmentation problem of ﬂuorescence microscope cell images.</p><p>3 0.47489211 <a title="19-lsi-3" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>Author: M. Schmitt</p><p>Abstract: Recurrent neural networks of analog units are computers for realvalued functions. We study the time complexity of real computation in general recurrent neural networks. These have sigmoidal, linear, and product units of unlimited order as nodes and no restrictions on the weights. For networks operating in discrete time, we exhibit a family of functions with arbitrarily high complexity, and we derive almost tight bounds on the time required to compute these functions. Thus, evidence is given of the computational limitations that time-bounded analog recurrent neural networks are subject to. 1</p><p>4 0.46557581 <a title="19-lsi-4" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>Author: Shimon Edelman, Benjamin P. Hiles, Hwajin Yang, Nathan Intrator</p><p>Abstract: To ﬁnd out how the representations of structured visual objects depend on the co-occurrence statistics of their constituents, we exposed subjects to a set of composite images with tight control exerted over (1) the conditional probabilities of the constituent fragments, and (2) the value of Barlow’s criterion of “suspicious coincidence” (the ratio of joint probability to the product of marginals). We then compared the part veriﬁcation response times for various probe/target combinations before and after the exposure. For composite probes, the speedup was much larger for targets that contained pairs of fragments perfectly predictive of each other, compared to those that did not. This effect was modulated by the signiﬁcance of their co-occurrence as estimated by Barlow’s criterion. For lone-fragment probes, the speedup in all conditions was generally lower than for composites. These results shed light on the brain’s strategies for unsupervised acquisition of structural information in vision. 1 Motivation How does the human visual system decide for which objects it should maintain distinct and persistent internal representations of the kind typically postulated by theories of object recognition? Consider, for example, the image shown in Figure 1, left. This image can be represented as a monolithic hieroglyph, a pair of Chinese characters (which we shall refer to as and ), a set of strokes, or, trivially, as a collection of pixels. Note that the second option is only available to a system previously exposed to various combinations of Chinese characters. Indeed, a principled decision whether to represent this image as , or otherwise can only be made on the basis of prior exposure to related images. £ ¡ £¦ ¡ £ ¥¨§¢   ¥¤¢   ¢ According to Barlow’s [1] insight, one useful principle is tallying suspicious coincidences: two candidate fragments and should be combined into a composite object if the probability of their joint appearance is much higher than , which is the probability expected in the case of their statistical independence. This criterion may be compared to the Minimum Description Length (MDL) principle, which has been previously discussed in the context of object representation [2, 3]. In a simpliﬁed form [4], MDL calls for representing explicitly as a whole if , just as the principle of suspicious coincidences does. £ ©¢  £  ¢ ¥¤¥  £¦ ¢ ¥  £  ¢   £¦ ¢ ¥¤¥! ¨§¥ £ ¢ £ ©¢  £¦  £ ¨§¢¥ ¡ ¢   While the Barlow/MDL criterion certainly indicates a suspicious coincidence, there are additional probabilistic considerations that may be used and . One example is the possiin setting the degree of association between ble perfect predictability of from and vice versa, as measured by . If , then and are perfectly predictive of each other and should really be coded by a single symbol, whereas the MDL criterion may suggest merely that some association between the representation of and that of be established. In comparison, if and are not perfectly predictive of each other ( ), there is a case to be made in favor of coding them separately to allow for a maximally expressive representation, whereas MDL may actually suggest a high degree of association ). In this study we investigated whether the human (if visual system uses a criterion based on alongside MDL while learning (in an unsupervised manner) to represent composite objects. £ £  £ ¢  ¥  ¥ © §¥ ¡ ¢  ¨¦¤</p><p>5 0.45601597 <a title="19-lsi-5" href="./nips-2001-Stochastic_Mixed-Signal_VLSI_Architecture_for_High-Dimensional_Kernel_Machines.html">176 nips-2001-Stochastic Mixed-Signal VLSI Architecture for High-Dimensional Kernel Machines</a></p>
<p>Author: Roman Genov, Gert Cauwenberghs</p><p>Abstract: A mixed-signal paradigm is presented for high-resolution parallel innerproduct computation in very high dimensions, suitable for efﬁcient implementation of kernels in image processing. At the core of the externally digital architecture is a high-density, low-power analog array performing binary-binary partial matrix-vector multiplication. Full digital resolution is maintained even with low-resolution analog-to-digital conversion, owing to random statistics in the analog summation of binary products. A random modulation scheme produces near-Bernoulli statistics even for highly correlated inputs. The approach is validated with real image data, and with experimental results from a CID/DRAM analog array prototype in 0.5 m CMOS. ¢</p><p>6 0.45161188 <a title="19-lsi-6" href="./nips-2001-Learning_Body_Pose_via_Specialized_Maps.html">108 nips-2001-Learning Body Pose via Specialized Maps</a></p>
<p>7 0.45009765 <a title="19-lsi-7" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>8 0.449101 <a title="19-lsi-8" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<p>9 0.44891629 <a title="19-lsi-9" href="./nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables.html">153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</a></p>
<p>10 0.4263868 <a title="19-lsi-10" href="./nips-2001-Receptive_field_structure_of_flow_detectors_for_heading_perception.html">158 nips-2001-Receptive field structure of flow detectors for heading perception</a></p>
<p>11 0.42135969 <a title="19-lsi-11" href="./nips-2001-Fast%2C_Large-Scale_Transformation-Invariant_Clustering.html">75 nips-2001-Fast, Large-Scale Transformation-Invariant Clustering</a></p>
<p>12 0.40756878 <a title="19-lsi-12" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>13 0.40555793 <a title="19-lsi-13" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>14 0.395423 <a title="19-lsi-14" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>15 0.39220634 <a title="19-lsi-15" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>16 0.38426846 <a title="19-lsi-16" href="./nips-2001-Gaussian_Process_Regression_with_Mismatched_Models.html">79 nips-2001-Gaussian Process Regression with Mismatched Models</a></p>
<p>17 0.38371262 <a title="19-lsi-17" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>18 0.38286868 <a title="19-lsi-18" href="./nips-2001-Kernel_Feature_Spaces_and_Nonlinear_Blind_Souce_Separation.html">103 nips-2001-Kernel Feature Spaces and Nonlinear Blind Souce Separation</a></p>
<p>19 0.38147151 <a title="19-lsi-19" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>20 0.37635919 <a title="19-lsi-20" href="./nips-2001-Laplacian_Eigenmaps_and_Spectral_Techniques_for_Embedding_and_Clustering.html">106 nips-2001-Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.025), (17, 0.031), (19, 0.028), (27, 0.114), (30, 0.084), (38, 0.488), (59, 0.023), (72, 0.032), (79, 0.026), (83, 0.023), (91, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94688302 <a title="19-lda-1" href="./nips-2001-Generalizable_Relational_Binding_from_Coarse-coded_Distributed_Representations.html">80 nips-2001-Generalizable Relational Binding from Coarse-coded Distributed Representations</a></p>
<p>Author: Randall C. O'Reilly, R. S. Busby</p><p>Abstract: We present a model of binding of relationship information in a spatial domain (e.g., square above triangle) that uses low-order coarse-coded conjunctive representations instead of more popular temporal synchrony mechanisms. Supporters of temporal synchrony argue that conjunctive representations lack both efﬁciency (i.e., combinatorial numbers of units are required) and systematicity (i.e., the resulting representations are overly speciﬁc and thus do not support generalization to novel exemplars). To counter these claims, we show that our model: a) uses far fewer hidden units than the number of conjunctions represented, by using coarse-coded, distributed representations where each unit has a broad tuning curve through high-dimensional conjunction space, and b) is capable of considerable generalization to novel inputs.</p><p>same-paper 2 0.88910842 <a title="19-lda-2" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>Author: Lance R. Williams, John W. Zweck</p><p>Abstract: We describe a neural network which enhances and completes salient closed contours. Our work is different from all previous work in three important ways. First, like the input provided to V1 by LGN, the input to our computation is isotropic. That is, the input is composed of spots not edges. Second, our network computes a well deﬁned function of the input based on a distribution of closed contours characterized by a random process. Third, even though our computation is implemented in a discrete network, its output is invariant to continuous rotations and translations of the input pattern.</p><p>3 0.84066427 <a title="19-lda-3" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>Author: Julian Eggert, Berthold BĂ¤uml</p><p>Abstract: Mesoscopical, mathematical descriptions of dynamics of populations of spiking neurons are getting increasingly important for the understanding of large-scale processes in the brain using simulations. In our previous work, integral equation formulations for population dynamics have been derived for a special type of spiking neurons. For Integrate- and- Fire type neurons , these formulations were only approximately correct. Here, we derive a mathematically compact, exact population dynamics formulation for Integrate- and- Fire type neurons. It can be shown quantitatively in simulations that the numerical correspondence with microscopically modeled neuronal populations is excellent. 1 Introduction and motivation The goal of the population dynamics approach is to model the time course of the collective activity of entire populations of functionally and dynamically similar neurons in a compact way, using a higher descriptionallevel than that of single neurons and spikes. The usual observable at the level of neuronal populations is the populationaveraged instantaneous firing rate A(t), with A(t)6.t being the number of neurons in the population that release a spike in an interval [t, t+6.t). Population dynamics are formulated in such a way, that they match quantitatively the time course of a given A(t), either gained experimentally or by microscopical, detailed simulation. At least three main reasons can be formulated which underline the importance of the population dynamics approach for computational neuroscience. First, it enables the simulation of extensive networks involving a massive number of neurons and connections, which is typically the case when dealing with biologically realistic functional models that go beyond the single neuron level. Second, it increases the analytical understanding of large-scale neuronal dynamics , opening the way towards better control and predictive capabilities when dealing with large networks. Third, it enables a systematic embedding of the numerous neuronal models operating at different descriptional scales into a generalized theoretic framework, explaining the relationships, dependencies and derivations of the respective models. Early efforts on population dynamics approaches date back as early as 1972, to the work of Wilson and Cowan [8] and Knight [4], which laid the basis for all current population-averaged graded-response models (see e.g. [6] for modeling work using these models). More recently, population-based approaches for spiking neurons were developed, mainly by Gerstner [3, 2] and Knight [5]. In our own previous work [1], we have developed a theoretical framework which enables to systematize and simulate a wide range of models for population-based dynamics. It was shown that the equations of the framework produce results that agree quantitatively well with detailed simulations using spiking neurons, so that they can be used for realistic simulations involving networks with large numbers of spiking neurons. Nevertheless, for neuronal populations composed of Integrate-and-Fire (I&F;) neurons, this framework was only correct in an approximation. In this paper, we derive the exact population dynamics formulation for I&F; neurons. This is achieved by reducing the I&F; population dynamics to a point process and by taking advantage of the particular properties of I&F; neurons. 2 2.1 Background: Integrate-and-Fire dynamics Differential form We start with the standard Integrate- and- Fire (I&F;) model in form of the wellknown differential equation [7] (1) which describes the dynamics of the membrane potential Vi of a neuron i that is modeled as a single compartment with RC circuit characteristics. The membrane relaxation time is in this case T = RC with R being the membrane resistance and C the membrane capacitance. The resting potential v R est is the stationary potential that is approached in the no-input case. The input arriving from other neurons is described in form of a current ji. In addition to eq. (1), which describes the integrate part of the I&F; model, the neuronal dynamics are completed by a nonlinear step. Every time the membrane potential Vi reaches a fixed threshold () from below, Vi is lowered by a fixed amount Ll > 0, and from the new value of the membrane potential integration according to eq. (1) starts again. if Vi(t) = () (from below) . (2) At the same time, it is said that the release of a spike occurred (i.e., the neuron fired), and the time ti = t of this singular event is stored. Here ti indicates the time of the most recent spike. Storing all the last firing times , we gain the sequence of spikes {t{} (spike ordering index j, neuronal index i). 2.2 Integral form Now we look at the single neuron in a neuronal compound. We assume that the input current contribution ji from presynaptic spiking neurons can be described using the presynaptic spike times tf, a response-function ~ and a connection weight WÂˇ . ',J ji(t) = Wi ,j ~(t - tf) (3) l: l: j f Integrating the I&F; equation (1) beginning at the last spiking time tT, which determines the initial condition by Vi(ti) = vi(ti - 0) - 6., where vi(ti - 0) is the membrane potential just before the neuron spikes, we get 1 Vi(t) = v Rest + fj(t - t:) + l: Wi ,j l: a(t - t:; t - tf) , j - Vi(t:)) e- S / T (4) f with the refractory function fj(s) = - (v Rest (5) and the alpha-function r ds</p><p>4 0.53267121 <a title="19-lda-4" href="./nips-2001-A_Model_of_the_Phonological_Loop%3A_Generalization_and_Binding.html">12 nips-2001-A Model of the Phonological Loop: Generalization and Binding</a></p>
<p>Author: Randall C. O'Reilly, R. Soto</p><p>Abstract: We present a neural network model that shows how the prefrontal cortex, interacting with the basal ganglia, can maintain a sequence of phonological information in activation-based working memory (i.e., the phonological loop). The primary function of this phonological loop may be to transiently encode arbitrary bindings of information necessary for tasks - the combinatorial expressive power of language enables very flexible binding of essentially arbitrary pieces of information. Our model takes advantage of the closed-class nature of phonemes, which allows different neural representations of all possible phonemes at each sequential position to be encoded. To make this work, we suggest that the basal ganglia provide a region-specific update signal that allocates phonemes to the appropriate sequential coding slot. To demonstrate that flexible, arbitrary binding of novel sequences can be supported by this mechanism, we show that the model can generalize to novel sequences after moderate amounts of training. 1</p><p>5 0.51961529 <a title="19-lda-5" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>Author: Thomas P. Trappenberg, Edmund T. Rolls, Simon M. Stringer</p><p>Abstract: Inferior temporal cortex (IT) neurons have large receptive ﬁelds when a single effective object stimulus is shown against a blank background, but have much smaller receptive ﬁelds when the object is placed in a natural scene. Thus, translation invariant object recognition is reduced in natural scenes, and this may help object selection. We describe a model which accounts for this by competition within an attractor in which the neurons are tuned to different objects in the scene, and the fovea has a higher cortical magniﬁcation factor than the peripheral visual ﬁeld. Furthermore, we show that top-down object bias can increase the receptive ﬁeld size, facilitating object search in complex visual scenes, and providing a model of object-based attention. The model leads to the prediction that introduction of a second object into a scene with blank background will reduce the receptive ﬁeld size to values that depend on the closeness of the second object to the target stimulus. We suggest that mechanisms of this type enable the output of IT to be primarily about one object, so that the areas that receive from IT can select the object as a potential target for action.</p><p>6 0.51721597 <a title="19-lda-6" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>7 0.48184797 <a title="19-lda-7" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>8 0.47351813 <a title="19-lda-8" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>9 0.46458191 <a title="19-lda-9" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>10 0.4381336 <a title="19-lda-10" href="./nips-2001-Learning_Hierarchical_Structures_with_Linear_Relational_Embedding.html">110 nips-2001-Learning Hierarchical Structures with Linear Relational Embedding</a></p>
<p>11 0.4363991 <a title="19-lda-11" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>12 0.43444046 <a title="19-lda-12" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>13 0.42779079 <a title="19-lda-13" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>14 0.41552082 <a title="19-lda-14" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>15 0.40834004 <a title="19-lda-15" href="./nips-2001-Fast%2C_Large-Scale_Transformation-Invariant_Clustering.html">75 nips-2001-Fast, Large-Scale Transformation-Invariant Clustering</a></p>
<p>16 0.4069947 <a title="19-lda-16" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>17 0.40665382 <a title="19-lda-17" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>18 0.40658873 <a title="19-lda-18" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>19 0.40546566 <a title="19-lda-19" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>20 0.4035643 <a title="19-lda-20" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
