<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-153" href="#">nips2001-153</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</h1>
<br/><p>Source: <a title="nips-2001-153-pdf" href="http://papers.nips.cc/paper/1973-product-analysis-learning-to-model-observations-as-products-of-hidden-variables.pdf">pdf</a></p><p>Author: Brendan J. Frey, Anitha Kannan, Nebojsa Jojic</p><p>Abstract: Factor analysis and principal components analysis can be used to model linear relationships between observed variables and linearly map high-dimensional data to a lower-dimensional hidden space. In factor analysis, the observations are modeled as a linear combination of normally distributed hidden variables. We describe a nonlinear generalization of factor analysis , called</p><p>Reference: <a title="nips-2001-153-reference" href="../nips2001_reference/nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Prod uct Analysis: Learning to model observations as products of hidden variables  Brendan J. [sent-1, score-0.344]
</p><p>2 edu 2 Vision Technology Group, Microsoft Research  Abstract Factor analysis and principal components analysis can be used to model linear relationships between observed variables and linearly map high-dimensional data to a lower-dimensional hidden space. [sent-5, score-0.78]
</p><p>3 In factor analysis, the observations are modeled as a linear combination of normally distributed hidden variables. [sent-6, score-0.734]
</p><p>4 We describe a nonlinear generalization of factor analysis , called "product analysis", that models the observed variables as a linear combination of products of normally distributed hidden variables. [sent-7, score-1.026]
</p><p>5 Just as factor analysis can be viewed as unsupervised linear regression on unobserved, normally distributed hidden variables, product analysis can be viewed as unsupervised linear regression on products of unobserved, normally distributed hidden variables. [sent-8, score-1.563]
</p><p>6 The mapping between the data and the hidden space is nonlinear, so we use an approximate variational technique for inference and learning. [sent-9, score-0.373]
</p><p>7 Since product analysis is a generalization of factor analysis, product analysis always finds a higher data likelihood than factor analysis. [sent-10, score-1.15]
</p><p>8 We give results on pattern recognition and illuminationinvariant image clustering. [sent-11, score-0.042]
</p><p>9 1  Introduction  Continuous-valued latent representations of observed feature vectors can be useful for pattern classification via Bayes rule, summarizing data sets, and producing lowdimensional representations of data for later processing. [sent-12, score-0.178]
</p><p>10 Linear techniques, including principal components analysis (J olliffe 1986), factor analysis (Rubin and Thayer 1982) and probabilistic principal components analysis (Tipping and Bishop 1999) , model the input as a linear combination of hidden variables, plus sensor noise. [sent-13, score-1.431]
</p><p>11 The noise models are quite different in all 3 cases (see Tipping and Bishop (1999) for a discussion). [sent-14, score-0.054]
</p><p>12 For example, whereas factor analysis can account for different noise variances in the coordinates of the input, principal components analysis assumes that the noise variances are the same in different  input coordinates. [sent-15, score-0.916]
</p><p>13 Also, whereas factor analysis accounts for the sensor noise when estimating the combination weights, principal components analysis does not. [sent-16, score-0.857]
</p><p>14 Often, the input coordinates are not linearly related, but instead the input vector is the result of a nonlinear generative process. [sent-17, score-0.272]
</p><p>15 In particular, data often can be accurately described as the product of unknown random variables. [sent-18, score-0.246]
</p><p>16 Examples include the combination of "style" and "content" (Tenenbaum and Freeman 1997), and the combination of a scalar light intensity and a reflectance image. [sent-19, score-0.27]
</p><p>17 We introduce a generalization of factor analysis , called "product analysis" , that performs maximum likelihood estimation to model the input as a linear combination of products of hidden variables. [sent-20, score-0.767]
</p><p>18 Although exact EM is not tractable because the hidden variables are nonlinearly related to the input, the form of the product analysis model makes it well-suited to a variational inference technique and a variational EM algorithm. [sent-21, score-0.96]
</p><p>19 2  Factor analysis model  Of the three linear techniques described above, factor analysis has the simplest description as a generative model of the data. [sent-24, score-0.517]
</p><p>20 The input vector x is modeled using a vector of hidden variables z. [sent-25, score-0.394]
</p><p>21 The hidden variables are independent and normally distributed with zero mean and unit variance:  p(z) = N(z ; 0, I). [sent-26, score-0.43]
</p><p>22 (1)  The input is modeled as a linear combination of the hidden variables, plus independent Gaussian noise: (2) p(xlz) = N(x; Az, \]f). [sent-27, score-0.436]
</p><p>23 The model parameters are the factor loading matrix A and the diagonal matrix of sensor noise variances, \]f. [sent-28, score-0.335]
</p><p>24 (Rubin and Thayer 1982)) is the procedure for estimating A and lJI using a training set. [sent-30, score-0.037]
</p><p>25 The marginal distribution over the input is p(x) = N(x; 0, AA T + lJI), so factor analysis can be viewed as estimating a lowrank parameterization of the covariance matrix of the data. [sent-31, score-0.381]
</p><p>26 3  Product analysis model  In the "product analyzer", the input vector x is modeled using a vector of hidden variables z, which are independent and normally distributed with zero mean and unit variance: p(z) = N(z; 0, I). [sent-32, score-0.687]
</p><p>27 (3) In factor analysis, the input is modeled as a linear combination of the hidden variables. [sent-33, score-0.629]
</p><p>28 In product analysis, the input is modeled as a linear combination of monomials in the hidden variables. [sent-34, score-0.682]
</p><p>29 II k  Denoting the vector of Ji(z) 's by f(z) , the density of the input given z is  p(xlz)  = N(x; Af(z) , lJI). [sent-37, score-0.138]
</p><p>30 Here, we learn A , maintaining the distribution over z constant. [sent-39, score-0.084]
</p><p>31 Alternatively, if A is known apriori, we can learn the distribution over z, maintaining A to be fixed. [sent-40, score-0.084]
</p><p>32 When S = I, J(z) = z and the product analyzer simplifies to the factor analyzer. [sent-42, score-0.565]
</p><p>33 If, for some i, Sik = 0, for all k, Ji(z) = 1 and this monomial will account for a constant offset in the input. [sent-43, score-0.115]
</p><p>34 4  Product analysis  Exact EM in the product analyzer is intractable, since the sufficient statistics require averaging over the posterior p(zlx), for which we do not have a tractable expression. [sent-44, score-0.489]
</p><p>35 Instead, we use a variational approximation (Jordan et al. [sent-45, score-0.152]
</p><p>36 1998) , where for each training case, the posterior p(zlx) is approximated by a factorized Gaussian distribution q(z) and the parameters of q(z) are adjusted to make the approximation accurate. [sent-46, score-0.037]
</p><p>37 Then, the approximation q(z) is used to compute the sufficient statistics for each training case in a generalized EM algorithm (Neal and Hinton 1993). [sent-47, score-0.037]
</p><p>38 The q-distribution is specified by the variational parameters 'f/ and  q(z) = N(z; 'f/, ~), where  ~  ~:  (6)  is a diagonal covariance matrix. [sent-48, score-0.152]
</p><p>39 Since lnp(x) does not directly depend on the variational parameters, maximizing B is equivalent to minimizing K. [sent-51, score-0.152]
</p><p>40 Using Lagrange multipliers, it is easy to show that the bound is maximized when q(z) = p(zlx), in which case K = 0 and B = lnp(x). [sent-53, score-0.037]
</p><p>41 j(z)] = E[II z:idSik ] = IIE[z:ik+sik ] = II mSik+Sik(T)k,¢k), k  k  (11)  k  where mn(T), ¢) is the nth moment under a Gaussian with mean T) and variance ¢. [sent-57, score-0.038]
</p><p>42 Closed forms for the mn(T), ¢) are found by setting derivatives of the Gaussian moment generating function to zero:  (12) After substituting the closed forms for the moments, B is a polynomial in the T)k 'S and the (Pk's. [sent-58, score-0.038]
</p><p>43 For each training case, B is maximized with respect to the T)k'S and the ¢k'S using, e. [sent-59, score-0.074]
</p><p>44 The model parameters A and W that maximize the sum of the bounds for the training cases can be computed directly, since W does not affect the solution for A, B is quadratic in A , and the optimal W can be written in terms of A and the variational parameters. [sent-62, score-0.189]
</p><p>45 If the power of each latent variable is restricted to be 0 or 1 in each monomial, o :S Sik :S 1, the above expressions simplify to  k  k  In this case, we can directly maximize B with respect to each T)k in turn, since B is quadratic in each T)k. [sent-63, score-0.116]
</p><p>46 1 Classification results on the Wisconsin breast cancer database: We obtained results on using product analysis for classification of malignant and benign cancer using the breast cancer database provided by Dr. [sent-65, score-1.156]
</p><p>47 Each observation in the database is characterized by nine cytological  a)  b)  . [sent-68, score-0.083]
</p><p>48 Mean images learned using b) product analysis c) mixture of gaussians  features, namely, lump thickness, uniformity of cell and shape, marginal adhesion , single epithelial cell size, bare nuclei, bland chromotin, normal nucleoli and mitoses. [sent-75, score-0.549]
</p><p>49 In their earlier work (Wolberg and Mangasarian 1990) , the authors used linear programming for classification. [sent-77, score-0.042]
</p><p>50 The objective was to find a hyperplane that separates the classes of malignant and benign cancer. [sent-78, score-0.308]
</p><p>51 In the absence of a separating plane, average sum of misclassifications of each class is minimized. [sent-79, score-0.046]
</p><p>52 Our approach is learn one density model for the benign feature vectors and a second density model for the malignant feature vectors and then use Bayes rule to classify an input vector. [sent-80, score-0.525]
</p><p>53 1 % error rate on 369 instances, we used the same set for our learning scheme and found that the product analysis produced 4% misclassfication. [sent-82, score-0.363]
</p><p>54 In addition, to compare the recognition rate of product analysis with the recognition rate of factor analysis, we divided the data set into 3 sets for training, validation and testing. [sent-83, score-0.633]
</p><p>55 The parameters of the model are learned using the training set, and tested on the validation set. [sent-84, score-0.158]
</p><p>56 This is repeated for 20 times , remembering the parameters that provided the best classification rate on the validation set. [sent-85, score-0.139]
</p><p>57 Finally, the parameters that provided the best performance on the validation set is used to classify the test set, only once. [sent-86, score-0.077]
</p><p>58 Since the data is limited, we perform this experimentation on 4 different random breakups of data into training, validation and test set. [sent-87, score-0.15]
</p><p>59 For product analysis model, we chose 3 hidden variables without optimization but for factor analysis , we chose the optimum number of factors. [sent-88, score-0.927]
</p><p>60 The average error rate on the 4 breakups was 5% using product analysis and 5. [sent-89, score-0.436]
</p><p>61 Figure 2: Images generated from the learned mixture of product analyzers  Figure 3: First row: Observation. [sent-91, score-0.467]
</p><p>62 Second row: corresponding image normalized for translation and lighting after lighting & transformation invariant model is learned 5. [sent-92, score-0.756]
</p><p>63 2 Mixture of lighting invariant appearance models: Often, objects are imaged under different illuminants. [sent-93, score-0.435]
</p><p>64 To learn an appearance model, we want to automatically remove the lighting effects and infer lighting-normalized images. [sent-94, score-0.418]
</p><p>65 Since ambient light intensity and reflectances of patches on the object multiply to produce a lighting-affected image, we can model lighting-invariance using a product analyzer. [sent-95, score-0.42]
</p><p>66 P(x,z) = P(xlz)P(z), where x is the vector of pixel intensities of the observation, Zl is the random variable describing the light intensity, and the remaining Zi are the pixel intensities in the lighting normalized image. [sent-96, score-0.422]
</p><p>67 We learn the distribution over z, where f(z) = [ZlZ2, ZlZ3, . [sent-97, score-0.049]
</p><p>68 By infering Zl, we can remove its effect on observation. [sent-101, score-0.063]
</p><p>69 The mixture model of product analyzer has joint distribution 7r c P(xlz)P(z), where 7rc is the probability of each class. [sent-102, score-0.442]
</p><p>70 It can be used to infer various kinds of images (e. [sent-103, score-0.072]
</p><p>71 We trained this model on images with 2 different poses of the same person(Fig. [sent-106, score-0.072]
</p><p>72 The variation in the images is governed by change in pose, light, and background clutter. [sent-108, score-0.108]
</p><p>73 lc compares the components learned using a mixture of product analyzers and a mixture of Gaussians. [sent-111, score-0.591]
</p><p>74 Due to limited variation in the pose and large variation in lighting, the mixture of gaussians is unable to extract the mean images. [sent-112, score-0.194]
</p><p>75 However, mixture of product analyzers is able to capture the distributions well. [sent-113, score-0.423]
</p><p>76 3 Transformation and lighting invariant appearance models: Geometric transformations like shift and shearing can occur when scenes are imaged. [sent-117, score-0.435]
</p><p>77 Transformation invariant mixtures of Guassians and factor analyzers (Frey and Jojic 2002; Jojic et al. [sent-118, score-0.416]
</p><p>78 Here, we add lighting-invariance to this framework enabling clustering based on interesting features such as pose, without concern for transformation and lighting effects. [sent-120, score-0.336]
</p><p>79 6  Conclusions  We introduced a density model that explains observations as products of hidden variables and we presented a variational technique for inference and learning in this model. [sent-121, score-0.602]
</p><p>80 On the Wisonsin breast cancer data, we found that product analysis outperforms factor analysis, when used with Bayes rule for pattern classification. [sent-122, score-0.723]
</p><p>81 We also found that product analysis was able to separate the two hidden causes, lighting and image noise in noisy images with varying illumination and varying pose. [sent-123, score-0.981]
</p><p>82 Neural networks and principal components analysis: Learning from examples without local minima. [sent-126, score-0.216]
</p><p>83 , editors, Advances in N eural Information Processing Systems 6, pages 152- 159. [sent-198, score-0.051]
</p><p>84 Unpublished manuscript available over the internet by ftp at ftp:/ /ftp. [sent-212, score-0.058]
</p><p>85 Multisurface method of pattern separation for medical diagnosis applied to breast cytology. [sent-254, score-0.082]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lighting', 0.268), ('product', 0.246), ('lnp', 0.221), ('factor', 0.193), ('sik', 0.189), ('hidden', 0.182), ('xlz', 0.182), ('principal', 0.162), ('variational', 0.152), ('lji', 0.145), ('malignant', 0.145), ('jojic', 0.144), ('zlx', 0.126), ('wolberg', 0.126), ('analyzer', 0.126), ('benign', 0.126), ('normally', 0.123), ('analysis', 0.117), ('latent', 0.116), ('bishop', 0.115), ('monomial', 0.115), ('thayer', 0.109), ('analyzers', 0.107), ('tipping', 0.101), ('appearance', 0.101), ('mangasarian', 0.095), ('products', 0.09), ('sensor', 0.088), ('cancer', 0.085), ('nonlinear', 0.082), ('breast', 0.082), ('em', 0.078), ('validation', 0.077), ('ji', 0.075), ('baldi', 0.073), ('breakups', 0.073), ('diamantaras', 0.073), ('kung', 0.073), ('zzik', 0.073), ('images', 0.072), ('combination', 0.072), ('variables', 0.072), ('input', 0.071), ('mixture', 0.07), ('modeled', 0.069), ('rubin', 0.069), ('transformation', 0.068), ('hinton', 0.067), ('density', 0.067), ('invariant', 0.066), ('light', 0.066), ('frey', 0.064), ('kambhatla', 0.063), ('infering', 0.063), ('classification', 0.062), ('intensity', 0.06), ('ftp', 0.058), ('hornik', 0.058), ('noise', 0.054), ('components', 0.054), ('distributed', 0.053), ('jordan', 0.053), ('ghahramani', 0.052), ('pose', 0.052), ('tr', 0.051), ('eural', 0.051), ('topographic', 0.051), ('mixtures', 0.05), ('learn', 0.049), ('generative', 0.048), ('patches', 0.048), ('variances', 0.047), ('separating', 0.046), ('zl', 0.046), ('learned', 0.044), ('intensities', 0.044), ('tenenbaum', 0.044), ('goals', 0.044), ('ii', 0.043), ('image', 0.042), ('neal', 0.042), ('mn', 0.042), ('linear', 0.042), ('observation', 0.042), ('database', 0.041), ('mackay', 0.04), ('toronto', 0.04), ('style', 0.04), ('inference', 0.039), ('moment', 0.038), ('finds', 0.038), ('separates', 0.037), ('maximized', 0.037), ('training', 0.037), ('unobserved', 0.036), ('freeman', 0.036), ('variation', 0.036), ('maintaining', 0.035), ('williams', 0.035), ('relationships', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="153-tfidf-1" href="./nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables.html">153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</a></p>
<p>Author: Brendan J. Frey, Anitha Kannan, Nebojsa Jojic</p><p>Abstract: Factor analysis and principal components analysis can be used to model linear relationships between observed variables and linearly map high-dimensional data to a lower-dimensional hidden space. In factor analysis, the observations are modeled as a linear combination of normally distributed hidden variables. We describe a nonlinear generalization of factor analysis , called</p><p>2 0.18520324 <a title="153-tfidf-2" href="./nips-2001-Fast%2C_Large-Scale_Transformation-Invariant_Clustering.html">75 nips-2001-Fast, Large-Scale Transformation-Invariant Clustering</a></p>
<p>Author: Brendan J. Frey, Nebojsa Jojic</p><p>Abstract: In previous work on “transformed mixtures of Gaussians” and “transformed hidden Markov models”, we showed how the EM algorithm in a discrete latent variable model can be used to jointly normalize data (e.g., center images, pitch-normalize spectrograms) and learn a mixture model of the normalized data. The only input to the algorithm is the data, a list of possible transformations, and the number of clusters to ﬁnd. The main criticism of this work was that the exhaustive computation of the posterior probabilities over transformations would make scaling up to large feature vectors and large sets of transformations intractable. Here, we describe how a tremendous speed-up is acheived through the use of a variational technique for decoupling transformations, and a fast Fourier transform method for computing posterior probabilities. For N ×N images, learning C clusters under N rotations, N scales, N x-translations and N y-translations takes only (C + 2 log N )N 2 scalar operations per iteration. In contrast, the original algorithm takes CN 6 operations to account for these transformations. We give results on learning a 4-component mixture model from a video sequence with frames of size 320 ×240. The model accounts for 360 rotations and 76,800 translations. Each iteration of EM takes only 10 seconds per frame in MATLAB, which is over 5 million times faster than the original algorithm. 1</p><p>3 0.16672152 <a title="153-tfidf-3" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>Author: Sam T. Roweis, Lawrence K. Saul, Geoffrey E. Hinton</p><p>Abstract: High dimensional data that lies on or near a low dimensional manifold can be described by a collection of local linear models. Such a description, however, does not provide a global parameterization of the manifold—arguably an important goal of unsupervised learning. In this paper, we show how to learn a collection of local linear models that solves this more difﬁcult problem. Our local linear models are represented by a mixture of factor analyzers, and the “global coordination” of these models is achieved by adding a regularizing term to the standard maximum likelihood objective function. The regularizer breaks a degeneracy in the mixture model’s parameter space, favoring models whose internal coordinate systems are aligned in a consistent way. As a result, the internal coordinates change smoothly and continuously as one traverses a connected path on the manifold—even when the path crosses the domains of many different local models. The regularizer takes the form of a Kullback-Leibler divergence and illustrates an unexpected application of variational methods: not to perform approximate inference in intractable probabilistic models, but to learn more useful internal representations in tractable ones. 1 Manifold Learning Consider an ensemble of images, each of which contains a face against a neutral background. Each image can be represented by a point in the high dimensional vector space of pixel intensities. This representation, however, does not exploit the strong correlations between pixels of the same image, nor does it support many useful operations for reasoning about faces. If, for example, we select two images with faces in widely different locations and then average their pixel intensities, we do not obtain an image of a face at their average location. Images of faces lie on or near a low-dimensional, curved manifold, and we can represent them more usefully by the coordinates on this manifold than by pixel intensities. Using these “intrinsic coordinates”, the average of two faces is another face with the average of their locations, poses and expressions. To analyze and manipulate faces, it is helpful to imagine a “magic black box” with levers or dials corresponding to the intrinsic coordinates on this manifold. Given a setting of the levers and dials, the box generates an image of a face. Given an image of a face, the box deduces the appropriate setting of the levers and dials. In this paper, we describe a fairly general way to construct such a box automatically from an ensemble of high-dimensional vectors. We assume only that there exists an underlying manifold of low dimensionality and that the relationship between the raw data and the manifold coordinates is locally linear and smoothly varying. Thus our method applies not only to images of faces, but also to many other forms of highly distributed perceptual and scientiﬁc data (e.g., spectrograms of speech, robotic sensors, gene expression arrays, document collections). 2 Local Linear Models The global structure of perceptual manifolds (such as images of faces) tends to be highly nonlinear. Fortunately, despite their complicated global structure, we can usually characterize these manifolds as locally linear. Thus, to a good approximation, they can be represented by collections of simpler models, each of which describes a locally linear neighborhood[3, 6, 8]. For unsupervised learning tasks, a probabilistic model that nicely captures this intuition is a mixture of factor analyzers (MFA)[5]. The model is used to describe high dimensional data that lies on or near a lower dimensional manifold. MFAs parameterize a joint distribution over observed and hidden variables: (1) where the observed variable, , represents the high dimensional data; the discrete hidden variables, , indexes different neighborhoods on the manifold; and the continuous hidden variables, , represent low dimensional local coordinates. The model assumes that data is sampled from different neighborhoods on the manifold with prior probabilities , and that within each neighborhood, the data’s local coordinates are normally distributed1 as:  RP&¤§¢  Q  ¡ I 0 (  3HGF D C¥@@@¥ 8¥75 ( E¨BAA9¨©©64§ 2 0 ( 31)£ ¥ ¡    ¡   ¥ ¡     ¥ §¥ ¡ &¤§¢'&§ %#¤¢$#¨</p><p>4 0.1230462 <a title="153-tfidf-4" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>Author: Peter Sykacek, Stephen J. Roberts</p><p>Abstract: This paper proposes an approach to classiﬁcation of adjacent segments of a time series as being either of classes. We use a hierarchical model that consists of a feature extraction stage and a generative classiﬁer which is built on top of these features. Such two stage approaches are often used in signal and image processing. The novel part of our work is that we link these stages probabilistically by using a latent feature space. To use one joint model is a Bayesian requirement, which has the advantage to fuse information according to its certainty. The classiﬁer is implemented as hidden Markov model with Gaussian and Multinomial observation distributions deﬁned on a suitably chosen representation of autoregressive models. The Markov dependency is motivated by the assumption that successive classiﬁcations will be correlated. Inference is done with Markov chain Monte Carlo (MCMC) techniques. We apply the proposed approach to synthetic data and to classiﬁcation of EEG that was recorded while the subjects performed different cognitive tasks. All experiments show that using a latent feature space results in a signiﬁcant improvement in generalization accuracy. Hence we expect that this idea generalizes well to other hierarchical models.</p><p>5 0.11279272 <a title="153-tfidf-5" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>Author: Matthias Seeger</p><p>Abstract: We propose the framework of mutual information kernels for learning covariance kernels, as used in Support Vector machines and Gaussian process classifiers, from unlabeled task data using Bayesian techniques. We describe an implementation of this framework which uses variational Bayesian mixtures of factor analyzers in order to attack classification problems in high-dimensional spaces where labeled data is sparse, but unlabeled data is abundant. 1</p><p>6 0.11060727 <a title="153-tfidf-6" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>7 0.10597898 <a title="153-tfidf-7" href="./nips-2001-Products_of_Gaussians.html">154 nips-2001-Products of Gaussians</a></p>
<p>8 0.099100783 <a title="153-tfidf-8" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>9 0.097026378 <a title="153-tfidf-9" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>10 0.095648751 <a title="153-tfidf-10" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>11 0.086472183 <a title="153-tfidf-11" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>12 0.083876006 <a title="153-tfidf-12" href="./nips-2001-A_Parallel_Mixture_of_SVMs_for_Very_Large_Scale_Problems.html">16 nips-2001-A Parallel Mixture of SVMs for Very Large Scale Problems</a></p>
<p>13 0.083738156 <a title="153-tfidf-13" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>14 0.082881011 <a title="153-tfidf-14" href="./nips-2001-Transform-invariant_Image_Decomposition_with_Similarity_Templates.html">191 nips-2001-Transform-invariant Image Decomposition with Similarity Templates</a></p>
<p>15 0.080686942 <a title="153-tfidf-15" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>16 0.077351891 <a title="153-tfidf-16" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>17 0.076954655 <a title="153-tfidf-17" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<p>18 0.075072125 <a title="153-tfidf-18" href="./nips-2001-ALGONQUIN_-_Learning_Dynamic_Noise_Models_From_Noisy_Speech_for_Robust_Speech_Recognition.html">4 nips-2001-ALGONQUIN - Learning Dynamic Noise Models From Noisy Speech for Robust Speech Recognition</a></p>
<p>19 0.074421324 <a title="153-tfidf-19" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>20 0.073555596 <a title="153-tfidf-20" href="./nips-2001-Multi_Dimensional_ICA_to_Separate_Correlated_Sources.html">127 nips-2001-Multi Dimensional ICA to Separate Correlated Sources</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.251), (1, 0.045), (2, -0.057), (3, -0.125), (4, -0.131), (5, -0.0), (6, -0.006), (7, -0.016), (8, 0.031), (9, -0.062), (10, 0.023), (11, 0.095), (12, 0.169), (13, -0.227), (14, -0.071), (15, 0.041), (16, -0.008), (17, -0.021), (18, -0.002), (19, 0.054), (20, -0.003), (21, 0.022), (22, 0.075), (23, -0.067), (24, -0.032), (25, -0.062), (26, 0.153), (27, 0.061), (28, -0.115), (29, -0.058), (30, 0.032), (31, -0.028), (32, -0.032), (33, 0.025), (34, -0.034), (35, -0.146), (36, 0.018), (37, -0.025), (38, -0.003), (39, -0.06), (40, 0.17), (41, 0.039), (42, 0.063), (43, -0.183), (44, -0.031), (45, 0.004), (46, -0.026), (47, -0.012), (48, -0.003), (49, -0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96243727 <a title="153-lsi-1" href="./nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables.html">153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</a></p>
<p>Author: Brendan J. Frey, Anitha Kannan, Nebojsa Jojic</p><p>Abstract: Factor analysis and principal components analysis can be used to model linear relationships between observed variables and linearly map high-dimensional data to a lower-dimensional hidden space. In factor analysis, the observations are modeled as a linear combination of normally distributed hidden variables. We describe a nonlinear generalization of factor analysis , called</p><p>2 0.85809201 <a title="153-lsi-2" href="./nips-2001-Fast%2C_Large-Scale_Transformation-Invariant_Clustering.html">75 nips-2001-Fast, Large-Scale Transformation-Invariant Clustering</a></p>
<p>Author: Brendan J. Frey, Nebojsa Jojic</p><p>Abstract: In previous work on “transformed mixtures of Gaussians” and “transformed hidden Markov models”, we showed how the EM algorithm in a discrete latent variable model can be used to jointly normalize data (e.g., center images, pitch-normalize spectrograms) and learn a mixture model of the normalized data. The only input to the algorithm is the data, a list of possible transformations, and the number of clusters to ﬁnd. The main criticism of this work was that the exhaustive computation of the posterior probabilities over transformations would make scaling up to large feature vectors and large sets of transformations intractable. Here, we describe how a tremendous speed-up is acheived through the use of a variational technique for decoupling transformations, and a fast Fourier transform method for computing posterior probabilities. For N ×N images, learning C clusters under N rotations, N scales, N x-translations and N y-translations takes only (C + 2 log N )N 2 scalar operations per iteration. In contrast, the original algorithm takes CN 6 operations to account for these transformations. We give results on learning a 4-component mixture model from a video sequence with frames of size 320 ×240. The model accounts for 360 rotations and 76,800 translations. Each iteration of EM takes only 10 seconds per frame in MATLAB, which is over 5 million times faster than the original algorithm. 1</p><p>3 0.68885165 <a title="153-lsi-3" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>Author: Sam T. Roweis, Lawrence K. Saul, Geoffrey E. Hinton</p><p>Abstract: High dimensional data that lies on or near a low dimensional manifold can be described by a collection of local linear models. Such a description, however, does not provide a global parameterization of the manifold—arguably an important goal of unsupervised learning. In this paper, we show how to learn a collection of local linear models that solves this more difﬁcult problem. Our local linear models are represented by a mixture of factor analyzers, and the “global coordination” of these models is achieved by adding a regularizing term to the standard maximum likelihood objective function. The regularizer breaks a degeneracy in the mixture model’s parameter space, favoring models whose internal coordinate systems are aligned in a consistent way. As a result, the internal coordinates change smoothly and continuously as one traverses a connected path on the manifold—even when the path crosses the domains of many different local models. The regularizer takes the form of a Kullback-Leibler divergence and illustrates an unexpected application of variational methods: not to perform approximate inference in intractable probabilistic models, but to learn more useful internal representations in tractable ones. 1 Manifold Learning Consider an ensemble of images, each of which contains a face against a neutral background. Each image can be represented by a point in the high dimensional vector space of pixel intensities. This representation, however, does not exploit the strong correlations between pixels of the same image, nor does it support many useful operations for reasoning about faces. If, for example, we select two images with faces in widely different locations and then average their pixel intensities, we do not obtain an image of a face at their average location. Images of faces lie on or near a low-dimensional, curved manifold, and we can represent them more usefully by the coordinates on this manifold than by pixel intensities. Using these “intrinsic coordinates”, the average of two faces is another face with the average of their locations, poses and expressions. To analyze and manipulate faces, it is helpful to imagine a “magic black box” with levers or dials corresponding to the intrinsic coordinates on this manifold. Given a setting of the levers and dials, the box generates an image of a face. Given an image of a face, the box deduces the appropriate setting of the levers and dials. In this paper, we describe a fairly general way to construct such a box automatically from an ensemble of high-dimensional vectors. We assume only that there exists an underlying manifold of low dimensionality and that the relationship between the raw data and the manifold coordinates is locally linear and smoothly varying. Thus our method applies not only to images of faces, but also to many other forms of highly distributed perceptual and scientiﬁc data (e.g., spectrograms of speech, robotic sensors, gene expression arrays, document collections). 2 Local Linear Models The global structure of perceptual manifolds (such as images of faces) tends to be highly nonlinear. Fortunately, despite their complicated global structure, we can usually characterize these manifolds as locally linear. Thus, to a good approximation, they can be represented by collections of simpler models, each of which describes a locally linear neighborhood[3, 6, 8]. For unsupervised learning tasks, a probabilistic model that nicely captures this intuition is a mixture of factor analyzers (MFA)[5]. The model is used to describe high dimensional data that lies on or near a lower dimensional manifold. MFAs parameterize a joint distribution over observed and hidden variables: (1) where the observed variable, , represents the high dimensional data; the discrete hidden variables, , indexes different neighborhoods on the manifold; and the continuous hidden variables, , represent low dimensional local coordinates. The model assumes that data is sampled from different neighborhoods on the manifold with prior probabilities , and that within each neighborhood, the data’s local coordinates are normally distributed1 as:  RP&¤§¢  Q  ¡ I 0 (  3HGF D C¥@@@¥ 8¥75 ( E¨BAA9¨©©64§ 2 0 ( 31)£ ¥ ¡    ¡   ¥ ¡     ¥ §¥ ¡ &¤§¢'&§ %#¤¢$#¨</p><p>4 0.51206541 <a title="153-lsi-4" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>Author: Lawrence K. Saul, Daniel D. Lee</p><p>Abstract: We investigate a learning algorithm for the classiﬁcation of nonnegative data by mixture models. Multiplicative update rules are derived that directly optimize the performance of these models as classiﬁers. The update rules have a simple closed form and an intuitive appeal. Our algorithm retains the main virtues of the Expectation-Maximization (EM) algorithm—its guarantee of monotonic improvement, and its absence of tuning parameters—with the added advantage of optimizing a discriminative objective function. The algorithm reduces as a special case to the method of generalized iterative scaling for log-linear models. The learning rate of the algorithm is controlled by the sparseness of the training data. We use the method of nonnegative matrix factorization (NMF) to discover sparse distributed representations of the data. This form of feature selection greatly accelerates learning and makes the algorithm practical on large problems. Experiments show that discriminatively trained mixture models lead to much better classiﬁcation than comparably sized models trained by EM. 1</p><p>5 0.50332606 <a title="153-lsi-5" href="./nips-2001-Products_of_Gaussians.html">154 nips-2001-Products of Gaussians</a></p>
<p>Author: Christopher Williams, Felix V. Agakov, Stephen N. Felderhof</p><p>Abstract: Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. Below we consider PoE models in which each expert is a Gaussian. Although the product of Gaussians is also a Gaussian, if each Gaussian has a simple structure the product can have a richer structure. We examine (1) Products of Gaussian pancakes which give rise to probabilistic Minor Components Analysis, (2) products of I-factor PPCA models and (3) a products of experts construction for an AR(l) process. Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. In this paper we consider PoE models in which each expert is a Gaussian. It is easy to see that in this case the product model will also be Gaussian. However, if each Gaussian has a simple structure, the product can have a richer structure. Using Gaussian experts is attractive as it permits a thorough analysis of the product architecture, which can be difficult with other models , e.g. models defined over discrete random variables. Below we examine three cases of the products of Gaussians construction: (1) Products of Gaussian pancakes (PoGP) which give rise to probabilistic Minor Components Analysis (MCA), providing a complementary result to probabilistic Principal Components Analysis (PPCA) obtained by Tipping and Bishop (1999); (2) Products of I-factor PPCA models; (3) A products of experts construction for an AR(l) process. Products of Gaussians If each expert is a Gaussian pi(xI8 i ) '</p><p>6 0.48547927 <a title="153-lsi-6" href="./nips-2001-Transform-invariant_Image_Decomposition_with_Similarity_Templates.html">191 nips-2001-Transform-invariant Image Decomposition with Similarity Templates</a></p>
<p>7 0.46628842 <a title="153-lsi-7" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>8 0.46089789 <a title="153-lsi-8" href="./nips-2001-Learning_Body_Pose_via_Specialized_Maps.html">108 nips-2001-Learning Body Pose via Specialized Maps</a></p>
<p>9 0.45714268 <a title="153-lsi-9" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>10 0.44075805 <a title="153-lsi-10" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>11 0.43534389 <a title="153-lsi-11" href="./nips-2001-Learning_Discriminative_Feature_Transforms_to_Low_Dimensions_in_Low_Dimentions.html">109 nips-2001-Learning Discriminative Feature Transforms to Low Dimensions in Low Dimentions</a></p>
<p>12 0.4251712 <a title="153-lsi-12" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>13 0.42473269 <a title="153-lsi-13" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>14 0.41479266 <a title="153-lsi-14" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<p>15 0.40742782 <a title="153-lsi-15" href="./nips-2001-Geometrical_Singularities_in_the_Neuromanifold_of_Multilayer_Perceptrons.html">83 nips-2001-Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons</a></p>
<p>16 0.40393889 <a title="153-lsi-16" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>17 0.390046 <a title="153-lsi-17" href="./nips-2001-A_Parallel_Mixture_of_SVMs_for_Very_Large_Scale_Problems.html">16 nips-2001-A Parallel Mixture of SVMs for Very Large Scale Problems</a></p>
<p>18 0.3806521 <a title="153-lsi-18" href="./nips-2001-Estimating_Car_Insurance_Premia%3A_a_Case_Study_in_High-Dimensional_Data_Inference.html">70 nips-2001-Estimating Car Insurance Premia: a Case Study in High-Dimensional Data Inference</a></p>
<p>19 0.3775748 <a title="153-lsi-19" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>20 0.37534535 <a title="153-lsi-20" href="./nips-2001-Quantizing_Density_Estimators.html">155 nips-2001-Quantizing Density Estimators</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.054), (17, 0.018), (19, 0.027), (27, 0.118), (30, 0.064), (38, 0.039), (49, 0.308), (59, 0.051), (72, 0.063), (79, 0.039), (83, 0.024), (88, 0.016), (91, 0.112)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87065077 <a title="153-lda-1" href="./nips-2001-Rao-Blackwellised_Particle_Filtering_via_Data_Augmentation.html">156 nips-2001-Rao-Blackwellised Particle Filtering via Data Augmentation</a></p>
<p>Author: Christophe Andrieu, Nando D. Freitas, Arnaud Doucet</p><p>Abstract: In this paper, we extend the Rao-Blackwellised particle filtering method to more complex hybrid models consisting of Gaussian latent variables and discrete observations. This is accomplished by augmenting the models with artificial variables that enable us to apply Rao-Blackwellisation. Other improvements include the design of an optimal importance proposal distribution and being able to swap the sampling an selection steps to handle outliers. We focus on sequential binary classifiers that consist of linear combinations of basis functions , whose coefficients evolve according to a Gaussian smoothness prior. Our results show significant improvements. 1</p><p>same-paper 2 0.79859054 <a title="153-lda-2" href="./nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables.html">153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</a></p>
<p>Author: Brendan J. Frey, Anitha Kannan, Nebojsa Jojic</p><p>Abstract: Factor analysis and principal components analysis can be used to model linear relationships between observed variables and linearly map high-dimensional data to a lower-dimensional hidden space. In factor analysis, the observations are modeled as a linear combination of normally distributed hidden variables. We describe a nonlinear generalization of factor analysis , called</p><p>3 0.73646319 <a title="153-lda-3" href="./nips-2001-Active_Learning_in_the_Drug_Discovery_Process.html">25 nips-2001-Active Learning in the Drug Discovery Process</a></p>
<p>Author: Manfred K. Warmuth, Gunnar Rätsch, Michael Mathieson, Jun Liao, Christian Lemmen</p><p>Abstract: We investigate the following data mining problem from Computational Chemistry: From a large data set of compounds, ﬁnd those that bind to a target molecule in as few iterations of biological testing as possible. In each iteration a comparatively small batch of compounds is screened for binding to the target. We apply active learning techniques for selecting the successive batches. One selection strategy picks unlabeled examples closest to the maximum margin hyperplane. Another produces many weight vectors by running perceptrons over multiple permutations of the data. Each weight vector votes with its prediction and we pick the unlabeled examples for which the prediction is most evenly split between and . For a third selection strategy note that each unlabeled example bisects the version space of consistent weight vectors. We estimate the volume on both sides of the split by bouncing a billiard through the version space and select unlabeled examples that cause the most even split of the version space. We demonstrate that on two data sets provided by DuPont Pharmaceuticals that all three selection strategies perform comparably well and are much better than selecting random batches for testing. § © ¨</p><p>4 0.55619419 <a title="153-lda-4" href="./nips-2001-Fast%2C_Large-Scale_Transformation-Invariant_Clustering.html">75 nips-2001-Fast, Large-Scale Transformation-Invariant Clustering</a></p>
<p>Author: Brendan J. Frey, Nebojsa Jojic</p><p>Abstract: In previous work on “transformed mixtures of Gaussians” and “transformed hidden Markov models”, we showed how the EM algorithm in a discrete latent variable model can be used to jointly normalize data (e.g., center images, pitch-normalize spectrograms) and learn a mixture model of the normalized data. The only input to the algorithm is the data, a list of possible transformations, and the number of clusters to ﬁnd. The main criticism of this work was that the exhaustive computation of the posterior probabilities over transformations would make scaling up to large feature vectors and large sets of transformations intractable. Here, we describe how a tremendous speed-up is acheived through the use of a variational technique for decoupling transformations, and a fast Fourier transform method for computing posterior probabilities. For N ×N images, learning C clusters under N rotations, N scales, N x-translations and N y-translations takes only (C + 2 log N )N 2 scalar operations per iteration. In contrast, the original algorithm takes CN 6 operations to account for these transformations. We give results on learning a 4-component mixture model from a video sequence with frames of size 320 ×240. The model accounts for 360 rotations and 76,800 translations. Each iteration of EM takes only 10 seconds per frame in MATLAB, which is over 5 million times faster than the original algorithm. 1</p><p>5 0.55516005 <a title="153-lda-5" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>Author: Sam T. Roweis, Lawrence K. Saul, Geoffrey E. Hinton</p><p>Abstract: High dimensional data that lies on or near a low dimensional manifold can be described by a collection of local linear models. Such a description, however, does not provide a global parameterization of the manifold—arguably an important goal of unsupervised learning. In this paper, we show how to learn a collection of local linear models that solves this more difﬁcult problem. Our local linear models are represented by a mixture of factor analyzers, and the “global coordination” of these models is achieved by adding a regularizing term to the standard maximum likelihood objective function. The regularizer breaks a degeneracy in the mixture model’s parameter space, favoring models whose internal coordinate systems are aligned in a consistent way. As a result, the internal coordinates change smoothly and continuously as one traverses a connected path on the manifold—even when the path crosses the domains of many different local models. The regularizer takes the form of a Kullback-Leibler divergence and illustrates an unexpected application of variational methods: not to perform approximate inference in intractable probabilistic models, but to learn more useful internal representations in tractable ones. 1 Manifold Learning Consider an ensemble of images, each of which contains a face against a neutral background. Each image can be represented by a point in the high dimensional vector space of pixel intensities. This representation, however, does not exploit the strong correlations between pixels of the same image, nor does it support many useful operations for reasoning about faces. If, for example, we select two images with faces in widely different locations and then average their pixel intensities, we do not obtain an image of a face at their average location. Images of faces lie on or near a low-dimensional, curved manifold, and we can represent them more usefully by the coordinates on this manifold than by pixel intensities. Using these “intrinsic coordinates”, the average of two faces is another face with the average of their locations, poses and expressions. To analyze and manipulate faces, it is helpful to imagine a “magic black box” with levers or dials corresponding to the intrinsic coordinates on this manifold. Given a setting of the levers and dials, the box generates an image of a face. Given an image of a face, the box deduces the appropriate setting of the levers and dials. In this paper, we describe a fairly general way to construct such a box automatically from an ensemble of high-dimensional vectors. We assume only that there exists an underlying manifold of low dimensionality and that the relationship between the raw data and the manifold coordinates is locally linear and smoothly varying. Thus our method applies not only to images of faces, but also to many other forms of highly distributed perceptual and scientiﬁc data (e.g., spectrograms of speech, robotic sensors, gene expression arrays, document collections). 2 Local Linear Models The global structure of perceptual manifolds (such as images of faces) tends to be highly nonlinear. Fortunately, despite their complicated global structure, we can usually characterize these manifolds as locally linear. Thus, to a good approximation, they can be represented by collections of simpler models, each of which describes a locally linear neighborhood[3, 6, 8]. For unsupervised learning tasks, a probabilistic model that nicely captures this intuition is a mixture of factor analyzers (MFA)[5]. The model is used to describe high dimensional data that lies on or near a lower dimensional manifold. MFAs parameterize a joint distribution over observed and hidden variables: (1) where the observed variable, , represents the high dimensional data; the discrete hidden variables, , indexes different neighborhoods on the manifold; and the continuous hidden variables, , represent low dimensional local coordinates. The model assumes that data is sampled from different neighborhoods on the manifold with prior probabilities , and that within each neighborhood, the data’s local coordinates are normally distributed1 as:  RP&¤§¢  Q  ¡ I 0 (  3HGF D C¥@@@¥ 8¥75 ( E¨BAA9¨©©64§ 2 0 ( 31)£ ¥ ¡    ¡   ¥ ¡     ¥ §¥ ¡ &¤§¢'&§ %#¤¢$#¨</p><p>6 0.54356426 <a title="153-lda-6" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>7 0.54323888 <a title="153-lda-7" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>8 0.5419715 <a title="153-lda-8" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>9 0.54024464 <a title="153-lda-9" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<p>10 0.53953183 <a title="153-lda-10" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>11 0.5393939 <a title="153-lda-11" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>12 0.53857589 <a title="153-lda-12" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>13 0.53697103 <a title="153-lda-13" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>14 0.53556383 <a title="153-lda-14" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>15 0.53470653 <a title="153-lda-15" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>16 0.53469241 <a title="153-lda-16" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>17 0.53439415 <a title="153-lda-17" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>18 0.53396416 <a title="153-lda-18" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>19 0.53270286 <a title="153-lda-19" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>20 0.53265315 <a title="153-lda-20" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
