<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>76 nips-2001-Fast Parameter Estimation Using Green's Functions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-76" href="#">nips2001-76</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>76 nips-2001-Fast Parameter Estimation Using Green's Functions</h1>
<br/><p>Source: <a title="nips-2001-76-pdf" href="http://papers.nips.cc/paper/2104-fast-parameter-estimation-using-greens-functions.pdf">pdf</a></p><p>Author: K. Wong, F. Li</p><p>Abstract: We propose a method for the fast estimation of hyperparameters in large networks, based on the linear response relation in the cavity method, and an empirical measurement of the Green's function. Simulation results show that it is efficient and precise, when compared with cross-validation and other techniques which require matrix inversion. 1</p><p>Reference: <a title="nips-2001-76-reference" href="../nips2001_reference/nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 en  Abstract We propose a method for the fast estimation of hyperparameters in large networks, based on the linear response relation in the cavity method, and an empirical measurement of the Green's function. [sent-6, score-0.993]
</p><p>2 1  Introduction  It is well known that correct choices of hyperparameters in classification and regression tasks can optimize the complexity of the data model , and hence achieve the best generalization [1]. [sent-8, score-0.22]
</p><p>3 In recent years various methods have been proposed to estimate the optimal hyperparameters in different contexts, such as neural networks [2], support vector machines [3, 4, 5] and Gaussian processes [5]. [sent-9, score-0.251]
</p><p>4 While the leave-one-out procedure gives an almost unbiased estimate of the generalization error, it is nevertheless very tedious. [sent-11, score-0.119]
</p><p>5 In this paper, we propose a new approach to hyperparameter estimation in large systems. [sent-14, score-0.21]
</p><p>6 It is known that large networks are mean-field systems, so that when one example is removed by the leave-one-out procedure, the background adjustment can be analyzed by a self-consistent perturbation approach. [sent-15, score-0.045]
</p><p>7 They usually involve a macroscopic number of unknown variables, whose solution is obtained through the inversion of a matrix of macroscopic size, or iteration. [sent-17, score-0.186]
</p><p>8 Here we take a further step to replace it by a direct measurement of the Green's function via a small number of learning processes. [sent-18, score-0.118]
</p><p>9 The proposed technique is based on the cavity method, which was adapted from disordered systems in many-body physics. [sent-21, score-0.599]
</p><p>10 The basis of the cavity method is a self-consistent argument addressing the situation of removing an example from the system. [sent-22, score-0.658]
</p><p>11 The change on removing an example is described by the Green's function, which is an extremely general technique used in a wide range of quantum and classical problems in many-body physics [8]. [sent-23, score-0.067]
</p><p>12 In this paper, we consider two applications of the cavity method to hyperparameter estimation, namely the optimal weight decay and the optimal learning time in feedforward networks. [sent-25, score-1.085]
</p><p>13 In the latter application, the cavity method provides, as far as we are aware of, the only estimate of the hyperparameter beyond empirical stopping criteria and brute force cross-validation. [sent-26, score-0.98]
</p><p>14 An energy function E is defined with respect to a set of p examples with inputs and outputs respectively given by {IL and y'", JL = 1, . [sent-28, score-0.048]
</p><p>15 We will first focus on the dynamics of a single-layer feedforward network and generalize the results to multilayer networks later. [sent-32, score-0.195]
</p><p>16 (1)  '"  Here f( x'" , y'") represents the error function with respect to example JL. [sent-34, score-0.047]
</p><p>17 It is expressed in terms of the activation x'" == w· (IL. [sent-35, score-0.064]
</p><p>18 R( w) represents a regularization term which is introduced to limit the complexity of the network and hence enhance the generalization ability. [sent-36, score-0.125]
</p><p>19 Hence if we compare the evolution of Wj(t) with another system Wj(t) with a continuous perturbative stimulus Jhj(t), we would have  dWj(t) = _~ oE dt Now. [sent-38, score-0.244]
</p><p>20 (4)  J  +  and the linear response relation  Wj(t) = Wj(t)  +L  J  k  Now we consider the evolution ofthe network w;'"(t) in which example JL is omitted from the training set. [sent-40, score-0.211]
</p><p>21 For a system learning macroscopic number of examples, the changes induced by the omission of an example are perturbative, and we can assume that the system has a linear response. [sent-41, score-0.075]
</p><p>22 Compared with the original network Wj(t), the gradient of the error of example JL now plays the role of the stimulus in (3). [sent-42, score-0.224]
</p><p>23 When the dynamics has reached the steady state, we arrive at  I-'  hI-' = x  where, = limt--+oo  +,  OE(XI-' , yl-') oxl-' '  JdS[Ljk ~fGjk (t , s)~r]jN  (7)  is the susceptibility. [sent-45, score-0.18]
</p><p>24 At time t , the generalization error is defined as the error function averaged over the distribution of input (, and their corresponding output y, i. [sent-46, score-0.179]
</p><p>25 The leave-one-out generalization error is an estimate of 109 given in terms ofthe cavity activations hI-' by fg = LI-' 10 (hI-' ,yl-')jp. [sent-50, score-0.926]
</p><p>26 Hence if we can estimate the Green's function, the cavity activation in (7) provides a convenient way to estimate the leave-one-out generalization error without really having to undergo the validation process. [sent-51, score-0.931]
</p><p>27 While self-consistent equations for the Green's function have been derived using diagrammatic methods [9], their solutions cannot be computed except for the specific case of time-translational invariant Green's functions , such as those in Adaline learning or linear regression. [sent-52, score-0.051]
</p><p>28 However, the linear response relation (4) provides a convenient way to measure the Green's function in the general case. [sent-53, score-0.106]
</p><p>29 The basic idea is to perform two learning processes in parallel, one following the original process (2) and the other having a constant stimulus as in (3) with 6hj (t) = TJ6jk, where 8j k is the Kronecka delta. [sent-54, score-0.106]
</p><p>30 When the dynamics has reached the steady state, the measurement Wj - Wj yields the quantity TJ Lk dsGjk(t, s). [sent-55, score-0.369]
</p><p>31 J  A simple averaging procedure, replacing all the pairwise measurements between the stimulation node k and observation node j, can be applied in the limit of large N. [sent-56, score-0.136]
</p><p>32 We first consider the case in which the inputs are independent and normalized, i. [sent-57, score-0.048]
</p><p>33 , Gjk(t , s) = G(t, s)8jk , independent of the node labels [9], rendering , = limt--+oo J dsG(t, s). [sent-62, score-0.068]
</p><p>34 In the case that the inputs are correlated and not normalized, we can apply standard procedures of whitening transformation to make them independent and normalized [1]. [sent-63, score-0.126]
</p><p>35 In large networks, one can use the diagrammatic analysis in [9] to show that the (unknown) distribution of inputs does not change the self-averaging property of the Green's functions after the whitening transformation. [sent-64, score-0.137]
</p><p>36 Thereafter, the measurement of Green's functions proceeds as described in the simpler case of independent and normalized inputs. [sent-65, score-0.158]
</p><p>37 Since hyperparameter estimation usually involves a series of computing fg at various hyperparameters, the one-time preprocessing does not increase the computational load significantly. [sent-66, score-0.255]
</p><p>38 Thus the susceptibility, can be measured by comparing the evolution of two processes: one following the original process (2), and the other having a constant stimulus as in (3) with 8h j (t) = TJ for all j. [sent-67, score-0.133]
</p><p>39 When the dynamics has reached the steady state, the measurement (Wj - Wj) yields the quantity TJ,. [sent-68, score-0.369]
</p><p>40 We illustrate the extension to two-layer networks by considering the committee machine, in which the errorfunction takes the form E(2:: a ! [sent-69, score-0.085]
</p><p>41 (x a), y) , where a = 1,· ··, nh is the label of a hidden node, Xa == wa . [sent-70, score-0.158]
</p><p>42 The generalization error is thus a function of the cavity activations of the hidden nodes, namely, E9 = 2::JL E(2::a ! [sent-73, score-0.855]
</p><p>43 When the inputs are independent and normalized, they are related to the generic activations by  hJL- JL+'" aE(2::c ! [sent-76, score-0.125]
</p><p>44 (X~) , yJL) a - Xa ~ "lab a JL ' Xb b  (9)  where "lab = limt~ oo J dsGab(t, s) is the susceptibility tensor. [sent-77, score-0.111]
</p><p>45 The Green's function Gab(t, s) represents the response of a weight feeding hidden node a due to a stimulus applied at the gradient with respect to a weight feeding node b. [sent-78, score-0.583]
</p><p>46 It is obtained by monitoring nh + 1 learning processes, one being original and each of the other nh processes having constant stimuli at the gradients with respect to one of the hidden nodes, viz. [sent-79, score-0.304]
</p><p>47 ,nh·  When the dynamics has reached the steady state, the measurement (w~7 yields the quantity 'f)'Yab. [sent-83, score-0.369]
</p><p>48 Consider the case that the regularization R(w) takes the form of a weight decay term, R(w) = N 2::ab AabWa . [sent-85, score-0.191]
</p><p>49 The cavity activations will be given by  hJL = JL + '" a Xa ~ b  ( 1-  ,"  11  iJ 2:: j k ~'j(A + Q)~}bk~r  ) aE(2:: c ! [sent-87, score-0.676]
</p><p>50 Note also that second derivatives of the error term have been neglected. [sent-90, score-0.047]
</p><p>51 To verify the proposed method by simulations, we generate examples from a noisy teacher network which is a committee machine  yJL = ~ erf nh  (1 a · f ) + yf2B  (Jzw  (13)  Here Ba is the teacher vector at the hidden node a. [sent-91, score-0.791]
</p><p>52 Learning is done by the gradient descent of the energy function  (14)  and the weight decay parameter ,X is the hyperparameter to be optimized. [sent-94, score-0.454]
</p><p>53 The generalization error fg is given by  where the averaging is performed over the distribution of input { and noise z. [sent-95, score-0.177]
</p><p>54 However, this target result is only known by the teacher , since Tab and Rab are not accessible by the student. [sent-100, score-0.213]
</p><p>55 Four results are compared: the target generalization error observed by the teacher, and those estimated by the cavity method, cross-validation and extended LULOO. [sent-102, score-0.763]
</p><p>56 It can be seen that the cavity method yields estimates of the optimal weight decay with comparable precision as the other methods. [sent-103, score-0.96]
</p><p>57 For a more systematic comparison, we search for the optimal weight decay in 10 samples using golden section search [11] for the same parameters as in Fig. [sent-104, score-0.374]
</p><p>58 Compared with the target results, the standard deviations of the estimated optimal weight decays are 0. [sent-106, score-0.164]
</p><p>59 24 for the cavity method, sevenfold cross-validation and extended LULOO respectively. [sent-109, score-0.682]
</p><p>60 In another simulation of 80 samples of the singlelayer perceptron, the estimated optimal weight decays have standard deviations of 1. [sent-110, score-0.205]
</p><p>61 6 for the cavity method, tenfold cross-validation and extended LULOO respectively (the parameters in the simulations are N = 500, p = 400 and a ranging from 0. [sent-113, score-0.682]
</p><p>62 To put these results in perspective, we mention that the computational resources needed by the cavity method is much less than the other estimations. [sent-116, score-0.694]
</p><p>63 For example, in the single-layer perceptrons, the CPU time needed to estimate the optimal weight decay using the golden section search by the teacher, the cavity method, tenfold cross-validation and extended LULOO are in the ratio of 1 : 1. [sent-117, score-1.055]
</p><p>64 Before concluding this section, we mention that it is possible to derive an expression of the gradient dEg I d,X of the estimated generalization error with respect to the weight decay. [sent-121, score-0.309]
</p><p>65 This provides us an even more powerful tool for hyperparameter estimation. [sent-122, score-0.164]
</p><p>66 In the case of the search for one hyperparameter, the gradient enables us to use the binary search for the zero of the gradient, which converges faster than the golden section search. [sent-123, score-0.213]
</p><p>67 In the single-layer experiment we mentioned, its precision is comparable to fivefold cross-validations, and its CPU time is only 4% more than the teacher's search. [sent-124, score-0.113]
</p><p>68 In the case of more than one hyperparameters, the gradient information will save us the need for an exhaustive search over a multidimensional hyperparameter space. [sent-126, score-0.265]
</p><p>69 3  Dynamical Hyperparameter Estimation  The second example concerns the estimation of a dynamical hyperparameter, namely the optimal early stopping time, in cases where overtraining may plague the generalization ability at the steady state. [sent-127, score-0.52]
</p><p>70 In perceptrons, when the examples are noisy and the weight decay is weak, the generalization error decreases in the early stage of learning, reaches a minimum and then increases towards its asymptotic value [12, 9]. [sent-128, score-0.38]
</p><p>71 Since the early stopping point sets in before the system reaches the steady state, most analyses based on the equilibrium state are not applicable. [sent-129, score-0.274]
</p><p>72 Cross-validation stopping has been proposed as an empirical method to control overtraining [13]. [sent-130, score-0.227]
</p><p>73 Here we propose the cavity method as a convenient alternative. [sent-131, score-0.694]
</p><p>74 52 G----8 target  e Q)  (b)  G----EJ cavity  0 - 0 LULOO  <=  0  ~  . [sent-133, score-0.599]
</p><p>75 40  o  0 weight decay A  2 weight decay A  Figure 1: (a-d) The dependence ofthe generalization error of the multilayer perceptron on the weight decay for N = 200, p = 700, nh = 3, (J = 0. [sent-142, score-0.92]
</p><p>76 The solid symbols locate the optimal weight decays estimated by the teacher (circle), the cavity method (square), extended LULOO (diamond) and sevenfold cross-validation (triangle) . [sent-144, score-1.169]
</p><p>77 In single-layer perceptrons, the cavity activations of the examples evolve according to (6), enabling us to estimate the dynamical evolution of the estimated generalization error when learning proceeds. [sent-145, score-0.939]
</p><p>78 The remaining issue is the measurement of the time-dependent Green's function. [sent-146, score-0.118]
</p><p>79 Again, assuming normalized and independent inputs with (~j) = 0 and (~j~k) = Jjk , we can see from (4) that the measurement (Wj(t) - Wj(t)) yields the quantity 1]G(t, 0). [sent-148, score-0.277]
</p><p>80 Such are the cases for Adaline learning and linear regression [9], where the cavity activation can be written as  h'"(t) = x'"(t) +  J  dsG(t - s, 0) OE(X'"(S), y'"). [sent-152, score-0.697]
</p><p>81 ox,"(s)  (16)  This allows us to estimate the generalization error Eg(t) via Eg(t) 2:. [sent-153, score-0.166]
</p><p>82 ," E(h'"(t), y'")/p, whose minimum in time determines the early stopping point. [sent-154, score-0.181]
</p><p>83 To verify the proposed method in linear regression, we randomly generate examples from a noisy teacher with y'" = iJ . [sent-155, score-0.272]
</p><p>84 f'" + (Jzw Here iJ is the teacher vector with B2 = 1. [sent-156, score-0.213]
</p><p>85 Learning is done by the gradient descent of the energy function E(t) = 2:. [sent-158, score-0.099]
</p><p>86 The generalization error Eg(t) is the error av-  e;  eraged over the distribution of input [ and their corresponding output y, i. [sent-161, score-0.179]
</p><p>87 As far as the teacher is concerned, Eg(t) can be computed as Eg(t) = (1 - 2R(t) + Q(t) + (J2)/2. [sent-165, score-0.213]
</p><p>88 Three results are compared: the teacher's estimate, the cavity method and cross-validation. [sent-169, score-0.658]
</p><p>89 Again, we see that the cavity method yields estimates of the early stopping time with comparable precision as cross-validation. [sent-171, score-0.914]
</p><p>90 The ratio of the CPU time between the cavity method and fivefold cross-validation is 1 : 1. [sent-172, score-0.735]
</p><p>91 For nonlinear regression and multilayer networks, the Green 's functions are not time-translational invariant. [sent-174, score-0.099]
</p><p>92 Preliminary results for the determination of the early stopping point are satisfactory and final results will be presented elsewhere. [sent-176, score-0.181]
</p><p>93 7  0  2 time t  0  2 time t  0  2  4  time t  Figure 2: (a-f) The evolution of the generalization error of linear regression for N = 500, p = 600 and (J = 1. [sent-191, score-0.228]
</p><p>94 The solid symbols locate the early stopping points estimated by the teacher (circle), the cavity method (square) and fivefold crossvalidation (diamond). [sent-192, score-1.18]
</p><p>95 4  Conclusion  We have proposed a method for the fast estimation of hyperparameters in large networks, based on the linear response relation in the cavity method, combined with an empirical method of measuring the Green's function. [sent-193, score-0.934]
</p><p>96 It does not require the validation process or the inversion of matrices of macroscopic size, and hence its speed compares favorably with cross-validation and other perturbative approaches such as extended LULOO. [sent-195, score-0.252]
</p><p>97 For multilayer networks, we will explore further speedup of the Green 's function measurement by multiplexing the stimuli to the different hidden units into a single network, to be compared with a reference network. [sent-196, score-0.23]
</p><p>98 Our initial success indicates that it is possible to generalize the method to more complicated systems in the future. [sent-198, score-0.059]
</p><p>99 The concept of Green's functions is very general, and its measurement by comparing the states of a stimulated system with a reference one can be adopted to general cases with suitable adaptation. [sent-199, score-0.118]
</p><p>100 It would be interesting to consider how the proposed method can contribute to these cases. [sent-201, score-0.059]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cavity', 0.599), ('green', 0.263), ('teacher', 0.213), ('wj', 0.213), ('luloo', 0.204), ('hyperparameter', 0.164), ('jl', 0.144), ('stopping', 0.124), ('measurement', 0.118), ('decay', 0.116), ('nh', 0.111), ('yjl', 0.102), ('hyperparameters', 0.101), ('steady', 0.093), ('oe', 0.089), ('eg', 0.086), ('generalization', 0.085), ('activations', 0.077), ('fivefold', 0.077), ('golden', 0.077), ('perturbative', 0.077), ('susceptibility', 0.077), ('macroscopic', 0.075), ('weight', 0.075), ('stimulus', 0.071), ('node', 0.068), ('gradient', 0.066), ('multilayer', 0.065), ('activation', 0.064), ('evolution', 0.062), ('ae', 0.061), ('hong', 0.061), ('method', 0.059), ('early', 0.057), ('xa', 0.057), ('decays', 0.053), ('adaline', 0.051), ('diagrammatic', 0.051), ('dsg', 0.051), ('dsgjk', 0.051), ('jhj', 0.051), ('jzw', 0.051), ('locate', 0.051), ('rab', 0.051), ('sevenfold', 0.051), ('tab', 0.051), ('tenfold', 0.051), ('xian', 0.051), ('cpu', 0.051), ('inputs', 0.048), ('hidden', 0.047), ('error', 0.047), ('perceptrons', 0.046), ('ij', 0.046), ('estimation', 0.046), ('networks', 0.045), ('dynamics', 0.045), ('fg', 0.045), ('overtraining', 0.044), ('gjk', 0.044), ('dwj', 0.044), ('winther', 0.044), ('lab', 0.042), ('reached', 0.042), ('simulation', 0.041), ('muller', 0.041), ('committee', 0.04), ('wb', 0.04), ('diamond', 0.04), ('kong', 0.04), ('network', 0.04), ('normalized', 0.04), ('ofthe', 0.039), ('yields', 0.039), ('feeding', 0.038), ('whitening', 0.038), ('wong', 0.038), ('response', 0.037), ('precision', 0.036), ('convenient', 0.036), ('il', 0.036), ('optimal', 0.036), ('mention', 0.036), ('quantum', 0.036), ('xb', 0.036), ('inversion', 0.036), ('search', 0.035), ('dynamical', 0.035), ('processes', 0.035), ('estimate', 0.034), ('dt', 0.034), ('oo', 0.034), ('regression', 0.034), ('descent', 0.033), ('relation', 0.033), ('validation', 0.032), ('quantity', 0.032), ('ba', 0.032), ('extended', 0.032), ('physics', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="76-tfidf-1" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>Author: K. Wong, F. Li</p><p>Abstract: We propose a method for the fast estimation of hyperparameters in large networks, based on the linear response relation in the cavity method, and an empirical measurement of the Green's function. Simulation results show that it is efficient and precise, when compared with cross-validation and other techniques which require matrix inversion. 1</p><p>2 0.13527831 <a title="76-tfidf-2" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>Author: Lehel Csató, Manfred Opper, Ole Winther</p><p>Abstract: The adaptive TAP Gibbs free energy for a general densely connected probabilistic model with quadratic interactions and arbritary single site constraints is derived. We show how a speciﬁc sequential minimization of the free energy leads to a generalization of Minka’s expectation propagation. Lastly, we derive a sparse representation version of the sequential algorithm. The usefulness of the approach is demonstrated on classiﬁcation and density estimation with Gaussian processes and on an independent component analysis problem.</p><p>3 0.13368493 <a title="76-tfidf-3" href="./nips-2001-Gaussian_Process_Regression_with_Mismatched_Models.html">79 nips-2001-Gaussian Process Regression with Mismatched Models</a></p>
<p>Author: Peter Sollich</p><p>Abstract: Learning curves for Gaussian process regression are well understood when the 'student' model happens to match the 'teacher' (true data generation process). I derive approximations to the learning curves for the more generic case of mismatched models, and find very rich behaviour: For large input space dimensionality, where the results become exact, there are universal (student-independent) plateaux in the learning curve, with transitions in between that can exhibit arbitrarily many over-fitting maxima; over-fitting can occur even if the student estimates the teacher noise level correctly. In lower dimensions, plateaux also appear, and the learning curve remains dependent on the mismatch between student and teacher even in the asymptotic limit of a large number of training examples. Learning with excessively strong smoothness assumptions can be particularly dangerous: For example, a student with a standard radial basis function covariance function will learn a rougher teacher function only logarithmically slowly. All predictions are confirmed by simulations. 1</p><p>4 0.092786171 <a title="76-tfidf-4" href="./nips-2001-Analysis_of_Sparse_Bayesian_Learning.html">35 nips-2001-Analysis of Sparse Bayesian Learning</a></p>
<p>Author: Anita C. Faul, Michael E. Tipping</p><p>Abstract: The recent introduction of the 'relevance vector machine' has effectively demonstrated how sparsity may be obtained in generalised linear models within a Bayesian framework. Using a particular form of Gaussian parameter prior, 'learning' is the maximisation, with respect to hyperparameters, of the marginal likelihood of the data. This paper studies the properties of that objective function, and demonstrates that conditioned on an individual hyperparameter, the marginal likelihood has a unique maximum which is computable in closed form. It is further shown that if a derived 'sparsity criterion' is satisfied, this maximum is exactly equivalent to 'pruning' the corresponding parameter from the model. 1</p><p>5 0.086750858 <a title="76-tfidf-5" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>Author: Manfred Opper, Robert Urbanczik</p><p>Abstract: Using methods of Statistical Physics, we investigate the rOle of model complexity in learning with support vector machines (SVMs). We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error. Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel. 1</p><p>6 0.07285022 <a title="76-tfidf-6" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>7 0.071912259 <a title="76-tfidf-7" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>8 0.064044029 <a title="76-tfidf-8" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>9 0.061775718 <a title="76-tfidf-9" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>10 0.058271121 <a title="76-tfidf-10" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>11 0.056834683 <a title="76-tfidf-11" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>12 0.055724356 <a title="76-tfidf-12" href="./nips-2001-Geometrical_Singularities_in_the_Neuromanifold_of_Multilayer_Perceptrons.html">83 nips-2001-Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons</a></p>
<p>13 0.055721056 <a title="76-tfidf-13" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>14 0.05290411 <a title="76-tfidf-14" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>15 0.05120676 <a title="76-tfidf-15" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>16 0.049910054 <a title="76-tfidf-16" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>17 0.047411412 <a title="76-tfidf-17" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>18 0.047143459 <a title="76-tfidf-18" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>19 0.044582736 <a title="76-tfidf-19" href="./nips-2001-Kernel_Logistic_Regression_and_the_Import_Vector_Machine.html">104 nips-2001-Kernel Logistic Regression and the Import Vector Machine</a></p>
<p>20 0.04410759 <a title="76-tfidf-20" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.171), (1, -0.039), (2, 0.002), (3, -0.016), (4, -0.012), (5, -0.046), (6, 0.076), (7, -0.024), (8, -0.001), (9, -0.01), (10, -0.026), (11, 0.072), (12, 0.01), (13, -0.094), (14, 0.04), (15, 0.092), (16, 0.017), (17, 0.02), (18, 0.103), (19, -0.014), (20, -0.01), (21, -0.117), (22, 0.051), (23, 0.119), (24, 0.13), (25, 0.092), (26, -0.002), (27, -0.051), (28, -0.018), (29, 0.215), (30, 0.066), (31, 0.044), (32, -0.028), (33, 0.038), (34, 0.024), (35, 0.079), (36, 0.03), (37, 0.036), (38, -0.015), (39, -0.136), (40, -0.072), (41, 0.194), (42, 0.027), (43, 0.133), (44, -0.008), (45, -0.014), (46, 0.059), (47, 0.061), (48, -0.028), (49, -0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92921072 <a title="76-lsi-1" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>Author: K. Wong, F. Li</p><p>Abstract: We propose a method for the fast estimation of hyperparameters in large networks, based on the linear response relation in the cavity method, and an empirical measurement of the Green's function. Simulation results show that it is efficient and precise, when compared with cross-validation and other techniques which require matrix inversion. 1</p><p>2 0.77066338 <a title="76-lsi-2" href="./nips-2001-Gaussian_Process_Regression_with_Mismatched_Models.html">79 nips-2001-Gaussian Process Regression with Mismatched Models</a></p>
<p>Author: Peter Sollich</p><p>Abstract: Learning curves for Gaussian process regression are well understood when the 'student' model happens to match the 'teacher' (true data generation process). I derive approximations to the learning curves for the more generic case of mismatched models, and find very rich behaviour: For large input space dimensionality, where the results become exact, there are universal (student-independent) plateaux in the learning curve, with transitions in between that can exhibit arbitrarily many over-fitting maxima; over-fitting can occur even if the student estimates the teacher noise level correctly. In lower dimensions, plateaux also appear, and the learning curve remains dependent on the mismatch between student and teacher even in the asymptotic limit of a large number of training examples. Learning with excessively strong smoothness assumptions can be particularly dangerous: For example, a student with a standard radial basis function covariance function will learn a rougher teacher function only logarithmically slowly. All predictions are confirmed by simulations. 1</p><p>3 0.55481654 <a title="76-lsi-3" href="./nips-2001-Geometrical_Singularities_in_the_Neuromanifold_of_Multilayer_Perceptrons.html">83 nips-2001-Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons</a></p>
<p>Author: Shun-ichi Amari, Hyeyoung Park, Tomoko Ozeki</p><p>Abstract: Singularities are ubiquitous in the parameter space of hierarchical models such as multilayer perceptrons. At singularities, the Fisher information matrix degenerates, and the Cramer-Rao paradigm does no more hold, implying that the classical model selection theory such as AIC and MDL cannot be applied. It is important to study the relation between the generalization error and the training error at singularities. The present paper demonstrates a method of analyzing these errors both for the maximum likelihood estimator and the Bayesian predictive distribution in terms of Gaussian random fields, by using simple models. 1</p><p>4 0.55399495 <a title="76-lsi-4" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We combine the replica approach from statistical physics with a variational approach to analyze learning curves analytically. We apply the method to Gaussian process regression. As a main result we derive approximative relations between empirical error measures, the generalization error and the posterior variance.</p><p>5 0.49549779 <a title="76-lsi-5" href="./nips-2001-Analysis_of_Sparse_Bayesian_Learning.html">35 nips-2001-Analysis of Sparse Bayesian Learning</a></p>
<p>Author: Anita C. Faul, Michael E. Tipping</p><p>Abstract: The recent introduction of the 'relevance vector machine' has effectively demonstrated how sparsity may be obtained in generalised linear models within a Bayesian framework. Using a particular form of Gaussian parameter prior, 'learning' is the maximisation, with respect to hyperparameters, of the marginal likelihood of the data. This paper studies the properties of that objective function, and demonstrates that conditioned on an individual hyperparameter, the marginal likelihood has a unique maximum which is computable in closed form. It is further shown that if a derived 'sparsity criterion' is satisfied, this maximum is exactly equivalent to 'pruning' the corresponding parameter from the model. 1</p><p>6 0.49423435 <a title="76-lsi-6" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>7 0.43942434 <a title="76-lsi-7" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>8 0.41960043 <a title="76-lsi-8" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<p>9 0.40488854 <a title="76-lsi-9" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>10 0.37352234 <a title="76-lsi-10" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>11 0.36534935 <a title="76-lsi-11" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>12 0.35528231 <a title="76-lsi-12" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>13 0.35500351 <a title="76-lsi-13" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>14 0.34626213 <a title="76-lsi-14" href="./nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes.html">133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</a></p>
<p>15 0.34501106 <a title="76-lsi-15" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>16 0.33774662 <a title="76-lsi-16" href="./nips-2001-Scaling_Laws_and_Local_Minima_in_Hebbian_ICA.html">165 nips-2001-Scaling Laws and Local Minima in Hebbian ICA</a></p>
<p>17 0.32937461 <a title="76-lsi-17" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>18 0.31694838 <a title="76-lsi-18" href="./nips-2001-A_Rational_Analysis_of_Cognitive_Control_in_a_Speeded_Discrimination_Task.html">18 nips-2001-A Rational Analysis of Cognitive Control in a Speeded Discrimination Task</a></p>
<p>19 0.31388313 <a title="76-lsi-19" href="./nips-2001-EM-DD%3A_An_Improved_Multiple-Instance_Learning_Technique.html">64 nips-2001-EM-DD: An Improved Multiple-Instance Learning Technique</a></p>
<p>20 0.31305698 <a title="76-lsi-20" href="./nips-2001-Distribution_of_Mutual_Information.html">61 nips-2001-Distribution of Mutual Information</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.051), (17, 0.04), (19, 0.04), (27, 0.089), (30, 0.086), (36, 0.342), (38, 0.011), (59, 0.032), (72, 0.075), (79, 0.051), (83, 0.02), (91, 0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79955846 <a title="76-lda-1" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>Author: K. Wong, F. Li</p><p>Abstract: We propose a method for the fast estimation of hyperparameters in large networks, based on the linear response relation in the cavity method, and an empirical measurement of the Green's function. Simulation results show that it is efficient and precise, when compared with cross-validation and other techniques which require matrix inversion. 1</p><p>2 0.73498636 <a title="76-lda-2" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>Author: T. Zhang</p><p>Abstract: We investigate the generalization performance of some learning problems in Hilbert functional Spaces. We introduce a notion of convergence of the estimated functional predictor to the best underlying predictor, and obtain an estimate on the rate of the convergence. This estimate allows us to derive generalization bounds on some learning formulations.</p><p>3 0.7004348 <a title="76-lda-3" href="./nips-2001-Unsupervised_Learning_of_Human_Motion_Models.html">193 nips-2001-Unsupervised Learning of Human Motion Models</a></p>
<p>Author: Yang Song, Luis Goncalves, Pietro Perona</p><p>Abstract: This paper presents an unsupervised learning algorithm that can derive the probabilistic dependence structure of parts of an object (a moving human body in our examples) automatically from unlabeled data. The distinguished part of this work is that it is based on unlabeled data, i.e., the training features include both useful foreground parts and background clutter and the correspondence between the parts and detected features are unknown. We use decomposable triangulated graphs to depict the probabilistic independence of parts, but the unsupervised technique is not limited to this type of graph. In the new approach, labeling of the data (part assignments) is taken as hidden variables and the EM algorithm is applied. A greedy algorithm is developed to select parts and to search for the optimal structure based on the differential entropy of these variables. The success of our algorithm is demonstrated by applying it to generate models of human motion automatically from unlabeled real image sequences.</p><p>4 0.56566972 <a title="76-lda-4" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>5 0.49652904 <a title="76-lda-5" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>6 0.49525324 <a title="76-lda-6" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>7 0.48428828 <a title="76-lda-7" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>8 0.48001739 <a title="76-lda-8" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>9 0.47169837 <a title="76-lda-9" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>10 0.47064775 <a title="76-lda-10" href="./nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">102 nips-2001-KLD-Sampling: Adaptive Particle Filters</a></p>
<p>11 0.46957952 <a title="76-lda-11" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>12 0.46909815 <a title="76-lda-12" href="./nips-2001-Learning_Hierarchical_Structures_with_Linear_Relational_Embedding.html">110 nips-2001-Learning Hierarchical Structures with Linear Relational Embedding</a></p>
<p>13 0.4652186 <a title="76-lda-13" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>14 0.4635545 <a title="76-lda-14" href="./nips-2001-Using_Vocabulary_Knowledge_in_Bayesian_Multinomial_Estimation.html">194 nips-2001-Using Vocabulary Knowledge in Bayesian Multinomial Estimation</a></p>
<p>15 0.46126035 <a title="76-lda-15" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>16 0.45979887 <a title="76-lda-16" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>17 0.45976263 <a title="76-lda-17" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>18 0.4595921 <a title="76-lda-18" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>19 0.4587034 <a title="76-lda-19" href="./nips-2001-Duality%2C_Geometry%2C_and_Support_Vector_Regression.html">62 nips-2001-Duality, Geometry, and Support Vector Regression</a></p>
<p>20 0.45866966 <a title="76-lda-20" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
