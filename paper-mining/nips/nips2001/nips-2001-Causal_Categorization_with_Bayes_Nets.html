<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>47 nips-2001-Causal Categorization with Bayes Nets</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-47" href="#">nips2001-47</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>47 nips-2001-Causal Categorization with Bayes Nets</h1>
<br/><p>Source: <a title="nips-2001-47-pdf" href="http://papers.nips.cc/paper/1993-causal-categorization-with-bayes-nets.pdf">pdf</a></p><p>Author: Bob Rehder</p><p>Abstract: A theory of categorization is presented in which knowledge of causal relationships between category features is represented as a Bayesian network. Referred to as causal-model theory, this theory predicts that objects are classified as category members to the extent they are likely to have been produced by a categorys causal model. On this view, people have models of the world that lead them to expect a certain distribution of features in category members (e.g., correlations between feature pairs that are directly connected by causal relationships), and consider exemplars good category members when they manifest those expectations. These expectations include sensitivity to higher-order feature interactions that emerge from the asymmetries inherent in causal relationships. Research on the topic of categorization has traditionally focused on the problem of learning new categories given observations of category members. In contrast, the theory-based view of categories emphasizes the influence of the prior theoretical knowledge that learners often contribute to their representations of categories [1]. However, in contrast to models accounting for the effects of empirical observations, there have been few models developed to account for the effects of prior knowledge. The purpose of this article is to present a model of categorization referred to as causal-model theory or CMT [2, 3]. According to CMT, people 's know ledge of many categories includes not only features, but also an explicit representation of the causal mechanisms that people believe link the features of many categories. In this article I apply CMT to the problem of establishing objects category membership. In the psychological literature one standard view of categorization is that objects are placed in a category to the extent they have features that have often been observed in members of that category. For example, an object that has most of the features of birds (e.g., wings, fly, build nests in trees, etc.) and few features of other categories is thought to be a bird. This view of categorization is formalized by prototype models in which classification is a function of the similarity (i.e. , number of shared features) between a mental representation of a category prototype and a to-be-classified object. However , a well-known difficulty with prototype models is that a features contribution to category membership is independent of the presence or absence of other features. In contrast , consideration of a categorys theoretical knowledge is likely to influence which combinations of features make for acceptable category members. For example , people believe that birds have nests in trees because they can fly , and in light of this knowledge an animal that doesnt fly and yet still builds nests in trees might be considered a less plausible bird than an animal that builds nests on the ground and doesnt fly (e.g., an ostrich) even though the latter animal has fewer features typical of birds. To assess whether knowledge in fact influences which feature combinations make for good category members , in the following experiment undergraduates were taught novel categories whose four binary features exhibited either a common-cause or a common-effect schema (Figure 1). In the common-cause schema, one category feature (PI) is described as causing the three other features (F 2, F 3, and F4). In the common-effect schema one feature (F4) is described as being caused by the three others (F I, F 2, and F3). CMT assumes that people represent causal knowledge such as that in Figure 1 as a kind of Bayesian network [4] in which nodes are variables representing binary category features and directed edges are causal relationships representing the presence of probabilistic causal mechanisms between features. Specifically , CMT assumes that when a cause feature is present it enables the operation of a causal mechanism that will, with some probability m , bring about the presence of the effect feature. CMT also allow for the possibility that effect features have potential background causes that are not explicitly represented in the network, as represented by parameter b which is the probability that an effect will be present even when its network causes are absent. Finally, each cause node has a parameter c that represents the probability that a cause feature will be present. ~ Common-Cause Schema ~ ® Common-Effect Schema Figure 1. ...(~~) @ ..... : ~~:f·</p><p>Reference: <a title="nips-2001-47-reference" href="../nips2001_reference/nips-2001-Causal_Categorization_with_Bayes_Nets_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract A theory of categorization is presented in which knowledge of causal relationships between category features is represented as a Bayesian network. [sent-3, score-1.27]
</p><p>2 Referred to as causal-model theory, this theory predicts that objects are classified as category members to the extent they are likely to have been produced by a categorys causal model. [sent-4, score-1.028]
</p><p>3 On this view, people have models of the world that lead them to expect a certain distribution of features in category members (e. [sent-5, score-0.612]
</p><p>4 , correlations between feature pairs that are directly connected by causal relationships), and consider exemplars good category members when they manifest those expectations. [sent-7, score-1.203]
</p><p>5 These expectations include sensitivity to higher-order feature interactions that emerge from the asymmetries inherent in causal relationships. [sent-8, score-0.862]
</p><p>6 Research on the topic of categorization has traditionally focused on the problem of learning new categories given observations of category members. [sent-9, score-0.579]
</p><p>7 In contrast, the theory-based view of categories emphasizes the influence of the prior theoretical knowledge that learners often contribute to their representations of categories [1]. [sent-10, score-0.22]
</p><p>8 However, in contrast to models accounting for the effects of empirical observations, there have been few models developed to account for the effects of prior knowledge. [sent-11, score-0.104]
</p><p>9 The purpose of this article is to present a model of categorization referred to as causal-model theory or CMT [2, 3]. [sent-12, score-0.215]
</p><p>10 According to CMT, people 's know ledge of many categories includes not only features, but also an explicit representation of the causal mechanisms that people believe link the features of many categories. [sent-13, score-0.949]
</p><p>11 In this article I apply CMT to the problem of establishing objects category membership. [sent-14, score-0.308]
</p><p>12 In the psychological literature one standard view of categorization is that objects are placed in a category to the extent they have features that have often been observed in members of that category. [sent-15, score-0.838]
</p><p>13 For example, an object that has most of the features of birds (e. [sent-16, score-0.188]
</p><p>14 ) and few features of other categories is thought to be a bird. [sent-19, score-0.249]
</p><p>15 This view of categorization is formalized by prototype models in which classification is a function of the similarity (i. [sent-20, score-0.24]
</p><p>16 , number of shared features) between a mental representation of a category prototype and a to-be-classified object. [sent-22, score-0.331]
</p><p>17 However , a well-known difficulty with prototype models is that a features contribution to category membership is independent of the presence or absence of other features. [sent-23, score-0.594]
</p><p>18 In contrast , consideration of a categorys theoretical knowledge is likely to influence which combinations of features make for acceptable category members. [sent-24, score-0.586]
</p><p>19 For example , people believe that birds have nests in trees because they can fly , and in light of this knowledge an animal that doesnt fly  and yet still builds nests in trees might be considered a less plausible bird than an animal that builds nests on the ground and doesnt fly (e. [sent-25, score-0.696]
</p><p>20 , an ostrich) even though the latter animal has fewer features typical of birds. [sent-27, score-0.186]
</p><p>21 To assess whether knowledge in fact influences which feature combinations make for good category members , in the following experiment undergraduates were taught novel categories whose four binary features exhibited either a common-cause or a common-effect schema (Figure 1). [sent-28, score-0.992]
</p><p>22 In the common-cause schema, one category feature (PI) is described as causing the three other features (F 2, F 3, and F4). [sent-29, score-0.488]
</p><p>23 In the common-effect schema one feature (F4) is described as being caused by the three others (F I, F 2, and F3). [sent-30, score-0.257]
</p><p>24 Specifically , CMT assumes that when a cause feature is present it enables the operation of a causal mechanism that will, with some probability m , bring about the presence of the effect feature. [sent-32, score-0.924]
</p><p>25 CMT also allow for the possibility that effect features have potential background causes that are not explicitly represented in the network, as represented by parameter b which is the probability that an effect will be present even when its network causes are absent. [sent-33, score-0.452]
</p><p>26 Finally, each cause node has a parameter c that represents the probability that a cause feature will be present. [sent-34, score-0.441]
</p><p>27 The central prediction of CMT is that an object is considered to be a category member to the extent that its features were likely to have been generated by a category's causal mechanisms. [sent-51, score-1.016]
</p><p>28 For example, Table 1 presents the likelihoods that the causal models of Figure 1 will generate the sixteen possible combinations of F I, F 2, F 3, and F 4. [sent-52, score-0.586]
</p><p>29 Note that these likelihoods assume that the causal mechanisms in each model operate independently and with the same probability m, restrictions that can be relaxed in other applications. [sent-56, score-0.576]
</p><p>30 This formalization of categorization offered by CMT implies that peoples theoretical knowledge leads them to expect a certain distribution of features in category members , and that they use this information when assigning category membership. [sent-57, score-1.034]
</p><p>31 Thus , to gain insight into the categorization performance predicted by CMT , we can examine the statistical properties of category features that one can  expect to be generated by a causal model. [sent-58, score-1.233]
</p><p>32 For example , dotted lines in Figure 2 represent the features correlations that are generated from the causal schemas of Figure 1. [sent-59, score-0.828]
</p><p>33 As one would expect, pairs of features directly linked by causal relationships are correlated in the common-cause schema F I is correlated with its effects and in the common-effect schema F4 is correlated with its causes. [sent-60, score-1.377]
</p><p>34 Thus, CMT predicts that combinations of features serve as evidence for category membership to the extent that they preserve these expected correlations (i. [sent-61, score-0.693]
</p><p>35 , both cause and effect present or both absent) , and against category membership to the extent that they break those correlations (one present and the other absent). [sent-63, score-0.786]
</p><p>36 6  Causal networks not only predict pairwise correlations between directly connected features. [sent-148, score-0.108]
</p><p>37 Use of these schemas in the following experiment enables a test of whether categorizers are sensitive the pattern of correlations between features directly-connected by causal laws, and also those that arise due to the asymmetries inherent in causal relationships shown in Figure 2. [sent-151, score-1.597]
</p><p>38 Moreover , I will show that CMT predicts, and humans exhibit, sensitivity to interactions among features of a higher-order than the pairwise interactions shown in Figure 2. [sent-152, score-0.398]
</p><p>39 Method Six novel categories were used in which the description of causal relationships between features consisted of one sentence indicating the cause and effect feature , and then one or two sentences describing the mechanism responsible for the causal relationship. [sent-153, score-1.699]
</p><p>40 For example , one of the novel categories , Lake Victoria Shrimp , was described as having four binary features (e. [sent-154, score-0.271]
</p><p>41 , "A high quantity of ACh neurotransmitter causes a long-lasting flight response. [sent-162, score-0.127]
</p><p>42 Participants first studied several computer screens of information about their assigned category at their own pace. [sent-165, score-0.299]
</p><p>43 All participants were first presented with the four features. [sent-166, score-0.247]
</p><p>44 Participants in the common-cause condition were categorys additionally instructed on the common-cause causal relationships (F 1-;' F 2 , F 1-;' F 3 , F 1-;' F 4) , and participants in the common-effect condition were instructed on the common-effect relationships (F 1-;. [sent-167, score-1.19]
</p><p>45 When ready , participants took a multiple-choice test that tested them on the knowledge they had just studied. [sent-171, score-0.263]
</p><p>46 Participants then performed a classification task in which they rated on a 0-100 scale the category membership of 16 exemplars , consisting of all possible objects that can be formed from four binary features. [sent-173, score-0.563]
</p><p>47 For example , those participants assigned to learn the Lake Victoria Shrimp category were asked to classify a shrimp that possessed "High amounts of the ACh neurotransmitter ," "A normal flight response ," "Accelerated sleep cycle ," and "Normal body weight. [sent-174, score-0.686]
</p><p>48 " The order of the test exemplars was randomized for each participant. [sent-175, score-0.113]
</p><p>49 Results Categorization ratings for the 16 test exemplars averaged over partIclpants in the common-cause , common-effect, and control conditions are presented in Table 1. [sent-178, score-0.381]
</p><p>50 The presence of causal knowledge had a large effect on the ratings. [sent-179, score-0.685]
</p><p>51 For instance, exemplars 0111 and 0001 were given lower ratings in the common-cause and common-effect conditions , respectively (39. [sent-180, score-0.318]
</p><p>52 0) presumably because in these exemplars correlations are broken (effect features are present even though their causes are absent). [sent-184, score-0.424]
</p><p>53 In contrast, exemplar 1111 received a significantly higher rating in the common-cause and common-effect conditions than in the control condition (90. [sent-185, score-0.322]
</p><p>54 To confirm that causal schemas induced a sensitivity to interactions between features, categorization ratings were analyzed by performing a multiple regression for each participant. [sent-190, score-1.135]
</p><p>55 Four predictor variables (f1 , f2, f3 , f4) were coded as -1 if the feature was absent , and + 1 if it was present. [sent-191, score-0.173]
</p><p>56 For those feature pairs connected by a causal relationship the two-way interaction terms represent whether the causal relationship is confirmed (+ 1, cause and effect both present or both absent) or violated (-1 , one present and one absent). [sent-193, score-1.509]
</p><p>57 Finally , the four three-way interactions (f123 , f124 , f134, and f234) , and the single four-way interaction (f1234) were also included as predictors. [sent-194, score-0.16]
</p><p>58 Regression weights averaged over participants are presented in Figure 3 as a function of causal schema condition. [sent-195, score-0.969]
</p><p>59 Figure 3 indicates that the interaction terms corresponding to those feature pairs assigned causal relationships had significantly positive weights in both the common-cause condition (f12 , f13 , f14) , and the common-effect condition (f14 , f24 , f34). [sent-196, score-0.955]
</p><p>60 That is , as predicted (Figure 2) an exemplar was rated a better category member when it preserved expected correlations (cause and effect feature either both present or both absent) , and a worse member when it broke those correlations (one absent and the other present). [sent-197, score-0.997]
</p><p>61 Consistent with this prediction, in this condition the three two-way interaction terms between the effect features (f24, f34, f23) are greater than those interactions in the control condition. [sent-203, score-0.482]
</p><p>62 In contrast, the common-effect schema does not imply that the three cause features will be correlated, and in fact in that condition the interactions between the cause attributes (f12, f13, f23) did not differ from those in the control condition (Figure 3). [sent-204, score-1.034]
</p><p>63 Figure 3 also reveals higher-order interactions among features in the common-effect condition: Weights on interaction terms f124, f134, f234, and f1234 (- 1. [sent-205, score-0.296]
</p><p>64 These higher-order interactions arose because a common-effect schema requires only one cause feature to explain the presence of the common effect. [sent-210, score-0.648]
</p><p>65 Figures 7b presents the logarithm of the ratings in the common-effect condition for those test exemplars in which the common effect is present as a function of the number of cause features present. [sent-211, score-0.863]
</p><p>66 5 of the first cause as compared to subsequent bO 4. [sent-213, score-0.193]
</p><p>67 That is, participants 'ill considered the presence of ~ 3. [sent-215, score-0.283]
</p><p>68 5 at least one cause Of) 0 explaining the presence of . [sent-216, score-0.251]
</p><p>69 5 Pred icted an exemplar a relatively high category membership o 2 3 o 2 3 rating in a common-effect # of Effects # of Causes category. [sent-223, score-0.506]
</p><p>70 =  •  increase in (the logarithm of) categorization ratings for those exemplars in which the common cause is present as a function of the number of effect features. [sent-225, score-0.815]
</p><p>71 In the presence of the common cause each additional effect produced a constant increment to log categorization ratings. [sent-226, score-0.551]
</p><p>72 Finally , Figure 3 also indicates that the simple feature weights differed as a function of causal schema. [sent-227, score-0.62]
</p><p>73 In contrast, in the common-effect condition it was the common-effect (f4) that had greater weight than the three causes (f1 , f2, f3). [sent-229, score-0.122]
</p><p>74 That is , causal networks promote the importance of not only specific feature combinations , but the importance of individual features as well. [sent-230, score-0.769]
</p><p>75 Model Fitting To assess whether CMT accounts for the patterns of classification found in this experiment, the causal models of Figure 1 were fitted to the category membership ratings of each participant in the common-cause and common-effect conditions, respectively. [sent-231, score-1.135]
</p><p>76 That is , the ratings were predicted from the equation , Rating (X) = K ¥ Likelihood (X; c, m , b) where Likelihood (X; c, m , b) is the likelihood of exemplar X as a function of c, m , and b. [sent-232, score-0.398]
</p><p>77 The likelihood equations for the common-cause and common-effect models shown in Table 1 were used for common-cause and common-effect participants , respectively. [sent-233, score-0.251]
</p><p>78 For each participant, the values for parameters K , c, m, and b that minimized the squared deviation between the predicted and observed ratings was computed. [sent-235, score-0.295]
</p><p>79 The best fitting values for parameters K , c, m , and b averaged over participants were 846 , . [sent-236, score-0.249]
</p><p>80 The predicted ratings for each exemplar are presented in Table 1. [sent-243, score-0.372]
</p><p>81 The significantly positive estimate for m in both conditions indicates that participants categorization performance was consistent with them assuming the presence of a probabilistic causal mechanisms linking category features. [sent-244, score-1.36]
</p><p>82 Ratings predicted by CMT did not differ from observed ratings according to chi-square tests: )(\16)=3. [sent-245, score-0.295]
</p><p>83 To demonstrate that CMT predicts participants sensitivity to particular combinations of features when categorizing , each participants predicted ratings were subjected to the same regressions that were performed on the observed ratings. [sent-248, score-1.02]
</p><p>84 The resulting regression weights averaged over participants are presented in Figure 3 superimposed on the weights from the observed data. [sent-249, score-0.316]
</p><p>85 First, Figure 3 indicates that CMT reproduces participants sensitivity to agreement between pairs of features directly connected by causal relationships (f12 , f13 , f14 in the common-cause condition , and f14 , f24 , f34 in the common-effect condition). [sent-250, score-1.195]
</p><p>86 That is , according to both CMT and human participants , category membership ratings increase when pairs of features confirm causal laws , and decrease when they violate those laws. [sent-251, score-1.512]
</p><p>87 Finally , CMT also accounts for the larger weight given to the common cause and common-effect features (Figure 3). [sent-254, score-0.431]
</p><p>88 Discussion The current results support CMTs claims that people have a representation of the probabilistic causal mechanisms that link category features, and that they classify by evaluating whether an objects combination of features was likely to have been generated by those mechanisms. [sent-255, score-1.089]
</p><p>89 That is , people have models of the world that lead them to expect a certain distribution of features in category members , and consider exemplars good category members to the extent they manifest those expectations. [sent-256, score-1.148]
</p><p>90 One way this effect manifested itself is in terms of the importance of preserved correlations between features directly connected by causal relationships. [sent-257, score-0.879]
</p><p>91 An alternative model that accounts for this particular result assumes that the feature space is expanded to include configural cues encoding the confirmation or violation of each causal relationship [6]. [sent-258, score-0.61]
</p><p>92 However , such a model treats causal links as symmetric and does not consider interactions among links. [sent-259, score-0.615]
</p><p>93 As a result , it does not fit the common effect data as well as CMT (Figure 4b) , because it is unable to account for categorizers sensitivity to the higher-order feature interactions that emerge as a result of causal asymmetries in a complex network. [sent-260, score-0.973]
</p><p>94 CMT diverges from traditional models of categorization by emphasizing the knowledge people possess as opposed to the examples they observe. [sent-261, score-0.301]
</p><p>95 Indeed , the current experiment differed from many categorization studies in not providing examples of category members. [sent-262, score-0.486]
</p><p>96 As a result , CMT is applicable to the many realworld categories about which people know far more than they have observed first hand (e. [sent-263, score-0.21]
</p><p>97 Of course, for many other categories people observe category members , and the nature of the interactions between knowledge and observations is an open question of considerable interest. [sent-266, score-0.684]
</p><p>98 Using the same materials as in the current study, the effects of knowledge and observations have been orthogonally manipulated with the finding that observations had little effect on classification performance as compared to the theories [7]. [sent-267, score-0.254]
</p><p>99 Thus , theories may often dominate categorization decisions even when observations are available. [sent-268, score-0.242]
</p><p>100 Causal knowledge and categories: The effects of causal beliefs on categorization , induction, and similarity. [sent-310, score-0.781]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('causal', 0.518), ('cmt', 0.426), ('category', 0.275), ('participants', 0.225), ('schema', 0.202), ('cause', 0.193), ('categorization', 0.186), ('ratings', 0.18), ('features', 0.158), ('exemplar', 0.119), ('absent', 0.118), ('exemplars', 0.113), ('interactions', 0.097), ('relationships', 0.095), ('categories', 0.091), ('asymmetries', 0.085), ('members', 0.079), ('correlations', 0.078), ('people', 0.077), ('condition', 0.076), ('schemas', 0.074), ('predicted', 0.073), ('membership', 0.072), ('effect', 0.071), ('fly', 0.068), ('nests', 0.059), ('presence', 0.058), ('feature', 0.055), ('brought', 0.054), ('categorys', 0.051), ('flight', 0.051), ('lmj', 0.051), ('rehder', 0.051), ('shrimp', 0.051), ('sensitivity', 0.046), ('correlated', 0.046), ('causes', 0.046), ('common', 0.043), ('observed', 0.042), ('interaction', 0.041), ('rating', 0.04), ('effects', 0.039), ('extent', 0.039), ('control', 0.039), ('combinations', 0.038), ('knowledge', 0.038), ('ach', 0.038), ('accounts', 0.037), ('inherent', 0.037), ('cc', 0.036), ('accelerated', 0.034), ('bob', 0.034), ('categorizers', 0.034), ('doesnt', 0.034), ('lake', 0.034), ('undergraduates', 0.034), ('confirm', 0.034), ('predicts', 0.033), ('objects', 0.033), ('background', 0.031), ('prototype', 0.031), ('connected', 0.03), ('likelihoods', 0.03), ('neurotransmitter', 0.03), ('birds', 0.03), ('manifest', 0.03), ('participant', 0.03), ('sleep', 0.03), ('present', 0.029), ('ce', 0.029), ('theories', 0.029), ('mechanisms', 0.028), ('animal', 0.028), ('observations', 0.027), ('instructed', 0.027), ('victoria', 0.027), ('six', 0.026), ('likelihood', 0.026), ('contrast', 0.026), ('psychological', 0.026), ('member', 0.026), ('superimposed', 0.025), ('albeit', 0.025), ('differed', 0.025), ('rated', 0.025), ('laws', 0.025), ('mental', 0.025), ('pairs', 0.025), ('conditions', 0.025), ('assigned', 0.024), ('averaged', 0.024), ('preserved', 0.024), ('emerge', 0.024), ('classification', 0.023), ('expect', 0.023), ('significantly', 0.023), ('builds', 0.023), ('indicates', 0.022), ('four', 0.022), ('weakly', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="47-tfidf-1" href="./nips-2001-Causal_Categorization_with_Bayes_Nets.html">47 nips-2001-Causal Categorization with Bayes Nets</a></p>
<p>Author: Bob Rehder</p><p>Abstract: A theory of categorization is presented in which knowledge of causal relationships between category features is represented as a Bayesian network. Referred to as causal-model theory, this theory predicts that objects are classified as category members to the extent they are likely to have been produced by a categorys causal model. On this view, people have models of the world that lead them to expect a certain distribution of features in category members (e.g., correlations between feature pairs that are directly connected by causal relationships), and consider exemplars good category members when they manifest those expectations. These expectations include sensitivity to higher-order feature interactions that emerge from the asymmetries inherent in causal relationships. Research on the topic of categorization has traditionally focused on the problem of learning new categories given observations of category members. In contrast, the theory-based view of categories emphasizes the influence of the prior theoretical knowledge that learners often contribute to their representations of categories [1]. However, in contrast to models accounting for the effects of empirical observations, there have been few models developed to account for the effects of prior knowledge. The purpose of this article is to present a model of categorization referred to as causal-model theory or CMT [2, 3]. According to CMT, people 's know ledge of many categories includes not only features, but also an explicit representation of the causal mechanisms that people believe link the features of many categories. In this article I apply CMT to the problem of establishing objects category membership. In the psychological literature one standard view of categorization is that objects are placed in a category to the extent they have features that have often been observed in members of that category. For example, an object that has most of the features of birds (e.g., wings, fly, build nests in trees, etc.) and few features of other categories is thought to be a bird. This view of categorization is formalized by prototype models in which classification is a function of the similarity (i.e. , number of shared features) between a mental representation of a category prototype and a to-be-classified object. However , a well-known difficulty with prototype models is that a features contribution to category membership is independent of the presence or absence of other features. In contrast , consideration of a categorys theoretical knowledge is likely to influence which combinations of features make for acceptable category members. For example , people believe that birds have nests in trees because they can fly , and in light of this knowledge an animal that doesnt fly and yet still builds nests in trees might be considered a less plausible bird than an animal that builds nests on the ground and doesnt fly (e.g., an ostrich) even though the latter animal has fewer features typical of birds. To assess whether knowledge in fact influences which feature combinations make for good category members , in the following experiment undergraduates were taught novel categories whose four binary features exhibited either a common-cause or a common-effect schema (Figure 1). In the common-cause schema, one category feature (PI) is described as causing the three other features (F 2, F 3, and F4). In the common-effect schema one feature (F4) is described as being caused by the three others (F I, F 2, and F3). CMT assumes that people represent causal knowledge such as that in Figure 1 as a kind of Bayesian network [4] in which nodes are variables representing binary category features and directed edges are causal relationships representing the presence of probabilistic causal mechanisms between features. Specifically , CMT assumes that when a cause feature is present it enables the operation of a causal mechanism that will, with some probability m , bring about the presence of the effect feature. CMT also allow for the possibility that effect features have potential background causes that are not explicitly represented in the network, as represented by parameter b which is the probability that an effect will be present even when its network causes are absent. Finally, each cause node has a parameter c that represents the probability that a cause feature will be present. ~ Common-Cause Schema ~ ® Common-Effect Schema Figure 1. ...(~~) @ ..... : ~~:f·</p><p>2 0.21596159 <a title="47-tfidf-2" href="./nips-2001-A_Quantitative_Model_of_Counterfactual_Reasoning.html">17 nips-2001-A Quantitative Model of Counterfactual Reasoning</a></p>
<p>Author: Daniel Yarlett, Michael Ramscar</p><p>Abstract: In this paper we explore two quantitative approaches to the modelling of counterfactual reasoning – a linear and a noisy-OR model – based on information contained in conceptual dependency networks. Empirical data is acquired in a study and the ﬁt of the models compared to it. We conclude by considering the appropriateness of non-parametric approaches to counterfactual reasoning, and examining the prospects for other parametric approaches in the future.</p><p>3 0.057344757 <a title="47-tfidf-3" href="./nips-2001-Hyperbolic_Self-Organizing_Maps_for_Semantic_Navigation.html">90 nips-2001-Hyperbolic Self-Organizing Maps for Semantic Navigation</a></p>
<p>Author: Jorg Ontrup, Helge Ritter</p><p>Abstract: We introduce a new type of Self-Organizing Map (SOM) to navigate in the Semantic Space of large text collections. We propose a “hyperbolic SOM” (HSOM) based on a regular tesselation of the hyperbolic plane, which is a non-euclidean space characterized by constant negative gaussian curvature. The exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relations. We describe experiments, showing that the HSOM can successfully be applied to text categorization tasks and yields results comparable to other state-of-the-art methods.</p><p>4 0.056430839 <a title="47-tfidf-4" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>Author: Heiko Wersing</p><p>Abstract: We present a new approach to the supervised learning of lateral interactions for the competitive layer model (CLM) dynamic feature binding architecture. The method is based on consistency conditions, which were recently shown to characterize the attractor states of this linear threshold recurrent network. For a given set of training examples the learning problem is formulated as a convex quadratic optimization problem in the lateral interaction weights. An efﬁcient dimension reduction of the learning problem can be achieved by using a linear superposition of basis interactions. We show the successful application of the method to a medical image segmentation problem of ﬂuorescence microscope cell images.</p><p>5 0.052867997 <a title="47-tfidf-5" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>Author: Antonio Torralba</p><p>Abstract: The most popular algorithms for object detection require the use of exhaustive spatial and scale search procedures. In such approaches, an object is defined by means of local features. fu this paper we show that including contextual information in object detection procedures provides an efficient way of cutting down the need for exhaustive search. We present results with real images showing that the proposed scheme is able to accurately predict likely object classes, locations and sizes. 1</p><p>6 0.049552843 <a title="47-tfidf-6" href="./nips-2001-Generalizable_Relational_Binding_from_Coarse-coded_Distributed_Representations.html">80 nips-2001-Generalizable Relational Binding from Coarse-coded Distributed Representations</a></p>
<p>7 0.045748908 <a title="47-tfidf-7" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<p>8 0.044548552 <a title="47-tfidf-8" href="./nips-2001-Pranking_with_Ranking.html">147 nips-2001-Pranking with Ranking</a></p>
<p>9 0.042942144 <a title="47-tfidf-9" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>10 0.039871607 <a title="47-tfidf-10" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>11 0.037780676 <a title="47-tfidf-11" href="./nips-2001-Unsupervised_Learning_of_Human_Motion_Models.html">193 nips-2001-Unsupervised Learning of Human Motion Models</a></p>
<p>12 0.037552737 <a title="47-tfidf-12" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>13 0.037337709 <a title="47-tfidf-13" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>14 0.035910565 <a title="47-tfidf-14" href="./nips-2001-Estimating_Car_Insurance_Premia%3A_a_Case_Study_in_High-Dimensional_Data_Inference.html">70 nips-2001-Estimating Car Insurance Premia: a Case Study in High-Dimensional Data Inference</a></p>
<p>15 0.035665244 <a title="47-tfidf-15" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>16 0.033769239 <a title="47-tfidf-16" href="./nips-2001-The_g_Factor%3A_Relating_Distributions_on_Features_to_Distributions_on_Images.html">189 nips-2001-The g Factor: Relating Distributions on Features to Distributions on Images</a></p>
<p>17 0.033732109 <a title="47-tfidf-17" href="./nips-2001-A_Bayesian_Model_Predicts_Human_Parse_Preference_and_Reading_Times_in_Sentence_Processing.html">5 nips-2001-A Bayesian Model Predicts Human Parse Preference and Reading Times in Sentence Processing</a></p>
<p>18 0.033659048 <a title="47-tfidf-18" href="./nips-2001-Efficiency_versus_Convergence_of_Boolean_Kernels_for_On-Line_Learning_Algorithms.html">66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</a></p>
<p>19 0.033387985 <a title="47-tfidf-19" href="./nips-2001-Optimising_Synchronisation_Times_for_Mobile_Devices.html">140 nips-2001-Optimising Synchronisation Times for Mobile Devices</a></p>
<p>20 0.032966379 <a title="47-tfidf-20" href="./nips-2001-Multi_Dimensional_ICA_to_Separate_Correlated_Sources.html">127 nips-2001-Multi Dimensional ICA to Separate Correlated Sources</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.106), (1, -0.035), (2, -0.033), (3, 0.001), (4, -0.041), (5, -0.05), (6, -0.102), (7, -0.001), (8, -0.063), (9, -0.023), (10, -0.033), (11, -0.006), (12, -0.003), (13, -0.027), (14, 0.015), (15, 0.007), (16, 0.046), (17, 0.023), (18, -0.016), (19, 0.01), (20, 0.043), (21, 0.019), (22, -0.06), (23, 0.08), (24, -0.028), (25, 0.005), (26, 0.12), (27, 0.24), (28, 0.135), (29, 0.03), (30, -0.142), (31, 0.169), (32, 0.18), (33, -0.057), (34, 0.435), (35, -0.039), (36, -0.225), (37, 0.109), (38, 0.144), (39, -0.055), (40, -0.063), (41, 0.004), (42, 0.039), (43, -0.087), (44, -0.005), (45, 0.003), (46, 0.122), (47, 0.02), (48, -0.014), (49, 0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97997087 <a title="47-lsi-1" href="./nips-2001-Causal_Categorization_with_Bayes_Nets.html">47 nips-2001-Causal Categorization with Bayes Nets</a></p>
<p>Author: Bob Rehder</p><p>Abstract: A theory of categorization is presented in which knowledge of causal relationships between category features is represented as a Bayesian network. Referred to as causal-model theory, this theory predicts that objects are classified as category members to the extent they are likely to have been produced by a categorys causal model. On this view, people have models of the world that lead them to expect a certain distribution of features in category members (e.g., correlations between feature pairs that are directly connected by causal relationships), and consider exemplars good category members when they manifest those expectations. These expectations include sensitivity to higher-order feature interactions that emerge from the asymmetries inherent in causal relationships. Research on the topic of categorization has traditionally focused on the problem of learning new categories given observations of category members. In contrast, the theory-based view of categories emphasizes the influence of the prior theoretical knowledge that learners often contribute to their representations of categories [1]. However, in contrast to models accounting for the effects of empirical observations, there have been few models developed to account for the effects of prior knowledge. The purpose of this article is to present a model of categorization referred to as causal-model theory or CMT [2, 3]. According to CMT, people 's know ledge of many categories includes not only features, but also an explicit representation of the causal mechanisms that people believe link the features of many categories. In this article I apply CMT to the problem of establishing objects category membership. In the psychological literature one standard view of categorization is that objects are placed in a category to the extent they have features that have often been observed in members of that category. For example, an object that has most of the features of birds (e.g., wings, fly, build nests in trees, etc.) and few features of other categories is thought to be a bird. This view of categorization is formalized by prototype models in which classification is a function of the similarity (i.e. , number of shared features) between a mental representation of a category prototype and a to-be-classified object. However , a well-known difficulty with prototype models is that a features contribution to category membership is independent of the presence or absence of other features. In contrast , consideration of a categorys theoretical knowledge is likely to influence which combinations of features make for acceptable category members. For example , people believe that birds have nests in trees because they can fly , and in light of this knowledge an animal that doesnt fly and yet still builds nests in trees might be considered a less plausible bird than an animal that builds nests on the ground and doesnt fly (e.g., an ostrich) even though the latter animal has fewer features typical of birds. To assess whether knowledge in fact influences which feature combinations make for good category members , in the following experiment undergraduates were taught novel categories whose four binary features exhibited either a common-cause or a common-effect schema (Figure 1). In the common-cause schema, one category feature (PI) is described as causing the three other features (F 2, F 3, and F4). In the common-effect schema one feature (F4) is described as being caused by the three others (F I, F 2, and F3). CMT assumes that people represent causal knowledge such as that in Figure 1 as a kind of Bayesian network [4] in which nodes are variables representing binary category features and directed edges are causal relationships representing the presence of probabilistic causal mechanisms between features. Specifically , CMT assumes that when a cause feature is present it enables the operation of a causal mechanism that will, with some probability m , bring about the presence of the effect feature. CMT also allow for the possibility that effect features have potential background causes that are not explicitly represented in the network, as represented by parameter b which is the probability that an effect will be present even when its network causes are absent. Finally, each cause node has a parameter c that represents the probability that a cause feature will be present. ~ Common-Cause Schema ~ ® Common-Effect Schema Figure 1. ...(~~) @ ..... : ~~:f·</p><p>2 0.92250413 <a title="47-lsi-2" href="./nips-2001-A_Quantitative_Model_of_Counterfactual_Reasoning.html">17 nips-2001-A Quantitative Model of Counterfactual Reasoning</a></p>
<p>Author: Daniel Yarlett, Michael Ramscar</p><p>Abstract: In this paper we explore two quantitative approaches to the modelling of counterfactual reasoning – a linear and a noisy-OR model – based on information contained in conceptual dependency networks. Empirical data is acquired in a study and the ﬁt of the models compared to it. We conclude by considering the appropriateness of non-parametric approaches to counterfactual reasoning, and examining the prospects for other parametric approaches in the future.</p><p>3 0.27877006 <a title="47-lsi-3" href="./nips-2001-Estimating_Car_Insurance_Premia%3A_a_Case_Study_in_High-Dimensional_Data_Inference.html">70 nips-2001-Estimating Car Insurance Premia: a Case Study in High-Dimensional Data Inference</a></p>
<p>Author: Nicolas Chapados, Yoshua Bengio, Pascal Vincent, Joumana Ghosn, Charles Dugas, Ichiro Takeuchi, Linyan Meng</p><p>Abstract: Estimating insurance premia from data is a difficult regression problem for several reasons: the large number of variables, many of which are .discrete, and the very peculiar shape of the noise distribution, asymmetric with fat tails, with a large majority zeros and a few unreliable and very large values. We compare several machine learning methods for estimating insurance premia, and test them on a large data base of car insurance policies. We find that function approximation methods that do not optimize a squared loss, like Support Vector Machines regression, do not work well in this context. Compared methods include decision trees and generalized linear models. The best results are obtained with a mixture of experts, which better identifies the least and most risky contracts, and allows to reduce the median premium by charging more to the most risky customers. 1</p><p>4 0.25152203 <a title="47-lsi-4" href="./nips-2001-Learning_Body_Pose_via_Specialized_Maps.html">108 nips-2001-Learning Body Pose via Specialized Maps</a></p>
<p>Author: Rómer Rosales, Stan Sclaroff</p><p>Abstract: A nonlinear supervised learning model, the Specialized Mappings Architecture (SMA), is described and applied to the estimation of human body pose from monocular images. The SMA consists of several specialized forward mapping functions and an inverse mapping function. Each specialized function maps certain domains of the input space (image features) onto the output space (body pose parameters). The key algorithmic problems faced are those of learning the specialized domains and mapping functions in an optimal way, as well as performing inference given inputs and knowledge of the inverse function. Solutions to these problems employ the EM algorithm and alternating choices of conditional independence assumptions. Performance of the approach is evaluated with synthetic and real video sequences of human motion. 1</p><p>5 0.25043383 <a title="47-lsi-5" href="./nips-2001-Hyperbolic_Self-Organizing_Maps_for_Semantic_Navigation.html">90 nips-2001-Hyperbolic Self-Organizing Maps for Semantic Navigation</a></p>
<p>Author: Jorg Ontrup, Helge Ritter</p><p>Abstract: We introduce a new type of Self-Organizing Map (SOM) to navigate in the Semantic Space of large text collections. We propose a “hyperbolic SOM” (HSOM) based on a regular tesselation of the hyperbolic plane, which is a non-euclidean space characterized by constant negative gaussian curvature. The exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relations. We describe experiments, showing that the HSOM can successfully be applied to text categorization tasks and yields results comparable to other state-of-the-art methods.</p><p>6 0.25022742 <a title="47-lsi-6" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>7 0.24860232 <a title="47-lsi-7" href="./nips-2001-Constructing_Distributed_Representations_Using_Additive_Clustering.html">53 nips-2001-Constructing Distributed Representations Using Additive Clustering</a></p>
<p>8 0.23763184 <a title="47-lsi-8" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>9 0.2308943 <a title="47-lsi-9" href="./nips-2001-Unsupervised_Learning_of_Human_Motion_Models.html">193 nips-2001-Unsupervised Learning of Human Motion Models</a></p>
<p>10 0.228035 <a title="47-lsi-10" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>11 0.2227771 <a title="47-lsi-11" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<p>12 0.20045188 <a title="47-lsi-12" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>13 0.18663283 <a title="47-lsi-13" href="./nips-2001-Modeling_the_Modulatory_Effect_of_Attention_on_Human_Spatial_Vision.html">124 nips-2001-Modeling the Modulatory Effect of Attention on Human Spatial Vision</a></p>
<p>14 0.18595955 <a title="47-lsi-14" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>15 0.17959002 <a title="47-lsi-15" href="./nips-2001-Minimax_Probability_Machine.html">120 nips-2001-Minimax Probability Machine</a></p>
<p>16 0.1793116 <a title="47-lsi-16" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>17 0.17905106 <a title="47-lsi-17" href="./nips-2001-Efficiency_versus_Convergence_of_Boolean_Kernels_for_On-Line_Learning_Algorithms.html">66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</a></p>
<p>18 0.17395936 <a title="47-lsi-18" href="./nips-2001-Learning_Hierarchical_Structures_with_Linear_Relational_Embedding.html">110 nips-2001-Learning Hierarchical Structures with Linear Relational Embedding</a></p>
<p>19 0.16541862 <a title="47-lsi-19" href="./nips-2001-Generalizable_Relational_Binding_from_Coarse-coded_Distributed_Representations.html">80 nips-2001-Generalizable Relational Binding from Coarse-coded Distributed Representations</a></p>
<p>20 0.16346417 <a title="47-lsi-20" href="./nips-2001-A_Rational_Analysis_of_Cognitive_Control_in_a_Speeded_Discrimination_Task.html">18 nips-2001-A Rational Analysis of Cognitive Control in a Speeded Discrimination Task</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.012), (17, 0.012), (19, 0.01), (27, 0.613), (30, 0.058), (38, 0.025), (59, 0.017), (72, 0.04), (79, 0.016), (91, 0.093)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99528968 <a title="47-lda-1" href="./nips-2001-MIME%3A_Mutual_Information_Minimization_and_Entropy_Maximization_for_Bayesian_Belief_Propagation.html">117 nips-2001-MIME: Mutual Information Minimization and Entropy Maximization for Bayesian Belief Propagation</a></p>
<p>Author: Anand Rangarajan, Alan L. Yuille</p><p>Abstract: Bayesian belief propagation in graphical models has been recently shown to have very close ties to inference methods based in statistical physics. After Yedidia et al. demonstrated that belief propagation ﬁxed points correspond to extrema of the so-called Bethe free energy, Yuille derived a double loop algorithm that is guaranteed to converge to a local minimum of the Bethe free energy. Yuille’s algorithm is based on a certain decomposition of the Bethe free energy and he mentions that other decompositions are possible and may even be fruitful. In the present work, we begin with the Bethe free energy and show that it has a principled interpretation as pairwise mutual information minimization and marginal entropy maximization (MIME). Next, we construct a family of free energy functions from a spectrum of decompositions of the original Bethe free energy. For each free energy in this family, we develop a new algorithm that is guaranteed to converge to a local minimum. Preliminary computer simulations are in agreement with this theoretical development. 1</p><p>2 0.99193954 <a title="47-lda-2" href="./nips-2001-Scaling_Laws_and_Local_Minima_in_Hebbian_ICA.html">165 nips-2001-Scaling Laws and Local Minima in Hebbian ICA</a></p>
<p>Author: Magnus Rattray, Gleb Basalyga</p><p>Abstract: We study the dynamics of a Hebbian ICA algorithm extracting a single non-Gaussian component from a high-dimensional Gaussian background. For both on-line and batch learning we ﬁnd that a surprisingly large number of examples are required to avoid trapping in a sub-optimal state close to the initial conditions. To extract a skewed signal at least examples are required for -dimensional data and examples are required to extract a symmetrical signal with non-zero kurtosis. § ¡ ©£¢  £ §¥ ¡ ¨¦¤£¢</p><p>3 0.99162853 <a title="47-lda-3" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>Author: Lawrence K. Saul, Daniel D. Lee</p><p>Abstract: We investigate a learning algorithm for the classiﬁcation of nonnegative data by mixture models. Multiplicative update rules are derived that directly optimize the performance of these models as classiﬁers. The update rules have a simple closed form and an intuitive appeal. Our algorithm retains the main virtues of the Expectation-Maximization (EM) algorithm—its guarantee of monotonic improvement, and its absence of tuning parameters—with the added advantage of optimizing a discriminative objective function. The algorithm reduces as a special case to the method of generalized iterative scaling for log-linear models. The learning rate of the algorithm is controlled by the sparseness of the training data. We use the method of nonnegative matrix factorization (NMF) to discover sparse distributed representations of the data. This form of feature selection greatly accelerates learning and makes the algorithm practical on large problems. Experiments show that discriminatively trained mixture models lead to much better classiﬁcation than comparably sized models trained by EM. 1</p><p>4 0.99004114 <a title="47-lda-4" href="./nips-2001-Laplacian_Eigenmaps_and_Spectral_Techniques_for_Embedding_and_Clustering.html">106 nips-2001-Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</a></p>
<p>Author: Mikhail Belkin, Partha Niyogi</p><p>Abstract: Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami op erator on a manifold , and the connections to the heat equation , we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering. Several applications are considered. In many areas of artificial intelligence, information retrieval and data mining, one is often confronted with intrinsically low dimensional data lying in a very high dimensional space. For example, gray scale n x n images of a fixed object taken with a moving camera yield data points in rn: n2 . However , the intrinsic dimensionality of the space of all images of t he same object is the number of degrees of freedom of the camera - in fact the space has the natural structure of a manifold embedded in rn: n2 . While there is a large body of work on dimensionality reduction in general, most existing approaches do not explicitly take into account the structure of the manifold on which the data may possibly reside. Recently, there has been some interest (Tenenbaum et aI, 2000 ; Roweis and Saul, 2000) in the problem of developing low dimensional representations of data in this particular context. In this paper , we present a new algorithm and an accompanying framework of analysis for geometrically motivated dimensionality reduction. The core algorithm is very simple, has a few local computations and one sparse eigenvalu e problem. The solution reflects th e intrinsic geom etric structure of the manifold. Th e justification comes from the role of the Laplacian op erator in providing an optimal emb edding. Th e Laplacian of the graph obtained from the data points may be viewed as an approximation to the Laplace-Beltrami operator defined on the manifold. The emb edding maps for the data come from approximations to a natural map that is defined on the entire manifold. The framework of analysis presented here makes this connection explicit. While this connection is known to geometers and specialists in sp ectral graph theory (for example , see [1, 2]) to the best of our knowledge we do not know of any application to data representation yet. The connection of the Laplacian to the heat kernel enables us to choose the weights of the graph in a principled manner. The locality preserving character of the Laplacian Eigenmap algorithm makes it relatively insensitive to outliers and noise. A byproduct of this is that the algorithm implicitly emphasizes the natural clusters in the data. Connections to spectral clustering algorithms developed in learning and computer vision (see Shi and Malik , 1997) become very clear. Following the discussion of Roweis and Saul (2000) , and Tenenbaum et al (2000), we note that the biological perceptual apparatus is confronted with high dimensional stimuli from which it must recover low dimensional structure. One might argue that if the approach to recovering such low-dimensional structure is inherently local , then a natural clustering will emerge and thus might serve as the basis for the development of categories in biological perception. 1 The Algorithm Given k points Xl , ... , Xk in ]]{ I, we construct a weighted graph with k nodes, one for each point , and the set of edges connecting neighboring points to each other. 1. Step 1. [Constru cting th e Graph] We put an edge between nodes i and j if Xi and Xj are</p><p>5 0.989694 <a title="47-lda-5" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>Author: Richard Hahnloser, Xiaohui Xie, H. S. Seung</p><p>Abstract: Integration in the head-direction system is a computation by which horizontal angular head velocity signals from the vestibular nuclei are integrated to yield a neural representation of head direction. In the thalamus, the postsubiculum and the mammillary nuclei, the head-direction representation has the form of a place code: neurons have a preferred head direction in which their ﬁring is maximal [Blair and Sharp, 1995, Blair et al., 1998, ?]. Integration is a difﬁcult computation, given that head-velocities can vary over a large range. Previous models of the head-direction system relied on the assumption that the integration is achieved in a ﬁring-rate-based attractor network with a ring structure. In order to correctly integrate head-velocity signals during high-speed head rotations, very fast synaptic dynamics had to be assumed. Here we address the question whether integration in the head-direction system is possible with slow synapses, for example excitatory NMDA and inhibitory GABA(B) type synapses. For neural networks with such slow synapses, rate-based dynamics are a good approximation of spiking neurons [Ermentrout, 1994]. We ﬁnd that correct integration during high-speed head rotations imposes strong constraints on possible network architectures.</p><p>6 0.98848832 <a title="47-lda-6" href="./nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes.html">133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</a></p>
<p>same-paper 7 0.98179018 <a title="47-lda-7" href="./nips-2001-Causal_Categorization_with_Bayes_Nets.html">47 nips-2001-Causal Categorization with Bayes Nets</a></p>
<p>8 0.90369558 <a title="47-lda-8" href="./nips-2001-Information_Geometrical_Framework_for_Analyzing_Belief_Propagation_Decoder.html">98 nips-2001-Information Geometrical Framework for Analyzing Belief Propagation Decoder</a></p>
<p>9 0.87199306 <a title="47-lda-9" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>10 0.83444762 <a title="47-lda-10" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>11 0.78827357 <a title="47-lda-11" href="./nips-2001-Kernel_Feature_Spaces_and_Nonlinear_Blind_Souce_Separation.html">103 nips-2001-Kernel Feature Spaces and Nonlinear Blind Souce Separation</a></p>
<p>12 0.78427505 <a title="47-lda-12" href="./nips-2001-Multi_Dimensional_ICA_to_Separate_Correlated_Sources.html">127 nips-2001-Multi Dimensional ICA to Separate Correlated Sources</a></p>
<p>13 0.78094679 <a title="47-lda-13" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>14 0.78047097 <a title="47-lda-14" href="./nips-2001-Information-Geometrical_Significance_of_Sparsity_in_Gallager_Codes.html">97 nips-2001-Information-Geometrical Significance of Sparsity in Gallager Codes</a></p>
<p>15 0.7745105 <a title="47-lda-15" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>16 0.77175176 <a title="47-lda-16" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>17 0.7680611 <a title="47-lda-17" href="./nips-2001-Learning_from_Infinite_Data_in_Finite_Time.html">114 nips-2001-Learning from Infinite Data in Finite Time</a></p>
<p>18 0.764404 <a title="47-lda-18" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>19 0.76296568 <a title="47-lda-19" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<p>20 0.76140922 <a title="47-lda-20" href="./nips-2001-Products_of_Gaussians.html">154 nips-2001-Products of Gaussians</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
