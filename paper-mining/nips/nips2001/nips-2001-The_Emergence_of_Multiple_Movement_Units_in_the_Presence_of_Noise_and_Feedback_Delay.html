<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-181" href="#">nips2001-181</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</h1>
<br/><p>Source: <a title="nips-2001-181-pdf" href="http://papers.nips.cc/paper/2080-the-emergence-of-multiple-movement-units-in-the-presence-of-noise-and-feedback-delay.pdf">pdf</a></p><p>Author: Michael Kositsky, Andrew G. Barto</p><p>Abstract: Tangential hand velocity proﬁles of rapid human arm movements often appear as sequences of several bell-shaped acceleration-deceleration phases called submovements or movement units. This suggests how the nervous system might efﬁciently control a motor plant in the presence of noise and feedback delay. Another critical observation is that stochasticity in a motor control problem makes the optimal control policy essentially different from the optimal control policy for the deterministic case. We use a simpliﬁed dynamic model of an arm and address rapid aimed arm movements. We use reinforcement learning as a tool to approximate the optimal policy in the presence of noise and feedback delay. Using a simpliﬁed model we show that multiple submovements emerge as an optimal policy in the presence of noise and feedback delay. The optimal policy in this situation is to drive the arm’s end point close to the target by one fast submovement and then apply a few slow submovements to accurately drive the arm’s end point into the target region. In our simulations, the controller sometimes generates corrective submovements before the initial fast submovement is completed, much like the predictive corrections observed in a number of psychophysical experiments.</p><p>Reference: <a title="nips-2001-181-reference" href="../nips2001_reference/nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu    ¡  Abstract Tangential hand velocity proﬁles of rapid human arm movements often appear as sequences of several bell-shaped acceleration-deceleration phases called submovements or movement units. [sent-4, score-1.436]
</p><p>2 This suggests how the nervous system might efﬁciently control a motor plant in the presence of noise and feedback delay. [sent-5, score-0.9]
</p><p>3 Another critical observation is that stochasticity in a motor control problem makes the optimal control policy essentially different from the optimal control policy for the deterministic case. [sent-6, score-0.887]
</p><p>4 We use a simpliﬁed dynamic model of an arm and address rapid aimed arm movements. [sent-7, score-0.468]
</p><p>5 We use reinforcement learning as a tool to approximate the optimal policy in the presence of noise and feedback delay. [sent-8, score-0.421]
</p><p>6 Using a simpliﬁed model we show that multiple submovements emerge as an optimal policy in the presence of noise and feedback delay. [sent-9, score-0.729]
</p><p>7 The optimal policy in this situation is to drive the arm’s end point close to the target by one fast submovement and then apply a few slow submovements to accurately drive the arm’s end point into the target region. [sent-10, score-0.877]
</p><p>8 In our simulations, the controller sometimes generates corrective submovements before the initial fast submovement is completed, much like the predictive corrections observed in a number of psychophysical experiments. [sent-11, score-1.132]
</p><p>9 1 Introduction It has been consistently observed that rapid human arm movements in both infants and adults often consist of several submovements, sometimes called “movement units” [21]. [sent-12, score-0.548]
</p><p>10 The tangential hand velocity proﬁles of such movements show sequences of several bellshaped acceleration-deceleration phases, sometimes overlapping in the time domain and sometimes completely separate. [sent-13, score-0.573]
</p><p>11 Multiple movement units are observed mostly in infant reaching [5, 21] and in reaching movements by adult subjects in the face of difﬁcult timeaccuracy requirements [15]. [sent-14, score-0.861]
</p><p>12 These data provide clues about how the nervous system efﬁciently produces fast and accurate movements in the presence of noise and signiﬁcant feedback delay. [sent-15, score-0.491]
</p><p>13 Most modeling efforts concerned with movement units have addressed only the kinematic aspects of movement, e. [sent-16, score-0.477]
</p><p>14 We show that multiple movement units might emerge as the result of a control policy that is optimal in the face of uncertainty and feedback delay. [sent-19, score-0.915]
</p><p>15 We use a simpliﬁed dynamic model  of an arm and address rapid aimed arm movements. [sent-20, score-0.468]
</p><p>16 We use reinforcement learning as a tool to approximate the optimal policy in the presence of noise and feedback delay. [sent-21, score-0.421]
</p><p>17 An important motivation for this research is that stochasticity inherent in the motor control problem has a signiﬁcant inﬂuence on the optimal control policy [9]. [sent-22, score-0.627]
</p><p>18 We are following the preliminary work of Zelevinsky [23] who showed that multiple movement units emerge from the stochasticity of the environment combined with a feedback delay. [sent-23, score-0.719]
</p><p>19 Whereas he restricted attention to a ﬁnite-state system to which he applied dynamic programming, our model has a continuous state space and we use reinforcement learning in a simulated realtime learning framework. [sent-24, score-0.152]
</p><p>20 ” The controller here represents some functionality of the central nervous system dealing with the control of reaching movements. [sent-27, score-0.614]
</p><p>21 The plant represents a simpliﬁed arm together with spinal circuitry. [sent-28, score-0.511]
</p><p>22 The controller generates the control signal, , which inﬂuences how the state, , of the plant changes over time. [sent-29, score-0.802]
</p><p>23 To simulate delayed feedback the state of the plant is made available to the controller after a delay period , so at time the controller can only observe . [sent-30, score-1.219]
</p><p>24 To introduce stochasticity, we disturbed by adding noise to it, to produce a corrupted control . [sent-31, score-0.231]
</p><p>25 The controller learns to move the plant state as quickly as possible into a small region about a target state . [sent-32, score-0.94]
</p><p>26 The reward structure block in Figure 1 provides a negative unit reward when the plant’s state is out of the target area of the state space, and it provides zero reward when the plant state is within the target area. [sent-33, score-0.88]
</p><p>27 The reinforcement learning controller tries to maximize the total cumulative reward for each movement. [sent-34, score-0.487]
</p><p>28 With the above mentioned reward structure, the faster the plant is driven into the target region, the less negative reward is accumulated during the movement. [sent-35, score-0.541]
</p><p>29 © ¢  ¡     § £¤ ¨¦¥¡  £  ¢         ¡  r  reward  RL controller  s u efferent copy u  r reward target state s T structure state s delay  plant  ~ u  noise  target  Figure 1: Sketch of the model used in our simulations. [sent-37, score-1.2]
</p><p>30 1 The plant To model arm dynamics together with the spinal reﬂex mechanisms we used a fractionalpower damping dynamic model [22]. [sent-40, score-0.65]
</p><p>31 Later in this paper, we call activation, referring to the activation level of a muscle pair. [sent-43, score-0.23]
</p><p>32 description threshold velocity radius standard deviation of the noise value function learning rate preferences learning rate discount factor, bootstrapping factor,     value 1 ms 200 ms 0 cm 0 cm/s 5 cm 5 cm 0. [sent-45, score-1.057]
</p><p>33 5 cm  ¡  description the basic simulation time step the feedback delay, initial position initial velocity target position target velocity target position radius  value 0. [sent-46, score-1.331]
</p><p>34 9  ¢  values for the mass, the damping coefﬁcient, and the stiffness coefﬁcient were taken from Barto et al. [sent-50, score-0.199]
</p><p>35 These values provide movement trajectories qualitatively similar to those observed in human wrist movements [22]. [sent-52, score-0.601]
</p><p>36  ¦ ¤  3 1  ( $& ©   ¦¤ ©¨¤ §¥1  ¢ £1    The fractional-power damping in this model is motivated by both biological evidence [8, 14] and computational considerations. [sent-53, score-0.164]
</p><p>37 Controlling a system with such a concave damping function is an easier control problem than for a system with apparently simpler linear damping. [sent-54, score-0.293]
</p><p>38 Fractional-power damping creates a qualitatively novel dynamical feature called a stiction region, a region in the position space around the equilibrium position consisting of pseudo-stable states, where the velocity of the plant remains very close to zero. [sent-55, score-0.938]
</p><p>39 For the parameter magnitudes used in our simulations, the stiction region is a region of radius 2. [sent-57, score-0.204]
</p><p>40 5 cm about the true equilibrium in the position space. [sent-58, score-0.315]
</p><p>41 Another essential feature of the neural signal transmission can be accounted for by using a cascade of low-pass temporal ﬁlters on the activation level [16]. [sent-59, score-0.189]
</p><p>42 2 The reinforcement learning controller We used the version of the actor-critic algorithm described by Sutton and Barto [20]. [sent-62, score-0.41]
</p><p>43 , activation level magnitudes evenly spaced every 1 cm between 0 cm and 10 cm. [sent-67, score-0.567]
</p><p>44 At the beginning of each episode the plant is at a ﬁxed initial state, and the episode is complete when the plant reaches the target region of the state space. [sent-71, score-0.94]
</p><p>45 3 Clocking the control signal For the controller to have sufﬁcient information about the current state of the plant, the controller internal representation of the state should be augmented by a vector of all the actions selected during the last delay period. [sent-76, score-1.038]
</p><p>46 To keep the dimension of the state space at a feasible level, we restrict the set of available policies and make the controller select a new activation level, , in a clocked manner at time intervals equal to the delay period. [sent-77, score-0.595]
</p><p>47 One step of the reinforcement learning controller is performed once a delay period, which corresponds to many steps of the underlying plant simulation. [sent-78, score-0.806]
</p><p>48 To simulate a stochastic plant we added Gaussian noise to . [sent-79, score-0.347]
</p><p>49 A new Gaussian disturbance was drawn every time a        new activation level was selected. [sent-80, score-0.151]
</p><p>50 Apart from the computational motivation, there is evidence of intermittent motor control by human subjects [13]. [sent-81, score-0.451]
</p><p>51 In our simulations we use an oversimpliﬁed special kind of intermittent control with a piecewise constant control signal whose magnitude changes at equal time intervals, but this is done for the sake of acceleration of the simulations and overall clarity. [sent-82, score-0.521]
</p><p>52 Intermittent control does not necessarily assume this particular kind of the control signal; the most important feature is that control segments are selected at particular points in time, and each control segment determines the control signal for an extended time interval. [sent-83, score-0.873]
</p><p>53 The time interval until selection of the next control segment can itself be one of the parameters [11]. [sent-84, score-0.154]
</p><p>54 3 Results The model learned to move the mass quickly and accurately to the target in approximately 1,000 episodes. [sent-85, score-0.241]
</p><p>55 Figure 3 shows a typical movement accomplished by the controller after learning. [sent-87, score-0.763]
</p><p>56 The movement shown in Figure 3 has two acceleration-deceleration phases called movement units or submovements. [sent-88, score-0.878]
</p><p>57 4000 3500  time per episode, ms  3000 2500 2000 1500 1000 500 0  0  100  200  300  400  500 episode #  600  700  800  900  1000  Figure 2: The learning curve averaged over 100 trials. [sent-89, score-0.163]
</p><p>58 Corrective submovements may occur before the plant reaches zero velocity. [sent-91, score-0.623]
</p><p>59 The controller generates this corrective submovement “on the ﬂy,” i. [sent-92, score-0.666]
</p><p>60 Figure 4 shows a sample movement accomplished by the controller after learning where such overlapping submovements occur. [sent-95, score-1.126]
</p><p>61 This can be seen clearly in panel (b) of Figure 4 where the velocity proﬁle of the movement is shown. [sent-96, score-0.612]
</p><p>62 Each of the submovements appears as a bell-shaped unit in the tangential velocity plot. [sent-97, score-0.573]
</p><p>63 Sometimes the controller accomplishes a movement with a single smooth submovement. [sent-98, score-0.777]
</p><p>64 A sample of such a movement is shown in Figure 5. [sent-99, score-0.393]
</p><p>65 4 Discussion The model learns to produce movements that are fast and accurate in the presence of noise and delayed sensory feedback. [sent-100, score-0.288]
</p><p>66 Panels (a) and (b) show the position and velocity time course respectively. [sent-103, score-0.286]
</p><p>67 The thin solid line shows the activation selected by the controller. [sent-105, score-0.125]
</p><p>68 The thick solid line shows the disturbed activation which is sent as the control signal to the plant. [sent-106, score-0.377]
</p><p>69 All of the subsequent submovements are much slower and cover much shorter segments in the position space. [sent-111, score-0.415]
</p><p>70 This feature stands in good agreement with the dual control model [12, 17], where the initial part of a movement is conducted in a ballistic manner, and the ﬁnal part is conducted under closed-loop control. [sent-112, score-0.636]
</p><p>71 Some evidence for this kind of dual control strategy comes from experiments in which subjects were given visual feedback only during the initial stage of movement. [sent-113, score-0.416]
</p><p>72 Subjects did not show signiﬁcant improvement under these conditions compared to trials in which they were deprived of visual feedback during the entire movement [4, 6]. [sent-114, score-0.523]
</p><p>73 In another set of experiments, proprioceptive feedback was altered by stimulations of muscle tendons. [sent-115, score-0.249]
</p><p>74 Movement accuracy decreased only when the stimulation was applied at the ﬁnal stages of movement [18]. [sent-116, score-0.393]
</p><p>75 Note, however, that the dual control strategy though is not explicitly designed into our model, but naturally emerges from the existing constraints and conditions. [sent-117, score-0.181]
</p><p>76 The reinforcement learning controller is encouraged by the reward structure to accomplish each movement as quickly as possible. [sent-118, score-0.939]
</p><p>77 On the other hand, it faces high uncertainty in the plant behavior. [sent-119, score-0.302]
</p><p>78 If the controller were to adopt a policy in which it attempts to directly hit the target in one fast submovement, then very often it would miss the target and spend long additional time to accomplish the task. [sent-122, score-0.637]
</p><p>79 The optimal policy in this situation is to move the arm close to the target by one fast submovement and then apply a few slow submovements to accurately move arm into the target region. [sent-123, score-1.303]
</p><p>80 The model learns to produce control sequences consisting of pairs of high activation steps followed by low activation steps. [sent-124, score-0.431]
</p><p>81 This feature stands in good agreement with pulse-step models of motor control [7, 19]. [sent-125, score-0.339]
</p><p>82 Each of the pulse-step combinations produces a submovement characterized by a bell-shaped unit in the velocity proﬁle. [sent-126, score-0.392]
</p><p>83 In biological motor control corrective submovements are observed very consistently, including both the overlapping and separate submovements. [sent-127, score-0.811]
</p><p>84 In the case of overlapping submovements, the corrective movement is called a predictive correction. [sent-128, score-0.58]
</p><p>85 Multiple submovements are observed mostly in infant reaching [5]. [sent-129, score-0.443]
</p><p>86 Adults perform routine everyday reaching movements ordinarily with a single smooth submovement, but in case of tight time constraints or accuracy requirements they revert to multiple submovements [15]. [sent-130, score-0.613]
</p><p>87 The suggested model sometimes accomplishes movements with a single smooth submovement (see Figure 5), but in most cases it produces multiple submovements much like an infant or an adult subject trying to move quickly and accurately. [sent-131, score-0.982]
</p><p>88 The suggested model is also consistent with theories of basal ganglia information processing for motor control [10]. [sent-132, score-0.466]
</p><p>89 Some of these theories suggest that dopamine neurons in the basal ganglia carry information similar to the secondary reinforcement (or temporal difference) in the actor-critic controller, i. [sent-133, score-0.256]
</p><p>90 A possible use of this kind of information is for initiating corrective submovements before the current movement is completed. [sent-137, score-0.873]
</p><p>91 A new approach to manipulator control: the cerebellar model articulation controller (CMAC). [sent-146, score-0.357]
</p><p>92 A cerebellar model of timing and prediction in the control of reaching. [sent-168, score-0.194]
</p><p>93 Contribution of visual information to feedforward and feedback processes in rapid pointing movements. [sent-173, score-0.208]
</p><p>94 Contributions of central programs to rapid limb movement in the cat. [sent-187, score-0.506]
</p><p>95 A model of the motor servo: incorporating nonlinear spindle receptor and muscle mechanical properties. [sent-200, score-0.228]
</p><p>96 A model of how the basal ganglia generates and uses neural signals that predict reinforcement. [sent-216, score-0.192]
</p><p>97 Optimality in human motor performance: ideal control of rapid aimed movements. [sent-240, score-0.473]
</p><p>98 Kinematic properties of rapid hand movements in a knob turning task. [sent-263, score-0.23]
</p><p>99 Proprioceptive control of goal directed movements in man studied by means of vibratory muscle tendon stimulation. [sent-287, score-0.385]
</p><p>100 Does time-optimal control of a stochastic system with sensory delay produce movement units? [sent-328, score-0.641]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('movement', 0.393), ('submovements', 0.321), ('controller', 0.317), ('plant', 0.302), ('submovement', 0.2), ('cm', 0.192), ('velocity', 0.192), ('arm', 0.177), ('control', 0.154), ('movements', 0.152), ('motor', 0.149), ('damping', 0.139), ('feedback', 0.13), ('activation', 0.125), ('corrective', 0.12), ('ms', 0.104), ('houk', 0.1), ('delay', 0.094), ('position', 0.094), ('reinforcement', 0.093), ('target', 0.085), ('basal', 0.084), ('policy', 0.081), ('ganglia', 0.079), ('muscle', 0.079), ('rapid', 0.078), ('reward', 0.077), ('nervous', 0.073), ('reaching', 0.07), ('stochasticity', 0.064), ('stiffness', 0.06), ('tangential', 0.06), ('episode', 0.059), ('state', 0.059), ('human', 0.056), ('barto', 0.055), ('accomplished', 0.053), ('units', 0.052), ('intermittent', 0.052), ('infant', 0.052), ('mass', 0.05), ('sometimes', 0.05), ('region', 0.048), ('presence', 0.047), ('noise', 0.045), ('fast', 0.044), ('overlapping', 0.042), ('emerge', 0.042), ('subjects', 0.04), ('beiser', 0.04), ('cerebellar', 0.04), ('fagg', 0.04), ('proprioceptive', 0.04), ('spring', 0.04), ('stiction', 0.04), ('tilings', 0.04), ('phases', 0.04), ('kind', 0.039), ('pro', 0.039), ('multiple', 0.038), ('signal', 0.038), ('stands', 0.036), ('radius', 0.036), ('aimed', 0.036), ('move', 0.036), ('accurately', 0.036), ('amherst', 0.035), ('accomplishes', 0.035), ('limb', 0.035), ('master', 0.035), ('adults', 0.035), ('cmac', 0.035), ('quickly', 0.034), ('simpli', 0.033), ('smooth', 0.032), ('adult', 0.032), ('disturbed', 0.032), ('spinal', 0.032), ('acceleration', 0.032), ('magnitudes', 0.032), ('kinematic', 0.032), ('rl', 0.03), ('davis', 0.03), ('equilibrium', 0.029), ('generates', 0.029), ('coef', 0.029), ('editors', 0.029), ('thick', 0.028), ('physiology', 0.028), ('dual', 0.027), ('sequences', 0.027), ('panel', 0.027), ('initial', 0.026), ('determines', 0.026), ('level', 0.026), ('simulations', 0.026), ('biological', 0.025), ('accomplish', 0.025), ('optimal', 0.025), ('predictive', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="181-tfidf-1" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>Author: Michael Kositsky, Andrew G. Barto</p><p>Abstract: Tangential hand velocity proﬁles of rapid human arm movements often appear as sequences of several bell-shaped acceleration-deceleration phases called submovements or movement units. This suggests how the nervous system might efﬁciently control a motor plant in the presence of noise and feedback delay. Another critical observation is that stochasticity in a motor control problem makes the optimal control policy essentially different from the optimal control policy for the deterministic case. We use a simpliﬁed dynamic model of an arm and address rapid aimed arm movements. We use reinforcement learning as a tool to approximate the optimal policy in the presence of noise and feedback delay. Using a simpliﬁed model we show that multiple submovements emerge as an optimal policy in the presence of noise and feedback delay. The optimal policy in this situation is to drive the arm’s end point close to the target by one fast submovement and then apply a few slow submovements to accurately drive the arm’s end point into the target region. In our simulations, the controller sometimes generates corrective submovements before the initial fast submovement is completed, much like the predictive corrections observed in a number of psychophysical experiments.</p><p>2 0.31636474 <a title="181-tfidf-2" href="./nips-2001-Linking_Motor_Learning_to_Function_Approximation%3A_Learning_in_an_Unlearnable_Force_Field.html">116 nips-2001-Linking Motor Learning to Function Approximation: Learning in an Unlearnable Force Field</a></p>
<p>Author: O. Donchin, Reza Shadmehr</p><p>Abstract: Reaching movements require the brain to generate motor commands that rely on an internal model of the task’s dynamics. Here we consider the errors that subjects make early in their reaching trajectories to various targets as they learn an internal model. Using a framework from function approximation, we argue that the sequence of errors should reﬂect the process of gradient descent. If so, then the sequence of errors should obey hidden state transitions of a simple dynamical system. Fitting the system to human data, we ﬁnd a surprisingly good ﬁt accounting for 98% of the variance. This allows us to draw tentative conclusions about the basis elements used by the brain in transforming sensory space to motor commands. To test the robustness of the results, we estimate the shape of the basis elements under two conditions: in a traditional learning paradigm with a consistent force ﬁeld, and in a random sequence of force ﬁelds where learning is not possible. Remarkably, we ﬁnd that the basis remains invariant. 1</p><p>3 0.22743773 <a title="181-tfidf-3" href="./nips-2001-Classifying_Single_Trial_EEG%3A_Towards_Brain_Computer_Interfacing.html">50 nips-2001-Classifying Single Trial EEG: Towards Brain Computer Interfacing</a></p>
<p>Author: Benjamin Blankertz, Gabriel Curio, Klaus-Robert Müller</p><p>Abstract: Driven by the progress in the ﬁeld of single-trial analysis of EEG, there is a growing interest in brain computer interfaces (BCIs), i.e., systems that enable human subjects to control a computer only by means of their brain signals. In a pseudo-online simulation our BCI detects upcoming ﬁnger movements in a natural keyboard typing condition and predicts their laterality. This can be done on average 100–230 ms before the respective key is actually pressed, i.e., long before the onset of EMG. Our approach is appealing for its short response time and high classiﬁcation accuracy (>96%) in a binary decision where no human training is involved. We compare discriminative classiﬁers like Support Vector Machines (SVMs) and different variants of Fisher Discriminant that possess favorable regularization properties for dealing with high noise cases (inter-trial variablity).</p><p>4 0.17350847 <a title="181-tfidf-4" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>Author: Yun Gao, Michael J. Black, Elie Bienenstock, Shy Shoham, John P. Donoghue</p><p>Abstract: Statistical learning and probabilistic inference techniques are used to infer the hand position of a subject from multi-electrode recordings of neural activity in motor cortex. First, an array of electrodes provides training data of neural ﬁring conditioned on hand kinematics. We learn a nonparametric representation of this ﬁring activity using a Bayesian model and rigorously compare it with previous models using cross-validation. Second, we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using Bayesian inference. The learned ﬁring models of multiple cells are used to deﬁne a nonGaussian likelihood term which is combined with a prior probability for the kinematics. A particle ﬁltering method is used to represent, update, and propagate the posterior distribution over time. The approach is compared with traditional linear ﬁltering methods; the results suggest that it may be appropriate for neural prosthetic applications.</p><p>5 0.1145839 <a title="181-tfidf-5" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>Author: Shie Mannor, Nahum Shimkin</p><p>Abstract: We consider the problem of learning to attain multiple goals in a dynamic environment, which is initially unknown. In addition, the environment may contain arbitrarily varying elements related to actions of other agents or to non-stationary moves of Nature. This problem is modelled as a stochastic (Markov) game between the learning agent and an arbitrary player, with a vector-valued reward function. The objective of the learning agent is to have its long-term average reward vector belong to a given target set. We devise an algorithm for achieving this task, which is based on the theory of approachability for stochastic games. This algorithm combines, in an appropriate way, a ﬁnite set of standard, scalar-reward learning algorithms. Suﬃcient conditions are given for the convergence of the learning algorithm to a general target set. The specialization of these results to the single-controller Markov decision problem are discussed as well. 1</p><p>6 0.10675156 <a title="181-tfidf-6" href="./nips-2001-Modularity_in_the_motor_system%3A_decomposition_of_muscle_patterns_as_combinations_of_time-varying_synergies.html">125 nips-2001-Modularity in the motor system: decomposition of muscle patterns as combinations of time-varying synergies</a></p>
<p>7 0.084062591 <a title="181-tfidf-7" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>8 0.08078637 <a title="181-tfidf-8" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>9 0.077319182 <a title="181-tfidf-9" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>10 0.077289678 <a title="181-tfidf-10" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>11 0.074887916 <a title="181-tfidf-11" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>12 0.070585445 <a title="181-tfidf-12" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>13 0.066759914 <a title="181-tfidf-13" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>14 0.06468457 <a title="181-tfidf-14" href="./nips-2001-A_Model_of_the_Phonological_Loop%3A_Generalization_and_Binding.html">12 nips-2001-A Model of the Phonological Loop: Generalization and Binding</a></p>
<p>15 0.063771687 <a title="181-tfidf-15" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>16 0.062005274 <a title="181-tfidf-16" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>17 0.059336375 <a title="181-tfidf-17" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>18 0.059225004 <a title="181-tfidf-18" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>19 0.058760282 <a title="181-tfidf-19" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>20 0.053446185 <a title="181-tfidf-20" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.159), (1, -0.182), (2, 0.064), (3, 0.071), (4, -0.078), (5, 0.092), (6, -0.083), (7, 0.16), (8, 0.147), (9, 0.142), (10, -0.008), (11, -0.048), (12, -0.266), (13, -0.018), (14, 0.012), (15, 0.236), (16, -0.29), (17, -0.09), (18, 0.101), (19, 0.007), (20, 0.079), (21, 0.023), (22, -0.073), (23, 0.079), (24, -0.049), (25, -0.195), (26, 0.072), (27, 0.078), (28, -0.083), (29, 0.062), (30, 0.045), (31, -0.088), (32, 0.181), (33, -0.041), (34, -0.083), (35, -0.017), (36, 0.005), (37, 0.049), (38, 0.042), (39, 0.014), (40, 0.08), (41, 0.055), (42, 0.024), (43, 0.05), (44, 0.04), (45, -0.088), (46, -0.004), (47, 0.028), (48, -0.002), (49, -0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96660799 <a title="181-lsi-1" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>Author: Michael Kositsky, Andrew G. Barto</p><p>Abstract: Tangential hand velocity proﬁles of rapid human arm movements often appear as sequences of several bell-shaped acceleration-deceleration phases called submovements or movement units. This suggests how the nervous system might efﬁciently control a motor plant in the presence of noise and feedback delay. Another critical observation is that stochasticity in a motor control problem makes the optimal control policy essentially different from the optimal control policy for the deterministic case. We use a simpliﬁed dynamic model of an arm and address rapid aimed arm movements. We use reinforcement learning as a tool to approximate the optimal policy in the presence of noise and feedback delay. Using a simpliﬁed model we show that multiple submovements emerge as an optimal policy in the presence of noise and feedback delay. The optimal policy in this situation is to drive the arm’s end point close to the target by one fast submovement and then apply a few slow submovements to accurately drive the arm’s end point into the target region. In our simulations, the controller sometimes generates corrective submovements before the initial fast submovement is completed, much like the predictive corrections observed in a number of psychophysical experiments.</p><p>2 0.90797633 <a title="181-lsi-2" href="./nips-2001-Linking_Motor_Learning_to_Function_Approximation%3A_Learning_in_an_Unlearnable_Force_Field.html">116 nips-2001-Linking Motor Learning to Function Approximation: Learning in an Unlearnable Force Field</a></p>
<p>Author: O. Donchin, Reza Shadmehr</p><p>Abstract: Reaching movements require the brain to generate motor commands that rely on an internal model of the task’s dynamics. Here we consider the errors that subjects make early in their reaching trajectories to various targets as they learn an internal model. Using a framework from function approximation, we argue that the sequence of errors should reﬂect the process of gradient descent. If so, then the sequence of errors should obey hidden state transitions of a simple dynamical system. Fitting the system to human data, we ﬁnd a surprisingly good ﬁt accounting for 98% of the variance. This allows us to draw tentative conclusions about the basis elements used by the brain in transforming sensory space to motor commands. To test the robustness of the results, we estimate the shape of the basis elements under two conditions: in a traditional learning paradigm with a consistent force ﬁeld, and in a random sequence of force ﬁelds where learning is not possible. Remarkably, we ﬁnd that the basis remains invariant. 1</p><p>3 0.69287711 <a title="181-lsi-3" href="./nips-2001-Modularity_in_the_motor_system%3A_decomposition_of_muscle_patterns_as_combinations_of_time-varying_synergies.html">125 nips-2001-Modularity in the motor system: decomposition of muscle patterns as combinations of time-varying synergies</a></p>
<p>Author: A. D'avella, M. C. Tresch</p><p>Abstract: The question of whether the nervous system produces movement through the combination of a few discrete elements has long been central to the study of motor control. Muscle synergies, i.e. coordinated patterns of muscle activity, have been proposed as possible building blocks. Here we propose a model based on combinations of muscle synergies with a speciﬁc amplitude and temporal structure. Time-varying synergies provide a realistic basis for the decomposition of the complex patterns observed in natural behaviors. To extract time-varying synergies from simultaneous recording of EMG activity we developed an algorithm which extends existing non-negative matrix factorization techniques.</p><p>4 0.51989776 <a title="181-lsi-4" href="./nips-2001-Classifying_Single_Trial_EEG%3A_Towards_Brain_Computer_Interfacing.html">50 nips-2001-Classifying Single Trial EEG: Towards Brain Computer Interfacing</a></p>
<p>Author: Benjamin Blankertz, Gabriel Curio, Klaus-Robert Müller</p><p>Abstract: Driven by the progress in the ﬁeld of single-trial analysis of EEG, there is a growing interest in brain computer interfaces (BCIs), i.e., systems that enable human subjects to control a computer only by means of their brain signals. In a pseudo-online simulation our BCI detects upcoming ﬁnger movements in a natural keyboard typing condition and predicts their laterality. This can be done on average 100–230 ms before the respective key is actually pressed, i.e., long before the onset of EMG. Our approach is appealing for its short response time and high classiﬁcation accuracy (>96%) in a binary decision where no human training is involved. We compare discriminative classiﬁers like Support Vector Machines (SVMs) and different variants of Fisher Discriminant that possess favorable regularization properties for dealing with high noise cases (inter-trial variablity).</p><p>5 0.41205662 <a title="181-lsi-5" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>Author: Yun Gao, Michael J. Black, Elie Bienenstock, Shy Shoham, John P. Donoghue</p><p>Abstract: Statistical learning and probabilistic inference techniques are used to infer the hand position of a subject from multi-electrode recordings of neural activity in motor cortex. First, an array of electrodes provides training data of neural ﬁring conditioned on hand kinematics. We learn a nonparametric representation of this ﬁring activity using a Bayesian model and rigorously compare it with previous models using cross-validation. Second, we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using Bayesian inference. The learned ﬁring models of multiple cells are used to deﬁne a nonGaussian likelihood term which is combined with a prior probability for the kinematics. A particle ﬁltering method is used to represent, update, and propagate the posterior distribution over time. The approach is compared with traditional linear ﬁltering methods; the results suggest that it may be appropriate for neural prosthetic applications.</p><p>6 0.31432807 <a title="181-lsi-6" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>7 0.30343857 <a title="181-lsi-7" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>8 0.29645357 <a title="181-lsi-8" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>9 0.28287813 <a title="181-lsi-9" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>10 0.27253649 <a title="181-lsi-10" href="./nips-2001-Geometrical_Singularities_in_the_Neuromanifold_of_Multilayer_Perceptrons.html">83 nips-2001-Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons</a></p>
<p>11 0.25850454 <a title="181-lsi-11" href="./nips-2001-Switch_Packet_Arbitration_via_Queue-Learning.html">177 nips-2001-Switch Packet Arbitration via Queue-Learning</a></p>
<p>12 0.25028649 <a title="181-lsi-12" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>13 0.23972644 <a title="181-lsi-13" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>14 0.21959509 <a title="181-lsi-14" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>15 0.21860687 <a title="181-lsi-15" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>16 0.21843244 <a title="181-lsi-16" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>17 0.21698058 <a title="181-lsi-17" href="./nips-2001-A_Rational_Analysis_of_Cognitive_Control_in_a_Speeded_Discrimination_Task.html">18 nips-2001-A Rational Analysis of Cognitive Control in a Speeded Discrimination Task</a></p>
<p>18 0.20986894 <a title="181-lsi-18" href="./nips-2001-Stabilizing_Value_Function_Approximation_with_the_BFBP_Algorithm.html">175 nips-2001-Stabilizing Value Function Approximation with the BFBP Algorithm</a></p>
<p>19 0.20924512 <a title="181-lsi-19" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>20 0.20251173 <a title="181-lsi-20" href="./nips-2001-A_Model_of_the_Phonological_Loop%3A_Generalization_and_Binding.html">12 nips-2001-A Model of the Phonological Loop: Generalization and Binding</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.023), (17, 0.021), (19, 0.025), (27, 0.093), (30, 0.123), (38, 0.029), (59, 0.018), (72, 0.041), (79, 0.025), (81, 0.337), (83, 0.047), (91, 0.123)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86527014 <a title="181-lda-1" href="./nips-2001-Analog_Soft-Pattern-Matching_Classifier_using_Floating-Gate_MOS_Technology.html">34 nips-2001-Analog Soft-Pattern-Matching Classifier using Floating-Gate MOS Technology</a></p>
<p>Author: Toshihiko Yamasaki, Tadashi Shibata</p><p>Abstract: A flexible pattern-matching analog classifier is presented in conjunction with a robust image representation algorithm called Principal Axes Projection (PAP). In the circuit, the functional form of matching is configurable in terms of the peak position, the peak height and the sharpness of the similarity evaluation. The test chip was fabricated in a 0.6-µm CMOS technology and successfully applied to hand-written pattern recognition and medical radiograph analysis using PAP as a feature extraction pre-processing step for robust image coding. The separation and classification of overlapping patterns is also experimentally demonstrated. 1 I ntr o du c ti o n Pattern classification using template matching techniques is a powerful tool in implementing human-like intelligent systems. However, the processing is computationally very expensive, consuming a lot of CPU time when implemented as software running on general-purpose computers. Therefore, software approaches are not practical for real-time applications. For systems working in mobile environment, in particular, they are not realistic because the memory and computational resources are severely limited. The development of analog VLSI chips having a fully parallel template matching architecture [1,2] would be a promising solution in such applications because they offer an opportunity of low-power operation as well as very compact implementation. In order to build a real human-like intelligent system, however, not only the pattern representation algorithm but also the matching hardware itself needs to be made flexible and robust in carrying out the pattern matching task. First of all, two-dimensional patterns need to be represented by feature vectors having substantially reduced dimensions, while at the same time preserving the human perception of similarity among patterns in the vector space mapping. For this purpose, an image representation algorithm called Principal Axes Projection (PAP) has been de- veloped [3] and its robust nature in pattern recognition has been demonstrated in the applications to medical radiograph analysis [3] and hand-written digits recognition [4]. However, the demonstration so far was only carried out by computer simulation. Regarding the matching hardware, high-flexibility analog template matching circuits have been developed for PAP vector representation. The circuits are flexible in a sense that the matching criteria (the weight to elements, the strictness in matching) are configurable. In Ref. [5], the fundamental characteristics of the building block circuits were presented, and their application to simple hand-written digits was presented in Ref. [6]. The purpose of this paper is to demonstrate the robust nature of the hardware matching system by experiments. The classification of simple hand-written patterns and the cephalometric landmark identification in gray-scale medical radiographs have been carried out and successful results are presented. In addition, multiple overlapping patterns can be separated without utilizing a priori knowledge, which is one of the most difficult problems at present in artificial intelligence. 2 I ma g e re pr es e n tati on by P AP PAP is a feature extraction technique using the edge information. The input image (64x64 pixels) is first subjected to pixel-by-pixel spatial filtering operations to detect edges in four directions: horizontal (HR); vertical (VR); +45 degrees (+45); and –45 degrees (-45). Each detected edge is represented by a binary flag and four edge maps are generated. The two-dimensional bit array in an edge map is reduced to a one-dimensional array of numerals by projection. The horizontal edge flags are accumulated in the horizontal direction and projected onto vertical axis. The vertical, +45-degree and –45-degree edge flags are similarly projected onto horizontal, -45-degree and +45-degree axes, respectively. Therefore the method is called “Principal Axes Projection (PAP)” [3,4]. Then each projection data set is series connected in the order of HR, +45, VR, -45 to form a feature vector. Neighboring four elements are averaged and merged to one element and a 64-dimensional vector is finally obtained. This vector representation very well preserves the human perception of similarity in the vector space. In the experiments below, we have further reduced the feature vector to 16 dimensions by merging each set of four neighboring elements into one, without any significant degradation in performance. C i r cui t c o nf i g ura ti ons A B C VGG A B C VGG IOUT IOUT 1 1 2 2 4 4 1 VIN 13 VIN RST RST £ ¡ ¤¢  £ ¥ §¦  3 Figure 1: Schematic of vector element matching circuit: (a) pyramid (gain reduction) type; (b) plateau (feedback) type. The capacitor area ratio is indicated in the figure. The basic functional form of the similarity evaluation is generated by the shortcut current flowing in a CMOS inverter as in Refs. [7,8,9]. However, their circuits were utilized to form radial basis functions and only the peak position was programmable. In our circuits, not only the peak position but also the peak height and the sharpness of the peak response shape are made configurable to realize flexible matching operations [5]. Two types of the element matching circuit are shown in Fig. 1. They evaluate the similarity between two vector elements. The result of the evaluation is given as an output current (IOUT ) from the pMOS current mirror. The peak position is temporarily memorized by auto-zeroing of the CMOS inverter. The common-gate transistor with VGG stabilizes the voltage supply to the inverter. By controlling the gate bias VGG, the peak height can be changed. This corresponds to multiplying a weight factor to the element. The sharpness of the functional form is taken as the strictness of the similarity evaluation. In the pyramid type circuit (Fig. 1(a)), the sharpness is controlled by the gain reduction in the input. In the plateau type (Fig. 1(b)), the output voltage of the inverter is fed back to input nodes and the sharpness changes in accordance with the amount of the feedback.    ¥£¡ ¦¤¢   £¨ 9&% ¦©§ (!! #$ 5 !' #$ &% 9 9 4 92 !¦ A1@9  ¨¥  5 4 52 (!  5 8765  9) 0 1 ¥ 1 ¨</p><p>same-paper 2 0.81385916 <a title="181-lda-2" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>Author: Michael Kositsky, Andrew G. Barto</p><p>Abstract: Tangential hand velocity proﬁles of rapid human arm movements often appear as sequences of several bell-shaped acceleration-deceleration phases called submovements or movement units. This suggests how the nervous system might efﬁciently control a motor plant in the presence of noise and feedback delay. Another critical observation is that stochasticity in a motor control problem makes the optimal control policy essentially different from the optimal control policy for the deterministic case. We use a simpliﬁed dynamic model of an arm and address rapid aimed arm movements. We use reinforcement learning as a tool to approximate the optimal policy in the presence of noise and feedback delay. Using a simpliﬁed model we show that multiple submovements emerge as an optimal policy in the presence of noise and feedback delay. The optimal policy in this situation is to drive the arm’s end point close to the target by one fast submovement and then apply a few slow submovements to accurately drive the arm’s end point into the target region. In our simulations, the controller sometimes generates corrective submovements before the initial fast submovement is completed, much like the predictive corrections observed in a number of psychophysical experiments.</p><p>3 0.5171898 <a title="181-lda-3" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<p>Author: Eran Segal, Daphne Koller, Dirk Ormoneit</p><p>Abstract: Many domains are naturally organized in an abstraction hierarchy or taxonomy, where the instances in “nearby” classes in the taxonomy are similar. In this paper, we provide a general probabilistic framework for clustering data into a set of classes organized as a taxonomy, where each class is associated with a probabilistic model from which the data was generated. The clustering algorithm simultaneously optimizes three things: the assignment of data instances to clusters, the models associated with the clusters, and the structure of the abstraction hierarchy. A unique feature of our approach is that it utilizes global optimization algorithms for both of the last two steps, reducing the sensitivity to noise and the propensity to local maxima that are characteristic of algorithms such as hierarchical agglomerative clustering that only take local steps. We provide a theoretical analysis for our algorithm, showing that it converges to a local maximum of the joint likelihood of model and data. We present experimental results on synthetic data, and on real data in the domains of gene expression and text.</p><p>4 0.5125761 <a title="181-lda-4" href="./nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">102 nips-2001-KLD-Sampling: Adaptive Particle Filters</a></p>
<p>Author: Dieter Fox</p><p>Abstract: Over the last years, particle ﬁlters have been applied with great success to a variety of state estimation problems. We present a statistical approach to increasing the efﬁciency of particle ﬁlters by adapting the size of sample sets on-the-ﬂy. The key idea of the KLD-sampling method is to bound the approximation error introduced by the sample-based representation of the particle ﬁlter. The name KLD-sampling is due to the fact that we measure the approximation error by the Kullback-Leibler distance. Our adaptation approach chooses a small number of samples if the density is focused on a small part of the state space, and it chooses a large number of samples if the state uncertainty is high. Both the implementation and computation overhead of this approach are small. Extensive experiments using mobile robot localization as a test application show that our approach yields drastic improvements over particle ﬁlters with ﬁxed sample set sizes and over a previously introduced adaptation technique.</p><p>5 0.51148981 <a title="181-lda-5" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>Author: Thomas P. Trappenberg, Edmund T. Rolls, Simon M. Stringer</p><p>Abstract: Inferior temporal cortex (IT) neurons have large receptive ﬁelds when a single effective object stimulus is shown against a blank background, but have much smaller receptive ﬁelds when the object is placed in a natural scene. Thus, translation invariant object recognition is reduced in natural scenes, and this may help object selection. We describe a model which accounts for this by competition within an attractor in which the neurons are tuned to different objects in the scene, and the fovea has a higher cortical magniﬁcation factor than the peripheral visual ﬁeld. Furthermore, we show that top-down object bias can increase the receptive ﬁeld size, facilitating object search in complex visual scenes, and providing a model of object-based attention. The model leads to the prediction that introduction of a second object into a scene with blank background will reduce the receptive ﬁeld size to values that depend on the closeness of the second object to the target stimulus. We suggest that mechanisms of this type enable the output of IT to be primarily about one object, so that the areas that receive from IT can select the object as a potential target for action.</p><p>6 0.50420785 <a title="181-lda-6" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>7 0.50369644 <a title="181-lda-7" href="./nips-2001-Linking_Motor_Learning_to_Function_Approximation%3A_Learning_in_an_Unlearnable_Force_Field.html">116 nips-2001-Linking Motor Learning to Function Approximation: Learning in an Unlearnable Force Field</a></p>
<p>8 0.50340575 <a title="181-lda-8" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>9 0.50245804 <a title="181-lda-9" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>10 0.5024389 <a title="181-lda-10" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>11 0.5016855 <a title="181-lda-11" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>12 0.49963397 <a title="181-lda-12" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>13 0.49928284 <a title="181-lda-13" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>14 0.49651295 <a title="181-lda-14" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>15 0.49630064 <a title="181-lda-15" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>16 0.4962455 <a title="181-lda-16" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>17 0.49611205 <a title="181-lda-17" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>18 0.49546903 <a title="181-lda-18" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>19 0.49486065 <a title="181-lda-19" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>20 0.49372691 <a title="181-lda-20" href="./nips-2001-Audio-Visual_Sound_Separation_Via_Hidden_Markov_Models.html">39 nips-2001-Audio-Visual Sound Separation Via Hidden Markov Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
