<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>169 nips-2001-Small-World Phenomena and the Dynamics of Information</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-169" href="#">nips2001-169</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>169 nips-2001-Small-World Phenomena and the Dynamics of Information</h1>
<br/><p>Source: <a title="nips-2001-169-pdf" href="http://papers.nips.cc/paper/2061-small-world-phenomena-and-the-dynamics-of-information.pdf">pdf</a></p><p>Author: Jon M. Kleinberg</p><p>Abstract: unkown-abstract</p><p>Reference: <a title="nips-2001-169-reference" href="../nips2001_reference/nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In recent work, we have been investigating the problem of decentralized search in large information networks [14, 15]. [sent-3, score-0.699]
</p><p>2 Our initial motivation was an experiment that dealt directly with the search problem in a decidedly pre-Internet context: Stanley Milgram’s famous study of the small-world phenomenon [16, 17]. [sent-4, score-0.166]
</p><p>3 Milgram was seeking to determine whether most pairs of people in society were linked by short chains of acquaintances, and for this purpose he recruited individuals to try forwarding a letter to a designated “target” through people they knew on a ﬁrstname basis. [sent-5, score-0.32]
</p><p>4 This latter point is concerned precisely with a type of decentralized navigation in a social network, consisting of people as nodes and links joining pairs of people who know each other. [sent-9, score-1.209]
</p><p>5 This is clearly what was taking place in Milgram’s experiments: participants, using the information available to them, were estimating which of their acquaintances would lead to the shortest path through the full social network. [sent-15, score-0.151]
</p><p>6 One starts with a p-dimensional lattice, in which nodes are joined only to their nearest neighbors. [sent-18, score-0.172]
</p><p>7 One then adds k directed long-range links out of each node v, for a constant k; the endpoint of each link is chosen uniformly at random. [sent-19, score-0.603]
</p><p>8 Results from the theory of random graphs can be used to show that with high probability, there will be short paths connecting all pairs of nodes (see e. [sent-20, score-0.31]
</p><p>9 This network model, a superposition of a lattice and a set of long-range links, is a natural one in which to study the behavior of a decentralized search algorithm. [sent-25, score-0.934]
</p><p>10 The algorithm knows the structure of the lattice; it starts from a node s, and is told the coordinates of a target node t. [sent-26, score-0.422]
</p><p>11 It successively traverses links of the network so as to reach the target as quickly as possible; but, crucially, it does not know the long-range links out of any node that it has not yet visited. [sent-27, score-0.789]
</p><p>12 In addition to moving forward across directed links, the algorithm may travel in reverse across any link that it has already followed in the forward direction; this allows it to back up when it does not want to continue exploring from its current node. [sent-28, score-0.22]
</p><p>13 We say that the algorithm has search time Y (n) if, on a randomly generated n-node network with s and t chosen uniformly at random, it reaches the target t in at most Y (n) steps with probability at least 1 − ε(n), for a function ε(·) going to 0 with n. [sent-30, score-0.307]
</p><p>14 The ﬁrst result in [15] is that no decentralized algorithm can achieve a polylogarithmic search time in this network model — even though, with high probability, there are paths of polylogarithmic length joining all pairs of nodes. [sent-31, score-1.71]
</p><p>15 Speciﬁcally, when we construct a long-range link (v, w) out of v, we do not choose w uniformly at random; rather, we choose it with probability proportional to d−α, where d is the lattice distance from v to w. [sent-33, score-0.395]
</p><p>16 In this way, the long-range links become correlated to the geometry of the lattice. [sent-34, score-0.251]
</p><p>17 We show in [15] that when α is equal to p, the dimension of the underlying lattice, then a decentralized greedy algorithm achieves search time proportional to log2 n; and for any other value of α, there is no decentralized algorithm with a polylogarithmic search time. [sent-35, score-1.757]
</p><p>18 Recent work by Zhang, Goel, and Govindan [26] has shown how the distribution of links associated with the optimal value of α may lead to improved performance for decentralized search in the Freenet peer-to-peer system. [sent-36, score-0.913]
</p><p>19 In joint work with Kempe and Demers [12], we have studied how distributions that are  inverse-polynomial in the distance between nodes can be used in the design of gossip protocols for spreading information in a network of communicating agents. [sent-39, score-0.402]
</p><p>20 The goal of the present paper is to consider more generally the problem of decentralized search in networks with partial information about the underlying structure. [sent-40, score-0.728]
</p><p>21 While a lattice makes for a natural network backbone, we would like to understand the extent to which the principles underlying eﬃcient decentralized search can be abstracted away from a lattice-like structure. [sent-41, score-0.963]
</p><p>22 We begin by considering networks that are generated from a hierarchical structure, and show that qualitatively similar results can be obtained; we then discuss a general model of group structures, which can be viewed as a simultaneous generalization of lattices and hierarchies. [sent-42, score-0.389]
</p><p>23 Thus we describe this case ﬁrst, and then move on to the case in which each node has only a constant number of out-links. [sent-45, score-0.182]
</p><p>24 2  Hierarchical Network Models  In a number of settings, nodes represent objects that can be classiﬁed according to a hierarchy or taxonomy; and nodes are more likely to form links if they belong to the same small sub-tree in the hierarchy, indicating they are more closely related. [sent-46, score-0.685]
</p><p>25 Let V denote the set of leaves of T ; we use n to denote the size of V , and for two leaves v and w, we use h(v, w) to denote the height of the least common ancestor of v and w in T . [sent-48, score-0.206]
</p><p>26 We are also given a monotone non-increasing function f(·) that will determine link probabilities. [sent-49, score-0.149]
</p><p>27 For each node v ∈ V , we create a random link to w with probability proportional to f(h(v, w)); in other words, the probability of choosing w is equal to f(h(v, w))/ x=v f(h(v, x)). [sent-50, score-0.297]
</p><p>28 We create k links out of each node v this way, choosing the endpoint w each time independently and with repetition allowed. [sent-51, score-0.474]
</p><p>29 It is important to note that the tree T is used only in the generation process of G; neither the edges nor the non-leaf nodes of T appear in G. [sent-54, score-0.172]
</p><p>30 (By way of contrast, the lattice model in [15] included both the long-range links and the nearest-neighbor links of the lattice itself. [sent-55, score-0.936]
</p><p>31 ) When we use the term “node” without any qualiﬁcation, we are referring to nodes of G, and hence to leaves of T ; we will use “internal node” to refer to non-leaf nodes of T . [sent-56, score-0.424]
</p><p>32 We refer to the process producing G as a hierarchical model with exponent α if the function f(h) grows asymptotically like b−αh: f(h) b−α h = 0 for all α < α and lim = 0 for all α > α. [sent-57, score-0.391]
</p><p>33 −α h h→∞ b h→∞ f(h) lim  There are several natural interpretations for a hierarchical network model. [sent-58, score-0.25]
</p><p>34 The linkage probabilities then have a simple meaning — they are based on the distance between the topics of the pages, as measured by the height of their least common ancestor in the topic hierarchy. [sent-64, score-0.212]
</p><p>35 A page on Sci-  ence/Computer Science/Algorithms may thus be more likely to link to one on Science/Computer Science/Machine Learning than to one on Arts/Music/Opera. [sent-65, score-0.194]
</p><p>36 It is worth noting that a number of recent models for the link structure of the Web, as well as other relational structures, have looked at diﬀerent ways in which similarities in content can aﬀect the probability of linkage; see e. [sent-67, score-0.166]
</p><p>37 Studies performed by Killworth and Bernard [13] showed that in choosing a recipient for the letter, participants were overwhelmingly guided by one of two criteria: similarity to the target in geography, or similarity to the target in occupation. [sent-71, score-0.172]
</p><p>38 If one views the lattice as forming a simple model for geographic factors, the hierarchical model can similarly be interpreted as forming a “topic hierarchy” of occupations, with individuals at the leaves. [sent-72, score-0.485]
</p><p>39 Thus, for example, the occupations of “banker” and “stock broker” may belong to the same small sub-tree; since the target in one of Milgram’s experiments was a stock broker, it might therefore be a good strategy to send the letter to a banker. [sent-73, score-0.244]
</p><p>40 Independently of our work here, Watts, Dodds, and Newman have recently studied hierarchical structures for modeling Milgram’s experiment in social networks [24]. [sent-74, score-0.323]
</p><p>41 We now consider the search problem in a graph G generated from a hierarchical model: A decentralized algorithm has knowledge of the tree T , and knows the location of a target leaf that it must reach; however, it only learns the structure of G as it visits nodes. [sent-75, score-1.048]
</p><p>42 The exponent α determines how the structures of G and T are related; how does this aﬀect the navigability of G? [sent-76, score-0.222]
</p><p>43 In the analysis of the lattice model [15], the key property of the optimal exponent was that, from any point, there was a reasonable probability of a long-range link that halved the distance to the target. [sent-77, score-0.529]
</p><p>44 We make use of a similar idea here: when α = 1, there is always a reasonable probability of ﬁnding a long-range link into a strictly smaller sub-tree containing the target. [sent-78, score-0.192]
</p><p>45 As mentioned above, we focus here on the case of polylogarithmic outdegree, with the case of constant out-degree deferred until later. [sent-79, score-0.44]
</p><p>46 1 (a) There is a hierarchical model with exponent α = 1 and polylogarithmic out-degree in which a decentralized algorithm can achieve search time O(log n). [sent-81, score-1.413]
</p><p>47 (b) For every α = 1, there is no hierarchical model with exponent α and polylogarithmic out-degree in which a decentralized algorithm can achieve polylogarithmic search time. [sent-82, score-1.817]
</p><p>48 To prove (a), we show that when the search is at a node v whose least common ancestor with the target has height h, there is a high probability that v has a link into the sub-tree of height h−1 containing the target. [sent-88, score-0.742]
</p><p>49 In this way, the search reaches the target in logarithmically many steps. [sent-89, score-0.252]
</p><p>50 To prove (b), we exhibit a sub-tree T containing the target such that, with high probability, it takes any decentralized algorithm more than a polylogarithmic number of steps to ﬁnd a link into T . [sent-90, score-1.178]
</p><p>51 3  Group Structures  The analysis of the search problem in a hierarchical model is similar to the analysis of the lattice-based approach in [15], although the two types of models seem  superﬁcially quite diﬀerent. [sent-91, score-0.307]
</p><p>52 Consider a collection of individuals in a social network, and suppose that we know of certain groups to which individuals belong — people who live in the same town, or work in the same profession, or have some other aﬃliation in common. [sent-93, score-0.377]
</p><p>53 In a lattice-based model, there may be a group for each subset of lattice points contained in a common ball (grouping based on proximity); in a hierarchical model, there may be a group for each subset of leaves contained in a common sub-tree. [sent-95, score-0.783]
</p><p>54 We now discuss the notion of a group structure, to make this precise; we follow a model proposed in joint work with Kempe and Demers [12], where we were concerned with designing gossip protocols for lattices and hierarchies. [sent-96, score-0.33]
</p><p>55 A group structure consists of an underlying set V of nodes, and a collection of subsets of V (the groups). [sent-98, score-0.261]
</p><p>56 (i) If R is a group of size q ≥ 2 containing a node v, then there is a group R ⊆ R containing v that is strictly smaller than R, but has size at least λq. [sent-100, score-0.58]
</p><p>57 are groups that all have size at most q and all contain a common node v, then their union has size at most βq. [sent-104, score-0.21]
</p><p>58 However, it is easy to construct examples of group structures that do not arise in this way from lattices or hierarchies. [sent-106, score-0.299]
</p><p>59 Given a group structure (V, {Ri}), and a monotone non-increasing function f(·), we now consider the following process for generating a graph on V . [sent-107, score-0.277]
</p><p>60 For two nodes v and w, we use q(v, w) to denote the minimum size of a group containing both v and w. [sent-108, score-0.389]
</p><p>61 ) For each node v ∈ V , we create a random link to w with probability proportional to f(q(v, w)); repeating this k times independently yields k links out of v. [sent-110, score-0.548]
</p><p>62 We refer to this as a group-induced model with exponent α if f(q) grows asymptotically like q −α : lim  h→∞  f(q) q−α = 0 for all α < α and lim = 0 for all α > α. [sent-111, score-0.304]
</p><p>63 h→∞ f(q) q−α  A decentralized search algorithm in such a network is given knowledge of the full group structure, and must follow links of G to a designated target t. [sent-112, score-1.201]
</p><p>64 1 (a) For every group structure, there is a group-induced model with exponent α = 1 and polylogarithmic out-degree in which a decentralized algorithm can achieve search time O(log n). [sent-116, score-1.419]
</p><p>65 (b) For every α < 1, there is no group-induced model with exponent α and polylogarithmic out-degree in which a decentralized algorithm can achieve polylogarithmic search time. [sent-117, score-1.676]
</p><p>66 Notice that in a hierarchical model, the smallest group (sub-tree) containing two nodes v and w has size bh(v,w) , and so Theorem 3. [sent-118, score-0.53]
</p><p>67 Similarly, on a lattice, the smallest group (ball) containing two nodes v and w at  lattice distance d has size Θ(dp ), and so Theorem 3. [sent-121, score-0.634]
</p><p>68 1(a) implies a version of the result from [15], that eﬃcient search is possible in a lattice model when nodes form links with probability d−p . [sent-122, score-0.806]
</p><p>69 (In the version of the lattice result implied here, there are no nearest-neighbor links at all; but each node has a polylogarithmic number of out-links. [sent-123, score-1.018]
</p><p>70 We consider a node v — the current point in the search — for which the smallest group containing v and the target t has size q. [sent-127, score-0.615]
</p><p>71 Using group structure properties (i) and (ii), we show there is a high probability that v has a link into a group containing t of size between λ2 q and λq. [sent-128, score-0.53]
</p><p>72 In this way, the search passes through groups containing t of sizes that diminish geometrically, and hence it terminates in logarithmic time. [sent-129, score-0.272]
</p><p>73 This is because there exist group-induced models with exponents α > 1 in which decentralized algorithms can achieve polylogarithmic search time. [sent-132, score-1.142]
</p><p>74 For example, consider an undirected graph G∗ in which each node has 3 neighbors, and each pair of nodes can be connected by a path of length O(log n). [sent-133, score-0.406]
</p><p>75 Give a group structure (V, {Ri}), and a cut-oﬀ value q, we deﬁne a graph H(q) on V by joining any two nodes that belong to a common group of size at most q. [sent-136, score-0.69]
</p><p>76 Note that H(q) is not a random graph; it is deﬁned simply in terms of the group structure and q. [sent-137, score-0.191]
</p><p>77 We now argue that if many pairs of nodes are far apart in H(q), for a suitably large value of q, then a decentralized algorithm cannot be eﬃcient when α > 1. [sent-138, score-0.699]
</p><p>78 Suppose there exist constants γ, δ > 0 so that a constant fraction of all pairs of nodes have shortest-path distance Ω(nδ ) in H(nγ ). [sent-141, score-0.267]
</p><p>79 Then for every α > 1, there is no group-induced model on (V, {R i}) with exponent α and a polylogarithmic number of out-links per node in which a decentralized algorithm can achieve polylogarithmic search time. [sent-142, score-1.822]
</p><p>80 Notice this property holds for group structures arising from both lattices and hierarchies; in a lattice, a constant fraction of all pairs in H(n1/2p) have distance Ω(n1/2p), while in a hierarchy, the graph H(nγ ) is disconnected for every γ < 1. [sent-143, score-0.425]
</p><p>81 4  Nodes with a Constant Number of Out-Links  Thus far, by giving each node more than a constant number of out-links, we have been able to design very simple search algorithms in networks generated according to the optimal exponent α. [sent-144, score-0.575]
</p><p>82 From each node, there is a way to make progress toward the target node t, and so the structure of the graph G funnels the search towards its destination. [sent-145, score-0.501]
</p><p>83 First of all, with high probability, many nodes will have all their links leading “away” from the target in the hierarchy. [sent-147, score-0.509]
</p><p>84 In this section, we work with a hierarchical model, and construct graphs with con-  stant out-degree k; the value of k will need to be suﬃciently large in terms of other parameters of the model. [sent-150, score-0.196]
</p><p>85 To deal with the problem that t itself may have no incoming links, we relax the search problem to that of ﬁnding a cluster of nodes containing t. [sent-152, score-0.436]
</p><p>86 We place r nodes at each leaf of T , forming a set V of n = mr nodes total. [sent-155, score-0.433]
</p><p>87 We then deﬁne a graph G on V as in Section 2: for a non-increasing function f(·), we create k links out of each node v ∈ V , choosing w as an endpoint with probability proportional to f(h(v, w)). [sent-156, score-0.533]
</p><p>88 As before, we refer to this process as a hierarchical model with exponent α, for the appropriate value of α. [sent-157, score-0.337]
</p><p>89 We refer to each set of r nodes at a common leaf of T as a cluster, and deﬁne the resolution of the hierarchical model to be the value r. [sent-158, score-0.465]
</p><p>90 A decentralized algorithm is given knowledge of T , and a target node t; it must reach any node in the cluster containing t. [sent-159, score-0.972]
</p><p>91 Unlike the previous algorithms we have developed, which only moved forward across links, the algorithm we design here will need to make use of the ability to travel in reverse across any link that it has already followed in the forward direction. [sent-160, score-0.248]
</p><p>92 Note also that we cannot easily reduce the current search problem to that of Section 2 by collapsing clusters into “super-nodes,” since there are not necessarily links joining nodes within the same cluster. [sent-161, score-0.647]
</p><p>93 The search task clearly becomes easier as the resolution of the model (i. [sent-162, score-0.2]
</p><p>94 Thus, our goal is to achieve polylogarithmic search time in a hierarchical model with polylogarithmic resolution. [sent-165, score-1.159]
</p><p>95 1 (a) There is a hierarchical model with exponent α = 1, constant out-degree, and polylogarithmic resolution in which a decentralized algorithm can achieve polylogarithmic search time. [sent-167, score-1.887]
</p><p>96 (b) For every α = 1, there is no hierarchical model with exponent α, constant outdegree, and polylogarithmic resolution in which a decentralized algorithm can achieve polylogarithmic search time. [sent-168, score-1.887]
</p><p>97 The search algorithm used to establish part (a) operates in phases. [sent-169, score-0.166]
</p><p>98 It begins each phase j with a collection of Θ(log n) nodes all belonging to the sub-tree Tj that contains the target t and whose root is at depth j. [sent-170, score-0.299]
</p><p>99 During phase j, it explores outward from each of these nodes until it has discovered a larger but still polylogarithmicsized set of nodes belonging to Tj . [sent-171, score-0.344]
</p><p>100 From among these, there is a high probability that at least Θ(log n) have links into the smaller sub-tree Tj+1 that contains t and whose root is at depth j + 1. [sent-172, score-0.251]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('decentralized', 0.496), ('polylogarithmic', 0.404), ('links', 0.251), ('lattice', 0.217), ('nodes', 0.172), ('search', 0.166), ('milgram', 0.165), ('exponent', 0.162), ('group', 0.147), ('node', 0.146), ('hierarchical', 0.141), ('web', 0.122), ('link', 0.122), ('watts', 0.11), ('newman', 0.092), ('strogatz', 0.092), ('target', 0.086), ('social', 0.085), ('freenet', 0.073), ('kempe', 0.073), ('page', 0.072), ('containing', 0.07), ('lattices', 0.064), ('protocols', 0.064), ('individuals', 0.061), ('structures', 0.06), ('graph', 0.059), ('people', 0.058), ('joining', 0.058), ('leaf', 0.056), ('hierarchy', 0.055), ('crawling', 0.055), ('gossip', 0.055), ('huberman', 0.055), ('lukose', 0.055), ('puniyani', 0.055), ('network', 0.055), ('lim', 0.054), ('letter', 0.054), ('paths', 0.052), ('endpoint', 0.048), ('ancestor', 0.048), ('theorem', 0.047), ('leaves', 0.046), ('world', 0.045), ('achieve', 0.044), ('structure', 0.044), ('collection', 0.041), ('scalable', 0.041), ('height', 0.038), ('topic', 0.038), ('networks', 0.037), ('acquaintances', 0.037), ('adamic', 0.037), ('broker', 0.037), ('dodds', 0.037), ('gnutella', 0.037), ('goel', 0.037), ('govindan', 0.037), ('killworth', 0.037), ('liation', 0.037), ('occupations', 0.037), ('outdegree', 0.037), ('sigcomm', 0.037), ('constant', 0.036), ('reverse', 0.036), ('groups', 0.036), ('belong', 0.035), ('acm', 0.035), ('refer', 0.034), ('resolution', 0.034), ('forming', 0.033), ('stock', 0.032), ('exponents', 0.032), ('demers', 0.032), ('linkage', 0.032), ('duncan', 0.032), ('achlioptas', 0.032), ('kleinberg', 0.032), ('forward', 0.031), ('tj', 0.031), ('pairs', 0.031), ('chains', 0.03), ('underlying', 0.029), ('create', 0.029), ('cornell', 0.029), ('ball', 0.029), ('bernard', 0.029), ('path', 0.029), ('common', 0.028), ('six', 0.028), ('di', 0.028), ('cluster', 0.028), ('short', 0.028), ('design', 0.028), ('construct', 0.028), ('distance', 0.028), ('proofs', 0.028), ('graphs', 0.027), ('monotone', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="169-tfidf-1" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>Author: Jon M. Kleinberg</p><p>Abstract: unkown-abstract</p><p>2 0.10842127 <a title="169-tfidf-2" href="./nips-2001-The_Intelligent_surfer%3A_Probabilistic_Combination_of_Link_and_Content_Information_in_PageRank.html">184 nips-2001-The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank</a></p>
<p>Author: Matthew Richardson, Pedro Domingos</p><p>Abstract: The PageRank algorithm, used in the Google search engine, greatly improves the results of Web search by taking into account the link structure of the Web. PageRank assigns to a page a score proportional to the number of times a random surfer would visit that page, if it surfed indefinitely from page to page, following all outlinks from a page with equal probability. We propose to improve PageRank by using a more intelligent surfer, one that is guided by a probabilistic model of the relevance of a page to a query. Efficient execution of our algorithm at query time is made possible by precomputing at crawl time (and thus once for all queries) the necessary terms. Experiments on two large subsets of the Web indicate that our algorithm significantly outperforms PageRank in the (human-rated) quality of the pages returned, while remaining efficient enough to be used in today’s large search engines. 1</p><p>3 0.09265995 <a title="169-tfidf-3" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>Author: M. Schmitt</p><p>Abstract: Recurrent neural networks of analog units are computers for realvalued functions. We study the time complexity of real computation in general recurrent neural networks. These have sigmoidal, linear, and product units of unlimited order as nodes and no restrictions on the weights. For networks operating in discrete time, we exhibit a family of functions with arbitrarily high complexity, and we derive almost tight bounds on the time required to compute these functions. Thus, evidence is given of the computational limitations that time-bounded analog recurrent neural networks are subject to. 1</p><p>4 0.084119968 <a title="169-tfidf-4" href="./nips-2001-Hyperbolic_Self-Organizing_Maps_for_Semantic_Navigation.html">90 nips-2001-Hyperbolic Self-Organizing Maps for Semantic Navigation</a></p>
<p>Author: Jorg Ontrup, Helge Ritter</p><p>Abstract: We introduce a new type of Self-Organizing Map (SOM) to navigate in the Semantic Space of large text collections. We propose a “hyperbolic SOM” (HSOM) based on a regular tesselation of the hyperbolic plane, which is a non-euclidean space characterized by constant negative gaussian curvature. The exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relations. We describe experiments, showing that the HSOM can successfully be applied to text categorization tasks and yields results comparable to other state-of-the-art methods.</p><p>5 0.078847617 <a title="169-tfidf-5" href="./nips-2001-Matching_Free_Trees_with_Replicator_Equations.html">118 nips-2001-Matching Free Trees with Replicator Equations</a></p>
<p>Author: Marcello Pelillo</p><p>Abstract: Motivated by our recent work on rooted tree matching, in this paper we provide a solution to the problem of matching two free (i.e., unrooted) trees by constructing an association graph whose maximal cliques are in one-to-one correspondence with maximal common subtrees. We then solve the problem using simple replicator dynamics from evolutionary game theory. Experiments on hundreds of uniformly random trees are presented. The results are impressive: despite the inherent inability of these simple dynamics to escape from local optima, they always returned a globally optimal solution.</p><p>6 0.077192448 <a title="169-tfidf-6" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<p>7 0.076205678 <a title="169-tfidf-7" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<p>8 0.061705422 <a title="169-tfidf-8" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>9 0.051418841 <a title="169-tfidf-9" href="./nips-2001-Learning_Hierarchical_Structures_with_Linear_Relational_Embedding.html">110 nips-2001-Learning Hierarchical Structures with Linear Relational Embedding</a></p>
<p>10 0.049843363 <a title="169-tfidf-10" href="./nips-2001-Incremental_A%2A.html">93 nips-2001-Incremental A*</a></p>
<p>11 0.047195278 <a title="169-tfidf-11" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>12 0.046900425 <a title="169-tfidf-12" href="./nips-2001-Active_Information_Retrieval.html">24 nips-2001-Active Information Retrieval</a></p>
<p>13 0.046313263 <a title="169-tfidf-13" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>14 0.046280932 <a title="169-tfidf-14" href="./nips-2001-The_Unified_Propagation_and_Scaling_Algorithm.html">188 nips-2001-The Unified Propagation and Scaling Algorithm</a></p>
<p>15 0.043733262 <a title="169-tfidf-15" href="./nips-2001-Unsupervised_Learning_of_Human_Motion_Models.html">193 nips-2001-Unsupervised Learning of Human Motion Models</a></p>
<p>16 0.043577738 <a title="169-tfidf-16" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>17 0.043071382 <a title="169-tfidf-17" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<p>18 0.041912008 <a title="169-tfidf-18" href="./nips-2001-Laplacian_Eigenmaps_and_Spectral_Techniques_for_Embedding_and_Clustering.html">106 nips-2001-Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</a></p>
<p>19 0.040419079 <a title="169-tfidf-19" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>20 0.039467335 <a title="169-tfidf-20" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.141), (1, -0.026), (2, 0.011), (3, -0.042), (4, -0.028), (5, -0.097), (6, -0.12), (7, -0.032), (8, -0.115), (9, -0.008), (10, -0.0), (11, -0.043), (12, -0.022), (13, 0.068), (14, -0.004), (15, 0.023), (16, -0.051), (17, 0.058), (18, 0.01), (19, -0.049), (20, -0.039), (21, 0.05), (22, 0.007), (23, 0.082), (24, -0.174), (25, 0.13), (26, -0.016), (27, 0.007), (28, 0.094), (29, 0.156), (30, 0.07), (31, 0.004), (32, 0.037), (33, 0.072), (34, -0.024), (35, -0.125), (36, -0.076), (37, -0.087), (38, 0.01), (39, -0.295), (40, -0.135), (41, 0.053), (42, -0.125), (43, -0.011), (44, 0.104), (45, 0.036), (46, -0.025), (47, 0.149), (48, 0.112), (49, -0.126)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96234673 <a title="169-lsi-1" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>Author: Jon M. Kleinberg</p><p>Abstract: unkown-abstract</p><p>2 0.64535916 <a title="169-lsi-2" href="./nips-2001-The_Intelligent_surfer%3A_Probabilistic_Combination_of_Link_and_Content_Information_in_PageRank.html">184 nips-2001-The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank</a></p>
<p>Author: Matthew Richardson, Pedro Domingos</p><p>Abstract: The PageRank algorithm, used in the Google search engine, greatly improves the results of Web search by taking into account the link structure of the Web. PageRank assigns to a page a score proportional to the number of times a random surfer would visit that page, if it surfed indefinitely from page to page, following all outlinks from a page with equal probability. We propose to improve PageRank by using a more intelligent surfer, one that is guided by a probabilistic model of the relevance of a page to a query. Efficient execution of our algorithm at query time is made possible by precomputing at crawl time (and thus once for all queries) the necessary terms. Experiments on two large subsets of the Web indicate that our algorithm significantly outperforms PageRank in the (human-rated) quality of the pages returned, while remaining efficient enough to be used in today’s large search engines. 1</p><p>3 0.61688572 <a title="169-lsi-3" href="./nips-2001-Hyperbolic_Self-Organizing_Maps_for_Semantic_Navigation.html">90 nips-2001-Hyperbolic Self-Organizing Maps for Semantic Navigation</a></p>
<p>Author: Jorg Ontrup, Helge Ritter</p><p>Abstract: We introduce a new type of Self-Organizing Map (SOM) to navigate in the Semantic Space of large text collections. We propose a “hyperbolic SOM” (HSOM) based on a regular tesselation of the hyperbolic plane, which is a non-euclidean space characterized by constant negative gaussian curvature. The exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relations. We describe experiments, showing that the HSOM can successfully be applied to text categorization tasks and yields results comparable to other state-of-the-art methods.</p><p>4 0.54321331 <a title="169-lsi-4" href="./nips-2001-Incremental_A%2A.html">93 nips-2001-Incremental A*</a></p>
<p>Author: S. Koenig, M. Likhachev</p><p>Abstract: Incremental search techniques ﬁnd optimal solutions to series of similar search tasks much faster than is possible by solving each search task from scratch. While researchers have developed incremental versions of uninformed search methods, we develop an incremental version of A*. The ﬁrst search of Lifelong Planning A* is the same as that of A* but all subsequent searches are much faster because it reuses those parts of the previous search tree that are identical to the new search tree. We then present experimental results that demonstrate the advantages of Lifelong Planning A* for simple route planning tasks. 1 Overview Artiﬁcial intelligence has investigated knowledge-based search techniques that allow one to solve search tasks in large domains. Most of the research on these methods has studied how to solve one-shot search problems. However, search is often a repetitive process, where one needs to solve a series of similar search tasks, for example, because the actual situation turns out to be slightly different from the one initially assumed or because the situation changes over time. An example for route planning tasks are changing trafﬁc conditions. Thus, one needs to replan for the new situation, for example if one always wants to display the least time-consuming route from the airport to the conference center on a web page. In these situations, most search methods replan from scratch, that is, solve the search problems independently. Incremental search techniques share with case-based planning, plan adaptation, repair-based planning, and learning search-control knowledge the property that they ﬁnd solutions to series of similar search tasks much faster than is possible by solving each search task from scratch. Incremental search techniques, however, differ from the other techniques in that the quality of their solutions is guaranteed to be as good as the quality of the solutions obtained by replanning from scratch. Although incremental search methods are not widely known in artiﬁcial intelligence and control, different researchers have developed incremental search versions of uninformed search methods in the algorithms literature. An overview can be found in [FMSN00]. We, on the other hand, develop an incremental version of A*, thus combining ideas from the algorithms literature and the artiﬁcial intelligence literature. We call the algorithm Lifelong Planning A* (LPA*), in analogy to “lifelong learning” [Thr98], because it reuses £ We thank Anthony Stentz for his support. The Intelligent Decision-Making Group is partly supported by NSF awards under contracts IIS9984827, IIS-0098807, and ITR/AP-0113881. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ofﬁcial policies, either expressed or implied, of the sponsoring organizations and agencies or the U.S. government. information from previous searches. LPA* uses heuristics to focus the search and always ﬁnds a shortest path for the current edge costs. The ﬁrst search of LPA* is the same as that of A* but all subsequent searches are much faster. LPA* produces at least the search tree that A* builds. However, it achieves a substantial speedup over A* because it reuses those parts of the previous search tree that are identical to the new search tree. 2 The Route Planning Task Lifelong Planning A* (LPA*) solves the following search task: It applies to ﬁnite graph search problems on known graphs whose edge costs can increase or decrease over time. denotes the ﬁnite set of vertices of the graph. denotes the set of successors of vertex . Similarly, denotes the set of predecessors of vertex . denotes the cost of moving from vertex to vertex . LPA* always determines a shortest path from a given start vertex to a given goal vertex , knowing both the topology of the graph and the current edge costs. We use to denote the start distance of vertex , that is, the length of a shortest path from to .      ¨     ¨¦ £ £ ¡ ©§¥¤¢     FP HFE TSRQIGD¨  ¨¦ £ £ ¡    4 ©D¥CBA@!¨ ¨     ¨¦</p><p>5 0.4872005 <a title="169-lsi-5" href="./nips-2001-Matching_Free_Trees_with_Replicator_Equations.html">118 nips-2001-Matching Free Trees with Replicator Equations</a></p>
<p>Author: Marcello Pelillo</p><p>Abstract: Motivated by our recent work on rooted tree matching, in this paper we provide a solution to the problem of matching two free (i.e., unrooted) trees by constructing an association graph whose maximal cliques are in one-to-one correspondence with maximal common subtrees. We then solve the problem using simple replicator dynamics from evolutionary game theory. Experiments on hundreds of uniformly random trees are presented. The results are impressive: despite the inherent inability of these simple dynamics to escape from local optima, they always returned a globally optimal solution.</p><p>6 0.44260302 <a title="169-lsi-6" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<p>7 0.43396759 <a title="169-lsi-7" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>8 0.37804309 <a title="169-lsi-8" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>9 0.35462984 <a title="169-lsi-9" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<p>10 0.32565722 <a title="169-lsi-10" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>11 0.30675384 <a title="169-lsi-11" href="./nips-2001-Learning_Hierarchical_Structures_with_Linear_Relational_Embedding.html">110 nips-2001-Learning Hierarchical Structures with Linear Relational Embedding</a></p>
<p>12 0.28935707 <a title="169-lsi-12" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>13 0.25447017 <a title="169-lsi-13" href="./nips-2001-Active_Information_Retrieval.html">24 nips-2001-Active Information Retrieval</a></p>
<p>14 0.25015607 <a title="169-lsi-14" href="./nips-2001-A_Quantitative_Model_of_Counterfactual_Reasoning.html">17 nips-2001-A Quantitative Model of Counterfactual Reasoning</a></p>
<p>15 0.22682087 <a title="169-lsi-15" href="./nips-2001-The_Noisy_Euclidean_Traveling_Salesman_Problem_and_Learning.html">186 nips-2001-The Noisy Euclidean Traveling Salesman Problem and Learning</a></p>
<p>16 0.22541094 <a title="169-lsi-16" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>17 0.22399543 <a title="169-lsi-17" href="./nips-2001-EM-DD%3A_An_Improved_Multiple-Instance_Learning_Technique.html">64 nips-2001-EM-DD: An Improved Multiple-Instance Learning Technique</a></p>
<p>18 0.22298323 <a title="169-lsi-18" href="./nips-2001-Tree-based_reparameterization_for_approximate_inference_on_loopy_graphs.html">192 nips-2001-Tree-based reparameterization for approximate inference on loopy graphs</a></p>
<p>19 0.21970533 <a title="169-lsi-19" href="./nips-2001-Improvisation_and_Learning.html">91 nips-2001-Improvisation and Learning</a></p>
<p>20 0.21893571 <a title="169-lsi-20" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.019), (17, 0.035), (19, 0.038), (27, 0.092), (30, 0.089), (38, 0.019), (54, 0.285), (59, 0.029), (72, 0.049), (79, 0.069), (83, 0.04), (91, 0.148)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90570384 <a title="169-lda-1" href="./nips-2001-Very_loopy_belief_propagation_for_unwrapping_phase_images.html">196 nips-2001-Very loopy belief propagation for unwrapping phase images</a></p>
<p>Author: Brendan J. Frey, Ralf Koetter, Nemanja Petrovic</p><p>Abstract: Since the discovery that the best error-correcting decoding algorithm can be viewed as belief propagation in a cycle-bound graph, researchers have been trying to determine under what circumstances</p><p>same-paper 2 0.80878627 <a title="169-lda-2" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>Author: Jon M. Kleinberg</p><p>Abstract: unkown-abstract</p><p>3 0.59246296 <a title="169-lda-3" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>Author: Andrew D. Brown, Geoffrey E. Hinton</p><p>Abstract: Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's. 1</p><p>4 0.59163773 <a title="169-lda-4" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>Author: Matthew J. Beal, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We show that it is possible to extend hidden Markov models to have a countably inﬁnite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the inﬁnitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters deﬁne a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a ﬁnite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be inﬁnite— consider, for example, symbols being possible words appearing in English text.</p><p>5 0.59110308 <a title="169-lda-5" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>Author: Bram Bakker</p><p>Abstract: This paper presents reinforcement learning with a Long ShortTerm Memory recurrent neural network: RL-LSTM. Model-free RL-LSTM using Advantage(,x) learning and directed exploration can solve non-Markovian tasks with long-term dependencies between relevant events. This is demonstrated in a T-maze task, as well as in a difficult variation of the pole balancing task. 1</p><p>6 0.58830917 <a title="169-lda-6" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>7 0.5878123 <a title="169-lda-7" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>8 0.5876109 <a title="169-lda-8" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>9 0.58524776 <a title="169-lda-9" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>10 0.58332163 <a title="169-lda-10" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>11 0.58272588 <a title="169-lda-11" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>12 0.58235085 <a title="169-lda-12" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>13 0.58210784 <a title="169-lda-13" href="./nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">102 nips-2001-KLD-Sampling: Adaptive Particle Filters</a></p>
<p>14 0.58149481 <a title="169-lda-14" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>15 0.58133495 <a title="169-lda-15" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>16 0.57940102 <a title="169-lda-16" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>17 0.5791378 <a title="169-lda-17" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>18 0.57802171 <a title="169-lda-18" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>19 0.5775044 <a title="169-lda-19" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>20 0.57749224 <a title="169-lda-20" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
