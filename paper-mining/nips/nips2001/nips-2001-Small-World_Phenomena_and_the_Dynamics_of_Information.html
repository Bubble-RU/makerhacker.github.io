<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>169 nips-2001-Small-World Phenomena and the Dynamics of Information</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-169" href="#">nips2001-169</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>169 nips-2001-Small-World Phenomena and the Dynamics of Information</h1>
<br/><p>Source: <a title="nips-2001-169-pdf" href="http://papers.nips.cc/paper/2061-small-world-phenomena-and-the-dynamics-of-information.pdf">pdf</a></p><p>Author: Jon M. Kleinberg</p><p>Abstract: unkown-abstract</p><p>Reference: <a title="nips-2001-169-reference" href="../nips2001_reference/nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('polylogarithm', 0.472), ('dec', 0.366), ('nod', 0.335), ('link', 0.317), ('milgram', 0.193), ('search', 0.182), ('group', 0.164), ('hierarch', 0.158), ('web', 0.136), ('newm', 0.107), ('strogatz', 0.107), ('wat', 0.095), ('target', 0.091), ('freenet', 0.086), ('kemp', 0.086), ('soc', 0.085), ('path', 0.076), ('hierarchy', 0.075), ('structures', 0.07), ('expon', 0.069)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="169-tfidf-1" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>Author: Jon M. Kleinberg</p><p>Abstract: unkown-abstract</p><p>2 0.15717413 <a title="169-tfidf-2" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>Author: M. Schmitt</p><p>Abstract: Recurrent neural networks of analog units are computers for realvalued functions. We study the time complexity of real computation in general recurrent neural networks. These have sigmoidal, linear, and product units of unlimited order as nodes and no restrictions on the weights. For networks operating in discrete time, we exhibit a family of functions with arbitrarily high complexity, and we derive almost tight bounds on the time required to compute these functions. Thus, evidence is given of the computational limitations that time-bounded analog recurrent neural networks are subject to. 1</p><p>3 0.1372344 <a title="169-tfidf-3" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>Author: Stella X. Yu, Jianbo Shi</p><p>Abstract: With the optimization of pattern discrimination as a goal, graph partitioning approaches often lack the capability to integrate prior knowledge to guide grouping. In this paper, we consider priors from unitary generative models, partially labeled data and spatial attention. These priors are modelled as constraints in the solution space. By imposing uniformity condition on the constraints, we restrict the feasible space to one of smooth solutions. A subspace projection method is developed to solve this constrained eigenproblema We demonstrate that simple priors can greatly improve image segmentation results. 1</p><p>4 0.1295567 <a title="169-tfidf-4" href="./nips-2001-Matching_Free_Trees_with_Replicator_Equations.html">118 nips-2001-Matching Free Trees with Replicator Equations</a></p>
<p>Author: Marcello Pelillo</p><p>Abstract: Motivated by our recent work on rooted tree matching, in this paper we provide a solution to the problem of matching two free (i.e., unrooted) trees by constructing an association graph whose maximal cliques are in one-to-one correspondence with maximal common subtrees. We then solve the problem using simple replicator dynamics from evolutionary game theory. Experiments on hundreds of uniformly random trees are presented. The results are impressive: despite the inherent inability of these simple dynamics to escape from local optima, they always returned a globally optimal solution.</p><p>5 0.11667386 <a title="169-tfidf-5" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<p>Author: Kevin P. Murphy, Mark A. Paskin</p><p>Abstract: The hierarchical hidden Markov model (HHMM) is a generalization of the hidden Markov model (HMM) that models sequences with structure at many length/time scales [FST98]. Unfortunately, the original infertime, where is ence algorithm is rather complicated, and takes the length of the sequence, making it impractical for many domains. In this paper, we show how HHMMs are a special kind of dynamic Bayesian network (DBN), and thereby derive a much simpler inference algorithm, which only takes time. Furthermore, by drawing the connection between HHMMs and DBNs, we enable the application of many standard approximation techniques to further speed up inference. ¥ ©§ £ ¨¦¥¤¢ © £ ¦¥¤¢</p><p>6 0.10983647 <a title="169-tfidf-6" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<p>7 0.10786738 <a title="169-tfidf-7" href="./nips-2001-The_Intelligent_surfer%3A_Probabilistic_Combination_of_Link_and_Content_Information_in_PageRank.html">184 nips-2001-The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank</a></p>
<p>8 0.096513957 <a title="169-tfidf-8" href="./nips-2001-The_Unified_Propagation_and_Scaling_Algorithm.html">188 nips-2001-The Unified Propagation and Scaling Algorithm</a></p>
<p>9 0.095160723 <a title="169-tfidf-9" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>10 0.090613805 <a title="169-tfidf-10" href="./nips-2001-Hyperbolic_Self-Organizing_Maps_for_Semantic_Navigation.html">90 nips-2001-Hyperbolic Self-Organizing Maps for Semantic Navigation</a></p>
<p>11 0.070959255 <a title="169-tfidf-11" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>12 0.068314567 <a title="169-tfidf-12" href="./nips-2001-Incremental_A%2A.html">93 nips-2001-Incremental A*</a></p>
<p>13 0.068169639 <a title="169-tfidf-13" href="./nips-2001-Active_Information_Retrieval.html">24 nips-2001-Active Information Retrieval</a></p>
<p>14 0.066242293 <a title="169-tfidf-14" href="./nips-2001-Laplacian_Eigenmaps_and_Spectral_Techniques_for_Embedding_and_Clustering.html">106 nips-2001-Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</a></p>
<p>15 0.061515231 <a title="169-tfidf-15" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>16 0.060537152 <a title="169-tfidf-16" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>17 0.058113117 <a title="169-tfidf-17" href="./nips-2001-Tree-based_reparameterization_for_approximate_inference_on_loopy_graphs.html">192 nips-2001-Tree-based reparameterization for approximate inference on loopy graphs</a></p>
<p>18 0.057504836 <a title="169-tfidf-18" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>19 0.057240862 <a title="169-tfidf-19" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>20 0.057127718 <a title="169-tfidf-20" href="./nips-2001-Unsupervised_Learning_of_Human_Motion_Models.html">193 nips-2001-Unsupervised Learning of Human Motion Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.176), (1, 0.013), (2, 0.039), (3, 0.001), (4, 0.15), (5, 0.076), (6, -0.094), (7, 0.052), (8, -0.037), (9, -0.001), (10, -0.016), (11, -0.072), (12, -0.203), (13, -0.063), (14, 0.046), (15, -0.044), (16, 0.035), (17, -0.002), (18, -0.071), (19, -0.12), (20, 0.053), (21, 0.143), (22, 0.104), (23, 0.228), (24, 0.04), (25, -0.045), (26, 0.008), (27, 0.006), (28, -0.051), (29, -0.166), (30, -0.065), (31, 0.02), (32, -0.042), (33, 0.136), (34, 0.071), (35, 0.217), (36, 0.08), (37, 0.028), (38, -0.062), (39, 0.029), (40, -0.105), (41, 0.145), (42, -0.032), (43, -0.031), (44, 0.03), (45, 0.043), (46, -0.061), (47, 0.017), (48, -0.095), (49, 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95976371 <a title="169-lsi-1" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>Author: Jon M. Kleinberg</p><p>Abstract: unkown-abstract</p><p>2 0.67068028 <a title="169-lsi-2" href="./nips-2001-Hyperbolic_Self-Organizing_Maps_for_Semantic_Navigation.html">90 nips-2001-Hyperbolic Self-Organizing Maps for Semantic Navigation</a></p>
<p>Author: Jorg Ontrup, Helge Ritter</p><p>Abstract: We introduce a new type of Self-Organizing Map (SOM) to navigate in the Semantic Space of large text collections. We propose a “hyperbolic SOM” (HSOM) based on a regular tesselation of the hyperbolic plane, which is a non-euclidean space characterized by constant negative gaussian curvature. The exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relations. We describe experiments, showing that the HSOM can successfully be applied to text categorization tasks and yields results comparable to other state-of-the-art methods.</p><p>3 0.61403209 <a title="169-lsi-3" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>Author: M. Schmitt</p><p>Abstract: Recurrent neural networks of analog units are computers for realvalued functions. We study the time complexity of real computation in general recurrent neural networks. These have sigmoidal, linear, and product units of unlimited order as nodes and no restrictions on the weights. For networks operating in discrete time, we exhibit a family of functions with arbitrarily high complexity, and we derive almost tight bounds on the time required to compute these functions. Thus, evidence is given of the computational limitations that time-bounded analog recurrent neural networks are subject to. 1</p><p>4 0.57616442 <a title="169-lsi-4" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<p>Author: Eran Segal, Daphne Koller, Dirk Ormoneit</p><p>Abstract: Many domains are naturally organized in an abstraction hierarchy or taxonomy, where the instances in “nearby” classes in the taxonomy are similar. In this paper, we provide a general probabilistic framework for clustering data into a set of classes organized as a taxonomy, where each class is associated with a probabilistic model from which the data was generated. The clustering algorithm simultaneously optimizes three things: the assignment of data instances to clusters, the models associated with the clusters, and the structure of the abstraction hierarchy. A unique feature of our approach is that it utilizes global optimization algorithms for both of the last two steps, reducing the sensitivity to noise and the propensity to local maxima that are characteristic of algorithms such as hierarchical agglomerative clustering that only take local steps. We provide a theoretical analysis for our algorithm, showing that it converges to a local maximum of the joint likelihood of model and data. We present experimental results on synthetic data, and on real data in the domains of gene expression and text.</p><p>5 0.53957105 <a title="169-lsi-5" href="./nips-2001-Matching_Free_Trees_with_Replicator_Equations.html">118 nips-2001-Matching Free Trees with Replicator Equations</a></p>
<p>Author: Marcello Pelillo</p><p>Abstract: Motivated by our recent work on rooted tree matching, in this paper we provide a solution to the problem of matching two free (i.e., unrooted) trees by constructing an association graph whose maximal cliques are in one-to-one correspondence with maximal common subtrees. We then solve the problem using simple replicator dynamics from evolutionary game theory. Experiments on hundreds of uniformly random trees are presented. The results are impressive: despite the inherent inability of these simple dynamics to escape from local optima, they always returned a globally optimal solution.</p><p>6 0.49939126 <a title="169-lsi-6" href="./nips-2001-Incremental_A%2A.html">93 nips-2001-Incremental A*</a></p>
<p>7 0.49683473 <a title="169-lsi-7" href="./nips-2001-The_Intelligent_surfer%3A_Probabilistic_Combination_of_Link_and_Content_Information_in_PageRank.html">184 nips-2001-The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank</a></p>
<p>8 0.48000768 <a title="169-lsi-8" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>9 0.45339489 <a title="169-lsi-9" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<p>10 0.38192812 <a title="169-lsi-10" href="./nips-2001-Learning_Hierarchical_Structures_with_Linear_Relational_Embedding.html">110 nips-2001-Learning Hierarchical Structures with Linear Relational Embedding</a></p>
<p>11 0.38045064 <a title="169-lsi-11" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>12 0.36540833 <a title="169-lsi-12" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<p>13 0.36452705 <a title="169-lsi-13" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>14 0.35527894 <a title="169-lsi-14" href="./nips-2001-Tree-based_reparameterization_for_approximate_inference_on_loopy_graphs.html">192 nips-2001-Tree-based reparameterization for approximate inference on loopy graphs</a></p>
<p>15 0.35032588 <a title="169-lsi-15" href="./nips-2001-The_Unified_Propagation_and_Scaling_Algorithm.html">188 nips-2001-The Unified Propagation and Scaling Algorithm</a></p>
<p>16 0.31155899 <a title="169-lsi-16" href="./nips-2001-Laplacian_Eigenmaps_and_Spectral_Techniques_for_Embedding_and_Clustering.html">106 nips-2001-Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</a></p>
<p>17 0.30403146 <a title="169-lsi-17" href="./nips-2001-A_Quantitative_Model_of_Counterfactual_Reasoning.html">17 nips-2001-A Quantitative Model of Counterfactual Reasoning</a></p>
<p>18 0.28903133 <a title="169-lsi-18" href="./nips-2001-Improvisation_and_Learning.html">91 nips-2001-Improvisation and Learning</a></p>
<p>19 0.28353456 <a title="169-lsi-19" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<p>20 0.27184027 <a title="169-lsi-20" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.285), (14, 0.026), (16, 0.145), (31, 0.091), (36, 0.056), (50, 0.075), (51, 0.019), (63, 0.024), (77, 0.035), (79, 0.016), (91, 0.041), (92, 0.088)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71576893 <a title="169-lda-1" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>Author: Jon M. Kleinberg</p><p>Abstract: unkown-abstract</p><p>2 0.59550828 <a title="169-lda-2" href="./nips-2001-Grammar_Transfer_in_a_Second_Order_Recurrent_Neural_Network.html">85 nips-2001-Grammar Transfer in a Second Order Recurrent Neural Network</a></p>
<p>Author: Michiro Negishi, Stephen J. Hanson</p><p>Abstract: It has been known that people, after being exposed to sentences generated by an artificial grammar, acquire implicit grammatical knowledge and are able to transfer the knowledge to inputs that are generated by a modified grammar. We show that a second order recurrent neural network is able to transfer grammatical knowledge from one language (generated by a Finite State Machine) to another language which differ both in vocabularies and syntax. Representation of the grammatical knowledge in the network is analyzed using linear discriminant analysis. 1</p><p>3 0.5922938 <a title="169-lda-3" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>Author: M. Schmitt</p><p>Abstract: Recurrent neural networks of analog units are computers for realvalued functions. We study the time complexity of real computation in general recurrent neural networks. These have sigmoidal, linear, and product units of unlimited order as nodes and no restrictions on the weights. For networks operating in discrete time, we exhibit a family of functions with arbitrarily high complexity, and we derive almost tight bounds on the time required to compute these functions. Thus, evidence is given of the computational limitations that time-bounded analog recurrent neural networks are subject to. 1</p><p>4 0.57245994 <a title="169-lda-4" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>Author: Aaron C. Courville, David S. Touretzky</p><p>Abstract: The Temporal Coding Hypothesis of Miller and colleagues [7] suggests that animals integrate related temporal patterns of stimuli into single memory representations. We formalize this concept using quasi-Bayes estimation to update the parameters of a constrained hidden Markov model. This approach allows us to account for some surprising temporal effects in the second order conditioning experiments of Miller et al. [1 , 2, 3], which other models are unable to explain. 1</p><p>5 0.57120019 <a title="169-lda-5" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>Author: Andrew D. Brown, Geoffrey E. Hinton</p><p>Abstract: Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's. 1</p><p>6 0.57083005 <a title="169-lda-6" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>7 0.56981462 <a title="169-lda-7" href="./nips-2001-Means%2C_Correlations_and_Bounds.html">119 nips-2001-Means, Correlations and Bounds</a></p>
<p>8 0.56744504 <a title="169-lda-8" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>9 0.56647503 <a title="169-lda-9" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>10 0.5650329 <a title="169-lda-10" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>11 0.56502616 <a title="169-lda-11" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>12 0.56452692 <a title="169-lda-12" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>13 0.56339598 <a title="169-lda-13" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>14 0.5607841 <a title="169-lda-14" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<p>15 0.56010103 <a title="169-lda-15" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>16 0.55817968 <a title="169-lda-16" href="./nips-2001-Audio-Visual_Sound_Separation_Via_Hidden_Markov_Models.html">39 nips-2001-Audio-Visual Sound Separation Via Hidden Markov Models</a></p>
<p>17 0.55787057 <a title="169-lda-17" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>18 0.55725485 <a title="169-lda-18" href="./nips-2001-Very_loopy_belief_propagation_for_unwrapping_phase_images.html">196 nips-2001-Very loopy belief propagation for unwrapping phase images</a></p>
<p>19 0.55682987 <a title="169-lda-19" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>20 0.55413926 <a title="169-lda-20" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
