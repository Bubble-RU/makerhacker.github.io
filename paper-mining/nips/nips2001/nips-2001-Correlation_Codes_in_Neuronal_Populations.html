<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>57 nips-2001-Correlation Codes in Neuronal Populations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-57" href="#">nips2001-57</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>57 nips-2001-Correlation Codes in Neuronal Populations</h1>
<br/><p>Source: <a title="nips-2001-57-pdf" href="http://papers.nips.cc/paper/2031-correlation-codes-in-neuronal-populations.pdf">pdf</a></p><p>Author: Maoz Shamir, Haim Sompolinsky</p><p>Abstract: Population codes often rely on the tuning of the mean responses to the stimulus parameters. However, this information can be greatly suppressed by long range correlations. Here we study the efﬁciency of coding information in the second order statistics of the population responses. We show that the Fisher Information of this system grows linearly with the size of the system. We propose a bilinear readout model for extracting information from correlation codes, and evaluate its performance in discrimination and estimation tasks. It is shown that the main source of information in this system is the stimulus dependence of the variances of the single neuron responses.</p><p>Reference: <a title="nips-2001-57-reference" href="../nips2001_reference/nips-2001-Correlation_Codes_in_Neuronal_Populations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1    ¡ £  © § ¥ £ ¡  Abstract Population codes often rely on the tuning of the mean responses to the stimulus parameters. [sent-4, score-0.399]
</p><p>2 However, this information can be greatly suppressed by long range correlations. [sent-5, score-0.039]
</p><p>3 Here we study the efﬁciency of coding information in the second order statistics of the population responses. [sent-6, score-0.3]
</p><p>4 We show that the Fisher Information of this system grows linearly with the size of the system. [sent-7, score-0.113]
</p><p>5 We propose a bilinear readout model for extracting information from correlation codes, and evaluate its performance in discrimination and estimation tasks. [sent-8, score-1.217]
</p><p>6 It is shown that the main source of information in this system is the stimulus dependence of the variances of the single neuron responses. [sent-9, score-0.336]
</p><p>7 1 Introduction Experiments in the last years have shown that in many cortical areas, the ﬂuctuations in the responses of neurons to external stimuli are signiﬁcantly correlated [1, 2, 3, 4], raising important questions regarding the computational implications of neuronal correlations. [sent-10, score-0.374]
</p><p>8 Recent theoretical studies have addressed the issue of how neuronal correlations affect the efﬁciency of population coding [4, 5, 6]. [sent-11, score-0.543]
</p><p>9 It is often assumed that the information about stimuli is coded mainly in the mean neuronal responses, e. [sent-12, score-0.217]
</p><p>10 , in the tuning of the mean ﬁring rates, and that by averaging the tuned responses across large populations, an accurate estimate can be obtained despite the signiﬁcant noise in the single neuron responses. [sent-14, score-0.296]
</p><p>11 Indeed, for uncorrelated neurons the Fisher Information of the population is extensive [7]; namely, it increases linearly with the number of neurons in the population. [sent-15, score-0.56]
</p><p>12 Furthermore, it has been shown that this extensive information can be extracted by relatively simple linear readout mechanisms [7, 8]. [sent-16, score-0.503]
</p><p>13 However, it was recently shown [6] that positive correlations which vary smoothly with space may drastically suppress the information in the mean responses. [sent-17, score-0.254]
</p><p>14 In particular, the Fisher Information of the system saturates to a ﬁnite value as the system size grows. [sent-18, score-0.15]
</p><p>15 This raises questions about the computational utility of neuronal population codes. [sent-19, score-0.357]
</p><p>16 Neuronal population responses can represent information in the higher order statistics of the responses [3], not only in their means. [sent-20, score-0.476]
</p><p>17 In this work, we study the accuracy of coding information in the second order statistics. [sent-21, score-0.076]
</p><p>18 Speciﬁcally, we assume that the neuronal responses obey multivariate Gaussian statistics governed by a stimulus-dependent correlation matrix. [sent-23, score-0.47]
</p><p>19 We ask whether the Fisher Information of such a system is extensive even in the presence of strong correlations in the neuronal  noise. [sent-24, score-0.387]
</p><p>20 2 Fisher Information of a Correlation Code Our model consists of a system of neurons that code a 2D angle , . [sent-26, score-0.199]
</p><p>21 753 ) ' & " 1 B Q@ %$"  " B C@ 1 A9 @  ' (¡ "    is the mean activity of the -th neuron and its dependence on is usually referred Here to as the tuning curve of the neuron; is the correlation matrix; and is a normalization constant. [sent-28, score-0.342]
</p><p>22 Here we shall limit ourselves to the case of multiplicative modulation of the correlations. [sent-29, score-0.133]
</p><p>23 It is important to note that the neuron, variance adds a contribution to which is larger than the contribution of the smooth part of the correlations. [sent-33, score-0.085]
</p><p>24 For reasons that will become clear below, we write,   d '  ' y ) ' aT F(¡ " S e X¡ " UTS qX¡ " UT S  (5)   S  aTy  S  denotes the smooth part of the correlation matrix and where diagonal part, which in the example of Eqs. [sent-34, score-0.235]
</p><p>25 (6)  ' W (¡ " w D' ( 1 " q(¡ " S ) '  @  A useful measure of the accuracy of a population code is the Fisher Information (FI). [sent-36, score-0.227]
</p><p>26 In the case of uncorrelated populations it is well known that FI increases linearly with system size [7], indicating that the accuracy of the population coding improves as the system size is increased. [sent-37, score-0.707]
</p><p>27 Furthermore, it has been shown that relatively simple, linear schemes can provide reasonable readout models for extracting the information in uncorrelated populations [8]. [sent-38, score-0.739]
</p><p>28 The form of these terms reveals that in general the correlations play two roles. [sent-40, score-0.147]
</p><p>29 First they control the efﬁciency of the information encoded in the mean activities (note the dependence of on ). [sent-41, score-0.174]
</p><p>30 Secondly, provides an additional source of information about the stimulus ( ). [sent-42, score-0.157]
</p><p>31 When the correlations are independent of the stimulus, , it was shown [6] that positive correlations, , with long correlation length, ,  ' h i 1 " i) (¡ W e  d¥ ( ) ' "  ¦ ' ! [sent-43, score-0.3]
</p><p>32 ' X¡ " ¡ B  ¢ g( f  ' (¡ " G   G  S     HH8   PF   φ=0o  C(φ,ψ) φ=−60o  o  φ=60  φ=−120o  −120  −60  ψ  0  60  120  180  [deg]  ¡ b ) @  −180  φ=120o  ¡ b ) b @  Figure 1: The stimulus-dependent correlation matrix, Eqs. [sent-45, score-0.153]
</p><p>33 (  ¢ i)     ' ¡© ub" S    § ¤© ) v £  cause the saturation of FI to a ﬁnite limit at large . [sent-49, score-0.063]
</p><p>34 This implies that in the presence of such correlations, population averaging cannot overcome the noise even in large networks. [sent-50, score-0.231]
</p><p>35 Evaluating these terms for the multiplicative is positive, so that . [sent-54, score-0.064]
</p><p>36 (2), we ﬁnd that tion of this term shows that saturates at large to a small ﬁnite value, so that for large (11)     as shown in Fig. [sent-56, score-0.107]
</p><p>37 We thus conclude that increases linearly with and is equal, for large , to the FI of variance coding namely to of an independent population in which information is encoded in their activity variances. [sent-58, score-0.504]
</p><p>38 Since in our system the information is encoded in the second order statistics of the population responses, it is obvious that linear readouts are inadequate. [sent-59, score-0.444]
</p><p>39 This raises the question of whether there are relatively simple nonlinear readout models for such systems. [sent-60, score-0.497]
</p><p>40 In the next sections we will study bilinear readouts and show that they are useful models for extracting information from correlation codes. [sent-61, score-0.805]
</p><p>41 3 A Bilinear Readout for Discrimination Tasks In a two-interval discrimination task the system is given two sets of neuronal activities generated by two proximal stimuli and and must infer which stimulus generated which activity. [sent-62, score-0.476]
</p><p>42 The Maximum-Likelihood (ML) discrimination yields the  ¡ d e ¡  ¡    w $ ©  $     −3  1  1  x 10  −2  [deg ]  −2  [deg ]  0. [sent-63, score-0.081]
</p><p>43 (2)(4), as a function of the number of neurons in the system. [sent-69, score-0.052]
</p><p>44 ¥)  (  probability of error given by discriminability equals  and the (12)        It has been previously shown that in the case of uncorrelated populations with mean coding, the optimal linear readouts achieves the Maximum-Likelihood discrimination performance in large N [7]. [sent-76, score-0.654]
</p><p>45 B  In order to isolate the properties of correlation coding we will assume that no information hereafter. [sent-77, score-0.229]
</p><p>46 We suggest a is coded in the average ﬁring rates of the neurons, and take bilinear readout as a simple generalization of the linear readout to correlation codes. [sent-78, score-1.502]
</p><p>47 In a discrimination task the bilinear readout makes a decision according to the sign of  )       w T   w    @    T      #UT "  $ %  (13)  ¥    aT )  ! [sent-79, score-0.981]
</p><p>48 Maximizing the signal-to-noise ratio of this rule, the optimal bilinear discriminator (OBD) matrix is given by (14)  Using the optimal weights to evaluate the discrimination error we obtain that in large the performance of the OBD saturates the ML performance, Eq. [sent-82, score-0.806]
</p><p>49 Thus, since FI of this model increases linearly with the size of the system, the discriminability increases as . [sent-84, score-0.217]
</p><p>50 £     Since the correlation matrix depends on the stimulus, , the OBD matrix, Eq. [sent-85, score-0.204]
</p><p>51 1 Optimal bilinear readout for estimation To study the global performance of bilinear readouts we investigate bilinear readouts which minimize the square error of estimating the angle averaged over the whole range of . [sent-89, score-2.325]
</p><p>52 For convenience we use complex notation for the encoded angle, and write as the estimator  ¡  &  §      ¡  ¥  UT )  &  . [sent-90, score-0.096]
</p><p>53 Let  T  Pu aT  §  §  )  of  (15)  UT  where are stimulus independent complex weights. [sent-91, score-0.157]
</p><p>54 We deﬁne the optimal bilinear estimator (OBE) as the set of weights that minimizes on average the quadratic estimation error of an unbiased estimator. [sent-92, score-0.709]
</p><p>55 This error is given by ¢  ) 1 '(¡ $   & § " ' (¡ " ¡ © §  §  (16)  ' (¡ "  §  &     ¨ ©     © § w&d; @ &§& ¡  1  ¥  ¦ §      § q' ) " ¢ ¤  £  ' (¡ "  where is the Lagrange multiplier of the constraint . [sent-93, score-0.036]
</p><p>56 In general, it is impossible to ﬁnd a perfectly unbiased estimator for a continuously varied stimulus, using a ﬁnite number of weights. [sent-94, score-0.071]
</p><p>57 However, in the case of angle estimation, we can employ the underlying rotational symmetry to generate such an estimator. [sent-95, score-0.121]
</p><p>58 For this we use the symmetry of the correlation matrix, Eq. [sent-96, score-0.195]
</p><p>59 Using these symmetry properties, can be written in the following form (for even )  ' ub"      (18)  % &         # ! [sent-101, score-0.042]
</p><p>60 3 (a)) Figure 3 (a) presents an example of the function also suggest that the function is mainly determined by a few harmonics plus a delta . [sent-104, score-0.073]
</p><p>61 Below we will use this fact to study simpler forms of bilinear readout. [sent-105, score-0.48]
</p><p>62   Figure 3 (b) shows the numerical calculation of the OBE error (open circles) as a function of . [sent-109, score-0.078]
</p><p>63 The dashed line is the asymptotic behavior, given by Eq. [sent-110, score-0.141]
</p><p>64 From the graph one can see that the estimation efﬁciency of this readout grows linearly with the size of the system, , but is lower than the bound. [sent-113, score-0.539]
</p><p>65 2 Truncated bilinear readout Motivated by the simple structure of the optimal readout matrix observed in Fig. [sent-115, score-1.411]
</p><p>66 3 (a), we studied a bilinear readout of the form of Eqs. [sent-116, score-0.9]
</p><p>67 Restricting the number of harmonics to relatively small integers, we evaluated numerically the optimal values of the coefﬁcients for large systems. [sent-118, score-0.126]
</p><p>68 Surprisingly we found that for small and large , these coefﬁcients approach a value which is independent of the speciﬁcs of the model and equals , yielding a bilinear weight matrix of the form  ' ub"     ! [sent-119, score-0.589]
</p><p>69 (20)    ts p § g 75 hf3 ' T b  ub" ' §  "  (fr    §   aT d ¥ Tb eb @ 1 @ @ 1 ¡  )   0  UT    f£ §  @  )      (         Figure 4 shows the numerical results for the squared average error of this readout for several values of and . [sent-120, score-0.498]
</p><p>70 (b) Numerical evaluation of one over the squared estimation error, for the optimal bilinear readout in the multiplicative modulation model (open circles). [sent-126, score-1.087]
</p><p>71 The dashed line is the asymptotic behavior, given by Eq. [sent-127, score-0.141]
</p><p>72 Here , for the optimal bilinear readout in the multiplicative modulation model. [sent-129, score-1.045]
</p><p>73 ¢ )    (       © ) v    ¤ £  inverse square error initially increases linearly with but saturates in the limit of large . [sent-133, score-0.356]
</p><p>74 The precise form of depends on the speciﬁcs of the correlation model. [sent-135, score-0.153]
</p><p>75 Figure 4 shows that for this range of , and the deviations of the inverse square error from linearity are small. [sent-138, score-0.124]
</p><p>76 Thus, in the regime , is given by the asymptotic behavior, Eq. [sent-139, score-0.065]
</p><p>77 (  E ' w X& ¡ d " ' "  y   E y   ' "Ey   (  ¡ ( ¢      (  ' "Ey     (  2  %  %     %  %  1)  (     We thus conclude that the OBE (with unlimited ) will generate an inverse square estimawith a coefﬁcient given by Eq. [sent-141, score-0.088]
</p><p>78 (19), and that tion error which increases linearly with this value can be achieved for reasonable values of by an approximate bilinear weight matrix, of the form of Eq. [sent-142, score-0.671]
</p><p>79 (19), is smaller than the optimal value given by the full FI, Eq. [sent-145, score-0.04]
</p><p>80 In fact, it is equal to the error of an independent population with a variance which equals and a quadratic population vector readout of the form (        (  ' (¡ "   S  £ ¤! [sent-148, score-0.998]
</p><p>81 (21)      w   ¥    &  §  It is important to note that in the presence of correlations, the quadratic readout of Eq. [sent-149, score-0.496]
</p><p>82 (21) is very inefﬁcient, yielding a ﬁnite error for large as shown in Fig. [sent-150, score-0.036]
</p><p>83 5 Discussion To understand the reason for the simple form of the approximately optimal bilinear weight matrix, Eq. [sent-152, score-0.52]
</p><p>84 1 quadratic 0 0  500  1000  1500  2000  N Figure 4: Inverse square estimation error of the ﬁnite- approximation for the OBE, Eq. [sent-163, score-0.164]
</p><p>85 The dashed line is the asymptotic behavior, given by Eq. [sent-167, score-0.141]
</p><p>86 The FI bound is shown by the dotted , and were used. [sent-169, score-0.047]
</p><p>87 (21) it can be seen that our readout is in the form of a bilinear population vector in which the lowest Fourier modes of the response vector have been removed. [sent-176, score-1.149]
</p><p>88 Retaining only the high Fourier modes in the response proﬁle suppresses the cross-correlations between the different components of the residual responses because the underlying correlations have smooth spatial dependence, whose power is concentrated mostly in the low Fourier modes. [sent-177, score-0.39]
</p><p>89 On the other hand, the information contained in the variance is not removed because the variance contains a discontinuous spatial component, . [sent-178, score-0.193]
</p><p>90 In other words, the variance of a correlation proﬁle which has only high Fourier modes can still preserve its slowly varying components. [sent-179, score-0.261]
</p><p>91 Thus, by projecting out the low Fourier modes of the spatial responses the spatial correlations are suppressed but the information in the response variance is retained. [sent-180, score-0.484]
</p><p>92  &  $  ' (¡ "   S  This interpretation of the bilinear readout implies that although all the elements of the correlation matrix depend on the stimulus, only the stimulus dependence of the diagonal elements is important. [sent-181, score-1.312]
</p><p>93 (11) and (19) show, the asymptotic performance of both the full FI as well as that of the OBE are equivalent to those of an . [sent-184, score-0.065]
</p><p>94 uncorrelated population with a stimulus dependent variance which equals  ' (¡ "   S  Although we have presented results here concerning a multiplicative model of correlations, we have studied other models of stimulus dependent correlations. [sent-185, score-0.804]
</p><p>95 These studies indicate that the above conclusions apply to a broad class of populations in which information is encoded in the second order statistics of the responses. [sent-186, score-0.228]
</p><p>96 Also, for the sake of clarity we have assumed here that the mean responses are untuned, . [sent-187, score-0.154]
</p><p>97 Our studies have shown that adding tuned mean inputs does not modify the picture since the smoothly varying positive correlations greatly suppress the information embedded in the ﬁrst order statistics. [sent-188, score-0.286]
</p><p>98   )  B  The relatively simple form of the readout Eq. [sent-189, score-0.46]
</p><p>99 (22) suggests that neuronal hardware may be able to extract efﬁciently information embedded in local populations of cells whose noisy responses are strongly correlated, provided that the variances of their responses are signiﬁcantly tuned to the stimulus. [sent-190, score-0.589]
</p><p>100 This latter condition is not too restrictive, since tuning of variances of neuronal ﬁring rates to stimulus and motor variables is quite common in the nervous system. [sent-191, score-0.368]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bilinear', 0.48), ('readout', 0.42), ('obe', 0.21), ('population', 0.195), ('ut', 0.167), ('stimulus', 0.157), ('correlation', 0.153), ('correlations', 0.147), ('fi', 0.146), ('populations', 0.146), ('ub', 0.136), ('cb', 0.134), ('readouts', 0.131), ('responses', 0.126), ('neuronal', 0.125), ('hh', 0.122), ('obd', 0.105), ('uncorrelated', 0.092), ('deg', 0.083), ('discrimination', 0.081), ('fisher', 0.081), ('angle', 0.079), ('saturates', 0.078), ('sompolinsky', 0.078), ('linearly', 0.077), ('fourier', 0.077), ('coding', 0.076), ('asymptotic', 0.065), ('multiplicative', 0.064), ('neuron', 0.058), ('equals', 0.058), ('ef', 0.058), ('variance', 0.054), ('modes', 0.054), ('encoded', 0.053), ('discontinuous', 0.053), ('shamir', 0.053), ('yoon', 0.053), ('neurons', 0.052), ('tuning', 0.052), ('matrix', 0.051), ('dependence', 0.051), ('ciency', 0.049), ('increases', 0.049), ('pf', 0.048), ('dotted', 0.047), ('square', 0.046), ('harmonics', 0.046), ('extensive', 0.043), ('estimator', 0.043), ('estimation', 0.042), ('numerical', 0.042), ('inverse', 0.042), ('activities', 0.042), ('symmetry', 0.042), ('ring', 0.042), ('le', 0.042), ('suppress', 0.042), ('discriminability', 0.042), ('abbott', 0.042), ('modulation', 0.041), ('extracting', 0.041), ('optimal', 0.04), ('quadratic', 0.04), ('line', 0.04), ('relatively', 0.04), ('qx', 0.039), ('suppressed', 0.039), ('fr', 0.039), ('pro', 0.038), ('multivariate', 0.037), ('nite', 0.037), ('tb', 0.037), ('smoothly', 0.037), ('jerusalem', 0.037), ('raises', 0.037), ('dashed', 0.036), ('system', 0.036), ('correlated', 0.036), ('codes', 0.036), ('error', 0.036), ('presence', 0.036), ('stimuli', 0.035), ('saturation', 0.035), ('variances', 0.034), ('tuned', 0.032), ('code', 0.032), ('spatial', 0.032), ('smooth', 0.031), ('statistics', 0.029), ('tion', 0.029), ('coded', 0.029), ('coef', 0.029), ('limit', 0.028), ('secondly', 0.028), ('unbiased', 0.028), ('mean', 0.028), ('delta', 0.027), ('concerning', 0.027), ('cs', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="57-tfidf-1" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>Author: Maoz Shamir, Haim Sompolinsky</p><p>Abstract: Population codes often rely on the tuning of the mean responses to the stimulus parameters. However, this information can be greatly suppressed by long range correlations. Here we study the efﬁciency of coding information in the second order statistics of the population responses. We show that the Fisher Information of this system grows linearly with the size of the system. We propose a bilinear readout model for extracting information from correlation codes, and evaluate its performance in discrimination and estimation tasks. It is shown that the main source of information in this system is the stimulus dependence of the variances of the single neuron responses.</p><p>2 0.16280952 <a title="57-tfidf-2" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>Author: Julian Eggert, Berthold BĂ¤uml</p><p>Abstract: Mesoscopical, mathematical descriptions of dynamics of populations of spiking neurons are getting increasingly important for the understanding of large-scale processes in the brain using simulations. In our previous work, integral equation formulations for population dynamics have been derived for a special type of spiking neurons. For Integrate- and- Fire type neurons , these formulations were only approximately correct. Here, we derive a mathematically compact, exact population dynamics formulation for Integrate- and- Fire type neurons. It can be shown quantitatively in simulations that the numerical correspondence with microscopically modeled neuronal populations is excellent. 1 Introduction and motivation The goal of the population dynamics approach is to model the time course of the collective activity of entire populations of functionally and dynamically similar neurons in a compact way, using a higher descriptionallevel than that of single neurons and spikes. The usual observable at the level of neuronal populations is the populationaveraged instantaneous firing rate A(t), with A(t)6.t being the number of neurons in the population that release a spike in an interval [t, t+6.t). Population dynamics are formulated in such a way, that they match quantitatively the time course of a given A(t), either gained experimentally or by microscopical, detailed simulation. At least three main reasons can be formulated which underline the importance of the population dynamics approach for computational neuroscience. First, it enables the simulation of extensive networks involving a massive number of neurons and connections, which is typically the case when dealing with biologically realistic functional models that go beyond the single neuron level. Second, it increases the analytical understanding of large-scale neuronal dynamics , opening the way towards better control and predictive capabilities when dealing with large networks. Third, it enables a systematic embedding of the numerous neuronal models operating at different descriptional scales into a generalized theoretic framework, explaining the relationships, dependencies and derivations of the respective models. Early efforts on population dynamics approaches date back as early as 1972, to the work of Wilson and Cowan [8] and Knight [4], which laid the basis for all current population-averaged graded-response models (see e.g. [6] for modeling work using these models). More recently, population-based approaches for spiking neurons were developed, mainly by Gerstner [3, 2] and Knight [5]. In our own previous work [1], we have developed a theoretical framework which enables to systematize and simulate a wide range of models for population-based dynamics. It was shown that the equations of the framework produce results that agree quantitatively well with detailed simulations using spiking neurons, so that they can be used for realistic simulations involving networks with large numbers of spiking neurons. Nevertheless, for neuronal populations composed of Integrate-and-Fire (I&F;) neurons, this framework was only correct in an approximation. In this paper, we derive the exact population dynamics formulation for I&F; neurons. This is achieved by reducing the I&F; population dynamics to a point process and by taking advantage of the particular properties of I&F; neurons. 2 2.1 Background: Integrate-and-Fire dynamics Differential form We start with the standard Integrate- and- Fire (I&F;) model in form of the wellknown differential equation [7] (1) which describes the dynamics of the membrane potential Vi of a neuron i that is modeled as a single compartment with RC circuit characteristics. The membrane relaxation time is in this case T = RC with R being the membrane resistance and C the membrane capacitance. The resting potential v R est is the stationary potential that is approached in the no-input case. The input arriving from other neurons is described in form of a current ji. In addition to eq. (1), which describes the integrate part of the I&F; model, the neuronal dynamics are completed by a nonlinear step. Every time the membrane potential Vi reaches a fixed threshold () from below, Vi is lowered by a fixed amount Ll > 0, and from the new value of the membrane potential integration according to eq. (1) starts again. if Vi(t) = () (from below) . (2) At the same time, it is said that the release of a spike occurred (i.e., the neuron fired), and the time ti = t of this singular event is stored. Here ti indicates the time of the most recent spike. Storing all the last firing times , we gain the sequence of spikes {t{} (spike ordering index j, neuronal index i). 2.2 Integral form Now we look at the single neuron in a neuronal compound. We assume that the input current contribution ji from presynaptic spiking neurons can be described using the presynaptic spike times tf, a response-function ~ and a connection weight WÂˇ . ',J ji(t) = Wi ,j ~(t - tf) (3) l: l: j f Integrating the I&F; equation (1) beginning at the last spiking time tT, which determines the initial condition by Vi(ti) = vi(ti - 0) - 6., where vi(ti - 0) is the membrane potential just before the neuron spikes, we get 1 Vi(t) = v Rest + fj(t - t:) + l: Wi ,j l: a(t - t:; t - tf) , j - Vi(t:)) e- S / T (4) f with the refractory function fj(s) = - (v Rest (5) and the alpha-function r ds</p><p>3 0.11955666 <a title="57-tfidf-3" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>Author: Si Wu, Shun-ichi Amari</p><p>Abstract: This study investigates a population decoding paradigm, in which the estimation of stimulus in the previous step is used as prior knowledge for consecutive decoding. We analyze the decoding accuracy of such a Bayesian decoder (Maximum a Posteriori Estimate), and show that it can be implemented by a biologically plausible recurrent network, where the prior knowledge of stimulus is conveyed by the change in recurrent interactions as a result of Hebbian learning. 1</p><p>4 0.10621262 <a title="57-tfidf-4" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>Author: Gal Chechik, Amir Globerson, M. J. Anderson, E. D. Young, Israel Nelken, Naftali Tishby</p><p>Abstract: The way groups of auditory neurons interact to code acoustic information is investigated using an information theoretic approach. We develop measures of redundancy among groups of neurons, and apply them to the study of collaborative coding efficiency in two processing stations in the auditory pathway: the inferior colliculus (IC) and the primary auditory cortex (AI). Under two schemes for the coding of the acoustic content, acoustic segments coding and stimulus identity coding, we show differences both in information content and group redundancies between IC and AI neurons. These results provide for the first time a direct evidence for redundancy reduction along the ascending auditory pathway, as has been hypothesized for theoretical considerations [Barlow 1959,2001]. The redundancy effects under the single-spikes coding scheme are significant only for groups larger than ten cells, and cannot be revealed with the redundancy measures that use only pairs of cells. The results suggest that the auditory system transforms low level representations that contain redundancies due to the statistical structure of natural stimuli, into a representation in which cortical neurons extract rare and independent component of complex acoustic signals, that are useful for auditory scene analysis. 1</p><p>5 0.10051191 <a title="57-tfidf-5" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>Author: Shih-Chii Liu, Jörg Kramer, Giacomo Indiveri, Tobi Delbrück, Rodney J. Douglas</p><p>Abstract: We describe a programmable multi-chip VLSI neuronal system that can be used for exploring spike-based information processing models. The system consists of a silicon retina, a PIC microcontroller, and a transceiver chip whose integrate-and-ﬁre neurons are connected in a soft winner-take-all architecture. The circuit on this multi-neuron chip approximates a cortical microcircuit. The neurons can be conﬁgured for different computational properties by the virtual connections of a selected set of pixels on the silicon retina. The virtual wiring between the different chips is effected by an event-driven communication protocol that uses asynchronous digital pulses, similar to spikes in a neuronal system. We used the multi-chip spike-based system to synthesize orientation-tuned neurons using both a feedforward model and a feedback model. The performance of our analog hardware spiking model matched the experimental observations and digital simulations of continuous-valued neurons. The multi-chip VLSI system has advantages over computer neuronal models in that it is real-time, and the computational time does not scale with the size of the neuronal network.</p><p>6 0.099572964 <a title="57-tfidf-6" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>7 0.089319512 <a title="57-tfidf-7" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>8 0.088958107 <a title="57-tfidf-8" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>9 0.07909368 <a title="57-tfidf-9" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>10 0.074799113 <a title="57-tfidf-10" href="./nips-2001-Modeling_the_Modulatory_Effect_of_Attention_on_Human_Spatial_Vision.html">124 nips-2001-Modeling the Modulatory Effect of Attention on Human Spatial Vision</a></p>
<p>11 0.073711492 <a title="57-tfidf-11" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>12 0.07198634 <a title="57-tfidf-12" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>13 0.071465552 <a title="57-tfidf-13" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>14 0.068244644 <a title="57-tfidf-14" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>15 0.067396499 <a title="57-tfidf-15" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>16 0.061843332 <a title="57-tfidf-16" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>17 0.058580954 <a title="57-tfidf-17" href="./nips-2001-Information-Geometric_Decomposition_in_Spike_Analysis.html">96 nips-2001-Information-Geometric Decomposition in Spike Analysis</a></p>
<p>18 0.05680887 <a title="57-tfidf-18" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>19 0.056240257 <a title="57-tfidf-19" href="./nips-2001-Characterizing_Neural_Gain_Control_using_Spike-triggered_Covariance.html">48 nips-2001-Characterizing Neural Gain Control using Spike-triggered Covariance</a></p>
<p>20 0.05288817 <a title="57-tfidf-20" href="./nips-2001-A_Maximum-Likelihood_Approach_to_Modeling_Multisensory_Enhancement.html">11 nips-2001-A Maximum-Likelihood Approach to Modeling Multisensory Enhancement</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.16), (1, -0.185), (2, -0.099), (3, 0.016), (4, 0.083), (5, -0.017), (6, 0.091), (7, 0.002), (8, 0.02), (9, 0.004), (10, -0.023), (11, 0.045), (12, -0.057), (13, -0.036), (14, 0.005), (15, -0.11), (16, 0.05), (17, 0.065), (18, -0.001), (19, -0.041), (20, -0.048), (21, 0.01), (22, 0.051), (23, -0.028), (24, 0.009), (25, 0.024), (26, 0.069), (27, 0.009), (28, 0.034), (29, 0.031), (30, -0.084), (31, -0.079), (32, 0.025), (33, -0.017), (34, 0.049), (35, -0.025), (36, 0.141), (37, -0.012), (38, 0.021), (39, -0.05), (40, -0.085), (41, -0.012), (42, 0.025), (43, -0.026), (44, -0.094), (45, -0.041), (46, 0.158), (47, -0.043), (48, 0.074), (49, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95722002 <a title="57-lsi-1" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>Author: Maoz Shamir, Haim Sompolinsky</p><p>Abstract: Population codes often rely on the tuning of the mean responses to the stimulus parameters. However, this information can be greatly suppressed by long range correlations. Here we study the efﬁciency of coding information in the second order statistics of the population responses. We show that the Fisher Information of this system grows linearly with the size of the system. We propose a bilinear readout model for extracting information from correlation codes, and evaluate its performance in discrimination and estimation tasks. It is shown that the main source of information in this system is the stimulus dependence of the variances of the single neuron responses.</p><p>2 0.68788987 <a title="57-lsi-2" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>Author: Si Wu, Shun-ichi Amari</p><p>Abstract: This study investigates a population decoding paradigm, in which the estimation of stimulus in the previous step is used as prior knowledge for consecutive decoding. We analyze the decoding accuracy of such a Bayesian decoder (Maximum a Posteriori Estimate), and show that it can be implemented by a biologically plausible recurrent network, where the prior knowledge of stimulus is conveyed by the change in recurrent interactions as a result of Hebbian learning. 1</p><p>3 0.64826208 <a title="57-lsi-3" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>Author: Julian Eggert, Berthold BĂ¤uml</p><p>Abstract: Mesoscopical, mathematical descriptions of dynamics of populations of spiking neurons are getting increasingly important for the understanding of large-scale processes in the brain using simulations. In our previous work, integral equation formulations for population dynamics have been derived for a special type of spiking neurons. For Integrate- and- Fire type neurons , these formulations were only approximately correct. Here, we derive a mathematically compact, exact population dynamics formulation for Integrate- and- Fire type neurons. It can be shown quantitatively in simulations that the numerical correspondence with microscopically modeled neuronal populations is excellent. 1 Introduction and motivation The goal of the population dynamics approach is to model the time course of the collective activity of entire populations of functionally and dynamically similar neurons in a compact way, using a higher descriptionallevel than that of single neurons and spikes. The usual observable at the level of neuronal populations is the populationaveraged instantaneous firing rate A(t), with A(t)6.t being the number of neurons in the population that release a spike in an interval [t, t+6.t). Population dynamics are formulated in such a way, that they match quantitatively the time course of a given A(t), either gained experimentally or by microscopical, detailed simulation. At least three main reasons can be formulated which underline the importance of the population dynamics approach for computational neuroscience. First, it enables the simulation of extensive networks involving a massive number of neurons and connections, which is typically the case when dealing with biologically realistic functional models that go beyond the single neuron level. Second, it increases the analytical understanding of large-scale neuronal dynamics , opening the way towards better control and predictive capabilities when dealing with large networks. Third, it enables a systematic embedding of the numerous neuronal models operating at different descriptional scales into a generalized theoretic framework, explaining the relationships, dependencies and derivations of the respective models. Early efforts on population dynamics approaches date back as early as 1972, to the work of Wilson and Cowan [8] and Knight [4], which laid the basis for all current population-averaged graded-response models (see e.g. [6] for modeling work using these models). More recently, population-based approaches for spiking neurons were developed, mainly by Gerstner [3, 2] and Knight [5]. In our own previous work [1], we have developed a theoretical framework which enables to systematize and simulate a wide range of models for population-based dynamics. It was shown that the equations of the framework produce results that agree quantitatively well with detailed simulations using spiking neurons, so that they can be used for realistic simulations involving networks with large numbers of spiking neurons. Nevertheless, for neuronal populations composed of Integrate-and-Fire (I&F;) neurons, this framework was only correct in an approximation. In this paper, we derive the exact population dynamics formulation for I&F; neurons. This is achieved by reducing the I&F; population dynamics to a point process and by taking advantage of the particular properties of I&F; neurons. 2 2.1 Background: Integrate-and-Fire dynamics Differential form We start with the standard Integrate- and- Fire (I&F;) model in form of the wellknown differential equation [7] (1) which describes the dynamics of the membrane potential Vi of a neuron i that is modeled as a single compartment with RC circuit characteristics. The membrane relaxation time is in this case T = RC with R being the membrane resistance and C the membrane capacitance. The resting potential v R est is the stationary potential that is approached in the no-input case. The input arriving from other neurons is described in form of a current ji. In addition to eq. (1), which describes the integrate part of the I&F; model, the neuronal dynamics are completed by a nonlinear step. Every time the membrane potential Vi reaches a fixed threshold () from below, Vi is lowered by a fixed amount Ll > 0, and from the new value of the membrane potential integration according to eq. (1) starts again. if Vi(t) = () (from below) . (2) At the same time, it is said that the release of a spike occurred (i.e., the neuron fired), and the time ti = t of this singular event is stored. Here ti indicates the time of the most recent spike. Storing all the last firing times , we gain the sequence of spikes {t{} (spike ordering index j, neuronal index i). 2.2 Integral form Now we look at the single neuron in a neuronal compound. We assume that the input current contribution ji from presynaptic spiking neurons can be described using the presynaptic spike times tf, a response-function ~ and a connection weight WÂˇ . ',J ji(t) = Wi ,j ~(t - tf) (3) l: l: j f Integrating the I&F; equation (1) beginning at the last spiking time tT, which determines the initial condition by Vi(ti) = vi(ti - 0) - 6., where vi(ti - 0) is the membrane potential just before the neuron spikes, we get 1 Vi(t) = v Rest + fj(t - t:) + l: Wi ,j l: a(t - t:; t - tf) , j - Vi(t:)) e- S / T (4) f with the refractory function fj(s) = - (v Rest (5) and the alpha-function r ds</p><p>4 0.62641293 <a title="57-lsi-4" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>Author: Xiaohui Xie, Martin A. Giese</p><p>Abstract: Asymmetric lateral connections are one possible mechanism that can account for the direction selectivity of cortical neurons. We present a mathematical analysis for a class of these models. Contrasting with earlier theoretical work that has relied on methods from linear systems theory, we study the network’s nonlinear dynamic properties that arise when the threshold nonlinearity of the neurons is taken into account. We show that such networks have stimulus-locked traveling pulse solutions that are appropriate for modeling the responses of direction selective cortical neurons. In addition, our analysis shows that outside a certain regime of stimulus speeds the stability of this solutions breaks down giving rise to another class of solutions that are characterized by speciﬁc spatiotemporal periodicity. This predicts that if direction selectivity in the cortex is mainly achieved by asymmetric lateral connections lurching activity waves might be observable in ensembles of direction selective cortical neurons within appropriate regimes of the stimulus speed.</p><p>5 0.59971946 <a title="57-lsi-5" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>Author: Shih-Chii Liu, Jörg Kramer, Giacomo Indiveri, Tobi Delbrück, Rodney J. Douglas</p><p>Abstract: We describe a programmable multi-chip VLSI neuronal system that can be used for exploring spike-based information processing models. The system consists of a silicon retina, a PIC microcontroller, and a transceiver chip whose integrate-and-ﬁre neurons are connected in a soft winner-take-all architecture. The circuit on this multi-neuron chip approximates a cortical microcircuit. The neurons can be conﬁgured for different computational properties by the virtual connections of a selected set of pixels on the silicon retina. The virtual wiring between the different chips is effected by an event-driven communication protocol that uses asynchronous digital pulses, similar to spikes in a neuronal system. We used the multi-chip spike-based system to synthesize orientation-tuned neurons using both a feedforward model and a feedback model. The performance of our analog hardware spiking model matched the experimental observations and digital simulations of continuous-valued neurons. The multi-chip VLSI system has advantages over computer neuronal models in that it is real-time, and the computational time does not scale with the size of the neuronal network.</p><p>6 0.59520394 <a title="57-lsi-6" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>7 0.50691897 <a title="57-lsi-7" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>8 0.4921383 <a title="57-lsi-8" href="./nips-2001-Modeling_the_Modulatory_Effect_of_Attention_on_Human_Spatial_Vision.html">124 nips-2001-Modeling the Modulatory Effect of Attention on Human Spatial Vision</a></p>
<p>9 0.48068509 <a title="57-lsi-9" href="./nips-2001-A_Maximum-Likelihood_Approach_to_Modeling_Multisensory_Enhancement.html">11 nips-2001-A Maximum-Likelihood Approach to Modeling Multisensory Enhancement</a></p>
<p>10 0.4619728 <a title="57-lsi-10" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>11 0.45863083 <a title="57-lsi-11" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>12 0.45848089 <a title="57-lsi-12" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>13 0.42228231 <a title="57-lsi-13" href="./nips-2001-A_Rational_Analysis_of_Cognitive_Control_in_a_Speeded_Discrimination_Task.html">18 nips-2001-A Rational Analysis of Cognitive Control in a Speeded Discrimination Task</a></p>
<p>14 0.36736971 <a title="57-lsi-14" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<p>15 0.36102122 <a title="57-lsi-15" href="./nips-2001-Characterizing_Neural_Gain_Control_using_Spike-triggered_Covariance.html">48 nips-2001-Characterizing Neural Gain Control using Spike-triggered Covariance</a></p>
<p>16 0.35906199 <a title="57-lsi-16" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>17 0.35571435 <a title="57-lsi-17" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>18 0.34641528 <a title="57-lsi-18" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>19 0.34627411 <a title="57-lsi-19" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>20 0.34534174 <a title="57-lsi-20" href="./nips-2001-Scaling_Laws_and_Local_Minima_in_Hebbian_ICA.html">165 nips-2001-Scaling Laws and Local Minima in Hebbian ICA</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.043), (17, 0.015), (19, 0.029), (27, 0.139), (30, 0.066), (38, 0.09), (59, 0.037), (72, 0.062), (74, 0.013), (79, 0.029), (83, 0.023), (91, 0.154), (97, 0.208)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87987781 <a title="57-lda-1" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>Author: Maoz Shamir, Haim Sompolinsky</p><p>Abstract: Population codes often rely on the tuning of the mean responses to the stimulus parameters. However, this information can be greatly suppressed by long range correlations. Here we study the efﬁciency of coding information in the second order statistics of the population responses. We show that the Fisher Information of this system grows linearly with the size of the system. We propose a bilinear readout model for extracting information from correlation codes, and evaluate its performance in discrimination and estimation tasks. It is shown that the main source of information in this system is the stimulus dependence of the variances of the single neuron responses.</p><p>2 0.82060957 <a title="57-lda-2" href="./nips-2001-Multi_Dimensional_ICA_to_Separate_Correlated_Sources.html">127 nips-2001-Multi Dimensional ICA to Separate Correlated Sources</a></p>
<p>Author: Roland Vollgraf, Klaus Obermayer</p><p>Abstract: We present a new method for the blind separation of sources, which do not fulfill the independence assumption. In contrast to standard methods we consider groups of neighboring samples (</p><p>3 0.74092376 <a title="57-lda-3" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>Author: Gregor Wenning, Klaus Obermayer</p><p>Abstract: Cortical neurons might be considered as threshold elements integrating in parallel many excitatory and inhibitory inputs. Due to the apparent variability of cortical spike trains this yields a strongly fluctuating membrane potential, such that threshold crossings are highly irregular. Here we study how a neuron could maximize its sensitivity w.r.t. a relatively small subset of excitatory input. Weak signals embedded in fluctuations is the natural realm of stochastic resonance. The neuron's response is described in a hazard-function approximation applied to an Ornstein-Uhlenbeck process. We analytically derive an optimality criterium and give a learning rule for the adjustment of the membrane fluctuations, such that the sensitivity is maximal exploiting stochastic resonance. We show that adaptation depends only on quantities that could easily be estimated locally (in space and time) by the neuron. The main results are compared with simulations of a biophysically more realistic neuron model. 1</p><p>4 0.72770518 <a title="57-lda-4" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>Author: Mário Figueiredo</p><p>Abstract: In this paper we introduce a new sparseness inducing prior which does not involve any (hyper)parameters that need to be adjusted or estimated. Although other applications are possible, we focus here on supervised learning problems: regression and classiﬁcation. Experiments with several publicly available benchmark data sets show that the proposed approach yields state-of-the-art performance. In particular, our method outperforms support vector machines and performs competitively with the best alternative techniques, both in terms of error rates and sparseness, although it involves no tuning or adjusting of sparsenesscontrolling hyper-parameters.</p><p>5 0.72621208 <a title="57-lda-5" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>Author: Gregory Z. Grudic, Lyle H. Ungar</p><p>Abstract: We address two open theoretical questions in Policy Gradient Reinforcement Learning. The ﬁrst concerns the efﬁcacy of using function approximation to represent the state action value function, . Theory is presented showing that linear function approximation representations of can degrade the rate of convergence of performance gradient estimates by a factor of relative to when no function approximation of is used, where is the number of possible actions and is the number of basis functions in the function approximation representation. The second concerns the use of a bias term in estimating the state action value function. Theory is presented showing that a non-zero bias term can improve the rate of convergence of performance gradient estimates by , where is the number of possible actions. Experimental evidence is presented showing that these theoretical results lead to signiﬁcant improvement in the convergence properties of Policy Gradient Reinforcement Learning algorithms.       ¤ ¨ ¦ ¢ ©§¥¤£¡ ¦ ¤ ¨ £¡ ¨ ¤¢  ¢</p><p>6 0.72525001 <a title="57-lda-6" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>7 0.72501385 <a title="57-lda-7" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>8 0.71817368 <a title="57-lda-8" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>9 0.71799982 <a title="57-lda-9" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>10 0.71743071 <a title="57-lda-10" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>11 0.71634197 <a title="57-lda-11" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>12 0.71620309 <a title="57-lda-12" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>13 0.71610266 <a title="57-lda-13" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>14 0.71545553 <a title="57-lda-14" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>15 0.71491611 <a title="57-lda-15" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>16 0.71416962 <a title="57-lda-16" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>17 0.71386999 <a title="57-lda-17" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>18 0.71296102 <a title="57-lda-18" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>19 0.71162093 <a title="57-lda-19" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>20 0.70986235 <a title="57-lda-20" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
