<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>191 nips-2001-Transform-invariant Image Decomposition with Similarity Templates</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-191" href="#">nips2001-191</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>191 nips-2001-Transform-invariant Image Decomposition with Similarity Templates</h1>
<br/><p>Source: <a title="nips-2001-191-pdf" href="http://papers.nips.cc/paper/2098-transform-invariant-image-decomposition-with-similarity-templates.pdf">pdf</a></p><p>Author: Chris Stauffer, Erik Miller, Kinh Tieu</p><p>Abstract: Recent work has shown impressive transform-invariant modeling and clustering for sets of images of objects with similar appearance. We seek to expand these capabilities to sets of images of an object class that show considerable variation across individual instances (e.g. pedestrian images) using a representation based on pixel-wise similarities, similarity templates. Because of its invariance to the colors of particular components of an object, this representation enables detection of instances of an object class and enables alignment of those instances. Further, this model implicitly represents the regions of color regularity in the class-speciﬁc image set enabling a decomposition of that object class into component regions. 1</p><p>Reference: <a title="nips-2001-191-reference" href="../nips2001_reference/nips-2001-Transform-invariant_Image_Decomposition_with_Similarity_Templates_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Transform-invariant image decomposition with similarity templates  Chris Stauﬀer, Erik Miller, and Kinh Tieu MIT Artiﬁcial Intelligence Lab Massachusetts Institute of Technology Cambridge, MA 02139 {stauﬀer,emiller,tieu}@ai. [sent-1, score-0.633]
</p><p>2 edu  Abstract Recent work has shown impressive transform-invariant modeling and clustering for sets of images of objects with similar appearance. [sent-3, score-0.322]
</p><p>3 We seek to expand these capabilities to sets of images of an object class that show considerable variation across individual instances (e. [sent-4, score-0.358]
</p><p>4 pedestrian images) using a representation based on pixel-wise similarities, similarity templates. [sent-6, score-0.562]
</p><p>5 Because of its invariance to the colors of particular components of an object, this representation enables detection of instances of an object class and enables alignment of those instances. [sent-7, score-0.507]
</p><p>6 Further, this model implicitly represents the regions of color regularity in the class-speciﬁc image set enabling a decomposition of that object class into component regions. [sent-8, score-0.833]
</p><p>7 In particular, we are interested in modeling classes of objects that are characterized by similarities and diﬀerences between image pixels rather than by the values of those pixels. [sent-10, score-0.486]
</p><p>8 For instance, images of pedestrians (at a certain scale and pose) can be characterized by a few regions of regularity (RORs) such as shirt, pants, background, and head, that have ﬁxed properties such as constant color or constant texture within the region, but tend to be diﬀerent from each other. [sent-11, score-1.113]
</p><p>9 The particular color (or texture) of those regions is largely irrelevant. [sent-12, score-0.528]
</p><p>10 We shall refer to sets of images that ﬁt this general description as images characterized by regions of regularity, or ICRORs. [sent-13, score-0.557]
</p><p>11 Jojic and Frey [1] and others [2] have investigated transform-invariant modeling and clustering for images of a particular object (e. [sent-14, score-0.348]
</p><p>12 Their work with transformed component analysis (TCA) shows promise for handling considerable variation within the images resulting from lighting or slight misalignments. [sent-19, score-0.307]
</p><p>13 However, because these models rely on an image set with a ﬁxed mean or mixture of means, they are not directly applicable to ICRORs. [sent-20, score-0.164]
</p><p>14 We would also like to address transform-invariant modeling, but use a model which is invariant to the particular color of component regions. [sent-21, score-0.416]
</p><p>15 One simple way to achieve this is to use edge templates to model local diﬀerences in image color. [sent-22, score-0.298]
</p><p>16 In contrast, we have chosen to model global similarities in color using a similarity template (ST). [sent-23, score-0.918]
</p><p>17 While representations of pixel similarity have previously been exploited for segmentation of single images [3, 4], we have chosen to use them for aggregate modeling of image sets. [sent-24, score-1.121]
</p><p>18 Similarity templates enable alignment of image sets and decomposition of images into class-speciﬁc pixel regions. [sent-25, score-0.897]
</p><p>19 We note also that registration of two ICRORs can be accomplished by minimizing the mutual information between corresponding pixels [5]. [sent-26, score-0.18]
</p><p>20 But, there is no obvious way of extending this method to large sets of images without a combinatorial explosion. [sent-27, score-0.205]
</p><p>21 Section 4 covers their application to decomposing a class-speciﬁc set of images into component regions. [sent-31, score-0.244]
</p><p>22 2  Similarity templates  This section begins with a brief explanation of the similarity template followed by the mechanics of computing and comparing similarity templates. [sent-33, score-0.905]
</p><p>23 A similarity template S for an N -pixel image is an N xN matrix. [sent-34, score-0.619]
</p><p>24 The element Si,j represents the probability that pixel locations pi and pj would result from choosing a region and drawing (iid) two samples (pixel locations) from it. [sent-35, score-0.81]
</p><p>25 More formally, Si,j =  p(r)p(pi |r)p(pj |r),  (1)  r  where p(r) is the probability of choosing region r and p(pi |r) is the probability of choosing pixel location pi from region r. [sent-36, score-0.89]
</p><p>26 1  The “ideal” similarity template  Consider sampling pixel pairs as described above from an N -pixel image of a particular object (e. [sent-38, score-0.931]
</p><p>27 , a pedestrian) segmented by an oracle into disjoint regions (e. [sent-40, score-0.143]
</p><p>28 Assuming each region is equally likely to be sampled and that the pixels in the region are selected with uniform probability, then 1 1 ( R )( Sr )2 if ri = rj Si,j = (2) 0 otherwise, where R is the number of regions, Sr is the number of pixels in region r, and ri is the region label of pi . [sent-43, score-1.425]
</p><p>29 If two pixels are from the same region, the corresponding 1 value is the product of the probability R of choosing a particular region and the 1 2 probability ( Sr ) of drawing that pixel pair. [sent-44, score-0.709]
</p><p>30 This can be interpreted as a block diagonal co-occurrence matrix of sampled pixel pairs. [sent-45, score-0.237]
</p><p>31 In this ideal case, two images of diﬀerent pedestrians with the same body size and shape would result in the same similarity template regardless of the colors of their clothes, since the ST is a function only of the segmentation. [sent-46, score-1.122]
</p><p>32 An ST of an image without a pedestrian would exhibit diﬀerent statistics. [sent-47, score-0.402]
</p><p>33 Note that even the ST of an image of a blank wall (segmented as a single region) would be diﬀerent because pixels that are in diﬀerent regions under the ideal pedestrian ST would be in the same region. [sent-48, score-0.746]
</p><p>34 Unfortunately, images do not typically come with labeled regions, and so computation of a similarity template is impossible. [sent-49, score-0.681]
</p><p>35 Using this observation, we can approximate true similarity templates from unsegmented images. [sent-51, score-0.429]
</p><p>36 2  Computing similarity templates  For the purposes of this paper, our model for similarity is based solely on color. [sent-53, score-0.703]
</p><p>37 If each latent region had a constant but unique color and the regions were of equal 2 size, then as σi approaches zero this process reconstructs the “ideal” similarity template deﬁned in Equation 1. [sent-57, score-1.281]
</p><p>38 Although region colors are neither constant nor unique, this approximation has proven to work well in practice. [sent-58, score-0.412]
</p><p>39 It is possible to add a spatial prior based on the relative pixel location to model the fact that similarities tend to local, but we will rely on the statistics of the images in our data set to determine whether (and to what extent) this is the case. [sent-59, score-0.529]
</p><p>40 Also, it may be possible to achieve better results using a more complex color model (e. [sent-60, score-0.395]
</p><p>41 , hsv with full covariance) or broadening the measure of similarity to include other modalities (e. [sent-62, score-0.308]
</p><p>42 Figure 1 shows two views of the same similarity template. [sent-66, score-0.274]
</p><p>43 The ﬁrst view represents each pixel’s similarity to every other pixel. [sent-67, score-0.319]
</p><p>44 The second view contains a sub-image for each pixel which highlights the pixels that are most likely produced by the same region. [sent-68, score-0.475]
</p><p>45 Pixels in the shirt tend to highlight the entire shirt and the pants (to a lesser amount). [sent-69, score-0.572]
</p><p>46 Pixels in the background tend to be very dissimilar to all pixels in the foreground. [sent-70, score-0.275]
</p><p>47 3  Aggregate similarity templates (AST)  We assume each estimated ST is a noisy measurement of the true underlying joint distribution. [sent-72, score-0.455]
</p><p>48 Hence we compute an aggregate similarity template (AST) as the ¯ mean S of the ST estimates over an entire class-speciﬁc set of K images: 1 ¯ Si,j = K  K  ˜k Si,j . [sent-73, score-0.625]
</p><p>49 Note that this is a less restrictive assumption than assuming edges of regions are in correspondence across an image set, since regions have greater support. [sent-75, score-0.394]
</p><p>50 (a)  (b)  Figure 1: (a) The N xN aggregate similarity template for pedestrian data set. [sent-77, score-0.884]
</p><p>51 Each sub-image highlights the pixels that are most similar to the pixel it represents. [sent-81, score-0.454]
</p><p>52 4  Comparing similarity templates  ˜ To compare an estimated similarity template S to an aggregate similarity template ¯ S we evaluate their dot product1 : ¯ ˜ s(S, S) =  ¯ ˜ Si,j Si,j . [sent-83, score-1.53]
</p><p>53 By thresholding the ratio of the dot product of a particular image patch under and AST trained on pedestrian image patches versus an AST trained on random image patches, we can determine whether a person is present in the image. [sent-85, score-0.737]
</p><p>54 3  Data set alignment  In this paper, we investigate a more diﬃcult problem: alignment of a set of images. [sent-87, score-0.192]
</p><p>55 To explore this problem, we created a set of 128x64 images of simulated pedestrians. [sent-88, score-0.205]
</p><p>56 These pedestrians were generated by creating four independently-colored regions corresponding to shirts, pants, head, and background. [sent-89, score-0.316]
</p><p>57 Then, independent Gaussian noise was added to each pixel (σ = . [sent-92, score-0.237]
</p><p>58 Finally the images were translated uniformly up to 25% of the size of the object. [sent-94, score-0.205]
</p><p>59 Figure 2: A set of randomly generated “pedestrian” images used in alignment experimetns. [sent-97, score-0.301]
</p><p>60 [2], we iteratively estimated the latent variables (translations) that maximized the probability of the image STs to the AST and re-estimated the AST. [sent-99, score-0.214]
</p><p>61 4  Decomposing the similarity template  This section explains how to derive a factorized representation from the AST that will be useful for recognition of particular instances of a class and for further reﬁnement of detection. [sent-102, score-0.554]
</p><p>62 This representation is also useful in approximating the template to avoid the O(N 2 ) storage requirements. [sent-103, score-0.231]
</p><p>63 An AST represents the similarity of pixels within an image across an entire classspeciﬁc data set. [sent-104, score-0.67]
</p><p>64 Also, rather than treating pixel brightness (darkness, redness, blueness, or hue) as a value to be reconstructed in the decomposition, we chose to represent pixel similarity. [sent-112, score-0.494]
</p><p>65 In contrast to simply treating images as additive mixtures of basis functions [9], our decomposition will get the same results on a database of images of digits written in black on white paper or in white on a black board and color images introduce no diﬃculties for our methods. [sent-113, score-1.225]
</p><p>66 Given the number of regions R, it is possible to estimate the priors for each region p(r) and the probability of each region producing each pixel p(pi |r). [sent-116, score-0.803]
</p><p>67 Because our model S is symmetric, this case can be updated with only two rules: ˆ S(pi , pj ) pnew (pi |r) ∝ p(pi |r) p(r)p(pj |r) ¯ , and (7) S(pi , pj ) p j  pnew (r) ∝ p(r) pi  pj  ˆ S(pi , pj ) p(pj |r)p(pi |r) ¯ . [sent-118, score-0.715]
</p><p>68 S(pi , pj )  (8)  50  100  150  200  250  300  350  400  450  500 50  100  150  200  250  300  350  400  450  500  Figure 3: The similarity template and the corresponding automatically generated binary decomposition of the images in the pedestrian data set. [sent-119, score-1.122]
</p><p>69 The root node represents every pixel in the image. [sent-120, score-0.261]
</p><p>70 The more underlying regions we allow our model, the closer our estimate will approximate the true joint distribution. [sent-124, score-0.138]
</p><p>71 These region models tend to represent parts of the object class. [sent-125, score-0.321]
</p><p>72 p(pi |r) will tend to have high probabilities for a set of pixels belonging to the same region. [sent-126, score-0.22]
</p><p>73 We take advantage of the fact that aligned pedestrian images are symmetric about the vertical axis by adding a “reﬂected” aggregate similarity template to the aggregate similarity template. [sent-127, score-1.56]
</p><p>74 Rather than performing a straight R-way decomposition of the AST to obtain R pixel region models, we extracted a hierarchical segmentation in the form of a binary tree. [sent-129, score-0.63]
</p><p>75 Given the initial region-conditioned marginals p(pi |r0 ) and p(pi |r1 ), each pixel was assigned to the region with higher likelihood. [sent-130, score-0.501]
</p><p>76 For instance, the mean pixel value can be calculated as a weighted mean where the pixels are weighted by p(pi |r). [sent-135, score-0.417]
</p><p>77 Figure 3 shows the resulting hierarchical segmentation for the pedestrian AST. [sent-138, score-0.364]
</p><p>78 In our experience, a color histogram of all the pixels within a pedestrian is not useful for recognition and was almost useless for data mining applications. [sent-140, score-0.856]
</p><p>79 It determines a color model over each region that our algorithm has determined contain similar color information within this class of objects. [sent-142, score-1.039]
</p><p>80 This allows us to obtain robust estimates of color in the regions  Figure 4: Results of automatic clustering on three components: shirt, pants, and the background. [sent-143, score-0.543]
</p><p>81 Each shows the feature, the most unusual examples of that region, followed by the 12 most likely examples for the eight prototypical colors of that region. [sent-144, score-0.255]
</p><p>82 Further, as a result of our probabilistic segmentation, the values of p(pi |r) indicate which pixels are most regular in a region which enables us to weight the contribution of each pixel to the color model. [sent-146, score-1.07]
</p><p>83 For the case of pedestrian-conditional color models, the regions roughly correspond to shirt color, pant color, feet color, head color, and some background color regions. [sent-147, score-1.327]
</p><p>84 The colors in a region of a single image can be modeled by color histograms, Gaussians, or mixtures of Gaussians. [sent-148, score-0.95]
</p><p>85 These region models can be clustered across images to determine a density of shirt colors, pant colors, and other region colors within a particular environment. [sent-149, score-1.191]
</p><p>86 This enables not only an eﬃcient factored color component codebook, but anomaly detection based on particular regions and higher order models of co-occurrences between particular types of regions. [sent-150, score-0.612]
</p><p>87 To illustrate the eﬀectiveness of our representation we chose the simplest model for the colors in each region–a single Gaussian in RGB space. [sent-151, score-0.214]
</p><p>88 The mean and variance of each Gaussian was computed by weighting the pixels represented by the corresponding node by p(pi |r). [sent-152, score-0.18]
</p><p>89 This biases the estimate towards the “most similar” pixels in the region (e. [sent-153, score-0.407]
</p><p>90 , the center of the shirt or the center of the legs). [sent-155, score-0.213]
</p><p>91 This allows us to represent the colors of each pedestrian image with 31 means and variances corresponding to the (2treeheight − 1) nodes. [sent-156, score-0.587]
</p><p>92 We investigated unsupervised clustering on components of the conditional color model. [sent-157, score-0.431]
</p><p>93 We ﬁt a mixture of eight Gaussians to the 924 color means for each region. [sent-158, score-0.449]
</p><p>94 Figure 4 shows the 12 pedestrians with the highest probability under each of the eight models and the 12 most unusual pedestrians with respect to that region for three of the nodes of the tree: shirt color, pant color, and color of the background. [sent-159, score-1.377]
</p><p>95 In the future, similarity templates could be applied to diﬀerent modalities including texture similarity, depth similarity, or motion similarity. [sent-166, score-0.563]
</p><p>96 While computationally intensive, we believe that similarity templates can provide a uniﬁed approach to the extraction of possible class-speciﬁc targets from an image database, alignment of the candidate images, and precomputation of meaningful features of that class. [sent-167, score-0.668]
</p><p>97 For the case of pedestrians, it could detect potential pedestrians in a database, align them, derive a model of pedestrians, and extract the parameters for each pedestrian. [sent-168, score-0.248]
</p><p>98 We have introduced a new image representation based on pixel-wise similarity. [sent-170, score-0.172]
</p><p>99 We have shown its application in both alignment and decomposition of pedestrian images. [sent-171, score-0.416]
</p><p>100 “Similarity templates for detection and recognition,” submitted to CVPR (2001). [sent-210, score-0.187]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('color', 0.395), ('similarity', 0.274), ('pedestrian', 0.259), ('pixel', 0.237), ('region', 0.227), ('shirt', 0.213), ('images', 0.205), ('pedestrians', 0.204), ('ast', 0.202), ('template', 0.202), ('colors', 0.185), ('pixels', 0.18), ('pi', 0.157), ('templates', 0.155), ('aggregate', 0.149), ('image', 0.143), ('pj', 0.121), ('regions', 0.112), ('pants', 0.106), ('alignment', 0.096), ('stau', 0.085), ('di', 0.082), ('segmentation', 0.081), ('st', 0.069), ('erent', 0.066), ('pant', 0.064), ('decomposition', 0.061), ('head', 0.056), ('texture', 0.056), ('background', 0.055), ('object', 0.054), ('ideal', 0.052), ('latent', 0.051), ('objects', 0.049), ('similarities', 0.047), ('regularity', 0.044), ('cvpr', 0.044), ('align', 0.044), ('rgb', 0.043), ('rors', 0.043), ('tend', 0.04), ('sr', 0.039), ('decomposing', 0.039), ('marginals', 0.037), ('highlights', 0.037), ('legs', 0.037), ('shirts', 0.037), ('feet', 0.037), ('pnew', 0.037), ('unusual', 0.037), ('miller', 0.036), ('clustering', 0.036), ('lab', 0.035), ('characterized', 0.035), ('modalities', 0.034), ('ectively', 0.034), ('viola', 0.034), ('jojic', 0.034), ('cbcl', 0.034), ('erences', 0.034), ('eight', 0.033), ('june', 0.033), ('detection', 0.032), ('database', 0.032), ('modeling', 0.032), ('segmented', 0.031), ('lighting', 0.031), ('enables', 0.031), ('representation', 0.029), ('instances', 0.028), ('promise', 0.028), ('patches', 0.028), ('cuts', 0.027), ('aligned', 0.027), ('across', 0.027), ('black', 0.026), ('zi', 0.026), ('joint', 0.026), ('white', 0.025), ('blue', 0.025), ('factorization', 0.025), ('hierarchical', 0.024), ('depth', 0.024), ('represents', 0.024), ('drawing', 0.023), ('seek', 0.023), ('within', 0.022), ('collected', 0.022), ('xn', 0.022), ('view', 0.021), ('choosing', 0.021), ('particular', 0.021), ('considerable', 0.021), ('mixture', 0.021), ('er', 0.021), ('symmetric', 0.021), ('treating', 0.02), ('motion', 0.02), ('unique', 0.02), ('iteratively', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999875 <a title="191-tfidf-1" href="./nips-2001-Transform-invariant_Image_Decomposition_with_Similarity_Templates.html">191 nips-2001-Transform-invariant Image Decomposition with Similarity Templates</a></p>
<p>Author: Chris Stauffer, Erik Miller, Kinh Tieu</p><p>Abstract: Recent work has shown impressive transform-invariant modeling and clustering for sets of images of objects with similar appearance. We seek to expand these capabilities to sets of images of an object class that show considerable variation across individual instances (e.g. pedestrian images) using a representation based on pixel-wise similarities, similarity templates. Because of its invariance to the colors of particular components of an object, this representation enables detection of instances of an object class and enables alignment of those instances. Further, this model implicitly represents the regions of color regularity in the class-speciﬁc image set enabling a decomposition of that object class into component regions. 1</p><p>2 0.14860475 <a title="191-tfidf-2" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>Author: Stella X. Yu, Jianbo Shi</p><p>Abstract: With the optimization of pattern discrimination as a goal, graph partitioning approaches often lack the capability to integrate prior knowledge to guide grouping. In this paper, we consider priors from unitary generative models, partially labeled data and spatial attention. These priors are modelled as constraints in the solution space. By imposing uniformity condition on the constraints, we restrict the feasible space to one of smooth solutions. A subspace projection method is developed to solve this constrained eigenproblema We demonstrate that simple priors can greatly improve image segmentation results. 1</p><p>3 0.13088034 <a title="191-tfidf-3" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>Author: Bernd Heisele, Thomas Serre, Massimiliano Pontil, Thomas Vetter, Tomaso Poggio</p><p>Abstract: We describe an algorithm for automatically learning discriminative components of objects with SVM classiﬁers. It is based on growing image parts by minimizing theoretical bounds on the error probability of an SVM. Component-based face classiﬁers are then combined in a second stage to yield a hierarchical SVM classiﬁer. Experimental results in face classiﬁcation show considerable robustness against rotations in depth and suggest performance at signiﬁcantly better level than other face detection systems. Novel aspects of our approach are: a) an algorithm to learn component-based classiﬁcation experts and their combination, b) the use of 3-D morphable models for training, and c) a maximum operation on the output of each component classiﬁer which may be relevant for biological models of visual recognition.</p><p>4 0.10623703 <a title="191-tfidf-4" href="./nips-2001-Analog_Soft-Pattern-Matching_Classifier_using_Floating-Gate_MOS_Technology.html">34 nips-2001-Analog Soft-Pattern-Matching Classifier using Floating-Gate MOS Technology</a></p>
<p>Author: Toshihiko Yamasaki, Tadashi Shibata</p><p>Abstract: A flexible pattern-matching analog classifier is presented in conjunction with a robust image representation algorithm called Principal Axes Projection (PAP). In the circuit, the functional form of matching is configurable in terms of the peak position, the peak height and the sharpness of the similarity evaluation. The test chip was fabricated in a 0.6-µm CMOS technology and successfully applied to hand-written pattern recognition and medical radiograph analysis using PAP as a feature extraction pre-processing step for robust image coding. The separation and classification of overlapping patterns is also experimentally demonstrated. 1 I ntr o du c ti o n Pattern classification using template matching techniques is a powerful tool in implementing human-like intelligent systems. However, the processing is computationally very expensive, consuming a lot of CPU time when implemented as software running on general-purpose computers. Therefore, software approaches are not practical for real-time applications. For systems working in mobile environment, in particular, they are not realistic because the memory and computational resources are severely limited. The development of analog VLSI chips having a fully parallel template matching architecture [1,2] would be a promising solution in such applications because they offer an opportunity of low-power operation as well as very compact implementation. In order to build a real human-like intelligent system, however, not only the pattern representation algorithm but also the matching hardware itself needs to be made flexible and robust in carrying out the pattern matching task. First of all, two-dimensional patterns need to be represented by feature vectors having substantially reduced dimensions, while at the same time preserving the human perception of similarity among patterns in the vector space mapping. For this purpose, an image representation algorithm called Principal Axes Projection (PAP) has been de- veloped [3] and its robust nature in pattern recognition has been demonstrated in the applications to medical radiograph analysis [3] and hand-written digits recognition [4]. However, the demonstration so far was only carried out by computer simulation. Regarding the matching hardware, high-flexibility analog template matching circuits have been developed for PAP vector representation. The circuits are flexible in a sense that the matching criteria (the weight to elements, the strictness in matching) are configurable. In Ref. [5], the fundamental characteristics of the building block circuits were presented, and their application to simple hand-written digits was presented in Ref. [6]. The purpose of this paper is to demonstrate the robust nature of the hardware matching system by experiments. The classification of simple hand-written patterns and the cephalometric landmark identification in gray-scale medical radiographs have been carried out and successful results are presented. In addition, multiple overlapping patterns can be separated without utilizing a priori knowledge, which is one of the most difficult problems at present in artificial intelligence. 2 I ma g e re pr es e n tati on by P AP PAP is a feature extraction technique using the edge information. The input image (64x64 pixels) is first subjected to pixel-by-pixel spatial filtering operations to detect edges in four directions: horizontal (HR); vertical (VR); +45 degrees (+45); and –45 degrees (-45). Each detected edge is represented by a binary flag and four edge maps are generated. The two-dimensional bit array in an edge map is reduced to a one-dimensional array of numerals by projection. The horizontal edge flags are accumulated in the horizontal direction and projected onto vertical axis. The vertical, +45-degree and –45-degree edge flags are similarly projected onto horizontal, -45-degree and +45-degree axes, respectively. Therefore the method is called “Principal Axes Projection (PAP)” [3,4]. Then each projection data set is series connected in the order of HR, +45, VR, -45 to form a feature vector. Neighboring four elements are averaged and merged to one element and a 64-dimensional vector is finally obtained. This vector representation very well preserves the human perception of similarity in the vector space. In the experiments below, we have further reduced the feature vector to 16 dimensions by merging each set of four neighboring elements into one, without any significant degradation in performance. C i r cui t c o nf i g ura ti ons A B C VGG A B C VGG IOUT IOUT 1 1 2 2 4 4 1 VIN 13 VIN RST RST £ ¡ ¤¢  £ ¥ §¦  3 Figure 1: Schematic of vector element matching circuit: (a) pyramid (gain reduction) type; (b) plateau (feedback) type. The capacitor area ratio is indicated in the figure. The basic functional form of the similarity evaluation is generated by the shortcut current flowing in a CMOS inverter as in Refs. [7,8,9]. However, their circuits were utilized to form radial basis functions and only the peak position was programmable. In our circuits, not only the peak position but also the peak height and the sharpness of the peak response shape are made configurable to realize flexible matching operations [5]. Two types of the element matching circuit are shown in Fig. 1. They evaluate the similarity between two vector elements. The result of the evaluation is given as an output current (IOUT ) from the pMOS current mirror. The peak position is temporarily memorized by auto-zeroing of the CMOS inverter. The common-gate transistor with VGG stabilizes the voltage supply to the inverter. By controlling the gate bias VGG, the peak height can be changed. This corresponds to multiplying a weight factor to the element. The sharpness of the functional form is taken as the strictness of the similarity evaluation. In the pyramid type circuit (Fig. 1(a)), the sharpness is controlled by the gain reduction in the input. In the plateau type (Fig. 1(b)), the output voltage of the inverter is fed back to input nodes and the sharpness changes in accordance with the amount of the feedback.    ¥£¡ ¦¤¢   £¨ 9&% ¦©§ (!! #$ 5 !' #$ &% 9 9 4 92 !¦ A1@9  ¨¥  5 4 52 (!  5 8765  9) 0 1 ¥ 1 ¨</p><p>5 0.10222959 <a title="191-tfidf-5" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>Author: Nello Cristianini, John Shawe-Taylor, André Elisseeff, Jaz S. Kandola</p><p>Abstract: We introduce the notion of kernel-alignment, a measure of similarity between two kernel functions or between a kernel and a target function. This quantity captures the degree of agreement between a kernel and a given learning task, and has very natural interpretations in machine learning, leading also to simple algorithms for model selection and learning. We analyse its theoretical properties, proving that it is sharply concentrated around its expected value, and we discuss its relation with other standard measures of performance. Finally we describe some of the algorithms that can be obtained within this framework, giving experimental results showing that adapting the kernel to improve alignment on the labelled data significantly increases the alignment on the test set, giving improved classification accuracy. Hence, the approach provides a principled method of performing transduction. Keywords: Kernels, alignment, eigenvectors, eigenvalues, transduction 1</p><p>6 0.099298976 <a title="191-tfidf-6" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>7 0.09333314 <a title="191-tfidf-7" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>8 0.088057593 <a title="191-tfidf-8" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>9 0.087808549 <a title="191-tfidf-9" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>10 0.085928358 <a title="191-tfidf-10" href="./nips-2001-The_g_Factor%3A_Relating_Distributions_on_Features_to_Distributions_on_Images.html">189 nips-2001-The g Factor: Relating Distributions on Features to Distributions on Images</a></p>
<p>11 0.085068293 <a title="191-tfidf-11" href="./nips-2001-Fast%2C_Large-Scale_Transformation-Invariant_Clustering.html">75 nips-2001-Fast, Large-Scale Transformation-Invariant Clustering</a></p>
<p>12 0.084899433 <a title="191-tfidf-12" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>13 0.082881011 <a title="191-tfidf-13" href="./nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables.html">153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</a></p>
<p>14 0.07155209 <a title="191-tfidf-14" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>15 0.068730511 <a title="191-tfidf-15" href="./nips-2001-Generalizable_Relational_Binding_from_Coarse-coded_Distributed_Representations.html">80 nips-2001-Generalizable Relational Binding from Coarse-coded Distributed Representations</a></p>
<p>16 0.067998081 <a title="191-tfidf-16" href="./nips-2001-Model_Based_Population_Tracking_and_Automatic_Detection_of_Distribution_Changes.html">122 nips-2001-Model Based Population Tracking and Automatic Detection of Distribution Changes</a></p>
<p>17 0.067121409 <a title="191-tfidf-17" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>18 0.065418094 <a title="191-tfidf-18" href="./nips-2001-Multi_Dimensional_ICA_to_Separate_Correlated_Sources.html">127 nips-2001-Multi Dimensional ICA to Separate Correlated Sources</a></p>
<p>19 0.063515469 <a title="191-tfidf-19" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>20 0.059340622 <a title="191-tfidf-20" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.166), (1, -0.009), (2, -0.085), (3, -0.064), (4, -0.056), (5, 0.027), (6, -0.23), (7, -0.025), (8, 0.049), (9, 0.004), (10, 0.058), (11, -0.014), (12, 0.186), (13, 0.016), (14, -0.154), (15, -0.036), (16, 0.026), (17, -0.014), (18, 0.031), (19, 0.059), (20, -0.023), (21, 0.11), (22, -0.073), (23, -0.075), (24, -0.013), (25, -0.133), (26, 0.007), (27, -0.076), (28, 0.052), (29, 0.025), (30, 0.08), (31, -0.059), (32, -0.142), (33, 0.12), (34, 0.082), (35, -0.021), (36, 0.198), (37, 0.086), (38, 0.057), (39, -0.007), (40, 0.086), (41, 0.138), (42, 0.049), (43, -0.088), (44, 0.063), (45, 0.063), (46, 0.002), (47, 0.194), (48, -0.038), (49, -0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97778606 <a title="191-lsi-1" href="./nips-2001-Transform-invariant_Image_Decomposition_with_Similarity_Templates.html">191 nips-2001-Transform-invariant Image Decomposition with Similarity Templates</a></p>
<p>Author: Chris Stauffer, Erik Miller, Kinh Tieu</p><p>Abstract: Recent work has shown impressive transform-invariant modeling and clustering for sets of images of objects with similar appearance. We seek to expand these capabilities to sets of images of an object class that show considerable variation across individual instances (e.g. pedestrian images) using a representation based on pixel-wise similarities, similarity templates. Because of its invariance to the colors of particular components of an object, this representation enables detection of instances of an object class and enables alignment of those instances. Further, this model implicitly represents the regions of color regularity in the class-speciﬁc image set enabling a decomposition of that object class into component regions. 1</p><p>2 0.67170382 <a title="191-lsi-2" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>Author: Javid Sadr, Sayan Mukherjee, Keith Thoresz, Pawan Sinha</p><p>Abstract: A key question in neuroscience is how to encode sensory stimuli such as images and sounds. Motivated by studies of response properties of neurons in the early cortical areas, we propose an encoding scheme that dispenses with absolute measures of signal intensity or contrast and uses, instead, only local ordinal measures. In this scheme, the structure of a signal is represented by a set of equalities and inequalities across adjacent regions. In this paper, we focus on characterizing the ﬁdelity of this representation strategy. We develop a regularization approach for image reconstruction from ordinal measures and thereby demonstrate that the ordinal representation scheme can faithfully encode signal structure. We also present a neurally plausible implementation of this computation that uses only local update rules. The results highlight the robustness and generalization ability of local ordinal encodings for the task of pattern classiﬁcation. 1</p><p>3 0.54438555 <a title="191-lsi-3" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>Author: Stella X. Yu, Jianbo Shi</p><p>Abstract: With the optimization of pattern discrimination as a goal, graph partitioning approaches often lack the capability to integrate prior knowledge to guide grouping. In this paper, we consider priors from unitary generative models, partially labeled data and spatial attention. These priors are modelled as constraints in the solution space. By imposing uniformity condition on the constraints, we restrict the feasible space to one of smooth solutions. A subspace projection method is developed to solve this constrained eigenproblema We demonstrate that simple priors can greatly improve image segmentation results. 1</p><p>4 0.49166569 <a title="191-lsi-4" href="./nips-2001-Analog_Soft-Pattern-Matching_Classifier_using_Floating-Gate_MOS_Technology.html">34 nips-2001-Analog Soft-Pattern-Matching Classifier using Floating-Gate MOS Technology</a></p>
<p>Author: Toshihiko Yamasaki, Tadashi Shibata</p><p>Abstract: A flexible pattern-matching analog classifier is presented in conjunction with a robust image representation algorithm called Principal Axes Projection (PAP). In the circuit, the functional form of matching is configurable in terms of the peak position, the peak height and the sharpness of the similarity evaluation. The test chip was fabricated in a 0.6-µm CMOS technology and successfully applied to hand-written pattern recognition and medical radiograph analysis using PAP as a feature extraction pre-processing step for robust image coding. The separation and classification of overlapping patterns is also experimentally demonstrated. 1 I ntr o du c ti o n Pattern classification using template matching techniques is a powerful tool in implementing human-like intelligent systems. However, the processing is computationally very expensive, consuming a lot of CPU time when implemented as software running on general-purpose computers. Therefore, software approaches are not practical for real-time applications. For systems working in mobile environment, in particular, they are not realistic because the memory and computational resources are severely limited. The development of analog VLSI chips having a fully parallel template matching architecture [1,2] would be a promising solution in such applications because they offer an opportunity of low-power operation as well as very compact implementation. In order to build a real human-like intelligent system, however, not only the pattern representation algorithm but also the matching hardware itself needs to be made flexible and robust in carrying out the pattern matching task. First of all, two-dimensional patterns need to be represented by feature vectors having substantially reduced dimensions, while at the same time preserving the human perception of similarity among patterns in the vector space mapping. For this purpose, an image representation algorithm called Principal Axes Projection (PAP) has been de- veloped [3] and its robust nature in pattern recognition has been demonstrated in the applications to medical radiograph analysis [3] and hand-written digits recognition [4]. However, the demonstration so far was only carried out by computer simulation. Regarding the matching hardware, high-flexibility analog template matching circuits have been developed for PAP vector representation. The circuits are flexible in a sense that the matching criteria (the weight to elements, the strictness in matching) are configurable. In Ref. [5], the fundamental characteristics of the building block circuits were presented, and their application to simple hand-written digits was presented in Ref. [6]. The purpose of this paper is to demonstrate the robust nature of the hardware matching system by experiments. The classification of simple hand-written patterns and the cephalometric landmark identification in gray-scale medical radiographs have been carried out and successful results are presented. In addition, multiple overlapping patterns can be separated without utilizing a priori knowledge, which is one of the most difficult problems at present in artificial intelligence. 2 I ma g e re pr es e n tati on by P AP PAP is a feature extraction technique using the edge information. The input image (64x64 pixels) is first subjected to pixel-by-pixel spatial filtering operations to detect edges in four directions: horizontal (HR); vertical (VR); +45 degrees (+45); and –45 degrees (-45). Each detected edge is represented by a binary flag and four edge maps are generated. The two-dimensional bit array in an edge map is reduced to a one-dimensional array of numerals by projection. The horizontal edge flags are accumulated in the horizontal direction and projected onto vertical axis. The vertical, +45-degree and –45-degree edge flags are similarly projected onto horizontal, -45-degree and +45-degree axes, respectively. Therefore the method is called “Principal Axes Projection (PAP)” [3,4]. Then each projection data set is series connected in the order of HR, +45, VR, -45 to form a feature vector. Neighboring four elements are averaged and merged to one element and a 64-dimensional vector is finally obtained. This vector representation very well preserves the human perception of similarity in the vector space. In the experiments below, we have further reduced the feature vector to 16 dimensions by merging each set of four neighboring elements into one, without any significant degradation in performance. C i r cui t c o nf i g ura ti ons A B C VGG A B C VGG IOUT IOUT 1 1 2 2 4 4 1 VIN 13 VIN RST RST £ ¡ ¤¢  £ ¥ §¦  3 Figure 1: Schematic of vector element matching circuit: (a) pyramid (gain reduction) type; (b) plateau (feedback) type. The capacitor area ratio is indicated in the figure. The basic functional form of the similarity evaluation is generated by the shortcut current flowing in a CMOS inverter as in Refs. [7,8,9]. However, their circuits were utilized to form radial basis functions and only the peak position was programmable. In our circuits, not only the peak position but also the peak height and the sharpness of the peak response shape are made configurable to realize flexible matching operations [5]. Two types of the element matching circuit are shown in Fig. 1. They evaluate the similarity between two vector elements. The result of the evaluation is given as an output current (IOUT ) from the pMOS current mirror. The peak position is temporarily memorized by auto-zeroing of the CMOS inverter. The common-gate transistor with VGG stabilizes the voltage supply to the inverter. By controlling the gate bias VGG, the peak height can be changed. This corresponds to multiplying a weight factor to the element. The sharpness of the functional form is taken as the strictness of the similarity evaluation. In the pyramid type circuit (Fig. 1(a)), the sharpness is controlled by the gain reduction in the input. In the plateau type (Fig. 1(b)), the output voltage of the inverter is fed back to input nodes and the sharpness changes in accordance with the amount of the feedback.    ¥£¡ ¦¤¢   £¨ 9&% ¦©§ (!! #$ 5 !' #$ &% 9 9 4 92 !¦ A1@9  ¨¥  5 4 52 (!  5 8765  9) 0 1 ¥ 1 ¨</p><p>5 0.46880576 <a title="191-lsi-5" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>Author: Heiko Wersing</p><p>Abstract: We present a new approach to the supervised learning of lateral interactions for the competitive layer model (CLM) dynamic feature binding architecture. The method is based on consistency conditions, which were recently shown to characterize the attractor states of this linear threshold recurrent network. For a given set of training examples the learning problem is formulated as a convex quadratic optimization problem in the lateral interaction weights. An efﬁcient dimension reduction of the learning problem can be achieved by using a linear superposition of basis interactions. We show the successful application of the method to a medical image segmentation problem of ﬂuorescence microscope cell images.</p><p>6 0.44974291 <a title="191-lsi-6" href="./nips-2001-Fast%2C_Large-Scale_Transformation-Invariant_Clustering.html">75 nips-2001-Fast, Large-Scale Transformation-Invariant Clustering</a></p>
<p>7 0.38944852 <a title="191-lsi-7" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>8 0.37890372 <a title="191-lsi-8" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>9 0.37627167 <a title="191-lsi-9" href="./nips-2001-The_g_Factor%3A_Relating_Distributions_on_Features_to_Distributions_on_Images.html">189 nips-2001-The g Factor: Relating Distributions on Features to Distributions on Images</a></p>
<p>10 0.36927727 <a title="191-lsi-10" href="./nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables.html">153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</a></p>
<p>11 0.2733238 <a title="191-lsi-11" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>12 0.27163088 <a title="191-lsi-12" href="./nips-2001-Constructing_Distributed_Representations_Using_Additive_Clustering.html">53 nips-2001-Constructing Distributed Representations Using Additive Clustering</a></p>
<p>13 0.25488251 <a title="191-lsi-13" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>14 0.25442341 <a title="191-lsi-14" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>15 0.25099885 <a title="191-lsi-15" href="./nips-2001-Receptive_field_structure_of_flow_detectors_for_heading_perception.html">158 nips-2001-Receptive field structure of flow detectors for heading perception</a></p>
<p>16 0.2503114 <a title="191-lsi-16" href="./nips-2001-Unsupervised_Learning_of_Human_Motion_Models.html">193 nips-2001-Unsupervised Learning of Human Motion Models</a></p>
<p>17 0.25011578 <a title="191-lsi-17" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>18 0.24634704 <a title="191-lsi-18" href="./nips-2001-Multi_Dimensional_ICA_to_Separate_Correlated_Sources.html">127 nips-2001-Multi Dimensional ICA to Separate Correlated Sources</a></p>
<p>19 0.24623883 <a title="191-lsi-19" href="./nips-2001-Learning_Hierarchical_Structures_with_Linear_Relational_Embedding.html">110 nips-2001-Learning Hierarchical Structures with Linear Relational Embedding</a></p>
<p>20 0.2438467 <a title="191-lsi-20" href="./nips-2001-Stochastic_Mixed-Signal_VLSI_Architecture_for_High-Dimensional_Kernel_Machines.html">176 nips-2001-Stochastic Mixed-Signal VLSI Architecture for High-Dimensional Kernel Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.036), (17, 0.014), (19, 0.024), (20, 0.011), (27, 0.137), (30, 0.075), (36, 0.013), (38, 0.024), (49, 0.013), (59, 0.035), (72, 0.09), (79, 0.033), (83, 0.018), (85, 0.237), (91, 0.157)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88230813 <a title="191-lda-1" href="./nips-2001-Transform-invariant_Image_Decomposition_with_Similarity_Templates.html">191 nips-2001-Transform-invariant Image Decomposition with Similarity Templates</a></p>
<p>Author: Chris Stauffer, Erik Miller, Kinh Tieu</p><p>Abstract: Recent work has shown impressive transform-invariant modeling and clustering for sets of images of objects with similar appearance. We seek to expand these capabilities to sets of images of an object class that show considerable variation across individual instances (e.g. pedestrian images) using a representation based on pixel-wise similarities, similarity templates. Because of its invariance to the colors of particular components of an object, this representation enables detection of instances of an object class and enables alignment of those instances. Further, this model implicitly represents the regions of color regularity in the class-speciﬁc image set enabling a decomposition of that object class into component regions. 1</p><p>2 0.80728173 <a title="191-lda-2" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>Author: Gregor Wenning, Klaus Obermayer</p><p>Abstract: Cortical neurons might be considered as threshold elements integrating in parallel many excitatory and inhibitory inputs. Due to the apparent variability of cortical spike trains this yields a strongly fluctuating membrane potential, such that threshold crossings are highly irregular. Here we study how a neuron could maximize its sensitivity w.r.t. a relatively small subset of excitatory input. Weak signals embedded in fluctuations is the natural realm of stochastic resonance. The neuron's response is described in a hazard-function approximation applied to an Ornstein-Uhlenbeck process. We analytically derive an optimality criterium and give a learning rule for the adjustment of the membrane fluctuations, such that the sensitivity is maximal exploiting stochastic resonance. We show that adaptation depends only on quantities that could easily be estimated locally (in space and time) by the neuron. The main results are compared with simulations of a biophysically more realistic neuron model. 1</p><p>3 0.70798481 <a title="191-lda-3" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>Author: Gregory Z. Grudic, Lyle H. Ungar</p><p>Abstract: We address two open theoretical questions in Policy Gradient Reinforcement Learning. The ﬁrst concerns the efﬁcacy of using function approximation to represent the state action value function, . Theory is presented showing that linear function approximation representations of can degrade the rate of convergence of performance gradient estimates by a factor of relative to when no function approximation of is used, where is the number of possible actions and is the number of basis functions in the function approximation representation. The second concerns the use of a bias term in estimating the state action value function. Theory is presented showing that a non-zero bias term can improve the rate of convergence of performance gradient estimates by , where is the number of possible actions. Experimental evidence is presented showing that these theoretical results lead to signiﬁcant improvement in the convergence properties of Policy Gradient Reinforcement Learning algorithms.       ¤ ¨ ¦ ¢ ©§¥¤£¡ ¦ ¤ ¨ £¡ ¨ ¤¢  ¢</p><p>4 0.70628786 <a title="191-lda-4" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>Author: Yun Gao, Michael J. Black, Elie Bienenstock, Shy Shoham, John P. Donoghue</p><p>Abstract: Statistical learning and probabilistic inference techniques are used to infer the hand position of a subject from multi-electrode recordings of neural activity in motor cortex. First, an array of electrodes provides training data of neural ﬁring conditioned on hand kinematics. We learn a nonparametric representation of this ﬁring activity using a Bayesian model and rigorously compare it with previous models using cross-validation. Second, we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using Bayesian inference. The learned ﬁring models of multiple cells are used to deﬁne a nonGaussian likelihood term which is combined with a prior probability for the kinematics. A particle ﬁltering method is used to represent, update, and propagate the posterior distribution over time. The approach is compared with traditional linear ﬁltering methods; the results suggest that it may be appropriate for neural prosthetic applications.</p><p>5 0.70518333 <a title="191-lda-5" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>6 0.70472223 <a title="191-lda-6" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>7 0.70449078 <a title="191-lda-7" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>8 0.70334125 <a title="191-lda-8" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>9 0.70285928 <a title="191-lda-9" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>10 0.70257866 <a title="191-lda-10" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>11 0.70121187 <a title="191-lda-11" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>12 0.70039093 <a title="191-lda-12" href="./nips-2001-The_Method_of_Quantum_Clustering.html">185 nips-2001-The Method of Quantum Clustering</a></p>
<p>13 0.7003535 <a title="191-lda-13" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>14 0.69852084 <a title="191-lda-14" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>15 0.69670033 <a title="191-lda-15" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>16 0.69662201 <a title="191-lda-16" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>17 0.69656098 <a title="191-lda-17" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>18 0.69651598 <a title="191-lda-18" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<p>19 0.69609761 <a title="191-lda-19" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>20 0.69596219 <a title="191-lda-20" href="./nips-2001-A_Parallel_Mixture_of_SVMs_for_Very_Large_Scale_Problems.html">16 nips-2001-A Parallel Mixture of SVMs for Very Large Scale Problems</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
