<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-160" href="#">nips2001-160</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</h1>
<br/><p>Source: <a title="nips-2001-160-pdf" href="http://papers.nips.cc/paper/2000-reinforcement-learning-and-time-perception-a-model-of-animal-experiments.pdf">pdf</a></p><p>Author: Jonathan L. Shapiro, J. Wearden</p><p>Abstract: Animal data on delayed-reward conditioning experiments shows a striking property - the data for different time intervals collapses into a single curve when the data is scaled by the time interval. This is called the scalar property of interval timing. Here a simple model of a neural clock is presented and shown to give rise to the scalar property. The model is an accumulator consisting of noisy, linear spiking neurons. It is analytically tractable and contains only three parameters. When coupled with reinforcement learning it simulates peak procedure experiments, producing both the scalar property and the pattern of single trial covariances. 1</p><p>Reference: <a title="nips-2001-160-reference" href="../nips2001_reference/nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Animal data on delayed-reward conditioning experiments shows a striking property - the data for different time intervals collapses into a single curve when the data is scaled by the time interval. [sent-10, score-0.603]
</p><p>2 This is called the scalar property of interval timing. [sent-11, score-0.386]
</p><p>3 Here a simple model of a neural clock is presented and shown to give rise to the scalar property. [sent-12, score-0.729]
</p><p>4 The model is an accumulator consisting of noisy, linear spiking neurons. [sent-13, score-0.503]
</p><p>5 When coupled with reinforcement learning it simulates peak procedure experiments, producing both the scalar property and the pattern of single trial covariances. [sent-15, score-0.654]
</p><p>6 1  Introduction  An aspect of delayed-reward reinforcement learning problem which has a long history of study in animal experiments, but has been overlooked by theorists, is the learning of the expected time to the reward. [sent-16, score-0.616]
</p><p>7 In a number of animal experiments, animals need to wait a given time interval after a stimulus before performing an action in order to receive the reward. [sent-17, score-0.689]
</p><p>8 In order to be able to do this , the animal requires an internal clock or mechanism for perceiving time intervals, as well as a learning system which can tackle more familiar aspects of delayed reward reinforcement learning problem. [sent-18, score-1.191]
</p><p>9 In this paper it is shown that a simple connectionist model of an accumulator used to measure time duration, coupled to a standard TD('\) reinforcement learning rule reproduces the most prominent features of the animal experiments. [sent-19, score-1.097]
</p><p>10 The reason it might be desirable for a learner to learn the expected time to receive a reward is that it allows it to perform the action for an appropriate length of time. [sent-20, score-0.212]
</p><p>11 An example described by Grossberg and Merrill [4] and modeled in animal experiments by Gibbon and Church [3] is foraging. [sent-21, score-0.328]
</p><p>12 An animal which had no sense of the typical time to find food might leave too often, thereby spending an inordinate amount of time flying between patches. [sent-22, score-0.519]
</p><p>13 1  Peak Procedure Experiments  A typical type of experiment which investigates how animals learn the time between stimulus and reward is the peak procedure. [sent-26, score-0.434]
</p><p>14 In this, the animal is trained to respond after a given time interval tr has elapsed. [sent-27, score-0.616]
</p><p>15 The animal receives a reward for the first response after the length of time t r . [sent-32, score-0.613]
</p><p>16 The trial ends when the animal receives the reward. [sent-33, score-0.411]
</p><p>17 On some trials, however, no reward is given even when the animal responds appropriately. [sent-34, score-0.403]
</p><p>18 This is to see when the animal would stop responding. [sent-35, score-0.408]
</p><p>19 What happens in non-reward trials is that the animal typically will start responding at a certain time, will respond for a period, and then stop responding. [sent-36, score-0.775]
</p><p>20 The highest response is at the time interval t r , and there is variation around this. [sent-38, score-0.372]
</p><p>21 The inaccuracy in the response (as measured by the standard deviation in the average response curves for non-reward trials) is also proportional to the time interval. [sent-39, score-0.404]
</p><p>22 In other words, the ratio of the standard deviation to the mean response time (the coefficient of variation) is a constant independent of the time interval. [sent-40, score-0.343]
</p><p>23 A more striking property of the timing curves is scalar property, of which the above are two consequences. [sent-41, score-0.364]
</p><p>24 When the average response rate for non-reward trials is multiplied by the time interval and plotted against the relative time (time divided by the time interval) the data from different time intervals collapse onto one curve. [sent-42, score-1.023]
</p><p>25 This strong form of the scalar property can be expressed mathematically as follows. [sent-43, score-0.244]
</p><p>26 Let T be the actual time since the start of the trial and T be subjective time. [sent-44, score-0.292]
</p><p>27 Subjective time is the time duration which the animal perceives to have occurred, (or at least appears to perceive judging from its behavior). [sent-45, score-0.519]
</p><p>28 This variation can be expressed as a conditional probability, the probability of acting as though the time is T given that the actual time is T, which is written P(TIT). [sent-47, score-0.226]
</p><p>29 This has been seen in many species, including rats, pigeons, turtles; humans will show similar results if the time intervals are short or if they are prevented from counting through distracting tasks. [sent-51, score-0.174]
</p><p>30 For reviews of interval timing phenomena, see [5] and [3] . [sent-52, score-0.204]
</p><p>31 A key question which remains unanswered is: what is the origin of the scalar property. [sent-53, score-0.187]
</p><p>32 Since the scalar property is ubiquitous, it may be revealing something fundamental about the nature of an internal clock or time perception system. [sent-54, score-0.842]
</p><p>33 It is well known that any model based on the accumulation of independent errors, such as a clock with a variable pulse-rate, does not produce the scalar property. [sent-56, score-0.7]
</p><p>34 In such a model it would be the ratio of the variance to the mean response time which would be independent of the time interval (a consequence of the law of large numbers). [sent-57, score-0.513]
</p><p>35 In section 2, a simple stochastic process will be presented which gives rise to scalar timing. [sent-58, score-0.216]
</p><p>36 In section 3 simulations of the model on the peak  procedure are presented. [sent-59, score-0.194]
</p><p>37 The model reproduces experimental results on the mean responses and the covariation between responses on non-reward trials. [sent-60, score-0.255]
</p><p>38 1  The model An accumulator network of spiking neurons  Here it is shown that a simple connectionist model of an accumulator can give rise to the strong scalar property. [sent-62, score-1.316]
</p><p>39 The network consists of noisy, linear, spiking neurons which are connected in a random, spatially homogeneous way. [sent-63, score-0.251]
</p><p>40 The network encodes time as the total activity in the network which grows during the measured time interval. [sent-64, score-0.481]
</p><p>41 An important parameter is the fan-out of the ith neuron Ci ; its average across the network is denoted C. [sent-68, score-0.236]
</p><p>42 Time is in discrete units of size T, the time required for a spike produced by a neuron to invoke a spike in a connected neuron. [sent-69, score-0.408]
</p><p>43 The neurons are linear - the expected number of spikes produced by a neuron is "( times the number of pre-synaptic spikes. [sent-71, score-0.307]
</p><p>44 Let ai(t) denote the number of spikes produced by neuron i at time t. [sent-72, score-0.316]
</p><p>45 This obeys hi(t)  ai(t + T) =  L  Va  + Ii(t),  (2)  a=l  where hi(t) is the number of spikes feeding into neuron i, hi(t) = E j CjiXj(t). [sent-73, score-0.212]
</p><p>46 Ii(t) is the external input at i , and V is a random variable which determines whether a pre-synaptic spike invokes one in a connected neuron. [sent-74, score-0.204]
</p><p>47 At the same time, the number of spikes will shrink due to the fact that a spike invokes another spike with a probability less than 1. [sent-79, score-0.329]
</p><p>48 Finally, in order for this network to act as an accumulator, it receives statistically stationary input during the time interval which is being measured, so I(t) is only present during the measured interval and statistically stationary then. [sent-81, score-0.646]
</p><p>49 2  Derivation of the strong scalar property  Here it is shown that the network activity obeys equation (1). [sent-83, score-0.441]
</p><p>50 For any probability distribution, p(x), the generating function for  cumulants is,  G(8)  (5) (6)  where n is the domain of p(x), "'i is the ith cumulant of p(x), and 8 is just a dummy variable. [sent-87, score-0.207]
</p><p>51 We will derive a recursion relation for the cumulant generating function for y(t), denoted G y (8; t). [sent-90, score-0.201]
</p><p>52 =  (7)  Ă˘&euro;Ë&tilde;  In deriving the above, it was assumed that the activity at each node is statistically the same, and that the fan-out at i is uncorrelated with the activity at i (this requires a sufficiently sparsely connectivity, i. [sent-94, score-0.281]
</p><p>53 Differentiating the last equation n times with respect to 8 and setting 8 to zero produces a set recursion relations for the cumulants of y, denoted It is necessary to take terms only up to first order in lit to find the fixed point distribution. [sent-97, score-0.241]
</p><p>54 3  Reinforcement learning of time intervals  The above model represents a way for a simple connectionist system to measure a time interval. [sent-110, score-0.386]
</p><p>55 In order to model behavior, the system must learn to association the external stimulus and the clock with the response and the reward. [sent-111, score-0.85]
</p><p>56 The output of the accumulator triggers a set of clock nodes which convert the quantity or activity encoding of time used by the accumulator into a "spatial code" in which particular nodes represent different network activities. [sent-114, score-1.711]
</p><p>57 This was done because it is difficult to use the accumulator activity directly, as this takes a wide range of values. [sent-115, score-0.499]
</p><p>58 Each clock node responds to a particular accumulator activity. [sent-116, score-1.013]
</p><p>59 The output of the ith clock node at time t is denoted Xi(t) ; it is one if the activity is i, zero otherwise. [sent-117, score-0.842]
</p><p>60 The accumulator feeds into a bank of clock nodes , Xi , which are tuned to accumulator activities . [sent-121, score-1.372]
</p><p>61 The response Vj is triggered by simultaneous presence of both the stimulus Si and the appropriate clock node. [sent-122, score-0.714]
</p><p>62 The stimulus and the clock nodes feed into response nodes. [sent-124, score-0.783]
</p><p>63 The output of the jth response node, Vj(t) is given by  (14) Here () is a threshold, Aij is the association between the stimulus and the response, and Wij is the association between a clock node and the response. [sent-125, score-0.842]
</p><p>64 Both the stimulus and the appropriate clock node must be present in order for there to be a reasonable probability of a response. [sent-126, score-0.665]
</p><p>65 TD-A is an important learning  rule for modeling associative conditioning; it has been used to model aspects of classical conditioning including Pavlovian conditioning and blocking. [sent-129, score-0.278]
</p><p>66 This model represents time using a tapped delay line; at each time-step, a different node in the delay line is activated. [sent-133, score-0.32]
</p><p>67 The conditioned stimsing temporal difference (TD) reinforcement learning is associated with the response through the unconditioned stimulus. [sent-135, score-0.329]
</p><p>68 These authors did not attempt to model the scalar property, and in their model time is represented accurately by the system. [sent-136, score-0.356]
</p><p>69 The clock nodes play the role of the tapped delay-line nodes in that model. [sent-138, score-0.674]
</p><p>70 It would make no difference if a eligibility trace were used for the stimulus Si, because that was held on during the learning. [sent-147, score-0.21]
</p><p>71 In the simulations, the model is forced to respond for the first set of trials (50 trials in the simulations); otherwise the model would never respond. [sent-149, score-0.498]
</p><p>72 After that the model learns using reward trials for an additional number of trials (150 trials in these simulations). [sent-151, score-0.648]
</p><p>73 Figure 2 shows average over non-reward trials for different time intervals. [sent-153, score-0.326]
</p><p>74 Gibbon and Church [3] have argued that the covariation between trials is a useful diagnostic to distinguish models of scalar timing. [sent-155, score-0.476]
</p><p>75 The methodology which they proposed is to fit the results of single non-reward trials from peak procedure experiments to a break-run-break pattern of response The animal is assumed to respond at a low rate until a start time is reached. [sent-156, score-0.973]
</p><p>76 The animal then responds at a high rate until a stop time is reached, whence it returns to the low response rate. [sent-157, score-0.689]
</p><p>77 The covariation between the start and stop times between trials is measured and compared to those predicted by theory. [sent-158, score-0.54]
</p><p>78 The question Gibbon and Church asked was, how does the start and stop time covary across trials. [sent-159, score-0.28]
</p><p>79 For example, if the animal starts responding early, does it stop  Q)  ~  O. [sent-160, score-0.468]
</p><p>80 2  time  time/t r Figure 2: Left) Average response of the spatially encoded network for non-reward trials. [sent-167, score-0.327]
</p><p>81 The accumulator parameters are: mI = 10, Cu 2 = 1 (Poisson limit); learning parameters are "( = 0. [sent-168, score-0.438]
</p><p>82 Right) Relative time plotted against response rate times time interval for reinforcement times of 40T, 80T, 160T, 240T, and 320T. [sent-171, score-0.743]
</p><p>83 responding early, as though it has a shifted estimate of the time interval? [sent-173, score-0.173]
</p><p>84 Or does it stop responding late, as though it has a more liberal view about what constitutes the particular interval. [sent-174, score-0.175]
</p><p>85 The pattern of covariation found in the simulations is qualitatively similar to that of the animal data. [sent-179, score-0.453]
</p><p>86 The interesting quantity is the correlation between the start time and the spread (difference between stop and start times). [sent-180, score-0.332]
</p><p>87 5 '----~____:,----____:-___:_-___:_-___:_----'  Figure 3: Left) Covariances across individual trials in experiments on rats. [sent-186, score-0.222]
</p><p>88 The black, gray, and white bars are for times of reinforcement tr of 15,30, and 60 seconds respectively. [sent-195, score-0.205]
</p><p>89 Right) Covariances across individual trials simulated by the model. [sent-196, score-0.187]
</p><p>90 4  Conclusion  Previous models of interval timing fail to explain its most striking feature - the collapse of the data when scaled by the time interval. [sent-199, score-0.454]
</p><p>91 We have presented a simple model of an accumulator clock based on spiking, noisy, linear neurons which produces this effect. [sent-200, score-0.973]
</p><p>92 The parameters are: T - the time for a spike on one neuron to excite spikes on connected neurons , mI - the average number of spikes excited externally at each short time interval T, and the variance of the spike transmission process, which in this model is (}"~. [sent-202, score-0.967]
</p><p>93 A weakness of this model is that it requires fine-tuning of a pair of parameters, so that the expected number of spikes grows in with external excitation only. [sent-203, score-0.181]
</p><p>94 Once a scalar clock is produced, simple reinforcement learning can be used to associate the clock signal with appropriate responses . [sent-204, score-1.385]
</p><p>95 A set of intermediate clock nodes was used to encode time. [sent-205, score-0.584]
</p><p>96 TD-'\ reinforcement learning between the intermediate nodes at reinforcement and an eligibility trace simulates peak procedure and the individual trial covariances. [sent-206, score-0.759]
</p><p>97 A neural network model of adaptively timed reinforcement learning and hippocampal dynamics. [sent-225, score-0.365]
</p><p>98 How time flies: Functional and neural mechansims of interval timing. [sent-232, score-0.255]
</p><p>99 Adaptively timed conditioned responses and the cerebellum: A neural network approach. [sent-245, score-0.196]
</p><p>100 Modelling scalar timing by an accumulator network of spiking neurons. [sent-258, score-0.791]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('clock', 0.485), ('accumulator', 0.409), ('animal', 0.293), ('scalar', 0.187), ('trials', 0.187), ('reinforcement', 0.152), ('interval', 0.142), ('response', 0.117), ('spikes', 0.115), ('stop', 0.115), ('time', 0.113), ('stimulus', 0.112), ('vj', 0.108), ('church', 0.102), ('covariation', 0.102), ('gibbon', 0.102), ('activity', 0.09), ('manchester', 0.089), ('trial', 0.087), ('spike', 0.085), ('conditioning', 0.081), ('peak', 0.081), ('nodes', 0.069), ('node', 0.068), ('respond', 0.068), ('network', 0.067), ('spiking', 0.066), ('timing', 0.062), ('eligibility', 0.061), ('intervals', 0.061), ('responding', 0.06), ('reward', 0.059), ('striking', 0.058), ('simulations', 0.058), ('property', 0.057), ('neuron', 0.057), ('cumulant', 0.057), ('cumulants', 0.057), ('moore', 0.056), ('covariances', 0.056), ('recursion', 0.053), ('times', 0.053), ('start', 0.052), ('bradford', 0.051), ('desmond', 0.051), ('tapped', 0.051), ('timed', 0.051), ('neurons', 0.051), ('john', 0.051), ('responds', 0.051), ('generating', 0.049), ('responses', 0.047), ('td', 0.046), ('collapses', 0.044), ('invokes', 0.044), ('shapiro', 0.044), ('ith', 0.044), ('hi', 0.042), ('connectionist', 0.042), ('denoted', 0.042), ('si', 0.042), ('scaled', 0.041), ('obeys', 0.04), ('grossberg', 0.04), ('subjective', 0.04), ('russell', 0.04), ('learn', 0.04), ('external', 0.038), ('pavlovian', 0.038), ('collapse', 0.038), ('adaptively', 0.038), ('connected', 0.037), ('trace', 0.037), ('fixed', 0.036), ('experiments', 0.035), ('simulates', 0.034), ('aij', 0.034), ('statistically', 0.033), ('measured', 0.031), ('receives', 0.031), ('aspects', 0.031), ('conditioned', 0.031), ('reproduces', 0.031), ('wij', 0.031), ('produced', 0.031), ('association', 0.03), ('spatially', 0.03), ('delay', 0.03), ('intermediate', 0.03), ('xi', 0.029), ('learning', 0.029), ('rise', 0.029), ('animals', 0.029), ('mi', 0.029), ('model', 0.028), ('classical', 0.028), ('connectivity', 0.027), ('stationary', 0.027), ('procedure', 0.027), ('average', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="160-tfidf-1" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>Author: Jonathan L. Shapiro, J. Wearden</p><p>Abstract: Animal data on delayed-reward conditioning experiments shows a striking property - the data for different time intervals collapses into a single curve when the data is scaled by the time interval. This is called the scalar property of interval timing. Here a simple model of a neural clock is presented and shown to give rise to the scalar property. The model is an accumulator consisting of noisy, linear spiking neurons. It is analytically tractable and contains only three parameters. When coupled with reinforcement learning it simulates peak procedure experiments, producing both the scalar property and the pattern of single trial covariances. 1</p><p>2 0.14480118 <a title="160-tfidf-2" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>Author: Aaron C. Courville, David S. Touretzky</p><p>Abstract: The Temporal Coding Hypothesis of Miller and colleagues [7] suggests that animals integrate related temporal patterns of stimuli into single memory representations. We formalize this concept using quasi-Bayes estimation to update the parameters of a constrained hidden Markov model. This approach allows us to account for some surprising temporal effects in the second order conditioning experiments of Miller et al. [1 , 2, 3], which other models are unable to explain. 1</p><p>3 0.12922886 <a title="160-tfidf-3" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>Author: B. D. Wright, Kamal Sen, William Bialek, A. J. Doupe</p><p>Abstract: In nature, animals encounter high dimensional sensory stimuli that have complex statistical and dynamical structure. Attempts to study the neural coding of these natural signals face challenges both in the selection of the signal ensemble and in the analysis of the resulting neural responses. For zebra ﬁnches, naturalistic stimuli can be deﬁned as sounds that they encounter in a colony of conspeciﬁc birds. We assembled an ensemble of these sounds by recording groups of 10-40 zebra ﬁnches, and then analyzed the response of single neurons in the songbird central auditory area (ﬁeld L) to continuous playback of long segments from this ensemble. Following methods developed in the ﬂy visual system, we measured the information that spike trains provide about the acoustic stimulus without any assumptions about which features of the stimulus are relevant. Preliminary results indicate that large amounts of information are carried by spike timing, with roughly half of the information accessible only at time resolutions better than 10 ms; additional information is still being revealed as time resolution is improved to 2 ms. Information can be decomposed into that carried by the locking of individual spikes to the stimulus (or modulations of spike rate) vs. that carried by timing in spike patterns. Initial results show that in ﬁeld L, temporal patterns give at least % extra information. Thus, single central auditory neurons can provide an informative representation of naturalistic sounds, in which spike timing may play a signiﬁcant role.   </p><p>4 0.11668295 <a title="160-tfidf-4" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>Author: Julian Eggert, Berthold BĂ¤uml</p><p>Abstract: Mesoscopical, mathematical descriptions of dynamics of populations of spiking neurons are getting increasingly important for the understanding of large-scale processes in the brain using simulations. In our previous work, integral equation formulations for population dynamics have been derived for a special type of spiking neurons. For Integrate- and- Fire type neurons , these formulations were only approximately correct. Here, we derive a mathematically compact, exact population dynamics formulation for Integrate- and- Fire type neurons. It can be shown quantitatively in simulations that the numerical correspondence with microscopically modeled neuronal populations is excellent. 1 Introduction and motivation The goal of the population dynamics approach is to model the time course of the collective activity of entire populations of functionally and dynamically similar neurons in a compact way, using a higher descriptionallevel than that of single neurons and spikes. The usual observable at the level of neuronal populations is the populationaveraged instantaneous firing rate A(t), with A(t)6.t being the number of neurons in the population that release a spike in an interval [t, t+6.t). Population dynamics are formulated in such a way, that they match quantitatively the time course of a given A(t), either gained experimentally or by microscopical, detailed simulation. At least three main reasons can be formulated which underline the importance of the population dynamics approach for computational neuroscience. First, it enables the simulation of extensive networks involving a massive number of neurons and connections, which is typically the case when dealing with biologically realistic functional models that go beyond the single neuron level. Second, it increases the analytical understanding of large-scale neuronal dynamics , opening the way towards better control and predictive capabilities when dealing with large networks. Third, it enables a systematic embedding of the numerous neuronal models operating at different descriptional scales into a generalized theoretic framework, explaining the relationships, dependencies and derivations of the respective models. Early efforts on population dynamics approaches date back as early as 1972, to the work of Wilson and Cowan [8] and Knight [4], which laid the basis for all current population-averaged graded-response models (see e.g. [6] for modeling work using these models). More recently, population-based approaches for spiking neurons were developed, mainly by Gerstner [3, 2] and Knight [5]. In our own previous work [1], we have developed a theoretical framework which enables to systematize and simulate a wide range of models for population-based dynamics. It was shown that the equations of the framework produce results that agree quantitatively well with detailed simulations using spiking neurons, so that they can be used for realistic simulations involving networks with large numbers of spiking neurons. Nevertheless, for neuronal populations composed of Integrate-and-Fire (I&F;) neurons, this framework was only correct in an approximation. In this paper, we derive the exact population dynamics formulation for I&F; neurons. This is achieved by reducing the I&F; population dynamics to a point process and by taking advantage of the particular properties of I&F; neurons. 2 2.1 Background: Integrate-and-Fire dynamics Differential form We start with the standard Integrate- and- Fire (I&F;) model in form of the wellknown differential equation [7] (1) which describes the dynamics of the membrane potential Vi of a neuron i that is modeled as a single compartment with RC circuit characteristics. The membrane relaxation time is in this case T = RC with R being the membrane resistance and C the membrane capacitance. The resting potential v R est is the stationary potential that is approached in the no-input case. The input arriving from other neurons is described in form of a current ji. In addition to eq. (1), which describes the integrate part of the I&F; model, the neuronal dynamics are completed by a nonlinear step. Every time the membrane potential Vi reaches a fixed threshold () from below, Vi is lowered by a fixed amount Ll > 0, and from the new value of the membrane potential integration according to eq. (1) starts again. if Vi(t) = () (from below) . (2) At the same time, it is said that the release of a spike occurred (i.e., the neuron fired), and the time ti = t of this singular event is stored. Here ti indicates the time of the most recent spike. Storing all the last firing times , we gain the sequence of spikes {t{} (spike ordering index j, neuronal index i). 2.2 Integral form Now we look at the single neuron in a neuronal compound. We assume that the input current contribution ji from presynaptic spiking neurons can be described using the presynaptic spike times tf, a response-function ~ and a connection weight WÂˇ . ',J ji(t) = Wi ,j ~(t - tf) (3) l: l: j f Integrating the I&F; equation (1) beginning at the last spiking time tT, which determines the initial condition by Vi(ti) = vi(ti - 0) - 6., where vi(ti - 0) is the membrane potential just before the neuron spikes, we get 1 Vi(t) = v Rest + fj(t - t:) + l: Wi ,j l: a(t - t:; t - tf) , j - Vi(t:)) e- S / T (4) f with the refractory function fj(s) = - (v Rest (5) and the alpha-function r ds</p><p>5 0.10816702 <a title="160-tfidf-5" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>Author: Shih-Chii Liu, Jörg Kramer, Giacomo Indiveri, Tobi Delbrück, Rodney J. Douglas</p><p>Abstract: We describe a programmable multi-chip VLSI neuronal system that can be used for exploring spike-based information processing models. The system consists of a silicon retina, a PIC microcontroller, and a transceiver chip whose integrate-and-ﬁre neurons are connected in a soft winner-take-all architecture. The circuit on this multi-neuron chip approximates a cortical microcircuit. The neurons can be conﬁgured for different computational properties by the virtual connections of a selected set of pixels on the silicon retina. The virtual wiring between the different chips is effected by an event-driven communication protocol that uses asynchronous digital pulses, similar to spikes in a neuronal system. We used the multi-chip spike-based system to synthesize orientation-tuned neurons using both a feedforward model and a feedback model. The performance of our analog hardware spiking model matched the experimental observations and digital simulations of continuous-valued neurons. The multi-chip VLSI system has advantages over computer neuronal models in that it is real-time, and the computational time does not scale with the size of the neuronal network.</p><p>6 0.097421885 <a title="160-tfidf-6" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>7 0.094712064 <a title="160-tfidf-7" href="./nips-2001-A_Rational_Analysis_of_Cognitive_Control_in_a_Speeded_Discrimination_Task.html">18 nips-2001-A Rational Analysis of Cognitive Control in a Speeded Discrimination Task</a></p>
<p>8 0.082148738 <a title="160-tfidf-8" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>9 0.078684255 <a title="160-tfidf-9" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>10 0.077990189 <a title="160-tfidf-10" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>11 0.07295163 <a title="160-tfidf-11" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>12 0.070759833 <a title="160-tfidf-12" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>13 0.063646354 <a title="160-tfidf-13" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>14 0.063573405 <a title="160-tfidf-14" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>15 0.063148782 <a title="160-tfidf-15" href="./nips-2001-Citcuits_for_VLSI_Implementation_of_Temporally_Asymmetric_Hebbian_Learning.html">49 nips-2001-Citcuits for VLSI Implementation of Temporally Asymmetric Hebbian Learning</a></p>
<p>16 0.062127922 <a title="160-tfidf-16" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>17 0.060554668 <a title="160-tfidf-17" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>18 0.057794049 <a title="160-tfidf-18" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>19 0.05680887 <a title="160-tfidf-19" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>20 0.056341872 <a title="160-tfidf-20" href="./nips-2001-Classifying_Single_Trial_EEG%3A_Towards_Brain_Computer_Interfacing.html">50 nips-2001-Classifying Single Trial EEG: Towards Brain Computer Interfacing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.17), (1, -0.212), (2, -0.024), (3, 0.001), (4, 0.043), (5, 0.016), (6, 0.047), (7, -0.013), (8, -0.022), (9, -0.022), (10, 0.018), (11, 0.034), (12, -0.109), (13, -0.0), (14, 0.004), (15, -0.015), (16, 0.019), (17, 0.065), (18, 0.106), (19, 0.023), (20, -0.173), (21, -0.14), (22, -0.026), (23, 0.048), (24, -0.041), (25, 0.064), (26, 0.08), (27, -0.008), (28, -0.011), (29, -0.025), (30, 0.023), (31, -0.097), (32, -0.015), (33, -0.081), (34, 0.045), (35, -0.157), (36, 0.04), (37, -0.037), (38, 0.001), (39, -0.131), (40, -0.026), (41, 0.123), (42, 0.006), (43, -0.038), (44, 0.071), (45, 0.042), (46, -0.136), (47, -0.016), (48, -0.029), (49, -0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94781303 <a title="160-lsi-1" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>Author: Jonathan L. Shapiro, J. Wearden</p><p>Abstract: Animal data on delayed-reward conditioning experiments shows a striking property - the data for different time intervals collapses into a single curve when the data is scaled by the time interval. This is called the scalar property of interval timing. Here a simple model of a neural clock is presented and shown to give rise to the scalar property. The model is an accumulator consisting of noisy, linear spiking neurons. It is analytically tractable and contains only three parameters. When coupled with reinforcement learning it simulates peak procedure experiments, producing both the scalar property and the pattern of single trial covariances. 1</p><p>2 0.64497459 <a title="160-lsi-2" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>Author: Peter Dayan</p><p>Abstract: The standard reinforcement learning view of the involvement of neuromodulatory systems in instrumental conditioning includes a rather straightforward conception of motivation as prediction of sum future reward. Competition between actions is based on the motivating characteristics of their consequent states in this sense. Substantial, careful, experiments reviewed in Dickinson & Balleine, 12,13 into the neurobiology and psychology of motivation shows that this view is incomplete. In many cases, animals are faced with the choice not between many different actions at a given state, but rather whether a single response is worth executing at all. Evidence suggests that the motivational process underlying this choice has different psychological and neural properties from that underlying action choice. We describe and model these motivational systems, and consider the way they interact.</p><p>3 0.58744437 <a title="160-lsi-3" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>Author: Aaron C. Courville, David S. Touretzky</p><p>Abstract: The Temporal Coding Hypothesis of Miller and colleagues [7] suggests that animals integrate related temporal patterns of stimuli into single memory representations. We formalize this concept using quasi-Bayes estimation to update the parameters of a constrained hidden Markov model. This approach allows us to account for some surprising temporal effects in the second order conditioning experiments of Miller et al. [1 , 2, 3], which other models are unable to explain. 1</p><p>4 0.55176347 <a title="160-lsi-4" href="./nips-2001-A_Rational_Analysis_of_Cognitive_Control_in_a_Speeded_Discrimination_Task.html">18 nips-2001-A Rational Analysis of Cognitive Control in a Speeded Discrimination Task</a></p>
<p>Author: Michael C. Mozer, Michael D. Colagrosso, David E. Huber</p><p>Abstract: We are interested in the mechanisms by which individuals monitor and adjust their performance of simple cognitive tasks. We model a speeded discrimination task in which individuals are asked to classify a sequence of stimuli (Jones & Braver, 2001). Response conﬂict arises when one stimulus class is infrequent relative to another, resulting in more errors and slower reaction times for the infrequent class. How do control processes modulate behavior based on the relative class frequencies? We explain performance from a rational perspective that casts the goal of individuals as minimizing a cost that depends both on error rate and reaction time. With two additional assumptions of rationality—that class prior probabilities are accurately estimated and that inference is optimal subject to limitations on rate of information transmission—we obtain a good ﬁt to overall RT and error data, as well as trial-by-trial variations in performance. Consider the following scenario: While driving, you approach an intersection at which the trafﬁc light has already turned yellow, signaling that it is about to turn red. You also notice that a car is approaching you rapidly from behind, with no indication of slowing. Should you stop or speed through the intersection? The decision is difﬁcult due to the presence of two conﬂicting signals. Such response conﬂict can be produced in a psychological laboratory as well. For example, Stroop (1935) asked individuals to name the color of ink on which a word is printed. When the words are color names incongruous with the ink color— e.g., “blue” printed in red—reaction times are slower and error rates are higher. We are interested in the control mechanisms underlying performance of high-conﬂict tasks. Conﬂict requires individuals to monitor and adjust their behavior, possibly responding more slowly if errors are too frequent. In this paper, we model a speeded discrimination paradigm in which individuals are asked to classify a sequence of stimuli (Jones & Braver, 2001). The stimuli are letters of the alphabet, A–Z, presented in rapid succession. In a choice task, individuals are asked to press one response key if the letter is an X or another response key for any letter other than X (as a shorthand, we will refer to non-X stimuli as Y). In a go/no-go task, individuals are asked to press a response key when X is presented and to make no response otherwise. We address both tasks because they elicit slightly different decision-making behavior. In both tasks, Jones and Braver (2001) manipulated the relative frequency of the X and Y stimuli; the ratio of presentation frequency was either 17:83, 50:50, or 83:17. Response conﬂict arises when the two stimulus classes are unbalanced in frequency, resulting in more errors and slower reaction times. For example, when X’s are frequent but Y is presented, individuals are predisposed toward producing the X response, and this predisposition must be overcome by the perceptual evidence from the Y. Jones and Braver (2001) also performed an fMRI study of this task and found that anterior cingulate cortex (ACC) becomes activated in situations involving response conﬂict. Specifically, when one stimulus occurs infrequently relative to the other, event-related fMRI response in the ACC is greater for the low frequency stimulus. Jones and Braver also extended a neural network model of Botvinick, Braver, Barch, Carter, and Cohen (2001) to account for human performance in the two discrimination tasks. The heart of the model is a mechanism that monitors conﬂict—the posited role of the ACC—and adjusts response biases accordingly. In this paper, we develop a parsimonious alternative account of the role of the ACC and of how control processes modulate behavior when response conﬂict arises. 1 A RATIONAL ANALYSIS Our account is based on a rational analysis of human cognition, which views cognitive processes as being optimized with respect to certain task-related goals, and being adaptive to the structure of the environment (Anderson, 1990). We make three assumptions of rationality: (1) perceptual inference is optimal but is subject to rate limitations on information transmission, (2) response class prior probabilities are accurately estimated, and (3) the goal of individuals is to minimize a cost that depends both on error rate and reaction time. The heart of our account is an existing probabilistic model that explains a variety of facilitation effects that arise from long-term repetition priming (Colagrosso, in preparation; Mozer, Colagrosso, & Huber, 2000), and more broadly, that addresses changes in the nature of information transmission in neocortex due to experience. We give a brief overview of this model; the details are not essential for the present work. The model posits that neocortex can be characterized by a collection of informationprocessing pathways, and any act of cognition involves coordination among pathways. To model a simple discrimination task, we might suppose a perceptual pathway to map the visual input to a semantic representation, and a response pathway to map the semantic representation to a response. The choice and go/no-go tasks described earlier share a perceptual pathway, but require different response pathways. The model is framed in terms of probability theory: pathway inputs and outputs are random variables and microinference in a pathway is carried out by Bayesian belief revision.   To elaborate, consider a pathway whose input at time is a discrete random variable, denoted , which can assume values corresponding to alternative input states. Similarly, the output of the pathway at time is a discrete random variable, denoted , which can assume values . For example, the input to the perceptual pathway in the discrimination task is one of visual patterns corresponding to the letters of the alphabet, and the output is one of letter identities. (This model is highly abstract: the visual patterns are enumerated, but the actual pixel patterns are not explicitly represented in the model. Nonetheless, the similarity structure among inputs can be captured, but we skip a discussion of this issue because it is irrelevant for the current work.) To present a particular input alternative, , to the model for time steps, we clamp for . The model computes a probability distribution over given , i.e., P . ¡ # 4 0 ©2' &  0 ' ! 1)(</p><p>5 0.53957599 <a title="160-lsi-5" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>Author: B. D. Wright, Kamal Sen, William Bialek, A. J. Doupe</p><p>Abstract: In nature, animals encounter high dimensional sensory stimuli that have complex statistical and dynamical structure. Attempts to study the neural coding of these natural signals face challenges both in the selection of the signal ensemble and in the analysis of the resulting neural responses. For zebra ﬁnches, naturalistic stimuli can be deﬁned as sounds that they encounter in a colony of conspeciﬁc birds. We assembled an ensemble of these sounds by recording groups of 10-40 zebra ﬁnches, and then analyzed the response of single neurons in the songbird central auditory area (ﬁeld L) to continuous playback of long segments from this ensemble. Following methods developed in the ﬂy visual system, we measured the information that spike trains provide about the acoustic stimulus without any assumptions about which features of the stimulus are relevant. Preliminary results indicate that large amounts of information are carried by spike timing, with roughly half of the information accessible only at time resolutions better than 10 ms; additional information is still being revealed as time resolution is improved to 2 ms. Information can be decomposed into that carried by the locking of individual spikes to the stimulus (or modulations of spike rate) vs. that carried by timing in spike patterns. Initial results show that in ﬁeld L, temporal patterns give at least % extra information. Thus, single central auditory neurons can provide an informative representation of naturalistic sounds, in which spike timing may play a signiﬁcant role.   </p><p>6 0.50476974 <a title="160-lsi-6" href="./nips-2001-Characterizing_Neural_Gain_Control_using_Spike-triggered_Covariance.html">48 nips-2001-Characterizing Neural Gain Control using Spike-triggered Covariance</a></p>
<p>7 0.49423599 <a title="160-lsi-7" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>8 0.48966193 <a title="160-lsi-8" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>9 0.48710802 <a title="160-lsi-9" href="./nips-2001-A_Maximum-Likelihood_Approach_to_Modeling_Multisensory_Enhancement.html">11 nips-2001-A Maximum-Likelihood Approach to Modeling Multisensory Enhancement</a></p>
<p>10 0.44541237 <a title="160-lsi-10" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>11 0.44271648 <a title="160-lsi-11" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>12 0.42251515 <a title="160-lsi-12" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>13 0.4157764 <a title="160-lsi-13" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>14 0.40929762 <a title="160-lsi-14" href="./nips-2001-Improvisation_and_Learning.html">91 nips-2001-Improvisation and Learning</a></p>
<p>15 0.39555544 <a title="160-lsi-15" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>16 0.38786483 <a title="160-lsi-16" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>17 0.38407391 <a title="160-lsi-17" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>18 0.38321185 <a title="160-lsi-18" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>19 0.38298991 <a title="160-lsi-19" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>20 0.35508403 <a title="160-lsi-20" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.037), (17, 0.014), (19, 0.04), (27, 0.118), (30, 0.095), (38, 0.045), (58, 0.255), (59, 0.021), (72, 0.058), (79, 0.061), (83, 0.043), (91, 0.136)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8562879 <a title="160-lda-1" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>Author: Antonio Torralba</p><p>Abstract: The most popular algorithms for object detection require the use of exhaustive spatial and scale search procedures. In such approaches, an object is defined by means of local features. fu this paper we show that including contextual information in object detection procedures provides an efficient way of cutting down the need for exhaustive search. We present results with real images showing that the proposed scheme is able to accurately predict likely object classes, locations and sizes. 1</p><p>same-paper 2 0.81443453 <a title="160-lda-2" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>Author: Jonathan L. Shapiro, J. Wearden</p><p>Abstract: Animal data on delayed-reward conditioning experiments shows a striking property - the data for different time intervals collapses into a single curve when the data is scaled by the time interval. This is called the scalar property of interval timing. Here a simple model of a neural clock is presented and shown to give rise to the scalar property. The model is an accumulator consisting of noisy, linear spiking neurons. It is analytically tractable and contains only three parameters. When coupled with reinforcement learning it simulates peak procedure experiments, producing both the scalar property and the pattern of single trial covariances. 1</p><p>3 0.80088592 <a title="160-lda-3" href="./nips-2001-A_kernel_method_for_multi-labelled_classification.html">22 nips-2001-A kernel method for multi-labelled classification</a></p>
<p>Author: André Elisseeff, Jason Weston</p><p>Abstract: This article presents a Support Vector Machine (SVM) like learning system to handle multi-label problems. Such problems are usually decomposed into many two-class problems but the expressive power of such a system can be weak [5, 7]. We explore a new direct approach. It is based on a large margin ranking system that shares a lot of common properties with SVMs. We tested it on a Yeast gene functional classiﬁcation problem with positive results.</p><p>4 0.64862454 <a title="160-lda-4" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>Author: Andrew D. Brown, Geoffrey E. Hinton</p><p>Abstract: Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's. 1</p><p>5 0.64831549 <a title="160-lda-5" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>6 0.64778346 <a title="160-lda-6" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>7 0.64709353 <a title="160-lda-7" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>8 0.64608586 <a title="160-lda-8" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>9 0.64541185 <a title="160-lda-9" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>10 0.64199662 <a title="160-lda-10" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>11 0.64163244 <a title="160-lda-11" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>12 0.64075935 <a title="160-lda-12" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>13 0.64071423 <a title="160-lda-13" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>14 0.64001441 <a title="160-lda-14" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>15 0.63946605 <a title="160-lda-15" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>16 0.63838112 <a title="160-lda-16" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>17 0.63750428 <a title="160-lda-17" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>18 0.63743597 <a title="160-lda-18" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>19 0.63632214 <a title="160-lda-19" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>20 0.63392907 <a title="160-lda-20" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
