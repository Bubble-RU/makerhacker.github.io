<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>95 nips-2001-Infinite Mixtures of Gaussian Process Experts</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-95" href="#">nips2001-95</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>95 nips-2001-Infinite Mixtures of Gaussian Process Experts</h1>
<br/><p>Source: <a title="nips-2001-95-pdf" href="http://papers.nips.cc/paper/2055-infinite-mixtures-of-gaussian-process-experts.pdf">pdf</a></p><p>Author: Carl E. Rasmussen, Zoubin Ghahramani</p><p>Abstract: We present an extension to the Mixture of Experts (ME) model, where the individual experts are Gaussian Process (GP) regression models. Using an input-dependent adaptation of the Dirichlet Process, we implement a gating network for an inﬁnite number of Experts. Inference in this model may be done efﬁciently using a Markov Chain relying on Gibbs sampling. The model allows the effective covariance function to vary with the inputs, and may handle large datasets – thus potentially overcoming two of the biggest hurdles with GP models. Simulations show the viability of this approach.</p><p>Reference: <a title="nips-2001-95-reference" href="../nips2001_reference/nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract We present an extension to the Mixture of Experts (ME) model, where the individual experts are Gaussian Process (GP) regression models. [sent-8, score-0.477]
</p><p>2 Using an input-dependent adaptation of the Dirichlet Process, we implement a gating network for an inﬁnite number of Experts. [sent-9, score-0.425]
</p><p>3 The model allows the effective covariance function to vary with the inputs, and may handle large datasets – thus potentially overcoming two of the biggest hurdles with GP models. [sent-11, score-0.103]
</p><p>4 First, because inference requires inversion of an the number of training data points, they are computationally impractical for large datasets. [sent-16, score-0.156]
</p><p>5 Second, the covariance function is commonly assumed to be stationary, limiting the modeling ﬂexibility. [sent-17, score-0.103]
</p><p>6 For example, if the noise variance is different in different parts of the input space, or if the function has a discontinuity, a stationary covariance function will not be adequate. [sent-18, score-0.277]
</p><p>7 Goldberg et al [1998] discussed the case of input dependent noise variance. [sent-19, score-0.095]
</p><p>8 These methods are based on selecting a projection of the covariance matrix onto a smaller subspace (e. [sent-21, score-0.103]
</p><p>9 There have also been attempts at deriving more complex covariance functions [Gibbs 1997] although it can be difﬁcult to decide a priori on a covariance function of sufﬁcient complexity which guarantees positive deﬁniteness. [sent-24, score-0.206]
</p><p>10 In this paper we will simultaneously address both the problem of computational complexity and the deﬁciencies in covariance functions using a divide and conquer strategy inspired by the Mixture of Experts (ME) architecture [Jacobs et al, 1991]. [sent-25, score-0.131]
</p><p>11 In this model the input  space is (probabilistically) divided by a gating network into regions within which speciﬁc separate experts make predictions. [sent-26, score-0.86]
</p><p>12 Of course, as in the ME, the learning of the experts and the gating network are intimately coupled. [sent-28, score-0.86]
</p><p>13 Unfortunately, it may be (practically and statistically) difﬁcult to infer the appropriate number of experts for a particular dataset. [sent-29, score-0.463]
</p><p>14 In the current paper we sidestep this difﬁcult problem by using an inﬁnite number of experts and employing a gating network related to the Dirichlet Process, to specify a spatially varying Dirichlet Process. [sent-30, score-0.86]
</p><p>15 An inﬁnite number of experts may also in many cases be more faithful to our prior expectations about complex real-word datasets. [sent-31, score-0.473]
</p><p>16 Integrating over the posterior distribution for the parameters is carried out using a Markov Chain Monte Carlo approach. [sent-32, score-0.085]
</p><p>17 In his approach both the experts and the gating network were implemented with GPs; the gating network being a softmax of GPs. [sent-34, score-1.285]
</p><p>18 2 Inﬁnite GP mixtures The traditional ME likelihood does not apply when the experts are non-parametric. [sent-36, score-0.513]
</p><p>19    © ¦ ¤¢   #     ¤  ¨  where and are inputs and outputs (boldface denotes vectors), are the parameters are the discrete indicator of expert , are the parameters of the gating network and variables assigning data points to experts. [sent-38, score-0.808]
</p><p>20 There is a joint distribution corresponding to every possible assignment of data points to experts; therefore the likelihood is a sum over (exponentially many) assignments:  (1)  a5234©¨ ¦ED¢£`Y ©BUA(" ' 0RWP¦VA) ' RQIH#  G8 ! [sent-40, score-0.175]
</p><p>21    © ¦ ¤¢ 24¨ ED£CBA@ §1£9$76¨ §1£¡  f '© a a a d '¢  g`6ReeR© Dcb@  Given the conﬁguration , the distribution factors into the product, over experts, of the joint Gaussian distribution of all data points assigned to each expert. [sent-42, score-0.224]
</p><p>22 Whereas the original ME formulation used expectations of assignment variables called responsibilities, this is inadequate for inference in the mixture of GP experts. [sent-43, score-0.144]
</p><p>23 We defer discussion of the second term deﬁning the gating network to the next section. [sent-46, score-0.425]
</p><p>24 As discussed, the ﬁrst term being the likelihood given the indicators factors into independent terms for each expert. [sent-47, score-0.114]
</p><p>25 Thus, for the GP expert, we compute the above conditional density by simply evaluating the GP on the data assigned to it. [sent-49, score-0.171]
</p><p>26 Although this equation looks computationally expensive, we can keep track of the inverse covariance matrices and reuse them for consecutive Gibbs updates by performing rank one updates (since Gibbs sampling changes at most one indicator at a time). [sent-50, score-0.353]
</p><p>27 We are free to choose any valid covariance function for the experts. [sent-51, score-0.103]
</p><p>28 In our simulations we employed the following Gaussian covariance function:  § A9 B@@§  H 1s7! [sent-52, score-0.103]
</p><p>29 6  controlling the signal variance, controlling the noise variance, with hyperparameters and controlling the length scale or (inverse) relevance of the -th dimension of in if , o. [sent-57, score-0.4]
</p><p>30 0  P  3   G u © u ¢ F  Gu  u  A  F &  3 The Gating network The gating network assigns probability to different experts based entirely on the input. [sent-62, score-0.92]
</p><p>31 We will derive a gating network based on the Dirichlet Process which can be deﬁned as the limit of a Dirichlet distribution when the number of classes tends to inﬁnity. [sent-63, score-0.456]
</p><p>32 The standard Dirichlet Process is not input dependent, but we will modify it to serve as a gating mechanism. [sent-64, score-0.365]
</p><p>33 We start from a symmetric Dirichlet distribution on proportions:  d iS #  h U i  p9 R¢  h U¢ f %d b `W YW V    ¦ S a Q¢ sr Q t&q;#  S TR¢ i q 79 g(8eca¦8XETU gTQ a a e© d R£¡  U  © #  U  where is the (positive) concentration parameter. [sent-65, score-0.124]
</p><p>34 These length scales allow dimensions of space to be more or less relevant to the gating network classiﬁcation. [sent-71, score-0.503]
</p><p>35 0  We Gibbs sample from the indicator variables by multiplying the input-dependent Dirichlet process prior eq. [sent-72, score-0.272]
</p><p>36 Gibbs sampling in an inﬁnite model requires that the indicator variables can take on values that no other indicator variable has already taken, thereby creating new experts. [sent-75, score-0.347]
</p><p>37 In this approach hyperparameters for new experts are sampled from their prior and the likelihood is evaluated based on these. [sent-77, score-0.678]
</p><p>38 If all data points are assigned to a single GP, the likelihood calculation will still be cubic in the number of data points (per Gibbs sweep over all indicators). [sent-83, score-0.342]
</p><p>39 We can reduce the computational complexity by introducing the constraint that no GP expert can have more than 2 max data points assigned to it. [sent-84, score-0.278]
</p><p>40 U  The hyperparameter controls the prior probability of assigning a data point to a new expert, and therefore inﬂuences the total number of experts used to model the data. [sent-86, score-0.507]
</p><p>41 As in Rasmussen [2000], we give a vague inverse gamma prior to , and sample from its posterior using Adaptive Rejection Sampling (ARS) [Gilks & Wild, 1992]. [sent-87, score-0.255]
</p><p>42 U  U  Finally we need to do inference for the parameters of the gating function. [sent-89, score-0.426]
</p><p>43 Given a set of indicator variables one could use standard methods from kernel classiﬁcation to optimize the kernel widths in different directions. [sent-90, score-0.322]
</p><p>44 These methods typically optimize the leave-oneout pseudo-likelihood (ie the product of the conditionals), since computing the likelihood in a model deﬁned purely from conditional distributions as in eq. [sent-91, score-0.082]
</p><p>45 4 The Algorithm  D  The individual GP experts are given a stationary Gaussian covariance function, with a single length scale per dimension, a signal variance and a noise variance, i. [sent-94, score-0.785]
</p><p>46 (where is the dimension of the input) hyperparameters per expert, eq. [sent-96, score-0.197]
</p><p>47 The signal and noise variances are given inverse gamma priors with hyper-hypers and (separately for the two variances). [sent-98, score-0.162]
</p><p>48 This serves to couple the hyperparameters between experts, and allows the priors on and (which are used when evaluating auxiliary classes) to adapt. [sent-99, score-0.166]
</p><p>49 Finally we give vague independent log normal priors to the lenght scale paramters and . [sent-100, score-0.119]
</p><p>50 The algorithm for learning an inﬁnite mixture of GP experts consists of the following steps:    '  1. [sent-103, score-0.49]
</p><p>51 Initialize indicator variables to a single value (or a few values if individual GPs are to be kept small for computational reasons). [sent-104, score-0.19]
</p><p>52 Do Hybrid Monte Carlo (HMC) [Duane et al, 1987] for hyperparameters of the GP covariance function, , for each expert in turn. [sent-108, score-0.413]
</p><p>53 On the right hand plot we show samples from the posterior distribution for the iMGPE of the (noise free) function evaluated intervals of 1 ms. [sent-118, score-0.195]
</p><p>54 We have jittered the points in the plot along the time dimension by adding uniform ms noise, so that the density can be seen more easily. [sent-119, score-0.225]
</p><p>55 Sample the gating kernel widths, ; we use the Metropolis method to sample from the pseudo-posterior with a Gaussian proposal ﬁt at the current 3  3  7. [sent-122, score-0.462]
</p><p>56 5 Simulations on a simple real-world data set To illustrate our algorithm, we used the motorcycle dataset, ﬁg. [sent-124, score-0.125]
</p><p>57 We noticed that the raw data is discretized into bins of size g; accordingly we cut off the prior for the noise variance at . [sent-127, score-0.182]
</p><p>58 5a  § ¨¦  3  3 5 9§  ¦  The model is able to capture the general shape of the function and also the input-dependent nature of the noise (ﬁg. [sent-128, score-0.097]
</p><p>59 Whereas the medians of the predictive distributions agree to a large extent (left hand plot), we see a huge difference in the predictive distributions (right hand). [sent-134, score-0.177]
</p><p>60 The homoscedastic GP cannot capture the very tight distribution for ms offered by iMGPE. [sent-135, score-0.109]
</p><p>61 Note that the predictive distribution of the function is multimodal, for example, around time 35 ms. [sent-137, score-0.097]
</p><p>62 Multimodal predictive distributions could in principle be obtained from an ordinary GP by integrating over hyperparameters, however, in a mixture of GP’s model they can arise naturally. [sent-138, score-0.155]
</p><p>63 The predictive distribution of the function appears to have signiﬁcant mass around g which seems somewhat artifactual. [sent-139, score-0.165]
</p><p>64 The     x  3  x  3   ©  x tv   ©  ©  x  3 The Gaussian ﬁt uses the derivative and Hessian of the log posterior wrt the log length scales. [sent-141, score-0.203]
</p><p>65 The data have been sorted from left-toright according to the value of the time variable (since the data is not equally spaced in time the axis of this matrix cannot be aligned with the plot in ﬁg. [sent-145, score-0.133]
</p><p>66 The right hand plot shows a histogram over the 100 samples of the number of GP experts used to model the data. [sent-147, score-0.545]
</p><p>67 Gaussian processes had zero mean a priori, which coupled with the concentration of data around zero may explain the posterior mass at zero. [sent-148, score-0.286]
</p><p>68 It would be more natural to treat the GP means as separate hyperparameters controlled by a hyper-hyperparameter (centered at zero) and do inference on them, rather than ﬁx them all at 0. [sent-149, score-0.227]
</p><p>69 Although the scatter of data from the predictive distribution for iMGPE looks somewhat messy with multimodality etc, it is important to note that it assigns high density to regions that seem probable. [sent-150, score-0.168]
</p><p>70 The motorcycle data appears to have roughly three regions: a ﬂat low-noise region, followed by a curved region, and a ﬂat high noise region. [sent-151, score-0.223]
</p><p>71 (left) shows the number of times two data points were assigned to the same expert. [sent-155, score-0.134]
</p><p>72 A clearly deﬁned block captures the initial ﬂat region and a few other points that lie near the g line; the middle block captures the curved region, with a more gradual transition to the last ﬂat region. [sent-156, score-0.182]
</p><p>73 A histogram of the number of GP experts used shows that the posterior distribution of number of needed GPs has a broad peak between and , where less than 3 occupied experts is very unlikely, and above becoming progressively less likely. [sent-157, score-1.023]
</p><p>74 Note that it never uses just a single GP to model the data which accords with the intuition that a single stationary covariance function would be inadequate. [sent-158, score-0.257]
</p><p>75 We should point out that the model is not trying to do model selection between ﬁnite GP mixtures, but rather always assumes that there are inﬁnitely many available, most of which contribute with small mass 4 to a diffuse density in the background. [sent-159, score-0.105]
</p><p>76 4  10  frequency  auto correlation coefficient  log number of occupied experts log gating kernel width log Dirichlet concentration  frequency  10 1  0. [sent-167, score-1.232]
</p><p>77 5 log (base 10) gating function kernel width  5 0  −0. [sent-170, score-0.532]
</p><p>78 5 1 log (base 10) Dirichlet process concentration  xx 7tx  Figure 3: The left hand plot shows the auto-correlation for various parameters of the model based on iterations. [sent-172, score-0.33]
</p><p>79 The right hand plots show the distribution of the (log) kernel width and (log) Dirichlet concentration parameter , based on samples from the posterior. [sent-173, score-0.284]
</p><p>80 x 7x  U  3  3  3 73 3  3  x  and the concentration parameter of the Dirichlet process. [sent-174, score-0.093]
</p><p>81 The posterior 6 kernel width lies between and ; comparing to the scale of the inputs these are quite short distances, corresponding to rapid transitions between experts (as opposed to lengthy intervals with multiple active experts). [sent-175, score-0.604]
</p><p>82 There are four ways in which the inﬁnite mixture of GP experts differs from, and we believe, improves upon the model presented by Tresp. [sent-178, score-0.49]
</p><p>83 First, in his model, although a gating network divides up the input space, each GP expert predicts on the basis of all of the data. [sent-179, score-0.569]
</p><p>84 Data that was not assigned to a GP expert can therefore spill over into predictions of a GP, which will lead to bias near region boundaries especially for experts with long length scales. [sent-180, score-0.728]
</p><p>85 Second, if there are experts, Tresp’s model has GPs (the experts, noise models, and separate gating functions) each of which requires computations over the entire dataset resulting in computations. [sent-181, score-0.499]
</p><p>86 In our model since the experts divide up the data points, if there are experts equally dividing the data an iteration takes computations (each of Gibbs updates requires a rank-one computation for each of the experts and the Hybrid Monte Carlo takes times ). [sent-182, score-1.401]
</p><p>87 Inference for the gating length scale parameters is if the full Hessian is used, but can be reduced to for a diagonal approximation, or Hybrid Monte Carlo if the input dimension is large. [sent-186, score-0.441]
</p><p>88 Third, by going to the Dirichlet process inﬁnite limit, we allow the model to infer the number of components required to capture the data. [sent-187, score-0.097]
</p><p>89 Finally, in our model the GP hyperparameters are not ﬁxed but are instead inferred from the data. [sent-188, score-0.166]
</p><p>90 R¡¢      ¢       x  §     §       ¢3     £ ¤             §  ¢    ¡   9 ¡  ¢    §   9 §  ¢      9 ¡  ¢      We have deﬁned the gating network prior implicitly in terms of the conditional distribution of an indicator variable given all the other indicator variables. [sent-189, score-0.805]
</p><p>91 Speciﬁcally, the distribution of this indicator variable is an input-dependent Dirichlet process with counts given by local estimates of the data density in each class eq. [sent-190, score-0.276]
</p><p>92 If indeed there does not exist a single consistent joint distribution the Gibbs sampler may converge to different distributions depending on the order of sampling. [sent-193, score-0.087]
</p><p>93 We are encouraged by the preliminary results obtained on the motorcycle data. [sent-194, score-0.091]
</p><p>94 We have argued here that single iterations of the MCMC inference are computationally tractable even for large data sets, experiments will show whether mixing is sufﬁciently rapid to allow practical application. [sent-196, score-0.185]
</p><p>95 We hope that the extra ﬂexibility of the effective covariance function will turn out to improve performance. [sent-197, score-0.103]
</p><p>96 Also, the automatic choice of the number of experts may make the model advantageous for practical modeling tasks. [sent-198, score-0.435]
</p><p>97 The computational problem in doing inference and prediction using Gaussian Processes arises out of the unrealistic assumption that a single covariance function captures the behavior of the data over its entire range. [sent-200, score-0.291]
</p><p>98 This leads to a cumbersome matrix inversion over the entire data set. [sent-201, score-0.1]
</p><p>99 Instead we ﬁnd that by making a more realistic assumption, that the data can be modeled by an inﬁnite mixture of local Gaussian processes, the computational problem also decomposes into smaller matrix inversions. [sent-202, score-0.089]
</p><p>100 Markov chain sampling methods for Dirichlet process mixture models. [sent-248, score-0.207]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gp', 0.453), ('experts', 0.435), ('gating', 0.365), ('dirichlet', 0.199), ('gibbs', 0.199), ('hyperparameters', 0.166), ('expert', 0.144), ('indicator', 0.134), ('rasmussen', 0.126), ('gps', 0.108), ('covariance', 0.103), ('concentration', 0.093), ('imgpe', 0.091), ('motorcycle', 0.091), ('nite', 0.076), ('indicators', 0.075), ('mass', 0.068), ('occupied', 0.068), ('noise', 0.068), ('vague', 0.067), ('williams', 0.067), ('predictive', 0.066), ('plot', 0.065), ('kernel', 0.065), ('stationary', 0.064), ('inference', 0.061), ('chain', 0.061), ('network', 0.06), ('assigned', 0.057), ('mixture', 0.055), ('occupation', 0.054), ('posterior', 0.054), ('gaussian', 0.054), ('log', 0.052), ('carlo', 0.052), ('monte', 0.052), ('sampling', 0.051), ('width', 0.05), ('ms', 0.049), ('region', 0.047), ('hybrid', 0.047), ('nips', 0.046), ('duane', 0.046), ('hmc', 0.046), ('silverman', 0.046), ('length', 0.045), ('hand', 0.045), ('conditional', 0.043), ('points', 0.043), ('variance', 0.042), ('regression', 0.042), ('process', 0.04), ('gilks', 0.04), ('wild', 0.04), ('tresp', 0.039), ('mixtures', 0.039), ('likelihood', 0.039), ('prior', 0.038), ('processes', 0.037), ('density', 0.037), ('inverse', 0.036), ('conditionals', 0.036), ('multimodal', 0.036), ('acceleration', 0.036), ('xx', 0.035), ('integrating', 0.034), ('data', 0.034), ('entire', 0.034), ('rejection', 0.034), ('goldberg', 0.034), ('scales', 0.033), ('mixing', 0.033), ('sample', 0.032), ('dataset', 0.032), ('etc', 0.032), ('inversion', 0.032), ('cubic', 0.032), ('sweep', 0.032), ('captures', 0.031), ('distribution', 0.031), ('dimension', 0.031), ('controlling', 0.03), ('seeger', 0.03), ('curved', 0.03), ('jacobs', 0.03), ('widths', 0.03), ('variances', 0.03), ('cult', 0.029), ('capture', 0.029), ('computationally', 0.029), ('exibility', 0.029), ('dif', 0.028), ('infer', 0.028), ('single', 0.028), ('variables', 0.028), ('distances', 0.028), ('divide', 0.028), ('gamma', 0.028), ('joint', 0.028), ('al', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="95-tfidf-1" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>Author: Carl E. Rasmussen, Zoubin Ghahramani</p><p>Abstract: We present an extension to the Mixture of Experts (ME) model, where the individual experts are Gaussian Process (GP) regression models. Using an input-dependent adaptation of the Dirichlet Process, we implement a gating network for an inﬁnite number of Experts. Inference in this model may be done efﬁciently using a Markov Chain relying on Gibbs sampling. The model allows the effective covariance function to vary with the inputs, and may handle large datasets – thus potentially overcoming two of the biggest hurdles with GP models. Simulations show the viability of this approach.</p><p>2 0.20862679 <a title="95-tfidf-2" href="./nips-2001-A_Parallel_Mixture_of_SVMs_for_Very_Large_Scale_Problems.html">16 nips-2001-A Parallel Mixture of SVMs for Very Large Scale Problems</a></p>
<p>Author: Ronan Collobert, Samy Bengio, Yoshua Bengio</p><p>Abstract: Support Vector Machines (SVMs) are currently the state-of-the-art models for many classification problems but they suffer from the complexity of their training algorithm which is at least quadratic with respect to the number of examples. Hence, it is hopeless to try to solve real-life problems having more than a few hundreds of thousands examples with SVMs. The present paper proposes a new mixture of SVMs that can be easily implemented in parallel and where each SVM is trained on a small subset of the whole dataset. Experiments on a large benchmark dataset (Forest) as well as a difficult speech database , yielded significant time improvement (time complexity appears empirically to locally grow linearly with the number of examples) . In addition, and that is a surprise, a significant improvement in generalization was observed on Forest. 1</p><p>3 0.17367113 <a title="95-tfidf-3" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>Author: Matthew J. Beal, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We show that it is possible to extend hidden Markov models to have a countably inﬁnite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the inﬁnitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters deﬁne a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a ﬁnite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be inﬁnite— consider, for example, symbols being possible words appearing in English text.</p><p>4 0.15827315 <a title="95-tfidf-4" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>Author: Lehel Csató, Manfred Opper, Ole Winther</p><p>Abstract: The adaptive TAP Gibbs free energy for a general densely connected probabilistic model with quadratic interactions and arbritary single site constraints is derived. We show how a speciﬁc sequential minimization of the free energy leads to a generalization of Minka’s expectation propagation. Lastly, we derive a sparse representation version of the sequential algorithm. The usefulness of the approach is demonstrated on classiﬁcation and density estimation with Gaussian processes and on an independent component analysis problem.</p><p>5 0.15120786 <a title="95-tfidf-5" href="./nips-2001-Products_of_Gaussians.html">154 nips-2001-Products of Gaussians</a></p>
<p>Author: Christopher Williams, Felix V. Agakov, Stephen N. Felderhof</p><p>Abstract: Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. Below we consider PoE models in which each expert is a Gaussian. Although the product of Gaussians is also a Gaussian, if each Gaussian has a simple structure the product can have a richer structure. We examine (1) Products of Gaussian pancakes which give rise to probabilistic Minor Components Analysis, (2) products of I-factor PPCA models and (3) a products of experts construction for an AR(l) process. Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. In this paper we consider PoE models in which each expert is a Gaussian. It is easy to see that in this case the product model will also be Gaussian. However, if each Gaussian has a simple structure, the product can have a richer structure. Using Gaussian experts is attractive as it permits a thorough analysis of the product architecture, which can be difficult with other models , e.g. models defined over discrete random variables. Below we examine three cases of the products of Gaussians construction: (1) Products of Gaussian pancakes (PoGP) which give rise to probabilistic Minor Components Analysis (MCA), providing a complementary result to probabilistic Principal Components Analysis (PPCA) obtained by Tipping and Bishop (1999); (2) Products of I-factor PPCA models; (3) A products of experts construction for an AR(l) process. Products of Gaussians If each expert is a Gaussian pi(xI8 i ) '</p><p>6 0.13543627 <a title="95-tfidf-6" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>7 0.12104672 <a title="95-tfidf-7" href="./nips-2001-Gaussian_Process_Regression_with_Mismatched_Models.html">79 nips-2001-Gaussian Process Regression with Mismatched Models</a></p>
<p>8 0.11560779 <a title="95-tfidf-8" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>9 0.10610874 <a title="95-tfidf-9" href="./nips-2001-Analysis_of_Sparse_Bayesian_Learning.html">35 nips-2001-Analysis of Sparse Bayesian Learning</a></p>
<p>10 0.09479022 <a title="95-tfidf-10" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>11 0.091702871 <a title="95-tfidf-11" href="./nips-2001-Distribution_of_Mutual_Information.html">61 nips-2001-Distribution of Mutual Information</a></p>
<p>12 0.081749439 <a title="95-tfidf-12" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>13 0.080651924 <a title="95-tfidf-13" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>14 0.08043021 <a title="95-tfidf-14" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>15 0.079195179 <a title="95-tfidf-15" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<p>16 0.074726716 <a title="95-tfidf-16" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>17 0.070779637 <a title="95-tfidf-17" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>18 0.069727011 <a title="95-tfidf-18" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<p>19 0.06907545 <a title="95-tfidf-19" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>20 0.065838419 <a title="95-tfidf-20" href="./nips-2001-A_Model_of_the_Phonological_Loop%3A_Generalization_and_Binding.html">12 nips-2001-A Model of the Phonological Loop: Generalization and Binding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.234), (1, 0.026), (2, -0.038), (3, -0.099), (4, -0.134), (5, -0.035), (6, 0.119), (7, 0.024), (8, -0.01), (9, -0.006), (10, 0.068), (11, 0.072), (12, -0.023), (13, -0.311), (14, -0.035), (15, 0.014), (16, 0.042), (17, 0.035), (18, 0.147), (19, -0.053), (20, 0.039), (21, -0.117), (22, 0.039), (23, 0.081), (24, 0.024), (25, -0.017), (26, -0.146), (27, -0.178), (28, 0.188), (29, -0.006), (30, 0.085), (31, 0.032), (32, 0.136), (33, 0.003), (34, -0.091), (35, 0.098), (36, -0.071), (37, -0.018), (38, -0.0), (39, 0.103), (40, 0.08), (41, -0.119), (42, 0.128), (43, -0.029), (44, 0.049), (45, -0.011), (46, 0.095), (47, 0.12), (48, -0.018), (49, 0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94228047 <a title="95-lsi-1" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>Author: Carl E. Rasmussen, Zoubin Ghahramani</p><p>Abstract: We present an extension to the Mixture of Experts (ME) model, where the individual experts are Gaussian Process (GP) regression models. Using an input-dependent adaptation of the Dirichlet Process, we implement a gating network for an inﬁnite number of Experts. Inference in this model may be done efﬁciently using a Markov Chain relying on Gibbs sampling. The model allows the effective covariance function to vary with the inputs, and may handle large datasets – thus potentially overcoming two of the biggest hurdles with GP models. Simulations show the viability of this approach.</p><p>2 0.76188481 <a title="95-lsi-2" href="./nips-2001-Products_of_Gaussians.html">154 nips-2001-Products of Gaussians</a></p>
<p>Author: Christopher Williams, Felix V. Agakov, Stephen N. Felderhof</p><p>Abstract: Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. Below we consider PoE models in which each expert is a Gaussian. Although the product of Gaussians is also a Gaussian, if each Gaussian has a simple structure the product can have a richer structure. We examine (1) Products of Gaussian pancakes which give rise to probabilistic Minor Components Analysis, (2) products of I-factor PPCA models and (3) a products of experts construction for an AR(l) process. Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. In this paper we consider PoE models in which each expert is a Gaussian. It is easy to see that in this case the product model will also be Gaussian. However, if each Gaussian has a simple structure, the product can have a richer structure. Using Gaussian experts is attractive as it permits a thorough analysis of the product architecture, which can be difficult with other models , e.g. models defined over discrete random variables. Below we examine three cases of the products of Gaussians construction: (1) Products of Gaussian pancakes (PoGP) which give rise to probabilistic Minor Components Analysis (MCA), providing a complementary result to probabilistic Principal Components Analysis (PPCA) obtained by Tipping and Bishop (1999); (2) Products of I-factor PPCA models; (3) A products of experts construction for an AR(l) process. Products of Gaussians If each expert is a Gaussian pi(xI8 i ) '</p><p>3 0.58057165 <a title="95-lsi-3" href="./nips-2001-A_Parallel_Mixture_of_SVMs_for_Very_Large_Scale_Problems.html">16 nips-2001-A Parallel Mixture of SVMs for Very Large Scale Problems</a></p>
<p>Author: Ronan Collobert, Samy Bengio, Yoshua Bengio</p><p>Abstract: Support Vector Machines (SVMs) are currently the state-of-the-art models for many classification problems but they suffer from the complexity of their training algorithm which is at least quadratic with respect to the number of examples. Hence, it is hopeless to try to solve real-life problems having more than a few hundreds of thousands examples with SVMs. The present paper proposes a new mixture of SVMs that can be easily implemented in parallel and where each SVM is trained on a small subset of the whole dataset. Experiments on a large benchmark dataset (Forest) as well as a difficult speech database , yielded significant time improvement (time complexity appears empirically to locally grow linearly with the number of examples) . In addition, and that is a surprise, a significant improvement in generalization was observed on Forest. 1</p><p>4 0.55219311 <a title="95-lsi-4" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>Author: Lehel Csató, Manfred Opper, Ole Winther</p><p>Abstract: The adaptive TAP Gibbs free energy for a general densely connected probabilistic model with quadratic interactions and arbritary single site constraints is derived. We show how a speciﬁc sequential minimization of the free energy leads to a generalization of Minka’s expectation propagation. Lastly, we derive a sparse representation version of the sequential algorithm. The usefulness of the approach is demonstrated on classiﬁcation and density estimation with Gaussian processes and on an independent component analysis problem.</p><p>5 0.53020346 <a title="95-lsi-5" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>Author: Peter Sykacek, Stephen J. Roberts</p><p>Abstract: This paper proposes an approach to classiﬁcation of adjacent segments of a time series as being either of classes. We use a hierarchical model that consists of a feature extraction stage and a generative classiﬁer which is built on top of these features. Such two stage approaches are often used in signal and image processing. The novel part of our work is that we link these stages probabilistically by using a latent feature space. To use one joint model is a Bayesian requirement, which has the advantage to fuse information according to its certainty. The classiﬁer is implemented as hidden Markov model with Gaussian and Multinomial observation distributions deﬁned on a suitably chosen representation of autoregressive models. The Markov dependency is motivated by the assumption that successive classiﬁcations will be correlated. Inference is done with Markov chain Monte Carlo (MCMC) techniques. We apply the proposed approach to synthetic data and to classiﬁcation of EEG that was recorded while the subjects performed different cognitive tasks. All experiments show that using a latent feature space results in a signiﬁcant improvement in generalization accuracy. Hence we expect that this idea generalizes well to other hierarchical models.</p><p>6 0.51732153 <a title="95-lsi-6" href="./nips-2001-Gaussian_Process_Regression_with_Mismatched_Models.html">79 nips-2001-Gaussian Process Regression with Mismatched Models</a></p>
<p>7 0.47750363 <a title="95-lsi-7" href="./nips-2001-Analysis_of_Sparse_Bayesian_Learning.html">35 nips-2001-Analysis of Sparse Bayesian Learning</a></p>
<p>8 0.47116274 <a title="95-lsi-8" href="./nips-2001-Estimating_Car_Insurance_Premia%3A_a_Case_Study_in_High-Dimensional_Data_Inference.html">70 nips-2001-Estimating Car Insurance Premia: a Case Study in High-Dimensional Data Inference</a></p>
<p>9 0.46564007 <a title="95-lsi-9" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>10 0.45012343 <a title="95-lsi-10" href="./nips-2001-Distribution_of_Mutual_Information.html">61 nips-2001-Distribution of Mutual Information</a></p>
<p>11 0.4249545 <a title="95-lsi-11" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>12 0.39720824 <a title="95-lsi-12" href="./nips-2001-Geometrical_Singularities_in_the_Neuromanifold_of_Multilayer_Perceptrons.html">83 nips-2001-Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons</a></p>
<p>13 0.38244671 <a title="95-lsi-13" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>14 0.37853509 <a title="95-lsi-14" href="./nips-2001-Using_Vocabulary_Knowledge_in_Bayesian_Multinomial_Estimation.html">194 nips-2001-Using Vocabulary Knowledge in Bayesian Multinomial Estimation</a></p>
<p>15 0.35990041 <a title="95-lsi-15" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>16 0.34269881 <a title="95-lsi-16" href="./nips-2001-Entropy_and_Inference%2C_Revisited.html">68 nips-2001-Entropy and Inference, Revisited</a></p>
<p>17 0.33581573 <a title="95-lsi-17" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>18 0.33078164 <a title="95-lsi-18" href="./nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables.html">153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</a></p>
<p>19 0.32069412 <a title="95-lsi-19" href="./nips-2001-Learning_Body_Pose_via_Specialized_Maps.html">108 nips-2001-Learning Body Pose via Specialized Maps</a></p>
<p>20 0.31997526 <a title="95-lsi-20" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.052), (17, 0.026), (19, 0.032), (27, 0.122), (30, 0.061), (34, 0.2), (38, 0.029), (59, 0.074), (72, 0.072), (79, 0.069), (83, 0.044), (91, 0.132)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89728999 <a title="95-lda-1" href="./nips-2001-Stochastic_Mixed-Signal_VLSI_Architecture_for_High-Dimensional_Kernel_Machines.html">176 nips-2001-Stochastic Mixed-Signal VLSI Architecture for High-Dimensional Kernel Machines</a></p>
<p>Author: Roman Genov, Gert Cauwenberghs</p><p>Abstract: A mixed-signal paradigm is presented for high-resolution parallel innerproduct computation in very high dimensions, suitable for efﬁcient implementation of kernels in image processing. At the core of the externally digital architecture is a high-density, low-power analog array performing binary-binary partial matrix-vector multiplication. Full digital resolution is maintained even with low-resolution analog-to-digital conversion, owing to random statistics in the analog summation of binary products. A random modulation scheme produces near-Bernoulli statistics even for highly correlated inputs. The approach is validated with real image data, and with experimental results from a CID/DRAM analog array prototype in 0.5 m CMOS. ¢</p><p>same-paper 2 0.85793519 <a title="95-lda-2" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>Author: Carl E. Rasmussen, Zoubin Ghahramani</p><p>Abstract: We present an extension to the Mixture of Experts (ME) model, where the individual experts are Gaussian Process (GP) regression models. Using an input-dependent adaptation of the Dirichlet Process, we implement a gating network for an inﬁnite number of Experts. Inference in this model may be done efﬁciently using a Markov Chain relying on Gibbs sampling. The model allows the effective covariance function to vary with the inputs, and may handle large datasets – thus potentially overcoming two of the biggest hurdles with GP models. Simulations show the viability of this approach.</p><p>3 0.72192067 <a title="95-lda-3" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>4 0.72019541 <a title="95-lda-4" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>Author: Hilbert J. Kappen, Wim Wiegerinck</p><p>Abstract: The Cluster Variation method is a class of approximation methods containing the Bethe and Kikuchi approximations as special cases. We derive two novel iteration schemes for the Cluster Variation Method. One is a fixed point iteration scheme which gives a significant improvement over loopy BP, mean field and TAP methods on directed graphical models. The other is a gradient based method, that is guaranteed to converge and is shown to give useful results on random graphs with mild frustration. We conclude that the methods are of significant practical value for large inference problems. 1</p><p>5 0.71594012 <a title="95-lda-5" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>Author: Si Wu, Shun-ichi Amari</p><p>Abstract: This study investigates a population decoding paradigm, in which the estimation of stimulus in the previous step is used as prior knowledge for consecutive decoding. We analyze the decoding accuracy of such a Bayesian decoder (Maximum a Posteriori Estimate), and show that it can be implemented by a biologically plausible recurrent network, where the prior knowledge of stimulus is conveyed by the change in recurrent interactions as a result of Hebbian learning. 1</p><p>6 0.7147885 <a title="95-lda-6" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>7 0.71459806 <a title="95-lda-7" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>8 0.71333396 <a title="95-lda-8" href="./nips-2001-Multi_Dimensional_ICA_to_Separate_Correlated_Sources.html">127 nips-2001-Multi Dimensional ICA to Separate Correlated Sources</a></p>
<p>9 0.7125802 <a title="95-lda-9" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>10 0.71203053 <a title="95-lda-10" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>11 0.71165502 <a title="95-lda-11" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>12 0.7116468 <a title="95-lda-12" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>13 0.71053284 <a title="95-lda-13" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>14 0.70761555 <a title="95-lda-14" href="./nips-2001-Blind_Source_Separation_via_Multinode_Sparse_Representation.html">44 nips-2001-Blind Source Separation via Multinode Sparse Representation</a></p>
<p>15 0.70696366 <a title="95-lda-15" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>16 0.70687246 <a title="95-lda-16" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>17 0.7068336 <a title="95-lda-17" href="./nips-2001-The_Unified_Propagation_and_Scaling_Algorithm.html">188 nips-2001-The Unified Propagation and Scaling Algorithm</a></p>
<p>18 0.70673966 <a title="95-lda-18" href="./nips-2001-Quantizing_Density_Estimators.html">155 nips-2001-Quantizing Density Estimators</a></p>
<p>19 0.70644885 <a title="95-lda-19" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>20 0.70624614 <a title="95-lda-20" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
