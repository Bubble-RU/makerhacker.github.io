<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-26" href="#">nips2001-26</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</h1>
<br/><p>Source: <a title="nips-2001-26-pdf" href="http://papers.nips.cc/paper/1969-active-portfolio-management-based-on-error-correction-neural-networks.pdf">pdf</a></p><p>Author: Hans-Georg Zimmermann, Ralph Neuneier, Ralph Grothmann</p><p>Abstract: This paper deals with a neural network architecture which establishes a portfolio management system similar to the Black / Litterman approach. This allocation scheme distributes funds across various securities or ﬁnancial markets while simultaneously complying with speciﬁc allocation constraints which meet the requirements of an investor. The portfolio optimization algorithm is modeled by a feedforward neural network. The underlying expected return forecasts are based on error correction neural networks (ECNN), which utilize the last model error as an auxiliary input to evaluate their own misspeciﬁcation. The portfolio optimization is implemented such that (i.) the allocations comply with investor’s constraints and that (ii.) the risk of the portfolio can be controlled. We demonstrate the proﬁtability of our approach by constructing internationally diversiﬁed portfolios across different ﬁnancial markets of the G7 contries. It turns out, that our approach is superior to a preset benchmark portfolio. ¢ £¡ 1 Introduction: Portfolio-Management We integrate the portfolio optimization algorithm suggested by Black / Litterman [1] into a neural network architecture. Combining the mean-variance theory [5] with the capital asset pricing model (CAPM) [7], this approach utilizes excess returns of the CAPM equilibrium to deﬁne a neutral, well balanced benchmark portfolio. Deviations from the benchmark allocation are only allowed within preset boundaries. Hence, as an advantage, there are no unrealistic solutions (e. g. large short positions, huge portfolio changes). Moreover, there is no need of formulating return expectations for all assets. In contrast to Black / Litterman, excess return forecasts are estimated by time-delay recurrent error correction neural networks [8]. Investment decisions which comply with given allocation constraints are derived from these predictions. The risk exposure of the portfolio is implicitly controlled by a parameter-optimizing task over time (sec. 3 and 5). Our approach consists of the following three steps: (i.) Construction of forecast models on the basis of error correction neural networks (ECNN) for all assets (sec. 2).   ¤§© © © § ¥ ¦¨¦¤ To whom correspondence should be addressed: Georg.Zimmermann@mchp.siemens.de.  ¤ ¤ (ii.) Computation of excess returns by a higher-level feedforward network (sec. 3 and 4). By this, the proﬁtability of an asset with respect to all others is measured. on the basis of the excess returns. (iii.) Optimization of the investment proportions Allocation constraints ensure, that the investment proportions may deviate from a given benchmark only within predeﬁned intervals (sec. 3 and 4). £ § ¨¡ ¥ £¡ ¦¤¢  ¡ © ¡ © Finally, we apply our neural network based portfolio management system to an asset allocation problem concerning the G7 countries (sec. 6). 2 Forecasting by Error Correction Neural Networks Most dynamical systems are driven by a superposition of autonomous development and external inﬂuences [8]. For discrete time grids, such a dynamics can be described by a recurrent state transition and an output equation (Eq. 1). ¥   § § state transition eq. output eq. (1)  $</p><p>Reference: <a title="nips-2001-26-reference" href="../nips2001_reference/nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Keywords: portfolio management, ﬁnancial forecasting, recurrent neural networks. [sent-1, score-0.585]
</p><p>2 This allocation scheme distributes funds across various securities or ﬁnancial markets while simultaneously complying with speciﬁc allocation constraints which meet the requirements of an investor. [sent-3, score-0.78]
</p><p>3 The portfolio optimization algorithm is modeled by a feedforward neural network. [sent-4, score-0.6]
</p><p>4 The underlying expected return forecasts are based on error correction neural networks (ECNN), which utilize the last model error as an auxiliary input to evaluate their own misspeciﬁcation. [sent-5, score-0.492]
</p><p>5 ) the allocations comply with investor’s constraints and that (ii. [sent-7, score-0.143]
</p><p>6 We demonstrate the proﬁtability of our approach by constructing internationally diversiﬁed portfolios across different ﬁnancial markets of the G7 contries. [sent-9, score-0.112]
</p><p>7 It turns out, that our approach is superior to a preset benchmark portfolio. [sent-10, score-0.21]
</p><p>8 ¢ £¡  1 Introduction: Portfolio-Management We integrate the portfolio optimization algorithm suggested by Black / Litterman [1] into a neural network architecture. [sent-11, score-0.646]
</p><p>9 Combining the mean-variance theory [5] with the capital asset pricing model (CAPM) [7], this approach utilizes excess returns of the CAPM equilibrium to deﬁne a neutral, well balanced benchmark portfolio. [sent-12, score-0.995]
</p><p>10 Deviations from the benchmark allocation are only allowed within preset boundaries. [sent-13, score-0.471]
</p><p>11 Moreover, there is no need of formulating return expectations for all assets. [sent-17, score-0.102]
</p><p>12 In contrast to Black / Litterman, excess return forecasts are estimated by time-delay recurrent error correction neural networks [8]. [sent-18, score-0.796]
</p><p>13 Investment decisions which comply with given allocation constraints are derived from these predictions. [sent-19, score-0.327]
</p><p>14 The risk exposure of the portfolio is implicitly controlled by a parameter-optimizing task over time (sec. [sent-20, score-0.743]
</p><p>15 ) Construction of forecast models on the basis of error correction neural networks (ECNN) for all assets (sec. [sent-23, score-0.473]
</p><p>16 ) Computation of excess returns by a higher-level feedforward network (sec. [sent-30, score-0.539]
</p><p>17 By this, the proﬁtability of an asset with respect to all others is measured. [sent-32, score-0.321]
</p><p>18 ) Optimization of the investment proportions Allocation constraints ensure, that the investment proportions may deviate from a given benchmark only within predeﬁned intervals (sec. [sent-35, score-0.823]
</p><p>19 £  § ¨¡  ¥ £¡ ¦¤¢   ¡ ©  ¡ ©  Finally, we apply our neural network based portfolio management system to an asset allocation problem concerning the G7 countries (sec. [sent-37, score-1.258]
</p><p>20 2 Forecasting by Error Correction Neural Networks Most dynamical systems are driven by a superposition of autonomous development and external inﬂuences [8]. [sent-39, score-0.104]
</p><p>21 For discrete time grids, such a dynamics can be described by a recurrent state transition and an output equation (Eq. [sent-40, score-0.084]
</p><p>22 §        $  %     ¤ ¥  ¥        ¥  ¥  is a mapping from the previous state , external inﬂuences and The state transition a comparison between the model output and observed data . [sent-45, score-0.089]
</p><p>23 Under such conditions, the model error quantiﬁes the model’s misﬁt and serves as an indicator of short-term effects or external shocks [8]. [sent-48, score-0.08]
</p><p>24   " '     $ #(§   "     ,  6  6        &      5 § 4§ 2§ 3 1    $ "  )(§      Using weight matrices of appropriate dimensions corresponding to , a neural network approach of Eq. [sent-49, score-0.071]
</p><p>25 3) is a parameter dimensions in optimization task of appropriate sized weight matrices [8]: " P     5 § 4§ 1 § 3  (3)   ¢  5  0  q fXA p dh b ge f ci f  aD`IY"  ! [sent-55, score-0.078]
</p><p>26 3 by ﬁnite unfolding in time using shared weights. [sent-58, score-0.109]
</p><p>27 Error correction neural network (ECNN) using unfolding in time and overshooting. [sent-64, score-0.272]
</p><p>28 Note, that is the ﬁxed negative of an appropriate sized identity matrix, while are output clusters with target values of zero in order to optimize the error correction mechanism. [sent-65, score-0.171]
</p><p>29 At all future time steps , we have no compensation of the internal expectations and thus, the system offers forecasts . [sent-70, score-0.184]
</p><p>30 A forecast of the ECNN is based on a modeling of the recursive structure of a dynamical system (coded in ), external inﬂuences (coded in ) and the error correction mechanism which is also acting as an external input (coded in , ). [sent-71, score-0.413]
</p><p>31 Due to the error correction, the ECNN has an explicit mechanism to handle the initialization shock of the unfolding [8]. [sent-73, score-0.143]
</p><p>32 0  3  6     3 The Asset Allocation Strategy Now, we explain how the forecasts are transformed into an asset allocation vector with investment proportions ( ). [sent-80, score-1.054]
</p><p>33 ) pays attention to the uncertainty of the forecasts and (ii. [sent-85, score-0.204]
</p><p>34 ¢  ¥  ¡ &©  § © © ©  § ¥     ¡ '©    ©  ¡ ©  ©      ©  ¤  In order to handle the uncertainty of the asset forecasts , we utilize the concept of excess is deﬁned as the difference between the expected returns and return. [sent-87, score-1.018]
</p><p>35 The investment proportions of assets which have a superior excess return should be enlarged, because they seem to be more valuable. [sent-91, score-0.856]
</p><p>36 with  ¡ #   $#  (5)  £  $  §© ©© ¤  ¤ § ¥ §     ¡ &©  ¥  $ £      $¡        ( & )'%  ( & 0'%  ¥  ¢    ¥ ¡ X©   U£  ¥ ¡ g&©  The market share constraints are given by the asset manager in form of intervals, which have a mean value of . [sent-93, score-0.466]
</p><p>37 The admissible spread deﬁnes how much the allocation may deviate from : ¡  ¡ S1  2  £   (6)  ¡  1  ¡ 2  E ¡ 1  § ¡  § S1 ¡  2 5  ©  ¡ )©  ¡ S1  ¡ ©  3 4  Since we have to level the excess returns around the mean of the intervals, Eq. [sent-95, score-0.788]
</p><p>38 The vector can be computed before-hand by solving the system of nonlinear equations which results by setting the excess returns (Eq. [sent-97, score-0.493]
</p><p>39  ¥  1    § © © © § ¥    Since the allocation represents the benchmark portfolio, the pre-condition leads to a non-unique solution (Eq. [sent-107, score-0.41]
</p><p>40 In the following, we choose  (9)    §  ©  The interval deﬁnes constraints for the parameters because the latter quantiﬁes the deviation of from the benchmark . [sent-111, score-0.184]
</p><p>41 § v  2 5    ©   with  (11)  (12)  Summarizing, the construction of the allocation scheme consists of the following two steps: (i. [sent-116, score-0.261]
</p><p>42 ) Train the error correction sub-networks and compute the excess returns . [sent-117, score-0.619]
</p><p>43 4 Modeling the Asset Allocation Strategy by Neural Networks A neural network approach of the allocation scheme (sec. [sent-122, score-0.332]
</p><p>44 The ﬁrst layer of the portfolio optimization neural network (Fig. [sent-125, score-0.684]
</p><p>45 The matrix entitled ’unfolding’ computes the excess returns for assets as a contour plot. [sent-127, score-0.728]
</p><p>46 The layer entitled ’excess returns’ is designed as an output cluster, i. [sent-129, score-0.106]
</p><p>47 it is associated with an error function which computes error signals for each training pattern. [sent-131, score-0.108]
</p><p>48 ) yt+6  yt+6  asset 1  asset 2  asset k  asset k−1  ECNN forecasts of k assets  Figure 2. [sent-145, score-1.636]
</p><p>49 Arranged on the top of the ECNN sub-networks, a higher-level neural network models the portfolio optimization algorithm on basis of the excess returns . [sent-146, score-1.139]
</p><p>50 The diagonal matrix which computes the weighted excess returns includes the only tunable parameters . [sent-147, score-0.541]
</p><p>51 ¤   ¥£r ¢  ¡  ¤¡ ¦  is weighted by a particular via a diagonal connection to Next, each excess return the layer entitled ’weighted excess returns’. [sent-149, score-0.809]
</p><p>52 Afterwards, the weighted excess returns are folded using the transpose of the sparse matrix called ’unfolding’ (see Fig. [sent-150, score-0.52]
</p><p>53 By this, we calculate the sum of the weighted excess returns for each asset . [sent-152, score-0.841]
</p><p>54 £ ¤¡  £¡      ¡  £ ¡ £ ¡    £    ¤    ¤  According to the predictions of the excess returns , the layer ’asset allocation’ computes proﬁtable investment decisions. [sent-153, score-0.782]
</p><p>55 the benchmark portfolio is reproduced by the offset Otherwise, funds are allocated within the preset investment boundaries , and . [sent-155, score-1.029]
</p><p>56 while simultaneously complying with the constraints  ¥   § ©©©  § ¥   ¡ 2 E ¡ 1 ¡ 2 § $ $ $ 1 Ay § § ©  © ©   $ §     $  ¢¡  1  ©  1  ¥ ¡ ¨©  A  £     § ¡  §© ©©  § ¥   ¥ $  6    6  ¡ P©   "  In order to prevent short selling, we assume for each investment proportion. [sent-156, score-0.35]
</p><p>57 Further on, we have to guarantee that the sum of the proportions invested in the securities equals , i. [sent-157, score-0.104]
</p><p>58 10 is also solved by this cluster by generating error signals utilizing the prof-max error function (Eq. [sent-163, score-0.087]
</p><p>59  "  ¡ #©  ¢  (14)  £  ¥  @ b &  a i$  § f ¦§ f ¦¤   ¤§© ©© ¥        ¡ © f ¡  ¤ ¥  T  ¥  U¡  R T ¢ U   ¥ ¡ ©    ¢  Q  The layer ’market shares’ takes care of the allocation constraints. [sent-165, score-0.299]
</p><p>60 15 is implemented to ensure, that the investments do not violate preset constraints. [sent-167, score-0.122]
</p><p>61 ¡ '©   (15)  £ &  ¥  @ b a     ¡   S 1  § &© ¡  ©  T  ©  § ¥  U¡  ¥  R T ¢ U   Q  The ’market shares’ cluster generates error signals for the penalized optimization problem stated in Eq. [sent-168, score-0.108]
</p><p>62 By this, we implement a penalty for exceeding the allocation intervals . [sent-170, score-0.29]
</p><p>63 1 we compare the mean-variance framework of Markowitz with our neural network based portfolio optimization algorithm. [sent-175, score-0.646]
</p><p>64 Comparison of the portfolio optimization algorithm of Markowitz with our approach. [sent-177, score-0.575]
</p><p>65 The most crucial difference between the mean-variance framework and our approach is the handling of the risk exposure (see Tab. [sent-178, score-0.223]
</p><p>66 The Markowitz algorithm optimizes the expected risk explicitly by quadratic programming. [sent-180, score-0.162]
</p><p>67 Assuming that it is not possible to forecast the expected returns of the assets (often referred to as random walk hypothesis), the forecasts are determined by an average of most recent observed returns, while the risk-covariance matrix is estimated by the historical volatility of the assets. [sent-181, score-0.711]
</p><p>68 Hence, the risk of the portfolio is determined by the volatility of the time series of the assets. [sent-182, score-0.713]
</p><p>69 f£££  ¤¤¤¤f ¥    U £f ¡ £ ¡  ¡   ©  ¤  However, insisting on the existence of useful forecast models, we propose to derive the covariance matrix from the forecast model residuals, i. [sent-183, score-0.266]
</p><p>70 Now, the risk of the portfolio is due to the nonforecastability of the assets only. [sent-186, score-0.85]
</p><p>71 Since our allocation scheme is based on the model uncertainty, we refer to this approach as causal risk. [sent-187, score-0.261]
</p><p>72 Using the covariances of the model errors as a measurement of risk still allows to apply the Markowitz optimization scheme. [sent-188, score-0.217]
</p><p>73 16 is optimized over time with respect to the parameters , which are used to evaluate the certainty of the excess return forecasts. [sent-192, score-0.424]
</p><p>74 By this, it is possible to construct an asset allocations strategy which implicitly controls the risk exposure of the portfolio according to the certainty of the forecasts . [sent-193, score-1.399]
</p><p>75 ¡  ¤  ¥  ¦  6  £¡  ¨ ©R  ©  $ f £ § f ¡ ¤ ¤    If the predicted excess returns are reliable, then the weights are greater than zero, because the optimization algorithm emphasizes the particular asset in comparison to £ ¤¡    other assets with less reliable forecasts. [sent-196, score-1.058]
</p><p>76 16 implicitly controls the risk exposure of the portfolio although it is formulated as a return maximization task. [sent-199, score-0.887]
</p><p>77 16 has to be optimized with respect to the allocation constraints . [sent-201, score-0.296]
</p><p>78 If , then the benchmark is recovered, while allows deviations from the benchmark within the bounds . [sent-203, score-0.298]
</p><p>79 Thus, the active risk parameter analysis the risk sensitivity of the portfolios with respect to the quality of the forecast models. [sent-204, score-0.503]
</p><p>80 We work on the basis of monthly data in order to forecast the semi-annual development of the stock, cash and bond markets of the G7 countries Spain, France, Germany, Italy, Japan, UK and USA. [sent-206, score-0.276]
</p><p>81 A separate ECNN is constructed for each market on the basis of country speciﬁc economic data. [sent-207, score-0.09]
</p><p>82 1992 to June 1993), which is used to learn the allocation parameters . [sent-217, score-0.261]
</p><p>83 £¡    We evaluate the performance of our approach by a comparison with the benchmark portwhich is calculated with respect to the market shares . [sent-221, score-0.3]
</p><p>84 The folio comparison of our strategy and the benchmark portfolio is drawn on the basis of the accumulated return of investment (Fig. [sent-222, score-1.09]
</p><p>85 Our strategy is able to outperform the benchmark portfolio on the generalization set by nearly . [sent-224, score-0.721]
</p><p>86 A further enhancement of the portfolio performance can only be achieved if one relaxes the market share constraints. [sent-225, score-0.63]
</p><p>87 This indicates, that the tight allocation boundaries, which prevent huge capital transactions from non-proﬁtable to booming markets, narrow additional gains. [sent-226, score-0.314]
</p><p>88 4 we compare the risk of our portfolio to the risk of the benchmark portfolio. [sent-228, score-0.993]
</p><p>89 Here, the portfolio risk is deﬁned analogous to the mean-variance framework. [sent-229, score-0.682]
</p><p>90 However, in contrast to this approach, the expected (co-)variances are replaced by the residuals of the underlying forecast models. [sent-230, score-0.157]
</p><p>91 The risk level which is induced by our strategy is comparable to the benchmark (Fig. [sent-231, score-0.342]
</p><p>92 5 compares the allocations of German bonds and stocks across the generalization set: A typical reciprocal investment behavior is depicted, e. [sent-235, score-0.42]
</p><p>93 enlarged positions in stocks often occur in parallel with smaller investments in bonds. [sent-237, score-0.146]
</p><p>94 Not all countries show such a coherent investment behavior. [sent-239, score-0.276]
</p><p>95 7 Conclusions and Future Work We described a neural network approach which adapts the Black / Litterman portfolio optimization algorithm. [sent-240, score-0.646]
</p><p>96 Here, funds are allocated across various securities while simultaneously complying with allocation constraints. [sent-241, score-0.442]
</p><p>97 In contrast to the mean-variance theory, the risk exposure of our approach focuses on the uncertainty of the underlying forecast models. [sent-242, score-0.376]
</p><p>98 14  portfolio risk benchmark risk  ECNN Benchmark 12  8  risk  accumulated return  10  6  4  2  0  −2 July 1993  May 1995  July 1993  May 1995  Date  date  Figure 3. [sent-243, score-1.32]
</p><p>99 The underlying forecasts are generated by ECNNs, since our empirical results indicate, that this is a very promising framework for ﬁnancial modeling. [sent-265, score-0.184]
</p><p>100 Extending the ECNN by using techniques like overshooting, variants-invariants separation or unfolding in space and time, one is able to include additional prior knowledge of the dynamics into the model [8, 9]. [sent-266, score-0.131]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('portfolio', 0.52), ('asset', 0.321), ('excess', 0.298), ('allocation', 0.261), ('investment', 0.23), ('ecnn', 0.214), ('returns', 0.195), ('forecasts', 0.184), ('assets', 0.168), ('risk', 0.162), ('benchmark', 0.149), ('forecast', 0.133), ('unfolding', 0.109), ('markowitz', 0.107), ('return', 0.102), ('correction', 0.092), ('market', 0.09), ('allocations', 0.077), ('litterman', 0.077), ('yt', 0.073), ('markets', 0.066), ('investments', 0.061), ('preset', 0.061), ('stocks', 0.061), ('exposure', 0.061), ('proportions', 0.058), ('nancial', 0.056), ('optimization', 0.055), ('german', 0.053), ('external', 0.046), ('complying', 0.046), ('countries', 0.046), ('entitled', 0.046), ('forecasting', 0.046), ('neuneier', 0.046), ('overshooting', 0.046), ('portfolios', 0.046), ('securities', 0.046), ('zimmermann', 0.046), ('network', 0.046), ('funds', 0.045), ('id', 0.041), ('shares', 0.04), ('recurrent', 0.04), ('management', 0.039), ('layer', 0.038), ('black', 0.037), ('accumulated', 0.037), ('constraints', 0.035), ('dynamical', 0.035), ('uences', 0.035), ('error', 0.034), ('deviate', 0.034), ('july', 0.032), ('capital', 0.032), ('strategy', 0.031), ('bond', 0.031), ('bonds', 0.031), ('capm', 0.031), ('comply', 0.031), ('cumulated', 0.031), ('diversi', 0.031), ('grothmann', 0.031), ('volatility', 0.031), ('zt', 0.03), ('intervals', 0.029), ('weighted', 0.027), ('modeling', 0.027), ('ralph', 0.027), ('stock', 0.027), ('tability', 0.027), ('date', 0.026), ('neural', 0.025), ('coded', 0.025), ('ut', 0.024), ('certainty', 0.024), ('allocated', 0.024), ('enlarged', 0.024), ('residuals', 0.024), ('sales', 0.024), ('maximization', 0.023), ('autonomous', 0.023), ('sized', 0.023), ('dynamics', 0.022), ('output', 0.022), ('pro', 0.022), ('computes', 0.021), ('huge', 0.021), ('comparison', 0.021), ('generalization', 0.021), ('networks', 0.021), ('simultaneously', 0.02), ('quanti', 0.02), ('uncertainty', 0.02), ('share', 0.02), ('signals', 0.019), ('financial', 0.019), ('gradients', 0.019), ('controls', 0.019), ('short', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="26-tfidf-1" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<p>Author: Hans-Georg Zimmermann, Ralph Neuneier, Ralph Grothmann</p><p>Abstract: This paper deals with a neural network architecture which establishes a portfolio management system similar to the Black / Litterman approach. This allocation scheme distributes funds across various securities or ﬁnancial markets while simultaneously complying with speciﬁc allocation constraints which meet the requirements of an investor. The portfolio optimization algorithm is modeled by a feedforward neural network. The underlying expected return forecasts are based on error correction neural networks (ECNN), which utilize the last model error as an auxiliary input to evaluate their own misspeciﬁcation. The portfolio optimization is implemented such that (i.) the allocations comply with investor’s constraints and that (ii.) the risk of the portfolio can be controlled. We demonstrate the proﬁtability of our approach by constructing internationally diversiﬁed portfolios across different ﬁnancial markets of the G7 contries. It turns out, that our approach is superior to a preset benchmark portfolio. ¢ £¡ 1 Introduction: Portfolio-Management We integrate the portfolio optimization algorithm suggested by Black / Litterman [1] into a neural network architecture. Combining the mean-variance theory [5] with the capital asset pricing model (CAPM) [7], this approach utilizes excess returns of the CAPM equilibrium to deﬁne a neutral, well balanced benchmark portfolio. Deviations from the benchmark allocation are only allowed within preset boundaries. Hence, as an advantage, there are no unrealistic solutions (e. g. large short positions, huge portfolio changes). Moreover, there is no need of formulating return expectations for all assets. In contrast to Black / Litterman, excess return forecasts are estimated by time-delay recurrent error correction neural networks [8]. Investment decisions which comply with given allocation constraints are derived from these predictions. The risk exposure of the portfolio is implicitly controlled by a parameter-optimizing task over time (sec. 3 and 5). Our approach consists of the following three steps: (i.) Construction of forecast models on the basis of error correction neural networks (ECNN) for all assets (sec. 2).   ¤§© © © § ¥ ¦¨¦¤ To whom correspondence should be addressed: Georg.Zimmermann@mchp.siemens.de.  ¤ ¤ (ii.) Computation of excess returns by a higher-level feedforward network (sec. 3 and 4). By this, the proﬁtability of an asset with respect to all others is measured. on the basis of the excess returns. (iii.) Optimization of the investment proportions Allocation constraints ensure, that the investment proportions may deviate from a given benchmark only within predeﬁned intervals (sec. 3 and 4). £ § ¨¡ ¥ £¡ ¦¤¢  ¡ © ¡ © Finally, we apply our neural network based portfolio management system to an asset allocation problem concerning the G7 countries (sec. 6). 2 Forecasting by Error Correction Neural Networks Most dynamical systems are driven by a superposition of autonomous development and external inﬂuences [8]. For discrete time grids, such a dynamics can be described by a recurrent state transition and an output equation (Eq. 1). ¥   § § state transition eq. output eq. (1)  $</p><p>2 0.076131709 <a title="26-tfidf-2" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>Author: Sebastian Thrun, John Langford, Vandi Verma</p><p>Abstract: We propose a new particle ﬁlter that incorporates a model of costs when generating particles. The approach is motivated by the observation that the costs of accidentally not tracking hypotheses might be signiﬁcant in some areas of state space, and next to irrelevant in others. By incorporating a cost model into particle ﬁltering, states that are more critical to the system performance are more likely to be tracked. Automatic calculation of the cost model is implemented using an MDP value function calculation that estimates the value of tracking a particular state. Experiments in two mobile robot domains illustrate the appropriateness of the approach.</p><p>3 0.064113393 <a title="26-tfidf-3" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>Author: Nicolò Cesa-bianchi, Alex Conconi, Claudio Gentile</p><p>Abstract: In this paper we show that on-line algorithms for classiﬁcation and regression can be naturally used to obtain hypotheses with good datadependent tail bounds on their risk. Our results are proven without requiring complicated concentration-of-measure arguments and they hold for arbitrary on-line learning algorithms. Furthermore, when applied to concrete on-line algorithms, our results yield tail bounds that in many cases are comparable or better than the best known bounds.</p><p>4 0.044340905 <a title="26-tfidf-4" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>Author: M. Schmitt</p><p>Abstract: Recurrent neural networks of analog units are computers for realvalued functions. We study the time complexity of real computation in general recurrent neural networks. These have sigmoidal, linear, and product units of unlimited order as nodes and no restrictions on the weights. For networks operating in discrete time, we exhibit a family of functions with arbitrarily high complexity, and we derive almost tight bounds on the time required to compute these functions. Thus, evidence is given of the computational limitations that time-bounded analog recurrent neural networks are subject to. 1</p><p>5 0.043533199 <a title="26-tfidf-5" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>Author: Jyrki Kivinen, Alex J. Smola, Robert C. Williamson</p><p>Abstract: We consider online learning in a Reproducing Kernel Hilbert Space. Our method is computationally efﬁcient and leads to simple algorithms. In particular we derive update equations for classiﬁcation, regression, and novelty detection. The inclusion of the -trick allows us to give a robust parameterization. Moreover, unlike in batch learning where the -trick only applies to the -insensitive loss function we are able to derive general trimmed-mean types of estimators such as for Huber’s robust loss.     ¡</p><p>6 0.039637528 <a title="26-tfidf-6" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>7 0.035688642 <a title="26-tfidf-7" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>8 0.035334688 <a title="26-tfidf-8" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>9 0.031878199 <a title="26-tfidf-9" href="./nips-2001-%28Not%29_Bounding_the_True_Error.html">1 nips-2001-(Not) Bounding the True Error</a></p>
<p>10 0.031116284 <a title="26-tfidf-10" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>11 0.030544227 <a title="26-tfidf-11" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>12 0.029277317 <a title="26-tfidf-12" href="./nips-2001-Rao-Blackwellised_Particle_Filtering_via_Data_Augmentation.html">156 nips-2001-Rao-Blackwellised Particle Filtering via Data Augmentation</a></p>
<p>13 0.027884748 <a title="26-tfidf-13" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>14 0.027495492 <a title="26-tfidf-14" href="./nips-2001-Grammar_Transfer_in_a_Second_Order_Recurrent_Neural_Network.html">85 nips-2001-Grammar Transfer in a Second Order Recurrent Neural Network</a></p>
<p>15 0.026582586 <a title="26-tfidf-15" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>16 0.025861563 <a title="26-tfidf-16" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>17 0.025750712 <a title="26-tfidf-17" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<p>18 0.025387244 <a title="26-tfidf-18" href="./nips-2001-Estimating_Car_Insurance_Premia%3A_a_Case_Study_in_High-Dimensional_Data_Inference.html">70 nips-2001-Estimating Car Insurance Premia: a Case Study in High-Dimensional Data Inference</a></p>
<p>19 0.0248765 <a title="26-tfidf-19" href="./nips-2001-Incremental_Learning_and_Selective_Sampling_via_Parametric_Optimization_Framework_for_SVM.html">94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</a></p>
<p>20 0.023995193 <a title="26-tfidf-20" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.093), (1, -0.016), (2, 0.014), (3, 0.015), (4, -0.011), (5, -0.046), (6, 0.022), (7, 0.077), (8, -0.005), (9, -0.009), (10, 0.005), (11, 0.014), (12, 0.007), (13, 0.048), (14, 0.015), (15, 0.035), (16, 0.033), (17, -0.001), (18, 0.024), (19, -0.018), (20, -0.012), (21, -0.044), (22, 0.053), (23, 0.037), (24, -0.009), (25, 0.036), (26, 0.014), (27, -0.027), (28, 0.052), (29, 0.025), (30, -0.033), (31, -0.047), (32, 0.007), (33, 0.001), (34, -0.018), (35, -0.002), (36, -0.003), (37, 0.062), (38, -0.009), (39, -0.126), (40, 0.077), (41, 0.016), (42, -0.008), (43, 0.087), (44, 0.036), (45, -0.132), (46, 0.048), (47, -0.078), (48, 0.004), (49, 0.127)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90474147 <a title="26-lsi-1" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<p>Author: Hans-Georg Zimmermann, Ralph Neuneier, Ralph Grothmann</p><p>Abstract: This paper deals with a neural network architecture which establishes a portfolio management system similar to the Black / Litterman approach. This allocation scheme distributes funds across various securities or ﬁnancial markets while simultaneously complying with speciﬁc allocation constraints which meet the requirements of an investor. The portfolio optimization algorithm is modeled by a feedforward neural network. The underlying expected return forecasts are based on error correction neural networks (ECNN), which utilize the last model error as an auxiliary input to evaluate their own misspeciﬁcation. The portfolio optimization is implemented such that (i.) the allocations comply with investor’s constraints and that (ii.) the risk of the portfolio can be controlled. We demonstrate the proﬁtability of our approach by constructing internationally diversiﬁed portfolios across different ﬁnancial markets of the G7 contries. It turns out, that our approach is superior to a preset benchmark portfolio. ¢ £¡ 1 Introduction: Portfolio-Management We integrate the portfolio optimization algorithm suggested by Black / Litterman [1] into a neural network architecture. Combining the mean-variance theory [5] with the capital asset pricing model (CAPM) [7], this approach utilizes excess returns of the CAPM equilibrium to deﬁne a neutral, well balanced benchmark portfolio. Deviations from the benchmark allocation are only allowed within preset boundaries. Hence, as an advantage, there are no unrealistic solutions (e. g. large short positions, huge portfolio changes). Moreover, there is no need of formulating return expectations for all assets. In contrast to Black / Litterman, excess return forecasts are estimated by time-delay recurrent error correction neural networks [8]. Investment decisions which comply with given allocation constraints are derived from these predictions. The risk exposure of the portfolio is implicitly controlled by a parameter-optimizing task over time (sec. 3 and 5). Our approach consists of the following three steps: (i.) Construction of forecast models on the basis of error correction neural networks (ECNN) for all assets (sec. 2).   ¤§© © © § ¥ ¦¨¦¤ To whom correspondence should be addressed: Georg.Zimmermann@mchp.siemens.de.  ¤ ¤ (ii.) Computation of excess returns by a higher-level feedforward network (sec. 3 and 4). By this, the proﬁtability of an asset with respect to all others is measured. on the basis of the excess returns. (iii.) Optimization of the investment proportions Allocation constraints ensure, that the investment proportions may deviate from a given benchmark only within predeﬁned intervals (sec. 3 and 4). £ § ¨¡ ¥ £¡ ¦¤¢  ¡ © ¡ © Finally, we apply our neural network based portfolio management system to an asset allocation problem concerning the G7 countries (sec. 6). 2 Forecasting by Error Correction Neural Networks Most dynamical systems are driven by a superposition of autonomous development and external inﬂuences [8]. For discrete time grids, such a dynamics can be described by a recurrent state transition and an output equation (Eq. 1). ¥   § § state transition eq. output eq. (1)  $</p><p>2 0.44750327 <a title="26-lsi-2" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>Author: Nicolò Cesa-bianchi, Alex Conconi, Claudio Gentile</p><p>Abstract: In this paper we show that on-line algorithms for classiﬁcation and regression can be naturally used to obtain hypotheses with good datadependent tail bounds on their risk. Our results are proven without requiring complicated concentration-of-measure arguments and they hold for arbitrary on-line learning algorithms. Furthermore, when applied to concrete on-line algorithms, our results yield tail bounds that in many cases are comparable or better than the best known bounds.</p><p>3 0.40819734 <a title="26-lsi-3" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>Author: Jyrki Kivinen, Alex J. Smola, Robert C. Williamson</p><p>Abstract: We consider online learning in a Reproducing Kernel Hilbert Space. Our method is computationally efﬁcient and leads to simple algorithms. In particular we derive update equations for classiﬁcation, regression, and novelty detection. The inclusion of the -trick allows us to give a robust parameterization. Moreover, unlike in batch learning where the -trick only applies to the -insensitive loss function we are able to derive general trimmed-mean types of estimators such as for Huber’s robust loss.     ¡</p><p>4 0.38553953 <a title="26-lsi-4" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>Author: M. Schmitt</p><p>Abstract: Recurrent neural networks of analog units are computers for realvalued functions. We study the time complexity of real computation in general recurrent neural networks. These have sigmoidal, linear, and product units of unlimited order as nodes and no restrictions on the weights. For networks operating in discrete time, we exhibit a family of functions with arbitrarily high complexity, and we derive almost tight bounds on the time required to compute these functions. Thus, evidence is given of the computational limitations that time-bounded analog recurrent neural networks are subject to. 1</p><p>5 0.37999389 <a title="26-lsi-5" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>Author: K. Wong, F. Li</p><p>Abstract: We propose a method for the fast estimation of hyperparameters in large networks, based on the linear response relation in the cavity method, and an empirical measurement of the Green's function. Simulation results show that it is efficient and precise, when compared with cross-validation and other techniques which require matrix inversion. 1</p><p>6 0.37890592 <a title="26-lsi-6" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>7 0.3767353 <a title="26-lsi-7" href="./nips-2001-Geometrical_Singularities_in_the_Neuromanifold_of_Multilayer_Perceptrons.html">83 nips-2001-Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons</a></p>
<p>8 0.35663539 <a title="26-lsi-8" href="./nips-2001-Grammar_Transfer_in_a_Second_Order_Recurrent_Neural_Network.html">85 nips-2001-Grammar Transfer in a Second Order Recurrent Neural Network</a></p>
<p>9 0.3540712 <a title="26-lsi-9" href="./nips-2001-Switch_Packet_Arbitration_via_Queue-Learning.html">177 nips-2001-Switch Packet Arbitration via Queue-Learning</a></p>
<p>10 0.35306755 <a title="26-lsi-10" href="./nips-2001-Estimating_Car_Insurance_Premia%3A_a_Case_Study_in_High-Dimensional_Data_Inference.html">70 nips-2001-Estimating Car Insurance Premia: a Case Study in High-Dimensional Data Inference</a></p>
<p>11 0.34240651 <a title="26-lsi-11" href="./nips-2001-Hyperbolic_Self-Organizing_Maps_for_Semantic_Navigation.html">90 nips-2001-Hyperbolic Self-Organizing Maps for Semantic Navigation</a></p>
<p>12 0.33508372 <a title="26-lsi-12" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>13 0.32810909 <a title="26-lsi-13" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>14 0.31960216 <a title="26-lsi-14" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>15 0.30288041 <a title="26-lsi-15" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<p>16 0.30029717 <a title="26-lsi-16" href="./nips-2001-Learning_Body_Pose_via_Specialized_Maps.html">108 nips-2001-Learning Body Pose via Specialized Maps</a></p>
<p>17 0.29896852 <a title="26-lsi-17" href="./nips-2001-Rao-Blackwellised_Particle_Filtering_via_Data_Augmentation.html">156 nips-2001-Rao-Blackwellised Particle Filtering via Data Augmentation</a></p>
<p>18 0.2958895 <a title="26-lsi-18" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>19 0.29462636 <a title="26-lsi-19" href="./nips-2001-Improvisation_and_Learning.html">91 nips-2001-Improvisation and Learning</a></p>
<p>20 0.29156461 <a title="26-lsi-20" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.016), (17, 0.022), (19, 0.022), (27, 0.078), (30, 0.088), (38, 0.013), (59, 0.017), (72, 0.079), (79, 0.013), (89, 0.446), (91, 0.1)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81586319 <a title="26-lda-1" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<p>Author: Hans-Georg Zimmermann, Ralph Neuneier, Ralph Grothmann</p><p>Abstract: This paper deals with a neural network architecture which establishes a portfolio management system similar to the Black / Litterman approach. This allocation scheme distributes funds across various securities or ﬁnancial markets while simultaneously complying with speciﬁc allocation constraints which meet the requirements of an investor. The portfolio optimization algorithm is modeled by a feedforward neural network. The underlying expected return forecasts are based on error correction neural networks (ECNN), which utilize the last model error as an auxiliary input to evaluate their own misspeciﬁcation. The portfolio optimization is implemented such that (i.) the allocations comply with investor’s constraints and that (ii.) the risk of the portfolio can be controlled. We demonstrate the proﬁtability of our approach by constructing internationally diversiﬁed portfolios across different ﬁnancial markets of the G7 contries. It turns out, that our approach is superior to a preset benchmark portfolio. ¢ £¡ 1 Introduction: Portfolio-Management We integrate the portfolio optimization algorithm suggested by Black / Litterman [1] into a neural network architecture. Combining the mean-variance theory [5] with the capital asset pricing model (CAPM) [7], this approach utilizes excess returns of the CAPM equilibrium to deﬁne a neutral, well balanced benchmark portfolio. Deviations from the benchmark allocation are only allowed within preset boundaries. Hence, as an advantage, there are no unrealistic solutions (e. g. large short positions, huge portfolio changes). Moreover, there is no need of formulating return expectations for all assets. In contrast to Black / Litterman, excess return forecasts are estimated by time-delay recurrent error correction neural networks [8]. Investment decisions which comply with given allocation constraints are derived from these predictions. The risk exposure of the portfolio is implicitly controlled by a parameter-optimizing task over time (sec. 3 and 5). Our approach consists of the following three steps: (i.) Construction of forecast models on the basis of error correction neural networks (ECNN) for all assets (sec. 2).   ¤§© © © § ¥ ¦¨¦¤ To whom correspondence should be addressed: Georg.Zimmermann@mchp.siemens.de.  ¤ ¤ (ii.) Computation of excess returns by a higher-level feedforward network (sec. 3 and 4). By this, the proﬁtability of an asset with respect to all others is measured. on the basis of the excess returns. (iii.) Optimization of the investment proportions Allocation constraints ensure, that the investment proportions may deviate from a given benchmark only within predeﬁned intervals (sec. 3 and 4). £ § ¨¡ ¥ £¡ ¦¤¢  ¡ © ¡ © Finally, we apply our neural network based portfolio management system to an asset allocation problem concerning the G7 countries (sec. 6). 2 Forecasting by Error Correction Neural Networks Most dynamical systems are driven by a superposition of autonomous development and external inﬂuences [8]. For discrete time grids, such a dynamics can be described by a recurrent state transition and an output equation (Eq. 1). ¥   § § state transition eq. output eq. (1)  $</p><p>2 0.36615837 <a title="26-lda-2" href="./nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">102 nips-2001-KLD-Sampling: Adaptive Particle Filters</a></p>
<p>Author: Dieter Fox</p><p>Abstract: Over the last years, particle ﬁlters have been applied with great success to a variety of state estimation problems. We present a statistical approach to increasing the efﬁciency of particle ﬁlters by adapting the size of sample sets on-the-ﬂy. The key idea of the KLD-sampling method is to bound the approximation error introduced by the sample-based representation of the particle ﬁlter. The name KLD-sampling is due to the fact that we measure the approximation error by the Kullback-Leibler distance. Our adaptation approach chooses a small number of samples if the density is focused on a small part of the state space, and it chooses a large number of samples if the state uncertainty is high. Both the implementation and computation overhead of this approach are small. Extensive experiments using mobile robot localization as a test application show that our approach yields drastic improvements over particle ﬁlters with ﬁxed sample set sizes and over a previously introduced adaptation technique.</p><p>3 0.36231285 <a title="26-lda-3" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<p>Author: Eran Segal, Daphne Koller, Dirk Ormoneit</p><p>Abstract: Many domains are naturally organized in an abstraction hierarchy or taxonomy, where the instances in “nearby” classes in the taxonomy are similar. In this paper, we provide a general probabilistic framework for clustering data into a set of classes organized as a taxonomy, where each class is associated with a probabilistic model from which the data was generated. The clustering algorithm simultaneously optimizes three things: the assignment of data instances to clusters, the models associated with the clusters, and the structure of the abstraction hierarchy. A unique feature of our approach is that it utilizes global optimization algorithms for both of the last two steps, reducing the sensitivity to noise and the propensity to local maxima that are characteristic of algorithms such as hierarchical agglomerative clustering that only take local steps. We provide a theoretical analysis for our algorithm, showing that it converges to a local maximum of the joint likelihood of model and data. We present experimental results on synthetic data, and on real data in the domains of gene expression and text.</p><p>4 0.35861045 <a title="26-lda-4" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>Author: Bernd Heisele, Thomas Serre, Massimiliano Pontil, Thomas Vetter, Tomaso Poggio</p><p>Abstract: We describe an algorithm for automatically learning discriminative components of objects with SVM classiﬁers. It is based on growing image parts by minimizing theoretical bounds on the error probability of an SVM. Component-based face classiﬁers are then combined in a second stage to yield a hierarchical SVM classiﬁer. Experimental results in face classiﬁcation show considerable robustness against rotations in depth and suggest performance at signiﬁcantly better level than other face detection systems. Novel aspects of our approach are: a) an algorithm to learn component-based classiﬁcation experts and their combination, b) the use of 3-D morphable models for training, and c) a maximum operation on the output of each component classiﬁer which may be relevant for biological models of visual recognition.</p><p>5 0.35672629 <a title="26-lda-5" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>Author: Mário Figueiredo</p><p>Abstract: In this paper we introduce a new sparseness inducing prior which does not involve any (hyper)parameters that need to be adjusted or estimated. Although other applications are possible, we focus here on supervised learning problems: regression and classiﬁcation. Experiments with several publicly available benchmark data sets show that the proposed approach yields state-of-the-art performance. In particular, our method outperforms support vector machines and performs competitively with the best alternative techniques, both in terms of error rates and sparseness, although it involves no tuning or adjusting of sparsenesscontrolling hyper-parameters.</p><p>6 0.35563114 <a title="26-lda-6" href="./nips-2001-%28Not%29_Bounding_the_True_Error.html">1 nips-2001-(Not) Bounding the True Error</a></p>
<p>7 0.35517174 <a title="26-lda-7" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>8 0.35514063 <a title="26-lda-8" href="./nips-2001-The_Method_of_Quantum_Clustering.html">185 nips-2001-The Method of Quantum Clustering</a></p>
<p>9 0.35467762 <a title="26-lda-9" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>10 0.35422689 <a title="26-lda-10" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>11 0.35418493 <a title="26-lda-11" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>12 0.35293105 <a title="26-lda-12" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>13 0.35236096 <a title="26-lda-13" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>14 0.35164559 <a title="26-lda-14" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>15 0.35117418 <a title="26-lda-15" href="./nips-2001-A_kernel_method_for_multi-labelled_classification.html">22 nips-2001-A kernel method for multi-labelled classification</a></p>
<p>16 0.35076946 <a title="26-lda-16" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>17 0.35063091 <a title="26-lda-17" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>18 0.34942362 <a title="26-lda-18" href="./nips-2001-Model_Based_Population_Tracking_and_Automatic_Detection_of_Distribution_Changes.html">122 nips-2001-Model Based Population Tracking and Automatic Detection of Distribution Changes</a></p>
<p>19 0.3491298 <a title="26-lda-19" href="./nips-2001-ALGONQUIN_-_Learning_Dynamic_Noise_Models_From_Noisy_Speech_for_Robust_Speech_Recognition.html">4 nips-2001-ALGONQUIN - Learning Dynamic Noise Models From Noisy Speech for Robust Speech Recognition</a></p>
<p>20 0.34844419 <a title="26-lda-20" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
