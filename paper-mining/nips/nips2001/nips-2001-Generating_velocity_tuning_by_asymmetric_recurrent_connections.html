<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-82" href="#">nips2001-82</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</h1>
<br/><p>Source: <a title="nips-2001-82-pdf" href="http://papers.nips.cc/paper/1995-generating-velocity-tuning-by-asymmetric-recurrent-connections.pdf">pdf</a></p><p>Author: Xiaohui Xie, Martin A. Giese</p><p>Abstract: Asymmetric lateral connections are one possible mechanism that can account for the direction selectivity of cortical neurons. We present a mathematical analysis for a class of these models. Contrasting with earlier theoretical work that has relied on methods from linear systems theory, we study the network’s nonlinear dynamic properties that arise when the threshold nonlinearity of the neurons is taken into account. We show that such networks have stimulus-locked traveling pulse solutions that are appropriate for modeling the responses of direction selective cortical neurons. In addition, our analysis shows that outside a certain regime of stimulus speeds the stability of this solutions breaks down giving rise to another class of solutions that are characterized by speciﬁc spatiotemporal periodicity. This predicts that if direction selectivity in the cortex is mainly achieved by asymmetric lateral connections lurching activity waves might be observable in ensembles of direction selective cortical neurons within appropriate regimes of the stimulus speed.</p><p>Reference: <a title="nips-2001-82-reference" href="../nips2001_reference/nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Generating velocity tuning by asymmetric recurrent connections £ ¢  ¡     Xiaohui Xie and Martin A. [sent-1, score-0.378]
</p><p>2 edu    £  ¤  ¥  Abstract Asymmetric lateral connections are one possible mechanism that can account for the direction selectivity of cortical neurons. [sent-5, score-0.556]
</p><p>3 Contrasting with earlier theoretical work that has relied on methods from linear systems theory, we study the network’s nonlinear dynamic properties that arise when the threshold nonlinearity of the neurons is taken into account. [sent-7, score-0.4]
</p><p>4 We show that such networks have stimulus-locked traveling pulse solutions that are appropriate for modeling the responses of direction selective cortical neurons. [sent-8, score-1.228]
</p><p>5 In addition, our analysis shows that outside a certain regime of stimulus speeds the stability of this solutions breaks down giving rise to another class of solutions that are characterized by speciﬁc spatiotemporal periodicity. [sent-9, score-0.836]
</p><p>6 This predicts that if direction selectivity in the cortex is mainly achieved by asymmetric lateral connections lurching activity waves might be observable in ensembles of direction selective cortical neurons within appropriate regimes of the stimulus speed. [sent-10, score-1.658]
</p><p>7 1 Introduction Classical models for the direction selectivity in the primary visual cortex have assumed feed-forward mechanisms, like multiplication or gating of afferent thalamo-cortical inputs (e. [sent-11, score-0.426]
</p><p>8 [1, 2, 3]), or linear spatio-temporal ﬁltering followed by a nonlinear operation (e. [sent-13, score-0.125]
</p><p>9 The existence of strong lateral connectivity has motivated modeling studies, which have shown that the properties of direction selective cortical neurons can also be accurately reproduced by recurrent neural network models with asymmetric lateral excitatory or inhibitory connections [6, 7]. [sent-16, score-0.908]
</p><p>10 Since these biophysically detailed models are not accessible for mathematical analysis, more simpliﬁed models appropriate for a mathematical analysis have been proposed. [sent-17, score-0.187]
</p><p>11 Such analysis was based on methods from linear systems theory by neglecting the nonlinear properties of the neurons [6, 8, 9]. [sent-18, score-0.289]
</p><p>12 The nonlinear dynamic phenomena resulting from the interplay between the recurrent connectivity and the nonlinear  threshold characteristics of the neurons have not been tractable in this theoretical framework. [sent-19, score-0.554]
</p><p>13 In this paper we present a mathematical analysis that takes the nonlinear behavior of the individual neurons into account. [sent-20, score-0.322]
</p><p>14 We present the result of the analysis of such networks for two types of threshold nonlinearities, for which closed-form analytical solutions of the network dynamics can be derived. [sent-21, score-0.491]
</p><p>15 We show that such nonlinear networks have a class of form-stable solutions, in the following signiﬁed as stimulus-locked traveling pulses, which are suitable for modeling the activity of direction selective neurons. [sent-22, score-0.865]
</p><p>16 Contrary to networks with linear neurons, the stability of the traveling pulse solutions in the nonlinear network can break down giving raise to another class of solutions (lurching activity waves) that is characterized by spatio-temporal periodicity. [sent-23, score-1.571]
</p><p>17 Our mathematical analysis and simulations showed that recurrent models with biologically realistic degrees of direction selectivity typically also show transitions between traveling pulse and lurching solutions. [sent-24, score-1.48]
</p><p>18 2 Basic model Dynamic neural ﬁelds have been proposed to model the average behavior of a large ensembles of neurons [10, 11, 12]. [sent-25, score-0.175]
</p><p>19 The scalar neural activity distribution characterizes the average activity at time of an ensemble of functionally similar neurons that code for the position , where can be any abstract stimulus parameter. [sent-26, score-0.584]
</p><p>20 By the continuous approximation of biophysically discrete neuronal dynamics it is in some cases possible to treat the nonlinear neural dynamics analytically. [sent-27, score-0.5]
</p><p>21 ¨ ¦ ¤ ¢ ©§¥£¡     ¦  ¢  ¨ ¦ ¤ ¢ ©§¥¡  is described by: 6 ¨ ¦ ¤ ¢ 7©©5£¡  & ¢ 1 ¨ ¨ ¦ ¤ & ¢ '2©©©§'£¡  3 4        ¢  The ﬁeld dynamics of neural activation variable  (1)  ¨ ¦ ¤ ¢ ©©¥¡  ¡ ) ¨ & ¢ 0©('%$£"! [sent-28, score-0.294]
</p><p>22 ©§¥£¡ # ¢ ¡    ¨ ¦ ¤ ¢           ¦   This dynamics is essentially a leaky integrator with a total input on the right hand side, which includes a feedfoward input term and a feedback term that integrates the recurrent contributions from other laterally connected neurons. [sent-29, score-0.244]
</p><p>23 The interaction kernel characterizes the average synaptic connection strength between the neurons coding position and the neurons coding position . [sent-30, score-0.416]
</p><p>24 This function is nonlinear and monotonically increasing, and introduces the nonlinearity that makes it difﬁcult to analyze the network dynamics. [sent-32, score-0.214]
</p><p>25 ¨ ¦ ¤ ¢ ©§¥¡  3  ¨ & 98" ¢ # ¢ ¡  )  ¢  & ¢  With a moving stimulus at constant velocity , it is often convenient to transform the static coordinate to the moving frame by changing variable . [sent-33, score-0.713]
</p><p>26 Therefore the traveling pulse solution driven by the moving stimulus can be found by solving Eq. [sent-37, score-1.187]
</p><p>27 (3), and the stability of the traveling pulse can be studied by perturbing the stationary solution in Eq. [sent-38, score-1.197]
</p><p>28 For this purpose we consider only one-dimensional neural ﬁelds and assume that the nonlinear activation function is either a step function or a linear threshold function. [sent-44, score-0.342]
</p><p>29 )  3 Step activation function © ¨  §  ¥ ¦  ¨ ¤¡    ¢  ¢ £  ¨ ¤¡    ¡  © ¨ §  ¨   ¡ ¡0)  We ﬁrst consider step activation function where when and zero otherwise. [sent-45, score-0.256]
</p><p>30 This form of activation function approximates activities of neurons, which, by saturation, are either active or inactive. [sent-46, score-0.155]
</p><p>31 For the one-dimensional case, we assume that only a single stationary excited regime with ( )exists that is located between the points . [sent-47, score-0.283]
</p><p>32 Only neurons inside this regime contribute to the integral, and accordingly Eq. [sent-48, score-0.281]
</p><p>33 The spatial shape of the stationary solution obeys the ordinary differential equation ¤ F¨ A  ¡ 3  ¨   ¡  A  Q  ¨   # ¡  A  ¨  #  Q  A    ¡  A  ¨  ¨ ¡  A  Q  ¡ A   D  Q A  1 D  1    Q£ A   ¨ ¤¡    ! [sent-50, score-0.221]
</p><p>34 The solution of the above equation can be found by treating as ﬁxed parameters, and solving Eq. [sent-53, score-0.131]
</p><p>35 1 Stability of the traveling pulse solution The stability of the traveling pulse solution can be analyzed by perturbing the stationary solution in the moving coordinate system. [sent-60, score-2.327]
</p><p>36 The traveling pulse solution is asymptotically stable only if the real parts of all eigenvalues are negative. [sent-68, score-1.002]
</p><p>37 2 Simulation results of step activation function model We use the following function " ¨  ! [sent-70, score-0.128]
</p><p>38   # ¡      ©¥  ¨ ¢ '£¡  as an example interaction kernel, numerically simulate the dynamics and compare the simulation results with the above mathematical analysis. [sent-73, score-0.332]
</p><p>39 The stimulus used is a moving bar with constant width and amplitude. [sent-74, score-0.33]
</p><p>40 Panel (a) shows the speed tuning curve plotted as the dependence of the peak activity of the traveling pulse as function of the stimulus velocity . [sent-77, score-1.341]
</p><p>41 Panel (b) shows the maximum real part of the eigenvalues obtained from Eq. [sent-79, score-0.145]
</p><p>42 For small and large stimulus velocities maximum of the real parts of becomes positive indicating a loss of stability of the form-stable solution. [sent-81, score-0.55]
</p><p>43 To verify this result we calculated the variability of the peak activity over time in simulation. [sent-82, score-0.237]
</p><p>44 Panel (c) shows the average variability as function of the stimulus velocity. [sent-83, score-0.223]
</p><p>45 At the velocities for which the eigenvalues indicate a loss of stability the variability of the amplitudes suddenly increases, consistent with our interpretation as a loss of the form stability of the solution. [sent-84, score-0.686]
</p><p>46 Panel (e) shows the propagation of the form-stable traveling pulse. [sent-86, score-0.417]
</p><p>47 Panel (d) shows the solution that arises when stability is lost. [sent-87, score-0.337]
</p><p>48 This solution is characterized by a spatio-temporal periodicity that is deﬁned in the moving coordinate system by , where and are constants that depend on the network dynamics. [sent-88, score-0.383]
</p><p>49 ) 05  ' (¡    D  4 Linear threshold activation function © 4¤    ¤ ¡@8  9 7       ¨ ¤0)   ¡  In this case, the activation function is taken to be . [sent-94, score-0.345]
</p><p>50 Cortical neurons typically operate far below the saturation level. [sent-95, score-0.171]
</p><p>51 The linear threshold activation function is thus more suitable to capture the properties of real neurons while still permitting a relatively simple theoretical analysis. [sent-96, score-0.436]
</p><p>52 We consider a ring network with periodic boundary conditions. [sent-97, score-0.17]
</p><p>53 The dynamics is given by ¥  6  (11)  H¨¦ A 6 I©§¤ (¡  3 4  6 51  ¨ ¦ ¤ ©§I&  &  A (¡  5 ¨ 2G"(" & A # A ¡  &  F #b A ! [sent-98, score-0.166]
</p><p>54 We chose this form because it simpliﬁes the mathematical analysis of ring networks. [sent-101, score-0.115]
</p><p>55 Again, we consider a moving stimulus with velocity and analyze the network in the moving frame. [sent-102, score-0.675]
</p><p>56 1 General solutions and stability analysis Because the activation function has linear threshold characteristics, inside the excited regime for which the total input ( ) is positive the system is linear. [sent-104, score-0.819]
</p><p>57 One approach to solve this dynamics is therefore to ﬁnd the solutions to the differential equation assuming the boundaries of the excited regime are given. [sent-105, score-0.557]
</p><p>58 The conditions at the boundaries lead to a set of self-consistent equations for the solutions to satisfy, from which the boundaries can be determined. [sent-106, score-0.221]
</p><p>59 The stationary solution in moving coordinates can ¤  D     ¦   © R  where then be written as  )  where matrix is deﬁned as the diagonal matrix . [sent-111, score-0.336]
</p><p>60 The above solution has to satisfy two boundary vector conditions, from which and can be determined. [sent-113, score-0.143]
</p><p>61 2  ¦ ¥  (  3  4  £ A  ¦ ¥ 95 2    A  (  )  Stability of this traveling pulse solution can be analyzed by linear perturbation. [sent-114, score-0.885]
</p><p>62 Note that perturbed boundaries points do not contribute to the linearized perturbed dynamics since ,where is the total input at the stationary solution of the moving frame on right hand side of Eq. [sent-115, score-0.817]
</p><p>63 Therefore, the linearized perturbation dynamics can be fully characterized by the perturbed Fourier modes with ﬁxed boundaries. [sent-117, score-0.385]
</p><p>64 Hence, the stability of the traveling pulse solution is determined by the eigenvalues of matrix . [sent-118, score-1.157]
</p><p>65 If the largest real part of eigenvalues of is negative, then the stimulus locking traveling pulse is stable. [sent-119, score-1.122]
</p><p>66 2 Simpliﬁed linear threshold network The general solution introduced above requires the solution of an equation system. [sent-121, score-0.416]
</p><p>67 Next we consider a special simple model for which an exact solution can be found that contains only two Fourier components for the interaction kernel and the input . [sent-123, score-0.211]
</p><p>68 For this model a closed form solution and stability analysis is presented, that at the same time provides insight in some rather general properties of linear threshold networks. [sent-124, score-0.453]
</p><p>69 3  The interaction kernel and feedforward input are assumed to have the following form: (13)  ¨  A G F D 5¡ ¢E   #  ! [sent-125, score-0.111]
</p><p>70 B C ¨  A¡ ("  This network was used by Hansel and Sompolinsky as model of cortical orientation selectivity [14]. [sent-127, score-0.409]
</p><p>71 However different from their network, we consider here an asymmetric interaction kernel and a form-constant moving stimulus . [sent-128, score-0.529]
</p><p>72 In terms of these two order parameters plus the phase variable, the stimulus-locked traveling pulse solution and its stability conditions can be expressed analytically. [sent-132, score-1.066]
</p><p>73 Similar to the results of step function model, panel (A) shows the speed tuning curve plotted as values of order parameters and as function of different stimulus velocities . [sent-136, score-0.505]
</p><p>74 Panel (B) shows the largest real part of the eigenvalues of a stability matrix that can be obtained by linearizing the order parameter dynamics around the stationary solution. [sent-137, score-0.646]
</p><p>75 Panel (C) shows the average variations as function of the stimulus velocity. [sent-138, score-0.184]
</p><p>76 The space-time evolution of the form-stable traveling pulse is shown in panel (E); the form-unstable lurching wave is shown in panel (D). [sent-139, score-1.432]
</p><p>77 Thus we found that lurching wave solution type arises very robustly for both types of threshold functions when the network achieved substantial direction selective behavior. [sent-140, score-0.785]
</p><p>78 5 Conclusion We have presented different methods for an analysis of the nonlinear dynamics of simple recurrent neural models for the direction selectivity of cortical neurons. [sent-142, score-0.814]
</p><p>79 Compared to earlier works, we have taken into account the essentially nonlinear effects that are introduced by the nonlinear threshold characteristics of the cortical neurons. [sent-143, score-0.435]
</p><p>80 The key result of our work is that such networks have a class of form-stable traveling pulse solutions that behave similar as the solutions of linear spatio-temporal ﬁltering models within a certain regime of stimulus speeds. [sent-144, score-1.335]
</p><p>81 By the essential nonlinearity of the network, however, bifurcations can arise for which the traveling pulse solutions become unstable. [sent-145, score-0.919]
</p><p>82 We observed that in this case a new class of spatio-temporally periodic solutions (”lurching activity waves”) arises. [sent-146, score-0.261]
</p><p>83 Since we found this solution type very frequently for networks with substantial direction selectivity our analysis predicts that such ”lurching behavior” might be observable in visual cortex areas if, in fact, the direction selectivity is essentially based on asymmetric lateral connectivity. [sent-147, score-1.146]
</p><p>84 The synaptic veto mechanism: does it underlie direction and orientation selectivity in the visual cortex. [sent-153, score-0.365]
</p><p>85 1  C  0 -40  -30  -20  10  0  0  2  Variation  a  Peak Activity  3  1 0 -50  10  0 Velocity  Velocity  D  E TIME  e TIME  d  SPACE  SPACE  Figure 1: Traveling pulse solution and its stability in two classes of models. [sent-179, score-0.649]
</p><p>86 In the left side shown is the step activation function model, while the linear threshold model is drawn in the right. [sent-180, score-0.245]
</p><p>87 Panel (a) and (A) show the velocity tuning curves of the traveling pulse in terms of its peak activity in (a) or order parameters in (A). [sent-181, score-1.127]
</p><p>88 Panel (b) and (B) plot the largest real parts of eigenvalues of a stability matrix obtained from perturbed linear dynamics around the stationary solution. [sent-183, score-0.749]
</p><p>89 Outside certain range of stimulus velocities the largest real part of the eigenvalues become positive indicating a loss of stability of the form-stable solution. [sent-184, score-0.677]
</p><p>90 A nonzero variance signiﬁes a loss of stability for traveling pulse solutions, which is consistent with eigenvalue analysis in Panel (b) and (B). [sent-186, score-1.028]
</p><p>91 A color coded plot of spatialtemporal evolution of the activity is shown in panels (d) and (e), and in (D) and (E). [sent-187, score-0.219]
</p><p>92 Panel (e) and (E) show the propagation of the form-stable peak over time; panel (d) and (D) show the lurching activity wave that arises when stability is lost. [sent-188, score-0.89]
</p><p>93 The stimulus is a moving bar with width and amplitude . [sent-190, score-0.33]
</p><p>94 Parameters used in linear threshold model are , , and . [sent-191, score-0.117]
</p><p>95 Modeling direction selectivity of simple cells in striate visual cortex within the framework of the canonical microcircuit. [sent-209, score-0.426]
</p><p>96 Model circuit of spiking neurons generating directional selectivity in simple cells. [sent-215, score-0.381]
</p><p>97 Analysis of direction selectivity arising from recurrent cortical interactions. [sent-220, score-0.524]
</p><p>98 An architectural hypothesis for direction selectivity in the visual cortex: the role of spatially asymmetric intracortical inhibition. [sent-226, score-0.483]
</p><p>99 A mathematical theory of the functional dynamics of cortical and thalamic nervous tissue. [sent-229, score-0.381]
</p><p>100 Effects of delay on the type and velocity of travelling pulses in neuronal networks with spatially decaying connectivity. [sent-248, score-0.289]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('traveling', 0.417), ('pulse', 0.34), ('selectivity', 0.218), ('stability', 0.209), ('lurching', 0.206), ('qy', 0.201), ('stimulus', 0.184), ('panel', 0.182), ('dynamics', 0.166), ('moving', 0.146), ('neurons', 0.137), ('velocity', 0.131), ('activation', 0.128), ('cortical', 0.123), ('activity', 0.116), ('solutions', 0.113), ('regime', 0.112), ('direction', 0.105), ('solution', 0.1), ('nonlinear', 0.097), ('eigenvalues', 0.091), ('fourier', 0.09), ('stationary', 0.09), ('threshold', 0.089), ('asymmetric', 0.088), ('peak', 0.082), ('excited', 0.081), ('recurrent', 0.078), ('waves', 0.077), ('perturbed', 0.075), ('interaction', 0.072), ('lateral', 0.07), ('velocities', 0.068), ('network', 0.068), ('wave', 0.067), ('panels', 0.065), ('selective', 0.064), ('cortex', 0.061), ('mathematical', 0.061), ('linearized', 0.061), ('soc', 0.061), ('opt', 0.057), ('boundaries', 0.054), ('real', 0.054), ('reads', 0.054), ('bingen', 0.051), ('giese', 0.051), ('koch', 0.051), ('frame', 0.05), ('nonlinearity', 0.049), ('hansel', 0.045), ('biol', 0.045), ('cybern', 0.045), ('simpli', 0.043), ('boundary', 0.043), ('visual', 0.042), ('characterized', 0.042), ('tuning', 0.041), ('perturbation', 0.041), ('perturbing', 0.041), ('connections', 0.04), ('kernel', 0.039), ('variability', 0.039), ('modeling', 0.038), ('biophysically', 0.038), ('pulses', 0.038), ('ensembles', 0.038), ('evolution', 0.038), ('largest', 0.036), ('spatiotemporal', 0.036), ('loss', 0.035), ('saturation', 0.034), ('transforming', 0.034), ('neuronal', 0.033), ('simulation', 0.033), ('inside', 0.032), ('periodic', 0.032), ('equation', 0.031), ('characterizes', 0.031), ('nervous', 0.031), ('curve', 0.03), ('spatially', 0.03), ('lines', 0.03), ('characteristics', 0.029), ('substantial', 0.029), ('static', 0.029), ('type', 0.029), ('linear', 0.028), ('biologically', 0.028), ('ltering', 0.028), ('networks', 0.028), ('arises', 0.028), ('activities', 0.027), ('connectivity', 0.027), ('rd', 0.027), ('ring', 0.027), ('coordinate', 0.027), ('analysis', 0.027), ('spiking', 0.026), ('observable', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="82-tfidf-1" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>Author: Xiaohui Xie, Martin A. Giese</p><p>Abstract: Asymmetric lateral connections are one possible mechanism that can account for the direction selectivity of cortical neurons. We present a mathematical analysis for a class of these models. Contrasting with earlier theoretical work that has relied on methods from linear systems theory, we study the network’s nonlinear dynamic properties that arise when the threshold nonlinearity of the neurons is taken into account. We show that such networks have stimulus-locked traveling pulse solutions that are appropriate for modeling the responses of direction selective cortical neurons. In addition, our analysis shows that outside a certain regime of stimulus speeds the stability of this solutions breaks down giving rise to another class of solutions that are characterized by speciﬁc spatiotemporal periodicity. This predicts that if direction selectivity in the cortex is mainly achieved by asymmetric lateral connections lurching activity waves might be observable in ensembles of direction selective cortical neurons within appropriate regimes of the stimulus speed.</p><p>2 0.1909226 <a title="82-tfidf-2" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>Author: Richard Hahnloser, Xiaohui Xie, H. S. Seung</p><p>Abstract: Integration in the head-direction system is a computation by which horizontal angular head velocity signals from the vestibular nuclei are integrated to yield a neural representation of head direction. In the thalamus, the postsubiculum and the mammillary nuclei, the head-direction representation has the form of a place code: neurons have a preferred head direction in which their ﬁring is maximal [Blair and Sharp, 1995, Blair et al., 1998, ?]. Integration is a difﬁcult computation, given that head-velocities can vary over a large range. Previous models of the head-direction system relied on the assumption that the integration is achieved in a ﬁring-rate-based attractor network with a ring structure. In order to correctly integrate head-velocity signals during high-speed head rotations, very fast synaptic dynamics had to be assumed. Here we address the question whether integration in the head-direction system is possible with slow synapses, for example excitatory NMDA and inhibitory GABA(B) type synapses. For neural networks with such slow synapses, rate-based dynamics are a good approximation of spiking neurons [Ermentrout, 1994]. We ﬁnd that correct integration during high-speed head rotations imposes strong constraints on possible network architectures.</p><p>3 0.15955989 <a title="82-tfidf-3" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>Author: Peter E. Latham</p><p>Abstract: Almost two decades ago , Hopfield [1] showed that networks of highly reduced model neurons can exhibit multiple attracting fixed points, thus providing a substrate for associative memory. It is still not clear, however, whether realistic neuronal networks can support multiple attractors. The main difficulty is that neuronal networks in vivo exhibit a stable background state at low firing rate, typically a few Hz. Embedding attractor is easy; doing so without destabilizing the background is not. Previous work [2, 3] focused on the sparse coding limit, in which a vanishingly small number of neurons are involved in any memory. Here we investigate the case in which the number of neurons involved in a memory scales with the number of neurons in the network. In contrast to the sparse coding limit, we find that multiple attractors can co-exist robustly with a stable background state. Mean field theory is used to understand how the behavior of the network scales with its parameters, and simulations with analog neurons are presented. One of the most important features of the nervous system is its ability to perform associative memory. It is generally believed that associative memory is implemented using attractor networks - experimental studies point in that direction [4- 7], and there are virtually no competing theoretical models. Perhaps surprisingly, however, it is still an open theoretical question whether attractors can exist in realistic neuronal networks. The</p><p>4 0.15761307 <a title="82-tfidf-4" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>Author: Shih-Chii Liu, Jörg Kramer, Giacomo Indiveri, Tobi Delbrück, Rodney J. Douglas</p><p>Abstract: We describe a programmable multi-chip VLSI neuronal system that can be used for exploring spike-based information processing models. The system consists of a silicon retina, a PIC microcontroller, and a transceiver chip whose integrate-and-ﬁre neurons are connected in a soft winner-take-all architecture. The circuit on this multi-neuron chip approximates a cortical microcircuit. The neurons can be conﬁgured for different computational properties by the virtual connections of a selected set of pixels on the silicon retina. The virtual wiring between the different chips is effected by an event-driven communication protocol that uses asynchronous digital pulses, similar to spikes in a neuronal system. We used the multi-chip spike-based system to synthesize orientation-tuned neurons using both a feedforward model and a feedback model. The performance of our analog hardware spiking model matched the experimental observations and digital simulations of continuous-valued neurons. The multi-chip VLSI system has advantages over computer neuronal models in that it is real-time, and the computational time does not scale with the size of the neuronal network.</p><p>5 0.13592425 <a title="82-tfidf-5" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>Author: Julian Eggert, Berthold BĂ¤uml</p><p>Abstract: Mesoscopical, mathematical descriptions of dynamics of populations of spiking neurons are getting increasingly important for the understanding of large-scale processes in the brain using simulations. In our previous work, integral equation formulations for population dynamics have been derived for a special type of spiking neurons. For Integrate- and- Fire type neurons , these formulations were only approximately correct. Here, we derive a mathematically compact, exact population dynamics formulation for Integrate- and- Fire type neurons. It can be shown quantitatively in simulations that the numerical correspondence with microscopically modeled neuronal populations is excellent. 1 Introduction and motivation The goal of the population dynamics approach is to model the time course of the collective activity of entire populations of functionally and dynamically similar neurons in a compact way, using a higher descriptionallevel than that of single neurons and spikes. The usual observable at the level of neuronal populations is the populationaveraged instantaneous firing rate A(t), with A(t)6.t being the number of neurons in the population that release a spike in an interval [t, t+6.t). Population dynamics are formulated in such a way, that they match quantitatively the time course of a given A(t), either gained experimentally or by microscopical, detailed simulation. At least three main reasons can be formulated which underline the importance of the population dynamics approach for computational neuroscience. First, it enables the simulation of extensive networks involving a massive number of neurons and connections, which is typically the case when dealing with biologically realistic functional models that go beyond the single neuron level. Second, it increases the analytical understanding of large-scale neuronal dynamics , opening the way towards better control and predictive capabilities when dealing with large networks. Third, it enables a systematic embedding of the numerous neuronal models operating at different descriptional scales into a generalized theoretic framework, explaining the relationships, dependencies and derivations of the respective models. Early efforts on population dynamics approaches date back as early as 1972, to the work of Wilson and Cowan [8] and Knight [4], which laid the basis for all current population-averaged graded-response models (see e.g. [6] for modeling work using these models). More recently, population-based approaches for spiking neurons were developed, mainly by Gerstner [3, 2] and Knight [5]. In our own previous work [1], we have developed a theoretical framework which enables to systematize and simulate a wide range of models for population-based dynamics. It was shown that the equations of the framework produce results that agree quantitatively well with detailed simulations using spiking neurons, so that they can be used for realistic simulations involving networks with large numbers of spiking neurons. Nevertheless, for neuronal populations composed of Integrate-and-Fire (I&F;) neurons, this framework was only correct in an approximation. In this paper, we derive the exact population dynamics formulation for I&F; neurons. This is achieved by reducing the I&F; population dynamics to a point process and by taking advantage of the particular properties of I&F; neurons. 2 2.1 Background: Integrate-and-Fire dynamics Differential form We start with the standard Integrate- and- Fire (I&F;) model in form of the wellknown differential equation [7] (1) which describes the dynamics of the membrane potential Vi of a neuron i that is modeled as a single compartment with RC circuit characteristics. The membrane relaxation time is in this case T = RC with R being the membrane resistance and C the membrane capacitance. The resting potential v R est is the stationary potential that is approached in the no-input case. The input arriving from other neurons is described in form of a current ji. In addition to eq. (1), which describes the integrate part of the I&F; model, the neuronal dynamics are completed by a nonlinear step. Every time the membrane potential Vi reaches a fixed threshold () from below, Vi is lowered by a fixed amount Ll > 0, and from the new value of the membrane potential integration according to eq. (1) starts again. if Vi(t) = () (from below) . (2) At the same time, it is said that the release of a spike occurred (i.e., the neuron fired), and the time ti = t of this singular event is stored. Here ti indicates the time of the most recent spike. Storing all the last firing times , we gain the sequence of spikes {t{} (spike ordering index j, neuronal index i). 2.2 Integral form Now we look at the single neuron in a neuronal compound. We assume that the input current contribution ji from presynaptic spiking neurons can be described using the presynaptic spike times tf, a response-function ~ and a connection weight WÂˇ . ',J ji(t) = Wi ,j ~(t - tf) (3) l: l: j f Integrating the I&F; equation (1) beginning at the last spiking time tT, which determines the initial condition by Vi(ti) = vi(ti - 0) - 6., where vi(ti - 0) is the membrane potential just before the neuron spikes, we get 1 Vi(t) = v Rest + fj(t - t:) + l: Wi ,j l: a(t - t:; t - tf) , j - Vi(t:)) e- S / T (4) f with the refractory function fj(s) = - (v Rest (5) and the alpha-function r ds</p><p>6 0.13526903 <a title="82-tfidf-6" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>7 0.13352412 <a title="82-tfidf-7" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>8 0.10954561 <a title="82-tfidf-8" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>9 0.10015196 <a title="82-tfidf-9" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>10 0.096127957 <a title="82-tfidf-10" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>11 0.094863333 <a title="82-tfidf-11" href="./nips-2001-A_Hierarchical_Model_of_Complex_Cells_in_Visual_Cortex_for_the_Binocular_Perception_of_Motion-in-Depth.html">10 nips-2001-A Hierarchical Model of Complex Cells in Visual Cortex for the Binocular Perception of Motion-in-Depth</a></p>
<p>12 0.089696981 <a title="82-tfidf-12" href="./nips-2001-Citcuits_for_VLSI_Implementation_of_Temporally_Asymmetric_Hebbian_Learning.html">49 nips-2001-Citcuits for VLSI Implementation of Temporally Asymmetric Hebbian Learning</a></p>
<p>13 0.089319512 <a title="82-tfidf-13" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>14 0.086433962 <a title="82-tfidf-14" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>15 0.085294046 <a title="82-tfidf-15" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>16 0.082148738 <a title="82-tfidf-16" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>17 0.081522748 <a title="82-tfidf-17" href="./nips-2001-Modeling_the_Modulatory_Effect_of_Attention_on_Human_Spatial_Vision.html">124 nips-2001-Modeling the Modulatory Effect of Attention on Human Spatial Vision</a></p>
<p>18 0.076310724 <a title="82-tfidf-18" href="./nips-2001-Characterizing_Neural_Gain_Control_using_Spike-triggered_Covariance.html">48 nips-2001-Characterizing Neural Gain Control using Spike-triggered Covariance</a></p>
<p>19 0.074887916 <a title="82-tfidf-19" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>20 0.073364221 <a title="82-tfidf-20" href="./nips-2001-Kernel_Feature_Spaces_and_Nonlinear_Blind_Souce_Separation.html">103 nips-2001-Kernel Feature Spaces and Nonlinear Blind Souce Separation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.199), (1, -0.251), (2, -0.148), (3, 0.019), (4, 0.12), (5, 0.044), (6, 0.003), (7, 0.027), (8, 0.057), (9, 0.049), (10, -0.047), (11, 0.087), (12, -0.043), (13, -0.019), (14, 0.042), (15, 0.027), (16, -0.143), (17, -0.016), (18, 0.025), (19, -0.033), (20, 0.007), (21, 0.041), (22, 0.108), (23, 0.011), (24, -0.024), (25, 0.129), (26, 0.047), (27, -0.18), (28, 0.074), (29, 0.029), (30, -0.135), (31, -0.038), (32, -0.041), (33, -0.03), (34, 0.05), (35, -0.013), (36, 0.089), (37, -0.105), (38, 0.027), (39, -0.074), (40, -0.021), (41, -0.014), (42, -0.0), (43, 0.049), (44, -0.03), (45, 0.111), (46, 0.087), (47, -0.082), (48, -0.09), (49, 0.16)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97138143 <a title="82-lsi-1" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>Author: Xiaohui Xie, Martin A. Giese</p><p>Abstract: Asymmetric lateral connections are one possible mechanism that can account for the direction selectivity of cortical neurons. We present a mathematical analysis for a class of these models. Contrasting with earlier theoretical work that has relied on methods from linear systems theory, we study the network’s nonlinear dynamic properties that arise when the threshold nonlinearity of the neurons is taken into account. We show that such networks have stimulus-locked traveling pulse solutions that are appropriate for modeling the responses of direction selective cortical neurons. In addition, our analysis shows that outside a certain regime of stimulus speeds the stability of this solutions breaks down giving rise to another class of solutions that are characterized by speciﬁc spatiotemporal periodicity. This predicts that if direction selectivity in the cortex is mainly achieved by asymmetric lateral connections lurching activity waves might be observable in ensembles of direction selective cortical neurons within appropriate regimes of the stimulus speed.</p><p>2 0.81865168 <a title="82-lsi-2" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>Author: Richard Hahnloser, Xiaohui Xie, H. S. Seung</p><p>Abstract: Integration in the head-direction system is a computation by which horizontal angular head velocity signals from the vestibular nuclei are integrated to yield a neural representation of head direction. In the thalamus, the postsubiculum and the mammillary nuclei, the head-direction representation has the form of a place code: neurons have a preferred head direction in which their ﬁring is maximal [Blair and Sharp, 1995, Blair et al., 1998, ?]. Integration is a difﬁcult computation, given that head-velocities can vary over a large range. Previous models of the head-direction system relied on the assumption that the integration is achieved in a ﬁring-rate-based attractor network with a ring structure. In order to correctly integrate head-velocity signals during high-speed head rotations, very fast synaptic dynamics had to be assumed. Here we address the question whether integration in the head-direction system is possible with slow synapses, for example excitatory NMDA and inhibitory GABA(B) type synapses. For neural networks with such slow synapses, rate-based dynamics are a good approximation of spiking neurons [Ermentrout, 1994]. We ﬁnd that correct integration during high-speed head rotations imposes strong constraints on possible network architectures.</p><p>3 0.58191365 <a title="82-lsi-3" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>Author: Maoz Shamir, Haim Sompolinsky</p><p>Abstract: Population codes often rely on the tuning of the mean responses to the stimulus parameters. However, this information can be greatly suppressed by long range correlations. Here we study the efﬁciency of coding information in the second order statistics of the population responses. We show that the Fisher Information of this system grows linearly with the size of the system. We propose a bilinear readout model for extracting information from correlation codes, and evaluate its performance in discrimination and estimation tasks. It is shown that the main source of information in this system is the stimulus dependence of the variances of the single neuron responses.</p><p>4 0.57459879 <a title="82-lsi-4" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>Author: Shih-Chii Liu, Jörg Kramer, Giacomo Indiveri, Tobi Delbrück, Rodney J. Douglas</p><p>Abstract: We describe a programmable multi-chip VLSI neuronal system that can be used for exploring spike-based information processing models. The system consists of a silicon retina, a PIC microcontroller, and a transceiver chip whose integrate-and-ﬁre neurons are connected in a soft winner-take-all architecture. The circuit on this multi-neuron chip approximates a cortical microcircuit. The neurons can be conﬁgured for different computational properties by the virtual connections of a selected set of pixels on the silicon retina. The virtual wiring between the different chips is effected by an event-driven communication protocol that uses asynchronous digital pulses, similar to spikes in a neuronal system. We used the multi-chip spike-based system to synthesize orientation-tuned neurons using both a feedforward model and a feedback model. The performance of our analog hardware spiking model matched the experimental observations and digital simulations of continuous-valued neurons. The multi-chip VLSI system has advantages over computer neuronal models in that it is real-time, and the computational time does not scale with the size of the neuronal network.</p><p>5 0.56014824 <a title="82-lsi-5" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>Author: Yun Gao, Michael J. Black, Elie Bienenstock, Shy Shoham, John P. Donoghue</p><p>Abstract: Statistical learning and probabilistic inference techniques are used to infer the hand position of a subject from multi-electrode recordings of neural activity in motor cortex. First, an array of electrodes provides training data of neural ﬁring conditioned on hand kinematics. We learn a nonparametric representation of this ﬁring activity using a Bayesian model and rigorously compare it with previous models using cross-validation. Second, we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using Bayesian inference. The learned ﬁring models of multiple cells are used to deﬁne a nonGaussian likelihood term which is combined with a prior probability for the kinematics. A particle ﬁltering method is used to represent, update, and propagate the posterior distribution over time. The approach is compared with traditional linear ﬁltering methods; the results suggest that it may be appropriate for neural prosthetic applications.</p><p>6 0.55021656 <a title="82-lsi-6" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>7 0.49920326 <a title="82-lsi-7" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>8 0.46210301 <a title="82-lsi-8" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>9 0.45238826 <a title="82-lsi-9" href="./nips-2001-Modeling_the_Modulatory_Effect_of_Attention_on_Human_Spatial_Vision.html">124 nips-2001-Modeling the Modulatory Effect of Attention on Human Spatial Vision</a></p>
<p>10 0.43821716 <a title="82-lsi-10" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>11 0.43641165 <a title="82-lsi-11" href="./nips-2001-Characterizing_Neural_Gain_Control_using_Spike-triggered_Covariance.html">48 nips-2001-Characterizing Neural Gain Control using Spike-triggered Covariance</a></p>
<p>12 0.42719576 <a title="82-lsi-12" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>13 0.41684484 <a title="82-lsi-13" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>14 0.40749684 <a title="82-lsi-14" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>15 0.39565471 <a title="82-lsi-15" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>16 0.37787375 <a title="82-lsi-16" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<p>17 0.36544055 <a title="82-lsi-17" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>18 0.35090029 <a title="82-lsi-18" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>19 0.33931151 <a title="82-lsi-19" href="./nips-2001-A_Hierarchical_Model_of_Complex_Cells_in_Visual_Cortex_for_the_Binocular_Perception_of_Motion-in-Depth.html">10 nips-2001-A Hierarchical Model of Complex Cells in Visual Cortex for the Binocular Perception of Motion-in-Depth</a></p>
<p>20 0.33823436 <a title="82-lsi-20" href="./nips-2001-Scaling_Laws_and_Local_Minima_in_Hebbian_ICA.html">165 nips-2001-Scaling Laws and Local Minima in Hebbian ICA</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.034), (17, 0.016), (19, 0.013), (27, 0.118), (30, 0.536), (38, 0.034), (59, 0.014), (72, 0.031), (79, 0.032), (83, 0.012), (91, 0.09)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98579353 <a title="82-lda-1" href="./nips-2001-An_Efficient_Clustering_Algorithm_Using_Stochastic_Association_Model_and_Its_Implementation_Using_Nanostructures.html">33 nips-2001-An Efficient Clustering Algorithm Using Stochastic Association Model and Its Implementation Using Nanostructures</a></p>
<p>Author: Takashi Morie, Tomohiro Matsuura, Makoto Nagata, Atsushi Iwata</p><p>Abstract: This paper describes a clustering algorithm for vector quantizers using a “stochastic association model”. It offers a new simple and powerful softmax adaptation rule. The adaptation process is the same as the on-line K-means clustering method except for adding random ﬂuctuation in the distortion error evaluation process. Simulation results demonstrate that the new algorithm can achieve efﬁcient adaptation as high as the “neural gas” algorithm, which is reported as one of the most efﬁcient clustering methods. It is a key to add uncorrelated random ﬂuctuation in the similarity evaluation process for each reference vector. For hardware implementation of this process, we propose a nanostructure, whose operation is described by a single-electron circuit. It positively uses ﬂuctuation in quantum mechanical tunneling processes.</p><p>same-paper 2 0.97430873 <a title="82-lda-2" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>Author: Xiaohui Xie, Martin A. Giese</p><p>Abstract: Asymmetric lateral connections are one possible mechanism that can account for the direction selectivity of cortical neurons. We present a mathematical analysis for a class of these models. Contrasting with earlier theoretical work that has relied on methods from linear systems theory, we study the network’s nonlinear dynamic properties that arise when the threshold nonlinearity of the neurons is taken into account. We show that such networks have stimulus-locked traveling pulse solutions that are appropriate for modeling the responses of direction selective cortical neurons. In addition, our analysis shows that outside a certain regime of stimulus speeds the stability of this solutions breaks down giving rise to another class of solutions that are characterized by speciﬁc spatiotemporal periodicity. This predicts that if direction selectivity in the cortex is mainly achieved by asymmetric lateral connections lurching activity waves might be observable in ensembles of direction selective cortical neurons within appropriate regimes of the stimulus speed.</p><p>3 0.97243285 <a title="82-lda-3" href="./nips-2001-Speech_Recognition_with_Missing_Data_using_Recurrent_Neural_Nets.html">173 nips-2001-Speech Recognition with Missing Data using Recurrent Neural Nets</a></p>
<p>Author: S. Parveen, P. Green</p><p>Abstract: In the ‘missing data’ approach to improving the robustness of automatic speech recognition to added noise, an initial process identiﬁes spectraltemporal regions which are dominated by the speech source. The remaining regions are considered to be ‘missing’. In this paper we develop a connectionist approach to the problem of adapting speech recognition to the missing data case, using Recurrent Neural Networks. In contrast to methods based on Hidden Markov Models, RNNs allow us to make use of long-term time constraints and to make the problems of classiﬁcation with incomplete data and imputing missing values interact. We report encouraging results on an isolated digit recognition task.</p><p>4 0.95847881 <a title="82-lda-4" href="./nips-2001-Reducing_multiclass_to_binary_by_coupling_probability_estimates.html">159 nips-2001-Reducing multiclass to binary by coupling probability estimates</a></p>
<p>Author: B. Zadrozny</p><p>Abstract: This paper presents a method for obtaining class membership probability estimates for multiclass classiﬁcation problems by coupling the probability estimates produced by binary classiﬁers. This is an extension for arbitrary code matrices of a method due to Hastie and Tibshirani for pairwise coupling of probability estimates. Experimental results with Boosted Naive Bayes show that our method produces calibrated class membership probability estimates, while having similar classiﬁcation accuracy as loss-based decoding, a method for obtaining the most likely class that does not generate probability estimates.</p><p>5 0.93184561 <a title="82-lda-5" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>Author: Sebastian Thrun, John Langford, Vandi Verma</p><p>Abstract: We propose a new particle ﬁlter that incorporates a model of costs when generating particles. The approach is motivated by the observation that the costs of accidentally not tracking hypotheses might be signiﬁcant in some areas of state space, and next to irrelevant in others. By incorporating a cost model into particle ﬁltering, states that are more critical to the system performance are more likely to be tracked. Automatic calculation of the cost model is implemented using an MDP value function calculation that estimates the value of tracking a particular state. Experiments in two mobile robot domains illustrate the appropriateness of the approach.</p><p>6 0.92609036 <a title="82-lda-6" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>7 0.80814302 <a title="82-lda-7" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>8 0.79782271 <a title="82-lda-8" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<p>9 0.78802955 <a title="82-lda-9" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>10 0.74682498 <a title="82-lda-10" href="./nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">102 nips-2001-KLD-Sampling: Adaptive Particle Filters</a></p>
<p>11 0.71034127 <a title="82-lda-11" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>12 0.67981762 <a title="82-lda-12" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>13 0.6743558 <a title="82-lda-13" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>14 0.66727823 <a title="82-lda-14" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>15 0.66680086 <a title="82-lda-15" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>16 0.66422129 <a title="82-lda-16" href="./nips-2001-A_Sequence_Kernel_and_its_Application_to_Speaker_Recognition.html">20 nips-2001-A Sequence Kernel and its Application to Speaker Recognition</a></p>
<p>17 0.65713376 <a title="82-lda-17" href="./nips-2001-Stochastic_Mixed-Signal_VLSI_Architecture_for_High-Dimensional_Kernel_Machines.html">176 nips-2001-Stochastic Mixed-Signal VLSI Architecture for High-Dimensional Kernel Machines</a></p>
<p>18 0.64690435 <a title="82-lda-18" href="./nips-2001-Analog_Soft-Pattern-Matching_Classifier_using_Floating-Gate_MOS_Technology.html">34 nips-2001-Analog Soft-Pattern-Matching Classifier using Floating-Gate MOS Technology</a></p>
<p>19 0.6374436 <a title="82-lda-19" href="./nips-2001-Linking_Motor_Learning_to_Function_Approximation%3A_Learning_in_an_Unlearnable_Force_Field.html">116 nips-2001-Linking Motor Learning to Function Approximation: Learning in an Unlearnable Force Field</a></p>
<p>20 0.63538259 <a title="82-lda-20" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
