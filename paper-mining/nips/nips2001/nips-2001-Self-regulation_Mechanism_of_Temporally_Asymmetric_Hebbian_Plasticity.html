<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-166" href="#">nips2001-166</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</h1>
<br/><p>Source: <a title="nips-2001-166-pdf" href="http://papers.nips.cc/paper/1972-self-regulation-mechanism-of-temporally-asymmetric-hebbian-plasticity.pdf">pdf</a></p><p>Author: N. Matsumoto, M. Okada</p><p>Abstract: Recent biological experimental ﬁndings have shown that the synaptic plasticity depends on the relative timing of the pre- and postsynaptic spikes which determines whether Long Term Potentiation (LTP) occurs or Long Term Depression (LTD) does. The synaptic plasticity has been called “Temporally Asymmetric Hebbian plasticity (TAH)”. Many authors have numerically shown that spatiotemporal patterns can be stored in neural networks. However, the mathematical mechanism for storage of the spatio-temporal patterns is still unknown, especially the eﬀects of LTD. In this paper, we employ a simple neural network model and show that interference of LTP and LTD disappears in a sparse coding scheme. On the other hand, it is known that the covariance learning is indispensable for storing sparse patterns. We also show that TAH qualitatively has the same eﬀect as the covariance learning when spatio-temporal patterns are embedded in the network. 1</p><p>Reference: <a title="nips-2001-166-reference" href="../nips2001_reference/nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp  Abstract Recent biological experimental ﬁndings have shown that the synaptic plasticity depends on the relative timing of the pre- and postsynaptic spikes which determines whether Long Term Potentiation (LTP) occurs or Long Term Depression (LTD) does. [sent-7, score-0.411]
</p><p>2 The synaptic plasticity has been called “Temporally Asymmetric Hebbian plasticity (TAH)”. [sent-8, score-0.302]
</p><p>3 Many authors have numerically shown that spatiotemporal patterns can be stored in neural networks. [sent-9, score-0.209]
</p><p>4 However, the mathematical mechanism for storage of the spatio-temporal patterns is still unknown, especially the eﬀects of LTD. [sent-10, score-0.331]
</p><p>5 In this paper, we employ a simple neural network model and show that interference of LTP and LTD disappears in a sparse coding scheme. [sent-11, score-0.344]
</p><p>6 On the other hand, it is known that the covariance learning is indispensable for storing sparse patterns. [sent-12, score-0.297]
</p><p>7 We also show that TAH qualitatively has the same eﬀect as the covariance learning when spatio-temporal patterns are embedded in the network. [sent-13, score-0.239]
</p><p>8 1  Introduction  Recent biological experimental ﬁndings have indicated that the synaptic plasticity depends on the relative timing of the pre- and post- synaptic spikes which determines whether Long Term Potentiation (LTP) occurs or Long Term Depression (LTD) does [1, 2, 3]. [sent-14, score-0.474]
</p><p>9 Many authors have numerically shown that spatio-temporal patterns can be stored in neural networks [6, 7, 8, 9, 10, 11]. [sent-19, score-0.209]
</p><p>10 discussed the variablity of spike generation about the network consisting of spiking neurons using TAH [6]. [sent-21, score-0.307]
</p><p>11 Yoshioka also discussed the associative memory network  consisting of spiking neurons using TAH [11]. [sent-24, score-0.383]
</p><p>12 Munro and Hernandez numerically showed that a network can retrieve spatio-temporal patterns even in a noisy environment owing to LTD [9]. [sent-26, score-0.328]
</p><p>13 However, they did not discuss the reason why TAH was eﬀective in terms of the storage and retrieval of the spatio-temporal patterns. [sent-27, score-0.298]
</p><p>14 Since TAH has not only the eﬀect of LTP but that of LTD, the interference of LTP and LTD may prevent retrieval of the patterns. [sent-28, score-0.188]
</p><p>15 To investigate this unknown mathematical mechanism for retrieval, we employ an associative memory network consisting of binary neurons. [sent-29, score-0.367]
</p><p>16 We show the mechanism that the spatio-temporal patterns can be retrieved in this network. [sent-32, score-0.223]
</p><p>17 There are many works concerned with associative memory networks that store spatio-temporal patterns by the covariance learning [12, 13]. [sent-33, score-0.414]
</p><p>18 It is wellknown that the covariance learning is indispensable when the sparse patterns are embedded in a network as attractors [15, 16]. [sent-35, score-0.544]
</p><p>19 The information on the ﬁring rate for the stored patterns is not indispensable for TAH, although it is indispensable for the covariance learning. [sent-36, score-0.619]
</p><p>20 We theoretically show that TAH qualitatively has the same eﬀect as the covariance learning when the spatio-temporal patterns are embedded in the network. [sent-37, score-0.239]
</p><p>21 This means that the diﬀerence in spike times induces LTP or LTD, and the eﬀect of the ﬁring rate information can be canceled out by this spike time diﬀerence. [sent-38, score-0.381]
</p><p>22 We also use discrete time steps and the following synchronous updating rule, N  ui (t) =  Jij xj (t),  (1)  j=1  xi (t + 1) = Θ(ui (t) − θ), Θ(u) =  1, u ≥ 0 0, u < 0,  (2) (3)  where xi (t) is the state of the i-th neuron at time t, ui (t) its internal potential, and θ a uniform threshold. [sent-43, score-0.519]
</p><p>23 If the i-th neuron ﬁres at time t, its state is xi (t) = 1; otherwise, xi (t) = 0. [sent-44, score-0.218]
</p><p>24 Jij is µ the synaptic weight from the j-th neuron to the i-th neuron. [sent-46, score-0.196]
</p><p>25 Each element ξi of µ µ µ µ the µ-th memory pattern ξ = (ξ1 , ξ2 , · · ·, ξ N ) is generated independently by, µ  µ µ Prob[ξi = 1] = 1 − Prob[ξi = 0] = f. [sent-47, score-0.192]
</p><p>26 µ E[ξi ]  (4)  The expectation of ξ is = f, and thus, f can be considered as the mean ﬁring rate of the memory pattern. [sent-48, score-0.226]
</p><p>27 The memory pattern is “sparse” when f → 0, and this coding scheme is called “sparse coding”. [sent-49, score-0.235]
</p><p>28 The synaptic weight Jij follows the synaptic plasticity that depends on the diﬀerence in spike times between the i-th (post-) and j-th (pre-) neurons. [sent-50, score-0.437]
</p><p>29 Figure 1(b) shows that LTP occurs when the j-th neuron ﬁres one time step before the i-th neuron µ+1 µ does, ξi = ξj = 1, and that LTD occurs when the j-th neuron ﬁres one time step µ−1 µ after the i-th neuron does, ξi = ξj = 1. [sent-55, score-0.402]
</p><p>30 Jij = N f(1 − f) µ=1 i  (5)  The number of memory patterns is p = αN where α is deﬁned as the “loading rate”. [sent-64, score-0.211]
</p><p>31 If the loading rate is larger than αC , the pattern sequence becomes unstable. [sent-66, score-0.325]
</p><p>32 We show that p memory patterns are retrieved periodically like ξ1 → ξ 2 → · · · → ξ p → ξ 1 → · · ·. [sent-69, score-0.283]
</p><p>33 One candidate algorithm for controlling the threshold value is to maintain the mean ﬁring rate of the network at that of memory pattern, f, as follows, N N 1 1 xi (t) = Θ(ui (t) − θ(t)). [sent-73, score-0.508]
</p><p>34 (6) f = N N i=1  i=1  It is known that the obtained threshold value is nearly optimal, since it approximately gives a maximal storage capacity value [16]. [sent-74, score-0.458]
</p><p>35 3  Theory  Many neural network models that store and retrieve sequential patterns by TAH have been discussed by many authors [7, 8, 9, 10]. [sent-75, score-0.38]
</p><p>36 For example, Munro and Hernandez showed that their model could retrieve a stored pattern sequence even in a noisy environment [9]. [sent-77, score-0.278]
</p><p>37 Here, we discuss the mechanism that the network learned by TAH can store and retrieve sequential patterns. [sent-80, score-0.343]
</p><p>38 Before providing details of the retrieval process, we discuss a simple situation where the number of memory patterns is very small relative to the number of neurons, i. [sent-81, score-0.329]
</p><p>39 Then, the internal potential u i (t) of the equation (1) is given by, t+1 t−1 ui (t) = ξi − ξi . [sent-85, score-0.196]
</p><p>40 The ﬁrst term ξi of the equation (7) is a signal term for the recall of the pattern ξt+1 , which is designed to be retrieved at time t+1, and the second term t−1 ξi can interfere in retrieval of ξt+1 . [sent-87, score-0.263]
</p><p>41 If the t+1 threshold θ(t) is set between 0 and +1, ξi = 0 isn’t inﬂuenced by the interference t−1 t+1 t−1 of ξi = 1. [sent-90, score-0.215]
</p><p>42 We consider the probability distribution of the internal potential ui (t) to examine how the interference of LTD inﬂuences the retrieval of ξt+1 . [sent-92, score-0.383]
</p><p>43 (8)  Since the threshold θ(t) is set between 0 and +1, the state xi (t + 1) is 1 with probability f − f 2 and 0 with 1 − f + f 2 . [sent-95, score-0.193]
</p><p>44 The overlap between the state x(t + 1) and the memory pattern ξ t+1 is given by, mt+1 (t + 1) =  1 N f(1 − f)  N  i=1  t+1 (ξi − f)xi (t + 1) = 1 − f. [sent-96, score-0.333]
</p><p>45 This means that the interference of LTD disappears in a sparse limit, and the model can retrieve the next pattern ξt+1 . [sent-98, score-0.4]
</p><p>46 Next, we discuss whether the information on the ﬁring rate is indispensable for TAH or not. [sent-100, score-0.277]
</p><p>47 To investigate this, we consider the case that the number of memory patterns is extensively large, i. [sent-101, score-0.238]
</p><p>48 Using the equation (9), the internal potential ui (t) of the i-th neuron at time t is represented as, t+1 t−1 ui (t) = (ξi − ξi )mt (t) + zi (t),  (10)  p  zi (t) = µ=t  µ+1 µ−1 (ξi − ξi )mµ (t). [sent-104, score-0.47]
</p><p>49 (11)  zi (t) is called the “cross-talk noise”, which represents contributions from non-target patterns excluding ξt−1 and prevents the target pattern ξt+1 from being retrieved. [sent-105, score-0.212]
</p><p>50 It is well-known that the covariance learning is indispensable when the sparse patterns are embedded in a network as attractors [15, 16]. [sent-107, score-0.544]
</p><p>51 Under sparse coding schemes,  unless the covariance learning is employed, the cross-talk noise does diverge in the large N limit. [sent-108, score-0.271]
</p><p>52 The information on the ﬁring rate for the stored patterns is not indispensable for TAH, although it is indispensable for the covariance learning. [sent-110, score-0.619]
</p><p>53 If a pattern sequence can be stored, the cross-talk noise is obeyed by a Gaussian distribution with mean 0 and time-dependent variance σ 2 (t). [sent-112, score-0.189]
</p><p>54 According to the statistical neurodynamics, we obtain the recursive equations for the overlap mt (t) between the network state x(t) and the target pattern ξt and the variance σ2 (t). [sent-115, score-0.502]
</p><p>55 2σ(t − 1) 2σ(t − 1) 2σ(t − 1) These equations reveal that the variance σ2 (t) of cross-talk noise does not diverge as long as a pattern sequence can be retrieved. [sent-122, score-0.241]
</p><p>56 Therefore, the variance of cross-talk noise doesn’t diverge, and this is another factor for the network learned by TAH to store and retrieve a pattern sequence. [sent-127, score-0.388]
</p><p>57 We conclude that the diﬀerence in spike times induces LTP or LTD, and the eﬀect of the ﬁring rate information can be canceled out by this spike times diﬀerence. [sent-128, score-0.381]
</p><p>58 4  Results  We investigate the property of our model and examine the following two conditions: a ﬁxed threshold and a time-dependent threshold, using the statistical neurodynamics and computer simulations. [sent-129, score-0.231]
</p><p>59 overlap (solid), activity/f (dashed)  Figure 2 shows how the overlap mt (t) and the mean ﬁring rate of the network, 1 x(t) = N i xi (t), depend on the loading rate α when the mean ﬁring rate of ¯ the memory pattern is f = 0. [sent-130, score-1.055]
</p><p>60 52, where the storage capacity is maximum with respect to the threshold θ. [sent-132, score-0.458]
</p><p>61 The stored pattern sequence can be retrieved when the initial overlap m1 (1) is greater than the critical value mC . [sent-133, score-0.398]
</p><p>62 The lower line indicates how the critical initial overlap m C depends on the loading rate α. [sent-134, score-0.403]
</p><p>63 In other words, the lower line represents the basin of attraction for the retrieved sequence. [sent-135, score-0.235]
</p><p>64 The upper line denotes a steady value of overlap mt (t) when the pattern sequence is retrieved. [sent-136, score-0.399]
</p><p>65 mt (t) is obtained by setting the initial state to the ﬁrst memory pattern: x(1) = ξ1 . [sent-137, score-0.287]
</p><p>66 The dashed line shows a steady value of the normalized mean ﬁring rate of network, x(t)/f, for the pattern sequence. [sent-140, score-0.263]
</p><p>67 The critical overlap (the lower line) and the overlap at the stationary state (the upper line). [sent-156, score-0.291]
</p><p>68 The dashed line shows the mean ﬁring rate of the network divided ﬁring rate which is 0. [sent-157, score-0.355]
</p><p>69 overlap (solid), activity/f (dashed)  Next, we examine the threshold control scheme in the equation (6), where the threshold is controlled to maintain the mean ﬁring rate of the network at f. [sent-165, score-0.618]
</p><p>70 q(t) 1 in equation (15) is equal to the mean ﬁring rate because q(t) = N N (xi (t))2 = i=1 N 1 i=1 xi (t) under the condition xi (t) = {0, 1}. [sent-166, score-0.256]
</p><p>71 (17) 2 Figure 3 shows the overlap mt (t) as a function of loading rate α with f = 0. [sent-168, score-0.469]
</p><p>72 The basin of attraction becomes larger than that of the ﬁxed threshold condition, θ = 0. [sent-172, score-0.232]
</p><p>73 This means that even if the initial state x(1) is diﬀerent from the ﬁrst memory pattern ξ1 , that is, the state includes a lot of noise, the pattern sequence can be retrieved. [sent-175, score-0.365]
</p><p>74 The critical overlap (the lower line) and the overlap at the stationary state (the upper line) when the threshold is changing over time to maintain mean ﬁring rate of the network at f. [sent-189, score-0.633]
</p><p>75 The dashed line shows the mean ﬁring rates of the network divided ﬁring rate which is 0. [sent-190, score-0.266]
</p><p>76 The basin of attraction become larger than that of the ﬁxed threshold condition: Figure 2. [sent-192, score-0.232]
</p><p>77 Finally, we discuss how the storage capacity depends on the ﬁring rate f of the 1 memory pattern. [sent-193, score-0.591]
</p><p>78 It is known that the storage capacity diverges as f | log f | in a sparse limit, f → 0 [19, 20]. [sent-194, score-0.489]
</p><p>79 Therefore, we investigate the asymptotic property  of the storage capacity in a sparse limit. [sent-195, score-0.462]
</p><p>80 Figure 4 shows how the storage capacity depends on the ﬁring rate where the threshold is controlled to maintain the network 1 activity at f (symbol ◦). [sent-196, score-0.668]
</p><p>81 The storage capacity diverges as f | log f | in a sparse limit. [sent-197, score-0.489]
</p><p>82 The storage capacity as a function of f in the case of maintaining activity at f (symbol ◦). [sent-202, score-0.352]
</p><p>83 Ths 1 storage capacity diverges as f | log f | in a sparse limit. [sent-203, score-0.489]
</p><p>84 45  1/|log f|  5  Discussion  Using a simple neural network model, we have discussed the mechanism that TAH enables the network to store and retrieve a pattern sequence. [sent-217, score-0.496]
</p><p>85 First, we showed that the interference of LTP and LTD disappeared in a sparse coding scheme. [sent-218, score-0.269]
</p><p>86 This is a factor to enable the network to store and retrieve a pattern sequence. [sent-219, score-0.334]
</p><p>87 Next, we showed the mechanism that TAH qualitatively had the same eﬀect as the covariance learning by analyzing the stability of the stored pattern sequence and the retrieval process by means of the statistical neurodynamics. [sent-220, score-0.407]
</p><p>88 Consequently, the variance of cross-talk noise didn’t diverge, and this is another factor for the network learned by TAH to store and retrieve a pattern sequence. [sent-221, score-0.388]
</p><p>89 We conclude that the diﬀerence in spike times induces LTP or LTD, and the eﬀect of the ﬁring rate information can be canceled out by this spike times diﬀerence. [sent-222, score-0.381]
</p><p>90 To improve the retrieval property of the basin of attraction, we introduced a threshold control algorithm where a threshold value was adjusted to maintain the mean ﬁring rate of the network at that of a memory pattern. [sent-224, score-0.701]
</p><p>91 We also found that the loading rate diverged as f | log f | in a sparse limit, f → 0. [sent-226, score-0.299]
</p><p>92 Here, we compare the storage capacity of our model with that of the model using the covariance learning (Figure 5). [sent-227, score-0.417]
</p><p>93 We calculate the storage capacity αCOV from their dynamical equations and compare these of our model, C αT AH , by the ratio of α T AH /αCOV . [sent-229, score-0.379]
</p><p>94 The contribution of LTD reduces the storage capacity of our model to half. [sent-233, score-0.352]
</p><p>95 Therefore, in terms of the storage capacity, the covariance learning is better than TAH. [sent-234, score-0.245]
</p><p>96 But, as we discussed previously, the information of the ﬁring rate is indispensable in TAH. [sent-235, score-0.265]
</p><p>97 The comparison of the storage capacity of our model with that of the model using the covariance learning. [sent-242, score-0.417]
</p><p>98 As f decreases, the ratio of storage capacity approaches 0. [sent-243, score-0.352]
</p><p>99 Synaptic modiﬁcations in cultured hippocampal neurons: Dependence on spike timing, synaptic strength, and postsynaptic cell  type. [sent-257, score-0.283]
</p><p>100 Temporally asymmetric hebbian learning, spike timing and neuronal response variability. [sent-283, score-0.349]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tah', 0.413), ('ltd', 0.381), ('ltp', 0.344), ('ring', 0.246), ('storage', 0.18), ('capacity', 0.172), ('indispensable', 0.149), ('erf', 0.149), ('mt', 0.144), ('ui', 0.136), ('loading', 0.127), ('synaptic', 0.12), ('jij', 0.12), ('memory', 0.111), ('overlap', 0.109), ('interference', 0.109), ('spike', 0.106), ('threshold', 0.106), ('retrieve', 0.102), ('patterns', 0.1), ('hebbian', 0.097), ('plasticity', 0.091), ('temporally', 0.089), ('rate', 0.089), ('network', 0.084), ('sparse', 0.083), ('pattern', 0.081), ('erence', 0.08), ('di', 0.079), ('retrieval', 0.079), ('neuron', 0.076), ('retrieved', 0.072), ('associative', 0.071), ('neurodynamics', 0.068), ('store', 0.067), ('stored', 0.067), ('covariance', 0.065), ('attraction', 0.063), ('basin', 0.063), ('asymmetric', 0.059), ('postsynaptic', 0.057), ('res', 0.057), ('ect', 0.055), ('xi', 0.055), ('diverges', 0.054), ('canceled', 0.052), ('munro', 0.052), ('mechanism', 0.051), ('diverge', 0.051), ('timing', 0.05), ('occurs', 0.049), ('biological', 0.044), ('coding', 0.043), ('numerically', 0.042), ('critical', 0.041), ('neurons', 0.04), ('discuss', 0.039), ('embedded', 0.038), ('sparsely', 0.038), ('maintain', 0.037), ('neuronal', 0.037), ('line', 0.037), ('qualitatively', 0.036), ('prob', 0.036), ('saitama', 0.036), ('disappeared', 0.034), ('kitano', 0.034), ('okada', 0.034), ('cov', 0.034), ('state', 0.032), ('equation', 0.031), ('zi', 0.031), ('ective', 0.031), ('examine', 0.03), ('ndings', 0.03), ('hernandez', 0.03), ('kempter', 0.03), ('stdp', 0.03), ('dashed', 0.03), ('noise', 0.029), ('internal', 0.029), ('induces', 0.028), ('sequence', 0.028), ('equations', 0.027), ('investigate', 0.027), ('discussed', 0.027), ('rule', 0.027), ('spiking', 0.027), ('mean', 0.026), ('physical', 0.026), ('potentiation', 0.025), ('ah', 0.025), ('attractors', 0.025), ('depression', 0.025), ('gerstner', 0.025), ('disappears', 0.025), ('variance', 0.025), ('consisting', 0.023), ('doesn', 0.023), ('riken', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="166-tfidf-1" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>Author: N. Matsumoto, M. Okada</p><p>Abstract: Recent biological experimental ﬁndings have shown that the synaptic plasticity depends on the relative timing of the pre- and postsynaptic spikes which determines whether Long Term Potentiation (LTP) occurs or Long Term Depression (LTD) does. The synaptic plasticity has been called “Temporally Asymmetric Hebbian plasticity (TAH)”. Many authors have numerically shown that spatiotemporal patterns can be stored in neural networks. However, the mathematical mechanism for storage of the spatio-temporal patterns is still unknown, especially the eﬀects of LTD. In this paper, we employ a simple neural network model and show that interference of LTP and LTD disappears in a sparse coding scheme. On the other hand, it is known that the covariance learning is indispensable for storing sparse patterns. We also show that TAH qualitatively has the same eﬀect as the covariance learning when spatio-temporal patterns are embedded in the network. 1</p><p>2 0.16110192 <a title="166-tfidf-2" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>Author: Yun Gao, Michael J. Black, Elie Bienenstock, Shy Shoham, John P. Donoghue</p><p>Abstract: Statistical learning and probabilistic inference techniques are used to infer the hand position of a subject from multi-electrode recordings of neural activity in motor cortex. First, an array of electrodes provides training data of neural ﬁring conditioned on hand kinematics. We learn a nonparametric representation of this ﬁring activity using a Bayesian model and rigorously compare it with previous models using cross-validation. Second, we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using Bayesian inference. The learned ﬁring models of multiple cells are used to deﬁne a nonGaussian likelihood term which is combined with a prior probability for the kinematics. A particle ﬁltering method is used to represent, update, and propagate the posterior distribution over time. The approach is compared with traditional linear ﬁltering methods; the results suggest that it may be appropriate for neural prosthetic applications.</p><p>3 0.16022617 <a title="166-tfidf-3" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>Author: Jesper Tegnér, Ádám Kepecs</p><p>Abstract: Hebbian learning rules are generally formulated as static rules. Under changing condition (e.g. neuromodulation, input statistics) most rules are sensitive to parameters. In particular, recent work has focused on two different formulations of spike-timing-dependent plasticity rules. Additive STDP [1] is remarkably versatile but also very fragile, whereas multiplicative STDP [2, 3] is more robust but lacks attractive features such as synaptic competition and rate stabilization. Here we address the problem of robustness in the additive STDP rule. We derive an adaptive control scheme, where the learning function is under fast dynamic control by postsynaptic activity to stabilize learning under a variety of conditions. Such a control scheme can be implemented using known biophysical mechanisms of synapses. We show that this adaptive rule makes the addit ive STDP more robust. Finally, we give an example how meta plasticity of the adaptive rule can be used to guide STDP into different type of learning regimes. 1</p><p>4 0.14236814 <a title="166-tfidf-4" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>Author: Peter E. Latham</p><p>Abstract: Almost two decades ago , Hopfield [1] showed that networks of highly reduced model neurons can exhibit multiple attracting fixed points, thus providing a substrate for associative memory. It is still not clear, however, whether realistic neuronal networks can support multiple attractors. The main difficulty is that neuronal networks in vivo exhibit a stable background state at low firing rate, typically a few Hz. Embedding attractor is easy; doing so without destabilizing the background is not. Previous work [2, 3] focused on the sparse coding limit, in which a vanishingly small number of neurons are involved in any memory. Here we investigate the case in which the number of neurons involved in a memory scales with the number of neurons in the network. In contrast to the sparse coding limit, we find that multiple attractors can co-exist robustly with a stable background state. Mean field theory is used to understand how the behavior of the network scales with its parameters, and simulations with analog neurons are presented. One of the most important features of the nervous system is its ability to perform associative memory. It is generally believed that associative memory is implemented using attractor networks - experimental studies point in that direction [4- 7], and there are virtually no competing theoretical models. Perhaps surprisingly, however, it is still an open theoretical question whether attractors can exist in realistic neuronal networks. The</p><p>5 0.13619876 <a title="166-tfidf-5" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>Author: Richard Hahnloser, Xiaohui Xie, H. S. Seung</p><p>Abstract: Integration in the head-direction system is a computation by which horizontal angular head velocity signals from the vestibular nuclei are integrated to yield a neural representation of head direction. In the thalamus, the postsubiculum and the mammillary nuclei, the head-direction representation has the form of a place code: neurons have a preferred head direction in which their ﬁring is maximal [Blair and Sharp, 1995, Blair et al., 1998, ?]. Integration is a difﬁcult computation, given that head-velocities can vary over a large range. Previous models of the head-direction system relied on the assumption that the integration is achieved in a ﬁring-rate-based attractor network with a ring structure. In order to correctly integrate head-velocity signals during high-speed head rotations, very fast synaptic dynamics had to be assumed. Here we address the question whether integration in the head-direction system is possible with slow synapses, for example excitatory NMDA and inhibitory GABA(B) type synapses. For neural networks with such slow synapses, rate-based dynamics are a good approximation of spiking neurons [Ermentrout, 1994]. We ﬁnd that correct integration during high-speed head rotations imposes strong constraints on possible network architectures.</p><p>6 0.11561381 <a title="166-tfidf-6" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>7 0.11319108 <a title="166-tfidf-7" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>8 0.11134611 <a title="166-tfidf-8" href="./nips-2001-Citcuits_for_VLSI_Implementation_of_Temporally_Asymmetric_Hebbian_Learning.html">49 nips-2001-Citcuits for VLSI Implementation of Temporally Asymmetric Hebbian Learning</a></p>
<p>9 0.092173457 <a title="166-tfidf-9" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>10 0.074259154 <a title="166-tfidf-10" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>11 0.067673348 <a title="166-tfidf-11" href="./nips-2001-Learning_Spike-Based_Correlations_and_Conditional_Probabilities_in_Silicon.html">112 nips-2001-Learning Spike-Based Correlations and Conditional Probabilities in Silicon</a></p>
<p>12 0.057794049 <a title="166-tfidf-12" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>13 0.057774045 <a title="166-tfidf-13" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>14 0.057494644 <a title="166-tfidf-14" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>15 0.052827492 <a title="166-tfidf-15" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>16 0.050591007 <a title="166-tfidf-16" href="./nips-2001-3_state_neurons_for_contextual_processing.html">2 nips-2001-3 state neurons for contextual processing</a></p>
<p>17 0.050117515 <a title="166-tfidf-17" href="./nips-2001-Fragment_Completion_in_Humans_and_Machines.html">78 nips-2001-Fragment Completion in Humans and Machines</a></p>
<p>18 0.04873484 <a title="166-tfidf-18" href="./nips-2001-A_Model_of_the_Phonological_Loop%3A_Generalization_and_Binding.html">12 nips-2001-A Model of the Phonological Loop: Generalization and Binding</a></p>
<p>19 0.048573434 <a title="166-tfidf-19" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>20 0.04626764 <a title="166-tfidf-20" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.144), (1, -0.213), (2, -0.099), (3, 0.05), (4, 0.083), (5, 0.024), (6, 0.114), (7, -0.021), (8, -0.041), (9, -0.009), (10, 0.026), (11, -0.1), (12, 0.016), (13, -0.035), (14, -0.043), (15, 0.035), (16, -0.053), (17, -0.091), (18, 0.037), (19, 0.002), (20, 0.039), (21, 0.01), (22, -0.042), (23, 0.075), (24, -0.063), (25, -0.068), (26, 0.018), (27, -0.102), (28, 0.07), (29, -0.082), (30, -0.075), (31, 0.098), (32, -0.014), (33, 0.062), (34, 0.066), (35, -0.104), (36, 0.106), (37, -0.255), (38, -0.028), (39, 0.12), (40, 0.005), (41, 0.065), (42, -0.122), (43, -0.046), (44, 0.071), (45, -0.016), (46, 0.057), (47, 0.018), (48, -0.054), (49, -0.126)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95559084 <a title="166-lsi-1" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>Author: N. Matsumoto, M. Okada</p><p>Abstract: Recent biological experimental ﬁndings have shown that the synaptic plasticity depends on the relative timing of the pre- and postsynaptic spikes which determines whether Long Term Potentiation (LTP) occurs or Long Term Depression (LTD) does. The synaptic plasticity has been called “Temporally Asymmetric Hebbian plasticity (TAH)”. Many authors have numerically shown that spatiotemporal patterns can be stored in neural networks. However, the mathematical mechanism for storage of the spatio-temporal patterns is still unknown, especially the eﬀects of LTD. In this paper, we employ a simple neural network model and show that interference of LTP and LTD disappears in a sparse coding scheme. On the other hand, it is known that the covariance learning is indispensable for storing sparse patterns. We also show that TAH qualitatively has the same eﬀect as the covariance learning when spatio-temporal patterns are embedded in the network. 1</p><p>2 0.63575047 <a title="166-lsi-2" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>Author: Richard Hahnloser, Xiaohui Xie, H. S. Seung</p><p>Abstract: Integration in the head-direction system is a computation by which horizontal angular head velocity signals from the vestibular nuclei are integrated to yield a neural representation of head direction. In the thalamus, the postsubiculum and the mammillary nuclei, the head-direction representation has the form of a place code: neurons have a preferred head direction in which their ﬁring is maximal [Blair and Sharp, 1995, Blair et al., 1998, ?]. Integration is a difﬁcult computation, given that head-velocities can vary over a large range. Previous models of the head-direction system relied on the assumption that the integration is achieved in a ﬁring-rate-based attractor network with a ring structure. In order to correctly integrate head-velocity signals during high-speed head rotations, very fast synaptic dynamics had to be assumed. Here we address the question whether integration in the head-direction system is possible with slow synapses, for example excitatory NMDA and inhibitory GABA(B) type synapses. For neural networks with such slow synapses, rate-based dynamics are a good approximation of spiking neurons [Ermentrout, 1994]. We ﬁnd that correct integration during high-speed head rotations imposes strong constraints on possible network architectures.</p><p>3 0.63228351 <a title="166-lsi-3" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>Author: Jesper Tegnér, Ádám Kepecs</p><p>Abstract: Hebbian learning rules are generally formulated as static rules. Under changing condition (e.g. neuromodulation, input statistics) most rules are sensitive to parameters. In particular, recent work has focused on two different formulations of spike-timing-dependent plasticity rules. Additive STDP [1] is remarkably versatile but also very fragile, whereas multiplicative STDP [2, 3] is more robust but lacks attractive features such as synaptic competition and rate stabilization. Here we address the problem of robustness in the additive STDP rule. We derive an adaptive control scheme, where the learning function is under fast dynamic control by postsynaptic activity to stabilize learning under a variety of conditions. Such a control scheme can be implemented using known biophysical mechanisms of synapses. We show that this adaptive rule makes the addit ive STDP more robust. Finally, we give an example how meta plasticity of the adaptive rule can be used to guide STDP into different type of learning regimes. 1</p><p>4 0.53202718 <a title="166-lsi-4" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>Author: Yun Gao, Michael J. Black, Elie Bienenstock, Shy Shoham, John P. Donoghue</p><p>Abstract: Statistical learning and probabilistic inference techniques are used to infer the hand position of a subject from multi-electrode recordings of neural activity in motor cortex. First, an array of electrodes provides training data of neural ﬁring conditioned on hand kinematics. We learn a nonparametric representation of this ﬁring activity using a Bayesian model and rigorously compare it with previous models using cross-validation. Second, we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using Bayesian inference. The learned ﬁring models of multiple cells are used to deﬁne a nonGaussian likelihood term which is combined with a prior probability for the kinematics. A particle ﬁltering method is used to represent, update, and propagate the posterior distribution over time. The approach is compared with traditional linear ﬁltering methods; the results suggest that it may be appropriate for neural prosthetic applications.</p><p>5 0.50645053 <a title="166-lsi-5" href="./nips-2001-Citcuits_for_VLSI_Implementation_of_Temporally_Asymmetric_Hebbian_Learning.html">49 nips-2001-Citcuits for VLSI Implementation of Temporally Asymmetric Hebbian Learning</a></p>
<p>Author: A. Bofill, D. P. Thompson, Alan F. Murray</p><p>Abstract: Experimental data has shown that synaptic strength modification in some types of biological neurons depends upon precise spike timing differences between presynaptic and postsynaptic spikes. Several temporally-asymmetric Hebbian learning rules motivated by this data have been proposed. We argue that such learning rules are suitable to analog VLSI implementation. We describe an easily tunable circuit to modify the weight of a silicon spiking neuron according to those learning rules. Test results from the fabrication of the circuit using a O.6J.lm CMOS process are given. 1</p><p>6 0.42438382 <a title="166-lsi-6" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>7 0.3998045 <a title="166-lsi-7" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>8 0.39098385 <a title="166-lsi-8" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>9 0.37279972 <a title="166-lsi-9" href="./nips-2001-Scaling_Laws_and_Local_Minima_in_Hebbian_ICA.html">165 nips-2001-Scaling Laws and Local Minima in Hebbian ICA</a></p>
<p>10 0.36455923 <a title="166-lsi-10" href="./nips-2001-Learning_Spike-Based_Correlations_and_Conditional_Probabilities_in_Silicon.html">112 nips-2001-Learning Spike-Based Correlations and Conditional Probabilities in Silicon</a></p>
<p>11 0.35661465 <a title="166-lsi-11" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>12 0.34698921 <a title="166-lsi-12" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>13 0.34661871 <a title="166-lsi-13" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>14 0.30451015 <a title="166-lsi-14" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>15 0.28216115 <a title="166-lsi-15" href="./nips-2001-The_Intelligent_surfer%3A_Probabilistic_Combination_of_Link_and_Content_Information_in_PageRank.html">184 nips-2001-The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank</a></p>
<p>16 0.26619142 <a title="166-lsi-16" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>17 0.26330075 <a title="166-lsi-17" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>18 0.26202285 <a title="166-lsi-18" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>19 0.25826076 <a title="166-lsi-19" href="./nips-2001-A_Model_of_the_Phonological_Loop%3A_Generalization_and_Binding.html">12 nips-2001-A Model of the Phonological Loop: Generalization and Binding</a></p>
<p>20 0.25088722 <a title="166-lsi-20" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.303), (14, 0.086), (19, 0.023), (20, 0.023), (27, 0.138), (30, 0.078), (38, 0.031), (59, 0.033), (72, 0.039), (79, 0.044), (91, 0.098)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81269979 <a title="166-lda-1" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>Author: N. Matsumoto, M. Okada</p><p>Abstract: Recent biological experimental ﬁndings have shown that the synaptic plasticity depends on the relative timing of the pre- and postsynaptic spikes which determines whether Long Term Potentiation (LTP) occurs or Long Term Depression (LTD) does. The synaptic plasticity has been called “Temporally Asymmetric Hebbian plasticity (TAH)”. Many authors have numerically shown that spatiotemporal patterns can be stored in neural networks. However, the mathematical mechanism for storage of the spatio-temporal patterns is still unknown, especially the eﬀects of LTD. In this paper, we employ a simple neural network model and show that interference of LTP and LTD disappears in a sparse coding scheme. On the other hand, it is known that the covariance learning is indispensable for storing sparse patterns. We also show that TAH qualitatively has the same eﬀect as the covariance learning when spatio-temporal patterns are embedded in the network. 1</p><p>2 0.79763484 <a title="166-lda-2" href="./nips-2001-Estimating_Car_Insurance_Premia%3A_a_Case_Study_in_High-Dimensional_Data_Inference.html">70 nips-2001-Estimating Car Insurance Premia: a Case Study in High-Dimensional Data Inference</a></p>
<p>Author: Nicolas Chapados, Yoshua Bengio, Pascal Vincent, Joumana Ghosn, Charles Dugas, Ichiro Takeuchi, Linyan Meng</p><p>Abstract: Estimating insurance premia from data is a difficult regression problem for several reasons: the large number of variables, many of which are .discrete, and the very peculiar shape of the noise distribution, asymmetric with fat tails, with a large majority zeros and a few unreliable and very large values. We compare several machine learning methods for estimating insurance premia, and test them on a large data base of car insurance policies. We find that function approximation methods that do not optimize a squared loss, like Support Vector Machines regression, do not work well in this context. Compared methods include decision trees and generalized linear models. The best results are obtained with a mixture of experts, which better identifies the least and most risky contracts, and allows to reduce the median premium by charging more to the most risky customers. 1</p><p>3 0.55223817 <a title="166-lda-3" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>Author: Gregor Wenning, Klaus Obermayer</p><p>Abstract: Cortical neurons might be considered as threshold elements integrating in parallel many excitatory and inhibitory inputs. Due to the apparent variability of cortical spike trains this yields a strongly fluctuating membrane potential, such that threshold crossings are highly irregular. Here we study how a neuron could maximize its sensitivity w.r.t. a relatively small subset of excitatory input. Weak signals embedded in fluctuations is the natural realm of stochastic resonance. The neuron's response is described in a hazard-function approximation applied to an Ornstein-Uhlenbeck process. We analytically derive an optimality criterium and give a learning rule for the adjustment of the membrane fluctuations, such that the sensitivity is maximal exploiting stochastic resonance. We show that adaptation depends only on quantities that could easily be estimated locally (in space and time) by the neuron. The main results are compared with simulations of a biophysically more realistic neuron model. 1</p><p>4 0.55193406 <a title="166-lda-4" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>Author: Gunnar Rätsch, Sebastian Mika, Manfred K. Warmuth</p><p>Abstract: We give an uniﬁed convergence analysis of ensemble learning methods including e.g. AdaBoost, Logistic Regression and the Least-SquareBoost algorithm for regression. These methods have in common that they iteratively call a base learning algorithm which returns hypotheses that are then linearly combined. We show that these methods are related to the Gauss-Southwell method known from numerical optimization and state non-asymptotical convergence results for all these methods. Our analysis includes -norm regularized cost functions leading to a clean and general way to regularize ensemble learning.</p><p>5 0.55174321 <a title="166-lda-5" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>Author: Jesper Tegnér, Ádám Kepecs</p><p>Abstract: Hebbian learning rules are generally formulated as static rules. Under changing condition (e.g. neuromodulation, input statistics) most rules are sensitive to parameters. In particular, recent work has focused on two different formulations of spike-timing-dependent plasticity rules. Additive STDP [1] is remarkably versatile but also very fragile, whereas multiplicative STDP [2, 3] is more robust but lacks attractive features such as synaptic competition and rate stabilization. Here we address the problem of robustness in the additive STDP rule. We derive an adaptive control scheme, where the learning function is under fast dynamic control by postsynaptic activity to stabilize learning under a variety of conditions. Such a control scheme can be implemented using known biophysical mechanisms of synapses. We show that this adaptive rule makes the addit ive STDP more robust. Finally, we give an example how meta plasticity of the adaptive rule can be used to guide STDP into different type of learning regimes. 1</p><p>6 0.54963791 <a title="166-lda-6" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>7 0.54760242 <a title="166-lda-7" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>8 0.53919029 <a title="166-lda-8" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>9 0.53881204 <a title="166-lda-9" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>10 0.53808886 <a title="166-lda-10" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>11 0.53789544 <a title="166-lda-11" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>12 0.53744429 <a title="166-lda-12" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>13 0.53710985 <a title="166-lda-13" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>14 0.53623575 <a title="166-lda-14" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>15 0.53613275 <a title="166-lda-15" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>16 0.53498393 <a title="166-lda-16" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>17 0.53487325 <a title="166-lda-17" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>18 0.53463531 <a title="166-lda-18" href="./nips-2001-Kernel_Feature_Spaces_and_Nonlinear_Blind_Souce_Separation.html">103 nips-2001-Kernel Feature Spaces and Nonlinear Blind Souce Separation</a></p>
<p>19 0.53454441 <a title="166-lda-19" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>20 0.53302968 <a title="166-lda-20" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
