<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-55" href="#">nips2001-55</a> knowledge-graph by maker-knowledge-mining</p><h1>55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</h1>
<br/><p>Source: <a title="nips-2001-55-pdf" href="http://papers.nips.cc/paper/1944-convergence-of-optimistic-and-incremental-q-learning.pdf">pdf</a></p><p>Author: Eyal Even-dar, Yishay Mansour</p><p>Abstract: Vie sho,v the convergence of tV/O deterministic variants of Qlearning. The first is the widely used optimistic Q-learning, which initializes the Q-values to large initial values and then follows a greedy policy with respect to the Q-values. We show that setting the initial value sufficiently large guarantees the converges to an Eoptimal policy. The second is a new and novel algorithm incremental Q-learning, which gradually promotes the values of actions that are not taken. We show that incremental Q-learning converges, in the limit, to the optimal policy. Our incremental Q-learning algorithm can be viewed as derandomization of the E-greedy Q-learning. 1</p><p>Reference: <a title="nips-2001-55-reference" href="../nips2001_reference/nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('qt', 0.498), ('incr', 0.287), ('vmax', 0.264), ('policy', 0.257), ('promot', 0.249), ('lemm', 0.231), ('infinit', 0.195), ('vm', 0.153), ('mdp', 0.147), ('si', 0.143), ('converg', 0.137), ('vt', 0.136), ('vii', 0.12), ('qlearn', 0.117), ('act', 0.116), ('execut', 0.114), ('sl', 0.111), ('rm', 0.103), ('behav', 0.1), ('greedy', 0.099)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="55-tfidf-1" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>2 0.23414873 <a title="55-tfidf-2" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>3 0.22318313 <a title="55-tfidf-3" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>4 0.19618158 <a title="55-tfidf-4" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>5 0.18943475 <a title="55-tfidf-5" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>6 0.17784576 <a title="55-tfidf-6" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>7 0.17659974 <a title="55-tfidf-7" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>8 0.15626721 <a title="55-tfidf-8" href="./nips-2001-Stabilizing_Value_Function_Approximation_with_the_BFBP_Algorithm.html">175 nips-2001-Stabilizing Value Function Approximation with the BFBP Algorithm</a></p>
<p>9 0.15379502 <a title="55-tfidf-9" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>10 0.14970225 <a title="55-tfidf-10" href="./nips-2001-Incremental_Learning_and_Selective_Sampling_via_Parametric_Optimization_Framework_for_SVM.html">94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</a></p>
<p>11 0.11766781 <a title="55-tfidf-11" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>12 0.11634313 <a title="55-tfidf-12" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>13 0.11228044 <a title="55-tfidf-13" href="./nips-2001-Analysis_of_Sparse_Bayesian_Learning.html">35 nips-2001-Analysis of Sparse Bayesian Learning</a></p>
<p>14 0.09213414 <a title="55-tfidf-14" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>15 0.085179485 <a title="55-tfidf-15" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>16 0.082402021 <a title="55-tfidf-16" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>17 0.080009885 <a title="55-tfidf-17" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>18 0.076016434 <a title="55-tfidf-18" href="./nips-2001-Efficiency_versus_Convergence_of_Boolean_Kernels_for_On-Line_Learning_Algorithms.html">66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</a></p>
<p>19 0.075741075 <a title="55-tfidf-19" href="./nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</a></p>
<p>20 0.069934554 <a title="55-tfidf-20" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.191), (1, 0.046), (2, 0.377), (3, 0.085), (4, -0.087), (5, -0.011), (6, 0.023), (7, 0.016), (8, -0.002), (9, 0.033), (10, 0.043), (11, 0.0), (12, -0.004), (13, -0.021), (14, -0.035), (15, -0.039), (16, 0.036), (17, 0.029), (18, 0.036), (19, 0.049), (20, 0.023), (21, -0.014), (22, -0.102), (23, -0.066), (24, 0.054), (25, -0.062), (26, -0.077), (27, -0.021), (28, 0.003), (29, -0.104), (30, 0.023), (31, 0.01), (32, -0.009), (33, -0.071), (34, 0.158), (35, 0.033), (36, 0.031), (37, 0.018), (38, -0.0), (39, 0.012), (40, -0.076), (41, -0.133), (42, -0.041), (43, -0.082), (44, 0.043), (45, -0.052), (46, 0.002), (47, 0.006), (48, 0.187), (49, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93813753 <a title="55-lsi-1" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>2 0.8324464 <a title="55-lsi-2" href="./nips-2001-Stabilizing_Value_Function_Approximation_with_the_BFBP_Algorithm.html">175 nips-2001-Stabilizing Value Function Approximation with the BFBP Algorithm</a></p>
<p>3 0.72267234 <a title="55-lsi-3" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>4 0.70226091 <a title="55-lsi-4" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>5 0.69352615 <a title="55-lsi-5" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>6 0.63246566 <a title="55-lsi-6" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>7 0.60541975 <a title="55-lsi-7" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>8 0.60036713 <a title="55-lsi-8" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>9 0.58553624 <a title="55-lsi-9" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>10 0.57236785 <a title="55-lsi-10" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>11 0.52962559 <a title="55-lsi-11" href="./nips-2001-Incremental_Learning_and_Selective_Sampling_via_Parametric_Optimization_Framework_for_SVM.html">94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</a></p>
<p>12 0.49618068 <a title="55-lsi-12" href="./nips-2001-Switch_Packet_Arbitration_via_Queue-Learning.html">177 nips-2001-Switch Packet Arbitration via Queue-Learning</a></p>
<p>13 0.4922747 <a title="55-lsi-13" href="./nips-2001-Analysis_of_Sparse_Bayesian_Learning.html">35 nips-2001-Analysis of Sparse Bayesian Learning</a></p>
<p>14 0.40902624 <a title="55-lsi-14" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>15 0.38371313 <a title="55-lsi-15" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>16 0.38131475 <a title="55-lsi-16" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>17 0.37855211 <a title="55-lsi-17" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<p>18 0.36275032 <a title="55-lsi-18" href="./nips-2001-The_Noisy_Euclidean_Traveling_Salesman_Problem_and_Learning.html">186 nips-2001-The Noisy Euclidean Traveling Salesman Problem and Learning</a></p>
<p>19 0.3514584 <a title="55-lsi-19" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>20 0.33504015 <a title="55-lsi-20" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.112), (16, 0.105), (31, 0.024), (36, 0.011), (39, 0.248), (50, 0.166), (63, 0.084), (79, 0.025), (81, 0.017), (91, 0.03), (92, 0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75325155 <a title="55-lda-1" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>2 0.64001018 <a title="55-lda-2" href="./nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</a></p>
<p>3 0.62573272 <a title="55-lda-3" href="./nips-2001-Stabilizing_Value_Function_Approximation_with_the_BFBP_Algorithm.html">175 nips-2001-Stabilizing Value Function Approximation with the BFBP Algorithm</a></p>
<p>4 0.62156427 <a title="55-lda-4" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>5 0.61850357 <a title="55-lda-5" href="./nips-2001-An_Efficient%2C_Exact_Algorithm_for_Solving_Tree-Structured_Graphical_Games.html">32 nips-2001-An Efficient, Exact Algorithm for Solving Tree-Structured Graphical Games</a></p>
<p>6 0.61016488 <a title="55-lda-6" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>7 0.60850441 <a title="55-lda-7" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>8 0.60220104 <a title="55-lda-8" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>9 0.59972495 <a title="55-lda-9" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>10 0.59407979 <a title="55-lda-10" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>11 0.59204787 <a title="55-lda-11" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>12 0.58901197 <a title="55-lda-12" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>13 0.58776116 <a title="55-lda-13" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>14 0.58721548 <a title="55-lda-14" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>15 0.58263797 <a title="55-lda-15" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>16 0.57114565 <a title="55-lda-16" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>17 0.56730545 <a title="55-lda-17" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>18 0.56707048 <a title="55-lda-18" href="./nips-2001-Switch_Packet_Arbitration_via_Queue-Learning.html">177 nips-2001-Switch Packet Arbitration via Queue-Learning</a></p>
<p>19 0.56607425 <a title="55-lda-19" href="./nips-2001-Predictive_Representations_of_State.html">148 nips-2001-Predictive Representations of State</a></p>
<p>20 0.56045437 <a title="55-lda-20" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
