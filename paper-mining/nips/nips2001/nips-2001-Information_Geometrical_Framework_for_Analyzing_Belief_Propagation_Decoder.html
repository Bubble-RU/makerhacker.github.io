<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>98 nips-2001-Information Geometrical Framework for Analyzing Belief Propagation Decoder</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-98" href="#">nips2001-98</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>98 nips-2001-Information Geometrical Framework for Analyzing Belief Propagation Decoder</h1>
<br/><p>Source: <a title="nips-2001-98-pdf" href="http://papers.nips.cc/paper/2010-information-geometrical-framework-for-analyzing-belief-propagation-decoder.pdf">pdf</a></p><p>Author: Shiro Ikeda, Toshiyuki Tanaka, Shun-ichi Amari</p><p>Abstract: The mystery of belief propagation (BP) decoder, especially of the turbo decoding, is studied from information geometrical viewpoint. The loopy belief network (BN) of turbo codes makes it difﬁcult to obtain the true “belief” by BP, and the characteristics of the algorithm and its equilibrium are not clearly understood. Our study gives an intuitive understanding of the mechanism, and a new framework for the analysis. Based on the framework, we reveal basic properties of the turbo decoding.</p><p>Reference: <a title="nips-2001-98-reference" href="../nips2001_reference/nips-2001-Information_Geometrical_Framework_for_Analyzing_Belief_Propagation_Decoder_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp  Abstract The mystery of belief propagation (BP) decoder, especially of the turbo decoding, is studied from information geometrical viewpoint. [sent-13, score-0.966]
</p><p>2 The loopy belief network (BN) of turbo codes makes it difﬁcult to obtain the true “belief” by BP, and the characteristics of the algorithm and its equilibrium are not clearly understood. [sent-14, score-1.063]
</p><p>3 Our study gives an intuitive understanding of the mechanism, and a new framework for the analysis. [sent-15, score-0.15]
</p><p>4 Based on the framework, we reveal basic properties of the turbo decoding. [sent-16, score-0.613]
</p><p>5 1 Introduction Since the proposal of turbo codes[2], they have been attracting a lot of interests because of their high performance of error correction. [sent-17, score-0.673]
</p><p>6 Although the thorough experimental results strongly support the potential of this iterative decoding method, the mathematical background is not sufﬁciently understood. [sent-18, score-0.525]
</p><p>7 [5] have shown its relation to the Pearl’s BP, but the BN for the turbo decoding is loopy, and the BP solution gives only an approximation. [sent-20, score-1.116]
</p><p>8 The problem of the turbo decoding is a speciﬁc example of a general problem of marginalizing an exponential family distribution. [sent-21, score-1.063]
</p><p>9 The distribution includes higher order correlations, and its direct marginalization is intractable. [sent-22, score-0.271]
</p><p>10 By collecting and exchanging the BP results of the partial models, the true “belief” is approximated. [sent-24, score-0.075]
</p><p>11 This structure is common among various iterative methods, such as Gallager codes, Beth´ approximation in statistical physics[4], and BP for loopy BN. [sent-25, score-0.106]
</p><p>12 e We investigate the problem from information geometrical viewpoint[1]. [sent-26, score-0.113]
</p><p>13 It gives a new framework for analyzing these iterative methods, and shows an intuitive understanding of them. [sent-27, score-0.227]
</p><p>14 Also it reveals a lot of basic properties, such as characteristics of the equilibrium, the condition of stability, the cost function related to the decoder, and the decoding error. [sent-28, score-0.551]
</p><p>15 In this  paper, we focus on the turbo decoding, because its structure is simple, but the framework is general, and the main results can be generalized. [sent-29, score-0.644]
</p><p>16 8 £  "   © ©  ¡  ©      9  4 9  £ 1 )' ¡    q  s © w G£  ¢u Fb$8 £  DA$# £ 5 47B$G8 £ " y I I xp P w © H s ¥ I xp s P w 96© wuvH tI ¥ q p©  £ ¡ c ©   fe T r   G© § 8pi¢h0$8 £ " gY¡UWV dc   S R § I ¦¥£ " ba`I XY¡UWV ! [sent-32, score-0.066]
</p><p>17 ¨¦¥¤¢    ¥© ©§ £ ¡  Let us consider a distribution of  which is deﬁned as follows  (1) is the linear function of , and each is the higher order correlations where, of . [sent-34, score-0.075]
</p><p>18 The problem of turbo codes and similar iterative methods are to marginalize this distribution. [sent-35, score-0.76]
</p><p>19 Let denote the operator of marginalization as, marginalization is equivalent to take the expectation of as  . [sent-36, score-0.442]
</p><p>20 The  In the case of MPM (maximization of the posterior marginals) decoding, and the sign of each is the decoding result. [sent-37, score-0.45]
</p><p>21 (1) is not tractable, but the marginalization of the following distribution is tractable. [sent-40, score-0.226]
</p><p>22 The iterative methods are exchanging information through for each , and ﬁnally approximate . [sent-43, score-0.12]
</p><p>23 GvGv¦D§   § © ©§§ £ ¡    In the case of turbo codes, is the information bits, from which the turbo encoder generates two sets of parity bits, , and , (Fig. [sent-53, score-1.316]
</p><p>24 Each parity bit is expressed as the form , where the product is taken over a subset of . [sent-55, score-0.04]
</p><p>25 The codeword is then transmitted over a noisy channel, which we assume BSC (binary symmetric channel) with ﬂipping probability . [sent-56, score-0.043]
</p><p>26 The ultimate goal of the turbo decoding is the MPM decoding of Since the channel is memoryless, the following relation holds  based on  . [sent-58, score-1.62]
</p><p>27 By assuming the uniform prior on , the posterior distribution is given as follows     © $         £ "   G8 £ ¡  £   £ 1 ) ¡   q ¤£ u  32' $! [sent-59, score-0.049]
</p><p>28 When is large, marginalization of is intractable since it needs summation over terms. [sent-64, score-0.203]
</p><p>29 Turbo codes utilize two decoders which solve the MPM decoding of in eq. [sent-65, score-0.595]
</p><p>30 The distribution is derived from and the prior of which has the form of  is a factorizable distribution. [sent-67, score-0.045]
</p><p>31 The marginalization of is feasible since its BN is loop free. [sent-68, score-0.203]
</p><p>32 The parameter serves as the window of exchanging the information between the two decoders. [sent-69, score-0.075]
</p><p>33 The MPM decoding is approximated by updating iteratively in “turbo” like way. [sent-70, score-0.45]
</p><p>34 This is the submanifold of  , every distribution of  deﬁned (4)  can be rewritten as follows  It shows that every distribution of is decomposable, or factorizable. [sent-75, score-0.273]
</p><p>35 From the information geometry[1], we have the following theorem of –projection. [sent-76, score-0.053]
</p><p>36 Let be an –ﬂat submanifold in , and let minimizes the KL-divergence from to , is denoted by, 4 53  . [sent-78, score-0.196]
</p><p>37 The point in  It is easy to show that the marginalization corresponds to the –projection to [7]. [sent-81, score-0.203]
</p><p>38 Since MPM decoding and marginalization is equivalent, MPM decoding is also equivalent to the –projection to . [sent-82, score-1.103]
</p><p>39 #  5  ¨  #  ¨  denote the parameters in  of the #  –projected distribution,  B V@ 2 0  ©  ¡  $S % q 9D8 £ " 6$# £ $ © QPI8) 1R( ¨ $$# £ 5HF T S U5   $ $8 £ ¥S  % GF  Let  The turbo decoding process is written as follows,  ¨  2. [sent-83, score-1.063]
</p><p>40 Let us deﬁne an –ﬂat version of the submanifold as , and in log-linear manner  ,  From its deﬁnition, for any , the expectation of is the same, and its – projection to coincides with . [sent-94, score-0.302]
</p><p>41 This is an –ﬂat submanifold[1], and we call an equimarginal submanifold. [sent-95, score-0.065]
</p><p>42 #  Let us deﬁne a manifold  as  (6) (7)  When the the turbo decoding converges, equilibrium solution deﬁnes three important distributions, , , and . [sent-98, score-1.185]
</p><p>43 and hold because includes cross terms of and in general. [sent-108, score-0.045]
</p><p>44 The information geometrical view of the turbo decoding is schematically shown in Fig. [sent-109, score-1.176]
</p><p>45   We now deﬁne the submanifold corresponding to each decoder,  in eq. [sent-111, score-0.166]
</p><p>46 (5) is An intuitive understanding of the turbo decoding is as follows. [sent-112, score-1.153]
</p><p>47 The distribution becomes , and is estimated by projecting it onto . [sent-114, score-0.053]
</p><p>48 (5) is replaced with , and is estimated by – projection of . [sent-116, score-0.068]
</p><p>49 #  (5)  The turbo decoding approximates the estimated parameter , the projection of onto , as , where the estimated distribution is for  , go to step 2. [sent-117, score-1.184]
</p><p>50 onto  onto  as  , and calculate  as  , and calculate  , and  . [sent-118, score-0.06]
</p><p>51 Following the same line for step 3, we derive the theorem which coincides with the result of Richardson[6]. [sent-125, score-0.085]
</p><p>52 The Fisher information matrix is deﬁned as follows    ! [sent-128, score-0.026]
</p><p>53 8 £  "  Here,  We give a sufﬁciently small perturbation to –projection from to gives,  and apply one turbo decoding step. [sent-129, score-1.089]
</p><p>54 The  Equation (6) is rewritten as follows with these parameters,  | { ¡   e T  c © £ {  © £ e T £5 q  © w ¡£   }v$! [sent-130, score-0.061]
</p><p>55 8 £  "   f ¤¡UWV 7£  6"©v5  R v¡ 8 v5 "   f i¡UWV 7"©vc  5  The expectation parameters are deﬁned as follows with  in eq. [sent-131, score-0.062]
</p><p>56 # C§ " 08 £ 5 " ©  £ © ©  If includes , is the true marginalization of . [sent-135, score-0.248]
</p><p>57 This fact means that and are not necessarily equimarginal, which is the origin of the decoding error. [sent-137, score-0.479]
</p><p>58 When the turbo decoding procedure converges, the convergent probability distributions , , and belong to equimarginal submanifold , while its –ﬂat version includes these three distributions and also the posterior distribution (Fig. [sent-139, score-1.395]
</p><p>59 9    © 9  £ 1 ) ¡   ©  # § $#¤¦¨G3© ©£ $# "  3$# £ 5 #420' $3© # £ "   © 3© 8 £ "  Let us consider , where every distribution tion parameter, that is, holds. [sent-169, score-0.023]
</p><p>60 Here, we deﬁne, the Taylor expansion, we have, This distribution includes ( , ), where parameter is deﬁned as,  ,  (  , and  ),  (  For the following discussion, we deﬁne a distribution  ,  (8)  has the same expecta. [sent-170, score-0.091]
</p><p>61 is always negative, and  is generally  R 'R  and  , we have     u 5 ¤  R  R  ©   ©  R 'R  And by transforming the variables as,           This shows how the algorithm works, but it does not give the characteristics of the equilibrium point. [sent-174, score-0.16]
</p><p>62 Direct calculation gives equilibrium,  , holds, and the proof is completed. [sent-176, score-0.029]
</p><p>63 We give the cost function which plays an important role in turbo decoding. [sent-182, score-0.635]
</p><p>64 3 Cost Function and Characteristics of Equilibrium  ¢    u   ¦£ §  § { © ©75 ³ £  ¡  holds for all , the equilibrium point is stable. [sent-184, score-0.166]
</p><p>65 (10), we have the following approximation, ©    §    §  ©  From the condition  , where is the parameter which is not necessarily included in ©  ©     Next, let , and we consider satisﬁes this equation. [sent-193, score-0.084]
</p><p>66 Also when we put following result,  ©  §  Let     ©  , and      where,    §    §  " #  This result gives the approximation accuracy of the BP decoding. [sent-197, score-0.029]
</p><p>67 Let the true belief be , and we evaluate the difference between and on . [sent-198, score-0.127]
</p><p>68 The true expectation of , which is , is approximated as, $ %  $ %  $ &     ©  §       ©    §  $ %  " #  Where  (11)  is the solution of the turbo decoding. [sent-201, score-0.649]
</p><p>69 The result can be  ) ' 0(  4 Discussion We have shown a new framework for understanding and analyzing the belief propagation decoder. [sent-205, score-0.32]
</p><p>70 Since the BN of turbo codes is loopy, we don’t have enough theoretical results for BP algorithm, while a lot of experiments show that it works surprisingly well in such cases. [sent-206, score-0.753]
</p><p>71 The mystery of the BP decoders is summarized in 2 points, the approximation accuracy and the convergence property. [sent-207, score-0.118]
</p><p>72 Our results elucidate the mathematical background of the BP decoding algorithm. [sent-208, score-0.48]
</p><p>73 The information geometrical structure of the equilibrium is summarized in Theorem 2. [sent-209, score-0.267]
</p><p>74 It shows  © ©£ © ©£   °  the –ﬂat submanifold plays an important role. [sent-210, score-0.188]
</p><p>75 Furthermore, Theorem 5 shows that the relation between and the –ﬂat submanifold causes the decoding error, and the principal component of the error is the curvature of . [sent-211, score-0.687]
</p><p>76 Since the curvature strongly depends on the codeword, we can control it by the encoder design. [sent-212, score-0.097]
</p><p>77 This shows a room for improvement of the “near optimum error correcting code”[2]. [sent-213, score-0.06]
</p><p>78 © "©£  © ©£   ¨  #    For the convergent property, we have shown the energy function, which is known as Beth´ e free energy[4, 9]. [sent-214, score-0.093]
</p><p>79 Unfortunately, the ﬁxed point of the turbo decoding algorithm is generally a saddle of the function, which makes further analysis difﬁcult. [sent-215, score-1.091]
</p><p>80 We have only shown a local stability condition, and the global property is one of our future works. [sent-216, score-0.03]
</p><p>81 This paper gives a ﬁrst step to the information geometrical understanding of the belief propagation decoder. [sent-217, score-0.399]
</p><p>82 The main results are for the turbo decoding, but the mechanism is common with wider class, and the framework is valid for them. [sent-218, score-0.644]
</p><p>83 We believe further study in this direction will lead us to better understanding and improvements of these methods. [sent-219, score-0.06]
</p><p>84 (1996) Near optimum error correcting coding and decoding: Turbo-codes. [sent-230, score-0.06]
</p><p>85 (2001) Information geometry of turbo codes and low-density parity-check codes. [sent-236, score-0.775]
</p><p>86 Saad, editors, Advanced Mean Field Methods – Theory and Practice, chapter 6, pages 65–84. [sent-244, score-0.03]
</p><p>87 (1998) Turbo decoding as an instance of Pearl’s “belief propagation” algorithm. [sent-254, score-0.45]
</p><p>88 Saad, editors, Advanced Mean Field Methods – Theory and Practice, chapter 17, pages 259–273. [sent-266, score-0.03]
</p><p>89 (2001) Bethe free energy, Kikuchi approximations, and belief propagation algorithms. [sent-285, score-0.219]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('turbo', 0.613), ('decoding', 0.45), ('marginalization', 0.203), ('uwv', 0.175), ('mpm', 0.174), ('submanifold', 0.166), ('bp', 0.155), ('belief', 0.127), ('equilibrium', 0.122), ('geometrical', 0.113), ('codes', 0.102), ('srq', 0.1), ('decoder', 0.087), ('exchanging', 0.075), ('vip', 0.075), ('tanaka', 0.074), ('vh', 0.074), ('propagation', 0.07), ('bn', 0.069), ('projection', 0.068), ('hf', 0.066), ('equimarginal', 0.065), ('loopy', 0.061), ('understanding', 0.06), ('geometry', 0.06), ('ikeda', 0.059), ('de', 0.053), ('theorem', 0.053), ('beth', 0.05), ('encoder', 0.05), ('curvature', 0.047), ('includes', 0.045), ('iterative', 0.045), ('holds', 0.044), ('mystery', 0.043), ('codeword', 0.043), ('decoders', 0.043), ('gallager', 0.043), ('kabashima', 0.043), ('shiro', 0.043), ('aii', 0.043), ('gf', 0.041), ('japan', 0.04), ('wy', 0.04), ('parity', 0.04), ('mceliece', 0.04), ('xf', 0.04), ('channel', 0.039), ('characteristics', 0.038), ('energy', 0.038), ('lot', 0.038), ('ned', 0.037), ('saad', 0.037), ('expectation', 0.036), ('amari', 0.036), ('rewritten', 0.035), ('correcting', 0.035), ('communications', 0.035), ('ii', 0.035), ('fisher', 0.033), ('xp', 0.033), ('tokyo', 0.033), ('convergent', 0.033), ('summarized', 0.032), ('analyzing', 0.032), ('coincides', 0.032), ('framework', 0.031), ('intuitive', 0.03), ('stability', 0.03), ('chapter', 0.03), ('opper', 0.03), ('onto', 0.03), ('mathematical', 0.03), ('let', 0.03), ('necessarily', 0.029), ('pearl', 0.029), ('gives', 0.029), ('advanced', 0.028), ('saddle', 0.028), ('ig', 0.027), ('ne', 0.027), ('ge', 0.026), ('bits', 0.026), ('perturbation', 0.026), ('follows', 0.026), ('correlations', 0.026), ('optimum', 0.025), ('condition', 0.025), ('relation', 0.024), ('ip', 0.024), ('distribution', 0.023), ('free', 0.022), ('plays', 0.022), ('wtu', 0.022), ('attracting', 0.022), ('mitsubishi', 0.022), ('uv', 0.022), ('xip', 0.022), ('bsc', 0.022), ('factorizable', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="98-tfidf-1" href="./nips-2001-Information_Geometrical_Framework_for_Analyzing_Belief_Propagation_Decoder.html">98 nips-2001-Information Geometrical Framework for Analyzing Belief Propagation Decoder</a></p>
<p>Author: Shiro Ikeda, Toshiyuki Tanaka, Shun-ichi Amari</p><p>Abstract: The mystery of belief propagation (BP) decoder, especially of the turbo decoding, is studied from information geometrical viewpoint. The loopy belief network (BN) of turbo codes makes it difﬁcult to obtain the true “belief” by BP, and the characteristics of the algorithm and its equilibrium are not clearly understood. Our study gives an intuitive understanding of the mechanism, and a new framework for the analysis. Based on the framework, we reveal basic properties of the turbo decoding.</p><p>2 0.51874119 <a title="98-tfidf-2" href="./nips-2001-Information-Geometrical_Significance_of_Sparsity_in_Gallager_Codes.html">97 nips-2001-Information-Geometrical Significance of Sparsity in Gallager Codes</a></p>
<p>Author: Toshiyuki Tanaka, Shiro Ikeda, Shun-ichi Amari</p><p>Abstract: We report a result of perturbation analysis on decoding error of the belief propagation decoder for Gallager codes. The analysis is based on information geometry, and it shows that the principal term of decoding error at equilibrium comes from the m-embedding curvature of the log-linear submanifold spanned by the estimated pseudoposteriors, one for the full marginal, and K for partial posteriors, each of which takes a single check into account, where K is the number of checks in the Gallager code. It is then shown that the principal error term vanishes when the parity-check matrix of the code is so sparse that there are no two columns with overlap greater than 1. 1</p><p>3 0.19952647 <a title="98-tfidf-3" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>Author: Si Wu, Shun-ichi Amari</p><p>Abstract: This study investigates a population decoding paradigm, in which the estimation of stimulus in the previous step is used as prior knowledge for consecutive decoding. We analyze the decoding accuracy of such a Bayesian decoder (Maximum a Posteriori Estimate), and show that it can be implemented by a biologically plausible recurrent network, where the prior knowledge of stimulus is conveyed by the change in recurrent interactions as a result of Hebbian learning. 1</p><p>4 0.14952168 <a title="98-tfidf-4" href="./nips-2001-The_Unified_Propagation_and_Scaling_Algorithm.html">188 nips-2001-The Unified Propagation and Scaling Algorithm</a></p>
<p>Author: Yee W. Teh, Max Welling</p><p>Abstract: In this paper we will show that a restricted class of constrained minimum divergence problems, named generalized inference problems, can be solved by approximating the KL divergence with a Bethe free energy. The algorithm we derive is closely related to both loopy belief propagation and iterative scaling. This uniﬁed propagation and scaling algorithm reduces to a convergent alternative to loopy belief propagation when no constraints are present. Experiments show the viability of our algorithm.</p><p>5 0.12382632 <a title="98-tfidf-5" href="./nips-2001-Tree-based_reparameterization_for_approximate_inference_on_loopy_graphs.html">192 nips-2001-Tree-based reparameterization for approximate inference on loopy graphs</a></p>
<p>Author: Martin J. Wainwright, Tommi Jaakkola, Alan S. Willsky</p><p>Abstract: We develop a tree-based reparameterization framework that provides a new conceptual view of a large class of iterative algorithms for computing approximate marginals in graphs with cycles. It includes belief propagation (BP), which can be reformulated as a very local form of reparameterization. More generally, we consider algorithms that perform exact computations over spanning trees of the full graph. On the practical side, we find that such tree reparameterization (TRP) algorithms have convergence properties superior to BP. The reparameterization perspective also provides a number of theoretical insights into approximate inference, including a new characterization of fixed points; and an invariance intrinsic to TRP /BP. These two properties enable us to analyze and bound the error between the TRP /BP approximations and the actual marginals. While our results arise naturally from the TRP perspective, most of them apply in an algorithm-independent manner to any local minimum of the Bethe free energy. Our results also have natural extensions to more structured approximations [e.g. , 1, 2]. 1</p><p>6 0.11907539 <a title="98-tfidf-6" href="./nips-2001-Very_loopy_belief_propagation_for_unwrapping_phase_images.html">196 nips-2001-Very loopy belief propagation for unwrapping phase images</a></p>
<p>7 0.096618935 <a title="98-tfidf-7" href="./nips-2001-Reducing_multiclass_to_binary_by_coupling_probability_estimates.html">159 nips-2001-Reducing multiclass to binary by coupling probability estimates</a></p>
<p>8 0.083518207 <a title="98-tfidf-8" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>9 0.05987709 <a title="98-tfidf-9" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>10 0.040683612 <a title="98-tfidf-10" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>11 0.040479701 <a title="98-tfidf-11" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>12 0.037073482 <a title="98-tfidf-12" href="./nips-2001-Geometrical_Singularities_in_the_Neuromanifold_of_Multilayer_Perceptrons.html">83 nips-2001-Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons</a></p>
<p>13 0.036352962 <a title="98-tfidf-13" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>14 0.033422757 <a title="98-tfidf-14" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>15 0.033027302 <a title="98-tfidf-15" href="./nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</a></p>
<p>16 0.032348882 <a title="98-tfidf-16" href="./nips-2001-MIME%3A_Mutual_Information_Minimization_and_Entropy_Maximization_for_Bayesian_Belief_Propagation.html">117 nips-2001-MIME: Mutual Information Minimization and Entropy Maximization for Bayesian Belief Propagation</a></p>
<p>17 0.032284096 <a title="98-tfidf-17" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>18 0.032067027 <a title="98-tfidf-18" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>19 0.031741798 <a title="98-tfidf-19" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>20 0.031582762 <a title="98-tfidf-20" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.128), (1, -0.032), (2, -0.008), (3, -0.039), (4, 0.027), (5, -0.299), (6, 0.094), (7, -0.338), (8, 0.307), (9, 0.282), (10, -0.236), (11, -0.071), (12, -0.074), (13, 0.049), (14, 0.013), (15, -0.001), (16, 0.225), (17, -0.115), (18, 0.014), (19, 0.149), (20, 0.011), (21, 0.195), (22, 0.103), (23, -0.026), (24, -0.001), (25, -0.016), (26, 0.05), (27, -0.039), (28, -0.015), (29, 0.009), (30, 0.026), (31, 0.024), (32, 0.107), (33, -0.019), (34, -0.071), (35, 0.02), (36, -0.003), (37, -0.013), (38, 0.056), (39, 0.024), (40, 0.033), (41, 0.042), (42, -0.04), (43, -0.008), (44, 0.011), (45, -0.029), (46, 0.003), (47, -0.033), (48, -0.035), (49, -0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97782272 <a title="98-lsi-1" href="./nips-2001-Information_Geometrical_Framework_for_Analyzing_Belief_Propagation_Decoder.html">98 nips-2001-Information Geometrical Framework for Analyzing Belief Propagation Decoder</a></p>
<p>Author: Shiro Ikeda, Toshiyuki Tanaka, Shun-ichi Amari</p><p>Abstract: The mystery of belief propagation (BP) decoder, especially of the turbo decoding, is studied from information geometrical viewpoint. The loopy belief network (BN) of turbo codes makes it difﬁcult to obtain the true “belief” by BP, and the characteristics of the algorithm and its equilibrium are not clearly understood. Our study gives an intuitive understanding of the mechanism, and a new framework for the analysis. Based on the framework, we reveal basic properties of the turbo decoding.</p><p>2 0.95959067 <a title="98-lsi-2" href="./nips-2001-Information-Geometrical_Significance_of_Sparsity_in_Gallager_Codes.html">97 nips-2001-Information-Geometrical Significance of Sparsity in Gallager Codes</a></p>
<p>Author: Toshiyuki Tanaka, Shiro Ikeda, Shun-ichi Amari</p><p>Abstract: We report a result of perturbation analysis on decoding error of the belief propagation decoder for Gallager codes. The analysis is based on information geometry, and it shows that the principal term of decoding error at equilibrium comes from the m-embedding curvature of the log-linear submanifold spanned by the estimated pseudoposteriors, one for the full marginal, and K for partial posteriors, each of which takes a single check into account, where K is the number of checks in the Gallager code. It is then shown that the principal error term vanishes when the parity-check matrix of the code is so sparse that there are no two columns with overlap greater than 1. 1</p><p>3 0.61971748 <a title="98-lsi-3" href="./nips-2001-Very_loopy_belief_propagation_for_unwrapping_phase_images.html">196 nips-2001-Very loopy belief propagation for unwrapping phase images</a></p>
<p>Author: Brendan J. Frey, Ralf Koetter, Nemanja Petrovic</p><p>Abstract: Since the discovery that the best error-correcting decoding algorithm can be viewed as belief propagation in a cycle-bound graph, researchers have been trying to determine under what circumstances</p><p>4 0.59153575 <a title="98-lsi-4" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>Author: Si Wu, Shun-ichi Amari</p><p>Abstract: This study investigates a population decoding paradigm, in which the estimation of stimulus in the previous step is used as prior knowledge for consecutive decoding. We analyze the decoding accuracy of such a Bayesian decoder (Maximum a Posteriori Estimate), and show that it can be implemented by a biologically plausible recurrent network, where the prior knowledge of stimulus is conveyed by the change in recurrent interactions as a result of Hebbian learning. 1</p><p>5 0.3592557 <a title="98-lsi-5" href="./nips-2001-Tree-based_reparameterization_for_approximate_inference_on_loopy_graphs.html">192 nips-2001-Tree-based reparameterization for approximate inference on loopy graphs</a></p>
<p>Author: Martin J. Wainwright, Tommi Jaakkola, Alan S. Willsky</p><p>Abstract: We develop a tree-based reparameterization framework that provides a new conceptual view of a large class of iterative algorithms for computing approximate marginals in graphs with cycles. It includes belief propagation (BP), which can be reformulated as a very local form of reparameterization. More generally, we consider algorithms that perform exact computations over spanning trees of the full graph. On the practical side, we find that such tree reparameterization (TRP) algorithms have convergence properties superior to BP. The reparameterization perspective also provides a number of theoretical insights into approximate inference, including a new characterization of fixed points; and an invariance intrinsic to TRP /BP. These two properties enable us to analyze and bound the error between the TRP /BP approximations and the actual marginals. While our results arise naturally from the TRP perspective, most of them apply in an algorithm-independent manner to any local minimum of the Bethe free energy. Our results also have natural extensions to more structured approximations [e.g. , 1, 2]. 1</p><p>6 0.34897408 <a title="98-lsi-6" href="./nips-2001-Reducing_multiclass_to_binary_by_coupling_probability_estimates.html">159 nips-2001-Reducing multiclass to binary by coupling probability estimates</a></p>
<p>7 0.28982323 <a title="98-lsi-7" href="./nips-2001-The_Unified_Propagation_and_Scaling_Algorithm.html">188 nips-2001-The Unified Propagation and Scaling Algorithm</a></p>
<p>8 0.26124597 <a title="98-lsi-8" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>9 0.21704102 <a title="98-lsi-9" href="./nips-2001-Geometrical_Singularities_in_the_Neuromanifold_of_Multilayer_Perceptrons.html">83 nips-2001-Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons</a></p>
<p>10 0.1834372 <a title="98-lsi-10" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>11 0.14090729 <a title="98-lsi-11" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>12 0.13617761 <a title="98-lsi-12" href="./nips-2001-Learning_Hierarchical_Structures_with_Linear_Relational_Embedding.html">110 nips-2001-Learning Hierarchical Structures with Linear Relational Embedding</a></p>
<p>13 0.1233076 <a title="98-lsi-13" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>14 0.12103004 <a title="98-lsi-14" href="./nips-2001-Efficiency_versus_Convergence_of_Boolean_Kernels_for_On-Line_Learning_Algorithms.html">66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</a></p>
<p>15 0.11993129 <a title="98-lsi-15" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>16 0.11797839 <a title="98-lsi-16" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>17 0.11408854 <a title="98-lsi-17" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>18 0.11342825 <a title="98-lsi-18" href="./nips-2001-Scaling_Laws_and_Local_Minima_in_Hebbian_ICA.html">165 nips-2001-Scaling Laws and Local Minima in Hebbian ICA</a></p>
<p>19 0.11322916 <a title="98-lsi-19" href="./nips-2001-Minimax_Probability_Machine.html">120 nips-2001-Minimax Probability Machine</a></p>
<p>20 0.11041331 <a title="98-lsi-20" href="./nips-2001-On_the_Concentration_of_Spectral_Properties.html">136 nips-2001-On the Concentration of Spectral Properties</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.018), (16, 0.13), (17, 0.018), (19, 0.03), (27, 0.258), (30, 0.061), (38, 0.012), (59, 0.036), (72, 0.05), (79, 0.026), (83, 0.03), (88, 0.149), (91, 0.065)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91396916 <a title="98-lda-1" href="./nips-2001-Information_Geometrical_Framework_for_Analyzing_Belief_Propagation_Decoder.html">98 nips-2001-Information Geometrical Framework for Analyzing Belief Propagation Decoder</a></p>
<p>Author: Shiro Ikeda, Toshiyuki Tanaka, Shun-ichi Amari</p><p>Abstract: The mystery of belief propagation (BP) decoder, especially of the turbo decoding, is studied from information geometrical viewpoint. The loopy belief network (BN) of turbo codes makes it difﬁcult to obtain the true “belief” by BP, and the characteristics of the algorithm and its equilibrium are not clearly understood. Our study gives an intuitive understanding of the mechanism, and a new framework for the analysis. Based on the framework, we reveal basic properties of the turbo decoding.</p><p>2 0.85542917 <a title="98-lda-2" href="./nips-2001-Information-Geometrical_Significance_of_Sparsity_in_Gallager_Codes.html">97 nips-2001-Information-Geometrical Significance of Sparsity in Gallager Codes</a></p>
<p>Author: Toshiyuki Tanaka, Shiro Ikeda, Shun-ichi Amari</p><p>Abstract: We report a result of perturbation analysis on decoding error of the belief propagation decoder for Gallager codes. The analysis is based on information geometry, and it shows that the principal term of decoding error at equilibrium comes from the m-embedding curvature of the log-linear submanifold spanned by the estimated pseudoposteriors, one for the full marginal, and K for partial posteriors, each of which takes a single check into account, where K is the number of checks in the Gallager code. It is then shown that the principal error term vanishes when the parity-check matrix of the code is so sparse that there are no two columns with overlap greater than 1. 1</p><p>3 0.8492707 <a title="98-lda-3" href="./nips-2001-Boosting_and_Maximum_Likelihood_for_Exponential_Models.html">45 nips-2001-Boosting and Maximum Likelihood for Exponential Models</a></p>
<p>Author: Guy Lebanon, John D. Lafferty</p><p>Abstract: We derive an equivalence between AdaBoost and the dual of a convex optimization problem, showing that the only difference between minimizing the exponential loss used by AdaBoost and maximum likelihood for exponential models is that the latter requires the model to be normalized to form a conditional probability distribution over labels. In addition to establishing a simple and easily understood connection between the two methods, this framework enables us to derive new regularization procedures for boosting that directly correspond to penalized maximum likelihood. Experiments on UCI datasets support our theoretical analysis and give additional insight into the relationship between boosting and logistic regression.</p><p>4 0.81100988 <a title="98-lda-4" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: We present an algorithm that induces a class of models with thin junction trees—models that are characterized by an upper bound on the size of the maximal cliques of their triangulated graph. By ensuring that the junction tree is thin, inference in our models remains tractable throughout the learning process. This allows both an efﬁcient implementation of an iterative scaling parameter estimation algorithm and also ensures that inference can be performed efﬁciently with the ﬁnal model. We illustrate the approach with applications in handwritten digit recognition and DNA splice site detection.</p><p>5 0.81032592 <a title="98-lda-5" href="./nips-2001-Fast%2C_Large-Scale_Transformation-Invariant_Clustering.html">75 nips-2001-Fast, Large-Scale Transformation-Invariant Clustering</a></p>
<p>Author: Brendan J. Frey, Nebojsa Jojic</p><p>Abstract: In previous work on “transformed mixtures of Gaussians” and “transformed hidden Markov models”, we showed how the EM algorithm in a discrete latent variable model can be used to jointly normalize data (e.g., center images, pitch-normalize spectrograms) and learn a mixture model of the normalized data. The only input to the algorithm is the data, a list of possible transformations, and the number of clusters to ﬁnd. The main criticism of this work was that the exhaustive computation of the posterior probabilities over transformations would make scaling up to large feature vectors and large sets of transformations intractable. Here, we describe how a tremendous speed-up is acheived through the use of a variational technique for decoupling transformations, and a fast Fourier transform method for computing posterior probabilities. For N ×N images, learning C clusters under N rotations, N scales, N x-translations and N y-translations takes only (C + 2 log N )N 2 scalar operations per iteration. In contrast, the original algorithm takes CN 6 operations to account for these transformations. We give results on learning a 4-component mixture model from a video sequence with frames of size 320 ×240. The model accounts for 360 rotations and 76,800 translations. Each iteration of EM takes only 10 seconds per frame in MATLAB, which is over 5 million times faster than the original algorithm. 1</p><p>6 0.80645299 <a title="98-lda-6" href="./nips-2001-Laplacian_Eigenmaps_and_Spectral_Techniques_for_Embedding_and_Clustering.html">106 nips-2001-Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</a></p>
<p>7 0.80458951 <a title="98-lda-7" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>8 0.80165398 <a title="98-lda-8" href="./nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes.html">133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</a></p>
<p>9 0.80109715 <a title="98-lda-9" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>10 0.79963136 <a title="98-lda-10" href="./nips-2001-Scaling_Laws_and_Local_Minima_in_Hebbian_ICA.html">165 nips-2001-Scaling Laws and Local Minima in Hebbian ICA</a></p>
<p>11 0.79807591 <a title="98-lda-11" href="./nips-2001-Causal_Categorization_with_Bayes_Nets.html">47 nips-2001-Causal Categorization with Bayes Nets</a></p>
<p>12 0.79442906 <a title="98-lda-12" href="./nips-2001-MIME%3A_Mutual_Information_Minimization_and_Entropy_Maximization_for_Bayesian_Belief_Propagation.html">117 nips-2001-MIME: Mutual Information Minimization and Entropy Maximization for Bayesian Belief Propagation</a></p>
<p>13 0.78043878 <a title="98-lda-13" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>14 0.76752335 <a title="98-lda-14" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>15 0.73522878 <a title="98-lda-15" href="./nips-2001-Kernel_Feature_Spaces_and_Nonlinear_Blind_Souce_Separation.html">103 nips-2001-Kernel Feature Spaces and Nonlinear Blind Souce Separation</a></p>
<p>16 0.72146612 <a title="98-lda-16" href="./nips-2001-Multi_Dimensional_ICA_to_Separate_Correlated_Sources.html">127 nips-2001-Multi Dimensional ICA to Separate Correlated Sources</a></p>
<p>17 0.72073245 <a title="98-lda-17" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>18 0.71448821 <a title="98-lda-18" href="./nips-2001-Learning_from_Infinite_Data_in_Finite_Time.html">114 nips-2001-Learning from Infinite Data in Finite Time</a></p>
<p>19 0.71402413 <a title="98-lda-19" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>20 0.70850658 <a title="98-lda-20" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
