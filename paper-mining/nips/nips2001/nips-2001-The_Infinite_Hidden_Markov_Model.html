<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>183 nips-2001-The Infinite Hidden Markov Model</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-183" href="#">nips2001-183</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>183 nips-2001-The Infinite Hidden Markov Model</h1>
<br/><p>Source: <a title="nips-2001-183-pdf" href="http://papers.nips.cc/paper/1956-the-infinite-hidden-markov-model.pdf">pdf</a></p><p>Author: Matthew J. Beal, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We show that it is possible to extend hidden Markov models to have a countably inﬁnite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the inﬁnitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters deﬁne a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a ﬁnite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be inﬁnite— consider, for example, symbols being possible words appearing in English text.</p><p>Reference: <a title="nips-2001-183-reference" href="../nips2001_reference/nips-2001-The_Infinite_Hidden_Markov_Model_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  ¡     Abstract We show that it is possible to extend hidden Markov models to have a countably inﬁnite number of hidden states. [sent-10, score-0.537]
</p><p>2 By using the theory of Dirichlet processes we can implicitly integrate out the inﬁnitely many transition parameters, leaving only three hyperparameters which can be learned from data. [sent-11, score-0.529]
</p><p>3 These three hyperparameters deﬁne a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. [sent-12, score-0.492]
</p><p>4 The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a ﬁnite sequence. [sent-13, score-0.664]
</p><p>5 In this framework it is also natural to allow the alphabet of emitted symbols to be inﬁnite— consider, for example, symbols being possible words appearing in English text. [sent-14, score-0.167]
</p><p>6 An HMM deﬁnes a probability distribution over sequences of observations (symbols) by invoking another sequence of unobserved, or hidden, discrete state variables . [sent-16, score-0.299]
</p><p>7 The basic idea in an HMM is that the seis independent of quence of hidden states has Markov dynamics—i. [sent-17, score-0.42]
</p><p>8 The model is deﬁned in terms of two sets of parameters, the transition matrix whose element is and the emission matrix whose element is . [sent-20, score-0.621]
</p><p>9 It has been proposed to approximate such Bayesian integration both using variational methods [3] and by conditioning on a single most likely hidden state sequence [8]. [sent-28, score-0.511]
</p><p>10 In this paper we start from the point of view that the basic modelling assumption of HMMs—that the data was generated by some discrete state variable which can take on one of several values—is unreasonable for most real-world problems. [sent-29, score-0.207]
</p><p>11 Instead we formulate the idea of HMMs with a countably inﬁnite number of hidden states. [sent-30, score-0.29]
</p><p>12 In principle, such models have inﬁnitely many parameters in the state transition matrix. [sent-31, score-0.4]
</p><p>13 Obviously it would not be sensible to optimise these parameters; instead we use the theory of Dirichlet processes (DPs) [2, 1] to implicitly integrate them out, leaving just three hyperparameters deﬁning the prior over transition dynamics. [sent-32, score-0.565]
</p><p>14 1 Because of this we have extended the notion of a DP to a two-stage hierarchical process which couples transitions between different states. [sent-35, score-0.204]
</p><p>15 It should be stressed that Dirichlet distributions have been used extensively both as priors for mixing proportions and to smooth n-gram models over ﬁnite alphabets [4], which differs considerably from the model presented here. [sent-36, score-0.142]
</p><p>16 We explore properties of the HDP prior, showing that it can generate interesting hidden state sequences and that it can also be used as an emission model for an inﬁnite alphabet of symbols. [sent-39, score-0.895]
</p><p>17 This inﬁnite emission model is controlled by two additional hyperparameters. [sent-40, score-0.389]
</p><p>18 In section 4 we describe the procedures for inference (Gibbs sampling the hidden states), learning (optimising the hyperparameters), and likelihood evaluation (inﬁnite-state particle ﬁltering). [sent-41, score-0.421]
</p><p>19 2 Properties of the Dirichlet Process  @ £ $   Let us examine in detail the statistics of hidden state transitions from a particular state to , with the number of hidden states ﬁnite and equal to . [sent-43, score-1.13]
</p><p>20 The transition probabilities row of the transition matrix can be interpreted as mixing proportions for given in the that we call . [sent-44, score-0.672]
</p><p>21 © ¡ ¡©¦¦0©© §¦  %   £ ED @C ©§ ¦ £ §¥  £ ¤   ¨ ¥  ¨ ©  samples from a discrete indicator variable which can take with proportions given by . [sent-46, score-0.139]
</p><p>22 Let us see what happens to the distribution of these indicators when we integrate out the mixing proportions under a conjugate prior. [sent-49, score-0.25]
</p><p>23 The conditional probability of an indicator given the setting of all other indicators (denoted ) is given by (4)  where is the counts as in (1) with the indicator removed. [sent-53, score-0.286]
</p><p>24 A key property of DPs, which is at the very heart of the model in this paper, is the expression for (4) when we take the limit as the number of hidden states tends to inﬁnity: i. [sent-55, score-0.42]
</p><p>25 T    Q I§  U ¦ R G £ P) © T ¨ SA £ ¦ ¨ HF  Q  Q T   for all unrepresented , combined  ) ¦ 0 ©T b  '  where is the number of represented states (i. [sent-58, score-0.295]
</p><p>26 (  ¡ b © ¦¦©   £  )  ¡  G   F  ¡  G    ¢ )  3 Hierarchical Dirichlet Process (HDP) We now consider modelling each row of the transition and emission matrices of an HMM as a DP. [sent-65, score-0.767]
</p><p>27 The ﬁrst is that we can integrate out the inﬁnite number of transition parameters, and represent the process with a ﬁnite number of indicator variables. [sent-67, score-0.37]
</p><p>28 The second is that under a DP there is a natural tendency to use existing transitions in proportion to their previous usage, which gives rise to typical trajectories. [sent-68, score-0.224]
</p><p>29 2 we describe in detail the HDP model for transitions and emissions for an inﬁnite-state HMM. [sent-71, score-0.212]
</p><p>30 1 Hidden state transition mechanism  £ 3b PBI $ 4 A @ 1§  Imagine we have generated a hidden state sequence up to and including time , building a table of counts for transitions that have occured so far from state to , i. [sent-73, score-1.355]
</p><p>31 Given that we are in state , we impose on state a DP (5) with parameter whose counts are those entries in the row of , i. [sent-76, score-0.491]
</p><p>32 Given that we have defaulted to the oracle DP, the probabilities of transitioning now become )      G H)             i. [sent-82, score-0.286]
</p><p>33 represented  (7)        2 Under the inﬁnite model, at any time, there are an inﬁnite number of (indistinguishable) unrepresented states available, each of which have inﬁnitesimal mass proportional to . [sent-86, score-0.327]
</p><p>34 R  a) nii + α  Σ nij + β + α j  self transition  nij  b)  c)  d)  β  Σ nij + β + α Σnij + β + α j  j  existing transition  oracle  j=i  njo  γ  Σ n jo + γ  Σ n jo + γ  existing state  new state  j  j  Figure 1:  (left) State transition generative mechanism. [sent-87, score-1.625]
</p><p>35 (right a-d) Sampled state trajectories (time along horizontal axis) from the HDP: we give examples of four modes of , explores many states with a sparse transition matrix. [sent-88, score-0.672]
</p><p>36 (c) , , has strict left-to-right transition switches between a few different states. [sent-90, score-0.232]
</p><p>37 (a)  £ ¡ £ ¡ 1 ¡ ¢(54¢&R; 320¨  §§§§ ¡  ¡  ¡ ¦895¦87R 6¨  8  Under the oracle, with probability proportional to an entirely new state is transitioned to. [sent-94, score-0.275]
</p><p>38 This is the only mechanism for visiting new states from the inﬁnitely many available to us. [sent-95, score-0.208]
</p><p>39 After each transition we set and, if we transitioned to the state via the . [sent-96, score-0.507]
</p><p>40 If we transitioned to a new oracle DP just described then in addition we set state then the size of and will increase. [sent-97, score-0.48]
</p><p>41 A      ¤  7b A 2b 1 @ 1  ¤ 2b @ 3b 9 9   9b  b  Self-transitions are special because their probability deﬁnes a time scale over which the dynamics of the hidden state evolves. [sent-98, score-0.446]
</p><p>42 We assign a ﬁnite prior mass to self transitions for each state; this is the third hyperparameter in our model. [sent-99, score-0.347]
</p><p>43 Therefore, when ﬁrst visited (via in the HDP), its self-transition count is initialised to . [sent-100, score-0.148]
</p><p>44 B  8  B  The full hidden state transition mechanism is a two-level DP hierarchy shown in decision tree form in Figure 1. [sent-101, score-0.682]
</p><p>45 Alongside are shown typical state trajectories under the prior with different hyperparameters. [sent-102, score-0.276]
</p><p>46 Note that controls the expected number of represented hidden states, and inﬂuences the tendency to explore new transitions, corresponding to the size and density respectively of the resulting transition count matrix. [sent-104, score-0.718]
</p><p>47 First it serves to couple the transition DPs from different hidden states. [sent-107, score-0.479]
</p><p>48 Since a newly visited state has no previous transitions to existing states, without an oracle (which necessarily has knowledge of all represented states as it created them) it would transition to itself or yet another new state with probability 1. [sent-108, score-1.174]
</p><p>49 By consulting the oracle, new states can have ﬁnite probability of transitioning to represented states. [sent-109, score-0.287]
</p><p>50 The second role of the oracle is to allow some states to be more inﬂuential (more commonly transitioned to) than others. [sent-110, score-0.485]
</p><p>51 2 Emission mechanism  § PBI $  C E $  ¥  C D $  The emission process is identical to the transition process in every respect except that there is no concept analogous to a self-transition. [sent-112, score-0.712]
</p><p>52 Therefore we need only introduce two further hyperparameters and for the emission HDP. [sent-113, score-0.572]
</p><p>53 5 4  x 10  10  0  20  40  60  80  100  Figure 2:  (left) State emission generative mechanism. [sent-117, score-0.389]
</p><p>54 (right) (Exp 1) Evolution of number of represented (vertical), plotted against iterations of Gibbs sweeps (horizontal) during learning of the states ascending-descending sequence which requires exactly 10 states to model the data perfectly. [sent-120, score-0.593]
</p><p>55 Each line represents initialising the hidden state to a random sequence containing distinct represented states. [sent-121, score-0.63]
</p><p>56 )     ¥ 1£ £  ¡ ¦¦$$¤3£ ¦4¢¡    has been emitted using the emission oracle. [sent-123, score-0.44]
</p><p>57 The combination of an HDP for both hidden states and emissions may well be able to capture the somewhat super-logarithmic word generation found in Alice. [sent-132, score-0.542]
</p><p>58 4 Inference, learning and likelihoods Given a sequence of observations, there are two sets of unknowns in the inﬁnite HMM: , and the ﬁve hyperparameters the hidden state sequence deﬁning the transition and emission HDPs. [sent-133, score-1.411]
</p><p>59 Note that by using HDPs for both states and observations, we have implicitly integrated out the inﬁnitely many transition and emission parameters. [sent-134, score-0.794]
</p><p>60 Making an analogy with non-parametric models such as Gaussian Processes, we deﬁne a learned model as a set of counts and optimised hyperparameters . [sent-135, score-0.297]
</p><p>61 ¡F 8©F © 8© © B  )  ¡ '$© ¦00  " ©§$ £  )  ¡9 9 %Qc © c © 3b © b    ¦¡F 8 © F © 8 © © B   )  )  We ﬁrst describe an approximate Gibbs sampling procedure for inferring the posterior over the hidden state sequence. [sent-136, score-0.473]
</p><p>62 5 $¦ ¡    - Gibbs sample given hyperparameter settings, count matrices, and observations. [sent-142, score-0.226]
</p><p>63 - Update count matrices to reﬂect new ; this may change , the number of represented hidden states. [sent-143, score-0.472]
</p><p>64 1 Gibbs sampling the hidden state sequence  c 9 cb 9 b  ¦$ c  b  Deﬁne and as the results of removing from and the transition and emission counts contributed by . [sent-150, score-1.276]
</p><p>65 Deﬁne similar items and related to the transition and emission  £ ¤   U G  ¡ F ©0PBI0 '©! [sent-151, score-0.621]
</p><p>66 An exact Gibbs sweep of the hidden state from takes operations, since under the HDP generative process changing affects the probability of all subsequent hidden state transitions and emissions. [sent-154, score-1.015]
</p><p>67 3 However this computation can be reasonably approximated in , by basing the Gibbs update for only on the state of its neigbours and the total counts . [sent-155, score-0.282]
</p><p>68 2 Hyperparameter optimisation  ¦¡A8 © F © 8 © © B   F  We place vague Gamma priors5 on the hyperparameters . [sent-158, score-0.183]
</p><p>69 I  ) ' 0(  where is the number of represented states that are transitioned to from state (includis the number of possible emissions from state . [sent-162, score-0.759]
</p><p>70 and ing itself); similarly are the number of times the oracle has been used for the transition and emission processes, calculated from the indicator variables . [sent-163, score-0.884]
</p><p>71 3 Inﬁnite-state particle ﬁlter The likelihood for a particular observable sequence of symbols involves intractable sums over the possible hidden state trajectories. [sent-165, score-0.669]
</p><p>72 In particular, in the DP, making the transition makes that transition more likely later on in the sequence, so we cannot use standard tricks like dynamic programming. [sent-167, score-0.464]
</p><p>73 Furthermore, the number of distinct states can grow with the sequence length as new states are generated. [sent-168, score-0.53]
</p><p>74 If the chain starts with distinct states, at time there could be possible distinct states making the total number of trajectories over the entire length of the sequence . [sent-169, score-0.49]
</p><p>75 '  6  4 ¤ '  ' G U ¤ 'G 6 7  A C@     4  3 Although the hidden states in an HMM satisfy the Markov condition, integrating out the parameters induces these long-range dependencies. [sent-170, score-0.42]
</p><p>76 Consider sampling parameters from the posterior distribution of parameter matrices, which will depend on the count matrices. [sent-172, score-0.159]
</p><p>77 8    ¦P © IH§  §  W  8  V    G E 7F D CA¤9 B 8@  ©  r p i IqH F H  a S CgRfec¡` YX R@ (¤S h G V@ d b a W G W V U T  ¡ A   © RQ §  We propose estimating the likelihood of a test sequence given a learned model using particle ﬁltering. [sent-175, score-0.211]
</p><p>78 The idea is to start with some number of particles distributed on the represented hidden states according to the ﬁnal state marginal from the training sequence (some of the may fall onto new states). [sent-176, score-0.801]
</p><p>79 Update transition and emission tables , for each particle. [sent-180, score-0.621]
</p><p>80 Since it is a discrete state space, with much of the probability mass concentrated on the represented states, it is feasible to use particles. [sent-194, score-0.258]
</p><p>81 8 6 5 7"C  U 'G  9  5 Synthetic experiments Exp 1: Discovering the number of hidden states We applied the inﬁnite HMM inference algorithm to the ascending-descending observation sequence consisting of 30 con. [sent-195, score-0.545]
</p><p>82 The most parsimonious HMM which models this catenated copies of data perfectly has exactly 10 hidden states. [sent-196, score-0.247]
</p><p>83 The inﬁnite HMM was initialised with a random hidden state sequence, containing distinct represented states. [sent-197, score-0.581]
</p><p>84 In Figure 2 (right) we show how the number of represented states evolves with successive Gibbs sweeps, starting from a variety of initial . [sent-198, score-0.231]
</p><p>85 '  £  '  A C E G I G E C A FSRQPHFDB@  '     Exp 2: Expansive A sequence of length was generated from a 4-state 8-symbol HMM with the transition and emission probabilities as shown in Figure 3 (top left). [sent-200, score-0.769]
</p><p>86 ( ( U T  £     was generated from a 4-state 3-symbol Exp 3: Compressive A sequence of length HMM with the transition and emission probabilities as shown in Figure 3 (bottom left). [sent-201, score-0.769]
</p><p>87 ( ( V T  In both Exp 2 and Exp 3 the inﬁnite HMM was initialised with a hidden state sequence with distinct states. [sent-202, score-0.619]
</p><p>88 Figure 3 shows that, over successive Gibbs sweeps and hyperparameter learning, the count matrices for the inﬁnite HMM converge to resemble the true probability matrices as shown on the far left. [sent-203, score-0.481]
</p><p>89 The HDP implicity integrates out the transition and emission parameters of the HMM. [sent-206, score-0.651]
</p><p>90 An advantage of this is that it is no longer necessary to constrain the HMM to have ﬁnitely many states and observation symbols. [sent-207, score-0.173]
</p><p>91 The prior over hidden state transitions deﬁned by the HDP is capable of producing a wealth of interesting trajectories by varying the three hyperparameters that control it. [sent-208, score-0.867]
</p><p>92 We have presented the necessary tools for using the inﬁnite HMM, namely a linear-time approximate Gibbs sampler for inference, equations for hyperparameter learning, and a particle ﬁlter for likelihood evaluation. [sent-209, score-0.27]
</p><p>93 6 Different particle initialisations apply if we do not assume that the test sequence immediately follows the training sequence. [sent-210, score-0.183]
</p><p>94 True transition and emission probability matrices used for Exp 2    £ ¤¥  H  £ ¤¥   ! [sent-211, score-0.687]
</p><p>95 ¡   True transition and emission probability matrices used for Exp 3   ¤©! [sent-215, score-0.687]
</p><p>96 ¡   H       Figure 3:  The far left pair of Hinton diagrams represent the true transition and emission probabilities used to generate the data for each experiment 2 and 3 (up to a permutation of the hidden states; lighter boxes correspond to higher values). [sent-219, score-0.893]
</p><p>97 Similar to top row displaying count matrices after Gibbs sampling. [sent-223, score-0.208]
</p><p>98 A%¡ '  &  On synthetic data we have shown that the inﬁnite HMM discovers both the appropriate number of states required to model the data and the structure of the emission and transition matrices. [sent-226, score-0.821]
</p><p>99 It is important to emphasise that although the count matrices found by the inﬁnite HMM resemble point estimates of HMM parameters (e. [sent-227, score-0.197]
</p><p>100 We believe that for many problems the inﬁnite HMM’s ﬂexibile nature and its ability to automatically determine the required number of hidden states make it superior to the conventional treatment of HMMs with its associated difﬁcult model selection problem. [sent-230, score-0.42]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('emission', 0.389), ('hidden', 0.247), ('pbi', 0.235), ('transition', 0.232), ('hdp', 0.222), ('oracle', 0.205), ('hmm', 0.185), ('hyperparameters', 0.183), ('nite', 0.179), ('dp', 0.175), ('states', 0.173), ('hf', 0.169), ('state', 0.168), ('gibbs', 0.163), ('dirichlet', 0.151), ('transitions', 0.127), ('hyperparameter', 0.125), ('counts', 0.114), ('transitioned', 0.107), ('count', 0.101), ('sequence', 0.096), ('sweeps', 0.093), ('particle', 0.087), ('dps', 0.085), ('emissions', 0.085), ('proportions', 0.081), ('nitely', 0.081), ('trajectories', 0.072), ('hmms', 0.068), ('matrices', 0.066), ('miq', 0.064), ('unrepresented', 0.064), ('nij', 0.063), ('mixing', 0.061), ('distinct', 0.061), ('qc', 0.059), ('particles', 0.059), ('indicator', 0.058), ('represented', 0.058), ('indicators', 0.056), ('transitioning', 0.056), ('exp', 0.055), ('tendency', 0.054), ('integrate', 0.052), ('emitted', 0.051), ('hierarchical', 0.049), ('markov', 0.049), ('initialised', 0.047), ('sa', 0.047), ('symbols', 0.043), ('bbg', 0.043), ('compressive', 0.043), ('countably', 0.043), ('expansive', 0.043), ('hdps', 0.043), ('jo', 0.043), ('linger', 0.043), ('mqo', 0.043), ('occurence', 0.043), ('existing', 0.043), ('row', 0.041), ('symbol', 0.041), ('map', 0.039), ('modelling', 0.039), ('nonparametric', 0.037), ('alongside', 0.037), ('word', 0.037), ('prior', 0.036), ('mechanism', 0.035), ('sequences', 0.035), ('processes', 0.035), ('de', 0.035), ('wealth', 0.034), ('goto', 0.034), ('alice', 0.034), ('mass', 0.032), ('vu', 0.031), ('dynamics', 0.031), ('sampling', 0.03), ('alphabet', 0.03), ('uf', 0.03), ('resemble', 0.03), ('integrates', 0.03), ('sampler', 0.03), ('sweep', 0.03), ('inference', 0.029), ('likelihood', 0.028), ('au', 0.028), ('hh', 0.028), ('posterior', 0.028), ('bayesian', 0.028), ('process', 0.028), ('ne', 0.027), ('self', 0.027), ('leaving', 0.027), ('synthetic', 0.027), ('horizontal', 0.027), ('length', 0.027), ('explore', 0.026), ('probabilities', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="183-tfidf-1" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>Author: Matthew J. Beal, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We show that it is possible to extend hidden Markov models to have a countably inﬁnite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the inﬁnitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters deﬁne a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a ﬁnite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be inﬁnite— consider, for example, symbols being possible words appearing in English text.</p><p>2 0.19998382 <a title="183-tfidf-2" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>Author: Andrew D. Brown, Geoffrey E. Hinton</p><p>Abstract: Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's. 1</p><p>3 0.18770547 <a title="183-tfidf-3" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<p>Author: Kevin P. Murphy, Mark A. Paskin</p><p>Abstract: The hierarchical hidden Markov model (HHMM) is a generalization of the hidden Markov model (HMM) that models sequences with structure at many length/time scales [FST98]. Unfortunately, the original infertime, where is ence algorithm is rather complicated, and takes the length of the sequence, making it impractical for many domains. In this paper, we show how HHMMs are a special kind of dynamic Bayesian network (DBN), and thereby derive a much simpler inference algorithm, which only takes time. Furthermore, by drawing the connection between HHMMs and DBNs, we enable the application of many standard approximation techniques to further speed up inference. ¥ ©§ £ ¨¦¥¤¢ © £ ¦¥¤¢</p><p>4 0.17367113 <a title="183-tfidf-4" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>Author: Carl E. Rasmussen, Zoubin Ghahramani</p><p>Abstract: We present an extension to the Mixture of Experts (ME) model, where the individual experts are Gaussian Process (GP) regression models. Using an input-dependent adaptation of the Dirichlet Process, we implement a gating network for an inﬁnite number of Experts. Inference in this model may be done efﬁciently using a Markov Chain relying on Gibbs sampling. The model allows the effective covariance function to vary with the inputs, and may handle large datasets – thus potentially overcoming two of the biggest hurdles with GP models. Simulations show the viability of this approach.</p><p>5 0.15708108 <a title="183-tfidf-5" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>Author: Peter Sykacek, Stephen J. Roberts</p><p>Abstract: This paper proposes an approach to classiﬁcation of adjacent segments of a time series as being either of classes. We use a hierarchical model that consists of a feature extraction stage and a generative classiﬁer which is built on top of these features. Such two stage approaches are often used in signal and image processing. The novel part of our work is that we link these stages probabilistically by using a latent feature space. To use one joint model is a Bayesian requirement, which has the advantage to fuse information according to its certainty. The classiﬁer is implemented as hidden Markov model with Gaussian and Multinomial observation distributions deﬁned on a suitably chosen representation of autoregressive models. The Markov dependency is motivated by the assumption that successive classiﬁcations will be correlated. Inference is done with Markov chain Monte Carlo (MCMC) techniques. We apply the proposed approach to synthetic data and to classiﬁcation of EEG that was recorded while the subjects performed different cognitive tasks. All experiments show that using a latent feature space results in a signiﬁcant improvement in generalization accuracy. Hence we expect that this idea generalizes well to other hierarchical models.</p><p>6 0.14145924 <a title="183-tfidf-6" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>7 0.12886883 <a title="183-tfidf-7" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>8 0.11896734 <a title="183-tfidf-8" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>9 0.11511179 <a title="183-tfidf-9" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>10 0.10163124 <a title="183-tfidf-10" href="./nips-2001-Analysis_of_Sparse_Bayesian_Learning.html">35 nips-2001-Analysis of Sparse Bayesian Learning</a></p>
<p>11 0.10066237 <a title="183-tfidf-11" href="./nips-2001-Audio-Visual_Sound_Separation_Via_Hidden_Markov_Models.html">39 nips-2001-Audio-Visual Sound Separation Via Hidden Markov Models</a></p>
<p>12 0.084706292 <a title="183-tfidf-12" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>13 0.084241971 <a title="183-tfidf-13" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>14 0.081149928 <a title="183-tfidf-14" href="./nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">102 nips-2001-KLD-Sampling: Adaptive Particle Filters</a></p>
<p>15 0.081046738 <a title="183-tfidf-15" href="./nips-2001-Distribution_of_Mutual_Information.html">61 nips-2001-Distribution of Mutual Information</a></p>
<p>16 0.075152799 <a title="183-tfidf-16" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>17 0.073777974 <a title="183-tfidf-17" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>18 0.072903253 <a title="183-tfidf-18" href="./nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables.html">153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</a></p>
<p>19 0.071912259 <a title="183-tfidf-19" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>20 0.069561593 <a title="183-tfidf-20" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.216), (1, -0.055), (2, 0.087), (3, -0.115), (4, -0.263), (5, -0.03), (6, 0.183), (7, 0.073), (8, -0.061), (9, -0.004), (10, 0.01), (11, 0.042), (12, 0.001), (13, -0.014), (14, -0.021), (15, -0.164), (16, 0.012), (17, -0.009), (18, 0.255), (19, -0.016), (20, 0.138), (21, 0.065), (22, -0.025), (23, -0.025), (24, 0.013), (25, 0.124), (26, -0.199), (27, -0.055), (28, -0.046), (29, 0.016), (30, 0.016), (31, -0.005), (32, 0.052), (33, 0.098), (34, -0.085), (35, -0.077), (36, -0.07), (37, -0.063), (38, 0.145), (39, 0.111), (40, 0.035), (41, -0.032), (42, 0.073), (43, -0.046), (44, -0.016), (45, -0.014), (46, 0.003), (47, -0.001), (48, -0.03), (49, -0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97343302 <a title="183-lsi-1" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>Author: Matthew J. Beal, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We show that it is possible to extend hidden Markov models to have a countably inﬁnite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the inﬁnitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters deﬁne a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a ﬁnite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be inﬁnite— consider, for example, symbols being possible words appearing in English text.</p><p>2 0.81058365 <a title="183-lsi-2" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<p>Author: Kevin P. Murphy, Mark A. Paskin</p><p>Abstract: The hierarchical hidden Markov model (HHMM) is a generalization of the hidden Markov model (HMM) that models sequences with structure at many length/time scales [FST98]. Unfortunately, the original infertime, where is ence algorithm is rather complicated, and takes the length of the sequence, making it impractical for many domains. In this paper, we show how HHMMs are a special kind of dynamic Bayesian network (DBN), and thereby derive a much simpler inference algorithm, which only takes time. Furthermore, by drawing the connection between HHMMs and DBNs, we enable the application of many standard approximation techniques to further speed up inference. ¥ ©§ £ ¨¦¥¤¢ © £ ¦¥¤¢</p><p>3 0.69778639 <a title="183-lsi-3" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>Author: Andrew D. Brown, Geoffrey E. Hinton</p><p>Abstract: Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's. 1</p><p>4 0.62593251 <a title="183-lsi-4" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>Author: Jens Kohlmorgen, Steven Lemm</p><p>Abstract: We propose a novel method for the analysis of sequential data that exhibits an inherent mode switching. In particular, the data might be a non-stationary time series from a dynamical system that switches between multiple operating modes. Unlike other approaches, our method processes the data incrementally and without any training of internal parameters. We use an HMM with a dynamically changing number of states and an on-line variant of the Viterbi algorithm that performs an unsupervised segmentation and classification of the data on-the-fly, i.e. the method is able to process incoming data in real-time. The main idea of the approach is to track and segment changes of the probability density of the data in a sliding window on the incoming data stream. The usefulness of the algorithm is demonstrated by an application to a switching dynamical system. 1</p><p>5 0.5886901 <a title="183-lsi-5" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>Author: Peter Sykacek, Stephen J. Roberts</p><p>Abstract: This paper proposes an approach to classiﬁcation of adjacent segments of a time series as being either of classes. We use a hierarchical model that consists of a feature extraction stage and a generative classiﬁer which is built on top of these features. Such two stage approaches are often used in signal and image processing. The novel part of our work is that we link these stages probabilistically by using a latent feature space. To use one joint model is a Bayesian requirement, which has the advantage to fuse information according to its certainty. The classiﬁer is implemented as hidden Markov model with Gaussian and Multinomial observation distributions deﬁned on a suitably chosen representation of autoregressive models. The Markov dependency is motivated by the assumption that successive classiﬁcations will be correlated. Inference is done with Markov chain Monte Carlo (MCMC) techniques. We apply the proposed approach to synthetic data and to classiﬁcation of EEG that was recorded while the subjects performed different cognitive tasks. All experiments show that using a latent feature space results in a signiﬁcant improvement in generalization accuracy. Hence we expect that this idea generalizes well to other hierarchical models.</p><p>6 0.58580548 <a title="183-lsi-6" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>7 0.58481759 <a title="183-lsi-7" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>8 0.51499957 <a title="183-lsi-8" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>9 0.48270813 <a title="183-lsi-9" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>10 0.42717171 <a title="183-lsi-10" href="./nips-2001-Predictive_Representations_of_State.html">148 nips-2001-Predictive Representations of State</a></p>
<p>11 0.3764115 <a title="183-lsi-11" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>12 0.35916361 <a title="183-lsi-12" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>13 0.34893665 <a title="183-lsi-13" href="./nips-2001-Tempo_tracking_and_rhythm_quantization_by_sequential_Monte_Carlo.html">179 nips-2001-Tempo tracking and rhythm quantization by sequential Monte Carlo</a></p>
<p>14 0.34616986 <a title="183-lsi-14" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>15 0.34466752 <a title="183-lsi-15" href="./nips-2001-Using_Vocabulary_Knowledge_in_Bayesian_Multinomial_Estimation.html">194 nips-2001-Using Vocabulary Knowledge in Bayesian Multinomial Estimation</a></p>
<p>16 0.34342393 <a title="183-lsi-16" href="./nips-2001-Entropy_and_Inference%2C_Revisited.html">68 nips-2001-Entropy and Inference, Revisited</a></p>
<p>17 0.34187078 <a title="183-lsi-17" href="./nips-2001-Distribution_of_Mutual_Information.html">61 nips-2001-Distribution of Mutual Information</a></p>
<p>18 0.33691692 <a title="183-lsi-18" href="./nips-2001-Grammar_Transfer_in_a_Second_Order_Recurrent_Neural_Network.html">85 nips-2001-Grammar Transfer in a Second Order Recurrent Neural Network</a></p>
<p>19 0.31617042 <a title="183-lsi-19" href="./nips-2001-Geometrical_Singularities_in_the_Neuromanifold_of_Multilayer_Perceptrons.html">83 nips-2001-Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons</a></p>
<p>20 0.30526516 <a title="183-lsi-20" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.027), (17, 0.017), (19, 0.014), (27, 0.098), (30, 0.082), (38, 0.02), (51, 0.214), (59, 0.054), (72, 0.069), (79, 0.124), (83, 0.04), (91, 0.154)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8484748 <a title="183-lda-1" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>Author: Matthew J. Beal, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We show that it is possible to extend hidden Markov models to have a countably inﬁnite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the inﬁnitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters deﬁne a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a ﬁnite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be inﬁnite— consider, for example, symbols being possible words appearing in English text.</p><p>2 0.72145188 <a title="183-lda-2" href="./nips-2001-Fragment_Completion_in_Humans_and_Machines.html">78 nips-2001-Fragment Completion in Humans and Machines</a></p>
<p>Author: David Jacobs, Bas Rokers, Archisman Rudra, Zili Liu</p><p>Abstract: Partial information can trigger a complete memory. At the same time, human memory is not perfect. A cue can contain enough information to specify an item in memory, but fail to trigger that item. In the context of word memory, we present experiments that demonstrate some basic patterns in human memory errors. We use cues that consist of word fragments. We show that short and long cues are completed more accurately than medium length ones and study some of the factors that lead to this behavior. We then present a novel computational model that shows some of the ﬂexibility and patterns of errors that occur in human memory. This model iterates between bottom-up and top-down computations. These are tied together using a Markov model of words that allows memory to be accessed with a simple feature set, and enables a bottom-up process to compute a probability distribution of possible completions of word fragments, in a manner similar to models of visual perceptual completion.</p><p>3 0.71697676 <a title="183-lda-3" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<p>Author: Kevin P. Murphy, Mark A. Paskin</p><p>Abstract: The hierarchical hidden Markov model (HHMM) is a generalization of the hidden Markov model (HMM) that models sequences with structure at many length/time scales [FST98]. Unfortunately, the original infertime, where is ence algorithm is rather complicated, and takes the length of the sequence, making it impractical for many domains. In this paper, we show how HHMMs are a special kind of dynamic Bayesian network (DBN), and thereby derive a much simpler inference algorithm, which only takes time. Furthermore, by drawing the connection between HHMMs and DBNs, we enable the application of many standard approximation techniques to further speed up inference. ¥ ©§ £ ¨¦¥¤¢ © £ ¦¥¤¢</p><p>4 0.70331073 <a title="183-lda-4" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>Author: Andrew D. Brown, Geoffrey E. Hinton</p><p>Abstract: Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's. 1</p><p>5 0.70107329 <a title="183-lda-5" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>Author: Carl E. Rasmussen, Zoubin Ghahramani</p><p>Abstract: We present an extension to the Mixture of Experts (ME) model, where the individual experts are Gaussian Process (GP) regression models. Using an input-dependent adaptation of the Dirichlet Process, we implement a gating network for an inﬁnite number of Experts. Inference in this model may be done efﬁciently using a Markov Chain relying on Gibbs sampling. The model allows the effective covariance function to vary with the inputs, and may handle large datasets – thus potentially overcoming two of the biggest hurdles with GP models. Simulations show the viability of this approach.</p><p>6 0.69616741 <a title="183-lda-6" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>7 0.69364309 <a title="183-lda-7" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>8 0.69353724 <a title="183-lda-8" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>9 0.69340014 <a title="183-lda-9" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>10 0.69328451 <a title="183-lda-10" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>11 0.69258392 <a title="183-lda-11" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>12 0.69143844 <a title="183-lda-12" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>13 0.68994379 <a title="183-lda-13" href="./nips-2001-The_Concave-Convex_Procedure_%28CCCP%29.html">180 nips-2001-The Concave-Convex Procedure (CCCP)</a></p>
<p>14 0.68673968 <a title="183-lda-14" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>15 0.68664217 <a title="183-lda-15" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>16 0.68577534 <a title="183-lda-16" href="./nips-2001-3_state_neurons_for_contextual_processing.html">2 nips-2001-3 state neurons for contextual processing</a></p>
<p>17 0.68479311 <a title="183-lda-17" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>18 0.6832754 <a title="183-lda-18" href="./nips-2001-Using_Vocabulary_Knowledge_in_Bayesian_Multinomial_Estimation.html">194 nips-2001-Using Vocabulary Knowledge in Bayesian Multinomial Estimation</a></p>
<p>19 0.68276823 <a title="183-lda-19" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>20 0.68271899 <a title="183-lda-20" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
