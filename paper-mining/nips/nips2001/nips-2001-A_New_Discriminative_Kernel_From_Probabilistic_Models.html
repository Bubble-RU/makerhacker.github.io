<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>15 nips-2001-A New Discriminative Kernel From Probabilistic Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-15" href="#">nips2001-15</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>15 nips-2001-A New Discriminative Kernel From Probabilistic Models</h1>
<br/><p>Source: <a title="nips-2001-15-pdf" href="http://papers.nips.cc/paper/2014-a-new-discriminative-kernel-from-probabilistic-models.pdf">pdf</a></p><p>Author: Koji Tsuda, Motoaki Kawanabe, Gunnar Rätsch, Sören Sonnenburg, Klaus-Robert Müller</p><p>Abstract: Recently, Jaakkola and Haussler proposed a method for constructing kernel functions from probabilistic models. Their so called</p><p>Reference: <a title="nips-2001-15-reference" href="../nips2001_reference/nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fk', 0.379), ('kernel', 0.372), ('fish', 0.334), ('protein', 0.267), ('fo', 0.252), ('top', 0.247), ('fij', 0.215), ('oc', 0.168), ('re', 0.153), ('feat', 0.141), ('wx', 0.133), ('class', 0.112), ('extract', 0.095), ('discrim', 0.091), ('ish', 0.091), ('rd', 0.08), ('famy', 0.079), ('pseudo', 0.079), ('vve', 0.079), ('ij', 0.075)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="15-tfidf-1" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>Author: Koji Tsuda, Motoaki Kawanabe, Gunnar Rätsch, Sören Sonnenburg, Klaus-Robert Müller</p><p>Abstract: Recently, Jaakkola and Haussler proposed a method for constructing kernel functions from probabilistic models. Their so called</p><p>2 0.26334506 <a title="15-tfidf-2" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>Author: Matthias Seeger</p><p>Abstract: We propose the framework of mutual information kernels for learning covariance kernels, as used in Support Vector machines and Gaussian process classifiers, from unlabeled task data using Bayesian techniques. We describe an implementation of this framework which uses variational Bayesian mixtures of factor analyzers in order to attack classification problems in high-dimensional spaces where labeled data is sparse, but unlabeled data is abundant. 1</p><p>3 0.23842885 <a title="15-tfidf-3" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>Author: Polina Golland</p><p>Abstract: In many scientiﬁc and engineering applications, detecting and understanding differences between two groups of examples can be reduced to a classical problem of training a classiﬁer for labeling new examples while making as few mistakes as possible. In the traditional classiﬁcation setting, the resulting classiﬁer is rarely analyzed in terms of the properties of the input data captured by the discriminative model. However, such analysis is crucial if we want to understand and visualize the detected differences. We propose an approach to interpretation of the statistical model in the original feature space that allows us to argue about the model in terms of the relevant changes to the input vectors. For each point in the input space, we deﬁne a discriminative direction to be the direction that moves the point towards the other class while introducing as little irrelevant change as possible with respect to the classiﬁer function. We derive the discriminative direction for kernel-based classiﬁers, demonstrate the technique on several examples and brieﬂy discuss its use in the statistical shape analysis, an application that originally motivated this work.</p><p>4 0.23555741 <a title="15-tfidf-4" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>Author: Dimitris Achlioptas, Frank Mcsherry, Bernhard Schölkopf</p><p>Abstract: We propose randomized techniques for speeding up Kernel Principal Component Analysis on three levels: sampling and quantization of the Gram matrix in training, randomized rounding in evaluating the kernel expansions, and random projections in evaluating the kernel itself. In all three cases, we give sharp bounds on the accuracy of the obtained approximations. Rather intriguingly, all three techniques can be viewed as instantiations of the following idea: replace the kernel function by a “randomized kernel” which behaves like in expectation.</p><p>5 0.22659574 <a title="15-tfidf-5" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>Author: Manfred Opper, Robert Urbanczik</p><p>Abstract: Using methods of Statistical Physics, we investigate the rOle of model complexity in learning with support vector machines (SVMs). We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error. Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel. 1</p><p>6 0.22521392 <a title="15-tfidf-6" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>7 0.18461369 <a title="15-tfidf-7" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>8 0.18247107 <a title="15-tfidf-8" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>9 0.17338203 <a title="15-tfidf-9" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>10 0.16655526 <a title="15-tfidf-10" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>11 0.14377241 <a title="15-tfidf-11" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>12 0.12698998 <a title="15-tfidf-12" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>13 0.11990844 <a title="15-tfidf-13" href="./nips-2001-A_Sequence_Kernel_and_its_Application_to_Speaker_Recognition.html">20 nips-2001-A Sequence Kernel and its Application to Speaker Recognition</a></p>
<p>14 0.11943019 <a title="15-tfidf-14" href="./nips-2001-Kernel_Feature_Spaces_and_Nonlinear_Blind_Souce_Separation.html">103 nips-2001-Kernel Feature Spaces and Nonlinear Blind Souce Separation</a></p>
<p>15 0.11930287 <a title="15-tfidf-15" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>16 0.11070018 <a title="15-tfidf-16" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>17 0.10836566 <a title="15-tfidf-17" href="./nips-2001-A_kernel_method_for_multi-labelled_classification.html">22 nips-2001-A kernel method for multi-labelled classification</a></p>
<p>18 0.10335086 <a title="15-tfidf-18" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>19 0.10242347 <a title="15-tfidf-19" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>20 0.10070906 <a title="15-tfidf-20" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.307), (1, 0.141), (2, -0.164), (3, 0.01), (4, -0.221), (5, -0.153), (6, -0.01), (7, 0.024), (8, -0.099), (9, -0.069), (10, -0.036), (11, -0.027), (12, -0.024), (13, 0.014), (14, 0.066), (15, 0.045), (16, 0.115), (17, -0.186), (18, 0.03), (19, 0.022), (20, 0.003), (21, 0.027), (22, 0.053), (23, 0.035), (24, 0.133), (25, 0.115), (26, 0.028), (27, 0.05), (28, 0.036), (29, 0.09), (30, -0.008), (31, -0.029), (32, 0.109), (33, 0.034), (34, 0.04), (35, -0.042), (36, 0.013), (37, -0.05), (38, 0.045), (39, -0.043), (40, -0.028), (41, 0.0), (42, -0.038), (43, -0.009), (44, 0.048), (45, 0.011), (46, -0.026), (47, 0.034), (48, -0.062), (49, 0.087)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95484632 <a title="15-lsi-1" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>Author: Koji Tsuda, Motoaki Kawanabe, Gunnar Rätsch, Sören Sonnenburg, Klaus-Robert Müller</p><p>Abstract: Recently, Jaakkola and Haussler proposed a method for constructing kernel functions from probabilistic models. Their so called</p><p>2 0.77934456 <a title="15-lsi-2" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>Author: Matthias Seeger</p><p>Abstract: We propose the framework of mutual information kernels for learning covariance kernels, as used in Support Vector machines and Gaussian process classifiers, from unlabeled task data using Bayesian techniques. We describe an implementation of this framework which uses variational Bayesian mixtures of factor analyzers in order to attack classification problems in high-dimensional spaces where labeled data is sparse, but unlabeled data is abundant. 1</p><p>3 0.7718755 <a title="15-lsi-3" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>Author: Polina Golland</p><p>Abstract: In many scientiﬁc and engineering applications, detecting and understanding differences between two groups of examples can be reduced to a classical problem of training a classiﬁer for labeling new examples while making as few mistakes as possible. In the traditional classiﬁcation setting, the resulting classiﬁer is rarely analyzed in terms of the properties of the input data captured by the discriminative model. However, such analysis is crucial if we want to understand and visualize the detected differences. We propose an approach to interpretation of the statistical model in the original feature space that allows us to argue about the model in terms of the relevant changes to the input vectors. For each point in the input space, we deﬁne a discriminative direction to be the direction that moves the point towards the other class while introducing as little irrelevant change as possible with respect to the classiﬁer function. We derive the discriminative direction for kernel-based classiﬁers, demonstrate the technique on several examples and brieﬂy discuss its use in the statistical shape analysis, an application that originally motivated this work.</p><p>4 0.7361595 <a title="15-lsi-4" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>Author: Manfred Opper, Robert Urbanczik</p><p>Abstract: Using methods of Statistical Physics, we investigate the rOle of model complexity in learning with support vector machines (SVMs). We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error. Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel. 1</p><p>5 0.71032172 <a title="15-lsi-5" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>Author: Dimitris Achlioptas, Frank Mcsherry, Bernhard Schölkopf</p><p>Abstract: We propose randomized techniques for speeding up Kernel Principal Component Analysis on three levels: sampling and quantization of the Gram matrix in training, randomized rounding in evaluating the kernel expansions, and random projections in evaluating the kernel itself. In all three cases, we give sharp bounds on the accuracy of the obtained approximations. Rather intriguingly, all three techniques can be viewed as instantiations of the following idea: replace the kernel function by a “randomized kernel” which behaves like in expectation.</p><p>6 0.7023862 <a title="15-lsi-6" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>7 0.68118197 <a title="15-lsi-7" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>8 0.67973977 <a title="15-lsi-8" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>9 0.63133246 <a title="15-lsi-9" href="./nips-2001-Learning_a_Gaussian_Process_Prior_for_Automatically_Generating_Music_Playlists.html">113 nips-2001-Learning a Gaussian Process Prior for Automatically Generating Music Playlists</a></p>
<p>10 0.62685704 <a title="15-lsi-10" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>11 0.61126184 <a title="15-lsi-11" href="./nips-2001-A_Sequence_Kernel_and_its_Application_to_Speaker_Recognition.html">20 nips-2001-A Sequence Kernel and its Application to Speaker Recognition</a></p>
<p>12 0.59884834 <a title="15-lsi-12" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>13 0.58324695 <a title="15-lsi-13" href="./nips-2001-Learning_Discriminative_Feature_Transforms_to_Low_Dimensions_in_Low_Dimentions.html">109 nips-2001-Learning Discriminative Feature Transforms to Low Dimensions in Low Dimentions</a></p>
<p>14 0.5799973 <a title="15-lsi-14" href="./nips-2001-Quantizing_Density_Estimators.html">155 nips-2001-Quantizing Density Estimators</a></p>
<p>15 0.54686338 <a title="15-lsi-15" href="./nips-2001-Kernel_Feature_Spaces_and_Nonlinear_Blind_Souce_Separation.html">103 nips-2001-Kernel Feature Spaces and Nonlinear Blind Souce Separation</a></p>
<p>16 0.52821487 <a title="15-lsi-16" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>17 0.52483243 <a title="15-lsi-17" href="./nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes.html">133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</a></p>
<p>18 0.51253855 <a title="15-lsi-18" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>19 0.51248407 <a title="15-lsi-19" href="./nips-2001-Minimax_Probability_Machine.html">120 nips-2001-Minimax Probability Machine</a></p>
<p>20 0.51166368 <a title="15-lsi-20" href="./nips-2001-Kernel_Logistic_Regression_and_the_Import_Vector_Machine.html">104 nips-2001-Kernel Logistic Regression and the Import Vector Machine</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(9, 0.269), (14, 0.029), (16, 0.241), (26, 0.01), (31, 0.062), (36, 0.013), (50, 0.061), (63, 0.079), (77, 0.028), (79, 0.014), (81, 0.021), (91, 0.013), (92, 0.069)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83697712 <a title="15-lda-1" href="./nips-2001-Bayesian_morphometry_of_hippocampal_cells_suggests_same-cell_somatodendritic_repulsion.html">42 nips-2001-Bayesian morphometry of hippocampal cells suggests same-cell somatodendritic repulsion</a></p>
<p>Author: Giorgio A. Ascoli, Alexei V. Samsonovich</p><p>Abstract: Visual inspection of neurons suggests that dendritic orientation may be determined both by internal constraints (e.g. membrane tension) and by external vector fields (e.g. neurotrophic gradients). For example, basal dendrites of pyramidal cells appear nicely fan-out. This regular orientation is hard to justify completely with a general tendency to grow straight, given the zigzags observed experimentally. Instead, dendrites could (A) favor a fixed (“external”) direction, or (B) repel from their own soma. To investigate these possibilities quantitatively, reconstructed hippocampal cells were subjected to Bayesian analysis. The statistical model combined linearly factors A and B, as well as the tendency to grow straight. For all morphological classes, B was found to be significantly positive and consistently greater than A. In addition, when dendrites were artificially re-oriented according to this model, the resulting structures closely resembled real morphologies. These results suggest that somatodendritic repulsion may play a role in determining dendritic orientation. Since hippocampal cells are very densely packed and their dendritic trees highly overlap, the repulsion must be cellspecific. We discuss possible mechanisms underlying such specificity. 1 I n t r od uc t i on The study of brain dynamics and development at the cellular level would greatly benefit from a standardized, accurate and yet succinct statistical model characterizing the morphology of major neuronal classes. Such model could also provide a basis for simulation of anatomically realistic virtual neurons [1]. The model should accurately distinguish among different neuronal classes: a morphological difference between classes would be captured by a difference in model parameters and reproduced in generated virtual neurons. In addition, the model should be self-consistent: there should be no statistical difference in model parameters measured from real neurons of a given class and from virtual neurons of the same class. The assumption that a simple statistical model of this sort exists relies on the similarity of average environmental and homeostatic conditions encountered by individual neurons during development and on the limited amount of genetic information that underlies differentiation of neuronal classes. Previous research in computational neuroanatomy has mainly focused on the topology and internal geometry of dendrites (i.e., the properties described in “dendrograms”) [2,3]. Recently, we attempted to include spatial orientation in the models, thus generating 2 virtual neurons in 3D [4]. Dendritic growth was assumed to deviate from the straight direction both randomly and based on a constant bias in a given direction, or “tropism”. Different models of tropism (e.g. along a fixed axis, towards a plane, or away from the soma) had dramatic effects on the shape of virtual neurons [5]. Our current strategy is to split the problem of finding a statistical model describing neuronal morphology in two parts. First, we maintain that the topology and the internal geometry of a particular dendritic tree can be described independently of its 3D embedding (i.e., the set of local dendritic orientations). At the same time, one and the same internal geometry (e.g., the experimental dendrograms obtained from real neurons) may have many equally plausible 3D embeddings that are statistically consistent with the anatomical characteristics of that neuronal class. The present work aims at finding a minimal statistical model describing local dendritic orientation in experimentally reconstructed hippocampal principal cells. Hippocampal neurons have a polarized shape: their dendrites tend to grow from the soma as if enclosed in cones. In pyramidal cells, basal and apical dendrites invade opposite hemispaces (fig. 1A), while granule cell dendrites all invade the same hemispace. This behavior could be caused by a tendency to grow towards the layers of incoming fibers to establish synapses. Such tendency would correspond to a tropism in a direction roughly parallel to the cell main axis. Alternatively, dendrites could initially stem in the appropriate (possibly genetically determined) directions, and then continue to grow approximately in a radial direction from the soma. A close inspection of pyramidal (basal) trees suggests that dendrites may indeed be repelled from their soma (Fig. 1B). A typical dendrite may reorient itself (arrow) to grow nearly straight along a radius from the soma. Remarkably, this happens even after many turns, when the initial direction is lost. Such behavior may be hard to explain without tropism. If the deviations from straight growth were random, one should be able to “remodel”th e trees by measuring and reproducing the statistics of local turn angles, assuming its independence of dendritic orientation and location. Figure 1C shows the cell from 1A after such remodeling. In this case basal and apical dendrites retain only their initial (stemming) orientations from the original data. The resulting “cotton ball” uggests that dendritic turns are not in dependent s of dendritic orientation. In this paper, we use Bayesian analysis to quantify the dendritic tropism. 2 M e t h od s Digital files of fully reconstructed rat hippocampal pyramidal cells (24 CA3 and 23 CA1 neurons) were kindly provided by Dr. D. Amaral. The overall morphology of these cells, as well as the experimental acquisition methods, were extensively described [6]. In these files, dendrites are represented as (branching) chains of cylindrical sections. Each section is connected to one other section in the path to the soma, and may be connected on the other extremity to two other sections (bifurcation), one other section (continuation point), or no other section (terminal tip). Each section is described in the file by its ending point coordinates, its diameter and its</p><p>same-paper 2 0.79430562 <a title="15-lda-2" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>Author: Koji Tsuda, Motoaki Kawanabe, Gunnar Rätsch, Sören Sonnenburg, Klaus-Robert Müller</p><p>Abstract: Recently, Jaakkola and Haussler proposed a method for constructing kernel functions from probabilistic models. Their so called</p><p>3 0.70583177 <a title="15-lda-3" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>Author: N. Smith, Mark Gales</p><p>Abstract: An important issue in applying SVMs to speech recognition is the ability to classify variable length sequences. This paper presents extensions to a standard scheme for handling this variable length data, the Fisher score. A more useful mapping is introduced based on the likelihood-ratio. The score-space defined by this mapping avoids some limitations of the Fisher score. Class-conditional generative models are directly incorporated into the definition of the score-space. The mapping, and appropriate normalisation schemes, are evaluated on a speaker-independent isolated letter task where the new mapping outperforms both the Fisher score and HMMs trained to maximise likelihood. 1</p><p>4 0.70081943 <a title="15-lda-4" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>Author: T. Zhang</p><p>Abstract: We investigate the generalization performance of some learning problems in Hilbert functional Spaces. We introduce a notion of convergence of the estimated functional predictor to the best underlying predictor, and obtain an estimate on the rate of the convergence. This estimate allows us to derive generalization bounds on some learning formulations.</p><p>5 0.6949814 <a title="15-lda-5" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>6 0.69325942 <a title="15-lda-6" href="./nips-2001-Spectral_Relaxation_for_K-means_Clustering.html">171 nips-2001-Spectral Relaxation for K-means Clustering</a></p>
<p>7 0.69323272 <a title="15-lda-7" href="./nips-2001-Partially_labeled_classification_with_Markov_random_walks.html">144 nips-2001-Partially labeled classification with Markov random walks</a></p>
<p>8 0.69278902 <a title="15-lda-8" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>9 0.69255435 <a title="15-lda-9" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>10 0.69192588 <a title="15-lda-10" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>11 0.69058937 <a title="15-lda-11" href="./nips-2001-Means%2C_Correlations_and_Bounds.html">119 nips-2001-Means, Correlations and Bounds</a></p>
<p>12 0.6904695 <a title="15-lda-12" href="./nips-2001-Active_Learning_in_the_Drug_Discovery_Process.html">25 nips-2001-Active Learning in the Drug Discovery Process</a></p>
<p>13 0.69038218 <a title="15-lda-13" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>14 0.69013637 <a title="15-lda-14" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>15 0.68997437 <a title="15-lda-15" href="./nips-2001-Duality%2C_Geometry%2C_and_Support_Vector_Regression.html">62 nips-2001-Duality, Geometry, and Support Vector Regression</a></p>
<p>16 0.68899518 <a title="15-lda-16" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>17 0.68688565 <a title="15-lda-17" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>18 0.68654245 <a title="15-lda-18" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>19 0.68573433 <a title="15-lda-19" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>20 0.6855396 <a title="15-lda-20" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
