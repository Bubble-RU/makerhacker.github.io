<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>21 nips-2001-A Variational Approach to Learning Curves</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-21" href="#">nips2001-21</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>21 nips-2001-A Variational Approach to Learning Curves</h1>
<br/><p>Source: <a title="nips-2001-21-pdf" href="http://papers.nips.cc/paper/2090-a-variational-approach-to-learning-curves.pdf">pdf</a></p><p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We combine the replica approach from statistical physics with a variational approach to analyze learning curves analytically. We apply the method to Gaussian process regression. As a main result we derive approximative relations between empirical error measures, the generalization error and the posterior variance.</p><p>Reference: <a title="nips-2001-21-reference" href="../nips2001_reference/nips-2001-A_Variational_Approach_to_Learning_Curves_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract We combine the replica approach from statistical physics with a variational approach to analyze learning curves analytically. [sent-4, score-0.72]
</p><p>2 As a main result we derive approximative relations between empirical error measures, the generalization error and the posterior variance. [sent-6, score-0.575]
</p><p>3 1 Introduction Approximate expressions for generalization errors for ﬁnite dimensional statistical data models can be often obtained in the large data limit using asymptotic expansions. [sent-7, score-0.209]
</p><p>4 Such methods can yield approximate relations for empirical and true errors which can be used to assess the quality of the trained model see e. [sent-8, score-0.3]
</p><p>5 Unfortunately, such an approximation scheme does not seem to be easily applicable to popular non-parametric models like Gaussian process (GP) models and Support Vector Machines (SVMs). [sent-11, score-0.078]
</p><p>6 We apply the replica approach of statistical physics to asses the average case learning performance of these kernel machines. [sent-12, score-0.386]
</p><p>7 So far, the tools of statistical physics have been successfully applied to a variety of learning problems [2]. [sent-13, score-0.054]
</p><p>8 However, this elegant method suffers from the drawback that data averages can be performed exactly only under very idealistic assumptions on the data distribution in the ”thermodynamic” limit of inﬁnite data space dimension. [sent-14, score-0.352]
</p><p>9 We try to overcome these limitations by combining the replica method with a variational approximation. [sent-15, score-0.594]
</p><p>10 For Bayesian models, our method allows us to express useful data averaged a-posteriori expectations by means of an approximate measure. [sent-16, score-0.227]
</p><p>11 The derivation of this measure requires no assumptions about the data density and no assumptions about the input dimension. [sent-17, score-0.266]
</p><p>12 The main focus of this article is Gaussian process regression where we demonstrate the various strengths of the presented method. [sent-18, score-0.194]
</p><p>13 For Gaussian process models we show that our method does not only give explicit approximations for generalization errors but also of their sample ﬂuctuations. [sent-20, score-0.218]
</p><p>14 Furthermore, we show how to compute corrections to our theory and demonstrate the possibility of deriving approximate universal relations between average empirical and true errors which might be of practical interest. [sent-21, score-0.522]
</p><p>15 An earlier version of our approach, which was still restricted to the assumption of idealized data distributions appeared in [4]. [sent-22, score-0.056]
</p><p>16 In a Bayesian formulation, we have a prior distribution over this class of functions . [sent-25, score-0.066]
</p><p>17 Assuming that a set of observations is conditionally independent given inputs , we assign a likelihood term of the form to each observation. [sent-26, score-0.059]
</p><p>18 Posterior expectations (denoted by angular brackets) of any functional are expressed in the form  ¥ £¡ ¦¤¢   ¥¥ 210  ¥0 £¡  ¡ & $ # 1¦)¢(''%¡  § 2  §¨ ©§  ! [sent-27, score-0.063]
</p><p>19 " B $ %# D 6 4 3 D '  5FEC A @'  53 7 9 86 4 G  where the partition function normalizes the posterior and denotes the expectation with respect to the prior. [sent-29, score-0.416]
</p><p>20 We are interested in computing averages of posterior expectations over different drawings of training data sets were all data examples are independently generated from the same distribution. [sent-30, score-0.56]
</p><p>21 In the next section we will show how to derive a measure which enables us to compute analytically approximate combined data and posterior averages. [sent-31, score-0.328]
</p><p>22 3 A Grand-Canonical Approach We utilize the statistical mechanics approach to the analysis of learning. [sent-32, score-0.078]
</p><p>23 Our aim is to which serves as a generating compute the so-called averaged ”free energy” function for suitable data averages of posterior expectations. [sent-33, score-0.506]
</p><p>24 The partition function is  W dV  B    (2)    R Q0 ¥  u    y w u s r p i h a 9 W Fp  x FvtqIeggfV  B Ia U c  § 2    B Ib# ca  ¥0 £¡  ¡ 1F¤¢('&  U    U W fV B ea c ¨ H I0 ! [sent-34, score-0.301]
</p><p>25 "  C 9 B $ %# D G  we use the replica trick , To perform the average where is computed for integer and the continuation is performed at the end [5]. [sent-35, score-0.245]
</p><p>26 " ¥ £  ¡ F)¡  ('& G %# D $   qp C fV p B  9 ¤p B 9 W ¥ ¡ p  p C  where  (3)  denotes the expectation over the replicated prior measure. [sent-37, score-0.166]
</p><p>27 " g d R ¥ £   f eI¢tz¥ ¦¤¡  1¡l& G %# D $  # 9 p yxwu p  q o s ! [sent-40, score-0.038]
</p><p>28 " j rspH p C 9t¥b)¡ p B ¥ p uv2¡ # m n ¥ k G  9 l¤¡ p  j  with the Hamiltonian  replicas of the predictor at the same data point is taken with respect to the true data density . [sent-41, score-0.309]
</p><p>29 ¥  § 2  £ )¡    The density evaluates all and the expectation  (5)  ¥  § 2  £ ¤¡  The ”grand canonical” partition function represents a ”poissonized” version of the original model with ﬂuctuating number of examples. [sent-42, score-0.249]
</p><p>30 The ”chemical potential” determines the expected value of which yields simply for . [sent-43, score-0.045]
</p><p>31 (4) by its dominating term (6)  k      g    u u  wx p  ¦x wTu #    p ¦ wx u #   g   thereby neglecting relative ﬂuctuations. [sent-45, score-0.15]
</p><p>32 We recover the original (canonical) free energy as . [sent-46, score-0.354]
</p><p>33 4 Variational Approximation  ¥ k¡ l¤p  For most interesting cases, the partition function can not be computed in closed form for given . [sent-47, score-0.108]
</p><p>34 Hence, we use a variational approach to approximate by a different tractable Hamiltonian . [sent-48, score-0.411]
</p><p>35 Although differentiating the bound with respect to will usually not preserve the inequality, we still is a sensible thing to do [7]. [sent-52, score-0.06]
</p><p>36 expect 1 that an optimization with respect to ¡  ¥ k¡ l¤p  ca eb#  j  sp u       } { FYo    4. [sent-53, score-0.265]
</p><p>37 (5) can be rewritten as an integral over a local quantity in the input variable , i. [sent-55, score-0.111]
</p><p>38 " ¦§  §  ¥ £  ¡ ¦¤¡  1l& G %# D $ ¥ ¦£ ¡       9 `F)¡    4 ¤s ¥6¥ £ £¡  We will now specialize to Gaussian priors over , for which a local quadratic expression  ¥ £  ¥ £ F¤¡  2¦¤¡   (9)  ¥ £ F¤¡    s8  %    # $  7  ! [sent-58, score-0.046]
</p><p>39  "¢  ¥ £    ¥ £    ¥ £ ¨ G lF)¡ 2¦¤¡ F)¡ A     £ G |   9 sp u  is a suitable trial Hamiltonian, leading to Gaussian averages . [sent-59, score-0.266]
</p><p>40 The functions and are variational parameters to be optimized. [sent-60, score-0.349]
</p><p>41 It is important to have an explicit dependence on the input variable in order to take a non uniform input density into account. [sent-61, score-0.224]
</p><p>42 (8) makes the ”variational free energy” an explicit function of the ﬁrst two local moments  ! [sent-65, score-0.26]
</p><p>43 " s z8 s p u %wu 7  ¥ s p vz¡ # p u # p C eb# ca  (10)    s8¥ £ S2¦¤¡    7  9 F¤¡  ¥ £  s8¥ £  ¥ £ SF¤¡  F)¡    7  9 F)¡ ¨ ¥ £  %  &  Hence, a straightforward variation yields   ¥ £ ¦¤¡   (11)  9 %  #  ¥ £ F)¡   %  s8¥¥ £¡ £¡ z2F¤¢  ) 7     ¥ £ ¦¤¡   ! [sent-66, score-0.121]
</p><p>44 9  ¥ £  F)¡ ¨ &  s8¥¥ £¡ £¡ z22¦¤¢  ¤s 7     To extend the variational solutions to non-integer values of , we assume that for all optimal parameters are replica symmetric, ie. [sent-67, score-0.594]
</p><p>45 2 Interpretation of  Note, that our approach is not equivalent to a variational approximation of the original posterior. [sent-76, score-0.349]
</p><p>46 We can use the distribution induced by the prior and in order to compute approximate combined data and posterior averages. [sent-78, score-0.394]
</p><p>47 As an example, we ﬁrst consider the expected local . [sent-79, score-0.046]
</p><p>48 The prior over functions has zero mean and covariance . [sent-82, score-0.066]
</p><p>49 ¥  § 2  { S¥ ¥ ¤¢F)¢  U C t¥ ¥ £  ¤¡  V £¡  ¥ £¡ 9 £ ¨ $  ¥ ¥ £ %)¡ # £ {   ¨    ¥¦¤¡ ¤ £ A  ¢   y9  ¤  §  § $  ¥¥ £¡ $ 'F)|¤  'Ia ¡ c # ¥ £ F)¡ s8¥ 6¥ £ S`F¤¡    4 ¤ 7  £¡ £ ¥ )¡ F£ ¡    which yields the set of variational equations (11). [sent-86, score-0.394]
</p><p>50 They become particularly easy when the and the input distribution regression model uses a translationally invariant kernel is homogeneous in a ﬁnite interval. [sent-87, score-0.362]
</p><p>51 The variational equations (11) can then be solved in terms of the eigenvalues of the Gaussian process kernel. [sent-88, score-0.427]
</p><p>52   [8, 9] studied learning curves for Gaussian process regression which are not only averaged over the data but also over the data generating process using a Gaussian process prior on . [sent-89, score-0.703]
</p><p>53 Applying these averages to our theory and adapting the notation of [9] simply replaces by while . [sent-90, score-0.181]
</p><p>54 The data generating process is unknown but assumed to be ﬁxed. [sent-94, score-0.191]
</p><p>55 Displayed are the mean square prediction error (circle and solid line) and its sample ﬂuctuations (error bars) with respect to the data average (cross and broken line). [sent-98, score-0.222]
</p><p>56 The target was a random but ﬁxed realization from a Gaussian process prior with a periodic Radial Basis Function kernel , . [sent-99, score-0.482]
</p><p>57 g the Gaussian process regression model used the same kernel and noise . [sent-101, score-0.322]
</p><p>58 The inputs are one dimensional, independent and uniformly distributed . [sent-102, score-0.059]
</p><p>59 A typical property of our theory (lines) is that it becomes very accurate for sufﬁciently large number of example data. [sent-104, score-0.044]
</p><p>60 0001  −2  −3  ∆ε −4  10 0  100 50 150 Number m of Example Data  −4 0  200  20 40 60 80 Number m of Example Data  100  Figure 1: Gaussian process regression using a periodic Radial Basis Function kernel, input dimension d=1, , and homogeneous input density. [sent-109, score-0.669]
</p><p>61 Left: Generalization error and ﬂuctuations for data noise . [sent-110, score-0.157]
</p><p>62 All y-data was set equal The value of the noise variance to zero. [sent-118, score-0.151]
</p><p>63 2 Corrections to the Variational Approximation It is a strength of our method that the quality of the variational approximation Eq. [sent-120, score-0.349]
</p><p>64 Since the posterior variance is independent of the data this is still an interesting model from which the posterior variance can be estimated. [sent-123, score-0.696]
</p><p>65 We consider the third term in the expansion to the free energy Eq. [sent-124, score-0.408]
</p><p>66 It is a correction to the variational free energy and evaluates to  §  ¡ (16)  ed d  ¥ £ ¦¤¡  p 8¤ p u s u # p u 7 # s ¥¥ s u # p (¡ 7  { ¨ $ s ¥ ¥ ¤¡ 8 £ { ¡   ¥ ¨ '¦¤¡ ¤ ¡ ¢ $  ¥ £ ed d  ¥  ¤ #  ¥ ! [sent-126, score-0.852]
</p><p>67 U    # § ¤s V¥¥ £¡ ¥¥ £ ¥ £ Sf¦£ ¤Y¤ f¦)¡ s F)¡ s y9 ¨¦8 A #  £  A  ¢    ¤      s ea p # i re h   # r ih   £ ¥F¤¡    #p¥ ¥ £¤¡  F)¡    7 s p eIa 9 ¥ ¥ £ ¤¡  ¥ £ £ { ¡   ¤   ¥ ¨ l¦¤#£ ¡ ¤ ¡ A $  ¥    ed d tR # D e1 ¤ ca  ¥ ¥ £ )¡ ¤ £    with . [sent-129, score-0.076]
</p><p>68 We considered a homogeneous input density, the input dimension is one and the regression model uses a periodic RBF kernel. [sent-133, score-0.591]
</p><p>69 1 show the difference between the true value of the free energy which is obtained by simulations and the ﬁrst two terms of Eq. [sent-135, score-0.354]
</p><p>70 The correction term is found to be qualitatively accurate and emphasizes a discrepancy between free energy and the ﬁrst two terms of the expansion Eq. [sent-137, score-0.505]
</p><p>71 3 Universal Relations  ¢     and the empirical posterior variance  ¤  ¨ W   IH0     ¥ 0 £¤Y¤ ¡ A 9 G ¤  ¢    §  W    ¤  ¥ ¦0  §  ¢ £¤  We can relate the training error  (17)  # £¡ ¢¥ 0 )¢  #  ¤  ¨  H e0   A 9 G ¢  ¤  0. [sent-141, score-0.447]
</p><p>72 6  2  Theory 1d, periodic 2d, periodic 3d, periodic  2  [βε(x,y)/(βσ (x)+1) ](x,y)  1  [βσ (x)/(βσ (x)+1)]x  1  0. [sent-145, score-0.753]
</p><p>73 4  Theory d=1, periodic d=2, periodic d=3, periodic d=2+2, non-periodic  0. [sent-146, score-0.753]
</p><p>74 Symbols show simulation results for Radial Basis Function (RBF) regression and a homogeneous input distribution in dimensions (square, circle, diamond). [sent-161, score-0.318]
</p><p>75 Additionally, the left ﬁgure shows an example were the inputs lie on a quasi two-dimensional manifold which is embedded in dimensions (cross). [sent-163, score-0.096]
</p><p>76  A    ¡  9      ¥  ¤  $  9 x  ¢    ¤ ¤  U ¡ 9 W V B e# ca  to the free energy . [sent-165, score-0.43]
</p><p>77 (18) yields (19)  ¥ £ F¤¡  ¤  ¥ £ F)¡  ¥ £ ¦¤¡ ¤     ¤  $    ¤  A  §  ¥ £ ¦¤¡   £    9 ¤  ¢     which relates the empirical posterior variance to the local posterior variance at test inputs . [sent-169, score-0.857]
</p><p>78 Similarly, we can derive an expression for the training error by using Eqs. [sent-170, score-0.06]
</p><p>79 (19)     ¤  ¢  ¢       A §  ¤  ¥¥ £ F)¡ 2¤§   ¡  § $ ¤ ¥ )¡ £ ¥ ¤¡ £  (20)    £  £  |19 ¢  ¤  It is interesting to note, that the relations (19,20) contain no assumptions about the data generating process. [sent-172, score-0.286]
</p><p>80 They hold in general for Gaussian process models with a Gaussian likelihood. [sent-173, score-0.078]
</p><p>81 2 for the example of Gaussian process regression with a Radial Basis Function kernel. [sent-176, score-0.194]
</p><p>82 2, learning is initially starts in the upper right corner as the rescaled empirical posterior variance one and decreases with increasing number of example data. [sent-178, score-0.445]
</p><p>83 The rescaled training error on the noisy data set is initially zero and increases to one with increasing number of example data. [sent-181, score-0.174]
</p><p>84 The theory (line) holds for a sufﬁciently large number of example data and its accuracy increases with the input dimension. [sent-182, score-0.165]
</p><p>85 For common benchmark sets such as Abalone and Boston Housing data we ﬁnd that Eqs. [sent-185, score-0.056]
</p><p>86 (19,20) hold well even for small and medium sizes of the training data set. [sent-186, score-0.112]
</p><p>87 ¤  ¢     $  ¢  ¤  $  6 Outlook One may question if our approximate universal relations are of any practical use as, for example, the relation between training error and generalization error involves also the unknown posterior variance . [sent-187, score-0.857]
</p><p>88 Nevertheless, this relation could be useful for cases, where a large number of data inputs without output labels are available. [sent-188, score-0.172]
</p><p>89 Since for regression, the posterior variance is independent of the output labels, we could use these extra input points to estimate . [sent-189, score-0.385]
</p><p>90 For example, replacing ing the kernel of the Gaussian process prior gives a model for hard margin Support Vector Machine Classiﬁcation with SVM kernel . [sent-192, score-0.318]
</p><p>91 ¥   ¥ ¥ £ )¡ £  ¥ ¦  &  £  A  § # ¥ £¡ ¡ FF)¢  5¢    ¡£  { o ¤7 ¥ ¥ £  )¡ £ £  &    9 t¥ ¥ £ )¡ £  Of particular interest is the computation of empirical estimators that can be used in practice for model selection as well as the calculation of ﬂuctuations (error bars) for such estimators. [sent-194, score-0.067]
</p><p>92 A prominent example is an efﬁcient approximate leave-one-out estimator for SVMs. [sent-195, score-0.062]
</p><p>93 Hibbs, Quantum mechanics and path integrals, Mc GrawHill Inc. [sent-233, score-0.078]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('variational', 0.349), ('periodic', 0.251), ('replica', 0.245), ('posterior', 0.21), ('energy', 0.183), ('free', 0.171), ('averages', 0.137), ('sp', 0.129), ('relations', 0.126), ('fv', 0.123), ('uctuations', 0.123), ('ea', 0.117), ('regression', 0.116), ('grand', 0.115), ('hamiltonian', 0.115), ('malzahn', 0.115), ('sollich', 0.115), ('canonical', 0.112), ('variance', 0.11), ('partition', 0.108), ('correction', 0.097), ('homogeneous', 0.094), ('gaussian', 0.093), ('tf', 0.092), ('symbols', 0.09), ('rbf', 0.089), ('wu', 0.089), ('kernel', 0.087), ('opper', 0.08), ('radial', 0.078), ('panel', 0.078), ('process', 0.078), ('universal', 0.078), ('eb', 0.078), ('mechanics', 0.078), ('ca', 0.076), ('curves', 0.072), ('empirical', 0.067), ('prior', 0.066), ('input', 0.065), ('expectations', 0.063), ('approximate', 0.062), ('replicated', 0.062), ('error', 0.06), ('respect', 0.06), ('inputs', 0.059), ('dv', 0.058), ('corrections', 0.058), ('rescaled', 0.058), ('relation', 0.057), ('generating', 0.057), ('medium', 0.056), ('wx', 0.056), ('data', 0.056), ('expansion', 0.054), ('physics', 0.054), ('qp', 0.054), ('generalization', 0.052), ('evaluates', 0.052), ('density', 0.051), ('rst', 0.051), ('lines', 0.051), ('brackets', 0.05), ('predictor', 0.048), ('circle', 0.048), ('assumptions', 0.047), ('averaged', 0.046), ('square', 0.046), ('local', 0.046), ('errors', 0.045), ('yields', 0.045), ('theory', 0.044), ('explicit', 0.043), ('ib', 0.043), ('simulation', 0.043), ('illustration', 0.042), ('practical', 0.042), ('noise', 0.041), ('ciently', 0.041), ('cross', 0.039), ('vz', 0.038), ('yoshizawa', 0.038), ('abalone', 0.038), ('bischof', 0.038), ('chemical', 0.038), ('dominating', 0.038), ('dorffner', 0.038), ('drawings', 0.038), ('engel', 0.038), ('parisi', 0.038), ('replicas', 0.038), ('singapore', 0.038), ('thermodynamic', 0.038), ('wtu', 0.038), ('yxwu', 0.038), ('expectation', 0.038), ('suf', 0.038), ('ts', 0.037), ('left', 0.037), ('nite', 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="21-tfidf-1" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We combine the replica approach from statistical physics with a variational approach to analyze learning curves analytically. We apply the method to Gaussian process regression. As a main result we derive approximative relations between empirical error measures, the generalization error and the posterior variance.</p><p>2 0.18647076 <a title="21-tfidf-2" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>Author: Manfred Opper, Robert Urbanczik</p><p>Abstract: Using methods of Statistical Physics, we investigate the rOle of model complexity in learning with support vector machines (SVMs). We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error. Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel. 1</p><p>3 0.18560237 <a title="21-tfidf-3" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>Author: Lehel Csató, Manfred Opper, Ole Winther</p><p>Abstract: The adaptive TAP Gibbs free energy for a general densely connected probabilistic model with quadratic interactions and arbritary single site constraints is derived. We show how a speciﬁc sequential minimization of the free energy leads to a generalization of Minka’s expectation propagation. Lastly, we derive a sparse representation version of the sequential algorithm. The usefulness of the approach is demonstrated on classiﬁcation and density estimation with Gaussian processes and on an independent component analysis problem.</p><p>4 0.16784793 <a title="21-tfidf-4" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>Author: Matthias Seeger</p><p>Abstract: We propose the framework of mutual information kernels for learning covariance kernels, as used in Support Vector machines and Gaussian process classifiers, from unlabeled task data using Bayesian techniques. We describe an implementation of this framework which uses variational Bayesian mixtures of factor analyzers in order to attack classification problems in high-dimensional spaces where labeled data is sparse, but unlabeled data is abundant. 1</p><p>5 0.14092274 <a title="21-tfidf-5" href="./nips-2001-Gaussian_Process_Regression_with_Mismatched_Models.html">79 nips-2001-Gaussian Process Regression with Mismatched Models</a></p>
<p>Author: Peter Sollich</p><p>Abstract: Learning curves for Gaussian process regression are well understood when the 'student' model happens to match the 'teacher' (true data generation process). I derive approximations to the learning curves for the more generic case of mismatched models, and find very rich behaviour: For large input space dimensionality, where the results become exact, there are universal (student-independent) plateaux in the learning curve, with transitions in between that can exhibit arbitrarily many over-fitting maxima; over-fitting can occur even if the student estimates the teacher noise level correctly. In lower dimensions, plateaux also appear, and the learning curve remains dependent on the mismatch between student and teacher even in the asymptotic limit of a large number of training examples. Learning with excessively strong smoothness assumptions can be particularly dangerous: For example, a student with a standard radial basis function covariance function will learn a rougher teacher function only logarithmically slowly. All predictions are confirmed by simulations. 1</p><p>6 0.14062266 <a title="21-tfidf-6" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>7 0.12131529 <a title="21-tfidf-7" href="./nips-2001-Fast%2C_Large-Scale_Transformation-Invariant_Clustering.html">75 nips-2001-Fast, Large-Scale Transformation-Invariant Clustering</a></p>
<p>8 0.11060727 <a title="21-tfidf-8" href="./nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables.html">153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</a></p>
<p>9 0.10567056 <a title="21-tfidf-9" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>10 0.10117277 <a title="21-tfidf-10" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>11 0.095603764 <a title="21-tfidf-11" href="./nips-2001-Distribution_of_Mutual_Information.html">61 nips-2001-Distribution of Mutual Information</a></p>
<p>12 0.09544763 <a title="21-tfidf-12" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>13 0.09479022 <a title="21-tfidf-13" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>14 0.092435107 <a title="21-tfidf-14" href="./nips-2001-ALGONQUIN_-_Learning_Dynamic_Noise_Models_From_Noisy_Speech_for_Robust_Speech_Recognition.html">4 nips-2001-ALGONQUIN - Learning Dynamic Noise Models From Noisy Speech for Robust Speech Recognition</a></p>
<p>15 0.091740787 <a title="21-tfidf-15" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>16 0.090704806 <a title="21-tfidf-16" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>17 0.089862652 <a title="21-tfidf-17" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>18 0.089174375 <a title="21-tfidf-18" href="./nips-2001-Intransitive_Likelihood-Ratio_Classifiers.html">99 nips-2001-Intransitive Likelihood-Ratio Classifiers</a></p>
<p>19 0.088397883 <a title="21-tfidf-19" href="./nips-2001-Means%2C_Correlations_and_Bounds.html">119 nips-2001-Means, Correlations and Bounds</a></p>
<p>20 0.081579499 <a title="21-tfidf-20" href="./nips-2001-%28Not%29_Bounding_the_True_Error.html">1 nips-2001-(Not) Bounding the True Error</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.288), (1, 0.066), (2, -0.04), (3, -0.069), (4, -0.007), (5, -0.084), (6, 0.122), (7, -0.015), (8, 0.051), (9, 0.037), (10, 0.002), (11, 0.073), (12, 0.024), (13, -0.24), (14, -0.054), (15, 0.177), (16, -0.033), (17, 0.074), (18, -0.082), (19, -0.086), (20, -0.056), (21, -0.126), (22, 0.013), (23, 0.025), (24, 0.093), (25, 0.045), (26, -0.012), (27, -0.053), (28, -0.068), (29, 0.136), (30, -0.079), (31, -0.008), (32, -0.121), (33, 0.131), (34, 0.056), (35, -0.128), (36, 0.042), (37, 0.043), (38, 0.059), (39, 0.016), (40, -0.108), (41, 0.083), (42, 0.003), (43, 0.052), (44, -0.061), (45, -0.165), (46, -0.067), (47, -0.023), (48, 0.019), (49, -0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95919538 <a title="21-lsi-1" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We combine the replica approach from statistical physics with a variational approach to analyze learning curves analytically. We apply the method to Gaussian process regression. As a main result we derive approximative relations between empirical error measures, the generalization error and the posterior variance.</p><p>2 0.74307495 <a title="21-lsi-2" href="./nips-2001-Gaussian_Process_Regression_with_Mismatched_Models.html">79 nips-2001-Gaussian Process Regression with Mismatched Models</a></p>
<p>Author: Peter Sollich</p><p>Abstract: Learning curves for Gaussian process regression are well understood when the 'student' model happens to match the 'teacher' (true data generation process). I derive approximations to the learning curves for the more generic case of mismatched models, and find very rich behaviour: For large input space dimensionality, where the results become exact, there are universal (student-independent) plateaux in the learning curve, with transitions in between that can exhibit arbitrarily many over-fitting maxima; over-fitting can occur even if the student estimates the teacher noise level correctly. In lower dimensions, plateaux also appear, and the learning curve remains dependent on the mismatch between student and teacher even in the asymptotic limit of a large number of training examples. Learning with excessively strong smoothness assumptions can be particularly dangerous: For example, a student with a standard radial basis function covariance function will learn a rougher teacher function only logarithmically slowly. All predictions are confirmed by simulations. 1</p><p>3 0.71949875 <a title="21-lsi-3" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>Author: Lehel Csató, Manfred Opper, Ole Winther</p><p>Abstract: The adaptive TAP Gibbs free energy for a general densely connected probabilistic model with quadratic interactions and arbritary single site constraints is derived. We show how a speciﬁc sequential minimization of the free energy leads to a generalization of Minka’s expectation propagation. Lastly, we derive a sparse representation version of the sequential algorithm. The usefulness of the approach is demonstrated on classiﬁcation and density estimation with Gaussian processes and on an independent component analysis problem.</p><p>4 0.61718237 <a title="21-lsi-4" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>Author: Manfred Opper, Robert Urbanczik</p><p>Abstract: Using methods of Statistical Physics, we investigate the rOle of model complexity in learning with support vector machines (SVMs). We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error. Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel. 1</p><p>5 0.61124218 <a title="21-lsi-5" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>Author: K. Wong, F. Li</p><p>Abstract: We propose a method for the fast estimation of hyperparameters in large networks, based on the linear response relation in the cavity method, and an empirical measurement of the Green's function. Simulation results show that it is efficient and precise, when compared with cross-validation and other techniques which require matrix inversion. 1</p><p>6 0.60410541 <a title="21-lsi-6" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>7 0.58334684 <a title="21-lsi-7" href="./nips-2001-Distribution_of_Mutual_Information.html">61 nips-2001-Distribution of Mutual Information</a></p>
<p>8 0.49517289 <a title="21-lsi-8" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>9 0.47660378 <a title="21-lsi-9" href="./nips-2001-Fast%2C_Large-Scale_Transformation-Invariant_Clustering.html">75 nips-2001-Fast, Large-Scale Transformation-Invariant Clustering</a></p>
<p>10 0.44243374 <a title="21-lsi-10" href="./nips-2001-Quantizing_Density_Estimators.html">155 nips-2001-Quantizing Density Estimators</a></p>
<p>11 0.43856251 <a title="21-lsi-11" href="./nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables.html">153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</a></p>
<p>12 0.42661399 <a title="21-lsi-12" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>13 0.40802166 <a title="21-lsi-13" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>14 0.39899808 <a title="21-lsi-14" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>15 0.39414775 <a title="21-lsi-15" href="./nips-2001-Analysis_of_Sparse_Bayesian_Learning.html">35 nips-2001-Analysis of Sparse Bayesian Learning</a></p>
<p>16 0.39389777 <a title="21-lsi-16" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>17 0.38860565 <a title="21-lsi-17" href="./nips-2001-Geometrical_Singularities_in_the_Neuromanifold_of_Multilayer_Perceptrons.html">83 nips-2001-Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons</a></p>
<p>18 0.38509643 <a title="21-lsi-18" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>19 0.38473377 <a title="21-lsi-19" href="./nips-2001-The_Concave-Convex_Procedure_%28CCCP%29.html">180 nips-2001-The Concave-Convex Procedure (CCCP)</a></p>
<p>20 0.38287956 <a title="21-lsi-20" href="./nips-2001-Scaling_Laws_and_Local_Minima_in_Hebbian_ICA.html">165 nips-2001-Scaling Laws and Local Minima in Hebbian ICA</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.036), (17, 0.014), (19, 0.026), (27, 0.087), (30, 0.039), (36, 0.013), (59, 0.028), (72, 0.585), (79, 0.029), (91, 0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97723979 <a title="21-lda-1" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We combine the replica approach from statistical physics with a variational approach to analyze learning curves analytically. We apply the method to Gaussian process regression. As a main result we derive approximative relations between empirical error measures, the generalization error and the posterior variance.</p><p>2 0.9356662 <a title="21-lda-2" href="./nips-2001-Constructing_Distributed_Representations_Using_Additive_Clustering.html">53 nips-2001-Constructing Distributed Representations Using Additive Clustering</a></p>
<p>Author: Wheeler Ruml</p><p>Abstract: If the promise of computational modeling is to be fully realized in higherlevel cognitive domains such as language processing, principled methods must be developed to construct the semantic representations used in such models. In this paper, we propose the use of an established formalism from mathematical psychology, additive clustering, as a means of automatically constructing binary representations for objects using only pairwise similarity data. However, existing methods for the unsupervised learning of additive clustering models do not scale well to large problems. We present a new algorithm for additive clustering, based on a novel heuristic technique for combinatorial optimization. The algorithm is simpler than previous formulations and makes fewer independence assumptions. Extensive empirical tests on both human and synthetic data suggest that it is more effective than previous methods and that it also scales better to larger problems. By making additive clustering practical, we take a signiﬁcant step toward scaling connectionist models beyond hand-coded examples.</p><p>3 0.92226845 <a title="21-lda-3" href="./nips-2001-Intransitive_Likelihood-Ratio_Classifiers.html">99 nips-2001-Intransitive Likelihood-Ratio Classifiers</a></p>
<p>Author: Jeff Bilmes, Gang Ji, Marina Meila</p><p>Abstract: In this work, we introduce an information-theoretic based correction term to the likelihood ratio classiﬁcation method for multiple classes. Under certain conditions, the term is sufﬁcient for optimally correcting the difference between the true and estimated likelihood ratio, and we analyze this in the Gaussian case. We ﬁnd that the new correction term significantly improves the classiﬁcation results when tested on medium vocabulary speech recognition tasks. Moreover, the addition of this term makes the class comparisons analogous to an intransitive game and we therefore use several tournament-like strategies to deal with this issue. We ﬁnd that further small improvements are obtained by using an appropriate tournament. Lastly, we ﬁnd that intransitivity appears to be a good measure of classiﬁcation conﬁdence.</p><p>4 0.90714312 <a title="21-lda-4" href="./nips-2001-K-Local_Hyperplane_and_Convex_Distance_Nearest_Neighbor_Algorithms.html">101 nips-2001-K-Local Hyperplane and Convex Distance Nearest Neighbor Algorithms</a></p>
<p>Author: Pascal Vincent, Yoshua Bengio</p><p>Abstract: Guided by an initial idea of building a complex (non linear) decision surface with maximal local margin in input space, we give a possible geometrical intuition as to why K-Nearest Neighbor (KNN) algorithms often perform more poorly than SVMs on classiﬁcation tasks. We then propose modiﬁed K-Nearest Neighbor algorithms to overcome the perceived problem. The approach is similar in spirit to Tangent Distance, but with invariances inferred from the local neighborhood rather than prior knowledge. Experimental results on real world classiﬁcation tasks suggest that the modiﬁed KNN algorithms often give a dramatic improvement over standard KNN and perform as well or better than SVMs. 1 Motivation The notion of margin for classiﬁcation tasks has been largely popularized by the success of the Support Vector Machine (SVM) [2, 15] approach. The margin of SVMs has a nice geometric interpretation1: it can be deﬁned informally as (twice) the smallest Euclidean distance between the decision surface and the closest training point. The decision surface produced by the original SVM algorithm is the hyperplane that maximizes this distance while still correctly separating the two classes. While the notion of keeping the largest possible safety margin between the decision surface and the data points seems very reasonable and intuitively appealing, questions arise when extending the approach to building more complex, non-linear decision surfaces. Non-linear SVMs usually use the “kernel trick” to achieve their non-linearity. This conceptually corresponds to ﬁrst mapping the input into a higher-dimensional feature space with some non-linear transformation and building a maximum-margin hyperplane (a linear decision surface) there. The “trick” is that this mapping is never computed directly, but implicitly induced by a kernel. In this setting, the margin being maximized is still the smallest Euclidean distance between the decision surface and the training points, but this time measured in some strange, sometimes inﬁnite dimensional, kernel-induced feature space rather than the original input space. It is less clear whether maximizing the margin in this new space, is meaningful in general (see [16]). 1 for the purpose of this discussion, we consider the original hard-margin SVM algorithm for two linearly separable classes. A different approach is to try and build a non-linear decision surface with maximal distance to the closest data point as measured directly in input space (as proposed in [14]). We could for instance restrict ourselves to a certain class of decision functions and try to ﬁnd the function with maximal margin among this class. But let us take this even further. Extending the idea of building a correctly separating non-linear decision surface as far away as possible from the data points, we deﬁne the notion of local margin as the Euclidean distance, in input space, between a given point on the decision surface and the closest training point. Now would it be possible to ﬁnd an algorithm that could produce a decision surface which correctly separates the classes and such that the local margin is everywhere maximal along its surface? Surprisingly, the plain old Nearest Neighbor algorithm (1NN) [5] does precisely this! So why does 1NN in practice often perform worse than SVMs? One typical explanation, is that it has too much capacity, compared to SVM, that the class of function it can produce is too rich. But, considering it has inﬁnite capacity (VC-dimension), 1NN is still performing quite well... This study is an attempt to better understand what is happening, based on geometrical intuition, and to derive an improved Nearest Neighbor algorithm from this understanding. 2 Fixing a broken Nearest Neighbor algorithm 2.1 Setting and deﬁnitions The setting is that of a classical classiﬁcation problem in (the input space).  wx u  r i q ©</p><p>5 0.79138178 <a title="21-lda-5" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>Author: Thomas G. Dietterich, Xin Wang</p><p>Abstract: We present three ways of combining linear programming with the kernel trick to find value function approximations for reinforcement learning. One formulation is based on SVM regression; the second is based on the Bellman equation; and the third seeks only to ensure that good moves have an advantage over bad moves. All formulations attempt to minimize the number of support vectors while fitting the data. Experiments in a difficult, synthetic maze problem show that all three formulations give excellent performance, but the advantage formulation is much easier to train. Unlike policy gradient methods, the kernel methods described here can easily 'adjust the complexity of the function approximator to fit the complexity of the value function. 1</p><p>6 0.72992122 <a title="21-lda-6" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>7 0.67583048 <a title="21-lda-7" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>8 0.6650933 <a title="21-lda-8" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>9 0.65304244 <a title="21-lda-9" href="./nips-2001-Model_Based_Population_Tracking_and_Automatic_Detection_of_Distribution_Changes.html">122 nips-2001-Model Based Population Tracking and Automatic Detection of Distribution Changes</a></p>
<p>10 0.64676815 <a title="21-lda-10" href="./nips-2001-Stabilizing_Value_Function_Approximation_with_the_BFBP_Algorithm.html">175 nips-2001-Stabilizing Value Function Approximation with the BFBP Algorithm</a></p>
<p>11 0.6438725 <a title="21-lda-11" href="./nips-2001-%28Not%29_Bounding_the_True_Error.html">1 nips-2001-(Not) Bounding the True Error</a></p>
<p>12 0.62707859 <a title="21-lda-12" href="./nips-2001-Gaussian_Process_Regression_with_Mismatched_Models.html">79 nips-2001-Gaussian Process Regression with Mismatched Models</a></p>
<p>13 0.61399823 <a title="21-lda-13" href="./nips-2001-A_Parallel_Mixture_of_SVMs_for_Very_Large_Scale_Problems.html">16 nips-2001-A Parallel Mixture of SVMs for Very Large Scale Problems</a></p>
<p>14 0.60790521 <a title="21-lda-14" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>15 0.60718513 <a title="21-lda-15" href="./nips-2001-Quantizing_Density_Estimators.html">155 nips-2001-Quantizing Density Estimators</a></p>
<p>16 0.60315025 <a title="21-lda-16" href="./nips-2001-A_Sequence_Kernel_and_its_Application_to_Speaker_Recognition.html">20 nips-2001-A Sequence Kernel and its Application to Speaker Recognition</a></p>
<p>17 0.601973 <a title="21-lda-17" href="./nips-2001-Distribution_of_Mutual_Information.html">61 nips-2001-Distribution of Mutual Information</a></p>
<p>18 0.59354579 <a title="21-lda-18" href="./nips-2001-Active_Learning_in_the_Drug_Discovery_Process.html">25 nips-2001-Active Learning in the Drug Discovery Process</a></p>
<p>19 0.59350997 <a title="21-lda-19" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>20 0.58610547 <a title="21-lda-20" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
