<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>167 nips-2001-Semi-supervised MarginBoost</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-167" href="#">nips2001-167</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>167 nips-2001-Semi-supervised MarginBoost</h1>
<br/><p>Source: <a title="nips-2001-167-pdf" href="http://papers.nips.cc/paper/2108-semi-supervised-marginboost.pdf">pdf</a></p><p>Author: Florence D'alché-buc, Yves Grandvalet, Christophe Ambroise</p><p>Abstract: In many discrimination problems a large amount of data is available but only a few of them are labeled. This provides a strong motivation to improve or develop methods for semi-supervised learning. In this paper, boosting is generalized to this task within the optimization framework of MarginBoost . We extend the margin definition to unlabeled data and develop the gradient descent algorithm that corresponds to the resulting margin cost function. This meta-learning scheme can be applied to any base classifier able to benefit from unlabeled data. We propose here to apply it to mixture models trained with an Expectation-Maximization algorithm. Promising results are presented on benchmarks with different rates of labeled data. 1</p><p>Reference: <a title="nips-2001-167-reference" href="../nips2001_reference/nips-2001-Semi-supervised_MarginBoost_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 fr  Abstract In many discrimination problems a large amount of data is available but only a few of them are labeled. [sent-13, score-0.05]
</p><p>2 In this paper, boosting is generalized to this task within the optimization framework of MarginBoost . [sent-15, score-0.27]
</p><p>3 We extend the margin definition to unlabeled data and develop the gradient descent algorithm that corresponds to the resulting margin cost function. [sent-16, score-0.917]
</p><p>4 This meta-learning scheme can be applied to any base classifier able to benefit from unlabeled data. [sent-17, score-0.66]
</p><p>5 We propose here to apply it to mixture models trained with an Expectation-Maximization algorithm. [sent-18, score-0.068]
</p><p>6 Promising results are presented on benchmarks with different rates of labeled data. [sent-19, score-0.209]
</p><p>7 1  Introduction  In semi-supervised classification tasks, a concept is to be learnt using both labeled and unlabeled examples. [sent-20, score-0.428]
</p><p>8 Such problems arise frequently in data-mining where the cost of the labeling process can be prohibitive because it requires human help as in video-indexing, text-categorization [12] and medical diagnosis. [sent-21, score-0.067]
</p><p>9 While some works proposed different methods [16] to learn mixture models [12], [1], SVM [3], cotrained machines [5] to solve this task, no extension has been developed so far for ensemble methods such as boosting [7, 6]. [sent-22, score-0.406]
</p><p>10 Boosting consists in building sequentially a linear combination of base classifiers that focus on the difficult examples. [sent-23, score-0.271]
</p><p>11 For AdaBoost and extensions such as MarginBoost [10], this stage-wise procedure corresponds to a gradient descent of a cost functional based on a decreasing function of the margin, in the space of linear combinations of base classifiers. [sent-24, score-0.464]
</p><p>12 We propose to generalize boosting to semi-supervised learning within the framework of optimization. [sent-25, score-0.27]
</p><p>13 We extend the margin notion to unlabeled data, derive the corresponding criterion to be maximized, and propose the resulting algorithm called Semi-Supervised MarginBoost (SSMBoost). [sent-26, score-0.514]
</p><p>14 This new method enhances our previ-  ous work [9] based on a direct plug-in extension of AdaBoost in the sense that all the ingredients of the gradient algorithm such as the gradient direction and the stopping rule are defined from the expression of the new cost function. [sent-27, score-0.398]
</p><p>15 Moreover, while the algorithm has been tested using the mixtures of models [1], 55MBoost is designed to combine any base classifiers that deals with both labeled and unlabeled data. [sent-28, score-0.744]
</p><p>16 Then, in section 3, the 55MBoost algorithm is presented. [sent-30, score-0.03]
</p><p>17 We have chosen the MarginBoost algorithm, a variant of a more general algorithm called Any Boost [10], that generalizes AdaBoost and formally justifies the interpretation in terms of margin. [sent-34, score-0.03]
</p><p>18 1, minimizes the cost functional C defined for any scalar decreasing function c of the margin p : I  C(gt) =  L c(p(gt(Xi), Yi)))  (2)  i=l  Instead of taking exactly ht+l = - \1C(gt) which does not ensure that the resulting function gt+! [sent-38, score-0.288]
</p><p>19 The equivalent weighted cost function to be maximized can thus be expressed as :  JF = L Wt(i)Yiht+! [sent-41, score-0.096]
</p><p>20 (Xi)  (3)  iES  3  Generalizing MarginBoost to semi-supervised classification  3. [sent-42, score-0.039]
</p><p>21 1  Margin Extension  For labeled data, the margin measures the quality of the classifier output. [sent-43, score-0.429]
</p><p>22 When no label is observed, the usual margin cannot be calculated and has to be estimated. [sent-44, score-0.22]
</p><p>23 A first estimation could be derived from the expected margin EypL(gt(X) , y). [sent-45, score-0.188]
</p><p>24 We can use the output of the classifier (gt(x) + 1)/2 as an estimate of the posterior probability P(Y = +llx). [sent-46, score-0.148]
</p><p>25 This leads to the following margin pi; which depends on the input and is linked with the response of the classifier: lOr>  0 and L1 norm is used for normalization:  2< f, 9 >=  LiE S  f(X;)g(Xi)  IOrl  = L~=l  Or  Let wo(i) = l/l , i = 1, . [sent-47, score-0.188]
</p><p>26 Learn a gradient direction htH E 1i with a high value of J{ = L,iEswt(i)YihtH(Xi)  2. [sent-55, score-0.103]
</p><p>27 Apply the stopping rule: if J{ ::::: L,iES Wt(i)Yigt(Xi) then return gt else go on. [sent-56, score-0.345]
</p><p>28 Choose a step-length for the obtained direction by a line-search or by fixing it as a constant f 4 . [sent-58, score-0.029]
</p><p>29 Add the new direction to obtain 9HI = (l a t I9t+ a ttlh t t') lattl l 5. [sent-59, score-0.029]
</p><p>30 Fix the weight distribution: Wt  +1  =  c'(p(9ttl(Xi),Yi))  2: jE S c'(p(9ttl(Xj),Yj))  Figure 1: MarginBoost algorithm (with L1 normalization of the combination coefficients)  Another way of defining the extended margin is to use directly the maximum a posteriori estimate of the true margin. [sent-60, score-0.218]
</p><p>31 This MAP estimate depends on the sign of the classifier output and provides the following margin definition pC; :  (5) 3. [sent-61, score-0.336]
</p><p>32 2  Semi-Supervised MarginBoost : generalization of marginBoost to deal with unlabeled data  The generalization of the margin can be used to define an appropriate cost functional for the semi-supervised learning task. [sent-62, score-0.584]
</p><p>33 with If z E U  IWt l  c'(PU(9t(Xi)))  IWt l  This expression of product:  IWt  I=  2:=  Wt  (i)  (9)  iES  JP comes directly from differential calculus and the chosen inner  ( )()  'VC gt Xi  if x = Xi and i E L if x = x, and i E U  YiC'(Pd9t(Xi),Yi))  = { c'(p U (g t (x. [sent-69, score-0.345]
</p><p>34 ))) apU(9t(Xi)) a9t( Xi) t  (10)  0  Pu  Implementation of 55MBoost with margins pI[; and requires their derivatives. [sent-70, score-0.08]
</p><p>35 The value of the sub derivative corresponds here to the average value of the right and left derivatives. [sent-73, score-0.033]
</p><p>36 apUS(gt(Xi)) = {sign(g(Xi)) agt (Xi) 0  if X :f": 0 if x = 0  (11)  And, for the "squared margin" Pu 9 , we have:  apu 9 (gt(Xi)) = 2g(Xi) agt(Xi)  (12)  This completes the set of ingredients that must be incorporated into the algorithm of Fig. [sent-74, score-0.181]
</p><p>37 4  Base Classifier  The base classifier should be able to make use of the unlabeled data provided by the boosting algorithm. [sent-76, score-0.93]
</p><p>38 Hierarchical mixtures provide flexible discrimination tools, where each conditional distribution f(xlY = k) is modelled by a mixture of components [4]. [sent-78, score-0.147]
</p><p>39 The high-level description can also be expressed as a low-level mixture of components, as shown here for binary classification: Kl  f(x;if» =  K2  2:= PkJkl(X;Okl) + 2:= Pk2! [sent-81, score-0.068]
</p><p>40 k2(X;Ok2)  (14)  With this setting, the EM algorithm is used to maximize the log-likelihood with respect to if> considering the incomplete data is {Xi, Yi}~= l and the missing data is the component label Cik, k = 1, . [sent-82, score-0.146]
</p><p>41 An original implementation of EM based on the concept of possible labels [1] is considered here. [sent-86, score-0.072]
</p><p>42 It is well adapted to hierarchical mixtures, where the class label Y provides a subset of possible components. [sent-87, score-0.075]
</p><p>43 When Y = 1 the first Kl modes are possible, when Y = -1 the last K2 modes are possible, and when an example is unlabeled, all modes are possible. [sent-88, score-0.174]
</p><p>44 A binary vector Zi E {0,1}(Kl+ K2) indicates the components from which feature vector Xi may have been generated, in agreement with the assumed mixture model and the (absence of) label Yi. [sent-89, score-0.1]
</p><p>45 Assuming that each mode k follows a normal distribution with mean ILk' and covariance ~k '  q+l = {ILk+! [sent-96, score-0.027]
</p><p>46 ;Pk+l}f ~iK2 is given by:  (17) (18)  5  Experimental results  Tests of the algorithm are performed on three benchmarks of the boosting literature: twonorm and ringnorm [6] and banana [13]. [sent-98, score-0.776]
</p><p>47 Information about these datasets and the results obtained in discrimination are available at www. [sent-99, score-0.05]
</p><p>48 We first study the behavior of 55MBoost according the evolution of the test error with increasing rates of unlabeled data (table 1). [sent-103, score-0.461]
</p><p>49 We consider five different settings where 0%, 50%, 75%, 90% and 95% of labels are missing. [sent-104, score-0.072]
</p><p>50 55MB is tested for the margins P~ and Pu with c(x) = exp( -x). [sent-105, score-0.08]
</p><p>51 55MBoost and AdaBoost are trained identically, the only difference being that AdaBoost is not provided with missing labels. [sent-107, score-0.084]
</p><p>52 Both algorithms are run for T = 100 boosting steps, without special care of overfitting. [sent-108, score-0.27]
</p><p>53 We report mean error rates together with the lower and upper quartiles in table 1. [sent-110, score-0.113]
</p><p>54 For sake of space, we did not display the results obtained without missing labels: in this case, AdaBoost and 55MBoost behave nearly identically and better than EM only for Banana. [sent-111, score-0.123]
</p><p>55 Ringnorm 50% 75% 90% 95% base(EM) AdaBoost 55MBoost pS 55MBoost pg Twonorm  2. [sent-113, score-0.056]
</p><p>56 0] 95%  base(EM) AdaBoost 55MBoost pS 55MBoost pg Banana  3. [sent-161, score-0.056]
</p><p>57 One possible explanation is that the discrimination frontiers involved in the banana problem are so complex that the labels really bring crucial informations and thus adding unlabeled data does not help in such a case. [sent-258, score-0.588]
</p><p>58 Pu  obtains Nevertheless, at rate 95% which is the most realistic situation, the margin the minimal error rate for each of the three problems. [sent-259, score-0.291]
</p><p>59 It shows that it is worth boosting and using unlabeled data. [sent-260, score-0.595]
</p><p>60 As there is no great difference between the two proposed margins, we conducted further experiments using only the  Pu'  Second, in order to study the relation between the presence of noise in the dataset and the ability of 55MBoost to enhance generalization performance, we draw in Fig. [sent-261, score-0.027]
</p><p>61 2, the test errors obtained for problems with different values of Bayes error when varying the rate of labeled examples. [sent-262, score-0.207]
</p><p>62 We see that even for difficult tasks (very noisy problems), the degradation in performance for large subsets of unlabeled data is still low. [sent-263, score-0.296]
</p><p>63 This reflects some consistency in the behavior of our algorithm. [sent-264, score-0.029]
</p><p>64 Third, we test the sensibility of 55MBoost to overfitting. [sent-265, score-0.038]
</p><p>65 Overfitting can usually be avoided by techniques such as early stopping, softenizing of the margin ([1 3], [14]) or using an adequate margin function such as 1 - tanh(p) instead of exp( -p) [10]. [sent-266, score-0.376]
</p><p>66 Here we keep using c = exp and ran 55MBoost with a maximal number of step T = 1000 with 95% of unlabeled data. [sent-267, score-0.296]
</p><p>67 Of course, this does not correspond to a realistic use of boosting in practice but it allows to check if the algorithm behaves consistently in terms of gradient steps number. [sent-268, score-0.374]
</p><p>68 It is remarkable that no overfitting is observed and in the Twonorm case (see Fig. [sent-269, score-0.063]
</p><p>69 We also observe that the standard error deviation is reduced at the end of the process. [sent-271, score-0.049]
</p><p>70 A massive presence of unlabeled data implies thus a regularizing effect. [sent-275, score-0.338]
</p><p>71 2%  50  40  20  10  °0 L---~ 0 --~7---~7---- 0--~5~ ,7 20 ~ 4~ 0--~6 0--~7= = 0----8 0----9 0~~ = = '00 Rate of missing labels (%)  Figure 2: Consistency of the 55MBoost behavior: evolution of test error versus the missing labels rate with respect to various Bayes error (twonorm ). [sent-279, score-0.516]
</p><p>72 Mean (Error Test) +/- 1 std  70  70  Mean (Error test) 60' ,  I~  Mean of Error Test +/- std Mean of Error test  I  60  ,  ~ 50  I  i \! [sent-280, score-0.112]
</p><p>73 6  Conclusion  MarginBoost algorithm has been extended to deal with both labeled and unlabeled data. [sent-287, score-0.419]
</p><p>74 Results obtained on three classical benchmarks of boosting litterature show that it is worth using additional information conveyed by the patterns alone. [sent-288, score-0.378]
</p><p>75 No overfitting was observed during processing 55MBoost on the benchmarks when 95% of the labels are missing: this should mean that the unlabeled data should playa regularizing role in the ensemble classifier during the boosting process. [sent-289, score-1.038]
</p><p>76 After applying this method to a large real dataset such as those of text-categorization, our future works on this theme will concern the use of the extended margin cost function on the base classifiers itself like multilayered perceptrons or decision trees. [sent-290, score-0.526]
</p><p>77 Another approach could also be conducted from the more general framework of AnyBoost that optimize any differential cost function. [sent-291, score-0.121]
</p><p>78 In Proceedings of th e 1998 Conference on Computational Learning Th eory, July 1998. [sent-321, score-0.036]
</p><p>79 In Machin e Learning: Proceedings of th e Thirteenth International Conference, pages 148- 156. [sent-331, score-0.036]
</p><p>80 Text classification from labeled and unlabeled documents using EM. [sent-364, score-0.428]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('marginboost', 0.368), ('unlabeled', 0.296), ('gt', 0.293), ('boosting', 0.27), ('xi', 0.229), ('base', 0.216), ('adaboost', 0.215), ('margin', 0.188), ('banana', 0.17), ('classifier', 0.148), ('twonorm', 0.142), ('pu', 0.12), ('wt', 0.116), ('compiegne', 0.113), ('labeled', 0.093), ('pk', 0.09), ('em', 0.087), ('cedex', 0.085), ('cnrs', 0.085), ('grandvalet', 0.085), ('iwt', 0.085), ('ringnorm', 0.085), ('missing', 0.084), ('ok', 0.084), ('margins', 0.08), ('benchmarks', 0.079), ('universite', 0.074), ('descent', 0.074), ('gradient', 0.074), ('labels', 0.072), ('ht', 0.072), ('mixture', 0.068), ('cost', 0.067), ('jf', 0.063), ('overfitting', 0.063), ('modes', 0.058), ('ambroise', 0.057), ('apu', 0.057), ('heudiasyc', 0.057), ('hth', 0.057), ('iel', 0.057), ('lwt', 0.057), ('pus', 0.057), ('riitsch', 0.057), ('technologie', 0.057), ('umr', 0.057), ('zdi', 0.057), ('france', 0.056), ('pg', 0.056), ('classifiers', 0.055), ('ps', 0.054), ('kl', 0.053), ('stopping', 0.052), ('discrimination', 0.05), ('error', 0.049), ('agt', 0.049), ('christophe', 0.049), ('ilk', 0.045), ('ies', 0.045), ('uik', 0.045), ('ingredients', 0.045), ('hierarchical', 0.043), ('onoda', 0.042), ('regularizing', 0.042), ('evolution', 0.041), ('ensemble', 0.041), ('bayes', 0.04), ('identically', 0.039), ('classification', 0.039), ('test', 0.038), ('std', 0.037), ('rates', 0.037), ('th', 0.036), ('pl', 0.036), ('zi', 0.034), ('sub', 0.033), ('functional', 0.033), ('bp', 0.032), ('label', 0.032), ('falls', 0.031), ('july', 0.03), ('algorithm', 0.03), ('mixtures', 0.029), ('worth', 0.029), ('consistency', 0.029), ('maximized', 0.029), ('direction', 0.029), ('freund', 0.028), ('rate', 0.027), ('extension', 0.027), ('conducted', 0.027), ('differential', 0.027), ('mean', 0.027), ('annals', 0.026), ('oo', 0.025), ('jp', 0.025), ('calculus', 0.025), ('deals', 0.025), ('lor', 0.025), ('je', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="167-tfidf-1" href="./nips-2001-Semi-supervised_MarginBoost.html">167 nips-2001-Semi-supervised MarginBoost</a></p>
<p>Author: Florence D'alché-buc, Yves Grandvalet, Christophe Ambroise</p><p>Abstract: In many discrimination problems a large amount of data is available but only a few of them are labeled. This provides a strong motivation to improve or develop methods for semi-supervised learning. In this paper, boosting is generalized to this task within the optimization framework of MarginBoost . We extend the margin definition to unlabeled data and develop the gradient descent algorithm that corresponds to the resulting margin cost function. This meta-learning scheme can be applied to any base classifier able to benefit from unlabeled data. We propose here to apply it to mixture models trained with an Expectation-Maximization algorithm. Promising results are presented on benchmarks with different rates of labeled data. 1</p><p>2 0.19734672 <a title="167-tfidf-2" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>Author: Gunnar Rätsch, Sebastian Mika, Manfred K. Warmuth</p><p>Abstract: We give an uniﬁed convergence analysis of ensemble learning methods including e.g. AdaBoost, Logistic Regression and the Least-SquareBoost algorithm for regression. These methods have in common that they iteratively call a base learning algorithm which returns hypotheses that are then linearly combined. We show that these methods are related to the Gauss-Southwell method known from numerical optimization and state non-asymptotical convergence results for all these methods. Our analysis includes -norm regularized cost functions leading to a clean and general way to regularize ensemble learning.</p><p>3 0.1821344 <a title="167-tfidf-3" href="./nips-2001-Partially_labeled_classification_with_Markov_random_walks.html">144 nips-2001-Partially labeled classification with Markov random walks</a></p>
<p>Author: Martin Szummer, Tommi Jaakkola</p><p>Abstract: To classify a large number of unlabeled examples we combine a limited number of labeled examples with a Markov random walk representation over the unlabeled examples. The random walk representation exploits any low dimensional structure in the data in a robust, probabilistic manner. We develop and compare several estimation criteria/algorithms suited to this representation. This includes in particular multi-way classiﬁcation with an average margin criterion which permits a closed form solution. The time scale of the random walk regularizes the representation and can be set through a margin-based criterion favoring unambiguous classiﬁcation. We also extend this basic regularization by adapting time scales for individual examples. We demonstrate the approach on synthetic examples and on text classiﬁcation problems.</p><p>4 0.16726798 <a title="167-tfidf-4" href="./nips-2001-Boosting_and_Maximum_Likelihood_for_Exponential_Models.html">45 nips-2001-Boosting and Maximum Likelihood for Exponential Models</a></p>
<p>Author: Guy Lebanon, John D. Lafferty</p><p>Abstract: We derive an equivalence between AdaBoost and the dual of a convex optimization problem, showing that the only difference between minimizing the exponential loss used by AdaBoost and maximum likelihood for exponential models is that the latter requires the model to be normalized to form a conditional probability distribution over labels. In addition to establishing a simple and easily understood connection between the two methods, this framework enables us to derive new regularization procedures for boosting that directly correspond to penalized maximum likelihood. Experiments on UCI datasets support our theoretical analysis and give additional insight into the relationship between boosting and logistic regression.</p><p>5 0.15264194 <a title="167-tfidf-5" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>6 0.14964007 <a title="167-tfidf-6" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>7 0.141543 <a title="167-tfidf-7" href="./nips-2001-MIME%3A_Mutual_Information_Minimization_and_Entropy_Maximization_for_Bayesian_Belief_Propagation.html">117 nips-2001-MIME: Mutual Information Minimization and Entropy Maximization for Bayesian Belief Propagation</a></p>
<p>8 0.12204999 <a title="167-tfidf-8" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>9 0.12125093 <a title="167-tfidf-9" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>10 0.10499822 <a title="167-tfidf-10" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>11 0.10325724 <a title="167-tfidf-11" href="./nips-2001-Active_Learning_in_the_Drug_Discovery_Process.html">25 nips-2001-Active Learning in the Drug Discovery Process</a></p>
<p>12 0.084144339 <a title="167-tfidf-12" href="./nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes.html">133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</a></p>
<p>13 0.08224842 <a title="167-tfidf-13" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>14 0.080659457 <a title="167-tfidf-14" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>15 0.079210825 <a title="167-tfidf-15" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>16 0.076755986 <a title="167-tfidf-16" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>17 0.074503481 <a title="167-tfidf-17" href="./nips-2001-Unsupervised_Learning_of_Human_Motion_Models.html">193 nips-2001-Unsupervised Learning of Human Motion Models</a></p>
<p>18 0.073894076 <a title="167-tfidf-18" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>19 0.073488839 <a title="167-tfidf-19" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>20 0.067115769 <a title="167-tfidf-20" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.217), (1, 0.117), (2, -0.009), (3, 0.142), (4, 0.082), (5, -0.054), (6, 0.019), (7, -0.052), (8, -0.019), (9, -0.051), (10, 0.122), (11, 0.096), (12, 0.001), (13, 0.014), (14, 0.138), (15, -0.122), (16, -0.013), (17, -0.121), (18, 0.007), (19, 0.195), (20, -0.048), (21, 0.097), (22, -0.341), (23, 0.111), (24, 0.109), (25, -0.002), (26, 0.116), (27, -0.146), (28, -0.055), (29, 0.041), (30, -0.055), (31, 0.043), (32, -0.064), (33, -0.108), (34, -0.153), (35, -0.013), (36, -0.106), (37, -0.036), (38, -0.003), (39, -0.098), (40, 0.014), (41, -0.03), (42, 0.068), (43, 0.011), (44, -0.03), (45, 0.017), (46, 0.069), (47, -0.032), (48, 0.093), (49, -0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95841593 <a title="167-lsi-1" href="./nips-2001-Semi-supervised_MarginBoost.html">167 nips-2001-Semi-supervised MarginBoost</a></p>
<p>Author: Florence D'alché-buc, Yves Grandvalet, Christophe Ambroise</p><p>Abstract: In many discrimination problems a large amount of data is available but only a few of them are labeled. This provides a strong motivation to improve or develop methods for semi-supervised learning. In this paper, boosting is generalized to this task within the optimization framework of MarginBoost . We extend the margin definition to unlabeled data and develop the gradient descent algorithm that corresponds to the resulting margin cost function. This meta-learning scheme can be applied to any base classifier able to benefit from unlabeled data. We propose here to apply it to mixture models trained with an Expectation-Maximization algorithm. Promising results are presented on benchmarks with different rates of labeled data. 1</p><p>2 0.5831269 <a title="167-lsi-2" href="./nips-2001-Boosting_and_Maximum_Likelihood_for_Exponential_Models.html">45 nips-2001-Boosting and Maximum Likelihood for Exponential Models</a></p>
<p>Author: Guy Lebanon, John D. Lafferty</p><p>Abstract: We derive an equivalence between AdaBoost and the dual of a convex optimization problem, showing that the only difference between minimizing the exponential loss used by AdaBoost and maximum likelihood for exponential models is that the latter requires the model to be normalized to form a conditional probability distribution over labels. In addition to establishing a simple and easily understood connection between the two methods, this framework enables us to derive new regularization procedures for boosting that directly correspond to penalized maximum likelihood. Experiments on UCI datasets support our theoretical analysis and give additional insight into the relationship between boosting and logistic regression.</p><p>3 0.50183755 <a title="167-lsi-3" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>Author: Gunnar Rätsch, Sebastian Mika, Manfred K. Warmuth</p><p>Abstract: We give an uniﬁed convergence analysis of ensemble learning methods including e.g. AdaBoost, Logistic Regression and the Least-SquareBoost algorithm for regression. These methods have in common that they iteratively call a base learning algorithm which returns hypotheses that are then linearly combined. We show that these methods are related to the Gauss-Southwell method known from numerical optimization and state non-asymptotical convergence results for all these methods. Our analysis includes -norm regularized cost functions leading to a clean and general way to regularize ensemble learning.</p><p>4 0.47931665 <a title="167-lsi-4" href="./nips-2001-MIME%3A_Mutual_Information_Minimization_and_Entropy_Maximization_for_Bayesian_Belief_Propagation.html">117 nips-2001-MIME: Mutual Information Minimization and Entropy Maximization for Bayesian Belief Propagation</a></p>
<p>Author: Anand Rangarajan, Alan L. Yuille</p><p>Abstract: Bayesian belief propagation in graphical models has been recently shown to have very close ties to inference methods based in statistical physics. After Yedidia et al. demonstrated that belief propagation ﬁxed points correspond to extrema of the so-called Bethe free energy, Yuille derived a double loop algorithm that is guaranteed to converge to a local minimum of the Bethe free energy. Yuille’s algorithm is based on a certain decomposition of the Bethe free energy and he mentions that other decompositions are possible and may even be fruitful. In the present work, we begin with the Bethe free energy and show that it has a principled interpretation as pairwise mutual information minimization and marginal entropy maximization (MIME). Next, we construct a family of free energy functions from a spectrum of decompositions of the original Bethe free energy. For each free energy in this family, we develop a new algorithm that is guaranteed to converge to a local minimum. Preliminary computer simulations are in agreement with this theoretical development. 1</p><p>5 0.47309396 <a title="167-lsi-5" href="./nips-2001-Active_Learning_in_the_Drug_Discovery_Process.html">25 nips-2001-Active Learning in the Drug Discovery Process</a></p>
<p>Author: Manfred K. Warmuth, Gunnar Rätsch, Michael Mathieson, Jun Liao, Christian Lemmen</p><p>Abstract: We investigate the following data mining problem from Computational Chemistry: From a large data set of compounds, ﬁnd those that bind to a target molecule in as few iterations of biological testing as possible. In each iteration a comparatively small batch of compounds is screened for binding to the target. We apply active learning techniques for selecting the successive batches. One selection strategy picks unlabeled examples closest to the maximum margin hyperplane. Another produces many weight vectors by running perceptrons over multiple permutations of the data. Each weight vector votes with its prediction and we pick the unlabeled examples for which the prediction is most evenly split between and . For a third selection strategy note that each unlabeled example bisects the version space of consistent weight vectors. We estimate the volume on both sides of the split by bouncing a billiard through the version space and select unlabeled examples that cause the most even split of the version space. We demonstrate that on two data sets provided by DuPont Pharmaceuticals that all three selection strategies perform comparably well and are much better than selecting random batches for testing. § © ¨</p><p>6 0.45535636 <a title="167-lsi-6" href="./nips-2001-Partially_labeled_classification_with_Markov_random_walks.html">144 nips-2001-Partially labeled classification with Markov random walks</a></p>
<p>7 0.44762745 <a title="167-lsi-7" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>8 0.44199741 <a title="167-lsi-8" href="./nips-2001-EM-DD%3A_An_Improved_Multiple-Instance_Learning_Technique.html">64 nips-2001-EM-DD: An Improved Multiple-Instance Learning Technique</a></p>
<p>9 0.42879474 <a title="167-lsi-9" href="./nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes.html">133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</a></p>
<p>10 0.37437981 <a title="167-lsi-10" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>11 0.34586421 <a title="167-lsi-11" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>12 0.33056003 <a title="167-lsi-12" href="./nips-2001-Unsupervised_Learning_of_Human_Motion_Models.html">193 nips-2001-Unsupervised Learning of Human Motion Models</a></p>
<p>13 0.32994229 <a title="167-lsi-13" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>14 0.32819322 <a title="167-lsi-14" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>15 0.29343355 <a title="167-lsi-15" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>16 0.28037497 <a title="167-lsi-16" href="./nips-2001-The_Concave-Convex_Procedure_%28CCCP%29.html">180 nips-2001-The Concave-Convex Procedure (CCCP)</a></p>
<p>17 0.27542686 <a title="167-lsi-17" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>18 0.27446988 <a title="167-lsi-18" href="./nips-2001-Estimating_Car_Insurance_Premia%3A_a_Case_Study_in_High-Dimensional_Data_Inference.html">70 nips-2001-Estimating Car Insurance Premia: a Case Study in High-Dimensional Data Inference</a></p>
<p>19 0.27158663 <a title="167-lsi-19" href="./nips-2001-Duality%2C_Geometry%2C_and_Support_Vector_Regression.html">62 nips-2001-Duality, Geometry, and Support Vector Regression</a></p>
<p>20 0.27115577 <a title="167-lsi-20" href="./nips-2001-Learning_from_Infinite_Data_in_Finite_Time.html">114 nips-2001-Learning from Infinite Data in Finite Time</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.32), (14, 0.042), (17, 0.023), (19, 0.028), (27, 0.155), (30, 0.056), (36, 0.012), (38, 0.013), (59, 0.029), (72, 0.064), (79, 0.03), (83, 0.012), (91, 0.143)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82802629 <a title="167-lda-1" href="./nips-2001-Semi-supervised_MarginBoost.html">167 nips-2001-Semi-supervised MarginBoost</a></p>
<p>Author: Florence D'alché-buc, Yves Grandvalet, Christophe Ambroise</p><p>Abstract: In many discrimination problems a large amount of data is available but only a few of them are labeled. This provides a strong motivation to improve or develop methods for semi-supervised learning. In this paper, boosting is generalized to this task within the optimization framework of MarginBoost . We extend the margin definition to unlabeled data and develop the gradient descent algorithm that corresponds to the resulting margin cost function. This meta-learning scheme can be applied to any base classifier able to benefit from unlabeled data. We propose here to apply it to mixture models trained with an Expectation-Maximization algorithm. Promising results are presented on benchmarks with different rates of labeled data. 1</p><p>2 0.58920527 <a title="167-lda-2" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>3 0.58591396 <a title="167-lda-3" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>4 0.5837118 <a title="167-lda-4" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>Author: Andrew Y. Ng, Michael I. Jordan, Yair Weiss</p><p>Abstract: Despite many empirical successes of spectral clustering methodsalgorithms that cluster points using eigenvectors of matrices derived from the data- there are several unresolved issues. First, there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems. 1</p><p>5 0.58369505 <a title="167-lda-5" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>Author: Marzia Polito, Pietro Perona</p><p>Abstract: Locally Linear Embedding (LLE) is an elegant nonlinear dimensionality-reduction technique recently introduced by Roweis and Saul [2]. It fails when the data is divided into separate groups. We study a variant of LLE that can simultaneously group the data and calculate local embedding of each group. An estimate for the upper bound on the intrinsic dimension of the data set is obtained automatically. 1</p><p>6 0.58263254 <a title="167-lda-6" href="./nips-2001-Multi_Dimensional_ICA_to_Separate_Correlated_Sources.html">127 nips-2001-Multi Dimensional ICA to Separate Correlated Sources</a></p>
<p>7 0.57887352 <a title="167-lda-7" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>8 0.57845932 <a title="167-lda-8" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>9 0.5768953 <a title="167-lda-9" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<p>10 0.57664984 <a title="167-lda-10" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>11 0.576114 <a title="167-lda-11" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>12 0.57528216 <a title="167-lda-12" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>13 0.57501841 <a title="167-lda-13" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>14 0.57431078 <a title="167-lda-14" href="./nips-2001-Blind_Source_Separation_via_Multinode_Sparse_Representation.html">44 nips-2001-Blind Source Separation via Multinode Sparse Representation</a></p>
<p>15 0.57381445 <a title="167-lda-15" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>16 0.57374537 <a title="167-lda-16" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>17 0.57355821 <a title="167-lda-17" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>18 0.57286394 <a title="167-lda-18" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>19 0.5720942 <a title="167-lda-19" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>20 0.57105881 <a title="167-lda-20" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
