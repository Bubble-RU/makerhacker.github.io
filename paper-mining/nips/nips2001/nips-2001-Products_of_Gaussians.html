<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>154 nips-2001-Products of Gaussians</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-154" href="#">nips2001-154</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>154 nips-2001-Products of Gaussians</h1>
<br/><p>Source: <a title="nips-2001-154-pdf" href="http://papers.nips.cc/paper/2102-products-of-gaussians.pdf">pdf</a></p><p>Author: Christopher Williams, Felix V. Agakov, Stephen N. Felderhof</p><p>Abstract: Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. Below we consider PoE models in which each expert is a Gaussian. Although the product of Gaussians is also a Gaussian, if each Gaussian has a simple structure the product can have a richer structure. We examine (1) Products of Gaussian pancakes which give rise to probabilistic Minor Components Analysis, (2) products of I-factor PPCA models and (3) a products of experts construction for an AR(l) process. Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. In this paper we consider PoE models in which each expert is a Gaussian. It is easy to see that in this case the product model will also be Gaussian. However, if each Gaussian has a simple structure, the product can have a richer structure. Using Gaussian experts is attractive as it permits a thorough analysis of the product architecture, which can be difficult with other models , e.g. models defined over discrete random variables. Below we examine three cases of the products of Gaussians construction: (1) Products of Gaussian pancakes (PoGP) which give rise to probabilistic Minor Components Analysis (MCA), providing a complementary result to probabilistic Principal Components Analysis (PPCA) obtained by Tipping and Bishop (1999); (2) Products of I-factor PPCA models; (3) A products of experts construction for an AR(l) process. Products of Gaussians If each expert is a Gaussian pi(xI8 i ) '</p><p>Reference: <a title="nips-2001-154-reference" href="../nips2001_reference/nips-2001-Products_of_Gaussians_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract  Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. [sent-18, score-0.183]
</p><p>2 Below we consider PoE models in which each expert is a Gaussian. [sent-19, score-0.112]
</p><p>3 Although the product of Gaussians is also a Gaussian, if each Gaussian has a simple structure the product can have a richer structure. [sent-20, score-0.21]
</p><p>4 We examine (1) Products of Gaussian pancakes which give rise to probabilistic Minor Components Analysis, (2) products of I-factor PPCA models and (3) a products of experts construction for an AR(l) process. [sent-21, score-0.873]
</p><p>5 Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. [sent-22, score-0.158]
</p><p>6 In this paper we consider PoE models in which each expert is a Gaussian. [sent-23, score-0.112]
</p><p>7 It is easy to see that in this case the product model will also be Gaussian. [sent-24, score-0.11]
</p><p>8 However, if each Gaussian has a simple structure, the product can have a richer structure. [sent-25, score-0.129]
</p><p>9 Using Gaussian experts is attractive as it permits a thorough analysis of the product architecture, which can be difficult with other models , e. [sent-26, score-0.353]
</p><p>10 Products of Gaussians If each expert is a Gaussian pi(xI8 i ) '" N(J1i' ( i), the resulting distribution of the product of m Gaussians may be expressed as  By completing the square in the exponent it may be easily shown that p(xI8) N(/1;E, (2:), where (E l = 2::1 (i l . [sent-30, score-0.179]
</p><p>11 °  1  Products of Gaussian Pancakes  A Gaussian "pancake" (GP) is a d-dimensional Gaussian, contracted in one dimension and elongated in the other d - 1 dimensions. [sent-33, score-0.136]
</p><p>12 In this section we show that the maximum likelihood solution for a product of Gaussian pancakes (PoGP) yields a probabilistic formulation of Minor Components Analysis (MCA). [sent-34, score-0.408]
</p><p>13 1  Covariance Structure of a GP Expert  Consider a d-dimensional Gaussian whose probability contours are contracted in the direction w and equally elongated in mutually orthogonal directions VI , . [sent-36, score-0.219]
</p><p>14 Its inverse covariance may be written as d- l  ( -1=  L ViV; /30 + ww  T  /3,;;,  (1)  i= l  where VI, . [sent-41, score-0.358]
</p><p>15 ,V d - l, W form a d x d matrix of normalized eigenvectors of the covariance C. [sent-44, score-0.273]
</p><p>16 /30 = 0"0 2 , /3,;; = 0";;2 define inverse variances in the directions of elongation and contraction respectively, so that 0"5 2 0"1. [sent-45, score-0.361]
</p><p>17 Notice that according to the constraint considerations /30 < /3,;;, and all elements of ware real-valued. [sent-47, score-0.048]
</p><p>18 Note the similarity of (2) with expression for the covariance of the data of a 1factor probabilistic principal component analysis model ( = 0"21d + ww T (Tipping and Bishop, 1999) , where 0"2 is the variance of the factor-independent spherical Gaussian noise. [sent-48, score-0.417]
</p><p>19 The only difference is that it is the inverse covariance matrix for the constrained Gaussian model rather than the covariance matrix which has the structure of a rank-1 update to a multiple of Id . [sent-49, score-0.562]
</p><p>20 2  Covariance of the PoGP Model  We now consider a product of m GP experts, each of which is contracted in a single dimension. [sent-51, score-0.172]
</p><p>21 We will refer to the model as a (I,m) PoGP, where 1 represents the number of directions of contraction of each expert. [sent-52, score-0.167]
</p><p>22 We also assume that all experts have identical means. [sent-53, score-0.161]
</p><p>23 From (1), the inverse covariance of the the resulting (I,m) PoGP model can be expressed as m  C;;l  =L  Ci l  (3)  i=l where columns of We Rdxm correspond to weight vectors of the m PoGP experts, and (3E = 2::1 (3~i) > o. [sent-54, score-0.311]
</p><p>24 In Williams and Agakov (2001) it is shown that stationarity of the log-likelihood with respect to the weight matrix Wand the noise parameter (3E results in three classes of solutions for the experts' weight matrix, namely W  5 5W  0; CE ; CEW, W:j:. [sent-58, score-0.081]
</p><p>25 CE ,  (4)  where 5 is the covariance matrix of the data (with an assumed mean of zero). [sent-60, score-0.212]
</p><p>26 In Appendix A and Williams and Agakov (2001) it is shown that the maximum likelihood solution for WML is given by:  (5) where R c Rmxm is an arbitrary rotation matrix, A is a m x m matrix containing the m smallest eigenvalues of 5 and U = [Ul , . [sent-62, score-0.196]
</p><p>27 ,u m ] c Rdxm is a matrix of the corresponding eigenvectors of 5. [sent-65, score-0.12]
</p><p>28 Thus, the maximum likelihood solution for the weights of the (1, m) PoG P model corresponds to m scaled and rotated minor eigenvectors of the sample covariance 5 and leads to a probabilistic model of minor component analysis. [sent-66, score-0.767]
</p><p>29 As in the PPCA model, the number of experts m is assumed to be lower than the dimension of the data space d. [sent-67, score-0.161]
</p><p>30 The correctness of this derivation has been confirmed experimentally by using a scaled conjugate gradient search to optimize the log likelihood as a function of W and (3E. [sent-68, score-0.09]
</p><p>31 4  Discussion of PoGP model  An intuitive interpretation of the PoGP model is as follows: Each Gaussian pancake imposes an approximate linear constraint in x space. [sent-70, score-0.171]
</p><p>32 Such a linear constraint is that x should lie close to a particular hyperplane. [sent-71, score-0.022]
</p><p>33 The conjunction of these constraints is given by the product of the Gaussian pancakes. [sent-72, score-0.081]
</p><p>34 If m « d it will make sense to lBecause equation 3 has the form of a factor analysis decomposition, but for the inverse covariance matrix, we sometimes refer to PoGP as the rotcaf model. [sent-73, score-0.262]
</p><p>35 define the resulting Gaussian distribution in terms of the constraints. [sent-74, score-0.027]
</p><p>36 However, if there are many constraints (m > d/2) then it can be more efficient to describe the directions of large variability using a PPCA model, rather than the directions of small variability using a PoGP model. [sent-75, score-0.178]
</p><p>37 (1991) in what they call the "Dual Subspace Pattern Recognition Method" where both PCA and MCA models are used (although their work does not use explicit probabilistic models such as PPCA and PoGP). [sent-77, score-0.135]
</p><p>38 MCA can be used , for example, for signal extraction in digital signal processing (Oja, 1992), dimensionality reduction, and data visualization. [sent-78, score-0.031]
</p><p>39 Extraction of the minor component is also used in the Pisarenko Harmonic Decomposition method for detecting sinusoids in white noise (see, e. [sent-79, score-0.159]
</p><p>40 2  Products of PPCA  In this section we analyze a product of m I-factor PPCA models, and compare it to am-factor PPCA model. [sent-84, score-0.081]
</p><p>41 1  I-factor PPCA model  Consider a I-factor PPCA model, having a latent variable Si and visible variables x. [sent-86, score-0.069]
</p><p>42 The joint distribution is given by P(Si, x) = P(si) P(xlsi). [sent-87, score-0.022]
</p><p>43 Integrating out Si we find that Pi(x) '" N(O, Ci ) where C = wiwT + (]"21d and  (6) where (3 = (]"-2 and "(i = (3/(1 + (3 llwi W). [sent-89, score-0.026]
</p><p>44 (3 and "(i are the inverse variances in the directions of contraction and elongation respectively. [sent-90, score-0.334]
</p><p>45 The joint distribution of Si and x is given by  (7)  [s;  (3 exp - - - - 2x T WiSi 2 "(i  + XT X]  . [sent-91, score-0.022]
</p><p>46 (8)  Tipping and Bishop (1999) showed that the general m-factor PPCA model (mPPCA) has covariance C = (]"21d + WW T , where W is the d x m matrix of factor loadings. [sent-92, score-0.241]
</p><p>47 When fitting this model to data, the maximum likelihood solution is to choose W proportional to the principal components of the data covariance matrix. [sent-93, score-0.378]
</p><p>48 2  Products of I-factor PPCA models  We now consider the product of m I-factor PPCA models, which we denote a (1, m)-PoPPCA model. [sent-95, score-0.116]
</p><p>49 Thus we see that the distribution of z is Gaussian with inverse covariance matrix 13M, where  -W)  r -1  (10)  ,  and r = diag("(l , . [sent-100, score-0.321]
</p><p>50 ,"(m)' Using the inversion equations for partitioned matrices (Press et al. [sent-103, score-0.042]
</p><p>51 77) we can show that  (11) where ~xx is the covariance of the x variables under this model. [sent-105, score-0.153]
</p><p>52 It is easy to confirm that this is also the result obtained from summing (6) over i = 1, . [sent-106, score-0.02]
</p><p>53 3  Maximum Likelihood solution for PoPPCA  Am-factor PPCA model has covariance a21d + WW T and thus, by the Woodbury formula, it has inverse covariance j3 ld - j3W(a2 lm + WT W) - lW T . [sent-111, score-0.519]
</p><p>54 The maximum likelihood solution for a m-PPCA model is similar to (5), i. [sent-112, score-0.139]
</p><p>55 W = U(A _a2Im)1/2 RT, but now A is a diagonal matrix of the m principal eigenvalues, and U is a matrix of the corresponding eigenvectors. [sent-114, score-0.17]
</p><p>56 If we choose RT = I then the columns of W are orthogonal and the inverse covariance of the maximum likelihood m-PPCA model has the form j3 ld - j3WrwT. [sent-115, score-0.45]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ppca', 0.606), ('pogp', 0.455), ('mca', 0.212), ('products', 0.189), ('experts', 0.161), ('covariance', 0.153), ('pancakes', 0.152), ('minor', 0.137), ('tipping', 0.126), ('agakov', 0.121), ('poe', 0.121), ('inverse', 0.109), ('bishop', 0.096), ('ww', 0.096), ('contracted', 0.091), ('pancake', 0.091), ('edinburgh', 0.089), ('si', 0.086), ('product', 0.081), ('contraction', 0.079), ('expert', 0.077), ('gaussian', 0.071), ('probabilistic', 0.065), ('eigenvectors', 0.061), ('elongation', 0.061), ('rdxm', 0.061), ('wisi', 0.061), ('williams', 0.059), ('matrix', 0.059), ('directions', 0.059), ('permits', 0.055), ('gaussians', 0.054), ('gp', 0.053), ('principal', 0.052), ('id', 0.048), ('informatics', 0.048), ('richer', 0.048), ('likelihood', 0.046), ('elongated', 0.045), ('xt', 0.043), ('visible', 0.04), ('ld', 0.04), ('appendix', 0.038), ('ar', 0.035), ('models', 0.035), ('solution', 0.035), ('ce', 0.034), ('components', 0.034), ('pi', 0.034), ('rt', 0.032), ('construction', 0.032), ('extraction', 0.031), ('variability', 0.03), ('model', 0.029), ('maximum', 0.029), ('ci', 0.029), ('hinton', 0.028), ('define', 0.027), ('examine', 0.027), ('eigenvalues', 0.027), ('llwi', 0.026), ('universitiit', 0.026), ('lw', 0.026), ('rhs', 0.026), ('woodbury', 0.026), ('manufacturing', 0.026), ('ware', 0.026), ('viv', 0.026), ('felix', 0.026), ('variances', 0.026), ('division', 0.026), ('pca', 0.025), ('uk', 0.025), ('scaled', 0.024), ('orthogonal', 0.024), ('isi', 0.024), ('advantages', 0.023), ('rise', 0.023), ('vi', 0.022), ('chair', 0.022), ('stationarity', 0.022), ('oja', 0.022), ('joint', 0.022), ('constraint', 0.022), ('decomposition', 0.022), ('component', 0.022), ('xu', 0.021), ('completing', 0.021), ('complementary', 0.021), ('partitioned', 0.021), ('inversion', 0.021), ('stephen', 0.021), ('thorough', 0.021), ('harmonic', 0.02), ('wand', 0.02), ('correctness', 0.02), ('zt', 0.02), ('diag', 0.02), ('confirm', 0.02), ('columns', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="154-tfidf-1" href="./nips-2001-Products_of_Gaussians.html">154 nips-2001-Products of Gaussians</a></p>
<p>Author: Christopher Williams, Felix V. Agakov, Stephen N. Felderhof</p><p>Abstract: Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. Below we consider PoE models in which each expert is a Gaussian. Although the product of Gaussians is also a Gaussian, if each Gaussian has a simple structure the product can have a richer structure. We examine (1) Products of Gaussian pancakes which give rise to probabilistic Minor Components Analysis, (2) products of I-factor PPCA models and (3) a products of experts construction for an AR(l) process. Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. In this paper we consider PoE models in which each expert is a Gaussian. It is easy to see that in this case the product model will also be Gaussian. However, if each Gaussian has a simple structure, the product can have a richer structure. Using Gaussian experts is attractive as it permits a thorough analysis of the product architecture, which can be difficult with other models , e.g. models defined over discrete random variables. Below we examine three cases of the products of Gaussians construction: (1) Products of Gaussian pancakes (PoGP) which give rise to probabilistic Minor Components Analysis (MCA), providing a complementary result to probabilistic Principal Components Analysis (PPCA) obtained by Tipping and Bishop (1999); (2) Products of I-factor PPCA models; (3) A products of experts construction for an AR(l) process. Products of Gaussians If each expert is a Gaussian pi(xI8 i ) '</p><p>2 0.15120786 <a title="154-tfidf-2" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>Author: Carl E. Rasmussen, Zoubin Ghahramani</p><p>Abstract: We present an extension to the Mixture of Experts (ME) model, where the individual experts are Gaussian Process (GP) regression models. Using an input-dependent adaptation of the Dirichlet Process, we implement a gating network for an inﬁnite number of Experts. Inference in this model may be done efﬁciently using a Markov Chain relying on Gibbs sampling. The model allows the effective covariance function to vary with the inputs, and may handle large datasets – thus potentially overcoming two of the biggest hurdles with GP models. Simulations show the viability of this approach.</p><p>3 0.10597898 <a title="154-tfidf-3" href="./nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables.html">153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</a></p>
<p>Author: Brendan J. Frey, Anitha Kannan, Nebojsa Jojic</p><p>Abstract: Factor analysis and principal components analysis can be used to model linear relationships between observed variables and linearly map high-dimensional data to a lower-dimensional hidden space. In factor analysis, the observations are modeled as a linear combination of normally distributed hidden variables. We describe a nonlinear generalization of factor analysis , called</p><p>4 0.077887334 <a title="154-tfidf-4" href="./nips-2001-A_Parallel_Mixture_of_SVMs_for_Very_Large_Scale_Problems.html">16 nips-2001-A Parallel Mixture of SVMs for Very Large Scale Problems</a></p>
<p>Author: Ronan Collobert, Samy Bengio, Yoshua Bengio</p><p>Abstract: Support Vector Machines (SVMs) are currently the state-of-the-art models for many classification problems but they suffer from the complexity of their training algorithm which is at least quadratic with respect to the number of examples. Hence, it is hopeless to try to solve real-life problems having more than a few hundreds of thousands examples with SVMs. The present paper proposes a new mixture of SVMs that can be easily implemented in parallel and where each SVM is trained on a small subset of the whole dataset. Experiments on a large benchmark dataset (Forest) as well as a difficult speech database , yielded significant time improvement (time complexity appears empirically to locally grow linearly with the number of examples) . In addition, and that is a surprise, a significant improvement in generalization was observed on Forest. 1</p><p>5 0.074513845 <a title="154-tfidf-5" href="./nips-2001-Analysis_of_Sparse_Bayesian_Learning.html">35 nips-2001-Analysis of Sparse Bayesian Learning</a></p>
<p>Author: Anita C. Faul, Michael E. Tipping</p><p>Abstract: The recent introduction of the 'relevance vector machine' has effectively demonstrated how sparsity may be obtained in generalised linear models within a Bayesian framework. Using a particular form of Gaussian parameter prior, 'learning' is the maximisation, with respect to hyperparameters, of the marginal likelihood of the data. This paper studies the properties of that objective function, and demonstrates that conditioned on an individual hyperparameter, the marginal likelihood has a unique maximum which is computable in closed form. It is further shown that if a derived 'sparsity criterion' is satisfied, this maximum is exactly equivalent to 'pruning' the corresponding parameter from the model. 1</p><p>6 0.067125261 <a title="154-tfidf-6" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>7 0.062839136 <a title="154-tfidf-7" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>8 0.056554358 <a title="154-tfidf-8" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>9 0.054940127 <a title="154-tfidf-9" href="./nips-2001-Spectral_Relaxation_for_K-means_Clustering.html">171 nips-2001-Spectral Relaxation for K-means Clustering</a></p>
<p>10 0.053512916 <a title="154-tfidf-10" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>11 0.051880319 <a title="154-tfidf-11" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>12 0.049037535 <a title="154-tfidf-12" href="./nips-2001-Gaussian_Process_Regression_with_Mismatched_Models.html">79 nips-2001-Gaussian Process Regression with Mismatched Models</a></p>
<p>13 0.048615035 <a title="154-tfidf-13" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>14 0.048588842 <a title="154-tfidf-14" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>15 0.047364295 <a title="154-tfidf-15" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>16 0.042940024 <a title="154-tfidf-16" href="./nips-2001-On_the_Concentration_of_Spectral_Properties.html">136 nips-2001-On the Concentration of Spectral Properties</a></p>
<p>17 0.041168593 <a title="154-tfidf-17" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>18 0.041074585 <a title="154-tfidf-18" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>19 0.040405441 <a title="154-tfidf-19" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>20 0.039710473 <a title="154-tfidf-20" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.119), (1, 0.029), (2, -0.02), (3, -0.111), (4, -0.012), (5, -0.011), (6, 0.02), (7, 0.015), (8, 0.016), (9, -0.035), (10, 0.046), (11, 0.051), (12, 0.028), (13, -0.159), (14, -0.021), (15, 0.008), (16, 0.012), (17, 0.007), (18, 0.108), (19, 0.006), (20, 0.015), (21, -0.08), (22, 0.16), (23, 0.084), (24, -0.001), (25, -0.06), (26, -0.021), (27, -0.105), (28, 0.033), (29, -0.091), (30, 0.054), (31, 0.087), (32, 0.139), (33, -0.084), (34, 0.012), (35, 0.142), (36, -0.041), (37, 0.009), (38, -0.043), (39, -0.029), (40, 0.102), (41, -0.142), (42, 0.084), (43, -0.143), (44, 0.107), (45, 0.02), (46, 0.038), (47, 0.121), (48, -0.024), (49, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92836356 <a title="154-lsi-1" href="./nips-2001-Products_of_Gaussians.html">154 nips-2001-Products of Gaussians</a></p>
<p>Author: Christopher Williams, Felix V. Agakov, Stephen N. Felderhof</p><p>Abstract: Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. Below we consider PoE models in which each expert is a Gaussian. Although the product of Gaussians is also a Gaussian, if each Gaussian has a simple structure the product can have a richer structure. We examine (1) Products of Gaussian pancakes which give rise to probabilistic Minor Components Analysis, (2) products of I-factor PPCA models and (3) a products of experts construction for an AR(l) process. Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. In this paper we consider PoE models in which each expert is a Gaussian. It is easy to see that in this case the product model will also be Gaussian. However, if each Gaussian has a simple structure, the product can have a richer structure. Using Gaussian experts is attractive as it permits a thorough analysis of the product architecture, which can be difficult with other models , e.g. models defined over discrete random variables. Below we examine three cases of the products of Gaussians construction: (1) Products of Gaussian pancakes (PoGP) which give rise to probabilistic Minor Components Analysis (MCA), providing a complementary result to probabilistic Principal Components Analysis (PPCA) obtained by Tipping and Bishop (1999); (2) Products of I-factor PPCA models; (3) A products of experts construction for an AR(l) process. Products of Gaussians If each expert is a Gaussian pi(xI8 i ) '</p><p>2 0.66919076 <a title="154-lsi-2" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>Author: Carl E. Rasmussen, Zoubin Ghahramani</p><p>Abstract: We present an extension to the Mixture of Experts (ME) model, where the individual experts are Gaussian Process (GP) regression models. Using an input-dependent adaptation of the Dirichlet Process, we implement a gating network for an inﬁnite number of Experts. Inference in this model may be done efﬁciently using a Markov Chain relying on Gibbs sampling. The model allows the effective covariance function to vary with the inputs, and may handle large datasets – thus potentially overcoming two of the biggest hurdles with GP models. Simulations show the viability of this approach.</p><p>3 0.4593603 <a title="154-lsi-3" href="./nips-2001-Analysis_of_Sparse_Bayesian_Learning.html">35 nips-2001-Analysis of Sparse Bayesian Learning</a></p>
<p>Author: Anita C. Faul, Michael E. Tipping</p><p>Abstract: The recent introduction of the 'relevance vector machine' has effectively demonstrated how sparsity may be obtained in generalised linear models within a Bayesian framework. Using a particular form of Gaussian parameter prior, 'learning' is the maximisation, with respect to hyperparameters, of the marginal likelihood of the data. This paper studies the properties of that objective function, and demonstrates that conditioned on an individual hyperparameter, the marginal likelihood has a unique maximum which is computable in closed form. It is further shown that if a derived 'sparsity criterion' is satisfied, this maximum is exactly equivalent to 'pruning' the corresponding parameter from the model. 1</p><p>4 0.45484555 <a title="154-lsi-4" href="./nips-2001-A_Parallel_Mixture_of_SVMs_for_Very_Large_Scale_Problems.html">16 nips-2001-A Parallel Mixture of SVMs for Very Large Scale Problems</a></p>
<p>Author: Ronan Collobert, Samy Bengio, Yoshua Bengio</p><p>Abstract: Support Vector Machines (SVMs) are currently the state-of-the-art models for many classification problems but they suffer from the complexity of their training algorithm which is at least quadratic with respect to the number of examples. Hence, it is hopeless to try to solve real-life problems having more than a few hundreds of thousands examples with SVMs. The present paper proposes a new mixture of SVMs that can be easily implemented in parallel and where each SVM is trained on a small subset of the whole dataset. Experiments on a large benchmark dataset (Forest) as well as a difficult speech database , yielded significant time improvement (time complexity appears empirically to locally grow linearly with the number of examples) . In addition, and that is a surprise, a significant improvement in generalization was observed on Forest. 1</p><p>5 0.3884708 <a title="154-lsi-5" href="./nips-2001-Estimating_Car_Insurance_Premia%3A_a_Case_Study_in_High-Dimensional_Data_Inference.html">70 nips-2001-Estimating Car Insurance Premia: a Case Study in High-Dimensional Data Inference</a></p>
<p>Author: Nicolas Chapados, Yoshua Bengio, Pascal Vincent, Joumana Ghosn, Charles Dugas, Ichiro Takeuchi, Linyan Meng</p><p>Abstract: Estimating insurance premia from data is a difficult regression problem for several reasons: the large number of variables, many of which are .discrete, and the very peculiar shape of the noise distribution, asymmetric with fat tails, with a large majority zeros and a few unreliable and very large values. We compare several machine learning methods for estimating insurance premia, and test them on a large data base of car insurance policies. We find that function approximation methods that do not optimize a squared loss, like Support Vector Machines regression, do not work well in this context. Compared methods include decision trees and generalized linear models. The best results are obtained with a mixture of experts, which better identifies the least and most risky contracts, and allows to reduce the median premium by charging more to the most risky customers. 1</p><p>6 0.37557879 <a title="154-lsi-6" href="./nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables.html">153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</a></p>
<p>7 0.3386668 <a title="154-lsi-7" href="./nips-2001-Geometrical_Singularities_in_the_Neuromanifold_of_Multilayer_Perceptrons.html">83 nips-2001-Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons</a></p>
<p>8 0.33776173 <a title="154-lsi-8" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>9 0.33625895 <a title="154-lsi-9" href="./nips-2001-Gaussian_Process_Regression_with_Mismatched_Models.html">79 nips-2001-Gaussian Process Regression with Mismatched Models</a></p>
<p>10 0.31942844 <a title="154-lsi-10" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>11 0.31144246 <a title="154-lsi-11" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>12 0.30001438 <a title="154-lsi-12" href="./nips-2001-Spectral_Relaxation_for_K-means_Clustering.html">171 nips-2001-Spectral Relaxation for K-means Clustering</a></p>
<p>13 0.29856491 <a title="154-lsi-13" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>14 0.29825386 <a title="154-lsi-14" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>15 0.29809079 <a title="154-lsi-15" href="./nips-2001-Learning_Body_Pose_via_Specialized_Maps.html">108 nips-2001-Learning Body Pose via Specialized Maps</a></p>
<p>16 0.27336761 <a title="154-lsi-16" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>17 0.26232797 <a title="154-lsi-17" href="./nips-2001-On_the_Concentration_of_Spectral_Properties.html">136 nips-2001-On the Concentration of Spectral Properties</a></p>
<p>18 0.2583636 <a title="154-lsi-18" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>19 0.24588302 <a title="154-lsi-19" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>20 0.24518499 <a title="154-lsi-20" href="./nips-2001-Characterizing_Neural_Gain_Control_using_Spike-triggered_Covariance.html">48 nips-2001-Characterizing Neural Gain Control using Spike-triggered Covariance</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.024), (17, 0.032), (19, 0.023), (27, 0.117), (30, 0.035), (38, 0.027), (43, 0.011), (59, 0.087), (72, 0.04), (79, 0.043), (83, 0.028), (87, 0.354), (91, 0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73272735 <a title="154-lda-1" href="./nips-2001-Products_of_Gaussians.html">154 nips-2001-Products of Gaussians</a></p>
<p>Author: Christopher Williams, Felix V. Agakov, Stephen N. Felderhof</p><p>Abstract: Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. Below we consider PoE models in which each expert is a Gaussian. Although the product of Gaussians is also a Gaussian, if each Gaussian has a simple structure the product can have a richer structure. We examine (1) Products of Gaussian pancakes which give rise to probabilistic Minor Components Analysis, (2) products of I-factor PPCA models and (3) a products of experts construction for an AR(l) process. Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. In this paper we consider PoE models in which each expert is a Gaussian. It is easy to see that in this case the product model will also be Gaussian. However, if each Gaussian has a simple structure, the product can have a richer structure. Using Gaussian experts is attractive as it permits a thorough analysis of the product architecture, which can be difficult with other models , e.g. models defined over discrete random variables. Below we examine three cases of the products of Gaussians construction: (1) Products of Gaussian pancakes (PoGP) which give rise to probabilistic Minor Components Analysis (MCA), providing a complementary result to probabilistic Principal Components Analysis (PPCA) obtained by Tipping and Bishop (1999); (2) Products of I-factor PPCA models; (3) A products of experts construction for an AR(l) process. Products of Gaussians If each expert is a Gaussian pi(xI8 i ) '</p><p>2 0.57883644 <a title="154-lda-2" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>Author: Hilbert J. Kappen, Wim Wiegerinck</p><p>Abstract: The Cluster Variation method is a class of approximation methods containing the Bethe and Kikuchi approximations as special cases. We derive two novel iteration schemes for the Cluster Variation Method. One is a fixed point iteration scheme which gives a significant improvement over loopy BP, mean field and TAP methods on directed graphical models. The other is a gradient based method, that is guaranteed to converge and is shown to give useful results on random graphs with mild frustration. We conclude that the methods are of significant practical value for large inference problems. 1</p><p>3 0.44423643 <a title="154-lda-3" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>Author: Dimitris Achlioptas, Frank Mcsherry, Bernhard Schölkopf</p><p>Abstract: We propose randomized techniques for speeding up Kernel Principal Component Analysis on three levels: sampling and quantization of the Gram matrix in training, randomized rounding in evaluating the kernel expansions, and random projections in evaluating the kernel itself. In all three cases, we give sharp bounds on the accuracy of the obtained approximations. Rather intriguingly, all three techniques can be viewed as instantiations of the following idea: replace the kernel function by a “randomized kernel” which behaves like in expectation.</p><p>4 0.43694124 <a title="154-lda-4" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>Author: Lehel Csató, Manfred Opper, Ole Winther</p><p>Abstract: The adaptive TAP Gibbs free energy for a general densely connected probabilistic model with quadratic interactions and arbritary single site constraints is derived. We show how a speciﬁc sequential minimization of the free energy leads to a generalization of Minka’s expectation propagation. Lastly, we derive a sparse representation version of the sequential algorithm. The usefulness of the approach is demonstrated on classiﬁcation and density estimation with Gaussian processes and on an independent component analysis problem.</p><p>5 0.43096757 <a title="154-lda-5" href="./nips-2001-Learning_Body_Pose_via_Specialized_Maps.html">108 nips-2001-Learning Body Pose via Specialized Maps</a></p>
<p>Author: Rómer Rosales, Stan Sclaroff</p><p>Abstract: A nonlinear supervised learning model, the Specialized Mappings Architecture (SMA), is described and applied to the estimation of human body pose from monocular images. The SMA consists of several specialized forward mapping functions and an inverse mapping function. Each specialized function maps certain domains of the input space (image features) onto the output space (body pose parameters). The key algorithmic problems faced are those of learning the specialized domains and mapping functions in an optimal way, as well as performing inference given inputs and knowledge of the inverse function. Solutions to these problems employ the EM algorithm and alternating choices of conditional independence assumptions. Performance of the approach is evaluated with synthetic and real video sequences of human motion. 1</p><p>6 0.4191196 <a title="154-lda-6" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>7 0.41783363 <a title="154-lda-7" href="./nips-2001-Multi_Dimensional_ICA_to_Separate_Correlated_Sources.html">127 nips-2001-Multi Dimensional ICA to Separate Correlated Sources</a></p>
<p>8 0.41460061 <a title="154-lda-8" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>9 0.41313794 <a title="154-lda-9" href="./nips-2001-The_Unified_Propagation_and_Scaling_Algorithm.html">188 nips-2001-The Unified Propagation and Scaling Algorithm</a></p>
<p>10 0.4118439 <a title="154-lda-10" href="./nips-2001-Blind_Source_Separation_via_Multinode_Sparse_Representation.html">44 nips-2001-Blind Source Separation via Multinode Sparse Representation</a></p>
<p>11 0.41036525 <a title="154-lda-11" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>12 0.40825748 <a title="154-lda-12" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>13 0.40815702 <a title="154-lda-13" href="./nips-2001-Kernel_Feature_Spaces_and_Nonlinear_Blind_Souce_Separation.html">103 nips-2001-Kernel Feature Spaces and Nonlinear Blind Souce Separation</a></p>
<p>14 0.4066003 <a title="154-lda-14" href="./nips-2001-Quantizing_Density_Estimators.html">155 nips-2001-Quantizing Density Estimators</a></p>
<p>15 0.40558878 <a title="154-lda-15" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>16 0.40398848 <a title="154-lda-16" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>17 0.40353516 <a title="154-lda-17" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<p>18 0.40326017 <a title="154-lda-18" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>19 0.40317866 <a title="154-lda-19" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>20 0.40252706 <a title="154-lda-20" href="./nips-2001-Sequential_Noise_Compensation_by_Sequential_Monte_Carlo_Method.html">168 nips-2001-Sequential Noise Compensation by Sequential Monte Carlo Method</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
