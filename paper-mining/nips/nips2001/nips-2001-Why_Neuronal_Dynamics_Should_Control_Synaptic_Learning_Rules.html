<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-197" href="#">nips2001-197</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</h1>
<br/><p>Source: <a title="nips-2001-197-pdf" href="http://papers.nips.cc/paper/1957-why-neuronal-dynamics-should-control-synaptic-learning-rules.pdf">pdf</a></p><p>Author: Jesper Tegnér, Ádám Kepecs</p><p>Abstract: Hebbian learning rules are generally formulated as static rules. Under changing condition (e.g. neuromodulation, input statistics) most rules are sensitive to parameters. In particular, recent work has focused on two different formulations of spike-timing-dependent plasticity rules. Additive STDP [1] is remarkably versatile but also very fragile, whereas multiplicative STDP [2, 3] is more robust but lacks attractive features such as synaptic competition and rate stabilization. Here we address the problem of robustness in the additive STDP rule. We derive an adaptive control scheme, where the learning function is under fast dynamic control by postsynaptic activity to stabilize learning under a variety of conditions. Such a control scheme can be implemented using known biophysical mechanisms of synapses. We show that this adaptive rule makes the addit ive STDP more robust. Finally, we give an example how meta plasticity of the adaptive rule can be used to guide STDP into different type of learning regimes. 1</p><p>Reference: <a title="nips-2001-197-reference" href="../nips2001_reference/nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Why neuronal dynamics should control synaptic learning rules  Jesper Tegner Stockholm Bioinformatics Center Dept. [sent-1, score-0.712]
</p><p>2 Additive STDP [1] is remarkably versatile but also very fragile, whereas multiplicative STDP [2, 3] is more robust but lacks attractive features such as synaptic competition and rate stabilization. [sent-10, score-0.64]
</p><p>3 We derive an adaptive control scheme, where the learning function is under fast dynamic control by postsynaptic activity to stabilize learning under a variety of conditions. [sent-12, score-0.683]
</p><p>4 We show that this adaptive rule makes the addit ive STDP more robust. [sent-14, score-0.298]
</p><p>5 Finally, we give an example how meta plasticity of the adaptive rule can be used to guide STDP into different type of learning regimes. [sent-15, score-0.461]
</p><p>6 1  Introduction  Hebbian learning rules are widely used to model synaptic modification shaping the functional connectivity of neural networks [4, 5]. [sent-16, score-0.46]
</p><p>7 Recent experiments revealed a mode of synaptic plasticity that provides new possibilities and constraints for synaptic learning rules [7, 8, 9]. [sent-18, score-0.979]
</p><p>8 It has been found that synapses are strengthened if a presynaptic spike precedes a postsynaptic spike within a short (::::: 20 ms) time window, while the reverse spike order leads to synaptic weakening. [sent-19, score-1.083]
</p><p>9 Computational models highlighted how STDP combines synaptic strengthening and weakening so that learning gives rise to synaptic competition in a way that neuronal firing rates are stabilized. [sent-21, score-1.514]
</p><p>10 Recent modeling studies have, however, demonstrated that whether an STDP type  rule results in competition or rate stabilization depends on exact formulation of the weight update scheme [3, 2]. [sent-22, score-0.508]
</p><p>11 In the additive version of an STDP update rule studied by Abbott and coworkers [1, 10], the magnitude of synaptic change is independent on synaptic strength. [sent-24, score-0.981]
</p><p>12 For this version of the rule (aSTDP), the steady-state synaptic weight distribution is bimodal. [sent-26, score-0.56]
</p><p>13 In sharp contrast to this, using a multiplicative STDP rule where the amount of weight increase scales inversely with present weight size produces neither synaptic competition nor rate normalization [3, 2]. [sent-27, score-0.923]
</p><p>14 In this multiplicative scenario the synaptic weight distribution is unimodal. [sent-28, score-0.479]
</p><p>15 Activity-dependent synaptic scaling has recently been proposed as a separate mechanism to ensure synaptic competition operating on a slow (days) time scale [3]. [sent-29, score-0.902]
</p><p>16 In the first section we show that the aSTDP rule normalizes postsynaptic firing rates only in a limited parameter range. [sent-32, score-0.792]
</p><p>17 The critical parameter of aSTDP becomes the ratio (0;) between the amount of synaptic depression and potentiation. [sent-33, score-0.595]
</p><p>18 This lead us to consider an adaptive version of aSTDP in order to create a rule that is both competitive as well as rate stabilizing under different circumstances. [sent-35, score-0.448]
</p><p>19 Next, we use a Fokker-Planck formalism to clarify what determines when an additive STDP rule fails to stabilize the postsynaptic firing rate. [sent-36, score-0.818]
</p><p>20 Here we derive the requirement for how the potentiation to depression ratio should change with neuronal activity. [sent-37, score-0.424]
</p><p>21 In the last section we provide a biologically realistic implementation of the adaptive rule and perform numerical simulations to show the how different parameterizations of the adaptive rule can guide STDP into differentially rate-sensitive regimes. [sent-38, score-0.683]
</p><p>22 The integral over the temporal window of the synaptic learning function (L) is always negative. [sent-44, score-0.449]
</p><p>23 Correlations between input rates were generated by adding a common bias rate in a graded manner across synapses so that the first afferent is has zero while the last afferent has the maximal correlation, Cmax . [sent-46, score-0.477]
</p><p>24 We first examine how the depression/potentiation ratio (0; = LTD / LT P) [2] controls the dependence of the output firing rate on the synaptic input rate, here referred to as the effective neuronal gain. [sent-47, score-1.365]
</p><p>25 Provided that 0; is sufficiently large, the STDP rule controls the postsynaptic firing rate (Fig. [sent-48, score-0.803]
</p><p>26 The stabilizing effect of the STDP rule is therefore equivalent to having weak a neuronal gain. [sent-50, score-0.359]
</p><p>27 The slope of the dependence of the postsynaptic output rate on the presynaptic input rate is referred to as the effective neuronal gain. [sent-54, score-1.25]
</p><p>28 The initial firing rate is shown by the upper curve while the lower line displays the final postsynaptic firing rate. [sent-55, score-0.883]
</p><p>29 When the synaptic input is strongly correlated the postsynaptic neuron operates in a high gain mode characterized by a larger slope and larger baseline rate. [sent-60, score-0.975]
</p><p>30 Note that for further increases in the presynaptic rates, postsynaptic firing can increase to over 1000 Hz. [sent-72, score-0.795]
</p><p>31 We find that the neuronal gain is extremely sensitive to the value of 0: as well as to the amount of afferent input correlations. [sent-83, score-0.513]
</p><p>32 Figure IB shows that increasing the amount of input correlations for a given 0: value increases the overall firing rate and the slope of the input-output curve, thus leading to larger effective gain. [sent-84, score-0.74]
</p><p>33 Increasing the amount of correlations between the synaptic afferents could therefore be interpreted as increasing the effective neuronal gain. [sent-85, score-0.809]
</p><p>34 Note that the baseline firing at a presynaptic drive of 20Hz is also increased. [sent-86, score-0.455]
</p><p>35 Next, we examined how neuronal gain depends on the value of 0: in the STDP rule (Figure IC). [sent-87, score-0.425]
</p><p>36 The high gain and high rate mode induced by strong input correlations was reduced to a lower gain and lower rate mode by increasing 0: (see arrow in Figure IC). [sent-88, score-0.769]
</p><p>37 3  Conditions for an adaptive additive STDP rule  Here we address how the learning ratio, 0:, should depend on the input rate in order to produce a given neuronal input-output relationship. [sent-90, score-0.891]
</p><p>38 Using this functional form we will be able to formulate constraints for an adaptive additive STDP rule. [sent-91, score-0.271]
</p><p>39 This will guide us in the derivation of a biophysical implementation of the adaptive control scheme. [sent-92, score-0.34]
</p><p>40 The problem in its generality is to find (i) how the learning ratio should depend on the postsynaptic rate and (ii) how the postsynaptic rate depends on the input rate and the synaptic weights. [sent-93, score-1.606]
</p><p>41 By performing self-consistent calculations using a Fokker-Planck formulation, the problem is reduced to finding conditions for how the learning ratio should depend on the input rates only. [sent-94, score-0.432]
</p><p>42 U A The output rate does not depend on the input rate. [sent-163, score-0.287]
</p><p>43 B Dependence of the mean synaptic weight on input rates . [sent-165, score-0.656]
</p><p>44 E,F A( w) and P( w) are functions of the synaptic strength and depend on the input rate . [sent-168, score-0.649]
</p><p>45 Note that eight different input rates are used but only traces 1, 3, 5, 7 are shown for A(w) and pew) in which the dashed line correspond to the case with the lowest presynaptic rate. [sent-170, score-0.432]
</p><p>46 determine how the parameter fJ = 0: - 1 should scale with presynaptic rates in order to control the neuronal gain. [sent-171, score-0.575]
</p><p>47 The Fokker-Planck formulation permits an analytic calculation of the steady state distribution of synaptic weights [3]. [sent-172, score-0.446]
</p><p>48 The competition parameter for N excitatory afferents is given by W tot = twrpreN < w > where the time window tw is defined as the probability for depression (Pd = tw/tisi) that a synaptic event occurs within the time window (tw < tisi ). [sent-173, score-0.76]
</p><p>49 Thus, A( w) determines whether a given synapse (w) will increase or decrease as a function of its synaptic weight. [sent-175, score-0.438]
</p><p>50 The steepness of the A( w) function determines the degree of synaptic competition. [sent-176, score-0.411]
</p><p>51 When Wmax > (1 - l/o:)Wtot the synaptic weight distribution is bimodal. [sent-178, score-0.442]
</p><p>52 Using these equations one can calculate how the parameter fJ  should scale with the presynaptic input rate in order to produce a given postsynaptic firing rate. [sent-181, score-0.973]
</p><p>53 At that point, the postsynaptic firing rate can be calculated. [sent-183, score-0.646]
</p><p>54 Here, instead we impose a fixed postsynaptic output rate for a given input rate and search for a self-consistent solution using (3 as a free parameter. [sent-184, score-0.664]
</p><p>55 Performing this calculation for a range of input rates provides us with the desired dependency of (3 on the presynaptic firing rate. [sent-185, score-0.669]
</p><p>56 Once a solution is reached we also examine the resulting steady state synaptic weight distribution (P(w)) and the corresponding drift term A( w) as a function of the presynaptic input rate. [sent-186, score-0.873]
</p><p>57 The neuronal gain, the ratio between the postsynaptic firing rate and the input rate is set to be zero (Fig 2A). [sent-188, score-1.203]
</p><p>58 To normalize postsynaptic firing rates the average synaptic weight has to decrease in order to compensate for the increasing presynaptic firing rate. [sent-189, score-1.586]
</p><p>59 The condition for a zero neuronal gain is that the average synaptic weight should decrease as 1 j r pre . [sent-191, score-0.802]
</p><p>60 The dependence of A( w) and the synaptic weight distribution P( w) on different presynaptic rates is illustrated in Fig 2E and F. [sent-195, score-0.826]
</p><p>61 As the presynaptic rates increase, the A(w) function is lowered (dashed line indicates the smallest presynaptic rate), thus pushing more synapses to smaller values since they experience a net negative "force field". [sent-196, score-0.598]
</p><p>62 This is also reflected in the synaptic weight distribution which is pushed to the lower boundary as the input rates increase. [sent-197, score-0.656]
</p><p>63 When enforcing a different neuronal gain, the dependence of the (3 term on the presynaptic rates remains approximately linear but with a different slope (not shown). [sent-198, score-0.663]
</p><p>64 4  Derivation of an adaptive learning rule with biophysical components  The key insight from the above calculations is the observed linear dependence of (3 on presynaptic rates. [sent-199, score-0.71]
</p><p>65 However, when implementing an adaptive rule with biophysical elements it is very likely that individual components will have a non-linear dependence on each other. [sent-200, score-0.443]
</p><p>66 A natural solution would be to use postsynaptic calcium to measure the postsynaptic firing and therefore indirectly the presynaptic firing rate. [sent-206, score-1.43]
</p><p>67 Moreover, the asymmetry ((3) of the learning ratio could depend on the level of postsynaptic calcium. [sent-207, score-0.458]
</p><p>68 It is known that increased resting calcium levels inhibit NMDA channels and thus calcium influx due to synaptic input. [sent-208, score-0.714]
</p><p>69 A biophysical formulation of the above scheme is the following  200  i-  No Adaptive Tracking  150  :WlUlliWWU]  2  ~-40  >  -60  0~----------~5~00~--------~10~0~0----------~1~500  ~loo  5  o  '·'r ~A_  50  ~'ll  Adaptive Tracking  20  40  60  input rat. [sent-213, score-0.284]
</p><p>70 When the STDP rule is extended with an adaptive control loop , the output rates are normalized in the presence of correlated input. [sent-216, score-0.497]
</p><p>71 Since (3 tracks changes in intracellular calcium on a rapid time-scale, every spike experiences a different learning ratio, 0:. [sent-218, score-0.347]
</p><p>72 Note that the adaptive scheme approximates the learning ratio (0: = 1. [sent-219, score-0.384]
</p><p>73 (4)  d(3  T(3 -  dt  = - (3  + [Ca]q  (5)  The parameter p determines how the calcium concentration scales with the postsynaptic firing rate (delta spikes r5 above) and q controls the learning sensitivity. [sent-221, score-0.9]
</p><p>74 "( controls the rise of steady-state calcium with increasing postsynaptic rates (rpost). [sent-222, score-0.655]
</p><p>75 The time constants TCa and T(3 determine the calcium dynamics and the time course of the adaptive rule respectively. [sent-223, score-0.462]
</p><p>76 The neuronal gain can parameter T Moreover , the drift term  (6) for (3 < < 1. [sent-226, score-0.344]
</p><p>77 Note also, that when W max > [TCa"(r~ost ]qWtot there is a bimodal synaptic weight distribution and synaptic competition is preserved. [sent-229, score-0.923]
</p><p>78 O)  1\1 :::J  Co :::J  0  0  0  25  50  75  100  0  0  Input rate (Hz)  25  50  75  100  Input rate (Hz)  0  0  25  50  75  100  Input rate (Hz)  F igure 4: Full numerical simulation of the adaptive additive STDP rule. [sent-246, score-0.661]
</p><p>79 5  Numerical simulations  Next, we examine whether the theory of adaptive normalization carryover to a full scale simulation of the integrate-and-fire model with the STDP rule and the biophysical adaptive scheme as described above. [sent-256, score-0.66]
</p><p>80 Driving a neuron with increasing input rates increases the output rate significantly when there is no adaptive scheme (squares, Figure 3 Left) as observed previously (cf. [sent-259, score-0.691]
</p><p>81 Adding the adaptive loop normalizes the output rates (circles, Figure 3 Left). [sent-261, score-0.385]
</p><p>82 This simulation shows that the average postsynaptic firing rate is regulated by the adaptive tracking scheme. [sent-262, score-0.884]
</p><p>83 This is expected since the Fokker-Planck analysis is based on the steady-state synaptic weight distribution. [sent-263, score-0.442]
</p><p>84 To further gain insight into the operation of the adaptive loop we examined the spike-to-spike dependence of the tracking scheme. [sent-264, score-0.424]
</p><p>85 The adaptive rule tracks fast changes in firing by adjusting the learning ratio for each spike. [sent-266, score-0.784]
</p><p>86 Our fast , spike-to-spike tracking scheme is in contrast to other homeostatic mechanisms operating on the time-scale of hours to days [11 , 12, 13, 14]. [sent-270, score-0.291]
</p><p>87 In our formulation , the learning ratio, via (3, tracks changes in intra-cellular calcium, which in turn reflects the instantaneous firing rate. [sent-271, score-0.373]
</p><p>88 Slower homeostatic mechanisms are unable to detect these rapid changes in firing statistics. [sent-272, score-0.381]
</p><p>89 Because this fast adaptive scheme depends on recent neuronal firing, pairing several spikes on the time-scale comparable to the calcium dynamics introduces non-linear summation effects. [sent-273, score-0.648]
</p><p>90 Neurons with this adaptive STDP control loop can detect changes in the input correlation while being only weakly dependent on the presynaptic firing rate. [sent-274, score-0.856]
</p><p>91 In a different regime where we introduce increasing correlations between the synaptic inputs [1] we find that the neuronal gain is changed little with increasing input rates but increases substantially with increasing input correlations (Fig 4c) . [sent-280, score-1.385]
</p><p>92 Thus, the adaptive aSTDP rule can normalize the mean postsynaptic rate even when the input statistics change. [sent-281, score-0.816]
</p><p>93 W ith other adaptive parameters we also found learning regimes where the responses to input correlations were affected differentially (not shown). [sent-282, score-0.433]
</p><p>94 We found that STDP is very sensitive to the ratio of synaptic strengthening to weakening, (t, and requires different values for different input statistics. [sent-286, score-0.67]
</p><p>95 To correct for this, we proposed an adaptive control scheme to adjust the plasticity rule. [sent-287, score-0.384]
</p><p>96 This adaptive mechanisms makes the learning rule more robust to changing input conditions while preserving its interesting properties, such as synaptic competition. [sent-288, score-0.868]
</p><p>97 Our adaptive STDP rule adjusts the learning ratio on a millisecond time-scale. [sent-290, score-0.437]
</p><p>98 Because the learning rule changes rapidly, it is very sensitive the input statistics. [sent-292, score-0.324]
</p><p>99 Furthermore, the synaptic weight changes add non-linearly due to the rapid self-regulation. [sent-293, score-0.512]
</p><p>100 Dan, personal communication) which might have roles in making synaptic plasticity adaptive. [sent-295, score-0.486]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stdp', 0.493), ('synaptic', 0.386), ('postsynaptic', 0.287), ('firing', 0.237), ('presynaptic', 0.218), ('neuronal', 0.213), ('adaptive', 0.18), ('calcium', 0.164), ('rate', 0.122), ('rule', 0.118), ('astdp', 0.113), ('ratio', 0.113), ('input', 0.109), ('rates', 0.105), ('plasticity', 0.1), ('competition', 0.095), ('gain', 0.094), ('additive', 0.091), ('biophysical', 0.084), ('fig', 0.075), ('abbott', 0.075), ('depression', 0.07), ('correlations', 0.066), ('tca', 0.066), ('slope', 0.066), ('scheme', 0.065), ('dependence', 0.061), ('increasing', 0.06), ('stabilize', 0.06), ('tracking', 0.058), ('synapses', 0.057), ('ost', 0.057), ('thrrigiano', 0.057), ('weight', 0.056), ('pre', 0.053), ('homeostatic', 0.049), ('tot', 0.049), ('hz', 0.049), ('rules', 0.048), ('normalizes', 0.045), ('nelson', 0.045), ('spike', 0.045), ('afferent', 0.042), ('tracks', 0.042), ('changes', 0.042), ('controls', 0.039), ('control', 0.039), ('cmax', 0.038), ('roi', 0.038), ('rossum', 0.038), ('stockholm', 0.038), ('wtot', 0.038), ('drift', 0.037), ('window', 0.037), ('multiplicative', 0.037), ('guide', 0.037), ('arrow', 0.036), ('linearity', 0.036), ('song', 0.036), ('operating', 0.035), ('steady', 0.034), ('examine', 0.033), ('mode', 0.033), ('kepecs', 0.033), ('weakening', 0.033), ('destabilizing', 0.033), ('strengthening', 0.033), ('days', 0.033), ('depend', 0.032), ('hebbian', 0.032), ('regime', 0.031), ('loop', 0.031), ('afferents', 0.03), ('ltd', 0.03), ('sensitive', 0.029), ('potentiation', 0.028), ('tw', 0.028), ('stabilizing', 0.028), ('sompolinsky', 0.028), ('effective', 0.028), ('excitatory', 0.028), ('rapid', 0.028), ('increase', 0.027), ('differentially', 0.026), ('stabilization', 0.026), ('regimes', 0.026), ('increases', 0.026), ('amount', 0.026), ('fast', 0.026), ('formulation', 0.026), ('learning', 0.026), ('mechanisms', 0.025), ('determines', 0.025), ('ms', 0.025), ('foundation', 0.025), ('numerical', 0.024), ('output', 0.024), ('ic', 0.024), ('conditions', 0.024), ('calculations', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="197-tfidf-1" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>Author: Jesper Tegnér, Ádám Kepecs</p><p>Abstract: Hebbian learning rules are generally formulated as static rules. Under changing condition (e.g. neuromodulation, input statistics) most rules are sensitive to parameters. In particular, recent work has focused on two different formulations of spike-timing-dependent plasticity rules. Additive STDP [1] is remarkably versatile but also very fragile, whereas multiplicative STDP [2, 3] is more robust but lacks attractive features such as synaptic competition and rate stabilization. Here we address the problem of robustness in the additive STDP rule. We derive an adaptive control scheme, where the learning function is under fast dynamic control by postsynaptic activity to stabilize learning under a variety of conditions. Such a control scheme can be implemented using known biophysical mechanisms of synapses. We show that this adaptive rule makes the addit ive STDP more robust. Finally, we give an example how meta plasticity of the adaptive rule can be used to guide STDP into different type of learning regimes. 1</p><p>2 0.29947674 <a title="197-tfidf-2" href="./nips-2001-Citcuits_for_VLSI_Implementation_of_Temporally_Asymmetric_Hebbian_Learning.html">49 nips-2001-Citcuits for VLSI Implementation of Temporally Asymmetric Hebbian Learning</a></p>
<p>Author: A. Bofill, D. P. Thompson, Alan F. Murray</p><p>Abstract: Experimental data has shown that synaptic strength modification in some types of biological neurons depends upon precise spike timing differences between presynaptic and postsynaptic spikes. Several temporally-asymmetric Hebbian learning rules motivated by this data have been proposed. We argue that such learning rules are suitable to analog VLSI implementation. We describe an easily tunable circuit to modify the weight of a silicon spiking neuron according to those learning rules. Test results from the fabrication of the circuit using a O.6J.lm CMOS process are given. 1</p><p>3 0.21138251 <a title="197-tfidf-3" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>Author: Peter E. Latham</p><p>Abstract: Almost two decades ago , Hopfield [1] showed that networks of highly reduced model neurons can exhibit multiple attracting fixed points, thus providing a substrate for associative memory. It is still not clear, however, whether realistic neuronal networks can support multiple attractors. The main difficulty is that neuronal networks in vivo exhibit a stable background state at low firing rate, typically a few Hz. Embedding attractor is easy; doing so without destabilizing the background is not. Previous work [2, 3] focused on the sparse coding limit, in which a vanishingly small number of neurons are involved in any memory. Here we investigate the case in which the number of neurons involved in a memory scales with the number of neurons in the network. In contrast to the sparse coding limit, we find that multiple attractors can co-exist robustly with a stable background state. Mean field theory is used to understand how the behavior of the network scales with its parameters, and simulations with analog neurons are presented. One of the most important features of the nervous system is its ability to perform associative memory. It is generally believed that associative memory is implemented using attractor networks - experimental studies point in that direction [4- 7], and there are virtually no competing theoretical models. Perhaps surprisingly, however, it is still an open theoretical question whether attractors can exist in realistic neuronal networks. The</p><p>4 0.21014108 <a title="197-tfidf-4" href="./nips-2001-Information-Geometric_Decomposition_in_Spike_Analysis.html">96 nips-2001-Information-Geometric Decomposition in Spike Analysis</a></p>
<p>Author: Hiroyuki Nakahara, Shun-ichi Amari</p><p>Abstract: We present an information-geometric measure to systematically investigate neuronal firing patterns, taking account not only of the second-order but also of higher-order interactions. We begin with the case of two neurons for illustration and show how to test whether or not any pairwise correlation in one period is significantly different from that in the other period. In order to test such a hypothesis of different firing rates, the correlation term needs to be singled out 'orthogonally' to the firing rates, where the null hypothesis might not be of independent firing. This method is also shown to directly associate neural firing with behavior via their mutual information, which is decomposed into two types of information, conveyed by mean firing rate and coincident firing, respectively. Then, we show that these results, using the 'orthogonal' decomposition, are naturally extended to the case of three neurons and n neurons in general. 1</p><p>5 0.16022617 <a title="197-tfidf-5" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>Author: N. Matsumoto, M. Okada</p><p>Abstract: Recent biological experimental ﬁndings have shown that the synaptic plasticity depends on the relative timing of the pre- and postsynaptic spikes which determines whether Long Term Potentiation (LTP) occurs or Long Term Depression (LTD) does. The synaptic plasticity has been called “Temporally Asymmetric Hebbian plasticity (TAH)”. Many authors have numerically shown that spatiotemporal patterns can be stored in neural networks. However, the mathematical mechanism for storage of the spatio-temporal patterns is still unknown, especially the eﬀects of LTD. In this paper, we employ a simple neural network model and show that interference of LTP and LTD disappears in a sparse coding scheme. On the other hand, it is known that the covariance learning is indispensable for storing sparse patterns. We also show that TAH qualitatively has the same eﬀect as the covariance learning when spatio-temporal patterns are embedded in the network. 1</p><p>6 0.15353589 <a title="197-tfidf-6" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>7 0.13397725 <a title="197-tfidf-7" href="./nips-2001-Learning_Spike-Based_Correlations_and_Conditional_Probabilities_in_Silicon.html">112 nips-2001-Learning Spike-Based Correlations and Conditional Probabilities in Silicon</a></p>
<p>8 0.13225819 <a title="197-tfidf-8" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>9 0.11869392 <a title="197-tfidf-9" href="./nips-2001-3_state_neurons_for_contextual_processing.html">2 nips-2001-3 state neurons for contextual processing</a></p>
<p>10 0.095904484 <a title="197-tfidf-10" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>11 0.073711492 <a title="197-tfidf-11" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>12 0.070067525 <a title="197-tfidf-12" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>13 0.068844371 <a title="197-tfidf-13" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>14 0.066761523 <a title="197-tfidf-14" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>15 0.065644875 <a title="197-tfidf-15" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>16 0.058698501 <a title="197-tfidf-16" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>17 0.05816194 <a title="197-tfidf-17" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>18 0.053209037 <a title="197-tfidf-18" href="./nips-2001-Orientational_and_Geometric_Determinants_of_Place_and_Head-direction.html">142 nips-2001-Orientational and Geometric Determinants of Place and Head-direction</a></p>
<p>19 0.051123906 <a title="197-tfidf-19" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>20 0.050748195 <a title="197-tfidf-20" href="./nips-2001-Modeling_the_Modulatory_Effect_of_Attention_on_Human_Spatial_Vision.html">124 nips-2001-Modeling the Modulatory Effect of Attention on Human Spatial Vision</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.154), (1, -0.251), (2, -0.137), (3, 0.095), (4, 0.163), (5, 0.037), (6, 0.179), (7, -0.052), (8, -0.094), (9, -0.06), (10, 0.088), (11, -0.203), (12, 0.152), (13, -0.027), (14, -0.045), (15, 0.034), (16, 0.014), (17, -0.074), (18, -0.01), (19, -0.04), (20, 0.138), (21, -0.013), (22, -0.085), (23, 0.058), (24, -0.025), (25, -0.163), (26, -0.046), (27, 0.11), (28, -0.075), (29, -0.076), (30, 0.115), (31, 0.226), (32, 0.09), (33, 0.044), (34, 0.034), (35, -0.05), (36, -0.001), (37, -0.148), (38, -0.042), (39, 0.009), (40, 0.004), (41, 0.009), (42, -0.088), (43, 0.159), (44, 0.013), (45, -0.105), (46, 0.055), (47, 0.058), (48, -0.039), (49, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96925974 <a title="197-lsi-1" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>Author: Jesper Tegnér, Ádám Kepecs</p><p>Abstract: Hebbian learning rules are generally formulated as static rules. Under changing condition (e.g. neuromodulation, input statistics) most rules are sensitive to parameters. In particular, recent work has focused on two different formulations of spike-timing-dependent plasticity rules. Additive STDP [1] is remarkably versatile but also very fragile, whereas multiplicative STDP [2, 3] is more robust but lacks attractive features such as synaptic competition and rate stabilization. Here we address the problem of robustness in the additive STDP rule. We derive an adaptive control scheme, where the learning function is under fast dynamic control by postsynaptic activity to stabilize learning under a variety of conditions. Such a control scheme can be implemented using known biophysical mechanisms of synapses. We show that this adaptive rule makes the addit ive STDP more robust. Finally, we give an example how meta plasticity of the adaptive rule can be used to guide STDP into different type of learning regimes. 1</p><p>2 0.73951 <a title="197-lsi-2" href="./nips-2001-Citcuits_for_VLSI_Implementation_of_Temporally_Asymmetric_Hebbian_Learning.html">49 nips-2001-Citcuits for VLSI Implementation of Temporally Asymmetric Hebbian Learning</a></p>
<p>Author: A. Bofill, D. P. Thompson, Alan F. Murray</p><p>Abstract: Experimental data has shown that synaptic strength modification in some types of biological neurons depends upon precise spike timing differences between presynaptic and postsynaptic spikes. Several temporally-asymmetric Hebbian learning rules motivated by this data have been proposed. We argue that such learning rules are suitable to analog VLSI implementation. We describe an easily tunable circuit to modify the weight of a silicon spiking neuron according to those learning rules. Test results from the fabrication of the circuit using a O.6J.lm CMOS process are given. 1</p><p>3 0.61212641 <a title="197-lsi-3" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>Author: N. Matsumoto, M. Okada</p><p>Abstract: Recent biological experimental ﬁndings have shown that the synaptic plasticity depends on the relative timing of the pre- and postsynaptic spikes which determines whether Long Term Potentiation (LTP) occurs or Long Term Depression (LTD) does. The synaptic plasticity has been called “Temporally Asymmetric Hebbian plasticity (TAH)”. Many authors have numerically shown that spatiotemporal patterns can be stored in neural networks. However, the mathematical mechanism for storage of the spatio-temporal patterns is still unknown, especially the eﬀects of LTD. In this paper, we employ a simple neural network model and show that interference of LTP and LTD disappears in a sparse coding scheme. On the other hand, it is known that the covariance learning is indispensable for storing sparse patterns. We also show that TAH qualitatively has the same eﬀect as the covariance learning when spatio-temporal patterns are embedded in the network. 1</p><p>4 0.60340053 <a title="197-lsi-4" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>Author: Peter E. Latham</p><p>Abstract: Almost two decades ago , Hopfield [1] showed that networks of highly reduced model neurons can exhibit multiple attracting fixed points, thus providing a substrate for associative memory. It is still not clear, however, whether realistic neuronal networks can support multiple attractors. The main difficulty is that neuronal networks in vivo exhibit a stable background state at low firing rate, typically a few Hz. Embedding attractor is easy; doing so without destabilizing the background is not. Previous work [2, 3] focused on the sparse coding limit, in which a vanishingly small number of neurons are involved in any memory. Here we investigate the case in which the number of neurons involved in a memory scales with the number of neurons in the network. In contrast to the sparse coding limit, we find that multiple attractors can co-exist robustly with a stable background state. Mean field theory is used to understand how the behavior of the network scales with its parameters, and simulations with analog neurons are presented. One of the most important features of the nervous system is its ability to perform associative memory. It is generally believed that associative memory is implemented using attractor networks - experimental studies point in that direction [4- 7], and there are virtually no competing theoretical models. Perhaps surprisingly, however, it is still an open theoretical question whether attractors can exist in realistic neuronal networks. The</p><p>5 0.59159935 <a title="197-lsi-5" href="./nips-2001-Information-Geometric_Decomposition_in_Spike_Analysis.html">96 nips-2001-Information-Geometric Decomposition in Spike Analysis</a></p>
<p>Author: Hiroyuki Nakahara, Shun-ichi Amari</p><p>Abstract: We present an information-geometric measure to systematically investigate neuronal firing patterns, taking account not only of the second-order but also of higher-order interactions. We begin with the case of two neurons for illustration and show how to test whether or not any pairwise correlation in one period is significantly different from that in the other period. In order to test such a hypothesis of different firing rates, the correlation term needs to be singled out 'orthogonally' to the firing rates, where the null hypothesis might not be of independent firing. This method is also shown to directly associate neural firing with behavior via their mutual information, which is decomposed into two types of information, conveyed by mean firing rate and coincident firing, respectively. Then, we show that these results, using the 'orthogonal' decomposition, are naturally extended to the case of three neurons and n neurons in general. 1</p><p>6 0.56693459 <a title="197-lsi-6" href="./nips-2001-Learning_Spike-Based_Correlations_and_Conditional_Probabilities_in_Silicon.html">112 nips-2001-Learning Spike-Based Correlations and Conditional Probabilities in Silicon</a></p>
<p>7 0.4137072 <a title="197-lsi-7" href="./nips-2001-3_state_neurons_for_contextual_processing.html">2 nips-2001-3 state neurons for contextual processing</a></p>
<p>8 0.39680618 <a title="197-lsi-8" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>9 0.36193061 <a title="197-lsi-9" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>10 0.28288653 <a title="197-lsi-10" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>11 0.25007465 <a title="197-lsi-11" href="./nips-2001-Geometrical_Singularities_in_the_Neuromanifold_of_Multilayer_Perceptrons.html">83 nips-2001-Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons</a></p>
<p>12 0.23874919 <a title="197-lsi-12" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>13 0.23754923 <a title="197-lsi-13" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>14 0.23124857 <a title="197-lsi-14" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>15 0.21472012 <a title="197-lsi-15" href="./nips-2001-Scaling_Laws_and_Local_Minima_in_Hebbian_ICA.html">165 nips-2001-Scaling Laws and Local Minima in Hebbian ICA</a></p>
<p>16 0.19849387 <a title="197-lsi-16" href="./nips-2001-Orientational_and_Geometric_Determinants_of_Place_and_Head-direction.html">142 nips-2001-Orientational and Geometric Determinants of Place and Head-direction</a></p>
<p>17 0.19570573 <a title="197-lsi-17" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>18 0.19057719 <a title="197-lsi-18" href="./nips-2001-Prodding_the_ROC_Curve%3A_Constrained_Optimization_of_Classifier_Performance.html">152 nips-2001-Prodding the ROC Curve: Constrained Optimization of Classifier Performance</a></p>
<p>19 0.1847284 <a title="197-lsi-19" href="./nips-2001-A_Model_of_the_Phonological_Loop%3A_Generalization_and_Binding.html">12 nips-2001-A Model of the Phonological Loop: Generalization and Binding</a></p>
<p>20 0.18232198 <a title="197-lsi-20" href="./nips-2001-Switch_Packet_Arbitration_via_Queue-Learning.html">177 nips-2001-Switch Packet Arbitration via Queue-Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.097), (17, 0.017), (19, 0.033), (27, 0.153), (30, 0.075), (38, 0.047), (39, 0.218), (59, 0.012), (67, 0.011), (72, 0.062), (74, 0.035), (79, 0.045), (91, 0.101)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85781932 <a title="197-lda-1" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>Author: Jesper Tegnér, Ádám Kepecs</p><p>Abstract: Hebbian learning rules are generally formulated as static rules. Under changing condition (e.g. neuromodulation, input statistics) most rules are sensitive to parameters. In particular, recent work has focused on two different formulations of spike-timing-dependent plasticity rules. Additive STDP [1] is remarkably versatile but also very fragile, whereas multiplicative STDP [2, 3] is more robust but lacks attractive features such as synaptic competition and rate stabilization. Here we address the problem of robustness in the additive STDP rule. We derive an adaptive control scheme, where the learning function is under fast dynamic control by postsynaptic activity to stabilize learning under a variety of conditions. Such a control scheme can be implemented using known biophysical mechanisms of synapses. We show that this adaptive rule makes the addit ive STDP more robust. Finally, we give an example how meta plasticity of the adaptive rule can be used to guide STDP into different type of learning regimes. 1</p><p>2 0.70236856 <a title="197-lda-2" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>Author: Gunnar Rätsch, Sebastian Mika, Manfred K. Warmuth</p><p>Abstract: We give an uniﬁed convergence analysis of ensemble learning methods including e.g. AdaBoost, Logistic Regression and the Least-SquareBoost algorithm for regression. These methods have in common that they iteratively call a base learning algorithm which returns hypotheses that are then linearly combined. We show that these methods are related to the Gauss-Southwell method known from numerical optimization and state non-asymptotical convergence results for all these methods. Our analysis includes -norm regularized cost functions leading to a clean and general way to regularize ensemble learning.</p><p>3 0.70062071 <a title="197-lda-3" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>Author: Olivier Chapelle, Bernhard Schćž&scaron;lkopf</p><p>Abstract: The choice of an SVM kernel corresponds to the choice of a representation of the data in a feature space and, to improve performance , it should therefore incorporate prior knowledge such as known transformation invariances. We propose a technique which extends earlier work and aims at incorporating invariances in nonlinear kernels. We show on a digit recognition task that the proposed approach is superior to the Virtual Support Vector method, which previously had been the method of choice. 1</p><p>4 0.70022392 <a title="197-lda-4" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>Author: Peter E. Latham</p><p>Abstract: Almost two decades ago , Hopfield [1] showed that networks of highly reduced model neurons can exhibit multiple attracting fixed points, thus providing a substrate for associative memory. It is still not clear, however, whether realistic neuronal networks can support multiple attractors. The main difficulty is that neuronal networks in vivo exhibit a stable background state at low firing rate, typically a few Hz. Embedding attractor is easy; doing so without destabilizing the background is not. Previous work [2, 3] focused on the sparse coding limit, in which a vanishingly small number of neurons are involved in any memory. Here we investigate the case in which the number of neurons involved in a memory scales with the number of neurons in the network. In contrast to the sparse coding limit, we find that multiple attractors can co-exist robustly with a stable background state. Mean field theory is used to understand how the behavior of the network scales with its parameters, and simulations with analog neurons are presented. One of the most important features of the nervous system is its ability to perform associative memory. It is generally believed that associative memory is implemented using attractor networks - experimental studies point in that direction [4- 7], and there are virtually no competing theoretical models. Perhaps surprisingly, however, it is still an open theoretical question whether attractors can exist in realistic neuronal networks. The</p><p>5 0.70015401 <a title="197-lda-5" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>6 0.69625938 <a title="197-lda-6" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>7 0.69608676 <a title="197-lda-7" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>8 0.68745953 <a title="197-lda-8" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>9 0.68621689 <a title="197-lda-9" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>10 0.68381721 <a title="197-lda-10" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>11 0.68244278 <a title="197-lda-11" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>12 0.68067992 <a title="197-lda-12" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>13 0.67932349 <a title="197-lda-13" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>14 0.67926729 <a title="197-lda-14" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>15 0.67647314 <a title="197-lda-15" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>16 0.67565954 <a title="197-lda-16" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>17 0.67521149 <a title="197-lda-17" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>18 0.67190504 <a title="197-lda-18" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>19 0.66957033 <a title="197-lda-19" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>20 0.66938245 <a title="197-lda-20" href="./nips-2001-Kernel_Feature_Spaces_and_Nonlinear_Blind_Souce_Separation.html">103 nips-2001-Kernel Feature Spaces and Nonlinear Blind Souce Separation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
