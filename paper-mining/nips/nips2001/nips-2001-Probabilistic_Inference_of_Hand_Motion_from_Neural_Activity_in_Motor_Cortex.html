<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-150" href="#">nips2001-150</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</h1>
<br/><p>Source: <a title="nips-2001-150-pdf" href="http://papers.nips.cc/paper/1997-probabilistic-inference-of-hand-motion-from-neural-activity-in-motor-cortex.pdf">pdf</a></p><p>Author: Yun Gao, Michael J. Black, Elie Bienenstock, Shy Shoham, John P. Donoghue</p><p>Abstract: Statistical learning and probabilistic inference techniques are used to infer the hand position of a subject from multi-electrode recordings of neural activity in motor cortex. First, an array of electrodes provides training data of neural ﬁring conditioned on hand kinematics. We learn a nonparametric representation of this ﬁring activity using a Bayesian model and rigorously compare it with previous models using cross-validation. Second, we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using Bayesian inference. The learned ﬁring models of multiple cells are used to deﬁne a nonGaussian likelihood term which is combined with a prior probability for the kinematics. A particle ﬁltering method is used to represent, update, and propagate the posterior distribution over time. The approach is compared with traditional linear ﬁltering methods; the results suggest that it may be appropriate for neural prosthetic applications.</p><p>Reference: <a title="nips-2001-150-reference" href="../nips2001_reference/nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu ¡  ¤  ¢  Abstract Statistical learning and probabilistic inference techniques are used to infer the hand position of a subject from multi-electrode recordings of neural activity in motor cortex. [sent-19, score-0.698]
</p><p>2 First, an array of electrodes provides training data of neural ﬁring conditioned on hand kinematics. [sent-20, score-0.365]
</p><p>3 We learn a nonparametric representation of this ﬁring activity using a Bayesian model and rigorously compare it with previous models using cross-validation. [sent-21, score-0.264]
</p><p>4 Second, we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using Bayesian inference. [sent-22, score-0.454]
</p><p>5 The learned ﬁring models of multiple cells are used to deﬁne a nonGaussian likelihood term which is combined with a prior probability for the kinematics. [sent-23, score-0.456]
</p><p>6 A particle ﬁltering method is used to represent, update, and propagate the posterior distribution over time. [sent-24, score-0.278]
</p><p>7 The approach is compared with traditional linear ﬁltering methods; the results suggest that it may be appropriate for neural prosthetic applications. [sent-25, score-0.092]
</p><p>8 1 Introduction This paper explores the use of statistical learning methods and probabilistic inference techniques for modeling the relationship between the motion of a monkey’s arm and neural activity in motor cortex. [sent-26, score-0.757]
</p><p>9 A multi-electrode array (Figure 1) is used to simultaneously record the activity of 24 neurons in the arm area of primary motor cortex (MI) in awake, behaving, macaque monkeys. [sent-28, score-0.645]
</p><p>10 This activity is recorded while the monkeys manually track a smoothly and randomly mov-  Connector  Acrylic  Bone  S    Silicone  RRR R 89 7 6  C. [sent-29, score-0.251]
</p><p>11 Statistical learning methods are used to derive Bayesian estimates of the conditional probability of ﬁring for each cell given the kinematic variables (we consider only hand velocity here). [sent-41, score-0.672]
</p><p>12 Speciﬁcally, we use non-parametric models of the conditional ﬁring, learned using regularization (smoothing) techniques with cross-validation. [sent-42, score-0.18]
</p><p>13 Our results suggest that the cells encode information about the position and velocity of the hand in space. [sent-43, score-0.679]
</p><p>14 Moreover, the non-parametric models provide a better explanation of the data than previous parametric models [6, 10] and provide new insight into neural coding in MI. [sent-44, score-0.22]
</p><p>15 Decoding involves the inference of the hand motion from the ﬁring rate of the cells. [sent-45, score-0.358]
</p><p>16 In particular, we represent the posterior probability of the entire hand trajectory conditioned on the observed sequence of neural activity (spike trains). [sent-46, score-0.589]
</p><p>17 The nature of this activity results in ambiguities and a non-Gaussian posterior probability distribution. [sent-47, score-0.306]
</p><p>18 Consequently, we represent the posterior non-parametrically using a discrete set of samples [8]. [sent-48, score-0.089]
</p><p>19 This distribution is predicted and updated in non-overlapping 50 ms time intervals using a Bayesian estimation method called particle ﬁltering [8]. [sent-49, score-0.295]
</p><p>20 Experiments with real and synthetic data suggest that this approach provides probabilistically sound estimates of kinematics and allows the probabilistic combination of information from multiple neurons, the use of priors, and the rigorous evaluation of models and results. [sent-50, score-0.319]
</p><p>21 Summarizing, a ten-by-ten array of electrodes is implanted in the primary motor cortex (MI) of a Macaque monkey (Figure 1) [7, 9, 12]. [sent-52, score-0.442]
</p><p>22 Neural activity in motor cortex has been shown to be related to the movement kinematics of the animal’s arm and, in particular, to the direction of hand motion [3, 6]. [sent-53, score-1.001]
</p><p>23 The monkey receives a reward upon completion of a successful trial in which the manipulandum is moved to keep the feedback dot within a pre-speciﬁed distance of the target. [sent-56, score-0.206]
</p><p>24 The path of the target is chosen to be a smooth random walk that effectively samples the space of hand positions and velocities: measured hand positions and velocities have a roughly Gaussian distribution (Figure 2b and c) [12]. [sent-57, score-0.439]
</p><p>25 Neural activity is ampliﬁed, waveforms are thresholded, and spike sorting is performed off-line to isolate the activity of individual cells [9]. [sent-58, score-0.695]
</p><p>26 Recordings from 24 motor cortical cells are measured simultaneously with hand kinematics. [sent-59, score-0.577]
</p><p>27 Distribution of the position (b) and velocity (c) of the hand. [sent-62, score-0.318]
</p><p>28 (b) Position: horizontal and vertical axes represent and position of the hand. [sent-64, score-0.157]
</p><p>29 (c) Velocity: the horizontal axis represents direction, , and the vertical axis represents speed, . [sent-65, score-0.211]
</p><p>30 "    cell 3  0  cell 16  cell 19  Figure 3: Observed mean conditional ﬁring rates in 50 ms intervals for three cells given hand velocity. [sent-70, score-0.999]
</p><p>31 The horizontal axis represents the direction of movement, , in radians (“wrapping” around from to ). [sent-71, score-0.144]
</p><p>32 The vertical axis represents speed, , and ranges from 0 cm/s to 12 cm/s. [sent-72, score-0.104]
</p><p>33 Color ranges from dark blue (no measurement) to red (approximately 3 spikes). [sent-73, score-0.149]
</p><p>34 §    £  £ ¤¢  3 Modeling Neural Activity Figure 3 shows the measured mean ﬁring rate within 50 ms time intervals for three cells conditioned on the subject’s hand velocity. [sent-74, score-0.604]
</p><p>35 We view the neural ﬁring activity in Figure 3 as a stochastic and sparse realization of some underlying model that relates neural ﬁring to hand motion. [sent-75, score-0.451]
</p><p>36 Similar plots are obtained as a function of hand position. [sent-76, score-0.146]
</p><p>37 Each plot can be thought of as a type of “tuning function” [12] that characterizes the response of the cell conditioned on hand velocity. [sent-77, score-0.355]
</p><p>38 In previous work, authors have considered a variety of models of this data including a cosine tuning function [6] and a modiﬁed cosine function [10]. [sent-78, score-0.261]
</p><p>39 Here we explore a non-parametric model of the underling activity and, adopting a Bayesian formulation, seek a maximum a posterior (MAP) estimate of a cell’s conditional ﬁring. [sent-79, score-0.357]
</p><p>40 Adopting a Markov Random Field (MRF) assumption [4], let the velocity space, , be discretized on a grid. [sent-80, score-0.258]
</p><p>41 Let g be the array of true (unobserved) conditional neural ﬁring and be the corresponding observed mean ﬁring. [sent-81, score-0.179]
</p><p>42 We seek the posterior probability g (1)  ) 0(  998 B 99 A@CA@8  F GE  P P s T a H T q fh R P T a H utccr`ca Gpigedcb`T  YF E V WGXWF  T R ) PD USQIIH  F GE D  65 §2 7343  1  0  0. [sent-82, score-0.089]
</p><p>43 ¦ ¤¢ §¥£9  )  a  ¡     Y  V  where is a normalizing constant independent of g, and are the observed and true mean ﬁring at velocity respectively, represents the ﬁring rate for the th neighboring velocity of , and the neighbors are taken to be the four nearest velocities ( ). [sent-95, score-0.65]
</p><p>44 Ta  ¨  T  sTa   ) ©  (  (  The ﬁrst term on the right hand side represents the likelihood of observing a particular ﬁring rate given that the true rate is . [sent-96, score-0.349]
</p><p>45 Here we compare two generative models of the neural spiking process within 50 ms; a Poisson model, , and a Gaussian model, :    ¢  E    YF  3 1 ¤ ¡ 1P a 2e¦¢  E  ¢  0  ()'$%¡#£c¤ & 8 "  T a  Y WF E   ) P rH a  2  T  ! [sent-97, score-0.091]
</p><p>46  Y   8  a  Y  YF E   ) P bH a  The second term is a spatial prior probability that encodes our expectations about , the variation of neural activity in velocity space. [sent-98, score-0.688]
</p><p>47 The MRF prior states that the ﬁring, , at velocity depends only on the ﬁring at neighboring velocities. [sent-99, score-0.316]
</p><p>48 A Gaussian prior possible prior models for the distribution of corresponds to an assumption that the ﬁring rate varies smoothly. [sent-101, score-0.217]
</p><p>49 A robust prior assumes a heavy-tailed distribution of the spatial variation (see Figure 4), , (derivatives of the ﬁring rate in the and directions) and implies piecewise smooth data. [sent-102, score-0.344]
</p><p>50 In the case of the Gaussian+Gaussian and Poisson+Robust models, the optimal value of the parameter is computed for each cell using cross validation. [sent-104, score-0.16]
</p><p>51 During cross-validation, each time 10 trials out of 180 are left out for testing and the models are ﬁt with the remaining training data. [sent-105, score-0.08]
</p><p>52 This provides a measure of how well the model captures the statistical variation in the training set and is used for quantitative comparison. [sent-107, score-0.074]
</p><p>53 Instead, we derive an approximate  B  By “Gaussian+Gaussian” we mean both the likelihood and prior terms are Gaussian whereas “Poisson+Robust” implies a Poisson likelihood and robust spatial prior. [sent-110, score-0.303]
</p><p>54 7  Moran&Schwartz; (M&S;)  Poisson+Robust  cell 19  Figure 5: Estimated ﬁring rate for cells in Figure 3 using different models. [sent-148, score-0.429]
</p><p>55 6294e-06  Table 1: Numerical comparison; log likelihood ratio of pairs of models and the signiﬁcance level given by Wilcoxon signed rank test (Splus, MathSoft Inc. [sent-157, score-0.107]
</p><p>56 solution for g in (1) by minimizing the negative logarithm of the distribution using standard regularization techniques [1, 13] with missing data, the learned prior model, and a Poisson likelihood term [11]. [sent-159, score-0.271]
</p><p>57 Note that the negative logarithm of the prior term can be approximated by the robust statistical error function which has been used extensively in machine vision and image processing for smoothing data with discontinuities [1, 5]. [sent-161, score-0.281]
</p><p>58 Moreover, some cells appear to be tuned to motion direction, , and not to speed, , resulting in vertically elongated patterns of ﬁring. [sent-164, score-0.341]
</p><p>59 cell 19) appear to be tuned to particular directions and speeds; this type of activity is not well ﬁt by the parametric models. [sent-167, score-0.419]
</p><p>60 The positive values in Table 1 indicate that the non-parametric models do a better job of explaining new data than the parametric models with the Poisson+Robust ﬁt providing the best description of the data. [sent-170, score-0.136]
</p><p>61 This P+R model implies that the conditional ﬁring rate is well described by regions of smooth activity with relatively sharp discontinuities between them. [sent-171, score-0.452]
</p><p>62 The non-parametric models reduce the strong bias of the parametric models with a slight increase in variance and hence achieve a lower total error. [sent-172, score-0.136]
</p><p>63 ¤ ¥£  H  H  4 Temporal Inference Given neural measurements our goal is to infer the motion of the hand over time. [sent-173, score-0.316]
</p><p>64 We note here however, that the learned models of neural activity are not-Gaussian and the dynamics of the hand motion may be non-linear. [sent-176, score-0.621]
</p><p>65 Furthermore with a small number of cells, our interpretation of the neural data may be ambiguous and the posterior probability of the kinematic variables, given the neural activity, may be best modeled by a non-Gaussian, multi-modal, distribution. [sent-177, score-0.234]
</p><p>66 To cope with these issues in a sound probabilistic framework we exploit a non-parametric approach that uses factored sampling to discretely approximate the posterior distribution, and particle ﬁltering to propagate and update this distribution over time [8]. [sent-178, score-0.413]
</p><p>67 Let be the mean ﬁring rate of cell Let the state of the system be s at time where the mean ﬁring rate is estimated within non-overlapping 50 ms temporal windows. [sent-180, score-0.426]
</p><p>68 Also, let c represent the ﬁring rate of all cells at time . [sent-181, score-0.302]
</p><p>69 Similarly let represent the sequence of these ﬁring rates for cell up to time and let C represent the ﬁring of all cells up to time . [sent-182, score-0.441]
</p><p>70 Plotting this likelihood term for a range of states reveals that its structure is highly non-Gaussian with multiple peaks. [sent-187, score-0.095]
</p><p>71 Note, s C is the posterior distribution over the state space at time . [sent-189, score-0.122]
</p><p>72 Pq       Hq       F E  Pq  8 ¢     H r    ¡  F E  The posterior, s C , is represented with a discrete, weighted set, of random samples which are propagated in time using a standard particle ﬁlter (see [8] for details). [sent-190, score-0.188]
</p><p>73 Unlike previous applications of particle ﬁltering, the likelihood of ﬁring for an individual cell in  999 A   P t H       trial No. [sent-191, score-0.41]
</p><p>74 ¡ ¢   £ ¤   £ ¦      £ ¦   9 ¢ ¦ £9 ) 1  9 ¥£¥¢ 9 ) 1   ¡ ¦   ¡¦  © § §¢  ¤¨£9  § ¦ £9 ) 1 ¢ ¥    ) 1  50 ms provides very little information. [sent-199, score-0.073]
</p><p>75 For the posterior to be meaningful we must combine evidence from multiple cells. [sent-200, score-0.089]
</p><p>76 Our experiments indicate that the responses from our 24 cells are insufﬁcient for this task. [sent-201, score-0.215]
</p><p>77 To demonstrate the feasibility of the particle ﬁltering method, we synthesized approximately 1000 cells by taking the learned models of the 24 cells and translating them along the axis to generate a more complete covering of the velocity space. [sent-202, score-0.988]
</p><p>78 Note that the assumption of such a set of cells in MI is quite reasonable give the sampling of cells we have observed in multiple monkeys. [sent-203, score-0.43]
</p><p>79 §  From the set of synthetic cells we then generate a synthetic spike train by taking a known sequence of hand velocities and stochastically generating spikes using the learned conditional ﬁring models with a Poisson generative model. [sent-204, score-0.746]
</p><p>80 Particle ﬁltering is used to estimate the posterior distribution over hand velocities given the synthetic neural data. [sent-205, score-0.419]
</p><p>81 The expected value of the horizontal and vertical velocity is displayed in Figure 6a. [sent-206, score-0.355]
</p><p>82 For comparison, a standard linear ﬁltering method [6, 14] was trained on the synthetic data from 50 ms intervals. [sent-207, score-0.133]
</p><p>83 The Bayesian analysis provides a probabilistic framework for sound causal estimates over short time intervals. [sent-210, score-0.167]
</p><p>84 We are currently experimenting with modiﬁed particle ﬁltering schemes in which linear ﬁltering methods provide a proposal distribution and importance sampling is used to construct a valid posterior distribution. [sent-211, score-0.244]
</p><p>85 5 Conclusions We have described a Bayesian model for neural activity in MI that relates this activity to actions in the world. [sent-213, score-0.478]
</p><p>86 Quantitative comparison with previous models of MI activity indicate that the non-parametric models computed using regularization more accurately describe the neural activity. [sent-214, score-0.396]
</p><p>87 In particular, the robust spatial prior term suggests that neural ﬁring in MI is not a smooth function of velocity but rather exhibits discontinuities between regions  of high and low activity. [sent-215, score-0.65]
</p><p>88 We have also described the Bayesian decoding of hand motion from ﬁring activity using a particle ﬁlter. [sent-216, score-0.683]
</p><p>89 Initial results suggest that measurements from several hundred cells may be required for accurate estimates of hand velocity. [sent-217, score-0.361]
</p><p>90 The application of particle ﬁltering to this problem has many advantages as it allows complex, non-Gaussian, likelihood models that may incorporate non-linear temporal properties of neural ﬁring (e. [sent-218, score-0.358]
</p><p>91 Unlike previous linear ﬁltering methods this Bayesian approach provides probabilistically sound, causal, estimates in short time windows of 50ms. [sent-221, score-0.067]
</p><p>92 Current work is exploring correlations between cells [7] and the relationship between the neural activity and other kinematic variables [12]. [sent-222, score-0.565]
</p><p>93 On the uniﬁcation of line processes, outlier rejection, and robust statistics with applications in early vision. [sent-233, score-0.089]
</p><p>94 A statistical paradigm for neural spike train decoding applied to position prediction from ensemble ﬁring patterns of rat hippocampal place cells. [sent-241, score-0.189]
</p><p>95 Temporal encoding of movement kinematics in the discharge of primate primary motor and premotor neurons. [sent-249, score-0.32]
</p><p>96 Information about movement direction obtained from synchronous activity of motor cortical neurons. [sent-276, score-0.561]
</p><p>97 Neuronal interaction improve cortical population coding of movement direction. [sent-293, score-0.239]
</p><p>98 Motor cortical representation of speed and direction during reaching. [sent-299, score-0.148]
</p><p>99 Temporal tuning properties for hand position and velocity in motor cortical neurons. [sent-313, score-0.68]
</p><p>100 Real-time prediction of hand trajectory by ensembles of cortical neurons in primates. [sent-331, score-0.253]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ring', 0.51), ('velocity', 0.258), ('activity', 0.217), ('cells', 0.215), ('ltering', 0.211), ('poisson', 0.169), ('cell', 0.16), ('particle', 0.155), ('motor', 0.153), ('hand', 0.146), ('pq', 0.142), ('motion', 0.126), ('arm', 0.107), ('cosine', 0.107), ('hatsopoulos', 0.105), ('ge', 0.102), ('mi', 0.095), ('movement', 0.091), ('robust', 0.089), ('posterior', 0.089), ('array', 0.084), ('velocities', 0.08), ('red', 0.079), ('kinematics', 0.076), ('ms', 0.073), ('manipulandum', 0.072), ('blue', 0.07), ('smooth', 0.067), ('monkey', 0.067), ('bayesian', 0.063), ('paninski', 0.063), ('discontinuities', 0.063), ('providence', 0.063), ('cortical', 0.063), ('position', 0.06), ('synthetic', 0.06), ('likelihood', 0.06), ('prior', 0.058), ('monitor', 0.057), ('yf', 0.057), ('kinematic', 0.057), ('axis', 0.057), ('sound', 0.056), ('rate', 0.054), ('temporal', 0.052), ('brown', 0.051), ('conditional', 0.051), ('horizontal', 0.05), ('conditioned', 0.049), ('fellows', 0.048), ('ijcv', 0.048), ('implanted', 0.048), ('llr', 0.048), ('prosthetic', 0.048), ('tablet', 0.048), ('cortex', 0.048), ('speed', 0.048), ('vertical', 0.047), ('models', 0.047), ('probabilistic', 0.046), ('spike', 0.046), ('gaussian', 0.045), ('population', 0.045), ('neural', 0.044), ('trajectory', 0.044), ('electrodes', 0.042), ('donoghue', 0.042), ('ojakangas', 0.042), ('vy', 0.042), ('moran', 0.042), ('parametric', 0.042), ('learned', 0.041), ('regularization', 0.041), ('variation', 0.04), ('coding', 0.04), ('decoding', 0.039), ('gao', 0.038), ('mrf', 0.038), ('tracking', 0.037), ('direction', 0.037), ('spatial', 0.036), ('logarithm', 0.036), ('macaque', 0.036), ('hq', 0.036), ('schwartz', 0.036), ('trial', 0.035), ('term', 0.035), ('intervals', 0.034), ('quantitative', 0.034), ('propagate', 0.034), ('smoothly', 0.034), ('probabilistically', 0.034), ('pami', 0.034), ('time', 0.033), ('ri', 0.033), ('inference', 0.032), ('relationship', 0.032), ('kalman', 0.032), ('causal', 0.032), ('moved', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="150-tfidf-1" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>Author: Yun Gao, Michael J. Black, Elie Bienenstock, Shy Shoham, John P. Donoghue</p><p>Abstract: Statistical learning and probabilistic inference techniques are used to infer the hand position of a subject from multi-electrode recordings of neural activity in motor cortex. First, an array of electrodes provides training data of neural ﬁring conditioned on hand kinematics. We learn a nonparametric representation of this ﬁring activity using a Bayesian model and rigorously compare it with previous models using cross-validation. Second, we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using Bayesian inference. The learned ﬁring models of multiple cells are used to deﬁne a nonGaussian likelihood term which is combined with a prior probability for the kinematics. A particle ﬁltering method is used to represent, update, and propagate the posterior distribution over time. The approach is compared with traditional linear ﬁltering methods; the results suggest that it may be appropriate for neural prosthetic applications.</p><p>2 0.29766646 <a title="150-tfidf-2" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>Author: Richard Hahnloser, Xiaohui Xie, H. S. Seung</p><p>Abstract: Integration in the head-direction system is a computation by which horizontal angular head velocity signals from the vestibular nuclei are integrated to yield a neural representation of head direction. In the thalamus, the postsubiculum and the mammillary nuclei, the head-direction representation has the form of a place code: neurons have a preferred head direction in which their ﬁring is maximal [Blair and Sharp, 1995, Blair et al., 1998, ?]. Integration is a difﬁcult computation, given that head-velocities can vary over a large range. Previous models of the head-direction system relied on the assumption that the integration is achieved in a ﬁring-rate-based attractor network with a ring structure. In order to correctly integrate head-velocity signals during high-speed head rotations, very fast synaptic dynamics had to be assumed. Here we address the question whether integration in the head-direction system is possible with slow synapses, for example excitatory NMDA and inhibitory GABA(B) type synapses. For neural networks with such slow synapses, rate-based dynamics are a good approximation of spiking neurons [Ermentrout, 1994]. We ﬁnd that correct integration during high-speed head rotations imposes strong constraints on possible network architectures.</p><p>3 0.18528655 <a title="150-tfidf-3" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>Author: Antonino Casile, Michele Rucci</p><p>Abstract: Neural activity appears to be a crucial component for shaping the receptive ﬁelds of cortical simple cells into adjacent, oriented subregions alternately receiving ON- and OFF-center excitatory geniculate inputs. It is known that the orientation selective responses of V1 neurons are reﬁned by visual experience. After eye opening, the spatiotemporal structure of neural activity in the early stages of the visual pathway depends both on the visual environment and on how the environment is scanned. We have used computational modeling to investigate how eye movements might affect the reﬁnement of the orientation tuning of simple cells in the presence of a Hebbian scheme of synaptic plasticity. Levels of correlation between the activity of simulated cells were examined while natural scenes were scanned so as to model sequences of saccades and ﬁxational eye movements, such as microsaccades, tremor and ocular drift. The speciﬁc patterns of activity required for a quantitatively accurate development of simple cell receptive ﬁelds with segregated ON and OFF subregions were observed during ﬁxational eye movements, but not in the presence of saccades or with static presentation of natural visual input. These results suggest an important role for the eye movements occurring during visual ﬁxation in the reﬁnement of orientation selectivity.</p><p>4 0.17350847 <a title="150-tfidf-4" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>Author: Michael Kositsky, Andrew G. Barto</p><p>Abstract: Tangential hand velocity proﬁles of rapid human arm movements often appear as sequences of several bell-shaped acceleration-deceleration phases called submovements or movement units. This suggests how the nervous system might efﬁciently control a motor plant in the presence of noise and feedback delay. Another critical observation is that stochasticity in a motor control problem makes the optimal control policy essentially different from the optimal control policy for the deterministic case. We use a simpliﬁed dynamic model of an arm and address rapid aimed arm movements. We use reinforcement learning as a tool to approximate the optimal policy in the presence of noise and feedback delay. Using a simpliﬁed model we show that multiple submovements emerge as an optimal policy in the presence of noise and feedback delay. The optimal policy in this situation is to drive the arm’s end point close to the target by one fast submovement and then apply a few slow submovements to accurately drive the arm’s end point into the target region. In our simulations, the controller sometimes generates corrective submovements before the initial fast submovement is completed, much like the predictive corrections observed in a number of psychophysical experiments.</p><p>5 0.16110192 <a title="150-tfidf-5" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>Author: N. Matsumoto, M. Okada</p><p>Abstract: Recent biological experimental ﬁndings have shown that the synaptic plasticity depends on the relative timing of the pre- and postsynaptic spikes which determines whether Long Term Potentiation (LTP) occurs or Long Term Depression (LTD) does. The synaptic plasticity has been called “Temporally Asymmetric Hebbian plasticity (TAH)”. Many authors have numerically shown that spatiotemporal patterns can be stored in neural networks. However, the mathematical mechanism for storage of the spatio-temporal patterns is still unknown, especially the eﬀects of LTD. In this paper, we employ a simple neural network model and show that interference of LTP and LTD disappears in a sparse coding scheme. On the other hand, it is known that the covariance learning is indispensable for storing sparse patterns. We also show that TAH qualitatively has the same eﬀect as the covariance learning when spatio-temporal patterns are embedded in the network. 1</p><p>6 0.14262708 <a title="150-tfidf-6" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>7 0.13972513 <a title="150-tfidf-7" href="./nips-2001-Linking_Motor_Learning_to_Function_Approximation%3A_Learning_in_an_Unlearnable_Force_Field.html">116 nips-2001-Linking Motor Learning to Function Approximation: Learning in an Unlearnable Force Field</a></p>
<p>8 0.13943404 <a title="150-tfidf-8" href="./nips-2001-A_Hierarchical_Model_of_Complex_Cells_in_Visual_Cortex_for_the_Binocular_Perception_of_Motion-in-Depth.html">10 nips-2001-A Hierarchical Model of Complex Cells in Visual Cortex for the Binocular Perception of Motion-in-Depth</a></p>
<p>9 0.13352412 <a title="150-tfidf-9" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>10 0.1279964 <a title="150-tfidf-10" href="./nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">102 nips-2001-KLD-Sampling: Adaptive Particle Filters</a></p>
<p>11 0.12202999 <a title="150-tfidf-11" href="./nips-2001-Classifying_Single_Trial_EEG%3A_Towards_Brain_Computer_Interfacing.html">50 nips-2001-Classifying Single Trial EEG: Towards Brain Computer Interfacing</a></p>
<p>12 0.10194837 <a title="150-tfidf-12" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>13 0.091342166 <a title="150-tfidf-13" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>14 0.090379529 <a title="150-tfidf-14" href="./nips-2001-Orientational_and_Geometric_Determinants_of_Place_and_Head-direction.html">142 nips-2001-Orientational and Geometric Determinants of Place and Head-direction</a></p>
<p>15 0.090366736 <a title="150-tfidf-15" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>16 0.087120913 <a title="150-tfidf-16" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>17 0.085396178 <a title="150-tfidf-17" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>18 0.081142128 <a title="150-tfidf-18" href="./nips-2001-Bayesian_morphometry_of_hippocampal_cells_suggests_same-cell_somatodendritic_repulsion.html">42 nips-2001-Bayesian morphometry of hippocampal cells suggests same-cell somatodendritic repulsion</a></p>
<p>19 0.07909368 <a title="150-tfidf-19" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>20 0.077211864 <a title="150-tfidf-20" href="./nips-2001-Stochastic_Mixed-Signal_VLSI_Architecture_for_High-Dimensional_Kernel_Machines.html">176 nips-2001-Stochastic Mixed-Signal VLSI Architecture for High-Dimensional Kernel Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.224), (1, -0.263), (2, -0.116), (3, 0.031), (4, -0.061), (5, -0.024), (6, 0.009), (7, 0.232), (8, 0.123), (9, 0.191), (10, 0.033), (11, 0.001), (12, -0.143), (13, -0.085), (14, 0.05), (15, 0.061), (16, -0.262), (17, -0.082), (18, -0.028), (19, 0.008), (20, 0.092), (21, 0.163), (22, 0.002), (23, 0.037), (24, 0.069), (25, 0.036), (26, 0.058), (27, -0.151), (28, 0.118), (29, -0.131), (30, -0.088), (31, 0.057), (32, -0.042), (33, 0.002), (34, 0.089), (35, -0.042), (36, 0.034), (37, -0.093), (38, -0.005), (39, 0.053), (40, -0.011), (41, 0.026), (42, -0.089), (43, -0.107), (44, 0.033), (45, 0.071), (46, -0.049), (47, 0.008), (48, 0.01), (49, -0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96014482 <a title="150-lsi-1" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>Author: Yun Gao, Michael J. Black, Elie Bienenstock, Shy Shoham, John P. Donoghue</p><p>Abstract: Statistical learning and probabilistic inference techniques are used to infer the hand position of a subject from multi-electrode recordings of neural activity in motor cortex. First, an array of electrodes provides training data of neural ﬁring conditioned on hand kinematics. We learn a nonparametric representation of this ﬁring activity using a Bayesian model and rigorously compare it with previous models using cross-validation. Second, we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using Bayesian inference. The learned ﬁring models of multiple cells are used to deﬁne a nonGaussian likelihood term which is combined with a prior probability for the kinematics. A particle ﬁltering method is used to represent, update, and propagate the posterior distribution over time. The approach is compared with traditional linear ﬁltering methods; the results suggest that it may be appropriate for neural prosthetic applications.</p><p>2 0.75778383 <a title="150-lsi-2" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>Author: Richard Hahnloser, Xiaohui Xie, H. S. Seung</p><p>Abstract: Integration in the head-direction system is a computation by which horizontal angular head velocity signals from the vestibular nuclei are integrated to yield a neural representation of head direction. In the thalamus, the postsubiculum and the mammillary nuclei, the head-direction representation has the form of a place code: neurons have a preferred head direction in which their ﬁring is maximal [Blair and Sharp, 1995, Blair et al., 1998, ?]. Integration is a difﬁcult computation, given that head-velocities can vary over a large range. Previous models of the head-direction system relied on the assumption that the integration is achieved in a ﬁring-rate-based attractor network with a ring structure. In order to correctly integrate head-velocity signals during high-speed head rotations, very fast synaptic dynamics had to be assumed. Here we address the question whether integration in the head-direction system is possible with slow synapses, for example excitatory NMDA and inhibitory GABA(B) type synapses. For neural networks with such slow synapses, rate-based dynamics are a good approximation of spiking neurons [Ermentrout, 1994]. We ﬁnd that correct integration during high-speed head rotations imposes strong constraints on possible network architectures.</p><p>3 0.59356117 <a title="150-lsi-3" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>Author: Antonino Casile, Michele Rucci</p><p>Abstract: Neural activity appears to be a crucial component for shaping the receptive ﬁelds of cortical simple cells into adjacent, oriented subregions alternately receiving ON- and OFF-center excitatory geniculate inputs. It is known that the orientation selective responses of V1 neurons are reﬁned by visual experience. After eye opening, the spatiotemporal structure of neural activity in the early stages of the visual pathway depends both on the visual environment and on how the environment is scanned. We have used computational modeling to investigate how eye movements might affect the reﬁnement of the orientation tuning of simple cells in the presence of a Hebbian scheme of synaptic plasticity. Levels of correlation between the activity of simulated cells were examined while natural scenes were scanned so as to model sequences of saccades and ﬁxational eye movements, such as microsaccades, tremor and ocular drift. The speciﬁc patterns of activity required for a quantitatively accurate development of simple cell receptive ﬁelds with segregated ON and OFF subregions were observed during ﬁxational eye movements, but not in the presence of saccades or with static presentation of natural visual input. These results suggest an important role for the eye movements occurring during visual ﬁxation in the reﬁnement of orientation selectivity.</p><p>4 0.550228 <a title="150-lsi-4" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>Author: Xiaohui Xie, Martin A. Giese</p><p>Abstract: Asymmetric lateral connections are one possible mechanism that can account for the direction selectivity of cortical neurons. We present a mathematical analysis for a class of these models. Contrasting with earlier theoretical work that has relied on methods from linear systems theory, we study the network’s nonlinear dynamic properties that arise when the threshold nonlinearity of the neurons is taken into account. We show that such networks have stimulus-locked traveling pulse solutions that are appropriate for modeling the responses of direction selective cortical neurons. In addition, our analysis shows that outside a certain regime of stimulus speeds the stability of this solutions breaks down giving rise to another class of solutions that are characterized by speciﬁc spatiotemporal periodicity. This predicts that if direction selectivity in the cortex is mainly achieved by asymmetric lateral connections lurching activity waves might be observable in ensembles of direction selective cortical neurons within appropriate regimes of the stimulus speed.</p><p>5 0.53714228 <a title="150-lsi-5" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>Author: N. Matsumoto, M. Okada</p><p>Abstract: Recent biological experimental ﬁndings have shown that the synaptic plasticity depends on the relative timing of the pre- and postsynaptic spikes which determines whether Long Term Potentiation (LTP) occurs or Long Term Depression (LTD) does. The synaptic plasticity has been called “Temporally Asymmetric Hebbian plasticity (TAH)”. Many authors have numerically shown that spatiotemporal patterns can be stored in neural networks. However, the mathematical mechanism for storage of the spatio-temporal patterns is still unknown, especially the eﬀects of LTD. In this paper, we employ a simple neural network model and show that interference of LTP and LTD disappears in a sparse coding scheme. On the other hand, it is known that the covariance learning is indispensable for storing sparse patterns. We also show that TAH qualitatively has the same eﬀect as the covariance learning when spatio-temporal patterns are embedded in the network. 1</p><p>6 0.47899604 <a title="150-lsi-6" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>7 0.45066309 <a title="150-lsi-7" href="./nips-2001-A_Hierarchical_Model_of_Complex_Cells_in_Visual_Cortex_for_the_Binocular_Perception_of_Motion-in-Depth.html">10 nips-2001-A Hierarchical Model of Complex Cells in Visual Cortex for the Binocular Perception of Motion-in-Depth</a></p>
<p>8 0.44952279 <a title="150-lsi-8" href="./nips-2001-Linking_Motor_Learning_to_Function_Approximation%3A_Learning_in_an_Unlearnable_Force_Field.html">116 nips-2001-Linking Motor Learning to Function Approximation: Learning in an Unlearnable Force Field</a></p>
<p>9 0.40844482 <a title="150-lsi-9" href="./nips-2001-Modularity_in_the_motor_system%3A_decomposition_of_muscle_patterns_as_combinations_of_time-varying_synergies.html">125 nips-2001-Modularity in the motor system: decomposition of muscle patterns as combinations of time-varying synergies</a></p>
<p>10 0.36972281 <a title="150-lsi-10" href="./nips-2001-Bayesian_morphometry_of_hippocampal_cells_suggests_same-cell_somatodendritic_repulsion.html">42 nips-2001-Bayesian morphometry of hippocampal cells suggests same-cell somatodendritic repulsion</a></p>
<p>11 0.32788718 <a title="150-lsi-11" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>12 0.32275969 <a title="150-lsi-12" href="./nips-2001-Orientational_and_Geometric_Determinants_of_Place_and_Head-direction.html">142 nips-2001-Orientational and Geometric Determinants of Place and Head-direction</a></p>
<p>13 0.31864259 <a title="150-lsi-13" href="./nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">102 nips-2001-KLD-Sampling: Adaptive Particle Filters</a></p>
<p>14 0.30041504 <a title="150-lsi-14" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>15 0.29887334 <a title="150-lsi-15" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>16 0.29166275 <a title="150-lsi-16" href="./nips-2001-Rao-Blackwellised_Particle_Filtering_via_Data_Augmentation.html">156 nips-2001-Rao-Blackwellised Particle Filtering via Data Augmentation</a></p>
<p>17 0.28439769 <a title="150-lsi-17" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>18 0.27751404 <a title="150-lsi-18" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>19 0.27429959 <a title="150-lsi-19" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>20 0.27422118 <a title="150-lsi-20" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.016), (19, 0.029), (20, 0.023), (27, 0.18), (30, 0.13), (38, 0.042), (45, 0.177), (59, 0.036), (72, 0.068), (79, 0.032), (83, 0.025), (91, 0.156)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91275489 <a title="150-lda-1" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>Author: Yun Gao, Michael J. Black, Elie Bienenstock, Shy Shoham, John P. Donoghue</p><p>Abstract: Statistical learning and probabilistic inference techniques are used to infer the hand position of a subject from multi-electrode recordings of neural activity in motor cortex. First, an array of electrodes provides training data of neural ﬁring conditioned on hand kinematics. We learn a nonparametric representation of this ﬁring activity using a Bayesian model and rigorously compare it with previous models using cross-validation. Second, we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using Bayesian inference. The learned ﬁring models of multiple cells are used to deﬁne a nonGaussian likelihood term which is combined with a prior probability for the kinematics. A particle ﬁltering method is used to represent, update, and propagate the posterior distribution over time. The approach is compared with traditional linear ﬁltering methods; the results suggest that it may be appropriate for neural prosthetic applications.</p><p>2 0.89120209 <a title="150-lda-2" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>Author: Dale Schuurmans, Relu Patrascu</p><p>Abstract: We present a simple approach for computing reasonable policies for factored Markov decision processes (MDPs), when the optimal value function can be approximated by a compact linear form. Our method is based on solving a single linear program that approximates the best linear fit to the optimal value function. By applying an efficient constraint generation procedure we obtain an iterative solution method that tackles concise linear programs. This direct linear programming approach experimentally yields a significant reduction in computation time over approximate value- and policy-iteration methods (sometimes reducing several hours to a few seconds). However, the quality of the solutions produced by linear programming is weaker-usually about twice the approximation error for the same approximating class. Nevertheless, the speed advantage allows one to use larger approximation classes to achieve similar error in reasonable time. 1</p><p>3 0.82053554 <a title="150-lda-3" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>Author: Paul Viola, Michael Jones</p><p>Abstract: This paper develops a new approach for extremely fast detection in domains where the distribution of positive and negative examples is highly skewed (e.g. face detection or database retrieval). In such domains a cascade of simple classiﬁers each trained to achieve high detection rates and modest false positive rates can yield a ﬁnal detector with many desirable features: including high detection rates, very low false positive rates, and fast performance. Achieving extremely high detection rates, rather than low error, is not a task typically addressed by machine learning algorithms. We propose a new variant of AdaBoost as a mechanism for training the simple classiﬁers used in the cascade. Experimental results in the domain of face detection show the training algorithm yields signiﬁcant improvements in performance over conventional AdaBoost. The ﬁnal face detection system can process 15 frames per second, achieves over 90% detection, and a false positive rate of 1 in a 1,000,000.</p><p>4 0.81872189 <a title="150-lda-4" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>Author: Bernd Heisele, Thomas Serre, Massimiliano Pontil, Thomas Vetter, Tomaso Poggio</p><p>Abstract: We describe an algorithm for automatically learning discriminative components of objects with SVM classiﬁers. It is based on growing image parts by minimizing theoretical bounds on the error probability of an SVM. Component-based face classiﬁers are then combined in a second stage to yield a hierarchical SVM classiﬁer. Experimental results in face classiﬁcation show considerable robustness against rotations in depth and suggest performance at signiﬁcantly better level than other face detection systems. Novel aspects of our approach are: a) an algorithm to learn component-based classiﬁcation experts and their combination, b) the use of 3-D morphable models for training, and c) a maximum operation on the output of each component classiﬁer which may be relevant for biological models of visual recognition.</p><p>5 0.81651253 <a title="150-lda-5" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>Author: Gregor Wenning, Klaus Obermayer</p><p>Abstract: Cortical neurons might be considered as threshold elements integrating in parallel many excitatory and inhibitory inputs. Due to the apparent variability of cortical spike trains this yields a strongly fluctuating membrane potential, such that threshold crossings are highly irregular. Here we study how a neuron could maximize its sensitivity w.r.t. a relatively small subset of excitatory input. Weak signals embedded in fluctuations is the natural realm of stochastic resonance. The neuron's response is described in a hazard-function approximation applied to an Ornstein-Uhlenbeck process. We analytically derive an optimality criterium and give a learning rule for the adjustment of the membrane fluctuations, such that the sensitivity is maximal exploiting stochastic resonance. We show that adaptation depends only on quantities that could easily be estimated locally (in space and time) by the neuron. The main results are compared with simulations of a biophysically more realistic neuron model. 1</p><p>6 0.8144626 <a title="150-lda-6" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<p>7 0.81387556 <a title="150-lda-7" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>8 0.81091928 <a title="150-lda-8" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>9 0.80724484 <a title="150-lda-9" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>10 0.80655241 <a title="150-lda-10" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<p>11 0.80535376 <a title="150-lda-11" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>12 0.80369759 <a title="150-lda-12" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>13 0.80357027 <a title="150-lda-13" href="./nips-2001-The_Method_of_Quantum_Clustering.html">185 nips-2001-The Method of Quantum Clustering</a></p>
<p>14 0.80270743 <a title="150-lda-14" href="./nips-2001-Classifying_Single_Trial_EEG%3A_Towards_Brain_Computer_Interfacing.html">50 nips-2001-Classifying Single Trial EEG: Towards Brain Computer Interfacing</a></p>
<p>15 0.80262816 <a title="150-lda-15" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>16 0.80229688 <a title="150-lda-16" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>17 0.80051041 <a title="150-lda-17" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>18 0.79867011 <a title="150-lda-18" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>19 0.79801321 <a title="150-lda-19" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>20 0.79749405 <a title="150-lda-20" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
