<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-150" href="#">nips2001-150</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</h1>
<br/><p>Source: <a title="nips-2001-150-pdf" href="http://papers.nips.cc/paper/1997-probabilistic-inference-of-hand-motion-from-neural-activity-in-motor-cortex.pdf">pdf</a></p><p>Author: Yun Gao, Michael J. Black, Elie Bienenstock, Shy Shoham, John P. Donoghue</p><p>Abstract: Statistical learning and probabilistic inference techniques are used to infer the hand position of a subject from multi-electrode recordings of neural activity in motor cortex. First, an array of electrodes provides training data of neural ﬁring conditioned on hand kinematics. We learn a nonparametric representation of this ﬁring activity using a Bayesian model and rigorously compare it with previous models using cross-validation. Second, we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using Bayesian inference. The learned ﬁring models of multiple cells are used to deﬁne a nonGaussian likelihood term which is combined with a prior probability for the kinematics. A particle ﬁltering method is used to represent, update, and propagate the posterior distribution over time. The approach is compared with traditional linear ﬁltering methods; the results suggest that it may be appropriate for neural prosthetic applications.</p><p>Reference: <a title="nips-2001-150-reference" href="../nips2001_reference/nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ring', 0.532), ('veloc', 0.35), ('cel', 0.347), ('lter', 0.175), ('poisson', 0.175), ('partic', 0.166), ('hand', 0.153), ('pq', 0.153), ('kinem', 0.126), ('cosin', 0.114), ('hatsopoulo', 0.112), ('ge', 0.11), ('mot', 0.104), ('arm', 0.103), ('mi', 0.102), ('postery', 0.095), ('monkey', 0.09), ('array', 0.085), ('robust', 0.079), ('ms', 0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="150-tfidf-1" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>Author: Yun Gao, Michael J. Black, Elie Bienenstock, Shy Shoham, John P. Donoghue</p><p>Abstract: Statistical learning and probabilistic inference techniques are used to infer the hand position of a subject from multi-electrode recordings of neural activity in motor cortex. First, an array of electrodes provides training data of neural ﬁring conditioned on hand kinematics. We learn a nonparametric representation of this ﬁring activity using a Bayesian model and rigorously compare it with previous models using cross-validation. Second, we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using Bayesian inference. The learned ﬁring models of multiple cells are used to deﬁne a nonGaussian likelihood term which is combined with a prior probability for the kinematics. A particle ﬁltering method is used to represent, update, and propagate the posterior distribution over time. The approach is compared with traditional linear ﬁltering methods; the results suggest that it may be appropriate for neural prosthetic applications.</p><p>2 0.41990328 <a title="150-tfidf-2" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>Author: Richard Hahnloser, Xiaohui Xie, H. S. Seung</p><p>Abstract: Integration in the head-direction system is a computation by which horizontal angular head velocity signals from the vestibular nuclei are integrated to yield a neural representation of head direction. In the thalamus, the postsubiculum and the mammillary nuclei, the head-direction representation has the form of a place code: neurons have a preferred head direction in which their ﬁring is maximal [Blair and Sharp, 1995, Blair et al., 1998, ?]. Integration is a difﬁcult computation, given that head-velocities can vary over a large range. Previous models of the head-direction system relied on the assumption that the integration is achieved in a ﬁring-rate-based attractor network with a ring structure. In order to correctly integrate head-velocity signals during high-speed head rotations, very fast synaptic dynamics had to be assumed. Here we address the question whether integration in the head-direction system is possible with slow synapses, for example excitatory NMDA and inhibitory GABA(B) type synapses. For neural networks with such slow synapses, rate-based dynamics are a good approximation of spiking neurons [Ermentrout, 1994]. We ﬁnd that correct integration during high-speed head rotations imposes strong constraints on possible network architectures.</p><p>3 0.21955265 <a title="150-tfidf-3" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>Author: Sebastian Thrun, John Langford, Vandi Verma</p><p>Abstract: We propose a new particle ﬁlter that incorporates a model of costs when generating particles. The approach is motivated by the observation that the costs of accidentally not tracking hypotheses might be signiﬁcant in some areas of state space, and next to irrelevant in others. By incorporating a cost model into particle ﬁltering, states that are more critical to the system performance are more likely to be tracked. Automatic calculation of the cost model is implemented using an MDP value function calculation that estimates the value of tracking a particular state. Experiments in two mobile robot domains illustrate the appropriateness of the approach.</p><p>4 0.20667534 <a title="150-tfidf-4" href="./nips-2001-A_Hierarchical_Model_of_Complex_Cells_in_Visual_Cortex_for_the_Binocular_Perception_of_Motion-in-Depth.html">10 nips-2001-A Hierarchical Model of Complex Cells in Visual Cortex for the Binocular Perception of Motion-in-Depth</a></p>
<p>Author: Silvio P. Sabatini, Fabio Solari, Giulia Andreani, Chiara Bartolozzi, Giacomo M. Bisio</p><p>Abstract: A cortical model for motion-in-depth selectivity of complex cells in the visual cortex is proposed. The model is based on a time extension of the phase-based techniques for disparity estimation. We consider the computation of the total temporal derivative of the time-varying disparity through the combination of the responses of disparity energy units. To take into account the physiological plausibility, the model is based on the combinations of binocular cells characterized by different ocular dominance indices. The resulting cortical units of the model show a sharp selectivity for motion-indepth that has been compared with that reported in the literature for real cortical cells. 1</p><p>5 0.20059516 <a title="150-tfidf-5" href="./nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">102 nips-2001-KLD-Sampling: Adaptive Particle Filters</a></p>
<p>Author: Dieter Fox</p><p>Abstract: Over the last years, particle ﬁlters have been applied with great success to a variety of state estimation problems. We present a statistical approach to increasing the efﬁciency of particle ﬁlters by adapting the size of sample sets on-the-ﬂy. The key idea of the KLD-sampling method is to bound the approximation error introduced by the sample-based representation of the particle ﬁlter. The name KLD-sampling is due to the fact that we measure the approximation error by the Kullback-Leibler distance. Our adaptation approach chooses a small number of samples if the density is focused on a small part of the state space, and it chooses a large number of samples if the state uncertainty is high. Both the implementation and computation overhead of this approach are small. Extensive experiments using mobile robot localization as a test application show that our approach yields drastic improvements over particle ﬁlters with ﬁxed sample set sizes and over a previously introduced adaptation technique.</p><p>6 0.1991829 <a title="150-tfidf-6" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>7 0.18365897 <a title="150-tfidf-7" href="./nips-2001-Linking_Motor_Learning_to_Function_Approximation%3A_Learning_in_an_Unlearnable_Force_Field.html">116 nips-2001-Linking Motor Learning to Function Approximation: Learning in an Unlearnable Force Field</a></p>
<p>8 0.17775932 <a title="150-tfidf-8" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>9 0.17098114 <a title="150-tfidf-9" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>10 0.15806589 <a title="150-tfidf-10" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>11 0.14907497 <a title="150-tfidf-11" href="./nips-2001-Orientational_and_Geometric_Determinants_of_Place_and_Head-direction.html">142 nips-2001-Orientational and Geometric Determinants of Place and Head-direction</a></p>
<p>12 0.13484026 <a title="150-tfidf-12" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>13 0.11857919 <a title="150-tfidf-13" href="./nips-2001-Stochastic_Mixed-Signal_VLSI_Architecture_for_High-Dimensional_Kernel_Machines.html">176 nips-2001-Stochastic Mixed-Signal VLSI Architecture for High-Dimensional Kernel Machines</a></p>
<p>14 0.10819415 <a title="150-tfidf-14" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>15 0.098074034 <a title="150-tfidf-15" href="./nips-2001-Characterizing_Neural_Gain_Control_using_Spike-triggered_Covariance.html">48 nips-2001-Characterizing Neural Gain Control using Spike-triggered Covariance</a></p>
<p>16 0.096510373 <a title="150-tfidf-16" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>17 0.09644495 <a title="150-tfidf-17" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>18 0.094350018 <a title="150-tfidf-18" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>19 0.093545236 <a title="150-tfidf-19" href="./nips-2001-Classifying_Single_Trial_EEG%3A_Towards_Brain_Computer_Interfacing.html">50 nips-2001-Classifying Single Trial EEG: Towards Brain Computer Interfacing</a></p>
<p>20 0.092474476 <a title="150-tfidf-20" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.241), (1, -0.246), (2, 0.021), (3, -0.104), (4, 0.099), (5, -0.039), (6, 0.18), (7, -0.297), (8, -0.149), (9, -0.043), (10, -0.077), (11, -0.108), (12, 0.204), (13, 0.075), (14, 0.13), (15, -0.148), (16, -0.153), (17, -0.21), (18, -0.168), (19, -0.168), (20, 0.133), (21, 0.005), (22, -0.057), (23, 0.004), (24, -0.124), (25, -0.036), (26, -0.077), (27, 0.05), (28, 0.03), (29, 0.024), (30, 0.008), (31, 0.056), (32, -0.053), (33, -0.059), (34, 0.12), (35, 0.037), (36, -0.088), (37, -0.097), (38, -0.009), (39, 0.071), (40, -0.006), (41, -0.031), (42, 0.017), (43, -0.035), (44, -0.057), (45, 0.062), (46, 0.102), (47, 0.048), (48, 0.021), (49, 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93437159 <a title="150-lsi-1" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>Author: Yun Gao, Michael J. Black, Elie Bienenstock, Shy Shoham, John P. Donoghue</p><p>Abstract: Statistical learning and probabilistic inference techniques are used to infer the hand position of a subject from multi-electrode recordings of neural activity in motor cortex. First, an array of electrodes provides training data of neural ﬁring conditioned on hand kinematics. We learn a nonparametric representation of this ﬁring activity using a Bayesian model and rigorously compare it with previous models using cross-validation. Second, we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using Bayesian inference. The learned ﬁring models of multiple cells are used to deﬁne a nonGaussian likelihood term which is combined with a prior probability for the kinematics. A particle ﬁltering method is used to represent, update, and propagate the posterior distribution over time. The approach is compared with traditional linear ﬁltering methods; the results suggest that it may be appropriate for neural prosthetic applications.</p><p>2 0.75599861 <a title="150-lsi-2" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>Author: Richard Hahnloser, Xiaohui Xie, H. S. Seung</p><p>Abstract: Integration in the head-direction system is a computation by which horizontal angular head velocity signals from the vestibular nuclei are integrated to yield a neural representation of head direction. In the thalamus, the postsubiculum and the mammillary nuclei, the head-direction representation has the form of a place code: neurons have a preferred head direction in which their ﬁring is maximal [Blair and Sharp, 1995, Blair et al., 1998, ?]. Integration is a difﬁcult computation, given that head-velocities can vary over a large range. Previous models of the head-direction system relied on the assumption that the integration is achieved in a ﬁring-rate-based attractor network with a ring structure. In order to correctly integrate head-velocity signals during high-speed head rotations, very fast synaptic dynamics had to be assumed. Here we address the question whether integration in the head-direction system is possible with slow synapses, for example excitatory NMDA and inhibitory GABA(B) type synapses. For neural networks with such slow synapses, rate-based dynamics are a good approximation of spiking neurons [Ermentrout, 1994]. We ﬁnd that correct integration during high-speed head rotations imposes strong constraints on possible network architectures.</p><p>3 0.59120154 <a title="150-lsi-3" href="./nips-2001-A_Hierarchical_Model_of_Complex_Cells_in_Visual_Cortex_for_the_Binocular_Perception_of_Motion-in-Depth.html">10 nips-2001-A Hierarchical Model of Complex Cells in Visual Cortex for the Binocular Perception of Motion-in-Depth</a></p>
<p>Author: Silvio P. Sabatini, Fabio Solari, Giulia Andreani, Chiara Bartolozzi, Giacomo M. Bisio</p><p>Abstract: A cortical model for motion-in-depth selectivity of complex cells in the visual cortex is proposed. The model is based on a time extension of the phase-based techniques for disparity estimation. We consider the computation of the total temporal derivative of the time-varying disparity through the combination of the responses of disparity energy units. To take into account the physiological plausibility, the model is based on the combinations of binocular cells characterized by different ocular dominance indices. The resulting cortical units of the model show a sharp selectivity for motion-indepth that has been compared with that reported in the literature for real cortical cells. 1</p><p>4 0.54691273 <a title="150-lsi-4" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>Author: N. Matsumoto, M. Okada</p><p>Abstract: Recent biological experimental ﬁndings have shown that the synaptic plasticity depends on the relative timing of the pre- and postsynaptic spikes which determines whether Long Term Potentiation (LTP) occurs or Long Term Depression (LTD) does. The synaptic plasticity has been called “Temporally Asymmetric Hebbian plasticity (TAH)”. Many authors have numerically shown that spatiotemporal patterns can be stored in neural networks. However, the mathematical mechanism for storage of the spatio-temporal patterns is still unknown, especially the eﬀects of LTD. In this paper, we employ a simple neural network model and show that interference of LTP and LTD disappears in a sparse coding scheme. On the other hand, it is known that the covariance learning is indispensable for storing sparse patterns. We also show that TAH qualitatively has the same eﬀect as the covariance learning when spatio-temporal patterns are embedded in the network. 1</p><p>5 0.51764786 <a title="150-lsi-5" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>Author: Antonino Casile, Michele Rucci</p><p>Abstract: Neural activity appears to be a crucial component for shaping the receptive ﬁelds of cortical simple cells into adjacent, oriented subregions alternately receiving ON- and OFF-center excitatory geniculate inputs. It is known that the orientation selective responses of V1 neurons are reﬁned by visual experience. After eye opening, the spatiotemporal structure of neural activity in the early stages of the visual pathway depends both on the visual environment and on how the environment is scanned. We have used computational modeling to investigate how eye movements might affect the reﬁnement of the orientation tuning of simple cells in the presence of a Hebbian scheme of synaptic plasticity. Levels of correlation between the activity of simulated cells were examined while natural scenes were scanned so as to model sequences of saccades and ﬁxational eye movements, such as microsaccades, tremor and ocular drift. The speciﬁc patterns of activity required for a quantitatively accurate development of simple cell receptive ﬁelds with segregated ON and OFF subregions were observed during ﬁxational eye movements, but not in the presence of saccades or with static presentation of natural visual input. These results suggest an important role for the eye movements occurring during visual ﬁxation in the reﬁnement of orientation selectivity.</p><p>6 0.48947799 <a title="150-lsi-6" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>7 0.45111498 <a title="150-lsi-7" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>8 0.40633276 <a title="150-lsi-8" href="./nips-2001-Linking_Motor_Learning_to_Function_Approximation%3A_Learning_in_an_Unlearnable_Force_Field.html">116 nips-2001-Linking Motor Learning to Function Approximation: Learning in an Unlearnable Force Field</a></p>
<p>9 0.39767948 <a title="150-lsi-9" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>10 0.39005059 <a title="150-lsi-10" href="./nips-2001-Bayesian_morphometry_of_hippocampal_cells_suggests_same-cell_somatodendritic_repulsion.html">42 nips-2001-Bayesian morphometry of hippocampal cells suggests same-cell somatodendritic repulsion</a></p>
<p>11 0.38635057 <a title="150-lsi-11" href="./nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">102 nips-2001-KLD-Sampling: Adaptive Particle Filters</a></p>
<p>12 0.31883773 <a title="150-lsi-12" href="./nips-2001-Orientational_and_Geometric_Determinants_of_Place_and_Head-direction.html">142 nips-2001-Orientational and Geometric Determinants of Place and Head-direction</a></p>
<p>13 0.31183568 <a title="150-lsi-13" href="./nips-2001-Stochastic_Mixed-Signal_VLSI_Architecture_for_High-Dimensional_Kernel_Machines.html">176 nips-2001-Stochastic Mixed-Signal VLSI Architecture for High-Dimensional Kernel Machines</a></p>
<p>14 0.29938442 <a title="150-lsi-14" href="./nips-2001-Modularity_in_the_motor_system%3A_decomposition_of_muscle_patterns_as_combinations_of_time-varying_synergies.html">125 nips-2001-Modularity in the motor system: decomposition of muscle patterns as combinations of time-varying synergies</a></p>
<p>15 0.29760507 <a title="150-lsi-15" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>16 0.29605135 <a title="150-lsi-16" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>17 0.29225442 <a title="150-lsi-17" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>18 0.28763875 <a title="150-lsi-18" href="./nips-2001-Incremental_A%2A.html">93 nips-2001-Incremental A*</a></p>
<p>19 0.28379288 <a title="150-lsi-19" href="./nips-2001-Rao-Blackwellised_Particle_Filtering_via_Data_Augmentation.html">156 nips-2001-Rao-Blackwellised Particle Filtering via Data Augmentation</a></p>
<p>20 0.28281948 <a title="150-lsi-20" href="./nips-2001-Learning_Body_Pose_via_Specialized_Maps.html">108 nips-2001-Learning Body Pose via Specialized Maps</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.048), (16, 0.223), (31, 0.051), (36, 0.01), (43, 0.141), (47, 0.014), (50, 0.055), (63, 0.011), (77, 0.018), (79, 0.035), (91, 0.162), (92, 0.069), (93, 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89172238 <a title="150-lda-1" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>Author: Yun Gao, Michael J. Black, Elie Bienenstock, Shy Shoham, John P. Donoghue</p><p>Abstract: Statistical learning and probabilistic inference techniques are used to infer the hand position of a subject from multi-electrode recordings of neural activity in motor cortex. First, an array of electrodes provides training data of neural ﬁring conditioned on hand kinematics. We learn a nonparametric representation of this ﬁring activity using a Bayesian model and rigorously compare it with previous models using cross-validation. Second, we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using Bayesian inference. The learned ﬁring models of multiple cells are used to deﬁne a nonGaussian likelihood term which is combined with a prior probability for the kinematics. A particle ﬁltering method is used to represent, update, and propagate the posterior distribution over time. The approach is compared with traditional linear ﬁltering methods; the results suggest that it may be appropriate for neural prosthetic applications.</p><p>2 0.87001109 <a title="150-lda-2" href="./nips-2001-Linking_Motor_Learning_to_Function_Approximation%3A_Learning_in_an_Unlearnable_Force_Field.html">116 nips-2001-Linking Motor Learning to Function Approximation: Learning in an Unlearnable Force Field</a></p>
<p>Author: O. Donchin, Reza Shadmehr</p><p>Abstract: Reaching movements require the brain to generate motor commands that rely on an internal model of the task’s dynamics. Here we consider the errors that subjects make early in their reaching trajectories to various targets as they learn an internal model. Using a framework from function approximation, we argue that the sequence of errors should reﬂect the process of gradient descent. If so, then the sequence of errors should obey hidden state transitions of a simple dynamical system. Fitting the system to human data, we ﬁnd a surprisingly good ﬁt accounting for 98% of the variance. This allows us to draw tentative conclusions about the basis elements used by the brain in transforming sensory space to motor commands. To test the robustness of the results, we estimate the shape of the basis elements under two conditions: in a traditional learning paradigm with a consistent force ﬁeld, and in a random sequence of force ﬁelds where learning is not possible. Remarkably, we ﬁnd that the basis remains invariant. 1</p><p>3 0.86763138 <a title="150-lda-3" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>Author: Jonathan L. Shapiro, J. Wearden</p><p>Abstract: Animal data on delayed-reward conditioning experiments shows a striking property - the data for different time intervals collapses into a single curve when the data is scaled by the time interval. This is called the scalar property of interval timing. Here a simple model of a neural clock is presented and shown to give rise to the scalar property. The model is an accumulator consisting of noisy, linear spiking neurons. It is analytically tractable and contains only three parameters. When coupled with reinforcement learning it simulates peak procedure experiments, producing both the scalar property and the pattern of single trial covariances. 1</p><p>4 0.86562157 <a title="150-lda-4" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>Author: Polina Golland</p><p>Abstract: In many scientiﬁc and engineering applications, detecting and understanding differences between two groups of examples can be reduced to a classical problem of training a classiﬁer for labeling new examples while making as few mistakes as possible. In the traditional classiﬁcation setting, the resulting classiﬁer is rarely analyzed in terms of the properties of the input data captured by the discriminative model. However, such analysis is crucial if we want to understand and visualize the detected differences. We propose an approach to interpretation of the statistical model in the original feature space that allows us to argue about the model in terms of the relevant changes to the input vectors. For each point in the input space, we deﬁne a discriminative direction to be the direction that moves the point towards the other class while introducing as little irrelevant change as possible with respect to the classiﬁer function. We derive the discriminative direction for kernel-based classiﬁers, demonstrate the technique on several examples and brieﬂy discuss its use in the statistical shape analysis, an application that originally motivated this work.</p><p>5 0.85874057 <a title="150-lda-5" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>Author: Shimon Edelman, Benjamin P. Hiles, Hwajin Yang, Nathan Intrator</p><p>Abstract: To ﬁnd out how the representations of structured visual objects depend on the co-occurrence statistics of their constituents, we exposed subjects to a set of composite images with tight control exerted over (1) the conditional probabilities of the constituent fragments, and (2) the value of Barlow’s criterion of “suspicious coincidence” (the ratio of joint probability to the product of marginals). We then compared the part veriﬁcation response times for various probe/target combinations before and after the exposure. For composite probes, the speedup was much larger for targets that contained pairs of fragments perfectly predictive of each other, compared to those that did not. This effect was modulated by the signiﬁcance of their co-occurrence as estimated by Barlow’s criterion. For lone-fragment probes, the speedup in all conditions was generally lower than for composites. These results shed light on the brain’s strategies for unsupervised acquisition of structural information in vision. 1 Motivation How does the human visual system decide for which objects it should maintain distinct and persistent internal representations of the kind typically postulated by theories of object recognition? Consider, for example, the image shown in Figure 1, left. This image can be represented as a monolithic hieroglyph, a pair of Chinese characters (which we shall refer to as and ), a set of strokes, or, trivially, as a collection of pixels. Note that the second option is only available to a system previously exposed to various combinations of Chinese characters. Indeed, a principled decision whether to represent this image as , or otherwise can only be made on the basis of prior exposure to related images. £ ¡ £¦ ¡ £ ¥¨§¢   ¥¤¢   ¢ According to Barlow’s [1] insight, one useful principle is tallying suspicious coincidences: two candidate fragments and should be combined into a composite object if the probability of their joint appearance is much higher than , which is the probability expected in the case of their statistical independence. This criterion may be compared to the Minimum Description Length (MDL) principle, which has been previously discussed in the context of object representation [2, 3]. In a simpliﬁed form [4], MDL calls for representing explicitly as a whole if , just as the principle of suspicious coincidences does. £ ©¢  £  ¢ ¥¤¥  £¦ ¢ ¥  £  ¢   £¦ ¢ ¥¤¥! ¨§¥ £ ¢ £ ©¢  £¦  £ ¨§¢¥ ¡ ¢   While the Barlow/MDL criterion certainly indicates a suspicious coincidence, there are additional probabilistic considerations that may be used and . One example is the possiin setting the degree of association between ble perfect predictability of from and vice versa, as measured by . If , then and are perfectly predictive of each other and should really be coded by a single symbol, whereas the MDL criterion may suggest merely that some association between the representation of and that of be established. In comparison, if and are not perfectly predictive of each other ( ), there is a case to be made in favor of coding them separately to allow for a maximally expressive representation, whereas MDL may actually suggest a high degree of association ). In this study we investigated whether the human (if visual system uses a criterion based on alongside MDL while learning (in an unsupervised manner) to represent composite objects. £ £  £ ¢  ¥  ¥ © §¥ ¡ ¢  ¨¦¤</p><p>6 0.85104996 <a title="150-lda-6" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>7 0.8502388 <a title="150-lda-7" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>8 0.84954077 <a title="150-lda-8" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>9 0.84777749 <a title="150-lda-9" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>10 0.84551316 <a title="150-lda-10" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>11 0.84507537 <a title="150-lda-11" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>12 0.8419885 <a title="150-lda-12" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>13 0.84050465 <a title="150-lda-13" href="./nips-2001-A_Rational_Analysis_of_Cognitive_Control_in_a_Speeded_Discrimination_Task.html">18 nips-2001-A Rational Analysis of Cognitive Control in a Speeded Discrimination Task</a></p>
<p>14 0.83755326 <a title="150-lda-14" href="./nips-2001-Classifying_Single_Trial_EEG%3A_Towards_Brain_Computer_Interfacing.html">50 nips-2001-Classifying Single Trial EEG: Towards Brain Computer Interfacing</a></p>
<p>15 0.83709204 <a title="150-lda-15" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>16 0.83394086 <a title="150-lda-16" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>17 0.83375776 <a title="150-lda-17" href="./nips-2001-Fragment_Completion_in_Humans_and_Machines.html">78 nips-2001-Fragment Completion in Humans and Machines</a></p>
<p>18 0.82963383 <a title="150-lda-18" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>19 0.82926381 <a title="150-lda-19" href="./nips-2001-A_Maximum-Likelihood_Approach_to_Modeling_Multisensory_Enhancement.html">11 nips-2001-A Maximum-Likelihood Approach to Modeling Multisensory Enhancement</a></p>
<p>20 0.82646686 <a title="150-lda-20" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
