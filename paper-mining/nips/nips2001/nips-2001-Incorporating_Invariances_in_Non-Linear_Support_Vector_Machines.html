<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-92" href="#">nips2001-92</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</h1>
<br/><p>Source: <a title="nips-2001-92-pdf" href="http://papers.nips.cc/paper/2024-incorporating-invariances-in-non-linear-support-vector-machines.pdf">pdf</a></p><p>Author: Olivier Chapelle, Bernhard Schćž&scaron;lkopf</p><p>Abstract: The choice of an SVM kernel corresponds to the choice of a representation of the data in a feature space and, to improve performance , it should therefore incorporate prior knowledge such as known transformation invariances. We propose a technique which extends earlier work and aims at incorporating invariances in nonlinear kernels. We show on a digit recognition task that the proposed approach is superior to the Virtual Support Vector method, which previously had been the method of choice. 1</p><p>Reference: <a title="nips-2001-92-reference" href="../nips2001_reference/nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de Max-Planck-Institute, Tiibingen, Germany Biowulf Technologies  Abstract The choice of an SVM kernel corresponds to the choice of a representation of the data in a feature space and, to improve performance , it should therefore incorporate prior knowledge such as known transformation invariances. [sent-6, score-0.261]
</p><p>2 We propose a technique which extends earlier work and aims at incorporating invariances in nonlinear kernels. [sent-7, score-0.428]
</p><p>3 We show on a digit recognition task that the proposed approach is superior to the Virtual Support Vector method, which previously had been the method of choice. [sent-8, score-0.204]
</p><p>4 1  Introduction  In some classification tasks, an a priori knowledge is known about the invariances related to the task. [sent-9, score-0.365]
</p><p>5 For instance, in image classification, we know that the label of a given image should not change after a small translation or rotation. [sent-10, score-0.131]
</p><p>6 More generally, we assume we know a local transformation Lt depending on a parameter t (for instance, a vertical translation of t pixels) such that any point x should be considered equivalent to LtX, the transformed point. [sent-11, score-0.223]
</p><p>7 Ideally, the output of the learned function should be constant when its inputs are transformed by the desired invariance. [sent-12, score-0.032]
</p><p>8 It has been shown [1] that one can not find a non-trivial kernel which is globally invariant. [sent-13, score-0.134]
</p><p>9 For this reason, we consider here local invariances and for this purpose we associate at each training point X i a tangent vector dXi, dXi =  1 lim - (LtXi t--+o  t  Xi)  81 LtXi t=o  = -  8t  In practice dXi can be either computed by finite difference or by differentiation. [sent-14, score-0.692]
</p><p>10 Note that generally one can consider more than one invariance transformation. [sent-15, score-0.189]
</p><p>11 A common way of introducing invariances in a learning system is to add the perturbed examples LtXi in the training set [7]. [sent-16, score-0.439]
</p><p>12 In the SVM framework , when applied only to the SVs, it leads to the Virtual Support Vector (VSV) method [10]. [sent-18, score-0.048]
</p><p>13 An alternative to this is to modify directly the cost function in order to take into account the tangent vectors. [sent-19, score-0.153]
</p><p>14 The aim of the present work is to extend these methods to the case of nonlinear SVMs which will be achieved mainly by using the kernel peA trick  [12]. [sent-21, score-0.216]
</p><p>15 After introducing the basics of Support Vector Machines in section 2, we recall the method proposed in [11] to train invariant linear SVMs (section 3). [sent-23, score-0.256]
</p><p>16 In section 4, we show how to extend it to the nonlinear case and finally experimental results are provided in section 5. [sent-24, score-0.148]
</p><p>17 2  Support Vector Learning  We introduce some standard notations for SVMs; for a complete description, see [15]. [sent-25, score-0.062]
</p><p>18 ,   (x n )} , the kernel peA map 'i/J : X -+ ~n is defined coordinatewise as  'i/Jp (x) =  (x) . [sent-29, score-0.167]
</p><p>19 Each principal direction has a linear expansion on the training points { (Xi)} and the coefficients of this expansion are obtained using kernel peA [12]. [sent-31, score-0.326]
</p><p>20 Writing the eigendecompostion of K as K = U AUT, with U an orthonormal matrix and A a diagonal one, it turns out that the the kernel peA map reads  'i/J(x) = A-1/2U T k(x),  (9)  where k (x) = (K(x, Xl)"'" K(x, xn)) T . [sent-32, score-0.167]
</p><p>21 This reflects the fact that if we retain all principal components, kernel peA is just a basis transform in E, leaving the dot product of training points invariant. [sent-36, score-0.268]
</p><p>22 As a consequence, training a nonlinear SVM on {Xl , . [sent-37, score-0.176]
</p><p>23 , xn} is equivalent to training a linear SVM on {'i/J(xd, . [sent-40, score-0.094]
</p><p>24 ,'i/J(xn )} and thus, thanks to the nonlinear mapping 'i/J, we can work directly in the linear space E and use exactly the technique described for invariant linear SVMs (section 3) . [sent-43, score-0.2]
</p><p>25 However the invariance directions d (Xi) do not necessarily belong to E. [sent-44, score-0.189]
</p><p>26 By projecting them onto E, some information might be lost. [sent-45, score-0.028]
</p><p>27 The hope is that this approximation will give a similar decision function to the exact one obtained in section 4. [sent-46, score-0.082]
</p><p>28 Finally, the proposed algorithm consists in training an invariant linear SVM as described in section 3 with training set { 'i/J(XI) , . [sent-48, score-0.339]
</p><p>29 3  Comparisons with the VSV method  One might wonder what is the difference between enforcing an invariance and just adding the virtual examples LtXi in the training set. [sent-55, score-0.783]
</p><p>30 This is the idea of the Virtual Support Vector (VSV) method [10] . [sent-58, score-0.048]
</p><p>31 The reason is the following: if a training point Xi is far from the margin, adding the virtual example LtXi will not change the decision boundary since neither of the points can become a support vector. [sent-59, score-0.729]
</p><p>32 Hence adding  virtual examples in the SVM framework enforces invariance only around the decision boundary, which, as an aside, is the main reason why the virtual SV method only adds virtual examples generated from points that were support vectors in the earlier iteration. [sent-60, score-1.493]
</p><p>33 One might argue that the points which are far from the decision boundary do not provide any information anyway. [sent-61, score-0.177]
</p><p>34 On the other hand, there is some merit in not only keeping the output label invariant under the transformation Lt, but also the real-valued output. [sent-62, score-0.213]
</p><p>35 This can be justified by seeing the distance of a given point to the margin as an indication of its class-conditional probability [8]. [sent-63, score-0.034]
</p><p>36 It appears reasonable that an invariance transformation should not affect this probability too much. [sent-64, score-0.257]
</p><p>37 5  Experiments  In our experiments, we compared a standard SVM with several methods taking into account invariances: standard SVM with virtual examples (cf. [sent-65, score-0.393]
</p><p>38 the VSV method [10]) [VSV], invariant SVM as described in section 4. [sent-66, score-0.199]
</p><p>39 1 [ISVM] and invariant hyperplane in kernel peA coordinates as described in section 4. [sent-67, score-0.285]
</p><p>40 The hybrid method described in [11] (see end of section 3) did not perform better than the VSV method and is not included in our experiments for this reason. [sent-69, score-0.129]
</p><p>41 Note that in the following experiments, each tangent vector d (Xi) has been normalized by the average length Ild (xi)W/n in order to be scale independent. [sent-70, score-0.208]
</p><p>42 1  Toy problem  The toy problem we considered is the following: the training data has been generated uniformly from [-1 , 1]2. [sent-72, score-0.224]
</p><p>43 The true decision boundary is a circle centered at the origin: f(x) = sign(x2 - 0. [sent-73, score-0.141]
</p><p>44 The a priori knowledge we want to encode in this toy problem is local invariance under rotations. [sent-75, score-0.381]
</p><p>45 Therefore, the output of the decision function on a given training point Xi and on its image R(Xi,C:) obtained by a small rotation should be as similar as possible. [sent-76, score-0.175]
</p><p>46 To each training point, we associate a tangent vector dXi which is actually orthogonal to Xi. [sent-77, score-0.344]
</p><p>47 A training set of 30 points was generated and the experiments were repeated 100 times. [sent-78, score-0.134]
</p><p>48 A Gaussian kernel K(x,y) = exp (_ II X2~~ 1I 2) was chosen. [sent-79, score-0.134]
</p><p>49 Adding virtual examples (VSV method) is already very useful since it made the test error decrease from 6. [sent-81, score-0.403]
</p><p>50 On this toy problem, the more the invariances are enforced b -+ 1), the better the performances are (see right side of figure 1), reaching a test error of 1. [sent-85, score-0.55]
</p><p>51 4 and log a = 0 (right side of of figure 1), one notices that the decrease in the test error does not have the same speed. [sent-88, score-0.153]
</p><p>52 This is actually the dual of the phenomenon observed on the left side of this figure: for a same value of gamma, the test error tends to increase, when a is larger. [sent-89, score-0.108]
</p><p>53 5 Log sigma  % '------~-~-~-~ 6 8-- 0 -~ ,~ , 2~  - Log (1-gamma)  Figure 1: Left: test error for different learning algorithms plotted against the width of a RBF kernel and "( fixed to 0. [sent-105, score-0.304]
</p><p>54 Right: test error of IHKPcA across "( and for different values of (5. [sent-107, score-0.076]
</p><p>55 The test errors are averaged over the 100 splits and the error bars correspond to the standard deviation of the means. [sent-108, score-0.179]
</p><p>56 2  Handwritten digit recognition  As a real world experiment, we tried to incorporate invariances for a handwritten digit recognition task. [sent-110, score-0.648]
</p><p>57 It consists of 7291 training and 2007 test examples. [sent-112, score-0.142]
</p><p>58 According to [9], the best performance has been obtained for a polynomial kernel of degree 3, and all the results described in this section were performed using this kernel. [sent-113, score-0.167]
</p><p>59 All the tangent vectors have been computed by a finite difference between the original digit and its I-pixel translated. [sent-115, score-0.274]
</p><p>60 We split the training set into 23 subsets of 317 training examples after a random permutation of the training and test set. [sent-116, score-0.381]
</p><p>61 Also we concentrated on a binary classification problem, namely separating digits a to 4 against 5 to 9. [sent-117, score-0.07]
</p><p>62 From those figures, it can be seen that the difference between ISVM (the original method) and IHKPcA (the approximation) is much larger than in the toy example. [sent-120, score-0.158]
</p><p>63 The difference to the toy example is probably due to the input dimensionality. [sent-121, score-0.158]
</p><p>64 In 2 dimensions, with an RBF kernel, the 30 examples of the toy problem "almost span" the whole feature space, whereas with 256 dimensions , this is no longer the case. [sent-122, score-0.181]
</p><p>65 What is noteworthy in these experiments is that our proposed method is much better than the standard VSV. [sent-123, score-0.081]
</p><p>66 3, the reason for this might be that invariance is enforced around all training points and not only around support vectors. [sent-125, score-0.567]
</p><p>67 Note that what we call VSV here is a standard SVM with a double size training set containing the original data points and their translates. [sent-126, score-0.167]
</p><p>68 The horizontal invariance yields larger improvements than the vertical one. [sent-127, score-0.29]
</p><p>69 One of the reason might be that the digits in the USPS database are already centered vertically. [sent-128, score-0.191]
</p><p>70 5  -Log (1-gamma)  Vertical translation (to the top)  0 . [sent-148, score-0.067]
</p><p>71 5  -Log (1-gamma)  Horizontal translation (to the right)  Figure 2: Comparison of ISVM, IHKPcA and VSV on the USPS dataset. [sent-153, score-0.067]
</p><p>72 The left of the plot ("( = 0) corresponds to standard SVM whereas the right part of the plot h -+ 1) means that a lot of emphasis is put on the enforcement of the constraints. [sent-154, score-0.033]
</p><p>73 The test errors are averaged over the 23 splits and the error bars correspond to the standard deviation of the means. [sent-155, score-0.179]
</p><p>74 6  Conclusion  We have extended a method for constructing invariant hyperplanes to the nonlinear case. [sent-156, score-0.248]
</p><p>75 We have shown results that are superior to the virtual SV method. [sent-157, score-0.303]
</p><p>76 The latter has recently broken the record on the NIST database which is the "gold standard" of handwritten digit benchmarks [5], therefore it appears promising to also try the new system on that task. [sent-158, score-0.193]
</p><p>77 For this propose, a large scale version of this method needs to be derived. [sent-159, score-0.048]
</p><p>78 The first idea we tried is to compute the kernel PCA map using only a subset of the training points. [sent-160, score-0.298]
</p><p>79 Encouraging results have been obtained on the lO-class USPS database (with the whole training set), but other methods are also currently under study. [sent-161, score-0.135]
</p><p>80 Incorporating prior information in machine learning by creating virtual examples. [sent-209, score-0.276]
</p><p>81 In Artificial Neural Networks - ICANN'96, volume 1112, pages 47- 52, Berlin, 1996. [sent-234, score-0.032]
</p><p>82 Transformation invariance in pattern recognition, tangent distance and tangent propagation. [sent-258, score-0.495]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('vsv', 0.433), ('ihkpca', 0.289), ('virtual', 0.276), ('invariances', 0.266), ('isvm', 0.253), ('invariance', 0.189), ('ltxi', 0.18), ('xi', 0.177), ('pea', 0.16), ('dxi', 0.157), ('tangent', 0.153), ('svm', 0.152), ('kernel', 0.134), ('toy', 0.13), ('invariant', 0.118), ('support', 0.113), ('usps', 0.106), ('training', 0.094), ('sigma', 0.094), ('digit', 0.093), ('chapelle', 0.086), ('nonlinear', 0.082), ('incorporating', 0.08), ('xn', 0.074), ('transformation', 0.068), ('translation', 0.067), ('svms', 0.061), ('burges', 0.061), ('boundary', 0.06), ('handwritten', 0.059), ('reason', 0.057), ('vertical', 0.056), ('xj', 0.056), ('smola', 0.055), ('vector', 0.055), ('simard', 0.053), ('examples', 0.051), ('decision', 0.049), ('test', 0.048), ('method', 0.048), ('lt', 0.048), ('biowulf', 0.046), ('enforced', 0.046), ('horizontal', 0.045), ('log', 0.045), ('associate', 0.042), ('splits', 0.042), ('database', 0.041), ('technologies', 0.041), ('points', 0.04), ('adding', 0.04), ('sv', 0.038), ('xd', 0.038), ('machines', 0.038), ('classification', 0.037), ('tried', 0.037), ('recognition', 0.036), ('rbf', 0.036), ('editors', 0.035), ('margin', 0.034), ('digits', 0.033), ('map', 0.033), ('standard', 0.033), ('section', 0.033), ('side', 0.032), ('volume', 0.032), ('centered', 0.032), ('transformed', 0.032), ('image', 0.032), ('aut', 0.031), ('orr', 0.031), ('nist', 0.031), ('ild', 0.031), ('niyogi', 0.031), ('knowledge', 0.031), ('priori', 0.031), ('editor', 0.03), ('expansion', 0.029), ('november', 0.029), ('bousquet', 0.029), ('wonder', 0.029), ('svs', 0.029), ('decoste', 0.029), ('basics', 0.029), ('notations', 0.029), ('difference', 0.028), ('bars', 0.028), ('incorporate', 0.028), ('error', 0.028), ('introducing', 0.028), ('might', 0.028), ('superior', 0.027), ('purpose', 0.027), ('enforces', 0.027), ('aside', 0.027), ('merit', 0.027), ('lecun', 0.027), ('denker', 0.027), ('gold', 0.027), ('lim', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="92-tfidf-1" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>Author: Olivier Chapelle, Bernhard Schćž&scaron;lkopf</p><p>Abstract: The choice of an SVM kernel corresponds to the choice of a representation of the data in a feature space and, to improve performance , it should therefore incorporate prior knowledge such as known transformation invariances. We propose a technique which extends earlier work and aims at incorporating invariances in nonlinear kernels. We show on a digit recognition task that the proposed approach is superior to the Virtual Support Vector method, which previously had been the method of choice. 1</p><p>2 0.14806961 <a title="92-tfidf-2" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>Author: Hiroshi Shimodaira, Ken-ichi Noma, Mitsuru Nakai, Shigeki Sagayama</p><p>Abstract: A new class of Support Vector Machine (SVM) that is applicable to sequential-pattern recognition such as speech recognition is developed by incorporating an idea of non-linear time alignment into the kernel function. Since the time-alignment operation of sequential pattern is embedded in the new kernel function, standard SVM training and classiﬁcation algorithms can be employed without further modiﬁcations. The proposed SVM (DTAK-SVM) is evaluated in speaker-dependent speech recognition experiments of hand-segmented phoneme recognition. Preliminary experimental results show comparable recognition performance with hidden Markov models (HMMs). 1</p><p>3 0.13572589 <a title="92-tfidf-3" href="./nips-2001-A_Parallel_Mixture_of_SVMs_for_Very_Large_Scale_Problems.html">16 nips-2001-A Parallel Mixture of SVMs for Very Large Scale Problems</a></p>
<p>Author: Ronan Collobert, Samy Bengio, Yoshua Bengio</p><p>Abstract: Support Vector Machines (SVMs) are currently the state-of-the-art models for many classification problems but they suffer from the complexity of their training algorithm which is at least quadratic with respect to the number of examples. Hence, it is hopeless to try to solve real-life problems having more than a few hundreds of thousands examples with SVMs. The present paper proposes a new mixture of SVMs that can be easily implemented in parallel and where each SVM is trained on a small subset of the whole dataset. Experiments on a large benchmark dataset (Forest) as well as a difficult speech database , yielded significant time improvement (time complexity appears empirically to locally grow linearly with the number of examples) . In addition, and that is a surprise, a significant improvement in generalization was observed on Forest. 1</p><p>4 0.1334165 <a title="92-tfidf-4" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>Author: Manfred Opper, Robert Urbanczik</p><p>Abstract: Using methods of Statistical Physics, we investigate the rOle of model complexity in learning with support vector machines (SVMs). We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error. Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel. 1</p><p>5 0.13037042 <a title="92-tfidf-5" href="./nips-2001-MIME%3A_Mutual_Information_Minimization_and_Entropy_Maximization_for_Bayesian_Belief_Propagation.html">117 nips-2001-MIME: Mutual Information Minimization and Entropy Maximization for Bayesian Belief Propagation</a></p>
<p>Author: Anand Rangarajan, Alan L. Yuille</p><p>Abstract: Bayesian belief propagation in graphical models has been recently shown to have very close ties to inference methods based in statistical physics. After Yedidia et al. demonstrated that belief propagation ﬁxed points correspond to extrema of the so-called Bethe free energy, Yuille derived a double loop algorithm that is guaranteed to converge to a local minimum of the Bethe free energy. Yuille’s algorithm is based on a certain decomposition of the Bethe free energy and he mentions that other decompositions are possible and may even be fruitful. In the present work, we begin with the Bethe free energy and show that it has a principled interpretation as pairwise mutual information minimization and marginal entropy maximization (MIME). Next, we construct a family of free energy functions from a spectrum of decompositions of the original Bethe free energy. For each free energy in this family, we develop a new algorithm that is guaranteed to converge to a local minimum. Preliminary computer simulations are in agreement with this theoretical development. 1</p><p>6 0.11984682 <a title="92-tfidf-6" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>7 0.11339685 <a title="92-tfidf-7" href="./nips-2001-K-Local_Hyperplane_and_Convex_Distance_Nearest_Neighbor_Algorithms.html">101 nips-2001-K-Local Hyperplane and Convex Distance Nearest Neighbor Algorithms</a></p>
<p>8 0.11273747 <a title="92-tfidf-8" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>9 0.11243951 <a title="92-tfidf-9" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>10 0.10650589 <a title="92-tfidf-10" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>11 0.10236863 <a title="92-tfidf-11" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>12 0.097481452 <a title="92-tfidf-12" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>13 0.092787504 <a title="92-tfidf-13" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>14 0.090411916 <a title="92-tfidf-14" href="./nips-2001-Kernel_Feature_Spaces_and_Nonlinear_Blind_Souce_Separation.html">103 nips-2001-Kernel Feature Spaces and Nonlinear Blind Souce Separation</a></p>
<p>15 0.088312447 <a title="92-tfidf-15" href="./nips-2001-On_the_Concentration_of_Spectral_Properties.html">136 nips-2001-On the Concentration of Spectral Properties</a></p>
<p>16 0.087746844 <a title="92-tfidf-16" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>17 0.087601684 <a title="92-tfidf-17" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>18 0.086749621 <a title="92-tfidf-18" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>19 0.084020168 <a title="92-tfidf-19" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>20 0.080659457 <a title="92-tfidf-20" href="./nips-2001-Semi-supervised_MarginBoost.html">167 nips-2001-Semi-supervised MarginBoost</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.224), (1, 0.145), (2, -0.062), (3, -0.044), (4, 0.059), (5, 0.152), (6, 0.019), (7, -0.076), (8, 0.003), (9, 0.086), (10, 0.019), (11, 0.021), (12, 0.086), (13, -0.041), (14, 0.271), (15, 0.056), (16, 0.014), (17, -0.007), (18, 0.019), (19, 0.037), (20, -0.084), (21, -0.03), (22, -0.012), (23, -0.042), (24, -0.079), (25, -0.063), (26, -0.036), (27, 0.034), (28, 0.049), (29, -0.017), (30, -0.064), (31, -0.001), (32, -0.026), (33, -0.03), (34, -0.068), (35, -0.024), (36, -0.016), (37, -0.044), (38, 0.062), (39, -0.065), (40, -0.006), (41, 0.081), (42, 0.114), (43, -0.019), (44, 0.038), (45, 0.022), (46, 0.041), (47, -0.059), (48, 0.074), (49, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95219386 <a title="92-lsi-1" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>Author: Olivier Chapelle, Bernhard Schćž&scaron;lkopf</p><p>Abstract: The choice of an SVM kernel corresponds to the choice of a representation of the data in a feature space and, to improve performance , it should therefore incorporate prior knowledge such as known transformation invariances. We propose a technique which extends earlier work and aims at incorporating invariances in nonlinear kernels. We show on a digit recognition task that the proposed approach is superior to the Virtual Support Vector method, which previously had been the method of choice. 1</p><p>2 0.67358541 <a title="92-lsi-2" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>Author: Patrick Haffner</p><p>Abstract: Maximum margin classifiers such as Support Vector Machines (SVMs) critically depends upon the convex hulls of the training samples of each class, as they implicitly search for the minimum distance between the convex hulls. We propose Extrapolated Vector Machines (XVMs) which rely on extrapolations outside these convex hulls. XVMs improve SVM generalization very significantly on the MNIST [7] OCR data. They share similarities with the Fisher discriminant: maximize the inter-class margin while minimizing the intra-class disparity. 1</p><p>3 0.6405077 <a title="92-lsi-3" href="./nips-2001-A_Parallel_Mixture_of_SVMs_for_Very_Large_Scale_Problems.html">16 nips-2001-A Parallel Mixture of SVMs for Very Large Scale Problems</a></p>
<p>Author: Ronan Collobert, Samy Bengio, Yoshua Bengio</p><p>Abstract: Support Vector Machines (SVMs) are currently the state-of-the-art models for many classification problems but they suffer from the complexity of their training algorithm which is at least quadratic with respect to the number of examples. Hence, it is hopeless to try to solve real-life problems having more than a few hundreds of thousands examples with SVMs. The present paper proposes a new mixture of SVMs that can be easily implemented in parallel and where each SVM is trained on a small subset of the whole dataset. Experiments on a large benchmark dataset (Forest) as well as a difficult speech database , yielded significant time improvement (time complexity appears empirically to locally grow linearly with the number of examples) . In addition, and that is a surprise, a significant improvement in generalization was observed on Forest. 1</p><p>4 0.60150671 <a title="92-lsi-4" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>Author: Carlotta Domeniconi, Dimitrios Gunopulos</p><p>Abstract: The nearest neighbor technique is a simple and appealing method to address classification problems. It relies on t he assumption of locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with a finite number of examples due to the curse of dimensionality. We propose a technique that computes a locally flexible metric by means of Support Vector Machines (SVMs). The maximum margin boundary found by the SVM is used to determine the most discriminant direction over the query's neighborhood. Such direction provides a local weighting scheme for input features. We present experimental evidence of classification performance improvement over the SVM algorithm alone and over a variety of adaptive learning schemes, by using both simulated and real data sets. 1</p><p>5 0.58210266 <a title="92-lsi-5" href="./nips-2001-K-Local_Hyperplane_and_Convex_Distance_Nearest_Neighbor_Algorithms.html">101 nips-2001-K-Local Hyperplane and Convex Distance Nearest Neighbor Algorithms</a></p>
<p>Author: Pascal Vincent, Yoshua Bengio</p><p>Abstract: Guided by an initial idea of building a complex (non linear) decision surface with maximal local margin in input space, we give a possible geometrical intuition as to why K-Nearest Neighbor (KNN) algorithms often perform more poorly than SVMs on classiﬁcation tasks. We then propose modiﬁed K-Nearest Neighbor algorithms to overcome the perceived problem. The approach is similar in spirit to Tangent Distance, but with invariances inferred from the local neighborhood rather than prior knowledge. Experimental results on real world classiﬁcation tasks suggest that the modiﬁed KNN algorithms often give a dramatic improvement over standard KNN and perform as well or better than SVMs. 1 Motivation The notion of margin for classiﬁcation tasks has been largely popularized by the success of the Support Vector Machine (SVM) [2, 15] approach. The margin of SVMs has a nice geometric interpretation1: it can be deﬁned informally as (twice) the smallest Euclidean distance between the decision surface and the closest training point. The decision surface produced by the original SVM algorithm is the hyperplane that maximizes this distance while still correctly separating the two classes. While the notion of keeping the largest possible safety margin between the decision surface and the data points seems very reasonable and intuitively appealing, questions arise when extending the approach to building more complex, non-linear decision surfaces. Non-linear SVMs usually use the “kernel trick” to achieve their non-linearity. This conceptually corresponds to ﬁrst mapping the input into a higher-dimensional feature space with some non-linear transformation and building a maximum-margin hyperplane (a linear decision surface) there. The “trick” is that this mapping is never computed directly, but implicitly induced by a kernel. In this setting, the margin being maximized is still the smallest Euclidean distance between the decision surface and the training points, but this time measured in some strange, sometimes inﬁnite dimensional, kernel-induced feature space rather than the original input space. It is less clear whether maximizing the margin in this new space, is meaningful in general (see [16]). 1 for the purpose of this discussion, we consider the original hard-margin SVM algorithm for two linearly separable classes. A different approach is to try and build a non-linear decision surface with maximal distance to the closest data point as measured directly in input space (as proposed in [14]). We could for instance restrict ourselves to a certain class of decision functions and try to ﬁnd the function with maximal margin among this class. But let us take this even further. Extending the idea of building a correctly separating non-linear decision surface as far away as possible from the data points, we deﬁne the notion of local margin as the Euclidean distance, in input space, between a given point on the decision surface and the closest training point. Now would it be possible to ﬁnd an algorithm that could produce a decision surface which correctly separates the classes and such that the local margin is everywhere maximal along its surface? Surprisingly, the plain old Nearest Neighbor algorithm (1NN) [5] does precisely this! So why does 1NN in practice often perform worse than SVMs? One typical explanation, is that it has too much capacity, compared to SVM, that the class of function it can produce is too rich. But, considering it has inﬁnite capacity (VC-dimension), 1NN is still performing quite well... This study is an attempt to better understand what is happening, based on geometrical intuition, and to derive an improved Nearest Neighbor algorithm from this understanding. 2 Fixing a broken Nearest Neighbor algorithm 2.1 Setting and deﬁnitions The setting is that of a classical classiﬁcation problem in (the input space).  wx u  r i q ©</p><p>6 0.56118077 <a title="92-lsi-6" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>7 0.55855501 <a title="92-lsi-7" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>8 0.53569508 <a title="92-lsi-8" href="./nips-2001-MIME%3A_Mutual_Information_Minimization_and_Entropy_Maximization_for_Bayesian_Belief_Propagation.html">117 nips-2001-MIME: Mutual Information Minimization and Entropy Maximization for Bayesian Belief Propagation</a></p>
<p>9 0.49914184 <a title="92-lsi-9" href="./nips-2001-Active_Learning_in_the_Drug_Discovery_Process.html">25 nips-2001-Active Learning in the Drug Discovery Process</a></p>
<p>10 0.4953948 <a title="92-lsi-10" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>11 0.47870457 <a title="92-lsi-11" href="./nips-2001-Kernel_Logistic_Regression_and_the_Import_Vector_Machine.html">104 nips-2001-Kernel Logistic Regression and the Import Vector Machine</a></p>
<p>12 0.47126818 <a title="92-lsi-12" href="./nips-2001-Incremental_Learning_and_Selective_Sampling_via_Parametric_Optimization_Framework_for_SVM.html">94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</a></p>
<p>13 0.46103856 <a title="92-lsi-13" href="./nips-2001-On_the_Concentration_of_Spectral_Properties.html">136 nips-2001-On the Concentration of Spectral Properties</a></p>
<p>14 0.44661167 <a title="92-lsi-14" href="./nips-2001-Semi-supervised_MarginBoost.html">167 nips-2001-Semi-supervised MarginBoost</a></p>
<p>15 0.44345105 <a title="92-lsi-15" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>16 0.43682849 <a title="92-lsi-16" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>17 0.41860825 <a title="92-lsi-17" href="./nips-2001-Minimax_Probability_Machine.html">120 nips-2001-Minimax Probability Machine</a></p>
<p>18 0.40772998 <a title="92-lsi-18" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>19 0.4024356 <a title="92-lsi-19" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>20 0.39989153 <a title="92-lsi-20" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.084), (17, 0.036), (19, 0.038), (27, 0.152), (30, 0.063), (38, 0.048), (59, 0.042), (64, 0.206), (72, 0.1), (79, 0.033), (83, 0.019), (88, 0.013), (91, 0.089)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86665654 <a title="92-lda-1" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>Author: Olivier Chapelle, Bernhard Schćž&scaron;lkopf</p><p>Abstract: The choice of an SVM kernel corresponds to the choice of a representation of the data in a feature space and, to improve performance , it should therefore incorporate prior knowledge such as known transformation invariances. We propose a technique which extends earlier work and aims at incorporating invariances in nonlinear kernels. We show on a digit recognition task that the proposed approach is superior to the Virtual Support Vector method, which previously had been the method of choice. 1</p><p>2 0.78915137 <a title="92-lda-2" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>Author: Javid Sadr, Sayan Mukherjee, Keith Thoresz, Pawan Sinha</p><p>Abstract: A key question in neuroscience is how to encode sensory stimuli such as images and sounds. Motivated by studies of response properties of neurons in the early cortical areas, we propose an encoding scheme that dispenses with absolute measures of signal intensity or contrast and uses, instead, only local ordinal measures. In this scheme, the structure of a signal is represented by a set of equalities and inequalities across adjacent regions. In this paper, we focus on characterizing the ﬁdelity of this representation strategy. We develop a regularization approach for image reconstruction from ordinal measures and thereby demonstrate that the ordinal representation scheme can faithfully encode signal structure. We also present a neurally plausible implementation of this computation that uses only local update rules. The results highlight the robustness and generalization ability of local ordinal encodings for the task of pattern classiﬁcation. 1</p><p>3 0.73084939 <a title="92-lda-3" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>Author: Mário Figueiredo</p><p>Abstract: In this paper we introduce a new sparseness inducing prior which does not involve any (hyper)parameters that need to be adjusted or estimated. Although other applications are possible, we focus here on supervised learning problems: regression and classiﬁcation. Experiments with several publicly available benchmark data sets show that the proposed approach yields state-of-the-art performance. In particular, our method outperforms support vector machines and performs competitively with the best alternative techniques, both in terms of error rates and sparseness, although it involves no tuning or adjusting of sparsenesscontrolling hyper-parameters.</p><p>4 0.7214331 <a title="92-lda-4" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>Author: Manfred Opper, Robert Urbanczik</p><p>Abstract: Using methods of Statistical Physics, we investigate the rOle of model complexity in learning with support vector machines (SVMs). We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error. Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel. 1</p><p>5 0.7211256 <a title="92-lda-5" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>6 0.72103965 <a title="92-lda-6" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>7 0.71652865 <a title="92-lda-7" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>8 0.71450281 <a title="92-lda-8" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>9 0.71443582 <a title="92-lda-9" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>10 0.71281534 <a title="92-lda-10" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>11 0.71156204 <a title="92-lda-11" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>12 0.71023566 <a title="92-lda-12" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>13 0.70936334 <a title="92-lda-13" href="./nips-2001-Quantizing_Density_Estimators.html">155 nips-2001-Quantizing Density Estimators</a></p>
<p>14 0.70816267 <a title="92-lda-14" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>15 0.70813334 <a title="92-lda-15" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>16 0.70728815 <a title="92-lda-16" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>17 0.70454681 <a title="92-lda-17" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>18 0.7037766 <a title="92-lda-18" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>19 0.70329547 <a title="92-lda-19" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>20 0.70192683 <a title="92-lda-20" href="./nips-2001-Learning_from_Infinite_Data_in_Finite_Time.html">114 nips-2001-Learning from Infinite Data in Finite Time</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
