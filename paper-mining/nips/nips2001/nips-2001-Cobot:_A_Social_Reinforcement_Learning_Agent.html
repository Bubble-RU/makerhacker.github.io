<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>51 nips-2001-Cobot: A Social Reinforcement Learning Agent</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-51" href="#">nips2001-51</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>51 nips-2001-Cobot: A Social Reinforcement Learning Agent</h1>
<br/><p>Source: <a title="nips-2001-51-pdf" href="http://papers.nips.cc/paper/2118-cobot-a-social-reinforcement-learning-agent.pdf">pdf</a></p><p>Author: Charles Lee Isbell Jr., Christian R. Shelton</p><p>Abstract: We report on the use of reinforcement learning with Cobot, a software agent residing in the well-known online community LambdaMOO. Our initial work on Cobot (Isbell et al.2000) provided him with the ability to collect social statistics and report them to users. Here we describe an application of RL allowing Cobot to take proactive actions in this complex social environment, and adapt behavior from multiple sources of human reward. After 5 months of training, and 3171 reward and punishment events from 254 different LambdaMOO users, Cobot learned nontrivial preferences for a number of users, modiÔ¨Ång his behavior based on his current state. Here we describe LambdaMOO and the state and action spaces of Cobot, and report the statistical results of the learning experiment. 1</p><p>Reference: <a title="nips-2001-51-reference" href="../nips2001_reference/nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cobot', 0.877), ('lambdamoo', 0.24), ('reward', 0.199), ('rl', 0.19), ('pun', 0.117), ('soc', 0.105), ('policy', 0.096), ('verb', 0.08), ('pref', 0.074), ('room', 0.056), ('act', 0.055), ('feedback', 0.05), ('ag', 0.049), ('bust', 0.047), ('isbel', 0.039), ('environ', 0.038), ('feat', 0.035), ('liv', 0.034), ('rol', 0.033), ('ded', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="51-tfidf-1" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>2 0.16347127 <a title="51-tfidf-2" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>3 0.11220075 <a title="51-tfidf-3" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>4 0.10054266 <a title="51-tfidf-4" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>5 0.093371131 <a title="51-tfidf-5" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>6 0.093218833 <a title="51-tfidf-6" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>7 0.092006139 <a title="51-tfidf-7" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>8 0.087483421 <a title="51-tfidf-8" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>9 0.084424764 <a title="51-tfidf-9" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>10 0.084029809 <a title="51-tfidf-10" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>11 0.072176054 <a title="51-tfidf-11" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>12 0.059341796 <a title="51-tfidf-12" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>13 0.058279797 <a title="51-tfidf-13" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>14 0.0536182 <a title="51-tfidf-14" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>15 0.052624345 <a title="51-tfidf-15" href="./nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</a></p>
<p>16 0.052370209 <a title="51-tfidf-16" href="./nips-2001-Stabilizing_Value_Function_Approximation_with_the_BFBP_Algorithm.html">175 nips-2001-Stabilizing Value Function Approximation with the BFBP Algorithm</a></p>
<p>17 0.046801422 <a title="51-tfidf-17" href="./nips-2001-A_Bayesian_Model_Predicts_Human_Parse_Preference_and_Reading_Times_in_Sentence_Processing.html">5 nips-2001-A Bayesian Model Predicts Human Parse Preference and Reading Times in Sentence Processing</a></p>
<p>18 0.046506595 <a title="51-tfidf-18" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>19 0.043378215 <a title="51-tfidf-19" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>20 0.035909168 <a title="51-tfidf-20" href="./nips-2001-A_Rational_Analysis_of_Cognitive_Control_in_a_Speeded_Discrimination_Task.html">18 nips-2001-A Rational Analysis of Cognitive Control in a Speeded Discrimination Task</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.099), (1, -0.008), (2, 0.211), (3, 0.026), (4, -0.019), (5, -0.08), (6, -0.049), (7, -0.003), (8, -0.013), (9, 0.001), (10, 0.02), (11, -0.003), (12, 0.033), (13, 0.006), (14, 0.022), (15, 0.006), (16, 0.042), (17, 0.012), (18, 0.029), (19, -0.014), (20, -0.03), (21, -0.01), (22, 0.038), (23, 0.044), (24, -0.087), (25, 0.026), (26, 0.03), (27, 0.049), (28, -0.026), (29, 0.001), (30, 0.075), (31, -0.046), (32, -0.0), (33, -0.009), (34, -0.068), (35, 0.062), (36, -0.012), (37, -0.05), (38, 0.027), (39, 0.042), (40, 0.169), (41, -0.012), (42, -0.04), (43, -0.018), (44, -0.126), (45, 0.031), (46, -0.026), (47, 0.044), (48, -0.158), (49, -0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9103719 <a title="51-lsi-1" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>2 0.6604777 <a title="51-lsi-2" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>3 0.626297 <a title="51-lsi-3" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>4 0.61053586 <a title="51-lsi-4" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>5 0.60030061 <a title="51-lsi-5" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>6 0.58677346 <a title="51-lsi-6" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>7 0.45047155 <a title="51-lsi-7" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>8 0.42983022 <a title="51-lsi-8" href="./nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</a></p>
<p>9 0.4264518 <a title="51-lsi-9" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>10 0.42617989 <a title="51-lsi-10" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>11 0.40515494 <a title="51-lsi-11" href="./nips-2001-Predictive_Representations_of_State.html">148 nips-2001-Predictive Representations of State</a></p>
<p>12 0.39436629 <a title="51-lsi-12" href="./nips-2001-A_Bayesian_Model_Predicts_Human_Parse_Preference_and_Reading_Times_in_Sentence_Processing.html">5 nips-2001-A Bayesian Model Predicts Human Parse Preference and Reading Times in Sentence Processing</a></p>
<p>13 0.37945747 <a title="51-lsi-13" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>14 0.36962488 <a title="51-lsi-14" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>15 0.36352822 <a title="51-lsi-15" href="./nips-2001-Stabilizing_Value_Function_Approximation_with_the_BFBP_Algorithm.html">175 nips-2001-Stabilizing Value Function Approximation with the BFBP Algorithm</a></p>
<p>16 0.32819092 <a title="51-lsi-16" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>17 0.32753301 <a title="51-lsi-17" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>18 0.31434393 <a title="51-lsi-18" href="./nips-2001-Learning_Hierarchical_Structures_with_Linear_Relational_Embedding.html">110 nips-2001-Learning Hierarchical Structures with Linear Relational Embedding</a></p>
<p>19 0.30468264 <a title="51-lsi-19" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>20 0.29952633 <a title="51-lsi-20" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.113), (16, 0.117), (18, 0.013), (31, 0.052), (36, 0.02), (50, 0.065), (63, 0.018), (70, 0.277), (77, 0.023), (79, 0.037), (91, 0.054), (92, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78108305 <a title="51-lda-1" href="./nips-2001-Incremental_A%2A.html">93 nips-2001-Incremental A*</a></p>
<p>same-paper 2 0.69310105 <a title="51-lda-2" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>3 0.55648375 <a title="51-lda-3" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>4 0.55101597 <a title="51-lda-4" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>5 0.54523855 <a title="51-lda-5" href="./nips-2001-Rao-Blackwellised_Particle_Filtering_via_Data_Augmentation.html">156 nips-2001-Rao-Blackwellised Particle Filtering via Data Augmentation</a></p>
<p>6 0.54111362 <a title="51-lda-6" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>7 0.53961015 <a title="51-lda-7" href="./nips-2001-Predictive_Representations_of_State.html">148 nips-2001-Predictive Representations of State</a></p>
<p>8 0.53121471 <a title="51-lda-8" href="./nips-2001-An_Efficient%2C_Exact_Algorithm_for_Solving_Tree-Structured_Graphical_Games.html">32 nips-2001-An Efficient, Exact Algorithm for Solving Tree-Structured Graphical Games</a></p>
<p>9 0.52497971 <a title="51-lda-9" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>10 0.52017593 <a title="51-lda-10" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>11 0.51971501 <a title="51-lda-11" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>12 0.51859355 <a title="51-lda-12" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>13 0.51306188 <a title="51-lda-13" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>14 0.50915289 <a title="51-lda-14" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>15 0.50800753 <a title="51-lda-15" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>16 0.50570953 <a title="51-lda-16" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>17 0.50058657 <a title="51-lda-17" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>18 0.5003531 <a title="51-lda-18" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>19 0.49999982 <a title="51-lda-19" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>20 0.49918896 <a title="51-lda-20" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
