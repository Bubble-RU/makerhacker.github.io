<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-65" href="#">nips2001-65</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</h1>
<br/><p>Source: <a title="nips-2001-65-pdf" href="http://papers.nips.cc/paper/2117-effective-size-of-receptive-fields-of-inferior-temporal-visual-cortex-neurons-in-natural-scenes.pdf">pdf</a></p><p>Author: Thomas P. Trappenberg, Edmund T. Rolls, Simon M. Stringer</p><p>Abstract: Inferior temporal cortex (IT) neurons have large receptive ﬁelds when a single effective object stimulus is shown against a blank background, but have much smaller receptive ﬁelds when the object is placed in a natural scene. Thus, translation invariant object recognition is reduced in natural scenes, and this may help object selection. We describe a model which accounts for this by competition within an attractor in which the neurons are tuned to different objects in the scene, and the fovea has a higher cortical magniﬁcation factor than the peripheral visual ﬁeld. Furthermore, we show that top-down object bias can increase the receptive ﬁeld size, facilitating object search in complex visual scenes, and providing a model of object-based attention. The model leads to the prediction that introduction of a second object into a scene with blank background will reduce the receptive ﬁeld size to values that depend on the closeness of the second object to the target stimulus. We suggest that mechanisms of this type enable the output of IT to be primarily about one object, so that the areas that receive from IT can select the object as a potential target for action.</p><p>Reference: <a title="nips-2001-65-reference" href="../nips2001_reference/nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Effective size of receptive ﬁelds of inferior temporal visual cortex neurons in natural scenes  Thomas P. [sent-1, score-1.39]
</p><p>2 uk  Abstract Inferior temporal cortex (IT) neurons have large receptive ﬁelds when a single effective object stimulus is shown against a blank background, but have much smaller receptive ﬁelds when the object is placed in a natural scene. [sent-11, score-2.544]
</p><p>3 Thus, translation invariant object recognition is reduced in natural scenes, and this may help object selection. [sent-12, score-1.169]
</p><p>4 We describe a model which accounts for this by competition within an attractor in which the neurons are tuned to different objects in the scene, and the fovea has a higher cortical magniﬁcation factor than the peripheral visual ﬁeld. [sent-13, score-1.098]
</p><p>5 Furthermore, we show that top-down object bias can increase the receptive ﬁeld size, facilitating object search in complex visual scenes, and providing a model of object-based attention. [sent-14, score-1.643]
</p><p>6 The model leads to the prediction that introduction of a second object into a scene with blank background will reduce the receptive ﬁeld size to values that depend on the closeness of the second object to the target stimulus. [sent-15, score-2.002]
</p><p>7 We suggest that mechanisms of this type enable the output of IT to be primarily about one object, so that the areas that receive from IT can select the object as a potential target for action. [sent-16, score-0.64]
</p><p>8 1 Introduction Neurons in the macaque inferior temporal visual cortex (IT) that respond to objects or faces have large receptive ﬁelds when a single object or image is shown on an otherwise blank screen [1, 2, 3]. [sent-17, score-2.154]
</p><p>9 The responsiveness of the neurons to their effective stimuli independent of their position on the retina over many degrees is termed translation invariance. [sent-18, score-0.38]
</p><p>10 This allows correct generalization over position, so that what is learned when an object is shown at one position on the retina generalizes correctly to other positions [4]. [sent-20, score-0.533]
</p><p>11 If more than one object is present on the screen, then there is evidence that the neuron responds more to the object at the fovea than in the parafoveal region [5, 6]. [sent-21, score-1.241]
</p><p>12 We deﬁne the size of a receptive ﬁeld as twice the distance from the fovea (the centre of the receptive ﬁeld) to locations at which the response decreases to half maximal. [sent-23, score-1.071]
</p><p>13 An analysis of IT neurons that responded to the target stimulus showed that the average size of the receptive ﬁelds shrinks from approximately 56 degrees in a blank background to approximately 12 degrees with a complex scene [8]. [sent-24, score-1.282]
</p><p>14 The responses of an IT cell with a large receptive ﬁeld are illustrated in Figure 1A. [sent-25, score-0.381]
</p><p>15 There the average ﬁring rates of the cell to an effective stimulus that the monkey had to touch on a touch-screen to receive reward is shown as a function of the angular distance of the object from the fovea. [sent-26, score-0.758]
</p><p>16 The solid line represents the results from experiments with the object placed in a blank background. [sent-27, score-0.878]
</p><p>17 This demonstrates the large receptive ﬁelds of IT cells that have often been reported in the literature [3]. [sent-28, score-0.322]
</p><p>18 In contrast, when the object is placed in a natural scene (cluttered background), the size of the receptive ﬁeld is markedly smaller (dashed line). [sent-29, score-1.15]
</p><p>19 2 The model We formalized our understanding of how the dependence of the receptive ﬁeld size on various conditions could be implemented in the ventral visual processing pathway by developing a neural network model with the components sufﬁcient to produce the above effects. [sent-30, score-0.667]
</p><p>20 The model utilizes an attractor network representing the inferior temporal visual cortex, and a neural input layer with several retinotopically organized modules representing the visual scene in an earlier visual cortical area such as V4 (see Figure 1B). [sent-31, score-1.51]
</p><p>21 Each independent module within ‘V4’ represents a small part of the visual ﬁeld and receives input from earlier visual areas represented by an input vector for each possible location which is unique for each object. [sent-32, score-0.656]
</p><p>22 Each module was 6 deg in width, matching the size of the objects presented to the network. [sent-33, score-0.328]
</p><p>23 For the simulations we chose binary random input vectors representing objects with components set to ones and the remaining components set to zeros. [sent-34, score-0.214]
</p><p>24 is the number of nodes in each module and is the sparseness of the representation. [sent-35, score-0.138]
</p><p>25 The constant represents the strength of the activity-dependent global inhibition simulating the effects of inhibitory interneurons. [sent-39, score-0.031]
</p><p>26 The external ‘top-down’ input vector produces object-selective inputs, which are used as the attentional drive when a visual search task is simulated. [sent-40, score-0.296]
</p><p>27 The strength of this    y    w  )0' F  3Ix472© e)1d& © C$  y   ' w bW ¤a `  A. [sent-41, score-0.031]
</p><p>28 130  Average firing rate  120  Object bias  blank background  110  IT  100 90 80 70  natural background  60 50 0  10  20  30  40  50  60  Distance of gaze from target object  C. [sent-43, score-1.289]
</p><p>29 B) Outline of the model used in this study with an attractor network labelled ‘IT’ that receives topgraphical organised inputs from an input neural layer labeled ‘V4’. [sent-49, score-0.23]
</p><p>30 Objects close to the fovea produce stronger inputs to reﬂect the higher magniﬁcation factor of the visual representation close to the fovea. [sent-50, score-0.607]
</p><p>31 The attractor network also receives top-down object-based inputs, to incorporate object-based attention in a visual search task. [sent-51, score-0.482]
</p><p>32 C) The modulation factor used to weight inputs to IT from V4 shown as a function of their distance from the fovea. [sent-52, score-0.234]
</p><p>33 The values on the solid line are derived from cortical magniﬁcation factors, and were used in the simulations, whereas the dotted line corresponds to a shifted Gaussian function. [sent-53, score-0.135]
</p><p>34 The recognition functionality of this object bias is modulated by the value of structure is modeled as an attractor neural network (ANN) with trained memories indexed by representing particular objects. [sent-55, score-0.825]
</p><p>35 The weights between the V4 nodes and IT nodes are trained by Hebbian learning of the form ¡ ¢  1  (3)  $  "  "   ©  £  1 © # ©V ¤¡ Ue)' V  ¤¡ T 0)'1 E ¥ ¥ @ £ T £ B B £ T ¤¡ Ue)B '1 E ¦ §¤  ¢ ¡      ¦ §¤  ¥  ¢ ¡   ¡  to produce object representations in IT based on inputs in V4. [sent-57, score-0.593]
</p><p>36 The normalizing modulation   £ T # ©V ¤¡ U0)' V  factor allows the gain of inputs to be modulated as a function of their distance from the fovea, and depends on the module to which the presynaptic node belongs. [sent-58, score-0.366]
</p><p>37 The weight values between V4 and IT support translation invariant object recognition of a single object in the visual ﬁeld if the normalization factor is the same for each module and the model is trained with the objects placed at every possible location in the visual ﬁeld. [sent-59, score-2.015]
</p><p>38 The translation invariance of the weight vectors between each V4 module and the IT nodes is however explicitly modulated in our model by the module-dependent modulation factor as indicated in Figure 1B by the width of the lines connecting V4 with IT. [sent-60, score-0.395]
</p><p>39 The strength of the foveal module is strongest, and the strength decreases for modules representing increasing eccentricity. [sent-61, score-0.262]
</p><p>40 The form of this modulation factor was derived from the parameterization of the cortical magniﬁcation factors given by [10], 1 and is illustrated in Figure 1C as a solid line. [sent-62, score-0.261]
</p><p>41 Similar results to the ones presented here can be achieved with different forms of the modulation factor such as a shifted Gaussian as illustrated by the dashed line in Figure 1C. [sent-63, score-0.185]
</p><p>42 The correlation was estimated from the normalized dot product between the target object vector and the state of the IT network after a ﬁxed amount of time sufﬁcient for the network to settle into a stable state. [sent-65, score-0.662]
</p><p>43 The objects were always presented on backgrounds with some noise (introduced by ﬂipping 2% of the bits in the scene) because the input to IT will inevitably be noisy under normal conditions of operation. [sent-66, score-0.257]
</p><p>44 All results shown in the following represent averages over 10 runs and over all patterns on which the network was trained. [sent-67, score-0.049]
</p><p>45 1 Receptive ﬁelds are large in scenes with blank backgrounds In the ﬁrst experiments we placed only one object in the visual scene with different eccentricities relative to the fovea. [sent-69, score-1.457]
</p><p>46 The results of this simulation are shown in Figure 2A with the line labeled ‘blank background’. [sent-70, score-0.049]
</p><p>47 The value of the object bias was set to 0 in these simulations. [sent-71, score-0.538]
</p><p>48 Good object retrieval (indicated by large correlations) was found even when the object was far from the fovea, indicating large IT receptive ﬁelds with a blank background. [sent-72, score-1.585]
</p><p>49 The reason that any drop is seen in performance as a function of eccentricity is because ﬂipping 2% of the bits in the V4 modules introduces some noise into the recall process. [sent-73, score-0.121]
</p><p>50 The results demonstrate that the attractor dynamics can support translation invariant object recognition even though the weight vectors between V4 and IT are not translation invariant but are explicitly modulated by the modulation factor derived from the cortical magniﬁcation factor. [sent-74, score-1.092]
</p><p>51 2 The receptive ﬁeld size is reduced in scenes with complex background In a second experiment we placed individual objects at all possible locations in the visual scene representing natural (cluttered) visual scenes. [sent-76, score-1.634]
</p><p>52 The resulting correlations between the target pattern and asymptotic IT state are shown in Figure 2A with the line labeled ’natural background’. [sent-77, score-0.13]
</p><p>53 Many objects in the visual scene are now competing for recognition by the attractor network, while the objects around the foveal position are enhanced through the modulation factor derived by the cortical magniﬁcation factor. [sent-78, score-1.2]
</p><p>54 This results in a much smaller size of the receptive ﬁeld of IT neurons when measured with objects in natural 1 This parameterization is based on V1 data. [sent-79, score-0.785]
</p><p>55 However, it was shown that similar forms of the magniﬁcation factor hold also in V4 [11]  A. [sent-80, score-0.058]
</p><p>56 8  Correlation  Correlation  blank background  1  1  0. [sent-83, score-0.451]
</p><p>57 There is no object bias included in the results shown in graph A, whereas an object bias is included in the results shown in B with in the experiments with a natural background and in the experiments with a blank background. [sent-91, score-1.594]
</p><p>58 Examples are shown in Figure 2B where we used an object bias. [sent-95, score-0.48]
</p><p>59 The object bias biasses the IT network towards the expected object with a strength determined by the value of , and has the effect of increasing the size of the receptive ﬁelds in both blank and natural backgrounds (see Figure 2B and compare to Figure 2A). [sent-96, score-1.946]
</p><p>60 4 A second object in a blank background reduces the receptive ﬁeld size depending on the distance between the second object and the fovea In the last set of experiments we placed two objects in an otherwise blank background. [sent-99, score-2.658]
</p><p>61 The IT network was biased towards one of the objects designated as the target object (in for example a visual search task), which was placed on one side of the fovea at different eccentricities from the fovea. [sent-100, score-1.422]
</p><p>62 The second object, a distractor object, was placed on the opposite side of the fovea at a ﬁxed distance of degrees from the fovea. [sent-101, score-0.523]
</p><p>63 The results indicate that the size of the receptive ﬁeld (for the target object) decreases with decreasing distance of the distractor object from the fovea. [sent-103, score-1.09]
</p><p>64 The size starts to increase linearly with increasing distance of the distractor object from the fovea until the inﬂuence of the distractor on the size of the receptive ﬁeld levels off and approaches the value expected for the situation with one object in a visual scene and a blank background. [sent-106, score-2.62]
</p><p>65 The different curves correspond to different distances of the distractor object from the fovea. [sent-114, score-0.586]
</p><p>66 The eccentricity refers to the distance between the target object and the fovea. [sent-115, score-0.673]
</p><p>67 B) The size of the receptive ﬁeld for the target as a function of the distance of the distractor object from the fovea. [sent-116, score-1.069]
</p><p>68 ¢  ¢  4 Discussion When single objects are shown in a scene with a blank background, the attractor network helps neurons to respond to an object with large eccentricities of this object relative to the fovea of the agent. [sent-117, score-2.238]
</p><p>69 When the object is presented in a natural scene, other neurons in the inferior temporal cortex become activated by the other effective stimuli present in the visual ﬁeld, and these forward inputs decrease the response of the network to the target stimulus by a competitive process. [sent-118, score-1.65]
</p><p>70 The results found ﬁt well with the neurophysiological data, in that IT operates with almost complete translation invariance when there is only one object in the scene, and reduces the receptive ﬁeld size of its neurons when the object is presented in a cluttered environment. [sent-119, score-1.643]
</p><p>71 The model here provides an explanation of the real IT neuronal responses in natural scenes and makes several predictions that can be explored experimentally. [sent-120, score-0.19]
</p><p>72 The model is compatible with the models developed by Gustavo Deco and colleagues (see, for example, [12, 13]) while speciﬁc simpliﬁcations and addition have been made to explore the variations in the size of receptive ﬁelds in IT. [sent-121, score-0.373]
</p><p>73 The model accounts for the larger receptive ﬁeld sizes from the fovea of IT neurons in natural backgrounds if the target is the object being selected compared to when it is not selected [8]. [sent-122, score-1.426]
</p><p>74 The model accounts for this by an effect of top-down bias which simply biasses the neurons towards particular objects compensating for their decreasing inputs produced by the decreasing magniﬁcation factor modulation with increasing distance from the fovea. [sent-123, score-0.724]
</p><p>75 Such object based attention signals could originate in the prefrontal cortex and could provide the object bias for the inferotemporal cortex [14]. [sent-124, score-1.46]
</p><p>76 3 This enables, for example, the correct reward association of an object to be determined by pattern association in the orbitofrontal cortex or amygdala, because they receive information essentially about the object at the fovea. [sent-126, score-1.312]
</p><p>77 Without this shrinkage in the receptive ﬁeld size, the areas that receive from the inferior temporal visual cortex would respond to essentially all objects in a visual scene, and would therefore provide an undecipherable babel of information about all objects present in the visual scene. [sent-127, score-2.053]
</p><p>78 It appears that part of the solution to this potential binding problem that is used by the brain is to limit the size of the receptive ﬁelds of inferior temporal cortex neurons when natural environments are being viewed. [sent-128, score-1.1]
</p><p>79 The suggestion is that by providing an output about what is at the fovea in complex scenes, the inferior temporal visual cortex enables the correct reward association to be looked up in succeeding brain regions, and then for the object to be selected for action if appropriate. [sent-129, score-1.593]
</p><p>80 Part of the hypothesis here is that the coordinates of the object in the visual scene being selected for action are provided by the position in space to which the gaze is directed [7]. [sent-130, score-0.941]
</p><p>81 Translation invariance and the responses of neurons in the temporal visual cortical areas of primates. [sent-147, score-0.695]
</p><p>82 Functions of the primate temporal lobe cortical visual areas in invariant visual object and face recognition. [sent-152, score-1.255]
</p><p>83 Interactions of visual stimuli in the receptive ﬁelds of inferior temporal neurons in macaque. [sent-162, score-1.024]
</p><p>84 The responses of single neurons in the temporal visual cortical areas of the macaque when more than one stimulus is present in the visual ﬁeld. [sent-169, score-1.001]
</p><p>85 Responses of inferior temporal cortex neurons to objects in natural scenes. [sent-178, score-0.875]
</p><p>86 Responses of inferior temporal cortex neurons to objects in natural scenes. [sent-185, score-0.875]
</p><p>87 View-invariant representations of familiar objects by neurons in the inferior temporal visual cortex. [sent-193, score-0.866]
</p><p>88 3 Note that it is possible that a “spotlight of attention” [15] can be moved away from the fovea, but at least during normal visual search tasks, the neurons are sensitive to the object at which the monkey is looking, that is which is on the fovea, as shown by [8]. [sent-195, score-0.94]
</p><p>89 Thus, spatial modulation of the responsiveness of neurons at the V4 level can be inﬂuenced by location-speciﬁc attentional modulations originating, for example, in the posterior parietal cortex, which may be involved in directing visual spatial attention [15]. [sent-196, score-0.56]
</p><p>90 Magniﬁcation factor and receptive ﬁeld size in foveal striate cortex of the monkey. [sent-205, score-0.663]
</p><p>91 Cortical visual areas of the macaque: Possible substrates for pattern recognition mechanisms. [sent-216, score-0.331]
</p><p>92 A recurrent model of the interaction between the prefrontal cortex and inferior temporal cortex in delay memory tasks. [sent-237, score-0.693]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('object', 0.48), ('receptive', 0.322), ('blank', 0.303), ('fovea', 0.262), ('visual', 0.245), ('cortex', 0.187), ('objects', 0.187), ('inferior', 0.186), ('scene', 0.161), ('eld', 0.151), ('background', 0.148), ('neurons', 0.139), ('elds', 0.131), ('magni', 0.115), ('temporal', 0.109), ('distractor', 0.106), ('rolls', 0.096), ('attractor', 0.095), ('module', 0.09), ('scenes', 0.084), ('cortical', 0.083), ('eccentricity', 0.083), ('modulation', 0.081), ('translation', 0.073), ('cluttered', 0.073), ('backgrounds', 0.07), ('placed', 0.069), ('natural', 0.067), ('bias', 0.058), ('factor', 0.058), ('target', 0.057), ('areas', 0.055), ('distance', 0.053), ('oxford', 0.052), ('macaque', 0.051), ('size', 0.051), ('network', 0.049), ('receive', 0.048), ('monkey', 0.048), ('eccentricities', 0.045), ('foveal', 0.045), ('attention', 0.044), ('modulated', 0.042), ('inputs', 0.042), ('brain', 0.039), ('responses', 0.039), ('modules', 0.038), ('invariant', 0.038), ('respond', 0.037), ('biasses', 0.035), ('stimulus', 0.035), ('reward', 0.033), ('degrees', 0.033), ('centre', 0.033), ('cation', 0.032), ('neuroscience', 0.032), ('strength', 0.031), ('recognition', 0.031), ('effective', 0.031), ('facilitating', 0.03), ('deco', 0.03), ('desimone', 0.03), ('orbitofrontal', 0.03), ('touch', 0.03), ('accounts', 0.029), ('locations', 0.028), ('search', 0.028), ('amygdala', 0.028), ('ipping', 0.028), ('bw', 0.028), ('responsiveness', 0.028), ('gaze', 0.028), ('association', 0.027), ('position', 0.027), ('representing', 0.027), ('correlation', 0.027), ('line', 0.026), ('nodes', 0.026), ('ex', 0.026), ('abstracts', 0.026), ('retina', 0.026), ('screen', 0.026), ('enables', 0.025), ('invariance', 0.025), ('correlations', 0.024), ('prefrontal', 0.024), ('memories', 0.024), ('ue', 0.024), ('stimuli', 0.023), ('attentional', 0.023), ('labeled', 0.023), ('sparseness', 0.022), ('receives', 0.021), ('faces', 0.021), ('decreasing', 0.021), ('illustrated', 0.02), ('hebbian', 0.02), ('trained', 0.019), ('neuron', 0.019), ('parameterization', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="65-tfidf-1" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>Author: Thomas P. Trappenberg, Edmund T. Rolls, Simon M. Stringer</p><p>Abstract: Inferior temporal cortex (IT) neurons have large receptive ﬁelds when a single effective object stimulus is shown against a blank background, but have much smaller receptive ﬁelds when the object is placed in a natural scene. Thus, translation invariant object recognition is reduced in natural scenes, and this may help object selection. We describe a model which accounts for this by competition within an attractor in which the neurons are tuned to different objects in the scene, and the fovea has a higher cortical magniﬁcation factor than the peripheral visual ﬁeld. Furthermore, we show that top-down object bias can increase the receptive ﬁeld size, facilitating object search in complex visual scenes, and providing a model of object-based attention. The model leads to the prediction that introduction of a second object into a scene with blank background will reduce the receptive ﬁeld size to values that depend on the closeness of the second object to the target stimulus. We suggest that mechanisms of this type enable the output of IT to be primarily about one object, so that the areas that receive from IT can select the object as a potential target for action.</p><p>2 0.34792215 <a title="65-tfidf-2" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>Author: Antonio Torralba</p><p>Abstract: The most popular algorithms for object detection require the use of exhaustive spatial and scale search procedures. In such approaches, an object is defined by means of local features. fu this paper we show that including contextual information in object detection procedures provides an efficient way of cutting down the need for exhaustive search. We present results with real images showing that the proposed scheme is able to accurately predict likely object classes, locations and sizes. 1</p><p>3 0.25463188 <a title="65-tfidf-3" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>Author: Antonino Casile, Michele Rucci</p><p>Abstract: Neural activity appears to be a crucial component for shaping the receptive ﬁelds of cortical simple cells into adjacent, oriented subregions alternately receiving ON- and OFF-center excitatory geniculate inputs. It is known that the orientation selective responses of V1 neurons are reﬁned by visual experience. After eye opening, the spatiotemporal structure of neural activity in the early stages of the visual pathway depends both on the visual environment and on how the environment is scanned. We have used computational modeling to investigate how eye movements might affect the reﬁnement of the orientation tuning of simple cells in the presence of a Hebbian scheme of synaptic plasticity. Levels of correlation between the activity of simulated cells were examined while natural scenes were scanned so as to model sequences of saccades and ﬁxational eye movements, such as microsaccades, tremor and ocular drift. The speciﬁc patterns of activity required for a quantitatively accurate development of simple cell receptive ﬁelds with segregated ON and OFF subregions were observed during ﬁxational eye movements, but not in the presence of saccades or with static presentation of natural visual input. These results suggest an important role for the eye movements occurring during visual ﬁxation in the reﬁnement of orientation selectivity.</p><p>4 0.20531452 <a title="65-tfidf-4" href="./nips-2001-Generalizable_Relational_Binding_from_Coarse-coded_Distributed_Representations.html">80 nips-2001-Generalizable Relational Binding from Coarse-coded Distributed Representations</a></p>
<p>Author: Randall C. O'Reilly, R. S. Busby</p><p>Abstract: We present a model of binding of relationship information in a spatial domain (e.g., square above triangle) that uses low-order coarse-coded conjunctive representations instead of more popular temporal synchrony mechanisms. Supporters of temporal synchrony argue that conjunctive representations lack both efﬁciency (i.e., combinatorial numbers of units are required) and systematicity (i.e., the resulting representations are overly speciﬁc and thus do not support generalization to novel exemplars). To counter these claims, we show that our model: a) uses far fewer hidden units than the number of conjunctions represented, by using coarse-coded, distributed representations where each unit has a broad tuning curve through high-dimensional conjunction space, and b) is capable of considerable generalization to novel inputs.</p><p>5 0.16782357 <a title="65-tfidf-5" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>Author: Bernd Heisele, Thomas Serre, Massimiliano Pontil, Thomas Vetter, Tomaso Poggio</p><p>Abstract: We describe an algorithm for automatically learning discriminative components of objects with SVM classiﬁers. It is based on growing image parts by minimizing theoretical bounds on the error probability of an SVM. Component-based face classiﬁers are then combined in a second stage to yield a hierarchical SVM classiﬁer. Experimental results in face classiﬁcation show considerable robustness against rotations in depth and suggest performance at signiﬁcantly better level than other face detection systems. Novel aspects of our approach are: a) an algorithm to learn component-based classiﬁcation experts and their combination, b) the use of 3-D morphable models for training, and c) a maximum operation on the output of each component classiﬁer which may be relevant for biological models of visual recognition.</p><p>6 0.15722682 <a title="65-tfidf-6" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>7 0.15523662 <a title="65-tfidf-7" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>8 0.13281225 <a title="65-tfidf-8" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>9 0.1053911 <a title="65-tfidf-9" href="./nips-2001-Modeling_the_Modulatory_Effect_of_Attention_on_Human_Spatial_Vision.html">124 nips-2001-Modeling the Modulatory Effect of Attention on Human Spatial Vision</a></p>
<p>10 0.10328522 <a title="65-tfidf-10" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>11 0.095412821 <a title="65-tfidf-11" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>12 0.090550728 <a title="65-tfidf-12" href="./nips-2001-A_Hierarchical_Model_of_Complex_Cells_in_Visual_Cortex_for_the_Binocular_Perception_of_Motion-in-Depth.html">10 nips-2001-A Hierarchical Model of Complex Cells in Visual Cortex for the Binocular Perception of Motion-in-Depth</a></p>
<p>13 0.086433962 <a title="65-tfidf-13" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>14 0.083083659 <a title="65-tfidf-14" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>15 0.077375442 <a title="65-tfidf-15" href="./nips-2001-A_Maximum-Likelihood_Approach_to_Modeling_Multisensory_Enhancement.html">11 nips-2001-A Maximum-Likelihood Approach to Modeling Multisensory Enhancement</a></p>
<p>16 0.076230913 <a title="65-tfidf-16" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>17 0.070109874 <a title="65-tfidf-17" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>18 0.069693319 <a title="65-tfidf-18" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>19 0.06781593 <a title="65-tfidf-19" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>20 0.065269269 <a title="65-tfidf-20" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.174), (1, -0.265), (2, -0.159), (3, 0.063), (4, -0.059), (5, 0.098), (6, -0.341), (7, 0.005), (8, 0.056), (9, 0.07), (10, -0.027), (11, 0.228), (12, 0.15), (13, 0.055), (14, -0.024), (15, 0.059), (16, 0.166), (17, 0.048), (18, -0.095), (19, -0.054), (20, 0.085), (21, -0.036), (22, -0.026), (23, -0.015), (24, 0.149), (25, -0.075), (26, -0.142), (27, -0.065), (28, -0.011), (29, 0.085), (30, -0.046), (31, -0.095), (32, 0.166), (33, -0.08), (34, -0.088), (35, -0.094), (36, -0.059), (37, -0.119), (38, 0.091), (39, -0.031), (40, -0.132), (41, -0.097), (42, 0.015), (43, 0.029), (44, -0.054), (45, 0.079), (46, -0.052), (47, 0.06), (48, 0.056), (49, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99244958 <a title="65-lsi-1" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>Author: Thomas P. Trappenberg, Edmund T. Rolls, Simon M. Stringer</p><p>Abstract: Inferior temporal cortex (IT) neurons have large receptive ﬁelds when a single effective object stimulus is shown against a blank background, but have much smaller receptive ﬁelds when the object is placed in a natural scene. Thus, translation invariant object recognition is reduced in natural scenes, and this may help object selection. We describe a model which accounts for this by competition within an attractor in which the neurons are tuned to different objects in the scene, and the fovea has a higher cortical magniﬁcation factor than the peripheral visual ﬁeld. Furthermore, we show that top-down object bias can increase the receptive ﬁeld size, facilitating object search in complex visual scenes, and providing a model of object-based attention. The model leads to the prediction that introduction of a second object into a scene with blank background will reduce the receptive ﬁeld size to values that depend on the closeness of the second object to the target stimulus. We suggest that mechanisms of this type enable the output of IT to be primarily about one object, so that the areas that receive from IT can select the object as a potential target for action.</p><p>2 0.8386057 <a title="65-lsi-2" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>Author: Antonio Torralba</p><p>Abstract: The most popular algorithms for object detection require the use of exhaustive spatial and scale search procedures. In such approaches, an object is defined by means of local features. fu this paper we show that including contextual information in object detection procedures provides an efficient way of cutting down the need for exhaustive search. We present results with real images showing that the proposed scheme is able to accurately predict likely object classes, locations and sizes. 1</p><p>3 0.61054164 <a title="65-lsi-3" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>Author: Antonino Casile, Michele Rucci</p><p>Abstract: Neural activity appears to be a crucial component for shaping the receptive ﬁelds of cortical simple cells into adjacent, oriented subregions alternately receiving ON- and OFF-center excitatory geniculate inputs. It is known that the orientation selective responses of V1 neurons are reﬁned by visual experience. After eye opening, the spatiotemporal structure of neural activity in the early stages of the visual pathway depends both on the visual environment and on how the environment is scanned. We have used computational modeling to investigate how eye movements might affect the reﬁnement of the orientation tuning of simple cells in the presence of a Hebbian scheme of synaptic plasticity. Levels of correlation between the activity of simulated cells were examined while natural scenes were scanned so as to model sequences of saccades and ﬁxational eye movements, such as microsaccades, tremor and ocular drift. The speciﬁc patterns of activity required for a quantitatively accurate development of simple cell receptive ﬁelds with segregated ON and OFF subregions were observed during ﬁxational eye movements, but not in the presence of saccades or with static presentation of natural visual input. These results suggest an important role for the eye movements occurring during visual ﬁxation in the reﬁnement of orientation selectivity.</p><p>4 0.53923839 <a title="65-lsi-4" href="./nips-2001-Generalizable_Relational_Binding_from_Coarse-coded_Distributed_Representations.html">80 nips-2001-Generalizable Relational Binding from Coarse-coded Distributed Representations</a></p>
<p>Author: Randall C. O'Reilly, R. S. Busby</p><p>Abstract: We present a model of binding of relationship information in a spatial domain (e.g., square above triangle) that uses low-order coarse-coded conjunctive representations instead of more popular temporal synchrony mechanisms. Supporters of temporal synchrony argue that conjunctive representations lack both efﬁciency (i.e., combinatorial numbers of units are required) and systematicity (i.e., the resulting representations are overly speciﬁc and thus do not support generalization to novel exemplars). To counter these claims, we show that our model: a) uses far fewer hidden units than the number of conjunctions represented, by using coarse-coded, distributed representations where each unit has a broad tuning curve through high-dimensional conjunction space, and b) is capable of considerable generalization to novel inputs.</p><p>5 0.5133096 <a title="65-lsi-5" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>Author: Shimon Edelman, Benjamin P. Hiles, Hwajin Yang, Nathan Intrator</p><p>Abstract: To ﬁnd out how the representations of structured visual objects depend on the co-occurrence statistics of their constituents, we exposed subjects to a set of composite images with tight control exerted over (1) the conditional probabilities of the constituent fragments, and (2) the value of Barlow’s criterion of “suspicious coincidence” (the ratio of joint probability to the product of marginals). We then compared the part veriﬁcation response times for various probe/target combinations before and after the exposure. For composite probes, the speedup was much larger for targets that contained pairs of fragments perfectly predictive of each other, compared to those that did not. This effect was modulated by the signiﬁcance of their co-occurrence as estimated by Barlow’s criterion. For lone-fragment probes, the speedup in all conditions was generally lower than for composites. These results shed light on the brain’s strategies for unsupervised acquisition of structural information in vision. 1 Motivation How does the human visual system decide for which objects it should maintain distinct and persistent internal representations of the kind typically postulated by theories of object recognition? Consider, for example, the image shown in Figure 1, left. This image can be represented as a monolithic hieroglyph, a pair of Chinese characters (which we shall refer to as and ), a set of strokes, or, trivially, as a collection of pixels. Note that the second option is only available to a system previously exposed to various combinations of Chinese characters. Indeed, a principled decision whether to represent this image as , or otherwise can only be made on the basis of prior exposure to related images. £ ¡ £¦ ¡ £ ¥¨§¢   ¥¤¢   ¢ According to Barlow’s [1] insight, one useful principle is tallying suspicious coincidences: two candidate fragments and should be combined into a composite object if the probability of their joint appearance is much higher than , which is the probability expected in the case of their statistical independence. This criterion may be compared to the Minimum Description Length (MDL) principle, which has been previously discussed in the context of object representation [2, 3]. In a simpliﬁed form [4], MDL calls for representing explicitly as a whole if , just as the principle of suspicious coincidences does. £ ©¢  £  ¢ ¥¤¥  £¦ ¢ ¥  £  ¢   £¦ ¢ ¥¤¥! ¨§¥ £ ¢ £ ©¢  £¦  £ ¨§¢¥ ¡ ¢   While the Barlow/MDL criterion certainly indicates a suspicious coincidence, there are additional probabilistic considerations that may be used and . One example is the possiin setting the degree of association between ble perfect predictability of from and vice versa, as measured by . If , then and are perfectly predictive of each other and should really be coded by a single symbol, whereas the MDL criterion may suggest merely that some association between the representation of and that of be established. In comparison, if and are not perfectly predictive of each other ( ), there is a case to be made in favor of coding them separately to allow for a maximally expressive representation, whereas MDL may actually suggest a high degree of association ). In this study we investigated whether the human (if visual system uses a criterion based on alongside MDL while learning (in an unsupervised manner) to represent composite objects. £ £  £ ¢  ¥  ¥ © §¥ ¡ ¢  ¨¦¤</p><p>6 0.50789261 <a title="65-lsi-6" href="./nips-2001-Modeling_the_Modulatory_Effect_of_Attention_on_Human_Spatial_Vision.html">124 nips-2001-Modeling the Modulatory Effect of Attention on Human Spatial Vision</a></p>
<p>7 0.39485231 <a title="65-lsi-7" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>8 0.39338291 <a title="65-lsi-8" href="./nips-2001-Receptive_field_structure_of_flow_detectors_for_heading_perception.html">158 nips-2001-Receptive field structure of flow detectors for heading perception</a></p>
<p>9 0.3919898 <a title="65-lsi-9" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>10 0.3492533 <a title="65-lsi-10" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>11 0.34081638 <a title="65-lsi-11" href="./nips-2001-A_Maximum-Likelihood_Approach_to_Modeling_Multisensory_Enhancement.html">11 nips-2001-A Maximum-Likelihood Approach to Modeling Multisensory Enhancement</a></p>
<p>12 0.31265712 <a title="65-lsi-12" href="./nips-2001-A_Hierarchical_Model_of_Complex_Cells_in_Visual_Cortex_for_the_Binocular_Perception_of_Motion-in-Depth.html">10 nips-2001-A Hierarchical Model of Complex Cells in Visual Cortex for the Binocular Perception of Motion-in-Depth</a></p>
<p>13 0.31029359 <a title="65-lsi-13" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>14 0.29397601 <a title="65-lsi-14" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>15 0.29139492 <a title="65-lsi-15" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>16 0.28034514 <a title="65-lsi-16" href="./nips-2001-3_state_neurons_for_contextual_processing.html">2 nips-2001-3 state neurons for contextual processing</a></p>
<p>17 0.27159381 <a title="65-lsi-17" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>18 0.26986861 <a title="65-lsi-18" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>19 0.2416659 <a title="65-lsi-19" href="./nips-2001-A_Neural_Oscillator_Model_of_Auditory_Selective_Attention.html">14 nips-2001-A Neural Oscillator Model of Auditory Selective Attention</a></p>
<p>20 0.23098551 <a title="65-lsi-20" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.025), (17, 0.025), (19, 0.035), (27, 0.087), (30, 0.21), (38, 0.096), (59, 0.013), (63, 0.01), (72, 0.034), (74, 0.012), (79, 0.039), (83, 0.014), (91, 0.144), (93, 0.173)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92761248 <a title="65-lda-1" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>Author: Thomas P. Trappenberg, Edmund T. Rolls, Simon M. Stringer</p><p>Abstract: Inferior temporal cortex (IT) neurons have large receptive ﬁelds when a single effective object stimulus is shown against a blank background, but have much smaller receptive ﬁelds when the object is placed in a natural scene. Thus, translation invariant object recognition is reduced in natural scenes, and this may help object selection. We describe a model which accounts for this by competition within an attractor in which the neurons are tuned to different objects in the scene, and the fovea has a higher cortical magniﬁcation factor than the peripheral visual ﬁeld. Furthermore, we show that top-down object bias can increase the receptive ﬁeld size, facilitating object search in complex visual scenes, and providing a model of object-based attention. The model leads to the prediction that introduction of a second object into a scene with blank background will reduce the receptive ﬁeld size to values that depend on the closeness of the second object to the target stimulus. We suggest that mechanisms of this type enable the output of IT to be primarily about one object, so that the areas that receive from IT can select the object as a potential target for action.</p><p>2 0.83324647 <a title="65-lda-2" href="./nips-2001-Information-Geometric_Decomposition_in_Spike_Analysis.html">96 nips-2001-Information-Geometric Decomposition in Spike Analysis</a></p>
<p>Author: Hiroyuki Nakahara, Shun-ichi Amari</p><p>Abstract: We present an information-geometric measure to systematically investigate neuronal firing patterns, taking account not only of the second-order but also of higher-order interactions. We begin with the case of two neurons for illustration and show how to test whether or not any pairwise correlation in one period is significantly different from that in the other period. In order to test such a hypothesis of different firing rates, the correlation term needs to be singled out 'orthogonally' to the firing rates, where the null hypothesis might not be of independent firing. This method is also shown to directly associate neural firing with behavior via their mutual information, which is decomposed into two types of information, conveyed by mean firing rate and coincident firing, respectively. Then, we show that these results, using the 'orthogonal' decomposition, are naturally extended to the case of three neurons and n neurons in general. 1</p><p>3 0.82343161 <a title="65-lda-3" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>Author: Shimon Edelman, Benjamin P. Hiles, Hwajin Yang, Nathan Intrator</p><p>Abstract: To ﬁnd out how the representations of structured visual objects depend on the co-occurrence statistics of their constituents, we exposed subjects to a set of composite images with tight control exerted over (1) the conditional probabilities of the constituent fragments, and (2) the value of Barlow’s criterion of “suspicious coincidence” (the ratio of joint probability to the product of marginals). We then compared the part veriﬁcation response times for various probe/target combinations before and after the exposure. For composite probes, the speedup was much larger for targets that contained pairs of fragments perfectly predictive of each other, compared to those that did not. This effect was modulated by the signiﬁcance of their co-occurrence as estimated by Barlow’s criterion. For lone-fragment probes, the speedup in all conditions was generally lower than for composites. These results shed light on the brain’s strategies for unsupervised acquisition of structural information in vision. 1 Motivation How does the human visual system decide for which objects it should maintain distinct and persistent internal representations of the kind typically postulated by theories of object recognition? Consider, for example, the image shown in Figure 1, left. This image can be represented as a monolithic hieroglyph, a pair of Chinese characters (which we shall refer to as and ), a set of strokes, or, trivially, as a collection of pixels. Note that the second option is only available to a system previously exposed to various combinations of Chinese characters. Indeed, a principled decision whether to represent this image as , or otherwise can only be made on the basis of prior exposure to related images. £ ¡ £¦ ¡ £ ¥¨§¢   ¥¤¢   ¢ According to Barlow’s [1] insight, one useful principle is tallying suspicious coincidences: two candidate fragments and should be combined into a composite object if the probability of their joint appearance is much higher than , which is the probability expected in the case of their statistical independence. This criterion may be compared to the Minimum Description Length (MDL) principle, which has been previously discussed in the context of object representation [2, 3]. In a simpliﬁed form [4], MDL calls for representing explicitly as a whole if , just as the principle of suspicious coincidences does. £ ©¢  £  ¢ ¥¤¥  £¦ ¢ ¥  £  ¢   £¦ ¢ ¥¤¥! ¨§¥ £ ¢ £ ©¢  £¦  £ ¨§¢¥ ¡ ¢   While the Barlow/MDL criterion certainly indicates a suspicious coincidence, there are additional probabilistic considerations that may be used and . One example is the possiin setting the degree of association between ble perfect predictability of from and vice versa, as measured by . If , then and are perfectly predictive of each other and should really be coded by a single symbol, whereas the MDL criterion may suggest merely that some association between the representation of and that of be established. In comparison, if and are not perfectly predictive of each other ( ), there is a case to be made in favor of coding them separately to allow for a maximally expressive representation, whereas MDL may actually suggest a high degree of association ). In this study we investigated whether the human (if visual system uses a criterion based on alongside MDL while learning (in an unsupervised manner) to represent composite objects. £ £  £ ¢  ¥  ¥ © §¥ ¡ ¢  ¨¦¤</p><p>4 0.80929822 <a title="65-lda-4" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>Author: Sebastian Thrun, John Langford, Vandi Verma</p><p>Abstract: We propose a new particle ﬁlter that incorporates a model of costs when generating particles. The approach is motivated by the observation that the costs of accidentally not tracking hypotheses might be signiﬁcant in some areas of state space, and next to irrelevant in others. By incorporating a cost model into particle ﬁltering, states that are more critical to the system performance are more likely to be tracked. Automatic calculation of the cost model is implemented using an MDP value function calculation that estimates the value of tracking a particular state. Experiments in two mobile robot domains illustrate the appropriateness of the approach.</p><p>5 0.79163551 <a title="65-lda-5" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>Author: Xiaohui Xie, Martin A. Giese</p><p>Abstract: Asymmetric lateral connections are one possible mechanism that can account for the direction selectivity of cortical neurons. We present a mathematical analysis for a class of these models. Contrasting with earlier theoretical work that has relied on methods from linear systems theory, we study the network’s nonlinear dynamic properties that arise when the threshold nonlinearity of the neurons is taken into account. We show that such networks have stimulus-locked traveling pulse solutions that are appropriate for modeling the responses of direction selective cortical neurons. In addition, our analysis shows that outside a certain regime of stimulus speeds the stability of this solutions breaks down giving rise to another class of solutions that are characterized by speciﬁc spatiotemporal periodicity. This predicts that if direction selectivity in the cortex is mainly achieved by asymmetric lateral connections lurching activity waves might be observable in ensembles of direction selective cortical neurons within appropriate regimes of the stimulus speed.</p><p>6 0.77904552 <a title="65-lda-6" href="./nips-2001-Reducing_multiclass_to_binary_by_coupling_probability_estimates.html">159 nips-2001-Reducing multiclass to binary by coupling probability estimates</a></p>
<p>7 0.77898186 <a title="65-lda-7" href="./nips-2001-Speech_Recognition_with_Missing_Data_using_Recurrent_Neural_Nets.html">173 nips-2001-Speech Recognition with Missing Data using Recurrent Neural Nets</a></p>
<p>8 0.77822429 <a title="65-lda-8" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<p>9 0.77389407 <a title="65-lda-9" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>10 0.76578057 <a title="65-lda-10" href="./nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">102 nips-2001-KLD-Sampling: Adaptive Particle Filters</a></p>
<p>11 0.76424217 <a title="65-lda-11" href="./nips-2001-An_Efficient_Clustering_Algorithm_Using_Stochastic_Association_Model_and_Its_Implementation_Using_Nanostructures.html">33 nips-2001-An Efficient Clustering Algorithm Using Stochastic Association Model and Its Implementation Using Nanostructures</a></p>
<p>12 0.76293564 <a title="65-lda-12" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>13 0.75448686 <a title="65-lda-13" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>14 0.74071914 <a title="65-lda-14" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>15 0.73482335 <a title="65-lda-15" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>16 0.7331785 <a title="65-lda-16" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>17 0.73089188 <a title="65-lda-17" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>18 0.72721601 <a title="65-lda-18" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>19 0.72332042 <a title="65-lda-19" href="./nips-2001-Stochastic_Mixed-Signal_VLSI_Architecture_for_High-Dimensional_Kernel_Machines.html">176 nips-2001-Stochastic Mixed-Signal VLSI Architecture for High-Dimensional Kernel Machines</a></p>
<p>20 0.72073805 <a title="65-lda-20" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
