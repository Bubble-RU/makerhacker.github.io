<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-174" href="#">nips2001-174</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</h1>
<br/><p>Source: <a title="nips-2001-174-pdf" href="http://papers.nips.cc/paper/2038-spike-timing-and-the-coding-of-naturalistic-sounds-in-a-central-auditory-area-of-songbirds.pdf">pdf</a></p><p>Author: B. D. Wright, Kamal Sen, William Bialek, A. J. Doupe</p><p>Abstract: In nature, animals encounter high dimensional sensory stimuli that have complex statistical and dynamical structure. Attempts to study the neural coding of these natural signals face challenges both in the selection of the signal ensemble and in the analysis of the resulting neural responses. For zebra ﬁnches, naturalistic stimuli can be deﬁned as sounds that they encounter in a colony of conspeciﬁc birds. We assembled an ensemble of these sounds by recording groups of 10-40 zebra ﬁnches, and then analyzed the response of single neurons in the songbird central auditory area (ﬁeld L) to continuous playback of long segments from this ensemble. Following methods developed in the ﬂy visual system, we measured the information that spike trains provide about the acoustic stimulus without any assumptions about which features of the stimulus are relevant. Preliminary results indicate that large amounts of information are carried by spike timing, with roughly half of the information accessible only at time resolutions better than 10 ms; additional information is still being revealed as time resolution is improved to 2 ms. Information can be decomposed into that carried by the locking of individual spikes to the stimulus (or modulations of spike rate) vs. that carried by timing in spike patterns. Initial results show that in ﬁeld L, temporal patterns give at least % extra information. Thus, single central auditory neurons can provide an informative representation of naturalistic sounds, in which spike timing may play a signiﬁcant role.   </p><p>Reference: <a title="nips-2001-174-reference" href="../nips2001_reference/nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds £ ¡ ¤¢   © ¨§ ¦  ¥ ¥  £ ¡ ¤¢   £ ¡ ¤¢   Brian D. [sent-1, score-0.897]
</p><p>2 edu    £    §  ©      Abstract In nature, animals encounter high dimensional sensory stimuli that have complex statistical and dynamical structure. [sent-6, score-0.241]
</p><p>3 Attempts to study the neural coding of these natural signals face challenges both in the selection of the signal ensemble and in the analysis of the resulting neural responses. [sent-7, score-0.206]
</p><p>4 For zebra ﬁnches, naturalistic stimuli can be deﬁned as sounds that they encounter in a colony of conspeciﬁc birds. [sent-8, score-0.784]
</p><p>5 We assembled an ensemble of these sounds by recording groups of 10-40 zebra ﬁnches, and then analyzed the response of single neurons in the songbird central auditory area (ﬁeld L) to continuous playback of long segments from this ensemble. [sent-9, score-1.147]
</p><p>6 Following methods developed in the ﬂy visual system, we measured the information that spike trains provide about the acoustic stimulus without any assumptions about which features of the stimulus are relevant. [sent-10, score-0.989]
</p><p>7 Preliminary results indicate that large amounts of information are carried by spike timing, with roughly half of the information accessible only at time resolutions better than 10 ms; additional information is still being revealed as time resolution is improved to 2 ms. [sent-11, score-0.909]
</p><p>8 Information can be decomposed into that carried by the locking of individual spikes to the stimulus (or modulations of spike rate) vs. [sent-12, score-0.719]
</p><p>9 Thus, single central auditory neurons can provide an informative representation of naturalistic sounds, in which spike timing may play a signiﬁcant role. [sent-15, score-1.151]
</p><p>10     1 Introduction Nearly ﬁfty years ago, Barlow [1] and Attneave [2] suggested that the brain may construct a neural code that provides an efﬁcient representation for the sensory stimuli that occur in the natural world. [sent-16, score-0.392]
</p><p>11 Slightly earlier, MacKay and McCulloch [3] emphasized that neurons that could make use of spike timing—rather than a coarser “rate code”—would have available a vastly larger capacity to convey information, although they left open the question of whether this capacity is used efﬁciently. [sent-17, score-0.746]
</p><p>12 A real attack on these issues requires (at least) that we actually measure the information content and efﬁciency of the neural code under stimulus conditions that approximate the natural ones. [sent-19, score-0.379]
</p><p>13 In practice, constructing an ensemble of “natural” stimuli inevitably involves compromises, and the responses to such complex dynamic signals can be very difﬁcult to analyze. [sent-20, score-0.376]
</p><p>14 At present the clearest evidence on efﬁciency and timing in the coding of naturalistic stimuli comes from central invertebrate neurons [4, 5] and from the sensory periphery [6, 7] and thalamus [8, 9] of vertebrates. [sent-21, score-0.78]
</p><p>15 The situation for central vertebrate brain areas is much less clear. [sent-22, score-0.203]
</p><p>16 Here we use the songbird auditory system as an accessible test case for these ideas. [sent-23, score-0.368]
</p><p>17 The set of songbird telencephalic auditory areas known as the ﬁeld L complex is analogous to mammalian auditory cortex and contains neurons that are strongly driven by natural sounds, including the songs of birds of the same species (conspeciﬁcs) [10, 11, 12, 13]. [sent-24, score-1.065]
</p><p>18 We record from the zebra ﬁnch ﬁeld L, using naturalistic stimuli that consist of recordings from groups of 10-40 conspeciﬁc birds. [sent-25, score-0.545]
</p><p>19 2 A naturalistic ensemble Auditory processing of complex sounds is critical for perception and communication in many species, including humans, but surprisingly little is known about how high level brain areas accomplish this task. [sent-28, score-0.689]
</p><p>20 Songbirds provide a useful model for tackling this issue, because each bird within a species produces a complex individualized acoustic signal known as a song, which reﬂects some innate information about the species’ song as well as information learned from a “tutor” in early life. [sent-29, score-0.443]
</p><p>21 In addition to learning their own song, birds use the acoustic information in songs of others to identify mates and group members, to discriminate neighbors from intruders, and to control their living space [14]. [sent-30, score-0.312]
</p><p>22 Consistent with how ethologically critical these functions are, songbirds have a large number of forebrain auditory areas with strong and increasingly specialized responses to songs [11, 15, 16]. [sent-31, score-0.631]
</p><p>23 The combination of a rich set of behaviorally relevant stimuli and a series of high-level auditory areas responsive to those sounds provides an opportunity to reveal general principles of central neural encoding of complex sensory stimuli. [sent-32, score-0.831]
</p><p>24 Many prior studies have chosen to study neural responses to individual songs or altered versions thereof. [sent-33, score-0.201]
</p><p>25 In order to make the sounds studied increasingly complex and natural, we have made recordings of the sounds encountered by birds in our colony of zebra ﬁnches. [sent-34, score-0.923]
</p><p>26 To generate the sound ensemble that was used in this study we ﬁrst created long records of the vocalizations of groups of 10-40 zebra ﬁnches in a soundproof acoustic chamber with a directional microphone above the bird cages. [sent-35, score-0.526]
</p><p>27 The group of birds generated a wide variety of vocalizations including songs and a variety of different types of calls. [sent-36, score-0.251]
</p><p>28 Segments of these sounds were then joined to create the sounds presented in the experiment. [sent-37, score-0.424]
</p><p>29     ¡   We recorded the neural responses in ﬁeld L of one of the birds from the group to the ensemble of natural sounds played back through a speaker, at an intensity approximately equal to that in the colony recording. [sent-39, score-0.669]
</p><p>30 Spike raster of 4 seconds of the responses of a single neuron in ﬁeld L to a 30 second segment of a natural sound ensemble of zebra ﬁnch sounds. [sent-45, score-0.575]
</p><p>31 Field L neurons respond to simple stimuli such as tone bursts, and are organized in a roughly tonotopic fashion [18], but also respond robustly to many complex sounds, including songs. [sent-55, score-0.292]
</p><p>32 Figure 1 shows 4 seconds of the responses of a cell in ﬁeld L to repeated presentations of a 30 sec segment from the natural ensemble described above. [sent-56, score-0.399]
</p><p>33 Averaging over presentations, we see that spike rates are well modulated. [sent-57, score-0.458]
</p><p>34 Looking at the responses on a ﬁner time resolution we see that aspects of the spike train are reproducible ms time scale. [sent-58, score-1.094]
</p><p>35 This encourages us to measure the information content of on at least a these responses over a range of time scales, down to millisecond resolution. [sent-59, score-0.199]
</p><p>36     ¡   Our approach to estimating the information content of spike trains follows Ref. [sent-60, score-0.542]
</p><p>37 Within this window we discretize the spike arrival times with resolution so that the response becomes a “word” with letters. [sent-63, score-0.703]
</p><p>38 If the time resolution is very small, the allowed letters are only 1 and 0, but as becomes larger one must keep track of multiple spikes within each bin. [sent-64, score-0.258]
</p><p>39 06  1/Nrepeats  Figure 2: Mutual information rate for the spike train is shown as a function of data size for ms and ms. [sent-71, score-0.871]
</p><p>40   © ¥ £ ¨§¦¤¢      £      ¥ ¡¦¤  the probability distribution of words, , and the entropy of this distribution sets the capacity of the code to convey information about the stimulus:  B@ 65© ¥ £ 1 (© ¥ £ " ! [sent-72, score-0.46]
</p><p>41 CA987)4§'3¢  2%0)¨§'&¢ %$#  ©  (1)   ¥ ¦¤  £ ¥   where the notation reminds us that the entropy depends both on the size of the words that we consider and on the time resolution with which we classify the responses. [sent-73, score-0.439]
</p><p>42 We can think of this entropy as measuring the size of the neuron’s vocabulary. [sent-74, score-0.214]
</p><p>43 Because the whole experiment contributes to deﬁning the vocabulary size, estimating the distribution and hence the total entropy is not signiﬁcantly limited by the problems of ﬁnite sample size. [sent-75, score-0.256]
</p><p>44 2 in the stability of the total entropy with changing the number of repeats used in the analysis. [sent-77, score-0.256]
</p><p>45 Here we show the total entropy as a rate in bits per second by dividing the entropy by the time window . [sent-78, score-0.673]
</p><p>46 © ¥£ ¨§D¤¢  £  While the capacity of the code is limited by the total entropy, to convey information particular words in the vocabulary must be associated, more or less reliably, with particular stimulus features. [sent-79, score-0.516]
</p><p>47 If we look at one time relative to the (long) stimulus, and examine the words generated on repeated presentations, we sample the conditional distribution . [sent-80, score-0.134]
</p><p>48 This distribution has an entropy that quantiﬁes the noise in the response at time , and averaging over all times we obtain the average noise entropy,  © ¢ G¨§'E¢ F ¥ £  (2)  ¢  ¢  B@ 65b ` F CA987cDa© ¢ G4§¥ £ ¢  Y90X© ¢ G4§¥ £ ¢ %$WVU  © 1 ( F " ! [sent-81, score-0.395]
</p><p>49 Technically, the above average should be an average over stimuli , however, for a sufﬁciently long and rich stimulus, the ensemble average over can be replaced by a time average. [sent-83, score-0.275]
</p><p>50   ¥  ¦¤ S£ ¥     © Q R R        This is what we expect for any entropy estimate if the distribution is well sampled, and if we make stronger assumptions about the sampling process (independence of trials etc. [sent-87, score-0.259]
</p><p>51 (3) is a strong indication that larger sample sizes will be consistent with ; further, this extrapolation can be tested against bounds on the entropy that are derived from more robust quantities [4]. [sent-90, score-0.269]
</p><p>52 "   © $  ¥  ¦¤ S£ ¥  ¥ ¦¤  £ ¥     ©  ¥ ¦¤    £  Ideally, to measure the spike train total and noise entropy rates, we want to go to the limit of inﬁnite word duration. [sent-93, score-0.972]
</p><p>53 A true entropy is extensive, which here means that it grows linearly with spike train word duration , so that the entropy rate is constant. [sent-94, score-1.263]
</p><p>54 For ﬁnite word duration however, words sampled at neighboring times will have correlations between them due, in part, to correlations in the stimulus (for birdsong these stimulus autocorrelation time scales can extend up to ms). [sent-95, score-0.662]
</p><p>55 Since the word samples are not completely independent, the raw entropy rate is an overestimate of the true entropy rate. [sent-96, score-0.669]
</p><p>56 The effect is larger for smaller word duration and the leading dependence of the raw estimate is  £§     £  %      ¨   ¡ ¡ B ¤¤¡  (4)  £   © $  ' (&  ¥ ¦¤     ¥ %       © $  ¥  ¦¤ S£ ¥  %  where and we have already taken the inﬁnite data size limit. [sent-97, score-0.225]
</p><p>57 We cannot directly take the large limit, since for large word lengths we eventually reach a data sampling limit beyond which we are unable to reliably compute the word distributions. [sent-98, score-0.281]
</p><p>58 We have checked that our data shows this behavior and that it sets in for word sizes below the limit where the data sampling problem occurs. [sent-101, score-0.203]
</p><p>59 For example, in the case of ms, it applies for below the limit of ms (above this we the noise entropy, for run into sampling problems). [sent-102, score-0.277]
</p><p>60 £  £  £   1         ) 0'  ¥ ¦¤  Finally, we combine estimates of total and noise entropies to obtain the information that words carry about the sensory stimulus, (5) #  @65 A987)©  ¥  ¦¤ S£ ¥  P QR  H  ! [sent-104, score-0.246]
</p><p>61 ©  ¥  ¦¤ S£ ¥ ¨     ©      £ ¥ ¥ ¦¤  £  2  Figure 2 shows the total and noise entropy rates as well as the mutual information rate for ms and time resolution ms. [sent-105, score-0.918]
</p><p>62 The error bars on the raw a time window entropy and information rates were estimated to be approximately bits/sec using a simple bootstrap procedure over the repeated trials. [sent-106, score-0.447]
</p><p>63 The extrapolation to inﬁnite data size is shown for the mutual information rate estimate (error bars in the extrapolated values will be bits/sec) and is consistent with the prediction of Eq. [sent-107, score-0.336]
</p><p>64 Since the total entropy is nearly extensive and the noise entropy rate decreases with word duration due to subextensive corrections as described above, the mutual information rate shown in Fig. [sent-109, score-1.034]
</p><p>65 We ﬁnd that there is an upward change in the mutual information #  3       ¥ ¦¤    3  #  54  5  Information Rate (bits/sec)  4. [sent-111, score-0.192]
</p><p>66 5 0  20  25  30  35  Figure 3: Information rates for the spike train ( ms) and single spike events as a function of time resolution of the spike rasters, corrected for ﬁnite data size effects. [sent-115, score-1.78]
</p><p>67   £  ¥ ©¤  ¥¦¤     £    ¡     ¥ ©¤  rate (computed at ms and ms) of %, in the large limit. [sent-116, score-0.278]
</p><p>68 For ms that is in the simplicity in the following, we shall look at a ﬁxed word duration well-sampled region for all time resolutions considered. [sent-117, score-0.482]
</p><p>69          £      The mutual information rate measures the rate at which the spike train removes uncertainty about the stimulus. [sent-118, score-0.885]
</p><p>70 However, the mutual information estimate does not depend on identifying either the relevant features of the stimulus or the relevant features of the response, which is crucial in analyzing the response to such complex stimuli. [sent-119, score-0.43]
</p><p>71 £  One way to look at the information results is to ﬁx our time window and ask what happens as we change our time resolution . [sent-121, score-0.341]
</p><p>72 When , the “word” describing the response is nothing but the number of spikes in the window, so we have a rate or counting code. [sent-122, score-0.202]
</p><p>73 We chose a range of values from ms in our analyses to cover previously observed response windows for ﬁeld L neurons and to probe the behaviorally relevant time scale ( ms) of individual song syllables or notes. [sent-124, score-0.538]
</p><p>74 The spike train mutual information shows a clear increase as the timing resolution is improved. [sent-127, score-0.946]
</p><p>75 3 shows that roughly half of the information is accessible at time resolutions better than ms and additional information is still being revealed as time resolution is improved to 2 ms. [sent-129, score-0.602]
</p><p>76 In the latter case, we have precisely what we mean by a temporal or timing code: there is information beyond that attributable to the probability of single spike events occurring at time relative to the onset of the stimulus. [sent-132, score-0.851]
</p><p>77 By event at time , we mean that the event occurs between time and time , where is the resolution at which we are looking at the spike train. [sent-133, score-0.825]
</p><p>78 This probability is simply proportional to the ﬁring rate (or peri-stimulus time histogram (PSTH)) at time normalized by the mean ﬁring rate . [sent-134, score-0.28]
</p><p>79 ¢ £ ©¢¥     ¥ ¢¢  ¢  ¢  ¢  ¡  spk @   ¥     ¢ ©¢¢¥  ¢  where denotes the stimulus history ( ). [sent-136, score-0.284]
</p><p>80 The probability of a spike event at , a priori of knowing the stimulus history, is ﬂat: spk @ . [sent-137, score-0.78]
</p><p>81 Thus, the mutual information between the stimulus and the single spike events is [20]:  ¥  © F ¢     ¥ ¦¤    ©¢  B b C@A98675 E` © ¢ ¡ ¥   ¢    ! [sent-138, score-0.922]
</p><p>82 Supposing that the individual spike events are inis approximately bit for dependent (i. [sent-142, score-0.545]
</p><p>83 no intrinsic spike train correlations), the information rate in single spike events is obtained by multiplying the mutual information per spike (Eq. [sent-144, score-1.87]
</p><p>84 This gives an upper bound to the single spike event contribution to the information rate and is shown in the lower curve of Fig. [sent-146, score-0.656]
</p><p>85 Comparing with the spike train information (upper curve), we ms, there is at least % of the total information in see that at a resolution of the spike train that cannot be attributable to single spike events. [sent-148, score-1.903]
</p><p>86 Thus there is some pattern of spikes that is contributing synergistically to the mutual information. [sent-149, score-0.183]
</p><p>87 The fact discussed, in the previous section, that the spike train information rate grows subextensively with the the word duration out to the point where data sampling becomes problematic is further conﬁrmation of the synergy from spike patterns. [sent-150, score-1.376]
</p><p>88 Thus we have shown model-independent evidence for a temporal code in the neural responses. [sent-151, score-0.14]
</p><p>89 ¥ ©¤       1#              ¥ V¦¤          #  3  4  5 Conclusion Until now, few experiments on neural responses in high level, central vertebrate brain areas have measured the information that these responses provide about dynamic, naturalistic sensory signals. [sent-152, score-0.755]
</p><p>90 As emphasized in earlier work on invertebrate systems, information theoretic approaches have the advantage that they require no assumptions about the features of the stimulus to which neurons respond. [sent-153, score-0.396]
</p><p>91 Using this method in the songbird auditory forebrain, we found that patterns of spikes seem to be special events in the neural code of these neurons, since they carry more information than expected by adding up the contributions of individual spikes. [sent-154, score-0.619]
</p><p>92 It remains to be determined what these spike patterns are, what stimulus features they may encode, and what mechanisms may be responsible for reading such codes at even higher levels of processing. [sent-155, score-0.65]
</p><p>93 Naturalistic stimuli increase the rate and efﬁciency of information transmission by primary auditory afferents. [sent-212, score-0.446]
</p><p>94 Variability and information in a neural code of the cat lateral geniculate nucleus. [sent-248, score-0.141]
</p><p>95 Responsiveness of units in the auditory neostriatum of the guinea fowl (Numida meleagris) to species-speciﬁc calls and synthetic stimuli II. [sent-257, score-0.365]
</p><p>96 Gradual emergence of song selectivity in sensorimotor structures of the male zebra ﬁnch song system. [sent-277, score-0.416]
</p><p>97 Spectral temporal receptive ﬁelds of nonlinear auditory neurons obtained using natural sounds. [sent-288, score-0.382]
</p><p>98 Acoustic parameters underlying the responses of song-speciﬁc neurons in the white-crowned sparrow. [sent-306, score-0.202]
</p><p>99 Feature analysis of natural sounds in the songbird auditory forebrain. [sent-317, score-0.58]
</p><p>100 Development of song responses in the zebra ﬁnch caudomedial neostriatum: role of genomic and electrophysiological activities. [sent-328, score-0.41]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spike', 0.458), ('entropy', 0.214), ('sounds', 0.212), ('auditory', 0.207), ('naturalistic', 0.207), ('stimulus', 0.192), ('ms', 0.189), ('zebra', 0.184), ('resolution', 0.138), ('birds', 0.12), ('word', 0.118), ('song', 0.116), ('songbird', 0.115), ('mutual', 0.114), ('ensemble', 0.112), ('stimuli', 0.112), ('responses', 0.11), ('code', 0.103), ('timing', 0.101), ('train', 0.097), ('nch', 0.092), ('spk', 0.092), ('neurons', 0.092), ('songs', 0.091), ('rate', 0.089), ('sensory', 0.087), ('events', 0.087), ('eld', 0.086), ('bialek', 0.085), ('duration', 0.073), ('bird', 0.073), ('species', 0.073), ('areas', 0.072), ('spikes', 0.069), ('colony', 0.069), ('conspeci', 0.069), ('doupe', 0.069), ('nches', 0.069), ('psth', 0.069), ('songbirds', 0.069), ('window', 0.063), ('acoustic', 0.063), ('capacity', 0.057), ('extrapolation', 0.055), ('sound', 0.054), ('central', 0.053), ('nite', 0.051), ('time', 0.051), ('resolutions', 0.051), ('sen', 0.051), ('segments', 0.049), ('presentations', 0.048), ('convey', 0.048), ('coding', 0.048), ('repeated', 0.047), ('natural', 0.046), ('attneave', 0.046), ('attributable', 0.046), ('behaviorally', 0.046), ('konishi', 0.046), ('margoliash', 0.046), ('mcculloch', 0.046), ('neostriatum', 0.046), ('playback', 0.046), ('theunissen', 0.046), ('tonotopic', 0.046), ('accessible', 0.046), ('trains', 0.046), ('sampling', 0.045), ('response', 0.044), ('brain', 0.044), ('ef', 0.044), ('ciency', 0.043), ('noise', 0.043), ('complex', 0.042), ('total', 0.042), ('recordings', 0.042), ('increasingly', 0.042), ('forebrain', 0.04), ('invertebrate', 0.04), ('thalamus', 0.04), ('extrapolated', 0.04), ('koberle', 0.04), ('vocalizations', 0.04), ('checked', 0.04), ('upward', 0.04), ('event', 0.038), ('information', 0.038), ('princeton', 0.038), ('temporal', 0.037), ('segment', 0.036), ('ring', 0.036), ('ruyter', 0.036), ('steveninck', 0.036), ('words', 0.036), ('raw', 0.034), ('vertebrate', 0.034), ('emphasized', 0.034), ('barlow', 0.034), ('single', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="174-tfidf-1" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>Author: B. D. Wright, Kamal Sen, William Bialek, A. J. Doupe</p><p>Abstract: In nature, animals encounter high dimensional sensory stimuli that have complex statistical and dynamical structure. Attempts to study the neural coding of these natural signals face challenges both in the selection of the signal ensemble and in the analysis of the resulting neural responses. For zebra ﬁnches, naturalistic stimuli can be deﬁned as sounds that they encounter in a colony of conspeciﬁc birds. We assembled an ensemble of these sounds by recording groups of 10-40 zebra ﬁnches, and then analyzed the response of single neurons in the songbird central auditory area (ﬁeld L) to continuous playback of long segments from this ensemble. Following methods developed in the ﬂy visual system, we measured the information that spike trains provide about the acoustic stimulus without any assumptions about which features of the stimulus are relevant. Preliminary results indicate that large amounts of information are carried by spike timing, with roughly half of the information accessible only at time resolutions better than 10 ms; additional information is still being revealed as time resolution is improved to 2 ms. Information can be decomposed into that carried by the locking of individual spikes to the stimulus (or modulations of spike rate) vs. that carried by timing in spike patterns. Initial results show that in ﬁeld L, temporal patterns give at least % extra information. Thus, single central auditory neurons can provide an informative representation of naturalistic sounds, in which spike timing may play a signiﬁcant role.   </p><p>2 0.24905477 <a title="174-tfidf-2" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>Author: Gal Chechik, Amir Globerson, M. J. Anderson, E. D. Young, Israel Nelken, Naftali Tishby</p><p>Abstract: The way groups of auditory neurons interact to code acoustic information is investigated using an information theoretic approach. We develop measures of redundancy among groups of neurons, and apply them to the study of collaborative coding efficiency in two processing stations in the auditory pathway: the inferior colliculus (IC) and the primary auditory cortex (AI). Under two schemes for the coding of the acoustic content, acoustic segments coding and stimulus identity coding, we show differences both in information content and group redundancies between IC and AI neurons. These results provide for the first time a direct evidence for redundancy reduction along the ascending auditory pathway, as has been hypothesized for theoretical considerations [Barlow 1959,2001]. The redundancy effects under the single-spikes coding scheme are significant only for groups larger than ten cells, and cannot be revealed with the redundancy measures that use only pairs of cells. The results suggest that the auditory system transforms low level representations that contain redundancies due to the statistical structure of natural stimuli, into a representation in which cortical neurons extract rare and independent component of complex acoustic signals, that are useful for auditory scene analysis. 1</p><p>3 0.19321631 <a title="174-tfidf-3" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>Author: Julian Eggert, Berthold BĂ¤uml</p><p>Abstract: Mesoscopical, mathematical descriptions of dynamics of populations of spiking neurons are getting increasingly important for the understanding of large-scale processes in the brain using simulations. In our previous work, integral equation formulations for population dynamics have been derived for a special type of spiking neurons. For Integrate- and- Fire type neurons , these formulations were only approximately correct. Here, we derive a mathematically compact, exact population dynamics formulation for Integrate- and- Fire type neurons. It can be shown quantitatively in simulations that the numerical correspondence with microscopically modeled neuronal populations is excellent. 1 Introduction and motivation The goal of the population dynamics approach is to model the time course of the collective activity of entire populations of functionally and dynamically similar neurons in a compact way, using a higher descriptionallevel than that of single neurons and spikes. The usual observable at the level of neuronal populations is the populationaveraged instantaneous firing rate A(t), with A(t)6.t being the number of neurons in the population that release a spike in an interval [t, t+6.t). Population dynamics are formulated in such a way, that they match quantitatively the time course of a given A(t), either gained experimentally or by microscopical, detailed simulation. At least three main reasons can be formulated which underline the importance of the population dynamics approach for computational neuroscience. First, it enables the simulation of extensive networks involving a massive number of neurons and connections, which is typically the case when dealing with biologically realistic functional models that go beyond the single neuron level. Second, it increases the analytical understanding of large-scale neuronal dynamics , opening the way towards better control and predictive capabilities when dealing with large networks. Third, it enables a systematic embedding of the numerous neuronal models operating at different descriptional scales into a generalized theoretic framework, explaining the relationships, dependencies and derivations of the respective models. Early efforts on population dynamics approaches date back as early as 1972, to the work of Wilson and Cowan [8] and Knight [4], which laid the basis for all current population-averaged graded-response models (see e.g. [6] for modeling work using these models). More recently, population-based approaches for spiking neurons were developed, mainly by Gerstner [3, 2] and Knight [5]. In our own previous work [1], we have developed a theoretical framework which enables to systematize and simulate a wide range of models for population-based dynamics. It was shown that the equations of the framework produce results that agree quantitatively well with detailed simulations using spiking neurons, so that they can be used for realistic simulations involving networks with large numbers of spiking neurons. Nevertheless, for neuronal populations composed of Integrate-and-Fire (I&F;) neurons, this framework was only correct in an approximation. In this paper, we derive the exact population dynamics formulation for I&F; neurons. This is achieved by reducing the I&F; population dynamics to a point process and by taking advantage of the particular properties of I&F; neurons. 2 2.1 Background: Integrate-and-Fire dynamics Differential form We start with the standard Integrate- and- Fire (I&F;) model in form of the wellknown differential equation [7] (1) which describes the dynamics of the membrane potential Vi of a neuron i that is modeled as a single compartment with RC circuit characteristics. The membrane relaxation time is in this case T = RC with R being the membrane resistance and C the membrane capacitance. The resting potential v R est is the stationary potential that is approached in the no-input case. The input arriving from other neurons is described in form of a current ji. In addition to eq. (1), which describes the integrate part of the I&F; model, the neuronal dynamics are completed by a nonlinear step. Every time the membrane potential Vi reaches a fixed threshold () from below, Vi is lowered by a fixed amount Ll > 0, and from the new value of the membrane potential integration according to eq. (1) starts again. if Vi(t) = () (from below) . (2) At the same time, it is said that the release of a spike occurred (i.e., the neuron fired), and the time ti = t of this singular event is stored. Here ti indicates the time of the most recent spike. Storing all the last firing times , we gain the sequence of spikes {t{} (spike ordering index j, neuronal index i). 2.2 Integral form Now we look at the single neuron in a neuronal compound. We assume that the input current contribution ji from presynaptic spiking neurons can be described using the presynaptic spike times tf, a response-function ~ and a connection weight WÂˇ . ',J ji(t) = Wi ,j ~(t - tf) (3) l: l: j f Integrating the I&F; equation (1) beginning at the last spiking time tT, which determines the initial condition by Vi(ti) = vi(ti - 0) - 6., where vi(ti - 0) is the membrane potential just before the neuron spikes, we get 1 Vi(t) = v Rest + fj(t - t:) + l: Wi ,j l: a(t - t:; t - tf) , j - Vi(t:)) e- S / T (4) f with the refractory function fj(s) = - (v Rest (5) and the alpha-function r ds</p><p>4 0.1667463 <a title="174-tfidf-4" href="./nips-2001-Citcuits_for_VLSI_Implementation_of_Temporally_Asymmetric_Hebbian_Learning.html">49 nips-2001-Citcuits for VLSI Implementation of Temporally Asymmetric Hebbian Learning</a></p>
<p>Author: A. Bofill, D. P. Thompson, Alan F. Murray</p><p>Abstract: Experimental data has shown that synaptic strength modification in some types of biological neurons depends upon precise spike timing differences between presynaptic and postsynaptic spikes. Several temporally-asymmetric Hebbian learning rules motivated by this data have been proposed. We argue that such learning rules are suitable to analog VLSI implementation. We describe an easily tunable circuit to modify the weight of a silicon spiking neuron according to those learning rules. Test results from the fabrication of the circuit using a O.6J.lm CMOS process are given. 1</p><p>5 0.16374825 <a title="174-tfidf-5" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>Author: Shih-Chii Liu, Jörg Kramer, Giacomo Indiveri, Tobi Delbrück, Rodney J. Douglas</p><p>Abstract: We describe a programmable multi-chip VLSI neuronal system that can be used for exploring spike-based information processing models. The system consists of a silicon retina, a PIC microcontroller, and a transceiver chip whose integrate-and-ﬁre neurons are connected in a soft winner-take-all architecture. The circuit on this multi-neuron chip approximates a cortical microcircuit. The neurons can be conﬁgured for different computational properties by the virtual connections of a selected set of pixels on the silicon retina. The virtual wiring between the different chips is effected by an event-driven communication protocol that uses asynchronous digital pulses, similar to spikes in a neuronal system. We used the multi-chip spike-based system to synthesize orientation-tuned neurons using both a feedforward model and a feedback model. The performance of our analog hardware spiking model matched the experimental observations and digital simulations of continuous-valued neurons. The multi-chip VLSI system has advantages over computer neuronal models in that it is real-time, and the computational time does not scale with the size of the neuronal network.</p><p>6 0.14354241 <a title="174-tfidf-6" href="./nips-2001-Characterizing_Neural_Gain_Control_using_Spike-triggered_Covariance.html">48 nips-2001-Characterizing Neural Gain Control using Spike-triggered Covariance</a></p>
<p>7 0.13413629 <a title="174-tfidf-7" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>8 0.12922886 <a title="174-tfidf-8" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>9 0.11319108 <a title="174-tfidf-9" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>10 0.10924076 <a title="174-tfidf-10" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>11 0.10422538 <a title="174-tfidf-11" href="./nips-2001-A_Maximum-Likelihood_Approach_to_Modeling_Multisensory_Enhancement.html">11 nips-2001-A Maximum-Likelihood Approach to Modeling Multisensory Enhancement</a></p>
<p>12 0.10194837 <a title="174-tfidf-12" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>13 0.099572964 <a title="174-tfidf-13" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>14 0.099313967 <a title="174-tfidf-14" href="./nips-2001-Classifying_Single_Trial_EEG%3A_Towards_Brain_Computer_Interfacing.html">50 nips-2001-Classifying Single Trial EEG: Towards Brain Computer Interfacing</a></p>
<p>15 0.099214382 <a title="174-tfidf-15" href="./nips-2001-A_Neural_Oscillator_Model_of_Auditory_Selective_Attention.html">14 nips-2001-A Neural Oscillator Model of Auditory Selective Attention</a></p>
<p>16 0.087886363 <a title="174-tfidf-16" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>17 0.087132119 <a title="174-tfidf-17" href="./nips-2001-Entropy_and_Inference%2C_Revisited.html">68 nips-2001-Entropy and Inference, Revisited</a></p>
<p>18 0.085294046 <a title="174-tfidf-18" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>19 0.083083659 <a title="174-tfidf-19" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>20 0.082189716 <a title="174-tfidf-20" href="./nips-2001-3_state_neurons_for_contextual_processing.html">2 nips-2001-3 state neurons for contextual processing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.199), (1, -0.3), (2, -0.163), (3, 0.04), (4, 0.051), (5, 0.01), (6, 0.091), (7, -0.004), (8, -0.052), (9, -0.029), (10, 0.019), (11, 0.005), (12, -0.187), (13, 0.008), (14, -0.057), (15, -0.125), (16, 0.044), (17, 0.099), (18, -0.021), (19, 0.101), (20, -0.204), (21, -0.141), (22, -0.065), (23, -0.115), (24, -0.07), (25, -0.013), (26, 0.058), (27, 0.065), (28, 0.006), (29, 0.005), (30, -0.08), (31, -0.055), (32, 0.043), (33, 0.071), (34, -0.045), (35, 0.063), (36, -0.05), (37, -0.039), (38, -0.025), (39, 0.136), (40, 0.025), (41, 0.081), (42, 0.017), (43, -0.055), (44, 0.074), (45, 0.014), (46, -0.016), (47, 0.13), (48, 0.011), (49, 0.101)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97645283 <a title="174-lsi-1" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>Author: B. D. Wright, Kamal Sen, William Bialek, A. J. Doupe</p><p>Abstract: In nature, animals encounter high dimensional sensory stimuli that have complex statistical and dynamical structure. Attempts to study the neural coding of these natural signals face challenges both in the selection of the signal ensemble and in the analysis of the resulting neural responses. For zebra ﬁnches, naturalistic stimuli can be deﬁned as sounds that they encounter in a colony of conspeciﬁc birds. We assembled an ensemble of these sounds by recording groups of 10-40 zebra ﬁnches, and then analyzed the response of single neurons in the songbird central auditory area (ﬁeld L) to continuous playback of long segments from this ensemble. Following methods developed in the ﬂy visual system, we measured the information that spike trains provide about the acoustic stimulus without any assumptions about which features of the stimulus are relevant. Preliminary results indicate that large amounts of information are carried by spike timing, with roughly half of the information accessible only at time resolutions better than 10 ms; additional information is still being revealed as time resolution is improved to 2 ms. Information can be decomposed into that carried by the locking of individual spikes to the stimulus (or modulations of spike rate) vs. that carried by timing in spike patterns. Initial results show that in ﬁeld L, temporal patterns give at least % extra information. Thus, single central auditory neurons can provide an informative representation of naturalistic sounds, in which spike timing may play a signiﬁcant role.   </p><p>2 0.7875964 <a title="174-lsi-2" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>Author: Gal Chechik, Amir Globerson, M. J. Anderson, E. D. Young, Israel Nelken, Naftali Tishby</p><p>Abstract: The way groups of auditory neurons interact to code acoustic information is investigated using an information theoretic approach. We develop measures of redundancy among groups of neurons, and apply them to the study of collaborative coding efficiency in two processing stations in the auditory pathway: the inferior colliculus (IC) and the primary auditory cortex (AI). Under two schemes for the coding of the acoustic content, acoustic segments coding and stimulus identity coding, we show differences both in information content and group redundancies between IC and AI neurons. These results provide for the first time a direct evidence for redundancy reduction along the ascending auditory pathway, as has been hypothesized for theoretical considerations [Barlow 1959,2001]. The redundancy effects under the single-spikes coding scheme are significant only for groups larger than ten cells, and cannot be revealed with the redundancy measures that use only pairs of cells. The results suggest that the auditory system transforms low level representations that contain redundancies due to the statistical structure of natural stimuli, into a representation in which cortical neurons extract rare and independent component of complex acoustic signals, that are useful for auditory scene analysis. 1</p><p>3 0.61132896 <a title="174-lsi-3" href="./nips-2001-Characterizing_Neural_Gain_Control_using_Spike-triggered_Covariance.html">48 nips-2001-Characterizing Neural Gain Control using Spike-triggered Covariance</a></p>
<p>Author: Odelia Schwartz, E. J. Chichilnisky, Eero P. Simoncelli</p><p>Abstract: Spike-triggered averaging techniques are effective for linear characterization of neural responses. But neurons exhibit important nonlinear behaviors, such as gain control, that are not captured by such analyses. We describe a spike-triggered covariance method for retrieving suppressive components of the gain control signal in a neuron. We demonstrate the method in simulation and on retinal ganglion cell data. Analysis of physiological data reveals signiﬁcant suppressive axes and explains neural nonlinearities. This method should be applicable to other sensory areas and modalities. White noise analysis has emerged as a powerful technique for characterizing response properties of spiking neurons. A sequence of stimuli are drawn randomly from an ensemble and presented in rapid succession, and one examines the subset that elicit action potentials. This “spike-triggered” stimulus ensemble can provide information about the neuron’s response characteristics. In the most widely used form of this analysis, one estimates an excitatory linear kernel by computing the spike-triggered average (STA); that is, the mean stimulus that elicited a spike [e.g., 1, 2]. Under the assumption that spikes are generated by a Poisson process with instantaneous rate determined by linear projection onto a kernel followed by a static nonlinearity, the STA provides an unbiased estimate of this kernel [3]. Recently, a number of authors have developed interesting extensions of white noise analysis. Some have examined spike-triggered averages in a reduced linear subspace of input stimuli [e.g., 4]. Others have recovered excitatory subspaces, by computing the spiketriggered covariance (STC), followed by an eigenvector analysis to determine the subspace axes [e.g., 5, 6]. Sensory neurons exhibit striking nonlinear behaviors that are not explained by fundamentally linear mechanisms. For example, the response of a neuron typically saturates for large amplitude stimuli; the response to the optimal stimulus is often suppressed by the presence of a non-optimal mask [e.g., 7]; and the kernel recovered from STA analysis may change shape as a function of stimulus amplitude [e.g., 8, 9]. A variety of these nonlinear behaviors can be attributed to gain control [e.g., 8, 10, 11, 12, 13, 14], in which neural responses are suppressively modulated by a gain signal derived from the stimulus. Although the underlying mechanisms and time scales associated with such gain control are current topics of research, the basic functional properties appear to be ubiquitous, occurring throughout the nervous system. a b 0 k0 0 Figure 1: Geometric depiction of spike-triggered analyses. a, Spike-triggered averaging with two-dimensional stimuli. Black points indicate raw stimuli. White points indicate stimuli eliciting a spike, and the STA (black vector), which provides an estimate of , corresponds to their center of mass. b, Spike-triggered covariance analysis of suppressive axes. Shown are a set of stimuli lying on a plane perpendicular to the excitatory kernel, . Within the plane, stimuli eliciting a spike are concentrated in an elliptical region. The minor axis of the ellipse corresponds to a suppressive stimulus direction: stimuli with a signiﬁcant component along this axis are less likely to elicit spikes. The stimulus component along the major axis of the ellipse has no inﬂuence on spiking. ¢ £  ¡ ¢  ¡ Here we develop a white noise methodology for characterizing a neuron with gain control. We show that a set of suppressive kernels may be recovered by ﬁnding the eigenvectors of the spike-triggered covariance matrix associated with smallest variance. We apply the technique to electrophysiological data obtained from ganglion cells in salamander and macaque retina, and recover a set of axes that are shown to reduce responses in the neuron. Moreover, when we ﬁt a gain control model to the data using a maximum likelihood procedure within this subspace, the model accounts for changes in the STA as a function of contrast. 1 Characterizing suppressive axes ¤¥ As in all white noise approaches, we assume that stimuli correspond to vectors, , in some ﬁnite-dimensional space (e.g., a neighborhood of pixels or an interval of time samples). We assume a gain control model in which the probability of a stimulus eliciting a spike grows monotonically with the halfwave-rectiﬁed projection onto an excitatory linear kernel, , and is suppressively modulated by the fullwave-rectiﬁed projection onto a set of . linear kernels, ¨ ¤§  ¤¥    ©¤ §   ¨ ¤ ¥ ©¤ § ¦ First, we recover the excitatory kernel, . This is achieved by presenting spherically symmetric input stimuli (e.g., Gaussian white noise) to the neuron and computing the STA (Fig. 1a). STA correctly recovers the excitatory kernel, under the assumption that each of the gain control kernels are orthogonal (or equal) to the excitatory kernel. The proof is essentially the same as that given for recovering the kernel of a linear model followed by a monotonic nonlinearity [3]. In particular, any stimulus can be decomposed into a component in the direction of the excitatory kernel, and a component in a perpendicular direction. This can be paired with another stimulus that is identical, except that its component in the perpendicular direction is negated. The two stimuli are equally likely to occur in a spherically Gaussian stimulus set (since they are equidistant from the origin), and they are equally likely to elicit a spike (since their excitatory components are equal, and their rectiﬁed perpendicular components are equal). Their vector average lies in the direction of the excitatory kernel. Thus, the STA (which is an average over all such stimuli, or all such stimulus pairs) must also lie in that direction. In a subsequent section we explain how to Model: Retrieved: Excitatory: Excitatory: Eigenvalues: Suppressive: Suppressive: Weights Variance (eigenvalue) { 1.5 { 2{ 2.5 { 3{ 1 1 Arbitrary 0 Axis number 350 Figure 2: Estimation of kernels from a simulated model (equation 2). Left: Model kernels. Right: Sorted eigenvalues of covariance matrix of stimuli eliciting spikes (STC). Five eigenvalues fall signiﬁcantly below the others. Middle: STA (excitatory kernel) and eigenvectors (suppressive kernels) associated with the lowest eigenvalues. recover the excitatory kernel when it is not orthogonal to the suppressive kernels. Next, we recover the suppressive subspace, assuming the excitatory kernel is known. Consider the stimuli lying on a plane perpendicular to this kernel. These stimuli all elicit the same response in the excitatory kernel, but they may produce different amounts of suppression. Figure 1b illustrates the behavior in a three-dimensional stimulus space, in which one axis is assumed to be suppressive. The distribution of raw stimuli on the plane is spherically symmetric about the origin. But the distribution of stimuli eliciting a spike is narrower along the suppressive direction: these stimuli have a component along the suppressive axis and are therefore less likely to elicit a spike. This behavior is easily generalized from this plane to the entire stimulus space. If we assume that the suppressive axes are ﬁxed, then we expect to see reductions in variance in the same directions for any level of numerator excitation. Given this behavior of the spike-triggered stimulus ensemble, we can recover the suppressive subspace using principal component analysis. We construct the sample covariance matrix of the stimuli eliciting a spike: £ §¥</p><p>4 0.60956115 <a title="174-lsi-4" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>Author: Shih-Chii Liu, Jörg Kramer, Giacomo Indiveri, Tobi Delbrück, Rodney J. Douglas</p><p>Abstract: We describe a programmable multi-chip VLSI neuronal system that can be used for exploring spike-based information processing models. The system consists of a silicon retina, a PIC microcontroller, and a transceiver chip whose integrate-and-ﬁre neurons are connected in a soft winner-take-all architecture. The circuit on this multi-neuron chip approximates a cortical microcircuit. The neurons can be conﬁgured for different computational properties by the virtual connections of a selected set of pixels on the silicon retina. The virtual wiring between the different chips is effected by an event-driven communication protocol that uses asynchronous digital pulses, similar to spikes in a neuronal system. We used the multi-chip spike-based system to synthesize orientation-tuned neurons using both a feedforward model and a feedback model. The performance of our analog hardware spiking model matched the experimental observations and digital simulations of continuous-valued neurons. The multi-chip VLSI system has advantages over computer neuronal models in that it is real-time, and the computational time does not scale with the size of the neuronal network.</p><p>5 0.56851292 <a title="174-lsi-5" href="./nips-2001-A_Maximum-Likelihood_Approach_to_Modeling_Multisensory_Enhancement.html">11 nips-2001-A Maximum-Likelihood Approach to Modeling Multisensory Enhancement</a></p>
<p>Author: H. Colonius, A. Diederich</p><p>Abstract: Multisensory response enhancement (MRE) is the augmentation of the response of a neuron to sensory input of one modality by simultaneous input from another modality. The maximum likelihood (ML) model presented here modifies the Bayesian model for MRE (Anastasio et al.) by incorporating a decision strategy to maximize the number of correct decisions. Thus the ML model can also deal with the important tasks of stimulus discrimination and identification in the presence of incongruent visual and auditory cues. It accounts for the inverse effectiveness observed in neurophysiological recording data, and it predicts a functional relation between uni- and bimodal levels of discriminability that is testable both in neurophysiological and behavioral experiments. 1</p><p>6 0.55724627 <a title="174-lsi-6" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>7 0.54537594 <a title="174-lsi-7" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>8 0.49975979 <a title="174-lsi-8" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>9 0.47440985 <a title="174-lsi-9" href="./nips-2001-A_Rational_Analysis_of_Cognitive_Control_in_a_Speeded_Discrimination_Task.html">18 nips-2001-A Rational Analysis of Cognitive Control in a Speeded Discrimination Task</a></p>
<p>10 0.46785522 <a title="174-lsi-10" href="./nips-2001-A_Neural_Oscillator_Model_of_Auditory_Selective_Attention.html">14 nips-2001-A Neural Oscillator Model of Auditory Selective Attention</a></p>
<p>11 0.4449707 <a title="174-lsi-11" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>12 0.43145841 <a title="174-lsi-12" href="./nips-2001-Citcuits_for_VLSI_Implementation_of_Temporally_Asymmetric_Hebbian_Learning.html">49 nips-2001-Citcuits for VLSI Implementation of Temporally Asymmetric Hebbian Learning</a></p>
<p>13 0.41374657 <a title="174-lsi-13" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>14 0.39987403 <a title="174-lsi-14" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>15 0.38144663 <a title="174-lsi-15" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>16 0.35219043 <a title="174-lsi-16" href="./nips-2001-Fragment_Completion_in_Humans_and_Machines.html">78 nips-2001-Fragment Completion in Humans and Machines</a></p>
<p>17 0.35188827 <a title="174-lsi-17" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>18 0.34472296 <a title="174-lsi-18" href="./nips-2001-Entropy_and_Inference%2C_Revisited.html">68 nips-2001-Entropy and Inference, Revisited</a></p>
<p>19 0.33259171 <a title="174-lsi-19" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>20 0.33072558 <a title="174-lsi-20" href="./nips-2001-3_state_neurons_for_contextual_processing.html">2 nips-2001-3 state neurons for contextual processing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.024), (17, 0.016), (19, 0.023), (27, 0.092), (30, 0.094), (38, 0.024), (59, 0.025), (67, 0.289), (72, 0.044), (79, 0.041), (83, 0.019), (91, 0.242)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87239861 <a title="174-lda-1" href="./nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</a></p>
<p>Author: Yu-Han Chang, Leslie Pack Kaelbling</p><p>Abstract: We propose a new classiﬁcation for multi-agent learning algorithms, with each league of players characterized by both their possible strategies and possible beliefs. Using this classiﬁcation, we review the optimality of existing algorithms, including the case of interleague play. We propose an incremental improvement to the existing algorithms that seems to achieve average payoffs that are at least the Nash equilibrium payoffs in the longrun against fair opponents.</p><p>same-paper 2 0.86433268 <a title="174-lda-2" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>Author: B. D. Wright, Kamal Sen, William Bialek, A. J. Doupe</p><p>Abstract: In nature, animals encounter high dimensional sensory stimuli that have complex statistical and dynamical structure. Attempts to study the neural coding of these natural signals face challenges both in the selection of the signal ensemble and in the analysis of the resulting neural responses. For zebra ﬁnches, naturalistic stimuli can be deﬁned as sounds that they encounter in a colony of conspeciﬁc birds. We assembled an ensemble of these sounds by recording groups of 10-40 zebra ﬁnches, and then analyzed the response of single neurons in the songbird central auditory area (ﬁeld L) to continuous playback of long segments from this ensemble. Following methods developed in the ﬂy visual system, we measured the information that spike trains provide about the acoustic stimulus without any assumptions about which features of the stimulus are relevant. Preliminary results indicate that large amounts of information are carried by spike timing, with roughly half of the information accessible only at time resolutions better than 10 ms; additional information is still being revealed as time resolution is improved to 2 ms. Information can be decomposed into that carried by the locking of individual spikes to the stimulus (or modulations of spike rate) vs. that carried by timing in spike patterns. Initial results show that in ﬁeld L, temporal patterns give at least % extra information. Thus, single central auditory neurons can provide an informative representation of naturalistic sounds, in which spike timing may play a signiﬁcant role.   </p><p>3 0.85047632 <a title="174-lda-3" href="./nips-2001-Learning_Hierarchical_Structures_with_Linear_Relational_Embedding.html">110 nips-2001-Learning Hierarchical Structures with Linear Relational Embedding</a></p>
<p>Author: Alberto Paccanaro, Geoffrey E. Hinton</p><p>Abstract: We present Linear Relational Embedding (LRE), a new method of learning a distributed representation of concepts from data consisting of instances of relations between given concepts. Its ﬁnal goal is to be able to generalize, i.e. infer new instances of these relations among the concepts. On a task involving family relationships we show that LRE can generalize better than any previously published method. We then show how LRE can be used effectively to ﬁnd compact distributed representations for variable-sized recursive data structures, such as trees and lists. 1 Linear Relational Embedding Our aim is to take a large set of facts about a domain expressed as tuples of arbitrary symbols in a simple and rigid syntactic format and to be able to infer other “common-sense” facts without having any prior knowledge about the domain. Let us imagine a situation in which we have a set of concepts and a set of relations among these concepts, and that our data consists of few instances of these relations that hold among the concepts. We want to be able to infer other instances of these relations. For example, if the concepts are the people in a certain family, the relations are kinship relations, and we are given the facts ”Alberto has-father Pietro” and ”Pietro has-brother Giovanni”, we would like to be able to infer ”Alberto has-uncle Giovanni”. Our approach is to learn appropriate distributed representations of the entities in the data, and then exploit the generalization properties of the distributed representations [2] to make the inferences. In this paper we present a method, which we have called Linear Relational Embedding (LRE), which learns a distributed representation for the concepts by embedding them in a space where the relations between concepts are linear transformations of their distributed representations. Let us consider the case in which all the relations are binary, i.e. involve two concepts. , and the problem In this case our data consists of triplets we are trying to solve is to infer missing triplets when we are given only few of them. Inferring a triplet is equivalent to being able to complete it, that is to come up with one of its elements, given the other two. Here we shall always try to complete the third element of the triplets 1 . LRE will then represent each concept in the data as a learned vector in a 2 0    £ § ¥ £  § ¥ % </p><p>4 0.68778741 <a title="174-lda-4" href="./nips-2001-Efficiency_versus_Convergence_of_Boolean_Kernels_for_On-Line_Learning_Algorithms.html">66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</a></p>
<p>Author: Roni Khardon, Dan Roth, Rocco A. Servedio</p><p>Abstract: We study online learning in Boolean domains using kernels which capture feature expansions equivalent to using conjunctions over basic features. We demonstrate a tradeoff between the computational efﬁciency with which these kernels can be computed and the generalization ability of the resulting classiﬁer. We ﬁrst describe several kernel functions which capture either limited forms of conjunctions or all conjunctions. We show that these kernels can be used to efﬁciently run the Perceptron algorithm over an exponential number of conjunctions; however we also prove that using such kernels the Perceptron algorithm can make an exponential number of mistakes even when learning simple functions. We also consider an analogous use of kernel functions to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions. While known upper bounds imply that Winnow can learn DNF formulae with a polynomial mistake bound in this setting, we prove that it is computationally hard to simulate Winnow’s behavior for learning DNF over such a feature set, and thus that such kernel functions for Winnow are not efﬁciently computable.</p><p>5 0.68703771 <a title="174-lda-5" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>Author: Heiko Wersing</p><p>Abstract: We present a new approach to the supervised learning of lateral interactions for the competitive layer model (CLM) dynamic feature binding architecture. The method is based on consistency conditions, which were recently shown to characterize the attractor states of this linear threshold recurrent network. For a given set of training examples the learning problem is formulated as a convex quadratic optimization problem in the lateral interaction weights. An efﬁcient dimension reduction of the learning problem can be achieved by using a linear superposition of basis interactions. We show the successful application of the method to a medical image segmentation problem of ﬂuorescence microscope cell images.</p><p>6 0.68239242 <a title="174-lda-6" href="./nips-2001-Partially_labeled_classification_with_Markov_random_walks.html">144 nips-2001-Partially labeled classification with Markov random walks</a></p>
<p>7 0.68102664 <a title="174-lda-7" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>8 0.67628819 <a title="174-lda-8" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<p>9 0.67565876 <a title="174-lda-9" href="./nips-2001-Predictive_Representations_of_State.html">148 nips-2001-Predictive Representations of State</a></p>
<p>10 0.67192107 <a title="174-lda-10" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>11 0.67071605 <a title="174-lda-11" href="./nips-2001-A_Rational_Analysis_of_Cognitive_Control_in_a_Speeded_Discrimination_Task.html">18 nips-2001-A Rational Analysis of Cognitive Control in a Speeded Discrimination Task</a></p>
<p>12 0.66828299 <a title="174-lda-12" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>13 0.66769987 <a title="174-lda-13" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>14 0.66514468 <a title="174-lda-14" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>15 0.66044164 <a title="174-lda-15" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>16 0.6582526 <a title="174-lda-16" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>17 0.65805739 <a title="174-lda-17" href="./nips-2001-The_g_Factor%3A_Relating_Distributions_on_Features_to_Distributions_on_Images.html">189 nips-2001-The g Factor: Relating Distributions on Features to Distributions on Images</a></p>
<p>18 0.65566707 <a title="174-lda-18" href="./nips-2001-Information-Geometric_Decomposition_in_Spike_Analysis.html">96 nips-2001-Information-Geometric Decomposition in Spike Analysis</a></p>
<p>19 0.65419954 <a title="174-lda-19" href="./nips-2001-Entropy_and_Inference%2C_Revisited.html">68 nips-2001-Entropy and Inference, Revisited</a></p>
<p>20 0.65392125 <a title="174-lda-20" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
