<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>184 nips-2001-The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-184" href="#">nips2001-184</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>184 nips-2001-The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank</h1>
<br/><p>Source: <a title="nips-2001-184-pdf" href="http://papers.nips.cc/paper/2047-the-intelligent-surfer-probabilistic-combination-of-link-and-content-information-in-pagerank.pdf">pdf</a></p><p>Author: Matthew Richardson, Pedro Domingos</p><p>Abstract: The PageRank algorithm, used in the Google search engine, greatly improves the results of Web search by taking into account the link structure of the Web. PageRank assigns to a page a score proportional to the number of times a random surfer would visit that page, if it surfed indefinitely from page to page, following all outlinks from a page with equal probability. We propose to improve PageRank by using a more intelligent surfer, one that is guided by a probabilistic model of the relevance of a page to a query. Efficient execution of our algorithm at query time is made possible by precomputing at crawl time (and thus once for all queries) the necessary terms. Experiments on two large subsets of the Web indicate that our algorithm significantly outperforms PageRank in the (human-rated) quality of the pages returned, while remaining efficient enough to be used in today’s large search engines. 1</p><p>Reference: <a title="nips-2001-184-reference" href="../nips2001_reference/nips-2001-The_Intelligent_surfer%3A_Probabilistic_Combination_of_Link_and_Content_Information_in_PageRank_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract The PageRank algorithm, used in the Google search engine, greatly improves the results of Web search by taking into account the link structure of the Web. [sent-3, score-0.207]
</p><p>2 PageRank assigns to a page a score proportional to the number of times a random surfer would visit that page, if it surfed indefinitely from page to page, following all outlinks from a page with equal probability. [sent-4, score-1.652]
</p><p>3 We propose to improve PageRank by using a more intelligent surfer, one that is guided by a probabilistic model of the relevance of a page to a query. [sent-5, score-0.44]
</p><p>4 Efficient execution of our algorithm at query time is made possible by precomputing at crawl time (and thus once for all queries) the necessary terms. [sent-6, score-0.445]
</p><p>5 Experiments on two large subsets of the Web indicate that our algorithm significantly outperforms PageRank in the (human-rated) quality of the pages returned, while remaining efficient enough to be used in today’s large search engines. [sent-7, score-0.211]
</p><p>6 1  Introduction  Traditional information retrieval techniques can give poor results on the Web, with its vast scale and highly variable content quality. [sent-8, score-0.13]
</p><p>7 Recently, however, it was found that Web search results can be much improved by using the information contained in the link structure between pages. [sent-9, score-0.131]
</p><p>8 The latter is used in the highly successful Google search engine [3]. [sent-11, score-0.174]
</p><p>9 The heuristic underlying both of these approaches is that pages with many inlinks are more likely to be of high quality than pages with few inlinks, given that the author of a page will presumably include in it links to pages that s/he believes are of high quality. [sent-12, score-0.855]
</p><p>10 Because this computation is carried out at query time, it is not feasible for today’s search engines, which need to handle tens of millions of queries per day. [sent-14, score-0.487]
</p><p>11 In contrast, PageRank computes a single measure of quality for a page at crawl time. [sent-15, score-0.471]
</p><p>12 This meas-  ure is then combined with a traditional information retrieval score at query time. [sent-16, score-0.418]
</p><p>13 Compared with HITS, this has the advantage of much greater efficiency, but the disadvantage that the PageRank score of a page ignores whether or not the page is relevant to the query at hand. [sent-17, score-1.101]
</p><p>14 Traditional information retrieval measures like TFIDF [4] rate a document highly if the query terms occur frequently in it. [sent-18, score-0.447]
</p><p>15 PageRank rates a page highly if it is at the center of a large sub-web (i. [sent-19, score-0.381]
</p><p>16 , if many pages point to it, many other pages point to those, etc. [sent-21, score-0.22]
</p><p>17 Intuitively, however, the best pages should be those that are at the center of a large sub-web relevant to the query. [sent-23, score-0.142]
</p><p>18 If one issues a query containing the word jaguar, then pages containing the word jaguar that are also pointed to by many other pages containing jaguar are more likely to be good choices than pages that contain jaguar but have no inlinks from pages containing it. [sent-24, score-1.32]
</p><p>19 This paper proposes a search algorithm that formalizes this intuition while, like PageRank, doing most of its computations at crawl time. [sent-25, score-0.17]
</p><p>20 The PageRank score of a page can be viewed as the rate at which a surfer would visit that page, if it surfed the Web indefinitely, blindly jumping from page to page. [sent-26, score-1.171]
</p><p>21 Our algorithm does something closer to what a human surfer would do, jumping preferentially to pages containing the query terms. [sent-27, score-0.836]
</p><p>22 Because they give the same weight to all edges, the pages with the most inlinks in the network being considered (either at crawl or query time) tend to dominate, whether or not they are the most relevant to the query. [sent-29, score-0.634]
</p><p>23 Rafiei and Mendelzon' [8] algorithm, which biases PageRank towards pages containing a spes cific word, is a predecessor of our work. [sent-34, score-0.148]
</p><p>24 Haveliwala [9] proposes applying an optimized version of PageRank to the subset of pages containing the query terms, and suggests that users do this on their own machines. [sent-35, score-0.468]
</p><p>25 2  PageRank : The Random Surfer  Imagine a web surfer who jumps from web page to web page, choosing with uniform probability which link to follow at each step. [sent-39, score-1.26]
</p><p>26 In order to reduce the effect of deadends or endless cycles the surfer will occasionally jump to a random page with some small probability β, or when on a page with no out-links. [sent-40, score-1.077]
</p><p>27 To reformulate this in graph terms, consider the web as a directed graph, where nodes represent web pages, and edges between nodes represent links between web pages. [sent-41, score-0.647]
</p><p>28 Let W be the set of nodes, N=|W|, Fi be the set of pages page i links to, and B i be the set pages which link to page i. [sent-42, score-1.03]
</p><p>29 For pages which have no outlinks we add a link to all pages in the graph1. [sent-43, score-0.412]
</p><p>30 In this way, rank which is lost due to pages with no outlinks is redistributed uniformly to all pages. [sent-44, score-0.24]
</p><p>31 If averaged over a sufficient number of steps, the probability the surfer is on page j at some point in time is given by the formula:  P( j ) =  1  (1 − β ) P (i ) +β ∑ N i∈B j Fi  (1)  For each page s with no outlinks, we set Fs={all N nodes}, and for all other nodes augment B i with s. [sent-45, score-1.075]
</p><p>32 PageRank is equivalent to the primary eigenvector of the transition matrix Z:  1 Z = (1 − β )   + β M ,with  N  NxN   1  M ji =  Fi 0   if there is an edge from i to j  (2)  otherwise  One iteration of equation (1) is equivalent to computing xt+1=Zxt, where xjt=P(j) at iteration t. [sent-49, score-0.18]
</p><p>33 After convergence, we have xT+1=xT, or xT=ZxT, which means xT is an eigenvector of Z. [sent-50, score-0.08]
</p><p>34 3  Directed Surfer Model  We propose a more intelligent surfer, who probabilistically hops from page to page, depending on the content of the pages and the query terms the surfer is looking for. [sent-52, score-1.25]
</p><p>35 The resulting probability distribution over pages is:  Pq ( j ) = (1 − β ) Pq′ ( j ) + β  ∑ Pq (i ) Pq (i → j )  (3)  i∈B j  where Pq(i→j) is the probability that the surfer transitions to page j given that he is on page i and is searching for the query q. [sent-53, score-1.477]
</p><p>36 Pq’(j) specifies where the surfer chooses to jump when not following links. [sent-54, score-0.373]
</p><p>37 Pq(j) is the resulting probability distribution over pages and corresponds to the query-dependent PageRank score (QD-PageRankq(j) ≡ Pq(j)). [sent-55, score-0.155]
</p><p>38 As with PageRank, QD-PageRank is determined by iterative evaluation of equation 3 from some initial distribution, and is equivalent to the primary eigenvector of the transition matrix Zq, where Z q ji = (1 − β ) Pq′ ( j ) + β ∑ Pq (i → j ) . [sent-56, score-0.162]
</p><p>39 Similarly to PageRank, when a page’s outlinks all have zero relevance, or has no outlinks, we add links from that page to all other pages in the network. [sent-58, score-0.65]
</p><p>40 On such a page, the surfer thus chooses a new page to jump to according to the distribution Pq’ (j). [sent-59, score-0.725]
</p><p>41 When given a multiple-term query, Q={q 1,q 2,…}, the surfer selects a q according to some probability distribution, P(q) and uses that term to guide its behavior (according to equation 3) for a large number of steps1. [sent-60, score-0.409]
</p><p>42 The resulting distribution over visited web pages is QD-PageRankQ and is given by 1  However many steps are needed to reach convergence of equation 3. [sent-62, score-0.292]
</p><p>43 QD − PageRank Q ( j ) ≡ PQ ( j ) =  ∑ P( q) Pq ( j )  (5)  q∈Q  For standard PageRank, the PageRank vector is equivalent to the primary eigenvector of the matrix Z. [sent-63, score-0.143]
</p><p>44 The vector of single-term QD-PageRankq is again equivalent to the primary eigenvector of the matrix Zq. [sent-64, score-0.143]
</p><p>45 An interesting question that arises is whether the QD-PageRankQ vector is equivalent to the primary eigenvector of a matrix ZQ = ∑ P ( q) Z q (corresponding to the combination performed by equation 5). [sent-65, score-0.162]
</p><p>46 Instead, the primary eigenvector of ZQ corresponds to the QD-PageRank obtained by a random surfer who, at each step, selects a new query according to the distribution P(q). [sent-67, score-0.81]
</p><p>47 Let xq be the L2-normalized primary eigenvector for matrix Zq (note element j of xq is QD-PageRankq(j)), thus satisfying xi=Tixi. [sent-69, score-0.299]
</p><p>48 Since xq is the primary eigenvector for Zq, we have [10]: ∀q, r ∈ Q : Z q x q ≥ Z q x r . [sent-70, score-0.212]
</p><p>49 Then     1   1 1 κ κ Z ZQ x Q =  ∑ Z  x  = x ≈ κZ q x q = ∑ x q = xQ  q∈Q Q q  q∑ q  Q q∑  q r∑ r  Q q∑ Q q∈Q n ∈Q  ∈Q  ∈Q   ∈Q  and thus xQ is approximately an eigenvector for ZQ. [sent-74, score-0.099]
</p><p>50 (  )  The choice of relevance function Rq(j) is arbitrary. [sent-77, score-0.056]
</p><p>51 In the simplest case, Rq(j)=R is independent of the query term and the document, and QD-PageRank reduces to PageRank. [sent-78, score-0.345]
</p><p>52 One simple content-dependent function could be Rq(j)=1 if the term q appears on page j, and 0 otherwise. [sent-79, score-0.377]
</p><p>53 Much more complex functions could be used, such as the well-known TFIDF information retrieval metric, a score obtained by latent semantic indexing, or any heuristic measure using text size, positioning, etc…. [sent-80, score-0.094]
</p><p>54 It is important to note that most current text ranking functions could be easily incorporated into the directed surfer model. [sent-81, score-0.403]
</p><p>55 4  Scalability  The difficulty with calculating a query dependent PageRank is that a search engine cannot perform the computation, which can take hours, at query time, when it is expected to return results in seconds (or less). [sent-82, score-0.785]
</p><p>56 We surmount this problem by precomputing the individual term rankings QD-PageRankq, and combining them at query time according to equation 5. [sent-83, score-0.395]
</p><p>57 We show that the computation and storage requirements for QD-PageRankq for hundreds of thousands of words is only approximately 100-200 times that of a single query independent PageRank. [sent-84, score-0.475]
</p><p>58 Let W={q1, q2, …, qm} be the set of words in our lexicon. [sent-85, score-0.059]
</p><p>59 That is, we assume all search queries contain terms in W, or we are willing to use plain PageRank for those terms not in W. [sent-86, score-0.185]
</p><p>60 Let d q be the number of documents which contain the term q. [sent-87, score-0.084]
</p><p>61 1  Disk St o ra g e  For each term q, we must store the results of the computation. [sent-90, score-0.055]
</p><p>62 We add the minor restriction that a search query will only return documents containing all of the terms 1. [sent-91, score-0.503]
</p><p>63 Thus, when merging QD-PageRankq’s, we need only to know the QD-PageRankq for documents that contain the term. [sent-92, score-0.079]
</p><p>64 Thus, the space required to store all of the PageRanks is S, a factor of S/N times the query independent PageRank alone (recall N is the number of web pages). [sent-94, score-0.531]
</p><p>65 Further, note that the storage space is still considerably less than that required for the search engine’s reverse index, which must store information about all document-term pairs, as opposed to our need to store information about every unique document term pair. [sent-95, score-0.287]
</p><p>66 2  Time Requirement s  If Rq(j)=0 for some document j, the directed surfer will never arrive at that page. [sent-97, score-0.444]
</p><p>67 We add the reasonable constraint that Rq(j)=0 if term q does not appear in document j, which is common for most information retrieval relevance metrics, such as TFIDF. [sent-99, score-0.207]
</p><p>68 Because it is proportional to the number of documents in the graph, the computation of QD-PageRankq for all q in W will require O(S) time, a factor of S/N times the computation of the query independent PageRank alone. [sent-101, score-0.379]
</p><p>69 If every document contained vastly different words, S/N would be proportional to the number of search terms, m. [sent-107, score-0.144]
</p><p>70 Instead, there are a very few words that are found in almost every document, and many words which are found in very few documents2; in both cases the contribution to S is small. [sent-109, score-0.118]
</p><p>71 7 million pages (see section 5), we let W be the set of all unique words, and removed the 100 most common words3. [sent-111, score-0.165]
</p><p>72 3 million words, and the ratio S/N was found to be 165. [sent-113, score-0.055]
</p><p>73 We expect that this ratio will remain relatively constant even for much larger sets of web pages. [sent-114, score-0.163]
</p><p>74 This means QDPageRank requires approximately 165 times the storage space and 124 times the computation time to allow for arbitrary queries over any of the 2. [sent-115, score-0.183]
</p><p>75 3 million words (which is still less storage space than is required by the search engine’s reverse index alone). [sent-116, score-0.248]
</p><p>76 This is because the distribution of words in text tends to follow an inverse power law [11]. [sent-122, score-0.08]
</p><p>77 We also verified experimentally that the same holds true for the distribution of the number of documents a word is found in. [sent-123, score-0.095]
</p><p>78 3 It is common to remove “stop” words such as the, is, etc. [sent-124, score-0.059]
</p><p>79 The crawler was seeded with the first 18 results of a search for “ University” on Google (www. [sent-169, score-0.076]
</p><p>80 WebBase is the first 15 million pages of the Stanford WebBase repository [12], which contains over 120 million pages. [sent-177, score-0.241]
</p><p>81 We calculated QD-PageRank as described above, using Rq(j) = the fraction of words equal to q in page j, and P(q)=1/|Q|. [sent-179, score-0.411]
</p><p>82 For content ranking, we used the same Rq(j) function as for QDPageRank, but, similarly to TFIDF, weighted the contribution of each search term by the log of its inverse document frequency. [sent-181, score-0.24]
</p><p>83 As there is nothing published about merging PageRank and content rank into one list, the approach we follow is to normalize the two scores and add them. [sent-182, score-0.161]
</p><p>84 This implicitly assumes that PageRank and content rank are equally important. [sent-183, score-0.092]
</p><p>85 This resulted in poor PageRank performance, which we found was because the distribution of PageRanks is much more skewed than the distribution of content ranks; normalizing the vectors resulted in PageRank primarily determining the final ranking. [sent-184, score-0.071]
</p><p>86 For educrawl, we requested a single word and two double word search queries from each of three volunteers, resulting in a total of nine queries. [sent-187, score-0.275]
</p><p>87 This rating was obtained by first summing the ratings for the ten pages from each method for each volunteer, and then averaging the individual ratings. [sent-190, score-0.135]
</p><p>88 For WebBase, we randomly selected the queries from Bharat and Henzinger [6]. [sent-192, score-0.091]
</p><p>89 The four volunteers for the WebBase evaluation were independent from the four for the educrawl evaluation, and none knew how the pages they were asked to rate were obtained. [sent-193, score-0.291]
</p><p>90 QD-PageRank performs better than PageRank, accomplishing a relative improvement in relevance of 20% on educrawl and 34% on WebBase. [sent-194, score-0.165]
</p><p>91 One item to note is that the results on multiple word queries are not as positive as the results on single word queries. [sent-199, score-0.199]
</p><p>92 As discussed in section 3, the combination of single word QD-PageRanks to calculate the QD-PageRank for a multiple word query is only an approximation, made for practical reasons. [sent-200, score-0.428]
</p><p>93 This approximation is worse when the words are highly dependent. [sent-201, score-0.088]
</p><p>94 Further, some queries, such as “ financial aid” have a different intended meaning as a phrase than simply the two words “ financial” and “ aid” . [sent-202, score-0.089]
</p><p>95 For queries such as these, the words are highly dependent. [sent-203, score-0.179]
</p><p>96 6  Conclusions  In this paper, we introduced a model that probabilistically combines page content and link structure in the form of an intelligent random surfer. [sent-205, score-0.532]
</p><p>97 The model can accommodate essentially any query relevance function in use today, and produces higherquality results than PageRank, while having time and storage requirements that are within reason for today’ s large scale search engines. [sent-206, score-0.511]
</p><p>98 Ackno w ledg ment s We would like to thank Gary Wesley and Taher Haveliwala for their help with WebBase, Frank McSherry for eigen-help, and our experiment volunteers for their time. [sent-207, score-0.054]
</p><p>99 The anatomy of a large-scale hypertextual Web search engine. [sent-224, score-0.076]
</p><p>100 The missing link - a probabilistic model of document content and hypertext connectivity. [sent-250, score-0.194]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pagerank', 0.569), ('page', 0.352), ('surfer', 0.343), ('query', 0.32), ('pq', 0.196), ('web', 0.163), ('webbase', 0.14), ('zq', 0.125), ('rq', 0.116), ('pages', 0.11), ('educrawl', 0.109), ('outlinks', 0.109), ('crawl', 0.094), ('queries', 0.091), ('xq', 0.087), ('eigenvector', 0.08), ('inlinks', 0.078), ('search', 0.076), ('content', 0.071), ('engine', 0.069), ('document', 0.068), ('jaguar', 0.062), ('words', 0.059), ('relevance', 0.056), ('link', 0.055), ('million', 0.055), ('volunteers', 0.054), ('word', 0.054), ('links', 0.051), ('google', 0.049), ('bharat', 0.047), ('haveliwala', 0.047), ('henzinger', 0.047), ('hits', 0.046), ('score', 0.045), ('primary', 0.045), ('documents', 0.041), ('tfidf', 0.041), ('today', 0.039), ('containing', 0.038), ('storage', 0.037), ('xt', 0.037), ('stanford', 0.035), ('ninth', 0.035), ('directed', 0.033), ('aid', 0.033), ('relevant', 0.032), ('intelligent', 0.032), ('mendelzon', 0.031), ('pageranks', 0.031), ('precomputing', 0.031), ('qdpagerank', 0.031), ('rafiei', 0.031), ('surfed', 0.031), ('volunteer', 0.031), ('zxt', 0.031), ('store', 0.03), ('retrieval', 0.03), ('financial', 0.03), ('jump', 0.03), ('highly', 0.029), ('fi', 0.029), ('nodes', 0.028), ('add', 0.028), ('ranking', 0.027), ('scalability', 0.027), ('brin', 0.027), ('hyperlinked', 0.027), ('indefinitely', 0.027), ('chakrabarti', 0.027), ('kleinberg', 0.027), ('quality', 0.025), ('term', 0.025), ('rating', 0.025), ('disk', 0.025), ('jumping', 0.025), ('visit', 0.023), ('traditional', 0.023), ('requirements', 0.022), ('wide', 0.022), ('world', 0.022), ('selects', 0.022), ('probabilistically', 0.022), ('follow', 0.021), ('rank', 0.021), ('repository', 0.021), ('reverse', 0.021), ('pr', 0.02), ('merging', 0.02), ('approximately', 0.019), ('equation', 0.019), ('hofmann', 0.019), ('seventh', 0.019), ('heuristic', 0.019), ('equivalent', 0.018), ('graph', 0.018), ('contain', 0.018), ('pointed', 0.018), ('asked', 0.018), ('times', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="184-tfidf-1" href="./nips-2001-The_Intelligent_surfer%3A_Probabilistic_Combination_of_Link_and_Content_Information_in_PageRank.html">184 nips-2001-The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank</a></p>
<p>Author: Matthew Richardson, Pedro Domingos</p><p>Abstract: The PageRank algorithm, used in the Google search engine, greatly improves the results of Web search by taking into account the link structure of the Web. PageRank assigns to a page a score proportional to the number of times a random surfer would visit that page, if it surfed indefinitely from page to page, following all outlinks from a page with equal probability. We propose to improve PageRank by using a more intelligent surfer, one that is guided by a probabilistic model of the relevance of a page to a query. Efficient execution of our algorithm at query time is made possible by precomputing at crawl time (and thus once for all queries) the necessary terms. Experiments on two large subsets of the Web indicate that our algorithm significantly outperforms PageRank in the (human-rated) quality of the pages returned, while remaining efficient enough to be used in today’s large search engines. 1</p><p>2 0.21734412 <a title="184-tfidf-2" href="./nips-2001-Active_Information_Retrieval.html">24 nips-2001-Active Information Retrieval</a></p>
<p>Author: Tommi Jaakkola, Hava T. Siegelmann</p><p>Abstract: In classical large information retrieval systems, the system responds to a user initiated query with a list of results ranked by relevance. The users may further refine their query as needed. This process may result in a lengthy correspondence without conclusion. We propose an alternative active learning approach, where the system responds to the initial user's query by successively probing the user for distinctions at multiple levels of abstraction. The system's initiated queries are optimized for speedy recovery and the user is permitted to respond with multiple selections or may reject the query. The information is in each case unambiguously incorporated by the system and the subsequent queries are adjusted to minimize the need for further exchange. The system's initiated queries are subject to resource constraints pertaining to the amount of information that can be presented to the user per iteration. 1</p><p>3 0.10842127 <a title="184-tfidf-3" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>Author: Jon M. Kleinberg</p><p>Abstract: unkown-abstract</p><p>4 0.10406072 <a title="184-tfidf-4" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>Author: Carlotta Domeniconi, Dimitrios Gunopulos</p><p>Abstract: The nearest neighbor technique is a simple and appealing method to address classification problems. It relies on t he assumption of locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with a finite number of examples due to the curse of dimensionality. We propose a technique that computes a locally flexible metric by means of Support Vector Machines (SVMs). The maximum margin boundary found by the SVM is used to determine the most discriminant direction over the query's neighborhood. Such direction provides a local weighting scheme for input features. We present experimental evidence of classification performance improvement over the SVM algorithm alone and over a variety of adaptive learning schemes, by using both simulated and real data sets. 1</p><p>5 0.094113477 <a title="184-tfidf-5" href="./nips-2001-Fragment_Completion_in_Humans_and_Machines.html">78 nips-2001-Fragment Completion in Humans and Machines</a></p>
<p>Author: David Jacobs, Bas Rokers, Archisman Rudra, Zili Liu</p><p>Abstract: Partial information can trigger a complete memory. At the same time, human memory is not perfect. A cue can contain enough information to specify an item in memory, but fail to trigger that item. In the context of word memory, we present experiments that demonstrate some basic patterns in human memory errors. We use cues that consist of word fragments. We show that short and long cues are completed more accurately than medium length ones and study some of the factors that lead to this behavior. We then present a novel computational model that shows some of the ﬂexibility and patterns of errors that occur in human memory. This model iterates between bottom-up and top-down computations. These are tied together using a Markov model of words that allows memory to be accessed with a simple feature set, and enables a bottom-up process to compute a probability distribution of possible completions of word fragments, in a manner similar to models of visual perceptual completion.</p><p>6 0.060217731 <a title="184-tfidf-6" href="./nips-2001-On_the_Concentration_of_Spectral_Properties.html">136 nips-2001-On the Concentration of Spectral Properties</a></p>
<p>7 0.058179844 <a title="184-tfidf-7" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<p>8 0.047420643 <a title="184-tfidf-8" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>9 0.04215727 <a title="184-tfidf-9" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>10 0.040155914 <a title="184-tfidf-10" href="./nips-2001-Spectral_Relaxation_for_K-means_Clustering.html">171 nips-2001-Spectral Relaxation for K-means Clustering</a></p>
<p>11 0.03867656 <a title="184-tfidf-11" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>12 0.036338054 <a title="184-tfidf-12" href="./nips-2001-Active_Learning_in_the_Drug_Discovery_Process.html">25 nips-2001-Active Learning in the Drug Discovery Process</a></p>
<p>13 0.035627462 <a title="184-tfidf-13" href="./nips-2001-A_kernel_method_for_multi-labelled_classification.html">22 nips-2001-A kernel method for multi-labelled classification</a></p>
<p>14 0.035504993 <a title="184-tfidf-14" href="./nips-2001-Pranking_with_Ranking.html">147 nips-2001-Pranking with Ranking</a></p>
<p>15 0.035339918 <a title="184-tfidf-15" href="./nips-2001-Hyperbolic_Self-Organizing_Maps_for_Semantic_Navigation.html">90 nips-2001-Hyperbolic Self-Organizing Maps for Semantic Navigation</a></p>
<p>16 0.032380521 <a title="184-tfidf-16" href="./nips-2001-Grammatical_Bigrams.html">86 nips-2001-Grammatical Bigrams</a></p>
<p>17 0.031644709 <a title="184-tfidf-17" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>18 0.028950684 <a title="184-tfidf-18" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>19 0.028654793 <a title="184-tfidf-19" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>20 0.028625043 <a title="184-tfidf-20" href="./nips-2001-Learning_from_Infinite_Data_in_Finite_Time.html">114 nips-2001-Learning from Infinite Data in Finite Time</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.102), (1, 0.005), (2, 0.001), (3, -0.063), (4, 0.011), (5, -0.074), (6, -0.069), (7, 0.008), (8, -0.132), (9, -0.029), (10, 0.144), (11, -0.074), (12, -0.146), (13, 0.05), (14, 0.074), (15, 0.058), (16, 0.092), (17, 0.048), (18, -0.078), (19, 0.043), (20, 0.072), (21, 0.09), (22, 0.062), (23, 0.014), (24, -0.205), (25, 0.007), (26, -0.07), (27, -0.086), (28, 0.022), (29, 0.194), (30, 0.029), (31, 0.123), (32, 0.046), (33, 0.161), (34, -0.05), (35, -0.065), (36, 0.008), (37, -0.113), (38, -0.141), (39, -0.134), (40, 0.13), (41, -0.006), (42, -0.178), (43, -0.149), (44, 0.043), (45, 0.047), (46, -0.006), (47, 0.103), (48, 0.024), (49, -0.167)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97325695 <a title="184-lsi-1" href="./nips-2001-The_Intelligent_surfer%3A_Probabilistic_Combination_of_Link_and_Content_Information_in_PageRank.html">184 nips-2001-The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank</a></p>
<p>Author: Matthew Richardson, Pedro Domingos</p><p>Abstract: The PageRank algorithm, used in the Google search engine, greatly improves the results of Web search by taking into account the link structure of the Web. PageRank assigns to a page a score proportional to the number of times a random surfer would visit that page, if it surfed indefinitely from page to page, following all outlinks from a page with equal probability. We propose to improve PageRank by using a more intelligent surfer, one that is guided by a probabilistic model of the relevance of a page to a query. Efficient execution of our algorithm at query time is made possible by precomputing at crawl time (and thus once for all queries) the necessary terms. Experiments on two large subsets of the Web indicate that our algorithm significantly outperforms PageRank in the (human-rated) quality of the pages returned, while remaining efficient enough to be used in today’s large search engines. 1</p><p>2 0.60822856 <a title="184-lsi-2" href="./nips-2001-Active_Information_Retrieval.html">24 nips-2001-Active Information Retrieval</a></p>
<p>Author: Tommi Jaakkola, Hava T. Siegelmann</p><p>Abstract: In classical large information retrieval systems, the system responds to a user initiated query with a list of results ranked by relevance. The users may further refine their query as needed. This process may result in a lengthy correspondence without conclusion. We propose an alternative active learning approach, where the system responds to the initial user's query by successively probing the user for distinctions at multiple levels of abstraction. The system's initiated queries are optimized for speedy recovery and the user is permitted to respond with multiple selections or may reject the query. The information is in each case unambiguously incorporated by the system and the subsequent queries are adjusted to minimize the need for further exchange. The system's initiated queries are subject to resource constraints pertaining to the amount of information that can be presented to the user per iteration. 1</p><p>3 0.5657742 <a title="184-lsi-3" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>Author: Jon M. Kleinberg</p><p>Abstract: unkown-abstract</p><p>4 0.44991854 <a title="184-lsi-4" href="./nips-2001-Fragment_Completion_in_Humans_and_Machines.html">78 nips-2001-Fragment Completion in Humans and Machines</a></p>
<p>Author: David Jacobs, Bas Rokers, Archisman Rudra, Zili Liu</p><p>Abstract: Partial information can trigger a complete memory. At the same time, human memory is not perfect. A cue can contain enough information to specify an item in memory, but fail to trigger that item. In the context of word memory, we present experiments that demonstrate some basic patterns in human memory errors. We use cues that consist of word fragments. We show that short and long cues are completed more accurately than medium length ones and study some of the factors that lead to this behavior. We then present a novel computational model that shows some of the ﬂexibility and patterns of errors that occur in human memory. This model iterates between bottom-up and top-down computations. These are tied together using a Markov model of words that allows memory to be accessed with a simple feature set, and enables a bottom-up process to compute a probability distribution of possible completions of word fragments, in a manner similar to models of visual perceptual completion.</p><p>5 0.33409318 <a title="184-lsi-5" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>Author: Carlotta Domeniconi, Dimitrios Gunopulos</p><p>Abstract: The nearest neighbor technique is a simple and appealing method to address classification problems. It relies on t he assumption of locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with a finite number of examples due to the curse of dimensionality. We propose a technique that computes a locally flexible metric by means of Support Vector Machines (SVMs). The maximum margin boundary found by the SVM is used to determine the most discriminant direction over the query's neighborhood. Such direction provides a local weighting scheme for input features. We present experimental evidence of classification performance improvement over the SVM algorithm alone and over a variety of adaptive learning schemes, by using both simulated and real data sets. 1</p><p>6 0.31611672 <a title="184-lsi-6" href="./nips-2001-Incremental_A%2A.html">93 nips-2001-Incremental A*</a></p>
<p>7 0.29401433 <a title="184-lsi-7" href="./nips-2001-Hyperbolic_Self-Organizing_Maps_for_Semantic_Navigation.html">90 nips-2001-Hyperbolic Self-Organizing Maps for Semantic Navigation</a></p>
<p>8 0.26857266 <a title="184-lsi-8" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<p>9 0.25806239 <a title="184-lsi-9" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>10 0.19202133 <a title="184-lsi-10" href="./nips-2001-EM-DD%3A_An_Improved_Multiple-Instance_Learning_Technique.html">64 nips-2001-EM-DD: An Improved Multiple-Instance Learning Technique</a></p>
<p>11 0.17421566 <a title="184-lsi-11" href="./nips-2001-Grammatical_Bigrams.html">86 nips-2001-Grammatical Bigrams</a></p>
<p>12 0.17193334 <a title="184-lsi-12" href="./nips-2001-Pranking_with_Ranking.html">147 nips-2001-Pranking with Ranking</a></p>
<p>13 0.16935532 <a title="184-lsi-13" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>14 0.16883038 <a title="184-lsi-14" href="./nips-2001-Optimising_Synchronisation_Times_for_Mobile_Devices.html">140 nips-2001-Optimising Synchronisation Times for Mobile Devices</a></p>
<p>15 0.16230981 <a title="184-lsi-15" href="./nips-2001-Matching_Free_Trees_with_Replicator_Equations.html">118 nips-2001-Matching Free Trees with Replicator Equations</a></p>
<p>16 0.15430117 <a title="184-lsi-16" href="./nips-2001-Analog_Soft-Pattern-Matching_Classifier_using_Floating-Gate_MOS_Technology.html">34 nips-2001-Analog Soft-Pattern-Matching Classifier using Floating-Gate MOS Technology</a></p>
<p>17 0.15353942 <a title="184-lsi-17" href="./nips-2001-Spectral_Relaxation_for_K-means_Clustering.html">171 nips-2001-Spectral Relaxation for K-means Clustering</a></p>
<p>18 0.15074226 <a title="184-lsi-18" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>19 0.14779231 <a title="184-lsi-19" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<p>20 0.14743911 <a title="184-lsi-20" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(12, 0.01), (14, 0.037), (17, 0.051), (19, 0.033), (27, 0.075), (30, 0.056), (38, 0.016), (54, 0.013), (59, 0.021), (65, 0.292), (72, 0.045), (79, 0.093), (83, 0.018), (91, 0.131)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82553542 <a title="184-lda-1" href="./nips-2001-The_Intelligent_surfer%3A_Probabilistic_Combination_of_Link_and_Content_Information_in_PageRank.html">184 nips-2001-The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank</a></p>
<p>Author: Matthew Richardson, Pedro Domingos</p><p>Abstract: The PageRank algorithm, used in the Google search engine, greatly improves the results of Web search by taking into account the link structure of the Web. PageRank assigns to a page a score proportional to the number of times a random surfer would visit that page, if it surfed indefinitely from page to page, following all outlinks from a page with equal probability. We propose to improve PageRank by using a more intelligent surfer, one that is guided by a probabilistic model of the relevance of a page to a query. Efficient execution of our algorithm at query time is made possible by precomputing at crawl time (and thus once for all queries) the necessary terms. Experiments on two large subsets of the Web indicate that our algorithm significantly outperforms PageRank in the (human-rated) quality of the pages returned, while remaining efficient enough to be used in today’s large search engines. 1</p><p>2 0.53824645 <a title="184-lda-2" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>Author: Matthew J. Beal, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We show that it is possible to extend hidden Markov models to have a countably inﬁnite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the inﬁnitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters deﬁne a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a ﬁnite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be inﬁnite— consider, for example, symbols being possible words appearing in English text.</p><p>3 0.53388119 <a title="184-lda-3" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>Author: Jon M. Kleinberg</p><p>Abstract: unkown-abstract</p><p>4 0.5331884 <a title="184-lda-4" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>5 0.5328275 <a title="184-lda-5" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>Author: Jens Kohlmorgen, Steven Lemm</p><p>Abstract: We propose a novel method for the analysis of sequential data that exhibits an inherent mode switching. In particular, the data might be a non-stationary time series from a dynamical system that switches between multiple operating modes. Unlike other approaches, our method processes the data incrementally and without any training of internal parameters. We use an HMM with a dynamically changing number of states and an on-line variant of the Viterbi algorithm that performs an unsupervised segmentation and classification of the data on-the-fly, i.e. the method is able to process incoming data in real-time. The main idea of the approach is to track and segment changes of the probability density of the data in a sliding window on the incoming data stream. The usefulness of the algorithm is demonstrated by an application to a switching dynamical system. 1</p><p>6 0.53171688 <a title="184-lda-6" href="./nips-2001-Fragment_Completion_in_Humans_and_Machines.html">78 nips-2001-Fragment Completion in Humans and Machines</a></p>
<p>7 0.52498209 <a title="184-lda-7" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>8 0.52492833 <a title="184-lda-8" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>9 0.52313364 <a title="184-lda-9" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>10 0.52273166 <a title="184-lda-10" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<p>11 0.52244455 <a title="184-lda-11" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>12 0.52112746 <a title="184-lda-12" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>13 0.5209682 <a title="184-lda-13" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>14 0.52091038 <a title="184-lda-14" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>15 0.51800138 <a title="184-lda-15" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>16 0.5156979 <a title="184-lda-16" href="./nips-2001-Using_Vocabulary_Knowledge_in_Bayesian_Multinomial_Estimation.html">194 nips-2001-Using Vocabulary Knowledge in Bayesian Multinomial Estimation</a></p>
<p>17 0.51529354 <a title="184-lda-17" href="./nips-2001-The_Concave-Convex_Procedure_%28CCCP%29.html">180 nips-2001-The Concave-Convex Procedure (CCCP)</a></p>
<p>18 0.51469576 <a title="184-lda-18" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>19 0.51459134 <a title="184-lda-19" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>20 0.51412314 <a title="184-lda-20" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
