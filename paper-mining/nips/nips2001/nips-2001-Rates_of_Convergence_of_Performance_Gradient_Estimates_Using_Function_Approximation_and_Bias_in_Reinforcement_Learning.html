<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-157" href="#">nips2001-157</a> knowledge-graph by maker-knowledge-mining</p><h1>157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2001-157-pdf" href="http://papers.nips.cc/paper/2053-rates-of-convergence-of-performance-gradient-estimates-using-function-approximation-and-bias-in-reinforcement-learning.pdf">pdf</a></p><p>Author: Gregory Z. Grudic, Lyle H. Ungar</p><p>Abstract: We address two open theoretical questions in Policy Gradient Reinforcement Learning. The ﬁrst concerns the efﬁcacy of using function approximation to represent the state action value function, . Theory is presented showing that linear function approximation representations of can degrade the rate of convergence of performance gradient estimates by a factor of relative to when no function approximation of is used, where is the number of possible actions and is the number of basis functions in the function approximation representation. The second concerns the use of a bias term in estimating the state action value function. Theory is presented showing that a non-zero bias term can improve the rate of convergence of performance gradient estimates by , where is the number of possible actions. Experimental evidence is presented showing that these theoretical results lead to signiﬁcant improvement in the convergence properties of Policy Gradient Reinforcement Learning algorithms.       ¤ ¨ ¦ ¢ ©§¥¤£¡ ¦ ¤ ¨ £¡ ¨ ¤¢  ¢</p><p>Reference: <a title="nips-2001-157-reference" href="../nips2001_reference/nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pgrl', 0.653), ('policy', 0.297), ('ag', 0.276), ('ab', 0.202), ('grady', 0.199), ('converg', 0.164), ('pif', 0.155), ('episod', 0.145), ('fa', 0.14), ('reinforc', 0.132), ('pg', 0.123), ('theorem', 0.122), ('reward', 0.119), ('wu', 0.109), ('execut', 0.108), ('grud', 0.093), ('act', 0.091), ('yxwu', 0.081), ('ung', 0.081), ('bia', 0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="157-tfidf-1" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>2 0.33945101 <a title="157-tfidf-2" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>3 0.26690173 <a title="157-tfidf-3" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>4 0.24954291 <a title="157-tfidf-4" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>5 0.23349494 <a title="157-tfidf-5" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>6 0.18806799 <a title="157-tfidf-6" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>7 0.18518803 <a title="157-tfidf-7" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>8 0.17659974 <a title="157-tfidf-8" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>9 0.16243424 <a title="157-tfidf-9" href="./nips-2001-Stabilizing_Value_Function_Approximation_with_the_BFBP_Algorithm.html">175 nips-2001-Stabilizing Value Function Approximation with the BFBP Algorithm</a></p>
<p>10 0.14173371 <a title="157-tfidf-10" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>11 0.13704722 <a title="157-tfidf-11" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>12 0.13297768 <a title="157-tfidf-12" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>13 0.1066661 <a title="157-tfidf-13" href="./nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</a></p>
<p>14 0.10054266 <a title="157-tfidf-14" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>15 0.094511278 <a title="157-tfidf-15" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>16 0.090988562 <a title="157-tfidf-16" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>17 0.083643667 <a title="157-tfidf-17" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>18 0.080907024 <a title="157-tfidf-18" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>19 0.080386184 <a title="157-tfidf-19" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>20 0.080268517 <a title="157-tfidf-20" href="./nips-2001-Generalizable_Relational_Binding_from_Coarse-coded_Distributed_Representations.html">80 nips-2001-Generalizable Relational Binding from Coarse-coded Distributed Representations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.205), (1, 0.036), (2, 0.458), (3, 0.06), (4, -0.093), (5, -0.057), (6, -0.014), (7, 0.002), (8, -0.002), (9, -0.006), (10, 0.01), (11, 0.009), (12, 0.019), (13, -0.02), (14, 0.024), (15, -0.015), (16, 0.017), (17, -0.022), (18, 0.064), (19, 0.048), (20, 0.01), (21, -0.038), (22, 0.015), (23, 0.053), (24, -0.057), (25, 0.044), (26, -0.025), (27, -0.004), (28, -0.042), (29, 0.06), (30, 0.068), (31, -0.027), (32, -0.03), (33, -0.039), (34, -0.062), (35, 0.072), (36, -0.004), (37, -0.095), (38, -0.046), (39, -0.001), (40, 0.037), (41, 0.045), (42, 0.02), (43, 0.089), (44, -0.026), (45, -0.039), (46, -0.029), (47, 0.114), (48, -0.095), (49, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92721957 <a title="157-lsi-1" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>2 0.81547123 <a title="157-lsi-2" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>3 0.80007029 <a title="157-lsi-3" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>4 0.77688223 <a title="157-lsi-4" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>5 0.77672875 <a title="157-lsi-5" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>6 0.77608228 <a title="157-lsi-6" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>7 0.71164244 <a title="157-lsi-7" href="./nips-2001-Stabilizing_Value_Function_Approximation_with_the_BFBP_Algorithm.html">175 nips-2001-Stabilizing Value Function Approximation with the BFBP Algorithm</a></p>
<p>8 0.69434154 <a title="157-lsi-8" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>9 0.65440542 <a title="157-lsi-9" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>10 0.63784057 <a title="157-lsi-10" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>11 0.62947726 <a title="157-lsi-11" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>12 0.61945283 <a title="157-lsi-12" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>13 0.57956362 <a title="157-lsi-13" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>14 0.52717334 <a title="157-lsi-14" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>15 0.4963834 <a title="157-lsi-15" href="./nips-2001-Predictive_Representations_of_State.html">148 nips-2001-Predictive Representations of State</a></p>
<p>16 0.490408 <a title="157-lsi-16" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>17 0.46474814 <a title="157-lsi-17" href="./nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</a></p>
<p>18 0.4076398 <a title="157-lsi-18" href="./nips-2001-Switch_Packet_Arbitration_via_Queue-Learning.html">177 nips-2001-Switch Packet Arbitration via Queue-Learning</a></p>
<p>19 0.36442935 <a title="157-lsi-19" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>20 0.33134636 <a title="157-lsi-20" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.099), (16, 0.196), (31, 0.027), (48, 0.176), (50, 0.177), (63, 0.047), (81, 0.011), (91, 0.064), (92, 0.083)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85759163 <a title="157-lda-1" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>2 0.85031569 <a title="157-lda-2" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>3 0.82790422 <a title="157-lda-3" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>4 0.82613176 <a title="157-lda-4" href="./nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</a></p>
<p>5 0.82353389 <a title="157-lda-5" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>6 0.8206991 <a title="157-lda-6" href="./nips-2001-An_Efficient%2C_Exact_Algorithm_for_Solving_Tree-Structured_Graphical_Games.html">32 nips-2001-An Efficient, Exact Algorithm for Solving Tree-Structured Graphical Games</a></p>
<p>7 0.81719005 <a title="157-lda-7" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>8 0.81001157 <a title="157-lda-8" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>9 0.80983049 <a title="157-lda-9" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>10 0.80823934 <a title="157-lda-10" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>11 0.8060379 <a title="157-lda-11" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>12 0.80505621 <a title="157-lda-12" href="./nips-2001-Stabilizing_Value_Function_Approximation_with_the_BFBP_Algorithm.html">175 nips-2001-Stabilizing Value Function Approximation with the BFBP Algorithm</a></p>
<p>13 0.79976994 <a title="157-lda-13" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>14 0.79715347 <a title="157-lda-14" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>15 0.79107487 <a title="157-lda-15" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>16 0.78947222 <a title="157-lda-16" href="./nips-2001-Predictive_Representations_of_State.html">148 nips-2001-Predictive Representations of State</a></p>
<p>17 0.78536397 <a title="157-lda-17" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>18 0.77915359 <a title="157-lda-18" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>19 0.77845526 <a title="157-lda-19" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>20 0.77841067 <a title="157-lda-20" href="./nips-2001-Optimising_Synchronisation_Times_for_Mobile_Devices.html">140 nips-2001-Optimising Synchronisation Times for Mobile Devices</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
