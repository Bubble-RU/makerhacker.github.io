<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-157" href="#">nips2001-157</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2001-157-pdf" href="http://papers.nips.cc/paper/2053-rates-of-convergence-of-performance-gradient-estimates-using-function-approximation-and-bias-in-reinforcement-learning.pdf">pdf</a></p><p>Author: Gregory Z. Grudic, Lyle H. Ungar</p><p>Abstract: We address two open theoretical questions in Policy Gradient Reinforcement Learning. The ﬁrst concerns the efﬁcacy of using function approximation to represent the state action value function, . Theory is presented showing that linear function approximation representations of can degrade the rate of convergence of performance gradient estimates by a factor of relative to when no function approximation of is used, where is the number of possible actions and is the number of basis functions in the function approximation representation. The second concerns the use of a bias term in estimating the state action value function. Theory is presented showing that a non-zero bias term can improve the rate of convergence of performance gradient estimates by , where is the number of possible actions. Experimental evidence is presented showing that these theoretical results lead to signiﬁcant improvement in the convergence properties of Policy Gradient Reinforcement Learning algorithms.       ¤ ¨ ¦ ¢ ©§¥¤£¡ ¦ ¤ ¨ £¡ ¨ ¤¢  ¢</p><p>Reference: <a title="nips-2001-157-reference" href="../nips2001_reference/nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We address two open theoretical questions in Policy Gradient Reinforcement Learning. [sent-7, score-0.143]
</p><p>2 The ﬁrst concerns the efﬁcacy of using function approximation to represent the state action value function, . [sent-8, score-0.442]
</p><p>3 The second concerns the use of a bias term in estimating the state action value function. [sent-10, score-0.537]
</p><p>4 Theory is presented showing that a non-zero bias term can improve the rate of convergence of performance gradient estimates by , where is the number of possible actions. [sent-11, score-0.765]
</p><p>5 Experimental evidence is presented showing that these theoretical results lead to signiﬁcant improvement in the convergence properties of Policy Gradient Reinforcement Learning algorithms. [sent-12, score-0.35]
</p><p>6 ¤ ¨ ¦ ¢ ©§¥¤£¡  ¦  ¤  ¨ £¡ ¨ ¤¢  ¢  1 Introduction Policy Gradient Reinforcement Learning (PGRL) algorithms have recently received attention because of their potential usefulness in addressing large continuous reinforcement Learning (RL) problems. [sent-13, score-0.17]
</p><p>7 However, there is still no widespread agreement on how PGRL algorithms should be implemented. [sent-14, score-0.064]
</p><p>8 In PGRL, the agent’s policy is characterized by a set of parameters which in turn implies a parameterization of the agent’s performance metric. [sent-15, score-0.442]
</p><p>9 Thus if represents a dimensional parameterization of the agent’s policy and is a performance metric the agent is meant to maximize, then the performance metric must have the form [6]. [sent-16, score-0.783]
</p><p>10 PGRL algorithms work by ﬁrst estimating the performance gradient (PG) and then using this gradient to update the agent’s policy using:  )  (  (1)  ! [sent-17, score-0.786]
</p><p>11 If the estimate of is accurate, then the agent can climb the performance gradient in the parameter space, toward locally optimal policies. [sent-27, score-0.667]
</p><p>12 In practice, is estimated using samples of the state action value function . [sent-28, score-0.431]
</p><p>13 The PGRL formulation is attractive because 1) the parameterization of the policy can directly imply     F  ! [sent-29, score-0.353]
</p><p>14 3) 0PI03  a generalization over the agent’s state space (e. [sent-30, score-0.146]
</p><p>15 algorithms exist which are guaranteed to give unbiased estimates of  ! [sent-33, score-0.157]
</p><p>16 3) 054H3  This paper addresses two open theoretical questions in PGRL formulations. [sent-37, score-0.143]
</p><p>17 In PGRL formulations performance gradient estimates typically have the following form:  ! [sent-38, score-0.393]
</p><p>18  5©¢  ¨ I5©¢ § ¦ ¥£¡A H3 G  (2)  9 %¨ 3 3 %¨  where is the estimate of the value of executing action in state (i. [sent-42, score-0.478]
</p><p>19 the the bias subtracted from in state , is the state action value function), number of steps the agent takes before estimating , and the form of the function depends on the PGRL algorithm being used (see Section 2, equation (3) for the form being considered here). [sent-44, score-0.965]
</p><p>20 The effectiveness of PGRL algorithms strongly depends on how is obtained and the form of . [sent-45, score-0.064]
</p><p>21 ¨ 35843%©¢ ¦    ¨ ¨ ¢     ¨ 72¨5 3¢  The ﬁrst open theoretical question addressed here is concerned with the use of function approximation (FA) to represent the state action value function , which is in turn used to estimate the performance gradient. [sent-47, score-0.69]
</p><p>22 The original formulation of PGRL [6], the REINFORCE algorithm, has been largely ignored because of the slow rate of convergence of the PG estimate. [sent-48, score-0.315]
</p><p>23 The use of FA techniques to represent based on its observations has been suggested as a way of improving convergence properties. [sent-49, score-0.254]
</p><p>24 It has been proven that speciﬁc linear FA formulations can be incorporated into PGRL algorithms, while still guaranteeing convergence to locally optimal solutions [5, 4]. [sent-50, score-0.3]
</p><p>25 However, whether linear FA representations actually improves the convergence properties of PGRL is an open question. [sent-51, score-0.327]
</p><p>26 We present theory showing that using linear basis function representations of , rather than direct observations of it, can slow the rate of convergence of PG estimates by a factor of (see Theorem 1 in Section 3. [sent-52, score-0.631]
</p><p>27 This result suggests that PGRL formulations should avoid the use of linear FA techniques to represent . [sent-54, score-0.059]
</p><p>28 In Section 4, experimental evidence is presented supporting this conjecture. [sent-55, score-0.068]
</p><p>29 ¨ ¦ ¢ ©§¥¤£¡     ¨ ¢ 2¨5  The second open theoretical question addressed here is can a non-zero bias term in (2) improve the convergence properties of PG estimates? [sent-56, score-0.494]
</p><p>30 There has been speculation that an appropriate choice of can improve convergence properties [6, 5], but theoretical support has been lacking. [sent-57, score-0.336]
</p><p>31 This paper presents theory showing that if , where is the number actions, then the rate of convergence of the PG estimate is improved by (see Theorem 2 in Section 3. [sent-58, score-0.349]
</p><p>32 This suggests that the convergence properties of PGRL algorithms can be improved by using a bias term that is the average of values in each state. [sent-60, score-0.402]
</p><p>33 At each time step the agent chooses from a ﬁnite set of actions and receives a reward . [sent-64, score-0.564]
</p><p>34 The dynamics of the environment are characterized by transition probabilities and expected rewards , . [sent-65, score-0.061]
</p><p>35 The policy followed by the agent is characterized by a parameter vector , and is deﬁned by the probability distribution , . [sent-66, score-0.623]
</p><p>36 [5] and limit our analysis to the start and state action value state discount reward formulation. [sent-77, score-0.606]
</p><p>37 Then the exact expression for the performance gradient is:  where  and  (3)  . [sent-83, score-0.241]
</p><p>38 This policy gradient formulation requires that the state-action value function, , under the current policy be estimated. [sent-84, score-0.794]
</p><p>39 We assume that has the following form: where has zero mean and ﬁnite variance . [sent-86, score-0.069]
</p><p>40 Therefore, if timate of obtained by averaging observations of and variance are given by: In addition, we assume that with the MDP assumption. [sent-87, score-0.16]
</p><p>41 1 Rate of Convergence of PIFA Algorithms Consider the PIFA algorithm [5] which uses a basis function representation for estimated state action value function, , of the following form:  U  ¨  9 ¨#¢ @ ¡ B   2! [sent-90, score-0.429]
</p><p>42 0  ) a ¨ 3 8C2¨¢ d 31¡    2 ¨ ©¢ 9Q@ B ¨    89 ¥ ¨ @ @ ¨2¢ 9 AB 9 AB A ¨©¢ A¡@ B ¡A ¨ 5G¨©¢ ¡ ¦   $ ¨   3    ¡ ¦   $ "   ¨ ©¨¢ 9 AB @ 9@ B     Q9@ B  are weights and are basis functions deﬁned in . [sent-93, score-0.054]
</p><p>43 ¡  Theorem 1: Let be an estimate of (8) obtained using the PIFA algorithm and the basis function representation (7). [sent-95, score-0.105]
</p><p>44 Then, given the assumptions deﬁned in Section 2 and equations (5) and (6), the rate of convergence of a PIFA algorithm is bounded below and above by:  B X WU " ¤ 03 " B s £ ! [sent-96, score-0.292]
</p><p>45 V ) 0  3 C §¤ t WU y ¦ y ¢       ¦ §¤  (9)  B  where is the number of basis functions, is the number of possible actions, and the number of independent estimates of the performance gradient. [sent-97, score-0.208]
</p><p>46 2 Rate of Convergence of Direct Sampling Algorithms  ¨ 3 8C#¢  ¨ ¡   ¥¨5G©¨¢ d 3  321¡    In the previous section, the observed are used to build a linear basis function representation of the state action value function, , which is in turn used to estimate the performance gradient. [sent-100, score-0.573]
</p><p>47 In this section we establish rate of convergence bounds for performance gradient estimates that directly use the observed without the intermediate step of building the FA representation. [sent-101, score-0.66]
</p><p>48 These bounds are established for the conditions and in (3). [sent-102, score-0.043]
</p><p>49 ¨ 58C2¨¢ 3   d 2 1¡     ¨ ¨ IG©¢     A ¨ ©¨5 ¢  ¡   y   ¡ G   ¡  B @ 5¢ G A 2¨5 ¨ ¤ ¨ ¢  Theorem 2: Let be a estimate of (3), be obtained using direct samples of . [sent-103, score-0.161]
</p><p>50 Then, , and given the assumptions deﬁned in Section 2 and equations (5) and (6), the if is bounded by:    (10)  B  where is the number of independent estimates of the performance gradient. [sent-104, score-0.192]
</p><p>51   © s V " £ H3 "  ¤ '  B t WU y  '   X WU y 2 03  C ) ¢ 2 ¡     then the rate of convergence of the performance gradient   §2¨5 ¦A ¨ ¢  B YXWU " ¥03 " B s £ ! [sent-106, score-0.495]
</p><p>52 V ) y HG3 ¢ C  t WU  A ¨ ¢ 2¨5  rate of convergence of  is bounded by: (12)  is the number of possible actions. [sent-107, score-0.292]
</p><p>53 Thus comparing (12) and (10) to (9) one can see that policy gradient algorithms such as PIFA which build FA representations of converge by a factor of slower than algorithms which directly sample . [sent-109, score-0.66]
</p><p>54 Furthermore, if the bias term is as deﬁned in (11), the bounds on the variance are further reduced by . [sent-110, score-0.206]
</p><p>55 In the next section experimental evidence is given showing that these theoretical consideration can be used to improve the convergence properties of PGRL algorithms. [sent-111, score-0.418]
</p><p>56 ¨ §¤ £¡ ¦ ¢  ¨¨  ¢ ¤ ¢  £¡     4 Experiments The Simulated Environment: The experiments simulate an agent episodically interacting in a continuous two dimensional environment. [sent-112, score-0.346]
</p><p>57 The agent starts each episode in the same state , and executes a ﬁnite number of steps following a policy to a ﬁxed goal state . [sent-113, score-1.093]
</p><p>58 The stochastic policy is deﬁned by a ﬁnite set of Gaussians, each associated with a speciﬁc     3 %  0. [sent-114, score-0.27]
</p><p>59 The Gaussian associated with action      2¢ ¡  G ¤C  ¢ ¡  G ¡C  0. [sent-138, score-0.193]
</p><p>60 5  14  is deﬁned as:  where , is the agents state, is the Gaussian center, and is the variance along each state space dimension. [sent-139, score-0.215]
</p><p>61 sponding Gaussian center  ,  where is the magnitude of the noise, is the state the agent observes and uses to choose actions, and is the actual state of the agent. [sent-142, score-0.611]
</p><p>62 Implementation of the PGRL algorithms: All the PGRL formulations studied here require observations (i. [sent-144, score-0.126]
</p><p>63 is sampled by executing action in state and thereafter following the policy. [sent-147, score-0.431]
</p><p>64 In the episodic formulation, where the agent executes a maximum of steps during each episode, at the end of each episode, for step can be evaluated as follows:  ¨3  8C2¨¢ d 2 1¡      09C ¥¤¢ ¨ d 2 1¡   ¤ 9 ¤ 9 ¨ ¨ $ 8 $ 2¨¢ d 2 1¡   "! [sent-148, score-0.404]
</p><p>65   ¨ 9I22¨¢ ¢  9 9 @¥ 8§ A 2C¡A 6 ¨ i  786 b 9  § $ A ¨6 8 6 ¨©¢ d 2 1¡   6 ¨ £ D ¨ 6 8 6 2¨¢ d 2 1¡   3  9  ¨  Thus, given that the agent executes a complete episode following the policy , at the completion of the episode we can calculate . [sent-151, score-0.854]
</p><p>66 Equation (3) tells us that we require a total of state action value function observations to estimate a performance gradient (assuming the agent can execute actions). [sent-153, score-1.053]
</p><p>67 Therefore, we can obtain the remaining observations of by sending the agent out on   ¢  9    9¨   epsisodes, each time allowing it to follow the policy for all steps, with the exception that action is executed when is being observed. [sent-154, score-0.849]
</p><p>68 This sampling procedure requires a total of episodes and gives a complete set of state action pairs for any path . [sent-155, score-0.362]
</p><p>69 2, these observations are directly used to estimate the performance gradient. [sent-157, score-0.179]
</p><p>70 For the linear basis function based PGRL algorithm in Section 3. [sent-158, score-0.054]
</p><p>71 1, these observations are as deﬁned in [5, 4], and then the performance gradient is ﬁrst used to calculate the calculated using (8). [sent-159, score-0.308]
</p><p>72 3)   G G     ¥¤¢    Experimental Results: Figure 1b shows a plot of average values over 10,000 estimates of the performance gradient. [sent-163, score-0.178]
</p><p>73 For each estimate, the goal state, start ; state, and Gaussian centers are all chosen using a uniform random distribution the Gaussian variances are sampled from a uniform distribution . [sent-164, score-0.183]
</p><p>74 2, as the number of actions increases, this ratio also increases. [sent-167, score-0.15]
</p><p>75 Note that Figure 1b plots average variance ratios, not the bounds in variance given in Theorem 1 and Theorem 2 (which have not been experimentally sampled), so the ratio predicted by the theorems is supported by the increase in increases. [sent-168, score-0.311]
</p><p>76 Figure 1c shows a plot of average values the ratio as over 10,000 estimates of the performance gradient. [sent-169, score-0.202]
</p><p>77 As above, for each estimate, the goal state, start state, and Gaussian centers are all chosen using a uniform random distribution ; the Gaussian variances are sampled from a uniform distribution . [sent-170, score-0.183]
</p><p>78 Finally, Figure 1a shows the average reward over 100 runs as the three algorithms converge on a two action problem. [sent-172, score-0.406]
</p><p>79 Each algorithm is given the same number of samples to estimate the gradient before each update. [sent-173, score-0.287]
</p><p>80 Because has the least variance, it allows the policy to converge . [sent-174, score-0.31]
</p><p>81 Similarly, because has the highest variance, its to the highest reward value policy updates converge to the worst . [sent-175, score-0.462]
</p><p>82 Note that because all three algorithms will converge to the same locally optimal policy given enough samples of , Figure 1a simply requires more samples than , which in turn requires more demonstrates that samples than . [sent-176, score-0.604]
</p><p>83 3 )  5 Conclusion The theoretical and experimental results presented here indicate that how PGRL algorithms are implemented can substantially affect the number of observations of the state action value function ( ) needed to obtain good estimates of the performance gradient. [sent-184, score-0.713]
</p><p>84 Furthermore, they suggest that an appropriately chosen bias term, speciﬁcally the average value of over all actions, and the direct use of observed values can improve the convergence of PGRL algorithms. [sent-185, score-0.41]
</p><p>85 In practice linear basis function representations of can signiﬁcantly degrade the convergence properties of policy gradient algorithms. [sent-186, score-0.81]
</p><p>86 nonlinear) function approximation representation of value functions can be used to improve convergence of such algorithms. [sent-189, score-0.292]
</p><p>87 Bartlett, Reinforcement learning in pomdp’s via direct gradient ascent, Proceedings of the Seventeenth International Conference on Machine Learning (ICML’2000) (Stanford University, CA), June 2000, pp. [sent-191, score-0.234]
</p><p>88 Ungar, Localizing policy gradient estimates to action transitions, Proceedings of the Seventeenth International Conference on Machine Learning, vol. [sent-197, score-0.736]
</p><p>89 [3]  , Localizing search in reinforcement learning, Proceedings of the Seventeenth National Conference on Artiﬁcial Intelligence, vol. [sent-200, score-0.106]
</p><p>90 Mansour, Policy gradient methods for reinforcement learning with function approximation, Advances in Neural Information Processing Systems (Cambridge, MA) (S. [sent-221, score-0.286]
</p><p>91 Williams, Simple statistical gradient-following algorithms for connectionist reinforcement learning, Machine Learning 8 (1992), no. [sent-232, score-0.17]
</p><p>92 3 8C2¨¢ §3 $ 2¨¢ ¡ ( $ s a ¡ ¢ @ a @  9 AB   9 B  Proof of Theorem 1: Consider the deﬁnition of there exist and such that:  ¨¨ ©¢  Let be the observation of following:  where the basis functions  and  Denoting  given in (7). [sent-247, score-0.054]
</p><p>93 Using (13), we get the  have the form  , with variance  as the least squares (LS) estimate of (3), its form is given by:  (14)  where are LS estimates of the weights and correspond to the basis functions . [sent-249, score-0.267]
</p><p>94 Then, it can be shown that any linear system of the type given in (14) has a rate of convergence given by:  Substituting (5) and (6) into the above equation completes the proof. [sent-250, score-0.288]
</p><p>95 03 © 9@¥&3 ¨ C $ 7 ¨2¨¢ ¡ ¥(¢ d $ a  Plugging the above into (16) and inserting from (6) completes the proof for the upper bound. [sent-259, score-0.092]
</p><p>96 The proof for the lower bound in the variance follows similar reasoning. [sent-260, score-0.127]
</p><p>97 Given (5) the variance  on the far left of (16) is bounded by:  (16)  Where  A similarly argument applies to the lower bound on convergence completing the proof for (10). [sent-261, score-0.352]
</p><p>98 Following the same argument for (12), we have Given (5) the worst rate of convergence is bounded by:  (15)  @AB 9x7d 6 7 2! [sent-262, score-0.323]
</p><p>99 03 )03 s G¢  d 321¡     Because each by  is independently distributed, the variance of the estimate is given  B  Proof of Theorem 2: We prove equation (10) ﬁrst. [sent-267, score-0.12]
</p><p>100 For gradient, we get independent samples of each and therefore:  estimates of the performance . [sent-268, score-0.21]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pgrl', 0.581), ('agent', 0.319), ('policy', 0.27), ('ab', 0.197), ('action', 0.193), ('convergence', 0.187), ('gradient', 0.18), ('state', 0.146), ('pifa', 0.138), ('actions', 0.126), ('fa', 0.125), ('pg', 0.11), ('reinforcement', 0.106), ('episode', 0.102), ('wu', 0.097), ('estimates', 0.093), ('theorem', 0.091), ('reward', 0.085), ('grudic', 0.083), ('yxwu', 0.072), ('ungar', 0.072), ('bias', 0.07), ('variance', 0.069), ('rate', 0.067), ('observations', 0.067), ('open', 0.065), ('algorithms', 0.064), ('ned', 0.062), ('executes', 0.061), ('performance', 0.061), ('formulations', 0.059), ('proof', 0.058), ('samples', 0.056), ('rl', 0.055), ('basis', 0.054), ('direct', 0.054), ('theoretical', 0.053), ('seventeenth', 0.052), ('executing', 0.052), ('theorems', 0.052), ('de', 0.052), ('estimate', 0.051), ('localizing', 0.048), ('parameterization', 0.045), ('showing', 0.044), ('degrade', 0.044), ('bounds', 0.043), ('representations', 0.042), ('mller', 0.041), ('converge', 0.04), ('sampled', 0.04), ('improve', 0.039), ('bd', 0.039), ('bounded', 0.038), ('formulation', 0.038), ('concerns', 0.037), ('value', 0.036), ('hp', 0.035), ('supporting', 0.035), ('receives', 0.034), ('characterized', 0.034), ('yx', 0.034), ('completes', 0.034), ('ls', 0.034), ('evidence', 0.033), ('properties', 0.033), ('uniform', 0.033), ('turn', 0.032), ('ha', 0.032), ('worst', 0.031), ('estimating', 0.031), ('nite', 0.031), ('approximation', 0.03), ('locally', 0.03), ('predicted', 0.03), ('section', 0.029), ('july', 0.029), ('centers', 0.028), ('june', 0.028), ('mdp', 0.028), ('dimensional', 0.027), ('rewards', 0.027), ('reaches', 0.026), ('toward', 0.026), ('sutton', 0.025), ('questions', 0.025), ('goal', 0.025), ('average', 0.024), ('steps', 0.024), ('timate', 0.024), ('contrasts', 0.024), ('guaranteeing', 0.024), ('speculation', 0.024), ('cacy', 0.024), ('variances', 0.024), ('term', 0.024), ('ratio', 0.024), ('slow', 0.023), ('addressed', 0.023), ('sampling', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="157-tfidf-1" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>Author: Gregory Z. Grudic, Lyle H. Ungar</p><p>Abstract: We address two open theoretical questions in Policy Gradient Reinforcement Learning. The ﬁrst concerns the efﬁcacy of using function approximation to represent the state action value function, . Theory is presented showing that linear function approximation representations of can degrade the rate of convergence of performance gradient estimates by a factor of relative to when no function approximation of is used, where is the number of possible actions and is the number of basis functions in the function approximation representation. The second concerns the use of a bias term in estimating the state action value function. Theory is presented showing that a non-zero bias term can improve the rate of convergence of performance gradient estimates by , where is the number of possible actions. Experimental evidence is presented showing that these theoretical results lead to signiﬁcant improvement in the convergence properties of Policy Gradient Reinforcement Learning algorithms.       ¤ ¨ ¦ ¢ ©§¥¤£¡ ¦ ¤ ¨ £¡ ¨ ¤¢  ¢</p><p>2 0.31493998 <a title="157-tfidf-2" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>3 0.26946798 <a title="157-tfidf-3" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>Author: Carlos Guestrin, Daphne Koller, Ronald Parr</p><p>Abstract: We present a principled and efﬁcient planning algorithm for cooperative multiagent dynamic systems. A striking feature of our method is that the coordination and communication between the agents is not imposed, but derived directly from the system dynamics and function approximation architecture. We view the entire multiagent system as a single, large Markov decision process (MDP), which we assume can be represented in a factored way using a dynamic Bayesian network (DBN). The action space of the resulting MDP is the joint action space of the entire set of agents. Our approach is based on the use of factored linear value functions as an approximation to the joint value function. This factorization of the value function allows the agents to coordinate their actions at runtime using a natural message passing scheme. We provide a simple and efﬁcient method for computing such an approximate value function by solving a single linear program, whose size is determined by the interaction between the value function structure and the DBN. We thereby avoid the exponential blowup in the state and action space. We show that our approach compares favorably with approaches based on reward sharing. We also show that our algorithm is an efﬁcient alternative to more complicated algorithms even in the single agent case.</p><p>4 0.22513962 <a title="157-tfidf-4" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>Author: Michail G. Lagoudakis, Ronald Parr</p><p>Abstract: We propose a new approach to reinforcement learning which combines least squares function approximation with policy iteration. Our method is model-free and completely off policy. We are motivated by the least squares temporal difference learning algorithm (LSTD), which is known for its efﬁcient use of sample experiences compared to pure temporal difference algorithms. LSTD is ideal for prediction problems, however it heretofore has not had a straightforward application to control problems. Moreover, approximations learned by LSTD are strongly inﬂuenced by the visitation distribution over states. Our new algorithm, Least Squares Policy Iteration (LSPI) addresses these issues. The result is an off-policy method which can use (or reuse) data collected from any source. We have tested LSPI on several problems, including a bicycle simulator in which it learns to guide the bicycle to a goal efﬁciently by merely observing a relatively small number of completely random trials.</p><p>5 0.21865356 <a title="157-tfidf-5" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>Author: Thomas G. Dietterich, Xin Wang</p><p>Abstract: We present three ways of combining linear programming with the kernel trick to find value function approximations for reinforcement learning. One formulation is based on SVM regression; the second is based on the Bellman equation; and the third seeks only to ensure that good moves have an advantage over bad moves. All formulations attempt to minimize the number of support vectors while fitting the data. Experiments in a difficult, synthetic maze problem show that all three formulations give excellent performance, but the advantage formulation is much easier to train. Unlike policy gradient methods, the kernel methods described here can easily 'adjust the complexity of the function approximator to fit the complexity of the value function. 1</p><p>6 0.20882966 <a title="157-tfidf-6" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>7 0.20058216 <a title="157-tfidf-7" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>8 0.1765842 <a title="157-tfidf-8" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>9 0.16837448 <a title="157-tfidf-9" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>10 0.1546239 <a title="157-tfidf-10" href="./nips-2001-Stabilizing_Value_Function_Approximation_with_the_BFBP_Algorithm.html">175 nips-2001-Stabilizing Value Function Approximation with the BFBP Algorithm</a></p>
<p>11 0.13193844 <a title="157-tfidf-11" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>12 0.12434395 <a title="157-tfidf-12" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>13 0.1113672 <a title="157-tfidf-13" href="./nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</a></p>
<p>14 0.10822537 <a title="157-tfidf-14" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>15 0.10692242 <a title="157-tfidf-15" href="./nips-2001-Predictive_Representations_of_State.html">148 nips-2001-Predictive Representations of State</a></p>
<p>16 0.10402967 <a title="157-tfidf-16" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>17 0.10276945 <a title="157-tfidf-17" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>18 0.1016895 <a title="157-tfidf-18" href="./nips-2001-Generalizable_Relational_Binding_from_Coarse-coded_Distributed_Representations.html">80 nips-2001-Generalizable Relational Binding from Coarse-coded Distributed Representations</a></p>
<p>19 0.090352513 <a title="157-tfidf-19" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>20 0.076745406 <a title="157-tfidf-20" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.235), (1, -0.14), (2, 0.461), (3, 0.057), (4, 0.032), (5, 0.113), (6, -0.008), (7, -0.044), (8, 0.026), (9, 0.01), (10, -0.035), (11, 0.007), (12, 0.013), (13, -0.035), (14, -0.054), (15, 0.022), (16, 0.076), (17, -0.011), (18, -0.046), (19, 0.0), (20, 0.041), (21, -0.032), (22, 0.015), (23, 0.035), (24, -0.024), (25, 0.012), (26, 0.065), (27, 0.038), (28, 0.01), (29, 0.023), (30, 0.002), (31, 0.07), (32, -0.084), (33, -0.046), (34, -0.09), (35, -0.049), (36, 0.069), (37, -0.017), (38, -0.004), (39, 0.026), (40, -0.051), (41, -0.026), (42, 0.028), (43, -0.01), (44, 0.002), (45, 0.076), (46, 0.027), (47, 0.034), (48, -0.029), (49, 0.086)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96802282 <a title="157-lsi-1" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>Author: Gregory Z. Grudic, Lyle H. Ungar</p><p>Abstract: We address two open theoretical questions in Policy Gradient Reinforcement Learning. The ﬁrst concerns the efﬁcacy of using function approximation to represent the state action value function, . Theory is presented showing that linear function approximation representations of can degrade the rate of convergence of performance gradient estimates by a factor of relative to when no function approximation of is used, where is the number of possible actions and is the number of basis functions in the function approximation representation. The second concerns the use of a bias term in estimating the state action value function. Theory is presented showing that a non-zero bias term can improve the rate of convergence of performance gradient estimates by , where is the number of possible actions. Experimental evidence is presented showing that these theoretical results lead to signiﬁcant improvement in the convergence properties of Policy Gradient Reinforcement Learning algorithms.       ¤ ¨ ¦ ¢ ©§¥¤£¡ ¦ ¤ ¨ £¡ ¨ ¤¢  ¢</p><p>2 0.82223916 <a title="157-lsi-2" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>Author: Michail G. Lagoudakis, Ronald Parr</p><p>Abstract: We propose a new approach to reinforcement learning which combines least squares function approximation with policy iteration. Our method is model-free and completely off policy. We are motivated by the least squares temporal difference learning algorithm (LSTD), which is known for its efﬁcient use of sample experiences compared to pure temporal difference algorithms. LSTD is ideal for prediction problems, however it heretofore has not had a straightforward application to control problems. Moreover, approximations learned by LSTD are strongly inﬂuenced by the visitation distribution over states. Our new algorithm, Least Squares Policy Iteration (LSPI) addresses these issues. The result is an off-policy method which can use (or reuse) data collected from any source. We have tested LSPI on several problems, including a bicycle simulator in which it learns to guide the bicycle to a goal efﬁciently by merely observing a relatively small number of completely random trials.</p><p>3 0.81669348 <a title="157-lsi-3" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>4 0.79407179 <a title="157-lsi-4" href="./nips-2001-Stabilizing_Value_Function_Approximation_with_the_BFBP_Algorithm.html">175 nips-2001-Stabilizing Value Function Approximation with the BFBP Algorithm</a></p>
<p>Author: Xin Wang, Thomas G. Dietterich</p><p>Abstract: We address the problem of non-convergence of online reinforcement learning algorithms (e.g., Q learning and SARSA(A)) by adopting an incremental-batch approach that separates the exploration process from the function fitting process. Our BFBP (Batch Fit to Best Paths) algorithm alternates between an exploration phase (during which trajectories are generated to try to find fragments of the optimal policy) and a function fitting phase (during which a function approximator is fit to the best known paths from start states to terminal states). An advantage of this approach is that batch value-function fitting is a global process, which allows it to address the tradeoffs in function approximation that cannot be handled by local, online algorithms. This approach was pioneered by Boyan and Moore with their GROWSUPPORT and ROUT algorithms. We show how to improve upon their work by applying a better exploration process and by enriching the function fitting procedure to incorporate Bellman error and advantage error measures into the objective function. The results show improved performance on several benchmark problems. 1</p><p>5 0.75792211 <a title="157-lsi-5" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>Author: Shie Mannor, Nahum Shimkin</p><p>Abstract: We consider the problem of learning to attain multiple goals in a dynamic environment, which is initially unknown. In addition, the environment may contain arbitrarily varying elements related to actions of other agents or to non-stationary moves of Nature. This problem is modelled as a stochastic (Markov) game between the learning agent and an arbitrary player, with a vector-valued reward function. The objective of the learning agent is to have its long-term average reward vector belong to a given target set. We devise an algorithm for achieving this task, which is based on the theory of approachability for stochastic games. This algorithm combines, in an appropriate way, a ﬁnite set of standard, scalar-reward learning algorithms. Suﬃcient conditions are given for the convergence of the learning algorithm to a general target set. The specialization of these results to the single-controller Markov decision problem are discussed as well. 1</p><p>6 0.74764299 <a title="157-lsi-6" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>7 0.74648315 <a title="157-lsi-7" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>8 0.73543072 <a title="157-lsi-8" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>9 0.69015265 <a title="157-lsi-9" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>10 0.64544874 <a title="157-lsi-10" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>11 0.62316495 <a title="157-lsi-11" href="./nips-2001-Predictive_Representations_of_State.html">148 nips-2001-Predictive Representations of State</a></p>
<p>12 0.60001278 <a title="157-lsi-12" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>13 0.59175688 <a title="157-lsi-13" href="./nips-2001-Switch_Packet_Arbitration_via_Queue-Learning.html">177 nips-2001-Switch Packet Arbitration via Queue-Learning</a></p>
<p>14 0.58317232 <a title="157-lsi-14" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>15 0.57994211 <a title="157-lsi-15" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>16 0.57746977 <a title="157-lsi-16" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>17 0.55558562 <a title="157-lsi-17" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>18 0.45129111 <a title="157-lsi-18" href="./nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</a></p>
<p>19 0.31997466 <a title="157-lsi-19" href="./nips-2001-Improvisation_and_Learning.html">91 nips-2001-Improvisation and Learning</a></p>
<p>20 0.31600285 <a title="157-lsi-20" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.032), (17, 0.067), (19, 0.046), (21, 0.168), (27, 0.124), (30, 0.061), (38, 0.061), (42, 0.013), (59, 0.024), (72, 0.092), (79, 0.033), (83, 0.048), (91, 0.148)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94273537 <a title="157-lda-1" href="./nips-2001-A_Neural_Oscillator_Model_of_Auditory_Selective_Attention.html">14 nips-2001-A Neural Oscillator Model of Auditory Selective Attention</a></p>
<p>Author: Stuart N. Wrigley, Guy J. Brown</p><p>Abstract: A model of auditory grouping is described in which auditory attention plays a key role. The model is based upon an oscillatory correlation framework, in which neural oscillators representing a single perceptual stream are synchronised, and are desynchronised from oscillators representing other streams. The model suggests a mechanism by which attention can be directed to the high or low tones in a repeating sequence of tones with alternating frequencies. In addition, it simulates the perceptual segregation of a mistuned harmonic from a complex tone. 1</p><p>same-paper 2 0.87640798 <a title="157-lda-2" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>Author: Gregory Z. Grudic, Lyle H. Ungar</p><p>Abstract: We address two open theoretical questions in Policy Gradient Reinforcement Learning. The ﬁrst concerns the efﬁcacy of using function approximation to represent the state action value function, . Theory is presented showing that linear function approximation representations of can degrade the rate of convergence of performance gradient estimates by a factor of relative to when no function approximation of is used, where is the number of possible actions and is the number of basis functions in the function approximation representation. The second concerns the use of a bias term in estimating the state action value function. Theory is presented showing that a non-zero bias term can improve the rate of convergence of performance gradient estimates by , where is the number of possible actions. Experimental evidence is presented showing that these theoretical results lead to signiﬁcant improvement in the convergence properties of Policy Gradient Reinforcement Learning algorithms.       ¤ ¨ ¦ ¢ ©§¥¤£¡ ¦ ¤ ¨ £¡ ¨ ¤¢  ¢</p><p>3 0.80430776 <a title="157-lda-3" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>4 0.78821588 <a title="157-lda-4" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>Author: Mário Figueiredo</p><p>Abstract: In this paper we introduce a new sparseness inducing prior which does not involve any (hyper)parameters that need to be adjusted or estimated. Although other applications are possible, we focus here on supervised learning problems: regression and classiﬁcation. Experiments with several publicly available benchmark data sets show that the proposed approach yields state-of-the-art performance. In particular, our method outperforms support vector machines and performs competitively with the best alternative techniques, both in terms of error rates and sparseness, although it involves no tuning or adjusting of sparsenesscontrolling hyper-parameters.</p><p>5 0.78807509 <a title="157-lda-5" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>Author: Michail G. Lagoudakis, Ronald Parr</p><p>Abstract: We propose a new approach to reinforcement learning which combines least squares function approximation with policy iteration. Our method is model-free and completely off policy. We are motivated by the least squares temporal difference learning algorithm (LSTD), which is known for its efﬁcient use of sample experiences compared to pure temporal difference algorithms. LSTD is ideal for prediction problems, however it heretofore has not had a straightforward application to control problems. Moreover, approximations learned by LSTD are strongly inﬂuenced by the visitation distribution over states. Our new algorithm, Least Squares Policy Iteration (LSPI) addresses these issues. The result is an off-policy method which can use (or reuse) data collected from any source. We have tested LSPI on several problems, including a bicycle simulator in which it learns to guide the bicycle to a goal efﬁciently by merely observing a relatively small number of completely random trials.</p><p>6 0.78365362 <a title="157-lda-6" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>7 0.78048891 <a title="157-lda-7" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>8 0.78046656 <a title="157-lda-8" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>9 0.77834821 <a title="157-lda-9" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>10 0.77818918 <a title="157-lda-10" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>11 0.77701348 <a title="157-lda-11" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>12 0.77654445 <a title="157-lda-12" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>13 0.77509916 <a title="157-lda-13" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>14 0.77484739 <a title="157-lda-14" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>15 0.77329558 <a title="157-lda-15" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>16 0.77281123 <a title="157-lda-16" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>17 0.7716859 <a title="157-lda-17" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>18 0.77143353 <a title="157-lda-18" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>19 0.77132058 <a title="157-lda-19" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>20 0.77016175 <a title="157-lda-20" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
