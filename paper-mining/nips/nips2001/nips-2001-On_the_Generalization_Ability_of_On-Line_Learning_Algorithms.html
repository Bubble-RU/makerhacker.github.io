<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-138" href="#">nips2001-138</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</h1>
<br/><p>Source: <a title="nips-2001-138-pdf" href="http://papers.nips.cc/paper/2113-on-the-generalization-ability-of-on-line-learning-algorithms.pdf">pdf</a></p><p>Author: Nicolò Cesa-bianchi, Alex Conconi, Claudio Gentile</p><p>Abstract: In this paper we show that on-line algorithms for classiﬁcation and regression can be naturally used to obtain hypotheses with good datadependent tail bounds on their risk. Our results are proven without requiring complicated concentration-of-measure arguments and they hold for arbitrary on-line learning algorithms. Furthermore, when applied to concrete on-line algorithms, our results yield tail bounds that in many cases are comparable or better than the best known bounds.</p><p>Reference: <a title="nips-2001-138-reference" href="../nips2001_reference/nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 it  Abstract In this paper we show that on-line algorithms for classiﬁcation and regression can be naturally used to obtain hypotheses with good datadependent tail bounds on their risk. [sent-7, score-0.702]
</p><p>2 Our results are proven without requiring complicated concentration-of-measure arguments and they hold for arbitrary on-line learning algorithms. [sent-8, score-0.112]
</p><p>3 Furthermore, when applied to concrete on-line algorithms, our results yield tail bounds that in many cases are comparable or better than the best known bounds. [sent-9, score-0.346]
</p><p>4 First, obtaining tight uniform risk bounds in terms of meaningful empirical quantities is generally a difﬁcult task. [sent-12, score-0.586]
</p><p>5 Second, searching for the hypothesis minimizing a given empirical functional is often computationally expensive and, furthermore, the minimizing algorithm is seldom incremental (if new data is added to the training set then the algorithm needs be run again from scratch). [sent-13, score-0.552]
</p><p>6 On-line learning algorithms, such as the Perceptron algorithm [17], the Winnow algorithm [14], and their many variants [16, 6, 13, 10, 2, 9], are general methods for solving classiﬁcation and regression problems that can be used in a fully incremental fashion. [sent-14, score-0.294]
</p><p>7 That is, they need (in most cases) a short time to process each new training example and adjust their current hypothesis. [sent-15, score-0.04]
</p><p>8 While the behavior of these algorithms is well understood in the so-called mistake bound model [14], where no assumptions are made on the way the training sequence is generated, there are fewer results concerning how to use these algorithms to obtain hypotheses with small statistical risk. [sent-16, score-0.592]
</p><p>9 Littlestone [15] proposed a method for obtaining small risk hypotheses from a run of an arbitrary on-line algorithm by using a cross validation set to test each one of the hypotheses generated during the run. [sent-17, score-1.142]
</p><p>10 This method does not require any convergence property of the online algorithm and provides risk tail bounds that are sharper than those obtainable choosing, for instance, the hypothesis in the run that survived the longest. [sent-18, score-1.054]
</p><p>11 Helmbold, Warmuth,  and others [11, 6, 8] showed that, without using any cross-validation sets, one can obtain expected risk bounds (as opposed to the more informative tail bounds) for a hypothesis randomly drawn among those generated during the run. [sent-19, score-0.863]
</p><p>12 In this paper we prove, via reﬁnements and extensions of the previous analyses, that online algorithms naturally lead to good data-dependent tail bounds without employing the complicated concentration-of-measure machinery needed by other frameworks [19]. [sent-20, score-0.386]
</p><p>13 When applied to concrete algorithms, the loss bound translates into a function of meaningful data-dependent quantities. [sent-22, score-0.332]
</p><p>14 For classiﬁcation problems, the mistake bound for the -norm Perceptron algorithm yields a tail risk bound in terms of the empirical distribution of the margins — see (4). [sent-23, score-0.862]
</p><p>15 For regression problems, the square loss bound for ridge regression yields a tail risk bound in terms of the eigenvalues of the Gram matrix — see (5). [sent-24, score-1.308]
</p><p>16 £    £¡ ¤¢   ¥     2 Preliminaries and notation be arbitrary sets and . [sent-25, score-0.083]
</p><p>17 An example is a pair , where is an Let instance belonging to and is the label associated with . [sent-26, score-0.07]
</p><p>18 We let be the pair of random variables , where and take values in and , respectively. [sent-28, score-0.035]
</p><p>19 Throughout the paper, we assume that data are generated i. [sent-29, score-0.032]
</p><p>20 All probabilities and expectations will be understood with to denote the vectorrespect to this underlying distribution. [sent-33, score-0.032]
</p><p>21    ¦    1  3 4(  $ 3 CB@A@875( (§ 999§6  ) $ 1§ 2#¢0) © & '¢" ¦ ©  ¦    ©§ ¨¦  A hypothesis is any (measurable) mapping from instances to predictions , where is a given decision space. [sent-36, score-0.187]
</p><p>22 The risk of is deﬁned by , where is a nonnegative loss function. [sent-37, score-0.469]
</p><p>23 The on-line algorithms we investigate are deﬁned within a well-known mathematical model, which is a generalization of a learning model introduced by Littlestone [14] and Angluin [1]. [sent-39, score-0.068]
</p><p>24 In this learning model, an on-line algorithm processes the examples in one at a time in trials, generating a sequence of hypotheses . [sent-41, score-0.372]
</p><p>25 At the beginning of the -th trial, the algorithm receives the and uses its current hypothesis to compute a prediction instance for the label associated with . [sent-42, score-0.375]
</p><p>26 Then, the true value of the label is disclosed and , measuring how bad is the prediction the algorithm suffers a loss for the label . [sent-43, score-0.319]
</p><p>27 Before the next trial begins, the algorithm generates a new hypothesis which may or may not be equal to . [sent-44, score-0.336]
</p><p>28 We measure the algorithm’s performance on by its cumulative loss  fedcba0)! [sent-45, score-0.258]
</p><p>29 In particular, throughout the paper will denote the (deterministic) initial hypothesis of an arbitrary on-line algorithm and, for each , will be a random variable denoting the -th hypothesis of the on-line algorithm and such that the value of does not change upon changes in the values of . [sent-47, score-0.641]
</p><p>30 Our goal is to relate the risk of the hypotheses produced by an on-line algorithm running on an i. [sent-48, score-0.643]
</p><p>31 sequence to the cumulative loss of the algorithm on that sequence. [sent-51, score-0.387]
</p><p>32 The cumulative loss will be our key empirical (data-dependent) quantity. [sent-52, score-0.296]
</p><p>33 Via our analysis we will obtain bounds of the form  $  t3 f(sh  § l k   £k          ¨ ©§  $3  tnf(£ sh  ¢ ¡  $ i§ 999 #$ 3 #BA@A§  gi ¤AP Q  ¥ ¦  £ ¤  where is a speciﬁc function of the sequence of hypotheses produced by the algorithm, and is a suitable positive constant. [sent-53, score-0.546]
</p><p>34 We will see that for speciﬁc on-line algorithms the ratio can be further bounded in terms of meaningful empirical quantities. [sent-54, score-0.181]
</p><p>35 3 #@AAWi i§ 999§   £ $3  8¡ tr5(sh  $ 3 #@A@`2gi i§ 999§  ¨  £  Our method centers on the following simple concentration lemma about bounded losses. [sent-55, score-0.267]
</p><p>36 Lemma 1 Let be an arbitrary bounded loss satisfying on-line algorithm output (not necessarily distinct) hypotheses on . [sent-56, score-0.727]
</p><p>37 Furthermore,  $  where denotes the -algebra generated by . [sent-65, score-0.032]
</p><p>38 A direct application of the Hoeffding-Azuma inequality [3] to the bounded random variables proves the lemma. [sent-66, score-0.13]
</p><p>39 2  '  3  4  3 Concentration for convex losses In this section we investigate the risk of the average hypothesis  6 F e 6 §   D 3 £k  D d 9 7 5 @86  £  where are the hypotheses generated by some on-line algorithm run on training examples. [sent-67, score-1.118]
</p><p>40 1 The average hypothesis generates valid predictions whenever the decision space is convex. [sent-68, score-0.187]
</p><p>41 9 l k £ u £ h 'i @AP $ Q k w 2t w 3 r( X e # Vt p ©  I g X u§  I I 3 cB@A@§ 6 c`D D§ 999 D§  Theorem 2 Let be convex and be convex in the ﬁrst argument. [sent-69, score-0.086]
</p><p>42 Let an arbitrary on-line algorithm for output (not necessarily distinct) hypotheses when the algorithm is run on . [sent-70, score-0.671]
</p><p>43 Then for any the following holds      B     § A  ¥   is not used in this average. [sent-71, score-0.034]
</p><p>44     D EC  ¢ ¡  Notice that the last hypothesis  3 #@AA§  i i§ 999  1  4  Proof. [sent-72, score-0.187]
</p><p>45 Since is convex in the ﬁrst argument, by Jensen’s inequality we have Taking expectation with respect to yields . [sent-73, score-0.146]
</p><p>46 Using the last inequality along with Lemma 1 yields the thesis. [sent-74, score-0.103]
</p><p>47 D ¡X ¢ "§$     6 Q $   gD AP 6 e 3 ¤¤ 3 9 $ "§$ 6   q%#bG08 RgDvX 6 e 3 ¥366 X  $ 1§ dc¢)   This theorem, which can be viewed as the tail bound version of the expected bound in [11], implies that the risk of the average hypothesis is close to for “most” samples . [sent-76, score-0.841]
</p><p>48 Here we just note that by applying this theorem to the Weighted Majority algorithm [16], we can prove a version of [5, Theorem 4] for the absolute loss without resorting to sophisticated concentration inequalities (details in the full paper). [sent-79, score-0.448]
</p><p>49 4 Penalized risk estimation for general losses If the loss function is nonconvex (such as the 0-1 loss) then the risk of the average hypothesis cannot be bounded in the way shown in the previous section. [sent-80, score-1.124]
</p><p>50 However, the risk of the best hypothesis, among those generated by the on-line algorithm, cannot be higher than the average risk of the same hypotheses. [sent-81, score-0.648]
</p><p>51 Hence, Lemma 1 immediately tells us that, at under no conditions on the loss function other than boundedness, for most samples least one of the hypotheses generated has risk close to . [sent-82, score-0.744]
</p><p>52 In this section we give a technique (Lemma 4) that, using a penalized risk estimate, ﬁnds with high probability such a hypothesis. [sent-83, score-0.393]
</p><p>53 Unlike Littlestone’s, our technique does not require a cross validation set. [sent-85, score-0.037]
</p><p>54 Therefore we are able to obtain bounds on the risk whose main term is , where is the size of the whole set of examples available to the learning algorithm (i. [sent-86, score-0.574]
</p><p>55 Similar observations are made in [4], though the analysis there does actually refer only to randomized hypotheses with 0-1 loss (namely, to absolute loss). [sent-89, score-0.404]
</p><p>56 X  3  £ $ ¤¡ t3      £  £ 8¡ $3      D  Let us deﬁne the penalized risk estimate of hypothesis  by      3 ! [sent-90, score-0.58]
</p><p>57  ¢£  where is the length of the sufﬁx of the training sequence that the on-line is the cumulative loss of on that algorithm had not seen yet when was generated, sufﬁx, and   RD  9 $k  £ £   §       k  ! [sent-93, score-0.427]
</p><p>58 $ ¦ SG0 §    ¨    Our algorithm chooses the hypothesis  , where  e § k@% Vt X 6 3 9 %$$t £  ¦§    ! [sent-94, score-0.279]
</p><p>59 For the sake of simplicity, we will restrict to losses with range . [sent-98, score-0.127]
</p><p>60 However, it should be clear that losses taking values in arbitrary bounded real interval can be handled using techniques similar to those shown in Section 3. [sent-99, score-0.243]
</p><p>61 Theorem 3 Let an arbitrary on-line algorithm output (not necessarily distinct) hypotheses when it is run on . [sent-101, score-0.579]
</p><p>62 Then, for any , the hypothesis chosen using the penalized risk estimate based on satisﬁes  ¨i    9 l $¤k  £k £ k l jt w     §       B    '& (¦ ¨  3 (  12 £ 0'¨ i  AP h )$ Q §  ¢  3 #! [sent-102, score-0.626]
</p><p>63 ¡  $  ¡ £  ¦ 7$  0i¤A $ gi @P w QP Q ¦ $ Q ) $ A ¡ £  § @ 0i¤AP ¡ ¤¨ $ ¦  QP ta £  § 5$j0i Ayl  ¤¨ ¨  ! [sent-123, score-0.051]
</p><p>64 Let , and set for brevity    ¡  Lemma 4 Let an arbitrary on-line algorithm output (not necessarily distinct) hypotheses when it is run on . [sent-144, score-0.579]
</p><p>65 Then for any the following holds:   The proof of this theorem is based on the two following technical lemmas. [sent-145, score-0.068]
</p><p>66 Lemma 5 Let an arbitrary on-line algorithm output (not necessarily distinct) hypotheses when it is run on . [sent-146, score-0.579]
</p><p>67 §         §  where the last inequality follows from  . [sent-160, score-0.066]
</p><p>68 The proof follows by combining Lemma 4 and Lemma 5, and by overapproximating the square root terms therein. [sent-164, score-0.031]
</p><p>69 4  5 Applications For the sake of concreteness we now sketch two generalization bounds which can be obtained through a direct application of our techniques. [sent-165, score-0.205]
</p><p>70 The -norm Perceptron algorithm [10, 9] is a linear threshold algorithm which keeps in the -th trial a weight vector . [sent-166, score-0.241]
</p><p>71 On instance , the algorithm predicts by sign , where and . [sent-167, score-0.129]
</p><p>72 , if ) then the algorithm performs the weight update . [sent-170, score-0.092]
</p><p>73 On the other hand, gets an algorithm which performs like a multiplicative algorithm, such as the Normalized Winnow algorithm [10]. [sent-172, score-0.184]
</p><p>74 Applying Theorem 3 to the bound on the number of mistakes for the -norm Perceptron algorithm shown in [9], we immediately obtain that, with probability at least with respect to the draw of the training sample , the risk of the penalized estimator is at most  $5  2 ! [sent-173, score-0.712]
</p><p>75 The margin-based quantity is called soft margin in [20] and accounts for the distribution of margin values achieved by the examples in with respect to hyperplane . [sent-182, score-0.13]
</p><p>76 , [19]) are typically expressed in terms of the sample margin , i. [sent-185, score-0.101]
</p><p>77 , in terms of the fraction of training points whose margin is at most . [sent-187, score-0.105]
</p><p>78   % E  %  We remark that bound (4) does not have the extra log factors appearing in the analyses based on uniform convergence. [sent-190, score-0.153]
</p><p>79 Furthermore, it is signiﬁcantly better than the bound in [20] whenever is constant, which typically occurs when the data sequence is not linearly separable. [sent-191, score-0.129]
</p><p>80 8 6  £ ¤¡  As a second application, we consider the ridge regression algorithm [12] for square loss. [sent-192, score-0.346]
</p><p>81 This algorithm computes at the beginning of the -th trial the vector which minimizes , where . [sent-194, score-0.175]
</p><p>82 On instance the algorithm predicts with , where is the “clipping” function if , if and if . [sent-195, score-0.129]
</p><p>83    §  ¢       §  % %  §  % %    for any , where , denotes the determinant of matrix , is the -dimensional identity matrix and is the transpose of . [sent-205, score-0.056]
</p><p>84 2 Let us denote by the matrix whose columns are the data vectors , . [sent-206, score-0.028]
</p><p>85 ¤  §  ¡          "  where the ’s are the eigenvalues of . [sent-213, score-0.09]
</p><p>86 Risk bounds in terms of same as the nonzero eigenvalues of the Gram matrix the eigenvalues of the Gram matrix were also derived in [23]; we defer to the full paper a comparison between these results and ours. [sent-215, score-0.416]
</p><p>87 Finally, our bound applies also to kernel ridge regression [18] by replacing the eigenvalues of with the eigenvalues of the kernel Gram matrix , , where is the kernel being considered. [sent-216, score-0.523]
</p><p>88 Relative loss bounds for on-line density estimation with the exponential family of distributions, Machine Learning, 43:211–246, 2001. [sent-230, score-0.306]
</p><p>89 2 Using a slightly different linear regression algorithm, Forster and Warmuth [7] have proven a sharper bound on the expected relative loss. [sent-235, score-0.248]
</p><p>90 In particular, they have exhibited an algorithm computing hypothesis such that in expectation (over ) the relative risk is bounded by . [sent-236, score-0.651]
</p><p>91 Beating the hold-out: bounds for k-fold and progressive cross-validation. [sent-241, score-0.145]
</p><p>92 Cristianini, On the generalization of soft margin algorithms, 2000. [sent-333, score-0.094]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ap', 0.328), ('risk', 0.308), ('qp', 0.248), ('hypotheses', 0.243), ('ffe', 0.233), ('gi', 0.208), ('littlestone', 0.203), ('hypothesis', 0.187), ('tail', 0.162), ('loss', 0.161), ('gd', 0.154), ('bounds', 0.145), ('ridge', 0.139), ('lemma', 0.135), ('fe', 0.124), ('cumulative', 0.097), ('losses', 0.096), ('aa', 0.093), ('algorithm', 0.092), ('sh', 0.092), ('bound', 0.092), ('eigenvalues', 0.09), ('vt', 0.09), ('perceptron', 0.09), ('gentile', 0.088), ('milan', 0.088), ('rgd', 0.088), ('penalized', 0.085), ('regression', 0.084), ('arbitrary', 0.083), ('colt', 0.077), ('run', 0.077), ('helmbold', 0.076), ('gram', 0.071), ('theorem', 0.068), ('concentration', 0.068), ('inequality', 0.066), ('margin', 0.065), ('bounded', 0.064), ('bramante', 0.058), ('conconi', 0.058), ('crema', 0.058), ('dti', 0.058), ('qap', 0.058), ('italy', 0.058), ('necessarily', 0.057), ('trial', 0.057), ('warmuth', 0.053), ('distinct', 0.052), ('ta', 0.051), ('forster', 0.051), ('angluin', 0.051), ('winnow', 0.051), ('baa', 0.051), ('jt', 0.046), ('ji', 0.045), ('convex', 0.043), ('sharper', 0.043), ('mistake', 0.041), ('training', 0.04), ('online', 0.04), ('meaningful', 0.04), ('algorithms', 0.039), ('concrete', 0.039), ('williamson', 0.039), ('empirical', 0.038), ('validation', 0.037), ('cb', 0.037), ('sequence', 0.037), ('classi', 0.037), ('instance', 0.037), ('yields', 0.037), ('ba', 0.036), ('sample', 0.036), ('nonzero', 0.035), ('let', 0.035), ('holds', 0.034), ('analyses', 0.033), ('label', 0.033), ('generated', 0.032), ('understood', 0.032), ('rd', 0.031), ('sake', 0.031), ('square', 0.031), ('cation', 0.03), ('draw', 0.03), ('prove', 0.03), ('freund', 0.029), ('proven', 0.029), ('inequalities', 0.029), ('obtain', 0.029), ('generalization', 0.029), ('matrix', 0.028), ('uniform', 0.028), ('fd', 0.028), ('majority', 0.028), ('obtaining', 0.027), ('output', 0.027), ('incremental', 0.026), ('beginning', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="138-tfidf-1" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>Author: Nicolò Cesa-bianchi, Alex Conconi, Claudio Gentile</p><p>Abstract: In this paper we show that on-line algorithms for classiﬁcation and regression can be naturally used to obtain hypotheses with good datadependent tail bounds on their risk. Our results are proven without requiring complicated concentration-of-measure arguments and they hold for arbitrary on-line learning algorithms. Furthermore, when applied to concrete on-line algorithms, our results yield tail bounds that in many cases are comparable or better than the best known bounds.</p><p>2 0.23972279 <a title="138-tfidf-2" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>Author: Jyrki Kivinen, Alex J. Smola, Robert C. Williamson</p><p>Abstract: We consider online learning in a Reproducing Kernel Hilbert Space. Our method is computationally efﬁcient and leads to simple algorithms. In particular we derive update equations for classiﬁcation, regression, and novelty detection. The inclusion of the -trick allows us to give a robust parameterization. Moreover, unlike in batch learning where the -trick only applies to the -insensitive loss function we are able to derive general trimmed-mean types of estimators such as for Huber’s robust loss.     ¡</p><p>3 0.19932057 <a title="138-tfidf-3" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>Author: Gunnar Rätsch, Sebastian Mika, Manfred K. Warmuth</p><p>Abstract: We give an uniﬁed convergence analysis of ensemble learning methods including e.g. AdaBoost, Logistic Regression and the Least-SquareBoost algorithm for regression. These methods have in common that they iteratively call a base learning algorithm which returns hypotheses that are then linearly combined. We show that these methods are related to the Gauss-Southwell method known from numerical optimization and state non-asymptotical convergence results for all these methods. Our analysis includes -norm regularized cost functions leading to a clean and general way to regularize ensemble learning.</p><p>4 0.17876296 <a title="138-tfidf-4" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>Author: T. Zhang</p><p>Abstract: We investigate the generalization performance of some learning problems in Hilbert functional Spaces. We introduce a notion of convergence of the estimated functional predictor to the best underlying predictor, and obtain an estimate on the rate of the convergence. This estimate allows us to derive generalization bounds on some learning formulations.</p><p>5 0.17081137 <a title="138-tfidf-5" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>6 0.15991823 <a title="138-tfidf-6" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>7 0.15672602 <a title="138-tfidf-7" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>8 0.14455266 <a title="138-tfidf-8" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>9 0.12523869 <a title="138-tfidf-9" href="./nips-2001-Efficiency_versus_Convergence_of_Boolean_Kernels_for_On-Line_Learning_Algorithms.html">66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</a></p>
<p>10 0.11990483 <a title="138-tfidf-10" href="./nips-2001-%28Not%29_Bounding_the_True_Error.html">1 nips-2001-(Not) Bounding the True Error</a></p>
<p>11 0.10743587 <a title="138-tfidf-11" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>12 0.09356717 <a title="138-tfidf-12" href="./nips-2001-On_the_Concentration_of_Spectral_Properties.html">136 nips-2001-On the Concentration of Spectral Properties</a></p>
<p>13 0.090986922 <a title="138-tfidf-13" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>14 0.089722671 <a title="138-tfidf-14" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>15 0.082885541 <a title="138-tfidf-15" href="./nips-2001-Means%2C_Correlations_and_Bounds.html">119 nips-2001-Means, Correlations and Bounds</a></p>
<p>16 0.079295367 <a title="138-tfidf-16" href="./nips-2001-A_kernel_method_for_multi-labelled_classification.html">22 nips-2001-A kernel method for multi-labelled classification</a></p>
<p>17 0.078567147 <a title="138-tfidf-17" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>18 0.076732062 <a title="138-tfidf-18" href="./nips-2001-Learning_from_Infinite_Data_in_Finite_Time.html">114 nips-2001-Learning from Infinite Data in Finite Time</a></p>
<p>19 0.075034603 <a title="138-tfidf-19" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>20 0.074401125 <a title="138-tfidf-20" href="./nips-2001-Pranking_with_Ranking.html">147 nips-2001-Pranking with Ranking</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.236), (1, 0.134), (2, 0.047), (3, 0.202), (4, 0.158), (5, -0.136), (6, 0.061), (7, 0.193), (8, -0.035), (9, -0.027), (10, -0.073), (11, 0.16), (12, 0.027), (13, 0.097), (14, -0.133), (15, 0.015), (16, 0.011), (17, 0.011), (18, -0.053), (19, 0.012), (20, 0.017), (21, -0.014), (22, 0.086), (23, 0.002), (24, -0.026), (25, -0.012), (26, -0.07), (27, 0.008), (28, 0.009), (29, -0.104), (30, 0.011), (31, 0.013), (32, 0.087), (33, 0.056), (34, -0.069), (35, 0.002), (36, -0.028), (37, 0.081), (38, 0.021), (39, -0.119), (40, 0.195), (41, 0.231), (42, 0.037), (43, 0.019), (44, -0.005), (45, 0.026), (46, 0.017), (47, -0.074), (48, -0.009), (49, 0.11)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96525043 <a title="138-lsi-1" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>Author: Nicolò Cesa-bianchi, Alex Conconi, Claudio Gentile</p><p>Abstract: In this paper we show that on-line algorithms for classiﬁcation and regression can be naturally used to obtain hypotheses with good datadependent tail bounds on their risk. Our results are proven without requiring complicated concentration-of-measure arguments and they hold for arbitrary on-line learning algorithms. Furthermore, when applied to concrete on-line algorithms, our results yield tail bounds that in many cases are comparable or better than the best known bounds.</p><p>2 0.74880081 <a title="138-lsi-2" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>Author: Jyrki Kivinen, Alex J. Smola, Robert C. Williamson</p><p>Abstract: We consider online learning in a Reproducing Kernel Hilbert Space. Our method is computationally efﬁcient and leads to simple algorithms. In particular we derive update equations for classiﬁcation, regression, and novelty detection. The inclusion of the -trick allows us to give a robust parameterization. Moreover, unlike in batch learning where the -trick only applies to the -insensitive loss function we are able to derive general trimmed-mean types of estimators such as for Huber’s robust loss.     ¡</p><p>3 0.73002797 <a title="138-lsi-3" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>Author: Ralf Herbrich, Robert C. Williamson</p><p>Abstract: In contrast to standard statistical learning theory which studies uniform bounds on the expected error we present a framework that exploits the specific learning algorithm used. Motivated by the luckiness framework [8] we are also able to exploit the serendipity of the training sample. The main difference to previous approaches lies in the complexity measure; rather than covering all hypotheses in a given hypothesis space it is only necessary to cover the functions which could have been learned using the fixed learning algorithm. We show how the resulting framework relates to the VC, luckiness and compression frameworks. Finally, we present an application of this framework to the maximum margin algorithm for linear classifiers which results in a bound that exploits both the margin and the distribution of the data in feature space. 1</p><p>4 0.65168411 <a title="138-lsi-4" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>Author: Gunnar Rätsch, Sebastian Mika, Manfred K. Warmuth</p><p>Abstract: We give an uniﬁed convergence analysis of ensemble learning methods including e.g. AdaBoost, Logistic Regression and the Least-SquareBoost algorithm for regression. These methods have in common that they iteratively call a base learning algorithm which returns hypotheses that are then linearly combined. We show that these methods are related to the Gauss-Southwell method known from numerical optimization and state non-asymptotical convergence results for all these methods. Our analysis includes -norm regularized cost functions leading to a clean and general way to regularize ensemble learning.</p><p>5 0.62708467 <a title="138-lsi-5" href="./nips-2001-Efficiency_versus_Convergence_of_Boolean_Kernels_for_On-Line_Learning_Algorithms.html">66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</a></p>
<p>Author: Roni Khardon, Dan Roth, Rocco A. Servedio</p><p>Abstract: We study online learning in Boolean domains using kernels which capture feature expansions equivalent to using conjunctions over basic features. We demonstrate a tradeoff between the computational efﬁciency with which these kernels can be computed and the generalization ability of the resulting classiﬁer. We ﬁrst describe several kernel functions which capture either limited forms of conjunctions or all conjunctions. We show that these kernels can be used to efﬁciently run the Perceptron algorithm over an exponential number of conjunctions; however we also prove that using such kernels the Perceptron algorithm can make an exponential number of mistakes even when learning simple functions. We also consider an analogous use of kernel functions to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions. While known upper bounds imply that Winnow can learn DNF formulae with a polynomial mistake bound in this setting, we prove that it is computationally hard to simulate Winnow’s behavior for learning DNF over such a feature set, and thus that such kernel functions for Winnow are not efﬁciently computable.</p><p>6 0.59635973 <a title="138-lsi-6" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>7 0.54826653 <a title="138-lsi-7" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>8 0.51845145 <a title="138-lsi-8" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<p>9 0.51514578 <a title="138-lsi-9" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>10 0.48743692 <a title="138-lsi-10" href="./nips-2001-Learning_from_Infinite_Data_in_Finite_Time.html">114 nips-2001-Learning from Infinite Data in Finite Time</a></p>
<p>11 0.37143856 <a title="138-lsi-11" href="./nips-2001-%28Not%29_Bounding_the_True_Error.html">1 nips-2001-(Not) Bounding the True Error</a></p>
<p>12 0.37100953 <a title="138-lsi-12" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>13 0.36925924 <a title="138-lsi-13" href="./nips-2001-On_the_Concentration_of_Spectral_Properties.html">136 nips-2001-On the Concentration of Spectral Properties</a></p>
<p>14 0.35768482 <a title="138-lsi-14" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>15 0.3399581 <a title="138-lsi-15" href="./nips-2001-Kernel_Logistic_Regression_and_the_Import_Vector_Machine.html">104 nips-2001-Kernel Logistic Regression and the Import Vector Machine</a></p>
<p>16 0.329487 <a title="138-lsi-16" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>17 0.32707936 <a title="138-lsi-17" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>18 0.32693195 <a title="138-lsi-18" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>19 0.32309353 <a title="138-lsi-19" href="./nips-2001-Estimating_Car_Insurance_Premia%3A_a_Case_Study_in_High-Dimensional_Data_Inference.html">70 nips-2001-Estimating Car Insurance Premia: a Case Study in High-Dimensional Data Inference</a></p>
<p>20 0.31948194 <a title="138-lsi-20" href="./nips-2001-A_kernel_method_for_multi-labelled_classification.html">22 nips-2001-A kernel method for multi-labelled classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.237), (14, 0.088), (17, 0.042), (19, 0.045), (27, 0.124), (30, 0.072), (36, 0.016), (38, 0.021), (59, 0.028), (70, 0.018), (72, 0.065), (79, 0.032), (83, 0.022), (91, 0.109)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84533197 <a title="138-lda-1" href="./nips-2001-Estimating_the_Reliability_of_ICA_Projections.html">71 nips-2001-Estimating the Reliability of ICA Projections</a></p>
<p>Author: Frank C. Meinecke, Andreas Ziehe, Motoaki Kawanabe, Klaus-Robert Müller</p><p>Abstract: When applying unsupervised learning techniques like ICA or temporal decorrelation, a key question is whether the discovered projections are reliable. In other words: can we give error bars or can we assess the quality of our separation? We use resampling methods to tackle these questions and show experimentally that our proposed variance estimations are strongly correlated to the separation error. We demonstrate that this reliability estimation can be used to choose the appropriate ICA-model, to enhance significantly the separation performance, and, most important, to mark the components that have a actual physical meaning. Application to 49-channel-data from an magneto encephalography (MEG) experiment underlines the usefulness of our approach. 1</p><p>same-paper 2 0.84422052 <a title="138-lda-2" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>Author: Nicolò Cesa-bianchi, Alex Conconi, Claudio Gentile</p><p>Abstract: In this paper we show that on-line algorithms for classiﬁcation and regression can be naturally used to obtain hypotheses with good datadependent tail bounds on their risk. Our results are proven without requiring complicated concentration-of-measure arguments and they hold for arbitrary on-line learning algorithms. Furthermore, when applied to concrete on-line algorithms, our results yield tail bounds that in many cases are comparable or better than the best known bounds.</p><p>3 0.66344559 <a title="138-lda-3" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>4 0.6565069 <a title="138-lda-4" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>5 0.65434849 <a title="138-lda-5" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>Author: Nello Cristianini, John Shawe-Taylor, André Elisseeff, Jaz S. Kandola</p><p>Abstract: We introduce the notion of kernel-alignment, a measure of similarity between two kernel functions or between a kernel and a target function. This quantity captures the degree of agreement between a kernel and a given learning task, and has very natural interpretations in machine learning, leading also to simple algorithms for model selection and learning. We analyse its theoretical properties, proving that it is sharply concentrated around its expected value, and we discuss its relation with other standard measures of performance. Finally we describe some of the algorithms that can be obtained within this framework, giving experimental results showing that adapting the kernel to improve alignment on the labelled data significantly increases the alignment on the test set, giving improved classification accuracy. Hence, the approach provides a principled method of performing transduction. Keywords: Kernels, alignment, eigenvectors, eigenvalues, transduction 1</p><p>6 0.65294325 <a title="138-lda-6" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>7 0.65048891 <a title="138-lda-7" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>8 0.64973557 <a title="138-lda-8" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>9 0.64405334 <a title="138-lda-9" href="./nips-2001-Quantizing_Density_Estimators.html">155 nips-2001-Quantizing Density Estimators</a></p>
<p>10 0.64403248 <a title="138-lda-10" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>11 0.64204818 <a title="138-lda-11" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>12 0.6399619 <a title="138-lda-12" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>13 0.63914037 <a title="138-lda-13" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>14 0.63806075 <a title="138-lda-14" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>15 0.637725 <a title="138-lda-15" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>16 0.63749278 <a title="138-lda-16" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>17 0.63724625 <a title="138-lda-17" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>18 0.6369592 <a title="138-lda-18" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>19 0.63662595 <a title="138-lda-19" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>20 0.63561666 <a title="138-lda-20" href="./nips-2001-A_kernel_method_for_multi-labelled_classification.html">22 nips-2001-A kernel method for multi-labelled classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
